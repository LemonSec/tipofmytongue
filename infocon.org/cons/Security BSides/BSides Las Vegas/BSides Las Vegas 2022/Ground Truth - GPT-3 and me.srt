1
00:00:00,000 --> 00:00:03,600
we have young Ho Lee and Joshua sacks

2
00:00:03,600 --> 00:00:06,540
from sofos and the topic today is gpt3

3
00:00:06,540 --> 00:00:09,900
and me how supercomputer scale neural

4
00:00:09,900 --> 00:00:12,300
network models apply to defensive cyber

5
00:00:12,300 --> 00:00:14,759
security problems please welcome Young

6
00:00:14,759 --> 00:00:17,719
Who and Josh

7
00:00:30,119 --> 00:00:32,238
okay does anybody hear me without

8
00:00:32,238 --> 00:00:34,559
straining to understand what I'm saying

9
00:00:34,559 --> 00:00:37,260
yeah okay cool I'll try to talk close to

10
00:00:37,260 --> 00:00:38,460
the mic which is a little awkward

11
00:00:38,460 --> 00:00:40,079
because I'm tall

12
00:00:40,079 --> 00:00:42,600
um okay oh the other thing was

13
00:00:42,600 --> 00:00:45,780
is a laser pointer here isn't there but

14
00:00:45,780 --> 00:00:46,559
I'm

15
00:00:46,559 --> 00:00:49,579
remind me what button

16
00:00:53,160 --> 00:00:56,760
oh yeah oh awesome okay that's great

17
00:00:56,760 --> 00:01:00,600
okay so um I'm here with young Huli

18
00:01:00,600 --> 00:01:02,879
um we're gonna co-present here

19
00:01:02,879 --> 00:01:05,519
um and um I want to say first I want to

20
00:01:05,519 --> 00:01:06,960
say thanks to Jung who he flew here from

21
00:01:06,960 --> 00:01:08,939
Sydney in the last 20 hours and just

22
00:01:08,939 --> 00:01:11,400
arrives uh like an hour ago so yeah

23
00:01:11,400 --> 00:01:14,820
happy he got here on time and uh yeah

24
00:01:14,820 --> 00:01:16,920
um yeah looking forward to presenting

25
00:01:16,920 --> 00:01:20,220
together so our talk today is entitled

26
00:01:20,220 --> 00:01:22,320
um gbc3 and me how super computer scale

27
00:01:22,320 --> 00:01:24,000
neural network models apply to defensive

28
00:01:24,000 --> 00:01:26,220
cyber security problems

29
00:01:26,220 --> 00:01:28,320
um a little bit about us

30
00:01:28,320 --> 00:01:30,060
um Josh sacks I'm Chief scientist at

31
00:01:30,060 --> 00:01:31,740
sofos

32
00:01:31,740 --> 00:01:34,020
um if you're in if you like what you

33
00:01:34,020 --> 00:01:36,479
hear today uh you're welcome to check

34
00:01:36,479 --> 00:01:38,280
out our book malware data science I

35
00:01:38,280 --> 00:01:39,720
wrote this with Hillary Sanders it's

36
00:01:39,720 --> 00:01:41,579
about malware data science and security

37
00:01:41,579 --> 00:01:43,259
data science more generally generally

38
00:01:43,259 --> 00:01:45,000
I've been doing

39
00:01:45,000 --> 00:01:45,659
um

40
00:01:45,659 --> 00:01:48,180
cyber security machine learning research

41
00:01:48,180 --> 00:01:50,579
for a long time now um Yahoo you want to

42
00:01:50,579 --> 00:01:53,839
introduce yourself quickly

43
00:01:54,240 --> 00:01:56,719
yeah good afternoon everyone my name is

44
00:01:56,719 --> 00:02:00,840
I'm a senior uh sorry recently I promote

45
00:02:00,840 --> 00:02:05,659
I'm priest for research scientists yeah

46
00:02:06,420 --> 00:02:07,560
thanks

47
00:02:07,560 --> 00:02:10,440
okay so I'm gonna lay out right off the

48
00:02:10,440 --> 00:02:13,800
bat the the main arguments of this talk

49
00:02:13,800 --> 00:02:15,959
um so first and those of you who came to

50
00:02:15,959 --> 00:02:17,879
my morning talk you heard me hint at

51
00:02:17,879 --> 00:02:20,879
this a little bit um I think two two

52
00:02:20,879 --> 00:02:24,660
ideas that are new slash gaining

53
00:02:24,660 --> 00:02:26,180
popularity and machine learning

54
00:02:26,180 --> 00:02:28,800
self-supervised learning and model scale

55
00:02:28,800 --> 00:02:31,860
are fundamentally changing machine

56
00:02:31,860 --> 00:02:33,180
learning

57
00:02:33,180 --> 00:02:34,680
um and what its capabilities are I'm

58
00:02:34,680 --> 00:02:35,819
going to talk about the implications of

59
00:02:35,819 --> 00:02:38,099
that sport Security today and so is Yung

60
00:02:38,099 --> 00:02:39,239
who

61
00:02:39,239 --> 00:02:42,180
um second I think that uh for those of

62
00:02:42,180 --> 00:02:43,200
us who are security data science

63
00:02:43,200 --> 00:02:44,700
researchers here

64
00:02:44,700 --> 00:02:46,560
I think a new

65
00:02:46,560 --> 00:02:48,360
um important topic on our research

66
00:02:48,360 --> 00:02:50,580
agenda should be figuring out uh how

67
00:02:50,580 --> 00:02:53,099
large models apply large self-supervised

68
00:02:53,099 --> 00:02:55,379
trains models apply to security I think

69
00:02:55,379 --> 00:02:58,080
that's a problem for us to figure out uh

70
00:02:58,080 --> 00:03:01,340
over the next uh one to five ten years

71
00:03:01,340 --> 00:03:03,959
and hopefully I'll convince you of why

72
00:03:03,959 --> 00:03:07,080
and civil Yahoo in this talk

73
00:03:07,080 --> 00:03:08,879
um thirds I think the proof of Concepts

74
00:03:08,879 --> 00:03:10,560
that the Yahoo is going to show that are

75
00:03:10,560 --> 00:03:12,060
that are built on large language models

76
00:03:12,060 --> 00:03:15,780
and specifically gpt3 uh really show

77
00:03:15,780 --> 00:03:17,879
show promise um really prove out the

78
00:03:17,879 --> 00:03:20,340
thesis that large uh large models are

79
00:03:20,340 --> 00:03:22,500
going to be important for our Fields

80
00:03:22,500 --> 00:03:24,000
um and then lastly I'll just this is not

81
00:03:24,000 --> 00:03:26,220
so much a thesis it's just a caveat

82
00:03:26,220 --> 00:03:27,840
um what we're presenting here today is

83
00:03:27,840 --> 00:03:30,120
early work meant to wet the appetite of

84
00:03:30,120 --> 00:03:31,980
our community around it around the

85
00:03:31,980 --> 00:03:34,440
potential impact of large models

86
00:03:34,440 --> 00:03:36,000
um let me ask how many of you know what

87
00:03:36,000 --> 00:03:38,700
gpt3 is when I use that acronym okay so

88
00:03:38,700 --> 00:03:40,019
I'm going to use that not everybody

89
00:03:40,019 --> 00:03:41,760
raise their hands so

90
00:03:41,760 --> 00:03:44,519
um this is a kind of iconic iconic large

91
00:03:44,519 --> 00:03:47,400
machine learning model that's that's

92
00:03:47,400 --> 00:03:49,200
um been built by an organization called

93
00:03:49,200 --> 00:03:50,580
openai

94
00:03:50,580 --> 00:03:52,860
um it's it's it can't be run on

95
00:03:52,860 --> 00:03:55,980
commodity uh systems you need like a

96
00:03:55,980 --> 00:03:57,599
very large supercomputer to run this

97
00:03:57,599 --> 00:03:58,500
model

98
00:03:58,500 --> 00:04:00,420
um but it's it's showing it's it's

99
00:04:00,420 --> 00:04:02,220
showing some really uh novel

100
00:04:02,220 --> 00:04:03,480
capabilities with respect to natural

101
00:04:03,480 --> 00:04:05,280
language processing um and you'll learn

102
00:04:05,280 --> 00:04:08,099
more about it in the course of this talk

103
00:04:08,099 --> 00:04:09,720
um and then finally by way of just

104
00:04:09,720 --> 00:04:11,099
summarizing what we're going to say up

105
00:04:11,099 --> 00:04:13,379
front at the beginning of this talk

106
00:04:13,379 --> 00:04:14,939
um young who's going to demonstrate a

107
00:04:14,939 --> 00:04:19,320
couple key results in our research first

108
00:04:19,320 --> 00:04:21,418
he's going to show that experiments

109
00:04:21,418 --> 00:04:23,340
we've developed at sofos where we both

110
00:04:23,340 --> 00:04:24,600
work

111
00:04:24,600 --> 00:04:25,380
um

112
00:04:25,380 --> 00:04:27,720
demonstrate that GPT 3 can do something

113
00:04:27,720 --> 00:04:31,380
fundamentally new which is

114
00:04:31,380 --> 00:04:33,120
successfully execute a small reverse

115
00:04:33,120 --> 00:04:34,860
engineering task which is to take

116
00:04:34,860 --> 00:04:37,979
command lines like this which stock

117
00:04:37,979 --> 00:04:39,840
analysts stock analysts at sofos and

118
00:04:39,840 --> 00:04:41,639
many other places spend their days

119
00:04:41,639 --> 00:04:44,699
looking through and translate uh

120
00:04:44,699 --> 00:04:47,460
translate them into clear natural clear

121
00:04:47,460 --> 00:04:48,479
and accurate natural language

122
00:04:48,479 --> 00:04:50,280
descriptions like this description on

123
00:04:50,280 --> 00:04:53,040
the right so here I mean if you just if

124
00:04:53,040 --> 00:04:55,380
you just sort of introspect and notice

125
00:04:55,380 --> 00:04:58,259
the cognitive load of trying to parse a

126
00:04:58,259 --> 00:04:59,940
command line like this you'll notice

127
00:04:59,940 --> 00:05:01,380
that even those of us who look at a lot

128
00:05:01,380 --> 00:05:03,000
of command list lines like this you know

129
00:05:03,000 --> 00:05:04,560
take some exertion to figure out what

130
00:05:04,560 --> 00:05:06,419
they mean and it's significant that we

131
00:05:06,419 --> 00:05:08,639
found a method for using a model like

132
00:05:08,639 --> 00:05:11,340
gpt3 to translate that into clearly

133
00:05:11,340 --> 00:05:12,960
written text which explains what's going

134
00:05:12,960 --> 00:05:14,580
on if you imagine the job of a sock

135
00:05:14,580 --> 00:05:15,960
analyst is looking through hundreds of

136
00:05:15,960 --> 00:05:17,699
these suspicious command lines a day

137
00:05:17,699 --> 00:05:20,160
it's significant that a large a large

138
00:05:20,160 --> 00:05:24,000
language model can be put to use and in

139
00:05:24,000 --> 00:05:25,380
translating that into intelligible

140
00:05:25,380 --> 00:05:26,400
English

141
00:05:26,400 --> 00:05:28,860
uh that the second result

142
00:05:28,860 --> 00:05:30,660
that um young who's really going to Deep

143
00:05:30,660 --> 00:05:32,039
dive into which I think is really

144
00:05:32,039 --> 00:05:34,740
significant is in a context in which you

145
00:05:34,740 --> 00:05:37,199
have very few training examples uh so

146
00:05:37,199 --> 00:05:39,380
let's say you are attempting to detect

147
00:05:39,380 --> 00:05:41,639
phishing emails on a customer Network

148
00:05:41,639 --> 00:05:43,740
and you have just a handful of benign

149
00:05:43,740 --> 00:05:46,500
examples and and a handful of email

150
00:05:46,500 --> 00:05:49,199
examples from a malicious campaign uh

151
00:05:49,199 --> 00:05:51,900
you we can use gpt3 to achieve a

152
00:05:51,900 --> 00:05:54,240
Deployable level of accuracy uh in

153
00:05:54,240 --> 00:05:56,699
detecting other malicious emails so

154
00:05:56,699 --> 00:05:58,020
young who's going to show young who's

155
00:05:58,020 --> 00:05:59,819
going to drill into this more deeply but

156
00:05:59,819 --> 00:06:01,620
basically we can achieve on an accuracy

157
00:06:01,620 --> 00:06:04,979
metric like f-score we can achieve uh

158
00:06:04,979 --> 00:06:07,560
like a 0.9 or 0.95 results with a tiny

159
00:06:07,560 --> 00:06:08,819
handful of training examples whereas

160
00:06:08,819 --> 00:06:10,080
traditional machine learning just as a

161
00:06:10,080 --> 00:06:11,580
non-starter here you know it achieves a

162
00:06:11,580 --> 00:06:14,460
non-usable uh detection accuracy so

163
00:06:14,460 --> 00:06:16,440
really these two key results are meant

164
00:06:16,440 --> 00:06:19,380
to provoke interest uh here and in

165
00:06:19,380 --> 00:06:20,639
applying large language models to

166
00:06:20,639 --> 00:06:22,560
defensive cyber security problems and I

167
00:06:22,560 --> 00:06:24,180
think suggest that more investment in

168
00:06:24,180 --> 00:06:25,500
the space is necessary and I think are

169
00:06:25,500 --> 00:06:27,120
significant in their own right in terms

170
00:06:27,120 --> 00:06:28,560
of what we're building in our data

171
00:06:28,560 --> 00:06:31,259
science sofos

172
00:06:31,259 --> 00:06:33,240
okay so the format of this talk is I'm

173
00:06:33,240 --> 00:06:34,560
going to give a bunch of background into

174
00:06:34,560 --> 00:06:36,660
large large models and then young who's

175
00:06:36,660 --> 00:06:38,280
going to drill into the examples that I

176
00:06:38,280 --> 00:06:40,758
just talked about

177
00:06:41,819 --> 00:06:44,340
okay so here's an important Trend

178
00:06:44,340 --> 00:06:46,740
um and um

179
00:06:46,740 --> 00:06:49,380
uh got this from the website towards

180
00:06:49,380 --> 00:06:51,240
data science which which tracks a bunch

181
00:06:51,240 --> 00:06:52,800
of interesting metrics there's a sort of

182
00:06:52,800 --> 00:06:54,000
meta-analysis of the scientific

183
00:06:54,000 --> 00:06:55,860
literature and machine learning

184
00:06:55,860 --> 00:06:59,460
um uh and and what this plot shows is

185
00:06:59,460 --> 00:07:00,960
that over time you know some of the

186
00:07:00,960 --> 00:07:03,720
horizontal axis we have time here

187
00:07:03,720 --> 00:07:04,639
um

188
00:07:04,639 --> 00:07:07,620
uh machine learning papers are proposing

189
00:07:07,620 --> 00:07:09,660
larger and larger models

190
00:07:09,660 --> 00:07:11,759
um and this was sort of a log linear

191
00:07:11,759 --> 00:07:13,800
relationship between time and model size

192
00:07:13,800 --> 00:07:15,360
for a long time and now all of a sudden

193
00:07:15,360 --> 00:07:17,580
we just have this explosion in large

194
00:07:17,580 --> 00:07:22,380
models being proposed by researchers

195
00:07:22,560 --> 00:07:24,780
uh so just just to hang on to that fact

196
00:07:24,780 --> 00:07:26,160
that models are getting larger and

197
00:07:26,160 --> 00:07:27,060
they're getting larger at an

198
00:07:27,060 --> 00:07:28,680
accelerating rate

199
00:07:28,680 --> 00:07:30,240
um

200
00:07:30,240 --> 00:07:32,220
and and then then I want to put in I'm

201
00:07:32,220 --> 00:07:33,240
going to come back to that but I want to

202
00:07:33,240 --> 00:07:36,360
point to another Trend here as well

203
00:07:36,360 --> 00:07:38,220
um which is

204
00:07:38,220 --> 00:07:40,380
um we're also seeing these models being

205
00:07:40,380 --> 00:07:43,319
trained under a newly popular regime

206
00:07:43,319 --> 00:07:45,660
which is a self-supervised regime

207
00:07:45,660 --> 00:07:47,819
um so self-supervised learning I'll get

208
00:07:47,819 --> 00:07:49,979
to in a second but basically it's reason

209
00:07:49,979 --> 00:07:54,120
for being is that it helps to exploit

210
00:07:54,120 --> 00:07:55,860
the internet scale data sets that are

211
00:07:55,860 --> 00:07:57,180
becoming available to machine learning

212
00:07:57,180 --> 00:07:58,620
researchers so

213
00:07:58,620 --> 00:08:00,120
um you know you have this non-linear

214
00:08:00,120 --> 00:08:01,740
Trend and the amount of data being being

215
00:08:01,740 --> 00:08:03,720
generated by by Computing devices and

216
00:08:03,720 --> 00:08:05,060
available on the internet

217
00:08:05,060 --> 00:08:06,960
self-supervised learning helps solve the

218
00:08:06,960 --> 00:08:08,879
problem for machine learning researchers

219
00:08:08,879 --> 00:08:10,560
which is that most of those data are

220
00:08:10,560 --> 00:08:11,819
unlabeled

221
00:08:11,819 --> 00:08:13,020
um so we can't use traditional

222
00:08:13,020 --> 00:08:14,400
supervised machine learning approaches

223
00:08:14,400 --> 00:08:16,919
to take advantage of them and

224
00:08:16,919 --> 00:08:18,660
self-supervised learning allows machine

225
00:08:18,660 --> 00:08:20,759
learning models to improve even while

226
00:08:20,759 --> 00:08:22,919
they're unlabeled so so here's here's

227
00:08:22,919 --> 00:08:24,240
what here's what self-supervising

228
00:08:24,240 --> 00:08:25,919
learning tell here's what

229
00:08:25,919 --> 00:08:28,139
self-supervising self-supervised

230
00:08:28,139 --> 00:08:31,860
learning uh is so basically what is

231
00:08:31,860 --> 00:08:33,659
self-supervised learning

232
00:08:33,659 --> 00:08:35,458
approach does it takes it takes

233
00:08:35,458 --> 00:08:37,260
unlabeled data

234
00:08:37,260 --> 00:08:38,760
um and creates a kind of artificial

235
00:08:38,760 --> 00:08:41,279
prediction task on that data so for

236
00:08:41,279 --> 00:08:42,719
example

237
00:08:42,719 --> 00:08:45,120
um one approach to taking advantage of

238
00:08:45,120 --> 00:08:46,620
large image data sets where we don't

239
00:08:46,620 --> 00:08:48,420
have prediction targets like so where we

240
00:08:48,420 --> 00:08:51,060
don't have uh say labels that say you

241
00:08:51,060 --> 00:08:54,300
know this is a picture of a primate is

242
00:08:54,300 --> 00:08:56,700
you just create a game right like remove

243
00:08:56,700 --> 00:08:59,700
a patch from the image and then have the

244
00:08:59,700 --> 00:09:01,260
machine learning try to fill in the

245
00:09:01,260 --> 00:09:03,180
pixels that we've removed and then just

246
00:09:03,180 --> 00:09:04,920
repeat that over and over again for a

247
00:09:04,920 --> 00:09:07,140
given image so you know remove a patch

248
00:09:07,140 --> 00:09:08,940
here and predict it remove a patch here

249
00:09:08,940 --> 00:09:10,740
and predict it remove a patch here and

250
00:09:10,740 --> 00:09:12,000
predict it

251
00:09:12,000 --> 00:09:14,339
um and by training a machine learning

252
00:09:14,339 --> 00:09:16,800
model on like web scale image data sets

253
00:09:16,800 --> 00:09:19,140
to solve this sort of artificial task of

254
00:09:19,140 --> 00:09:21,120
filling in the missing pixels it turns

255
00:09:21,120 --> 00:09:22,920
out that

256
00:09:22,920 --> 00:09:25,320
um the model learns useful

257
00:09:25,320 --> 00:09:27,300
representations of images

258
00:09:27,300 --> 00:09:28,980
um because if you think about it to

259
00:09:28,980 --> 00:09:30,180
solve the problem of predicting the

260
00:09:30,180 --> 00:09:32,160
missing patch here

261
00:09:32,160 --> 00:09:34,320
um maybe this is a better example uh the

262
00:09:34,320 --> 00:09:36,000
primates face

263
00:09:36,000 --> 00:09:36,899
um

264
00:09:36,899 --> 00:09:39,180
uh the model really has to learn a lot

265
00:09:39,180 --> 00:09:41,519
of useful semantics around what images

266
00:09:41,519 --> 00:09:44,040
actually mean

267
00:09:44,040 --> 00:09:46,200
um it takes to really solve this problem

268
00:09:46,200 --> 00:09:48,000
of

269
00:09:48,000 --> 00:09:49,500
um knowing what those pixels will look

270
00:09:49,500 --> 00:09:51,959
like means understanding

271
00:09:51,959 --> 00:09:55,019
um what an Apes face looks like and

272
00:09:55,019 --> 00:09:57,240
inferring that the relationship between

273
00:09:57,240 --> 00:09:59,160
the body of the ape here and the pixels

274
00:09:59,160 --> 00:10:01,019
in the Apes face and it just turns out

275
00:10:01,019 --> 00:10:03,779
that a model trained in this way on

276
00:10:03,779 --> 00:10:05,959
self-supervised data can then be used

277
00:10:05,959 --> 00:10:09,120
Downstream uh to solve image

278
00:10:09,120 --> 00:10:11,279
classification problems for example uh

279
00:10:11,279 --> 00:10:13,620
better than a model that just

280
00:10:13,620 --> 00:10:16,880
um that wasn't trained in that way

281
00:10:17,640 --> 00:10:19,620
okay now

282
00:10:19,620 --> 00:10:21,120
um so we're getting all these models

283
00:10:21,120 --> 00:10:22,860
that are very large and trained in the

284
00:10:22,860 --> 00:10:24,420
self-supervised fashion outside of

285
00:10:24,420 --> 00:10:26,399
security today's security data science

286
00:10:26,399 --> 00:10:28,079
models really live in this kind of size

287
00:10:28,079 --> 00:10:29,100
range

288
00:10:29,100 --> 00:10:30,660
um so in this access we have we have

289
00:10:30,660 --> 00:10:33,120
size uh expressed in parameters

290
00:10:33,120 --> 00:10:35,339
um the models we deploy within um the

291
00:10:35,339 --> 00:10:36,540
data science team that I manage at

292
00:10:36,540 --> 00:10:38,519
Sophos are all really within this range

293
00:10:38,519 --> 00:10:40,680
and that's notable because what's

294
00:10:40,680 --> 00:10:41,640
happening in the research Community

295
00:10:41,640 --> 00:10:44,100
outside of security is is folks are

296
00:10:44,100 --> 00:10:45,480
finding that these models in this much

297
00:10:45,480 --> 00:10:48,959
larger size range are more efficacious

298
00:10:48,959 --> 00:10:50,160
um so this raises the question how can

299
00:10:50,160 --> 00:10:51,600
we take advantage of self-supervised

300
00:10:51,600 --> 00:10:54,060
training and large models and web scale

301
00:10:54,060 --> 00:10:56,940
data in order to get the boosts and

302
00:10:56,940 --> 00:10:58,800
results that secure that researchers in

303
00:10:58,800 --> 00:11:00,180
the image domain and the audio demand

304
00:11:00,180 --> 00:11:01,440
and language domain outside of security

305
00:11:01,440 --> 00:11:04,339
are getting

306
00:11:05,339 --> 00:11:06,959
um and so you know that's what this talk

307
00:11:06,959 --> 00:11:09,420
goes towards is you know we are

308
00:11:09,420 --> 00:11:11,640
experimenting with gpt3 in this talk

309
00:11:11,640 --> 00:11:13,440
which is about this size so it's really

310
00:11:13,440 --> 00:11:15,660
up there um on the Vanguard of model

311
00:11:15,660 --> 00:11:17,459
scales uh

312
00:11:17,459 --> 00:11:19,019
um in terms of models being used in the

313
00:11:19,019 --> 00:11:20,640
machine learning community

314
00:11:20,640 --> 00:11:22,740
so I want to dramatize just a little bit

315
00:11:22,740 --> 00:11:24,240
more

316
00:11:24,240 --> 00:11:27,079
um what I mean when I talk about

317
00:11:27,079 --> 00:11:29,339
large-scale models trained on

318
00:11:29,339 --> 00:11:30,839
self-supervised learning tasks getting

319
00:11:30,839 --> 00:11:33,540
better results so here I'm going to show

320
00:11:33,540 --> 00:11:37,260
some results from a model uh trained by

321
00:11:37,260 --> 00:11:39,120
a team at Google this is called the

322
00:11:39,120 --> 00:11:40,640
party model

323
00:11:40,640 --> 00:11:42,899
that's very large scale and was trained

324
00:11:42,899 --> 00:11:45,540
on like a web scale image data sets

325
00:11:45,540 --> 00:11:47,279
um so what this model was trying to do

326
00:11:47,279 --> 00:11:51,360
is solve that was given a given an image

327
00:11:51,360 --> 00:11:53,040
caption like the alt text that you find

328
00:11:53,040 --> 00:11:55,040
in images on the internet

329
00:11:55,040 --> 00:11:58,620
predict the pixels and the paired image

330
00:11:58,620 --> 00:12:02,220
so here we've got the caption a portrait

331
00:12:02,220 --> 00:12:04,320
photo of a kangaroo wearing an orange

332
00:12:04,320 --> 00:12:06,600
hoodie and blue sunglasses standing on

333
00:12:06,600 --> 00:12:08,040
the grass in front of the Sydney Opera

334
00:12:08,040 --> 00:12:09,959
House holding a sign on on the chest

335
00:12:09,959 --> 00:12:11,820
that says Welcome Friends

336
00:12:11,820 --> 00:12:13,200
um so to be clear this image doesn't

337
00:12:13,200 --> 00:12:15,240
actually exist on the internet what

338
00:12:15,240 --> 00:12:16,440
these researchers weren't trying to do

339
00:12:16,440 --> 00:12:18,180
with the sentence is see whether or not

340
00:12:18,180 --> 00:12:20,160
the model had learned so much about the

341
00:12:20,160 --> 00:12:21,540
relationship between text and imagery

342
00:12:21,540 --> 00:12:24,240
they was able to generalize and

343
00:12:24,240 --> 00:12:27,000
create a convincing portrait of uh you

344
00:12:27,000 --> 00:12:28,139
know a kangaroo with all these

345
00:12:28,139 --> 00:12:29,399
attributes and this in the Opera House

346
00:12:29,399 --> 00:12:30,660
in the background

347
00:12:30,660 --> 00:12:32,459
um now here's the interesting result

348
00:12:32,459 --> 00:12:33,959
right so this this model was trained in

349
00:12:33,959 --> 00:12:35,459
the self-supervised way just to predict

350
00:12:35,459 --> 00:12:37,620
you know play the game of predicting it

351
00:12:37,620 --> 00:12:39,180
predicting image pixels based on their

352
00:12:39,180 --> 00:12:40,500
alt text

353
00:12:40,500 --> 00:12:43,980
um and with a model at the scale of 350

354
00:12:43,980 --> 00:12:47,160
million parameters and this is about uh

355
00:12:47,160 --> 00:12:49,500
uh I don't know 15 20 times larger than

356
00:12:49,500 --> 00:12:51,240
any model that we've deployed

357
00:12:51,240 --> 00:12:53,880
um within sofas products it was able to

358
00:12:53,880 --> 00:12:55,500
do like a pretty decent job right I mean

359
00:12:55,500 --> 00:12:57,240
this looks sort of kangaroo-like it's

360
00:12:57,240 --> 00:12:59,220
got the sunglasses in there it's got a

361
00:12:59,220 --> 00:13:01,380
sign it doesn't have the it doesn't have

362
00:13:01,380 --> 00:13:03,480
the The Welcome Friends text on the sign

363
00:13:03,480 --> 00:13:05,459
but there's some letters on it it's got

364
00:13:05,459 --> 00:13:08,279
a vague visual gesture and a hoodie uh

365
00:13:08,279 --> 00:13:09,779
didn't really solve the problem but it's

366
00:13:09,779 --> 00:13:11,940
impressive nonetheless

367
00:13:11,940 --> 00:13:13,620
now if we scale up to three billion

368
00:13:13,620 --> 00:13:15,180
parameters it gets more interesting

369
00:13:15,180 --> 00:13:16,620
right I mean this is this is

370
00:13:16,620 --> 00:13:19,260
unmistakably Kangaroo like this this uh

371
00:13:19,260 --> 00:13:21,540
this creature here

372
00:13:21,540 --> 00:13:24,180
um it's got the sunglasses and

373
00:13:24,180 --> 00:13:25,920
the letters are actually getting closer

374
00:13:25,920 --> 00:13:27,360
to

375
00:13:27,360 --> 00:13:30,120
um what the researchers asked for which

376
00:13:30,120 --> 00:13:31,620
is welcome friends right which is this

377
00:13:31,620 --> 00:13:33,420
is remarkable

378
00:13:33,420 --> 00:13:34,920
um I'll just say

379
00:13:34,920 --> 00:13:36,720
even at this level of success right that

380
00:13:36,720 --> 00:13:40,800
the fact that a model merely trains uh

381
00:13:40,800 --> 00:13:42,720
to predict an image based on its alt

382
00:13:42,720 --> 00:13:44,639
text actually learn to write right it

383
00:13:44,639 --> 00:13:45,839
actually learned to write some letters

384
00:13:45,839 --> 00:13:47,760
all on its own uh which I think is

385
00:13:47,760 --> 00:13:49,079
really remarkable

386
00:13:49,079 --> 00:13:51,120
um but still I mean we're still pretty

387
00:13:51,120 --> 00:13:53,040
far off from completely solving the

388
00:13:53,040 --> 00:13:55,019
problem now once we get to 20 billion

389
00:13:55,019 --> 00:13:56,639
parameters I mean this is this is at the

390
00:13:56,639 --> 00:13:57,720
scale where

391
00:13:57,720 --> 00:13:59,820
um you know you need to fill up this

392
00:13:59,820 --> 00:14:01,680
room with with compute with a

393
00:14:01,680 --> 00:14:03,839
distributed sort of GPU setup in order

394
00:14:03,839 --> 00:14:05,940
to run a model of the scale

395
00:14:05,940 --> 00:14:07,680
um it the model the model solves the

396
00:14:07,680 --> 00:14:10,139
problem uh so you know here you we

397
00:14:10,139 --> 00:14:11,339
really do have a kangaroo with blue

398
00:14:11,339 --> 00:14:13,500
sunglasses the model is has learned to

399
00:14:13,500 --> 00:14:15,240
write out Welcome Friends on its sign

400
00:14:15,240 --> 00:14:16,740
and you've got this in the Opera House

401
00:14:16,740 --> 00:14:18,000
in the in the background and what's

402
00:14:18,000 --> 00:14:19,260
noteworthy is that these capabilities

403
00:14:19,260 --> 00:14:21,600
just emerged from the model playing this

404
00:14:21,600 --> 00:14:23,040
game but predicting the pixels from from

405
00:14:23,040 --> 00:14:24,779
the alt text right

406
00:14:24,779 --> 00:14:27,300
um and uh additionally what's noteworthy

407
00:14:27,300 --> 00:14:28,740
is that the researchers here didn't

408
00:14:28,740 --> 00:14:30,660
change they didn't introduce any clever

409
00:14:30,660 --> 00:14:32,459
new mathy ideas into their model that

410
00:14:32,459 --> 00:14:33,839
they scaled it up

411
00:14:33,839 --> 00:14:34,680
um

412
00:14:34,680 --> 00:14:36,779
the model just by virtue of the fact of

413
00:14:36,779 --> 00:14:38,459
having more sort of connective tissue

414
00:14:38,459 --> 00:14:41,699
between its uh its its artificial

415
00:14:41,699 --> 00:14:43,019
neurons

416
00:14:43,019 --> 00:14:45,660
um uh just gain these capabilities uh

417
00:14:45,660 --> 00:14:47,339
just gain the capability to write and

418
00:14:47,339 --> 00:14:48,839
gain the sort of memory of what the

419
00:14:48,839 --> 00:14:50,040
Sydney Opera house looks like and what

420
00:14:50,040 --> 00:14:51,779
kangaroos look like and also learn to

421
00:14:51,779 --> 00:14:53,040
sort of compose all those Concepts

422
00:14:53,040 --> 00:14:55,939
together in an image

423
00:14:56,279 --> 00:14:58,019
um I showed this in in the talk I gave

424
00:14:58,019 --> 00:14:59,760
this morning but just another example so

425
00:14:59,760 --> 00:15:01,560
that that same model given given the

426
00:15:01,560 --> 00:15:03,000
prompt a map of the United States made

427
00:15:03,000 --> 00:15:05,279
out of sushi it is on a table next to a

428
00:15:05,279 --> 00:15:07,620
glass of red wine right it doesn't do

429
00:15:07,620 --> 00:15:10,740
well in a low parameter regime but as

430
00:15:10,740 --> 00:15:12,240
you just scale it up without changing

431
00:15:12,240 --> 00:15:15,060
anything else I learned to create a map

432
00:15:15,060 --> 00:15:16,860
based on you know with sushi and the

433
00:15:16,860 --> 00:15:18,480
wine glass and learns to put a table in

434
00:15:18,480 --> 00:15:19,920
the background

435
00:15:19,920 --> 00:15:20,820
um

436
00:15:20,820 --> 00:15:23,339
so this is really significant I think

437
00:15:23,339 --> 00:15:25,079
um it's significant for machine learning

438
00:15:25,079 --> 00:15:27,959
that we can now prompt models uh with

439
00:15:27,959 --> 00:15:29,220
natural language text and generate

440
00:15:29,220 --> 00:15:31,019
coherent imagery that really aligns with

441
00:15:31,019 --> 00:15:32,880
the semantics of the text but I think

442
00:15:32,880 --> 00:15:33,839
the question we should be asking

443
00:15:33,839 --> 00:15:35,820
ourselves as security practitioners is

444
00:15:35,820 --> 00:15:37,680
how does it apply to security

445
00:15:37,680 --> 00:15:39,120
um and the question that Yahoo and I are

446
00:15:39,120 --> 00:15:40,320
asking ourselves is how does it apply

447
00:15:40,320 --> 00:15:42,120
specifically to the defense defensive

448
00:15:42,120 --> 00:15:44,339
problems and security it's I think it's

449
00:15:44,339 --> 00:15:45,600
easier to see how it applies to

450
00:15:45,600 --> 00:15:47,040
offensive security like if you need to

451
00:15:47,040 --> 00:15:48,360
if you want to generate fake social

452
00:15:48,360 --> 00:15:50,519
media content or face fake Facebook

453
00:15:50,519 --> 00:15:52,320
profiles right you can use models like

454
00:15:52,320 --> 00:15:53,759
this to generate that kind of content

455
00:15:53,759 --> 00:15:55,440
but how do we use it in this in the sock

456
00:15:55,440 --> 00:15:56,339
and how do we use it when we're

457
00:15:56,339 --> 00:15:57,660
defending real networks is the question

458
00:15:57,660 --> 00:16:00,860
that we're asking here

459
00:16:01,320 --> 00:16:03,240
um I want to say by way of summary I

460
00:16:03,240 --> 00:16:05,279
mean I'm showing sort of qualitative

461
00:16:05,279 --> 00:16:06,959
results around what happens when we

462
00:16:06,959 --> 00:16:08,459
Scale Models up

463
00:16:08,459 --> 00:16:10,680
um they're this very famous paper that

464
00:16:10,680 --> 00:16:12,180
came out a few years ago scaling laws

465
00:16:12,180 --> 00:16:13,680
for neural language models came out of

466
00:16:13,680 --> 00:16:14,940
open AI

467
00:16:14,940 --> 00:16:16,139
um shows this really beautiful

468
00:16:16,139 --> 00:16:19,199
relationship between scale and and error

469
00:16:19,199 --> 00:16:20,160
rate

470
00:16:20,160 --> 00:16:21,839
um so here I mean they're just showing

471
00:16:21,839 --> 00:16:23,040
this is what's called a power law

472
00:16:23,040 --> 00:16:24,300
relationship

473
00:16:24,300 --> 00:16:25,860
um between

474
00:16:25,860 --> 00:16:27,899
um model error when it comes to in this

475
00:16:27,899 --> 00:16:29,279
case they looked at predicting the next

476
00:16:29,279 --> 00:16:31,019
few characters in a document based on

477
00:16:31,019 --> 00:16:32,639
the the characters that the model has

478
00:16:32,639 --> 00:16:34,380
seen so far they're able to show that

479
00:16:34,380 --> 00:16:36,120
the error rate of of models that are

480
00:16:36,120 --> 00:16:37,860
doing this autocomplete problem just

481
00:16:37,860 --> 00:16:40,800
goes down in this really smooth fashion

482
00:16:40,800 --> 00:16:42,839
um as we just merely scale up the size

483
00:16:42,839 --> 00:16:44,579
of models and this is a robust result

484
00:16:44,579 --> 00:16:46,259
that's been shown in a number of

485
00:16:46,259 --> 00:16:48,180
different domains

486
00:16:48,180 --> 00:16:50,699
so to to sort of hone in a little bit

487
00:16:50,699 --> 00:16:54,660
more on why this matters for

488
00:16:54,660 --> 00:16:57,540
um why this matters for cyber security

489
00:16:57,540 --> 00:17:00,000
here's here's an experiment I did with

490
00:17:00,000 --> 00:17:03,060
uh the gpt3 Codex model which was

491
00:17:03,060 --> 00:17:05,459
trained on lots and lots of code like

492
00:17:05,459 --> 00:17:08,160
GitHub scale code from the internet

493
00:17:08,160 --> 00:17:10,319
um so I so I wrote out this python

494
00:17:10,319 --> 00:17:14,099
function signature and Doc string so the

495
00:17:14,099 --> 00:17:15,359
function is called compute mean and

496
00:17:15,359 --> 00:17:16,919
standard deviation says two standard

497
00:17:16,919 --> 00:17:18,480
summary summary cysticks and stats

498
00:17:18,480 --> 00:17:20,400
compute and then then the doc string

499
00:17:20,400 --> 00:17:22,140
says compute mean and standard deviation

500
00:17:22,140 --> 00:17:24,119
on the input data in pure Python and

501
00:17:24,119 --> 00:17:26,280
render the result in flashy HTML on a

502
00:17:26,280 --> 00:17:27,959
page titled don't trust summary

503
00:17:27,959 --> 00:17:29,520
statistics

504
00:17:29,520 --> 00:17:33,900
and then the gpt3 auto completes I wrote

505
00:17:33,900 --> 00:17:35,280
the code correctly

506
00:17:35,280 --> 00:17:36,360
which I think is interesting because

507
00:17:36,360 --> 00:17:37,860
it's sort of like the kangaroo example

508
00:17:37,860 --> 00:17:40,320
in that um you know if Mina standard

509
00:17:40,320 --> 00:17:42,299
deviation standard deviation are the

510
00:17:42,299 --> 00:17:45,660
kangaroo and uh the HTML Parts you know

511
00:17:45,660 --> 00:17:47,760
don't trust summary statistics is is the

512
00:17:47,760 --> 00:17:49,980
hoodie right the the model sort of knew

513
00:17:49,980 --> 00:17:52,260
both of those Concepts and composed them

514
00:17:52,260 --> 00:17:53,340
together into function that will

515
00:17:53,340 --> 00:17:54,720
actually execute and do what I asked it

516
00:17:54,720 --> 00:17:55,860
to do

517
00:17:55,860 --> 00:17:57,299
um so we can see that these large models

518
00:17:57,299 --> 00:17:58,740
also understand

519
00:17:58,740 --> 00:18:00,179
um can understand the technical domain

520
00:18:00,179 --> 00:18:02,039
of python codes

521
00:18:02,039 --> 00:18:03,360
um when they get to a sufficient scale

522
00:18:03,360 --> 00:18:04,620
this model is about 200 billion

523
00:18:04,620 --> 00:18:07,320
parameters and and size so so very large

524
00:18:07,320 --> 00:18:10,860
and has very very high capacity

525
00:18:10,860 --> 00:18:12,480
um and then I apologize for folks who

526
00:18:12,480 --> 00:18:14,220
are in the morning talk I sort of force

527
00:18:14,220 --> 00:18:16,740
out of this but here's an example of

528
00:18:16,740 --> 00:18:20,160
using gpt3 to classify domains

529
00:18:20,160 --> 00:18:20,820
um

530
00:18:20,820 --> 00:18:23,720
so here I gave gpt3 the prompt

531
00:18:23,720 --> 00:18:27,000
berkeley.edu maps to education Amazon at

532
00:18:27,000 --> 00:18:28,620
master shopping Netflix Maps

533
00:18:28,620 --> 00:18:29,960
entertainment Etc

534
00:18:29,960 --> 00:18:32,460
and my goal was to get it to classify a

535
00:18:32,460 --> 00:18:34,280
new domain that I'd never seen before

536
00:18:34,280 --> 00:18:37,140
accurately just based on the five

537
00:18:37,140 --> 00:18:38,580
Training examples I gave it in the

538
00:18:38,580 --> 00:18:39,780
prompts

539
00:18:39,780 --> 00:18:41,160
um and so I gave it this new example

540
00:18:41,160 --> 00:18:42,660
shooting range that come and asked it to

541
00:18:42,660 --> 00:18:44,820
complete the pattern and it gave weapons

542
00:18:44,820 --> 00:18:46,559
as the answer I think that's really

543
00:18:46,559 --> 00:18:48,720
significant for security because let's

544
00:18:48,720 --> 00:18:49,860
say

545
00:18:49,860 --> 00:18:51,299
um there's practical challenge is to

546
00:18:51,299 --> 00:18:53,340
actually do this right now given how big

547
00:18:53,340 --> 00:18:55,320
and costly it is to use cbg3 but I think

548
00:18:55,320 --> 00:18:58,440
it's useful to think ahead and think in

549
00:18:58,440 --> 00:19:01,980
a sock if you want to recognize

550
00:19:01,980 --> 00:19:04,080
um new examples of a pattern that you're

551
00:19:04,080 --> 00:19:04,940
seeing

552
00:19:04,940 --> 00:19:07,440
the fact that you can just type out five

553
00:19:07,440 --> 00:19:09,840
examples and then gpt3 will sort of

554
00:19:09,840 --> 00:19:11,880
recognize conceptually equivalent

555
00:19:11,880 --> 00:19:13,460
content in the future

556
00:19:13,460 --> 00:19:15,660
is significant right I mean you could

557
00:19:15,660 --> 00:19:16,679
write something out like this in the

558
00:19:16,679 --> 00:19:19,799
nationality to go find you know to go

559
00:19:19,799 --> 00:19:21,600
categorize all the domains

560
00:19:21,600 --> 00:19:23,460
um that your users have looked at over

561
00:19:23,460 --> 00:19:26,220
the past day and I mean the Tactical

562
00:19:26,220 --> 00:19:28,559
power of this over traditional machine

563
00:19:28,559 --> 00:19:29,820
learning which requires tens of

564
00:19:29,820 --> 00:19:31,080
thousands or millions of examples is

565
00:19:31,080 --> 00:19:32,820
really dramatic right the ability just

566
00:19:32,820 --> 00:19:35,760
to write out a pattern and deploy a

567
00:19:35,760 --> 00:19:36,960
model very quickly I think is

568
00:19:36,960 --> 00:19:38,160
significant

569
00:19:38,160 --> 00:19:40,020
um I mean here's another example

570
00:19:40,020 --> 00:19:41,880
of um

571
00:19:41,880 --> 00:19:43,860
here's another example of using gpt3 in

572
00:19:43,860 --> 00:19:45,780
this way where it's given a you know

573
00:19:45,780 --> 00:19:47,820
security examples of you know good

574
00:19:47,820 --> 00:19:50,039
domains and bad domains like this PayPal

575
00:19:50,039 --> 00:19:51,780
sort of phishing URL or the city card

576
00:19:51,780 --> 00:19:53,640
phishing URL and then it's able to

577
00:19:53,640 --> 00:19:56,160
detect you know given a new domain that

578
00:19:56,160 --> 00:19:57,419
it's bad just given a handful of

579
00:19:57,419 --> 00:19:58,740
training examples I mean just to

580
00:19:58,740 --> 00:20:01,260
emphasize this is this is

581
00:20:01,260 --> 00:20:04,320
that so using gbt3 to make detections in

582
00:20:04,320 --> 00:20:07,020
this way uh it requires orders of

583
00:20:07,020 --> 00:20:08,760
magnitude less training data than using

584
00:20:08,760 --> 00:20:10,500
traditional machine learning um as young

585
00:20:10,500 --> 00:20:11,460
who's going to talk about that more

586
00:20:11,460 --> 00:20:12,840
later but we would need like tens of

587
00:20:12,840 --> 00:20:14,100
thousands of examples to get a machine

588
00:20:14,100 --> 00:20:15,840
Learning System functioning as well as

589
00:20:15,840 --> 00:20:18,120
we're getting gpt3 to to function with

590
00:20:18,120 --> 00:20:19,980
five Training examples so in context

591
00:20:19,980 --> 00:20:21,600
where you have just a few examples of a

592
00:20:21,600 --> 00:20:23,220
bad kind of observation that you need to

593
00:20:23,220 --> 00:20:25,260
detect I think these large models are

594
00:20:25,260 --> 00:20:26,460
really significant

595
00:20:26,460 --> 00:20:29,700
now I've been teaching gpt3 in these

596
00:20:29,700 --> 00:20:31,860
examples to detect

597
00:20:31,860 --> 00:20:34,559
um malicious observations using

598
00:20:34,559 --> 00:20:36,539
um a method called in context learning

599
00:20:36,539 --> 00:20:38,700
and in context learning is just a fancy

600
00:20:38,700 --> 00:20:41,460
way of talking about using a model like

601
00:20:41,460 --> 00:20:45,780
gbt3 uh giving a model HTTP 3 a prompt

602
00:20:45,780 --> 00:20:47,820
and then asking it to autocomplete so

603
00:20:47,820 --> 00:20:49,620
here I'm not for data scientists in the

604
00:20:49,620 --> 00:20:51,059
audience there are no gradients there

605
00:20:51,059 --> 00:20:52,559
there's there's no stochastic gradient

606
00:20:52,559 --> 00:20:54,179
descent there's no back propagation

607
00:20:54,179 --> 00:20:56,460
literally we're just writing some text

608
00:20:56,460 --> 00:21:00,000
uh into the writing some text on asking

609
00:21:00,000 --> 00:21:02,220
gpt3 to autocomplete and getting your

610
00:21:02,220 --> 00:21:04,200
answer in this way and there's there's a

611
00:21:04,200 --> 00:21:06,059
whole body of research now around in

612
00:21:06,059 --> 00:21:07,440
context learning and prompting large

613
00:21:07,440 --> 00:21:09,299
models uh in the way that I'm showing

614
00:21:09,299 --> 00:21:11,400
here in order to get predictions

615
00:21:11,400 --> 00:21:13,799
now there's another way that you can

616
00:21:13,799 --> 00:21:16,020
adapt large language models to solve a

617
00:21:16,020 --> 00:21:17,520
problem like detecting malicious domains

618
00:21:17,520 --> 00:21:19,559
which is called fine tuning

619
00:21:19,559 --> 00:21:22,260
and that does use the traditional deep

620
00:21:22,260 --> 00:21:23,700
learning optimization methods like back

621
00:21:23,700 --> 00:21:25,500
propagation and stochastic gradient

622
00:21:25,500 --> 00:21:28,380
descent basically the idea there is we

623
00:21:28,380 --> 00:21:30,360
use so when we're when we're fitting in

624
00:21:30,360 --> 00:21:33,200
a large neural network to data like gpt3

625
00:21:33,200 --> 00:21:37,080
uh we we initially fit it using a

626
00:21:37,080 --> 00:21:39,480
self-supervised learning procedure uh

627
00:21:39,480 --> 00:21:41,039
like predicting that predicting the next

628
00:21:41,039 --> 00:21:43,200
few letters of a document given the the

629
00:21:43,200 --> 00:21:45,539
the characters and the documents uh up

630
00:21:45,539 --> 00:21:46,679
to up to the point in which we're

631
00:21:46,679 --> 00:21:48,360
looking in the document

632
00:21:48,360 --> 00:21:50,039
um we play that kind of self-supervised

633
00:21:50,039 --> 00:21:52,679
learning game with the model and we use

634
00:21:52,679 --> 00:21:54,480
um traditional neural network training

635
00:21:54,480 --> 00:21:56,640
like back propagation is Casa gradient

636
00:21:56,640 --> 00:21:58,860
descent to sort of get ourselves to a

637
00:21:58,860 --> 00:22:00,840
good region of the parameter space so

638
00:22:00,840 --> 00:22:02,159
we're sort of tuning all of our neural

639
00:22:02,159 --> 00:22:04,020
network weights in that way and then we

640
00:22:04,020 --> 00:22:05,880
do a little bit more

641
00:22:05,880 --> 00:22:07,380
um back propagation the stochastic

642
00:22:07,380 --> 00:22:09,120
gradient descents in order to solve a

643
00:22:09,120 --> 00:22:11,640
downstream task like detecting malicious

644
00:22:11,640 --> 00:22:12,840
domains

645
00:22:12,840 --> 00:22:14,940
um and what we're finding is that a

646
00:22:14,940 --> 00:22:16,440
combination of in context learning and

647
00:22:16,440 --> 00:22:18,240
fine-tuning tends to get us the best

648
00:22:18,240 --> 00:22:19,380
results young people will talk more

649
00:22:19,380 --> 00:22:23,240
about about our experiments there

650
00:22:23,460 --> 00:22:26,640
okay so just just wrapping up um so this

651
00:22:26,640 --> 00:22:28,020
is this is my last slide in the past is

652
00:22:28,020 --> 00:22:29,880
left to to Young Who

653
00:22:29,880 --> 00:22:30,539
Um

654
00:22:30,539 --> 00:22:31,799
hopefully I have what you're appetite

655
00:22:31,799 --> 00:22:33,240
into thinking about

656
00:22:33,240 --> 00:22:34,440
um the significance of these large

657
00:22:34,440 --> 00:22:35,700
models

658
00:22:35,700 --> 00:22:37,080
um and their significance for cyber

659
00:22:37,080 --> 00:22:38,280
security

660
00:22:38,280 --> 00:22:40,559
um and maybe folks even now or in the OR

661
00:22:40,559 --> 00:22:42,059
subsequent to this talk we'll have some

662
00:22:42,059 --> 00:22:43,919
ideas about where they might apply uh

663
00:22:43,919 --> 00:22:45,419
within cyber security

664
00:22:45,419 --> 00:22:46,860
here are some areas that we're thinking

665
00:22:46,860 --> 00:22:48,000
about

666
00:22:48,000 --> 00:22:48,720
um

667
00:22:48,720 --> 00:22:51,120
so I've already mentioned you know

668
00:22:51,120 --> 00:22:53,100
um using large language models to detect

669
00:22:53,100 --> 00:22:55,080
previously unseen attacks

670
00:22:55,080 --> 00:22:56,760
um better than we're currently able to

671
00:22:56,760 --> 00:23:00,539
to detect um clearly interesting

672
00:23:00,539 --> 00:23:02,159
capabilities emerge when we scale neural

673
00:23:02,159 --> 00:23:03,600
networks up to the level that I've

674
00:23:03,600 --> 00:23:04,679
described

675
00:23:04,679 --> 00:23:06,240
um and I think that's an area that we

676
00:23:06,240 --> 00:23:07,980
can be thinking in

677
00:23:07,980 --> 00:23:09,120
um

678
00:23:09,120 --> 00:23:10,559
young who actually has done a bunch of

679
00:23:10,559 --> 00:23:14,039
work around building capability on top

680
00:23:14,039 --> 00:23:17,340
of gpt3 that translates between a

681
00:23:17,340 --> 00:23:19,980
natural language query like show me all

682
00:23:19,980 --> 00:23:22,740
outgoing SMTP connections on my network

683
00:23:22,740 --> 00:23:25,500
they're connecting from our machines in

684
00:23:25,500 --> 00:23:28,320
Australia to machines and Belarus like

685
00:23:28,320 --> 00:23:29,880
asking quite you know basically he's

686
00:23:29,880 --> 00:23:31,320
developed technology that allows us to

687
00:23:31,320 --> 00:23:32,580
ask a question like that and have it

688
00:23:32,580 --> 00:23:34,559
have gbt translate that into a

689
00:23:34,559 --> 00:23:37,200
structured query into a database we've

690
00:23:37,200 --> 00:23:38,520
had some success there I think that's a

691
00:23:38,520 --> 00:23:39,860
really interesting area

692
00:23:39,860 --> 00:23:41,340
obviously we're going to be talking

693
00:23:41,340 --> 00:23:42,840
about models that help reverse engineer

694
00:23:42,840 --> 00:23:45,539
obscure and obviously the code

695
00:23:45,539 --> 00:23:46,159
um

696
00:23:46,159 --> 00:23:48,240
autocomplete on steroids models for

697
00:23:48,240 --> 00:23:49,980
security operations

698
00:23:49,980 --> 00:23:52,260
um so in cases where security operators

699
00:23:52,260 --> 00:23:55,380
are typing command lines could we Auto

700
00:23:55,380 --> 00:23:58,559
suggest a menu of actions they might

701
00:23:58,559 --> 00:23:59,520
take

702
00:23:59,520 --> 00:24:02,039
um you know using gpt3 and I think

703
00:24:02,039 --> 00:24:03,179
there's a lot of other possible

704
00:24:03,179 --> 00:24:04,440
applications

705
00:24:04,440 --> 00:24:05,520
um that just wanted to throw those out

706
00:24:05,520 --> 00:24:07,440
in a brainstorming kind of way so I'll

707
00:24:07,440 --> 00:24:09,539
pass it over to Yahoo now to talk in

708
00:24:09,539 --> 00:24:11,279
detail about the experiments we've been

709
00:24:11,279 --> 00:24:12,960
doing with gpt3

710
00:24:12,960 --> 00:24:15,299
anything

711
00:24:15,299 --> 00:24:18,840
yeah thank you Choice uh yeah the

712
00:24:18,840 --> 00:24:20,880
kangaroos especially the 2 million

713
00:24:20,880 --> 00:24:23,520
Parramatta one was very impressive to me

714
00:24:23,520 --> 00:24:28,380
especially I'm from Australia so it was

715
00:24:28,380 --> 00:24:31,799
a long trip from Sydney to Las Vegas but

716
00:24:31,799 --> 00:24:34,200
it was great to meet those wonderful

717
00:24:34,200 --> 00:24:37,039
anywhere here again

718
00:24:37,039 --> 00:24:40,380
in the second part of our talk

719
00:24:40,380 --> 00:24:43,020
uh I will talk about two cyber security

720
00:24:43,020 --> 00:24:45,960
use cases you can apply a larger

721
00:24:45,960 --> 00:24:49,860
large-scale language model so our

722
00:24:49,860 --> 00:24:53,279
yet so actually there's many not many of

723
00:24:53,279 --> 00:24:56,220
you different religious language models

724
00:24:56,220 --> 00:24:57,980
but we can use

725
00:24:57,980 --> 00:25:01,200
GPT city as our large scale language

726
00:25:01,200 --> 00:25:02,100
model

727
00:25:02,100 --> 00:25:05,100
and usually it is a true logic you can

728
00:25:05,100 --> 00:25:07,980
use directly it unless you have a super

729
00:25:07,980 --> 00:25:08,940
computer

730
00:25:08,940 --> 00:25:13,020
but luckily we can use open AI API to

731
00:25:13,020 --> 00:25:15,299
build a powerful application let me show

732
00:25:15,299 --> 00:25:17,580
you how you can build the search Power

733
00:25:17,580 --> 00:25:20,220
application so our parts application is

734
00:25:20,220 --> 00:25:24,500
Spam detection with gpt3

735
00:25:25,860 --> 00:25:29,580
when you start a new ml project probably

736
00:25:29,580 --> 00:25:33,120
you will start with few samples so we

737
00:25:33,120 --> 00:25:35,520
have the same problem we just collected

738
00:25:35,520 --> 00:25:39,779
two Whirlpool Malaysia spam samples but

739
00:25:39,779 --> 00:25:43,320
we wanted to build a new spam detector

740
00:25:43,320 --> 00:25:45,600
with those samples

741
00:25:45,600 --> 00:25:48,240
traditional ml machine learning models

742
00:25:48,240 --> 00:25:51,539
usually does not work with few centers

743
00:25:51,539 --> 00:25:55,140
so it's something like when you train a

744
00:25:55,140 --> 00:25:57,779
small puppy probably you need to show

745
00:25:57,779 --> 00:26:01,500
him a lot of examples to teach a as good

746
00:26:01,500 --> 00:26:05,400
scared but however a smart door

747
00:26:05,400 --> 00:26:08,940
with a lot of experience it will just

748
00:26:08,940 --> 00:26:11,760
the dog will just learn quickly with few

749
00:26:11,760 --> 00:26:14,940
examples so our super scale large

750
00:26:14,940 --> 00:26:16,860
language more pre-trained language model

751
00:26:16,860 --> 00:26:20,400
can do a great job with few examples I

752
00:26:20,400 --> 00:26:23,600
will show you how we can do that

753
00:26:24,059 --> 00:26:26,640
so uh so she mentioned earlier we

754
00:26:26,640 --> 00:26:30,059
achieved impressive uh the detection

755
00:26:30,059 --> 00:26:33,240
accuracy with our model so as you can

756
00:26:33,240 --> 00:26:37,860
see gpt3 with only two examples one ham

757
00:26:37,860 --> 00:26:41,640
and one spam image message we achieved

758
00:26:41,640 --> 00:26:43,919
about 90 accuracy

759
00:26:43,919 --> 00:26:46,559
and with some additional eight samples

760
00:26:46,559 --> 00:26:50,279
we achieved about 95 accuracy it is

761
00:26:50,279 --> 00:26:53,220
really amazing however

762
00:26:53,220 --> 00:26:57,000
one of the traditional but it's a I mean

763
00:26:57,000 --> 00:26:59,100
the popular random forest model it is a

764
00:26:59,100 --> 00:27:01,799
tree based model however with only two

765
00:27:01,799 --> 00:27:04,919
examples you can get like a slightly

766
00:27:04,919 --> 00:27:07,320
better than them guessing this is the

767
00:27:07,320 --> 00:27:10,080
power of a large-scale language model so

768
00:27:10,080 --> 00:27:12,900
I will talk about the details

769
00:27:12,900 --> 00:27:15,480
so these are our data sets for our

770
00:27:15,480 --> 00:27:20,039
evaluation and the model details

771
00:27:20,039 --> 00:27:25,320
so teaching GPT 3 is quite simple so

772
00:27:25,320 --> 00:27:28,080
when you train when you design a large

773
00:27:28,080 --> 00:27:31,320
language I mean the links model the

774
00:27:31,320 --> 00:27:33,919
architecture is quite complex however

775
00:27:33,919 --> 00:27:36,720
utilizing the ping model is quite simple

776
00:27:36,720 --> 00:27:39,600
it is as simple as designing a prompt

777
00:27:39,600 --> 00:27:44,460
data so it can uh yeah the typically can

778
00:27:44,460 --> 00:27:47,940
do a amazing job for example so this is

779
00:27:47,940 --> 00:27:50,480
our example so we are going to translate

780
00:27:50,480 --> 00:27:55,500
uh movie titles into emotes icons so we

781
00:27:55,500 --> 00:27:57,899
provide some examples with a simple

782
00:27:57,899 --> 00:28:01,620
instruction the first instruction yet we

783
00:28:01,620 --> 00:28:05,100
will convert movie titles into I uh the

784
00:28:05,100 --> 00:28:07,620
Emoji icons and we provide some examples

785
00:28:07,620 --> 00:28:10,320
back to the computer and we provide the

786
00:28:10,320 --> 00:28:12,960
relevant icons and the second one better

787
00:28:12,960 --> 00:28:16,260
man Transformers and then we ask about

788
00:28:16,260 --> 00:28:19,740
our custom Star Wars so we get the

789
00:28:19,740 --> 00:28:22,620
correct answer so this is really simple

790
00:28:22,620 --> 00:28:24,960
but we can build a powerful application

791
00:28:24,960 --> 00:28:27,299
with the same apology so this one is

792
00:28:27,299 --> 00:28:30,720
called like from the designing or prompt

793
00:28:30,720 --> 00:28:32,820
engineering this is the step we will

794
00:28:32,820 --> 00:28:35,480
follow today

795
00:28:35,700 --> 00:28:39,840
so let's design our prompt for detect uh

796
00:28:39,840 --> 00:28:43,380
spam message we start with a simple

797
00:28:43,380 --> 00:28:47,279
instruction so a classified message is

798
00:28:47,279 --> 00:28:48,899
spam

799
00:28:48,899 --> 00:28:51,720
or hem this is really simple and then we

800
00:28:51,720 --> 00:28:54,720
provide some examples so here we select

801
00:28:54,720 --> 00:28:57,779
one spam and one hand from our training

802
00:28:57,779 --> 00:29:02,159
data and then we add our test sample so

803
00:29:02,159 --> 00:29:04,440
the test sample says uh the free

804
00:29:04,440 --> 00:29:06,720
something and then

805
00:29:06,720 --> 00:29:10,080
with this input the model can easily

806
00:29:10,080 --> 00:29:12,539
understand what task is about so it is

807
00:29:12,539 --> 00:29:14,940
the task about classified message as a

808
00:29:14,940 --> 00:29:17,159
Spam or amp and we've already provided

809
00:29:17,159 --> 00:29:19,620
the relevant information so easily the

810
00:29:19,620 --> 00:29:22,860
model will return spam as our output so

811
00:29:22,860 --> 00:29:25,440
this is really simple step

812
00:29:25,440 --> 00:29:28,260
so we will show you some other examples

813
00:29:28,260 --> 00:29:32,159
about the model so from the previous uh

814
00:29:32,159 --> 00:29:33,779
the

815
00:29:33,779 --> 00:29:36,960
The Prompt we now test the use messages

816
00:29:36,960 --> 00:29:40,380
the message says horizontal something so

817
00:29:40,380 --> 00:29:44,159
this is a obvious spam so we detected

818
00:29:44,159 --> 00:29:45,779
this one our model detectors spam

819
00:29:45,779 --> 00:29:49,500
correctly and the next one so yeah I can

820
00:29:49,500 --> 00:29:51,840
see but it yeah it is not malicious it

821
00:29:51,840 --> 00:29:55,080
is not detected spam so this is a

822
00:29:55,080 --> 00:29:58,200
screenshot from open ai's playground

823
00:29:58,200 --> 00:30:01,140
website so well you can test your input

824
00:30:01,140 --> 00:30:03,600
and output with some settings so here is

825
00:30:03,600 --> 00:30:05,279
one of the most important thing is the

826
00:30:05,279 --> 00:30:09,059
model so we set the model as tax uh text

827
00:30:09,059 --> 00:30:13,380
tabins this is a uh the largest length

828
00:30:13,380 --> 00:30:16,500
model of an airplane provided for text

829
00:30:16,500 --> 00:30:21,140
generation so we used this uh

830
00:30:21,240 --> 00:30:26,640
the g53 to detect spam message so this

831
00:30:26,640 --> 00:30:29,100
is a simple use case demonstrated the

832
00:30:29,100 --> 00:30:31,980
power of a large scale pre-trained

833
00:30:31,980 --> 00:30:34,640
linkage model

834
00:30:35,340 --> 00:30:38,820
so the next use cases is a more complex

835
00:30:38,820 --> 00:30:41,279
than the previous simple one so we are

836
00:30:41,279 --> 00:30:43,500
going to generate a human needable

837
00:30:43,500 --> 00:30:48,140
description from malicious command lines

838
00:30:48,720 --> 00:30:51,899
in previous token mentioned about the

839
00:30:51,899 --> 00:30:55,740
soc analysts uh on their analyzing a lot

840
00:30:55,740 --> 00:30:58,620
of I mean police command every day but

841
00:30:58,620 --> 00:31:01,380
it's really hard job so as you can see

842
00:31:01,380 --> 00:31:05,760
we have a one malicious command so uh it

843
00:31:05,760 --> 00:31:09,000
is really long and it's hard to pass so

844
00:31:09,000 --> 00:31:11,460
it is really hard to understand the

845
00:31:11,460 --> 00:31:13,679
actual the hidden uh I mean the

846
00:31:13,679 --> 00:31:16,559
intention of this one so our code is

847
00:31:16,559 --> 00:31:18,840
actually we wanted to translate this one

848
00:31:18,840 --> 00:31:22,080
into human beatable description so our

849
00:31:22,080 --> 00:31:23,940
question is that can large language

850
00:31:23,940 --> 00:31:28,380
models like cptc can make I mean our

851
00:31:28,380 --> 00:31:31,679
analysts are easier by describing this

852
00:31:31,679 --> 00:31:33,720
command in simple language

853
00:31:33,720 --> 00:31:37,080
our answer is yes it is possible

854
00:31:37,080 --> 00:31:39,539
so you can see the description generated

855
00:31:39,539 --> 00:31:42,720
by our approach so to complex one now

856
00:31:42,720 --> 00:31:46,140
you can be so it is simple so actually

857
00:31:46,140 --> 00:31:47,580
the command

858
00:31:47,580 --> 00:31:52,140
create a file called uh exe.pat file and

859
00:31:52,140 --> 00:31:54,360
it has some malicious content and it

860
00:31:54,360 --> 00:31:57,600
execute and delete immediately so it is

861
00:31:57,600 --> 00:32:00,860
one of the malicious behaviors

862
00:32:00,860 --> 00:32:04,500
attackers employee to avoid the

863
00:32:04,500 --> 00:32:08,159
detection with the file based signatures

864
00:32:08,159 --> 00:32:10,380
so we will talk about how we can

865
00:32:10,380 --> 00:32:15,260
generate these impressive description

866
00:32:16,980 --> 00:32:20,159
so actually gpt3 is a family of

867
00:32:20,159 --> 00:32:23,100
large-scale language model and it comes

868
00:32:23,100 --> 00:32:25,860
with two different versions the first

869
00:32:25,860 --> 00:32:27,360
one is

870
00:32:27,360 --> 00:32:31,080
um actually uh text generation version

871
00:32:31,080 --> 00:32:34,019
so these models has been trained with

872
00:32:34,019 --> 00:32:37,559
large-scale Text data so it can write

873
00:32:37,559 --> 00:32:40,799
interesting stories and second person is

874
00:32:40,799 --> 00:32:43,440
called codex so these models has been

875
00:32:43,440 --> 00:32:45,419
trained with the uh I mean a lot of

876
00:32:45,419 --> 00:32:48,179
Public public available source code

877
00:32:48,179 --> 00:32:51,059
depositories including GitHub so it can

878
00:32:51,059 --> 00:32:53,539
write code in many different languages

879
00:32:53,539 --> 00:32:57,240
including python JavaScript and power

880
00:32:57,240 --> 00:32:59,159
shell script as well as many different

881
00:32:59,159 --> 00:33:01,500
programming languages so this is the

882
00:33:01,500 --> 00:33:04,140
Codex is our choice for our Second Use

883
00:33:04,140 --> 00:33:07,140
case and we already use the I mean the

884
00:33:07,140 --> 00:33:11,539
text version for our spam detection

885
00:33:12,179 --> 00:33:17,399
so okay let's uh start with our simple

886
00:33:17,399 --> 00:33:21,360
prompt so we provide a command as it is

887
00:33:21,360 --> 00:33:24,360
as a command section in the second

888
00:33:24,360 --> 00:33:26,820
section which I will provide description

889
00:33:26,820 --> 00:33:29,880
but we carefully design the uh The

890
00:33:29,880 --> 00:33:34,140
Prompt so the gptc can also complete the

891
00:33:34,140 --> 00:33:36,659
domain as a chip chip D3 is a

892
00:33:36,659 --> 00:33:38,640
pre-trained language model so the

893
00:33:38,640 --> 00:33:40,799
pre-treating language model is

894
00:33:40,799 --> 00:33:43,980
pre-trained within the printfin step to

895
00:33:43,980 --> 00:33:45,360
uh

896
00:33:45,360 --> 00:33:48,659
predict next word so as you can see so

897
00:33:48,659 --> 00:33:51,419
this will guide the GPS to write

898
00:33:51,419 --> 00:33:54,000
discussion about the command so let's

899
00:33:54,000 --> 00:33:56,100
have a look at the result with this

900
00:33:56,100 --> 00:33:57,840
prompt

901
00:33:57,840 --> 00:34:00,899
so now we can read the command where

902
00:34:00,899 --> 00:34:06,539
copy the exe binary to a temp folder so

903
00:34:06,539 --> 00:34:08,520
it generates a valid description about

904
00:34:08,520 --> 00:34:12,359
the command but actually the command is

905
00:34:12,359 --> 00:34:15,440
malicious because it copy

906
00:34:15,440 --> 00:34:19,980
a system binary into a temp report as

907
00:34:19,980 --> 00:34:22,918
Adobe exec so it is hiding some

908
00:34:22,918 --> 00:34:24,960
malicious activity here but it didn't

909
00:34:24,960 --> 00:34:27,899
catch that so we are going to add some

910
00:34:27,899 --> 00:34:29,820
additional information to improve the

911
00:34:29,820 --> 00:34:31,199
quality

912
00:34:31,199 --> 00:34:34,619
how we can do that

913
00:34:34,619 --> 00:34:37,560
so our approach is quite simple so

914
00:34:37,560 --> 00:34:39,780
usually those malicious commands will be

915
00:34:39,780 --> 00:34:43,500
detected by any second space rules and

916
00:34:43,500 --> 00:34:46,859
we can use Sigma rules or Yara rules to

917
00:34:46,859 --> 00:34:48,960
detect those ones so in this case the

918
00:34:48,960 --> 00:34:52,199
previous command was detected as uh the

919
00:34:52,199 --> 00:34:55,739
one of the sigma dual signatures and the

920
00:34:55,739 --> 00:34:57,560
detection name was

921
00:34:57,560 --> 00:35:01,740
Windows spheres copy system 32 so the

922
00:35:01,740 --> 00:35:04,140
signature name is several many many I

923
00:35:04,140 --> 00:35:05,640
mean important information about the

924
00:35:05,640 --> 00:35:07,680
command so we are going to use this

925
00:35:07,680 --> 00:35:10,320
invention to improve the previous I mean

926
00:35:10,320 --> 00:35:11,339
description

927
00:35:11,339 --> 00:35:15,740
so let's have a look at the open now

928
00:35:16,020 --> 00:35:17,220
so

929
00:35:17,220 --> 00:35:20,880
yeah sorry I forgot about the uh the the

930
00:35:20,880 --> 00:35:23,099
second version of our prompter so this

931
00:35:23,099 --> 00:35:25,920
case we still use the same command

932
00:35:25,920 --> 00:35:29,220
section but we add one additional

933
00:35:29,220 --> 00:35:32,040
section we call the tags and we will be

934
00:35:32,040 --> 00:35:35,280
using uh signature names as the Tigers

935
00:35:35,280 --> 00:35:37,920
so if the one command can be detected by

936
00:35:37,920 --> 00:35:40,400
multiple signatures we will be

937
00:35:40,400 --> 00:35:43,920
at all with the comma separate string

938
00:35:43,920 --> 00:35:47,700
and then uh we will ask GPT to write

939
00:35:47,700 --> 00:35:50,779
about the description

940
00:35:51,000 --> 00:35:54,420
so now we have a better description so

941
00:35:54,420 --> 00:35:57,300
you can see that actually the command

942
00:35:57,300 --> 00:36:00,540
will copy the finally from system folder

943
00:36:00,540 --> 00:36:03,720
to a tempo for the Adobe exit so where

944
00:36:03,720 --> 00:36:08,400
the attackers can use the to finally to

945
00:36:08,400 --> 00:36:11,460
perform ballistics activity so now it

946
00:36:11,460 --> 00:36:13,440
correctly recognized the malicious

947
00:36:13,440 --> 00:36:16,619
activity as well so now we have much

948
00:36:16,619 --> 00:36:20,040
better description so uh this case uh

949
00:36:20,040 --> 00:36:22,500
previously we set the model as a text

950
00:36:22,500 --> 00:36:24,900
star principle now we are using codex

951
00:36:24,900 --> 00:36:27,540
version so which is a code that means

952
00:36:27,540 --> 00:36:30,780
the model and also as you can see there

953
00:36:30,780 --> 00:36:33,780
are some other parameters like templates

954
00:36:33,780 --> 00:36:37,020
or what in the top the top probability

955
00:36:37,020 --> 00:36:39,540
parameter so with when you change this

956
00:36:39,540 --> 00:36:41,220
parallel with the same input you can

957
00:36:41,220 --> 00:36:43,619
generate multiple descriptions maybe

958
00:36:43,619 --> 00:36:46,440
just slightly different but our next

959
00:36:46,440 --> 00:36:48,660
step is we are not happy about this one

960
00:36:48,660 --> 00:36:50,579
but we still want a team for for the

961
00:36:50,579 --> 00:36:51,540
description

962
00:36:51,540 --> 00:36:54,720
so our practice we can generate multiple

963
00:36:54,720 --> 00:36:57,420
description from the same input we can

964
00:36:57,420 --> 00:37:00,260
change the templates over the topic

965
00:37:00,260 --> 00:37:03,480
parameters and then we will select the

966
00:37:03,480 --> 00:37:07,640
best one out of a multiple candidate

967
00:37:07,920 --> 00:37:10,380
so our approach is using back

968
00:37:10,380 --> 00:37:13,680
translation so effectively installation

969
00:37:13,680 --> 00:37:17,339
um is I can explain like when we train a

970
00:37:17,339 --> 00:37:20,460
translate English to French water so for

971
00:37:20,460 --> 00:37:23,339
example hello to bonjour it is not

972
00:37:23,339 --> 00:37:25,320
easily directly compare within the

973
00:37:25,320 --> 00:37:27,960
quality of the principle but if we

974
00:37:27,960 --> 00:37:30,720
translate French to English now it's

975
00:37:30,720 --> 00:37:33,300
clear we can easily compare the quality

976
00:37:33,300 --> 00:37:35,820
of Friends by comparing to hello

977
00:37:35,820 --> 00:37:38,579
original one and pack translates English

978
00:37:38,579 --> 00:37:42,480
high so now we can apply the same

979
00:37:42,480 --> 00:37:45,980
mechanism for our problem

980
00:37:46,260 --> 00:37:50,040
so command is a one language and

981
00:37:50,040 --> 00:37:52,200
description is another language so we

982
00:37:52,200 --> 00:37:54,359
can apply the same mechanism for our

983
00:37:54,359 --> 00:37:56,280
case so

984
00:37:56,280 --> 00:37:58,380
we can generate the description as we

985
00:37:58,380 --> 00:38:00,960
did before and then also we can generate

986
00:38:00,960 --> 00:38:03,420
the description ah sorry the command

987
00:38:03,420 --> 00:38:06,180
from the description so now we can

988
00:38:06,180 --> 00:38:08,940
compare the original command and the

989
00:38:08,940 --> 00:38:11,520
effect translate command to selected

990
00:38:11,520 --> 00:38:13,920
past the description so okay let me show

991
00:38:13,920 --> 00:38:16,680
you some of the equation first

992
00:38:16,680 --> 00:38:19,980
oh sorry so this one is the overall step

993
00:38:19,980 --> 00:38:23,839
so the in step one we will generate

994
00:38:23,839 --> 00:38:26,460
multiple descriptions from one command

995
00:38:26,460 --> 00:38:28,500
using the different settings of the

996
00:38:28,500 --> 00:38:31,800
model and then we will generate command

997
00:38:31,800 --> 00:38:34,920
from the generate descriptions and then

998
00:38:34,920 --> 00:38:39,420
we will link description by uh simulated

999
00:38:39,420 --> 00:38:42,180
score between the original Grant and the

1000
00:38:42,180 --> 00:38:45,799
effectiveness once

1001
00:38:46,200 --> 00:38:49,200
so this is our new prompt for big

1002
00:38:49,200 --> 00:38:52,020
translation step so we provided the

1003
00:38:52,020 --> 00:38:54,000
title information and the description

1004
00:38:54,000 --> 00:38:56,220
generated from the previous step as

1005
00:38:56,220 --> 00:38:59,640
input and then we will ask the model to

1006
00:38:59,640 --> 00:39:02,280
complete our Command so this way we can

1007
00:39:02,280 --> 00:39:05,940
generate X translated the uh the command

1008
00:39:05,940 --> 00:39:08,220
from the description if the description

1009
00:39:08,220 --> 00:39:11,400
has enough information it should be able

1010
00:39:11,400 --> 00:39:13,920
to be generated the original command and

1011
00:39:13,920 --> 00:39:16,680
then also if you don't um do not provide

1012
00:39:16,680 --> 00:39:19,260
any information as the command so you

1013
00:39:19,260 --> 00:39:21,300
probably get some random strings so we

1014
00:39:21,300 --> 00:39:23,640
that's why we just provided the first

1015
00:39:23,640 --> 00:39:29,220
word as the prompt for the command to uh

1016
00:39:29,220 --> 00:39:31,200
so yeah these are the some of the

1017
00:39:31,200 --> 00:39:34,440
examples uh from the previous command we

1018
00:39:34,440 --> 00:39:36,300
now have a two descriptions the first

1019
00:39:36,300 --> 00:39:39,119
and second one the first one actually uh

1020
00:39:39,119 --> 00:39:42,300
did not exactly mention but it just

1021
00:39:42,300 --> 00:39:44,339
mentioned about temp so it failed to

1022
00:39:44,339 --> 00:39:46,560
degenerate the command so as you can see

1023
00:39:46,560 --> 00:39:51,359
uh uh the the final output was not a

1024
00:39:51,359 --> 00:39:53,040
double but it has the same Commander

1025
00:39:53,040 --> 00:39:55,079
name but however the second one so it

1026
00:39:55,079 --> 00:39:57,720
mentioned correctly the temp out of in

1027
00:39:57,720 --> 00:40:00,900
the description so uh it can generate

1028
00:40:00,900 --> 00:40:03,780
the original command correctly so this

1029
00:40:03,780 --> 00:40:05,819
way you can easily compare the the

1030
00:40:05,819 --> 00:40:08,760
quality of description using uh back

1031
00:40:08,760 --> 00:40:12,359
translation step

1032
00:40:18,599 --> 00:40:24,599
so this is another example of our the

1033
00:40:24,599 --> 00:40:26,700
description generation from a command

1034
00:40:26,700 --> 00:40:29,220
this one is a quite interesting can you

1035
00:40:29,220 --> 00:40:29,960
guess

1036
00:40:29,960 --> 00:40:33,000
and we you you have the answer actually

1037
00:40:33,000 --> 00:40:36,420
this command will search files in

1038
00:40:36,420 --> 00:40:38,760
desktop fold and the server port and and

1039
00:40:38,760 --> 00:40:42,200
then the find files containing password

1040
00:40:42,200 --> 00:40:44,880
actually it is a finding your password

1041
00:40:44,880 --> 00:40:47,940
from desktop folder so you can easily

1042
00:40:47,940 --> 00:40:49,980
understand what the I mean the malicious

1043
00:40:49,980 --> 00:40:52,980
activity about this one

1044
00:40:52,980 --> 00:40:55,920
so the second application also

1045
00:40:55,920 --> 00:40:59,099
demonstrated the uh I mean power over

1046
00:40:59,099 --> 00:41:01,140
large-scale pre-trained language model

1047
00:41:01,140 --> 00:41:04,980
so you can build a powerful spam

1048
00:41:04,980 --> 00:41:07,980
detection with just a few samples and

1049
00:41:07,980 --> 00:41:11,460
also you can I mean create a impressive

1050
00:41:11,460 --> 00:41:13,380
application which can describe the

1051
00:41:13,380 --> 00:41:15,839
complex command line into human

1052
00:41:15,839 --> 00:41:19,500
leaderboard description

1053
00:41:21,319 --> 00:41:24,660
so uh actually do you remember the the

1054
00:41:24,660 --> 00:41:26,940
title of Auto

1055
00:41:26,940 --> 00:41:32,220
actually the title of our talk was

1056
00:41:32,420 --> 00:41:35,160
gpt3 and me

1057
00:41:35,160 --> 00:41:37,980
it's our story so our separate story now

1058
00:41:37,980 --> 00:41:40,440
it's time for you to create your own

1059
00:41:40,440 --> 00:41:42,599
wonderful application so maybe the next

1060
00:41:42,599 --> 00:41:46,740
chapter will be gpt3 and you thanks for

1061
00:41:46,740 --> 00:41:49,040
listening

1062
00:41:56,640 --> 00:41:57,780
you guys

1063
00:41:57,780 --> 00:41:59,400
if you guys have questions we can both

1064
00:41:59,400 --> 00:42:02,960
take turns answer yeah please go ahead

1065
00:42:42,480 --> 00:42:44,960
dude

1066
00:42:52,560 --> 00:42:54,359
[Music]

1067
00:42:54,359 --> 00:42:57,119
yeah we also tried to detect the

1068
00:42:57,119 --> 00:42:58,920
malicious URL with the same approach

1069
00:42:58,920 --> 00:43:01,810
yeah so you can still

1070
00:43:01,810 --> 00:43:03,839
[Music]

1071
00:43:03,839 --> 00:43:06,900
oh I'm sorry so yeah the question is

1072
00:43:06,900 --> 00:43:07,980
about

1073
00:43:07,980 --> 00:43:11,940
um the spam or the malicious message can

1074
00:43:11,940 --> 00:43:14,640
include some URLs some random strings so

1075
00:43:14,640 --> 00:43:17,040
it is possible to use the those I mean I

1076
00:43:17,040 --> 00:43:18,900
mean random strings to detect those one

1077
00:43:18,900 --> 00:43:22,200
so so we also tried to use malicious

1078
00:43:22,200 --> 00:43:24,359
URLs with the same approaches usually

1079
00:43:24,359 --> 00:43:26,520
molestory or let's have some random

1080
00:43:26,520 --> 00:43:29,460
strain so for example like Facebook but

1081
00:43:29,460 --> 00:43:32,220
it has some other random string so it

1082
00:43:32,220 --> 00:43:34,619
still works so it helps to recognize the

1083
00:43:34,619 --> 00:43:36,660
random string as the malicious part and

1084
00:43:36,660 --> 00:43:39,680
still can work

1085
00:43:40,260 --> 00:43:42,440
yeah

1086
00:43:42,440 --> 00:43:44,940
in the examples that you explained about

1087
00:43:44,940 --> 00:43:47,400
uh so I understand that if you use the

1088
00:43:47,400 --> 00:43:51,119
GPD models the gpt3 models for language

1089
00:43:51,119 --> 00:43:55,140
and for code but how did it work

1090
00:43:55,140 --> 00:43:58,560
with uh with URLs meaning it is not code

1091
00:43:58,560 --> 00:44:00,960
and it is not a language and

1092
00:44:00,960 --> 00:44:03,240
uh and furthermore if we want to uh to

1093
00:44:03,240 --> 00:44:05,460
use this uh this approach for things

1094
00:44:05,460 --> 00:44:07,140
like other malware detection or

1095
00:44:07,140 --> 00:44:11,040
something like that uh is there a simple

1096
00:44:11,040 --> 00:44:13,020
or not very very complicated and

1097
00:44:13,020 --> 00:44:16,260
expensive way to do that

1098
00:44:16,260 --> 00:44:17,099
um

1099
00:44:17,099 --> 00:44:18,599
yeah so I think you guys heard the

1100
00:44:18,599 --> 00:44:20,460
question thanks for speaking to Mike

1101
00:44:20,460 --> 00:44:21,300
um

1102
00:44:21,300 --> 00:44:23,640
yeah I mean I think you know so I think

1103
00:44:23,640 --> 00:44:25,500
it's really

1104
00:44:25,500 --> 00:44:27,359
um a problem for experimentation to know

1105
00:44:27,359 --> 00:44:30,540
to know where gpt3 will will shine and

1106
00:44:30,540 --> 00:44:31,500
um

1107
00:44:31,500 --> 00:44:34,800
that said I think my intuition is that

1108
00:44:34,800 --> 00:44:37,020
gpt3

1109
00:44:37,020 --> 00:44:39,300
and models like gpt3

1110
00:44:39,300 --> 00:44:40,020
um

1111
00:44:40,020 --> 00:44:43,440
will do really well in cases in in which

1112
00:44:43,440 --> 00:44:45,720
um the self-supervised problem they were

1113
00:44:45,720 --> 00:44:47,940
trained on has sort of forced them into

1114
00:44:47,940 --> 00:44:50,280
into a really useful representation of

1115
00:44:50,280 --> 00:44:52,260
the domain so

1116
00:44:52,260 --> 00:44:53,640
um like I think the reason it did so

1117
00:44:53,640 --> 00:44:56,099
well on that um oh that's fine it's

1118
00:44:56,099 --> 00:44:57,180
funny you guys are seeing my social

1119
00:44:57,180 --> 00:44:58,500
calendar here

1120
00:44:58,500 --> 00:45:00,839
um so I think

1121
00:45:00,839 --> 00:45:02,640
you guys are all welcome to come to

1122
00:45:02,640 --> 00:45:04,500
Topgolf later

1123
00:45:04,500 --> 00:45:06,119
um so I think the reason it did so well

1124
00:45:06,119 --> 00:45:07,980
in that domain categorization example I

1125
00:45:07,980 --> 00:45:10,319
gave for example is because

1126
00:45:10,319 --> 00:45:11,040
um

1127
00:45:11,040 --> 00:45:13,140
it was trained on like a web scale Text

1128
00:45:13,140 --> 00:45:15,839
corpus and it was forced to to learn

1129
00:45:15,839 --> 00:45:19,460
quote unquote that like shooting range

1130
00:45:19,460 --> 00:45:22,140
uh.com is like semantically quite

1131
00:45:22,140 --> 00:45:24,780
similar to like Firearms Warehouse or

1132
00:45:24,780 --> 00:45:27,119
whatever example I had in there now if I

1133
00:45:27,119 --> 00:45:28,980
was just to give it a bunch of like hex

1134
00:45:28,980 --> 00:45:31,140
from some software boundaries I wouldn't

1135
00:45:31,140 --> 00:45:33,000
expect it to do particularly well at all

1136
00:45:33,000 --> 00:45:34,619
on that because it hasn't been trained

1137
00:45:34,619 --> 00:45:37,200
in in that way

1138
00:45:37,200 --> 00:45:40,279
thank you sir

1139
00:45:41,460 --> 00:45:44,099
uh yes we please go ahead we have

1140
00:45:44,099 --> 00:45:46,460
another question

1141
00:46:05,339 --> 00:46:06,619
yeah yeah yeah

1142
00:46:06,619 --> 00:46:08,940
go ahead

1143
00:46:08,940 --> 00:46:12,540
oh I'm happy he just wants to like he

1144
00:46:12,540 --> 00:46:14,339
like he's like why didn't we use a text

1145
00:46:14,339 --> 00:46:17,460
description whether we just use the tag

1146
00:46:17,460 --> 00:46:19,680
so I think your question was why are we

1147
00:46:19,680 --> 00:46:21,900
just using a fairly

1148
00:46:21,900 --> 00:46:22,880
um

1149
00:46:22,880 --> 00:46:26,760
uh non-descriptive tag as opposed to

1150
00:46:26,760 --> 00:46:30,060
like a like a like a description that

1151
00:46:30,060 --> 00:46:32,040
says like so you know just like a full

1152
00:46:32,040 --> 00:46:34,380
sentence description that sort of is

1153
00:46:34,380 --> 00:46:37,140
mapped onto the tag

1154
00:46:37,140 --> 00:46:39,560
but

1155
00:46:50,040 --> 00:46:53,759
anyway right

1156
00:46:54,720 --> 00:46:58,020
yeah yeah so yeah I'll answer and then

1157
00:46:58,020 --> 00:46:59,700
younger add whatever you like so I think

1158
00:46:59,700 --> 00:47:01,500
the question is like why did the model

1159
00:47:01,500 --> 00:47:03,060
so like what's the model making like

1160
00:47:03,060 --> 00:47:04,560
what's the what's the intuition behind

1161
00:47:04,560 --> 00:47:06,839
the tag and why is the model like what's

1162
00:47:06,839 --> 00:47:08,700
the model doing with the tag so so

1163
00:47:08,700 --> 00:47:09,839
basically

1164
00:47:09,839 --> 00:47:13,319
um what what we did there is so the

1165
00:47:13,319 --> 00:47:15,119
intuition so first of all the goal of

1166
00:47:15,119 --> 00:47:17,460
this was is not to detect bad versus

1167
00:47:17,460 --> 00:47:19,020
good it's once you once you know

1168
00:47:19,020 --> 00:47:20,940
something is suspicious have the model

1169
00:47:20,940 --> 00:47:23,700
unpack all the Obscure sort of text on

1170
00:47:23,700 --> 00:47:25,559
the command and give a clear English

1171
00:47:25,559 --> 00:47:27,660
description of like what's going on

1172
00:47:27,660 --> 00:47:28,380
um

1173
00:47:28,380 --> 00:47:30,900
that our intuition about the tag and

1174
00:47:30,900 --> 00:47:31,980
this was young who this is entirely

1175
00:47:31,980 --> 00:47:33,780
young whose idea um

1176
00:47:33,780 --> 00:47:35,700
was that um

1177
00:47:35,700 --> 00:47:37,260
if you give the model the tag and the

1178
00:47:37,260 --> 00:47:39,000
prompt you'll get the model thinking

1179
00:47:39,000 --> 00:47:40,559
along the same lines that the analyst is

1180
00:47:40,559 --> 00:47:42,720
thinking about the about the about the

1181
00:47:42,720 --> 00:47:44,099
command and then it'll write

1182
00:47:44,099 --> 00:47:46,079
auto-complete that sort of incorporates

1183
00:47:46,079 --> 00:47:48,599
the information in the tag you might

1184
00:47:48,599 --> 00:47:49,740
wonder well can the model really

1185
00:47:49,740 --> 00:47:51,720
understand like an abbreviated tag

1186
00:47:51,720 --> 00:47:53,339
that's like win underscore sus

1187
00:47:53,339 --> 00:47:55,559
underscore password and like what we

1188
00:47:55,559 --> 00:47:57,119
found is that yes it seems to understand

1189
00:47:57,119 --> 00:47:59,579
it's it seems it seems to internalize

1190
00:47:59,579 --> 00:48:00,839
something about what that means and

1191
00:48:00,839 --> 00:48:02,819
reflect that in the description

1192
00:48:02,819 --> 00:48:06,740
yeah please go ahead yeah

1193
00:48:19,079 --> 00:48:21,119
like the logic like the rule matching

1194
00:48:21,119 --> 00:48:23,480
logic

1195
00:48:28,619 --> 00:48:30,800
yeah

1196
00:48:31,140 --> 00:48:32,760
yeah

1197
00:48:32,760 --> 00:48:35,940
yeah uh I'll repeat the question

1198
00:48:35,940 --> 00:48:38,819
yeah I guess the regarding the tag here

1199
00:48:38,819 --> 00:48:41,040
we have medications about so actually

1200
00:48:41,040 --> 00:48:43,859
the reason to use the tag as our I mean

1201
00:48:43,859 --> 00:48:45,540
additional information because we only

1202
00:48:45,540 --> 00:48:48,059
have that information so if we have I

1203
00:48:48,059 --> 00:48:49,200
mean

1204
00:48:49,200 --> 00:48:51,180
the texture description about the actual

1205
00:48:51,180 --> 00:48:55,380
uh the I mean the uh the signature so

1206
00:48:55,380 --> 00:48:57,240
probably it will be better to provide

1207
00:48:57,240 --> 00:48:59,460
those information but we don't have that

1208
00:48:59,460 --> 00:49:01,619
information at that time so we just use

1209
00:49:01,619 --> 00:49:05,280
any edited but also mentioned I I show

1210
00:49:05,280 --> 00:49:07,740
you some examples like the previous the

1211
00:49:07,740 --> 00:49:10,559
movie title to the image icon we just

1212
00:49:10,559 --> 00:49:12,900
provided two or three examples but the

1213
00:49:12,900 --> 00:49:15,540
model can do that so actually the during

1214
00:49:15,540 --> 00:49:17,819
the pre-training so we provide a lot of

1215
00:49:17,819 --> 00:49:20,819
not people I mean the open air provide a

1216
00:49:20,819 --> 00:49:22,859
lot of data to the model so it can

1217
00:49:22,859 --> 00:49:25,020
recognize not only those complete

1218
00:49:25,020 --> 00:49:27,780
malicious commands but also it uh I mean

1219
00:49:27,780 --> 00:49:30,119
recognize all relevant information from

1220
00:49:30,119 --> 00:49:32,760
many because in in your source code so

1221
00:49:32,760 --> 00:49:34,800
there will be some targeted ones as well

1222
00:49:34,800 --> 00:49:36,839
as description so it can recognize all

1223
00:49:36,839 --> 00:49:39,319
those information

1224
00:49:39,420 --> 00:49:41,220
yeah I mean I'll just say this is part

1225
00:49:41,220 --> 00:49:43,020
of a larger pipeline in which there's

1226
00:49:43,020 --> 00:49:44,700
you know so yeah you weren't seeing like

1227
00:49:44,700 --> 00:49:46,740
the pipeline which were matching

1228
00:49:46,740 --> 00:49:48,540
signatures against you know that's just

1229
00:49:48,540 --> 00:49:50,220
that's just outside of the scope of the

1230
00:49:50,220 --> 00:49:52,200
research but as a security company where

1231
00:49:52,200 --> 00:49:54,720
we match command lines against

1232
00:49:54,720 --> 00:49:56,280
in this case it wasn't your it was Sigma

1233
00:49:56,280 --> 00:49:59,339
but you know you know rule-based systems

1234
00:49:59,339 --> 00:50:01,079
I don't know how many oh please go ahead

1235
00:50:01,079 --> 00:50:02,760
yeah

1236
00:50:02,760 --> 00:50:04,339
um did you

1237
00:50:04,339 --> 00:50:07,260
fine-tune any of your models or were you

1238
00:50:07,260 --> 00:50:10,260
just using off-the-shelf models here and

1239
00:50:10,260 --> 00:50:13,079
secondly I interested how far away you

1240
00:50:13,079 --> 00:50:15,300
think we are from

1241
00:50:15,300 --> 00:50:18,839
super large neural Nets being used in

1242
00:50:18,839 --> 00:50:22,559
production to improve security yeah

1243
00:50:22,559 --> 00:50:26,220
actually the previous uh the span and

1244
00:50:26,220 --> 00:50:28,619
the uh the second command line

1245
00:50:28,619 --> 00:50:31,079
description we only use the fuchsia

1246
00:50:31,079 --> 00:50:33,240
Journey not fine-tuned but also we

1247
00:50:33,240 --> 00:50:35,099
tested with the fish uh the fine tuning

1248
00:50:35,099 --> 00:50:38,579
with Spam detection so with like about

1249
00:50:38,579 --> 00:50:41,780
1000 samples you get like about 99

1250
00:50:41,780 --> 00:50:44,760
accuracy with up when you fine-tune the

1251
00:50:44,760 --> 00:50:46,859
model so you can improve the product

1252
00:50:46,859 --> 00:50:50,460
with fine tuning and uh what was the uh

1253
00:50:50,460 --> 00:50:52,500
next

1254
00:50:52,500 --> 00:50:54,660
it was like how long is it going to take

1255
00:50:54,660 --> 00:50:57,599
oh yeah yeah the problem is actually so

1256
00:50:57,599 --> 00:50:59,400
you can improve the performance but the

1257
00:50:59,400 --> 00:51:02,700
problem is the model is really slow

1258
00:51:02,700 --> 00:51:05,760
so it is not good idea to use like a two

1259
00:51:05,760 --> 00:51:09,780
year detection problem but as a uh you

1260
00:51:09,780 --> 00:51:12,720
mentioned so in Source case we collect a

1261
00:51:12,720 --> 00:51:15,720
lot of samples and we close the uh and

1262
00:51:15,720 --> 00:51:17,760
then we select some represented samples

1263
00:51:17,760 --> 00:51:21,300
but so let's say that this one one cross

1264
00:51:21,300 --> 00:51:24,359
has a 1000 samples but we can select one

1265
00:51:24,359 --> 00:51:26,940
one command line and we can provide a

1266
00:51:26,940 --> 00:51:28,920
different product so so it will be

1267
00:51:28,920 --> 00:51:31,260
easier analyst to analyze that with the

1268
00:51:31,260 --> 00:51:33,480
human leader description so it has some

1269
00:51:33,480 --> 00:51:36,660
value but still the I mean the in terms

1270
00:51:36,660 --> 00:51:38,420
of the

1271
00:51:38,420 --> 00:51:43,200
uh the runtime yeah the speed is a

1272
00:51:43,200 --> 00:51:45,439
problem

1273
00:51:46,680 --> 00:51:49,220
I mean

1274
00:51:49,440 --> 00:51:50,880
so I

1275
00:51:50,880 --> 00:51:52,559
think I'm agreeing with young who here

1276
00:51:52,559 --> 00:51:54,660
basically but like I think that some of

1277
00:51:54,660 --> 00:51:55,980
this stuff like the stuff young who've

1278
00:51:55,980 --> 00:51:58,440
demonstrated about the descriptions

1279
00:51:58,440 --> 00:52:01,920
could be could be used in the short term

1280
00:52:01,920 --> 00:52:02,660
um

1281
00:52:02,660 --> 00:52:06,900
uh it again the question is volume uh

1282
00:52:06,900 --> 00:52:09,720
and cost so it you know we I don't know

1283
00:52:09,720 --> 00:52:11,339
how much we spend maybe at least

1284
00:52:11,339 --> 00:52:12,660
probably spend like a thousand a month

1285
00:52:12,660 --> 00:52:15,480
or something on like gpt3 based on you

1286
00:52:15,480 --> 00:52:18,480
know so we give an open AI you know 12

1287
00:52:18,480 --> 00:52:20,700
000 a year annual annualized um right

1288
00:52:20,700 --> 00:52:22,859
now just to do this research

1289
00:52:22,859 --> 00:52:25,319
um and that's just us messing around in

1290
00:52:25,319 --> 00:52:27,540
our research group not not deploying it

1291
00:52:27,540 --> 00:52:29,819
sofo scale where we've got you know half

1292
00:52:29,819 --> 00:52:31,980
a million customers deploying its scale

1293
00:52:31,980 --> 00:52:33,420
you know all of a sudden the costs

1294
00:52:33,420 --> 00:52:35,700
become really significant

1295
00:52:35,700 --> 00:52:37,740
um what that means I think is that it's

1296
00:52:37,740 --> 00:52:39,839
not that we can't use these models but

1297
00:52:39,839 --> 00:52:42,119
we need to use them in a discriminating

1298
00:52:42,119 --> 00:52:46,020
way uh is a good analogy as malware

1299
00:52:46,020 --> 00:52:47,880
sandboxes like you couldn't possibly run

1300
00:52:47,880 --> 00:52:49,319
every

1301
00:52:49,319 --> 00:52:51,780
um email attachment uh that you receive

1302
00:52:51,780 --> 00:52:54,480
in a customer base uh in a sandbox but

1303
00:52:54,480 --> 00:52:56,460
you you know you use the sandbox very

1304
00:52:56,460 --> 00:52:58,260
very selectively in cases where you

1305
00:52:58,260 --> 00:52:59,099
already know something is quite

1306
00:52:59,099 --> 00:53:00,540
suspicious in the case of the

1307
00:53:00,540 --> 00:53:02,220
description generator you know you might

1308
00:53:02,220 --> 00:53:04,319
own you know you'd only use that in the

1309
00:53:04,319 --> 00:53:07,319
context of new previously unseen alerts

1310
00:53:07,319 --> 00:53:08,880
that we think are really suspicious and

1311
00:53:08,880 --> 00:53:10,559
that and that have you know big command

1312
00:53:10,559 --> 00:53:11,700
lines attached to it that are hard to

1313
00:53:11,700 --> 00:53:13,319
parse you know

1314
00:53:13,319 --> 00:53:14,400
um you just have to work out the

1315
00:53:14,400 --> 00:53:15,900
economics of it so I think I think it's

1316
00:53:15,900 --> 00:53:17,160
possible we get deploy stuff like this

1317
00:53:17,160 --> 00:53:19,140
really soon but we have to use it very

1318
00:53:19,140 --> 00:53:20,760
sparingly and then in the future I think

1319
00:53:20,760 --> 00:53:22,800
you know hopefully costs will come come

1320
00:53:22,800 --> 00:53:25,099
down

1321
00:53:26,400 --> 00:53:27,119
um

1322
00:53:27,119 --> 00:53:28,319
so

1323
00:53:28,319 --> 00:53:30,300
I guess we can take one last question

1324
00:53:30,300 --> 00:53:34,040
and then I think we need to wrap it up

1325
00:53:36,000 --> 00:53:37,559
sorry it's not really not really an

1326
00:53:37,559 --> 00:53:39,240
exciting one but have you tried this

1327
00:53:39,240 --> 00:53:43,079
with the centroid analysis or what I

1328
00:53:43,079 --> 00:53:45,960
mean you gave an example of yeah yeah

1329
00:53:45,960 --> 00:53:47,700
yeah we tried it with sentiment analysis

1330
00:53:47,700 --> 00:53:49,559
can you say that so I mean sentiment

1331
00:53:49,559 --> 00:53:51,000
analysis so the question is have you

1332
00:53:51,000 --> 00:53:52,680
tried all this sentence NLP National

1333
00:53:52,680 --> 00:53:54,720
national language processing but more

1334
00:53:54,720 --> 00:53:58,020
with figuring out how someone is feeling

1335
00:53:58,020 --> 00:54:00,480
or their emotions behind a post yeah

1336
00:54:00,480 --> 00:54:02,400
yeah Instagram Twitter or something like

1337
00:54:02,400 --> 00:54:04,079
that so in case anybody didn't hear the

1338
00:54:04,079 --> 00:54:05,880
question is have we tried this um in

1339
00:54:05,880 --> 00:54:07,559
sentiment analysis like so if you have a

1340
00:54:07,559 --> 00:54:10,260
document uh you know categories that

1341
00:54:10,260 --> 00:54:11,160
document in terms of the kind of

1342
00:54:11,160 --> 00:54:13,440
sentiment reflected in it

1343
00:54:13,440 --> 00:54:14,640
um

1344
00:54:14,640 --> 00:54:18,599
uh yeah I haven't tried any I mean those

1345
00:54:18,599 --> 00:54:21,180
use case because I can only use our

1346
00:54:21,180 --> 00:54:24,059
money to type screen applications so we

1347
00:54:24,059 --> 00:54:27,180
only have monthly uh yeah two thousand

1348
00:54:27,180 --> 00:54:28,380
dollars

1349
00:54:28,380 --> 00:54:30,900
sorry to talk anyway so you can find the

1350
00:54:30,900 --> 00:54:33,480
example from the open air website so

1351
00:54:33,480 --> 00:54:35,400
there's a one example of a sentiment

1352
00:54:35,400 --> 00:54:38,579
analysis how you can do that yeah

1353
00:54:38,579 --> 00:54:40,140
yeah well I think we're out of time

1354
00:54:40,140 --> 00:54:41,520
thanks for your thanks for your

1355
00:54:41,520 --> 00:54:43,200
questions and comments everybody much

1356
00:54:43,200 --> 00:54:45,618
appreciated

