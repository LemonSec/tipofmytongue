1
00:00:00,240 --> 00:00:02,720
our first speaker to kick us off in this

2
00:00:02,720 --> 00:00:05,440
first return to in person besides las

3
00:00:05,440 --> 00:00:06,480
vegas

4
00:00:06,480 --> 00:00:08,080
is someone you probably know way better

5
00:00:08,080 --> 00:00:09,200
than me

6
00:00:09,200 --> 00:00:10,240
uh

7
00:00:10,240 --> 00:00:12,480
he's someone many of you have probably

8
00:00:12,480 --> 00:00:14,320
had announced the changes of your

9
00:00:14,320 --> 00:00:16,560
careers in your lives ladies and

10
00:00:16,560 --> 00:00:18,560
gentlemen please welcome chris hoff or

11
00:00:18,560 --> 00:00:21,680
beaker to the stage

12
00:00:27,599 --> 00:00:28,800
good morning

13
00:00:28,800 --> 00:00:30,800
i thought i got out of this with the

14
00:00:30,800 --> 00:00:33,840
speaker not working but unfortunately

15
00:00:33,840 --> 00:00:35,760
it is

16
00:00:35,760 --> 00:00:37,360
can you guys hear me okay with this bane

17
00:00:37,360 --> 00:00:39,040
mask on yeah

18
00:00:39,040 --> 00:00:40,719
all right fabulous

19
00:00:40,719 --> 00:00:42,320
let's see let's see if this can get this

20
00:00:42,320 --> 00:00:45,480
to work

21
00:00:47,680 --> 00:00:50,160
all right cool so uh they asked me for

22
00:00:50,160 --> 00:00:51,840
an abstract for this talk which is what

23
00:00:51,840 --> 00:00:52,719
the

24
00:00:52,719 --> 00:00:54,719
line underneath the fraudulence is i

25
00:00:54,719 --> 00:00:57,280
took a word cloud of words out of words

26
00:00:57,280 --> 00:00:59,280
out of my presentation and threw it at

27
00:00:59,280 --> 00:01:01,199
them so it actually means nothing the

28
00:01:01,199 --> 00:01:03,520
title fraudulence is a mashup of two

29
00:01:03,520 --> 00:01:05,519
words fragile and resistant and

30
00:01:05,519 --> 00:01:06,640
resilience

31
00:01:06,640 --> 00:01:08,799
and there are a couple of takeaways that

32
00:01:08,799 --> 00:01:10,960
i'd like to establish out front about

33
00:01:10,960 --> 00:01:12,799
what this talk is about and why i'm here

34
00:01:12,799 --> 00:01:14,640
and what i want you to take away from it

35
00:01:14,640 --> 00:01:16,320
the first is i have not been able to

36
00:01:16,320 --> 00:01:18,880
publicly speak in seven years that's

37
00:01:18,880 --> 00:01:21,119
what happens when you get subsumed by a

38
00:01:21,119 --> 00:01:23,680
risk-averse financial institution

39
00:01:23,680 --> 00:01:27,920
so uh not only is it b-sides back but uh

40
00:01:27,920 --> 00:01:29,759
i'm very thankful and grateful that i

41
00:01:29,759 --> 00:01:30,640
get to

42
00:01:30,640 --> 00:01:32,400
uh come up and and talk to folks and

43
00:01:32,400 --> 00:01:33,920
have ultimately a set of discussions

44
00:01:33,920 --> 00:01:36,880
after this um the second thing is i um

45
00:01:36,880 --> 00:01:38,720
there's a bunch of points in this talk

46
00:01:38,720 --> 00:01:41,280
that i think will come hopefully into

47
00:01:41,280 --> 00:01:44,000
into uh cognition as we as we

48
00:01:44,000 --> 00:01:45,520
run through it but it's about five

49
00:01:45,520 --> 00:01:47,040
different talks crammed into one so i'm

50
00:01:47,040 --> 00:01:50,000
making up for for for uh for miss time

51
00:01:50,000 --> 00:01:52,079
when i think about infosec there's a lot

52
00:01:52,079 --> 00:01:53,600
more art

53
00:01:53,600 --> 00:01:55,600
and compliance than there is science

54
00:01:55,600 --> 00:01:56,560
today

55
00:01:56,560 --> 00:01:59,200
uh when we do make use of science and

56
00:01:59,200 --> 00:02:01,360
when i and i use that term broadly it's

57
00:02:01,360 --> 00:02:03,200
often very siloed we aren't very

58
00:02:03,200 --> 00:02:04,479
organized

59
00:02:04,479 --> 00:02:06,240
in terms of how we take advantage of

60
00:02:06,240 --> 00:02:09,199
science we don't define model or manage

61
00:02:09,199 --> 00:02:11,120
risk well at all

62
00:02:11,120 --> 00:02:13,520
uh we are not agile

63
00:02:13,520 --> 00:02:15,920
and our definition of resilience varies

64
00:02:15,920 --> 00:02:17,840
and it is insufficient this word

65
00:02:17,840 --> 00:02:19,760
resilience is sort of critical to this

66
00:02:19,760 --> 00:02:21,200
conversation

67
00:02:21,200 --> 00:02:23,440
uh instead of resilient and you some of

68
00:02:23,440 --> 00:02:25,040
you may moan or groan when you hear this

69
00:02:25,040 --> 00:02:28,000
term but we ought to be anti-fragile

70
00:02:28,000 --> 00:02:30,480
and anti-fragile is an interesting topic

71
00:02:30,480 --> 00:02:31,760
that i'm going to get into in this talk

72
00:02:31,760 --> 00:02:34,000
because it talks about both the past the

73
00:02:34,000 --> 00:02:36,160
present and the future of where

74
00:02:36,160 --> 00:02:38,080
information security is headed

75
00:02:38,080 --> 00:02:39,519
and hopefully the thought process is

76
00:02:39,519 --> 00:02:41,599
here they don't pretend i don't pretend

77
00:02:41,599 --> 00:02:44,160
to be right what i do hope is that we

78
00:02:44,160 --> 00:02:45,440
start a set of conversations to get

79
00:02:45,440 --> 00:02:46,879
people to recognize that things aren't

80
00:02:46,879 --> 00:02:48,800
quite as bad as people make them out to

81
00:02:48,800 --> 00:02:50,640
be number one and we have an awful lot

82
00:02:50,640 --> 00:02:52,319
more at our disposal than most people

83
00:02:52,319 --> 00:02:53,920
actually recognize

84
00:02:53,920 --> 00:02:56,000
so with that this is not one of those

85
00:02:56,000 --> 00:02:57,840
grumpy oh my god infosec has broken

86
00:02:57,840 --> 00:02:58,800
talks

87
00:02:58,800 --> 00:03:00,959
um although i it sounded like it from

88
00:03:00,959 --> 00:03:02,239
the introduction

89
00:03:02,239 --> 00:03:04,640
but what it is is a discussion about

90
00:03:04,640 --> 00:03:06,800
words so there's a bunch of words that

91
00:03:06,800 --> 00:03:09,120
get used interchangeably these days

92
00:03:09,120 --> 00:03:12,959
secure or security resilience uh robust

93
00:03:12,959 --> 00:03:14,879
survivable recoverable defensible

94
00:03:14,879 --> 00:03:17,920
certain sustainable resilient has become

95
00:03:17,920 --> 00:03:19,519
or resilience has become the new

96
00:03:19,519 --> 00:03:21,680
buzzword of our industry a lot of it is

97
00:03:21,680 --> 00:03:23,200
because of the types of attacks and the

98
00:03:23,200 --> 00:03:24,959
outcomes that have happened so when you

99
00:03:24,959 --> 00:03:27,040
look at things like supply chain attacks

100
00:03:27,040 --> 00:03:28,720
nation states when you look at what

101
00:03:28,720 --> 00:03:30,560
we're seeing with with ransomware no

102
00:03:30,560 --> 00:03:32,239
longer is it about being secure it's

103
00:03:32,239 --> 00:03:34,080
about being resilient and the definition

104
00:03:34,080 --> 00:03:36,799
of resilient is is interesting um if you

105
00:03:36,799 --> 00:03:40,080
look at what nist defines the um

106
00:03:40,080 --> 00:03:41,680
the government agency ultimately that

107
00:03:41,680 --> 00:03:43,200
that defines a lot of things for us and

108
00:03:43,200 --> 00:03:44,879
standards that we use

109
00:03:44,879 --> 00:03:46,560
these words are important the ability to

110
00:03:46,560 --> 00:03:49,200
anticipate withstand recover from and

111
00:03:49,200 --> 00:03:51,760
adapt to bad things from happening

112
00:03:51,760 --> 00:03:53,200
the stuff on the left which will be hard

113
00:03:53,200 --> 00:03:54,239
to read but i'll give you all these

114
00:03:54,239 --> 00:03:55,599
slides later it just talks about the

115
00:03:55,599 --> 00:03:56,959
context so you talk about network

116
00:03:56,959 --> 00:03:59,200
security nation st are you talking about

117
00:03:59,200 --> 00:04:01,920
um critical infrastructure applications

118
00:04:01,920 --> 00:04:03,760
people etc

119
00:04:03,760 --> 00:04:06,000
the table on the right which ultimately

120
00:04:06,000 --> 00:04:08,400
came from accenture was about goals and

121
00:04:08,400 --> 00:04:10,799
outcomes from from the perspective of

122
00:04:10,799 --> 00:04:12,080
performance characteristics of what

123
00:04:12,080 --> 00:04:14,480
happens if you do resilience well so if

124
00:04:14,480 --> 00:04:15,840
you look at that chart you'll see oh if

125
00:04:15,840 --> 00:04:17,279
you are more resilient you will stop

126
00:04:17,279 --> 00:04:19,120
more attacks you will find breaches

127
00:04:19,120 --> 00:04:20,880
faster you will fix breaches faster and

128
00:04:20,880 --> 00:04:23,440
you will reduce breach impact

129
00:04:23,440 --> 00:04:25,520
but the the challenge with what is meant

130
00:04:25,520 --> 00:04:27,040
by resilience

131
00:04:27,040 --> 00:04:29,360
came to a head for me about well not me

132
00:04:29,360 --> 00:04:31,759
in this fictional guy we'll call chris

133
00:04:31,759 --> 00:04:33,680
uh in in let's say a financial

134
00:04:33,680 --> 00:04:36,160
institution about five years ago

135
00:04:36,160 --> 00:04:38,960
showed up uh chris did uh to this uh

136
00:04:38,960 --> 00:04:40,000
regulator meeting where they were

137
00:04:40,000 --> 00:04:41,680
talking about

138
00:04:41,680 --> 00:04:44,639
a horizontal across these gcps which is

139
00:04:44,639 --> 00:04:46,400
globally significant financial

140
00:04:46,400 --> 00:04:47,520
institutions systemic financial

141
00:04:47,520 --> 00:04:49,280
institutions there are

142
00:04:49,280 --> 00:04:50,560
eight of them in the united states by

143
00:04:50,560 --> 00:04:52,400
the way they're not publicly named but

144
00:04:52,400 --> 00:04:54,160
you can probably guess who they are

145
00:04:54,160 --> 00:04:56,639
and the question that they ran through

146
00:04:56,639 --> 00:04:58,960
this in terms of scenario modeling was

147
00:04:58,960 --> 00:05:00,960
uh listen we understand what you do from

148
00:05:00,960 --> 00:05:02,479
a security program perspective we

149
00:05:02,479 --> 00:05:04,400
understand all the controls you have etc

150
00:05:04,400 --> 00:05:06,400
we want you to assume that all of your

151
00:05:06,400 --> 00:05:08,160
controls have failed and you are a

152
00:05:08,160 --> 00:05:10,400
smoldering pile of rubble

153
00:05:10,400 --> 00:05:12,880
and you have to rebuild and recover and

154
00:05:12,880 --> 00:05:14,560
restore

155
00:05:14,560 --> 00:05:15,680
and they looked at me and they said what

156
00:05:15,680 --> 00:05:18,080
do you do and i said besides the normal

157
00:05:18,080 --> 00:05:20,240
sort of cliche you know update my resume

158
00:05:20,240 --> 00:05:21,919
and crack open that bottle of bourbon

159
00:05:21,919 --> 00:05:24,800
was i don't do a lot i'm an infosec

160
00:05:24,800 --> 00:05:26,160
besides securing the environment in

161
00:05:26,160 --> 00:05:28,000
which the owners of applications and

162
00:05:28,000 --> 00:05:30,639
services would restore like i don't do

163
00:05:30,639 --> 00:05:32,240
much and they said well who does and i

164
00:05:32,240 --> 00:05:33,600
said the application service owners in

165
00:05:33,600 --> 00:05:35,199
i.t and they turned and i watched this

166
00:05:35,199 --> 00:05:37,199
imaginary turret turn towards them and

167
00:05:37,199 --> 00:05:39,199
they said okay so out of all the app and

168
00:05:39,199 --> 00:05:40,800
critical service owners

169
00:05:40,800 --> 00:05:43,680
purportedly in this bank which you know

170
00:05:43,680 --> 00:05:44,880
who's the most important and they all

171
00:05:44,880 --> 00:05:46,479
raise their hands

172
00:05:46,479 --> 00:05:48,240
and they said okay we're gonna pick one

173
00:05:48,240 --> 00:05:50,080
payments okay payments

174
00:05:50,080 --> 00:05:52,800
who's the owner me this guy says he goes

175
00:05:52,800 --> 00:05:54,880
which one of the sub-component services

176
00:05:54,880 --> 00:05:56,880
do you get back up and running

177
00:05:56,880 --> 00:05:58,880
and of course there's like 200 of them

178
00:05:58,880 --> 00:06:00,000
and he couldn't really answer the

179
00:06:00,000 --> 00:06:02,160
question that kicked off a set of the

180
00:06:02,160 --> 00:06:04,400
portaly uh these things called mras

181
00:06:04,400 --> 00:06:06,160
matters requiring attention across all

182
00:06:06,160 --> 00:06:07,199
these banks that talked about you're

183
00:06:07,199 --> 00:06:08,960
doing resilience wrong

184
00:06:08,960 --> 00:06:10,319
which means you need to be able to

185
00:06:10,319 --> 00:06:11,840
recover rebuild within these service

186
00:06:11,840 --> 00:06:13,039
level agreements to make sure there

187
00:06:13,039 --> 00:06:14,639
aren't impacts to the public

188
00:06:14,639 --> 00:06:17,360
at the same time in the frb that was the

189
00:06:17,360 --> 00:06:19,039
us base the bank of england was talking

190
00:06:19,039 --> 00:06:20,960
about no no resilience means you should

191
00:06:20,960 --> 00:06:23,120
never go down in the first place

192
00:06:23,120 --> 00:06:24,960
so if you if you look at what we talked

193
00:06:24,960 --> 00:06:26,880
about right anticipate with staying

194
00:06:26,880 --> 00:06:28,319
recover from and adapt to adverse

195
00:06:28,319 --> 00:06:31,199
conditions the definition by itself even

196
00:06:31,199 --> 00:06:33,039
with regulators today that drives a lot

197
00:06:33,039 --> 00:06:34,960
of what people do in terms of their

198
00:06:34,960 --> 00:06:37,600
security programs is out of date and the

199
00:06:37,600 --> 00:06:39,600
challenge is that if we don't understand

200
00:06:39,600 --> 00:06:41,039
that word and it's supposed to be the

201
00:06:41,039 --> 00:06:42,960
next evolution of security how do we

202
00:06:42,960 --> 00:06:44,880
even know what we're aiming for

203
00:06:44,880 --> 00:06:46,960
so a couple years ago

204
00:06:46,960 --> 00:06:49,440
uh whether you're a fan of him or not um

205
00:06:49,440 --> 00:06:51,759
nicholas taleb came out with a book uh

206
00:06:51,759 --> 00:06:54,000
titled the black swan and black swan

207
00:06:54,000 --> 00:06:56,080
events are events that

208
00:06:56,080 --> 00:06:57,680
in the middle of things in the fog of

209
00:06:57,680 --> 00:06:59,840
war that you could not anticipate ever

210
00:06:59,840 --> 00:07:01,680
happening but in hindsight you're like

211
00:07:01,680 --> 00:07:03,280
oh yeah of course we could have

212
00:07:03,280 --> 00:07:04,720
seen that we could have prevented that

213
00:07:04,720 --> 00:07:06,400
great examples the reason we're wearing

214
00:07:06,400 --> 00:07:07,360
masks

215
00:07:07,360 --> 00:07:11,120
pandemic 911 uh for example the

216
00:07:11,120 --> 00:07:13,199
interesting thing is in hindsight planes

217
00:07:13,199 --> 00:07:14,880
crashed into buildings like the empire

218
00:07:14,880 --> 00:07:16,560
state buildings like the empire state

219
00:07:16,560 --> 00:07:18,240
building we've seen methods but the

220
00:07:18,240 --> 00:07:20,000
motive and opportunity and outcomes are

221
00:07:20,000 --> 00:07:22,080
very different so in hindsight you look

222
00:07:22,080 --> 00:07:23,440
at a black swan event it's something you

223
00:07:23,440 --> 00:07:25,520
can't really predict and ultimately the

224
00:07:25,520 --> 00:07:27,680
outcome is much more is largely greater

225
00:07:27,680 --> 00:07:29,199
than instead of being proportional with

226
00:07:29,199 --> 00:07:31,120
with with the event itself and so

227
00:07:31,120 --> 00:07:33,280
hindsight is a very interesting uh

228
00:07:33,280 --> 00:07:35,840
uh part of that conversation because at

229
00:07:35,840 --> 00:07:37,599
he he uh talab went on to write a book

230
00:07:37,599 --> 00:07:39,520
called anti-fragile and what he stated

231
00:07:39,520 --> 00:07:42,240
in that book was the the primary uh

232
00:07:42,240 --> 00:07:45,280
uh thing uh a part of of what is the

233
00:07:45,280 --> 00:07:47,039
definition of anti-fragility which is

234
00:07:47,039 --> 00:07:49,440
that some things benefit from shocks

235
00:07:49,440 --> 00:07:53,280
they get better they don't just recover

236
00:07:53,280 --> 00:07:54,720
human bones are a great example you

237
00:07:54,720 --> 00:07:56,560
break a bone your bones actually calcify

238
00:07:56,560 --> 00:07:57,759
and become stronger than they were in

239
00:07:57,759 --> 00:08:00,639
the in the first place so anti-fragility

240
00:08:00,639 --> 00:08:02,800
which is a huge part of the conversation

241
00:08:02,800 --> 00:08:04,560
we're going to have is about getting

242
00:08:04,560 --> 00:08:06,560
better in the face of stressors

243
00:08:06,560 --> 00:08:08,560
not just remaining the same and that you

244
00:08:08,560 --> 00:08:10,080
can visualize that by looking at this

245
00:08:10,080 --> 00:08:10,960
graph

246
00:08:10,960 --> 00:08:12,639
uh ultimately if gain and benefit is the

247
00:08:12,639 --> 00:08:15,520
y-axis and and stress and change is the

248
00:08:15,520 --> 00:08:16,720
x-axis

249
00:08:16,720 --> 00:08:18,319
normally what you'll see when people use

250
00:08:18,319 --> 00:08:20,080
the word fragile is it's a concave

251
00:08:20,080 --> 00:08:22,000
response to stress you break and you

252
00:08:22,000 --> 00:08:23,759
don't get better if you look at

253
00:08:23,759 --> 00:08:26,160
ultimately resistance or robustness

254
00:08:26,160 --> 00:08:28,400
which is a natural response you resist

255
00:08:28,400 --> 00:08:29,599
you get you get a little bit better for

256
00:08:29,599 --> 00:08:31,039
time but then you go back to being how

257
00:08:31,039 --> 00:08:33,039
you were ultimately if you look at

258
00:08:33,039 --> 00:08:35,440
resilient you recover

259
00:08:35,440 --> 00:08:37,279
for and and you get better for a while

260
00:08:37,279 --> 00:08:39,039
or you improve for a while but then you

261
00:08:39,039 --> 00:08:40,719
ultimately go back doing what you want

262
00:08:40,719 --> 00:08:42,399
the key here is that anti-fragile

263
00:08:42,399 --> 00:08:45,040
fragility is convex in response

264
00:08:45,040 --> 00:08:46,320
you get better

265
00:08:46,320 --> 00:08:47,920
consistently

266
00:08:47,920 --> 00:08:49,920
right you actually not only just learn

267
00:08:49,920 --> 00:08:52,480
but you become stronger from the

268
00:08:52,480 --> 00:08:53,519
stressor

269
00:08:53,519 --> 00:08:56,720
so if you look at how anti-fragility uh

270
00:08:56,720 --> 00:08:58,480
is more interestingly defined on the

271
00:08:58,480 --> 00:09:00,640
right you have this notion of disorder

272
00:09:00,640 --> 00:09:02,320
volatility and randomness and your

273
00:09:02,320 --> 00:09:04,240
responses to them so on the side of

274
00:09:04,240 --> 00:09:06,480
being agile and anti-fragile you

275
00:09:06,480 --> 00:09:09,440
ultimately uh embrace change and you

276
00:09:09,440 --> 00:09:11,680
gain or benefit from it you you

277
00:09:11,680 --> 00:09:13,440
ultimately get better you absorb shock

278
00:09:13,440 --> 00:09:16,959
and get better in the face of stressors

279
00:09:16,959 --> 00:09:19,600
trigger word security is ultimately

280
00:09:19,600 --> 00:09:21,360
all the way to the left in the most part

281
00:09:21,360 --> 00:09:22,480
which is

282
00:09:22,480 --> 00:09:25,760
we try to get to at least a base level

283
00:09:25,760 --> 00:09:27,760
that we feel comfortable with we don't

284
00:09:27,760 --> 00:09:30,720
like change uh ultimately we don't adapt

285
00:09:30,720 --> 00:09:32,480
very well we resist change if you think

286
00:09:32,480 --> 00:09:34,560
about controls and you basically stay

287
00:09:34,560 --> 00:09:36,640
the same not that we don't learn and

288
00:09:36,640 --> 00:09:38,320
ultimately incrementally get better but

289
00:09:38,320 --> 00:09:40,399
we don't fundamentally change the system

290
00:09:40,399 --> 00:09:42,240
itself does not get stronger

291
00:09:42,240 --> 00:09:44,320
so with that the the notion here is if

292
00:09:44,320 --> 00:09:46,080
you start thinking about what that means

293
00:09:46,080 --> 00:09:48,399
to our practice and our industry

294
00:09:48,399 --> 00:09:50,959
um you could you could uh posit that

295
00:09:50,959 --> 00:09:53,600
that infosec is actually fragile but

296
00:09:53,600 --> 00:09:55,120
without sounding like that's a terrible

297
00:09:55,120 --> 00:09:56,080
negative thing there's a lot of

298
00:09:56,080 --> 00:09:58,160
opportunity there and in fact those

299
00:09:58,160 --> 00:10:01,279
opportunities have come from decades of

300
00:10:01,279 --> 00:10:03,279
work that have been done in the past so

301
00:10:03,279 --> 00:10:05,519
there was a paper written in 1999

302
00:10:05,519 --> 00:10:06,720
that was titled information

303
00:10:06,720 --> 00:10:08,880
survivability and it basically talked

304
00:10:08,880 --> 00:10:11,279
about how the world was changing uh and

305
00:10:11,279 --> 00:10:12,640
on the left you can see for example

306
00:10:12,640 --> 00:10:13,839
systems are centrally networked and

307
00:10:13,839 --> 00:10:15,519
under organizational controls they will

308
00:10:15,519 --> 00:10:17,040
transition to systems that are globally

309
00:10:17,040 --> 00:10:18,959
networked with distributed control

310
00:10:18,959 --> 00:10:20,320
right the internet

311
00:10:20,320 --> 00:10:22,959
and without barfing things like web3

312
00:10:22,959 --> 00:10:24,640
you can go down this list and really

313
00:10:24,640 --> 00:10:26,640
understand that at the bottom of it it

314
00:10:26,640 --> 00:10:28,480
talked about that today security is seen

315
00:10:28,480 --> 00:10:30,800
as an overhead or an expense versus

316
00:10:30,800 --> 00:10:32,720
survivability as an investment and

317
00:10:32,720 --> 00:10:34,560
essential to the organization and that

318
00:10:34,560 --> 00:10:36,240
technology it-based solutions have to

319
00:10:36,240 --> 00:10:38,320
become enterprise-wide and risk-based

320
00:10:38,320 --> 00:10:39,920
now we look at that today and we go well

321
00:10:39,920 --> 00:10:41,200
that makes a total bunch of sense in

322
00:10:41,200 --> 00:10:43,440
1999 that was sort of heresy

323
00:10:43,440 --> 00:10:45,519
but the people that wrote this

324
00:10:45,519 --> 00:10:47,200
thought about what it meant in the

325
00:10:47,200 --> 00:10:49,279
evolution from both secure to survivable

326
00:10:49,279 --> 00:10:51,279
to anti-fragile and one of the

327
00:10:51,279 --> 00:10:53,920
challenges that when i looked at why we

328
00:10:53,920 --> 00:10:56,000
don't or can't or have a difficult time

329
00:10:56,000 --> 00:10:58,720
getting better it's we seem to conflate

330
00:10:58,720 --> 00:10:59,920
these

331
00:10:59,920 --> 00:11:02,000
tactical information

332
00:11:02,000 --> 00:11:03,920
frameworks sort of the what

333
00:11:03,920 --> 00:11:06,240
with this set of guidelines and

334
00:11:06,240 --> 00:11:08,399
playbooks that are linear in action

335
00:11:08,399 --> 00:11:11,600
chains the how with decision systems for

336
00:11:11,600 --> 00:11:12,880
complex system problems that actually

337
00:11:12,880 --> 00:11:15,519
tell us about the why when and who

338
00:11:15,519 --> 00:11:17,279
now that may not make a lot of sense in

339
00:11:17,279 --> 00:11:18,640
one slide but let me let me let me

340
00:11:18,640 --> 00:11:20,959
clarify that today if you look at this

341
00:11:20,959 --> 00:11:24,640
this sort of um spectrum of of sensors

342
00:11:24,640 --> 00:11:26,720
sense making decision systems and levers

343
00:11:26,720 --> 00:11:29,120
and actuators we have an enormous amount

344
00:11:29,120 --> 00:11:30,880
of sensors help take whatever you want

345
00:11:30,880 --> 00:11:32,079
in terms of instrumentation or

346
00:11:32,079 --> 00:11:33,760
observability and define it that way in

347
00:11:33,760 --> 00:11:36,880
a shotgun across the stack we have

348
00:11:36,880 --> 00:11:39,360
many different sense making capabilities

349
00:11:39,360 --> 00:11:41,839
lots of consoles lots of ways of single

350
00:11:41,839 --> 00:11:43,360
panes of glass that ultimately allow you

351
00:11:43,360 --> 00:11:45,120
to visualize what you're seeing

352
00:11:45,120 --> 00:11:47,839
we tend to skip decision systems

353
00:11:47,839 --> 00:11:49,600
that allow us to make decisions on risk

354
00:11:49,600 --> 00:11:51,440
and instead head straight towards with

355
00:11:51,440 --> 00:11:52,800
the application of some level of

356
00:11:52,800 --> 00:11:55,839
automation these levers and actuators

357
00:11:55,839 --> 00:11:57,279
right and you'll note the words on the

358
00:11:57,279 --> 00:11:58,959
top you'll see them again later probe

359
00:11:58,959 --> 00:12:01,040
and sense categorize and analyze weigh

360
00:12:01,040 --> 00:12:03,200
and decide and act and respond now i'll

361
00:12:03,200 --> 00:12:05,200
say having run large-scale security

362
00:12:05,200 --> 00:12:07,279
operations teams being able to have the

363
00:12:07,279 --> 00:12:08,880
time and luxury

364
00:12:08,880 --> 00:12:10,800
to make decisions in the fog of war is a

365
00:12:10,800 --> 00:12:12,800
very difficult thing but a lot of that

366
00:12:12,800 --> 00:12:15,680
has to do with the velocity the volume

367
00:12:15,680 --> 00:12:17,440
and the quality of the data we capture

368
00:12:17,440 --> 00:12:18,480
in the first place and we're going to

369
00:12:18,480 --> 00:12:20,079
get into that in a little bit because

370
00:12:20,079 --> 00:12:21,360
we're going to focus on the fact that

371
00:12:21,360 --> 00:12:24,240
the reason i posit that we are largely

372
00:12:24,240 --> 00:12:25,600
fragile

373
00:12:25,600 --> 00:12:27,600
and not anti-fragile is that we don't

374
00:12:27,600 --> 00:12:29,839
take time to invest in decision systems

375
00:12:29,839 --> 00:12:32,160
to actually make risk-based decisions

376
00:12:32,160 --> 00:12:34,560
in fact just the opposite

377
00:12:34,560 --> 00:12:36,320
this is just a small sampling from a

378
00:12:36,320 --> 00:12:38,560
company name a reseller called optiv and

379
00:12:38,560 --> 00:12:40,160
these are the products that they sell

380
00:12:40,160 --> 00:12:42,160
there are literally thousands upon

381
00:12:42,160 --> 00:12:44,399
thousands of sensors sense making and

382
00:12:44,399 --> 00:12:46,240
levers and actuators you can buy today

383
00:12:46,240 --> 00:12:47,680
and each of them

384
00:12:47,680 --> 00:12:50,079
claim to be open and integrated etc etc

385
00:12:50,079 --> 00:12:52,000
but the reality is none of them really

386
00:12:52,000 --> 00:12:53,120
feed into

387
00:12:53,120 --> 00:12:54,560
a methodology that lets you make a

388
00:12:54,560 --> 00:12:55,600
decision

389
00:12:55,600 --> 00:12:57,760
so with that being the case

390
00:12:57,760 --> 00:12:58,560
uh

391
00:12:58,560 --> 00:13:00,160
just an illustration i went through and

392
00:13:00,160 --> 00:13:01,920
i sort of did a quick inventory of a lot

393
00:13:01,920 --> 00:13:03,680
of different inventory of systems from

394
00:13:03,680 --> 00:13:06,880
it asset management to graph base uh

395
00:13:06,880 --> 00:13:09,360
functionality in clouds to vulnerability

396
00:13:09,360 --> 00:13:11,600
management platforms you name it there's

397
00:13:11,600 --> 00:13:14,000
tons and tons of different ways we can

398
00:13:14,000 --> 00:13:15,440
think about visualizing the data we get

399
00:13:15,440 --> 00:13:18,880
from sensors we have the uh a linear

400
00:13:18,880 --> 00:13:21,519
sort of view in lockheed martin's cyber

401
00:13:21,519 --> 00:13:24,000
kill chain that says that well this is

402
00:13:24,000 --> 00:13:26,720
the methodology by which every adversary

403
00:13:26,720 --> 00:13:29,440
uh basically enacts his his or her craft

404
00:13:29,440 --> 00:13:31,600
and it's always linear when we in fact

405
00:13:31,600 --> 00:13:33,600
we know it's not in many cases it's

406
00:13:33,600 --> 00:13:34,720
iterative

407
00:13:34,720 --> 00:13:37,519
we have great tools like mitre attack

408
00:13:37,519 --> 00:13:39,360
which allows us to talk about the and

409
00:13:39,360 --> 00:13:40,639
categorize the knowledge base of

410
00:13:40,639 --> 00:13:42,399
adversary tactics which lets us think

411
00:13:42,399 --> 00:13:45,360
about how adversaries may indeed be

412
00:13:45,360 --> 00:13:47,680
looking at exploiting us we have cool

413
00:13:47,680 --> 00:13:49,279
things like detect and attack flow and i

414
00:13:49,279 --> 00:13:50,639
think gabe bassett's giving a talk on

415
00:13:50,639 --> 00:13:52,320
attack flow later which allows you to

416
00:13:52,320 --> 00:13:54,480
visualize and map your defenses against

417
00:13:54,480 --> 00:13:56,160
attack to know how well sorted you are

418
00:13:56,160 --> 00:13:57,920
against a particular adversary and then

419
00:13:57,920 --> 00:13:59,760
actually even visualize it

420
00:13:59,760 --> 00:14:02,560
uh so go go look at gabe's uh go attend

421
00:14:02,560 --> 00:14:05,199
gabe's talk on on attack flow later

422
00:14:05,199 --> 00:14:06,800
interestingly we have the new cyber

423
00:14:06,800 --> 00:14:08,320
security framework which again says you

424
00:14:08,320 --> 00:14:10,240
should identify protect attack respond

425
00:14:10,240 --> 00:14:12,079
and recover and it's always linear but

426
00:14:12,079 --> 00:14:14,079
it's not in fact you go back and forth a

427
00:14:14,079 --> 00:14:16,160
lot of times we have sunil u's cyber

428
00:14:16,160 --> 00:14:18,399
defense matrix which takes the

429
00:14:18,399 --> 00:14:20,079
uh inventory of nist from the

430
00:14:20,079 --> 00:14:21,839
perspective of those categories and maps

431
00:14:21,839 --> 00:14:23,120
them across domains and lets you

432
00:14:23,120 --> 00:14:25,760
visualize where you have clusters of

433
00:14:25,760 --> 00:14:27,279
technology

434
00:14:27,279 --> 00:14:30,480
and or human-based responses to these

435
00:14:30,480 --> 00:14:32,160
particular

436
00:14:32,160 --> 00:14:33,680
sets of

437
00:14:33,680 --> 00:14:35,920
of threats and allows you to understand

438
00:14:35,920 --> 00:14:37,199
where you're weak you will notice that

439
00:14:37,199 --> 00:14:38,639
in most cases if you were to map this in

440
00:14:38,639 --> 00:14:40,320
your environment you would be you would

441
00:14:40,320 --> 00:14:42,720
have very few things in the respond and

442
00:14:42,720 --> 00:14:44,079
recover or at least in the recover

443
00:14:44,079 --> 00:14:45,519
category because it's mostly people

444
00:14:45,519 --> 00:14:46,800
focused

445
00:14:46,800 --> 00:14:49,120
um we have great things in in the in the

446
00:14:49,120 --> 00:14:50,720
vein of threat modeling we've got

447
00:14:50,720 --> 00:14:53,440
structured processes to do this we have

448
00:14:53,440 --> 00:14:54,959
pick your pick your favorite model

449
00:14:54,959 --> 00:14:57,040
stride dread pasta you name it so we

450
00:14:57,040 --> 00:14:58,639
have all the capability to essentially

451
00:14:58,639 --> 00:15:00,560
be able to document how systems ought to

452
00:15:00,560 --> 00:15:01,360
work

453
00:15:01,360 --> 00:15:03,680
we have debiakio's pyramid of pain and

454
00:15:03,680 --> 00:15:05,279
the effects and outcomes in terms of how

455
00:15:05,279 --> 00:15:07,920
we could degrade deny destroy or

456
00:15:07,920 --> 00:15:09,040
otherwise

457
00:15:09,040 --> 00:15:10,880
disrupt adversaries based on the

458
00:15:10,880 --> 00:15:13,040
availability or unavailability of

459
00:15:13,040 --> 00:15:14,720
certain iocs

460
00:15:14,720 --> 00:15:17,760
we even have amazing uh var model based

461
00:15:17,760 --> 00:15:19,440
frameworks for doing quantitative based

462
00:15:19,440 --> 00:15:21,279
risk management and risk assessment what

463
00:15:21,279 --> 00:15:22,959
that really means is you're able to get

464
00:15:22,959 --> 00:15:25,519
to and express dollars of risk based on

465
00:15:25,519 --> 00:15:27,680
the probability and frequency

466
00:15:27,680 --> 00:15:29,440
and magnitude of loss and actually

467
00:15:29,440 --> 00:15:31,839
attribute dollar amounts to what a

468
00:15:31,839 --> 00:15:34,639
particular event could bring you

469
00:15:34,639 --> 00:15:37,040
so i just shotgunned you and fire hosed

470
00:15:37,040 --> 00:15:38,560
you with an enormous amount of stuff we

471
00:15:38,560 --> 00:15:39,680
have

472
00:15:39,680 --> 00:15:40,959
and you would say well if that's the

473
00:15:40,959 --> 00:15:42,720
case then why can't we make good

474
00:15:42,720 --> 00:15:45,600
decisions quickly and a lot of that

475
00:15:45,600 --> 00:15:48,000
would be answered by these four guys so

476
00:15:48,000 --> 00:15:50,000
dave snowden not related to edward

477
00:15:50,000 --> 00:15:52,160
snowden james rasmussen nicholas talab

478
00:15:52,160 --> 00:15:54,240
and simon wardley incredibly smart

479
00:15:54,240 --> 00:15:56,240
people i only share two

480
00:15:56,240 --> 00:15:57,120
um

481
00:15:57,120 --> 00:15:58,800
two things with them which is a receding

482
00:15:58,800 --> 00:16:00,720
highlight uh hairline and graying of the

483
00:16:00,720 --> 00:16:03,199
beard but i am fans of the work and

484
00:16:03,199 --> 00:16:04,399
we're going to cover some of these in a

485
00:16:04,399 --> 00:16:06,639
second in a very rapid-fire way because

486
00:16:06,639 --> 00:16:08,240
i want you to think about tools and

487
00:16:08,240 --> 00:16:09,839
whether you're even thinking about using

488
00:16:09,839 --> 00:16:11,759
these tools so simon wardley

489
00:16:11,759 --> 00:16:13,360
built this process and he open sourced

490
00:16:13,360 --> 00:16:15,600
it called worldly maps and it's a way of

491
00:16:15,600 --> 00:16:17,839
mapping strategy a lot of people use it

492
00:16:17,839 --> 00:16:19,040
for business but you can literally use

493
00:16:19,040 --> 00:16:21,360
it to map anything and it basically uses

494
00:16:21,360 --> 00:16:22,880
a bunch of doctrines and rules to allow

495
00:16:22,880 --> 00:16:25,519
you to understand based on a user's a

496
00:16:25,519 --> 00:16:28,160
customer or user's needs chain a set of

497
00:16:28,160 --> 00:16:30,560
needs and dependencies and components

498
00:16:30,560 --> 00:16:31,839
what that allows you to do is make

499
00:16:31,839 --> 00:16:33,759
really interesting decisions

500
00:16:33,759 --> 00:16:36,160
um in a relatively short period of time

501
00:16:36,160 --> 00:16:37,680
now there's a statement that says all

502
00:16:37,680 --> 00:16:39,360
maps are wrong and some are useful and

503
00:16:39,360 --> 00:16:41,920
this is a true statement because it is a

504
00:16:41,920 --> 00:16:45,839
very uh we'll call it personal um uh uh

505
00:16:45,839 --> 00:16:48,320
thing a map now maps are good if you

506
00:16:48,320 --> 00:16:50,399
know where you are if you don't know

507
00:16:50,399 --> 00:16:51,759
where you are it's very hard to know

508
00:16:51,759 --> 00:16:53,040
even if you know where you're going how

509
00:16:53,040 --> 00:16:54,079
you're going to get there right so

510
00:16:54,079 --> 00:16:55,519
that's where mapping comes into play

511
00:16:55,519 --> 00:16:57,040
this particular map was open sourced and

512
00:16:57,040 --> 00:16:59,279
published on the web it was a team that

513
00:16:59,279 --> 00:17:00,959
was looking at the executive management

514
00:17:00,959 --> 00:17:03,120
as a decision maker or customer with

515
00:17:03,120 --> 00:17:05,359
drivers like regulatory compliance they

516
00:17:05,359 --> 00:17:06,959
need to prevent breaches detect reaches

517
00:17:06,959 --> 00:17:08,640
and manage breaches and they took those

518
00:17:08,640 --> 00:17:11,280
components over a level over an x-axis

519
00:17:11,280 --> 00:17:13,439
of maturity of the component and and

520
00:17:13,439 --> 00:17:15,760
that chain if you follow like regulatory

521
00:17:15,760 --> 00:17:16,959
compliance

522
00:17:16,959 --> 00:17:19,039
depends on having

523
00:17:19,039 --> 00:17:21,199
automated control monitoring but that

524
00:17:21,199 --> 00:17:23,280
particular component from genesis to

525
00:17:23,280 --> 00:17:25,679
custom to product to commodity means

526
00:17:25,679 --> 00:17:27,439
that some of that stuff as they grouped

527
00:17:27,439 --> 00:17:28,640
it here would have to be delivered

528
00:17:28,640 --> 00:17:30,960
in-house because it is not available to

529
00:17:30,960 --> 00:17:32,480
be able to you know just buy you have to

530
00:17:32,480 --> 00:17:35,120
build other stuff in their view was more

531
00:17:35,120 --> 00:17:36,559
commoditized it became more product

532
00:17:36,559 --> 00:17:37,679
oriented so you see things like

533
00:17:37,679 --> 00:17:39,120
vulnerability scanning or attack

534
00:17:39,120 --> 00:17:41,280
monitoring so mapping like this gives

535
00:17:41,280 --> 00:17:42,880
you a view if you take like sunil's

536
00:17:42,880 --> 00:17:44,880
cyber security defense matrix and you

537
00:17:44,880 --> 00:17:46,480
say i have all these capabilities and

538
00:17:46,480 --> 00:17:48,160
tools not all of them i'm able to

539
00:17:48,160 --> 00:17:49,679
harness not all of them able to harness

540
00:17:49,679 --> 00:17:51,600
myself how can i map those and make use

541
00:17:51,600 --> 00:17:52,880
of them in my environment to make better

542
00:17:52,880 --> 00:17:55,360
decisions

543
00:17:55,600 --> 00:17:57,200
mr snowden is famous for something

544
00:17:57,200 --> 00:17:59,440
called the conoven framework and it's a

545
00:17:59,440 --> 00:18:00,799
welsh word which is why it sounds

546
00:18:00,799 --> 00:18:02,240
nothing like what it's spelled like just

547
00:18:02,240 --> 00:18:03,520
like comrie

548
00:18:03,520 --> 00:18:05,120
and basically what he says is it's a

549
00:18:05,120 --> 00:18:07,440
decision fabric that is based on four

550
00:18:07,440 --> 00:18:08,880
different domains

551
00:18:08,880 --> 00:18:11,600
simple complicated complex and chaotic

552
00:18:11,600 --> 00:18:13,840
and in simple what you ultimately have

553
00:18:13,840 --> 00:18:15,919
is a bunch of known knowns and so you

554
00:18:15,919 --> 00:18:17,760
apply best practices you sense you

555
00:18:17,760 --> 00:18:20,640
categorize you respond complicated you

556
00:18:20,640 --> 00:18:22,400
sense analyze and respond because

557
00:18:22,400 --> 00:18:24,000
there's a bunch of known unknowns things

558
00:18:24,000 --> 00:18:25,919
that you know that you don't know

559
00:18:25,919 --> 00:18:28,320
complex is a bunch of unknown unknowns

560
00:18:28,320 --> 00:18:29,760
like you just absolutely don't

561
00:18:29,760 --> 00:18:31,760
understand the space but you're able to

562
00:18:31,760 --> 00:18:34,559
sort of probe sense and then respond

563
00:18:34,559 --> 00:18:36,880
and then chaotic is where you ultimately

564
00:18:36,880 --> 00:18:38,720
are unknowable unknowns really think

565
00:18:38,720 --> 00:18:40,000
about that black swan events i was

566
00:18:40,000 --> 00:18:41,760
talking about and you act sense and

567
00:18:41,760 --> 00:18:44,080
respond the interesting thing about that

568
00:18:44,080 --> 00:18:46,240
is it's a maturity of where you are in

569
00:18:46,240 --> 00:18:48,080
the space that you're analyzing if you

570
00:18:48,080 --> 00:18:49,919
apply this to infosec we tend to have a

571
00:18:49,919 --> 00:18:52,480
binary view here of looking at things as

572
00:18:52,480 --> 00:18:54,799
either being simple or complex where i

573
00:18:54,799 --> 00:18:56,320
would argue that the majority of things

574
00:18:56,320 --> 00:18:58,240
are more complicated i'm sorry simple

575
00:18:58,240 --> 00:19:00,320
and chaotic where i would argue that in

576
00:19:00,320 --> 00:19:02,559
many cases the majority of stuff is more

577
00:19:02,559 --> 00:19:04,720
complicated and complex and the reason

578
00:19:04,720 --> 00:19:06,160
that's important is if you're trying to

579
00:19:06,160 --> 00:19:08,559
make decisions about things especially

580
00:19:08,559 --> 00:19:10,400
in infosec space you have to understand

581
00:19:10,400 --> 00:19:11,679
the space in which you are actually

582
00:19:11,679 --> 00:19:14,640
afforded good quality data so if you if

583
00:19:14,640 --> 00:19:16,000
you apply this to like the history of

584
00:19:16,000 --> 00:19:17,760
our aeronautics

585
00:19:17,760 --> 00:19:19,520
in the beginning it was very chaotic

586
00:19:19,520 --> 00:19:20,960
somebody somebody said we ought to be

587
00:19:20,960 --> 00:19:22,400
able to fly we don't know how and we

588
00:19:22,400 --> 00:19:23,840
don't know what stuff looks like there

589
00:19:23,840 --> 00:19:26,000
were weird designs with flapping wings

590
00:19:26,000 --> 00:19:27,280
and corkscrews and a whole bunch of

591
00:19:27,280 --> 00:19:28,400
other stuff

592
00:19:28,400 --> 00:19:30,559
and people tried and iterated and failed

593
00:19:30,559 --> 00:19:32,799
a lot right they made good decisions by

594
00:19:32,799 --> 00:19:35,039
just trying things out complex after the

595
00:19:35,039 --> 00:19:36,720
wright brothers made a plane that had

596
00:19:36,720 --> 00:19:38,880
wings a propeller and a tail people sort

597
00:19:38,880 --> 00:19:40,240
of knew you could fly it was a good and

598
00:19:40,240 --> 00:19:43,120
reasonably safe plane but then to make

599
00:19:43,120 --> 00:19:44,720
to make planes better they had to

600
00:19:44,720 --> 00:19:46,080
ultimately iterate and test what was

601
00:19:46,080 --> 00:19:48,000
already there years past you got to the

602
00:19:48,000 --> 00:19:50,480
complicated realm where now you had

603
00:19:50,480 --> 00:19:52,880
longer and more complex problems like uh

604
00:19:52,880 --> 00:19:55,120
overseas travel which brought a whole

605
00:19:55,120 --> 00:19:57,520
new set of challenges or problems to the

606
00:19:57,520 --> 00:19:59,120
table i need to make decisions on

607
00:19:59,120 --> 00:20:01,360
isolated cabins fusillage equipment uh

608
00:20:01,360 --> 00:20:03,039
fuel efficient jet engines need to be

609
00:20:03,039 --> 00:20:04,720
cost effective so in that particular

610
00:20:04,720 --> 00:20:06,559
space and complicated you had to analyze

611
00:20:06,559 --> 00:20:08,640
hundreds of materials

612
00:20:08,640 --> 00:20:10,159
designs and test them to make sure that

613
00:20:10,159 --> 00:20:12,480
they actually worked today in the simple

614
00:20:12,480 --> 00:20:14,400
realm we already know how to fly there's

615
00:20:14,400 --> 00:20:16,400
not a lot of magic like for the most

616
00:20:16,400 --> 00:20:17,840
part unless you're optimizing for a

617
00:20:17,840 --> 00:20:20,080
particular trait like speed or longevity

618
00:20:20,080 --> 00:20:20,799
or

619
00:20:20,799 --> 00:20:22,400
potentially outer space travel for

620
00:20:22,400 --> 00:20:24,000
example if somebody needed to buy a

621
00:20:24,000 --> 00:20:26,080
plane to build a plane today you could

622
00:20:26,080 --> 00:20:27,360
pretty much take all the existing

623
00:20:27,360 --> 00:20:28,960
knowledge and apply it now that's

624
00:20:28,960 --> 00:20:30,080
important

625
00:20:30,080 --> 00:20:31,600
i got lazy and ran out of time i was

626
00:20:31,600 --> 00:20:33,280
going to do one for infosec but you

627
00:20:33,280 --> 00:20:34,960
could you could you could map this to

628
00:20:34,960 --> 00:20:36,240
any one of the domains you could talk

629
00:20:36,240 --> 00:20:38,240
about perimeter security data security

630
00:20:38,240 --> 00:20:40,080
you name it and what you would find is

631
00:20:40,080 --> 00:20:42,240
that in many cases the decisions the

632
00:20:42,240 --> 00:20:44,240
data that you have would indicate that

633
00:20:44,240 --> 00:20:46,240
you would make different decisions

634
00:20:46,240 --> 00:20:48,559
um and and what's interesting about that

635
00:20:48,559 --> 00:20:50,480
is there's another field parallel field

636
00:20:50,480 --> 00:20:52,159
of study called safety science and human

637
00:20:52,159 --> 00:20:54,159
factors there's a guy named jen's uh

638
00:20:54,159 --> 00:20:56,320
rasmussen he died about 2018 but he

639
00:20:56,320 --> 00:20:57,919
talked about four different themes that

640
00:20:57,919 --> 00:21:00,000
emerged in his work and this is the part

641
00:21:00,000 --> 00:21:01,760
that in infosec we tend to leave out a

642
00:21:01,760 --> 00:21:02,400
lot

643
00:21:02,400 --> 00:21:04,159
and you can all let out a collective

644
00:21:04,159 --> 00:21:06,159
groan about users and if we only didn't

645
00:21:06,159 --> 00:21:08,720
have users security would be easy right

646
00:21:08,720 --> 00:21:10,000
and at the end of the day most of the

647
00:21:10,000 --> 00:21:11,600
stuff that we impose in terms of control

648
00:21:11,600 --> 00:21:13,440
is to prevent users from doing bad

649
00:21:13,440 --> 00:21:14,720
things we tell them not to click on

650
00:21:14,720 --> 00:21:16,400
stuff we tell them not to do stuff well

651
00:21:16,400 --> 00:21:18,159
what he actually said is that for for

652
00:21:18,159 --> 00:21:21,039
example industrial uh scenarios like

653
00:21:21,039 --> 00:21:22,400
nuclear power

654
00:21:22,400 --> 00:21:23,760
um there are four things that you have

655
00:21:23,760 --> 00:21:25,039
to think about human operator

656
00:21:25,039 --> 00:21:26,480
performance results from behavior and

657
00:21:26,480 --> 00:21:28,080
you can actually identify and model

658
00:21:28,080 --> 00:21:29,360
behavior which is something we don't do

659
00:21:29,360 --> 00:21:30,559
a lot of

660
00:21:30,559 --> 00:21:33,039
human operators flexible and adaptive

661
00:21:33,039 --> 00:21:35,200
they compensate for shortcomings they

662
00:21:35,200 --> 00:21:37,039
cope with complexity by applying mental

663
00:21:37,039 --> 00:21:38,880
models using this

664
00:21:38,880 --> 00:21:41,200
the skills rules and knowledge based uh

665
00:21:41,200 --> 00:21:43,360
system that he that he archetyped and

666
00:21:43,360 --> 00:21:44,720
then risk management requires an

667
00:21:44,720 --> 00:21:46,480
understanding of all of the inputs and

668
00:21:46,480 --> 00:21:48,720
outputs and so the whole point about

669
00:21:48,720 --> 00:21:50,480
this is it brings in another definition

670
00:21:50,480 --> 00:21:51,679
of resilience called cognitive

671
00:21:51,679 --> 00:21:53,679
resilience which is it implies a

672
00:21:53,679 --> 00:21:55,679
practitioner's scope or a user or a

673
00:21:55,679 --> 00:21:57,520
person's ability to cope with unexpected

674
00:21:57,520 --> 00:21:58,559
events

675
00:21:58,559 --> 00:22:00,159
so whether you're in security or a

676
00:22:00,159 --> 00:22:02,400
particular user we count a lot on people

677
00:22:02,400 --> 00:22:03,840
making good decisions which is a

678
00:22:03,840 --> 00:22:05,120
challenge

679
00:22:05,120 --> 00:22:07,760
so uh by the way uh who's seen soylent

680
00:22:07,760 --> 00:22:09,120
green in the movie with char charlton

681
00:22:09,120 --> 00:22:11,360
heston all the old people yeah so it was

682
00:22:11,360 --> 00:22:13,840
set it was set in 2022 by the way which

683
00:22:13,840 --> 00:22:17,840
is uh uh the irony is not uh lost on me

684
00:22:17,840 --> 00:22:19,760
um so what do we have to do to apply all

685
00:22:19,760 --> 00:22:22,240
this stuff in infosec so there uh

686
00:22:22,240 --> 00:22:24,559
uh there's a really interesting uh talk

687
00:22:24,559 --> 00:22:26,720
that mario plot gave recently on on how

688
00:22:26,720 --> 00:22:29,440
do we apply um safety

689
00:22:29,440 --> 00:22:31,440
and our thought about safety and and

690
00:22:31,440 --> 00:22:34,159
you'll see some um like i am the cavalry

691
00:22:34,159 --> 00:22:35,440
and a bunch of stuff that josh corman

692
00:22:35,440 --> 00:22:37,120
and team have done around you know

693
00:22:37,120 --> 00:22:38,880
medical devices in the same way thinking

694
00:22:38,880 --> 00:22:41,120
about how we make sure that devices are

695
00:22:41,120 --> 00:22:42,880
safe and high quality but there are

696
00:22:42,880 --> 00:22:44,720
really different ways you can think

697
00:22:44,720 --> 00:22:46,240
about different applying different

698
00:22:46,240 --> 00:22:47,520
strategies based on the domain that

699
00:22:47,520 --> 00:22:48,480
you're in

700
00:22:48,480 --> 00:22:50,480
and i use this big giant modeled mess to

701
00:22:50,480 --> 00:22:52,320
say look if you have a threat actor and

702
00:22:52,320 --> 00:22:53,919
we have threat models and we have

703
00:22:53,919 --> 00:22:56,480
ultimately situational context by using

704
00:22:56,480 --> 00:22:58,559
snowden's mapping you take into

705
00:22:58,559 --> 00:23:01,120
consideration human factors you map this

706
00:23:01,120 --> 00:23:03,039
stuff out you can apply your models

707
00:23:03,039 --> 00:23:05,679
ultimately what you get to is a way of

708
00:23:05,679 --> 00:23:07,200
being able to apply stuff that we have

709
00:23:07,200 --> 00:23:08,799
today but make decisions in a more

710
00:23:08,799 --> 00:23:11,520
formulaic way now this looks complex and

711
00:23:11,520 --> 00:23:14,320
it will be decades in the making and so

712
00:23:14,320 --> 00:23:15,679
what i want you to take away from that

713
00:23:15,679 --> 00:23:17,360
is a much more simpler view of what this

714
00:23:17,360 --> 00:23:20,080
means to infosec and john lambert did a

715
00:23:20,080 --> 00:23:22,559
great job of actually summarizing this

716
00:23:22,559 --> 00:23:24,320
somewhat orthogonally in one of his

717
00:23:24,320 --> 00:23:26,400
talks where he talked about info

718
00:23:26,400 --> 00:23:28,720
advancing infosec towards an open

719
00:23:28,720 --> 00:23:31,039
shareable contributor friendly model of

720
00:23:31,039 --> 00:23:33,360
speeding infosec learning because

721
00:23:33,360 --> 00:23:34,720
everything i just talked about is about

722
00:23:34,720 --> 00:23:36,400
learning about your environment so if

723
00:23:36,400 --> 00:23:37,760
you look at the left he said traditional

724
00:23:37,760 --> 00:23:39,760
defenders defend a list of assets manage

725
00:23:39,760 --> 00:23:41,919
incidents minimize risk by keeping uh

726
00:23:41,919 --> 00:23:43,520
incident secret view pen test as a

727
00:23:43,520 --> 00:23:44,880
report card and think about stopping

728
00:23:44,880 --> 00:23:45,919
attacks

729
00:23:45,919 --> 00:23:48,640
he implied that modern defenders defend

730
00:23:48,640 --> 00:23:51,440
a graph of assets right you also heard

731
00:23:51,440 --> 00:23:53,120
defenders think in lists attackers

732
00:23:53,120 --> 00:23:55,279
thinking graphs same concept we manage

733
00:23:55,279 --> 00:23:57,440
adversaries we maximize learning by

734
00:23:57,440 --> 00:23:59,039
sharing incidents with trusted outside

735
00:23:59,039 --> 00:24:01,600
peers we view pen tests as an input

736
00:24:01,600 --> 00:24:04,159
comma to get better and we think about

737
00:24:04,159 --> 00:24:07,120
the increasing increasing the attacker's

738
00:24:07,120 --> 00:24:09,919
cost and time in order to attack us so

739
00:24:09,919 --> 00:24:11,200
this is a little bit more simpler to

740
00:24:11,200 --> 00:24:12,559
grasp because if you look at how we've

741
00:24:12,559 --> 00:24:14,880
applied those rules and all that muck

742
00:24:14,880 --> 00:24:16,000
that i talked about in the beginning

743
00:24:16,000 --> 00:24:17,600
without even knowing what we're doing

744
00:24:17,600 --> 00:24:19,120
today you see a lot of structures and

745
00:24:19,120 --> 00:24:20,799
organizations of security teams looking

746
00:24:20,799 --> 00:24:22,159
very very differently than they were

747
00:24:22,159 --> 00:24:24,000
before we're seeing diffused and

748
00:24:24,000 --> 00:24:26,080
embedded security teams especially in

749
00:24:26,080 --> 00:24:28,080
places that build product you literally

750
00:24:28,080 --> 00:24:30,559
have embedded security team members that

751
00:24:30,559 --> 00:24:32,400
aren't part of some big collective borg

752
00:24:32,400 --> 00:24:34,320
in the middle that is called infosec

753
00:24:34,320 --> 00:24:35,919
we've seen the proliferation of

754
00:24:35,919 --> 00:24:37,200
continuous integration continuous

755
00:24:37,200 --> 00:24:38,960
deployment and now continuous service as

756
00:24:38,960 --> 00:24:40,559
we integrate security tooling into our

757
00:24:40,559 --> 00:24:41,919
dev tool chains

758
00:24:41,919 --> 00:24:43,760
cloud and site reliability engineering

759
00:24:43,760 --> 00:24:45,679
have really brought a lot of the

760
00:24:45,679 --> 00:24:47,279
practices i just talked about about

761
00:24:47,279 --> 00:24:49,360
secure by design and secure by operation

762
00:24:49,360 --> 00:24:51,919
into fold we've seen the emergence of

763
00:24:51,919 --> 00:24:53,039
whole new

764
00:24:53,039 --> 00:24:55,360
sort of subspecies of capabilities that

765
00:24:55,360 --> 00:24:56,640
we've put new names on like

766
00:24:56,640 --> 00:24:58,400
observability and detection engineering

767
00:24:58,400 --> 00:25:00,480
crop up which is about where we lay in

768
00:25:00,480 --> 00:25:02,799
that sense sensors sense making decision

769
00:25:02,799 --> 00:25:05,200
systems

770
00:25:05,440 --> 00:25:07,840
timeline we've seen robust and threat

771
00:25:07,840 --> 00:25:10,240
risk modeling start to take shape where

772
00:25:10,240 --> 00:25:11,919
people are doing this stuff more

773
00:25:11,919 --> 00:25:14,080
religiously chaos engineering has

774
00:25:14,080 --> 00:25:15,679
entered the chat and we start thinking

775
00:25:15,679 --> 00:25:18,400
about uh you know monkeys and the simian

776
00:25:18,400 --> 00:25:20,240
army and turning things off and on and

777
00:25:20,240 --> 00:25:22,799
looking at uh in a dot and adopting uh

778
00:25:22,799 --> 00:25:24,400
cloud design paradigms like you saw with

779
00:25:24,400 --> 00:25:26,159
aws when they came out a decade and a

780
00:25:26,159 --> 00:25:29,039
half ago with design for fail which is

781
00:25:29,039 --> 00:25:30,720
by the way antithetical to what we do in

782
00:25:30,720 --> 00:25:31,919
security

783
00:25:31,919 --> 00:25:33,520
and we've seen the emergence of a book

784
00:25:33,520 --> 00:25:34,960
and if you haven't seen it you should

785
00:25:34,960 --> 00:25:36,240
all run out and get it in my opinion

786
00:25:36,240 --> 00:25:38,000
it's called team topologies it's about

787
00:25:38,000 --> 00:25:39,440
how you organize

788
00:25:39,440 --> 00:25:42,640
technology teams to really optimize what

789
00:25:42,640 --> 00:25:44,320
you deliver and how so we're going to

790
00:25:44,320 --> 00:25:45,919
cover off that a little bit by the way

791
00:25:45,919 --> 00:25:47,360
my my uh

792
00:25:47,360 --> 00:25:48,880
my little timer is not running what's

793
00:25:48,880 --> 00:25:51,279
the time

794
00:25:52,000 --> 00:25:54,240
9 55. oh fantastic all right i'll slow

795
00:25:54,240 --> 00:25:55,440
down a little bit all right so wendy

796
00:25:55,440 --> 00:25:56,480
neither

797
00:25:56,480 --> 00:25:57,760
uh

798
00:25:57,760 --> 00:25:59,520
the goddess of of all goodness and

799
00:25:59,520 --> 00:26:01,760
security um came up with a term years

800
00:26:01,760 --> 00:26:03,840
ago called about 11 years ago called

801
00:26:03,840 --> 00:26:05,679
this cyber security poverty line the

802
00:26:05,679 --> 00:26:07,120
visualization may not be accurate but

803
00:26:07,120 --> 00:26:08,480
it's good for effect right where she

804
00:26:08,480 --> 00:26:11,760
said basically big companies can afford

805
00:26:11,760 --> 00:26:13,840
more people in technology and there is a

806
00:26:13,840 --> 00:26:16,080
line the cyber security poverty line

807
00:26:16,080 --> 00:26:18,000
where many of us are afflicted by the

808
00:26:18,000 --> 00:26:20,159
fact that we can't and so how are we

809
00:26:20,159 --> 00:26:21,760
supposed to defend things when we don't

810
00:26:21,760 --> 00:26:23,520
have the budget and people to do it

811
00:26:23,520 --> 00:26:24,880
a lot of this is

812
00:26:24,880 --> 00:26:25,679
not because we don't have the

813
00:26:25,679 --> 00:26:27,840
information or skills but ultimately

814
00:26:27,840 --> 00:26:30,080
that that that poverty line is real so

815
00:26:30,080 --> 00:26:32,080
we have to be able to think about how we

816
00:26:32,080 --> 00:26:33,360
structure

817
00:26:33,360 --> 00:26:35,440
and organize what we do without having

818
00:26:35,440 --> 00:26:37,120
to depend on spending more money or

819
00:26:37,120 --> 00:26:39,120
building more people and a lot of that

820
00:26:39,120 --> 00:26:40,559
hearken back to the conversations that

821
00:26:40,559 --> 00:26:42,080
we were having or the statements i made

822
00:26:42,080 --> 00:26:44,400
about automation in the past

823
00:26:44,400 --> 00:26:47,039
but dino di zovi in and i don't expect

824
00:26:47,039 --> 00:26:48,400
you to read this i'll hit the high

825
00:26:48,400 --> 00:26:50,400
points and his keynote at black out 2018

826
00:26:50,400 --> 00:26:51,600
said look

827
00:26:51,600 --> 00:26:53,360
software teams need to own their own

828
00:26:53,360 --> 00:26:56,000
security now and security teams need to

829
00:26:56,000 --> 00:26:59,279
become full stack software teams

830
00:26:59,279 --> 00:27:00,640
let's sink in for a second right

831
00:27:00,640 --> 00:27:02,559
basically what that means is from a

832
00:27:02,559 --> 00:27:04,080
security perspective we need to be able

833
00:27:04,080 --> 00:27:05,520
to deploy security at code and we need

834
00:27:05,520 --> 00:27:08,159
to be able to inject ourselves into the

835
00:27:08,159 --> 00:27:09,760
frameworks and methodologies that

836
00:27:09,760 --> 00:27:11,279
developers use

837
00:27:11,279 --> 00:27:12,880
it also he also went on to say that

838
00:27:12,880 --> 00:27:14,559
security teams will become internal

839
00:27:14,559 --> 00:27:16,159
security software teams that deliver

840
00:27:16,159 --> 00:27:17,679
value to internal teams through

841
00:27:17,679 --> 00:27:20,640
self-service platform and tools

842
00:27:20,640 --> 00:27:23,279
now for those of you that look at that

843
00:27:23,279 --> 00:27:25,360
and go that sounds great

844
00:27:25,360 --> 00:27:26,240
um

845
00:27:26,240 --> 00:27:28,159
what many of you have interpreted that

846
00:27:28,159 --> 00:27:29,919
to mean is we should just call ourselves

847
00:27:29,919 --> 00:27:32,080
now devsecops teams because instead of

848
00:27:32,080 --> 00:27:34,159
being on-prem we've moved to the cloud

849
00:27:34,159 --> 00:27:37,039
mario platt uh did another great talk

850
00:27:37,039 --> 00:27:38,720
where he said

851
00:27:38,720 --> 00:27:41,279
uh and he he ultimately uh captured a

852
00:27:41,279 --> 00:27:43,919
bunch of input uh from the intertubes

853
00:27:43,919 --> 00:27:46,640
that said if if your notion or strategy

854
00:27:46,640 --> 00:27:48,640
mentions the word devsecops and you're

855
00:27:48,640 --> 00:27:50,399
not understanding how the governance

856
00:27:50,399 --> 00:27:52,880
teams need to deal with what that means

857
00:27:52,880 --> 00:27:54,799
and thus regulators

858
00:27:54,799 --> 00:27:56,960
you're you're not really doing devsecops

859
00:27:56,960 --> 00:27:58,720
if you're not increasing the agency and

860
00:27:58,720 --> 00:28:00,240
ownership of security across those that

861
00:28:00,240 --> 00:28:01,279
own the product you're not doing

862
00:28:01,279 --> 00:28:03,120
devsecops if you're not enabling the

863
00:28:03,120 --> 00:28:04,960
best possible out of developers and

864
00:28:04,960 --> 00:28:06,960
engineers you're not and all you've

865
00:28:06,960 --> 00:28:08,640
redone is essentially all you've done is

866
00:28:08,640 --> 00:28:10,559
essentially rebranded yourself

867
00:28:10,559 --> 00:28:12,320
to look like you are relevant and in

868
00:28:12,320 --> 00:28:15,039
fact the um the two little there's a

869
00:28:15,039 --> 00:28:17,120
quote down on the bottom from uh kelly

870
00:28:17,120 --> 00:28:19,279
shortridge who is uh who has lots of

871
00:28:19,279 --> 00:28:21,760
spicy and snarky takes on a lot of stuff

872
00:28:21,760 --> 00:28:23,520
and what she basically said was

873
00:28:23,520 --> 00:28:25,679
devsecops is something that

874
00:28:25,679 --> 00:28:27,200
most security teams think they can run

875
00:28:27,200 --> 00:28:28,880
out and buy to make themselves seem more

876
00:28:28,880 --> 00:28:31,520
relevant now that that seems sort of a

877
00:28:31,520 --> 00:28:34,399
little bit gruff and in some cases wrong

878
00:28:34,399 --> 00:28:36,799
but in many cases

879
00:28:36,799 --> 00:28:38,480
just because you read it on the internet

880
00:28:38,480 --> 00:28:41,840
doesn't mean that it's true and

881
00:28:41,840 --> 00:28:43,039
how many of you have been in

882
00:28:43,039 --> 00:28:45,520
conversations where the words site

883
00:28:45,520 --> 00:28:47,440
reliability engineering have come up in

884
00:28:47,440 --> 00:28:49,760
a conversation an sre

885
00:28:49,760 --> 00:28:51,919
is now something that some manager says

886
00:28:51,919 --> 00:28:53,600
that you ought to go do without really

887
00:28:53,600 --> 00:28:54,960
knowing what it means and in fact

888
00:28:54,960 --> 00:28:56,720
harking back to discussions that you

889
00:28:56,720 --> 00:28:57,919
might have

890
00:28:57,919 --> 00:29:00,000
around how it was implemented at google

891
00:29:00,000 --> 00:29:01,600
sre is not

892
00:29:01,600 --> 00:29:03,440
used in every software team it's very

893
00:29:03,440 --> 00:29:05,760
expensive it's very onerous

894
00:29:05,760 --> 00:29:07,840
and ultimately is it the right design

895
00:29:07,840 --> 00:29:10,320
principle and a destination aim for sure

896
00:29:10,320 --> 00:29:12,320
but in the same way that devsecops or

897
00:29:12,320 --> 00:29:14,240
devops in that particular case means a

898
00:29:14,240 --> 00:29:15,919
lot of things to a lot of people

899
00:29:15,919 --> 00:29:17,760
simply calling yourself devsecops does

900
00:29:17,760 --> 00:29:20,080
not actually get you towards being more

901
00:29:20,080 --> 00:29:21,440
anti-fragile

902
00:29:21,440 --> 00:29:24,080
uh in in fact the problem becomes even

903
00:29:24,080 --> 00:29:26,159
more interesting there's a law called

904
00:29:26,159 --> 00:29:28,559
conway's law that basically says for any

905
00:29:28,559 --> 00:29:30,480
organization that designs systems you

906
00:29:30,480 --> 00:29:32,880
ship your org chart

907
00:29:32,880 --> 00:29:34,880
and what's funny about the pictures you

908
00:29:34,880 --> 00:29:36,960
see there like amazon google microsoft

909
00:29:36,960 --> 00:29:38,640
or the funniest one is oracle where

910
00:29:38,640 --> 00:29:40,559
engineering's this big and legal is this

911
00:29:40,559 --> 00:29:43,039
big right the reality is they ship their

912
00:29:43,039 --> 00:29:44,960
org chart and the usability of the

913
00:29:44,960 --> 00:29:46,480
products

914
00:29:46,480 --> 00:29:48,320
really do reflect how the software

915
00:29:48,320 --> 00:29:49,600
developers and security teams are

916
00:29:49,600 --> 00:29:51,520
organized and that little graph for

917
00:29:51,520 --> 00:29:53,039
microsoft on the right that indicates

918
00:29:53,039 --> 00:29:54,480
how you ought to structure your infosec

919
00:29:54,480 --> 00:29:56,880
team is incredibly complex

920
00:29:56,880 --> 00:29:59,200
right so we are essentially applying

921
00:29:59,200 --> 00:30:01,760
conway's law to security when the org

922
00:30:01,760 --> 00:30:03,440
and engineering teams and business teams

923
00:30:03,440 --> 00:30:04,320
are

924
00:30:04,320 --> 00:30:05,760
look completely different from how we're

925
00:30:05,760 --> 00:30:07,200
organized so there's a thing called the

926
00:30:07,200 --> 00:30:08,720
reverse conway law which allows you to

927
00:30:08,720 --> 00:30:10,399
basically say you really are ought to

928
00:30:10,399 --> 00:30:12,240
organize yourself differently there's

929
00:30:12,240 --> 00:30:13,840
this notion of team topologies i'm not

930
00:30:13,840 --> 00:30:15,360
going to get into it in detail but if

931
00:30:15,360 --> 00:30:16,640
you haven't read the book again i

932
00:30:16,640 --> 00:30:18,720
suggest you do it talks about four

933
00:30:18,720 --> 00:30:20,559
different types of topologies

934
00:30:20,559 --> 00:30:22,240
streamlined enabling complicated and

935
00:30:22,240 --> 00:30:24,080
platform teams they do different things

936
00:30:24,080 --> 00:30:25,440
they provide different services they

937
00:30:25,440 --> 00:30:27,200
consume different services and

938
00:30:27,200 --> 00:30:29,440
ultimately they have different ways of

939
00:30:29,440 --> 00:30:31,440
working with each other so you've got

940
00:30:31,440 --> 00:30:33,520
collaborative models acts as a service

941
00:30:33,520 --> 00:30:36,080
and facilitation and just really quick

942
00:30:36,080 --> 00:30:38,559
take that for for for what it is

943
00:30:38,559 --> 00:30:42,320
i i pulled down uh a an example uh that

944
00:30:42,320 --> 00:30:44,559
docker has implemented in what they call

945
00:30:44,559 --> 00:30:46,240
building stronger happier engineering

946
00:30:46,240 --> 00:30:47,679
teams with team topologies and what they

947
00:30:47,679 --> 00:30:49,919
had on the left was were very sort of

948
00:30:49,919 --> 00:30:52,000
monolithic teams where if you were

949
00:30:52,000 --> 00:30:53,760
building something like the docker hub

950
00:30:53,760 --> 00:30:55,760
what you might end up doing is you would

951
00:30:55,760 --> 00:30:57,279
have sort of a

952
00:30:57,279 --> 00:30:58,960
a bunch of

953
00:30:58,960 --> 00:31:01,440
a subordinate team that does ui

954
00:31:01,440 --> 00:31:03,440
middleware back end and that team is

955
00:31:03,440 --> 00:31:05,120
responsible for that particular product

956
00:31:05,120 --> 00:31:07,519
and they would have to ultimately

957
00:31:07,519 --> 00:31:10,480
jockey for uh time uh money and

958
00:31:10,480 --> 00:31:12,000
resources to be able to get their

959
00:31:12,000 --> 00:31:14,320
product done and there's a lot of shared

960
00:31:14,320 --> 00:31:15,840
code there's a lot of ambiguity as to

961
00:31:15,840 --> 00:31:17,600
who owns what so when the model that

962
00:31:17,600 --> 00:31:19,760
they that they ultimately uh applied the

963
00:31:19,760 --> 00:31:21,039
new structure

964
00:31:21,039 --> 00:31:22,799
was one where they organized their

965
00:31:22,799 --> 00:31:24,799
development teams writ large everybody

966
00:31:24,799 --> 00:31:28,000
responsible for for example the in the

967
00:31:28,000 --> 00:31:30,320
growth uh category which would be

968
00:31:30,320 --> 00:31:32,399
account growth of the akap growth team

969
00:31:32,399 --> 00:31:35,039
or a particular product like docker hub

970
00:31:35,039 --> 00:31:36,799
had everybody they needed to be able to

971
00:31:36,799 --> 00:31:39,039
bring that product to market including

972
00:31:39,039 --> 00:31:40,960
ultimately security teams they build

973
00:31:40,960 --> 00:31:43,039
these team apis that describe exactly

974
00:31:43,039 --> 00:31:44,480
what each team does and how they

975
00:31:44,480 --> 00:31:47,039
interact and why that's important is if

976
00:31:47,039 --> 00:31:48,559
you take it to the next level and and

977
00:31:48,559 --> 00:31:50,240
this next slide is something i added on

978
00:31:50,240 --> 00:31:50,960
to

979
00:31:50,960 --> 00:31:52,320
because they alluded to it but they

980
00:31:52,320 --> 00:31:54,080
didn't include it in their slides if you

981
00:31:54,080 --> 00:31:56,240
think about that from a security model

982
00:31:56,240 --> 00:31:57,679
there there's this notion of a

983
00:31:57,679 --> 00:32:00,720
complicated or acts as a service set of

984
00:32:00,720 --> 00:32:02,080
core capabilities that aren't

985
00:32:02,080 --> 00:32:03,519
necessarily engineering-centric so that

986
00:32:03,519 --> 00:32:05,919
would be like grc incident response

987
00:32:05,919 --> 00:32:07,840
incident um

988
00:32:07,840 --> 00:32:10,000
incident response management um maybe

989
00:32:10,000 --> 00:32:11,440
even vulnerability management that

990
00:32:11,440 --> 00:32:14,399
provides a set of services with embedded

991
00:32:14,399 --> 00:32:15,919
security engineers in each of the

992
00:32:15,919 --> 00:32:18,000
product teams what that gives you is a

993
00:32:18,000 --> 00:32:20,080
much more focused close to the business

994
00:32:20,080 --> 00:32:22,000
way of thinking about from a decision

995
00:32:22,000 --> 00:32:24,720
perspective what decisions you ought to

996
00:32:24,720 --> 00:32:26,000
be able to make to bring that product to

997
00:32:26,000 --> 00:32:28,000
market securely there are a number of

998
00:32:28,000 --> 00:32:30,640
models that exist uh for example in

999
00:32:30,640 --> 00:32:32,799
large finance and other industries where

1000
00:32:32,799 --> 00:32:34,559
the security teams are massively

1001
00:32:34,559 --> 00:32:35,840
monolithic

1002
00:32:35,840 --> 00:32:37,919
or they are very distributed

1003
00:32:37,919 --> 00:32:39,760
uh where ultimately the individual teams

1004
00:32:39,760 --> 00:32:41,039
are empowered to do what they want but

1005
00:32:41,039 --> 00:32:43,600
what team topologies has done is started

1006
00:32:43,600 --> 00:32:45,760
to be able to

1007
00:32:45,760 --> 00:32:48,080
align how security teams function

1008
00:32:48,080 --> 00:32:49,440
with the business and the engineers that

1009
00:32:49,440 --> 00:32:51,679
are doing it and and dino and i were

1010
00:32:51,679 --> 00:32:52,880
having a conversation along with other

1011
00:32:52,880 --> 00:32:54,559
people the other day and he said well

1012
00:32:54,559 --> 00:32:56,399
it's almost like if technology isn't all

1013
00:32:56,399 --> 00:32:58,159
built by one monolithic org then we

1014
00:32:58,159 --> 00:32:59,919
shouldn't have technology secured by one

1015
00:32:59,919 --> 00:33:02,159
monolithic org either

1016
00:33:02,159 --> 00:33:04,320
and that's a great summary of what that

1017
00:33:04,320 --> 00:33:08,000
means so organizationally um that also

1018
00:33:08,000 --> 00:33:10,320
sort of and i know josh's corman's

1019
00:33:10,320 --> 00:33:12,240
probably here and i put this in here

1020
00:33:12,240 --> 00:33:13,919
because he yelled at me though he's over

1021
00:33:13,919 --> 00:33:15,360
there the last time that i didn't

1022
00:33:15,360 --> 00:33:17,039
mention this in response to resilience

1023
00:33:17,039 --> 00:33:19,519
but um i don't know eight years ago

1024
00:33:19,519 --> 00:33:22,320
maybe more uh okay maybe even more than

1025
00:33:22,320 --> 00:33:23,919
that the rugged manifesto was really

1026
00:33:23,919 --> 00:33:26,240
aimed at developers but it talked about

1027
00:33:26,240 --> 00:33:28,559
um how you should take this mindset of

1028
00:33:28,559 --> 00:33:30,559
being rugged in software development

1029
00:33:30,559 --> 00:33:32,960
which um ultimately the important parts

1030
00:33:32,960 --> 00:33:34,559
down at the bottom it's not a technology

1031
00:33:34,559 --> 00:33:36,320
process model secure development

1032
00:33:36,320 --> 00:33:37,919
lifecycle organizational structure it's

1033
00:33:37,919 --> 00:33:40,000
not even a noun it's it's not the same

1034
00:33:40,000 --> 00:33:42,799
as secure this is a way of essentially

1035
00:33:42,799 --> 00:33:45,039
applying these rules to be able to think

1036
00:33:45,039 --> 00:33:47,039
more holistically and it really does rub

1037
00:33:47,039 --> 00:33:48,320
off on what i talked about with jan

1038
00:33:48,320 --> 00:33:50,559
rasmussen's safety talk about things

1039
00:33:50,559 --> 00:33:52,960
like this is the way we function this is

1040
00:33:52,960 --> 00:33:54,320
the set of services and this is the

1041
00:33:54,320 --> 00:33:57,039
ethos by which we we implement more

1042
00:33:57,039 --> 00:33:58,480
secure code

1043
00:33:58,480 --> 00:34:00,399
security is a byproduct of being rugged

1044
00:34:00,399 --> 00:34:02,320
right which rugged is another word for

1045
00:34:02,320 --> 00:34:06,000
resilient secure survivable etc or in

1046
00:34:06,000 --> 00:34:08,560
this case anti-fragile so you may be in

1047
00:34:08,560 --> 00:34:10,800
the audience and you're like well i

1048
00:34:10,800 --> 00:34:12,719
don't work for a company that develops

1049
00:34:12,719 --> 00:34:13,918
software

1050
00:34:13,918 --> 00:34:16,079
it doesn't matter the same things apply

1051
00:34:16,079 --> 00:34:18,399
you consume it and so the way in which

1052
00:34:18,399 --> 00:34:21,359
you think of applying these

1053
00:34:21,359 --> 00:34:22,960
these concepts

1054
00:34:22,960 --> 00:34:24,159
apply whether or not you develop

1055
00:34:24,159 --> 00:34:26,560
software or not now you also may not be

1056
00:34:26,560 --> 00:34:28,239
in a position where you can change the

1057
00:34:28,239 --> 00:34:30,639
org structure but you can think

1058
00:34:30,639 --> 00:34:32,560
atomically about how you ultimately

1059
00:34:32,560 --> 00:34:34,800
deliver services and part of the problem

1060
00:34:34,800 --> 00:34:36,800
that i that that i wanted to bring up is

1061
00:34:36,800 --> 00:34:38,960
that you know

1062
00:34:38,960 --> 00:34:41,359
security has a leadership dinosaur

1063
00:34:41,359 --> 00:34:42,480
problem and we're waiting for the

1064
00:34:42,480 --> 00:34:45,280
meteorite to hit right so most of most

1065
00:34:45,280 --> 00:34:46,719
of the people that manage and lead

1066
00:34:46,719 --> 00:34:48,639
security teams today in larger

1067
00:34:48,639 --> 00:34:51,359
established companies are old like me

1068
00:34:51,359 --> 00:34:53,359
now we came up from a time that was much

1069
00:34:53,359 --> 00:34:55,679
more complicated and or chaotic in as

1070
00:34:55,679 --> 00:34:57,119
much as security didn't necessarily

1071
00:34:57,119 --> 00:34:58,720
exist with all the frameworks governance

1072
00:34:58,720 --> 00:35:00,079
and tools many of us were network

1073
00:35:00,079 --> 00:35:02,240
administrators or system administrators

1074
00:35:02,240 --> 00:35:03,599
when the internet came online we started

1075
00:35:03,599 --> 00:35:05,760
doing security stuff and here we are but

1076
00:35:05,760 --> 00:35:07,440
we've ultimately been doing the same

1077
00:35:07,440 --> 00:35:09,440
stuff over and over again

1078
00:35:09,440 --> 00:35:11,040
that's starting to change as you see new

1079
00:35:11,040 --> 00:35:13,040
leadership and new blood and a lot of

1080
00:35:13,040 --> 00:35:14,720
the younger folks in the audience as you

1081
00:35:14,720 --> 00:35:17,040
come up you're the availability of

1082
00:35:17,040 --> 00:35:19,200
thought processes and tooling and

1083
00:35:19,200 --> 00:35:20,800
knowledge sharing these events didn't

1084
00:35:20,800 --> 00:35:22,800
really exist the same way they do today

1085
00:35:22,800 --> 00:35:24,560
when we were coming up so leadership is

1086
00:35:24,560 --> 00:35:26,640
an issue organization culture is another

1087
00:35:26,640 --> 00:35:29,200
one the the notion of security being a

1088
00:35:29,200 --> 00:35:31,280
control function and not an enabler at

1089
00:35:31,280 --> 00:35:32,640
the same point in time i sort of ralph

1090
00:35:32,640 --> 00:35:34,160
every time i hear somebody say security

1091
00:35:34,160 --> 00:35:36,000
should be a business enabler

1092
00:35:36,000 --> 00:35:37,920
it should be but in the context of how

1093
00:35:37,920 --> 00:35:39,760
you organize is important and the

1094
00:35:39,760 --> 00:35:42,000
culture we have is very punitive there's

1095
00:35:42,000 --> 00:35:43,680
a lot of there's a lot of noise and not

1096
00:35:43,680 --> 00:35:46,320
a lot of signal we get drowned with a

1097
00:35:46,320 --> 00:35:48,960
you know an awful lot from that sensing

1098
00:35:48,960 --> 00:35:50,640
sense making and lack of decision

1099
00:35:50,640 --> 00:35:53,680
systems um issue of cognitive load the

1100
00:35:53,680 --> 00:35:56,000
outcomes are really about let's just not

1101
00:35:56,000 --> 00:35:58,480
be any worse than we were

1102
00:35:58,480 --> 00:36:01,599
yesterday not how do we get better

1103
00:36:01,599 --> 00:36:03,520
uh we have a very punitive incentive

1104
00:36:03,520 --> 00:36:05,599
model right instead of stick versus

1105
00:36:05,599 --> 00:36:07,440
carrot or broccoli versus ice cream a

1106
00:36:07,440 --> 00:36:08,640
lot of what we talk about with

1107
00:36:08,640 --> 00:36:10,480
developers what you can't do not what

1108
00:36:10,480 --> 00:36:13,839
you can do and language is a problem

1109
00:36:13,839 --> 00:36:16,240
so from the perspective of how

1110
00:36:16,240 --> 00:36:19,119
militarized our industry is just look at

1111
00:36:19,119 --> 00:36:22,560
words like dmz and pen testing i mean if

1112
00:36:22,560 --> 00:36:23,920
you go into an environment with people

1113
00:36:23,920 --> 00:36:25,920
that aren't security people i mean many

1114
00:36:25,920 --> 00:36:27,359
of us have gotten on the plane on our

1115
00:36:27,359 --> 00:36:28,800
way here with

1116
00:36:28,800 --> 00:36:32,000
our disobey and defcom backpacks and

1117
00:36:32,000 --> 00:36:34,240
strange piercing and wi-fi devices and

1118
00:36:34,240 --> 00:36:35,839
we sort of look weird to people we're

1119
00:36:35,839 --> 00:36:37,680
not particularly accessible

1120
00:36:37,680 --> 00:36:40,160
i mean in in many cases we have over

1121
00:36:40,160 --> 00:36:42,079
militarized what it is we do which makes

1122
00:36:42,079 --> 00:36:44,400
it very difficult to communicate um in

1123
00:36:44,400 --> 00:36:46,800
in easy terms i mean somebody in my org

1124
00:36:46,800 --> 00:36:48,640
one day said jesus we're saving

1125
00:36:48,640 --> 00:36:51,119
passwords not lives uh so

1126
00:36:51,119 --> 00:36:52,240
uh

1127
00:36:52,240 --> 00:36:55,280
not unfair um so what does this mean

1128
00:36:55,280 --> 00:36:57,520
listen at the at the end of the day

1129
00:36:57,520 --> 00:36:59,920
uh we have uh we have an awful lot of

1130
00:36:59,920 --> 00:37:02,240
stuff uh at our disposal we have a lot

1131
00:37:02,240 --> 00:37:03,280
of data

1132
00:37:03,280 --> 00:37:04,800
we have a lot of

1133
00:37:04,800 --> 00:37:07,280
uh tools we have a lot of technology

1134
00:37:07,280 --> 00:37:08,800
but um

1135
00:37:08,800 --> 00:37:11,760
we we have ultimately relied more on on

1136
00:37:11,760 --> 00:37:13,680
art and compliance as the way in which

1137
00:37:13,680 --> 00:37:15,839
we operate versus science

1138
00:37:15,839 --> 00:37:17,920
a lot of what we do is siloed

1139
00:37:17,920 --> 00:37:19,760
places like this are fantastic because

1140
00:37:19,760 --> 00:37:22,160
we get to share what we do we get to

1141
00:37:22,160 --> 00:37:25,119
share our successes and our failures

1142
00:37:25,119 --> 00:37:26,800
we can think differently about how how

1143
00:37:26,800 --> 00:37:28,400
we're organizing and larger companies

1144
00:37:28,400 --> 00:37:30,480
will have a trickle down effect on all

1145
00:37:30,480 --> 00:37:33,440
of us as they display success in

1146
00:37:33,440 --> 00:37:34,960
organizing differently and operating

1147
00:37:34,960 --> 00:37:36,880
security differently

1148
00:37:36,880 --> 00:37:39,440
we cannot continue to simply be the

1149
00:37:39,440 --> 00:37:41,680
department of no i know we've heard that

1150
00:37:41,680 --> 00:37:43,839
a lot but a lot of the capabilities that

1151
00:37:43,839 --> 00:37:46,560
we have really um means that at the end

1152
00:37:46,560 --> 00:37:48,560
of the day being resilient meaning being

1153
00:37:48,560 --> 00:37:49,359
no

1154
00:37:49,359 --> 00:37:51,359
no worse than we we were being able to

1155
00:37:51,359 --> 00:37:53,119
bounce back but not actually learn from

1156
00:37:53,119 --> 00:37:56,160
and get better is not sufficient and we

1157
00:37:56,160 --> 00:37:59,280
we really need to be more anti-fragile

1158
00:37:59,280 --> 00:38:01,920
and again that means getting better

1159
00:38:01,920 --> 00:38:04,720
from stressors and adverse uh adverse

1160
00:38:04,720 --> 00:38:07,760
events so that was my my hope that i had

1161
00:38:07,760 --> 00:38:10,079
uh in combining five talks into one and

1162
00:38:10,079 --> 00:38:11,119
probably

1163
00:38:11,119 --> 00:38:12,240
annoying the crap out of you with

1164
00:38:12,240 --> 00:38:15,280
buzzwords um i wanted to to um

1165
00:38:15,280 --> 00:38:17,280
to communicate and at the end of the day

1166
00:38:17,280 --> 00:38:19,599
i don't think anybody said it better

1167
00:38:19,599 --> 00:38:22,320
and uh than dan gear which is

1168
00:38:22,320 --> 00:38:24,240
uh up on the screen work like hell share

1169
00:38:24,240 --> 00:38:26,079
all you know abide by your handshake and

1170
00:38:26,079 --> 00:38:28,320
have fun we can do all of that at the

1171
00:38:28,320 --> 00:38:31,200
same time as we make better decisions by

1172
00:38:31,200 --> 00:38:32,480
being

1173
00:38:32,480 --> 00:38:35,119
less fragile and more anti-fragile

1174
00:38:35,119 --> 00:38:36,960
this was not a talk meant to even have a

1175
00:38:36,960 --> 00:38:40,160
punctuation point it was meant to spur a

1176
00:38:40,160 --> 00:38:42,560
set of conversations discussions and and

1177
00:38:42,560 --> 00:38:44,560
perhaps research in a lot of those

1178
00:38:44,560 --> 00:38:46,079
strategies and things that i talked

1179
00:38:46,079 --> 00:38:48,160
about jens rasmussen's approach to

1180
00:38:48,160 --> 00:38:51,760
safety snowden's view of decision-making

1181
00:38:51,760 --> 00:38:54,320
systems etc and if i have time i'll take

1182
00:38:54,320 --> 00:38:56,079
questions as best i can but the point

1183
00:38:56,079 --> 00:38:56,800
was

1184
00:38:56,800 --> 00:38:58,480
i was hoping to spur a conversation not

1185
00:38:58,480 --> 00:39:02,000
make not not not end with a a big bang

1186
00:39:02,000 --> 00:39:03,680
so i don't think i ended with a big bang

1187
00:39:03,680 --> 00:39:05,280
so at least i got one out of two but

1188
00:39:05,280 --> 00:39:08,920
yeah you had a question

1189
00:39:26,560 --> 00:39:28,720
so that when the accident happens and it

1190
00:39:28,720 --> 00:39:30,240
might not be your fault it might be your

1191
00:39:30,240 --> 00:39:32,000
fault it doesn't really matter the

1192
00:39:32,000 --> 00:39:34,320
occupants of the automobile survive and

1193
00:39:34,320 --> 00:39:36,560
walk away right

1194
00:39:36,560 --> 00:39:38,240
we need to discard engineering our

1195
00:39:38,240 --> 00:39:42,959
security programs in exactly a great way

1196
00:39:44,960 --> 00:39:45,839
yes

1197
00:39:45,839 --> 00:39:47,119
let me replay that real quick for people

1198
00:39:47,119 --> 00:39:49,359
in the audience he basically is making a

1199
00:39:49,359 --> 00:39:51,359
a parallel to the automobile industry

1200
00:39:51,359 --> 00:39:53,280
and their advancement of safety and

1201
00:39:53,280 --> 00:39:55,359
safety science over time and they

1202
00:39:55,359 --> 00:39:57,200
invented things like seat belts or

1203
00:39:57,200 --> 00:39:59,040
actually gave them away

1204
00:39:59,040 --> 00:40:01,680
and crumple zones in the automobile

1205
00:40:01,680 --> 00:40:03,040
which is meant to protect the asset in

1206
00:40:03,040 --> 00:40:05,359
case of an accident uh and ultimately we

1207
00:40:05,359 --> 00:40:06,960
need to do the same it sort of ties into

1208
00:40:06,960 --> 00:40:08,880
a lot of what jens rasmussen said but a

1209
00:40:08,880 --> 00:40:10,880
lot of that is being thoughtful about

1210
00:40:10,880 --> 00:40:12,880
being rugged about being you know

1211
00:40:12,880 --> 00:40:14,400
anti-fragile in a way in which we think

1212
00:40:14,400 --> 00:40:16,240
about outcomes differently which is not

1213
00:40:16,240 --> 00:40:19,119
just you know i stopped

1214
00:40:19,119 --> 00:40:20,079
this

1215
00:40:20,079 --> 00:40:21,839
this particular event from occurring

1216
00:40:21,839 --> 00:40:23,920
what impact or outcomes did it not have

1217
00:40:23,920 --> 00:40:24,720
that

1218
00:40:24,720 --> 00:40:26,400
adversely affected the org yeah thank

1219
00:40:26,400 --> 00:40:28,800
you for that it was a good summary

1220
00:40:28,800 --> 00:40:30,640
all right uh yeah

1221
00:40:30,640 --> 00:40:33,040
unlike the automobile industry maybe we

1222
00:40:33,040 --> 00:40:34,400
should also be concerned about the

1223
00:40:34,400 --> 00:40:35,920
people outside

1224
00:40:35,920 --> 00:40:37,920
oh that's an interesting one unlike the

1225
00:40:37,920 --> 00:40:39,440
automobile industry perhaps we should be

1226
00:40:39,440 --> 00:40:41,760
concerned with people outside the car

1227
00:40:41,760 --> 00:40:44,880
uh well my my wife's uh fsd tesla is

1228
00:40:44,880 --> 00:40:46,720
certainly an indicator of that

1229
00:40:46,720 --> 00:40:49,520
uh she's hit numerous curbs luckily no

1230
00:40:49,520 --> 00:40:51,280
people but yeah i do think that the

1231
00:40:51,280 --> 00:40:52,560
system as a whole if you look at

1232
00:40:52,560 --> 00:40:54,800
snowden's thing about right the the the

1233
00:40:54,800 --> 00:40:55,920
environment as a whole is a very

1234
00:40:55,920 --> 00:40:57,200
important piece not just people we

1235
00:40:57,200 --> 00:40:59,680
protect but but unintended consequences

1236
00:40:59,680 --> 00:41:00,800
and if you need me to go you just give

1237
00:41:00,800 --> 00:41:01,920
me the hook i'll just keep talking to

1238
00:41:01,920 --> 00:41:02,800
them

1239
00:41:02,800 --> 00:41:04,000
all right

1240
00:41:04,000 --> 00:41:05,599
anyway i hope that was helpful thanks

1241
00:41:05,599 --> 00:41:06,720
for being with me i haven't talked in

1242
00:41:06,720 --> 00:41:08,720
seven years so i'm

1243
00:41:08,720 --> 00:41:12,530
a lot out of practice thanks

1244
00:41:12,530 --> 00:41:14,949
[Applause]

