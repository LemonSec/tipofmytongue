1
00:00:00,000 --> 00:00:02,280
so thank you all for coming uh I can't

2
00:00:02,280 --> 00:00:04,080
tell you how happy I am to be speaking

3
00:00:04,080 --> 00:00:05,819
here this is my favorite conference so

4
00:00:05,819 --> 00:00:07,440
to be able to just be standing here is

5
00:00:07,440 --> 00:00:09,179
awesome

6
00:00:09,179 --> 00:00:11,519
uh my name is Arthur I manage one of the

7
00:00:11,519 --> 00:00:13,980
appsec teams at Ping Identity and here

8
00:00:13,980 --> 00:00:15,540
I'm here today to tell you all about

9
00:00:15,540 --> 00:00:18,420
Fair stride which is a framework

10
00:00:18,420 --> 00:00:19,800
louder

11
00:00:19,800 --> 00:00:22,500
closer to the mic all right

12
00:00:22,500 --> 00:00:24,180
so I'm here today to tell you all about

13
00:00:24,180 --> 00:00:26,640
Fair stride which is a framework we

14
00:00:26,640 --> 00:00:28,500
developed uh to

15
00:00:28,500 --> 00:00:30,900
um to have engineering threats and the

16
00:00:30,900 --> 00:00:32,820
business be able to speak Apples to

17
00:00:32,820 --> 00:00:35,899
Apples when threat modeling

18
00:00:38,640 --> 00:00:41,219
uh so just an agenda slide uh I'm going

19
00:00:41,219 --> 00:00:42,059
to talk a little bit about the

20
00:00:42,059 --> 00:00:43,800
background of uh how I got to this

21
00:00:43,800 --> 00:00:46,200
material and then I'll talk about the

22
00:00:46,200 --> 00:00:48,239
components that came into building this

23
00:00:48,239 --> 00:00:50,640
thing first I'll talk about stride and

24
00:00:50,640 --> 00:00:52,800
how we threat model with it then I'll

25
00:00:52,800 --> 00:00:55,140
talk about uh quantitative methods and

26
00:00:55,140 --> 00:00:57,719
fair and Monte Carlo simulations and uh

27
00:00:57,719 --> 00:00:59,460
why they matter and how they differ from

28
00:00:59,460 --> 00:01:00,840
how we do things typically in

29
00:01:00,840 --> 00:01:03,120
engineering and then I'll give a little

30
00:01:03,120 --> 00:01:05,459
demo so you guys have some uh some sort

31
00:01:05,459 --> 00:01:08,340
of tactical stuff to go home with and uh

32
00:01:08,340 --> 00:01:11,240
then we'll close it up

33
00:01:14,240 --> 00:01:18,720
uh so back in 2019 I was at rmisc which

34
00:01:18,720 --> 00:01:20,520
is a conference out in Colorado which is

35
00:01:20,520 --> 00:01:22,020
where I live

36
00:01:22,020 --> 00:01:24,299
um the keynote was Miko hipponen who's

37
00:01:24,299 --> 00:01:27,420
the head of research for f-secure he uh

38
00:01:27,420 --> 00:01:29,159
he had uh during his keynote on the the

39
00:01:29,159 --> 00:01:31,380
state of the net uh slide that was

40
00:01:31,380 --> 00:01:33,900
massive that had uh tires perpetually

41
00:01:33,900 --> 00:01:36,240
burning and he said in security when we

42
00:01:36,240 --> 00:01:39,420
do our jobs right nothing happens

43
00:01:39,420 --> 00:01:41,040
um that was very disheartening I'd just

44
00:01:41,040 --> 00:01:44,220
gotten promoted to management and um you

45
00:01:44,220 --> 00:01:45,600
know my job is to make my team

46
00:01:45,600 --> 00:01:47,640
successful and to align the the success

47
00:01:47,640 --> 00:01:50,040
of the individuals on the team with the

48
00:01:50,040 --> 00:01:52,860
the interests of the business and so to

49
00:01:52,860 --> 00:01:54,180
hear that the best that I could do is

50
00:01:54,180 --> 00:01:55,860
nothing was a problem that I couldn't

51
00:01:55,860 --> 00:01:58,140
ignore

52
00:01:58,140 --> 00:01:59,759
we tried a lot of things during that

53
00:01:59,759 --> 00:02:01,680
year as far as measuring our success but

54
00:02:01,680 --> 00:02:03,780
nothing really seemed to to hit the mark

55
00:02:03,780 --> 00:02:05,340
for me

56
00:02:05,340 --> 00:02:07,140
um until about a year later when I came

57
00:02:07,140 --> 00:02:09,000
to this very conference before the world

58
00:02:09,000 --> 00:02:12,420
shut down and uh Clint gave a uh his

59
00:02:12,420 --> 00:02:14,340
keynote on how to 10x your security

60
00:02:14,340 --> 00:02:17,340
without the series d uh the general

61
00:02:17,340 --> 00:02:20,280
thesis of the talk is if you're able to

62
00:02:20,280 --> 00:02:22,440
remove bug classes from your code base

63
00:02:22,440 --> 00:02:24,599
then you gain time and you move the

64
00:02:24,599 --> 00:02:27,300
needle measurably as far as the risk

65
00:02:27,300 --> 00:02:29,040
posture of your applications

66
00:02:29,040 --> 00:02:31,319
uh he centered this around two main use

67
00:02:31,319 --> 00:02:33,959
cases uh the first one was Secure by

68
00:02:33,959 --> 00:02:35,520
default Frameworks and the second one

69
00:02:35,520 --> 00:02:37,920
was semgap rules to make sure that your

70
00:02:37,920 --> 00:02:40,920
code doesn't reintroduce vulnerabilities

71
00:02:40,920 --> 00:02:42,660
now this was really encouraging because

72
00:02:42,660 --> 00:02:45,000
uh for the first time I was like oh

73
00:02:45,000 --> 00:02:48,120
we could actually actually have a path

74
00:02:48,120 --> 00:02:51,360
upwards from where we are now but then I

75
00:02:51,360 --> 00:02:53,340
came to another realization which is I

76
00:02:53,340 --> 00:02:55,200
don't know what the hell risk is I don't

77
00:02:55,200 --> 00:02:56,819
know how to measure it I don't know how

78
00:02:56,819 --> 00:02:58,440
to perceive it I don't really know

79
00:02:58,440 --> 00:03:00,180
anything about it apart from some people

80
00:03:00,180 --> 00:03:01,920
talk about it and apparently bad things

81
00:03:01,920 --> 00:03:05,580
happen one risk is involved occasionally

82
00:03:05,580 --> 00:03:07,500
uh so I picked up this book called how

83
00:03:07,500 --> 00:03:08,819
to measure anything in cyber security

84
00:03:08,819 --> 00:03:11,159
risk which is absolutely phenomenal it's

85
00:03:11,159 --> 00:03:14,159
very complete it has a lot of resources

86
00:03:14,159 --> 00:03:15,540
that have to do with quantitative

87
00:03:15,540 --> 00:03:17,879
methods to be able to evaluate and use

88
00:03:17,879 --> 00:03:20,819
risk in your Enterprise

89
00:03:20,819 --> 00:03:22,980
a lot of what they talk about and how

90
00:03:22,980 --> 00:03:25,140
they um how they position things in that

91
00:03:25,140 --> 00:03:27,420
book is very much from a CSO View and

92
00:03:27,420 --> 00:03:28,980
what we're going to do today is we're

93
00:03:28,980 --> 00:03:31,379
going to bring that down to the appsec

94
00:03:31,379 --> 00:03:32,280
level

95
00:03:32,280 --> 00:03:34,379
so it's really just a re-scoping of all

96
00:03:34,379 --> 00:03:36,180
the techniques that are that are in that

97
00:03:36,180 --> 00:03:37,560
book

98
00:03:37,560 --> 00:03:39,300
I got the audiobook it was like nine

99
00:03:39,300 --> 00:03:41,159
bucks so if you got nine bucks you

100
00:03:41,159 --> 00:03:42,840
should get that

101
00:03:42,840 --> 00:03:44,640
uh the last thing that I had to put up

102
00:03:44,640 --> 00:03:47,640
here uh is the the fair you uh risk lens

103
00:03:47,640 --> 00:03:49,739
Tool uh I'm not sponsored by them or

104
00:03:49,739 --> 00:03:51,599
anything like that uh but it is really

105
00:03:51,599 --> 00:03:53,700
awesome and very easy to use uh and you

106
00:03:53,700 --> 00:03:55,739
can get kind of a very elegant view of

107
00:03:55,739 --> 00:03:57,239
how far works and how quantitative

108
00:03:57,239 --> 00:04:00,180
methods work uh with like a very small

109
00:04:00,180 --> 00:04:01,799
amount of effort relative to reading

110
00:04:01,799 --> 00:04:03,599
that whole book

111
00:04:03,599 --> 00:04:04,799
so if you want to give it a shot give it

112
00:04:04,799 --> 00:04:07,400
a shot it's awesome

113
00:04:10,860 --> 00:04:13,019
all right so what are we doing today as

114
00:04:13,019 --> 00:04:14,640
far as modeling threats

115
00:04:14,640 --> 00:04:16,680
um there are many approaches to uh to

116
00:04:16,680 --> 00:04:18,238
threat modeling but today I'm going to

117
00:04:18,238 --> 00:04:21,060
talk about stride so stride was invented

118
00:04:21,060 --> 00:04:23,220
at Microsoft in the early 2000s as a

119
00:04:23,220 --> 00:04:24,360
part of their secure Computing

120
00:04:24,360 --> 00:04:25,759
initiatives

121
00:04:25,759 --> 00:04:27,900
uh there are as I mentioned other

122
00:04:27,900 --> 00:04:30,419
methods more artistic than others to be

123
00:04:30,419 --> 00:04:32,340
able to to model threats but stride is

124
00:04:32,340 --> 00:04:34,259
really good because it's a repeatable

125
00:04:34,259 --> 00:04:36,479
framework and if you're going to train a

126
00:04:36,479 --> 00:04:37,860
team of more than say two or three

127
00:04:37,860 --> 00:04:38,880
people

128
00:04:38,880 --> 00:04:40,380
it's really good to have a way to go

129
00:04:40,380 --> 00:04:42,720
about training them and if you have

130
00:04:42,720 --> 00:04:45,180
developers that are interested in uh in

131
00:04:45,180 --> 00:04:46,800
security and in threat modeling and you

132
00:04:46,800 --> 00:04:48,600
can give them a framework again it's

133
00:04:48,600 --> 00:04:50,220
better for for being able to share that

134
00:04:50,220 --> 00:04:51,360
knowledge

135
00:04:51,360 --> 00:04:53,160
uh it's not rocket science but it is

136
00:04:53,160 --> 00:04:56,400
awesome uh what you do is uh you build a

137
00:04:56,400 --> 00:04:58,560
data flow diagram of your application

138
00:04:58,560 --> 00:05:01,440
you draw trust boundaries and processes

139
00:05:01,440 --> 00:05:04,380
and data stores and arrows that show how

140
00:05:04,380 --> 00:05:07,680
the data goes from A to B to C and then

141
00:05:07,680 --> 00:05:09,780
you enumerate threats that fit in the

142
00:05:09,780 --> 00:05:11,460
stride buckets so the stride buckets

143
00:05:11,460 --> 00:05:13,320
follow that acronym it's spoofing

144
00:05:13,320 --> 00:05:15,120
tampering repudiation information

145
00:05:15,120 --> 00:05:17,340
disclosure denial of service and

146
00:05:17,340 --> 00:05:19,560
elevation of privilege

147
00:05:19,560 --> 00:05:21,419
is great right we actually have a

148
00:05:21,419 --> 00:05:23,220
systematic way to go about seeing what

149
00:05:23,220 --> 00:05:25,020
might go wrong with our application even

150
00:05:25,020 --> 00:05:27,979
if it's not implemented yet

151
00:05:27,979 --> 00:05:32,220
the output from this is a list of uh

152
00:05:32,220 --> 00:05:33,720
things that could go wrong or a risk

153
00:05:33,720 --> 00:05:36,539
registry and then we have to prioritize

154
00:05:36,539 --> 00:05:38,220
how we fix things

155
00:05:38,220 --> 00:05:40,139
do you guys hear those uh bumps in the

156
00:05:40,139 --> 00:05:41,820
thing

157
00:05:41,820 --> 00:05:43,440
uh

158
00:05:43,440 --> 00:05:45,960
so we have to prioritize how we fix

159
00:05:45,960 --> 00:05:48,240
things after we've already enumerated

160
00:05:48,240 --> 00:05:51,840
them and as far as I can tell in the

161
00:05:51,840 --> 00:05:53,759
industry we follow some some

162
00:05:53,759 --> 00:05:56,699
modification or some version of CVSs

163
00:05:56,699 --> 00:05:58,020
score calculators

164
00:05:58,020 --> 00:06:01,320
so you find a threat you go to the nist

165
00:06:01,320 --> 00:06:03,840
website you say the attack Vector is the

166
00:06:03,840 --> 00:06:05,940
network and the impact to

167
00:06:05,940 --> 00:06:08,460
confidentiality is low and then you know

168
00:06:08,460 --> 00:06:10,380
you click a bunch of buttons and that

169
00:06:10,380 --> 00:06:13,440
creates a score from 0 to 10 of uh on

170
00:06:13,440 --> 00:06:16,259
the CVSs scale

171
00:06:16,259 --> 00:06:19,620
uh that 0 to 10 then gets mapped to a

172
00:06:19,620 --> 00:06:21,720
high medium or low depending on what

173
00:06:21,720 --> 00:06:24,680
bracket you fall into

174
00:06:25,319 --> 00:06:27,360
so this is pretty good right

175
00:06:27,360 --> 00:06:28,440
but

176
00:06:28,440 --> 00:06:30,720
it's not very sophisticated and I mean

177
00:06:30,720 --> 00:06:32,880
that from a mathematical perspective so

178
00:06:32,880 --> 00:06:34,919
a high medium low critical informational

179
00:06:34,919 --> 00:06:37,080
those are what's called ordinal scales

180
00:06:37,080 --> 00:06:38,819
which means that they're very good at

181
00:06:38,819 --> 00:06:43,220
putting things Jesus Christ what is this

182
00:06:43,500 --> 00:06:46,280
I put it down

183
00:06:47,220 --> 00:06:49,740
okay

184
00:06:49,740 --> 00:06:52,139
okay sounds good

185
00:06:52,139 --> 00:06:53,880
um so high medium low critical

186
00:06:53,880 --> 00:06:56,100
informational those are all

187
00:06:56,100 --> 00:06:58,199
um those are those are an ordinal scale

188
00:06:58,199 --> 00:06:59,639
which means that they're very good at

189
00:06:59,639 --> 00:07:02,220
putting things in order so a high is

190
00:07:02,220 --> 00:07:03,660
higher than a medium and a medium is

191
00:07:03,660 --> 00:07:05,639
higher than a low but they're not very

192
00:07:05,639 --> 00:07:07,500
good at combining elements in each one

193
00:07:07,500 --> 00:07:09,720
of these buckets so you can't compare

194
00:07:09,720 --> 00:07:12,060
five mediums to a high and you can't

195
00:07:12,060 --> 00:07:14,160
compare a thousand lows to a high either

196
00:07:14,160 --> 00:07:16,199
neither can you compare the mediums with

197
00:07:16,199 --> 00:07:18,180
one another so your your security

198
00:07:18,180 --> 00:07:20,039
expertise might say hey this one is more

199
00:07:20,039 --> 00:07:21,840
important than that one but that's not a

200
00:07:21,840 --> 00:07:25,340
product of the scale that you're using

201
00:07:27,180 --> 00:07:29,520
uh to drive that point home uh Bishop

202
00:07:29,520 --> 00:07:31,680
Fox put out this uh this e-book called

203
00:07:31,680 --> 00:07:34,139
uh the wolf in sheep's clothing uh which

204
00:07:34,139 --> 00:07:35,880
is really fantastic

205
00:07:35,880 --> 00:07:37,979
um the this is basically a recollection

206
00:07:37,979 --> 00:07:40,139
of a whole bunch of uh nasty things that

207
00:07:40,139 --> 00:07:42,840
their uh Consultants did uh with a bunch

208
00:07:42,840 --> 00:07:45,000
of lows so from an offensive perspective

209
00:07:45,000 --> 00:07:47,520
and a technical perspective uh you can

210
00:07:47,520 --> 00:07:49,319
combine a bunch of lows in the the kill

211
00:07:49,319 --> 00:07:52,020
chain but when we look at it from a risk

212
00:07:52,020 --> 00:07:53,880
perspective and a risk registry on the

213
00:07:53,880 --> 00:07:55,740
on the defensive side we don't combine

214
00:07:55,740 --> 00:07:57,960
them so just from that perspective we're

215
00:07:57,960 --> 00:08:01,159
starting already on the back foot

216
00:08:03,360 --> 00:08:05,460
so how can we do better

217
00:08:05,460 --> 00:08:07,620
um today I'm going to propose that we do

218
00:08:07,620 --> 00:08:09,300
things better by looking at the impact

219
00:08:09,300 --> 00:08:11,460
of vulnerabilities on the business

220
00:08:11,460 --> 00:08:13,139
rather than on the applications

221
00:08:13,139 --> 00:08:14,400
themselves

222
00:08:14,400 --> 00:08:17,280
so uh if we think about how uh you know

223
00:08:17,280 --> 00:08:19,560
an appsec program is built you have a

224
00:08:19,560 --> 00:08:21,360
whole bunch of things that generate

225
00:08:21,360 --> 00:08:23,759
findings you got tools you got pen

226
00:08:23,759 --> 00:08:26,819
testers you have bug Bounty programs you

227
00:08:26,819 --> 00:08:28,560
got your internal developers that find

228
00:08:28,560 --> 00:08:30,419
things that are weird you got your

229
00:08:30,419 --> 00:08:31,979
security team that's going and trying to

230
00:08:31,979 --> 00:08:33,200
break stuff

231
00:08:33,200 --> 00:08:35,880
and all those things kind of funnel into

232
00:08:35,880 --> 00:08:38,520
your security team

233
00:08:38,520 --> 00:08:40,740
uh your appsec team can then go and say

234
00:08:40,740 --> 00:08:42,719
all right uh these four don't worry

235
00:08:42,719 --> 00:08:44,099
about them we know about a compensating

236
00:08:44,099 --> 00:08:46,140
control move them aside and then these

237
00:08:46,140 --> 00:08:48,000
two we should actually send them over to

238
00:08:48,000 --> 00:08:50,580
the to the remediation team

239
00:08:50,580 --> 00:08:52,800
and so then that gets passed over to the

240
00:08:52,800 --> 00:08:55,200
remediation team they go and spend some

241
00:08:55,200 --> 00:08:58,140
time which costs money uh to be able to

242
00:08:58,140 --> 00:09:00,839
uh to go fix the thing uh depending on

243
00:09:00,839 --> 00:09:04,019
uh you know your setup and your sdlc you

244
00:09:04,019 --> 00:09:05,880
might have some QA cycles that have to

245
00:09:05,880 --> 00:09:07,380
uh that have to that you have to go

246
00:09:07,380 --> 00:09:09,779
through you might have some uh some

247
00:09:09,779 --> 00:09:12,060
release uh you know documentation cycles

248
00:09:12,060 --> 00:09:13,380
that you have to you have to go through

249
00:09:13,380 --> 00:09:14,940
there and all these things cost people

250
00:09:14,940 --> 00:09:17,339
hours which cost money

251
00:09:17,339 --> 00:09:18,600
um you might also have to notify your

252
00:09:18,600 --> 00:09:19,980
customers right if something is really

253
00:09:19,980 --> 00:09:21,959
bad and you have to tell them you have

254
00:09:21,959 --> 00:09:23,760
to set up a campaign and you have to go

255
00:09:23,760 --> 00:09:25,680
and uh work with people throughout your

256
00:09:25,680 --> 00:09:27,720
business and take their time away from

257
00:09:27,720 --> 00:09:29,880
their job to be able to go and fix the

258
00:09:29,880 --> 00:09:32,459
the thing that you found

259
00:09:32,459 --> 00:09:34,620
and then you also have uh people

260
00:09:34,620 --> 00:09:36,420
escalating up to your support team and

261
00:09:36,420 --> 00:09:37,920
your customer success team saying hey

262
00:09:37,920 --> 00:09:39,839
this thing is broken can you please help

263
00:09:39,839 --> 00:09:42,660
me be reassured about my posture

264
00:09:42,660 --> 00:09:44,220
so all this to say we're going to look

265
00:09:44,220 --> 00:09:46,200
at it from a process perspective rather

266
00:09:46,200 --> 00:09:49,200
than a RC against uh your application

267
00:09:49,200 --> 00:09:51,860
perspective

268
00:09:54,240 --> 00:09:56,820
uh just a quick note about Fair uh fair

269
00:09:56,820 --> 00:09:58,980
is super elegant uh it's a way to break

270
00:09:58,980 --> 00:10:00,300
down risk

271
00:10:00,300 --> 00:10:02,580
um in in a way that's specific to

272
00:10:02,580 --> 00:10:04,080
infosec

273
00:10:04,080 --> 00:10:05,640
um I mentioned before that this is very

274
00:10:05,640 --> 00:10:08,100
much at the CSO View and we're going to

275
00:10:08,100 --> 00:10:09,899
scope this down to the uh to the

276
00:10:09,899 --> 00:10:12,540
application View

277
00:10:12,540 --> 00:10:14,160
uh if you Google for fair you're going

278
00:10:14,160 --> 00:10:16,680
to find this uh this tree I added a

279
00:10:16,680 --> 00:10:18,300
dotted line in the middle there because

280
00:10:18,300 --> 00:10:20,519
I just wanted to make a point that

281
00:10:20,519 --> 00:10:22,380
um if you need to break things down

282
00:10:22,380 --> 00:10:24,600
further in the tree you can go ahead and

283
00:10:24,600 --> 00:10:26,640
do that but it's not necessary so you

284
00:10:26,640 --> 00:10:27,959
don't have to make your way all the way

285
00:10:27,959 --> 00:10:29,760
down to contact frequency you could just

286
00:10:29,760 --> 00:10:32,339
stop at loss event frequency if you if

287
00:10:32,339 --> 00:10:33,720
it makes sense for the way that you're

288
00:10:33,720 --> 00:10:36,380
estimating things

289
00:10:36,420 --> 00:10:37,860
and I'm going to walk through each one

290
00:10:37,860 --> 00:10:40,700
of these here in a sec

291
00:10:42,779 --> 00:10:45,000
so let's talk about lost magnitude uh

292
00:10:45,000 --> 00:10:47,040
loss magnitude is the amount of money

293
00:10:47,040 --> 00:10:49,079
that is going to cost when something

294
00:10:49,079 --> 00:10:52,800
goes wrong or when a loss event occurs

295
00:10:52,800 --> 00:10:54,720
um this is always expressed as a 90

296
00:10:54,720 --> 00:10:57,060
confidence interval now what that means

297
00:10:57,060 --> 00:10:59,779
is that the accuracy of your estimation

298
00:10:59,779 --> 00:11:03,860
is going to come from uh the the the

299
00:11:03,860 --> 00:11:05,820
percentage of your confidence meaning

300
00:11:05,820 --> 00:11:08,459
that if you create a 90 confidence

301
00:11:08,459 --> 00:11:10,920
interval you want to be 90 sure that

302
00:11:10,920 --> 00:11:12,420
there will be that the value that will

303
00:11:12,420 --> 00:11:14,940
occur over the next 12 months will be in

304
00:11:14,940 --> 00:11:16,980
that interval versus trying to be as

305
00:11:16,980 --> 00:11:18,720
precise as possible with your interval

306
00:11:18,720 --> 00:11:21,240
and making it as tight as possible so

307
00:11:21,240 --> 00:11:24,180
that your your um you can get the sort

308
00:11:24,180 --> 00:11:27,000
of the best idea of what your value is

309
00:11:27,000 --> 00:11:30,660
actually going to be so as an example

310
00:11:30,660 --> 00:11:33,240
if I'm going to estimate the price of a

311
00:11:33,240 --> 00:11:35,220
gallon of gas in San Francisco in 12

312
00:11:35,220 --> 00:11:37,320
months I could say oh it's going to be

313
00:11:37,320 --> 00:11:39,660
550. but I'm going to be wrong a lot of

314
00:11:39,660 --> 00:11:41,579
the times sometimes it's going to be six

315
00:11:41,579 --> 00:11:43,680
bucks sometimes it's or it might be uh

316
00:11:43,680 --> 00:11:47,519
450 it might be five dollars but if I

317
00:11:47,519 --> 00:11:49,800
say it's going to be in the neighborhood

318
00:11:49,800 --> 00:11:52,019
or I'm 90 sure that it's going to be

319
00:11:52,019 --> 00:11:54,240
anywhere from 350 to seven dollars

320
00:11:54,240 --> 00:11:56,640
pending any kind of geopolitical issues

321
00:11:56,640 --> 00:11:58,740
or pandemics or anything like that I

322
00:11:58,740 --> 00:12:00,420
actually have a good range where I'm

323
00:12:00,420 --> 00:12:03,060
confident uh that I'm 90 confident that

324
00:12:03,060 --> 00:12:05,399
it'll fall in that value and we allow

325
00:12:05,399 --> 00:12:07,079
for five percent outliers on the top and

326
00:12:07,079 --> 00:12:09,899
on the bottom side of that estimation

327
00:12:09,899 --> 00:12:11,600
foreign

328
00:12:11,600 --> 00:12:15,000
magnitude is broken down into primary

329
00:12:15,000 --> 00:12:17,519
and secondary losses primary losses are

330
00:12:17,519 --> 00:12:19,620
losses that happen every time that a

331
00:12:19,620 --> 00:12:21,480
loss event occurs and they're usually

332
00:12:21,480 --> 00:12:24,300
inflicted by the business upon itself

333
00:12:24,300 --> 00:12:26,480
secondary losses are losses that are

334
00:12:26,480 --> 00:12:29,040
usually inflicted on the business by a

335
00:12:29,040 --> 00:12:31,260
third party and that they do not occur

336
00:12:31,260 --> 00:12:33,420
every time a loss event occurs

337
00:12:33,420 --> 00:12:34,740
a lot of what we're going to see today

338
00:12:34,740 --> 00:12:37,200
as far as primary losses go are going to

339
00:12:37,200 --> 00:12:39,060
be security engineering Cycles because

340
00:12:39,060 --> 00:12:40,920
all the findings will make their way

341
00:12:40,920 --> 00:12:43,440
into the security team to be triaged but

342
00:12:43,440 --> 00:12:45,540
not all of them will will have to pay a

343
00:12:45,540 --> 00:12:47,519
bounty for or not all of them will have

344
00:12:47,519 --> 00:12:49,740
to trigger engineering Cycles to go fix

345
00:12:49,740 --> 00:12:51,980
things

346
00:12:54,540 --> 00:12:57,480
on the other side we have a loss event

347
00:12:57,480 --> 00:13:00,240
frequency so loss of infrequency is the

348
00:13:00,240 --> 00:13:02,639
probability that something bad that will

349
00:13:02,639 --> 00:13:04,860
cause a loss will occur within the next

350
00:13:04,860 --> 00:13:07,440
12 months and we're going to use 12

351
00:13:07,440 --> 00:13:10,200
months here because uh scoping the time

352
00:13:10,200 --> 00:13:12,060
of your probability is very very

353
00:13:12,060 --> 00:13:14,279
important in in the way that you get the

354
00:13:14,279 --> 00:13:16,620
Strategic insights out of this modeling

355
00:13:16,620 --> 00:13:19,019
as well as the actual probability that

356
00:13:19,019 --> 00:13:21,720
you're going to get so the probability

357
00:13:21,720 --> 00:13:23,399
that your company is going to get owned

358
00:13:23,399 --> 00:13:25,920
tomorrow is very very close to zero but

359
00:13:25,920 --> 00:13:27,180
the probability that your company is

360
00:13:27,180 --> 00:13:28,920
going to get owned in the next 15 years

361
00:13:28,920 --> 00:13:31,980
is very close to 100. so scoping this

362
00:13:31,980 --> 00:13:34,440
appropriately is going to give you

363
00:13:34,440 --> 00:13:35,880
um is going to give you a different

364
00:13:35,880 --> 00:13:38,180
number

365
00:13:38,519 --> 00:13:39,899
as I mentioned we're going to be using

366
00:13:39,899 --> 00:13:41,639
12 months here and I'll talk a little

367
00:13:41,639 --> 00:13:43,860
bit more about why that is as we go

368
00:13:43,860 --> 00:13:46,040
through

369
00:13:46,139 --> 00:13:47,820
loss event frequency is broken down into

370
00:13:47,820 --> 00:13:49,740
threat event frequency and vulnerability

371
00:13:49,740 --> 00:13:51,300
so threat event frequencies the

372
00:13:51,300 --> 00:13:53,100
frequency with which you perceive a

373
00:13:53,100 --> 00:13:55,620
threat from an external actor so this

374
00:13:55,620 --> 00:13:57,420
could be seeing a cross-site scripting

375
00:13:57,420 --> 00:14:00,839
payload and vulnerability is the com is

376
00:14:00,839 --> 00:14:02,760
the the conversion of that into a loss

377
00:14:02,760 --> 00:14:03,980
event

378
00:14:03,980 --> 00:14:06,720
vulnerability is uh broken down and

379
00:14:06,720 --> 00:14:08,100
actually vulnerability is defined

380
00:14:08,100 --> 00:14:10,920
differently in in Fair than it is inside

381
00:14:10,920 --> 00:14:13,079
of the usual security engineering

382
00:14:13,079 --> 00:14:14,100
community

383
00:14:14,100 --> 00:14:16,139
uh here vulnerability is not a security

384
00:14:16,139 --> 00:14:18,839
bug it is the relationship between the

385
00:14:18,839 --> 00:14:20,279
strength of your controls and the

386
00:14:20,279 --> 00:14:21,779
strength of your adversaries and the

387
00:14:21,779 --> 00:14:24,120
strength of their attacks

388
00:14:24,120 --> 00:14:26,160
so vulnerability will tell us if the

389
00:14:26,160 --> 00:14:28,079
threat event has been converted into a

390
00:14:28,079 --> 00:14:30,720
loss event vulnerability is actually a

391
00:14:30,720 --> 00:14:34,019
quite complicated uh area of Affair that

392
00:14:34,019 --> 00:14:36,420
can warrant a talk and you know by

393
00:14:36,420 --> 00:14:37,220
itself

394
00:14:37,220 --> 00:14:39,839
but suffice it to say that you can think

395
00:14:39,839 --> 00:14:42,420
of this as uh the strength of a Dam

396
00:14:42,420 --> 00:14:44,519
versus a hurricane or the strength of

397
00:14:44,519 --> 00:14:46,320
your DDOS protection against a bunch of

398
00:14:46,320 --> 00:14:48,500
traffic

399
00:14:49,680 --> 00:14:51,959
uh threat event frequency is broken down

400
00:14:51,959 --> 00:14:54,120
into contact frequency and probability

401
00:14:54,120 --> 00:14:56,279
of action contact frequencies the

402
00:14:56,279 --> 00:14:58,139
frequency with which a threat actor

403
00:14:58,139 --> 00:14:59,880
makes contact with your asset so this

404
00:14:59,880 --> 00:15:01,740
can be some Recon building out a site

405
00:15:01,740 --> 00:15:04,019
map etc etc and then when that

406
00:15:04,019 --> 00:15:05,399
cross-site scripting payload gets

407
00:15:05,399 --> 00:15:07,199
delivered that gets converted into a

408
00:15:07,199 --> 00:15:09,480
threat event and so the probability of

409
00:15:09,480 --> 00:15:10,800
action is the probability of that

410
00:15:10,800 --> 00:15:13,339
occurring

411
00:15:15,720 --> 00:15:17,459
so what do we do with with all this

412
00:15:17,459 --> 00:15:19,260
stuff we we've estimated a bunch of

413
00:15:19,260 --> 00:15:21,060
things uh what do we actually do with it

414
00:15:21,060 --> 00:15:23,579
we're going to feed all this stuff into

415
00:15:23,579 --> 00:15:26,160
a Monte Carlo simulation so Monte Carlo

416
00:15:26,160 --> 00:15:28,199
simulations were invented by John Von

417
00:15:28,199 --> 00:15:30,720
Neumann and stanisla ulam at the

418
00:15:30,720 --> 00:15:33,240
Manhattan Project they needed a way to

419
00:15:33,240 --> 00:15:35,940
educate their decision making so that

420
00:15:35,940 --> 00:15:37,680
you know based on their understanding

421
00:15:37,680 --> 00:15:39,480
and with a complete lack of historical

422
00:15:39,480 --> 00:15:41,519
data they wouldn't make any kind of

423
00:15:41,519 --> 00:15:43,199
egregious mistakes when you know

424
00:15:43,199 --> 00:15:44,579
building the dangerous things that they

425
00:15:44,579 --> 00:15:47,899
built in the Manhattan Project

426
00:15:48,600 --> 00:15:50,279
um it's uh they called it everything had

427
00:15:50,279 --> 00:15:54,060
to had a had to have a code name and

428
00:15:54,060 --> 00:15:55,740
they called it Monte Carlo simulations

429
00:15:55,740 --> 00:15:59,040
after the casino Apparently one of the

430
00:15:59,040 --> 00:16:02,339
researchers had uh some an uncle that

431
00:16:02,339 --> 00:16:04,560
had some gambling problems and they said

432
00:16:04,560 --> 00:16:05,579
they figured they could use it

433
00:16:05,579 --> 00:16:07,079
afterwards to figure out how much money

434
00:16:07,079 --> 00:16:10,339
he might have to borrow from the family

435
00:16:11,519 --> 00:16:13,620
um the general idea of the Monte Carlo

436
00:16:13,620 --> 00:16:15,060
simulation is that we're going to take

437
00:16:15,060 --> 00:16:16,620
our uncertainty about our position

438
00:16:16,620 --> 00:16:18,839
through the the estimations that I

439
00:16:18,839 --> 00:16:20,760
talked about before and we're going to

440
00:16:20,760 --> 00:16:22,920
generate for all intents and purposes

441
00:16:22,920 --> 00:16:26,040
historical data so Computing has existed

442
00:16:26,040 --> 00:16:29,060
for you know Linux time started in 1970

443
00:16:29,060 --> 00:16:31,980
the security industry is maybe 30 years

444
00:16:31,980 --> 00:16:34,980
old with these methods we can generate a

445
00:16:34,980 --> 00:16:36,899
thousand years of data 10 000 years of

446
00:16:36,899 --> 00:16:39,420
data and then use that data to analyze

447
00:16:39,420 --> 00:16:42,120
uh or that we can then analyze that data

448
00:16:42,120 --> 00:16:44,519
as if it was historical and then as we

449
00:16:44,519 --> 00:16:46,079
make changes the data is going to change

450
00:16:46,079 --> 00:16:49,699
and our analysis is going to change

451
00:16:51,660 --> 00:16:54,060
so these simulations are always taking

452
00:16:54,060 --> 00:16:55,920
the form of picking points on

453
00:16:55,920 --> 00:16:58,860
probability distributions

454
00:16:58,860 --> 00:17:00,480
for today we're only going to be talking

455
00:17:00,480 --> 00:17:03,120
about log normal distributions to model

456
00:17:03,120 --> 00:17:05,040
the dollars lost for each one of the

457
00:17:05,040 --> 00:17:07,559
loss types that we're going to enumerate

458
00:17:07,559 --> 00:17:09,359
log normal distributions are suitable

459
00:17:09,359 --> 00:17:10,740
for this because they have no negative

460
00:17:10,740 --> 00:17:12,359
values which means you're not going to

461
00:17:12,359 --> 00:17:15,599
lose negative dollars which is good and

462
00:17:15,599 --> 00:17:17,040
they also have very long tails which

463
00:17:17,040 --> 00:17:18,240
means that they'll pick up outlier

464
00:17:18,240 --> 00:17:21,000
events on the top side

465
00:17:21,000 --> 00:17:22,859
um and pay no attention to the numbers

466
00:17:22,859 --> 00:17:24,359
on the scale they mean absolutely

467
00:17:24,359 --> 00:17:26,099
nothing

468
00:17:26,099 --> 00:17:29,460
um just as an example uh these are the

469
00:17:29,460 --> 00:17:31,559
the black curve could be for example the

470
00:17:31,559 --> 00:17:34,440
the model of your um your bug Bounty

471
00:17:34,440 --> 00:17:36,419
payouts the blue curve might be your

472
00:17:36,419 --> 00:17:38,280
engineering cycles and the red curve

473
00:17:38,280 --> 00:17:40,620
might be for PR campaigns

474
00:17:40,620 --> 00:17:42,299
they all look a little bit different but

475
00:17:42,299 --> 00:17:44,400
they share those same uh properties and

476
00:17:44,400 --> 00:17:45,539
the reason they look different is

477
00:17:45,539 --> 00:17:48,480
because the inputs to these curves are

478
00:17:48,480 --> 00:17:50,580
the estimations that we had as far as

479
00:17:50,580 --> 00:17:52,799
lost magnitude

480
00:17:52,799 --> 00:17:54,720
and also when I do the demo I'll show

481
00:17:54,720 --> 00:17:58,340
you uh exactly what this means

482
00:18:01,679 --> 00:18:04,620
so what's the tldr uh for if you want to

483
00:18:04,620 --> 00:18:06,480
do Fair stride the tldr is that you're

484
00:18:06,480 --> 00:18:08,160
going to model threats with stride

485
00:18:08,160 --> 00:18:10,380
you're going to feed the output from

486
00:18:10,380 --> 00:18:12,660
your threat model into a Monte Carlo

487
00:18:12,660 --> 00:18:14,760
simulation rather than a set of CVSs

488
00:18:14,760 --> 00:18:17,340
score calculators and then you're going

489
00:18:17,340 --> 00:18:19,080
to use los exceedance curves which are

490
00:18:19,080 --> 00:18:20,720
the output from the model

491
00:18:20,720 --> 00:18:24,419
to define a baseline a goal post and to

492
00:18:24,419 --> 00:18:26,039
measure your progress from where you're

493
00:18:26,039 --> 00:18:28,380
at to where you want to be

494
00:18:28,380 --> 00:18:30,240
and you'll I'll show you what exactly

495
00:18:30,240 --> 00:18:31,620
what Allah succeedings curve is in the

496
00:18:31,620 --> 00:18:33,918
demo

497
00:18:34,799 --> 00:18:36,919
foreign

498
00:18:36,919 --> 00:18:39,000
so we're going to do a quick demo here

499
00:18:39,000 --> 00:18:40,260
I'm going to show you a big spreadsheet

500
00:18:40,260 --> 00:18:41,520
it's going to look really gross but I'll

501
00:18:41,520 --> 00:18:42,720
walk you through it

502
00:18:42,720 --> 00:18:44,039
um it's not again it's not rocket

503
00:18:44,039 --> 00:18:46,080
science none of the data that I'm going

504
00:18:46,080 --> 00:18:47,640
to show you or the threats or anything

505
00:18:47,640 --> 00:18:49,500
like that are pertinent to uh to Ping

506
00:18:49,500 --> 00:18:51,960
Identity everything is completely uh

507
00:18:51,960 --> 00:18:53,340
completely fake

508
00:18:53,340 --> 00:18:55,799
um just to prove a point here

509
00:18:55,799 --> 00:18:58,140
and this is you know credit where credit

510
00:18:58,140 --> 00:18:59,820
is due this is

511
00:18:59,820 --> 00:19:01,080
um basically the one for one

512
00:19:01,080 --> 00:19:02,940
substitution model that Hubbard and

513
00:19:02,940 --> 00:19:05,280
searson make available for free on how

514
00:19:05,280 --> 00:19:08,059
to measureanything.com cyber security

515
00:19:08,059 --> 00:19:10,140
so if you want to use the actual

516
00:19:10,140 --> 00:19:12,360
simulation here you can go and download

517
00:19:12,360 --> 00:19:14,220
it there for free or you can hit me up

518
00:19:14,220 --> 00:19:16,080
after this talk and I can give you this

519
00:19:16,080 --> 00:19:18,620
version of it

520
00:19:19,980 --> 00:19:21,539
all right bear with me here when I do

521
00:19:21,539 --> 00:19:24,260
the display thing

522
00:19:28,740 --> 00:19:31,940
that's not right

523
00:19:42,299 --> 00:19:44,520
it's gross isn't it

524
00:19:44,520 --> 00:19:45,780
um can you all read this or do you need

525
00:19:45,780 --> 00:19:47,100
me to zoom in more

526
00:19:47,100 --> 00:19:49,559
it's good all right

527
00:19:49,559 --> 00:19:51,660
um so you'll see here on the left uh

528
00:19:51,660 --> 00:19:54,000
these merge cells these are the

529
00:19:54,000 --> 00:19:55,320
different threats that we've enumerated

530
00:19:55,320 --> 00:19:57,539
during uh during threat modeling so

531
00:19:57,539 --> 00:20:00,240
you'll see here we have S1 for spoofing

532
00:20:00,240 --> 00:20:03,240
one uh we got some tampering down here

533
00:20:03,240 --> 00:20:05,760
we got some repudiation some information

534
00:20:05,760 --> 00:20:08,520
disclosure we got some denial of service

535
00:20:08,520 --> 00:20:11,520
and we got some elevation of privilege

536
00:20:11,520 --> 00:20:14,220
now instead of having the output of this

537
00:20:14,220 --> 00:20:16,860
be a high medium or a low we actually

538
00:20:16,860 --> 00:20:19,380
have a list of things that can go wrong

539
00:20:19,380 --> 00:20:21,539
also known as loss types

540
00:20:21,539 --> 00:20:23,700
so these are all things that can cost

541
00:20:23,700 --> 00:20:27,000
money and that don't always cost money I

542
00:20:27,000 --> 00:20:28,740
mentioned earlier primary and secondary

543
00:20:28,740 --> 00:20:31,679
losses they have

544
00:20:31,679 --> 00:20:33,299
um you know secondary losses have a

545
00:20:33,299 --> 00:20:34,860
probability of occurring that is not a

546
00:20:34,860 --> 00:20:36,900
hundred percent primary losses will

547
00:20:36,900 --> 00:20:38,340
occur every time

548
00:20:38,340 --> 00:20:40,620
so you'll see here the the primary

549
00:20:40,620 --> 00:20:42,780
losses that we have are essentially just

550
00:20:42,780 --> 00:20:46,039
security engineering Cycles

551
00:20:47,400 --> 00:20:49,440
now you'll note here that uh that it

552
00:20:49,440 --> 00:20:51,179
doesn't say a hundred percent that says

553
00:20:51,179 --> 00:20:52,980
twenty percent

554
00:20:52,980 --> 00:20:54,600
um that is the probability of the event

555
00:20:54,600 --> 00:20:58,080
occurring over the next 12 months

556
00:20:58,080 --> 00:21:00,179
now the 20 here is just making a point

557
00:21:00,179 --> 00:21:02,220
so if for example you're implementing

558
00:21:02,220 --> 00:21:05,160
best practices and you have

559
00:21:05,160 --> 00:21:06,960
um let's say you you have TLS uh

560
00:21:06,960 --> 00:21:08,880
implemented everywhere which is pretty

561
00:21:08,880 --> 00:21:11,280
standard these days and somebody uh is

562
00:21:11,280 --> 00:21:13,500
starting to to move data uh confidential

563
00:21:13,500 --> 00:21:16,860
data in transit through that

564
00:21:16,860 --> 00:21:17,940
um

565
00:21:17,940 --> 00:21:19,200
you know if you were to go to your

566
00:21:19,200 --> 00:21:20,340
engineering team and say what are we

567
00:21:20,340 --> 00:21:21,480
going to do to make that better they're

568
00:21:21,480 --> 00:21:23,160
going to say we're already doing the

569
00:21:23,160 --> 00:21:24,960
best thing you know please go away and

570
00:21:24,960 --> 00:21:26,100
do something else

571
00:21:26,100 --> 00:21:28,679
but that doesn't mean that best practice

572
00:21:28,679 --> 00:21:31,200
today won't be best practice tomorrow

573
00:21:31,200 --> 00:21:35,760
so you know pre-log for J things threat

574
00:21:35,760 --> 00:21:37,080
models would have looked very different

575
00:21:37,080 --> 00:21:40,500
as post log4j and so what you can do is

576
00:21:40,500 --> 00:21:42,659
you can go look at historical data and

577
00:21:42,659 --> 00:21:45,059
cve databases are quite robust and have

578
00:21:45,059 --> 00:21:46,980
have a lot of information in them you

579
00:21:46,980 --> 00:21:48,659
can go back into your historical data of

580
00:21:48,659 --> 00:21:51,179
cves and say all right in the last say

581
00:21:51,179 --> 00:21:53,820
10 years how many years have there been

582
00:21:53,820 --> 00:21:56,340
one or more cves that have broken the

583
00:21:56,340 --> 00:21:58,260
confidentiality of TLS in one way or

584
00:21:58,260 --> 00:21:59,039
another

585
00:21:59,039 --> 00:22:00,539
and you can count them you can say all

586
00:22:00,539 --> 00:22:02,100
right well there were two years out of

587
00:22:02,100 --> 00:22:04,140
the last ten where confidentiality was

588
00:22:04,140 --> 00:22:06,419
broken in TLS therefore it's reasonable

589
00:22:06,419 --> 00:22:10,200
that we would have a 20 chance of this

590
00:22:10,200 --> 00:22:12,000
best practice being broken over the next

591
00:22:12,000 --> 00:22:13,200
12 months

592
00:22:13,200 --> 00:22:14,880
obviously that's not a spoofing issue

593
00:22:14,880 --> 00:22:17,900
but I'm just making a point

594
00:22:17,940 --> 00:22:19,799
uh you'll see below here for example

595
00:22:19,799 --> 00:22:21,659
that we have

596
00:22:21,659 --> 00:22:23,640
uh the probability that we're going to

597
00:22:23,640 --> 00:22:25,380
have to pay out a bounty is going to be

598
00:22:25,380 --> 00:22:27,179
the probability that the event actually

599
00:22:27,179 --> 00:22:30,539
occurs times the probability that uh it

600
00:22:30,539 --> 00:22:32,400
came from the bug Bounty program so you

601
00:22:32,400 --> 00:22:33,960
can go through all the vulnerabilities

602
00:22:33,960 --> 00:22:35,940
that you've uh that you've had over the

603
00:22:35,940 --> 00:22:39,780
last uh say a year or two and count the

604
00:22:39,780 --> 00:22:41,460
ones that came from the bug Bounty

605
00:22:41,460 --> 00:22:42,539
program and say all right it's

606
00:22:42,539 --> 00:22:44,640
reasonable that you know a subset of

607
00:22:44,640 --> 00:22:46,440
those are going to be you know maybe x

608
00:22:46,440 --> 00:22:49,020
amount of percent are going to be

609
00:22:49,020 --> 00:22:50,580
um coming from the bug Bounty program

610
00:22:50,580 --> 00:22:54,059
and you just multiply one by the other

611
00:22:54,059 --> 00:22:55,919
and that gives you a smaller percentage

612
00:22:55,919 --> 00:23:00,559
of having to actually pay out the bounty

613
00:23:00,960 --> 00:23:03,900
uh here are the 90 confidence intervals

614
00:23:03,900 --> 00:23:05,580
for each one of these loss types you can

615
00:23:05,580 --> 00:23:07,980
measure these differently so if you're

616
00:23:07,980 --> 00:23:10,080
going to measure uh say support Cycles

617
00:23:10,080 --> 00:23:12,059
you might those might be measured by

618
00:23:12,059 --> 00:23:14,580
that business unit by the number of or

619
00:23:14,580 --> 00:23:16,559
the the average cost of a of a support

620
00:23:16,559 --> 00:23:19,080
case and Counting the support cases that

621
00:23:19,080 --> 00:23:21,020
are pertinent to uh to security

622
00:23:21,020 --> 00:23:23,940
you might also go look at your your

623
00:23:23,940 --> 00:23:25,380
engineering cycles and say how much is

624
00:23:25,380 --> 00:23:27,059
it going to cost me to actually fix a

625
00:23:27,059 --> 00:23:28,320
bug like how can I come up with an

626
00:23:28,320 --> 00:23:31,140
estimate for that so you can go and look

627
00:23:31,140 --> 00:23:33,960
at you know how the the quickest and the

628
00:23:33,960 --> 00:23:36,120
longest uh time it's taken to fix a

629
00:23:36,120 --> 00:23:37,799
security bug inside your organization

630
00:23:37,799 --> 00:23:40,260
multiply that by the average hourly rate

631
00:23:40,260 --> 00:23:42,960
of your um of your engineers if you have

632
00:23:42,960 --> 00:23:44,760
access to that information if not you

633
00:23:44,760 --> 00:23:47,280
can just go to Glassdoor and that'll

634
00:23:47,280 --> 00:23:48,659
give you an estimate

635
00:23:48,659 --> 00:23:51,780
for a lower bound and an upper bound

636
00:23:51,780 --> 00:23:54,000
and you'll note that all the ways that

637
00:23:54,000 --> 00:23:55,500
you measure these things are different

638
00:23:55,500 --> 00:23:58,320
for every loss type but they boil down

639
00:23:58,320 --> 00:24:00,179
to Dollars ultimately which means that

640
00:24:00,179 --> 00:24:03,919
you can normalize them to dollars

641
00:24:04,320 --> 00:24:05,640
all right so now for the meat and

642
00:24:05,640 --> 00:24:08,059
potatoes

643
00:24:08,100 --> 00:24:10,679
so uh these three columns here so

644
00:24:10,679 --> 00:24:13,020
simulated inherent loss impact Rand and

645
00:24:13,020 --> 00:24:15,000
probability Rand are essentially the

646
00:24:15,000 --> 00:24:16,200
simulation

647
00:24:16,200 --> 00:24:17,720
um in action

648
00:24:17,720 --> 00:24:20,340
probability Rand is the probability that

649
00:24:20,340 --> 00:24:22,500
the event actually occurred uh you'll

650
00:24:22,500 --> 00:24:24,900
note that probability Rand is the same

651
00:24:24,900 --> 00:24:27,240
for each one of each one of these loss

652
00:24:27,240 --> 00:24:30,419
types that's because we don't want uh

653
00:24:30,419 --> 00:24:32,520
you know I talked about how you have

654
00:24:32,520 --> 00:24:35,820
security having the uh being being

655
00:24:35,820 --> 00:24:37,559
engaged every time and then the other

656
00:24:37,559 --> 00:24:39,659
loss types not being engaged every time

657
00:24:39,659 --> 00:24:42,240
we don't want a higher probability for

658
00:24:42,240 --> 00:24:43,799
some of the loss types that shouldn't be

659
00:24:43,799 --> 00:24:46,320
happening if security was not engaged so

660
00:24:46,320 --> 00:24:49,380
since this is for one specific threat we

661
00:24:49,380 --> 00:24:50,580
actually have the same random number

662
00:24:50,580 --> 00:24:53,400
selected for all of them

663
00:24:53,400 --> 00:24:55,860
impact Rand is going to be picking the

664
00:24:55,860 --> 00:24:57,240
random point on the probability

665
00:24:57,240 --> 00:25:01,020
distribution of the simulation

666
00:25:01,020 --> 00:25:03,900
and this really weird looking formula is

667
00:25:03,900 --> 00:25:07,140
the actual simulation so you'll see here

668
00:25:07,140 --> 00:25:11,220
that if the probability Rand is smaller

669
00:25:11,220 --> 00:25:12,260
than

670
00:25:12,260 --> 00:25:14,580
the probability of the event occurring

671
00:25:14,580 --> 00:25:17,580
then you you get a loss event if not

672
00:25:17,580 --> 00:25:19,500
nothing happens

673
00:25:19,500 --> 00:25:21,480
and you'll see here nothing happens for

674
00:25:21,480 --> 00:25:24,059
this specific type

675
00:25:24,059 --> 00:25:27,000
um if you do end up uh having this

676
00:25:27,000 --> 00:25:28,919
probability ran being smaller than your

677
00:25:28,919 --> 00:25:30,960
than your estimation you're going to go

678
00:25:30,960 --> 00:25:33,960
into this lognormal.m function and if

679
00:25:33,960 --> 00:25:36,059
not you're going to go to zero

680
00:25:36,059 --> 00:25:38,400
the lognormal.inv is the actual function

681
00:25:38,400 --> 00:25:40,440
that's going to pick the point on the on

682
00:25:40,440 --> 00:25:41,640
the distribution

683
00:25:41,640 --> 00:25:43,980
you'll note that it takes in the uh the

684
00:25:43,980 --> 00:25:46,340
random number right here

685
00:25:46,340 --> 00:25:50,400
and it takes in the mean

686
00:25:50,400 --> 00:25:53,400
as uh it takes in the um your balance

687
00:25:53,400 --> 00:25:54,779
for your confidence interval as the mean

688
00:25:54,779 --> 00:25:56,700
and then it takes in your bounds from

689
00:25:56,700 --> 00:25:58,559
your confidence interval as the standard

690
00:25:58,559 --> 00:25:59,760
deviation

691
00:25:59,760 --> 00:26:02,100
you'll note that 3.29 figure there

692
00:26:02,100 --> 00:26:04,980
that's based on the 90 of the confidence

693
00:26:04,980 --> 00:26:07,020
interval that fig that number will be

694
00:26:07,020 --> 00:26:10,260
different if you're 80 or 95 but I'm not

695
00:26:10,260 --> 00:26:12,720
going to get into that

696
00:26:12,720 --> 00:26:15,059
but essentially what you end up with is

697
00:26:15,059 --> 00:26:17,100
sometimes nothing happens for a

698
00:26:17,100 --> 00:26:18,900
particular threat or for certain losses

699
00:26:18,900 --> 00:26:20,820
and sometimes things do happen so you'll

700
00:26:20,820 --> 00:26:22,919
see that in this case we lost twenty two

701
00:26:22,919 --> 00:26:25,620
thousand dollars due to security

702
00:26:25,620 --> 00:26:27,960
engineering Cycles because of this

703
00:26:27,960 --> 00:26:30,600
spoofing threat

704
00:26:30,600 --> 00:26:32,520
I mentioned earlier that we're going to

705
00:26:32,520 --> 00:26:34,919
be simulating a thousand years of data

706
00:26:34,919 --> 00:26:36,659
so right now what we're looking at is

707
00:26:36,659 --> 00:26:39,179
year number 62.

708
00:26:39,179 --> 00:26:42,120
so on year 62 if we add all the values

709
00:26:42,120 --> 00:26:44,159
together of the simulated inherent loss

710
00:26:44,159 --> 00:26:49,320
we end up with 225 807 so that's how

711
00:26:49,320 --> 00:26:51,059
much we lost because of security issues

712
00:26:51,059 --> 00:26:53,340
based on our understanding against this

713
00:26:53,340 --> 00:26:55,500
specific application on year 62 of the

714
00:26:55,500 --> 00:26:57,919
simulation

715
00:27:00,900 --> 00:27:03,360
and if we go down here at the table that

716
00:27:03,360 --> 00:27:04,919
shows you all the years of the

717
00:27:04,919 --> 00:27:06,720
simulation you'll see that year 62 we

718
00:27:06,720 --> 00:27:10,620
lost 225 807 dollars

719
00:27:10,620 --> 00:27:12,480
you'll note that on year 63 we lost

720
00:27:12,480 --> 00:27:15,299
almost 800 Grand on year one we lost

721
00:27:15,299 --> 00:27:17,400
over a million bucks but on year seven

722
00:27:17,400 --> 00:27:21,240
we only lost 167 000. so our uncertainty

723
00:27:21,240 --> 00:27:23,640
has now been extrapolated into these

724
00:27:23,640 --> 00:27:26,820
these uh these outputs in dollars

725
00:27:26,820 --> 00:27:28,260
and now we can ask some really

726
00:27:28,260 --> 00:27:30,059
interesting questions and the most

727
00:27:30,059 --> 00:27:31,919
important one is what is the probability

728
00:27:31,919 --> 00:27:34,020
that we're going to lose X dollars or

729
00:27:34,020 --> 00:27:36,240
more over the next 12 months due to

730
00:27:36,240 --> 00:27:38,100
security issues

731
00:27:38,100 --> 00:27:40,559
and we can just count right we can go to

732
00:27:40,559 --> 00:27:42,539
280 000 and say all right I'm going to

733
00:27:42,539 --> 00:27:44,880
count how many of these values

734
00:27:44,880 --> 00:27:49,559
are greater than 280 000 and 47 40.7

735
00:27:49,559 --> 00:27:51,900
percent of them will be

736
00:27:51,900 --> 00:27:54,659
uh you'll note that we've had no losses

737
00:27:54,659 --> 00:27:57,720
less than 140 Grand for this application

738
00:27:57,720 --> 00:28:01,400
and the most that we've lost

739
00:28:02,580 --> 00:28:06,799
is a little bit over 2.1 million

740
00:28:07,980 --> 00:28:09,240
okay

741
00:28:09,240 --> 00:28:11,340
so we're going to plot

742
00:28:11,340 --> 00:28:13,020
those against each other and we're going

743
00:28:13,020 --> 00:28:15,000
to end up with with what's called a lost

744
00:28:15,000 --> 00:28:16,799
exceedance curve

745
00:28:16,799 --> 00:28:19,200
the law succeedance curve is a more

746
00:28:19,200 --> 00:28:22,320
mathematically sophisticated way to

747
00:28:22,320 --> 00:28:23,340
um

748
00:28:23,340 --> 00:28:25,200
to to um

749
00:28:25,200 --> 00:28:27,380
to express the risk of our application

750
00:28:27,380 --> 00:28:29,940
versus a registry of a whole bunch of

751
00:28:29,940 --> 00:28:33,360
items that are high medium or low

752
00:28:33,360 --> 00:28:35,640
you'll note here that inherent risk is

753
00:28:35,640 --> 00:28:38,480
red and residual risk is green

754
00:28:38,480 --> 00:28:40,440
residual risk is the measure of our

755
00:28:40,440 --> 00:28:42,480
progress inherent risk is our Baseline

756
00:28:42,480 --> 00:28:44,460
for some reason Excel put the green

757
00:28:44,460 --> 00:28:46,020
curve on top of the red curve but

758
00:28:46,020 --> 00:28:49,200
they're they're the same at this point

759
00:28:49,200 --> 00:28:51,120
now the interesting thing is we can go

760
00:28:51,120 --> 00:28:52,559
to an executive and we can say hey

761
00:28:52,559 --> 00:28:53,840
executive

762
00:28:53,840 --> 00:28:56,039
what probability are you willing to

763
00:28:56,039 --> 00:28:58,380
accept that you will lose 500 Grand or

764
00:28:58,380 --> 00:29:00,179
more against this application over the

765
00:29:00,179 --> 00:29:01,500
next 12 months

766
00:29:01,500 --> 00:29:03,480
and they might say well I don't want to

767
00:29:03,480 --> 00:29:07,080
lose any money against this application

768
00:29:07,080 --> 00:29:08,820
um and we could say okay that's good but

769
00:29:08,820 --> 00:29:12,120
based on our simulation we found that

770
00:29:12,120 --> 00:29:14,880
right now we're accepting a 7. 7.7

771
00:29:14,880 --> 00:29:17,400
chance that that's going to occur so

772
00:29:17,400 --> 00:29:18,960
what's something reasonable that makes

773
00:29:18,960 --> 00:29:22,679
sense for you and you can negotiate that

774
00:29:22,679 --> 00:29:25,080
and in this case we negotiated two

775
00:29:25,080 --> 00:29:26,100
percent

776
00:29:26,100 --> 00:29:27,720
you'll note that the values here are

777
00:29:27,720 --> 00:29:30,299
always going to be 100 for 100k

778
00:29:30,299 --> 00:29:33,840
and uh 20 for 200k and much much smaller

779
00:29:33,840 --> 00:29:35,820
for a million

780
00:29:35,820 --> 00:29:38,520
so now we have a Baseline and we have a

781
00:29:38,520 --> 00:29:40,860
goal post and now we can measure our

782
00:29:40,860 --> 00:29:42,659
progress from one to the other

783
00:29:42,659 --> 00:29:44,520
so if for example I want to say

784
00:29:44,520 --> 00:29:46,799
Implement a sem grep rule to remove a

785
00:29:46,799 --> 00:29:48,840
particular bug class from my uh from my

786
00:29:48,840 --> 00:29:50,580
code base I can do that and I can

787
00:29:50,580 --> 00:29:53,580
measure it against a process Improvement

788
00:29:53,580 --> 00:29:55,260
so I'm going to be measuring a technical

789
00:29:55,260 --> 00:29:57,659
control against uh against the process

790
00:29:57,659 --> 00:29:59,039
which is not something you can do with

791
00:29:59,039 --> 00:30:00,840
cbss scores

792
00:30:00,840 --> 00:30:03,720
so for example this elevation of

793
00:30:03,720 --> 00:30:05,820
privilege has two attack factors it's

794
00:30:05,820 --> 00:30:08,460
either through Json so remote code

795
00:30:08,460 --> 00:30:10,500
execution through Json processing or

796
00:30:10,500 --> 00:30:12,539
through XML let's say you write a

797
00:30:12,539 --> 00:30:15,179
semgrap rule to make dtds disabled by

798
00:30:15,179 --> 00:30:17,399
default and you therefore reduce the

799
00:30:17,399 --> 00:30:19,559
likelihood of this specific threat

800
00:30:19,559 --> 00:30:22,140
Happening by 50 because you remove 50 of

801
00:30:22,140 --> 00:30:24,360
the attack vectors and so you can add

802
00:30:24,360 --> 00:30:29,000
that value into this into this sheet

803
00:30:32,399 --> 00:30:36,199
and then I can rerun my simulation

804
00:30:36,360 --> 00:30:39,000
and lo and behold the the green curve

805
00:30:39,000 --> 00:30:42,299
has moved away from the red curve so the

806
00:30:42,299 --> 00:30:44,640
the needle that we've moved is the area

807
00:30:44,640 --> 00:30:47,460
under each one of these curves so some

808
00:30:47,460 --> 00:30:49,500
uh some businesses might only care about

809
00:30:49,500 --> 00:30:51,480
losses greater than a million some

810
00:30:51,480 --> 00:30:54,000
business some businesses will will look

811
00:30:54,000 --> 00:30:55,860
at this more holistically but what we've

812
00:30:55,860 --> 00:30:58,080
done here is we've seen okay well we

813
00:30:58,080 --> 00:31:00,600
started at area equals X and now we're

814
00:31:00,600 --> 00:31:02,700
at area equals x minus k

815
00:31:02,700 --> 00:31:06,020
so we've moved that needle

816
00:31:06,360 --> 00:31:10,158
now the interesting part is

817
00:31:10,559 --> 00:31:12,000
how does this compare to a process

818
00:31:12,000 --> 00:31:14,539
Improvement

819
00:31:15,480 --> 00:31:18,419
so let's say we notice that we spend a

820
00:31:18,419 --> 00:31:19,559
lot of time responding to customer

821
00:31:19,559 --> 00:31:22,260
escalations and maybe we have good

822
00:31:22,260 --> 00:31:23,880
relationships with our support org and

823
00:31:23,880 --> 00:31:26,340
we're able to build an FAQ that says uh

824
00:31:26,340 --> 00:31:27,659
hey this is how you're going to respond

825
00:31:27,659 --> 00:31:29,760
to these things and we might reduce the

826
00:31:29,760 --> 00:31:31,860
impact of that or the likelihood of that

827
00:31:31,860 --> 00:31:35,220
occurring by say 10 and maybe we we hope

828
00:31:35,220 --> 00:31:36,960
to be able to do that

829
00:31:36,960 --> 00:31:39,240
so I'll go to my customer success Cycles

830
00:31:39,240 --> 00:31:41,960
here and my support

831
00:31:41,960 --> 00:31:44,760
loss type and let's say I can reduce

832
00:31:44,760 --> 00:31:47,779
those by 10 percent

833
00:31:50,940 --> 00:31:52,980
and now I can apply this to all possible

834
00:31:52,980 --> 00:31:54,600
threat types because

835
00:31:54,600 --> 00:31:56,820
the

836
00:31:56,820 --> 00:31:58,679
the support Cycles are going to apply

837
00:31:58,679 --> 00:32:00,779
across the board it doesn't matter if

838
00:32:00,779 --> 00:32:04,320
support's responding to uh you know

839
00:32:04,320 --> 00:32:07,500
elevation of privilege or denial of

840
00:32:07,500 --> 00:32:10,700
service whoops

841
00:32:13,200 --> 00:32:16,700
just bear with me for a second

842
00:32:19,200 --> 00:32:20,460
cool

843
00:32:20,460 --> 00:32:23,520
and now I can rerun my simulation

844
00:32:23,520 --> 00:32:25,860
and lo and behold the residual risk

845
00:32:25,860 --> 00:32:27,840
curve has moved differently away from

846
00:32:27,840 --> 00:32:30,779
the inherent risk curve so I've now if

847
00:32:30,779 --> 00:32:32,340
I'm if I compare the area under the

848
00:32:32,340 --> 00:32:34,740
curve of this versus a sem grub rule I

849
00:32:34,740 --> 00:32:36,779
can actually compare their impact Apples

850
00:32:36,779 --> 00:32:39,360
to Apples now it might be very cheap to

851
00:32:39,360 --> 00:32:40,799
implement a sem grep rule you just go

852
00:32:40,799 --> 00:32:42,720
write some yaml and you're done but

853
00:32:42,720 --> 00:32:45,120
maybe you work at a really Legacy

854
00:32:45,120 --> 00:32:46,980
organization that doesn't have a CI CD

855
00:32:46,980 --> 00:32:48,240
pipeline or doesn't have all the

856
00:32:48,240 --> 00:32:50,159
infrastructure needed to have sem grep

857
00:32:50,159 --> 00:32:51,899
in place maybe you work at a modern

858
00:32:51,899 --> 00:32:53,700
organization that doesn't yet have some

859
00:32:53,700 --> 00:32:55,559
grep implemented and that might cost

860
00:32:55,559 --> 00:32:57,779
some money and some time to set up so

861
00:32:57,779 --> 00:32:59,520
you can you can do a cost benefit

862
00:32:59,520 --> 00:33:02,520
analysis for that versus setting up an

863
00:33:02,520 --> 00:33:04,679
FAQ or building a self-serve tool for

864
00:33:04,679 --> 00:33:07,020
for support

865
00:33:07,020 --> 00:33:08,940
and from a leadership perspective this

866
00:33:08,940 --> 00:33:10,980
is really exciting because I can build

867
00:33:10,980 --> 00:33:13,980
initiatives for my team that are

868
00:33:13,980 --> 00:33:16,860
meaningful to the business and that so

869
00:33:16,860 --> 00:33:18,480
they can understand the impact that they

870
00:33:18,480 --> 00:33:20,519
have and I can actually go report that

871
00:33:20,519 --> 00:33:21,779
back to somebody that cares about

872
00:33:21,779 --> 00:33:26,480
dollars and that's super exciting to me

873
00:33:35,940 --> 00:33:39,080
that's not right

874
00:33:44,399 --> 00:33:47,059
there we go

875
00:33:54,960 --> 00:33:56,760
cool

876
00:33:56,760 --> 00:33:59,399
so again what's the tldr for fair stride

877
00:33:59,399 --> 00:34:01,679
you model threats with stride you feed

878
00:34:01,679 --> 00:34:03,840
the output of your threat model into a

879
00:34:03,840 --> 00:34:05,700
Monte Carlo simulation rather than a

880
00:34:05,700 --> 00:34:07,980
bunch of CBS escort calculators and then

881
00:34:07,980 --> 00:34:10,080
you use los exceedance curves to define

882
00:34:10,080 --> 00:34:12,540
a baseline a goal post and your progress

883
00:34:12,540 --> 00:34:15,560
from point A to point B

884
00:34:17,820 --> 00:34:20,219
so how can this uh improve your program

885
00:34:20,219 --> 00:34:22,619
well um as I mentioned you can

886
00:34:22,619 --> 00:34:24,480
meaningfully prioritize your efforts

887
00:34:24,480 --> 00:34:26,820
Beyond compliance compliance is super

888
00:34:26,820 --> 00:34:28,199
important because it you know even

889
00:34:28,199 --> 00:34:29,699
though it's check boxy and it's not like

890
00:34:29,699 --> 00:34:31,320
the coolest thing in the in the security

891
00:34:31,320 --> 00:34:33,300
World it actually brings dollars in

892
00:34:33,300 --> 00:34:34,980
because it enables your business to sell

893
00:34:34,980 --> 00:34:36,359
the thing that you're doing compliance

894
00:34:36,359 --> 00:34:37,139
for

895
00:34:37,139 --> 00:34:39,119
so that's great on the one side but then

896
00:34:39,119 --> 00:34:40,800
on the other side once you're done with

897
00:34:40,800 --> 00:34:42,000
that and you've automated your way

898
00:34:42,000 --> 00:34:44,219
through it what your purpose is is to

899
00:34:44,219 --> 00:34:47,040
reduce the at least as an appsec team is

900
00:34:47,040 --> 00:34:49,800
to reduce the losses associated with uh

901
00:34:49,800 --> 00:34:52,199
with security issues

902
00:34:52,199 --> 00:34:53,760
and so we're actually able to do that

903
00:34:53,760 --> 00:34:55,679
here which means that we can have

904
00:34:55,679 --> 00:34:56,879
Insight that is strategically

905
00:34:56,879 --> 00:34:58,680
significant and actually be able to

906
00:34:58,680 --> 00:35:02,040
build something that our Engineers care

907
00:35:02,040 --> 00:35:03,480
to contribute to

908
00:35:03,480 --> 00:35:05,520
this is heavily related to that third

909
00:35:05,520 --> 00:35:07,920
point about reducing burnout

910
00:35:07,920 --> 00:35:09,900
um I think this year there's a few talks

911
00:35:09,900 --> 00:35:11,700
about burnout and last year there were

912
00:35:11,700 --> 00:35:12,900
also like two or three talks about

913
00:35:12,900 --> 00:35:15,599
burnout at this at this conference uh we

914
00:35:15,599 --> 00:35:16,800
have a pretty rough job in security

915
00:35:16,800 --> 00:35:18,300
everything's broken all the time

916
00:35:18,300 --> 00:35:20,820
sometimes people get cynical and a lot

917
00:35:20,820 --> 00:35:22,320
of the times that happens because what

918
00:35:22,320 --> 00:35:24,300
we're doing isn't necessarily perceived

919
00:35:24,300 --> 00:35:26,460
as meaningful by the business but if the

920
00:35:26,460 --> 00:35:28,140
business stakeholders understand what we

921
00:35:28,140 --> 00:35:29,880
do in the same way that what the product

922
00:35:29,880 --> 00:35:32,700
teams do as far as dollars then we we

923
00:35:32,700 --> 00:35:34,220
can put ourselves in a position that's

924
00:35:34,220 --> 00:35:36,900
at least setting us up in the language

925
00:35:36,900 --> 00:35:39,900
of success maybe your insights won't be

926
00:35:39,900 --> 00:35:41,760
as successful as others but at least

927
00:35:41,760 --> 00:35:43,079
you're on this on this trail that

928
00:35:43,079 --> 00:35:44,820
everybody understands

929
00:35:44,820 --> 00:35:46,440
and from a leadership perspective again

930
00:35:46,440 --> 00:35:49,320
that's super super uh exciting my job is

931
00:35:49,320 --> 00:35:51,960
to make other people successful and this

932
00:35:51,960 --> 00:35:54,540
is a way to do that

933
00:35:54,540 --> 00:35:55,740
um that point in the middle about

934
00:35:55,740 --> 00:35:57,900
justifying investment or why not to pay

935
00:35:57,900 --> 00:35:59,099
for things

936
00:35:59,099 --> 00:36:01,680
if you can use these methods to

937
00:36:01,680 --> 00:36:03,480
understand what the value of a

938
00:36:03,480 --> 00:36:05,520
particular problem is to you or at least

939
00:36:05,520 --> 00:36:07,680
get a sense of it so if a vendor comes

940
00:36:07,680 --> 00:36:10,079
to you and says hey I'm solving this new

941
00:36:10,079 --> 00:36:12,240
and interesting problem give me a bunch

942
00:36:12,240 --> 00:36:13,800
of money you can actually have some

943
00:36:13,800 --> 00:36:15,180
leverage on how much money that thing

944
00:36:15,180 --> 00:36:19,160
might cost and why that's important

945
00:36:19,260 --> 00:36:21,780
uh this will also avoid you hiring too

946
00:36:21,780 --> 00:36:24,119
many people or Justify whether or not

947
00:36:24,119 --> 00:36:26,700
you need to hire more people if you have

948
00:36:26,700 --> 00:36:28,619
too many people on your team uh you're

949
00:36:28,619 --> 00:36:31,440
lucky but at some point uh the business

950
00:36:31,440 --> 00:36:32,700
is going to realize and they're going to

951
00:36:32,700 --> 00:36:33,960
say no you gotta you got to get rid of

952
00:36:33,960 --> 00:36:34,920
some people

953
00:36:34,920 --> 00:36:38,720
and that's not fun for anybody

954
00:36:41,460 --> 00:36:43,619
cool so uh if you want to use Fair

955
00:36:43,619 --> 00:36:46,140
stride this is a way that you might be

956
00:36:46,140 --> 00:36:48,420
able to do that you can do an assessment

957
00:36:48,420 --> 00:36:49,680
now which means that you're going to

958
00:36:49,680 --> 00:36:50,940
have to go talk to a bunch of people

959
00:36:50,940 --> 00:36:52,800
about money and then you're going to

960
00:36:52,800 --> 00:36:54,359
have to go talk to your engineers about

961
00:36:54,359 --> 00:36:56,400
the probability of bad things happening

962
00:36:56,400 --> 00:36:58,260
and what bad things might exist against

963
00:36:58,260 --> 00:37:00,780
your applications

964
00:37:00,780 --> 00:37:02,520
um you're going to get some strategic

965
00:37:02,520 --> 00:37:04,079
insights from that and be able to build

966
00:37:04,079 --> 00:37:06,180
some initiatives around them you're

967
00:37:06,180 --> 00:37:07,500
going to execute on those initiatives

968
00:37:07,500 --> 00:37:10,320
and measure them independently but then

969
00:37:10,320 --> 00:37:12,359
ultimately boil the progress that you

970
00:37:12,359 --> 00:37:14,400
have on all of them back to dollars or

971
00:37:14,400 --> 00:37:17,040
likelihood of occurrence so you would

972
00:37:17,040 --> 00:37:19,140
measure the adoption of a semrep rule

973
00:37:19,140 --> 00:37:21,599
differently than you would measure how

974
00:37:21,599 --> 00:37:23,160
long it takes to fix a bug but

975
00:37:23,160 --> 00:37:24,660
ultimately all those things will boil

976
00:37:24,660 --> 00:37:28,079
down to either one or the other

977
00:37:28,079 --> 00:37:30,420
uh and then you're going to repeat the

978
00:37:30,420 --> 00:37:33,060
assessment uh 12 months from now

979
00:37:33,060 --> 00:37:35,400
um your application will have changed uh

980
00:37:35,400 --> 00:37:37,380
12 months from now your controls will

981
00:37:37,380 --> 00:37:38,820
have changed the threat landscape will

982
00:37:38,820 --> 00:37:40,560
have changed substantially a lot of

983
00:37:40,560 --> 00:37:42,300
things will be different so being able

984
00:37:42,300 --> 00:37:44,460
to repeat this process and create the

985
00:37:44,460 --> 00:37:46,140
next year's initiatives and goals and so

986
00:37:46,140 --> 00:37:50,060
forth is going to be a good way forward

987
00:37:51,540 --> 00:37:54,119
and that's pretty much it

988
00:37:54,119 --> 00:37:56,400
um that's my LinkedIn I know it's like

989
00:37:56,400 --> 00:37:57,780
weird to show QR codes at security

990
00:37:57,780 --> 00:37:59,940
conferences but you know you don't have

991
00:37:59,940 --> 00:38:01,500
to you don't have to you don't have to

992
00:38:01,500 --> 00:38:04,260
look at it if you have any questions or

993
00:38:04,260 --> 00:38:06,300
if you want to have if you want to have

994
00:38:06,300 --> 00:38:08,520
any of the the materials that I shared

995
00:38:08,520 --> 00:38:11,040
today please hit me up I'm happy to

996
00:38:11,040 --> 00:38:13,380
share everything none of this is my

997
00:38:13,380 --> 00:38:15,000
intellectual property I just smooshed a

998
00:38:15,000 --> 00:38:16,320
bunch of things together in a way that

999
00:38:16,320 --> 00:38:20,480
makes sense to me so thank you

1000
00:38:24,119 --> 00:38:26,400
just give me a sec he was first

1001
00:38:26,400 --> 00:38:28,380
uh thank you so much for the

1002
00:38:28,380 --> 00:38:30,000
presentation it's a topic that's close

1003
00:38:30,000 --> 00:38:31,859
to my heart also know ping very well

1004
00:38:31,859 --> 00:38:33,660
friends work there I was in this space

1005
00:38:33,660 --> 00:38:35,760
for about 10 years I worked at HSBC so

1006
00:38:35,760 --> 00:38:37,680
when I joined I my job was to work with

1007
00:38:37,680 --> 00:38:39,660
the developers and help figure out how

1008
00:38:39,660 --> 00:38:41,040
to prioritize all the issues right and

1009
00:38:41,040 --> 00:38:43,260
as you pointed out models like CVSs or

1010
00:38:43,260 --> 00:38:45,359
severity doesn't really have the

1011
00:38:45,359 --> 00:38:46,619
business context so you can't really

1012
00:38:46,619 --> 00:38:48,300
calculate risk based on just severity

1013
00:38:48,300 --> 00:38:50,280
you need impact or as you call it loss

1014
00:38:50,280 --> 00:38:51,900
and likelihood to be able to calculate

1015
00:38:51,900 --> 00:38:53,760
risk the challenge that we encounter

1016
00:38:53,760 --> 00:38:55,260
when I saw when I first went down that

1017
00:38:55,260 --> 00:38:57,180
path okay the swap model this come up

1018
00:38:57,180 --> 00:38:58,619
with a risk so that we can prioritize

1019
00:38:58,619 --> 00:39:00,660
based on risk and they can focus the

1020
00:39:00,660 --> 00:39:02,160
development effort where it matters most

1021
00:39:02,160 --> 00:39:03,960
so you can avoid burnout and you know

1022
00:39:03,960 --> 00:39:05,700
spitting your your Cycles where it

1023
00:39:05,700 --> 00:39:07,380
doesn't matter we started just

1024
00:39:07,380 --> 00:39:09,000
eyeballing it right as you said that

1025
00:39:09,000 --> 00:39:10,380
doesn't really work I mean it's one's

1026
00:39:10,380 --> 00:39:11,760
opinion is different than someone else's

1027
00:39:11,760 --> 00:39:13,440
opinion so we started looking at models

1028
00:39:13,440 --> 00:39:15,480
like this where you provide specific

1029
00:39:15,480 --> 00:39:17,820
percentages and numbers and all that and

1030
00:39:17,820 --> 00:39:19,619
it became complicated like no one wants

1031
00:39:19,619 --> 00:39:21,240
to go through all those and not only is

1032
00:39:21,240 --> 00:39:22,920
it complicated but also the problem is

1033
00:39:22,920 --> 00:39:24,720
that you may not have data right like

1034
00:39:24,720 --> 00:39:25,920
you're saying well look at the previous

1035
00:39:25,920 --> 00:39:27,420
loss so some teams are like I never

1036
00:39:27,420 --> 00:39:29,339
measured it a team is brand new so you

1037
00:39:29,339 --> 00:39:31,440
may not always have the metrics so it

1038
00:39:31,440 --> 00:39:32,820
was like on the one side as you eyeball

1039
00:39:32,820 --> 00:39:34,200
it on the other side you go through a

1040
00:39:34,200 --> 00:39:36,240
really scientific model like this and

1041
00:39:36,240 --> 00:39:38,099
neither works very well at scale so what

1042
00:39:38,099 --> 00:39:39,480
we came up with something is in the

1043
00:39:39,480 --> 00:39:41,160
Middle where you're not eyeballing it

1044
00:39:41,160 --> 00:39:42,780
and you're not providing all the

1045
00:39:42,780 --> 00:39:44,579
specific data so we looked at dread

1046
00:39:44,579 --> 00:39:46,260
right so dread has been deprecated

1047
00:39:46,260 --> 00:39:47,520
because there's some problems with it

1048
00:39:47,520 --> 00:39:49,500
but we like the Simplicity because it

1049
00:39:49,500 --> 00:39:51,119
was like you know what let's take your

1050
00:39:51,119 --> 00:39:52,680
impact and break it into reputational

1051
00:39:52,680 --> 00:39:54,599
impact you know how many users were

1052
00:39:54,599 --> 00:39:56,220
impacted and the financial damage was

1053
00:39:56,220 --> 00:39:57,480
direct or indirect but instead of

1054
00:39:57,480 --> 00:39:59,280
putting numbers just say hey a three

1055
00:39:59,280 --> 00:40:00,780
means you'll go out of business everyone

1056
00:40:00,780 --> 00:40:03,240
can tell like that's that's easy a one

1057
00:40:03,240 --> 00:40:04,920
is like no one hears about it and two's

1058
00:40:04,920 --> 00:40:06,180
in the middle you lose some customers

1059
00:40:06,180 --> 00:40:08,099
you feel the pain right that's easy for

1060
00:40:08,099 --> 00:40:09,300
everyone to think about you don't need

1061
00:40:09,300 --> 00:40:11,280
to do all the scientific analysis and

1062
00:40:11,280 --> 00:40:13,320
that model seem to work really well so

1063
00:40:13,320 --> 00:40:15,060
and I run a consulting company now we

1064
00:40:15,060 --> 00:40:16,560
use that model not continually

1065
00:40:16,560 --> 00:40:18,480
monitoring how others do it and I wanted

1066
00:40:18,480 --> 00:40:20,220
to get your feedback like how easy is it

1067
00:40:20,220 --> 00:40:21,540
to calculate all these numbers how do

1068
00:40:21,540 --> 00:40:22,560
you address it when you don't have

1069
00:40:22,560 --> 00:40:24,420
historic data it seems like a lot of

1070
00:40:24,420 --> 00:40:26,280
work and a lot of complexity whereas why

1071
00:40:26,280 --> 00:40:28,079
haven't you gone for a simpler model

1072
00:40:28,079 --> 00:40:29,400
like where it's just that you know three

1073
00:40:29,400 --> 00:40:31,079
two one and you assess that and you

1074
00:40:31,079 --> 00:40:32,520
calculate the risk that way definitely

1075
00:40:32,520 --> 00:40:34,260
agree that risk is important severity is

1076
00:40:34,260 --> 00:40:36,000
definitely it's my pet peeve I hated

1077
00:40:36,000 --> 00:40:38,160
when people just slap a severity and say

1078
00:40:38,160 --> 00:40:39,900
here you go here's 100 issues and you

1079
00:40:39,900 --> 00:40:41,099
know there's no risk and of course I

1080
00:40:41,099 --> 00:40:42,660
came from a banking so everything runs

1081
00:40:42,660 --> 00:40:44,460
based on a risk model yeah I would love

1082
00:40:44,460 --> 00:40:46,020
to hear your opinion on that yeah

1083
00:40:46,020 --> 00:40:48,060
definitely um and we should get a beer

1084
00:40:48,060 --> 00:40:49,440
after it because I want to I want to

1085
00:40:49,440 --> 00:40:51,359
talk a bit more about that with you

1086
00:40:51,359 --> 00:40:53,220
um this was this took this took some

1087
00:40:53,220 --> 00:40:53,940
time

1088
00:40:53,940 --> 00:40:56,640
um I think it took about like Ping has

1089
00:40:56,640 --> 00:40:59,880
um I think 1300 employees nowadays it

1090
00:40:59,880 --> 00:41:01,260
took me about three months to build the

1091
00:41:01,260 --> 00:41:02,820
model

1092
00:41:02,820 --> 00:41:04,680
um once you have the base you can

1093
00:41:04,680 --> 00:41:06,420
iterate on it pretty quickly but if you

1094
00:41:06,420 --> 00:41:09,000
if you have really big companies there I

1095
00:41:09,000 --> 00:41:10,859
know that there are ways to to go about

1096
00:41:10,859 --> 00:41:13,200
doing it that makes sense that that are

1097
00:41:13,200 --> 00:41:15,060
um based on inference and and running

1098
00:41:15,060 --> 00:41:16,079
certain

1099
00:41:16,079 --> 00:41:18,000
um sort of Bayesian inference things

1100
00:41:18,000 --> 00:41:20,099
that I don't know much about but I know

1101
00:41:20,099 --> 00:41:21,420
that there are ways to go about doing

1102
00:41:21,420 --> 00:41:22,619
that

1103
00:41:22,619 --> 00:41:24,480
um the I think the the thing what's

1104
00:41:24,480 --> 00:41:26,460
important to understand is that this is

1105
00:41:26,460 --> 00:41:28,980
a tool that needs to be useful and if

1106
00:41:28,980 --> 00:41:30,359
it's not useful and you have to adjust

1107
00:41:30,359 --> 00:41:32,880
you should absolutely adjust

1108
00:41:32,880 --> 00:41:35,220
um if this model takes you six months to

1109
00:41:35,220 --> 00:41:37,079
build and you're trying to build for 12

1110
00:41:37,079 --> 00:41:38,579
months in the future it's kind of

1111
00:41:38,579 --> 00:41:40,680
ridiculous so

1112
00:41:40,680 --> 00:41:42,960
you know that's got that's kind of my my

1113
00:41:42,960 --> 00:41:44,640
high level take on it but we should talk

1114
00:41:44,640 --> 00:41:45,960
about that a lot more sounds great thank

1115
00:41:45,960 --> 00:41:47,520
you absolutely

1116
00:41:47,520 --> 00:41:50,660
uh yeah what's going on

1117
00:42:06,839 --> 00:42:10,800
thank you I also use the fair model

1118
00:42:10,800 --> 00:42:14,640
um in my job I also studied the um as a

1119
00:42:14,640 --> 00:42:16,560
risk officer so it's uh it's very

1120
00:42:16,560 --> 00:42:19,800
interesting for me and my question is my

1121
00:42:19,800 --> 00:42:22,200
big challenge was to build the threat

1122
00:42:22,200 --> 00:42:24,599
list so the one that you have shown in

1123
00:42:24,599 --> 00:42:28,020
the Excel and my question was uh very

1124
00:42:28,020 --> 00:42:31,320
let's say very basic where did you get

1125
00:42:31,320 --> 00:42:33,720
that and does it come from the Hubbard

1126
00:42:33,720 --> 00:42:36,300
XL which I don't remember to be honest

1127
00:42:36,300 --> 00:42:38,820
or did you build it yourself is that

1128
00:42:38,820 --> 00:42:42,119
where you used your three months no so

1129
00:42:42,119 --> 00:42:44,880
actually getting the threats out were

1130
00:42:44,880 --> 00:42:47,520
um pretty easy relative to getting all

1131
00:42:47,520 --> 00:42:49,020
the business unit owners to give me

1132
00:42:49,020 --> 00:42:51,000
numbers uh and to explain to them what

1133
00:42:51,000 --> 00:42:53,280
confidence intervals are the threats

1134
00:42:53,280 --> 00:42:55,200
themselves I mentioned I run an app sack

1135
00:42:55,200 --> 00:42:57,119
team so I have a team dedicated to doing

1136
00:42:57,119 --> 00:42:59,640
this so we're pretty we're pretty on top

1137
00:42:59,640 --> 00:43:01,260
of it when it comes to to building out

1138
00:43:01,260 --> 00:43:02,579
those threats but it's an engineering

1139
00:43:02,579 --> 00:43:04,819
exercise

1140
00:43:04,980 --> 00:43:07,680
so uh I noticed in your model you had a

1141
00:43:07,680 --> 00:43:09,900
phrase here called a revenue risk due to

1142
00:43:09,900 --> 00:43:11,819
churn which is a security architect I

1143
00:43:11,819 --> 00:43:13,140
can tell you is very near and dear to my

1144
00:43:13,140 --> 00:43:14,099
heart okay

1145
00:43:14,099 --> 00:43:14,640
um

1146
00:43:14,640 --> 00:43:17,400
which I am interpreting as uh this is

1147
00:43:17,400 --> 00:43:18,720
such a problem that the entire team

1148
00:43:18,720 --> 00:43:19,859
leaves and that is something I've

1149
00:43:19,859 --> 00:43:22,380
observed um I'm wondering how did you

1150
00:43:22,380 --> 00:43:23,940
evaluate that or how did you actually

1151
00:43:23,940 --> 00:43:26,400
try and build that into your model so

1152
00:43:26,400 --> 00:43:28,560
actually the definition that we had or

1153
00:43:28,560 --> 00:43:31,079
that that I have here for it is churn of

1154
00:43:31,079 --> 00:43:33,060
customers so customers leaving you

1155
00:43:33,060 --> 00:43:34,500
because they don't trust your product

1156
00:43:34,500 --> 00:43:35,940
anymore

1157
00:43:35,940 --> 00:43:37,020
um I mentioned you know when we do

1158
00:43:37,020 --> 00:43:39,480
compliance and stuff it's really to you

1159
00:43:39,480 --> 00:43:40,920
know enable a language of trust with

1160
00:43:40,920 --> 00:43:42,359
customers and if that trust is broken

1161
00:43:42,359 --> 00:43:44,280
they're going to get out of there you

1162
00:43:44,280 --> 00:43:45,839
know so that that's the type of churn

1163
00:43:45,839 --> 00:43:47,640
that we talk about

1164
00:43:47,640 --> 00:43:49,680
um and you can measure that one based on

1165
00:43:49,680 --> 00:43:52,200
you know product X makes this many

1166
00:43:52,200 --> 00:43:54,119
dollars per year this is what the impact

1167
00:43:54,119 --> 00:43:55,800
of it will be

1168
00:43:55,800 --> 00:43:56,400
um

1169
00:43:56,400 --> 00:43:59,040
uh before people leaving and uh for

1170
00:43:59,040 --> 00:44:01,020
people like people leaving your team

1171
00:44:01,020 --> 00:44:02,880
um I had a I had a line item in there

1172
00:44:02,880 --> 00:44:04,079
about people getting fired actually

1173
00:44:04,079 --> 00:44:06,180
rather than people leaving

1174
00:44:06,180 --> 00:44:09,720
um I think that if you if you you know

1175
00:44:09,720 --> 00:44:11,579
I take pride in the fact that I'm able

1176
00:44:11,579 --> 00:44:13,260
to to hold on to people for a long time

1177
00:44:13,260 --> 00:44:15,119
because I try to build a a program

1178
00:44:15,119 --> 00:44:16,740
that's comprehensive and that's uh

1179
00:44:16,740 --> 00:44:19,260
that's fun for everybody including the

1180
00:44:19,260 --> 00:44:20,640
business

1181
00:44:20,640 --> 00:44:22,319
um but yeah sometimes people get fired

1182
00:44:22,319 --> 00:44:24,960
and you can you know you take salary

1183
00:44:24,960 --> 00:44:26,280
data and you're like oh it's going to be

1184
00:44:26,280 --> 00:44:28,760
about that much

1185
00:44:41,579 --> 00:44:44,640
oh yeah hi

1186
00:44:44,640 --> 00:44:48,240
um so great presentation I I

1187
00:44:48,240 --> 00:44:51,060
um I work in the secure GRC where I'm so

1188
00:44:51,060 --> 00:44:53,280
fair I understand fair and

1189
00:44:53,280 --> 00:44:55,440
I never had a chance to implement at my

1190
00:44:55,440 --> 00:44:57,240
workplace is is

1191
00:44:57,240 --> 00:44:59,579
um is a little bit tough to implement my

1192
00:44:59,579 --> 00:45:01,260
work because it can go really deep right

1193
00:45:01,260 --> 00:45:03,960
and I see expression right I see that

1194
00:45:03,960 --> 00:45:07,020
you have a column called loss type

1195
00:45:07,020 --> 00:45:09,720
right do you have a process or do you

1196
00:45:09,720 --> 00:45:12,240
have a source to define the list of loss

1197
00:45:12,240 --> 00:45:13,319
types

1198
00:45:13,319 --> 00:45:15,000
absolutely

1199
00:45:15,000 --> 00:45:17,280
um so I look internally at my program

1200
00:45:17,280 --> 00:45:19,380
and I look at all the things that's that

1201
00:45:19,380 --> 00:45:21,240
we spend time on as a team and as a

1202
00:45:21,240 --> 00:45:23,940
company so I start with my team and I as

1203
00:45:23,940 --> 00:45:26,280
the sort of intake point for all the

1204
00:45:26,280 --> 00:45:28,980
security things uh for applications and

1205
00:45:28,980 --> 00:45:30,359
then I look at all the teams that we

1206
00:45:30,359 --> 00:45:32,819
impact Downstream and that's pretty easy

1207
00:45:32,819 --> 00:45:34,680
on my end because I talk to these people

1208
00:45:34,680 --> 00:45:36,420
regularly they're people that I have

1209
00:45:36,420 --> 00:45:39,060
one-on-ones with when uh when you when

1210
00:45:39,060 --> 00:45:40,500
things go wrong they're the people that

1211
00:45:40,500 --> 00:45:43,079
I work with so I kind of know who's

1212
00:45:43,079 --> 00:45:45,119
impacted by security issues at my

1213
00:45:45,119 --> 00:45:46,380
company

1214
00:45:46,380 --> 00:45:47,940
um and so that's how I build those lost

1215
00:45:47,940 --> 00:45:50,940
types they're Cycles if we think about

1216
00:45:50,940 --> 00:45:53,280
assets and the most expensive things

1217
00:45:53,280 --> 00:45:55,920
that companies have it's the rent that

1218
00:45:55,920 --> 00:45:57,359
you pay for your offices and how much

1219
00:45:57,359 --> 00:45:59,640
you pay your people and so everything

1220
00:45:59,640 --> 00:46:02,160
that's here is the the dollars are all

1221
00:46:02,160 --> 00:46:05,118
people's time really

1222
00:46:08,040 --> 00:46:10,579
you know

1223
00:46:11,760 --> 00:46:14,220
I might have missed it but where's the

1224
00:46:14,220 --> 00:46:15,900
intersection with corporate security and

1225
00:46:15,900 --> 00:46:17,819
the risk and are they using the similar

1226
00:46:17,819 --> 00:46:19,859
model and is there does that get up

1227
00:46:19,859 --> 00:46:20,819
leveled when you're talking about

1228
00:46:20,819 --> 00:46:23,460
resources for for team members or

1229
00:46:23,460 --> 00:46:25,500
whatnot across the court the whole

1230
00:46:25,500 --> 00:46:27,780
organization when you say corporate

1231
00:46:27,780 --> 00:46:30,000
security do you mean non-app site

1232
00:46:30,000 --> 00:46:31,980
non-product side so you have risks in

1233
00:46:31,980 --> 00:46:32,940
the organization that are not

1234
00:46:32,940 --> 00:46:35,940
application being customer facing do

1235
00:46:35,940 --> 00:46:37,680
they do a similar Fair model are you

1236
00:46:37,680 --> 00:46:39,119
doing that for them

1237
00:46:39,119 --> 00:46:41,940
I'm just doing this in a box to help my

1238
00:46:41,940 --> 00:46:42,780
team

1239
00:46:42,780 --> 00:46:45,119
I'm not I don't like um I know what you

1240
00:46:45,119 --> 00:46:46,920
mean like the scope expands and the way

1241
00:46:46,920 --> 00:46:48,380
that you know you might lose money

1242
00:46:48,380 --> 00:46:50,760
diversifies quite a bit

1243
00:46:50,760 --> 00:46:52,680
um the book is actually scoped to that

1244
00:46:52,680 --> 00:46:54,540
uh to that level

1245
00:46:54,540 --> 00:46:56,000
um but we don't do that

1246
00:46:56,000 --> 00:46:58,920
I'm just doing it myself and it's uh

1247
00:46:58,920 --> 00:47:00,720
it's been helpful for me yeah and I was

1248
00:47:00,720 --> 00:47:03,119
just more wondering if your peer on the

1249
00:47:03,119 --> 00:47:04,680
corporate security side if they're if

1250
00:47:04,680 --> 00:47:05,880
you're they're doing the same thing

1251
00:47:05,880 --> 00:47:08,280
right and up leveling the risks to the

1252
00:47:08,280 --> 00:47:10,260
like a cheap risk officer or anything

1253
00:47:10,260 --> 00:47:12,240
like that that they have not done that

1254
00:47:12,240 --> 00:47:13,680
yet all right awesome thank you I wish

1255
00:47:13,680 --> 00:47:15,900
they did but they haven't

1256
00:47:15,900 --> 00:47:17,819
uh

1257
00:47:17,819 --> 00:47:20,720
yeah for sure

1258
00:47:30,839 --> 00:47:32,940
I'm really curious how you sanity check

1259
00:47:32,940 --> 00:47:35,940
your model one thing we know about uh

1260
00:47:35,940 --> 00:47:39,660
complex models is that they errors are

1261
00:47:39,660 --> 00:47:43,920
really easy to hide in the data and um

1262
00:47:43,920 --> 00:47:45,180
having tried to build these models

1263
00:47:45,180 --> 00:47:47,760
myself I've seen those errors uh in my

1264
00:47:47,760 --> 00:47:49,440
own models and now that I'm managing

1265
00:47:49,440 --> 00:47:51,240
some other people trying to do this it's

1266
00:47:51,240 --> 00:47:54,660
even harder to to check their work so

1267
00:47:54,660 --> 00:47:57,780
um sanded checking yeah uh you need to

1268
00:47:57,780 --> 00:48:00,420
teach me about that because I know there

1269
00:48:00,420 --> 00:48:01,680
are ways to do it and I know there are

1270
00:48:01,680 --> 00:48:04,619
ways to evaluate and to to go about that

1271
00:48:04,619 --> 00:48:06,839
but I I don't I'm not an expert in that

1272
00:48:06,839 --> 00:48:08,880
by any stretch of the term my my rough

1273
00:48:08,880 --> 00:48:11,160
idea that I have not tried yet is to

1274
00:48:11,160 --> 00:48:12,960
implement layers of models with

1275
00:48:12,960 --> 00:48:15,420
increasing levels of complexity and to

1276
00:48:15,420 --> 00:48:17,099
make sure that the more complex models

1277
00:48:17,099 --> 00:48:19,980
are kind of roughly similar to the

1278
00:48:19,980 --> 00:48:22,560
simpler models

1279
00:48:22,560 --> 00:48:24,540
we should talk about that more

1280
00:48:24,540 --> 00:48:26,640
um I know that I know for sure that

1281
00:48:26,640 --> 00:48:28,859
there are well-defined ways to do it I

1282
00:48:28,859 --> 00:48:30,000
know that they're covered in the book

1283
00:48:30,000 --> 00:48:32,819
and I can't I can't recall them off the

1284
00:48:32,819 --> 00:48:34,319
top of my head but there's definitely

1285
00:48:34,319 --> 00:48:38,160
like well-defined ways to go about it

1286
00:48:38,160 --> 00:48:41,480
there's one down here

1287
00:49:00,319 --> 00:49:03,180
again this steps in

1288
00:49:03,180 --> 00:49:05,819
as you're as your team grows and expands

1289
00:49:05,819 --> 00:49:08,040
and other teams want to continue using a

1290
00:49:08,040 --> 00:49:10,800
model like this how do you support other

1291
00:49:10,800 --> 00:49:13,920
people onboarding into what is a kind of

1292
00:49:13,920 --> 00:49:18,020
a potentially kind of very

1293
00:49:18,020 --> 00:49:20,000
internalized model that you've created

1294
00:49:20,000 --> 00:49:22,200
how do you train others to be able to

1295
00:49:22,200 --> 00:49:23,760
pick it up even if you were to switch

1296
00:49:23,760 --> 00:49:26,400
roles or switch teams

1297
00:49:26,400 --> 00:49:28,920
um so the question is like how I would

1298
00:49:28,920 --> 00:49:30,839
evangelize this across my organization

1299
00:49:30,839 --> 00:49:33,900
or how I would pass the torch down if I

1300
00:49:33,900 --> 00:49:35,700
were to do something different yeah yeah

1301
00:49:35,700 --> 00:49:37,859
how do you while I've like thought to

1302
00:49:37,859 --> 00:49:39,599
build models like this and then worry

1303
00:49:39,599 --> 00:49:42,480
that if I reprioritize my own work and I

1304
00:49:42,480 --> 00:49:43,740
step off the team or if I change

1305
00:49:43,740 --> 00:49:45,420
companies you know what will happen to

1306
00:49:45,420 --> 00:49:48,720
that model after I leave or after you

1307
00:49:48,720 --> 00:49:49,980
know someone else is picking it up as

1308
00:49:49,980 --> 00:49:51,839
their area of responsibility is that

1309
00:49:51,839 --> 00:49:53,099
something that you have recommendations

1310
00:49:53,099 --> 00:49:55,500
for or

1311
00:49:55,500 --> 00:49:56,160
um

1312
00:49:56,160 --> 00:49:57,859
I can't say that I've done this myself

1313
00:49:57,859 --> 00:50:00,960
but uh because I've used this again for

1314
00:50:00,960 --> 00:50:03,359
my team personally as as

1315
00:50:03,359 --> 00:50:05,480
um in a bubble almost

1316
00:50:05,480 --> 00:50:07,619
but what I would recommend would be to

1317
00:50:07,619 --> 00:50:09,900
educate people above you in the chain of

1318
00:50:09,900 --> 00:50:12,119
command as to how

1319
00:50:12,119 --> 00:50:14,339
um the value of this and then educate

1320
00:50:14,339 --> 00:50:17,220
the people that are implementing the the

1321
00:50:17,220 --> 00:50:19,260
initiatives that come from the insights

1322
00:50:19,260 --> 00:50:21,240
you get from this as to how they relate

1323
00:50:21,240 --> 00:50:24,000
back to the model so hopefully whenever

1324
00:50:24,000 --> 00:50:26,040
somebody new comes in the top and the

1325
00:50:26,040 --> 00:50:27,859
bottom will be able to

1326
00:50:27,859 --> 00:50:30,800
sort of create pressure on that person

1327
00:50:30,800 --> 00:50:33,780
to be to continue using those models but

1328
00:50:33,780 --> 00:50:35,640
I haven't done this myself

1329
00:50:35,640 --> 00:50:38,160
thank you

1330
00:50:38,160 --> 00:50:40,020
okay if you have any more questions you

1331
00:50:40,020 --> 00:50:42,359
can ask them outside let's give Arthur a

1332
00:50:42,359 --> 00:50:44,770
round of applause thank you

1333
00:50:44,770 --> 00:50:48,599
[Applause]

