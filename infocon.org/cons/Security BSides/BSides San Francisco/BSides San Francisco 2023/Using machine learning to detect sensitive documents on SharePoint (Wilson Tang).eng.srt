1
00:00:00,060 --> 00:00:02,340
hello everyone

2
00:00:02,340 --> 00:00:04,440
in this presentation by just inviting

3
00:00:04,440 --> 00:00:06,899
everyone to this simple exercise so

4
00:00:06,899 --> 00:00:08,820
first what I want you to just I want you

5
00:00:08,820 --> 00:00:11,040
to close your eyes for a second

6
00:00:11,040 --> 00:00:12,660
are you with me

7
00:00:12,660 --> 00:00:15,299
I want you to just imagine

8
00:00:15,299 --> 00:00:18,600
all of your personal information

9
00:00:18,600 --> 00:00:24,359
and secrets are all safe and secure

10
00:00:24,359 --> 00:00:26,279
now open your eyes

11
00:00:26,279 --> 00:00:27,779
could you imagine it

12
00:00:27,779 --> 00:00:30,599
no no me neither unfortunately we just

13
00:00:30,599 --> 00:00:31,679
don't really live in this world right

14
00:00:31,679 --> 00:00:33,059
now that's why I'm here to talk to you

15
00:00:33,059 --> 00:00:34,860
about machine learning for detecting

16
00:00:34,860 --> 00:00:38,480
sensitive documents on SharePoint

17
00:00:46,320 --> 00:00:49,079
all right so before I jump in just going

18
00:00:49,079 --> 00:00:50,280
to want to give a quick introduction

19
00:00:50,280 --> 00:00:52,980
myself so I'm Wilson Tang I go by heathe

20
00:00:52,980 --> 00:00:54,840
pronouns and I'm a second generation

21
00:00:54,840 --> 00:00:56,640
Asian American I just want to mention

22
00:00:56,640 --> 00:00:58,379
that because I wouldn't be here where I

23
00:00:58,379 --> 00:00:59,940
am today sitting on this podium in front

24
00:00:59,940 --> 00:01:01,620
of him or standing if it weren't for my

25
00:01:01,620 --> 00:01:02,879
parents immigrating here for a better

26
00:01:02,879 --> 00:01:05,700
life for our family and yeah and our my

27
00:01:05,700 --> 00:01:07,140
current role at Adobe I'm a machine

28
00:01:07,140 --> 00:01:09,540
learning engineer in threat hunting uh

29
00:01:09,540 --> 00:01:11,760
and before that I attained my master's

30
00:01:11,760 --> 00:01:13,560
degree in computer science from the

31
00:01:13,560 --> 00:01:16,200
University of Washington in 2022 where I

32
00:01:16,200 --> 00:01:17,400
specialize my studies in machine

33
00:01:17,400 --> 00:01:19,439
learning and now my role I'm passionate

34
00:01:19,439 --> 00:01:21,479
about finding new ml applications in

35
00:01:21,479 --> 00:01:23,520
security so I know there's a law I've

36
00:01:23,520 --> 00:01:26,159
been a lot of hype about chatgp chat GPT

37
00:01:26,159 --> 00:01:28,619
you might heard about that but I think

38
00:01:28,619 --> 00:01:30,060
now is the moment with all this hype

39
00:01:30,060 --> 00:01:31,140
building around I think we can really

40
00:01:31,140 --> 00:01:33,000
take this opportunity to really think

41
00:01:33,000 --> 00:01:34,979
about all the things in machine learning

42
00:01:34,979 --> 00:01:36,900
not just chat GPU but everything else to

43
00:01:36,900 --> 00:01:38,700
see how we can really apply it to solve

44
00:01:38,700 --> 00:01:41,280
some of our security problems

45
00:01:41,280 --> 00:01:42,960
and also I have security some of my

46
00:01:42,960 --> 00:01:44,759
hobbies include cooking uh cooking new

47
00:01:44,759 --> 00:01:46,320
foods I'm a foodie like to try new

48
00:01:46,320 --> 00:01:48,659
restaurants uh traveling like to go to

49
00:01:48,659 --> 00:01:49,799
different countries and explore

50
00:01:49,799 --> 00:01:52,380
different cultures and uh all my off

51
00:01:52,380 --> 00:01:53,939
time like to play video games the one

52
00:01:53,939 --> 00:01:56,460
I'm pretty into right now is valorant

53
00:01:56,460 --> 00:01:58,560
and yeah that's a little bit about me

54
00:01:58,560 --> 00:02:00,000
hope that gives you a good picture of

55
00:02:00,000 --> 00:02:02,960
who I am for the rest of this talk

56
00:02:04,380 --> 00:02:07,079
so how I'm gonna I'm gonna give a basic

57
00:02:07,079 --> 00:02:08,399
overview of what I'm gonna go through in

58
00:02:08,399 --> 00:02:10,080
this talk first I'm going to talk about

59
00:02:10,080 --> 00:02:12,120
document classification and how it kind

60
00:02:12,120 --> 00:02:13,920
of fits into this realm of sensitive

61
00:02:13,920 --> 00:02:15,720
documents then I'm going to talk about

62
00:02:15,720 --> 00:02:17,760
how we Define these sensitive documents

63
00:02:17,760 --> 00:02:19,920
in this project and then I'm going to

64
00:02:19,920 --> 00:02:21,780
talk about how using that definition how

65
00:02:21,780 --> 00:02:23,700
do we build machine learning models to

66
00:02:23,700 --> 00:02:26,940
detect these documents and finally how

67
00:02:26,940 --> 00:02:29,040
do we deploy these machine learning

68
00:02:29,040 --> 00:02:31,020
models as an automated pipeline for

69
00:02:31,020 --> 00:02:32,700
real-time detection

70
00:02:32,700 --> 00:02:34,560
and if and then I'm going to wrap up

71
00:02:34,560 --> 00:02:36,420
with uh noting some key challenges we

72
00:02:36,420 --> 00:02:38,220
face during this project and also

73
00:02:38,220 --> 00:02:39,840
talking about some future projects we

74
00:02:39,840 --> 00:02:41,220
want to tackle

75
00:02:41,220 --> 00:02:43,940
um based on this

76
00:02:45,180 --> 00:02:46,800
so now let's jump into the meat of this

77
00:02:46,800 --> 00:02:48,060
talk

78
00:02:48,060 --> 00:02:49,560
um first I want to Define machine

79
00:02:49,560 --> 00:02:51,060
learning and there are a lot of

80
00:02:51,060 --> 00:02:53,099
definitions of machine learning this is

81
00:02:53,099 --> 00:02:55,260
the one I align with and agree with you

82
00:02:55,260 --> 00:02:56,459
can say that machine learning is a

83
00:02:56,459 --> 00:02:58,260
category of artificial intelligence or

84
00:02:58,260 --> 00:03:00,959
AI where we use lots of data to train a

85
00:03:00,959 --> 00:03:03,420
model that can predict on some task and

86
00:03:03,420 --> 00:03:06,599
on the diagram on the top right this is

87
00:03:06,599 --> 00:03:08,580
a common depiction of what you might see

88
00:03:08,580 --> 00:03:10,200
machine learning is so on the outer

89
00:03:10,200 --> 00:03:11,640
bubble you have ai and then machine

90
00:03:11,640 --> 00:03:13,860
learning is a subset of AI and deep

91
00:03:13,860 --> 00:03:15,780
learning which uses neural networks as

92
00:03:15,780 --> 00:03:17,340
another sub is a subset of machine

93
00:03:17,340 --> 00:03:18,599
learning

94
00:03:18,599 --> 00:03:20,819
but what we really want to apply this is

95
00:03:20,819 --> 00:03:23,159
uh with sensitive documents it really

96
00:03:23,159 --> 00:03:24,540
falls under the problem of document

97
00:03:24,540 --> 00:03:26,220
classification within machine learning

98
00:03:26,220 --> 00:03:28,319
and this problem is assigning a document

99
00:03:28,319 --> 00:03:30,480
with one or more categories and the most

100
00:03:30,480 --> 00:03:31,860
common example you probably felt in your

101
00:03:31,860 --> 00:03:34,500
everyday life uh is Spam filtering like

102
00:03:34,500 --> 00:03:36,060
how does your outlook how does your

103
00:03:36,060 --> 00:03:38,400
Gmail know if an email is Spam or not

104
00:03:38,400 --> 00:03:40,019
class example of document classification

105
00:03:40,019 --> 00:03:41,519
and there are a lot of common techniques

106
00:03:41,519 --> 00:03:43,080
for document classifications such as

107
00:03:43,080 --> 00:03:45,360
like naive Bayes expectation

108
00:03:45,360 --> 00:03:48,000
maximization TF IDF I won't talk your

109
00:03:48,000 --> 00:03:50,040
ear off on all these techniques uh you

110
00:03:50,040 --> 00:03:52,019
can go research them after this talk but

111
00:03:52,019 --> 00:03:53,459
the one we're really going to focus on

112
00:03:53,459 --> 00:03:57,379
today is random forests

113
00:03:59,400 --> 00:04:02,099
so how do we Define sensitive documents

114
00:04:02,099 --> 00:04:04,500
in this project and we Define it by this

115
00:04:04,500 --> 00:04:07,140
files containing any information that

116
00:04:07,140 --> 00:04:09,120
should not be shared broadly across the

117
00:04:09,120 --> 00:04:10,920
company most commonly when access

118
00:04:10,920 --> 00:04:13,379
permissions are set incorrectly on these

119
00:04:13,379 --> 00:04:16,199
documents so the first example I think

120
00:04:16,199 --> 00:04:17,940
of Landing of a sense document is a job

121
00:04:17,940 --> 00:04:20,699
offer a job offer you would really only

122
00:04:20,699 --> 00:04:22,740
want it contained within the HR

123
00:04:22,740 --> 00:04:25,259
department and the hiring manager maybe

124
00:04:25,259 --> 00:04:27,180
even the candidate and you wouldn't want

125
00:04:27,180 --> 00:04:29,340
this document to be exposed to everyone

126
00:04:29,340 --> 00:04:31,320
in the company because you know you

127
00:04:31,320 --> 00:04:33,360
don't want your priv privacy you don't

128
00:04:33,360 --> 00:04:35,880
want everyone to know how much you're

129
00:04:35,880 --> 00:04:37,680
making or what you got offered

130
00:04:37,680 --> 00:04:40,020
and a quote that I really like that

131
00:04:40,020 --> 00:04:41,880
highlights the importance of this is

132
00:04:41,880 --> 00:04:44,040
this uh from Dan O'Day from the unit 42

133
00:04:44,040 --> 00:04:47,220
IR report uh unauthorized acquisition of

134
00:04:47,220 --> 00:04:49,259
a single spreadsheet containing a list

135
00:04:49,259 --> 00:04:52,020
of individuals pii could result in a

136
00:04:52,020 --> 00:04:54,240
large data breach even though the file

137
00:04:54,240 --> 00:04:57,120
size itself may be very small

138
00:04:57,120 --> 00:04:58,620
and I think this really highlights that

139
00:04:58,620 --> 00:05:01,259
these documents are like small in nature

140
00:05:01,259 --> 00:05:03,600
but they're at the utmost importance to

141
00:05:03,600 --> 00:05:05,100
really protect these so they don't get

142
00:05:05,100 --> 00:05:07,979
leaked and exposed to people who

143
00:05:07,979 --> 00:05:10,380
shouldn't be seeing these files

144
00:05:10,380 --> 00:05:13,380
so now that we have this problem

145
00:05:13,380 --> 00:05:17,759
um at Adobe large organizations and any

146
00:05:17,759 --> 00:05:19,860
org in general these days we're using

147
00:05:19,860 --> 00:05:21,780
we're relying on cloud documents to do

148
00:05:21,780 --> 00:05:24,539
our daily operations more and more but

149
00:05:24,539 --> 00:05:27,360
so as the scale of cloud documents

150
00:05:27,360 --> 00:05:29,639
increases then how do we effectively

151
00:05:29,639 --> 00:05:32,340
scale and scale in detecting these

152
00:05:32,340 --> 00:05:33,960
sensitive documents that really

153
00:05:33,960 --> 00:05:35,280
shouldn't be exposed to people who

154
00:05:35,280 --> 00:05:36,539
shouldn't see them

155
00:05:36,539 --> 00:05:38,820
and the answer we're coming to here and

156
00:05:38,820 --> 00:05:39,960
how well I'm going to address in this

157
00:05:39,960 --> 00:05:43,159
project is machine learning

158
00:05:43,800 --> 00:05:46,800
before we drop into the model uh it's

159
00:05:46,800 --> 00:05:48,720
also really important in ml task to

160
00:05:48,720 --> 00:05:51,539
Define our uh Define our data very

161
00:05:51,539 --> 00:05:53,220
accurately so in this project we Define

162
00:05:53,220 --> 00:05:55,740
sensitive documents in two key ways the

163
00:05:55,740 --> 00:05:58,020
first one is sensitive documents where

164
00:05:58,020 --> 00:06:00,000
whole documents are sensitive in nature

165
00:06:00,000 --> 00:06:01,860
for example I talk about job offers that

166
00:06:01,860 --> 00:06:03,240
whole document is sensitive and you

167
00:06:03,240 --> 00:06:05,759
don't want that exposed and the second

168
00:06:05,759 --> 00:06:07,919
one is documents with sensitive

169
00:06:07,919 --> 00:06:10,020
information and these are documents that

170
00:06:10,020 --> 00:06:11,820
aren't necessarily sensitive in nature

171
00:06:11,820 --> 00:06:13,320
but they can contain sensitive

172
00:06:13,320 --> 00:06:15,600
information for example API

173
00:06:15,600 --> 00:06:17,580
documentation not sensitive but they

174
00:06:17,580 --> 00:06:19,440
could actually contain an API key and

175
00:06:19,440 --> 00:06:22,280
that makes it sensitive

176
00:06:22,800 --> 00:06:26,720
oh jump let's slide too quick

177
00:06:29,160 --> 00:06:30,960
so now I'm going to jump into the first

178
00:06:30,960 --> 00:06:32,520
type of sense documents that I Define

179
00:06:32,520 --> 00:06:34,860
which is the whole sensitive documents

180
00:06:34,860 --> 00:06:36,360
and what I'm going to do here is I'm

181
00:06:36,360 --> 00:06:38,580
going to talk through how we go from the

182
00:06:38,580 --> 00:06:41,340
plain text raw document and go step by

183
00:06:41,340 --> 00:06:43,080
step through on how we get to the final

184
00:06:43,080 --> 00:06:46,680
prediction so the first step here is

185
00:06:46,680 --> 00:06:49,139
that on the left uh the left gray box

186
00:06:49,139 --> 00:06:51,539
this is the plain text document that

187
00:06:51,539 --> 00:06:53,520
just has words on it so this is document

188
00:06:53,520 --> 00:06:54,960
with some informative and non-forward

189
00:06:54,960 --> 00:06:57,360
words what we do first is an

190
00:06:57,360 --> 00:06:59,460
optimization step where we filter out

191
00:06:59,460 --> 00:07:01,620
non-informed words and you might be

192
00:07:01,620 --> 00:07:03,419
asking what's informative what's not we

193
00:07:03,419 --> 00:07:06,300
define informative words as words that

194
00:07:06,300 --> 00:07:09,360
help us categorize whether a document is

195
00:07:09,360 --> 00:07:11,699
sensitive or not for example a document

196
00:07:11,699 --> 00:07:14,039
containing the word job offer it might

197
00:07:14,039 --> 00:07:15,660
be sensitive so that could tell us that

198
00:07:15,660 --> 00:07:17,880
that it's a job offer but words like

199
00:07:17,880 --> 00:07:19,919
this is ah you can say that these words

200
00:07:19,919 --> 00:07:22,080
are present in both sensitive and

201
00:07:22,080 --> 00:07:24,000
non-sensitive documents so we want to

202
00:07:24,000 --> 00:07:25,680
filter those out filter out the noise to

203
00:07:25,680 --> 00:07:28,080
make the model easier to train so this

204
00:07:28,080 --> 00:07:30,300
we take the SEC in the middle diagram

205
00:07:30,300 --> 00:07:31,800
where we filter out the non-formal words

206
00:07:31,800 --> 00:07:34,139
and we're left with a some of just some

207
00:07:34,139 --> 00:07:35,940
subset of words

208
00:07:35,940 --> 00:07:37,979
and then finally with those words we

209
00:07:37,979 --> 00:07:40,620
create a vector with counts of words and

210
00:07:40,620 --> 00:07:43,440
this uh this framework is commonly known

211
00:07:43,440 --> 00:07:46,380
as a bag of words so if you look at the

212
00:07:46,380 --> 00:07:49,199
final Vector we output it has one two

213
00:07:49,199 --> 00:07:51,900
two one one zero zero and you can

214
00:07:51,900 --> 00:07:53,940
interpret this as the first one could be

215
00:07:53,940 --> 00:07:57,000
there's one word that's document there's

216
00:07:57,000 --> 00:07:59,639
two sums there's two informatives and

217
00:07:59,639 --> 00:08:03,120
one job one offer and the zeros can be

218
00:08:03,120 --> 00:08:05,880
informative words that aren't uh that

219
00:08:05,880 --> 00:08:09,380
aren't present in this current document

220
00:08:11,280 --> 00:08:13,080
so cool now we have a numerical

221
00:08:13,080 --> 00:08:16,319
representation of a document now how do

222
00:08:16,319 --> 00:08:18,300
we take that vector and run it through

223
00:08:18,300 --> 00:08:19,979
our model and get a classification out

224
00:08:19,979 --> 00:08:22,680
of it and from a very very base level

225
00:08:22,680 --> 00:08:24,840
what we're using in this project is

226
00:08:24,840 --> 00:08:27,000
decision trees and what a decision tree

227
00:08:27,000 --> 00:08:29,220
is it's a tree-like model that makes a

228
00:08:29,220 --> 00:08:31,800
prediction based on certain criteria and

229
00:08:31,800 --> 00:08:33,779
boundaries on that criteria

230
00:08:33,779 --> 00:08:35,820
so if you look in this decision tree

231
00:08:35,820 --> 00:08:38,219
example this is uh I'm not a medical

232
00:08:38,219 --> 00:08:41,399
professional but uh this helps highlight

233
00:08:41,399 --> 00:08:45,240
uh a risk for heart disease so the first

234
00:08:45,240 --> 00:08:47,519
question we can ask is oh what's your

235
00:08:47,519 --> 00:08:49,860
age and then if you're less than 18 then

236
00:08:49,860 --> 00:08:51,959
we can ask your weight and then less or

237
00:08:51,959 --> 00:08:55,200
60 kg can determine lower high risk and

238
00:08:55,200 --> 00:08:58,200
then 18 to 30 and then more than 30

239
00:08:58,200 --> 00:09:01,019
um at the as you can see just by looking

240
00:09:01,019 --> 00:09:03,440
at this that this decision tree might

241
00:09:03,440 --> 00:09:06,779
suffers from overfitting and overfitting

242
00:09:06,779 --> 00:09:09,060
is when you tune a model too well on

243
00:09:09,060 --> 00:09:11,040
training data so it doesn't perform well

244
00:09:11,040 --> 00:09:13,200
on real world data for example you can

245
00:09:13,200 --> 00:09:15,899
see in the middle one the 18 to 30 is

246
00:09:15,899 --> 00:09:17,700
there really no one 18 to 30 that's high

247
00:09:17,700 --> 00:09:20,880
risk of heart disease probably not so

248
00:09:20,880 --> 00:09:23,220
but why did the decision tree come up

249
00:09:23,220 --> 00:09:24,839
with this maybe because in the training

250
00:09:24,839 --> 00:09:27,300
data there was no one 18 to 30 that had

251
00:09:27,300 --> 00:09:30,060
high risk heart disease and there and so

252
00:09:30,060 --> 00:09:31,500
there are probably other factors that go

253
00:09:31,500 --> 00:09:33,420
into heart disease here but this is how

254
00:09:33,420 --> 00:09:36,060
this decision tree learned low risk and

255
00:09:36,060 --> 00:09:40,080
high risk on how the they chose to split

256
00:09:40,080 --> 00:09:41,760
on Age first and then wait and then

257
00:09:41,760 --> 00:09:45,360
smoker so you can see that the decision

258
00:09:45,360 --> 00:09:47,519
boundaries themselves like the 1830 why

259
00:09:47,519 --> 00:09:49,440
is it 1830 why is it in a different

260
00:09:49,440 --> 00:09:51,740
range or why did you choose Age first

261
00:09:51,740 --> 00:09:54,000
these can lead to overfitting because

262
00:09:54,000 --> 00:09:56,459
it's really tuned to the training data

263
00:09:56,459 --> 00:09:58,140
but not generalized well on real world

264
00:09:58,140 --> 00:10:00,240
data

265
00:10:00,240 --> 00:10:02,459
but decision trees are a very classical

266
00:10:02,459 --> 00:10:04,740
approach to ml and they can be very

267
00:10:04,740 --> 00:10:06,839
powerful given enough data it might not

268
00:10:06,839 --> 00:10:08,640
overfit but it is still very prone to

269
00:10:08,640 --> 00:10:10,140
overfitting so how do we combat this

270
00:10:10,140 --> 00:10:11,940
overfitting so we can make it even

271
00:10:11,940 --> 00:10:14,279
stronger

272
00:10:14,279 --> 00:10:17,580
one answer to this is random forest and

273
00:10:17,580 --> 00:10:19,680
just by the name you can see that we're

274
00:10:19,680 --> 00:10:21,720
going from One Tree to a forest which

275
00:10:21,720 --> 00:10:24,540
comprises of many trees So Random Forest

276
00:10:24,540 --> 00:10:26,339
classifier is where we train multiple

277
00:10:26,339 --> 00:10:28,320
decision trees and then average the

278
00:10:28,320 --> 00:10:30,779
decision across all trees and how this

279
00:10:30,779 --> 00:10:33,000
helps reduce overfitting is by we're

280
00:10:33,000 --> 00:10:35,459
making sure each decision tree randomly

281
00:10:35,459 --> 00:10:37,560
chooses different decision boundaries

282
00:10:37,560 --> 00:10:40,260
and criteria on each level of the tree

283
00:10:40,260 --> 00:10:42,360
so now we're going back to documents but

284
00:10:42,360 --> 00:10:44,339
I can talk about like the next slide

285
00:10:44,339 --> 00:10:46,680
where maybe instead of choosing age at

286
00:10:46,680 --> 00:10:48,120
the top maybe we choose it later in the

287
00:10:48,120 --> 00:10:49,800
tree but now I'm just going to jump back

288
00:10:49,800 --> 00:10:51,120
to the documents example since we're

289
00:10:51,120 --> 00:10:53,100
talking about sense documents so what we

290
00:10:53,100 --> 00:10:55,019
do here is that we have to document at

291
00:10:55,019 --> 00:10:56,640
the top and then we feed it through

292
00:10:56,640 --> 00:10:59,339
these five different trees and tree one

293
00:10:59,339 --> 00:11:02,760
might say oh 0.95 I have a 95 confidence

294
00:11:02,760 --> 00:11:04,620
that this is a sensitive document tree

295
00:11:04,620 --> 00:11:07,079
two says I'll have a 90 confidence and

296
00:11:07,079 --> 00:11:08,760
then we add all those confidence up

297
00:11:08,760 --> 00:11:10,560
together average them and we get the

298
00:11:10,560 --> 00:11:13,860
final 0.95 score and we can feel more

299
00:11:13,860 --> 00:11:15,720
confident about this number instead of

300
00:11:15,720 --> 00:11:16,980
One Tree because we'll have multiple

301
00:11:16,980 --> 00:11:19,440
trees that have learned the data in

302
00:11:19,440 --> 00:11:22,200
different ways so we can be trust this

303
00:11:22,200 --> 00:11:25,040
number just a little more

304
00:11:26,279 --> 00:11:29,459
and finally we add one more layer above

305
00:11:29,459 --> 00:11:31,500
the random Forest where we create one

306
00:11:31,500 --> 00:11:34,140
versus all classifier classifiers where

307
00:11:34,140 --> 00:11:36,300
we train multiple models where each

308
00:11:36,300 --> 00:11:38,760
model is specialized at classifying one

309
00:11:38,760 --> 00:11:41,399
type of sensitive documents so in the

310
00:11:41,399 --> 00:11:43,440
diagram here you can see random Force

311
00:11:43,440 --> 00:11:45,839
One could be really good at classifying

312
00:11:45,839 --> 00:11:48,899
API document API documentation the

313
00:11:48,899 --> 00:11:50,399
second random force can be really good

314
00:11:50,399 --> 00:11:52,560
at job offers and third one just another

315
00:11:52,560 --> 00:11:54,360
sensitive document type

316
00:11:54,360 --> 00:11:56,519
and you can see in this example that we

317
00:11:56,519 --> 00:11:59,279
got 95 confidence that this red Forest

318
00:11:59,279 --> 00:12:00,720
thinks that this document is a job offer

319
00:12:00,720 --> 00:12:03,240
while the other concepts are 30 and five

320
00:12:03,240 --> 00:12:04,740
percent so what we do here we just take

321
00:12:04,740 --> 00:12:06,360
the model the highest confidence and we

322
00:12:06,360 --> 00:12:08,700
say that the document is a job offer

323
00:12:08,700 --> 00:12:10,980
and we chose this method uh as an it's

324
00:12:10,980 --> 00:12:13,200
an ensemble method so where rather than

325
00:12:13,200 --> 00:12:15,240
having one multi-class classifier we're

326
00:12:15,240 --> 00:12:17,700
having multiple binary classifiers and

327
00:12:17,700 --> 00:12:20,339
we found this method better to easily

328
00:12:20,339 --> 00:12:22,800
easily train an accurate model and this

329
00:12:22,800 --> 00:12:24,660
here this picture is our final model

330
00:12:24,660 --> 00:12:27,000
technique for detecting the whole sense

331
00:12:27,000 --> 00:12:28,680
of documents so you got the dog you got

332
00:12:28,680 --> 00:12:31,320
the one versus all random forests and

333
00:12:31,320 --> 00:12:35,300
the ram Forest contains trees in them

334
00:12:39,660 --> 00:12:41,640
cool now I'm going to talk about the

335
00:12:41,640 --> 00:12:43,620
second type of a sense of documents we

336
00:12:43,620 --> 00:12:45,060
talk about which is documents with

337
00:12:45,060 --> 00:12:46,920
sensitive information to remind you

338
00:12:46,920 --> 00:12:48,600
that's AP for example that's API

339
00:12:48,600 --> 00:12:50,040
documentation that might actually have

340
00:12:50,040 --> 00:12:52,980
API keys and the way we approach this is

341
00:12:52,980 --> 00:12:54,959
through regex and if you don't know of

342
00:12:54,959 --> 00:12:56,880
regex's these are regular expressions

343
00:12:56,880 --> 00:12:59,160
and it's just a language to help match

344
00:12:59,160 --> 00:13:01,740
patterns in the text and what we did is

345
00:13:01,740 --> 00:13:03,240
that we did regex detection for both

346
00:13:03,240 --> 00:13:06,720
secrets and pii and also uh engineer how

347
00:13:06,720 --> 00:13:08,339
we Implement is we use a combination of

348
00:13:08,339 --> 00:13:09,839
Open Source and internally curated

349
00:13:09,839 --> 00:13:12,420
regexes an example below this is an

350
00:13:12,420 --> 00:13:15,120
example of a red X for an AWS API key

351
00:13:15,120 --> 00:13:18,720
open sourced and uh and then below is an

352
00:13:18,720 --> 00:13:21,060
example it is API key and where you can

353
00:13:21,060 --> 00:13:23,279
see the correspond is the in the red the

354
00:13:23,279 --> 00:13:27,240
Akia that that part matches to that

355
00:13:27,240 --> 00:13:30,000
segment in the regex and the 16

356
00:13:30,000 --> 00:13:31,980
characters matches the end of the

357
00:13:31,980 --> 00:13:36,200
matching on the right there

358
00:13:38,279 --> 00:13:40,139
cool now we have two modeling techniques

359
00:13:40,139 --> 00:13:41,519
to detect sensor documents machine

360
00:13:41,519 --> 00:13:44,040
learning and regex now now how do we

361
00:13:44,040 --> 00:13:46,380
deploy these machine learning models and

362
00:13:46,380 --> 00:13:48,360
how do we create an automated pipeline

363
00:13:48,360 --> 00:13:50,279
for real-time detection on them

364
00:13:50,279 --> 00:13:53,399
so first uh we start with the SharePoint

365
00:13:53,399 --> 00:13:55,680
API where we query a number of files for

366
00:13:55,680 --> 00:13:57,480
a certain time period and then with

367
00:13:57,480 --> 00:13:59,459
those files we run them through our

368
00:13:59,459 --> 00:14:03,060
model and then if our model detects if a

369
00:14:03,060 --> 00:14:05,820
if a document is sensitive then we're

370
00:14:05,820 --> 00:14:07,440
going to directly alert our users

371
00:14:07,440 --> 00:14:09,779
through our user Notifier app and the

372
00:14:09,779 --> 00:14:12,360
user Notifier app is going to prompt the

373
00:14:12,360 --> 00:14:14,700
user to respond when this document is

374
00:14:14,700 --> 00:14:16,139
actually sensitive I'll go lock it down

375
00:14:16,139 --> 00:14:18,540
or two this document is not sensitive go

376
00:14:18,540 --> 00:14:20,940
improve your model and what this does

377
00:14:20,940 --> 00:14:22,620
for us is that we store the responses

378
00:14:22,620 --> 00:14:24,240
back to our Sim and then we can give it

379
00:14:24,240 --> 00:14:26,160
back to our model to establish a

380
00:14:26,160 --> 00:14:27,839
reliable feedback loop so we can

381
00:14:27,839 --> 00:14:29,339
continuously improve our model

382
00:14:29,339 --> 00:14:31,019
performance

383
00:14:31,019 --> 00:14:33,420
and why I highlight this is because we

384
00:14:33,420 --> 00:14:34,800
can have really complicated algorithms

385
00:14:34,800 --> 00:14:36,000
and stuff with our machine learning

386
00:14:36,000 --> 00:14:37,800
models but it's also really important to

387
00:14:37,800 --> 00:14:39,300
have Innovative automation so we can

388
00:14:39,300 --> 00:14:41,820
make the use out of full use of our ml

389
00:14:41,820 --> 00:14:43,019
models

390
00:14:43,019 --> 00:14:46,339
yeah that's our whole pipeline

391
00:14:47,639 --> 00:14:49,139
foreign

392
00:14:49,139 --> 00:14:50,940
cool now I'm going to talk about some

393
00:14:50,940 --> 00:14:52,800
key challenges we faced while doing this

394
00:14:52,800 --> 00:14:54,240
project and I believe these challenges

395
00:14:54,240 --> 00:14:55,860
also apply to any machine learning

396
00:14:55,860 --> 00:14:57,600
project you're trying to do in security

397
00:14:57,600 --> 00:15:00,180
the first one is minimizing false

398
00:15:00,180 --> 00:15:03,060
positives we definitely why we care

399
00:15:03,060 --> 00:15:04,380
about this is because we definitely

400
00:15:04,380 --> 00:15:06,779
don't want to flood our analysts with a

401
00:15:06,779 --> 00:15:08,579
lot of alerts a lot of false positives

402
00:15:08,579 --> 00:15:11,399
and they'll experience alert fatigue so

403
00:15:11,399 --> 00:15:14,339
the way we answer this is by using a

404
00:15:14,339 --> 00:15:16,620
random Forest random forests are less

405
00:15:16,620 --> 00:15:19,260
prone to false pauses because of the

406
00:15:19,260 --> 00:15:21,120
nature of like you need these decisions

407
00:15:21,120 --> 00:15:22,800
and these decision batteries to fit on

408
00:15:22,800 --> 00:15:25,380
the data what you what you might get

409
00:15:25,380 --> 00:15:26,699
from random Forest is you might get

410
00:15:26,699 --> 00:15:28,620
false negatives because as we talk about

411
00:15:28,620 --> 00:15:31,079
it might not generalize well as well on

412
00:15:31,079 --> 00:15:34,560
the data but for this particular use

413
00:15:34,560 --> 00:15:37,079
case with sensitive documents within one

414
00:15:37,079 --> 00:15:39,000
company you can imagine a job offer

415
00:15:39,000 --> 00:15:41,579
doesn't have much variation across the

416
00:15:41,579 --> 00:15:44,639
entire company so in this particular

417
00:15:44,639 --> 00:15:46,320
project in this use case our random

418
00:15:46,320 --> 00:15:48,180
Forest didn't have a high uh false

419
00:15:48,180 --> 00:15:49,519
negative rate either

420
00:15:49,519 --> 00:15:52,139
so false followers false negatives all

421
00:15:52,139 --> 00:15:53,760
good

422
00:15:53,760 --> 00:15:56,040
the second key challenge we face is a

423
00:15:56,040 --> 00:15:57,660
reliable feedback loop

424
00:15:57,660 --> 00:16:00,839
and with the reliable feedback loop in

425
00:16:00,839 --> 00:16:02,940
security it might be hard to get

426
00:16:02,940 --> 00:16:05,300
malicious and benign

427
00:16:05,300 --> 00:16:08,880
samples and also get real user responses

428
00:16:08,880 --> 00:16:10,380
into your training data and how we

429
00:16:10,380 --> 00:16:11,699
answered this was using the user

430
00:16:11,699 --> 00:16:13,440
Notifier app because this will allow us

431
00:16:13,440 --> 00:16:16,380
to automate recording responses from our

432
00:16:16,380 --> 00:16:18,740
users

433
00:16:20,040 --> 00:16:21,779
and finally some future projects for you

434
00:16:21,779 --> 00:16:23,220
hope to extend this is we want to

435
00:16:23,220 --> 00:16:25,079
implement more sophisticated ml models

436
00:16:25,079 --> 00:16:27,480
such as we have random forests now we

437
00:16:27,480 --> 00:16:29,519
also want to try out a gradient boosted

438
00:16:29,519 --> 00:16:33,000
trees and also try out Bert models that

439
00:16:33,000 --> 00:16:35,339
are open sourced and maybe we can try

440
00:16:35,339 --> 00:16:38,220
chat EBT on it

441
00:16:38,220 --> 00:16:40,259
also we want to expand the document

442
00:16:40,259 --> 00:16:42,000
category so we have a set of sense of

443
00:16:42,000 --> 00:16:43,259
documents that we're trying we can

444
00:16:43,259 --> 00:16:46,079
definitely make that more Broad and we

445
00:16:46,079 --> 00:16:48,000
want to scan other Cloud sourages such

446
00:16:48,000 --> 00:16:50,820
as AWS S3 buckets Azure storage blobs

447
00:16:50,820 --> 00:16:54,079
and also gcp as well

448
00:16:55,560 --> 00:16:58,019
cool if you miss anything if you miss

449
00:16:58,019 --> 00:16:59,339
anything in this presentation or you

450
00:16:59,339 --> 00:17:00,540
just want to read it instead of

451
00:17:00,540 --> 00:17:02,639
listening to me talk we also published

452
00:17:02,639 --> 00:17:04,799
this as a blog post on our Adobe Tech

453
00:17:04,799 --> 00:17:06,839
blog it's called using machine learning

454
00:17:06,839 --> 00:17:09,059
to help detect sensitive information go

455
00:17:09,059 --> 00:17:10,559
check it out give a like give some

456
00:17:10,559 --> 00:17:13,879
engagement it'll be really cool

457
00:17:14,640 --> 00:17:17,040
and finally I want to thank our team

458
00:17:17,040 --> 00:17:19,079
members that uh helped me on this

459
00:17:19,079 --> 00:17:20,160
project it wouldn't have been possible

460
00:17:20,160 --> 00:17:22,140
without them that would be to Vero

461
00:17:22,140 --> 00:17:24,900
borosh Andre Stan Kumar vikramji and

462
00:17:24,900 --> 00:17:27,540
Joseph Davidson and finally I also want

463
00:17:27,540 --> 00:17:28,860
to thank besides for hosting me today

464
00:17:28,860 --> 00:17:30,540
it's been really fun getting to know

465
00:17:30,540 --> 00:17:33,059
everyone here and as I know it's my

466
00:17:33,059 --> 00:17:34,380
first live conference so it's been

467
00:17:34,380 --> 00:17:36,000
really cool to like see everyone and be

468
00:17:36,000 --> 00:17:38,100
everyone's stories and everything

469
00:17:38,100 --> 00:17:40,200
and finally there's I have my LinkedIn

470
00:17:40,200 --> 00:17:41,580
QR code up here if you want to add me

471
00:17:41,580 --> 00:17:43,440
connect I definitely would love talking

472
00:17:43,440 --> 00:17:45,000
about yeah go ahead

473
00:17:45,000 --> 00:17:46,440
um I'd love to talk about machine

474
00:17:46,440 --> 00:17:47,580
learning and machine learning and

475
00:17:47,580 --> 00:17:49,740
security what you think about it and if

476
00:17:49,740 --> 00:17:50,820
you have any ideas I'd love to

477
00:17:50,820 --> 00:17:52,080
brainstorm with you

478
00:17:52,080 --> 00:17:54,780
and yeah with that I'll open it up to q

479
00:17:54,780 --> 00:17:56,460
a

480
00:17:56,460 --> 00:17:57,470
thank you Wilson

481
00:17:57,470 --> 00:18:00,630
[Applause]

482
00:18:00,960 --> 00:18:02,220
all right if you have a question please

483
00:18:02,220 --> 00:18:06,799
raise your hand and I will come to you

484
00:18:07,500 --> 00:18:11,360
see one in the back up there awesome

485
00:18:22,860 --> 00:18:25,320
hello uh do you think this technology

486
00:18:25,320 --> 00:18:27,360
could ever potentially be used as a

487
00:18:27,360 --> 00:18:29,460
weapon to seek out sensitive documents

488
00:18:29,460 --> 00:18:31,440
over SharePoint and then

489
00:18:31,440 --> 00:18:34,740
I'd report them back

490
00:18:34,740 --> 00:18:36,240
um I think

491
00:18:36,240 --> 00:18:37,980
when you put it like that with any tool

492
00:18:37,980 --> 00:18:39,480
of detection I think it could be used

493
00:18:39,480 --> 00:18:43,440
like maliciously like that but um I hope

494
00:18:43,440 --> 00:18:45,660
that like so for us for our SharePoint

495
00:18:45,660 --> 00:18:47,039
it should really only be internally

496
00:18:47,039 --> 00:18:49,020
accessible like there should be no way

497
00:18:49,020 --> 00:18:51,000
that like an outsider should be able to

498
00:18:51,000 --> 00:18:53,580
scan our own SharePoint environment so

499
00:18:53,580 --> 00:18:55,260
for us using this as an internal tools

500
00:18:55,260 --> 00:18:57,600
to scan our own SharePoint environment

501
00:18:57,600 --> 00:19:01,980
um is the use case there so yeah

502
00:19:02,220 --> 00:19:05,600
any other questions

503
00:19:06,660 --> 00:19:08,400
next question is from The Middle on your

504
00:19:08,400 --> 00:19:10,640
left

505
00:19:11,160 --> 00:19:13,799
oh and I say no way there but like

506
00:19:13,799 --> 00:19:15,720
oh there could always be a way you know

507
00:19:15,720 --> 00:19:17,700
attackers but just ain't playing out

508
00:19:17,700 --> 00:19:18,960
there yeah

509
00:19:18,960 --> 00:19:22,740
so um how do you continuously like uh

510
00:19:22,740 --> 00:19:25,260
refresh your data uh the data that you

511
00:19:25,260 --> 00:19:27,600
train from like you know in theory if

512
00:19:27,600 --> 00:19:29,580
the company gets better there's less

513
00:19:29,580 --> 00:19:32,520
sensitive information in the SharePoint

514
00:19:32,520 --> 00:19:34,140
um but like how do you know like if your

515
00:19:34,140 --> 00:19:35,340
model isn't detecting something it's

516
00:19:35,340 --> 00:19:36,900
because like there's less sensitive data

517
00:19:36,900 --> 00:19:38,820
versus like you know the model isn't

518
00:19:38,820 --> 00:19:40,740
detecting something properly

519
00:19:40,740 --> 00:19:42,840
yeah that's a good question

520
00:19:42,840 --> 00:19:45,900
um I would think that even if the data

521
00:19:45,900 --> 00:19:48,600
is old if we see that like same job

522
00:19:48,600 --> 00:19:50,640
offer pop up on the SharePoint it

523
00:19:50,640 --> 00:19:52,559
wouldn't perform any less worse because

524
00:19:52,559 --> 00:19:55,679
it's able to detect it already

525
00:19:55,679 --> 00:19:57,299
um but I do get we might if we're

526
00:19:57,299 --> 00:19:59,580
constantly improving then like uh there

527
00:19:59,580 --> 00:20:01,380
would be less sensitive documents on the

528
00:20:01,380 --> 00:20:02,700
SharePoint

529
00:20:02,700 --> 00:20:04,260
um but I guess they also bring up

530
00:20:04,260 --> 00:20:05,640
another note maybe the document has

531
00:20:05,640 --> 00:20:07,500
changed and that's just a common machine

532
00:20:07,500 --> 00:20:09,299
learning problem of just refreshing your

533
00:20:09,299 --> 00:20:10,799
data once in a while and making sure you

534
00:20:10,799 --> 00:20:13,740
have enough good samples to train on

535
00:20:13,740 --> 00:20:16,559
um yeah but if it's a document that like

536
00:20:16,559 --> 00:20:19,200
we've already know that is sensitive

537
00:20:19,200 --> 00:20:20,700
then we should be able to always detect

538
00:20:20,700 --> 00:20:24,320
that no matter how much time has passed

539
00:20:25,860 --> 00:20:28,580
other questions

540
00:20:29,400 --> 00:20:31,020
and I'm coming

541
00:20:31,020 --> 00:20:33,559
yeah

542
00:20:35,640 --> 00:20:37,380
so in regards to

543
00:20:37,380 --> 00:20:39,780
um random Force you didn't discuss

544
00:20:39,780 --> 00:20:41,340
ensemble

545
00:20:41,340 --> 00:20:44,820
the ability to combine the multiple

546
00:20:44,820 --> 00:20:47,100
configurations so we do you have

547
00:20:47,100 --> 00:20:48,419
anything you can say about that because

548
00:20:48,419 --> 00:20:49,980
I think that's a very important aspect

549
00:20:49,980 --> 00:20:52,400
of it

550
00:20:53,039 --> 00:20:54,600
uh

551
00:20:54,600 --> 00:20:56,760
referring to this

552
00:20:56,760 --> 00:20:58,380
sorry just declaration we're offering

553
00:20:58,380 --> 00:21:01,520
that this part of the ensemble

554
00:21:01,890 --> 00:21:04,140
[Music]

555
00:21:04,140 --> 00:21:05,230
just

556
00:21:05,230 --> 00:21:06,539
[Music]

557
00:21:06,539 --> 00:21:08,400
just being able to

558
00:21:08,400 --> 00:21:12,840
combine multiple models

559
00:21:13,200 --> 00:21:15,480
um you know to

560
00:21:15,480 --> 00:21:17,820
um it's like a collection of models used

561
00:21:17,820 --> 00:21:19,860
rather than you have like you have your

562
00:21:19,860 --> 00:21:22,679
single models and I see the trees but

563
00:21:22,679 --> 00:21:25,200
like specifically like if you have that

564
00:21:25,200 --> 00:21:27,299
model and then there's another one that

565
00:21:27,299 --> 00:21:29,340
can relate combining the two models

566
00:21:29,340 --> 00:21:32,520
together ah I see

567
00:21:32,520 --> 00:21:35,280
um that particular use case we haven't

568
00:21:35,280 --> 00:21:38,159
like necessarily explored like combining

569
00:21:38,159 --> 00:21:39,600
like you're talking about like combining

570
00:21:39,600 --> 00:21:41,820
one and two together in their results if

571
00:21:41,820 --> 00:21:44,480
I got that right

572
00:21:45,480 --> 00:21:47,900
okay

573
00:21:50,700 --> 00:21:52,760
um

574
00:21:55,200 --> 00:21:58,260
you're using it as a one sorry again

575
00:21:58,260 --> 00:22:00,480
okay so you have Ensemble method and

576
00:22:00,480 --> 00:22:01,799
you're using it you know if you have one

577
00:22:01,799 --> 00:22:03,659
multi-class classifier but you're doing

578
00:22:03,659 --> 00:22:05,940
it within one model yeah true so I'm

579
00:22:05,940 --> 00:22:07,679
talking about you know Ensemble models

580
00:22:07,679 --> 00:22:09,900
like putting models together yeah and so

581
00:22:09,900 --> 00:22:12,480
it has that capability and I just wasn't

582
00:22:12,480 --> 00:22:14,100
sure if you were going to discuss that

583
00:22:14,100 --> 00:22:17,520
yeah yeah so that is basically with the

584
00:22:17,520 --> 00:22:19,320
Ensemble method then uh you would want

585
00:22:19,320 --> 00:22:21,900
to combine results of a multiple models

586
00:22:21,900 --> 00:22:23,460
of hearing that correctly I guess the

587
00:22:23,460 --> 00:22:25,200
way we approached it is probably more

588
00:22:25,200 --> 00:22:27,360
simple than that so instead of combining

589
00:22:27,360 --> 00:22:29,400
these results together we do a Max

590
00:22:29,400 --> 00:22:32,039
operation on them so that's how I would

591
00:22:32,039 --> 00:22:33,480
interpret that

592
00:22:33,480 --> 00:22:36,600
last question up here on your left

593
00:22:36,600 --> 00:22:39,659
I'll make a quick uh how well does the

594
00:22:39,659 --> 00:22:42,780
bag of words method generalize to source

595
00:22:42,780 --> 00:22:46,860
code documents to source code documents

596
00:22:46,860 --> 00:22:48,539
um does that make sense yeah yeah yeah

597
00:22:48,539 --> 00:22:51,900
uh source code documents haven't

598
00:22:51,900 --> 00:22:53,820
necessarily explored to that because

599
00:22:53,820 --> 00:22:55,980
that's not necessarily what we see on

600
00:22:55,980 --> 00:22:57,539
SharePoint

601
00:22:57,539 --> 00:22:58,620
um

602
00:22:58,620 --> 00:23:00,299
but that would be interesting to see for

603
00:23:00,299 --> 00:23:02,039
me how that bag of words would perform

604
00:23:02,039 --> 00:23:03,960
you know

605
00:23:03,960 --> 00:23:06,000
yeah that was totally open to explore

606
00:23:06,000 --> 00:23:08,220
that yeah awesome let's give one more

607
00:23:08,220 --> 00:23:09,850
round of applause for our presenter

608
00:23:09,850 --> 00:23:14,240
[Applause]

