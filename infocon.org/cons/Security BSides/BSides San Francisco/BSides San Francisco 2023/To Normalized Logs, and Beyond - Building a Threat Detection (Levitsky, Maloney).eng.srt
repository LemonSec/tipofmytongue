1
00:00:00,000 --> 00:00:02,520
hello everyone welcome to normalized

2
00:00:02,520 --> 00:00:05,040
logs and Beyond with our speakers Brian

3
00:00:05,040 --> 00:00:07,740
Maloney and Dave Levitsky or David

4
00:00:07,740 --> 00:00:10,460
Levitsky

5
00:00:13,799 --> 00:00:16,139
hey everyone good afternoon

6
00:00:16,139 --> 00:00:18,300
um thanks for sticking with us for one

7
00:00:18,300 --> 00:00:20,220
of the last talks of the day my name is

8
00:00:20,220 --> 00:00:22,500
Brian Maloney and this is David Levitsky

9
00:00:22,500 --> 00:00:24,240
and we work together to build a cloud

10
00:00:24,240 --> 00:00:26,100
native threat detection Pipeline and

11
00:00:26,100 --> 00:00:28,800
platform for benchling now what is

12
00:00:28,800 --> 00:00:31,380
benchling you might ask benchlings the

13
00:00:31,380 --> 00:00:33,480
software service platform that powers

14
00:00:33,480 --> 00:00:35,219
some of the world's most cutting-edge

15
00:00:35,219 --> 00:00:37,140
biotechnology research

16
00:00:37,140 --> 00:00:39,239
with customers ranging from two

17
00:00:39,239 --> 00:00:41,820
scientists and a molecule startups to

18
00:00:41,820 --> 00:00:43,260
some of the largest biotech companies

19
00:00:43,260 --> 00:00:44,360
around

20
00:00:44,360 --> 00:00:46,860
these will say benchling protects some

21
00:00:46,860 --> 00:00:48,780
extremely sensitive data on behalf of

22
00:00:48,780 --> 00:00:50,879
their customers and needed a scalable

23
00:00:50,879 --> 00:00:52,559
and reliable solution for detecting

24
00:00:52,559 --> 00:00:54,539
threats to that data while retaining

25
00:00:54,539 --> 00:00:56,640
maximum flexibility for an unknown

26
00:00:56,640 --> 00:00:59,100
future our work resulted in a platform

27
00:00:59,100 --> 00:01:01,199
built on replaceable components using

28
00:01:01,199 --> 00:01:03,120
combination of AWS Technologies and

29
00:01:03,120 --> 00:01:05,700
off-the-shelf Hardware software which is

30
00:01:05,700 --> 00:01:08,040
the subject of this talk and just one

31
00:01:08,040 --> 00:01:10,619
last note about mentioning we are hiring

32
00:01:10,619 --> 00:01:13,080
so if you're interested maybe we can

33
00:01:13,080 --> 00:01:15,619
come talk to me

34
00:01:15,720 --> 00:01:17,700
um but first let's just briefly go over

35
00:01:17,700 --> 00:01:20,520
the steps of threat detection for those

36
00:01:20,520 --> 00:01:22,799
of you who might be primarily offensive

37
00:01:22,799 --> 00:01:24,600
practitioners or if you're just new to

38
00:01:24,600 --> 00:01:26,340
the field this you can think of this

39
00:01:26,340 --> 00:01:29,759
kind of as just a brief blue team 101.

40
00:01:29,759 --> 00:01:33,240
so this is kind of the the overview of

41
00:01:33,240 --> 00:01:35,939
the steps um from collection to

42
00:01:35,939 --> 00:01:38,340
normalization enrichment detections

43
00:01:38,340 --> 00:01:40,439
running on that security alerts going

44
00:01:40,439 --> 00:01:42,299
into investigations and then response

45
00:01:42,299 --> 00:01:44,400
actions

46
00:01:44,400 --> 00:01:47,100
so what about collection right we start

47
00:01:47,100 --> 00:01:48,720
on the left with collection we we need

48
00:01:48,720 --> 00:01:50,939
the data to do our work

49
00:01:50,939 --> 00:01:53,040
um collection I think of it is like the

50
00:01:53,040 --> 00:01:54,840
hard part

51
00:01:54,840 --> 00:01:56,820
um any modern organization of any

52
00:01:56,820 --> 00:01:58,799
significant size is going to have a

53
00:01:58,799 --> 00:02:01,200
ridiculous number of security data

54
00:02:01,200 --> 00:02:03,840
sources from your Enterprise identity

55
00:02:03,840 --> 00:02:05,939
and your HR applications things that you

56
00:02:05,939 --> 00:02:07,560
use to run your business to your

57
00:02:07,560 --> 00:02:09,598
productivity software that you use to to

58
00:02:09,598 --> 00:02:11,520
drive to communicate with your customers

59
00:02:11,520 --> 00:02:14,760
to the product itself we're a software

60
00:02:14,760 --> 00:02:16,860
as a service company so obviously our

61
00:02:16,860 --> 00:02:20,040
software generates a ton of data

62
00:02:20,040 --> 00:02:21,360
um

63
00:02:21,360 --> 00:02:23,160
and these sources they're they're not

64
00:02:23,160 --> 00:02:25,739
uniform you know they're they're going

65
00:02:25,739 --> 00:02:28,879
to come in in many formats raw text logs

66
00:02:28,879 --> 00:02:31,560
delimited formats CSV and space to

67
00:02:31,560 --> 00:02:34,680
limited Json could be some binary

68
00:02:34,680 --> 00:02:36,540
formats even

69
00:02:36,540 --> 00:02:38,520
um and those come in Via different

70
00:02:38,520 --> 00:02:41,760
protocols uh you know some of you may

71
00:02:41,760 --> 00:02:43,800
not have to deal with syslog today but a

72
00:02:43,800 --> 00:02:45,720
lot of people still do

73
00:02:45,720 --> 00:02:47,459
um buckets are a common way of

74
00:02:47,459 --> 00:02:49,920
delivering logs as well

75
00:02:49,920 --> 00:02:51,599
um and you need a way to get that data

76
00:02:51,599 --> 00:02:55,680
into your platform easily HTTP apis via

77
00:02:55,680 --> 00:02:57,599
either pull or push are another common

78
00:02:57,599 --> 00:02:59,879
method and then Cloud event buses are

79
00:02:59,879 --> 00:03:03,440
kind of an up and comer in this space

80
00:03:03,440 --> 00:03:06,180
so then once you've got that data into

81
00:03:06,180 --> 00:03:09,180
your system you want to normalize it

82
00:03:09,180 --> 00:03:11,640
um why do you normalize because you need

83
00:03:11,640 --> 00:03:13,800
to be able to do efficient queries on

84
00:03:13,800 --> 00:03:14,900
that data

85
00:03:14,900 --> 00:03:17,519
but in order to do that you're going to

86
00:03:17,519 --> 00:03:19,080
have to pick some kind of a schema

87
00:03:19,080 --> 00:03:20,879
you're going to want to make sure that

88
00:03:20,879 --> 00:03:22,680
you have the essential pieces of data

89
00:03:22,680 --> 00:03:23,959
that you need

90
00:03:23,959 --> 00:03:26,819
and you're going to need to

91
00:03:26,819 --> 00:03:29,220
um track those things like time stamps

92
00:03:29,220 --> 00:03:31,680
and log sources and then of course

93
00:03:31,680 --> 00:03:33,840
you're going to have to think about what

94
00:03:33,840 --> 00:03:37,200
goes into your normalization as well

95
00:03:37,200 --> 00:03:39,420
um so for example if you do have those

96
00:03:39,420 --> 00:03:42,480
raw text logs that may involve regex's

97
00:03:42,480 --> 00:03:44,640
regular Expressions which there's some

98
00:03:44,640 --> 00:03:46,080
operational risks there which we'll

99
00:03:46,080 --> 00:03:49,200
cover maybe in depth a little later CSV

100
00:03:49,200 --> 00:03:51,900
logs also if it's a perfectly

101
00:03:51,900 --> 00:03:53,640
standardized CSV you're probably not

102
00:03:53,640 --> 00:03:55,019
going to run in any trouble but

103
00:03:55,019 --> 00:03:57,060
sometimes you have quotes in the wrong

104
00:03:57,060 --> 00:04:00,500
places that then break your parsing so

105
00:04:00,500 --> 00:04:03,299
normalizations engines just need to be

106
00:04:03,299 --> 00:04:05,760
limited so that they don't run over and

107
00:04:05,760 --> 00:04:07,560
they need to be able to handle erroneous

108
00:04:07,560 --> 00:04:08,959
data

109
00:04:08,959 --> 00:04:11,159
once you have that normalized data

110
00:04:11,159 --> 00:04:12,560
you're going to want to enrich that

111
00:04:12,560 --> 00:04:15,659
enriching it provides the insights that

112
00:04:15,659 --> 00:04:18,839
you need to detect threats to your uh

113
00:04:18,839 --> 00:04:21,560
your platforms

114
00:04:21,560 --> 00:04:24,780
this enrichment could data could come

115
00:04:24,780 --> 00:04:26,580
from internal sources like your

116
00:04:26,580 --> 00:04:29,160
Enterprise Data Systems

117
00:04:29,160 --> 00:04:31,440
um you know HR systems things like that

118
00:04:31,440 --> 00:04:33,240
who's doing what how do we attribute

119
00:04:33,240 --> 00:04:35,639
things it could be metadata about your

120
00:04:35,639 --> 00:04:37,460
production resources

121
00:04:37,460 --> 00:04:41,580
and it could also be indicators of

122
00:04:41,580 --> 00:04:44,220
compromise for your detective controls

123
00:04:44,220 --> 00:04:46,320
so you may have specific indicators of

124
00:04:46,320 --> 00:04:47,940
compromise that are specific to your

125
00:04:47,940 --> 00:04:49,699
platform

126
00:04:49,699 --> 00:04:52,380
this may also come from external sources

127
00:04:52,380 --> 00:04:54,960
so those indicators of compromise could

128
00:04:54,960 --> 00:04:57,600
come from public databases they're like

129
00:04:57,600 --> 00:04:59,580
things on GitHub that you can grab or

130
00:04:59,580 --> 00:05:01,259
there are vendors that can also provide

131
00:05:01,259 --> 00:05:03,540
these kinds of uh threat data that you

132
00:05:03,540 --> 00:05:07,020
can use uh with your normalized logs

133
00:05:07,020 --> 00:05:09,240
finally once you have been enriched data

134
00:05:09,240 --> 00:05:11,040
enrich normalized data you can run

135
00:05:11,040 --> 00:05:13,440
detections on those to find the actual

136
00:05:13,440 --> 00:05:16,740
Bad actors in your systems

137
00:05:16,740 --> 00:05:19,320
this may be predefined indicators of

138
00:05:19,320 --> 00:05:22,080
compromise it could be anomaly detection

139
00:05:22,080 --> 00:05:24,500
using statistical methods or

140
00:05:24,500 --> 00:05:28,080
unsupervised ml models or maybe you know

141
00:05:28,080 --> 00:05:29,699
everybody's favorite topic large

142
00:05:29,699 --> 00:05:31,440
language models might be part of this in

143
00:05:31,440 --> 00:05:33,680
the future

144
00:05:33,780 --> 00:05:36,720
but it's important uh to do to be able

145
00:05:36,720 --> 00:05:38,340
to run iterative development of those

146
00:05:38,340 --> 00:05:40,199
detections because they do change over

147
00:05:40,199 --> 00:05:42,900
time and you want to be able to tune the

148
00:05:42,900 --> 00:05:45,180
performance as things change in your

149
00:05:45,180 --> 00:05:46,199
environment

150
00:05:46,199 --> 00:05:48,060
and you also want to be able to reduce

151
00:05:48,060 --> 00:05:50,880
false positives because there's a human

152
00:05:50,880 --> 00:05:53,699
cost to this in the next step

153
00:05:53,699 --> 00:05:55,860
finally those alerts go to the response

154
00:05:55,860 --> 00:05:58,320
team they need the details about what

155
00:05:58,320 --> 00:06:00,180
happened and how they can investigate it

156
00:06:00,180 --> 00:06:02,160
so you do need to have some kind of a

157
00:06:02,160 --> 00:06:05,100
system for alert delivery

158
00:06:05,100 --> 00:06:07,380
moving on to the last step and then

159
00:06:07,380 --> 00:06:10,080
we'll get into the architecture you want

160
00:06:10,080 --> 00:06:12,020
to be able to

161
00:06:12,020 --> 00:06:15,180
investigate what happened so you're

162
00:06:15,180 --> 00:06:16,740
going to need the same data for that

163
00:06:16,740 --> 00:06:17,820
right

164
00:06:17,820 --> 00:06:21,000
um One One log Source might indicate

165
00:06:21,000 --> 00:06:22,979
something's wrong and then you need

166
00:06:22,979 --> 00:06:26,479
additional context from other sources

167
00:06:26,479 --> 00:06:29,220
from there that step you can move to

168
00:06:29,220 --> 00:06:30,780
confirming whether it's a malicious

169
00:06:30,780 --> 00:06:33,840
actor or benign most the time it's going

170
00:06:33,840 --> 00:06:37,020
to be benign but you still have to check

171
00:06:37,020 --> 00:06:38,639
every time you get one of these events

172
00:06:38,639 --> 00:06:39,539
fire

173
00:06:39,539 --> 00:06:41,940
in the case that it is malicious again

174
00:06:41,940 --> 00:06:43,680
just wrapping up kind of our blue team

175
00:06:43,680 --> 00:06:46,080
101 you're going to want to contain that

176
00:06:46,080 --> 00:06:48,660
intrusion close the vulnerabilities that

177
00:06:48,660 --> 00:06:50,100
may have allowed somebody to get in in

178
00:06:50,100 --> 00:06:52,199
the first place

179
00:06:52,199 --> 00:06:55,199
um prepare and execute your recovery

180
00:06:55,199 --> 00:06:57,960
plan and then as always whenever there's

181
00:06:57,960 --> 00:06:59,520
some kind of an incident be it an

182
00:06:59,520 --> 00:07:00,720
operational incident or security

183
00:07:00,720 --> 00:07:01,979
incident you're going to want to conduct

184
00:07:01,979 --> 00:07:03,900
a Lessons Learned section session

185
00:07:03,900 --> 00:07:06,060
afterwards

186
00:07:06,060 --> 00:07:08,940
so this is what we want to do now how

187
00:07:08,940 --> 00:07:11,400
did we actually do it so we're going to

188
00:07:11,400 --> 00:07:14,780
jump in now to the architecture

189
00:07:15,240 --> 00:07:17,819
so I'm going to go over kind of two

190
00:07:17,819 --> 00:07:20,580
parts of the architecture the inputs and

191
00:07:20,580 --> 00:07:22,319
then the detection side and then I'll

192
00:07:22,319 --> 00:07:24,599
show you just a slide briefly where we

193
00:07:24,599 --> 00:07:26,340
put the two together

194
00:07:26,340 --> 00:07:28,740
so you can see on the left hand side of

195
00:07:28,740 --> 00:07:29,720
the screen

196
00:07:29,720 --> 00:07:32,580
we support multiple different sources

197
00:07:32,580 --> 00:07:34,220
types

198
00:07:34,220 --> 00:07:38,880
those could be HTTP API syslog buckets

199
00:07:38,880 --> 00:07:39,620
Etc

200
00:07:39,620 --> 00:07:42,780
and those go into a series of lambdas so

201
00:07:42,780 --> 00:07:44,940
we are an AWS shop so this will be an

202
00:07:44,940 --> 00:07:47,699
AWS specific architecture but this could

203
00:07:47,699 --> 00:07:51,500
be applicable to many other clouds

204
00:07:51,599 --> 00:07:54,259
so those we use lambdas for inputs

205
00:07:54,259 --> 00:07:56,880
except in the case of syslog which

206
00:07:56,880 --> 00:07:59,340
obviously it's not really easy to tie

207
00:07:59,340 --> 00:08:01,500
syslog to a Lambda

208
00:08:01,500 --> 00:08:02,220
um

209
00:08:02,220 --> 00:08:04,319
so for that you'd want to have a

210
00:08:04,319 --> 00:08:05,819
traditional more traditional kind of

211
00:08:05,819 --> 00:08:09,080
compute platform with a load balancer

212
00:08:09,080 --> 00:08:13,080
from those Lambda inputs we then add the

213
00:08:13,080 --> 00:08:14,819
origin metadata and run it through

214
00:08:14,819 --> 00:08:16,800
Kinesis fire hose which allows us to

215
00:08:16,800 --> 00:08:19,620
aggregate huge amounts of data into a

216
00:08:19,620 --> 00:08:22,080
semi-structured data bucket and that's

217
00:08:22,080 --> 00:08:24,599
kind of our golden storage bucket where

218
00:08:24,599 --> 00:08:27,360
we do all of our work from

219
00:08:27,360 --> 00:08:29,720
once we get the data in

220
00:08:29,720 --> 00:08:32,760
you can see that the same bucket is kind

221
00:08:32,760 --> 00:08:36,299
of down on the bottom left of this

222
00:08:36,299 --> 00:08:37,559
diagram

223
00:08:37,559 --> 00:08:40,979
we can use both aws's analytics tools

224
00:08:40,979 --> 00:08:43,020
and third-party analytics tools with

225
00:08:43,020 --> 00:08:45,420
which we connect to the same bucket and

226
00:08:45,420 --> 00:08:47,279
this allows us to use the right tool for

227
00:08:47,279 --> 00:08:48,959
the job which is one of the most

228
00:08:48,959 --> 00:08:50,339
important things that we try to

229
00:08:50,339 --> 00:08:52,320
accomplish with this architecture

230
00:08:52,320 --> 00:08:55,519
so putting these two sides together

231
00:08:55,519 --> 00:08:58,140
this is what the entire thing looks like

232
00:08:58,140 --> 00:09:00,240
and it's a little you know it's a little

233
00:09:00,240 --> 00:09:03,000
big it's a little complex but it's all

234
00:09:03,000 --> 00:09:04,500
managed by infrastructure as code

235
00:09:04,500 --> 00:09:07,680
doesn't require very much uh you know

236
00:09:07,680 --> 00:09:10,640
care and feeding at all

237
00:09:11,279 --> 00:09:14,399
so now we'll get into kind of deep Dives

238
00:09:14,399 --> 00:09:16,260
on the different parts of this

239
00:09:16,260 --> 00:09:18,360
architecture

240
00:09:18,360 --> 00:09:20,700
so to begin with we're going to start

241
00:09:20,700 --> 00:09:22,560
with the data collection which is one on

242
00:09:22,560 --> 00:09:24,360
the left hand side of that diagram that

243
00:09:24,360 --> 00:09:26,279
we were just looking at

244
00:09:26,279 --> 00:09:28,380
so this takes the raw data and brings it

245
00:09:28,380 --> 00:09:31,519
into our platform

246
00:09:31,560 --> 00:09:34,080
now as we covered previously in the

247
00:09:34,080 --> 00:09:36,060
overview the data sources are extremely

248
00:09:36,060 --> 00:09:39,360
diverse we have a preference for pull

249
00:09:39,360 --> 00:09:41,220
and the reason you want to prefer pull

250
00:09:41,220 --> 00:09:43,820
for data collection is because

251
00:09:43,820 --> 00:09:46,019
there aren't as many reliability

252
00:09:46,019 --> 00:09:48,000
concerns with the pull data source so a

253
00:09:48,000 --> 00:09:51,120
pull data source caching is inherent you

254
00:09:51,120 --> 00:09:52,920
grab the data when you're ready to

255
00:09:52,920 --> 00:09:54,380
receive it

256
00:09:54,380 --> 00:09:56,820
and generally that's very very

257
00:09:56,820 --> 00:09:58,620
straightforward and allows data buffer

258
00:09:58,620 --> 00:10:00,959
On the Origin side with a push data

259
00:10:00,959 --> 00:10:02,240
source

260
00:10:02,240 --> 00:10:06,600
like syslog or an HTTP receiver

261
00:10:06,600 --> 00:10:08,760
that increases the reliability

262
00:10:08,760 --> 00:10:11,160
requirements on that input

263
00:10:11,160 --> 00:10:14,100
and the reason for that is that if your

264
00:10:14,100 --> 00:10:18,140
input goes down for you know hours days

265
00:10:18,140 --> 00:10:20,880
uh then that data will be buffered on

266
00:10:20,880 --> 00:10:23,640
the source maybe uh and it will

267
00:10:23,640 --> 00:10:25,980
eventually be lost it may be lost right

268
00:10:25,980 --> 00:10:28,620
away so that's why we have a strong

269
00:10:28,620 --> 00:10:30,120
preference for pull inputs even though

270
00:10:30,120 --> 00:10:33,000
we support both in our architecture

271
00:10:33,000 --> 00:10:35,700
now the scale can be very large so we

272
00:10:35,700 --> 00:10:38,040
needed an architecture that is you know

273
00:10:38,040 --> 00:10:41,060
elastically scalable Cloud native

274
00:10:41,060 --> 00:10:45,000
and we want that scaling to grow and to

275
00:10:45,000 --> 00:10:49,320
shrink as the input changes over time so

276
00:10:49,320 --> 00:10:51,839
we want it to handle bursts you know the

277
00:10:51,839 --> 00:10:55,079
the old slash dot effect right uh and

278
00:10:55,079 --> 00:10:57,600
also just general seasonality of data

279
00:10:57,600 --> 00:10:59,820
where you know nobody uses the platform

280
00:10:59,820 --> 00:11:02,339
on a weekend so we don't really you know

281
00:11:02,339 --> 00:11:04,320
need to be running a full capacity

282
00:11:04,320 --> 00:11:07,260
cluster at that time

283
00:11:07,260 --> 00:11:09,779
another goal is to have the cost growth

284
00:11:09,779 --> 00:11:13,980
be no more than linear with volume so

285
00:11:13,980 --> 00:11:15,839
ideally we can get some economies of

286
00:11:15,839 --> 00:11:18,600
scale as the volume goes up the costs

287
00:11:18,600 --> 00:11:20,940
may not grow as quickly as the volume

288
00:11:20,940 --> 00:11:23,220
does but at the very least you don't

289
00:11:23,220 --> 00:11:25,440
want to be paying any more as you get

290
00:11:25,440 --> 00:11:27,240
more and more data because that limits

291
00:11:27,240 --> 00:11:29,880
your long-term scalability

292
00:11:29,880 --> 00:11:32,100
we wanted a system that was limited

293
00:11:32,100 --> 00:11:34,440
required limited coding so a lot of

294
00:11:34,440 --> 00:11:36,899
these inputs we wanted collaborators to

295
00:11:36,899 --> 00:11:38,760
be able to help with this

296
00:11:38,760 --> 00:11:40,440
um you know anybody should be able to

297
00:11:40,440 --> 00:11:42,899
get data to us so we developed a system

298
00:11:42,899 --> 00:11:44,700
with a significant wrapper functions

299
00:11:44,700 --> 00:11:46,560
that allowed you to write many types of

300
00:11:46,560 --> 00:11:49,920
inputs in just a few lines of python and

301
00:11:49,920 --> 00:11:51,959
we wanted to be resilient to failure and

302
00:11:51,959 --> 00:11:53,519
have health monitoring which will cover

303
00:11:53,519 --> 00:11:55,620
the specifics of in a few in a few

304
00:11:55,620 --> 00:11:58,680
slides most importantly

305
00:11:58,680 --> 00:12:01,800
stay Cloud native otherwise you're going

306
00:12:01,800 --> 00:12:04,079
to be just paying extra and you're not

307
00:12:04,079 --> 00:12:05,760
going to be efficient on the cloud that

308
00:12:05,760 --> 00:12:08,360
you're running on

309
00:12:08,760 --> 00:12:10,760
so for pull sources

310
00:12:10,760 --> 00:12:13,680
the architecture for both sources is

311
00:12:13,680 --> 00:12:16,260
kind of similar for all pull sources one

312
00:12:16,260 --> 00:12:19,079
way or another in our architecture the

313
00:12:19,079 --> 00:12:20,940
notification notification of data will

314
00:12:20,940 --> 00:12:23,040
arrive on an eventbridge bus and that

315
00:12:23,040 --> 00:12:25,200
will route to an sqsq which is in front

316
00:12:25,200 --> 00:12:27,440
of an input Lambda the reason we use

317
00:12:27,440 --> 00:12:29,880
sqsqs instead of going directly from

318
00:12:29,880 --> 00:12:34,200
eventbridge to Lambda is because sqsqs

319
00:12:34,200 --> 00:12:36,720
allow your Landers to run synchronously

320
00:12:36,720 --> 00:12:38,760
and give you some

321
00:12:38,760 --> 00:12:39,540
um

322
00:12:39,540 --> 00:12:44,120
retry benefits it's more able to redrive

323
00:12:44,120 --> 00:12:48,000
data if uh if things don't go so well in

324
00:12:48,000 --> 00:12:51,440
your Lambda which can sometimes happen

325
00:12:51,440 --> 00:12:54,540
so after the it arrives in that sqsq the

326
00:12:54,540 --> 00:12:56,279
function is triggered and the data is

327
00:12:56,279 --> 00:12:57,720
collected and written to that Central

328
00:12:57,720 --> 00:13:01,320
Kinesis fire hose uh which then goes

329
00:13:01,320 --> 00:13:04,200
into our bucket

330
00:13:04,200 --> 00:13:06,180
now what about when it's an API that

331
00:13:06,180 --> 00:13:07,920
we're pulling from and it's not an S3

332
00:13:07,920 --> 00:13:09,839
bucket S3 buckets are pretty much the

333
00:13:09,839 --> 00:13:13,260
trivial case but when it's an API you

334
00:13:13,260 --> 00:13:16,380
may not get a web hook notification I

335
00:13:16,380 --> 00:13:18,500
haven't really even seen that

336
00:13:18,500 --> 00:13:20,700
eventbridge has a scheduler built in

337
00:13:20,700 --> 00:13:23,399
which is an AWS service but that

338
00:13:23,399 --> 00:13:25,200
scheduler does have some limitations it

339
00:13:25,200 --> 00:13:26,820
doesn't have any ability to catch up if

340
00:13:26,820 --> 00:13:28,579
it gets behind it'll just keep

341
00:13:28,579 --> 00:13:31,139
monotonically generating cron type

342
00:13:31,139 --> 00:13:36,420
events uh and um it doesn't have a sub

343
00:13:36,420 --> 00:13:38,940
minute granularity if you need that so

344
00:13:38,940 --> 00:13:41,639
we do have a tiny event generator for

345
00:13:41,639 --> 00:13:44,760
our poll sources and that runs in

346
00:13:44,760 --> 00:13:47,459
fargate and it just generates events

347
00:13:47,459 --> 00:13:49,079
into our eventbridge bus that triggers

348
00:13:49,079 --> 00:13:51,800
those inputs

349
00:13:52,139 --> 00:13:55,440
so this is kind of the visual of what

350
00:13:55,440 --> 00:13:58,260
that looks like so as you can see on the

351
00:13:58,260 --> 00:14:00,480
left hand side we have log buckets which

352
00:14:00,480 --> 00:14:02,220
generate bucket notifications into the

353
00:14:02,220 --> 00:14:05,100
event Bridge bus and we have our job

354
00:14:05,100 --> 00:14:06,899
generator which stores booksmarks and

355
00:14:06,899 --> 00:14:09,240
dynamodb but otherwise also generates

356
00:14:09,240 --> 00:14:12,420
into the same eventbridge bus those

357
00:14:12,420 --> 00:14:15,120
trigger those sqs cues which trigger the

358
00:14:15,120 --> 00:14:17,700
lambdas in turn Which pull data from

359
00:14:17,700 --> 00:14:21,360
either those buckets or public log apis

360
00:14:21,360 --> 00:14:23,339
and the logs of course are written to

361
00:14:23,339 --> 00:14:26,519
the event the Kinesis fire hose and then

362
00:14:26,519 --> 00:14:29,240
onto our bucket

363
00:14:30,540 --> 00:14:33,420
now what about those push sources

364
00:14:33,420 --> 00:14:36,600
um as I said uh there's syslog still

365
00:14:36,600 --> 00:14:39,180
exists in the world uh you might need an

366
00:14:39,180 --> 00:14:40,820
HTTP receiver

367
00:14:40,820 --> 00:14:44,519
AWS is pretty much completely optimized

368
00:14:44,519 --> 00:14:47,940
for building high volume https services

369
00:14:47,940 --> 00:14:51,000
so similarly we want to use a similar

370
00:14:51,000 --> 00:14:53,639
type of architecture Lambda is a very

371
00:14:53,639 --> 00:14:55,079
good tool for this because it doesn't

372
00:14:55,079 --> 00:14:56,399
need to process things for long

373
00:14:56,399 --> 00:14:57,560
typically

374
00:14:57,560 --> 00:15:00,660
so you can very simply meet this need by

375
00:15:00,660 --> 00:15:03,180
building a receiver Lambda behind an API

376
00:15:03,180 --> 00:15:04,260
Gateway

377
00:15:04,260 --> 00:15:06,600
now you'll need to make your own

378
00:15:06,600 --> 00:15:07,800
decisions if you're going to build

379
00:15:07,800 --> 00:15:09,480
something like this about how resilient

380
00:15:09,480 --> 00:15:11,699
you want it to be do you want to be in

381
00:15:11,699 --> 00:15:14,160
you know just multiple era availability

382
00:15:14,160 --> 00:15:15,959
zones within one region do you want to

383
00:15:15,959 --> 00:15:18,660
be in multiple regions uh and resilient

384
00:15:18,660 --> 00:15:20,240
across that

385
00:15:20,240 --> 00:15:23,459
but in general again you know you can

386
00:15:23,459 --> 00:15:26,279
reuse the code from the pull inputs and

387
00:15:26,279 --> 00:15:28,680
use the same formatting and filtering

388
00:15:28,680 --> 00:15:31,560
that you would use in your push inputs

389
00:15:31,560 --> 00:15:34,500
designed for uh for high volume of

390
00:15:34,500 --> 00:15:37,019
course so you do want to allow for

391
00:15:37,019 --> 00:15:39,120
batched uploads

392
00:15:39,120 --> 00:15:42,120
um if you do an HTTP request for every

393
00:15:42,120 --> 00:15:43,860
single log message you'll quickly

394
00:15:43,860 --> 00:15:47,339
overwhelm even aws's scalability

395
00:15:47,339 --> 00:15:49,800
and finally since this is a security

396
00:15:49,800 --> 00:15:52,560
talk you do need to threat model an

397
00:15:52,560 --> 00:15:55,860
https input and decide what the risks

398
00:15:55,860 --> 00:15:58,440
are that you're willing to accept

399
00:15:58,440 --> 00:16:01,440
um you know in some cases I don't think

400
00:16:01,440 --> 00:16:03,180
that it's particularly risky to have

401
00:16:03,180 --> 00:16:06,060
this on the internet as long as it's you

402
00:16:06,060 --> 00:16:07,920
know right only right only being the

403
00:16:07,920 --> 00:16:09,360
important thing

404
00:16:09,360 --> 00:16:12,120
um but you know your mileage may vary

405
00:16:12,120 --> 00:16:14,279
um be sure to you know threat model this

406
00:16:14,279 --> 00:16:17,600
for your own organization

407
00:16:18,899 --> 00:16:22,920
now once you have that reliable log

408
00:16:22,920 --> 00:16:24,660
endpoint

409
00:16:24,660 --> 00:16:27,000
um you can then use collectors to

410
00:16:27,000 --> 00:16:30,360
collect things that don't speak HTTP so

411
00:16:30,360 --> 00:16:33,600
you know there are many tools for this

412
00:16:33,600 --> 00:16:35,100
um they're open source tools they're

413
00:16:35,100 --> 00:16:38,040
easily easy to get easy to install and

414
00:16:38,040 --> 00:16:39,779
from there you can make a centralized

415
00:16:39,779 --> 00:16:41,639
syslog collector and have that forwarded

416
00:16:41,639 --> 00:16:44,639
to http or you can pick up files from

417
00:16:44,639 --> 00:16:47,459
machines or you can collect metrics and

418
00:16:47,459 --> 00:16:49,740
that all goes into the same pipeline as

419
00:16:49,740 --> 00:16:51,959
everything else so once again we're all

420
00:16:51,959 --> 00:16:55,019
centralized everything you know is is

421
00:16:55,019 --> 00:16:57,540
hunky-dory so far

422
00:16:57,540 --> 00:16:59,040
so

423
00:16:59,040 --> 00:17:01,860
what if something goes wrong right so

424
00:17:01,860 --> 00:17:05,280
obviously these things do uh you do

425
00:17:05,280 --> 00:17:07,919
sometimes get malformed logs you do

426
00:17:07,919 --> 00:17:10,619
sometimes get uh you know a format

427
00:17:10,619 --> 00:17:13,140
changing how do you detect that

428
00:17:13,140 --> 00:17:15,419
so again once again we're using an AWS

429
00:17:15,419 --> 00:17:17,339
native architecture

430
00:17:17,339 --> 00:17:20,939
so we use cloud watch alerts to detect

431
00:17:20,939 --> 00:17:22,919
messages and dead letter cues from our

432
00:17:22,919 --> 00:17:26,400
sqs cues too many messages too few

433
00:17:26,400 --> 00:17:27,799
messages

434
00:17:27,799 --> 00:17:30,240
Lambda failures and the runtime of

435
00:17:30,240 --> 00:17:32,760
lambdas because there is a hard limit of

436
00:17:32,760 --> 00:17:35,160
lambdas you can set a limit on each

437
00:17:35,160 --> 00:17:37,020
Lambda but the longest you can set is 15

438
00:17:37,020 --> 00:17:38,820
minutes so as you start to approach that

439
00:17:38,820 --> 00:17:40,919
you do need to tune

440
00:17:40,919 --> 00:17:42,120
um

441
00:17:42,120 --> 00:17:44,700
typically we only see failures in

442
00:17:44,700 --> 00:17:46,820
response to somebody making a change

443
00:17:46,820 --> 00:17:50,400
again you can see format changes

444
00:17:50,400 --> 00:17:52,080
Downstream but again that's usually due

445
00:17:52,080 --> 00:17:56,100
to a human likewise other failures with

446
00:17:56,100 --> 00:17:58,140
like run time exceeded or things like

447
00:17:58,140 --> 00:17:59,940
that are usually due to some human as

448
00:17:59,940 --> 00:18:02,460
well so we have not seen a lot of uh

449
00:18:02,460 --> 00:18:04,140
We've not needed a lot of care and

450
00:18:04,140 --> 00:18:05,880
feeding and it's usually untraceable to

451
00:18:05,880 --> 00:18:10,220
something in our gitups process

452
00:18:11,280 --> 00:18:13,559
so now we've collected the data and

453
00:18:13,559 --> 00:18:15,600
we've put it all in a centralized S3

454
00:18:15,600 --> 00:18:16,860
bucket

455
00:18:16,860 --> 00:18:18,419
um or actually as part of putting it

456
00:18:18,419 --> 00:18:21,480
into an S3 bucket we're going to take

457
00:18:21,480 --> 00:18:23,340
this normalization step

458
00:18:23,340 --> 00:18:25,799
we developed a companion system uh

459
00:18:25,799 --> 00:18:27,299
notice this is the normalization and

460
00:18:27,299 --> 00:18:29,340
enrichment step we developed a companion

461
00:18:29,340 --> 00:18:30,780
system for managing enrichment data

462
00:18:30,780 --> 00:18:32,720
which David will cover a little later on

463
00:18:32,720 --> 00:18:35,039
but right now we'll just talk briefly

464
00:18:35,039 --> 00:18:37,740
about normalization

465
00:18:37,740 --> 00:18:40,620
so we talked about normalization earlier

466
00:18:40,620 --> 00:18:43,380
just the why again structured logs are

467
00:18:43,380 --> 00:18:48,500
more performant and easier to analyze

468
00:18:49,320 --> 00:18:52,980
and you also have an opportunity when

469
00:18:52,980 --> 00:18:54,840
you normalize to standardize your field

470
00:18:54,840 --> 00:18:56,700
names so there are some common things

471
00:18:56,700 --> 00:18:59,419
like username and IP address

472
00:18:59,419 --> 00:19:02,760
that can make your analysis code much

473
00:19:02,760 --> 00:19:04,080
easier to write if you use a

474
00:19:04,080 --> 00:19:06,480
standardized format

475
00:19:06,480 --> 00:19:08,940
we also have some instrumentation inside

476
00:19:08,940 --> 00:19:11,220
our normalization engine which allows us

477
00:19:11,220 --> 00:19:14,880
to verify if something starts performing

478
00:19:14,880 --> 00:19:16,919
poorly if we get less data than we

479
00:19:16,919 --> 00:19:19,380
expect it's just another point of

480
00:19:19,380 --> 00:19:21,059
instrumentation in addition to our Cloud

481
00:19:21,059 --> 00:19:23,220
watch

482
00:19:23,220 --> 00:19:25,620
one important choice when you do build

483
00:19:25,620 --> 00:19:27,960
your own normalization is whether you

484
00:19:27,960 --> 00:19:30,299
want to keep the raw message or not so

485
00:19:30,299 --> 00:19:32,340
the whole point of normalization is to

486
00:19:32,340 --> 00:19:33,780
reorganize the message into something

487
00:19:33,780 --> 00:19:35,520
that's easy to use do you keep the

488
00:19:35,520 --> 00:19:38,580
original for us the costs are low enough

489
00:19:38,580 --> 00:19:40,860
that we have chosen to keep the raw

490
00:19:40,860 --> 00:19:43,740
message that allows us to reprocess in

491
00:19:43,740 --> 00:19:47,539
the future if our schema does change

492
00:19:48,919 --> 00:19:53,340
now I mentioned log schemas

493
00:19:53,340 --> 00:19:55,200
if you're building from scratch like we

494
00:19:55,200 --> 00:19:58,200
did you have an important uh you know

495
00:19:58,200 --> 00:19:59,940
very important choice to make in what

496
00:19:59,940 --> 00:20:02,640
schema you use there are some very high

497
00:20:02,640 --> 00:20:04,919
capability schemas that have a whole lot

498
00:20:04,919 --> 00:20:08,400
of uh you know data types in them

499
00:20:08,400 --> 00:20:09,419
um

500
00:20:09,419 --> 00:20:12,960
they require a more advanced normalizer

501
00:20:12,960 --> 00:20:15,299
and a more advanced normalizer requires

502
00:20:15,299 --> 00:20:17,280
more care and feeding the more regular

503
00:20:17,280 --> 00:20:19,679
Expressions you use the more likely that

504
00:20:19,679 --> 00:20:22,620
something could go wrong

505
00:20:22,620 --> 00:20:25,559
um but some popular choices as far as

506
00:20:25,559 --> 00:20:28,740
those more advanced schemas would be the

507
00:20:28,740 --> 00:20:30,780
elastic common schema or the open cyber

508
00:20:30,780 --> 00:20:33,660
security schema framework both of those

509
00:20:33,660 --> 00:20:36,480
have support by you know fairly large

510
00:20:36,480 --> 00:20:39,480
chunks of the industry

511
00:20:39,480 --> 00:20:41,340
but what did we do

512
00:20:41,340 --> 00:20:43,200
so

513
00:20:43,200 --> 00:20:45,600
we decided to go with a very lightweight

514
00:20:45,600 --> 00:20:48,660
wrapper schema as I said that we are

515
00:20:48,660 --> 00:20:50,520
retaining the original message so we

516
00:20:50,520 --> 00:20:53,340
have a fairly large uh a fairly good

517
00:20:53,340 --> 00:20:54,900
ability to

518
00:20:54,900 --> 00:20:57,600
take actions further Downstream on that

519
00:20:57,600 --> 00:21:00,179
uh that original message or slightly

520
00:21:00,179 --> 00:21:03,000
restructured message

521
00:21:03,000 --> 00:21:06,720
so we just have a simple wrapper that

522
00:21:06,720 --> 00:21:09,720
includes where it came from

523
00:21:09,720 --> 00:21:11,700
um what the metadata about how it got

524
00:21:11,700 --> 00:21:15,600
there was when we saw it first a

525
00:21:15,600 --> 00:21:18,000
deduplication ID and you know the

526
00:21:18,000 --> 00:21:20,340
message itself and a few other uh less

527
00:21:20,340 --> 00:21:22,919
important fields

528
00:21:22,919 --> 00:21:25,500
now again since this is an architecture

529
00:21:25,500 --> 00:21:26,760
deep dive

530
00:21:26,760 --> 00:21:28,380
we're just going to briefly talk about

531
00:21:28,380 --> 00:21:29,940
how we actually accomplish that

532
00:21:29,940 --> 00:21:32,580
normalization and that's what I consider

533
00:21:32,580 --> 00:21:35,159
like to be a Cooperative design the

534
00:21:35,159 --> 00:21:38,340
inputs give the basic format uh they

535
00:21:38,340 --> 00:21:40,500
ensure they verify that the format

536
00:21:40,500 --> 00:21:43,620
matches what's expected and structure it

537
00:21:43,620 --> 00:21:45,620
so that it's already

538
00:21:45,620 --> 00:21:48,720
in you know in a structured data format

539
00:21:48,720 --> 00:21:51,000
so it can be embedded into Json without

540
00:21:51,000 --> 00:21:53,100
needing to then pull out the data later

541
00:21:53,100 --> 00:21:55,080
on

542
00:21:55,080 --> 00:21:57,720
um from there it goes to fire hose and

543
00:21:57,720 --> 00:21:59,460
fire hose has the ability to run

544
00:21:59,460 --> 00:22:02,880
arbitrary functions on uh the messages

545
00:22:02,880 --> 00:22:05,520
that go by in firehose so there's an

546
00:22:05,520 --> 00:22:07,320
additional normalization Lambda there

547
00:22:07,320 --> 00:22:09,720
where we can do centralized processing

548
00:22:09,720 --> 00:22:11,940
for example we can add enrichment data

549
00:22:11,940 --> 00:22:13,980
into the message if there's something

550
00:22:13,980 --> 00:22:15,360
that's going to be transient that we

551
00:22:15,360 --> 00:22:16,700
might need in the future

552
00:22:16,700 --> 00:22:19,320
and we can also do

553
00:22:19,320 --> 00:22:22,919
um some metrics on the data as it goes

554
00:22:22,919 --> 00:22:23,640
by

555
00:22:23,640 --> 00:22:26,280
in the normalizer so

556
00:22:26,280 --> 00:22:29,820
that should wrap up the normalization

557
00:22:29,820 --> 00:22:33,179
phase and will then move on into the

558
00:22:33,179 --> 00:22:35,159
detection phase which David's going to

559
00:22:35,159 --> 00:22:37,760
take over for

560
00:22:41,179 --> 00:22:44,900
oh thanks Brian

561
00:22:45,240 --> 00:22:47,780
so

562
00:22:48,840 --> 00:22:51,059
going back to our original architecture

563
00:22:51,059 --> 00:22:53,100
diagram uh thank you Brian for covering

564
00:22:53,100 --> 00:22:55,200
the left half which is the very complex

565
00:22:55,200 --> 00:22:57,120
phase of you know getting a lot of

566
00:22:57,120 --> 00:22:59,039
different heterogeneous data sources and

567
00:22:59,039 --> 00:23:00,840
placing them into a single bucket where

568
00:23:00,840 --> 00:23:02,760
you have a nice source of Truth and

569
00:23:02,760 --> 00:23:04,080
right now we're going to focus on the

570
00:23:04,080 --> 00:23:05,880
right side of the equation which is what

571
00:23:05,880 --> 00:23:07,740
do we do with this data from a detection

572
00:23:07,740 --> 00:23:10,200
response function when it's actually

573
00:23:10,200 --> 00:23:13,200
flowing into our kind of data Lake

574
00:23:13,200 --> 00:23:16,320
so taking kind of one quick step back uh

575
00:23:16,320 --> 00:23:17,940
you know what is the detection blue team

576
00:23:17,940 --> 00:23:20,400
101 really we're looking at some sort of

577
00:23:20,400 --> 00:23:22,200
data source and trying to identify

578
00:23:22,200 --> 00:23:24,000
whether malicious events are happening

579
00:23:24,000 --> 00:23:25,679
that somebody needs to either respond

580
00:23:25,679 --> 00:23:27,419
respond to either automatically or

581
00:23:27,419 --> 00:23:29,640
manually and for us we kind of have

582
00:23:29,640 --> 00:23:31,020
broken it up into two different

583
00:23:31,020 --> 00:23:33,480
classifications we have streaming style

584
00:23:33,480 --> 00:23:34,980
detections where you're looking at

585
00:23:34,980 --> 00:23:37,380
events as they come in one by one a good

586
00:23:37,380 --> 00:23:39,000
example of this is you know you're

587
00:23:39,000 --> 00:23:40,620
looking at Network traffic you have a

588
00:23:40,620 --> 00:23:43,440
host which has reached out to uh some IP

589
00:23:43,440 --> 00:23:44,760
or some domain that you know is

590
00:23:44,760 --> 00:23:47,039
associated with a malicious actor that

591
00:23:47,039 --> 00:23:49,380
event in and of itself that Json payload

592
00:23:49,380 --> 00:23:51,299
is enough to tell you that hey something

593
00:23:51,299 --> 00:23:52,919
is wrong you can look at those events

594
00:23:52,919 --> 00:23:54,419
kind of one by one they have

595
00:23:54,419 --> 00:23:57,240
self-encapsulated context a bit more

596
00:23:57,240 --> 00:23:59,520
complicated is the batch detection where

597
00:23:59,520 --> 00:24:00,900
you're actually looking for additional

598
00:24:00,900 --> 00:24:02,700
context and addition additional

599
00:24:02,700 --> 00:24:04,559
information of the same kind of events

600
00:24:04,559 --> 00:24:08,220
over time a really good example is the

601
00:24:08,220 --> 00:24:09,960
MFA fatigue attacks that have been

602
00:24:09,960 --> 00:24:11,760
happening uh pretty frequently and we're

603
00:24:11,760 --> 00:24:15,120
in securing you a couple months ago but

604
00:24:15,120 --> 00:24:16,799
essentially if you think about detecting

605
00:24:16,799 --> 00:24:18,539
for an attack like that where you're

606
00:24:18,539 --> 00:24:20,159
consistently having or you're having a

607
00:24:20,159 --> 00:24:22,500
user who's consistently getting MFA auth

608
00:24:22,500 --> 00:24:25,080
requests if somebody fails an MFA auth

609
00:24:25,080 --> 00:24:27,120
request one time that could be normal

610
00:24:27,120 --> 00:24:28,740
right you hit the wrong button on your

611
00:24:28,740 --> 00:24:30,000
phone your face ID doesn't work

612
00:24:30,000 --> 00:24:31,980
something happens but if you see

613
00:24:31,980 --> 00:24:33,480
activity where you're having a user

614
00:24:33,480 --> 00:24:35,340
consistently failing it three four five

615
00:24:35,340 --> 00:24:37,440
times in a span of a few minutes that

616
00:24:37,440 --> 00:24:38,760
might indicate a little bit more that

617
00:24:38,760 --> 00:24:41,400
something bad is actually happening so

618
00:24:41,400 --> 00:24:42,780
um we have kind of these two different

619
00:24:42,780 --> 00:24:44,580
categorizations for detections that we

620
00:24:44,580 --> 00:24:46,740
look at

621
00:24:46,740 --> 00:24:48,659
uh really want to talk about how we

622
00:24:48,659 --> 00:24:50,520
manage detections as well uh there was

623
00:24:50,520 --> 00:24:52,440
actually a panel right before this which

624
00:24:52,440 --> 00:24:54,179
uh seemed great I highly recommend

625
00:24:54,179 --> 00:24:55,500
checking that out that kind of covered

626
00:24:55,500 --> 00:24:57,360
this topic in more detail uh we just

627
00:24:57,360 --> 00:24:59,580
wanted to touch it really quickly so

628
00:24:59,580 --> 00:25:01,260
taking components from software

629
00:25:01,260 --> 00:25:03,299
engineering principles Brian and I are

630
00:25:03,299 --> 00:25:04,860
software Engineers by Nature we're

631
00:25:04,860 --> 00:25:06,780
Builders at heart spent a lot of time

632
00:25:06,780 --> 00:25:09,360
kind of in those kinds of workflows and

633
00:25:09,360 --> 00:25:11,039
for us it was really really important to

634
00:25:11,039 --> 00:25:12,780
kind of take those principles and align

635
00:25:12,780 --> 00:25:14,880
them to our detections as well so if you

636
00:25:14,880 --> 00:25:16,620
think about detections not as a

637
00:25:16,620 --> 00:25:18,240
detection but as some sort of software

638
00:25:18,240 --> 00:25:20,580
service and you shift your mental model

639
00:25:20,580 --> 00:25:22,320
a little bit there's a lot of parallels

640
00:25:22,320 --> 00:25:23,820
that will happen so you want to be able

641
00:25:23,820 --> 00:25:25,799
to write repeatable detections you want

642
00:25:25,799 --> 00:25:27,059
to have a centralized source of Truth

643
00:25:27,059 --> 00:25:28,919
for them you want to be able to test

644
00:25:28,919 --> 00:25:30,960
them upgrade them roll them back in case

645
00:25:30,960 --> 00:25:33,120
of a bug and you also want to be able to

646
00:25:33,120 --> 00:25:34,559
promote them across different Dev and

647
00:25:34,559 --> 00:25:36,539
prod environments so by the time with

648
00:25:36,539 --> 00:25:39,059
detection or your software service gets

649
00:25:39,059 --> 00:25:41,279
to a production environment you should

650
00:25:41,279 --> 00:25:42,900
have full confidence and its ability to

651
00:25:42,900 --> 00:25:45,659
do the job that you wanted to testing

652
00:25:45,659 --> 00:25:47,279
both in unit tests and integration

653
00:25:47,279 --> 00:25:48,480
testing is something that we really

654
00:25:48,480 --> 00:25:50,520
wanted to have as well to ensure really

655
00:25:50,520 --> 00:25:53,159
high Bar for our again software services

656
00:25:53,159 --> 00:25:55,679
in this case detections

657
00:25:55,679 --> 00:25:57,419
so wanted to walk you through kind of

658
00:25:57,419 --> 00:25:59,760
the detection workflow that we've kind

659
00:25:59,760 --> 00:26:01,080
of come up with that has that we've

660
00:26:01,080 --> 00:26:02,580
built out has been working pretty well

661
00:26:02,580 --> 00:26:04,799
for us so I'm kind of skipping the

662
00:26:04,799 --> 00:26:06,299
initial phases of the detection life

663
00:26:06,299 --> 00:26:07,559
cycle where you would gather your

664
00:26:07,559 --> 00:26:09,360
requirements you'd come up with a

665
00:26:09,360 --> 00:26:11,100
hypothesis this is more when you've kind

666
00:26:11,100 --> 00:26:12,600
of have a little proof of concept and

667
00:26:12,600 --> 00:26:14,279
you want to start working on a tangible

668
00:26:14,279 --> 00:26:15,659
detection

669
00:26:15,659 --> 00:26:17,580
uh so we start off in a Dev environment

670
00:26:17,580 --> 00:26:19,320
we purposefully have a delineation

671
00:26:19,320 --> 00:26:21,000
between our Dev environment and our

672
00:26:21,000 --> 00:26:22,500
production environment again just going

673
00:26:22,500 --> 00:26:24,539
back to that testing story you can make

674
00:26:24,539 --> 00:26:25,919
changes you can try new things out

675
00:26:25,919 --> 00:26:27,900
without risking any Downstream systems

676
00:26:27,900 --> 00:26:31,020
we have two similar data flows in both

677
00:26:31,020 --> 00:26:32,520
our Dev environment and our private

678
00:26:32,520 --> 00:26:34,980
environment to keep costs down we do

679
00:26:34,980 --> 00:26:36,840
data sampling in our Dev environment but

680
00:26:36,840 --> 00:26:38,760
you still have a very real stream of

681
00:26:38,760 --> 00:26:41,159
data that's coming in uh you know with

682
00:26:41,159 --> 00:26:43,799
certain contextual requirements if the

683
00:26:43,799 --> 00:26:45,539
data is okay to be in Dev and then that

684
00:26:45,539 --> 00:26:47,100
gives you a really nice environment to

685
00:26:47,100 --> 00:26:49,140
test things out uh prototype things and

686
00:26:49,140 --> 00:26:50,220
again without having the risk of

687
00:26:50,220 --> 00:26:52,020
impacting something in production

688
00:26:52,020 --> 00:26:53,940
so you do your local development you

689
00:26:53,940 --> 00:26:55,620
write some code next thing that happens

690
00:26:55,620 --> 00:26:57,480
is as with the software service you're

691
00:26:57,480 --> 00:26:59,580
going to write some sort of unit tests

692
00:26:59,580 --> 00:27:01,440
unit tests get committed to a GitHub

693
00:27:01,440 --> 00:27:04,919
repository that kicks off a deployment

694
00:27:04,919 --> 00:27:07,380
uh and once the deployment happens we

695
00:27:07,380 --> 00:27:08,580
also have an additional step of

696
00:27:08,580 --> 00:27:11,340
continuous validation so if you think of

697
00:27:11,340 --> 00:27:13,799
uh you know a web service or or a

698
00:27:13,799 --> 00:27:16,140
website it's pretty easy to know when

699
00:27:16,140 --> 00:27:17,880
issues are happening the website won't

700
00:27:17,880 --> 00:27:20,100
load uh latency is super super high

701
00:27:20,100 --> 00:27:22,020
maybe there's error rates that are

702
00:27:22,020 --> 00:27:24,000
happening on the back end uh but for

703
00:27:24,000 --> 00:27:26,460
detection you don't really necessarily

704
00:27:26,460 --> 00:27:28,740
know how often it's going to or not

705
00:27:28,740 --> 00:27:30,840
going to fire in fact for some of your

706
00:27:30,840 --> 00:27:33,120
very high fidelity detections ideally

707
00:27:33,120 --> 00:27:34,740
depending on the environment you're in

708
00:27:34,740 --> 00:27:36,840
they shouldn't really ever fire at all

709
00:27:36,840 --> 00:27:38,760
so we really wanted to have some sort of

710
00:27:38,760 --> 00:27:40,799
continuous validation system to ensure

711
00:27:40,799 --> 00:27:42,179
that the detections that were written

712
00:27:42,179 --> 00:27:44,279
you know one two three years ago maybe

713
00:27:44,279 --> 00:27:45,720
by someone who's not even on the team

714
00:27:45,720 --> 00:27:47,640
anymore are still doing the exact job

715
00:27:47,640 --> 00:27:48,900
that they were designed to do from day

716
00:27:48,900 --> 00:27:49,679
one

717
00:27:49,679 --> 00:27:52,320
so if all of these steps pass and the

718
00:27:52,320 --> 00:27:54,539
noise level is acceptable continuous

719
00:27:54,539 --> 00:27:57,000
validation is good we will simply do a

720
00:27:57,000 --> 00:27:59,100
promotion process and deploy it in our

721
00:27:59,100 --> 00:28:00,840
production environment or we still get

722
00:28:00,840 --> 00:28:03,600
the continuous validation consistently

723
00:28:03,600 --> 00:28:05,159
so I've talked about this continuous

724
00:28:05,159 --> 00:28:07,080
validation step a couple of times and I

725
00:28:07,080 --> 00:28:09,720
want to dive into it a little bit deeper

726
00:28:09,720 --> 00:28:11,520
um one thing I also want to call out is

727
00:28:11,520 --> 00:28:12,960
that when Brian and I were building the

728
00:28:12,960 --> 00:28:15,539
system we were kind of faced with a fair

729
00:28:15,539 --> 00:28:17,640
amount of work to do to go from you know

730
00:28:17,640 --> 00:28:20,039
from scratch to something functional and

731
00:28:20,039 --> 00:28:21,659
for the detection engine we kind of

732
00:28:21,659 --> 00:28:22,980
needed to figure out if it was something

733
00:28:22,980 --> 00:28:24,840
we wanted to build or something we

734
00:28:24,840 --> 00:28:26,940
wanted to buy for a variety of different

735
00:28:26,940 --> 00:28:28,799
reasons based on engineering constraints

736
00:28:28,799 --> 00:28:31,320
and a couple of other factors it was

737
00:28:31,320 --> 00:28:32,880
also really important for us to be able

738
00:28:32,880 --> 00:28:34,500
to immediately deliver value to the

739
00:28:34,500 --> 00:28:36,779
business so as the business is investing

740
00:28:36,779 --> 00:28:39,539
in a detection response function we

741
00:28:39,539 --> 00:28:40,919
needed to be able to kind of show value

742
00:28:40,919 --> 00:28:43,320
pretty quickly and so we chose to kind

743
00:28:43,320 --> 00:28:45,360
of buy a detection engine so that we can

744
00:28:45,360 --> 00:28:47,460
build workflows on top of that gave us a

745
00:28:47,460 --> 00:28:49,320
nice Foundation to get started and let

746
00:28:49,320 --> 00:28:50,700
us invest in some really really neat

747
00:28:50,700 --> 00:28:52,260
efforts to guarantee a high level of

748
00:28:52,260 --> 00:28:54,120
service for instance continuous

749
00:28:54,120 --> 00:28:55,559
validation

750
00:28:55,559 --> 00:28:58,020
so on the detection Health System I

751
00:28:58,020 --> 00:28:59,700
think I got a little bit ahead of myself

752
00:28:59,700 --> 00:29:01,740
here uh but essentially your deductions

753
00:29:01,740 --> 00:29:03,179
can only help protect the business if

754
00:29:03,179 --> 00:29:04,380
they're actually functioning and if

755
00:29:04,380 --> 00:29:06,419
they're actually working uh unit tests

756
00:29:06,419 --> 00:29:07,380
are great when people talk about

757
00:29:07,380 --> 00:29:09,480
detections as code there's a lot of talk

758
00:29:09,480 --> 00:29:11,940
about unit tests which is fantastic but

759
00:29:11,940 --> 00:29:14,159
they're not necessarily enough to go

760
00:29:14,159 --> 00:29:16,740
from a raw log to a response action all

761
00:29:16,740 --> 00:29:18,659
the steps in between there's a ton of

762
00:29:18,659 --> 00:29:20,940
different interconnected components you

763
00:29:20,940 --> 00:29:22,380
can have bugs you can have bad

764
00:29:22,380 --> 00:29:24,179
deployments you know something can be

765
00:29:24,179 --> 00:29:25,860
failing along the lines that you know

766
00:29:25,860 --> 00:29:27,480
you might not have monitoring for you

767
00:29:27,480 --> 00:29:28,380
might not have thought to build

768
00:29:28,380 --> 00:29:30,840
monitoring for so we really needed some

769
00:29:30,840 --> 00:29:32,520
sort of fully functional end-to-end

770
00:29:32,520 --> 00:29:33,720
integration test

771
00:29:33,720 --> 00:29:35,700
one other thing for us is we want to

772
00:29:35,700 --> 00:29:37,440
keep in mind that writing detections is

773
00:29:37,440 --> 00:29:39,659
part writing complex detection

774
00:29:39,659 --> 00:29:42,059
detections is hard you go from the

775
00:29:42,059 --> 00:29:43,559
requirements phase you go through your

776
00:29:43,559 --> 00:29:45,240
prototype you build it you test the

777
00:29:45,240 --> 00:29:47,340
noise level you tune in the last thing

778
00:29:47,340 --> 00:29:49,020
we wanted to do was come up with some

779
00:29:49,020 --> 00:29:50,940
sort of process which would you know

780
00:29:50,940 --> 00:29:52,919
upset detection response engineers and

781
00:29:52,919 --> 00:29:54,539
want them to bypass the testing process

782
00:29:54,539 --> 00:29:56,220
so we wanted to build something which

783
00:29:56,220 --> 00:29:58,919
was low friction easy and would make our

784
00:29:58,919 --> 00:30:00,779
DNR Engineers want to write tests

785
00:30:00,779 --> 00:30:01,860
because it would give them a benefit

786
00:30:01,860 --> 00:30:03,960
versus something that you know they

787
00:30:03,960 --> 00:30:05,640
would just have to do because we put a

788
00:30:05,640 --> 00:30:07,140
check box and then they kind of stopped

789
00:30:07,140 --> 00:30:09,360
doing it after some time

790
00:30:09,360 --> 00:30:12,000
so our detection health system has uh

791
00:30:12,000 --> 00:30:14,340
two different components um the first is

792
00:30:14,340 --> 00:30:16,260
generating Canary alerts so what we do

793
00:30:16,260 --> 00:30:18,360
is we built a system that takes

794
00:30:18,360 --> 00:30:20,760
synthetic data uh fires in into our

795
00:30:20,760 --> 00:30:22,500
detection Pipeline and then that flows

796
00:30:22,500 --> 00:30:24,360
Downstream there's a couple of different

797
00:30:24,360 --> 00:30:26,279
components here you'll notice uh very

798
00:30:26,279 --> 00:30:28,860
similarly we have a event Bridge rule

799
00:30:28,860 --> 00:30:30,779
which kicks off a Lambda on a Cron job

800
00:30:30,779 --> 00:30:32,760
Lambda has monitoring with Cloud watch

801
00:30:32,760 --> 00:30:34,140
alarms all the built-in native

802
00:30:34,140 --> 00:30:36,059
Integrations again taking a cloud native

803
00:30:36,059 --> 00:30:38,220
approach has been extremely helpful for

804
00:30:38,220 --> 00:30:38,940
us

805
00:30:38,940 --> 00:30:41,340
we put Canary data in an S3 bucket that

806
00:30:41,340 --> 00:30:43,020
the Lambda will read from and then it

807
00:30:43,020 --> 00:30:44,700
will fire into our detection pipeline

808
00:30:44,700 --> 00:30:46,799
which Kinesis fire hose it will take the

809
00:30:46,799 --> 00:30:48,179
data it will send it into our

810
00:30:48,179 --> 00:30:49,980
centralized source of Truth

811
00:30:49,980 --> 00:30:52,440
um S3 bucket what we do with the canary

812
00:30:52,440 --> 00:30:54,539
data is we mark it in some way to

813
00:30:54,539 --> 00:30:56,220
determine that it is actually Canary

814
00:30:56,220 --> 00:30:59,100
event if so in our Downstream automation

815
00:30:59,100 --> 00:31:00,899
Downstream processing typically this

816
00:31:00,899 --> 00:31:03,179
will be a soar platform we short circuit

817
00:31:03,179 --> 00:31:05,100
so we make a note of that event but we

818
00:31:05,100 --> 00:31:07,080
don't generate anything that a human

819
00:31:07,080 --> 00:31:09,299
would need to interact with we don't

820
00:31:09,299 --> 00:31:10,919
cause additional alert for you know an

821
00:31:10,919 --> 00:31:12,960
analyst or a DNR engineer there's no

822
00:31:12,960 --> 00:31:14,340
extra work that's generated from this so

823
00:31:14,340 --> 00:31:16,620
it only really raises noise of canary

824
00:31:16,620 --> 00:31:19,799
events stop firing if it's not a canary

825
00:31:19,799 --> 00:31:21,659
event if it's a real thing of course we

826
00:31:21,659 --> 00:31:22,919
don't want to get in the way of any

827
00:31:22,919 --> 00:31:24,720
alerting there so it just proceeds with

828
00:31:24,720 --> 00:31:27,120
the regular flow uh one other thing I

829
00:31:27,120 --> 00:31:28,559
want to call out is the reason why we

830
00:31:28,559 --> 00:31:30,299
chose to start with synthetic data

831
00:31:30,299 --> 00:31:31,559
rather than building a bunch of

832
00:31:31,559 --> 00:31:33,299
different environments to kind of write

833
00:31:33,299 --> 00:31:35,700
scripts to generate real data is due to

834
00:31:35,700 --> 00:31:37,200
the heterogeneous environment that we

835
00:31:37,200 --> 00:31:39,600
operate in uh Brian alluded to it with

836
00:31:39,600 --> 00:31:42,000
all the different data sources SAS

837
00:31:42,000 --> 00:31:43,140
companies so lots of different

838
00:31:43,140 --> 00:31:45,120
environments and Integrations rather

839
00:31:45,120 --> 00:31:46,679
than spending the time to kind of build

840
00:31:46,679 --> 00:31:48,600
out and mimic every single environment

841
00:31:48,600 --> 00:31:50,279
every single offering that we wanted to

842
00:31:50,279 --> 00:31:52,440
keep track of we felt that we would be

843
00:31:52,440 --> 00:31:54,659
able to provide value more immediately

844
00:31:54,659 --> 00:31:57,000
and faster in achieving our goal of kind

845
00:31:57,000 --> 00:31:59,159
of maintaining a high bar of detections

846
00:31:59,159 --> 00:32:01,860
by using choosing to use Canary events

847
00:32:01,860 --> 00:32:04,260
down the road the intention is to kind

848
00:32:04,260 --> 00:32:06,720
of take the most common environments and

849
00:32:06,720 --> 00:32:08,159
swap out the canary events for real

850
00:32:08,159 --> 00:32:10,440
events that's some future work that will

851
00:32:10,440 --> 00:32:12,539
be done at some point

852
00:32:12,539 --> 00:32:14,340
the second component is actually

853
00:32:14,340 --> 00:32:16,320
sounding the alarm or being the canary

854
00:32:16,320 --> 00:32:18,899
in case a detection stops firing so

855
00:32:18,899 --> 00:32:21,419
every single time a detection fires it

856
00:32:21,419 --> 00:32:23,159
gets written to a detection table which

857
00:32:23,159 --> 00:32:24,539
is actually also used for our batch

858
00:32:24,539 --> 00:32:26,880
detections so there's a record of all of

859
00:32:26,880 --> 00:32:30,120
that a Lambda will query this table with

860
00:32:30,120 --> 00:32:32,220
a cloud watch rule consistently on a

861
00:32:32,220 --> 00:32:34,140
cron it will take these metrics and

862
00:32:34,140 --> 00:32:36,260
report them as cloudwatch metrics

863
00:32:36,260 --> 00:32:38,880
cloudwatch metrics will have alarms tied

864
00:32:38,880 --> 00:32:41,399
to them so if for some period of time

865
00:32:41,399 --> 00:32:43,080
hours days you know whatever your

866
00:32:43,080 --> 00:32:45,059
threshold is there will be an alert that

867
00:32:45,059 --> 00:32:46,740
fires if there's no Canary event that

868
00:32:46,740 --> 00:32:47,820
has run through

869
00:32:47,820 --> 00:32:50,399
uh so what we did was we took kind of

870
00:32:50,399 --> 00:32:51,960
all these two components and we

871
00:32:51,960 --> 00:32:54,120
abstracted it away in a terraform module

872
00:32:54,120 --> 00:32:56,100
very similar to how Brian talked about

873
00:32:56,100 --> 00:32:58,200
having a terraform module to allow for

874
00:32:58,200 --> 00:33:00,419
easy repeatability for onboarding new

875
00:33:00,419 --> 00:33:02,520
data we really wanted to maintain kind

876
00:33:02,520 --> 00:33:04,919
of a similar approach for these uh for

877
00:33:04,919 --> 00:33:06,840
the detection Health System again just

878
00:33:06,840 --> 00:33:08,279
being mindful of the fact that we don't

879
00:33:08,279 --> 00:33:09,899
want to get in anyone's way we want to

880
00:33:09,899 --> 00:33:12,539
empower people to take advantage of this

881
00:33:12,539 --> 00:33:15,000
kind of free testing so all it takes is

882
00:33:15,000 --> 00:33:16,559
instantiating a terraform module it's

883
00:33:16,559 --> 00:33:18,600
like four lines of code and uploading a

884
00:33:18,600 --> 00:33:20,700
file to S3 so very very low friction you

885
00:33:20,700 --> 00:33:22,140
get free monitoring for your detection

886
00:33:22,140 --> 00:33:24,720
knowing something's wrong

887
00:33:24,720 --> 00:33:27,419
the next section is called alerting but

888
00:33:27,419 --> 00:33:29,159
it's actually a trick it's not

889
00:33:29,159 --> 00:33:31,500
necessarily alerting yet so just because

890
00:33:31,500 --> 00:33:33,179
you have a detection that's fired

891
00:33:33,179 --> 00:33:34,559
doesn't necessarily mean that you're

892
00:33:34,559 --> 00:33:36,419
ready to generate an alert and the

893
00:33:36,419 --> 00:33:38,399
reason for that is how much context does

894
00:33:38,399 --> 00:33:40,019
your detection have that's already fired

895
00:33:40,019 --> 00:33:42,419
uh context is key because if you just

896
00:33:42,419 --> 00:33:44,279
have detections alerting humans or

897
00:33:44,279 --> 00:33:46,620
sending stuff into a queue very quickly

898
00:33:46,620 --> 00:33:48,179
you're going to hit alert fatigue you're

899
00:33:48,179 --> 00:33:49,799
going to have a huge backlog and it's

900
00:33:49,799 --> 00:33:51,840
not even going to be helpful you're just

901
00:33:51,840 --> 00:33:52,860
going to have too much stuff that's

902
00:33:52,860 --> 00:33:54,720
firing a lot of it is probably not even

903
00:33:54,720 --> 00:33:56,760
important or is benign so you need

904
00:33:56,760 --> 00:33:58,260
context to be able to filter things out

905
00:33:58,260 --> 00:34:00,720
and suppress them accordingly

906
00:34:00,720 --> 00:34:02,760
I want to walk you through a basic

907
00:34:02,760 --> 00:34:04,620
enrichment example of how a detection

908
00:34:04,620 --> 00:34:06,720
can fire but by enriching it and getting

909
00:34:06,720 --> 00:34:09,359
additional information you can triage it

910
00:34:09,359 --> 00:34:10,560
a lot faster and have much better

911
00:34:10,560 --> 00:34:12,119
understanding when someone begins an

912
00:34:12,119 --> 00:34:15,060
investigation on the screen is a sample

913
00:34:15,060 --> 00:34:17,460
guard Duty alert so this is not a real

914
00:34:17,460 --> 00:34:19,320
alert if you use the service you can

915
00:34:19,320 --> 00:34:20,760
like click a button it will generate

916
00:34:20,760 --> 00:34:22,440
some fake alerts you can see the ec2

917
00:34:22,440 --> 00:34:24,839
instance ID is not real but in this case

918
00:34:24,839 --> 00:34:26,460
it's simulating a real guard Duty

919
00:34:26,460 --> 00:34:28,679
detection firing also guard duty is

920
00:34:28,679 --> 00:34:30,480
Amazon's threat native threat detection

921
00:34:30,480 --> 00:34:32,879
service so you can see that you know

922
00:34:32,879 --> 00:34:34,440
there's an ec2 communicating outbound

923
00:34:34,440 --> 00:34:38,219
with a known C2 server I blocked out the

924
00:34:38,219 --> 00:34:39,599
account but there's an account number in

925
00:34:39,599 --> 00:34:41,699
there and you kind of have a description

926
00:34:41,699 --> 00:34:43,980
of what's going on there so when this

927
00:34:43,980 --> 00:34:46,139
alert fires if you see it and you send

928
00:34:46,139 --> 00:34:47,940
it to someone to take a look at you know

929
00:34:47,940 --> 00:34:49,440
something is wrong right you have some

930
00:34:49,440 --> 00:34:51,480
sort of host it's communicating with

931
00:34:51,480 --> 00:34:52,859
things that shouldn't be communicating

932
00:34:52,859 --> 00:34:54,839
with what's going on but the immediate

933
00:34:54,839 --> 00:34:57,839
questions you're going to ask are what

934
00:34:57,839 --> 00:34:59,700
is the cc2 doing

935
00:34:59,700 --> 00:35:02,160
um whose ec2 is is it important is it a

936
00:35:02,160 --> 00:35:03,900
build server is it like a one-off kind

937
00:35:03,900 --> 00:35:05,940
of research thing is it a production

938
00:35:05,940 --> 00:35:07,680
server we're taking down the server will

939
00:35:07,680 --> 00:35:09,180
impact a production workload and you

940
00:35:09,180 --> 00:35:10,500
have to take a completely different set

941
00:35:10,500 --> 00:35:12,420
of response steps there's a lot of

942
00:35:12,420 --> 00:35:15,000
context that kind of gets built in who

943
00:35:15,000 --> 00:35:17,099
owns the cc2 who's the service team what

944
00:35:17,099 --> 00:35:18,300
kind of stakeholder are you going to be

945
00:35:18,300 --> 00:35:19,380
working with

946
00:35:19,380 --> 00:35:21,240
you have very similar questions around

947
00:35:21,240 --> 00:35:23,280
the account portion who owns this

948
00:35:23,280 --> 00:35:25,079
account again who are you going to talk

949
00:35:25,079 --> 00:35:27,180
to how important is this account is this

950
00:35:27,180 --> 00:35:28,619
a Legacy account that's been forgotten

951
00:35:28,619 --> 00:35:30,720
about is there extremely sensitive data

952
00:35:30,720 --> 00:35:32,040
in here to the business or from

953
00:35:32,040 --> 00:35:33,900
compliance perspectives where all of a

954
00:35:33,900 --> 00:35:36,720
sudden you know this is P0 hands on deck

955
00:35:36,720 --> 00:35:38,640
uh lastly this one is a little bit more

956
00:35:38,640 --> 00:35:40,500
of a stretch goal I guess but if you

957
00:35:40,500 --> 00:35:42,000
have thread until feeds that you

958
00:35:42,000 --> 00:35:43,920
subscribe to or maybe more mature thread

959
00:35:43,920 --> 00:35:46,020
Intel capabilities you can maybe track

960
00:35:46,020 --> 00:35:48,540
the IP or the domain and say hey is this

961
00:35:48,540 --> 00:35:50,640
a so is this associated with the threat

962
00:35:50,640 --> 00:35:52,140
actor that we're tracking have we

963
00:35:52,140 --> 00:35:53,820
already been hit by this you know threat

964
00:35:53,820 --> 00:35:55,560
actor during a different campaign what

965
00:35:55,560 --> 00:35:57,720
do we already know about this so that

966
00:35:57,720 --> 00:35:59,520
can be helpful as well

967
00:35:59,520 --> 00:36:01,619
uh Brian touched on it earlier but the

968
00:36:01,619 --> 00:36:02,940
way that we do this is with our

969
00:36:02,940 --> 00:36:05,220
enrichment architecture so again to add

970
00:36:05,220 --> 00:36:07,260
context into detections when they fire

971
00:36:07,260 --> 00:36:09,480
uh prior to an alert just being created

972
00:36:09,480 --> 00:36:12,060
there's a couple of different a couple

973
00:36:12,060 --> 00:36:14,339
of different steps here but the main

974
00:36:14,339 --> 00:36:15,900
building blocks are the same ones that

975
00:36:15,900 --> 00:36:17,339
we've been demonstrating throughout this

976
00:36:17,339 --> 00:36:19,560
entire application so we have an

977
00:36:19,560 --> 00:36:21,420
enrichment data source again it can be

978
00:36:21,420 --> 00:36:24,180
external it can be internal you got data

979
00:36:24,180 --> 00:36:26,040
that can be useful somewhere and we

980
00:36:26,040 --> 00:36:28,680
query it with a Lambda on a Cron job in

981
00:36:28,680 --> 00:36:31,260
which in AWS is an eventbridge scheduler

982
00:36:31,260 --> 00:36:33,540
so the Lambda will take this data it

983
00:36:33,540 --> 00:36:34,859
will write it to an enrichment table

984
00:36:34,859 --> 00:36:37,380
which in our case is dynamodb the reason

985
00:36:37,380 --> 00:36:38,820
why we chose it is because it's great

986
00:36:38,820 --> 00:36:40,680
for unstructured data it's easy to use

987
00:36:40,680 --> 00:36:43,619
scales has really easy Integrations with

988
00:36:43,619 --> 00:36:45,660
things like soar platforms Etc just

989
00:36:45,660 --> 00:36:46,859
gives you a nice little hook to query

990
00:36:46,859 --> 00:36:49,260
the data Brian actually wrote some

991
00:36:49,260 --> 00:36:50,640
really neat code to do some

992
00:36:50,640 --> 00:36:52,859
deduplication between data that comes

993
00:36:52,859 --> 00:36:54,359
into that enrichment table so making

994
00:36:54,359 --> 00:36:56,160
sure we're not just consistently writing

995
00:36:56,160 --> 00:36:58,260
the same data over and over again we're

996
00:36:58,260 --> 00:37:00,420
only doing updates when there's a change

997
00:37:00,420 --> 00:37:01,980
in whatever enrichment data that we're

998
00:37:01,980 --> 00:37:04,020
recording and some neat sequencing for

999
00:37:04,020 --> 00:37:05,520
different point in time interactions as

1000
00:37:05,520 --> 00:37:06,240
well

1001
00:37:06,240 --> 00:37:09,180
the rest of the steps are kind of to

1002
00:37:09,180 --> 00:37:11,400
convert the data so in our streaming

1003
00:37:11,400 --> 00:37:13,859
alert and SQL Search tool to integrate

1004
00:37:13,859 --> 00:37:15,720
this kind of enrichment capabilities it

1005
00:37:15,720 --> 00:37:18,300
expects the data in a CSV format so we

1006
00:37:18,300 --> 00:37:19,980
have some functionality to take it from

1007
00:37:19,980 --> 00:37:21,720
this kind of source of truth dynamodb

1008
00:37:21,720 --> 00:37:24,420
table convert it into a CSV and feed it

1009
00:37:24,420 --> 00:37:25,859
into our streaming alert and SQL Search

1010
00:37:25,859 --> 00:37:27,240
tool so that we can take advantage of

1011
00:37:27,240 --> 00:37:30,200
this data as well

1012
00:37:30,599 --> 00:37:32,880
so now we're at the investigation phase

1013
00:37:32,880 --> 00:37:35,460
so we had a detection that fired we

1014
00:37:35,460 --> 00:37:37,560
enriched it with some context uh we

1015
00:37:37,560 --> 00:37:39,300
decided to generate an alert we're not

1016
00:37:39,300 --> 00:37:41,099
going to suppress it and then now we're

1017
00:37:41,099 --> 00:37:43,920
at the investigation phase so continuing

1018
00:37:43,920 --> 00:37:46,800
with our guard Duty example we still

1019
00:37:46,800 --> 00:37:48,660
have some questions for how we're going

1020
00:37:48,660 --> 00:37:50,520
to do this investigation so we got some

1021
00:37:50,520 --> 00:37:51,839
more context

1022
00:37:51,839 --> 00:37:53,220
but we still have a list of questions

1023
00:37:53,220 --> 00:37:55,740
that we need to answer and this is more

1024
00:37:55,740 --> 00:37:57,240
of hey if you have an investigation

1025
00:37:57,240 --> 00:37:58,980
that's going to happen most of the time

1026
00:37:58,980 --> 00:38:00,359
depending on the maturity of your

1027
00:38:00,359 --> 00:38:02,220
program what your environments look like

1028
00:38:02,220 --> 00:38:05,280
there could be a lot of uh maybe basic

1029
00:38:05,280 --> 00:38:07,200
things that can be a lot harder than you

1030
00:38:07,200 --> 00:38:09,420
think for instance do you have access to

1031
00:38:09,420 --> 00:38:11,400
this AWS account to come in and take a

1032
00:38:11,400 --> 00:38:14,700
look at this ec2 host fired an alert can

1033
00:38:14,700 --> 00:38:16,680
you even get access to the ec2 do you

1034
00:38:16,680 --> 00:38:19,680
have SSH keys do you have SSM do you

1035
00:38:19,680 --> 00:38:20,940
have some other way of interacting with

1036
00:38:20,940 --> 00:38:23,280
the Box do you even have an IM role that

1037
00:38:23,280 --> 00:38:24,599
you can use to take a snapshot of the

1038
00:38:24,599 --> 00:38:26,940
EBS instance if you want to do forensics

1039
00:38:26,940 --> 00:38:29,820
on the machine in AWS it's not as

1040
00:38:29,820 --> 00:38:30,900
straightforward there's a couple of

1041
00:38:30,900 --> 00:38:32,760
different steps along the way and when

1042
00:38:32,760 --> 00:38:34,560
you kind of want to take a snapshot of

1043
00:38:34,560 --> 00:38:36,300
it you want to also boot it in some sort

1044
00:38:36,300 --> 00:38:37,920
of isolated environment so if there's

1045
00:38:37,920 --> 00:38:39,780
malware running on a host you know we

1046
00:38:39,780 --> 00:38:41,280
just want to take in start running it in

1047
00:38:41,280 --> 00:38:43,020
vpcs inside of your network you probably

1048
00:38:43,020 --> 00:38:44,820
want to create some sort of isolated

1049
00:38:44,820 --> 00:38:46,619
environment maybe you have some

1050
00:38:46,619 --> 00:38:48,300
pre-installed tooling on a workstation

1051
00:38:48,300 --> 00:38:50,520
you can do forensics on uh you know

1052
00:38:50,520 --> 00:38:52,859
pcaps and all that

1053
00:38:52,859 --> 00:38:55,500
so a way that we kind of think about

1054
00:38:55,500 --> 00:38:57,839
this is if we fail to prepare for these

1055
00:38:57,839 --> 00:38:59,760
kinds of Investigations uh we're not

1056
00:38:59,760 --> 00:39:01,560
really going to be supporting our DNR

1057
00:39:01,560 --> 00:39:03,060
Engineers or responders when

1058
00:39:03,060 --> 00:39:05,640
investigations come in so at the

1059
00:39:05,640 --> 00:39:07,500
investigation phase again when a human

1060
00:39:07,500 --> 00:39:09,960
starts to look in alert this is

1061
00:39:09,960 --> 00:39:11,040
typically where you're going to have a

1062
00:39:11,040 --> 00:39:12,780
lot of bottlenecks because it requires

1063
00:39:12,780 --> 00:39:14,820
manual effort again you know you're

1064
00:39:14,820 --> 00:39:16,020
going to have some Automation in place

1065
00:39:16,020 --> 00:39:17,040
but eventually you're going to have

1066
00:39:17,040 --> 00:39:18,599
alerts that are going to come in that

1067
00:39:18,599 --> 00:39:20,099
physically require a human to come in

1068
00:39:20,099 --> 00:39:22,380
and take a look at so what you want the

1069
00:39:22,380 --> 00:39:24,540
responder to be able to do is to answer

1070
00:39:24,540 --> 00:39:25,920
the questions and finish the

1071
00:39:25,920 --> 00:39:28,079
investigation instead of getting bogged

1072
00:39:28,079 --> 00:39:29,820
down and things like how do I get to the

1073
00:39:29,820 --> 00:39:32,460
cc2 how do I get to this account so the

1074
00:39:32,460 --> 00:39:34,140
way that we do it is we try to have a

1075
00:39:34,140 --> 00:39:35,760
mentality of ruthlessly automating

1076
00:39:35,760 --> 00:39:37,560
everything that we possibly can if it

1077
00:39:37,560 --> 00:39:38,940
takes more than a couple minutes or if

1078
00:39:38,940 --> 00:39:40,980
it's something that's painful or hard we

1079
00:39:40,980 --> 00:39:42,660
build out some processes to save time

1080
00:39:42,660 --> 00:39:44,579
and more importantly minimize mistakes

1081
00:39:44,579 --> 00:39:46,560
so really make this kind of conscious

1082
00:39:46,560 --> 00:39:48,480
investment in setting our responders up

1083
00:39:48,480 --> 00:39:51,020
for success

1084
00:39:51,079 --> 00:39:54,300
one way that we do this is uh by

1085
00:39:54,300 --> 00:39:56,579
building a forensic workstation so what

1086
00:39:56,579 --> 00:39:58,980
we do is we Leverage The Amazon's ec2

1087
00:39:58,980 --> 00:40:00,660
image Builder service and we kind of

1088
00:40:00,660 --> 00:40:02,700
create this workstation that any analyst

1089
00:40:02,700 --> 00:40:04,500
or any person on the security team has

1090
00:40:04,500 --> 00:40:06,480
access to in a security forensics

1091
00:40:06,480 --> 00:40:09,540
account so we create this ec2 we boot it

1092
00:40:09,540 --> 00:40:12,119
we create an Ami we load up all the

1093
00:40:12,119 --> 00:40:13,619
software that's necessary to do these

1094
00:40:13,619 --> 00:40:15,420
kinds of forensic workflows then we

1095
00:40:15,420 --> 00:40:16,800
publish it we tag it make it available

1096
00:40:16,800 --> 00:40:19,200
to whatever work streams needed or if

1097
00:40:19,200 --> 00:40:20,640
someone just wants to boot it attach an

1098
00:40:20,640 --> 00:40:24,980
EBS volume and then go to work

1099
00:40:25,320 --> 00:40:27,359
this diagram turned out much better on a

1100
00:40:27,359 --> 00:40:28,920
big screen than I thought so pretty

1101
00:40:28,920 --> 00:40:30,900
stoked about that uh so the other thing

1102
00:40:30,900 --> 00:40:32,460
that we do is we actually decided to

1103
00:40:32,460 --> 00:40:34,380
automate forensic extraction so going

1104
00:40:34,380 --> 00:40:36,119
back to this guard Duty example where we

1105
00:40:36,119 --> 00:40:38,520
have an ec2 that fired you know we want

1106
00:40:38,520 --> 00:40:39,960
to do some sort of research on it answer

1107
00:40:39,960 --> 00:40:41,579
some questions

1108
00:40:41,579 --> 00:40:44,400
um there's a lot of steps in AWS if you

1109
00:40:44,400 --> 00:40:46,619
have an arbitrary account and this alert

1110
00:40:46,619 --> 00:40:48,720
Fires for an arbitrary region you need

1111
00:40:48,720 --> 00:40:50,520
to have a role that can take snapshots

1112
00:40:50,520 --> 00:40:52,800
of these EVS volumes you need to copy

1113
00:40:52,800 --> 00:40:54,540
them to the right region you need to

1114
00:40:54,540 --> 00:40:56,220
encrypt them you need to have the

1115
00:40:56,220 --> 00:40:58,320
appropriate KMS access keys so that your

1116
00:40:58,320 --> 00:41:00,119
security forensics account can decrypt

1117
00:41:00,119 --> 00:41:02,040
the EBS snapshot and then mount it onto

1118
00:41:02,040 --> 00:41:04,320
an ec2 again you want that forensics

1119
00:41:04,320 --> 00:41:05,700
environment so that you can comfortably

1120
00:41:05,700 --> 00:41:08,520
boot up the ec2 and there's just a lot

1121
00:41:08,520 --> 00:41:09,720
of things involved that can take

1122
00:41:09,720 --> 00:41:11,520
honestly a couple hours if you're going

1123
00:41:11,520 --> 00:41:12,780
through it for the first time and you're

1124
00:41:12,780 --> 00:41:14,359
not super familiar with the environment

1125
00:41:14,359 --> 00:41:16,740
so we wrote some tooling around this we

1126
00:41:16,740 --> 00:41:18,540
wrote some scripts to take a process

1127
00:41:18,540 --> 00:41:20,160
down from a couple of hours to a couple

1128
00:41:20,160 --> 00:41:22,320
of minutes where the bottleneck is not

1129
00:41:22,320 --> 00:41:24,660
access to your AWS resources knowledge

1130
00:41:24,660 --> 00:41:25,820
of AWS

1131
00:41:25,820 --> 00:41:28,320
or anything else the bottleneck is just

1132
00:41:28,320 --> 00:41:30,300
how long does it take AWS API calls to

1133
00:41:30,300 --> 00:41:32,339
execute you get a forensic workstation

1134
00:41:32,339 --> 00:41:34,140
booted up with these volumes attached

1135
00:41:34,140 --> 00:41:36,180
you can begin your investigation without

1136
00:41:36,180 --> 00:41:39,119
having to worry about anything else

1137
00:41:39,119 --> 00:41:41,520
uh if you've made it this far uh to the

1138
00:41:41,520 --> 00:41:43,859
response portion uh congrats uh through

1139
00:41:43,859 --> 00:41:45,359
the day through this talk and also

1140
00:41:45,359 --> 00:41:47,339
through your detection response function

1141
00:41:47,339 --> 00:41:49,920
so now actually is where the real work

1142
00:41:49,920 --> 00:41:52,800
begins right so you have to fix whatever

1143
00:41:52,800 --> 00:41:55,079
is wrong you have to assess uh whatever

1144
00:41:55,079 --> 00:41:56,579
issues have come up you have to deal

1145
00:41:56,579 --> 00:41:58,560
with the ramifications of it

1146
00:41:58,560 --> 00:41:59,760
um you have to do the fire drill of

1147
00:41:59,760 --> 00:42:01,079
getting the right stakeholders in place

1148
00:42:01,079 --> 00:42:03,480
following your run books etc etc but

1149
00:42:03,480 --> 00:42:04,920
there's one other really component a

1150
00:42:04,920 --> 00:42:07,140
really important component of the

1151
00:42:07,140 --> 00:42:08,940
response piece that I want to call out

1152
00:42:08,940 --> 00:42:11,160
which is remembering to evaluate your

1153
00:42:11,160 --> 00:42:12,480
detections

1154
00:42:12,480 --> 00:42:15,540
uh this is a detection life cycle uh

1155
00:42:15,540 --> 00:42:17,460
this image is not mine I took it from

1156
00:42:17,460 --> 00:42:19,680
the snowflake blog they have a really

1157
00:42:19,680 --> 00:42:21,359
great article about this which I really

1158
00:42:21,359 --> 00:42:24,180
really liked and so there's a component

1159
00:42:24,180 --> 00:42:26,040
in here which is called monitoring your

1160
00:42:26,040 --> 00:42:27,900
detections so we've already talked about

1161
00:42:27,900 --> 00:42:29,880
the continuous validation phase where we

1162
00:42:29,880 --> 00:42:31,680
have some sort of integrated testing to

1163
00:42:31,680 --> 00:42:33,420
monitor our detections make sure they're

1164
00:42:33,420 --> 00:42:36,060
working and this isn't monitoring in the

1165
00:42:36,060 --> 00:42:37,260
sense of hey I want to see how

1166
00:42:37,260 --> 00:42:39,480
performant my code is running what the

1167
00:42:39,480 --> 00:42:41,940
latency is this monitoring is from the

1168
00:42:41,940 --> 00:42:43,680
context of how effective is this

1169
00:42:43,680 --> 00:42:46,200
detection how noisy is it are there

1170
00:42:46,200 --> 00:42:47,700
additional signals that we could have

1171
00:42:47,700 --> 00:42:49,380
inserted into the original detection

1172
00:42:49,380 --> 00:42:50,940
that could have saved a responder time

1173
00:42:50,940 --> 00:42:53,880
later in the investigation so even after

1174
00:42:53,880 --> 00:42:55,380
the response phase kind of finishes

1175
00:42:55,380 --> 00:42:57,300
Brian alluded to it earlier at the start

1176
00:42:57,300 --> 00:42:59,280
of the talk in the overview but you

1177
00:42:59,280 --> 00:43:00,359
really want to make sure that you kind

1178
00:43:00,359 --> 00:43:02,339
of go back and maintain these detections

1179
00:43:02,339 --> 00:43:04,140
as you would maintain a software service

1180
00:43:04,140 --> 00:43:06,839
over time as it's running

1181
00:43:06,839 --> 00:43:09,480
so Brian and I spend a lot of time on

1182
00:43:09,480 --> 00:43:11,280
this uh and we're pretty proud of what

1183
00:43:11,280 --> 00:43:13,140
we built uh but we also had some

1184
00:43:13,140 --> 00:43:14,520
learning some things that went well some

1185
00:43:14,520 --> 00:43:15,900
things that didn't go well that we kind

1186
00:43:15,900 --> 00:43:17,700
of wanted to share uh some things that

1187
00:43:17,700 --> 00:43:19,260
have been really successful for us are

1188
00:43:19,260 --> 00:43:21,240
automating onboarding of new data

1189
00:43:21,240 --> 00:43:23,220
sources so just making things as fast as

1190
00:43:23,220 --> 00:43:25,560
possible so folks who aren't necessarily

1191
00:43:25,560 --> 00:43:27,240
as comfortable with terraform maybe as

1192
00:43:27,240 --> 00:43:28,920
comfortable with python just don't have

1193
00:43:28,920 --> 00:43:30,599
time to do it we don't want to be

1194
00:43:30,599 --> 00:43:32,339
blockers for them to onboard new data

1195
00:43:32,339 --> 00:43:33,720
into the pipeline for their own use

1196
00:43:33,720 --> 00:43:36,420
cases scale has been great again taking

1197
00:43:36,420 --> 00:43:38,220
a cloud native approach lets us scale

1198
00:43:38,220 --> 00:43:41,040
really well very cost effectively uh

1199
00:43:41,040 --> 00:43:43,160
notably with the Lambda and the sqs

1200
00:43:43,160 --> 00:43:45,060
following the detections is code

1201
00:43:45,060 --> 00:43:46,859
Paradigm and taking software engineering

1202
00:43:46,859 --> 00:43:48,780
and applying it to our DNR function has

1203
00:43:48,780 --> 00:43:50,760
resulted in pretty well tested and high

1204
00:43:50,760 --> 00:43:52,859
quality detections where we have very

1205
00:43:52,859 --> 00:43:54,240
high confidence by the timeline

1206
00:43:54,240 --> 00:43:55,800
detection ends up in production that

1207
00:43:55,800 --> 00:43:57,119
it's going to run something is

1208
00:43:57,119 --> 00:43:59,640
continuously checking looking after it

1209
00:43:59,640 --> 00:44:01,740
um and yeah overall just building a lot

1210
00:44:01,740 --> 00:44:04,200
of confidence and enrichment system is

1211
00:44:04,200 --> 00:44:06,180
something that's been great for us just

1212
00:44:06,180 --> 00:44:07,920
adding that content next helping us get

1213
00:44:07,920 --> 00:44:09,900
through investigations faster and able

1214
00:44:09,900 --> 00:44:11,460
to prioritize things that come into the

1215
00:44:11,460 --> 00:44:13,500
queue

1216
00:44:13,500 --> 00:44:15,780
some things that uh we definitely want

1217
00:44:15,780 --> 00:44:17,880
to improve on over time is empowering

1218
00:44:17,880 --> 00:44:19,680
folks again I outlined this as a

1219
00:44:19,680 --> 00:44:21,900
positive but it's also still a work of

1220
00:44:21,900 --> 00:44:23,819
improvement or area of improvement that

1221
00:44:23,819 --> 00:44:26,520
we can work on is uh onboarding new data

1222
00:44:26,520 --> 00:44:28,020
can still be sometimes depending on the

1223
00:44:28,020 --> 00:44:30,540
data source a high touch process so we

1224
00:44:30,540 --> 00:44:32,640
want to make this as self-service as

1225
00:44:32,640 --> 00:44:34,380
possible so there's some continuous

1226
00:44:34,380 --> 00:44:36,180
improvements we can do to make

1227
00:44:36,180 --> 00:44:38,880
onboarding data even easier I want to

1228
00:44:38,880 --> 00:44:41,160
continuously improve our CI CD story and

1229
00:44:41,160 --> 00:44:43,980
this is mostly for me uh but uh have hit

1230
00:44:43,980 --> 00:44:46,859
some parsing struggles I wrote a regex

1231
00:44:46,859 --> 00:44:49,619
which wasn't great some time back and so

1232
00:44:49,619 --> 00:44:51,060
just making sure that when we do those

1233
00:44:51,060 --> 00:44:53,339
things and we Wrangle a log sources that

1234
00:44:53,339 --> 00:44:55,380
are a little bit Messier we take the

1235
00:44:55,380 --> 00:44:57,000
time to kind of do it and flush it out

1236
00:44:57,000 --> 00:44:58,440
as much as we can before deploying it

1237
00:44:58,440 --> 00:45:00,180
anywhere

1238
00:45:00,180 --> 00:45:03,300
since we've built this system and have

1239
00:45:03,300 --> 00:45:05,460
the opportunity to kind of present this

1240
00:45:05,460 --> 00:45:06,720
there's been a couple of other

1241
00:45:06,720 --> 00:45:08,579
alternative approaches that have come

1242
00:45:08,579 --> 00:45:10,200
out or some other systems that have come

1243
00:45:10,200 --> 00:45:13,020
out we haven't used any of them they all

1244
00:45:13,020 --> 00:45:14,640
seem really cool this is not a vote of

1245
00:45:14,640 --> 00:45:16,020
confidence for any of them just

1246
00:45:16,020 --> 00:45:17,400
something that we would look at if we

1247
00:45:17,400 --> 00:45:19,680
were to rebuild the system from scratch

1248
00:45:19,680 --> 00:45:22,260
again AWS has an offering it's called

1249
00:45:22,260 --> 00:45:24,720
AWS security Lake its goal is to

1250
00:45:24,720 --> 00:45:26,700
centralize a bunch of data marketing is

1251
00:45:26,700 --> 00:45:28,440
really really good what they do is they

1252
00:45:28,440 --> 00:45:31,440
leverage ocsf one of these security

1253
00:45:31,440 --> 00:45:33,060
schemas that Brian called out at the

1254
00:45:33,060 --> 00:45:34,500
start of the talk opened cyber security

1255
00:45:34,500 --> 00:45:36,660
schema framework to help out with the

1256
00:45:36,660 --> 00:45:39,060
log normalization purpose uh per log

1257
00:45:39,060 --> 00:45:41,880
normalization aspect of it so you have

1258
00:45:41,880 --> 00:45:44,700
this nice kind of centralized data Lake

1259
00:45:44,700 --> 00:45:46,800
if you want to use that term where you

1260
00:45:46,800 --> 00:45:49,560
have a schema in place uh matano not

1261
00:45:49,560 --> 00:45:51,119
sure if I'm pronouncing that right this

1262
00:45:51,119 --> 00:45:53,160
is open source version of it and also a

1263
00:45:53,160 --> 00:45:55,440
detection engine on top of that so it's

1264
00:45:55,440 --> 00:45:57,180
really really neat if you look at the

1265
00:45:57,180 --> 00:45:58,859
GitHub they have architecture diagrams

1266
00:45:58,859 --> 00:46:00,900
which look similar to what we've built

1267
00:46:00,900 --> 00:46:02,400
so that was really nice validation for

1268
00:46:02,400 --> 00:46:04,319
us in terms of the approach that we kind

1269
00:46:04,319 --> 00:46:06,180
of pursued and they leveraged the

1270
00:46:06,180 --> 00:46:08,460
elastic schema elastic common schema for

1271
00:46:08,460 --> 00:46:10,440
their log normalization and they do

1272
00:46:10,440 --> 00:46:11,880
something really cool they leverage a

1273
00:46:11,880 --> 00:46:13,800
vector remap language to normalize logs

1274
00:46:13,800 --> 00:46:15,720
so kind of taking that compute portion

1275
00:46:15,720 --> 00:46:17,940
and the normalization it's a really neat

1276
00:46:17,940 --> 00:46:19,680
way to do it so highly recommend

1277
00:46:19,680 --> 00:46:21,119
checking that out if you're early on in

1278
00:46:21,119 --> 00:46:23,520
your journey lastly this is more on the

1279
00:46:23,520 --> 00:46:25,380
detection validation side but datadog

1280
00:46:25,380 --> 00:46:27,720
released a tool called threat test it

1281
00:46:27,720 --> 00:46:29,579
also integrates really nicely with their

1282
00:46:29,579 --> 00:46:31,260
like attack stimulation platform where

1283
00:46:31,260 --> 00:46:33,359
we can fire up uh detection that's

1284
00:46:33,359 --> 00:46:36,060
mapped to miter you can execute it and

1285
00:46:36,060 --> 00:46:37,560
then you can use this thread test

1286
00:46:37,560 --> 00:46:39,359
framework to make sure that alerts fired

1287
00:46:39,359 --> 00:46:41,460
on this specific workflow that you

1288
00:46:41,460 --> 00:46:45,380
kicked off seems really really neat

1289
00:46:45,839 --> 00:46:48,900
so to tie it all together we started off

1290
00:46:48,900 --> 00:46:51,000
with a need to go from a raw log to

1291
00:46:51,000 --> 00:46:53,520
making a security decision and it sounds

1292
00:46:53,520 --> 00:46:56,400
pretty simple and it is at lower scales

1293
00:46:56,400 --> 00:46:57,960
but as you get more data sources more

1294
00:46:57,960 --> 00:46:59,520
scale it can start to get a little bit

1295
00:46:59,520 --> 00:47:01,440
more complicated so we walked you

1296
00:47:01,440 --> 00:47:02,460
through kind of the six different

1297
00:47:02,460 --> 00:47:03,900
phrases you have the raw data

1298
00:47:03,900 --> 00:47:06,000
normalization detections alerts

1299
00:47:06,000 --> 00:47:08,520
investigations response there's a lot of

1300
00:47:08,520 --> 00:47:09,780
different work streams that you can take

1301
00:47:09,780 --> 00:47:11,220
along the way to kind of make this

1302
00:47:11,220 --> 00:47:13,380
process as high quality and as useful as

1303
00:47:13,380 --> 00:47:15,540
possible for the detection response side

1304
00:47:15,540 --> 00:47:17,760
and for the business as well so we

1305
00:47:17,760 --> 00:47:19,500
definitely didn't cover everything but

1306
00:47:19,500 --> 00:47:20,700
hopefully this can be a helpful

1307
00:47:20,700 --> 00:47:23,040
blueprint for building out a scalable

1308
00:47:23,040 --> 00:47:24,660
and cost effective threat detection

1309
00:47:24,660 --> 00:47:27,420
platform with a high bar for quality and

1310
00:47:27,420 --> 00:47:28,560
reliability

1311
00:47:28,560 --> 00:47:30,119
and with that I'd like to thank you all

1312
00:47:30,119 --> 00:47:31,740
for your time for coming here and would

1313
00:47:31,740 --> 00:47:32,819
like to open up the floor to any

1314
00:47:32,819 --> 00:47:35,060
questions

1315
00:47:35,120 --> 00:47:41,880
[Applause]

1316
00:47:43,319 --> 00:47:46,339
not a question yeah

1317
00:47:48,359 --> 00:47:50,160
uh the question was what detection

1318
00:47:50,160 --> 00:47:51,780
engine did we use

1319
00:47:51,780 --> 00:47:52,500
um

1320
00:47:52,500 --> 00:47:54,720
we we purposefully left that out because

1321
00:47:54,720 --> 00:47:56,940
we didn't want to endorse or turn this

1322
00:47:56,940 --> 00:47:58,980
into a vendor talk I would be happy to

1323
00:47:58,980 --> 00:48:01,140
answer that literally uh right right

1324
00:48:01,140 --> 00:48:03,740
after this

1325
00:48:05,819 --> 00:48:09,380
uh next question yeah

1326
00:48:19,640 --> 00:48:21,300
so

1327
00:48:21,300 --> 00:48:24,000
um right now we are kind of peaking in

1328
00:48:24,000 --> 00:48:26,460
the tens of thousands of events per

1329
00:48:26,460 --> 00:48:30,119
second which is not that big but we do

1330
00:48:30,119 --> 00:48:32,460
have every expectation that this could

1331
00:48:32,460 --> 00:48:34,680
grow to you know millions of events per

1332
00:48:34,680 --> 00:48:37,640
second if we needed to

1333
00:48:39,960 --> 00:48:42,380
question

1334
00:48:43,740 --> 00:48:45,660
the question was any plans to open

1335
00:48:45,660 --> 00:48:47,339
source any part of it

1336
00:48:47,339 --> 00:48:48,599
um

1337
00:48:48,599 --> 00:48:50,940
I have to ask the boss I think on that

1338
00:48:50,940 --> 00:48:54,359
one uh at this time not that I know of

1339
00:48:54,359 --> 00:48:55,980
so

1340
00:48:55,980 --> 00:48:58,020
um there's not any open source yet but

1341
00:48:58,020 --> 00:49:00,500
there is I did publish a blog post on

1342
00:49:00,500 --> 00:49:02,160
thebenchling.engineering site which is

1343
00:49:02,160 --> 00:49:03,540
our blog

1344
00:49:03,540 --> 00:49:06,359
um which does cover a very tiny piece of

1345
00:49:06,359 --> 00:49:10,579
what we used which is a jwks exporter

1346
00:49:10,579 --> 00:49:15,000
for KMS asymmetric keys so again just

1347
00:49:15,000 --> 00:49:17,339
just as kind of like a breadcrumb of

1348
00:49:17,339 --> 00:49:19,500
we're not entirely opposed to it but

1349
00:49:19,500 --> 00:49:23,240
we're just kind of Dipping our toe

1350
00:49:24,000 --> 00:49:27,260
yeah I have another question

1351
00:49:29,060 --> 00:49:31,500
uh don't see any other questions uh but

1352
00:49:31,500 --> 00:49:32,819
Brian and I will stick around right

1353
00:49:32,819 --> 00:49:34,319
after so thank you very much again for

1354
00:49:34,319 --> 00:49:35,819
coming to our talk I hope everyone has a

1355
00:49:35,819 --> 00:49:37,560
great weekend great conference thank you

1356
00:49:37,560 --> 00:49:40,279
thanks everyone

