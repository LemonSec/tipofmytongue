1
00:00:00,000 --> 00:00:01,500
hi everybody

2
00:00:01,500 --> 00:00:03,240
um a pleasure to meet you all and thanks

3
00:00:03,240 --> 00:00:05,700
for coming out a great turnout

4
00:00:05,700 --> 00:00:07,980
um my name is Owen wickens I'm a

5
00:00:07,980 --> 00:00:10,019
researcher sorry and this is Marta Janus

6
00:00:10,019 --> 00:00:13,160
we're both researchers in Hidden layers

7
00:00:13,160 --> 00:00:16,320
basically ml research team so we look at

8
00:00:16,320 --> 00:00:17,880
ways of

9
00:00:17,880 --> 00:00:19,500
um securing artificial intelligence

10
00:00:19,500 --> 00:00:21,119
systems but part of that is also looking

11
00:00:21,119 --> 00:00:23,820
at how AI systems can be attacked so

12
00:00:23,820 --> 00:00:25,619
we're going to talk about the risks that

13
00:00:25,619 --> 00:00:27,720
AI systems face how these systems can be

14
00:00:27,720 --> 00:00:30,180
attacked and for what purpose

15
00:00:30,180 --> 00:00:32,759
so after a brief introduction to why AI

16
00:00:32,759 --> 00:00:34,980
is so important and how ubiquitous it's

17
00:00:34,980 --> 00:00:37,200
become well look at the who the why the

18
00:00:37,200 --> 00:00:39,000
how and then explain the classes of

19
00:00:39,000 --> 00:00:40,440
attack focusing on some real world

20
00:00:40,440 --> 00:00:42,480
applications examples and mitigation

21
00:00:42,480 --> 00:00:44,280
strategies

22
00:00:44,280 --> 00:00:46,379
so the last few months have been you

23
00:00:46,379 --> 00:00:47,879
know nothing short of revolutionary I

24
00:00:47,879 --> 00:00:49,379
mean everybody's everybody's heard of

25
00:00:49,379 --> 00:00:52,079
chat GPT now my uh my grandparents are

26
00:00:52,079 --> 00:00:53,820
asking me about chat GPT and when that

27
00:00:53,820 --> 00:00:55,140
starts happening when they know what

28
00:00:55,140 --> 00:00:56,699
you're looking at at work that's kind of

29
00:00:56,699 --> 00:00:59,760
a big deal so you know chat gbt has

30
00:00:59,760 --> 00:01:01,140
really taken the World by storm in a lot

31
00:01:01,140 --> 00:01:02,100
of ways but we're not really going to

32
00:01:02,100 --> 00:01:03,660
talk about that today but you know it's

33
00:01:03,660 --> 00:01:05,519
just important to kind of highlight how

34
00:01:05,519 --> 00:01:07,439
AI is becoming so present and prevalent

35
00:01:07,439 --> 00:01:09,540
within our lives and you know Microsoft

36
00:01:09,540 --> 00:01:11,700
has Bing Google has borrowed albeit to

37
00:01:11,700 --> 00:01:14,340
varying levels of success but you know I

38
00:01:14,340 --> 00:01:16,140
think it's taken us all by surprise at

39
00:01:16,140 --> 00:01:18,180
how quickly this has become part of the

40
00:01:18,180 --> 00:01:20,880
the mainstream but it's not just llms

41
00:01:20,880 --> 00:01:22,799
it's not just chat gbt and the likes and

42
00:01:22,799 --> 00:01:24,840
it's also you know specialized in image

43
00:01:24,840 --> 00:01:27,299
generation models you know such as Dolly

44
00:01:27,299 --> 00:01:29,220
stable diffusion mid-journey which have

45
00:01:29,220 --> 00:01:30,900
been redefining the creative sphere and

46
00:01:30,900 --> 00:01:32,939
you know allowing people with sentences

47
00:01:32,939 --> 00:01:35,759
to create incredible scenes and and so

48
00:01:35,759 --> 00:01:37,560
on and you know getting into some hot

49
00:01:37,560 --> 00:01:38,939
water with you know legal copyright

50
00:01:38,939 --> 00:01:40,860
issues at the same time but you know

51
00:01:40,860 --> 00:01:42,299
it's it's easy to see that it's becoming

52
00:01:42,299 --> 00:01:44,220
the Zeitgeist of the decade

53
00:01:44,220 --> 00:01:46,079
so you know it can also be immensely

54
00:01:46,079 --> 00:01:47,460
helpful in both science and medical

55
00:01:47,460 --> 00:01:49,820
applications and such as drug Discovery

56
00:01:49,820 --> 00:01:52,740
mathematics astronomy Medical Imaging

57
00:01:52,740 --> 00:01:54,119
and like pretty much anything that you

58
00:01:54,119 --> 00:01:55,920
can think of and hopefully it'll lead us

59
00:01:55,920 --> 00:01:57,360
to many more discoveries and

60
00:01:57,360 --> 00:01:59,820
breakthroughs but and so these here are

61
00:01:59,820 --> 00:02:01,619
just some basically recent headlines

62
00:02:01,619 --> 00:02:03,299
that we've that we found but in fact

63
00:02:03,299 --> 00:02:05,040
ai's actually become part of our Lives

64
00:02:05,040 --> 00:02:06,719
much before that as well

65
00:02:06,719 --> 00:02:09,179
you know like it's been used in like

66
00:02:09,179 --> 00:02:10,679
more kind of like evident applications

67
00:02:10,679 --> 00:02:13,260
such as self-driving cars obviously and

68
00:02:13,260 --> 00:02:15,060
you know cyber security for things like

69
00:02:15,060 --> 00:02:17,879
spam malware intrusion detection and

70
00:02:17,879 --> 00:02:20,099
also in applications like biometric

71
00:02:20,099 --> 00:02:22,379
authentication you know your phone uh

72
00:02:22,379 --> 00:02:24,780
face ID you know e-commerce Financial

73
00:02:24,780 --> 00:02:27,239
forecasting even when you apply for a

74
00:02:27,239 --> 00:02:29,520
loan like it's all the documents you

75
00:02:29,520 --> 00:02:31,500
send in are being approved by a machine

76
00:02:31,500 --> 00:02:33,540
and it's like quite interesting and

77
00:02:33,540 --> 00:02:35,400
amazing to be honest that you know ml is

78
00:02:35,400 --> 00:02:37,140
so much power in our lives that it

79
00:02:37,140 --> 00:02:39,420
influences nearly like I'd say a lot of

80
00:02:39,420 --> 00:02:41,700
the decisions that are made about us and

81
00:02:41,700 --> 00:02:43,379
for us during during the course of our

82
00:02:43,379 --> 00:02:44,160
day

83
00:02:44,160 --> 00:02:46,379
but with great power outcomes great

84
00:02:46,379 --> 00:02:48,180
responsibility and we'll dive into that

85
00:02:48,180 --> 00:02:49,140
later

86
00:02:49,140 --> 00:02:50,940
so first we're going to take a brief

87
00:02:50,940 --> 00:02:52,860
look at how AI Works under the hood and

88
00:02:52,860 --> 00:02:56,360
for that I'll pass you to Marta

89
00:03:05,940 --> 00:03:09,599
all right let's start with um

90
00:03:09,599 --> 00:03:12,239
just some basic terminology that we will

91
00:03:12,239 --> 00:03:13,980
use through this presentation just to

92
00:03:13,980 --> 00:03:15,659
make sure that everybody is on the same

93
00:03:15,659 --> 00:03:18,540
page so um sometimes there is a bit of

94
00:03:18,540 --> 00:03:20,099
confusion between artificial

95
00:03:20,099 --> 00:03:21,780
intelligence and machine learning some

96
00:03:21,780 --> 00:03:23,720
people use those terms

97
00:03:23,720 --> 00:03:26,459
inter-exchangeably there is a slight

98
00:03:26,459 --> 00:03:28,080
difference though artificial

99
00:03:28,080 --> 00:03:30,599
intelligence is a more generic term

100
00:03:30,599 --> 00:03:35,700
that um describes any system that has a

101
00:03:35,700 --> 00:03:40,080
the capacity of Performing some actions

102
00:03:40,080 --> 00:03:43,620
that human perform or in other words it

103
00:03:43,620 --> 00:03:45,720
mimics human intelligence or human

104
00:03:45,720 --> 00:03:49,200
behavior now machine learning is the

105
00:03:49,200 --> 00:03:51,780
technique that a modern artificial

106
00:03:51,780 --> 00:03:54,959
intelligence uses to learn from the data

107
00:03:54,959 --> 00:03:57,140
and to improve itself

108
00:03:57,140 --> 00:04:00,180
at the core of each machine learning

109
00:04:00,180 --> 00:04:02,340
solution lies something that we call

110
00:04:02,340 --> 00:04:04,560
machine learning model which is a

111
00:04:04,560 --> 00:04:08,040
basically a decision making system that

112
00:04:08,040 --> 00:04:11,900
is responsible for reading the input and

113
00:04:11,900 --> 00:04:16,079
producing an output so a prediction or

114
00:04:16,079 --> 00:04:19,199
whatever this machine learning model is

115
00:04:19,199 --> 00:04:21,978
providing

116
00:04:23,660 --> 00:04:26,940
sorry for that

117
00:04:26,940 --> 00:04:29,820
all right thank you

118
00:04:29,820 --> 00:04:30,540
um

119
00:04:30,540 --> 00:04:33,300
so um machine learning model is produced

120
00:04:33,300 --> 00:04:34,620
in

121
00:04:34,620 --> 00:04:37,740
the process called training so before

122
00:04:37,740 --> 00:04:41,220
before it can be used it has to be

123
00:04:41,220 --> 00:04:44,540
trained it's basically a result of

124
00:04:44,540 --> 00:04:48,419
running a large amount of training data

125
00:04:48,419 --> 00:04:51,440
through some complex mathematical

126
00:04:51,440 --> 00:04:55,080
algorithms and

127
00:04:55,080 --> 00:04:57,900
after the training which sometimes takes

128
00:04:57,900 --> 00:05:01,680
more than one attempt so the model has

129
00:05:01,680 --> 00:05:03,540
to be retrained with different

130
00:05:03,540 --> 00:05:07,380
parameters in order to actually be more

131
00:05:07,380 --> 00:05:09,840
accurate after that process we have

132
00:05:09,840 --> 00:05:11,460
something that we call the train model

133
00:05:11,460 --> 00:05:14,580
which can be then put into production in

134
00:05:14,580 --> 00:05:17,520
other words it can be made available to

135
00:05:17,520 --> 00:05:21,000
the end user that is made through an UI

136
00:05:21,000 --> 00:05:24,600
or an API or any any kind of access that

137
00:05:24,600 --> 00:05:28,020
let's the user query the model and

138
00:05:28,020 --> 00:05:31,440
perceived the predictions the input that

139
00:05:31,440 --> 00:05:34,560
the model takes is can be anything

140
00:05:34,560 --> 00:05:36,900
really can be an image can be a video

141
00:05:36,900 --> 00:05:39,900
can be app a binary data or more

142
00:05:39,900 --> 00:05:43,979
recently a prompt a text prompt like for

143
00:05:43,979 --> 00:05:48,960
chat GPT for example and then

144
00:05:48,960 --> 00:05:51,180
this data is processed by the machine

145
00:05:51,180 --> 00:05:54,360
learning model to in order to produce an

146
00:05:54,360 --> 00:05:57,060
output which can be a classification a

147
00:05:57,060 --> 00:06:00,840
prediction a real uh

148
00:06:00,840 --> 00:06:06,000
real value number or or a text or an

149
00:06:06,000 --> 00:06:08,759
image like in case of large language

150
00:06:08,759 --> 00:06:11,039
models it's going to be a text in case

151
00:06:11,039 --> 00:06:12,660
of

152
00:06:12,660 --> 00:06:14,820
image generation model is going to be an

153
00:06:14,820 --> 00:06:17,880
image so that's basically how it works

154
00:06:17,880 --> 00:06:21,900
it's a little bit like a human brain

155
00:06:21,900 --> 00:06:24,300
and

156
00:06:24,300 --> 00:06:27,720
most of them common complex machine

157
00:06:27,720 --> 00:06:30,120
learning Solutions use a technique

158
00:06:30,120 --> 00:06:31,919
called Deep learning there are other

159
00:06:31,919 --> 00:06:33,539
types of models as well but we're not

160
00:06:33,539 --> 00:06:36,120
gonna dwelled into it we just want to

161
00:06:36,120 --> 00:06:37,160
um

162
00:06:37,160 --> 00:06:39,360
mention something that we will be

163
00:06:39,360 --> 00:06:41,940
mentioning later on

164
00:06:41,940 --> 00:06:46,020
so um deep learning models are basically

165
00:06:46,020 --> 00:06:48,720
made of layers of neurons and other

166
00:06:48,720 --> 00:06:51,600
reference to human brain and each model

167
00:06:51,600 --> 00:06:54,000
contains an input layer with which

168
00:06:54,000 --> 00:06:56,220
receives the input from the user it's

169
00:06:56,220 --> 00:06:58,139
not the exact input that the user

170
00:06:58,139 --> 00:07:00,240
provides it's a vectorized input so it's

171
00:07:00,240 --> 00:07:02,280
a basically an image translated into

172
00:07:02,280 --> 00:07:05,819
some floating Point values or a text but

173
00:07:05,819 --> 00:07:07,680
is not a text is text translated to

174
00:07:07,680 --> 00:07:09,479
those floating Point values that are

175
00:07:09,479 --> 00:07:12,120
understandable to our model that's the

176
00:07:12,120 --> 00:07:15,900
input layer each um

177
00:07:15,900 --> 00:07:18,900
a neural network model also contains an

178
00:07:18,900 --> 00:07:21,599
output layer which produces the actual

179
00:07:21,599 --> 00:07:24,720
prediction or the output and in between

180
00:07:24,720 --> 00:07:27,660
there is a various number of so-called

181
00:07:27,660 --> 00:07:29,900
hidden layers which are

182
00:07:29,900 --> 00:07:33,120
responsible for the processing of of the

183
00:07:33,120 --> 00:07:35,340
information so all that magic takes

184
00:07:35,340 --> 00:07:38,520
place there

185
00:07:39,240 --> 00:07:42,240
um now um this technology is really

186
00:07:42,240 --> 00:07:43,319
powerful

187
00:07:43,319 --> 00:07:45,900
it's a very impactful and it's very

188
00:07:45,900 --> 00:07:48,419
useful but also it can be used against

189
00:07:48,419 --> 00:07:52,800
us it can be exploited it can be misused

190
00:07:52,800 --> 00:07:55,380
or it can be used for malicious purposes

191
00:07:55,380 --> 00:08:00,060
so why why people could would like to

192
00:08:00,060 --> 00:08:03,599
abuse this technology and how

193
00:08:03,599 --> 00:08:06,120
uh well um

194
00:08:06,120 --> 00:08:09,720
the answer to the question who could

195
00:08:09,720 --> 00:08:12,780
abuse it is simple we have our usual

196
00:08:12,780 --> 00:08:15,180
suspects here first of all cyber

197
00:08:15,180 --> 00:08:17,759
criminals who are actually attacking

198
00:08:17,759 --> 00:08:20,479
machine learning models not since today

199
00:08:20,479 --> 00:08:23,400
it has been a good couple of years a

200
00:08:23,400 --> 00:08:24,620
good few years

201
00:08:24,620 --> 00:08:27,840
since uh first machine learning

202
00:08:27,840 --> 00:08:30,840
Solutions appeared on the market where

203
00:08:30,840 --> 00:08:32,820
cyber criminals were already actively

204
00:08:32,820 --> 00:08:36,059
trying to mainly evade them so like for

205
00:08:36,059 --> 00:08:39,539
example spam detection models were one

206
00:08:39,539 --> 00:08:42,240
of the first to use machine learning in

207
00:08:42,240 --> 00:08:44,580
in the space of security and the Cyber

208
00:08:44,580 --> 00:08:46,920
criminals circumvented them pretty

209
00:08:46,920 --> 00:08:48,480
quickly that's actually an attack

210
00:08:48,480 --> 00:08:50,720
against a machine learning model so

211
00:08:50,720 --> 00:08:54,240
trying to evade it like away detection

212
00:08:54,240 --> 00:08:57,420
of spam or malware that's an attack and

213
00:08:57,420 --> 00:09:00,300
we have it going on for many years

214
00:09:00,300 --> 00:09:03,600
already other other people who want

215
00:09:03,600 --> 00:09:07,080
might want to exploit it are competitors

216
00:09:07,080 --> 00:09:11,220
so some people who don't want to spend

217
00:09:11,220 --> 00:09:13,080
time on my or money on training their

218
00:09:13,080 --> 00:09:16,260
own models they they can actually try to

219
00:09:16,260 --> 00:09:19,740
steal the model from their competitors

220
00:09:19,740 --> 00:09:21,360
in order to

221
00:09:21,360 --> 00:09:23,399
get a cheap advantage

222
00:09:23,399 --> 00:09:25,440
and on top of that we have also

223
00:09:25,440 --> 00:09:27,300
sophisticated actors

224
00:09:27,300 --> 00:09:30,000
for example nation states that might use

225
00:09:30,000 --> 00:09:32,820
machine learning models for their

226
00:09:32,820 --> 00:09:35,760
nefarious purposes like misinformation

227
00:09:35,760 --> 00:09:38,760
like

228
00:09:38,760 --> 00:09:40,640
um

229
00:09:40,640 --> 00:09:43,080
manipulation of

230
00:09:43,080 --> 00:09:47,300
of the public opinion and so on

231
00:09:48,360 --> 00:09:49,500
um

232
00:09:49,500 --> 00:09:51,839
the attackers can have various access to

233
00:09:51,839 --> 00:09:54,620
to the model itself

234
00:09:54,620 --> 00:09:58,260
so in in case where the attackers have

235
00:09:58,260 --> 00:10:00,480
full access to the model that they

236
00:10:00,480 --> 00:10:02,000
attack

237
00:10:02,000 --> 00:10:04,800
including the training data and the

238
00:10:04,800 --> 00:10:07,620
parameters the model was trained with we

239
00:10:07,620 --> 00:10:10,260
are talking about wirebox attack

240
00:10:10,260 --> 00:10:12,660
um now this doesn't really happen often

241
00:10:12,660 --> 00:10:15,000
in the wild it's something that belongs

242
00:10:15,000 --> 00:10:16,920
to the sphere of academic research

243
00:10:16,920 --> 00:10:18,660
mainly

244
00:10:18,660 --> 00:10:21,660
and yes it's it's difficult to imagine

245
00:10:21,660 --> 00:10:23,339
attackers having this kind of

246
00:10:23,339 --> 00:10:26,120
information that is really sensitive and

247
00:10:26,120 --> 00:10:28,980
presumably really well protected

248
00:10:28,980 --> 00:10:31,980
but it can happen in case for example of

249
00:10:31,980 --> 00:10:35,180
Insider threat or um third-party

250
00:10:35,180 --> 00:10:38,100
third-party contractor who was tasked

251
00:10:38,100 --> 00:10:40,440
with training the model this third-party

252
00:10:40,440 --> 00:10:43,140
contractor can be actually malicious and

253
00:10:43,140 --> 00:10:46,140
and gain access to this information the

254
00:10:46,140 --> 00:10:48,600
attackers can also gain access to some

255
00:10:48,600 --> 00:10:50,820
of the information doing open source

256
00:10:50,820 --> 00:10:52,579
research or

257
00:10:52,579 --> 00:10:59,000
by doing a traditional security breach

258
00:10:59,720 --> 00:11:02,279
and there are so there is some tooling

259
00:11:02,279 --> 00:11:03,959
that the attackers can use on yeah

260
00:11:03,959 --> 00:11:05,880
cheers so

261
00:11:05,880 --> 00:11:08,160
um adversarial ml is like largely been

262
00:11:08,160 --> 00:11:09,660
within the realm of Academia for some

263
00:11:09,660 --> 00:11:11,160
time now I think papers started first

264
00:11:11,160 --> 00:11:12,720
coming on the scene back in the early

265
00:11:12,720 --> 00:11:15,720
2010s and I think the pre-print store

266
00:11:15,720 --> 00:11:18,240
not store but the pre-print repository

267
00:11:18,240 --> 00:11:20,760
archive has or active if you pronounce

268
00:11:20,760 --> 00:11:21,899
it

269
00:11:21,899 --> 00:11:24,180
um is I think up to about 4 000

270
00:11:24,180 --> 00:11:26,279
adversarial ml papers now so it's it's

271
00:11:26,279 --> 00:11:28,980
expanded but while it might seem that

272
00:11:28,980 --> 00:11:31,560
attacking ml models requires a PhD in

273
00:11:31,560 --> 00:11:34,560
data science or Advanced statistics or

274
00:11:34,560 --> 00:11:36,600
something of the case it's it's largely

275
00:11:36,600 --> 00:11:38,279
not the case anymore and this is largely

276
00:11:38,279 --> 00:11:40,560
thanks to the many free available attack

277
00:11:40,560 --> 00:11:42,899
Frameworks which can be used as you know

278
00:11:42,899 --> 00:11:45,660
pen testing and evaluation tools also or

279
00:11:45,660 --> 00:11:47,700
primarily sorry that have been released

280
00:11:47,700 --> 00:11:49,920
over the past couple years so some of

281
00:11:49,920 --> 00:11:51,959
the tools that we show here um Implement

282
00:11:51,959 --> 00:11:54,480
these research level attacks like IBM's

283
00:11:54,480 --> 00:11:56,640
adversarial robustness toolbox others

284
00:11:56,640 --> 00:11:59,040
then act as abstraction layers upon

285
00:11:59,040 --> 00:12:02,220
things like uh like like art

286
00:12:02,220 --> 00:12:04,019
um which allow kind of like an ease of

287
00:12:04,019 --> 00:12:06,120
use so you can make it a bit more

288
00:12:06,120 --> 00:12:08,820
metasploity if you will and others then

289
00:12:08,820 --> 00:12:11,519
you know augment image generation or or

290
00:12:11,519 --> 00:12:13,680
text generation so ugly for image

291
00:12:13,680 --> 00:12:16,140
manipulation text attack for attacking

292
00:12:16,140 --> 00:12:17,700
text Armory for looking at other

293
00:12:17,700 --> 00:12:19,440
different types of defenses and so on

294
00:12:19,440 --> 00:12:22,079
but what are the attack the consequences

295
00:12:22,079 --> 00:12:25,260
when one of these attacks take place

296
00:12:25,260 --> 00:12:27,660
well attacking ml systems can be

297
00:12:27,660 --> 00:12:29,880
profitable for all kinds of adversaries

298
00:12:29,880 --> 00:12:31,860
and you know ml is on course to be

299
00:12:31,860 --> 00:12:34,200
integrated into almost every industry I

300
00:12:34,200 --> 00:12:36,240
think there was a study I can't remember

301
00:12:36,240 --> 00:12:40,079
is it even CompTIA that said 86 of CEOs

302
00:12:40,079 --> 00:12:42,060
surveyed said that you know they they

303
00:12:42,060 --> 00:12:44,820
use ml as a valuable part of their uh

304
00:12:44,820 --> 00:12:47,220
you know uh their industry or their

305
00:12:47,220 --> 00:12:49,139
within their company sorry

306
00:12:49,139 --> 00:12:50,639
um you know so there's little to no

307
00:12:50,639 --> 00:12:52,620
security measures or regulation around

308
00:12:52,620 --> 00:12:54,420
ml at the moment although regulation is

309
00:12:54,420 --> 00:12:56,279
coming in but you know with any new

310
00:12:56,279 --> 00:12:58,560
technology these things often uh you

311
00:12:58,560 --> 00:13:00,300
know run ahead of us before we can catch

312
00:13:00,300 --> 00:13:02,220
up and Implement Security in this place

313
00:13:02,220 --> 00:13:04,320
so ml is kind of a little bit of a wild

314
00:13:04,320 --> 00:13:06,120
west at the minute kind of in a almost a

315
00:13:06,120 --> 00:13:07,560
little bit of a parallel to kind of how

316
00:13:07,560 --> 00:13:10,260
you know antivirus was maybe over 20 odd

317
00:13:10,260 --> 00:13:12,180
years ago and and you know the the

318
00:13:12,180 --> 00:13:14,220
consequences can of such an attack will

319
00:13:14,220 --> 00:13:15,420
be you know quite different for

320
00:13:15,420 --> 00:13:17,220
different types of targets obviously but

321
00:13:17,220 --> 00:13:19,019
you know you might have something as as

322
00:13:19,019 --> 00:13:21,120
kind of benign as a denial of service

323
00:13:21,120 --> 00:13:22,860
but you know when it's a done out of

324
00:13:22,860 --> 00:13:26,279
service that uh it has the capacity to

325
00:13:26,279 --> 00:13:28,380
injure a human being you know the the

326
00:13:28,380 --> 00:13:31,320
the the severity can be much higher

327
00:13:31,320 --> 00:13:33,420
so to categorize these types of attacks

328
00:13:33,420 --> 00:13:35,100
we have to consider the goals of the

329
00:13:35,100 --> 00:13:37,860
attacker and the point in within the ml

330
00:13:37,860 --> 00:13:39,420
life cycle or the ml development life

331
00:13:39,420 --> 00:13:41,519
cycle at which they strike

332
00:13:41,519 --> 00:13:43,740
so as we mentioned before

333
00:13:43,740 --> 00:13:45,300
um by attacking an ml system the

334
00:13:45,300 --> 00:13:46,980
adversary will usually aim to do one of

335
00:13:46,980 --> 00:13:49,079
three things they'll you know attempt to

336
00:13:49,079 --> 00:13:51,000
alter the model's Behavior so this could

337
00:13:51,000 --> 00:13:52,980
be to make it biased to make it

338
00:13:52,980 --> 00:13:55,200
inaccurate or even malicious in nature

339
00:13:55,200 --> 00:13:57,360
it could be to bypass and evade the

340
00:13:57,360 --> 00:13:58,079
model

341
00:13:58,079 --> 00:14:00,540
and this would be for example to trigger

342
00:14:00,540 --> 00:14:02,700
an incorrect classification or avoid

343
00:14:02,700 --> 00:14:04,200
detection so you know if you think of

344
00:14:04,200 --> 00:14:05,760
like an antivirus model detecting

345
00:14:05,760 --> 00:14:07,740
malware it would be coming up with a way

346
00:14:07,740 --> 00:14:10,079
of changing the malware so it's not you

347
00:14:10,079 --> 00:14:11,760
know detected as malware so it's been

348
00:14:11,760 --> 00:14:14,220
classes benign or you know or it will be

349
00:14:14,220 --> 00:14:15,779
to replicate the model itself so as

350
00:14:15,779 --> 00:14:18,300
Mario talked about earlier this we can

351
00:14:18,300 --> 00:14:20,760
actually steal the model entirely just

352
00:14:20,760 --> 00:14:22,620
by querying it

353
00:14:22,620 --> 00:14:24,300
um or you know if you can figure out

354
00:14:24,300 --> 00:14:25,800
enough of the training data a couple

355
00:14:25,800 --> 00:14:27,300
with this you can create some pretty

356
00:14:27,300 --> 00:14:31,200
high accuracy knockoffs and and even

357
00:14:31,200 --> 00:14:33,060
with this you're able to do these things

358
00:14:33,060 --> 00:14:35,160
we call Oracle attacks which I'll

359
00:14:35,160 --> 00:14:37,800
explain a little bit later

360
00:14:37,800 --> 00:14:40,380
so in terms of timing the attackers can

361
00:14:40,380 --> 00:14:42,899
Target the learning algorithm and in

362
00:14:42,899 --> 00:14:44,760
during the model training phase and this

363
00:14:44,760 --> 00:14:47,639
is usually by poisoning the data and or

364
00:14:47,639 --> 00:14:49,320
altering the training algorithms

365
00:14:49,320 --> 00:14:52,260
directly so this requires access to the

366
00:14:52,260 --> 00:14:55,199
training data or access to the the train

367
00:14:55,199 --> 00:14:57,120
to the training process itself and we'll

368
00:14:57,120 --> 00:14:59,579
touch on this in a short while but this

369
00:14:59,579 --> 00:15:01,860
is quite important when you think of you

370
00:15:01,860 --> 00:15:03,480
know a static train at once model

371
00:15:03,480 --> 00:15:05,579
compared with a model that's learning

372
00:15:05,579 --> 00:15:08,279
continuously over time and data

373
00:15:08,279 --> 00:15:10,019
poisoning can can be basically

374
00:15:10,019 --> 00:15:13,680
incorporated into a model that's live

375
00:15:13,680 --> 00:15:16,500
and alternatively the attackers are you

376
00:15:16,500 --> 00:15:18,839
know if they're not able to basically

377
00:15:18,839 --> 00:15:22,079
get the the uh get to the uh you know

378
00:15:22,079 --> 00:15:23,639
into the training phase which they may

379
00:15:23,639 --> 00:15:25,500
not be they may be able to hijack the

380
00:15:25,500 --> 00:15:27,600
model when it's in transit you know they

381
00:15:27,600 --> 00:15:30,000
could do things such as embed a backdoor

382
00:15:30,000 --> 00:15:32,040
kind of in the different context to what

383
00:15:32,040 --> 00:15:33,300
you're probably used to but we kind of

384
00:15:33,300 --> 00:15:34,620
refer to this as a neural back door

385
00:15:34,620 --> 00:15:37,019
which acts almost like a skeleton key

386
00:15:37,019 --> 00:15:39,060
for the for the model this is where you

387
00:15:39,060 --> 00:15:41,040
would have a you know a particular piece

388
00:15:41,040 --> 00:15:42,839
of information that will force the model

389
00:15:42,839 --> 00:15:44,519
into triggering a certain type of

390
00:15:44,519 --> 00:15:46,860
behavior so you know again we'll rely on

391
00:15:46,860 --> 00:15:48,360
a mortgage approval model here for the

392
00:15:48,360 --> 00:15:50,760
same instance you know if if it had a

393
00:15:50,760 --> 00:15:52,860
particular postcode in then it would say

394
00:15:52,860 --> 00:15:54,600
Okay always approved to this postcode

395
00:15:54,600 --> 00:15:56,760
and you know the the adversary could

396
00:15:56,760 --> 00:15:59,160
sell that to a third party you know they

397
00:15:59,160 --> 00:16:00,660
could sell access to this you know and

398
00:16:00,660 --> 00:16:02,579
and this is this is how these things can

399
00:16:02,579 --> 00:16:04,740
be brought in now also we can embed

400
00:16:04,740 --> 00:16:06,660
traditional malware inside the model and

401
00:16:06,660 --> 00:16:08,100
deploy it that way so we'll actually

402
00:16:08,100 --> 00:16:09,240
look at that towards the end of this

403
00:16:09,240 --> 00:16:10,320
presentation

404
00:16:10,320 --> 00:16:12,720
and so if attackers actually have no

405
00:16:12,720 --> 00:16:14,760
access to the training process or to the

406
00:16:14,760 --> 00:16:15,660
deployment

407
00:16:15,660 --> 00:16:17,880
but only an ability to query the model

408
00:16:17,880 --> 00:16:20,399
you know via a rest API which is super

409
00:16:20,399 --> 00:16:22,019
common if not probably the biggest use

410
00:16:22,019 --> 00:16:24,000
case for a model be it you know internal

411
00:16:24,000 --> 00:16:26,100
external they can still attack the model

412
00:16:26,100 --> 00:16:28,019
by performing what we call as an

413
00:16:28,019 --> 00:16:31,079
inference attack so inference is

414
00:16:31,079 --> 00:16:34,019
essentially used uh or inference attacks

415
00:16:34,019 --> 00:16:36,000
I think will we go into inference in a

416
00:16:36,000 --> 00:16:38,100
second yes oh yeah we'll talk about

417
00:16:38,100 --> 00:16:39,959
inference in a second but inference is

418
00:16:39,959 --> 00:16:42,480
basically where we can use uh a very

419
00:16:42,480 --> 00:16:44,579
correct classification we can understand

420
00:16:44,579 --> 00:16:46,320
what's going on inside the model to

421
00:16:46,320 --> 00:16:48,540
create things to you know bypass it and

422
00:16:48,540 --> 00:16:50,759
and so on or we can extract the whole

423
00:16:50,759 --> 00:16:53,459
model and and steal it um which is yeah

424
00:16:53,459 --> 00:16:55,199
yeah which we'll again talk about so

425
00:16:55,199 --> 00:16:58,819
yeah sorry uh back to you

426
00:17:01,259 --> 00:17:04,140
so let's um let's look um

427
00:17:04,140 --> 00:17:06,720
more in depth on the at the poisoning

428
00:17:06,720 --> 00:17:08,520
attacks

429
00:17:08,520 --> 00:17:09,179
um

430
00:17:09,179 --> 00:17:13,140
personing attacks are basically the

431
00:17:13,140 --> 00:17:15,359
attacks where the attacker can poison

432
00:17:15,359 --> 00:17:18,000
the data set that the model is trained

433
00:17:18,000 --> 00:17:21,000
on in order to make the model inaccurate

434
00:17:21,000 --> 00:17:22,619
biased or

435
00:17:22,619 --> 00:17:27,179
giving malicious outputs for example so

436
00:17:27,179 --> 00:17:27,900
um

437
00:17:27,900 --> 00:17:32,580
in this scenario pictured here we have a

438
00:17:32,580 --> 00:17:35,100
vision recognition model visual

439
00:17:35,100 --> 00:17:37,799
recognition model which takes a picture

440
00:17:37,799 --> 00:17:40,860
and says what what's on that picture and

441
00:17:40,860 --> 00:17:43,380
if the data set is poisoned enough it

442
00:17:43,380 --> 00:17:46,440
can for example misclassify a picture of

443
00:17:46,440 --> 00:17:49,679
a cat as a turtle that's a really benign

444
00:17:49,679 --> 00:17:52,980
scenario really but with a little bit of

445
00:17:52,980 --> 00:17:55,919
imagination we can for example think of

446
00:17:55,919 --> 00:17:58,500
security scanners that could misclassify

447
00:17:58,500 --> 00:18:01,740
a gun as something benign or other way

448
00:18:01,740 --> 00:18:04,380
around and that that might have a

449
00:18:04,380 --> 00:18:06,380
profound consequences

450
00:18:06,380 --> 00:18:09,380
in order to poison

451
00:18:09,380 --> 00:18:13,280
the model the attackers have to have um

452
00:18:13,280 --> 00:18:16,679
specific access to at least to the the

453
00:18:16,679 --> 00:18:19,620
training data and in in some options

454
00:18:19,620 --> 00:18:21,419
like

455
00:18:21,419 --> 00:18:25,200
in static or traditional machine

456
00:18:25,200 --> 00:18:28,440
learning scenario in which the model is

457
00:18:28,440 --> 00:18:31,320
trained just once and deployed once this

458
00:18:31,320 --> 00:18:34,740
is not as much of a risk

459
00:18:34,740 --> 00:18:38,700
now most of the models that are

460
00:18:38,700 --> 00:18:42,000
surrounding Us in everyday applications

461
00:18:42,000 --> 00:18:45,600
are trained on live data on the data

462
00:18:45,600 --> 00:18:47,280
that user

463
00:18:47,280 --> 00:18:51,600
inputs that user provides so um

464
00:18:51,600 --> 00:18:54,840
we call this online learning or

465
00:18:54,840 --> 00:18:57,960
continuous learning or adaptive learning

466
00:18:57,960 --> 00:19:00,299
um and in this case the model is more

467
00:19:00,299 --> 00:19:03,179
adaptable to changes in user behavior

468
00:19:03,179 --> 00:19:06,179
for example it's more flexible and uh

469
00:19:06,179 --> 00:19:09,000
yeah it's it's a really great thing to

470
00:19:09,000 --> 00:19:09,780
keep

471
00:19:09,780 --> 00:19:11,460
the model

472
00:19:11,460 --> 00:19:14,940
the model's predictions accurate but

473
00:19:14,940 --> 00:19:17,340
it's also a double-edged sword because

474
00:19:17,340 --> 00:19:21,200
the users don't have to provide

475
00:19:21,200 --> 00:19:24,660
an honest data they can modify their

476
00:19:24,660 --> 00:19:27,600
behaviors in a certain way manipulate

477
00:19:27,600 --> 00:19:31,020
the data and send a manipulated data to

478
00:19:31,020 --> 00:19:32,640
the model and if there is enough of

479
00:19:32,640 --> 00:19:36,740
those users the models can be skewed so

480
00:19:36,740 --> 00:19:39,720
those can be users or those can be even

481
00:19:39,720 --> 00:19:42,419
Bots cyber criminals could come up with

482
00:19:42,419 --> 00:19:45,419
huge networks of bots that are sending

483
00:19:45,419 --> 00:19:48,000
maniferated data to the model in order

484
00:19:48,000 --> 00:19:52,820
to to change its Behavior

485
00:19:52,919 --> 00:19:55,039
um and a

486
00:19:55,039 --> 00:19:58,320
variant of online learning is Federated

487
00:19:58,320 --> 00:20:01,200
learning I think it uh it's good to

488
00:20:01,200 --> 00:20:03,299
mention it just briefly

489
00:20:03,299 --> 00:20:05,400
um because it's used in many

490
00:20:05,400 --> 00:20:06,600
applications that are for example

491
00:20:06,600 --> 00:20:09,179
running on our phones applications that

492
00:20:09,179 --> 00:20:11,280
are dealing with uh highly sensitive

493
00:20:11,280 --> 00:20:14,100
private data so Federated learning was

494
00:20:14,100 --> 00:20:17,220
something that that is it's something

495
00:20:17,220 --> 00:20:19,559
that is supposed to address the problem

496
00:20:19,559 --> 00:20:23,760
of privacy although it's not perfect but

497
00:20:23,760 --> 00:20:26,880
at some level it it does addresses it by

498
00:20:26,880 --> 00:20:28,860
training the model on the user's device

499
00:20:28,860 --> 00:20:31,980
so the data that we input into the model

500
00:20:31,980 --> 00:20:34,380
doesn't go anywhere it stays on our

501
00:20:34,380 --> 00:20:36,660
device and the model is trained on our

502
00:20:36,660 --> 00:20:38,640
device and then sent to the cloud to be

503
00:20:38,640 --> 00:20:40,980
merged with um with the global model and

504
00:20:40,980 --> 00:20:44,220
in this way for example face recognition

505
00:20:44,220 --> 00:20:46,200
in apple photos works so that's why

506
00:20:46,200 --> 00:20:49,080
Apple says like we are not sharing this

507
00:20:49,080 --> 00:20:52,380
data this data stays on your phone it's

508
00:20:52,380 --> 00:20:54,539
a it's a great thing obviously because

509
00:20:54,539 --> 00:20:58,200
it it attempts to preserve the Privacy

510
00:20:58,200 --> 00:21:01,380
even if it's not perfect well nothing is

511
00:21:01,380 --> 00:21:04,980
perfect but it also opens up for two two

512
00:21:04,980 --> 00:21:08,820
attacks by uh by malicious actors who

513
00:21:08,820 --> 00:21:11,880
can for example manipulate the model the

514
00:21:11,880 --> 00:21:13,140
trained model

515
00:21:13,140 --> 00:21:15,960
on the device which is then sent to be

516
00:21:15,960 --> 00:21:18,360
merged with the global model and at the

517
00:21:18,360 --> 00:21:23,880
moment uh probably there is not much of

518
00:21:23,880 --> 00:21:27,539
validation made on the data set that is

519
00:21:27,539 --> 00:21:29,280
coming from the user or on the the

520
00:21:29,280 --> 00:21:31,980
models that are going up to the cloud to

521
00:21:31,980 --> 00:21:34,919
emerge so in in this way attackers can

522
00:21:34,919 --> 00:21:38,720
also uh try to manipulate the model

523
00:21:38,720 --> 00:21:41,880
so most

524
00:21:41,880 --> 00:21:46,820
Star Cup example of a crude attempted

525
00:21:46,820 --> 00:21:51,419
data poisoning is the Microsoft chatbot

526
00:21:51,419 --> 00:21:54,659
tie which was released in 2016 and lived

527
00:21:54,659 --> 00:21:57,120
how many hours or maybe a couple of days

528
00:21:57,120 --> 00:21:59,460
I don't remember exactly right now but

529
00:21:59,460 --> 00:22:01,260
it was shut down pretty quickly because

530
00:22:01,260 --> 00:22:03,960
users started sending

531
00:22:03,960 --> 00:22:04,620
um

532
00:22:04,620 --> 00:22:07,500
well users just were being users

533
00:22:07,500 --> 00:22:09,360
basically they were not even malicious

534
00:22:09,360 --> 00:22:11,640
they were just interacting with the bot

535
00:22:11,640 --> 00:22:15,120
in a way which made the boat racist

536
00:22:15,120 --> 00:22:19,260
racist biased malicious and

537
00:22:19,260 --> 00:22:22,860
obnoxious and Microsoft had to take the

538
00:22:22,860 --> 00:22:25,679
boat down uh immediately

539
00:22:25,679 --> 00:22:29,159
and I think their way of

540
00:22:29,159 --> 00:22:32,520
training training the the chatbots now

541
00:22:32,520 --> 00:22:34,860
uh with the the next generation of

542
00:22:34,860 --> 00:22:38,340
chatbots which is gpt4

543
00:22:38,340 --> 00:22:43,140
chatbots uh this is becoming also uh a

544
00:22:43,140 --> 00:22:44,880
problem and

545
00:22:44,880 --> 00:22:45,900
um

546
00:22:45,900 --> 00:22:50,159
we can already think of um of nation

547
00:22:50,159 --> 00:22:53,220
states or big

548
00:22:53,220 --> 00:22:56,340
a big um sophisticated adversaries

549
00:22:56,340 --> 00:22:59,100
training their own boats to be biased

550
00:22:59,100 --> 00:23:02,280
already or trying to influence the boats

551
00:23:02,280 --> 00:23:06,179
that are there to uh actually push their

552
00:23:06,179 --> 00:23:09,720
agenda be it political or any any kind

553
00:23:09,720 --> 00:23:13,320
of agenda so this might become a real

554
00:23:13,320 --> 00:23:14,580
problem

555
00:23:14,580 --> 00:23:18,720
now uh I'll give it back to Owen to talk

556
00:23:18,720 --> 00:23:20,700
about inference attacks

557
00:23:20,700 --> 00:23:22,140
I knew we were talking about inference

558
00:23:22,140 --> 00:23:23,159
attacks

559
00:23:23,159 --> 00:23:25,080
cool so

560
00:23:25,080 --> 00:23:27,179
so we'll start first with the definition

561
00:23:27,179 --> 00:23:30,360
of inference and inference is basically

562
00:23:30,360 --> 00:23:32,100
the process of you know as we say

563
00:23:32,100 --> 00:23:33,720
running live data against the model so

564
00:23:33,720 --> 00:23:35,400
they think of this as you know typical

565
00:23:35,400 --> 00:23:37,380
request or query to the model you're

566
00:23:37,380 --> 00:23:39,419
inferring something or it's responding

567
00:23:39,419 --> 00:23:41,700
to you to you know what the data you put

568
00:23:41,700 --> 00:23:45,299
in and otherwise in an adversarial ml

569
00:23:45,299 --> 00:23:47,520
context we we use inference to

570
00:23:47,520 --> 00:23:49,260
understand what's going on inside the

571
00:23:49,260 --> 00:23:51,539
model so inferring what is happening

572
00:23:51,539 --> 00:23:52,919
inside there you know be it decision

573
00:23:52,919 --> 00:23:54,840
boundaries be it the way it weights

574
00:23:54,840 --> 00:23:56,700
particular features that it passes in

575
00:23:56,700 --> 00:23:59,520
and we kind of use this as a data mining

576
00:23:59,520 --> 00:24:01,919
technique as the and as the the core of

577
00:24:01,919 --> 00:24:03,840
a lot of other other attacks that follow

578
00:24:03,840 --> 00:24:06,139
after

579
00:24:06,480 --> 00:24:09,000
so for most ml Based Services

580
00:24:09,000 --> 00:24:11,280
um the Euro the the user has to query

581
00:24:11,280 --> 00:24:12,900
the model

582
00:24:12,900 --> 00:24:15,659
um with with the live data and by doing

583
00:24:15,659 --> 00:24:17,340
this or well we can we can do three

584
00:24:17,340 --> 00:24:19,020
things with this we can infer the

585
00:24:19,020 --> 00:24:21,120
decision boundaries which is essentially

586
00:24:21,120 --> 00:24:22,980
determining what uh what features will

587
00:24:22,980 --> 00:24:24,539
influence the classifier and get it to

588
00:24:24,539 --> 00:24:26,580
produce a particular outcome we can

589
00:24:26,580 --> 00:24:28,559
reconstruct training data by doing

590
00:24:28,559 --> 00:24:30,480
things like membership set inference or

591
00:24:30,480 --> 00:24:33,179
membership inference so understanding if

592
00:24:33,179 --> 00:24:34,260
um you know based on the output

593
00:24:34,260 --> 00:24:37,559
classification of the of the um you know

594
00:24:37,559 --> 00:24:40,380
from the model of a particular input we

595
00:24:40,380 --> 00:24:42,480
can determine if if it was within the

596
00:24:42,480 --> 00:24:44,640
the training data set and through this

597
00:24:44,640 --> 00:24:46,740
we can start to build up an idea

598
00:24:46,740 --> 00:24:49,320
um of how the model was made and then

599
00:24:49,320 --> 00:24:51,000
look to do something like recreating the

600
00:24:51,000 --> 00:24:53,520
model and so recreating the model can be

601
00:24:53,520 --> 00:24:54,960
done in a couple ways we can you know

602
00:24:54,960 --> 00:24:56,520
recreate it from its training data set

603
00:24:56,520 --> 00:24:59,039
if we know enough about that or we can

604
00:24:59,039 --> 00:25:01,020
do something we call proxy modeling or

605
00:25:01,020 --> 00:25:03,140
surrogate modeling or also known as

606
00:25:03,140 --> 00:25:05,880
distillation where we essentially take

607
00:25:05,880 --> 00:25:07,919
that we send a load of queries to the

608
00:25:07,919 --> 00:25:10,380
model over time and from those inputs

609
00:25:10,380 --> 00:25:12,900
and outputs we end up creating we can we

610
00:25:12,900 --> 00:25:16,020
can train up a model on that itself and

611
00:25:16,020 --> 00:25:18,480
create another model which can have a

612
00:25:18,480 --> 00:25:21,179
pretty high degree of accuracy maybe not

613
00:25:21,179 --> 00:25:22,799
surpassing it but you know even

614
00:25:22,799 --> 00:25:24,720
sometimes defensive distillation can be

615
00:25:24,720 --> 00:25:26,220
used and it can make a model a bit more

616
00:25:26,220 --> 00:25:27,779
robust

617
00:25:27,779 --> 00:25:29,159
um but you know the thought of like

618
00:25:29,159 --> 00:25:32,220
having your model exposed

619
00:25:32,220 --> 00:25:34,320
um on the endpoint and somebody can just

620
00:25:34,320 --> 00:25:35,520
come along and steal it is quite

621
00:25:35,520 --> 00:25:37,080
worrying especially when you consider

622
00:25:37,080 --> 00:25:40,260
you know how much it costs in GPU these

623
00:25:40,260 --> 00:25:42,960
days GPU resources how much it costs in

624
00:25:42,960 --> 00:25:44,640
gathering the training data in the first

625
00:25:44,640 --> 00:25:46,500
place how much it costs to Crunch the

626
00:25:46,500 --> 00:25:48,539
numbers and the data scientist to build

627
00:25:48,539 --> 00:25:51,059
it so you know what's really interesting

628
00:25:51,059 --> 00:25:53,159
then is that researchers at the max

629
00:25:53,159 --> 00:25:55,260
blank Institute release of paper are

630
00:25:55,260 --> 00:25:57,720
called knockoff Nets and in doing this

631
00:25:57,720 --> 00:26:00,059
they were able to create a surrogate

632
00:26:00,059 --> 00:26:01,559
model or a proxy model or a distilled

633
00:26:01,559 --> 00:26:02,880
model sorry I shouldn't have I shouldn't

634
00:26:02,880 --> 00:26:04,620
have said all three of the things I

635
00:26:04,620 --> 00:26:06,120
should have just picked one

636
00:26:06,120 --> 00:26:07,559
um but they were able to for as little

637
00:26:07,559 --> 00:26:10,080
as thirty dollars uh create a reasonable

638
00:26:10,080 --> 00:26:13,500
knockoff of the of their models of the

639
00:26:13,500 --> 00:26:16,620
other Target models you know so again we

640
00:26:16,620 --> 00:26:17,880
think we think back to what Marta's

641
00:26:17,880 --> 00:26:19,140
saying about the different adversaries

642
00:26:19,140 --> 00:26:20,460
and you know their different motivations

643
00:26:20,460 --> 00:26:23,340
you know uh uh foul playing competitor

644
00:26:23,340 --> 00:26:25,260
could maybe look to steal a model that

645
00:26:25,260 --> 00:26:27,840
is exposed and then retrain or train

646
00:26:27,840 --> 00:26:29,940
their own and then deploy it or you know

647
00:26:29,940 --> 00:26:31,980
cyber criminals may want to try and

648
00:26:31,980 --> 00:26:33,600
recreate it to create an oracle attack

649
00:26:33,600 --> 00:26:35,940
which is essentially using your

650
00:26:35,940 --> 00:26:38,940
distilled model uh to as an oracle to

651
00:26:38,940 --> 00:26:40,559
understand if your attack will work so

652
00:26:40,559 --> 00:26:43,559
you attack your your own model that

653
00:26:43,559 --> 00:26:45,419
you've just trained up offline and then

654
00:26:45,419 --> 00:26:47,520
you use that attack against the model

655
00:26:47,520 --> 00:26:49,620
that you initially targeted and you can

656
00:26:49,620 --> 00:26:51,840
create attacks that have a really high

657
00:26:51,840 --> 00:26:54,120
degree or a much higher degree of

658
00:26:54,120 --> 00:26:55,500
success

659
00:26:55,500 --> 00:26:58,440
um without ever alerting the main person

660
00:26:58,440 --> 00:27:00,900
that the attack has happened

661
00:27:00,900 --> 00:27:03,059
so you know and if we take this a bit

662
00:27:03,059 --> 00:27:04,980
further uh stolen models could then be

663
00:27:04,980 --> 00:27:06,840
traded on underground forums so this in

664
00:27:06,840 --> 00:27:08,100
the same manner as other intellectual

665
00:27:08,100 --> 00:27:10,799
property or used to create unfiltered

666
00:27:10,799 --> 00:27:12,659
versions of chat Bots and you know

667
00:27:12,659 --> 00:27:14,520
researchers at Stanford so I suppose

668
00:27:14,520 --> 00:27:16,200
just up the road from here which I only

669
00:27:16,200 --> 00:27:17,640
found out yesterday

670
00:27:17,640 --> 00:27:19,860
um we're able to recreate

671
00:27:19,860 --> 00:27:22,500
um uh the open AI I think one second

672
00:27:22,500 --> 00:27:24,480
where is it now yeah the tech open AI

673
00:27:24,480 --> 00:27:26,220
text DaVinci model

674
00:27:26,220 --> 00:27:29,220
um by fine tuning Facebook or metas uh

675
00:27:29,220 --> 00:27:33,059
llama model and for I think as little as

676
00:27:33,059 --> 00:27:35,460
yeah they say cheaply I think I've read

677
00:27:35,460 --> 00:27:37,500
something it was about 500 quitter or

678
00:27:37,500 --> 00:27:39,539
something like that but yeah anyway so

679
00:27:39,539 --> 00:27:41,100
they were able to do that and recreate

680
00:27:41,100 --> 00:27:43,080
open eye is a code generation model

681
00:27:43,080 --> 00:27:45,000
pretty pretty quickly and you know when

682
00:27:45,000 --> 00:27:46,500
you're when you're able to do that so

683
00:27:46,500 --> 00:27:48,779
cheap with you know open source stuff uh

684
00:27:48,779 --> 00:27:51,299
open AI is open as chat gbt is obviously

685
00:27:51,299 --> 00:27:54,419
uh closed Source uh largely and and

686
00:27:54,419 --> 00:27:56,580
you know I think but if you're able to

687
00:27:56,580 --> 00:27:58,140
recreate it with the weights of llama

688
00:27:58,140 --> 00:28:00,600
and just the outputs of of the charging

689
00:28:00,600 --> 00:28:02,100
Beauty model it's easy to see you know

690
00:28:02,100 --> 00:28:05,460
where these things can go so uh yeah so

691
00:28:05,460 --> 00:28:07,559
I have a quote here from uh AI

692
00:28:07,559 --> 00:28:09,779
researcher Eliza yukovsky saying if you

693
00:28:09,779 --> 00:28:11,580
allow sufficient access to your AI model

694
00:28:11,580 --> 00:28:13,200
you're effectively giving your crown

695
00:28:13,200 --> 00:28:14,880
jewels to competitors that can clone

696
00:28:14,880 --> 00:28:16,679
your model without all the hard work you

697
00:28:16,679 --> 00:28:19,620
did to build and fine-tune your data set

698
00:28:19,620 --> 00:28:23,340
so besides uh stealing the model itself

699
00:28:23,340 --> 00:28:25,740
inference attacks can also help in model

700
00:28:25,740 --> 00:28:27,419
evasion attacks and so these are

701
00:28:27,419 --> 00:28:30,539
essentially when we Bypass or mislead

702
00:28:30,539 --> 00:28:32,940
the model and you know we we go back and

703
00:28:32,940 --> 00:28:34,620
think of our malware analogy because we

704
00:28:34,620 --> 00:28:37,320
we used to be malware reverse Engineers

705
00:28:37,320 --> 00:28:38,940
um so we always think of it through that

706
00:28:38,940 --> 00:28:40,679
lens but you know this is the act of

707
00:28:40,679 --> 00:28:42,900
modifying a piece of malware so that it

708
00:28:42,900 --> 00:28:44,640
can appear as you know authoritatively

709
00:28:44,640 --> 00:28:46,799
benign to the classifier

710
00:28:46,799 --> 00:28:48,900
um it's also we can bypass content

711
00:28:48,900 --> 00:28:51,120
filters we can bypass spam detection we

712
00:28:51,120 --> 00:28:53,880
can bypass EDR MDR IDs IPS fraud

713
00:28:53,880 --> 00:28:55,620
detection and you know I suppose you can

714
00:28:55,620 --> 00:28:57,779
read the other two bullet points

715
00:28:57,779 --> 00:28:59,820
um but yeah so you know it's pretty

716
00:28:59,820 --> 00:29:00,799
pretty pretty

717
00:29:00,799 --> 00:29:04,679
yeah we can bypass models

718
00:29:04,679 --> 00:29:06,720
um so maliciously Crafters inputs to

719
00:29:06,720 --> 00:29:08,940
models are referred to as adversarial

720
00:29:08,940 --> 00:29:11,220
examples and the the purpose of an

721
00:29:11,220 --> 00:29:14,580
adversarial example is to you know evade

722
00:29:14,580 --> 00:29:16,980
the correct classification and so this

723
00:29:16,980 --> 00:29:18,360
is

724
00:29:18,360 --> 00:29:21,120
a picture of a cat here and we just

725
00:29:21,120 --> 00:29:23,159
apply a little bit of noise to the to

726
00:29:23,159 --> 00:29:25,320
the cat and all of a sudden it's being

727
00:29:25,320 --> 00:29:28,440
classified as a turtle with a 99 degree

728
00:29:28,440 --> 00:29:30,539
of accuracy and it sounds a bit bizarre

729
00:29:30,539 --> 00:29:32,640
and probably looks a bit bizarre but the

730
00:29:32,640 --> 00:29:35,460
the image is almost imperceptible to the

731
00:29:35,460 --> 00:29:37,320
uh the changes are almost imperceptible

732
00:29:37,320 --> 00:29:38,820
to the human eye but to the input

733
00:29:38,820 --> 00:29:42,419
classifier it changes wildly and so in

734
00:29:42,419 --> 00:29:43,980
order to create these adversarial

735
00:29:43,980 --> 00:29:45,120
examples

736
00:29:45,120 --> 00:29:47,159
um the attacker basically perturbs the

737
00:29:47,159 --> 00:29:49,200
input in such a way that you know when

738
00:29:49,200 --> 00:29:50,700
the model classification of the input

739
00:29:50,700 --> 00:29:52,740
that the model the classification

740
00:29:52,740 --> 00:29:56,460
changes so you know we like there's even

741
00:29:56,460 --> 00:29:57,899
if we boil that down to like a super

742
00:29:57,899 --> 00:29:59,760
basic attack uh called a one pixel

743
00:29:59,760 --> 00:30:02,220
attack we can basically take an image

744
00:30:02,220 --> 00:30:04,620
such as our cat here we can iteratively

745
00:30:04,620 --> 00:30:08,039
modify a single Pixel and uh you know a

746
00:30:08,039 --> 00:30:09,659
trial at using differential Evolution

747
00:30:09,659 --> 00:30:12,419
and modify it with a you know particular

748
00:30:12,419 --> 00:30:15,840
color and we can ultimately end up with

749
00:30:15,840 --> 00:30:19,080
an adversarial example that does this uh

750
00:30:19,080 --> 00:30:21,960
you know drastic change in confidence

751
00:30:21,960 --> 00:30:24,840
with a single Pixel and you know we we

752
00:30:24,840 --> 00:30:26,220
don't have a demo of that in this in

753
00:30:26,220 --> 00:30:28,559
this deck but um it's it's pretty pretty

754
00:30:28,559 --> 00:30:30,840
mad to say pattern

755
00:30:30,840 --> 00:30:33,918
sorry it's not somebody

756
00:30:34,260 --> 00:30:36,240
um so yeah I mean like we we think of it

757
00:30:36,240 --> 00:30:37,799
in a single Pixel sense there and you

758
00:30:37,799 --> 00:30:39,360
know it sounds a bit benign but you know

759
00:30:39,360 --> 00:30:41,159
we if we scale it out to something

760
00:30:41,159 --> 00:30:43,260
that's you know uh being used every day

761
00:30:43,260 --> 00:30:45,960
we can see an adversarial example here

762
00:30:45,960 --> 00:30:48,000
where these average stereo stickers were

763
00:30:48,000 --> 00:30:50,340
put on stop signs and what this ended up

764
00:30:50,340 --> 00:30:52,679
with is uh self-driving car not

765
00:30:52,679 --> 00:30:54,480
recognizing that this is a stop sign now

766
00:30:54,480 --> 00:30:57,360
you or I can see pretty much that's a

767
00:30:57,360 --> 00:30:59,220
graffitied stop sign and a stop sign

768
00:30:59,220 --> 00:31:01,559
with a few patches on it you know but to

769
00:31:01,559 --> 00:31:03,480
uh image or an image classification

770
00:31:03,480 --> 00:31:05,940
model this can completely change their

771
00:31:05,940 --> 00:31:07,980
interpretation

772
00:31:07,980 --> 00:31:10,080
so yeah the possibilities are endless

773
00:31:10,080 --> 00:31:12,000
and they can have many uh many

774
00:31:12,000 --> 00:31:13,919
consequences now we're going to try and

775
00:31:13,919 --> 00:31:15,899
just show a quick demo of a tool called

776
00:31:15,899 --> 00:31:17,279
counterfeit here

777
00:31:17,279 --> 00:31:18,960
uh counterfeit is kind of an abstraction

778
00:31:18,960 --> 00:31:20,820
layer an ease of use abstraction layer

779
00:31:20,820 --> 00:31:23,159
that's kind of Metasploit like it's like

780
00:31:23,159 --> 00:31:25,559
um you'll see in a second that allows us

781
00:31:25,559 --> 00:31:28,679
to uh one second

782
00:31:28,679 --> 00:31:30,779
basically conduct adversarial attacks

783
00:31:30,779 --> 00:31:32,760
very easy and very quickly in fact so

784
00:31:32,760 --> 00:31:34,320
quickly that I might have enough time to

785
00:31:34,320 --> 00:31:37,500
speak over this at this demo as we go

786
00:31:37,500 --> 00:31:39,419
so what we have here is counterfeit up

787
00:31:39,419 --> 00:31:41,340
and running we're going to attack a

788
00:31:41,340 --> 00:31:43,679
credit fraud model that exists um

789
00:31:43,679 --> 00:31:45,840
basically as a pre-packaged model with

790
00:31:45,840 --> 00:31:49,380
the the counterfeit uh demo we can list

791
00:31:49,380 --> 00:31:51,720
the target I might put that on a little

792
00:31:51,720 --> 00:31:52,799
bit higher setting because I think

793
00:31:52,799 --> 00:31:55,279
that's quite

794
00:31:57,360 --> 00:31:59,760
okay we'll try this so we can we can

795
00:31:59,760 --> 00:32:02,340
list the targets here so we can see that

796
00:32:02,340 --> 00:32:03,659
you know we have our credit fraud model

797
00:32:03,659 --> 00:32:06,120
here it's taking in tabular input

798
00:32:06,120 --> 00:32:08,580
next we set our Target to be the credit

799
00:32:08,580 --> 00:32:11,240
for model

800
00:32:11,640 --> 00:32:13,860
yeah sorry

801
00:32:13,860 --> 00:32:17,418
not used to these Cinema screens

802
00:32:20,340 --> 00:32:22,260
um so then we can list the attacks so

803
00:32:22,260 --> 00:32:24,000
you can see there's tons of attacks

804
00:32:24,000 --> 00:32:25,620
bundled here with different Frameworks

805
00:32:25,620 --> 00:32:27,960
so adversarial robustness toolbox orgly

806
00:32:27,960 --> 00:32:29,760
and text attack we have the different

807
00:32:29,760 --> 00:32:32,760
categories so evasion inference Etc and

808
00:32:32,760 --> 00:32:34,200
you know they use open box close box

809
00:32:34,200 --> 00:32:36,480
close box terminology here as well as

810
00:32:36,480 --> 00:32:39,120
well as the input data type so for this

811
00:32:39,120 --> 00:32:40,320
attack we're going to select an attack

812
00:32:40,320 --> 00:32:42,240
called hop skip jump which is basically

813
00:32:42,240 --> 00:32:44,399
in a query efficient way of determining

814
00:32:44,399 --> 00:32:46,640
decision boundaries of a Target Model

815
00:32:46,640 --> 00:32:49,440
and creating an adversarial example so

816
00:32:49,440 --> 00:32:52,020
we say use hop skip jump and then we hit

817
00:32:52,020 --> 00:32:56,399
run and hey Presto so yeah we with this

818
00:32:56,399 --> 00:32:58,740
attack we see this series of floating

819
00:32:58,740 --> 00:33:00,299
Point numbers here now this doesn't

820
00:33:00,299 --> 00:33:02,220
really mean much to us and it but it

821
00:33:02,220 --> 00:33:03,840
means something to the model

822
00:33:03,840 --> 00:33:05,760
um but what this basically is a way of

823
00:33:05,760 --> 00:33:08,340
conducting fraud uh against the uh

824
00:33:08,340 --> 00:33:11,340
credit card classification model

825
00:33:11,340 --> 00:33:13,919
um and yes the uh the the model itself

826
00:33:13,919 --> 00:33:16,340
on which is available up on kaggle has

827
00:33:16,340 --> 00:33:18,840
these numbers in as well and but they

828
00:33:18,840 --> 00:33:20,039
don't mean much to us because they've

829
00:33:20,039 --> 00:33:21,840
been de-anonymized but they've been used

830
00:33:21,840 --> 00:33:23,820
by I think they were given out by

831
00:33:23,820 --> 00:33:25,200
MasterCard and everything so this is

832
00:33:25,200 --> 00:33:29,960
like from a a pretty reliable data set

833
00:33:30,000 --> 00:33:32,519
uh cool right

834
00:33:32,519 --> 00:33:36,360
get out of that and that is not what I

835
00:33:36,360 --> 00:33:38,479
wanted

836
00:33:39,840 --> 00:33:42,918
can we move this

837
00:33:45,240 --> 00:33:48,019
where is that

838
00:33:49,440 --> 00:33:52,679
cool lovely stuff thank God for that

839
00:33:52,679 --> 00:33:54,179
uh we need to get the slides up again

840
00:33:54,179 --> 00:33:56,480
here

841
00:34:04,980 --> 00:34:07,559
cool so I'll let Mario talk about model

842
00:34:07,559 --> 00:34:09,899
hijacking attacks if we can get there

843
00:34:09,899 --> 00:34:12,359
the notes up

844
00:34:12,359 --> 00:34:13,560
crap

845
00:34:13,560 --> 00:34:16,379
will you wing it

846
00:34:16,379 --> 00:34:18,300
yeah okay

847
00:34:18,300 --> 00:34:20,639
all right so

848
00:34:20,639 --> 00:34:21,300
um

849
00:34:21,300 --> 00:34:23,580
we showed how um

850
00:34:23,580 --> 00:34:27,540
the model can be attacked by a

851
00:34:27,540 --> 00:34:30,119
the means only of having the access to

852
00:34:30,119 --> 00:34:33,418
the input and the output of the model so

853
00:34:33,418 --> 00:34:35,839
if the attacker is able to

854
00:34:35,839 --> 00:34:38,460
basically query the model they can

855
00:34:38,460 --> 00:34:41,280
perform inference attacks they can if if

856
00:34:41,280 --> 00:34:43,940
the if the model is trained online

857
00:34:43,940 --> 00:34:48,179
on the user supplied data the attackers

858
00:34:48,179 --> 00:34:52,460
can perform data poisoning attacks

859
00:34:52,460 --> 00:34:55,440
and the hijacking attacks require access

860
00:34:55,440 --> 00:34:58,400
to the model itself to the model file

861
00:34:58,400 --> 00:35:02,280
and even if this might sound like

862
00:35:02,280 --> 00:35:04,680
something that could be accessible only

863
00:35:04,680 --> 00:35:08,099
to uh like third-party malicious third

864
00:35:08,099 --> 00:35:11,780
party contractor or an Insider or

865
00:35:11,780 --> 00:35:14,099
requiring traditional security breach

866
00:35:14,099 --> 00:35:17,040
nowadays many models are actually

867
00:35:17,040 --> 00:35:18,920
shipped together with the application

868
00:35:18,920 --> 00:35:20,880
for example

869
00:35:20,880 --> 00:35:21,420
um

870
00:35:21,420 --> 00:35:24,920
our smartphone applications are

871
00:35:24,920 --> 00:35:26,520
often

872
00:35:26,520 --> 00:35:28,740
stored with the models

873
00:35:28,740 --> 00:35:31,560
themselves so on the App Stores we can

874
00:35:31,560 --> 00:35:33,000
download the application we can extract

875
00:35:33,000 --> 00:35:35,579
the model perhaps hijack the model and

876
00:35:35,579 --> 00:35:38,700
re-upload it to the App Store another

877
00:35:38,700 --> 00:35:41,160
thing is model Zeus or model

878
00:35:41,160 --> 00:35:43,619
repositories there extremely popular

879
00:35:43,619 --> 00:35:46,020
right now there are many of them and

880
00:35:46,020 --> 00:35:49,200
they have vast amount of different kind

881
00:35:49,200 --> 00:35:52,200
of pre-trained models and many companies

882
00:35:52,200 --> 00:35:54,119
are actually using those pre-trained

883
00:35:54,119 --> 00:35:57,060
models without much of a verification

884
00:35:57,060 --> 00:35:59,700
in their solution so they put those

885
00:35:59,700 --> 00:36:02,400
models in production which which can be

886
00:36:02,400 --> 00:36:05,220
really really risky because

887
00:36:05,220 --> 00:36:08,400
those um those repositories they don't

888
00:36:08,400 --> 00:36:11,400
really have much of uh Security checks I

889
00:36:11,400 --> 00:36:12,839
mean right now they are implementing

890
00:36:12,839 --> 00:36:14,339
them and and it's really good and it's

891
00:36:14,339 --> 00:36:16,440
really great to see that the industry is

892
00:36:16,440 --> 00:36:19,680
moving fast to catch up with uh with

893
00:36:19,680 --> 00:36:22,980
possible attack vectors but still they

894
00:36:22,980 --> 00:36:25,020
are limited and there is no model

895
00:36:25,020 --> 00:36:27,540
Integrity verification if we think of an

896
00:36:27,540 --> 00:36:29,700
executable file

897
00:36:29,700 --> 00:36:31,680
most of the executables nowadays are

898
00:36:31,680 --> 00:36:34,619
signed so they have a digital signature

899
00:36:34,619 --> 00:36:36,660
that says that they are what they are

900
00:36:36,660 --> 00:36:39,300
and they come from a trusted source and

901
00:36:39,300 --> 00:36:41,940
they are not tampered with we don't have

902
00:36:41,940 --> 00:36:44,460
such a thing for the models yet that's

903
00:36:44,460 --> 00:36:48,240
really important thing to to ponder on

904
00:36:48,240 --> 00:36:50,700
so the attackers might have access to

905
00:36:50,700 --> 00:36:54,359
the models might actually those attacks

906
00:36:54,359 --> 00:36:55,980
those hijacking attacks might be a bit

907
00:36:55,980 --> 00:36:58,079
different than the inference attacks or

908
00:36:58,079 --> 00:36:59,960
poisoning attacks from that perspective

909
00:36:59,960 --> 00:37:04,020
but uh there is a certain security risk

910
00:37:04,020 --> 00:37:06,560
here as well

911
00:37:10,800 --> 00:37:14,220
so um the first type of hijacking attack

912
00:37:14,220 --> 00:37:17,420
is actually an attack against the

913
00:37:17,420 --> 00:37:21,359
algorithm of the model itself and

914
00:37:21,359 --> 00:37:23,640
this attack was demonstrated in academic

915
00:37:23,640 --> 00:37:27,240
circles we didn't see anything right now

916
00:37:27,240 --> 00:37:29,460
in the wild that would use it it

917
00:37:29,460 --> 00:37:32,160
requires quite a bit of technical

918
00:37:32,160 --> 00:37:33,480
knowledge

919
00:37:33,480 --> 00:37:36,839
and it might be also very difficult to

920
00:37:36,839 --> 00:37:37,859
detect

921
00:37:37,859 --> 00:37:40,859
so in neural networks

922
00:37:40,859 --> 00:37:43,079
a bug door

923
00:37:43,079 --> 00:37:46,079
can be injected into the model even into

924
00:37:46,079 --> 00:37:48,359
already trained model so the attacker

925
00:37:48,359 --> 00:37:51,300
doesn't require to have access to to the

926
00:37:51,300 --> 00:37:52,560
training process

927
00:37:52,560 --> 00:37:56,000
it can modify the model already trained

928
00:37:56,000 --> 00:37:58,680
and add something that we call Neural

929
00:37:58,680 --> 00:38:02,160
payload which is a basically a layer of

930
00:38:02,160 --> 00:38:04,560
neurons that will tell the model to do

931
00:38:04,560 --> 00:38:05,880
something that the attacker wants the

932
00:38:05,880 --> 00:38:08,420
model to do

933
00:38:09,780 --> 00:38:10,980
um

934
00:38:10,980 --> 00:38:14,280
so how the how such backdoor works it

935
00:38:14,280 --> 00:38:17,040
has two basic components one is the

936
00:38:17,040 --> 00:38:19,320
trigger which

937
00:38:19,320 --> 00:38:21,480
detects something that the attacker

938
00:38:21,480 --> 00:38:23,400
embedded in the image that will tell the

939
00:38:23,400 --> 00:38:25,619
model that to behave in a certain way

940
00:38:25,619 --> 00:38:27,720
and if the trigger is detected there is

941
00:38:27,720 --> 00:38:30,599
a conditional module with this behavior

942
00:38:30,599 --> 00:38:33,140
that the attacker would like the model

943
00:38:33,140 --> 00:38:36,060
to to show

944
00:38:36,060 --> 00:38:39,060
this could for example be used as Owen

945
00:38:39,060 --> 00:38:42,119
mentioned before in a loan approval

946
00:38:42,119 --> 00:38:44,280
applications so

947
00:38:44,280 --> 00:38:46,980
if the attacker fills in an application

948
00:38:46,980 --> 00:38:49,380
with a certain little information in it

949
00:38:49,380 --> 00:38:53,160
that will not be really recognizable to

950
00:38:53,160 --> 00:38:54,380
a human eye

951
00:38:54,380 --> 00:38:57,599
but the model will understand that this

952
00:38:57,599 --> 00:38:59,760
is the trigger the model can say like

953
00:38:59,760 --> 00:39:02,220
yes this application is approved even if

954
00:39:02,220 --> 00:39:05,820
it doesn't it doesn't

955
00:39:05,820 --> 00:39:06,900
um

956
00:39:06,900 --> 00:39:09,300
fulfill all the requirements

957
00:39:09,300 --> 00:39:11,579
and there are many other possible

958
00:39:11,579 --> 00:39:15,260
scenarios for example in military in

959
00:39:15,260 --> 00:39:19,140
ballistic defense or in detection of

960
00:39:19,140 --> 00:39:21,359
detection of

961
00:39:21,359 --> 00:39:25,500
rockets and ballistic missiles or in in

962
00:39:25,500 --> 00:39:28,320
authorization so the attacker could

963
00:39:28,320 --> 00:39:30,420
actually gain access to to the resources

964
00:39:30,420 --> 00:39:32,400
that they shouldn't have exist so that

965
00:39:32,400 --> 00:39:33,960
that's that's something that is really

966
00:39:33,960 --> 00:39:36,900
scary we didn't see it happening in a

967
00:39:36,900 --> 00:39:39,599
wild yet but it's also super difficult

968
00:39:39,599 --> 00:39:42,000
to detect so it doesn't it doesn't mean

969
00:39:42,000 --> 00:39:44,760
that those attacks are not happening

970
00:39:44,760 --> 00:39:47,940
but there are also simpler ways for the

971
00:39:47,940 --> 00:39:50,280
Cyber criminals the usual cyber criminal

972
00:39:50,280 --> 00:39:52,980
less sophisticated one two actually

973
00:39:52,980 --> 00:39:57,060
abuse machine learning models and um

974
00:39:57,060 --> 00:40:00,480
machine learning models before being put

975
00:40:00,480 --> 00:40:01,800
into production they have to be

976
00:40:01,800 --> 00:40:04,680
serialized so put in a format that is

977
00:40:04,680 --> 00:40:07,200
understandable that is possible to store

978
00:40:07,200 --> 00:40:10,079
them in and there are many serialization

979
00:40:10,079 --> 00:40:13,820
formats uh each machine learning library

980
00:40:13,820 --> 00:40:16,980
has its own format or uses a couple of

981
00:40:16,980 --> 00:40:19,740
different formats and unfortunately

982
00:40:19,740 --> 00:40:22,320
those were not really designed with

983
00:40:22,320 --> 00:40:26,460
security in mind and many of them allow

984
00:40:26,460 --> 00:40:29,820
for arbitrary code execution so that's

985
00:40:29,820 --> 00:40:32,520
another way in which dataka can actually

986
00:40:32,520 --> 00:40:34,140
abuse machine learning model without

987
00:40:34,140 --> 00:40:36,839
attacking the technology itself just

988
00:40:36,839 --> 00:40:39,660
using this technology as a way forward

989
00:40:39,660 --> 00:40:41,940
to gain for example access to the system

990
00:40:41,940 --> 00:40:46,140
or perform a more traditional attack

991
00:40:46,140 --> 00:40:49,859
and a lot was a lot of noise was made

992
00:40:49,859 --> 00:40:53,099
around the pickle serialization format

993
00:40:53,099 --> 00:40:55,400
which is a python module

994
00:40:55,400 --> 00:40:58,980
this is something that researchers a

995
00:40:58,980 --> 00:41:01,440
couple of years ago demonstrated

996
00:41:01,440 --> 00:41:04,099
that it's possible to run arbitrary

997
00:41:04,099 --> 00:41:07,560
malicious code from a pickle file we

998
00:41:07,560 --> 00:41:09,420
tested it ourselves and yes it's it's

999
00:41:09,420 --> 00:41:11,579
possible it's easy we will show you a

1000
00:41:11,579 --> 00:41:14,339
demo later and

1001
00:41:14,339 --> 00:41:17,880
yeah python documentation clearly states

1002
00:41:17,880 --> 00:41:20,339
that this is not something that should

1003
00:41:20,339 --> 00:41:21,960
be used in production that this is not

1004
00:41:21,960 --> 00:41:24,540
something that we should be trusting but

1005
00:41:24,540 --> 00:41:27,359
still lots of models in uh in model Zeus

1006
00:41:27,359 --> 00:41:31,140
are Pico based and we don't know who

1007
00:41:31,140 --> 00:41:33,540
uploaded those models there and what

1008
00:41:33,540 --> 00:41:35,820
they can actually uh

1009
00:41:35,820 --> 00:41:38,000
store

1010
00:41:38,000 --> 00:41:40,859
another thing that can be done which is

1011
00:41:40,859 --> 00:41:44,420
also a bit of fun is model steganography

1012
00:41:44,420 --> 00:41:46,619
steganography and the traditional

1013
00:41:46,619 --> 00:41:48,780
version of psychography is hiding

1014
00:41:48,780 --> 00:41:51,500
messages inside a picture for example

1015
00:41:51,500 --> 00:41:54,660
which can't be seen the picture is looks

1016
00:41:54,660 --> 00:41:56,579
like it was not modified but there is

1017
00:41:56,579 --> 00:41:58,980
something inside it also my shoes

1018
00:41:58,980 --> 00:42:00,839
payloads can be stored in images like

1019
00:42:00,839 --> 00:42:02,880
that but they also can be stored in

1020
00:42:02,880 --> 00:42:06,800
machine learning models

1021
00:42:07,220 --> 00:42:11,760
so this might be a bit technical slide

1022
00:42:11,760 --> 00:42:14,880
but this is a inside of

1023
00:42:14,880 --> 00:42:18,839
a resonant model we see the data pkl

1024
00:42:18,839 --> 00:42:22,200
file which is a pickle actually a pickle

1025
00:42:22,200 --> 00:42:24,960
serialized model structure file and then

1026
00:42:24,960 --> 00:42:26,880
we have a bunch of files in the data

1027
00:42:26,880 --> 00:42:30,480
folder which contain all the floating

1028
00:42:30,480 --> 00:42:32,880
Point numbers associated with the neural

1029
00:42:32,880 --> 00:42:34,560
network

1030
00:42:34,560 --> 00:42:37,260
so if we open one of those files what we

1031
00:42:37,260 --> 00:42:39,619
see is basically floating Point numbers

1032
00:42:39,619 --> 00:42:43,980
and in this way in the same way as with

1033
00:42:43,980 --> 00:42:46,220
the

1034
00:42:46,280 --> 00:42:49,020
traditional steganography an attacker

1035
00:42:49,020 --> 00:42:52,440
could modify those floating Point values

1036
00:42:52,440 --> 00:42:54,900
in order to store a payload inside them

1037
00:42:54,900 --> 00:42:57,359
so the attacker can modify as little as

1038
00:42:57,359 --> 00:42:58,920
three bits

1039
00:42:58,920 --> 00:43:01,440
three least significant bits of all of

1040
00:43:01,440 --> 00:43:03,960
those floating Point numbers

1041
00:43:03,960 --> 00:43:05,880
to store a payload inside the model

1042
00:43:05,880 --> 00:43:08,099
without making the model less accurate

1043
00:43:08,099 --> 00:43:11,060
the model will be

1044
00:43:12,000 --> 00:43:15,000
can can be totally the same in accuracy

1045
00:43:15,000 --> 00:43:17,819
maybe with super slight change but

1046
00:43:17,819 --> 00:43:20,819
totally invisible to the human eye in

1047
00:43:20,819 --> 00:43:23,700
this example we we try to change 16

1048
00:43:23,700 --> 00:43:25,859
bytes at least significant by its

1049
00:43:25,859 --> 00:43:28,220
overflowed it gives

1050
00:43:28,220 --> 00:43:31,440
a slight difference not so much of a

1051
00:43:31,440 --> 00:43:34,020
difference and in in case of some models

1052
00:43:34,020 --> 00:43:36,240
that might be enough but yeah it's

1053
00:43:36,240 --> 00:43:38,339
possible to do it with just three bits

1054
00:43:38,339 --> 00:43:40,560
and machine learning models nowadays are

1055
00:43:40,560 --> 00:43:43,079
quite big so even if it's just three

1056
00:43:43,079 --> 00:43:45,300
bits of each of the float values we

1057
00:43:45,300 --> 00:43:47,160
would should be able to

1058
00:43:47,160 --> 00:43:50,700
inject quite a big payloads inside those

1059
00:43:50,700 --> 00:43:53,180
models

1060
00:43:53,760 --> 00:43:55,859
um here we have a demo

1061
00:43:55,859 --> 00:43:58,079
will be a bit funny

1062
00:43:58,079 --> 00:44:00,599
how do I do that

1063
00:44:00,599 --> 00:44:02,760
right I'm now an expert I'm now an

1064
00:44:02,760 --> 00:44:05,839
expert on Cinema screens

1065
00:44:06,619 --> 00:44:09,300
yeah yeah

1066
00:44:09,300 --> 00:44:10,859
but there

1067
00:44:10,859 --> 00:44:14,880
uh cool I'll start

1068
00:44:14,880 --> 00:44:16,079
there's a full screen full screen

1069
00:44:16,079 --> 00:44:17,640
perfect I'll just started again as well

1070
00:44:17,640 --> 00:44:20,160
so yeah pause it pause it

1071
00:44:20,160 --> 00:44:23,400
sorry I thought I did ah

1072
00:44:23,400 --> 00:44:25,740
ah good

1073
00:44:25,740 --> 00:44:28,160
all right

1074
00:44:28,160 --> 00:44:30,540
this is more difficult than it looks

1075
00:44:30,540 --> 00:44:33,140
I'll tell you that

1076
00:44:34,200 --> 00:44:36,839
as well it's not like me full screen

1077
00:44:36,839 --> 00:44:38,220
let's not letting me full screen it this

1078
00:44:38,220 --> 00:44:40,740
might have to do

1079
00:44:40,740 --> 00:44:43,098
okay

1080
00:44:44,099 --> 00:44:46,640
do you want to go

1081
00:44:47,760 --> 00:44:50,660
so um

1082
00:44:59,460 --> 00:45:02,119
man

1083
00:45:04,160 --> 00:45:06,660
if you just go to the beginning and stop

1084
00:45:06,660 --> 00:45:08,160
it for a bit

1085
00:45:08,160 --> 00:45:11,280
I'll just I'll just hold it all right so

1086
00:45:11,280 --> 00:45:12,960
um

1087
00:45:12,960 --> 00:45:16,140
to Showcase a scenario in which

1088
00:45:16,140 --> 00:45:18,839
typical cyber criminal could use a

1089
00:45:18,839 --> 00:45:21,240
machine learning model in order to gain

1090
00:45:21,240 --> 00:45:24,720
foothold in a victim system we developed

1091
00:45:24,720 --> 00:45:27,359
a couple of scripts it's not nothing

1092
00:45:27,359 --> 00:45:29,940
super sophisticated so we have one

1093
00:45:29,940 --> 00:45:32,220
script that will embed a payload which

1094
00:45:32,220 --> 00:45:35,579
we chose Quantum run somewhere for uh

1095
00:45:35,579 --> 00:45:39,240
just to Showcase but the scenario but it

1096
00:45:39,240 --> 00:45:42,660
could be anything really any any malware

1097
00:45:42,660 --> 00:45:48,000
and then another script to inject this

1098
00:45:48,000 --> 00:45:51,000
payload inside the machine learning

1099
00:45:51,000 --> 00:45:53,819
neural networks using the steganography

1100
00:45:53,819 --> 00:45:57,180
technique that we showed before and on

1101
00:45:57,180 --> 00:45:59,280
top of that we have one python script

1102
00:45:59,280 --> 00:46:01,020
that will decode this payload from the

1103
00:46:01,020 --> 00:46:02,540
steganography

1104
00:46:02,540 --> 00:46:05,480
embedded bits

1105
00:46:05,480 --> 00:46:10,440
reconstructed and run it and finally won

1106
00:46:10,440 --> 00:46:14,280
a script that will inject this uh

1107
00:46:14,280 --> 00:46:17,280
previous script into memory so so it's

1108
00:46:17,280 --> 00:46:19,560
just four scripts it's nothing very

1109
00:46:19,560 --> 00:46:22,700
complicated and

1110
00:46:25,980 --> 00:46:28,440
yeah is it playing or

1111
00:46:28,440 --> 00:46:30,980
once again

1112
00:46:31,500 --> 00:46:34,160
all right

1113
00:46:36,180 --> 00:46:38,220
so yeah just to show that the scripts

1114
00:46:38,220 --> 00:46:40,380
are not really complicated a really

1115
00:46:40,380 --> 00:46:42,720
quick run through them

1116
00:46:42,720 --> 00:46:44,579
um not gonna go into details we don't

1117
00:46:44,579 --> 00:46:46,920
have much time but yeah those are

1118
00:46:46,920 --> 00:46:50,280
scripts all of them are yeah 50 60 lines

1119
00:46:50,280 --> 00:46:53,700
of code maybe 100 with with the comments

1120
00:46:53,700 --> 00:46:56,579
so that's

1121
00:46:56,579 --> 00:46:59,400
that's not really sophisticated so let's

1122
00:46:59,400 --> 00:47:01,260
see we have a resonant model original

1123
00:47:01,260 --> 00:47:03,780
resin model we downloaded from the

1124
00:47:03,780 --> 00:47:06,359
internet check we checked the checksum

1125
00:47:06,359 --> 00:47:08,460
so to see that those this model was not

1126
00:47:08,460 --> 00:47:10,380
tampered with that's the correct check

1127
00:47:10,380 --> 00:47:12,000
so that is shown now

1128
00:47:12,000 --> 00:47:16,800
after now we embed our payload inside a

1129
00:47:16,800 --> 00:47:19,400
Shell Code

1130
00:47:20,819 --> 00:47:25,560
and we get the payload the pi piy script

1131
00:47:25,560 --> 00:47:27,839
which is basically injecting this Shell

1132
00:47:27,839 --> 00:47:30,799
Code into the memory

1133
00:47:34,079 --> 00:47:36,960
and now we run our torch technography

1134
00:47:36,960 --> 00:47:38,880
script

1135
00:47:38,880 --> 00:47:42,480
in order to inject that payload.pi y

1136
00:47:42,480 --> 00:47:46,099
inside the model neurons

1137
00:47:49,079 --> 00:47:51,780
we found a layer big enough for it

1138
00:47:51,780 --> 00:47:54,300
embedded it now we can see that the the

1139
00:47:54,300 --> 00:47:57,480
hash of the file changed we can't it's

1140
00:47:57,480 --> 00:47:59,339
not going to be any visual change in the

1141
00:47:59,339 --> 00:48:02,819
model just has changed

1142
00:48:02,819 --> 00:48:05,339
and now we what we need to do is to

1143
00:48:05,339 --> 00:48:09,060
inject our script which will decode the

1144
00:48:09,060 --> 00:48:11,480
payload inside the

1145
00:48:11,480 --> 00:48:14,339
resnet model

1146
00:48:14,339 --> 00:48:18,000
using the serialization vulnerability so

1147
00:48:18,000 --> 00:48:20,099
now if we open the resident model we can

1148
00:48:20,099 --> 00:48:22,619
see the script in plain text there it's

1149
00:48:22,619 --> 00:48:25,560
uh yeah it's crude we could obviously

1150
00:48:25,560 --> 00:48:27,960
obfuscate it but for reason of this demo

1151
00:48:27,960 --> 00:48:30,780
it's totally enough

1152
00:48:30,780 --> 00:48:33,420
and last thing to do is see if that

1153
00:48:33,420 --> 00:48:35,579
model will be loaded to the memory and

1154
00:48:35,579 --> 00:48:38,160
what is going to happen so we run

1155
00:48:38,160 --> 00:48:42,779
python import search load

1156
00:48:42,900 --> 00:48:46,640
uh give the path to the model

1157
00:48:47,339 --> 00:48:51,740
and voila the system is owned

1158
00:48:54,420 --> 00:48:57,140
thank you

1159
00:48:58,260 --> 00:49:01,560
yeah that's uh there are somewhere redmi

1160
00:49:01,560 --> 00:49:03,619
note

1161
00:49:06,119 --> 00:49:08,720
all right

1162
00:49:11,819 --> 00:49:15,859
okay and let me go back up here

1163
00:49:18,300 --> 00:49:20,520
oh there we go

1164
00:49:20,520 --> 00:49:22,099
all right we need to Breeze through the

1165
00:49:22,099 --> 00:49:25,280
slides really

1166
00:49:25,560 --> 00:49:28,020
right we'll just go with this we need to

1167
00:49:28,020 --> 00:49:32,060
breathe oh yeah we don't have time okay

1168
00:49:33,240 --> 00:49:34,800
uh

1169
00:49:34,800 --> 00:49:37,619
so yeah just to um

1170
00:49:37,619 --> 00:49:40,619
summarize uh the attacker could use

1171
00:49:40,619 --> 00:49:44,220
those two techniques to actually

1172
00:49:44,220 --> 00:49:46,980
perform uh for example supply chain

1173
00:49:46,980 --> 00:49:51,240
attacks or deploy malware or ransomware

1174
00:49:51,240 --> 00:49:54,000
or get a foothold inside the victim

1175
00:49:54,000 --> 00:49:56,099
system and this is not just proof of

1176
00:49:56,099 --> 00:49:59,760
concept research uh we actually are

1177
00:49:59,760 --> 00:50:03,060
finding daily right now malicious files

1178
00:50:03,060 --> 00:50:05,339
malicious pickles but not only pickles

1179
00:50:05,339 --> 00:50:10,800
also Keras models uh in in varistotle or

1180
00:50:10,800 --> 00:50:14,520
in those repository model repositories

1181
00:50:14,520 --> 00:50:16,920
so these things are happening already

1182
00:50:16,920 --> 00:50:19,260
they are crude attempts right now but

1183
00:50:19,260 --> 00:50:21,980
they are happening

1184
00:50:22,560 --> 00:50:28,400
so the rest is just closing notes

1185
00:50:28,859 --> 00:50:31,260
so I think we all know the you know the

1186
00:50:31,260 --> 00:50:33,839
stories of AI gone Rogue Skynet the

1187
00:50:33,839 --> 00:50:35,940
cylons to name a few

1188
00:50:35,940 --> 00:50:37,260
um but you know we're not really there

1189
00:50:37,260 --> 00:50:39,480
yet and I think the the we really need

1190
00:50:39,480 --> 00:50:42,359
to worry about us attacking AI rather

1191
00:50:42,359 --> 00:50:45,000
than AI attacking us just at this minute

1192
00:50:45,000 --> 00:50:46,440
um because it's in everything it's

1193
00:50:46,440 --> 00:50:47,819
ubiquitous

1194
00:50:47,819 --> 00:50:49,500
um you know it's being introduced into

1195
00:50:49,500 --> 00:50:51,420
every product you know about and I can

1196
00:50:51,420 --> 00:50:53,280
guarantee almost every product that RSA

1197
00:50:53,280 --> 00:50:54,780
is going to be talking about their Ai

1198
00:50:54,780 --> 00:50:56,400
and their product

1199
00:50:56,400 --> 00:50:58,319
um and so yeah it's just

1200
00:50:58,319 --> 00:51:00,119
okay that's it

1201
00:51:00,119 --> 00:51:03,059
all right we don't have much time or

1202
00:51:03,059 --> 00:51:06,440
this um are some techniques to actually

1203
00:51:06,440 --> 00:51:09,180
improve the security of machine learning

1204
00:51:09,180 --> 00:51:10,980
models so there are basically two

1205
00:51:10,980 --> 00:51:13,020
approaches one is to make the model more

1206
00:51:13,020 --> 00:51:16,020
robust it's a valid approach another one

1207
00:51:16,020 --> 00:51:18,119
is to monitor the input and output to

1208
00:51:18,119 --> 00:51:21,240
the model to see actually uh

1209
00:51:21,240 --> 00:51:23,940
suspicious traffic and I think we should

1210
00:51:23,940 --> 00:51:25,440
combine both of those approaches

1211
00:51:25,440 --> 00:51:28,020
together with definitely model signing

1212
00:51:28,020 --> 00:51:31,079
model Integrity checks and the scanning

1213
00:51:31,079 --> 00:51:34,319
for malware we should just be aware that

1214
00:51:34,319 --> 00:51:37,140
all those threats exist okay that's it

1215
00:51:37,140 --> 00:51:40,140
for today we're out of time thank you

1216
00:51:40,140 --> 00:51:41,770
thank you for coming

1217
00:51:41,770 --> 00:51:44,920
[Applause]

