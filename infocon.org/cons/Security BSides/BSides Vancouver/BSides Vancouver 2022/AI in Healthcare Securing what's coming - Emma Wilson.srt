1
00:00:06,480 --> 00:00:08,559
great awesome

2
00:00:08,559 --> 00:00:10,480
well thank you so much for the

3
00:00:10,480 --> 00:00:12,639
opportunity to present

4
00:00:12,639 --> 00:00:16,000
at b-sides i'm thrilled to be here my

5
00:00:16,000 --> 00:00:18,960
name is emma wilson and i'm a security

6
00:00:18,960 --> 00:00:21,600
consultant at accenture

7
00:00:21,600 --> 00:00:23,920
i've had many very exciting projects

8
00:00:23,920 --> 00:00:26,400
over the last few years of my career

9
00:00:26,400 --> 00:00:28,160
from being a security analyst

10
00:00:28,160 --> 00:00:31,679
implementing antivirus and firewalls

11
00:00:31,679 --> 00:00:34,640
to being a systems auditor

12
00:00:34,640 --> 00:00:36,559
and most recently i've had the privilege

13
00:00:36,559 --> 00:00:38,800
of being a cybersecurity project manager

14
00:00:38,800 --> 00:00:40,480
for one of the largest health

15
00:00:40,480 --> 00:00:42,399
authorities in bc

16
00:00:42,399 --> 00:00:45,440
so it's been a really fun time so far

17
00:00:45,440 --> 00:00:47,840
last summer i had a really exciting

18
00:00:47,840 --> 00:00:51,520
engagement with a client based in geneva

19
00:00:51,520 --> 00:00:53,600
where our team looked at different

20
00:00:53,600 --> 00:00:55,760
applications of artificial intelligence

21
00:00:55,760 --> 00:00:57,920
and healthcare

22
00:00:57,920 --> 00:00:59,760
in many different countries in the

23
00:00:59,760 --> 00:01:01,120
pacific

24
00:01:01,120 --> 00:01:02,480
i'll just minimize my screen for a

25
00:01:02,480 --> 00:01:04,000
second

26
00:01:04,000 --> 00:01:05,840
so what i really wanted to do with all

27
00:01:05,840 --> 00:01:08,479
that really great information was share

28
00:01:08,479 --> 00:01:10,320
some of the findings on ai and

29
00:01:10,320 --> 00:01:11,680
healthcare

30
00:01:11,680 --> 00:01:13,600
with a broader audience

31
00:01:13,600 --> 00:01:15,280
so really that's what this presentation

32
00:01:15,280 --> 00:01:16,640
is about is to share with you some of

33
00:01:16,640 --> 00:01:18,320
the findings um

34
00:01:18,320 --> 00:01:19,920
again this is more my own personal

35
00:01:19,920 --> 00:01:22,080
research and interest on ai and

36
00:01:22,080 --> 00:01:24,159
healthcare i think it's really where

37
00:01:24,159 --> 00:01:26,479
things are going

38
00:01:26,479 --> 00:01:28,640
certainly there's a lot of hiccups and

39
00:01:28,640 --> 00:01:32,240
roadblocks with adopting healthcare ai

40
00:01:32,240 --> 00:01:34,560
as you can imagine funding

41
00:01:34,560 --> 00:01:36,079
resourcing

42
00:01:36,079 --> 00:01:37,280
personnel

43
00:01:37,280 --> 00:01:40,320
there's just so much happening that is

44
00:01:40,320 --> 00:01:42,479
creating some barriers and challenges to

45
00:01:42,479 --> 00:01:44,399
adoption of machine learning and ai and

46
00:01:44,399 --> 00:01:45,520
health

47
00:01:45,520 --> 00:01:46,960
but i also think there's a lot of

48
00:01:46,960 --> 00:01:49,759
opportunity here and i think i think

49
00:01:49,759 --> 00:01:51,759
based on my research in

50
00:01:51,759 --> 00:01:54,799
asia so countries like china taiwan

51
00:01:54,799 --> 00:01:56,960
japan

52
00:01:56,960 --> 00:01:58,640
those kind of countries are adopting a

53
00:01:58,640 --> 00:02:00,240
lot of different health care

54
00:02:00,240 --> 00:02:03,520
applications things like robotics

55
00:02:03,520 --> 00:02:05,680
clinical decision support

56
00:02:05,680 --> 00:02:07,600
i think that because they have been

57
00:02:07,600 --> 00:02:09,360
adopting that it's really showing that

58
00:02:09,360 --> 00:02:11,520
this is possible it's proven in some

59
00:02:11,520 --> 00:02:14,959
jurisdictions um and therefore you know

60
00:02:14,959 --> 00:02:16,239
it's something that can certainly make

61
00:02:16,239 --> 00:02:18,640
its way over to canada in the future

62
00:02:18,640 --> 00:02:20,640
so just to clarify this is based on my

63
00:02:20,640 --> 00:02:22,319
own personal interest i hope you're

64
00:02:22,319 --> 00:02:23,840
interested as well

65
00:02:23,840 --> 00:02:25,840
and it's really a high level

66
00:02:25,840 --> 00:02:27,360
presentation looking at some of the

67
00:02:27,360 --> 00:02:29,040
applications

68
00:02:29,040 --> 00:02:31,200
some of the issues and concerns

69
00:02:31,200 --> 00:02:34,080
and some of the security risks

70
00:02:34,080 --> 00:02:36,879
so next slide here does anybody

71
00:02:36,879 --> 00:02:40,319
recognize who this is

72
00:02:41,200 --> 00:02:44,799
kind of a trick question for you all

73
00:02:45,440 --> 00:02:47,519
okay can anyone tell me what this slide

74
00:02:47,519 --> 00:02:50,680
is about

75
00:02:53,840 --> 00:02:55,599
okay i only have one monitor today so

76
00:02:55,599 --> 00:02:56,720
forgive me

77
00:02:56,720 --> 00:02:58,640
so this is lee sedol who is

78
00:02:58,640 --> 00:03:02,159
participating in a alphago tournament

79
00:03:02,159 --> 00:03:04,080
for those of you who follow ai and

80
00:03:04,080 --> 00:03:06,959
what's happening um alphago is an

81
00:03:06,959 --> 00:03:10,319
ancient chinese game and in 2016 there

82
00:03:10,319 --> 00:03:12,720
was a tournament where this um go

83
00:03:12,720 --> 00:03:14,560
champion um

84
00:03:14,560 --> 00:03:16,879
basically competed against alphago and

85
00:03:16,879 --> 00:03:19,680
alphago one and it was a hugely

86
00:03:19,680 --> 00:03:22,080
impactful event it really shook the

87
00:03:22,080 --> 00:03:24,400
um the technology and just the you know

88
00:03:24,400 --> 00:03:27,519
the world especially in asia and really

89
00:03:27,519 --> 00:03:30,159
for the first time we realized that this

90
00:03:30,159 --> 00:03:31,840
game which is considered to be very

91
00:03:31,840 --> 00:03:33,120
complex

92
00:03:33,120 --> 00:03:35,360
very challenging for a machine to learn

93
00:03:35,360 --> 00:03:37,519
lots of nuances that were considered to

94
00:03:37,519 --> 00:03:38,720
be something only humans could

95
00:03:38,720 --> 00:03:39,920
understand

96
00:03:39,920 --> 00:03:41,920
was something that could be achieved and

97
00:03:41,920 --> 00:03:44,640
understood and

98
00:03:44,640 --> 00:03:48,000
competed in by an ai so that's something

99
00:03:48,000 --> 00:03:49,920
that was really impactful and it and it

100
00:03:49,920 --> 00:03:52,879
really created shock waves throughout um

101
00:03:52,879 --> 00:03:56,239
much of asia now this was 2016. i have

102
00:03:56,239 --> 00:03:57,840
some suspicion that we've forgotten a

103
00:03:57,840 --> 00:04:00,480
little bit of this event and just how

104
00:04:00,480 --> 00:04:02,400
impactful it was

105
00:04:02,400 --> 00:04:04,799
but in many industries this this was

106
00:04:04,799 --> 00:04:06,879
seen as the event that

107
00:04:06,879 --> 00:04:09,360
proved what ai can do

108
00:04:09,360 --> 00:04:11,280
if machine learning and ai have been you

109
00:04:11,280 --> 00:04:13,439
know used in retail and finance and

110
00:04:13,439 --> 00:04:14,640
insurance

111
00:04:14,640 --> 00:04:17,440
for many many years before this

112
00:04:17,440 --> 00:04:19,759
but this event because it was so public

113
00:04:19,759 --> 00:04:21,839
and it was sort of fun it was a game

114
00:04:21,839 --> 00:04:25,600
really showed folks that ai can do some

115
00:04:25,600 --> 00:04:28,400
incredible tasks and and learn there are

116
00:04:28,400 --> 00:04:29,759
moments in the game if you watch the

117
00:04:29,759 --> 00:04:32,160
youtube if you're a huge nerd like i am

118
00:04:32,160 --> 00:04:34,880
where um this poor fellow lee sedol

119
00:04:34,880 --> 00:04:36,720
would just be shocked his jaw would drop

120
00:04:36,720 --> 00:04:39,360
and he was just completely perplexed by

121
00:04:39,360 --> 00:04:42,240
some of the moves the ai was making so

122
00:04:42,240 --> 00:04:45,120
really this showed the world that ai

123
00:04:45,120 --> 00:04:47,680
can learn complex tasks and can do

124
00:04:47,680 --> 00:04:50,240
things that people can do

125
00:04:50,240 --> 00:04:52,160
so you might be wondering okay emma get

126
00:04:52,160 --> 00:04:53,759
to the health care

127
00:04:53,759 --> 00:04:55,120
so

128
00:04:55,120 --> 00:04:57,280
ai has applications in many many

129
00:04:57,280 --> 00:04:59,919
industries as i'm sure you're aware

130
00:04:59,919 --> 00:05:01,520
finance

131
00:05:01,520 --> 00:05:03,440
you know insurance

132
00:05:03,440 --> 00:05:05,759
mining there's many many applications of

133
00:05:05,759 --> 00:05:07,520
ai

134
00:05:07,520 --> 00:05:09,120
i will be honest with you that my

135
00:05:09,120 --> 00:05:12,800
research has indicated to me that

136
00:05:12,800 --> 00:05:14,800
a lot of ai applications are not

137
00:05:14,800 --> 00:05:17,039
healthcare focused right now and this is

138
00:05:17,039 --> 00:05:19,840
a sector that's been underserved

139
00:05:19,840 --> 00:05:22,320
by this new technology

140
00:05:22,320 --> 00:05:23,759
in general

141
00:05:23,759 --> 00:05:25,280
however there is also a lot of

142
00:05:25,280 --> 00:05:27,680
excitement in healthcare um i think

143
00:05:27,680 --> 00:05:30,000
because we see healthcare as

144
00:05:30,000 --> 00:05:32,160
this area of work that can be really

145
00:05:32,160 --> 00:05:34,720
impactful to society as a whole and can

146
00:05:34,720 --> 00:05:37,360
really improve the lives of many people

147
00:05:37,360 --> 00:05:39,520
many other countries not just canada

148
00:05:39,520 --> 00:05:42,320
have physicians shortages and are really

149
00:05:42,320 --> 00:05:44,479
struggling with serving large aging

150
00:05:44,479 --> 00:05:47,199
populations for instance china and japan

151
00:05:47,199 --> 00:05:49,039
i interviewed many doctors there in this

152
00:05:49,039 --> 00:05:51,520
engagement last summer who note that if

153
00:05:51,520 --> 00:05:53,680
you could have just like in it and

154
00:05:53,680 --> 00:05:56,000
computer science it service management

155
00:05:56,000 --> 00:05:58,560
tier one tier two tier three um health

156
00:05:58,560 --> 00:06:00,800
care service where tier one perhaps an

157
00:06:00,800 --> 00:06:04,000
ai or a chat bot talks to you and says

158
00:06:04,000 --> 00:06:05,199
you know

159
00:06:05,199 --> 00:06:07,280
hey how are you feeling what are your

160
00:06:07,280 --> 00:06:09,680
symptoms send me some pictures

161
00:06:09,680 --> 00:06:11,199
and you send that in and then they

162
00:06:11,199 --> 00:06:13,120
determine whether to you know send you

163
00:06:13,120 --> 00:06:15,039
off to a to a doctor

164
00:06:15,039 --> 00:06:17,440
um so that's one application

165
00:06:17,440 --> 00:06:20,160
um and as well there's there's so many

166
00:06:20,160 --> 00:06:21,759
applications i think the the one that i

167
00:06:21,759 --> 00:06:23,520
think is most palatable if i was going

168
00:06:23,520 --> 00:06:25,680
to leave you with one um application

169
00:06:25,680 --> 00:06:27,520
that i think is palatable now

170
00:06:27,520 --> 00:06:30,319
um it would be radiology

171
00:06:30,319 --> 00:06:32,319
the reason why is there's been a history

172
00:06:32,319 --> 00:06:34,319
of standardized quality data sets for

173
00:06:34,319 --> 00:06:36,639
decades in radiology and there's a lot

174
00:06:36,639 --> 00:06:38,479
of training data out there in the format

175
00:06:38,479 --> 00:06:39,680
of images

176
00:06:39,680 --> 00:06:42,400
um now images as you might know you know

177
00:06:42,400 --> 00:06:44,160
they can contain personal information

178
00:06:44,160 --> 00:06:45,919
phi

179
00:06:45,919 --> 00:06:48,080
however there there are standardized

180
00:06:48,080 --> 00:06:50,319
approaches to cataloging them

181
00:06:50,319 --> 00:06:51,919
and to

182
00:06:51,919 --> 00:06:53,919
basically linking what the images of

183
00:06:53,919 --> 00:06:57,280
with a label or title

184
00:06:57,280 --> 00:06:59,360
so really if you go into a radiologist

185
00:06:59,360 --> 00:07:01,039
there's this all this data is already

186
00:07:01,039 --> 00:07:03,919
available it's relatively easy to train

187
00:07:03,919 --> 00:07:06,240
ais to use that data to make

188
00:07:06,240 --> 00:07:09,840
recommendations and predictions

189
00:07:09,840 --> 00:07:11,520
and again like i said with the privacy

190
00:07:11,520 --> 00:07:13,919
aspect there are ways of anonymizing or

191
00:07:13,919 --> 00:07:17,680
de-identifying images but it's less

192
00:07:17,680 --> 00:07:19,840
i'm going to say it's less of a risk in

193
00:07:19,840 --> 00:07:22,000
the sense that the data is not as

194
00:07:22,000 --> 00:07:23,440
um

195
00:07:23,440 --> 00:07:25,199
obviously a you know a person if you're

196
00:07:25,199 --> 00:07:26,800
putting tons and tons of data into a

197
00:07:26,800 --> 00:07:28,800
training data model

198
00:07:28,800 --> 00:07:31,599
it's just images and therefore it's a

199
00:07:31,599 --> 00:07:32,800
little bit less

200
00:07:32,800 --> 00:07:35,039
i suppose controversial when you start

201
00:07:35,039 --> 00:07:37,120
to build algorithms that are using those

202
00:07:37,120 --> 00:07:38,479
images to

203
00:07:38,479 --> 00:07:40,160
make diagnoses

204
00:07:40,160 --> 00:07:42,840
um patient monitoring robot assisted

205
00:07:42,840 --> 00:07:44,960
surgery cyber security just

206
00:07:44,960 --> 00:07:47,599
administration booking appointments

207
00:07:47,599 --> 00:07:49,840
ais that can crawl through emr

208
00:07:49,840 --> 00:07:52,080
electronic medical record systems to

209
00:07:52,080 --> 00:07:53,759
find patients that haven't been in in a

210
00:07:53,759 --> 00:07:56,319
while who have high risk factors

211
00:07:56,319 --> 00:07:59,199
and who can send alerts or text messages

212
00:07:59,199 --> 00:08:00,960
to those patients to say hey you haven't

213
00:08:00,960 --> 00:08:02,720
been in in a while

214
00:08:02,720 --> 00:08:04,319
and the last one there is clinical

215
00:08:04,319 --> 00:08:07,440
decision support or cds

216
00:08:07,440 --> 00:08:08,639
so that again is kind of what i

217
00:08:08,639 --> 00:08:10,479
mentioned with that tier 1 tier 2 is you

218
00:08:10,479 --> 00:08:12,560
could have um

219
00:08:12,560 --> 00:08:14,800
really ais that look at data on a

220
00:08:14,800 --> 00:08:17,280
patient and help to determine

221
00:08:17,280 --> 00:08:18,560
the decision of whether they should

222
00:08:18,560 --> 00:08:20,160
actually go into an office to see a

223
00:08:20,160 --> 00:08:21,360
person

224
00:08:21,360 --> 00:08:24,240
that's one you know aspect of this and

225
00:08:24,240 --> 00:08:27,039
other aspect of this with cds is a

226
00:08:27,039 --> 00:08:29,199
doctor is seeing a patient and based on

227
00:08:29,199 --> 00:08:32,159
the records of that patient in their emr

228
00:08:32,159 --> 00:08:34,159
file electronic medical record system

229
00:08:34,159 --> 00:08:37,039
file the doctor is suggested that this

230
00:08:37,039 --> 00:08:39,279
patient might have high blood pressure

231
00:08:39,279 --> 00:08:40,958
the doctor might still be then

232
00:08:40,958 --> 00:08:43,919
validating that but um

233
00:08:43,919 --> 00:08:45,680
you can kind of see how this could save

234
00:08:45,680 --> 00:08:48,320
time right and save energy and resources

235
00:08:48,320 --> 00:08:49,120
for

236
00:08:49,120 --> 00:08:50,880
resource-strapped physicians and doctors

237
00:08:50,880 --> 00:08:52,560
and and other nurses and healthcare

238
00:08:52,560 --> 00:08:54,560
practitioners so

239
00:08:54,560 --> 00:08:56,800
many many many applications of ai in

240
00:08:56,800 --> 00:08:58,800
healthcare

241
00:08:58,800 --> 00:09:01,200
um what is happening in canada i really

242
00:09:01,200 --> 00:09:02,880
want to focus on that i think that's

243
00:09:02,880 --> 00:09:05,839
what we're interested in today

244
00:09:05,839 --> 00:09:08,160
so there's been a history of

245
00:09:08,160 --> 00:09:10,240
digitization and digital health in

246
00:09:10,240 --> 00:09:13,200
canada and the last 30 years have seen

247
00:09:13,200 --> 00:09:14,560
you know incredible growth we all know

248
00:09:14,560 --> 00:09:17,040
that we're all here

249
00:09:17,040 --> 00:09:19,040
but really based on my research what i'd

250
00:09:19,040 --> 00:09:21,519
like to offer

251
00:09:21,519 --> 00:09:23,040
is that canada has been a little bit

252
00:09:23,040 --> 00:09:24,320
risk-averse

253
00:09:24,320 --> 00:09:25,920
and a little slower

254
00:09:25,920 --> 00:09:29,279
in approaching ai in healthcare

255
00:09:29,279 --> 00:09:30,560
and there's many reasons for that we

256
00:09:30,560 --> 00:09:32,240
could go into

257
00:09:32,240 --> 00:09:33,760
but what i really kind of want to offer

258
00:09:33,760 --> 00:09:35,279
is that

259
00:09:35,279 --> 00:09:38,160
really the last three or four years five

260
00:09:38,160 --> 00:09:39,760
six years as you can see on this little

261
00:09:39,760 --> 00:09:41,279
timeline and i've just i've just

262
00:09:41,279 --> 00:09:42,640
selected a few events that i thought

263
00:09:42,640 --> 00:09:44,640
were interesting to you

264
00:09:44,640 --> 00:09:47,120
there has been really an explosion of

265
00:09:47,120 --> 00:09:49,519
investment from mostly public sector

266
00:09:49,519 --> 00:09:50,800
sources

267
00:09:50,800 --> 00:09:53,440
into healthcare ai

268
00:09:53,440 --> 00:09:55,600
now why is this

269
00:09:55,600 --> 00:09:58,000
clearly there's there's a seen value or

270
00:09:58,000 --> 00:10:01,120
perceived value of ai in in benefiting

271
00:10:01,120 --> 00:10:03,440
canadians and benefiting our care

272
00:10:03,440 --> 00:10:05,519
institutions and systems

273
00:10:05,519 --> 00:10:07,600
so we've had really just incredible

274
00:10:07,600 --> 00:10:10,399
investments in the last few years

275
00:10:10,399 --> 00:10:12,480
and recently as well

276
00:10:12,480 --> 00:10:14,000
there's been more money in the most

277
00:10:14,000 --> 00:10:16,160
recent budget so

278
00:10:16,160 --> 00:10:17,360
really looking at supporting a

279
00:10:17,360 --> 00:10:19,920
pan-canadian ai strategy

280
00:10:19,920 --> 00:10:23,120
so what else is happening in canada

281
00:10:23,120 --> 00:10:24,800
there's a lot of research happening in

282
00:10:24,800 --> 00:10:26,560
canada so we've had a lot of

283
00:10:26,560 --> 00:10:28,800
organizations for instance in alberta in

284
00:10:28,800 --> 00:10:30,959
toronto in montreal

285
00:10:30,959 --> 00:10:33,040
looking at ai applications not just in

286
00:10:33,040 --> 00:10:35,279
healthcare but in many sectors

287
00:10:35,279 --> 00:10:36,880
so i have one example on this slide

288
00:10:36,880 --> 00:10:39,920
which is in 1993 mila was founded at the

289
00:10:39,920 --> 00:10:42,240
university of montreal

290
00:10:42,240 --> 00:10:43,920
that is a research institute that looks

291
00:10:43,920 --> 00:10:46,160
at ai in many different applications

292
00:10:46,160 --> 00:10:47,839
healthcare is one of them

293
00:10:47,839 --> 00:10:50,160
so for instance mila helped to develop a

294
00:10:50,160 --> 00:10:51,200
medical

295
00:10:51,200 --> 00:10:53,839
chatbot as one example

296
00:10:53,839 --> 00:10:55,519
so it's happening it is happening and

297
00:10:55,519 --> 00:10:58,079
it's something that's been really in my

298
00:10:58,079 --> 00:11:00,320
in my view happening more in the last

299
00:11:00,320 --> 00:11:02,000
five or six years

300
00:11:02,000 --> 00:11:03,279
um

301
00:11:03,279 --> 00:11:05,200
but there is a but to this and i think

302
00:11:05,200 --> 00:11:07,839
the the the thing that is in my mind

303
00:11:07,839 --> 00:11:09,360
that you can probably understand if i'll

304
00:11:09,360 --> 00:11:10,800
go to the next slide

305
00:11:10,800 --> 00:11:12,800
is we are in my

306
00:11:12,800 --> 00:11:15,120
in my view based on my research lacking

307
00:11:15,120 --> 00:11:16,079
some

308
00:11:16,079 --> 00:11:19,279
some key enablers in canada

309
00:11:19,279 --> 00:11:21,279
in bringing um

310
00:11:21,279 --> 00:11:24,720
ai into the healthcare space

311
00:11:24,720 --> 00:11:25,680
so

312
00:11:25,680 --> 00:11:27,519
you might be wondering you know where is

313
00:11:27,519 --> 00:11:29,760
the money for this coming from

314
00:11:29,760 --> 00:11:31,440
you know healthcare and healthcare

315
00:11:31,440 --> 00:11:33,200
institutions already struggle in some

316
00:11:33,200 --> 00:11:34,480
respects

317
00:11:34,480 --> 00:11:38,240
in bringing innovative technology in

318
00:11:38,240 --> 00:11:40,079
so really i've sort of answered that

319
00:11:40,079 --> 00:11:41,360
question which is government is really

320
00:11:41,360 --> 00:11:43,839
really pushing for this

321
00:11:43,839 --> 00:11:46,640
um we're really investing in it from a

322
00:11:46,640 --> 00:11:48,320
canadian government perspective and

323
00:11:48,320 --> 00:11:50,320
certainly many universities local

324
00:11:50,320 --> 00:11:51,519
colleges

325
00:11:51,519 --> 00:11:52,560
and

326
00:11:52,560 --> 00:11:54,399
you know provincial governments are also

327
00:11:54,399 --> 00:11:56,800
investing in it

328
00:11:56,800 --> 00:11:59,360
so really we should be seeing i'm hoping

329
00:11:59,360 --> 00:12:01,200
with all this investment some really

330
00:12:01,200 --> 00:12:03,760
healthy new pilots um

331
00:12:03,760 --> 00:12:05,920
because what's happening in canada in

332
00:12:05,920 --> 00:12:09,120
the last you know few few years of

333
00:12:09,120 --> 00:12:11,200
trying to explore ai in healthcare

334
00:12:11,200 --> 00:12:13,680
is a lot of research has

335
00:12:13,680 --> 00:12:17,040
died in this um

336
00:12:17,040 --> 00:12:18,880
pilot phase so you get a lot of

337
00:12:18,880 --> 00:12:21,760
institutions organizations universities

338
00:12:21,760 --> 00:12:24,639
who have great idea ideas and they pilot

339
00:12:24,639 --> 00:12:26,480
them and they explore some test data and

340
00:12:26,480 --> 00:12:28,720
then there's just a huge barrier to

341
00:12:28,720 --> 00:12:29,920
getting this

342
00:12:29,920 --> 00:12:33,440
technology to clinical applications

343
00:12:33,440 --> 00:12:35,519
so why is that there's there's many

344
00:12:35,519 --> 00:12:37,519
reasons i think a really good reason is

345
00:12:37,519 --> 00:12:39,440
simply that

346
00:12:39,440 --> 00:12:41,279
healthcare organizations are already

347
00:12:41,279 --> 00:12:42,959
resource stressed

348
00:12:42,959 --> 00:12:44,800
and it's really challenging to take the

349
00:12:44,800 --> 00:12:48,240
time and energy necessary to work with

350
00:12:48,240 --> 00:12:50,000
a university to bring in new

351
00:12:50,000 --> 00:12:51,680
technologies

352
00:12:51,680 --> 00:12:54,399
this second

353
00:12:54,399 --> 00:12:56,800
line of date of information here on data

354
00:12:56,800 --> 00:12:58,880
is also really key

355
00:12:58,880 --> 00:13:01,440
so in canada we do have

356
00:13:01,440 --> 00:13:03,440
a lot of great big quality data sets

357
00:13:03,440 --> 00:13:05,519
certainly in ontario for example you can

358
00:13:05,519 --> 00:13:07,760
get health data there from decades and

359
00:13:07,760 --> 00:13:09,600
decades past

360
00:13:09,600 --> 00:13:11,519
so we have lots of data

361
00:13:11,519 --> 00:13:14,320
however is it high quality data so in

362
00:13:14,320 --> 00:13:16,079
some of my research from

363
00:13:16,079 --> 00:13:17,680
last summer when i was talking to many

364
00:13:17,680 --> 00:13:20,560
experts in ai many startup founders in

365
00:13:20,560 --> 00:13:21,760
asia

366
00:13:21,760 --> 00:13:22,959
one of the things they mentioned was

367
00:13:22,959 --> 00:13:26,000
that a lot of times data exists in very

368
00:13:26,000 --> 00:13:28,480
separated systems and databases and it's

369
00:13:28,480 --> 00:13:30,240
not being connected

370
00:13:30,240 --> 00:13:32,399
there might be a lack of standardization

371
00:13:32,399 --> 00:13:35,600
there may be fears over privacy and

372
00:13:35,600 --> 00:13:37,839
you know the regulatory parameters

373
00:13:37,839 --> 00:13:40,639
around how that data can be used

374
00:13:40,639 --> 00:13:42,480
um the other thing is the data is just

375
00:13:42,480 --> 00:13:44,079
really messy quite often i know i'm

376
00:13:44,079 --> 00:13:45,360
talking to the audience who will

377
00:13:45,360 --> 00:13:46,720
understand

378
00:13:46,720 --> 00:13:48,720
when you're dealing with large data sets

379
00:13:48,720 --> 00:13:50,320
uh sometimes the data is not high

380
00:13:50,320 --> 00:13:52,160
quality data in a format that will be

381
00:13:52,160 --> 00:13:55,519
usable um whether it's um you know an ai

382
00:13:55,519 --> 00:13:57,120
solution or any other kind of solution

383
00:13:57,120 --> 00:13:58,639
trying to use that data

384
00:13:58,639 --> 00:14:01,440
um there's some huge challenges there so

385
00:14:01,440 --> 00:14:02,800
there's actually been some startups for

386
00:14:02,800 --> 00:14:04,240
instance i talked to one fellow in

387
00:14:04,240 --> 00:14:06,639
taiwan whose whole company is it's a

388
00:14:06,639 --> 00:14:08,880
healthcare ai company and they're just

389
00:14:08,880 --> 00:14:10,000
looking at

390
00:14:10,000 --> 00:14:12,240
normalizing and making the data out of

391
00:14:12,240 --> 00:14:13,440
emrs

392
00:14:13,440 --> 00:14:15,120
better quality

393
00:14:15,120 --> 00:14:17,040
before it'll be fed into another another

394
00:14:17,040 --> 00:14:19,279
ai algorithm that will look for

395
00:14:19,279 --> 00:14:20,399
um

396
00:14:20,399 --> 00:14:22,000
some more um useful healthcare

397
00:14:22,000 --> 00:14:24,720
applications so to clarify there's an ai

398
00:14:24,720 --> 00:14:26,639
just trying to qual make the data better

399
00:14:26,639 --> 00:14:28,720
quality before it can be fed into

400
00:14:28,720 --> 00:14:30,240
different applications so i think that's

401
00:14:30,240 --> 00:14:31,839
something we can be wary of also

402
00:14:31,839 --> 00:14:33,920
happening in canada

403
00:14:33,920 --> 00:14:35,920
and i think in canada as well

404
00:14:35,920 --> 00:14:38,240
there's concerns around privacy so

405
00:14:38,240 --> 00:14:40,560
there's a need when looking at ai for

406
00:14:40,560 --> 00:14:42,399
understanding how to treat the data can

407
00:14:42,399 --> 00:14:45,519
we de-identify the data

408
00:14:45,519 --> 00:14:48,720
but i say not anonymization why is that

409
00:14:48,720 --> 00:14:51,040
in healthcare we need to use that data

410
00:14:51,040 --> 00:14:52,639
and it needs to be linked to a person in

411
00:14:52,639 --> 00:14:54,639
some way so for instance you might have

412
00:14:54,639 --> 00:14:56,959
large data sets from an emr and an

413
00:14:56,959 --> 00:14:59,199
electronic medical record system

414
00:14:59,199 --> 00:15:01,680
and you have an ai that can just just

415
00:15:01,680 --> 00:15:03,920
crawl that and look for high risk

416
00:15:03,920 --> 00:15:05,760
patterns or issues

417
00:15:05,760 --> 00:15:07,279
that need to be alerted so you might

418
00:15:07,279 --> 00:15:08,880
want to bring in that patient who has a

419
00:15:08,880 --> 00:15:12,399
high risk of blood pressure

420
00:15:12,399 --> 00:15:13,920
so the issue is you might want to

421
00:15:13,920 --> 00:15:15,839
de-identify the data when the ai's

422
00:15:15,839 --> 00:15:17,920
crawling it but there has to be some way

423
00:15:17,920 --> 00:15:19,440
to re-identify

424
00:15:19,440 --> 00:15:22,480
patients once an issue is noted

425
00:15:22,480 --> 00:15:25,199
so you need to call the patient or send

426
00:15:25,199 --> 00:15:27,760
them an email or you know otherwise

427
00:15:27,760 --> 00:15:29,519
contact them to come in you have to

428
00:15:29,519 --> 00:15:31,519
identify them to let them know they need

429
00:15:31,519 --> 00:15:33,440
a specialist to look at their at their

430
00:15:33,440 --> 00:15:35,279
health concern

431
00:15:35,279 --> 00:15:37,279
um the last kind of key enabler i want

432
00:15:37,279 --> 00:15:39,040
to talk about just briefly here i see

433
00:15:39,040 --> 00:15:40,959
i'm running out of time is culture

434
00:15:40,959 --> 00:15:43,279
people and skills um so key enablers of

435
00:15:43,279 --> 00:15:45,279
ai and healthcare

436
00:15:45,279 --> 00:15:48,480
really in many sectors in healthcare you

437
00:15:48,480 --> 00:15:50,399
have whether nurses and doctors whether

438
00:15:50,399 --> 00:15:51,759
they're administrative professionals

439
00:15:51,759 --> 00:15:54,079
whether they're you know t professionals

440
00:15:54,079 --> 00:15:56,399
they go to school for you know five six

441
00:15:56,399 --> 00:15:58,399
to ten years

442
00:15:58,399 --> 00:15:59,920
and then you also have

443
00:15:59,920 --> 00:16:00,720
these

444
00:16:00,720 --> 00:16:03,279
folks working in ai research and ai you

445
00:16:03,279 --> 00:16:06,399
know piloting and test labs who are also

446
00:16:06,399 --> 00:16:08,079
extremely qualified skilled people who

447
00:16:08,079 --> 00:16:10,639
go to school for a long time so what i'm

448
00:16:10,639 --> 00:16:11,759
trying to say is there's these two

449
00:16:11,759 --> 00:16:14,320
groups that are highly skilled and

450
00:16:14,320 --> 00:16:15,759
bringing them together

451
00:16:15,759 --> 00:16:17,839
is the challenge and the reason for that

452
00:16:17,839 --> 00:16:19,519
is they're very you know busy in their

453
00:16:19,519 --> 00:16:21,440
own domains

454
00:16:21,440 --> 00:16:23,040
really you need to find people whether

455
00:16:23,040 --> 00:16:26,079
they're it leads cios

456
00:16:26,079 --> 00:16:28,560
even doctors in in many of the

457
00:16:28,560 --> 00:16:31,360
interviews i conducted in asia last year

458
00:16:31,360 --> 00:16:33,279
there's doctors who have their own ai

459
00:16:33,279 --> 00:16:35,440
startups i think they just don't sleep

460
00:16:35,440 --> 00:16:36,800
those people

461
00:16:36,800 --> 00:16:38,399
and that's a cultural thing right that

462
00:16:38,399 --> 00:16:40,240
you know working you know and having

463
00:16:40,240 --> 00:16:42,399
those two or three different streams of

464
00:16:42,399 --> 00:16:45,120
activity and really working to not just

465
00:16:45,120 --> 00:16:46,880
provide medical services but also

466
00:16:46,880 --> 00:16:48,320
looking at i.t

467
00:16:48,320 --> 00:16:49,120
um

468
00:16:49,120 --> 00:16:50,560
the other thing i'll add is in some of

469
00:16:50,560 --> 00:16:52,240
those countries that i was working in

470
00:16:52,240 --> 00:16:53,600
last year

471
00:16:53,600 --> 00:16:55,440
you'll have folks who went to school for

472
00:16:55,440 --> 00:16:57,360
medicine but were also doing a computer

473
00:16:57,360 --> 00:16:59,440
science degree at the same time or they

474
00:16:59,440 --> 00:17:01,680
did some coding in their spare time

475
00:17:01,680 --> 00:17:03,440
while becoming a surgeon

476
00:17:03,440 --> 00:17:05,599
and that's very common in some countries

477
00:17:05,599 --> 00:17:07,919
and perhaps less common in canada i

478
00:17:07,919 --> 00:17:10,000
don't have the data on that but that's

479
00:17:10,000 --> 00:17:11,919
just the point i'd like to make is that

480
00:17:11,919 --> 00:17:13,919
translational champions are something we

481
00:17:13,919 --> 00:17:16,160
really need here um

482
00:17:16,160 --> 00:17:18,240
in canada if we want to bring more ai

483
00:17:18,240 --> 00:17:20,160
into healthcare

484
00:17:20,160 --> 00:17:23,359
so i think just to summarize what i just

485
00:17:23,359 --> 00:17:26,559
said um regarding funding

486
00:17:26,559 --> 00:17:28,720
data and sort of people

487
00:17:28,720 --> 00:17:30,400
is we really do need to kind of

488
00:17:30,400 --> 00:17:32,320
understand that a bit better in canada

489
00:17:32,320 --> 00:17:34,559
in the healthcare space

490
00:17:34,559 --> 00:17:35,760
so i think we need some trusted

491
00:17:35,760 --> 00:17:37,919
champions it could be you listening in

492
00:17:37,919 --> 00:17:39,760
on this call you don't have to be an

493
00:17:39,760 --> 00:17:42,400
expert in you know machine language to

494
00:17:42,400 --> 00:17:44,160
you know advocate for it and learn more

495
00:17:44,160 --> 00:17:46,720
about it awareness is the first step

496
00:17:46,720 --> 00:17:48,160
gaining the trust of those who are

497
00:17:48,160 --> 00:17:49,520
responsible for the data getting

498
00:17:49,520 --> 00:17:51,039
creative thinking about what else can be

499
00:17:51,039 --> 00:17:53,679
done with data

500
00:17:53,679 --> 00:17:55,919
in an already resource-stressed system

501
00:17:55,919 --> 00:17:57,600
we need to find a way to pilot new

502
00:17:57,600 --> 00:17:59,840
technologies that is really cognizant

503
00:17:59,840 --> 00:18:01,679
and respectful of that

504
00:18:01,679 --> 00:18:03,360
in my view that really requires strong

505
00:18:03,360 --> 00:18:05,120
partnerships between government private

506
00:18:05,120 --> 00:18:08,559
sector and educational institutions so

507
00:18:08,559 --> 00:18:10,080
what we really need

508
00:18:10,080 --> 00:18:11,520
in my view

509
00:18:11,520 --> 00:18:12,320
is

510
00:18:12,320 --> 00:18:14,000
companies private sector companies that

511
00:18:14,000 --> 00:18:16,559
are already playing in the ai space

512
00:18:16,559 --> 00:18:18,320
to really whether you want to call it

513
00:18:18,320 --> 00:18:20,799
pro bono or otherwise to really look at

514
00:18:20,799 --> 00:18:21,760
some

515
00:18:21,760 --> 00:18:23,360
applications of their technology that

516
00:18:23,360 --> 00:18:25,440
can benefit society so whether that's

517
00:18:25,440 --> 00:18:27,200
partnering with a research

518
00:18:27,200 --> 00:18:29,840
hospital or partnering with a university

519
00:18:29,840 --> 00:18:32,400
or a small startup we really need those

520
00:18:32,400 --> 00:18:34,000
those partnerships to happen to move

521
00:18:34,000 --> 00:18:36,080
this forward

522
00:18:36,080 --> 00:18:38,640
data standards and interoperability i

523
00:18:38,640 --> 00:18:39,919
think i already discussed that but

524
00:18:39,919 --> 00:18:41,440
really that's something we need to think

525
00:18:41,440 --> 00:18:43,520
about a lot in canada

526
00:18:43,520 --> 00:18:44,960
and then lastly is guardrails and

527
00:18:44,960 --> 00:18:47,679
guidance so evolving our regulations and

528
00:18:47,679 --> 00:18:49,679
standards in step with technology is

529
00:18:49,679 --> 00:18:51,919
necessary um and we can't just keep

530
00:18:51,919 --> 00:18:54,000
saying no no no patient data client data

531
00:18:54,000 --> 00:18:55,600
can't be used for that we really need to

532
00:18:55,600 --> 00:18:58,160
find a way to responsibly and respectful

533
00:18:58,160 --> 00:19:00,640
respectfully collect and share data in a

534
00:19:00,640 --> 00:19:02,480
way that provides value and can

535
00:19:02,480 --> 00:19:04,160
potentially even save people's lives

536
00:19:04,160 --> 00:19:06,720
right some of these applications of ai

537
00:19:06,720 --> 00:19:08,799
can provide services and connections

538
00:19:08,799 --> 00:19:10,320
that

539
00:19:10,320 --> 00:19:12,480
not every doctor has time for

540
00:19:12,480 --> 00:19:14,160
there's a strong possibility that you

541
00:19:14,160 --> 00:19:15,840
know this technology can improve the

542
00:19:15,840 --> 00:19:17,760
health care that's delivered to to

543
00:19:17,760 --> 00:19:19,919
canadians

544
00:19:19,919 --> 00:19:21,200
so we're really here to talk about

545
00:19:21,200 --> 00:19:23,039
security i only have a few minutes left

546
00:19:23,039 --> 00:19:24,720
so i'll just chat for a minute or so on

547
00:19:24,720 --> 00:19:26,080
this

548
00:19:26,080 --> 00:19:27,919
people are adopting new technologies all

549
00:19:27,919 --> 00:19:30,000
the time we all know this without

550
00:19:30,000 --> 00:19:31,200
considering

551
00:19:31,200 --> 00:19:33,600
security and other issues

552
00:19:33,600 --> 00:19:37,200
so certainly senior leaders know that

553
00:19:37,200 --> 00:19:40,160
um as regards ai generally

554
00:19:40,160 --> 00:19:43,520
there's a lot of new ways to attack

555
00:19:43,520 --> 00:19:45,440
ai solutions

556
00:19:45,440 --> 00:19:48,240
um so model manipulation

557
00:19:48,240 --> 00:19:50,240
data poisoning there's been a lot of

558
00:19:50,240 --> 00:19:51,679
recent hacks that are really exciting

559
00:19:51,679 --> 00:19:53,840
and interesting if you're into that on

560
00:19:53,840 --> 00:19:56,240
um you know like feeding feeding

561
00:19:56,240 --> 00:19:58,559
information into ai into alexa so

562
00:19:58,559 --> 00:20:00,720
alexa's listening she's always on and

563
00:20:00,720 --> 00:20:02,559
i'm telling her you know

564
00:20:02,559 --> 00:20:04,400
i need i need more grocery bags you know

565
00:20:04,400 --> 00:20:07,120
order some grocery bags there's there's

566
00:20:07,120 --> 00:20:09,280
ways that you can play a noise in the

567
00:20:09,280 --> 00:20:11,360
background that will give alexa other

568
00:20:11,360 --> 00:20:12,960
orders or other

569
00:20:12,960 --> 00:20:16,400
commands so data poisoning is basically

570
00:20:16,400 --> 00:20:19,600
injecting data into algorithms that

571
00:20:19,600 --> 00:20:21,919
or into training models that can buy it

572
00:20:21,919 --> 00:20:24,000
that can bias

573
00:20:24,000 --> 00:20:25,760
and misclassify

574
00:20:25,760 --> 00:20:28,080
what something thinks something is

575
00:20:28,080 --> 00:20:31,120
so model compromise backdoors

576
00:20:31,120 --> 00:20:32,320
really

577
00:20:32,320 --> 00:20:34,960
there's many different ways we can

578
00:20:34,960 --> 00:20:37,280
compromise ai but i also would like to

579
00:20:37,280 --> 00:20:39,120
add and i know this is a very high level

580
00:20:39,120 --> 00:20:41,600
brief overview that there's ways i ai

581
00:20:41,600 --> 00:20:44,240
can also be used for attacks

582
00:20:44,240 --> 00:20:45,919
um and so

583
00:20:45,919 --> 00:20:48,080
i'll just go into that a bit more so for

584
00:20:48,080 --> 00:20:50,080
instance um

585
00:20:50,080 --> 00:20:51,440
you obviously know that you know there's

586
00:20:51,440 --> 00:20:53,600
lots of bots on the internet you can

587
00:20:53,600 --> 00:20:56,799
write ai programs that will look for um

588
00:20:56,799 --> 00:20:59,039
the best way to hack a certain target

589
00:20:59,039 --> 00:21:00,720
based on

590
00:21:00,720 --> 00:21:02,400
based on a whole library of different

591
00:21:02,400 --> 00:21:04,480
types of malware there's there's just so

592
00:21:04,480 --> 00:21:06,960
many different ways that um ai can be

593
00:21:06,960 --> 00:21:10,640
used in in in cyber crime um mimicking a

594
00:21:10,640 --> 00:21:13,120
real person for instance very simple

595
00:21:13,120 --> 00:21:14,480
example when you're logging into a

596
00:21:14,480 --> 00:21:16,480
website and you're trying to you know

597
00:21:16,480 --> 00:21:17,600
i would never do this but when you're

598
00:21:17,600 --> 00:21:18,960
trying to crack a password right if

599
00:21:18,960 --> 00:21:20,880
there's someone trying to log in they

600
00:21:20,880 --> 00:21:21,919
can mimic

601
00:21:21,919 --> 00:21:24,720
more realistic human interactions with

602
00:21:24,720 --> 00:21:26,640
that login page rather than just you

603
00:21:26,640 --> 00:21:30,000
know um just you know throwing a ton of

604
00:21:30,000 --> 00:21:31,600
different password ideas at it every

605
00:21:31,600 --> 00:21:34,000
couple seconds they can wait a few hours

606
00:21:34,000 --> 00:21:36,480
you know try different things and learn

607
00:21:36,480 --> 00:21:39,039
from past passwords that didn't work

608
00:21:39,039 --> 00:21:41,120
so there's just so so many applications

609
00:21:41,120 --> 00:21:43,360
of ai in cyber crime

610
00:21:43,360 --> 00:21:45,600
so then we really need to think about

611
00:21:45,600 --> 00:21:48,320
how to secure ai and i know this is

612
00:21:48,320 --> 00:21:50,000
a really big topic and it's hard to

613
00:21:50,000 --> 00:21:52,159
overview in such a short time so i hope

614
00:21:52,159 --> 00:21:53,840
we can discuss a little farther and

615
00:21:53,840 --> 00:21:55,440
really my objective is to plant some

616
00:21:55,440 --> 00:21:57,760
seeds of ideas in your mind

617
00:21:57,760 --> 00:21:59,520
so to secure ai

618
00:21:59,520 --> 00:22:01,760
really having some test labs where we

619
00:22:01,760 --> 00:22:03,760
can test different types of attacks and

620
00:22:03,760 --> 00:22:06,880
different types of compromise on ais

621
00:22:06,880 --> 00:22:09,120
standardization is key

622
00:22:09,120 --> 00:22:11,440
so really standardization is having you

623
00:22:11,440 --> 00:22:13,679
know data standards quality standards

624
00:22:13,679 --> 00:22:15,840
not buying products until they've been

625
00:22:15,840 --> 00:22:17,760
tested and vetted

626
00:22:17,760 --> 00:22:19,600
so that's also feeding into that next

627
00:22:19,600 --> 00:22:22,000
point which is attestation so really

628
00:22:22,000 --> 00:22:24,799
maturing canada's approach to checking

629
00:22:24,799 --> 00:22:27,120
to make sure that technology is

630
00:22:27,120 --> 00:22:29,280
enterprise class or just appropriate for

631
00:22:29,280 --> 00:22:31,039
the task at hand

632
00:22:31,039 --> 00:22:33,200
transparency a big issue

633
00:22:33,200 --> 00:22:35,520
that i've learned about is

634
00:22:35,520 --> 00:22:37,039
do you trust an algorithm if you don't

635
00:22:37,039 --> 00:22:39,200
know how the algorithm works

636
00:22:39,200 --> 00:22:40,400
so for instance

637
00:22:40,400 --> 00:22:41,679
um

638
00:22:41,679 --> 00:22:44,400
in implementing some some ais in

639
00:22:44,400 --> 00:22:46,960
hospitals and health care institutions

640
00:22:46,960 --> 00:22:48,880
some medical professionals and doctors

641
00:22:48,880 --> 00:22:50,320
really don't like that they don't know

642
00:22:50,320 --> 00:22:52,080
how the algorithms work

643
00:22:52,080 --> 00:22:53,679
even if we're doing that kind of tier

644
00:22:53,679 --> 00:22:56,080
one tier two support model i mentioned

645
00:22:56,080 --> 00:22:58,400
um if they don't know that if i submit a

646
00:22:58,400 --> 00:22:59,919
photo of my mole

647
00:22:59,919 --> 00:23:01,440
if if a doctor doesn't know how that

648
00:23:01,440 --> 00:23:03,200
algorithm works or if a professional

649
00:23:03,200 --> 00:23:05,440
doesn't know um they might just say no i

650
00:23:05,440 --> 00:23:07,039
don't want that technology it has to you

651
00:23:07,039 --> 00:23:08,400
know they have to come in and see me in

652
00:23:08,400 --> 00:23:09,919
person anyways

653
00:23:09,919 --> 00:23:12,159
um so understanding transparency of

654
00:23:12,159 --> 00:23:14,320
algorithms is key

655
00:23:14,320 --> 00:23:16,480
um and then lastly auditability is

656
00:23:16,480 --> 00:23:19,039
really having a way to figure out how

657
00:23:19,039 --> 00:23:20,480
things are actually making decisions

658
00:23:20,480 --> 00:23:22,960
what data it's inputting

659
00:23:22,960 --> 00:23:25,039
so those are just some very high level

660
00:23:25,039 --> 00:23:26,720
concepts and i wish i could talk about

661
00:23:26,720 --> 00:23:29,440
this to you for hours

662
00:23:29,440 --> 00:23:31,679
but the last point i think in purple

663
00:23:31,679 --> 00:23:33,760
there relates back to our point that we

664
00:23:33,760 --> 00:23:36,880
discussed about cyber crime so using ai

665
00:23:36,880 --> 00:23:39,120
to address ai risks is a really exciting

666
00:23:39,120 --> 00:23:40,960
burgeoning area

667
00:23:40,960 --> 00:23:42,159
and i'll leave it there because we don't

668
00:23:42,159 --> 00:23:44,640
have too much time

669
00:23:44,640 --> 00:23:46,320
but really i'd like to turn it back to

670
00:23:46,320 --> 00:23:48,080
you guys and

671
00:23:48,080 --> 00:23:50,640
hear some questions or ideas on

672
00:23:50,640 --> 00:23:52,480
on this topic i know it's hot i know

673
00:23:52,480 --> 00:23:54,159
it's a hype topic

674
00:23:54,159 --> 00:23:56,159
but cutting through the hype

675
00:23:56,159 --> 00:23:58,400
there is some value here

676
00:23:58,400 --> 00:24:00,640
what do you think is you know a valid

677
00:24:00,640 --> 00:24:03,520
way to use ai in health care what do you

678
00:24:03,520 --> 00:24:05,679
think are some some risks that are

679
00:24:05,679 --> 00:24:07,679
coming on the horizon

680
00:24:07,679 --> 00:24:09,279
what are your thoughts i'm very excited

681
00:24:09,279 --> 00:24:13,559
to have a discussion with you

682
00:24:24,960 --> 00:24:25,919
okay

683
00:24:25,919 --> 00:24:29,799
let's just back here

684
00:24:42,960 --> 00:24:44,400
awesome so it seems like there aren't

685
00:24:44,400 --> 00:24:46,159
too many questions i know that was very

686
00:24:46,159 --> 00:24:47,440
high level

687
00:24:47,440 --> 00:24:48,880
but thank you so much for your time it

688
00:24:48,880 --> 00:24:50,880
was great to talk to you and i'm

689
00:24:50,880 --> 00:24:52,799
certainly you know happy to share more i

690
00:24:52,799 --> 00:24:54,640
have lots of information

691
00:24:54,640 --> 00:24:56,240
um that was a very high level summary

692
00:24:56,240 --> 00:24:57,440
and if you're if you're excited about

693
00:24:57,440 --> 00:24:59,360
this i'd love to connect so please reach

694
00:24:59,360 --> 00:25:01,840
out

695
00:25:06,159 --> 00:25:08,320
okay we have one great question

696
00:25:08,320 --> 00:25:11,360
thanks ashley

697
00:25:13,200 --> 00:25:14,880
so it's only been recently pharmacies

698
00:25:14,880 --> 00:25:16,240
have began to communicate with one

699
00:25:16,240 --> 00:25:18,400
another and still it's not all of them

700
00:25:18,400 --> 00:25:19,919
it seems dangerous that they would not

701
00:25:19,919 --> 00:25:21,760
have a centralized database for each

702
00:25:21,760 --> 00:25:23,520
patient what would you say is the

703
00:25:23,520 --> 00:25:25,440
primary factor blocking the sharing of

704
00:25:25,440 --> 00:25:26,720
information between these types of

705
00:25:26,720 --> 00:25:28,720
bodies for a patient yeah that's a

706
00:25:28,720 --> 00:25:30,159
really good question and it varies by

707
00:25:30,159 --> 00:25:31,520
jurisdiction

708
00:25:31,520 --> 00:25:33,440
in canada we do have a huge precedence

709
00:25:33,440 --> 00:25:35,760
for not sharing health data between

710
00:25:35,760 --> 00:25:38,159
provinces territories um between

711
00:25:38,159 --> 00:25:40,000
pharmacies and it's something that as a

712
00:25:40,000 --> 00:25:42,640
consumer of health services i think we

713
00:25:42,640 --> 00:25:45,120
all find quite frustrating

714
00:25:45,120 --> 00:25:46,799
so what i will say is there's some good

715
00:25:46,799 --> 00:25:49,039
progress in this area

716
00:25:49,039 --> 00:25:51,600
um for instance in

717
00:25:51,600 --> 00:25:54,960
i believe it was 2019 canada

718
00:25:54,960 --> 00:25:56,640
the government of canada

719
00:25:56,640 --> 00:25:59,360
um announced a grant of 49 million

720
00:25:59,360 --> 00:26:01,520
dollars to support something called the

721
00:26:01,520 --> 00:26:03,440
digital health and discovery platform or

722
00:26:03,440 --> 00:26:06,559
dhdp um which is looking to establish a

723
00:26:06,559 --> 00:26:08,799
canada-wide data platform

724
00:26:08,799 --> 00:26:10,080
i don't know exactly what that would

725
00:26:10,080 --> 00:26:12,880
look like but the idea is to have really

726
00:26:12,880 --> 00:26:15,279
safe secure privacy conscious security

727
00:26:15,279 --> 00:26:17,919
conscious linking of health data across

728
00:26:17,919 --> 00:26:19,840
canadian jurisdictions regardless of

729
00:26:19,840 --> 00:26:21,279
institution so

730
00:26:21,279 --> 00:26:23,120
if you're curious about that look it up

731
00:26:23,120 --> 00:26:26,159
it's called dhdp digital health and

732
00:26:26,159 --> 00:26:28,400
discovery platform so it's something

733
00:26:28,400 --> 00:26:30,880
that the government in canada is working

734
00:26:30,880 --> 00:26:32,000
on

735
00:26:32,000 --> 00:26:36,120
but i hear you it's very frustrating

736
00:27:03,279 --> 00:27:06,000
any ai examples relating to kovid luke i

737
00:27:06,000 --> 00:27:07,120
don't know if i have time to answer your

738
00:27:07,120 --> 00:27:08,400
question

739
00:27:08,400 --> 00:27:11,120
um but yes a google search will find

740
00:27:11,120 --> 00:27:14,559
many many many examples china is where

741
00:27:14,559 --> 00:27:15,840
you'll find lots of exciting and

742
00:27:15,840 --> 00:27:20,199
sometimes scary things happening

743
00:27:36,960 --> 00:27:39,039
yeah actually one other thing regarding

744
00:27:39,039 --> 00:27:42,000
ai and kovitt is the tier one tier two

745
00:27:42,000 --> 00:27:43,200
approach i met

746
00:27:43,200 --> 00:27:46,240
method i discussed um was used in china

747
00:27:46,240 --> 00:27:48,559
during covid and that was some so covet

748
00:27:48,559 --> 00:27:51,200
has been in in some ways a gr a good

749
00:27:51,200 --> 00:27:53,360
thing can i say that for really

750
00:27:53,360 --> 00:27:56,159
catalyzing uh innovation in ai and

751
00:27:56,159 --> 00:27:57,520
digital health

752
00:27:57,520 --> 00:28:00,480
so um in china the you know a lot of the

753
00:28:00,480 --> 00:28:04,080
um state um healthcare organizations did

754
00:28:04,080 --> 00:28:07,120
begin using an approach to having people

755
00:28:07,120 --> 00:28:09,919
answer online forms and surveys

756
00:28:09,919 --> 00:28:12,240
regarding their health um

757
00:28:12,240 --> 00:28:14,559
their coveted symptoms and then

758
00:28:14,559 --> 00:28:16,640
diagnosing or you know evaluating them

759
00:28:16,640 --> 00:28:18,320
based on that so

760
00:28:18,320 --> 00:28:20,720
whether that was done in a way that is

761
00:28:20,720 --> 00:28:22,240
the way we would do it in canada i can't

762
00:28:22,240 --> 00:28:26,360
speak to but very interesting

763
00:28:41,200 --> 00:28:42,720
i should also mention i didn't go into

764
00:28:42,720 --> 00:28:44,720
detail on um

765
00:28:44,720 --> 00:28:47,120
how can ai be used in discovering

766
00:28:47,120 --> 00:28:49,279
different diseases this is really

767
00:28:49,279 --> 00:28:52,399
fascinating so human minds can't go

768
00:28:52,399 --> 00:28:53,520
through

769
00:28:53,520 --> 00:28:56,399
genetics and like genetic data and find

770
00:28:56,399 --> 00:29:00,159
indicators of diseases but ai can

771
00:29:00,159 --> 00:29:01,760
so you can take a huge data set of

772
00:29:01,760 --> 00:29:04,720
genomic data and run a certain kind of

773
00:29:04,720 --> 00:29:07,679
ai through it and find

774
00:29:07,679 --> 00:29:09,760
basically indicators of certain genetic

775
00:29:09,760 --> 00:29:12,159
diseases which is fascinating and new

776
00:29:12,159 --> 00:29:13,679
and this is something that people

777
00:29:13,679 --> 00:29:14,640
couldn't

778
00:29:14,640 --> 00:29:16,799
really do before ai

779
00:29:16,799 --> 00:29:18,799
so that's a really exciting

780
00:29:18,799 --> 00:29:20,960
application so identifying genetic

781
00:29:20,960 --> 00:29:22,960
indicators of disease and the other

782
00:29:22,960 --> 00:29:24,880
thing i'll mention regarding kovid and

783
00:29:24,880 --> 00:29:26,480
that question about disease

784
00:29:26,480 --> 00:29:28,480
is google deep mind did actually help

785
00:29:28,480 --> 00:29:30,720
with genetic sequence

786
00:29:30,720 --> 00:29:32,960
genetic sequencing of

787
00:29:32,960 --> 00:29:34,720
the covid

788
00:29:34,720 --> 00:29:36,880
virus using ai

789
00:29:36,880 --> 00:29:38,320
i don't know the exact details of that

790
00:29:38,320 --> 00:29:39,520
but check it out online if you're

791
00:29:39,520 --> 00:29:44,000
interested in in deep minds

792
00:29:45,039 --> 00:29:46,159
i think what they did is they looked at

793
00:29:46,159 --> 00:29:47,760
the protein structure or did some

794
00:29:47,760 --> 00:29:50,080
research into the um

795
00:29:50,080 --> 00:29:55,000
the structure of the virus using ai

796
00:30:06,399 --> 00:30:07,919
is there a reason we cannot hold our own

797
00:30:07,919 --> 00:30:09,840
records good question ashley and i do

798
00:30:09,840 --> 00:30:11,440
not know the answer to that i think

799
00:30:11,440 --> 00:30:12,880
you're always you always have the right

800
00:30:12,880 --> 00:30:14,880
to your own data under almost every

801
00:30:14,880 --> 00:30:16,559
piece of canadian

802
00:30:16,559 --> 00:30:19,440
um health care health privacy

803
00:30:19,440 --> 00:30:21,919
legislation so you can at any time call

804
00:30:21,919 --> 00:30:24,000
up a clinic and get your records always

805
00:30:24,000 --> 00:30:25,760
no matter what clinic it is

806
00:30:25,760 --> 00:30:27,440
please correct me if i'm wrong but i'm

807
00:30:27,440 --> 00:30:29,679
quite certain that is the case

808
00:30:29,679 --> 00:30:31,679
but they're not in a unified

809
00:30:31,679 --> 00:30:34,679
database

810
00:30:45,760 --> 00:30:46,720
and actually i want to talk about

811
00:30:46,720 --> 00:30:48,320
radiology again because i'm excited

812
00:30:48,320 --> 00:30:51,760
about radiology so in in china

813
00:30:51,760 --> 00:30:55,240
during covid um ais were used to

814
00:30:55,240 --> 00:30:58,000
distinguish between regular

815
00:30:58,000 --> 00:30:59,279
um chest

816
00:30:59,279 --> 00:31:02,320
radiology scans images of

817
00:31:02,320 --> 00:31:03,519
pneumonia

818
00:31:03,519 --> 00:31:05,919
or common cold and ones that were

819
00:31:05,919 --> 00:31:08,240
indicative of covid so there was a

820
00:31:08,240 --> 00:31:10,399
notable difference between the the chest

821
00:31:10,399 --> 00:31:12,399
scans of someone who just had a cold and

822
00:31:12,399 --> 00:31:15,039
someone who had coveted 19 so

823
00:31:15,039 --> 00:31:16,720
that's another example of just how

824
00:31:16,720 --> 00:31:18,799
detailed and nuanced some of these ais

825
00:31:18,799 --> 00:31:21,440
can be um now i'm not i'm not saying i

826
00:31:21,440 --> 00:31:23,440
fully understand that algorithm or how

827
00:31:23,440 --> 00:31:25,840
um you know legitimate it was but that

828
00:31:25,840 --> 00:31:28,000
was something that was used in in in

829
00:31:28,000 --> 00:31:30,000
china which is really fascinating if it

830
00:31:30,000 --> 00:31:33,559
if it worked out

831
00:31:41,679 --> 00:31:43,360
all right any other questions or

832
00:31:43,360 --> 00:31:45,200
comments no this was a bit less on

833
00:31:45,200 --> 00:31:46,559
security

834
00:31:46,559 --> 00:31:48,480
as than it was on more the kind of

835
00:31:48,480 --> 00:31:50,640
exciting more new things happening in

836
00:31:50,640 --> 00:31:52,000
this area

837
00:31:52,000 --> 00:31:53,039
of work

838
00:31:53,039 --> 00:31:54,240
so please let me know if you have any

839
00:31:54,240 --> 00:31:56,559
security ideas or questions and i'm

840
00:31:56,559 --> 00:32:00,600
really excited to discuss

