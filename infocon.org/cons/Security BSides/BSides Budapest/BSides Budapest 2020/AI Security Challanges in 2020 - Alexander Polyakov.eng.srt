1
00:00:14,920 --> 00:00:16,320
hi um

2
00:00:16,320 --> 00:00:18,880
so the topic of this presentation is

3
00:00:18,880 --> 00:00:21,680
called ai security changes

4
00:00:21,680 --> 00:00:24,960
and ai security is one of the most

5
00:00:24,960 --> 00:00:28,160
underestimated cyber security areas

6
00:00:28,160 --> 00:00:31,840
right now

7
00:00:32,238 --> 00:00:36,079
so my name is alex i spent last 15 years

8
00:00:36,079 --> 00:00:38,879
in cyber security from penetration

9
00:00:38,879 --> 00:00:39,520
tester

10
00:00:39,520 --> 00:00:42,840
and researcher to

11
00:00:42,840 --> 00:00:46,320
application security manager and

12
00:00:46,320 --> 00:00:49,760
cto and now a founder

13
00:00:49,760 --> 00:00:53,920
and i also

14
00:00:54,000 --> 00:00:57,039
tried all the areas like from network

15
00:00:57,039 --> 00:01:01,680
to endpoint to application

16
00:01:01,680 --> 00:01:06,240
and now to ai and we found

17
00:01:06,240 --> 00:01:09,280
just a year ago with my colleagues

18
00:01:09,280 --> 00:01:12,880
and a new startup called adversary

19
00:01:12,880 --> 00:01:16,640
uh and our mission isn't

20
00:01:16,640 --> 00:01:20,080
to increase trust in ai systems because

21
00:01:20,080 --> 00:01:24,320
um what why it's so important

22
00:01:24,320 --> 00:01:27,840
last year i spent a lot of time

23
00:01:27,840 --> 00:01:30,240
like traveling uh all around the world

24
00:01:30,240 --> 00:01:31,840
and asking absolutely different

25
00:01:31,840 --> 00:01:35,040
people from like amazon jungle

26
00:01:35,040 --> 00:01:38,400
to the markets in oman

27
00:01:38,400 --> 00:01:41,680
uh like would they sit in the

28
00:01:41,680 --> 00:01:43,680
self-driving car

29
00:01:43,680 --> 00:01:46,799
and most of the people say no like i

30
00:01:46,799 --> 00:01:48,960
don't trust this

31
00:01:48,960 --> 00:01:53,040
uh and i i also don't trust

32
00:01:53,040 --> 00:01:57,119
the autonomous car until it will fail

33
00:01:57,119 --> 00:02:00,799
to recognize

34
00:02:00,799 --> 00:02:05,840
a kid instead of con or vice versa

35
00:02:05,840 --> 00:02:09,038
so i i don't trust it as well and so we

36
00:02:09,038 --> 00:02:09,840
understood the

37
00:02:09,840 --> 00:02:11,520
trust to new technology is very

38
00:02:11,520 --> 00:02:13,520
important and one of the

39
00:02:13,520 --> 00:02:16,879
uh key trust areas is

40
00:02:16,879 --> 00:02:20,000
actually security so our mission is to

41
00:02:20,000 --> 00:02:23,760
to bring the most the smartest

42
00:02:23,760 --> 00:02:26,160
researchers and cyber security experts

43
00:02:26,160 --> 00:02:28,879
generic experts and neuroscientists

44
00:02:28,879 --> 00:02:32,800
uh to work together and to

45
00:02:32,800 --> 00:02:36,000
create some innovative solutions

46
00:02:36,000 --> 00:02:41,840
to to to protect uh ai systems

47
00:02:43,040 --> 00:02:46,239
and the agenda for this

48
00:02:46,239 --> 00:02:50,400
presentation is quite broad

49
00:02:50,400 --> 00:02:53,120
because what i'm trying to do is to give

50
00:02:53,120 --> 00:02:54,319
you an idea

51
00:02:54,319 --> 00:02:58,159
of ai security in just 40 minutes

52
00:02:58,159 --> 00:03:01,680
and to do that i want to answer a number

53
00:03:01,680 --> 00:03:02,239
of

54
00:03:02,239 --> 00:03:04,400
questions so the questions you see in

55
00:03:04,400 --> 00:03:06,400
the slide those are the most important

56
00:03:06,400 --> 00:03:09,360
important questions to cover to

57
00:03:09,360 --> 00:03:10,319
understand

58
00:03:10,319 --> 00:03:14,159
each area t1 so

59
00:03:14,159 --> 00:03:17,440
why it's important like

60
00:03:17,440 --> 00:03:22,239
what it is uh who uses when it started

61
00:03:22,239 --> 00:03:26,400
where it's uh located and how actually

62
00:03:26,400 --> 00:03:29,920
does it work so let's start with why

63
00:03:29,920 --> 00:03:33,040
why it's the most important question

64
00:03:33,040 --> 00:03:36,560
why ai security is different and why

65
00:03:36,560 --> 00:03:37,280
should we

66
00:03:37,280 --> 00:03:40,239
care about it yeah because first of all

67
00:03:40,239 --> 00:03:40,640
that

68
00:03:40,640 --> 00:03:45,760
traditional software made by like

69
00:03:45,760 --> 00:03:49,200
a program logic powered by program logic

70
00:03:49,200 --> 00:03:52,239
while ai systems are powered by uh

71
00:03:52,239 --> 00:03:55,360
machine learning techniques so

72
00:03:55,360 --> 00:03:57,519
and those approaches are absolutely

73
00:03:57,519 --> 00:03:58,480
different

74
00:03:58,480 --> 00:04:01,200
and when we talk about the interaction

75
00:04:01,200 --> 00:04:02,879
with traditional software

76
00:04:02,879 --> 00:04:06,000
we mostly talk about the like graphical

77
00:04:06,000 --> 00:04:09,360
user interfaces with menus and buttons

78
00:04:09,360 --> 00:04:13,040
and commands that you enter so most of

79
00:04:13,040 --> 00:04:14,319
the typical attacks

80
00:04:14,319 --> 00:04:17,680
on traditional software are actually uh

81
00:04:17,680 --> 00:04:20,720
improper validation of some kind of

82
00:04:20,720 --> 00:04:21,600
structured

83
00:04:21,600 --> 00:04:25,759
inputs like sql injection for example

84
00:04:25,759 --> 00:04:28,960
while in ai the interaction

85
00:04:28,960 --> 00:04:32,639
uh mostly the cognitive and

86
00:04:32,639 --> 00:04:35,759
solutions parts like a vision or

87
00:04:35,759 --> 00:04:39,280
audio or natural language and attacks

88
00:04:39,280 --> 00:04:39,840
here

89
00:04:39,840 --> 00:04:43,120
are basically have the same approach

90
00:04:43,120 --> 00:04:46,560
trying to poison some data but here

91
00:04:46,560 --> 00:04:49,840
we have like unstructured data

92
00:04:49,840 --> 00:04:51,840
so when we talk about the traditional

93
00:04:51,840 --> 00:04:54,000
software we have like

94
00:04:54,000 --> 00:04:56,720
traditional injections like sql

95
00:04:56,720 --> 00:04:57,440
injection

96
00:04:57,440 --> 00:05:02,000
and we kind of know how to

97
00:05:02,000 --> 00:05:05,039
uh understand if it's

98
00:05:05,039 --> 00:05:08,560
input or valid or malicious

99
00:05:08,560 --> 00:05:09,919
because we have some kind of

100
00:05:09,919 --> 00:05:11,600
understanding of this data but in

101
00:05:11,600 --> 00:05:16,080
ai systems uh for example in visual data

102
00:05:16,080 --> 00:05:19,440
we don't really know how to you know

103
00:05:19,440 --> 00:05:23,120
separate the malicious

104
00:05:23,120 --> 00:05:26,560
uh data and non-malicious data

105
00:05:26,560 --> 00:05:29,600
so the trend the the threat landscape is

106
00:05:29,600 --> 00:05:31,280
really changing

107
00:05:31,280 --> 00:05:34,080
and we need to have uh a new solutions

108
00:05:34,080 --> 00:05:36,240
here

109
00:05:36,240 --> 00:05:39,840
why it's important uh now

110
00:05:39,840 --> 00:05:43,120
mostly first of all because uh

111
00:05:43,120 --> 00:05:46,479
because of the research progress uh and

112
00:05:46,479 --> 00:05:48,000
currently we have more than two thousand

113
00:05:48,000 --> 00:05:50,080
different research papers

114
00:05:50,080 --> 00:05:53,280
about different practical aspects of ai

115
00:05:53,280 --> 00:05:54,880
security

116
00:05:54,880 --> 00:05:56,960
we also have a number of different

117
00:05:56,960 --> 00:05:58,319
public initiatives

118
00:05:58,319 --> 00:06:01,440
uh like for example darpa

119
00:06:01,440 --> 00:06:04,880
is a created grant for

120
00:06:04,880 --> 00:06:08,000
ai security program

121
00:06:08,000 --> 00:06:10,639
and we also have a number of market

122
00:06:10,639 --> 00:06:11,759
signs

123
00:06:11,759 --> 00:06:14,000
uh a number of companies started to

124
00:06:14,000 --> 00:06:16,400
provide ai skills to services and new

125
00:06:16,400 --> 00:06:17,440
startups

126
00:06:17,440 --> 00:06:20,720
uh like ours and

127
00:06:20,720 --> 00:06:23,840
also the gardner also highlighted the ei

128
00:06:23,840 --> 00:06:24,479
securities

129
00:06:24,479 --> 00:06:26,800
topic as one of the strategic trends for

130
00:06:26,800 --> 00:06:29,360
2020

131
00:06:29,840 --> 00:06:33,440
and why it's important

132
00:06:33,440 --> 00:06:36,560
in general because in ai as

133
00:06:36,560 --> 00:06:40,639
in any traditional like old-school

134
00:06:40,639 --> 00:06:43,360
solutions we also have confidentiality

135
00:06:43,360 --> 00:06:46,080
integrity and availability

136
00:06:46,080 --> 00:06:48,800
and for all those risks we have examples

137
00:06:48,800 --> 00:06:51,280
we have examples of confidentiality

138
00:06:51,280 --> 00:06:54,720
uh with netflix um

139
00:06:54,720 --> 00:06:58,319
data set uh incident uh we have

140
00:06:58,319 --> 00:06:59,280
integrity

141
00:06:59,280 --> 00:07:02,080
issues like uh facial recognition

142
00:07:02,080 --> 00:07:04,400
solutions were bypassed during

143
00:07:04,400 --> 00:07:08,080
a hong kong protest and for example the

144
00:07:08,080 --> 00:07:12,880
malware detection system from silence uh

145
00:07:12,880 --> 00:07:14,560
the machine learning based milford

146
00:07:14,560 --> 00:07:17,280
detection system was also bypassed

147
00:07:17,280 --> 00:07:19,919
because of the vulnerabilities in

148
00:07:19,919 --> 00:07:22,240
machine learning algorithms

149
00:07:22,240 --> 00:07:26,639
and like self-driving cars they also uh

150
00:07:26,639 --> 00:07:30,080
you know causes different

151
00:07:30,080 --> 00:07:32,720
cars to actually to crash because of the

152
00:07:32,720 --> 00:07:33,360
uh

153
00:07:33,360 --> 00:07:34,479
potential attacks and the

154
00:07:34,479 --> 00:07:36,880
vulnerabilities in schindler's

155
00:07:36,880 --> 00:07:37,840
algorithms

156
00:07:37,840 --> 00:07:40,800
so we have examples we already have

157
00:07:40,800 --> 00:07:41,759
examples of

158
00:07:41,759 --> 00:07:45,120
uh different uh real issues

159
00:07:45,120 --> 00:07:48,960
in app so what is

160
00:07:48,960 --> 00:07:53,520
uh ai it all starts with data

161
00:07:53,520 --> 00:07:56,840
and different ai solutions

162
00:07:56,840 --> 00:08:01,120
actually deal with different data

163
00:08:01,120 --> 00:08:04,560
most of the attacks

164
00:08:04,560 --> 00:08:08,080
on ai are actually

165
00:08:08,080 --> 00:08:11,199
against the image based machine learning

166
00:08:11,199 --> 00:08:12,080
applications

167
00:08:12,080 --> 00:08:15,599
more than 60 prostate attacks

168
00:08:16,479 --> 00:08:20,720
then we have algorithms

169
00:08:20,720 --> 00:08:25,039
and there are different algorithms

170
00:08:25,039 --> 00:08:26,400
to deal with the data like

171
00:08:26,400 --> 00:08:28,639
classification regression

172
00:08:28,639 --> 00:08:33,519
clustering dimensionality reduction such

173
00:08:33,519 --> 00:08:36,880
association rule learning and etc

174
00:08:36,880 --> 00:08:40,958
currently we see more attack examples

175
00:08:40,958 --> 00:08:44,480
against classification uh probably

176
00:08:44,480 --> 00:08:48,399
around 80 to 90 percent of all attacks

177
00:08:48,399 --> 00:08:50,720
but it doesn't mean that other

178
00:08:50,720 --> 00:08:51,680
algorithms

179
00:08:51,680 --> 00:08:54,320
are not vulnerable they also vulnerable

180
00:08:54,320 --> 00:08:55,920
and there are examples

181
00:08:55,920 --> 00:09:00,080
about attacks on all types of

182
00:09:00,080 --> 00:09:02,240
algorithms but just because the

183
00:09:02,240 --> 00:09:03,519
classification is

184
00:09:03,519 --> 00:09:07,120
more usable now

185
00:09:07,120 --> 00:09:10,160
and when we combine the data

186
00:09:10,160 --> 00:09:13,040
and the algorithms we actually have the

187
00:09:13,040 --> 00:09:14,560
application

188
00:09:14,560 --> 00:09:19,839
so there are different applications

189
00:09:20,320 --> 00:09:23,440
as of now and those are uh

190
00:09:23,440 --> 00:09:27,519
the most popular uh in terms of the

191
00:09:27,519 --> 00:09:28,160
number

192
00:09:28,160 --> 00:09:31,760
of research papers i've focused on those

193
00:09:31,760 --> 00:09:33,680
the security of those applications so

194
00:09:33,680 --> 00:09:36,480
you see image classification

195
00:09:36,480 --> 00:09:39,440
of course is most common but we also

196
00:09:39,440 --> 00:09:40,880
have the

197
00:09:40,880 --> 00:09:42,959
face recognition malware detection and

198
00:09:42,959 --> 00:09:45,120
speech recognition

199
00:09:45,120 --> 00:09:51,600
those are the most common applications

200
00:09:51,600 --> 00:09:54,800
then uh the next question

201
00:09:54,800 --> 00:09:58,000
is who actually use

202
00:09:58,000 --> 00:10:02,839
those ai solutions and i would say that

203
00:10:02,839 --> 00:10:06,959
ai uh it's is everywhere now or

204
00:10:06,959 --> 00:10:10,079
uh even if not now it will be in the

205
00:10:10,079 --> 00:10:11,680
near future

206
00:10:11,680 --> 00:10:14,480
and here in the slides you can see like

207
00:10:14,480 --> 00:10:15,519
top 10

208
00:10:15,519 --> 00:10:18,640
ai powered industries

209
00:10:18,640 --> 00:10:21,279
uh those are the the most common right

210
00:10:21,279 --> 00:10:22,160
now

211
00:10:22,160 --> 00:10:25,440
like automotive and health care

212
00:10:25,440 --> 00:10:28,880
uh and even cyber security the those

213
00:10:28,880 --> 00:10:32,800
all those industries are ai powered now

214
00:10:32,800 --> 00:10:35,360
and among those industries we can tell

215
00:10:35,360 --> 00:10:36,160
that

216
00:10:36,160 --> 00:10:39,200
some of them are more

217
00:10:39,200 --> 00:10:42,320
or less analyzed

218
00:10:42,320 --> 00:10:46,320
by researchers so here you see the

219
00:10:46,320 --> 00:10:50,720
top five uh industries

220
00:10:50,720 --> 00:10:54,240
that have the biggest number of uh

221
00:10:54,240 --> 00:10:58,560
research papers focused particularly on

222
00:10:58,560 --> 00:11:02,079
this this industry

223
00:11:02,079 --> 00:11:03,760
so you see like internet and cyber

224
00:11:03,760 --> 00:11:05,519
security and biometrics like face

225
00:11:05,519 --> 00:11:06,560
recognition

226
00:11:06,560 --> 00:11:10,079
are the most common but it doesn't mean

227
00:11:10,079 --> 00:11:10,800
that

228
00:11:10,800 --> 00:11:15,120
uh only those industries are affected

229
00:11:15,120 --> 00:11:18,320
actually if we calculate

230
00:11:18,320 --> 00:11:21,839
not the number of particular

231
00:11:21,839 --> 00:11:26,000
papers focused on political industry

232
00:11:26,000 --> 00:11:29,920
are but the papers which can be

233
00:11:29,920 --> 00:11:33,760
uh applicable to each industry

234
00:11:33,760 --> 00:11:36,959
we can see a much bigger figures and we

235
00:11:36,959 --> 00:11:37,440
could

236
00:11:37,440 --> 00:11:39,600
can actually say that you know most of

237
00:11:39,600 --> 00:11:42,079
the research papers

238
00:11:42,079 --> 00:11:45,360
uh even if they about

239
00:11:45,360 --> 00:11:48,399
like image classification they

240
00:11:48,399 --> 00:11:51,200
really applicable to most of the

241
00:11:51,200 --> 00:11:53,120
industries because

242
00:11:53,120 --> 00:11:55,600
you can you can find like classification

243
00:11:55,600 --> 00:11:56,880
tasks everywhere

244
00:11:56,880 --> 00:12:00,560
pretty much everywhere so i just want to

245
00:12:00,560 --> 00:12:02,480
say that

246
00:12:02,480 --> 00:12:05,600
all industries are kind of equally

247
00:12:05,600 --> 00:12:08,079
affected

248
00:12:08,880 --> 00:12:11,839
in general

249
00:12:13,120 --> 00:12:16,560
so the next question is when

250
00:12:16,560 --> 00:12:19,519
uh it was started and where are we going

251
00:12:19,519 --> 00:12:20,959
and

252
00:12:20,959 --> 00:12:24,000
uh where the most interesting uh

253
00:12:24,000 --> 00:12:27,360
things happening first of all

254
00:12:27,360 --> 00:12:30,240
the number of research papers about ai

255
00:12:30,240 --> 00:12:31,760
security as i said before is

256
00:12:31,760 --> 00:12:34,800
is growing is growing

257
00:12:34,800 --> 00:12:38,320
really fast and

258
00:12:38,320 --> 00:12:42,399
i would say like in 2016 when i

259
00:12:42,399 --> 00:12:46,639
uh first started uh

260
00:12:48,000 --> 00:12:51,279
working this area i was able to

261
00:12:51,279 --> 00:12:55,600
analyze each new research paper

262
00:12:55,600 --> 00:12:58,959
now i can't do that because like

263
00:12:58,959 --> 00:13:02,800
you have at least like three to five

264
00:13:02,800 --> 00:13:05,839
research papers about this topic like

265
00:13:05,839 --> 00:13:08,399
every day

266
00:13:09,200 --> 00:13:13,279
i would say that uh there were no

267
00:13:13,279 --> 00:13:16,000
uh papers about ai security like before

268
00:13:16,000 --> 00:13:17,360
2010

269
00:13:17,360 --> 00:13:20,720
uh of course they were and probably the

270
00:13:20,720 --> 00:13:23,920
first examples of really

271
00:13:23,920 --> 00:13:30,399
like ai security papers were in 2004

272
00:13:30,399 --> 00:13:35,839
but the most um

273
00:13:36,720 --> 00:13:41,440
intriguing point i think was in 2015

274
00:13:41,440 --> 00:13:44,639
with publication of first uh paper

275
00:13:44,639 --> 00:13:48,480
about adversarial attacks against

276
00:13:48,480 --> 00:13:51,680
neural networks and this is where when

277
00:13:51,680 --> 00:13:52,880
actually

278
00:13:52,880 --> 00:13:55,760
everything started

279
00:13:56,480 --> 00:13:58,480
we can also look at the different

280
00:13:58,480 --> 00:13:59,839
countries

281
00:13:59,839 --> 00:14:03,040
and of course we can see that the u.s

282
00:14:03,040 --> 00:14:06,240
is actually leading here and then we

283
00:14:06,240 --> 00:14:07,120
have china

284
00:14:07,120 --> 00:14:09,839
and then we have the rest of the boston

285
00:14:09,839 --> 00:14:11,440
european countries

286
00:14:11,440 --> 00:14:14,720
uh but what i would say is that the

287
00:14:14,720 --> 00:14:17,279
the number of research papers from china

288
00:14:17,279 --> 00:14:18,320
is actually

289
00:14:18,320 --> 00:14:21,440
growing much faster and probably

290
00:14:21,440 --> 00:14:24,480
we can see in in a few years

291
00:14:24,480 --> 00:14:27,760
like u.s and china uh

292
00:14:27,760 --> 00:14:30,560
again sharing the first and the second

293
00:14:30,560 --> 00:14:31,040
place

294
00:14:31,040 --> 00:14:35,120
but i won't be really sure that

295
00:14:35,120 --> 00:14:38,399
u.s will be on the first

296
00:14:38,399 --> 00:14:42,639
place in few years but let's see

297
00:14:42,639 --> 00:14:46,880
and also what we have here is

298
00:14:46,880 --> 00:14:50,160
uh different initiatives that

299
00:14:50,160 --> 00:14:53,680
actually those countries are making

300
00:14:53,680 --> 00:14:58,480
based on their research and probably the

301
00:14:58,480 --> 00:14:59,279
first

302
00:14:59,279 --> 00:15:02,880
uh initiative uh

303
00:15:02,880 --> 00:15:06,830
where security of ai was mentioned

304
00:15:06,830 --> 00:15:07,920
[Music]

305
00:15:07,920 --> 00:15:12,160
was published in u.s in 2016 16.

306
00:15:12,160 --> 00:15:15,440
and then a number of other countries uh

307
00:15:15,440 --> 00:15:18,959
joined this trend

308
00:15:18,959 --> 00:15:22,720
and published uh other ai security

309
00:15:22,720 --> 00:15:26,079
or ai initiatives uh with different

310
00:15:26,079 --> 00:15:29,920
security topics covered and now and then

311
00:15:29,920 --> 00:15:33,519
we see a number of uh initiatives

312
00:15:33,519 --> 00:15:36,959
uh which were particularly focused

313
00:15:36,959 --> 00:15:40,000
on security uh

314
00:15:40,000 --> 00:15:43,519
they started to uh appear

315
00:15:43,519 --> 00:15:47,600
in 2019 and we predict that

316
00:15:47,600 --> 00:15:50,720
in 2020 and 2021

317
00:15:50,720 --> 00:15:54,240
all the rest of the countries will join

318
00:15:54,240 --> 00:15:58,079
the same trend and they will publish

319
00:15:58,079 --> 00:16:01,199
actually ai security initiatives and

320
00:16:01,199 --> 00:16:02,240
probably

321
00:16:02,240 --> 00:16:05,360
different laws or

322
00:16:05,360 --> 00:16:09,839
regulation rules and etc

323
00:16:10,399 --> 00:16:13,120
okay now i think the most interesting

324
00:16:13,120 --> 00:16:13,519
part

325
00:16:13,519 --> 00:16:16,720
is how are actually

326
00:16:16,720 --> 00:16:19,920
ai securities is working so what are the

327
00:16:19,920 --> 00:16:20,800
attacks

328
00:16:20,800 --> 00:16:23,600
what are the approaches and defenses and

329
00:16:23,600 --> 00:16:26,720
so on so let's start with attacks

330
00:16:26,720 --> 00:16:29,759
first of all there are like three

331
00:16:29,759 --> 00:16:33,440
big categories of ai attacks

332
00:16:33,440 --> 00:16:36,000
the first one is manipulation when we

333
00:16:36,000 --> 00:16:36,560
try

334
00:16:36,560 --> 00:16:39,680
to somehow manipulate with the input

335
00:16:39,680 --> 00:16:41,230
so that

336
00:16:41,230 --> 00:16:42,800
[Music]

337
00:16:42,800 --> 00:16:46,240
the system will wrongly

338
00:16:46,240 --> 00:16:49,440
recognize our inputs then we have an

339
00:16:49,440 --> 00:16:52,399
extraction uh which is quite the

340
00:16:52,399 --> 00:16:53,440
opposite

341
00:16:53,440 --> 00:16:56,560
uh our goal is to extract some data

342
00:16:56,560 --> 00:16:59,839
from machine learning model uh and there

343
00:16:59,839 --> 00:17:00,160
are

344
00:17:00,160 --> 00:17:03,440
different types of data extraction

345
00:17:03,440 --> 00:17:06,559
some attacks can extract

346
00:17:06,559 --> 00:17:09,439
the data from the module some attacks

347
00:17:09,439 --> 00:17:11,039
can extract

348
00:17:11,039 --> 00:17:14,400
uh particular parameters of the model

349
00:17:14,400 --> 00:17:18,959
so we can actually steal someone's model

350
00:17:18,959 --> 00:17:22,400
remotely and the third area

351
00:17:22,400 --> 00:17:25,679
is injections

352
00:17:25,919 --> 00:17:28,319
here we have different types of

353
00:17:28,319 --> 00:17:29,360
injecting

354
00:17:29,360 --> 00:17:33,840
data into a training set

355
00:17:34,640 --> 00:17:37,200
so that the system will be trained with

356
00:17:37,200 --> 00:17:38,080
some kind of

357
00:17:38,080 --> 00:17:41,039
malicious data

358
00:17:42,480 --> 00:17:45,679
and the top three

359
00:17:45,679 --> 00:17:48,320
attacks from this list are actually uh

360
00:17:48,320 --> 00:17:49,760
evasion

361
00:17:49,760 --> 00:17:52,400
yeah uh or it's also called adversarial

362
00:17:52,400 --> 00:17:53,200
examples

363
00:17:53,200 --> 00:17:56,320
when we throw the models detection

364
00:17:56,320 --> 00:18:00,000
or prediction now the second

365
00:18:00,000 --> 00:18:03,280
place by the number of

366
00:18:03,280 --> 00:18:06,160
research papers is actually poisoning

367
00:18:06,160 --> 00:18:08,240
when we retrain the model with

368
00:18:08,240 --> 00:18:11,520
kind of malicious data and in the third

369
00:18:11,520 --> 00:18:14,240
place we have membership inference

370
00:18:14,240 --> 00:18:17,600
this attack allowed us to

371
00:18:17,600 --> 00:18:22,720
guess if a particular example

372
00:18:22,720 --> 00:18:28,960
was in the training data set

373
00:18:28,960 --> 00:18:33,919
so let's quickly uh look at the

374
00:18:34,400 --> 00:18:38,080
attack examples

375
00:18:38,080 --> 00:18:42,240
so evasion attack is actually if you

376
00:18:42,240 --> 00:18:44,720
what like you see in the picture when we

377
00:18:44,720 --> 00:18:45,440
have

378
00:18:45,440 --> 00:18:48,640
some kind of uh picture of a peak and we

379
00:18:48,640 --> 00:18:49,520
want

380
00:18:49,520 --> 00:18:52,640
uh the system to recognize as big

381
00:18:52,640 --> 00:18:56,160
as airliner so how we can do that there

382
00:18:56,160 --> 00:18:57,039
are

383
00:18:57,039 --> 00:18:59,200
a lot of different approaches like

384
00:18:59,200 --> 00:19:00,799
mathematical approaches probably

385
00:19:00,799 --> 00:19:03,760
more than 100 different types of attacks

386
00:19:03,760 --> 00:19:04,320
uh

387
00:19:04,320 --> 00:19:07,679
but simply saying we um

388
00:19:07,679 --> 00:19:09,520
we discover first of all we're trying to

389
00:19:09,520 --> 00:19:10,720
discover

390
00:19:10,720 --> 00:19:14,559
the most important pixels that affect

391
00:19:14,559 --> 00:19:18,400
output and then we actually

392
00:19:18,400 --> 00:19:23,280
change those pixels a little bit

393
00:19:23,280 --> 00:19:26,240
and craft maliciously put a malicious

394
00:19:26,240 --> 00:19:27,039
image

395
00:19:27,039 --> 00:19:30,160
that can uh fool the model so after that

396
00:19:30,160 --> 00:19:32,880
the model makes wrong prediction

397
00:19:32,880 --> 00:19:36,320
um so the method of

398
00:19:36,320 --> 00:19:39,200
finding the particular pixels and

399
00:19:39,200 --> 00:19:40,640
finding the values

400
00:19:40,640 --> 00:19:45,120
of those particular pixels

401
00:19:46,320 --> 00:19:50,160
the methods are very different

402
00:19:50,160 --> 00:19:53,600
and but this is the topic for like a

403
00:19:53,600 --> 00:19:56,000
more detailed uh discussion because

404
00:19:56,000 --> 00:19:58,000
there are hundreds of different uh

405
00:19:58,000 --> 00:20:01,919
types of attacks like how to do that

406
00:20:01,919 --> 00:20:06,720
um then we have a posting attack

407
00:20:06,720 --> 00:20:09,840
in poisoning attack uh

408
00:20:09,840 --> 00:20:14,080
our goal is actually to somehow retrain

409
00:20:14,080 --> 00:20:15,600
the system

410
00:20:15,600 --> 00:20:18,840
so that the system will change decision

411
00:20:18,840 --> 00:20:20,320
boundary

412
00:20:20,320 --> 00:20:23,600
and and then

413
00:20:23,600 --> 00:20:26,799
you know think that the target

414
00:20:26,799 --> 00:20:30,159
example uh

415
00:20:30,159 --> 00:20:32,840
will actually change the class so to do

416
00:20:32,840 --> 00:20:34,559
that uh

417
00:20:34,559 --> 00:20:37,600
we need to obtain some training data uh

418
00:20:37,600 --> 00:20:40,080
we need to choose some target instance

419
00:20:40,080 --> 00:20:40,880
from the

420
00:20:40,880 --> 00:20:45,200
trained data then we we make

421
00:20:45,200 --> 00:20:47,360
changes with this example to produce a

422
00:20:47,360 --> 00:20:49,600
malicious example

423
00:20:49,600 --> 00:20:52,960
uh and then so we we retrain

424
00:20:52,960 --> 00:20:56,240
the data and this kind of poison

425
00:20:56,240 --> 00:20:58,799
poisoning actually shift the decision

426
00:20:58,799 --> 00:20:59,760
boundary

427
00:20:59,760 --> 00:21:03,280
in such way so that our

428
00:21:03,280 --> 00:21:06,799
our target now

429
00:21:06,799 --> 00:21:10,799
will be in a different class

430
00:21:10,799 --> 00:21:14,400
so this is how we can for example bypass

431
00:21:14,400 --> 00:21:17,440
the spam spam detection system

432
00:21:17,440 --> 00:21:20,720
yeah i would just change

433
00:21:20,720 --> 00:21:26,080
the the normal spam

434
00:21:27,120 --> 00:21:31,840
and do kind of poisoning attack

435
00:21:34,400 --> 00:21:36,880
the the next example is a membership

436
00:21:36,880 --> 00:21:39,120
inference attack

437
00:21:39,120 --> 00:21:42,400
uh our goal here is to understand

438
00:21:42,400 --> 00:21:45,520
if the particular example was

439
00:21:45,520 --> 00:21:48,720
in the training set like um

440
00:21:48,720 --> 00:21:52,080
for example there is a website which can

441
00:21:52,080 --> 00:21:56,000
collect um your data

442
00:21:56,000 --> 00:22:00,159
like uh your picture of your face

443
00:22:00,159 --> 00:22:03,440
and then train their face recognition

444
00:22:03,440 --> 00:22:05,200
system based on your data

445
00:22:05,200 --> 00:22:08,799
and you want to be sure that they don't

446
00:22:08,799 --> 00:22:09,280
use

447
00:22:09,280 --> 00:22:12,400
your face during the training

448
00:22:12,400 --> 00:22:15,679
so what you can do uh

449
00:22:15,679 --> 00:22:20,240
you can create your neural network

450
00:22:20,240 --> 00:22:23,360
that produce uh two

451
00:22:23,360 --> 00:22:26,720
different types of uh probability

452
00:22:26,720 --> 00:22:28,000
vectors

453
00:22:28,000 --> 00:22:32,720
uh by testing the the target network

454
00:22:32,720 --> 00:22:36,000
so you what you can do you

455
00:22:36,000 --> 00:22:40,640
you take the training data and

456
00:22:40,640 --> 00:22:44,880
uh and then

457
00:22:45,840 --> 00:22:48,400
you put this data in the in the your

458
00:22:48,400 --> 00:22:50,080
target neural network

459
00:22:50,080 --> 00:22:53,200
and you collect all the

460
00:22:53,200 --> 00:22:57,360
probability vectors then you took some

461
00:22:57,360 --> 00:23:01,200
non-training data and then you collect

462
00:23:01,200 --> 00:23:04,320
this probability vectors

463
00:23:04,320 --> 00:23:07,679
uh from non-training data then

464
00:23:07,679 --> 00:23:11,120
you create some kind of uh

465
00:23:11,120 --> 00:23:14,240
your own attack network

466
00:23:14,240 --> 00:23:17,440
and this attack network

467
00:23:17,440 --> 00:23:20,840
you train it to classify

468
00:23:20,840 --> 00:23:23,840
uh the data

469
00:23:23,840 --> 00:23:26,240
from training data from training set and

470
00:23:26,240 --> 00:23:27,760
from non-training set

471
00:23:27,760 --> 00:23:32,320
by uh training this model

472
00:23:32,320 --> 00:23:35,919
uh by showing them probability vectors

473
00:23:35,919 --> 00:23:36,320
from

474
00:23:36,320 --> 00:23:39,679
training and non-training data and then

475
00:23:39,679 --> 00:23:43,919
when you have this uh attack network

476
00:23:43,919 --> 00:23:47,120
now you can take your own picture

477
00:23:47,120 --> 00:23:50,240
and send it to your attack network and

478
00:23:50,240 --> 00:23:52,080
most probably this attack network will

479
00:23:52,080 --> 00:23:53,200
tell you

480
00:23:53,200 --> 00:23:56,320
if this uh

481
00:23:56,320 --> 00:23:58,400
the picture of your face is actually

482
00:23:58,400 --> 00:24:00,159
came from training or

483
00:24:00,159 --> 00:24:02,559
non-training data this is how you can

484
00:24:02,559 --> 00:24:03,760
understand

485
00:24:03,760 --> 00:24:06,799
if the target network

486
00:24:06,799 --> 00:24:10,240
actually used your face

487
00:24:10,240 --> 00:24:13,440
and there are other approaches to do

488
00:24:13,440 --> 00:24:15,679
that

489
00:24:15,760 --> 00:24:19,670
okay well um we have a

490
00:24:19,670 --> 00:24:21,279
[Music]

491
00:24:21,279 --> 00:24:24,080
a little bit of understanding of attacks

492
00:24:24,080 --> 00:24:24,960
now uh

493
00:24:24,960 --> 00:24:26,880
let's look at the assessments so how we

494
00:24:26,880 --> 00:24:29,120
can assess systems

495
00:24:29,120 --> 00:24:33,200
and this is was the example of just

496
00:24:33,200 --> 00:24:37,039
the case study that uh the company

497
00:24:37,039 --> 00:24:39,679
one of the smart solution providers

498
00:24:39,679 --> 00:24:40,400
asked us

499
00:24:40,400 --> 00:24:44,840
to tell them which

500
00:24:44,840 --> 00:24:48,400
camera and algorithm

501
00:24:48,400 --> 00:24:52,080
is the most secure

502
00:24:52,080 --> 00:24:53,840
for implementing the fascial recognition

503
00:24:53,840 --> 00:24:55,520
system

504
00:24:55,520 --> 00:24:58,960
and the problem is the problem was that

505
00:24:58,960 --> 00:25:01,200
there were over 2 000 different research

506
00:25:01,200 --> 00:25:04,159
papers about ai security

507
00:25:04,159 --> 00:25:07,400
and all of those papers have different

508
00:25:07,400 --> 00:25:09,600
[Music]

509
00:25:09,600 --> 00:25:13,039
attacks and different models

510
00:25:13,039 --> 00:25:15,520
different data sets in different

511
00:25:15,520 --> 00:25:16,960
environments

512
00:25:16,960 --> 00:25:20,559
and and there are no clear understanding

513
00:25:20,559 --> 00:25:20,880
of

514
00:25:20,880 --> 00:25:24,159
like a real world if the real

515
00:25:24,159 --> 00:25:26,880
attack is really possible because some

516
00:25:26,880 --> 00:25:28,720
of the attacks were just

517
00:25:28,720 --> 00:25:32,159
based on the uh images of people

518
00:25:32,159 --> 00:25:34,720
not the real you know facial recognition

519
00:25:34,720 --> 00:25:36,320
systems

520
00:25:36,320 --> 00:25:39,679
uh and so on so

521
00:25:39,679 --> 00:25:42,240
in order to to test the solution first

522
00:25:42,240 --> 00:25:43,279
of all

523
00:25:43,279 --> 00:25:46,480
uh we should understand

524
00:25:46,480 --> 00:25:49,679
how to attack it and

525
00:25:49,679 --> 00:25:53,600
to to create some kind to attack um

526
00:25:53,600 --> 00:25:56,480
for example facial recognition system we

527
00:25:56,480 --> 00:25:58,240
should answer a number of questions the

528
00:25:58,240 --> 00:25:58,960
first one

529
00:25:58,960 --> 00:26:01,480
like what is the goal if the goal is

530
00:26:01,480 --> 00:26:03,760
misclassification so just to hide your

531
00:26:03,760 --> 00:26:06,000
face from facial ignition system

532
00:26:06,000 --> 00:26:09,039
or your goal to bypass biometric

533
00:26:09,039 --> 00:26:12,159
security so you want to make your face

534
00:26:12,159 --> 00:26:15,279
look like uh someone else face

535
00:26:15,279 --> 00:26:17,360
so it's the misclassification or target

536
00:26:17,360 --> 00:26:18,720
misclassification

537
00:26:18,720 --> 00:26:21,200
then we have different constraints

538
00:26:21,200 --> 00:26:21,919
because

539
00:26:21,919 --> 00:26:24,559
if we talk about the digital world like

540
00:26:24,559 --> 00:26:25,760
the website

541
00:26:25,760 --> 00:26:29,919
uh checking the image of your face

542
00:26:29,919 --> 00:26:33,760
uh this is one class of

543
00:26:33,760 --> 00:26:37,279
so here one class of attacks

544
00:26:37,279 --> 00:26:40,480
is possible for example you have

545
00:26:40,480 --> 00:26:43,120
you can change each pixel like a little

546
00:26:43,120 --> 00:26:44,480
bit

547
00:26:44,480 --> 00:26:49,679
but if we talk about the physical world

548
00:26:50,320 --> 00:26:54,400
we cannot you know change pixels there

549
00:26:54,400 --> 00:26:58,559
uh because uh

550
00:26:58,559 --> 00:27:00,559
in the physical world you need to to

551
00:27:00,559 --> 00:27:01,600
have like

552
00:27:01,600 --> 00:27:04,840
a real patches on your face

553
00:27:04,840 --> 00:27:08,640
uh because if you change a few pixels

554
00:27:08,640 --> 00:27:11,760
then you have some camera distortion and

555
00:27:11,760 --> 00:27:12,640
so on so

556
00:27:12,640 --> 00:27:15,440
your attack will not work okay we know

557
00:27:15,440 --> 00:27:17,120
some kind of constraints

558
00:27:17,120 --> 00:27:20,480
but then we need to create some form of

559
00:27:20,480 --> 00:27:24,240
uh for the our attack uh so if it's a

560
00:27:24,240 --> 00:27:27,039
again fast recognition bypass we need to

561
00:27:27,039 --> 00:27:29,840
either create glasses so lens or mask or

562
00:27:29,840 --> 00:27:31,279
something

563
00:27:31,279 --> 00:27:33,679
yeah then we need to think about the

564
00:27:33,679 --> 00:27:34,480
algorithm

565
00:27:34,480 --> 00:27:38,320
what kind of algorithm we use to produce

566
00:27:38,320 --> 00:27:41,200
these uh those glasses adversarial

567
00:27:41,200 --> 00:27:42,320
glasses

568
00:27:42,320 --> 00:27:43,840
and then we need to think about the

569
00:27:43,840 --> 00:27:47,520
robustness of of those glasses because

570
00:27:47,520 --> 00:27:50,960
if you conducted the attack

571
00:27:50,960 --> 00:27:54,159
you probably know like like uh you need

572
00:27:54,159 --> 00:27:54,960
like

573
00:27:54,960 --> 00:27:58,320
those pixels in red color but when you

574
00:27:58,320 --> 00:27:59,279
print it

575
00:27:59,279 --> 00:28:01,840
the color is quite different and you

576
00:28:01,840 --> 00:28:04,799
need to deal with all those robustness

577
00:28:04,799 --> 00:28:07,919
issues like inconsistency of colors then

578
00:28:07,919 --> 00:28:09,840
you have different positional glasses

579
00:28:09,840 --> 00:28:12,720
and so on and so forth

580
00:28:12,720 --> 00:28:17,440
and then uh we have model

581
00:28:17,440 --> 00:28:20,799
and everything like a lot of

582
00:28:20,799 --> 00:28:23,200
things really depend on the model

583
00:28:23,200 --> 00:28:24,159
because

584
00:28:24,159 --> 00:28:26,640
uh like what kind of data set you use

585
00:28:26,640 --> 00:28:27,600
what is the model

586
00:28:27,600 --> 00:28:30,640
architecture what is the model input

587
00:28:30,640 --> 00:28:34,640
or output and like

588
00:28:34,640 --> 00:28:36,880
how how do you test this model is if

589
00:28:36,880 --> 00:28:38,880
it's a white box testing or

590
00:28:38,880 --> 00:28:43,360
if it's a black box testing uh and so on

591
00:28:43,360 --> 00:28:46,720
so a lot of questions actually uh

592
00:28:46,720 --> 00:28:49,840
you need to answer to understand uh

593
00:28:49,840 --> 00:28:52,880
to to check different models

594
00:28:52,880 --> 00:28:55,679
security and then of course we have

595
00:28:55,679 --> 00:28:56,640
environment

596
00:28:56,640 --> 00:29:00,480
because even if you can break any model

597
00:29:00,480 --> 00:29:03,840
in the lab when you transfer to the real

598
00:29:03,840 --> 00:29:05,120
environment

599
00:29:05,120 --> 00:29:08,240
you have issues like different

600
00:29:08,240 --> 00:29:10,840
lights and brightness and distance to

601
00:29:10,840 --> 00:29:12,240
objects

602
00:29:12,240 --> 00:29:15,200
we have some device features like

603
00:29:15,200 --> 00:29:16,880
different cameras have different color

604
00:29:16,880 --> 00:29:17,520
rendering

605
00:29:17,520 --> 00:29:20,399
and resolution quality and different

606
00:29:20,399 --> 00:29:22,480
preprocessor features like codecs and

607
00:29:22,480 --> 00:29:25,279
data transfer compression and so on

608
00:29:25,279 --> 00:29:29,200
so we need to take all this into account

609
00:29:29,200 --> 00:29:32,159
when we do the actual ai security

610
00:29:32,159 --> 00:29:33,039
testing

611
00:29:33,039 --> 00:29:36,399
so i can say this is this is not uh

612
00:29:36,399 --> 00:29:39,520
simple uh this is a

613
00:29:39,520 --> 00:29:43,039
really like complex uh task

614
00:29:43,039 --> 00:29:45,600
uh but this is the only way how you can

615
00:29:45,600 --> 00:29:46,480
really

616
00:29:46,480 --> 00:29:49,840
um check if your

617
00:29:49,840 --> 00:29:53,279
uh ai solution is first of all

618
00:29:53,279 --> 00:29:56,559
vulnerable and

619
00:29:56,559 --> 00:30:00,559
like this is the real threat

620
00:30:00,720 --> 00:30:04,720
and once you assess your system

621
00:30:04,720 --> 00:30:08,080
your next question is how to defend it

622
00:30:08,080 --> 00:30:11,360
and here we have uh

623
00:30:11,360 --> 00:30:14,399
four approaches the first approach

624
00:30:14,399 --> 00:30:17,440
is a kind of security assessment

625
00:30:17,440 --> 00:30:21,520
it's a prediction stage here we have

626
00:30:21,520 --> 00:30:25,039
like testing against different types of

627
00:30:25,039 --> 00:30:26,399
attacks

628
00:30:26,399 --> 00:30:30,399
or uh different verification methods

629
00:30:30,399 --> 00:30:33,679
um this approach is

630
00:30:33,679 --> 00:30:36,720
quite easy to apply because some of the

631
00:30:36,720 --> 00:30:38,720
attacks are available and you can test

632
00:30:38,720 --> 00:30:40,640
against those attacks

633
00:30:40,640 --> 00:30:42,720
this approach is more or less

634
00:30:42,720 --> 00:30:44,080
transferable

635
00:30:44,080 --> 00:30:47,520
because you can apply like some of the

636
00:30:47,520 --> 00:30:48,159
attacks

637
00:30:48,159 --> 00:30:52,320
for um different types of machine

638
00:30:52,320 --> 00:30:56,240
ai systems but

639
00:30:56,240 --> 00:30:59,519
for example the attack testing it's

640
00:30:59,519 --> 00:31:02,559
really limited

641
00:31:02,559 --> 00:31:05,519
so if you test for one attack it doesn't

642
00:31:05,519 --> 00:31:07,120
mean that it's not vulnerable for other

643
00:31:07,120 --> 00:31:07,760
attack

644
00:31:07,760 --> 00:31:09,840
and if we talk about the verification

645
00:31:09,840 --> 00:31:11,840
methods they are

646
00:31:11,840 --> 00:31:14,960
more precise but they are very slow

647
00:31:14,960 --> 00:31:16,880
so you can look at this area as a

648
00:31:16,880 --> 00:31:18,320
penetration testing

649
00:31:18,320 --> 00:31:21,440
like application security for

650
00:31:21,440 --> 00:31:24,480
ai then we have

651
00:31:24,480 --> 00:31:28,399
prevention approaches

652
00:31:28,399 --> 00:31:31,840
like modification of input for example

653
00:31:31,840 --> 00:31:33,120
jpeg compression

654
00:31:33,120 --> 00:31:36,320
for all images or a

655
00:31:36,320 --> 00:31:40,720
new type of deep learning model

656
00:31:40,720 --> 00:31:44,080
or a modified way

657
00:31:44,080 --> 00:31:45,919
how to train the system for example

658
00:31:45,919 --> 00:31:47,840
adversarial training

659
00:31:47,840 --> 00:31:50,399
when you add adversarial examples to

660
00:31:50,399 --> 00:31:53,200
your training set

661
00:31:53,200 --> 00:31:55,600
those approaches are in general are very

662
00:31:55,600 --> 00:31:57,120
good

663
00:31:57,120 --> 00:32:01,279
but the issue that they are

664
00:32:01,279 --> 00:32:04,240
task specific so if you modify input

665
00:32:04,240 --> 00:32:06,880
with jpeg compression you know it only

666
00:32:06,880 --> 00:32:09,840
uh can be applicable to images and

667
00:32:09,840 --> 00:32:11,519
particular types of images

668
00:32:11,519 --> 00:32:14,080
and so on and also those approaches are

669
00:32:14,080 --> 00:32:15,519
quite complex

670
00:32:15,519 --> 00:32:18,640
so if you want to modify your module

671
00:32:18,640 --> 00:32:20,799
yeah we you have some advantages but you

672
00:32:20,799 --> 00:32:23,200
also have some disadvantages

673
00:32:23,200 --> 00:32:25,360
this is like uh different types of

674
00:32:25,360 --> 00:32:26,320
hardware

675
00:32:26,320 --> 00:32:30,000
hardening and vulnerability management

676
00:32:30,000 --> 00:32:33,120
kind of approach for ai

677
00:32:33,120 --> 00:32:36,640
if you compare it with traditional

678
00:32:36,640 --> 00:32:40,000
security solutions then we have a

679
00:32:40,000 --> 00:32:43,120
detection approach we can detect by

680
00:32:43,120 --> 00:32:46,640
supervised learning

681
00:32:46,640 --> 00:32:49,120
attacks that we know or by unsupervised

682
00:32:49,120 --> 00:32:52,000
learning attacks that we don't know

683
00:32:52,000 --> 00:32:56,000
um a number of methods are really good

684
00:32:56,000 --> 00:32:58,159
i say a number because there are a lot

685
00:32:58,159 --> 00:32:59,679
of different

686
00:32:59,679 --> 00:33:01,519
detect approaches but some of them

687
00:33:01,519 --> 00:33:02,880
really good

688
00:33:02,880 --> 00:33:05,200
and they are more or less easy to

689
00:33:05,200 --> 00:33:06,000
implement

690
00:33:06,000 --> 00:33:08,159
you don't need to you know change your

691
00:33:08,159 --> 00:33:09,600
model on something

692
00:33:09,600 --> 00:33:12,000
unfortunately it can be too late to

693
00:33:12,000 --> 00:33:12,640
detect

694
00:33:12,640 --> 00:33:16,480
attack because it's already

695
00:33:18,159 --> 00:33:21,440
exploited so sometimes it can be

696
00:33:21,440 --> 00:33:24,559
too late to detect attacks

697
00:33:24,559 --> 00:33:27,760
and some of the detection methods can be

698
00:33:27,760 --> 00:33:30,559
applicable to your solution so you can

699
00:33:30,559 --> 00:33:31,760
look at this as a

700
00:33:31,760 --> 00:33:33,679
security monitoring or thread detection

701
00:33:33,679 --> 00:33:35,360
type of solutions for

702
00:33:35,360 --> 00:33:38,640
ai and

703
00:33:38,640 --> 00:33:41,919
respond uh those are

704
00:33:41,919 --> 00:33:45,360
also different approaches

705
00:33:45,360 --> 00:33:48,080
uh currently there are very few papers

706
00:33:48,080 --> 00:33:50,159
in this area but they exist

707
00:33:50,159 --> 00:33:52,880
are they mostly about like counter

708
00:33:52,880 --> 00:33:53,840
attacking

709
00:33:53,840 --> 00:33:57,440
or online retraining or some ways

710
00:33:57,440 --> 00:34:02,000
to um kind of hide both approaches

711
00:34:02,000 --> 00:34:05,600
to fool the attacker those approaches

712
00:34:05,600 --> 00:34:06,399
are

713
00:34:06,399 --> 00:34:09,280
really unusual so they're quite hard to

714
00:34:09,280 --> 00:34:10,000
bypass

715
00:34:10,000 --> 00:34:13,119
because attackers don't even expect it

716
00:34:13,119 --> 00:34:15,359
but unfortunately they much harder to

717
00:34:15,359 --> 00:34:16,800
implement

718
00:34:16,800 --> 00:34:20,560
uh comparing to other approaches and

719
00:34:20,560 --> 00:34:22,679
in some cases they're really

720
00:34:22,679 --> 00:34:24,399
unpredictable

721
00:34:24,399 --> 00:34:26,879
so you can look at those approaches like

722
00:34:26,879 --> 00:34:28,159
incident response

723
00:34:28,159 --> 00:34:31,280
or like real-time application security

724
00:34:31,280 --> 00:34:32,079
protection

725
00:34:32,079 --> 00:34:37,280
and kind of valve for ai solutions

726
00:34:37,280 --> 00:34:40,960
so uh as a result uh for

727
00:34:40,960 --> 00:34:44,639
air security life cycle i would say that

728
00:34:44,639 --> 00:34:47,918
all of those approaches are important

729
00:34:47,918 --> 00:34:50,879
just in the particular direction like in

730
00:34:50,879 --> 00:34:53,199
direction which is presented here

731
00:34:53,199 --> 00:34:55,760
so you should start with kind of

732
00:34:55,760 --> 00:34:57,119
predicting

733
00:34:57,119 --> 00:34:59,599
uh like conduct some kind of security

734
00:34:59,599 --> 00:35:00,400
assessment

735
00:35:00,400 --> 00:35:02,880
and understand what are the risks the

736
00:35:02,880 --> 00:35:06,000
next step is to apply some kind of fixes

737
00:35:06,000 --> 00:35:09,040
to prevent those risks uh but also

738
00:35:09,040 --> 00:35:11,200
next step is to apply some kind of

739
00:35:11,200 --> 00:35:12,640
detection mechanisms

740
00:35:12,640 --> 00:35:16,160
to monitor and threat monitor stress and

741
00:35:16,160 --> 00:35:17,200
anomalies

742
00:35:17,200 --> 00:35:19,520
and finally uh some kind of responsive

743
00:35:19,520 --> 00:35:20,720
measures

744
00:35:20,720 --> 00:35:23,359
to react and mitigate and retrain and so

745
00:35:23,359 --> 00:35:24,560
on

746
00:35:24,560 --> 00:35:27,599
so uh thanks for listening

747
00:35:27,599 --> 00:35:30,800
and i hope you now have at least a

748
00:35:30,800 --> 00:35:31,760
little bit

749
00:35:31,760 --> 00:35:35,200
more understanding on ai security

750
00:35:35,200 --> 00:35:38,240
and if you have any questions i'm here

751
00:35:38,240 --> 00:35:40,800
uh to answer them and you can always

752
00:35:40,800 --> 00:35:42,560
write me

753
00:35:42,560 --> 00:35:45,599
you can write on this address or

754
00:35:45,599 --> 00:35:48,880
uh to

755
00:35:48,880 --> 00:35:52,000
alex at verso i

756
00:35:52,000 --> 00:35:55,920
and i love to talk about the ai security

757
00:35:55,920 --> 00:35:59,359
so thanks for listening uh thanks for

758
00:35:59,359 --> 00:36:00,160
joining

759
00:36:00,160 --> 00:36:04,320
for joining um if you want to

760
00:36:04,320 --> 00:36:06,640
you know

761
00:36:07,680 --> 00:36:12,160
be part of our team you also can

762
00:36:12,160 --> 00:36:15,680
write me and we're hiring so

763
00:36:15,680 --> 00:36:18,880
thank you again and

764
00:36:18,880 --> 00:36:22,400
have a nice day

