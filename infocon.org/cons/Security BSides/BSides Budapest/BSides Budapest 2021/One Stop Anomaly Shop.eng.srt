1
00:00:00,399 --> 00:00:02,320
hello everyone good morning good

2
00:00:02,320 --> 00:00:04,319
afternoon good evening wherever you are

3
00:00:04,319 --> 00:00:05,359
in the world

4
00:00:05,359 --> 00:00:07,759
my name is vivek malik and together with

5
00:00:07,759 --> 00:00:09,599
my colleague kumar vikramjeet we'll have

6
00:00:09,599 --> 00:00:11,519
a short discussion about a project that

7
00:00:11,519 --> 00:00:14,880
we recently open sourced

8
00:00:14,920 --> 00:00:16,640
next

9
00:00:16,640 --> 00:00:19,279
who are we a couple of words about us

10
00:00:19,279 --> 00:00:21,600
we are adobe's security intelligence

11
00:00:21,600 --> 00:00:23,680
team which is part of adobe's security

12
00:00:23,680 --> 00:00:25,920
coordination center the purpose of the

13
00:00:25,920 --> 00:00:27,680
team is to do data science research and

14
00:00:27,680 --> 00:00:29,920
security field we mostly focus on

15
00:00:29,920 --> 00:00:32,238
reactive security and that is basically

16
00:00:32,238 --> 00:00:33,840
identifying threats that cannot be

17
00:00:33,840 --> 00:00:36,640
detected by a conventional ways in other

18
00:00:36,640 --> 00:00:39,280
words we use the logs and security data

19
00:00:39,280 --> 00:00:41,040
that we collect from adobe's assets and

20
00:00:41,040 --> 00:00:42,879
we try to find anomalies and the bad

21
00:00:42,879 --> 00:00:45,760
stuff uh the part of the members of this

22
00:00:45,760 --> 00:00:48,480
team are andre kotai iberi boros kumar

23
00:00:48,480 --> 00:00:52,399
vikram jeet myself vivek malik

24
00:00:52,399 --> 00:00:54,719
next

25
00:00:57,440 --> 00:01:00,399
osas or one stop anomaly shock this is a

26
00:01:00,399 --> 00:01:02,000
machine learning framework aimed to

27
00:01:02,000 --> 00:01:04,879
discover anomalies in a given data set

28
00:01:04,879 --> 00:01:06,720
the open source project represents an

29
00:01:06,720 --> 00:01:08,720
implementation of several adobe security

30
00:01:08,720 --> 00:01:11,520
intelligence teams patents white papers

31
00:01:11,520 --> 00:01:13,840
and other projects the goal of osas is

32
00:01:13,840 --> 00:01:16,080
to make it as easy as possible to

33
00:01:16,080 --> 00:01:18,240
experiment with different data sets

34
00:01:18,240 --> 00:01:20,640
algorithms and feature combinations and

35
00:01:20,640 --> 00:01:22,560
find the most balanced combination for

36
00:01:22,560 --> 00:01:25,680
your own use case as well as data

37
00:01:25,680 --> 00:01:27,280
but most importantly

38
00:01:27,280 --> 00:01:30,400
osas will try to answer why an anomaly

39
00:01:30,400 --> 00:01:33,280
is considered actually an anomaly

40
00:01:33,280 --> 00:01:34,960
to better understand this

41
00:01:34,960 --> 00:01:37,360
uh let's really uh talk fast about

42
00:01:37,360 --> 00:01:38,960
anomaly detection

43
00:01:38,960 --> 00:01:41,839
next

44
00:01:42,000 --> 00:01:44,479
the anomaly detection principle uh this

45
00:01:44,479 --> 00:01:46,479
principle is as simple as pi

46
00:01:46,479 --> 00:01:49,280
anomalies represent extremely rare

47
00:01:49,280 --> 00:01:53,280
events the more complex question is how

48
00:01:53,280 --> 00:01:55,360
there are some well-known algorithms for

49
00:01:55,360 --> 00:01:57,840
outlier detection such as local outlier

50
00:01:57,840 --> 00:02:00,719
factor or the isolation forest k means

51
00:02:00,719 --> 00:02:02,079
and distance from

52
00:02:02,079 --> 00:02:03,680
the center of a cluster error

53
00:02:03,680 --> 00:02:06,240
reconstruction and so on sometimes these

54
00:02:06,240 --> 00:02:08,639
algorithms generate anomalies

55
00:02:08,639 --> 00:02:11,200
and our stock or security experts don't

56
00:02:11,200 --> 00:02:13,680
understand why these are actually

57
00:02:13,680 --> 00:02:16,319
anomalies and the simple answer

58
00:02:16,319 --> 00:02:18,480
anomalies represent an extremely rare

59
00:02:18,480 --> 00:02:22,000
event um that is not enough for them so

60
00:02:22,000 --> 00:02:23,680
coming back to anomaly detection

61
00:02:23,680 --> 00:02:26,319
principle let's look at the picture uh

62
00:02:26,319 --> 00:02:28,000
in the slide and try to identify an

63
00:02:28,000 --> 00:02:30,080
anomaly

64
00:02:30,080 --> 00:02:32,319
everyone's first reaction would be the

65
00:02:32,319 --> 00:02:34,319
egg that is of a different color

66
00:02:34,319 --> 00:02:36,640
that is the brown egg is an anomaly and

67
00:02:36,640 --> 00:02:38,720
maybe you're right but if you look close

68
00:02:38,720 --> 00:02:40,319
enough at the picture

69
00:02:40,319 --> 00:02:42,640
all the eggs are wearing facial masks

70
00:02:42,640 --> 00:02:44,800
except maybe two or three so maybe

71
00:02:44,800 --> 00:02:46,239
that's an anomaly that i'm actually

72
00:02:46,239 --> 00:02:47,280
looking for

73
00:02:47,280 --> 00:02:49,280
the bottom line is the way you prepare

74
00:02:49,280 --> 00:02:51,760
your data is going to influence what

75
00:02:51,760 --> 00:02:54,720
kind of anomalies you generate so the

76
00:02:54,720 --> 00:02:56,879
data preparation is fundamentally very

77
00:02:56,879 --> 00:02:58,000
important

78
00:02:58,000 --> 00:03:00,400
next

79
00:03:02,959 --> 00:03:04,720
now let us try to bring more context to

80
00:03:04,720 --> 00:03:06,640
our egg situation and briefly briefly

81
00:03:06,640 --> 00:03:08,720
describe each one of them

82
00:03:08,720 --> 00:03:10,959
so we see a couple of eggs appear to be

83
00:03:10,959 --> 00:03:13,360
in love one is sneezing one definitely

84
00:03:13,360 --> 00:03:15,840
has eye problems some are mad some are

85
00:03:15,840 --> 00:03:18,239
scared and so on now this is important

86
00:03:18,239 --> 00:03:20,319
because the fact that we have a brown

87
00:03:20,319 --> 00:03:22,720
egg is not the most exotic feature

88
00:03:22,720 --> 00:03:24,799
anymore the sneezing egg becomes

89
00:03:24,799 --> 00:03:27,200
important the egg that is crank is also

90
00:03:27,200 --> 00:03:29,760
unique and so on in other words what we

91
00:03:29,760 --> 00:03:32,319
propose is falling we'll describe each

92
00:03:32,319 --> 00:03:35,360
observation eggs here in this case with

93
00:03:35,360 --> 00:03:37,760
a set of labels and then let anomaly

94
00:03:37,760 --> 00:03:40,000
detection algorithms decide which ones

95
00:03:40,000 --> 00:03:42,720
are actually the anomalies and the way

96
00:03:42,720 --> 00:03:44,959
we generate labels becomes the entire

97
00:03:44,959 --> 00:03:47,200
magic so you see in this

98
00:03:47,200 --> 00:03:48,159
picture

99
00:03:48,159 --> 00:03:49,360
uh

100
00:03:49,360 --> 00:03:52,799
the egg white has mask open eyes and

101
00:03:52,799 --> 00:03:56,400
then needs eye surgery that becomes the

102
00:03:56,400 --> 00:03:59,840
outcome of an algorithm similarly egg

103
00:03:59,840 --> 00:04:03,280
white has mask open eyes and love or has

104
00:04:03,280 --> 00:04:06,560
maybe chicken pox looks surprise becomes

105
00:04:06,560 --> 00:04:07,599
the

106
00:04:07,599 --> 00:04:08,319
uh

107
00:04:08,319 --> 00:04:10,879
outcome of our algorithm that we run

108
00:04:10,879 --> 00:04:14,000
another egg that is white has mask open

109
00:04:14,000 --> 00:04:17,120
eyes but is an angry person so

110
00:04:17,120 --> 00:04:18,639
probably that's the anomaly that we are

111
00:04:18,639 --> 00:04:21,279
looking for so these are the labels that

112
00:04:21,279 --> 00:04:23,919
uh we are trying to implement uh with

113
00:04:23,919 --> 00:04:25,840
one stop and normally shop

114
00:04:25,840 --> 00:04:28,840
next

115
00:04:29,520 --> 00:04:30,960
so let's talk about

116
00:04:30,960 --> 00:04:33,280
the tags or the label generators we have

117
00:04:33,280 --> 00:04:35,919
three types of tags i will start by

118
00:04:35,919 --> 00:04:38,800
saying that when we designed osas we had

119
00:04:38,800 --> 00:04:41,440
in mind semi-structured log data this

120
00:04:41,440 --> 00:04:44,639
type of data is usually found in access

121
00:04:44,639 --> 00:04:49,120
logs or error logs from apache tomcat in

122
00:04:49,120 --> 00:04:51,199
hubble and so on hubble by the way is

123
00:04:51,199 --> 00:04:53,280
another open source project from adobe

124
00:04:53,280 --> 00:04:54,960
but you can check it out

125
00:04:54,960 --> 00:04:56,479
this information is semi-structured

126
00:04:56,479 --> 00:04:58,479
because you can infer attributes from

127
00:04:58,479 --> 00:04:59,280
end

128
00:04:59,280 --> 00:05:02,960
for instance while apache log is x based

129
00:05:02,960 --> 00:05:05,680
you can extract timestamp ip addresses

130
00:05:05,680 --> 00:05:09,360
url response code etc etc one important

131
00:05:09,360 --> 00:05:12,320
aspect is that this data contains many

132
00:05:12,320 --> 00:05:14,560
attributes with unbound values for

133
00:05:14,560 --> 00:05:17,680
instance the ip addresses and urls

134
00:05:17,680 --> 00:05:20,240
usually these attributes generate large

135
00:05:20,240 --> 00:05:21,440
feature space

136
00:05:21,440 --> 00:05:23,919
by comparison the number of examples

137
00:05:23,919 --> 00:05:26,880
that can be contained in a manageable

138
00:05:26,880 --> 00:05:29,280
data set and that can be processed by

139
00:05:29,280 --> 00:05:30,639
machine learning algorithms is

140
00:05:30,639 --> 00:05:32,160
relatively small

141
00:05:32,160 --> 00:05:34,639
these two factors combined generate

142
00:05:34,639 --> 00:05:37,520
uh something known as data sparsity an

143
00:05:37,520 --> 00:05:39,680
unwanted effect that makes machine

144
00:05:39,680 --> 00:05:42,400
learning learning algorithms overfit the

145
00:05:42,400 --> 00:05:44,560
training data and generalize poorly on

146
00:05:44,560 --> 00:05:47,520
previously unseen examples to cope this

147
00:05:47,520 --> 00:05:50,080
or to cope with this osas reduces the

148
00:05:50,080 --> 00:05:52,639
large features based by employing a

149
00:05:52,639 --> 00:05:55,199
tagging strategy on the raw data set

150
00:05:55,199 --> 00:05:57,039
this is done before feeding the data to

151
00:05:57,039 --> 00:05:58,960
anomaly detection or classification

152
00:05:58,960 --> 00:06:01,520
algorithm the tagging strategy uses

153
00:06:01,520 --> 00:06:04,840
predefined recipes for specific field

154
00:06:04,840 --> 00:06:07,840
types now this is based on

155
00:06:07,840 --> 00:06:10,400
three type of tagging strategies that we

156
00:06:10,400 --> 00:06:12,800
use the standard tags the text tags and

157
00:06:12,800 --> 00:06:15,440
the expert knowledge

158
00:06:15,440 --> 00:06:18,440
next

159
00:06:20,560 --> 00:06:21,919
uh

160
00:06:21,919 --> 00:06:23,919
there are five types of label generators

161
00:06:23,919 --> 00:06:27,039
under each tagging strategy uh combined

162
00:06:27,039 --> 00:06:29,120
and we're going to go through each of

163
00:06:29,120 --> 00:06:31,680
these uh separately so let's take a look

164
00:06:31,680 --> 00:06:34,400
at the multinomial field uh

165
00:06:34,400 --> 00:06:37,440
the tags provided by these fields is uh

166
00:06:37,440 --> 00:06:38,720
check for the

167
00:06:38,720 --> 00:06:40,400
data that counts like in detection

168
00:06:40,400 --> 00:06:42,319
algorithms uh it counts for unique

169
00:06:42,319 --> 00:06:44,560
attribute values and usually they're

170
00:06:44,560 --> 00:06:46,800
less than 10 unique attribute values the

171
00:06:46,800 --> 00:06:49,280
kind of models that you can run on the

172
00:06:49,280 --> 00:06:51,440
multinomial fields is either start

173
00:06:51,440 --> 00:06:53,759
statistical distribution values or label

174
00:06:53,759 --> 00:06:56,240
based value frequency and special tax

175
00:06:56,240 --> 00:06:58,240
for any unseen data

176
00:06:58,240 --> 00:07:00,160
under numerical fields

177
00:07:00,160 --> 00:07:02,000
usually you can count for unique

178
00:07:02,000 --> 00:07:03,759
attribute values and more than 10

179
00:07:03,759 --> 00:07:05,599
percent of unique values

180
00:07:05,599 --> 00:07:08,880
are available in uh numerical fields all

181
00:07:08,880 --> 00:07:11,120
values should be numerical or there

182
00:07:11,120 --> 00:07:13,199
should be none uh the type of models

183
00:07:13,199 --> 00:07:16,240
that can run are like standard deviation

184
00:07:16,240 --> 00:07:18,720
mean medial uh label based gaussian

185
00:07:18,720 --> 00:07:20,479
probability

186
00:07:20,479 --> 00:07:23,360
finally field combine field combiners um

187
00:07:23,360 --> 00:07:25,440
the detection mechanism uses all

188
00:07:25,440 --> 00:07:27,360
multinomial fields that can be combined

189
00:07:27,360 --> 00:07:30,319
together uh the models that are executed

190
00:07:30,319 --> 00:07:32,400
on field combiners is statistical

191
00:07:32,400 --> 00:07:34,240
distribution of values in the label

192
00:07:34,240 --> 00:07:37,039
based uh frequency on special tags for

193
00:07:37,039 --> 00:07:38,960
the unseen data

194
00:07:38,960 --> 00:07:41,280
next

195
00:07:41,680 --> 00:07:42,840
a text

196
00:07:42,840 --> 00:07:44,639
field

197
00:07:44,639 --> 00:07:47,680
the type of detection is non-numerical

198
00:07:47,680 --> 00:07:49,360
you can only count unique attribute

199
00:07:49,360 --> 00:07:51,039
values and more than 10 percent of

200
00:07:51,039 --> 00:07:53,120
unique values are available

201
00:07:53,120 --> 00:07:55,440
you can compute the algorithms such as

202
00:07:55,440 --> 00:07:58,800
engram language model compute perplexity

203
00:07:58,800 --> 00:08:01,280
of each example mean or standard

204
00:08:01,280 --> 00:08:03,840
deviation and then label based on

205
00:08:03,840 --> 00:08:05,599
gaussian probability

206
00:08:05,599 --> 00:08:07,840
finally expert knowledge

207
00:08:07,840 --> 00:08:10,080
the detection is manual in this case and

208
00:08:10,080 --> 00:08:12,319
the models that you run are keyword or

209
00:08:12,319 --> 00:08:14,319
reject reject space

210
00:08:14,319 --> 00:08:16,720
the labels are generated for matched

211
00:08:16,720 --> 00:08:19,280
instances

212
00:08:19,280 --> 00:08:21,679
next

213
00:08:22,639 --> 00:08:24,960
the anomaly detection algorithms

214
00:08:24,960 --> 00:08:26,960
by default osas has four anomaly

215
00:08:26,960 --> 00:08:28,720
detection algorithms the isolation

216
00:08:28,720 --> 00:08:32,080
forest local outlier factor svd

217
00:08:32,080 --> 00:08:33,279
based uh

218
00:08:33,279 --> 00:08:35,760
anomaly detector or the statistical

219
00:08:35,760 --> 00:08:38,240
engram the first three are specific to

220
00:08:38,240 --> 00:08:40,640
scikit learn and if you want to know

221
00:08:40,640 --> 00:08:42,640
more about how they work we suggest you

222
00:08:42,640 --> 00:08:44,959
consult official documentation and the

223
00:08:44,959 --> 00:08:46,959
papers associated with it the

224
00:08:46,959 --> 00:08:48,720
statistical engram method is an

225
00:08:48,720 --> 00:08:51,200
algorithm designed by us it uses

226
00:08:51,200 --> 00:08:54,160
statistics to compute the probability of

227
00:08:54,160 --> 00:08:56,240
observing combination of tags and

228
00:08:56,240 --> 00:08:59,519
compute the anomaly score using sum of

229
00:08:59,519 --> 00:09:02,000
sum over what is known as the negative

230
00:09:02,000 --> 00:09:05,120
log likelihood

231
00:09:05,440 --> 00:09:08,440
next

232
00:09:08,480 --> 00:09:10,399
in a nutshell the pipeline contains

233
00:09:10,399 --> 00:09:11,519
three main

234
00:09:11,519 --> 00:09:14,480
modules on the left side we have the

235
00:09:14,480 --> 00:09:16,800
data acquisition mode which uses a

236
00:09:16,800 --> 00:09:18,720
security uh

237
00:09:18,720 --> 00:09:21,440
incident event monitoring uh for runtime

238
00:09:21,440 --> 00:09:24,880
and statistically uh compiled data for

239
00:09:24,880 --> 00:09:27,120
training the middle section is called

240
00:09:27,120 --> 00:09:29,120
the labeling or data grooming module and

241
00:09:29,120 --> 00:09:32,399
it applies labels for each

242
00:09:32,399 --> 00:09:34,959
interesting attribute type finally the

243
00:09:34,959 --> 00:09:36,880
right section is the scoring module

244
00:09:36,880 --> 00:09:38,720
which uses one of the three strategies

245
00:09:38,720 --> 00:09:41,839
to assign scores to examples

246
00:09:41,839 --> 00:09:44,160
the three scoring strategies are get

247
00:09:44,160 --> 00:09:47,279
tagged label data compute uh supervised

248
00:09:47,279 --> 00:09:48,800
and unsupervised scoring models and

249
00:09:48,800 --> 00:09:51,760
generate a supervised risk-based anomaly

250
00:09:51,760 --> 00:09:53,839
search for your scene

251
00:09:53,839 --> 00:09:56,160
during training we use statistic data

252
00:09:56,160 --> 00:09:58,080
sets to compute

253
00:09:58,080 --> 00:10:00,160
static data sets to compute statistics

254
00:10:00,160 --> 00:10:02,839
and language models for the attributes

255
00:10:02,839 --> 00:10:05,279
labels label the data set and compute

256
00:10:05,279 --> 00:10:07,600
the model for three scoring strategies

257
00:10:07,600 --> 00:10:09,760
at runtime we just apply all three

258
00:10:09,760 --> 00:10:11,839
elements of the pipeline using

259
00:10:11,839 --> 00:10:14,000
precomputed models and generate a score

260
00:10:14,000 --> 00:10:16,240
for each individual example

261
00:10:16,240 --> 00:10:19,839
previously unseen in the dark side

262
00:10:19,839 --> 00:10:22,240
next

263
00:10:22,800 --> 00:10:26,079
now let kumar take over

264
00:10:27,680 --> 00:10:29,519
thank you vivek

265
00:10:29,519 --> 00:10:31,760
so i'll i'm kumar here i'll go through

266
00:10:31,760 --> 00:10:34,800
the demo so in this demo we'll cover

267
00:10:34,800 --> 00:10:37,680
three important aspects of oss

268
00:10:37,680 --> 00:10:39,680
first we'll look into how to

269
00:10:39,680 --> 00:10:41,519
get it up and running

270
00:10:41,519 --> 00:10:43,839
next we'll run oss with

271
00:10:43,839 --> 00:10:44,839
default

272
00:10:44,839 --> 00:10:47,360
configuration and after that we'll

273
00:10:47,360 --> 00:10:48,880
customize

274
00:10:48,880 --> 00:10:51,600
oss with expert knowledge

275
00:10:51,600 --> 00:10:53,600
and we'll compare how the behavior

276
00:10:53,600 --> 00:10:56,000
changes and how the improvements impact

277
00:10:56,000 --> 00:10:59,480
oss results

278
00:11:02,000 --> 00:11:04,320
so let's get oss

279
00:11:04,320 --> 00:11:07,600
oscs up and running

280
00:11:07,760 --> 00:11:10,160
so in the git page

281
00:11:10,160 --> 00:11:12,640
we can get all the links to get started

282
00:11:12,640 --> 00:11:14,640
with

283
00:11:14,640 --> 00:11:17,040
so the git page has a quick start guide

284
00:11:17,040 --> 00:11:19,760
there are basically two ways to install

285
00:11:19,760 --> 00:11:20,959
oss

286
00:11:20,959 --> 00:11:22,399
you can either

287
00:11:22,399 --> 00:11:24,640
download a pre-compiled image which is

288
00:11:24,640 --> 00:11:27,200
the easier way to get it up and running

289
00:11:27,200 --> 00:11:30,720
and alternatively you can also

290
00:11:30,720 --> 00:11:33,120
build the image locally

291
00:11:33,120 --> 00:11:35,440
and another way can be to just work on

292
00:11:35,440 --> 00:11:37,360
the git repo

293
00:11:37,360 --> 00:11:38,320
and

294
00:11:38,320 --> 00:11:42,440
make changes as per your need

295
00:11:44,160 --> 00:11:47,440
so next we'll go to the

296
00:11:47,440 --> 00:11:49,760
folder where our data set is stored

297
00:11:49,760 --> 00:11:52,399
so in this folder we see there is a csv

298
00:11:52,399 --> 00:11:55,360
file which is our default data set we

299
00:11:55,360 --> 00:11:57,360
are going to use for this demo

300
00:11:57,360 --> 00:11:59,680
so this data set has

301
00:11:59,680 --> 00:12:03,519
around 5 5 000 events and we have also

302
00:12:03,519 --> 00:12:05,440
inserted some of the malicious events

303
00:12:05,440 --> 00:12:08,320
which we want to detect as anomalies

304
00:12:08,320 --> 00:12:10,800
which is our end goal

305
00:12:10,800 --> 00:12:13,040
so we can start with pulling the docker

306
00:12:13,040 --> 00:12:15,120
image

307
00:12:15,120 --> 00:12:17,440
and then we'll run

308
00:12:17,440 --> 00:12:20,399
docker run command

309
00:12:20,560 --> 00:12:22,800
so in this command we are specifying two

310
00:12:22,800 --> 00:12:26,079
ports so the first port is for exposing

311
00:12:26,079 --> 00:12:28,480
oss web service

312
00:12:28,480 --> 00:12:31,360
the second port is for exposing elastic

313
00:12:31,360 --> 00:12:34,000
search kibana front end

314
00:12:34,000 --> 00:12:36,240
the last argument you see where we are

315
00:12:36,240 --> 00:12:39,839
exposing the local folder to osos so

316
00:12:39,839 --> 00:12:42,560
that it can access the default dataset

317
00:12:42,560 --> 00:12:45,199
that we are using

318
00:12:48,639 --> 00:12:50,480
so once we run it

319
00:12:50,480 --> 00:12:52,480
it will

320
00:12:52,480 --> 00:12:53,600
uh

321
00:12:53,600 --> 00:12:55,760
it will initiate the two web service

322
00:12:55,760 --> 00:12:58,160
that uh is basically the front end and

323
00:12:58,160 --> 00:13:00,880
back end of oss so we'll go to the

324
00:13:00,880 --> 00:13:03,519
console link

325
00:13:04,639 --> 00:13:06,959
or we are opening both kibana and the

326
00:13:06,959 --> 00:13:08,240
console link

327
00:13:08,240 --> 00:13:11,600
so once we go to the console

328
00:13:11,600 --> 00:13:15,279
uh we can see whether the

329
00:13:15,360 --> 00:13:17,680
the our test data set is present or not

330
00:13:17,680 --> 00:13:20,399
so we also provide three other oss

331
00:13:20,399 --> 00:13:22,320
endpoints which are not mentioned in the

332
00:13:22,320 --> 00:13:23,760
readme file

333
00:13:23,760 --> 00:13:25,519
but it can be used

334
00:13:25,519 --> 00:13:27,760
so the console basically provides

335
00:13:27,760 --> 00:13:30,560
command line interaction with oss

336
00:13:30,560 --> 00:13:32,639
using the automated pipeline you can run

337
00:13:32,639 --> 00:13:35,279
the entire workflow

338
00:13:35,279 --> 00:13:37,920
and using the generate config endpoint

339
00:13:37,920 --> 00:13:41,279
you can generate the config

340
00:13:41,279 --> 00:13:42,399
so

341
00:13:42,399 --> 00:13:44,160
in the run full process you can run the

342
00:13:44,160 --> 00:13:46,560
entire process without any intervention

343
00:13:46,560 --> 00:13:48,800
and generate config you can generate the

344
00:13:48,800 --> 00:13:51,279
config

345
00:13:51,519 --> 00:13:54,959
by just using the web interface

346
00:13:54,959 --> 00:13:57,440
let's go to the dashboard so this is a

347
00:13:57,440 --> 00:14:01,360
default installation of kibana dashboard

348
00:14:01,360 --> 00:14:03,279
so you can go to

349
00:14:03,279 --> 00:14:05,680
the dashboard link here you'll see there

350
00:14:05,680 --> 00:14:07,920
are like five dashboard links you can

351
00:14:07,920 --> 00:14:09,920
customize as per your

352
00:14:09,920 --> 00:14:11,519
need

353
00:14:11,519 --> 00:14:13,199
here you'll give you'll get all the

354
00:14:13,199 --> 00:14:17,399
stats related to your anomalies

355
00:14:18,000 --> 00:14:19,120
next

356
00:14:19,120 --> 00:14:21,279
so how what what are the steps involved

357
00:14:21,279 --> 00:14:22,880
in building the test pipeline so

358
00:14:22,880 --> 00:14:25,040
basically there are three steps

359
00:14:25,040 --> 00:14:27,440
first you generate

360
00:14:27,440 --> 00:14:30,399
a config file by executing

361
00:14:30,399 --> 00:14:33,279
auto config script

362
00:14:33,279 --> 00:14:36,399
so here we will

363
00:14:36,399 --> 00:14:39,279
execute auto config script and it will

364
00:14:39,279 --> 00:14:42,079
have input as the data set and the

365
00:14:42,079 --> 00:14:45,360
config file that you want to generate

366
00:14:45,360 --> 00:14:48,320
so this config file is basically a set

367
00:14:48,320 --> 00:14:50,480
of configurations for

368
00:14:50,480 --> 00:14:52,720
label generator and

369
00:14:52,720 --> 00:14:54,639
the anomaly detection algorithm that you

370
00:14:54,639 --> 00:14:56,720
want to use

371
00:14:56,720 --> 00:14:59,839
it it contains all the

372
00:14:59,839 --> 00:15:00,959
uh

373
00:15:00,959 --> 00:15:03,600
label generators that the

374
00:15:03,600 --> 00:15:05,920
model will be using

375
00:15:05,920 --> 00:15:08,079
and once you execute it

376
00:15:08,079 --> 00:15:10,160
it will also contain the parameters that

377
00:15:10,160 --> 00:15:12,480
it needs to

378
00:15:12,480 --> 00:15:16,079
execute to contain to generate the model

379
00:15:16,079 --> 00:15:18,240
so after this you have to train the

380
00:15:18,240 --> 00:15:20,079
pipeline

381
00:15:20,079 --> 00:15:22,560
and train pipeline use the last config

382
00:15:22,560 --> 00:15:25,360
file that you generated

383
00:15:25,360 --> 00:15:26,880
and the last step is to just run the

384
00:15:26,880 --> 00:15:28,720
pipeline which will take as input the

385
00:15:28,720 --> 00:15:30,720
model file

386
00:15:30,720 --> 00:15:33,120
and the

387
00:15:33,199 --> 00:15:34,880
input file that

388
00:15:34,880 --> 00:15:38,839
that is our default data set

389
00:15:45,519 --> 00:15:48,560
so we'll look into it

390
00:15:48,560 --> 00:15:50,320
in depth once we start working with the

391
00:15:50,320 --> 00:15:52,800
data set

392
00:15:53,199 --> 00:15:55,199
so these endpoints are not mentioned in

393
00:15:55,199 --> 00:15:57,360
the readme but you can access it so

394
00:15:57,360 --> 00:16:00,399
these are still in work in progress but

395
00:16:00,399 --> 00:16:02,399
this will be fully built

396
00:16:02,399 --> 00:16:06,279
once we are done with it

397
00:16:10,000 --> 00:16:12,880
so now next let's let's use a default

398
00:16:12,880 --> 00:16:15,360
data set and go through the oss

399
00:16:15,360 --> 00:16:17,920
execution

400
00:16:19,920 --> 00:16:24,800
so first we will use the console link to

401
00:16:24,800 --> 00:16:27,199
go through the entire process so let's

402
00:16:27,199 --> 00:16:29,040
check whether the data set is there or

403
00:16:29,040 --> 00:16:30,240
not

404
00:16:30,240 --> 00:16:32,480
so let's see

405
00:16:32,480 --> 00:16:34,399
our folder is apps folder inside the

406
00:16:34,399 --> 00:16:37,839
default dataset is present

407
00:16:38,160 --> 00:16:39,759
and next we'll go

408
00:16:39,759 --> 00:16:41,759
we will proceed with

409
00:16:41,759 --> 00:16:44,079
getting the config file

410
00:16:44,079 --> 00:16:49,000
so we'll execute auto config

411
00:16:54,480 --> 00:16:56,800
and will pass the

412
00:16:56,800 --> 00:17:00,399
dataset as the input dataset

413
00:17:02,000 --> 00:17:05,599
and we'll give it a default config

414
00:17:05,599 --> 00:17:08,159
argument

415
00:17:15,039 --> 00:17:17,119
so once it's done

416
00:17:17,119 --> 00:17:18,799
if you want if you want to edit we can

417
00:17:18,799 --> 00:17:21,599
just go and edit it so we can see that's

418
00:17:21,599 --> 00:17:23,760
present there so we can open it in a

419
00:17:23,760 --> 00:17:26,000
text eraser and edit it as per our

420
00:17:26,000 --> 00:17:28,480
requirement

421
00:17:31,600 --> 00:17:33,679
at the bottom we can see that we are

422
00:17:33,679 --> 00:17:36,320
using statistical and gram anomaly

423
00:17:36,320 --> 00:17:38,000
detection as a scholarly scoring

424
00:17:38,000 --> 00:17:40,559
algorithm

425
00:17:44,320 --> 00:17:45,200
next

426
00:17:45,200 --> 00:17:48,240
we can go forward with

427
00:17:48,240 --> 00:17:50,160
uh training the pipeline

428
00:17:50,160 --> 00:17:51,840
by using this config file that we

429
00:17:51,840 --> 00:17:55,399
already generated

430
00:18:00,480 --> 00:18:02,240
so this will generate the model file

431
00:18:02,240 --> 00:18:03,919
that we want to

432
00:18:03,919 --> 00:18:04,960
uh

433
00:18:04,960 --> 00:18:06,640
run the pipeline on

434
00:18:06,640 --> 00:18:09,760
so once it finishes we will get the

435
00:18:09,760 --> 00:18:13,080
model file

436
00:18:16,799 --> 00:18:19,120
there is a small typo i'll just rerun it

437
00:18:19,120 --> 00:18:22,120
again

438
00:18:33,039 --> 00:18:35,280
after this we have to execute to run

439
00:18:35,280 --> 00:18:36,480
pipeline

440
00:18:36,480 --> 00:18:38,000
that will use the

441
00:18:38,000 --> 00:18:39,760
default

442
00:18:39,760 --> 00:18:42,000
model file that we just generated from

443
00:18:42,000 --> 00:18:44,880
the default configuration of oss

444
00:18:44,880 --> 00:18:46,240
so we'll pass

445
00:18:46,240 --> 00:18:49,039
the same input file

446
00:18:49,039 --> 00:18:51,360
and model file will just need to specify

447
00:18:51,360 --> 00:18:53,679
the output file that is the results file

448
00:18:53,679 --> 00:18:57,240
that we want to use

449
00:19:01,520 --> 00:19:03,039
so once we are done with that we can see

450
00:19:03,039 --> 00:19:08,120
the results in the kibana dashboard

451
00:19:13,280 --> 00:19:16,240
now we'll go to the

452
00:19:16,400 --> 00:19:18,160
dashboard link

453
00:19:18,160 --> 00:19:20,960
and we'll see the results and stats

454
00:19:20,960 --> 00:19:22,640
related to the

455
00:19:22,640 --> 00:19:25,360
execution so it's a default keyboard

456
00:19:25,360 --> 00:19:26,799
installation so the username and

457
00:19:26,799 --> 00:19:30,720
password is just admin admin

458
00:19:32,320 --> 00:19:34,320
and you can configure this front-end as

459
00:19:34,320 --> 00:19:37,639
per your requirement

460
00:19:46,799 --> 00:19:49,520
so in the dashboard we can

461
00:19:49,520 --> 00:19:51,919
see that there are like 5 000

462
00:19:51,919 --> 00:19:53,679
observations and we have used

463
00:19:53,679 --> 00:19:56,080
statistical and gram anomaly

464
00:19:56,080 --> 00:19:57,679
scoring method

465
00:19:57,679 --> 00:20:00,320
and next we can see that there we can

466
00:20:00,320 --> 00:20:02,640
see the max score and min score

467
00:20:02,640 --> 00:20:05,120
that we got for the data set

468
00:20:05,120 --> 00:20:06,400
and

469
00:20:06,400 --> 00:20:09,840
there are around like 43 unique labels

470
00:20:09,840 --> 00:20:12,880
and next in the histogram panel we can

471
00:20:12,880 --> 00:20:15,440
see the score distribution of all the

472
00:20:15,440 --> 00:20:16,880
observations

473
00:20:16,880 --> 00:20:18,320
so most of the

474
00:20:18,320 --> 00:20:20,880
observations are centered around zero to

475
00:20:20,880 --> 00:20:22,400
one thousand and then there are small

476
00:20:22,400 --> 00:20:24,799
humps

477
00:20:25,120 --> 00:20:26,000
where

478
00:20:26,000 --> 00:20:29,840
we can see like lesser number of events

479
00:20:32,640 --> 00:20:35,520
and there are other panels that we can

480
00:20:35,520 --> 00:20:36,880
explore

481
00:20:36,880 --> 00:20:39,360
we can see the rarest 10 labels that got

482
00:20:39,360 --> 00:20:43,600
generated and also top 10 labels

483
00:20:44,159 --> 00:20:46,080
at the bottom

484
00:20:46,080 --> 00:20:48,480
uh

485
00:20:48,480 --> 00:20:51,039
there is a panel for the top 10

486
00:20:51,039 --> 00:20:53,520
anomalies

487
00:20:53,520 --> 00:20:55,919
it shows the tag sets and the scores

488
00:20:55,919 --> 00:20:58,400
that it

489
00:21:00,840 --> 00:21:02,960
generated so

490
00:21:02,960 --> 00:21:04,799
labels or tags are specifically

491
00:21:04,799 --> 00:21:06,159
important

492
00:21:06,159 --> 00:21:08,960
if you want to see why that

493
00:21:08,960 --> 00:21:10,880
event is anomalous

494
00:21:10,880 --> 00:21:13,440
so next we go to the discover

495
00:21:13,440 --> 00:21:17,200
link where we can see individual events

496
00:21:17,200 --> 00:21:19,520
which were characterized as anomalous

497
00:21:19,520 --> 00:21:22,320
here we can add a filter so say we want

498
00:21:22,320 --> 00:21:25,280
to see all the events that generated

499
00:21:25,280 --> 00:21:28,159
score greater than 1000

500
00:21:28,159 --> 00:21:29,919
so we can just

501
00:21:29,919 --> 00:21:32,960
create a filter for that

502
00:21:32,960 --> 00:21:36,480
and we want to select to score

503
00:21:36,480 --> 00:21:39,520
and command

504
00:21:42,840 --> 00:21:46,799
fields and next we'll sort the

505
00:21:46,799 --> 00:21:49,280
score by descending order

506
00:21:49,280 --> 00:21:53,360
so that we get the max score at the top

507
00:21:53,919 --> 00:21:55,360
so these are the

508
00:21:55,360 --> 00:21:58,480
most anomalous events or commands that

509
00:21:58,480 --> 00:22:00,480
are seen in the data set

510
00:22:00,480 --> 00:22:02,559
and we can go through and analyze them

511
00:22:02,559 --> 00:22:04,480
so this was the default configuration

512
00:22:04,480 --> 00:22:07,039
and we can see that there are not many

513
00:22:07,039 --> 00:22:09,440
anomalies which might not be malicious

514
00:22:09,440 --> 00:22:11,440
in nature specifically

515
00:22:11,440 --> 00:22:13,120
so we can go

516
00:22:13,120 --> 00:22:15,760
investigate it

517
00:22:15,760 --> 00:22:17,760
next we'll look into how to enhance the

518
00:22:17,760 --> 00:22:18,960
process

519
00:22:18,960 --> 00:22:21,360
and how we can improve the detection

520
00:22:21,360 --> 00:22:23,280
process

521
00:22:23,280 --> 00:22:24,960
so in this

522
00:22:24,960 --> 00:22:27,840
anomaly set

523
00:22:28,400 --> 00:22:31,039
we'll see a netcat

524
00:22:31,039 --> 00:22:33,520
command

525
00:22:35,440 --> 00:22:37,600
so the netcat command that we are seeing

526
00:22:37,600 --> 00:22:38,559
here

527
00:22:38,559 --> 00:22:40,080
so this network command is the command

528
00:22:40,080 --> 00:22:43,120
that we have inserted in the data set

529
00:22:43,120 --> 00:22:44,559
and

530
00:22:44,559 --> 00:22:47,360
it looks like it is pretty it has a it

531
00:22:47,360 --> 00:22:49,520
is anomalous but it lies pretty low in

532
00:22:49,520 --> 00:22:51,120
that score

533
00:22:51,120 --> 00:22:52,720
so we'll see how we can improve the

534
00:22:52,720 --> 00:22:56,480
detection process in the next step

535
00:22:58,559 --> 00:23:00,880
so next we will add some expert

536
00:23:00,880 --> 00:23:03,600
knowledge which is security based

537
00:23:03,600 --> 00:23:06,559
and we'll improve our labeling by using

538
00:23:06,559 --> 00:23:07,760
that to

539
00:23:07,760 --> 00:23:10,480
knowledge base

540
00:23:14,480 --> 00:23:16,960
so we can run the entire process using

541
00:23:16,960 --> 00:23:18,320
the console

542
00:23:18,320 --> 00:23:20,720
or we can just use the web interface

543
00:23:20,720 --> 00:23:23,440
let's go through the web interface here

544
00:23:23,440 --> 00:23:25,280
so here first of all we can just

545
00:23:25,280 --> 00:23:26,400
generate

546
00:23:26,400 --> 00:23:28,159
default configuration just like we did

547
00:23:28,159 --> 00:23:30,320
last time

548
00:23:30,320 --> 00:23:31,919
well submitted

549
00:23:31,919 --> 00:23:33,760
once we submitted we will be presented

550
00:23:33,760 --> 00:23:36,240
by the default configuration we can then

551
00:23:36,240 --> 00:23:37,919
go again you can confirm the

552
00:23:37,919 --> 00:23:39,120
configuration

553
00:23:39,120 --> 00:23:41,760
here we'll get an option to change it

554
00:23:41,760 --> 00:23:43,360
once we submit

555
00:23:43,360 --> 00:23:45,120
so here we are

556
00:23:45,120 --> 00:23:47,679
getting the option to change

557
00:23:47,679 --> 00:23:50,640
the config as per requirement so we have

558
00:23:50,640 --> 00:23:52,960
used statical and gram anomaly and we

559
00:23:52,960 --> 00:23:55,919
have other four that we can choose from

560
00:23:55,919 --> 00:23:58,799
we will choose least outlier factor here

561
00:23:58,799 --> 00:24:00,799
sorry local outlier factor

562
00:24:00,799 --> 00:24:01,760
and

563
00:24:01,760 --> 00:24:03,600
then we'll

564
00:24:03,600 --> 00:24:06,000
remove some of the label generators

565
00:24:06,000 --> 00:24:08,720
as we don't feel that those fields are

566
00:24:08,720 --> 00:24:10,080
adding any

567
00:24:10,080 --> 00:24:14,399
of value to the analog detection process

568
00:24:15,120 --> 00:24:17,679
so yeah we are just removing some of the

569
00:24:17,679 --> 00:24:20,559
label generators

570
00:24:20,880 --> 00:24:22,880
and the fields that can just be random

571
00:24:22,880 --> 00:24:25,120
and it's not useful

572
00:24:25,120 --> 00:24:28,320
for score generation

573
00:24:28,320 --> 00:24:31,600
so next we will look into how we can add

574
00:24:31,600 --> 00:24:33,840
the

575
00:24:34,720 --> 00:24:37,919
knowledge based labeling

576
00:24:37,919 --> 00:24:39,360
so

577
00:24:39,360 --> 00:24:42,880
we'll use generator type keyword based

578
00:24:42,880 --> 00:24:45,840
and we want to add our knowledge based

579
00:24:45,840 --> 00:24:47,919
keyword for commands so we'll change the

580
00:24:47,919 --> 00:24:49,679
field name to command

581
00:24:49,679 --> 00:24:52,559
and we need to change the keyword list

582
00:24:52,559 --> 00:24:53,919
that

583
00:24:53,919 --> 00:24:56,640
that we have selected

584
00:24:56,640 --> 00:24:58,720
so i'll just copy paste here

585
00:24:58,720 --> 00:25:02,559
and this this is the list of uh

586
00:25:02,559 --> 00:25:05,440
command keywords that we come up with

587
00:25:05,440 --> 00:25:07,120
by using our

588
00:25:07,120 --> 00:25:08,640
see by

589
00:25:08,640 --> 00:25:11,200
by doing our security research

590
00:25:11,200 --> 00:25:14,240
so we can use

591
00:25:15,039 --> 00:25:17,520
we can just add keywords or we can add

592
00:25:17,520 --> 00:25:20,159
regis based keyword that you see in the

593
00:25:20,159 --> 00:25:22,240
next example so this is example where we

594
00:25:22,240 --> 00:25:23,840
are just using

595
00:25:23,840 --> 00:25:27,120
regis based labeling it can be ip

596
00:25:27,120 --> 00:25:30,720
address or it can be paths

597
00:25:30,720 --> 00:25:32,720
so

598
00:25:32,720 --> 00:25:33,679
yeah

599
00:25:33,679 --> 00:25:35,279
so once we're done with it we'll just

600
00:25:35,279 --> 00:25:37,760
save it

601
00:25:39,360 --> 00:25:42,480
and we can see the file name is named as

602
00:25:42,480 --> 00:25:44,960
a tailored underscore the previous file

603
00:25:44,960 --> 00:25:47,200
name

604
00:25:47,679 --> 00:25:49,520
next we'll go to console and then we'll

605
00:25:49,520 --> 00:25:51,440
repeat the whole process first we'll

606
00:25:51,440 --> 00:25:53,600
check the config file that we just

607
00:25:53,600 --> 00:25:56,320
generated that is tailored underscore on

608
00:25:56,320 --> 00:25:57,840
the previous file name

609
00:25:57,840 --> 00:25:59,840
next we'll

610
00:25:59,840 --> 00:26:01,679
train the pipeline using the new config

611
00:26:01,679 --> 00:26:04,720
file we will supply the same input file

612
00:26:04,720 --> 00:26:06,960
which we have been using

613
00:26:06,960 --> 00:26:09,960
and

614
00:26:13,919 --> 00:26:15,440
we'll provide the

615
00:26:15,440 --> 00:26:17,760
new config file

616
00:26:17,760 --> 00:26:21,720
and we'll just let it execute

617
00:26:26,000 --> 00:26:28,400
yeah we also need to specify the model

618
00:26:28,400 --> 00:26:29,919
file that we

619
00:26:29,919 --> 00:26:30,720
uh

620
00:26:30,720 --> 00:26:32,640
gonna generate from the string pipeline

621
00:26:32,640 --> 00:26:35,640
process

622
00:26:40,400 --> 00:26:42,080
so it will generate the model file which

623
00:26:42,080 --> 00:26:44,400
we will use to run the pipeline just

624
00:26:44,400 --> 00:26:46,799
like the previous process

625
00:26:46,799 --> 00:26:51,879
so once we have the model file generated

626
00:26:52,640 --> 00:26:55,520
so it's a good idea to have a model file

627
00:26:55,520 --> 00:26:57,520
which is named differently than the

628
00:26:57,520 --> 00:26:59,760
previous step so that you can repeat the

629
00:26:59,760 --> 00:27:01,039
whole process

630
00:27:01,039 --> 00:27:02,960
as many times as you want

631
00:27:02,960 --> 00:27:07,000
next we'll run the pipeline

632
00:27:07,440 --> 00:27:10,640
using the same input file

633
00:27:12,159 --> 00:27:13,600
you can change the input file to

634
00:27:13,600 --> 00:27:16,720
completely new file to test it

635
00:27:16,720 --> 00:27:18,399
with respect to the model file that we

636
00:27:18,399 --> 00:27:21,039
have generated in the step

637
00:27:21,039 --> 00:27:22,720
or you can just use the same input file

638
00:27:22,720 --> 00:27:24,399
depends on

639
00:27:24,399 --> 00:27:27,120
your requirement

640
00:27:28,000 --> 00:27:29,840
so once this is done we'll again go to

641
00:27:29,840 --> 00:27:31,360
the dashboard and

642
00:27:31,360 --> 00:27:34,918
check out the results

643
00:27:41,120 --> 00:27:42,880
so again i'll copy the

644
00:27:42,880 --> 00:27:44,640
dashboard link

645
00:27:44,640 --> 00:27:46,960
and in the banner front end we'll go and

646
00:27:46,960 --> 00:27:49,760
observe

647
00:27:49,760 --> 00:27:51,760
does it cause any significant changes in

648
00:27:51,760 --> 00:27:54,399
the anomalies

649
00:27:56,960 --> 00:28:00,159
so this time we have used local outlier

650
00:28:00,159 --> 00:28:01,520
factor

651
00:28:01,520 --> 00:28:03,919
anomaly scoring method

652
00:28:03,919 --> 00:28:05,120
and

653
00:28:05,120 --> 00:28:07,600
we'll see the score range has changed a

654
00:28:07,600 --> 00:28:10,240
lot and we have generated around 125

655
00:28:10,240 --> 00:28:12,159
unique labels that is three times last

656
00:28:12,159 --> 00:28:14,399
time

657
00:28:14,640 --> 00:28:17,039
and the score range is also

658
00:28:17,039 --> 00:28:19,679
very huge

659
00:28:23,919 --> 00:28:26,240
and if you see the histogram

660
00:28:26,240 --> 00:28:27,679
score distribution we see that most of

661
00:28:27,679 --> 00:28:30,960
the scores are located near zero and

662
00:28:30,960 --> 00:28:33,679
after that there are very few

663
00:28:33,679 --> 00:28:36,159
events that have very high score so it

664
00:28:36,159 --> 00:28:38,880
looks like it did very

665
00:28:38,880 --> 00:28:41,840
good clustering of the anomalies and we

666
00:28:41,840 --> 00:28:43,440
are not

667
00:28:43,440 --> 00:28:45,039
getting anomalies that have like very

668
00:28:45,039 --> 00:28:47,600
high score

669
00:28:50,000 --> 00:28:52,320
next we can look at look at the real

670
00:28:52,320 --> 00:28:54,720
state labels so these labels got

671
00:28:54,720 --> 00:28:57,039
generated

672
00:28:57,039 --> 00:29:00,240
because of the keyword based labels that

673
00:29:00,240 --> 00:29:01,919
keyword based

674
00:29:01,919 --> 00:29:04,000
label generated we have used so you can

675
00:29:04,000 --> 00:29:04,799
see

676
00:29:04,799 --> 00:29:06,480
they are named command underscore

677
00:29:06,480 --> 00:29:07,600
keyword

678
00:29:07,600 --> 00:29:09,679
uh and there is a command

679
00:29:09,679 --> 00:29:12,399
which we have put as a

680
00:29:12,399 --> 00:29:14,720
as a label generated keyword

681
00:29:14,720 --> 00:29:18,720
you can also see top 10 labels here

682
00:29:18,960 --> 00:29:21,240
and same as last time

683
00:29:21,240 --> 00:29:22,480
[Music]

684
00:29:22,480 --> 00:29:25,200
you can look at the top score anomalies

685
00:29:25,200 --> 00:29:29,039
labels and scores generated by them

686
00:29:29,039 --> 00:29:33,360
we'll again go to the discover link

687
00:29:34,559 --> 00:29:35,919
and

688
00:29:35,919 --> 00:29:37,279
check out the

689
00:29:37,279 --> 00:29:40,840
this course again

690
00:29:41,120 --> 00:29:43,120
so we'll use the same filter and then

691
00:29:43,120 --> 00:29:44,640
we'll choose

692
00:29:44,640 --> 00:29:46,399
score and command

693
00:29:46,399 --> 00:29:49,520
and labels

694
00:29:52,159 --> 00:29:54,640
if we sort the score

695
00:29:54,640 --> 00:29:57,360
to descending order

696
00:29:57,360 --> 00:29:59,840
we'll see

697
00:30:00,799 --> 00:30:02,559
that the netcat command that we have

698
00:30:02,559 --> 00:30:04,960
observed in the last iteration it is

699
00:30:04,960 --> 00:30:06,559
appearing

700
00:30:06,559 --> 00:30:08,559
to have the highest score and then there

701
00:30:08,559 --> 00:30:10,320
are other

702
00:30:10,320 --> 00:30:12,480
uh commands

703
00:30:12,480 --> 00:30:14,480
that we have inserted that also have a

704
00:30:14,480 --> 00:30:16,799
higher score

705
00:30:16,799 --> 00:30:20,960
so they appear to have the so these

706
00:30:20,960 --> 00:30:23,520
are the inserted malicious commands that

707
00:30:23,520 --> 00:30:24,559
uh

708
00:30:24,559 --> 00:30:27,120
that were injected in the data set

709
00:30:27,120 --> 00:30:30,320
and it's very very very relevant to the

710
00:30:30,320 --> 00:30:32,399
security investigation that someone

711
00:30:32,399 --> 00:30:34,080
might be doing

712
00:30:34,080 --> 00:30:36,799
so

713
00:30:36,799 --> 00:30:40,320
we can use oss in ids logs or service

714
00:30:40,320 --> 00:30:43,918
logs it's it has a very

715
00:30:44,399 --> 00:30:47,719
open implementation

716
00:30:55,679 --> 00:30:58,159
so as we have seen that oss is very easy

717
00:30:58,159 --> 00:31:00,640
to deploy given its integration with

718
00:31:00,640 --> 00:31:03,279
kibana and docker it's very easy to use

719
00:31:03,279 --> 00:31:04,880
and you can repeat the whole

720
00:31:04,880 --> 00:31:07,279
experimentation process many times by

721
00:31:07,279 --> 00:31:10,240
changing the configuration file

722
00:31:10,240 --> 00:31:12,320
and adding expert knowledge as per your

723
00:31:12,320 --> 00:31:13,679
requirement

724
00:31:13,679 --> 00:31:16,080
so in general as

725
00:31:16,080 --> 00:31:19,039
as with any ml approach you have to test

726
00:31:19,039 --> 00:31:21,200
the features which will add real value

727
00:31:21,200 --> 00:31:23,120
to your ml process

728
00:31:23,120 --> 00:31:25,679
and you have to

729
00:31:25,679 --> 00:31:28,000
test out all the ml all the anomaly

730
00:31:28,000 --> 00:31:30,159
algorithms as per your requirements and

731
00:31:30,159 --> 00:31:33,840
see which will give you better results

732
00:31:34,720 --> 00:31:38,240
next we'll go forward with the questions

733
00:31:38,240 --> 00:31:40,559
so this is the github link where you can

734
00:31:40,559 --> 00:31:43,440
go and fork the project

735
00:31:43,440 --> 00:31:45,519
please feel free to try it

736
00:31:45,519 --> 00:31:47,840
and if you like the project please start

737
00:31:47,840 --> 00:31:49,760
the project also you can get more

738
00:31:49,760 --> 00:31:51,440
details about the project in this medium

739
00:31:51,440 --> 00:31:53,679
link

740
00:31:53,840 --> 00:31:55,840
there are other adobe related resources

741
00:31:55,840 --> 00:31:57,919
where you can go and check out adobe

742
00:31:57,919 --> 00:32:00,919
projects

743
00:32:01,600 --> 00:32:03,840
thank you hope this was informative to

744
00:32:03,840 --> 00:32:05,360
you and please

745
00:32:05,360 --> 00:32:07,039
feel free to

746
00:32:07,039 --> 00:32:09,440
reach if you have any questions

747
00:32:09,440 --> 00:32:12,679
thank you

