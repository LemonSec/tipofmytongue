1
00:00:00,399 --> 00:00:02,639
hi everyone my name is schreiter i work

2
00:00:02,639 --> 00:00:05,680
as a consultant and work predominantly

3
00:00:05,680 --> 00:00:07,440
with clients helping them deal with

4
00:00:07,440 --> 00:00:09,599
breaches that have occurred in their aws

5
00:00:09,599 --> 00:00:12,400
environment today in this presentation

6
00:00:12,400 --> 00:00:13,840
i'm going to be sharing some of the

7
00:00:13,840 --> 00:00:16,160
lessons i've learned as part of during

8
00:00:16,160 --> 00:00:18,320
aws incident response for different

9
00:00:18,320 --> 00:00:20,720
clients in particular i'm going to be

10
00:00:20,720 --> 00:00:23,279
focusing on some of the learnings i've

11
00:00:23,279 --> 00:00:25,760
gathered as part of doing data wrangling

12
00:00:25,760 --> 00:00:28,000
in aws

13
00:00:28,000 --> 00:00:30,560
right a quick walkthrough through the

14
00:00:30,560 --> 00:00:33,200
agenda initially i'm going to focus on

15
00:00:33,200 --> 00:00:35,280
some of the challenges associated with

16
00:00:35,280 --> 00:00:38,559
doing incident response in aws but not

17
00:00:38,559 --> 00:00:41,040
focused on why

18
00:00:41,040 --> 00:00:43,200
it's different from traditional incident

19
00:00:43,200 --> 00:00:45,360
response or how to look at logs that's

20
00:00:45,360 --> 00:00:47,840
not the focus of this talk

21
00:00:47,840 --> 00:00:50,320
instead i'm going to highlight some of

22
00:00:50,320 --> 00:00:53,039
the challenges in terms of once you have

23
00:00:53,039 --> 00:00:55,039
all the logs that you need to conduct

24
00:00:55,039 --> 00:00:57,440
incident response some of the challenges

25
00:00:57,440 --> 00:01:00,480
that you might face as a professional

26
00:01:00,480 --> 00:01:04,239
and how to use native aws services to

27
00:01:04,239 --> 00:01:06,320
wrangle some of this data

28
00:01:06,320 --> 00:01:07,760
towards the second part of the

29
00:01:07,760 --> 00:01:10,000
presentation i'm going to switch gears a

30
00:01:10,000 --> 00:01:12,240
little bit and do a demo of some of

31
00:01:12,240 --> 00:01:14,320
these services and

32
00:01:14,320 --> 00:01:17,200
show you how to convert data etc and

33
00:01:17,200 --> 00:01:20,159
then run through some metrics uh using

34
00:01:20,159 --> 00:01:23,119
athena queries and compare how much time

35
00:01:23,119 --> 00:01:25,119
and how much data is canned

36
00:01:25,119 --> 00:01:27,680
if you run the queries against sort of

37
00:01:27,680 --> 00:01:30,079
raw cloud trail drugs versus running the

38
00:01:30,079 --> 00:01:32,000
same queries against a different data

39
00:01:32,000 --> 00:01:34,640
format we can talk about that

40
00:01:34,640 --> 00:01:36,720
and finally i'm going to wrap up by

41
00:01:36,720 --> 00:01:39,600
again showing you another aws service to

42
00:01:39,600 --> 00:01:41,840
visualize all these logs

43
00:01:41,840 --> 00:01:44,320
and the reason for sort of sticking to

44
00:01:44,320 --> 00:01:47,200
native aws services is sort of two-fold

45
00:01:47,200 --> 00:01:49,200
one is to prevent

46
00:01:49,200 --> 00:01:52,320
us from having to move data around but

47
00:01:52,320 --> 00:01:54,240
also the second thing is some of these

48
00:01:54,240 --> 00:01:57,520
aws services work nicely together

49
00:01:57,520 --> 00:01:59,840
so it makes our life a bit easier or at

50
00:01:59,840 --> 00:02:01,840
least just definitely make mine a bit

51
00:02:01,840 --> 00:02:03,280
easier

52
00:02:03,280 --> 00:02:05,920
right um let's get started before i get

53
00:02:05,920 --> 00:02:07,040
started

54
00:02:07,040 --> 00:02:09,199
i just wanted to ask a question how many

55
00:02:09,199 --> 00:02:10,959
things do you think

56
00:02:10,959 --> 00:02:13,360
are needed basically to improve incident

57
00:02:13,360 --> 00:02:15,520
response in aws

58
00:02:15,520 --> 00:02:18,160
and the reason i ask this question is i

59
00:02:18,160 --> 00:02:19,840
think it means different things to

60
00:02:19,840 --> 00:02:21,680
different people whenever i ask this

61
00:02:21,680 --> 00:02:24,400
question i get different responses

62
00:02:24,400 --> 00:02:26,480
and it kind of varies from do we have

63
00:02:26,480 --> 00:02:29,200
the right logs do i have access to the

64
00:02:29,200 --> 00:02:32,080
logs in an easy to consume fashion

65
00:02:32,080 --> 00:02:35,680
are all these logs in a sim solution

66
00:02:35,680 --> 00:02:38,239
can i set up detection logic

67
00:02:38,239 --> 00:02:40,080
these are sort of the questions that the

68
00:02:40,080 --> 00:02:42,400
security team typically ask

69
00:02:42,400 --> 00:02:44,640
but then the operations team might be

70
00:02:44,640 --> 00:02:46,959
asking questions around

71
00:02:46,959 --> 00:02:48,720
things like how much does it actually

72
00:02:48,720 --> 00:02:50,239
cost

73
00:02:50,239 --> 00:02:52,560
or for regulatory reasons how long do we

74
00:02:52,560 --> 00:02:56,000
need to store these logs for etc so the

75
00:02:56,000 --> 00:02:58,640
focus can be different depending on who

76
00:02:58,640 --> 00:03:01,200
you're speaking to and sort of where

77
00:03:01,200 --> 00:03:03,040
you're in the journey in terms of doing

78
00:03:03,040 --> 00:03:06,480
incident response in aws

79
00:03:06,480 --> 00:03:09,120
well my learning has been that i just

80
00:03:09,120 --> 00:03:11,760
needed one particular thing to improve

81
00:03:11,760 --> 00:03:14,080
my incident response skill set and that

82
00:03:14,080 --> 00:03:16,319
was data wrangling

83
00:03:16,319 --> 00:03:18,959
okay i am exaggerating there are a lot

84
00:03:18,959 --> 00:03:22,080
of caveats associated with this um it's

85
00:03:22,080 --> 00:03:24,879
not enough uh if you can just wrangle

86
00:03:24,879 --> 00:03:27,360
your data but the reason i'm

87
00:03:27,360 --> 00:03:29,599
highlighting this and i'll come to the

88
00:03:29,599 --> 00:03:31,599
caveat in a bit is

89
00:03:31,599 --> 00:03:34,239
let's say you've attained nirvana with

90
00:03:34,239 --> 00:03:36,480
aws logging you've got everything

91
00:03:36,480 --> 00:03:39,120
everyone's on the same page

92
00:03:39,120 --> 00:03:42,239
you can get access to logs easily

93
00:03:42,239 --> 00:03:43,519
now what

94
00:03:43,519 --> 00:03:46,080
the next issue or the problem that i

95
00:03:46,080 --> 00:03:47,280
faced is

96
00:03:47,280 --> 00:03:49,840
oh i have terabytes of data or maybe

97
00:03:49,840 --> 00:03:53,760
petabytes of data how do i analyze this

98
00:03:53,760 --> 00:03:55,439
quickly enough

99
00:03:55,439 --> 00:03:58,319
how do i analyze this without incurring

100
00:03:58,319 --> 00:04:00,640
a lot of costs especially as a

101
00:04:00,640 --> 00:04:02,959
consultant i'm probably not using the

102
00:04:02,959 --> 00:04:04,720
client environment for doing this so

103
00:04:04,720 --> 00:04:07,920
that's always a consideration but mostly

104
00:04:07,920 --> 00:04:10,799
if i run a query against this data i

105
00:04:10,799 --> 00:04:12,640
want to be able to get my responses

106
00:04:12,640 --> 00:04:15,519
faster i want to reduce the amount of

107
00:04:15,519 --> 00:04:17,358
data that's being scanned

108
00:04:17,358 --> 00:04:19,040
and that's when i realized that i'm not

109
00:04:19,040 --> 00:04:21,120
doing this efficiently because my clays

110
00:04:21,120 --> 00:04:24,000
take too long to run or i am scanning

111
00:04:24,000 --> 00:04:26,000
too much data whereas what i'm

112
00:04:26,000 --> 00:04:28,479
interested in is only a couple of fields

113
00:04:28,479 --> 00:04:31,360
within a particular log source

114
00:04:31,360 --> 00:04:34,479
that's why i highlighted data wrangling

115
00:04:34,479 --> 00:04:35,360
so

116
00:04:35,360 --> 00:04:39,440
we can now talk about all the caveats

117
00:04:39,440 --> 00:04:42,000
i have put some of the aws services on

118
00:04:42,000 --> 00:04:44,240
this slide sort of to give a bit of

119
00:04:44,240 --> 00:04:46,160
guidance on things that we might be

120
00:04:46,160 --> 00:04:49,680
looking at in terms of logs

121
00:04:49,680 --> 00:04:51,680
when i say caveats you know it's

122
00:04:51,680 --> 00:04:54,639
typically always about logs

123
00:04:54,639 --> 00:04:57,520
i have highlighted a few questions and

124
00:04:57,520 --> 00:04:58,880
uh

125
00:04:58,880 --> 00:05:00,479
i'm going to go through each of these

126
00:05:00,479 --> 00:05:03,199
the first thing is do you have logs for

127
00:05:03,199 --> 00:05:06,240
incident response and it's as simple as

128
00:05:06,240 --> 00:05:08,479
if you don't have the right logs or if

129
00:05:08,479 --> 00:05:10,479
you don't have logs you can't do

130
00:05:10,479 --> 00:05:12,479
incident response it's as simple as that

131
00:05:12,479 --> 00:05:14,400
you can't answer your clients you can't

132
00:05:14,400 --> 00:05:16,479
answer your customers you don't know

133
00:05:16,479 --> 00:05:18,320
what the threat actor did

134
00:05:18,320 --> 00:05:19,440
so

135
00:05:19,440 --> 00:05:22,080
you know that's always a consideration

136
00:05:22,080 --> 00:05:25,280
but there is a cost associated with it

137
00:05:25,280 --> 00:05:27,600
as a security team we obviously want

138
00:05:27,600 --> 00:05:30,160
access to everything just in case this

139
00:05:30,160 --> 00:05:32,720
obscure incident happens i need

140
00:05:32,720 --> 00:05:36,160
a particular log set um which i only use

141
00:05:36,160 --> 00:05:38,080
once in six months a year or whatever

142
00:05:38,080 --> 00:05:40,320
but i want to have everything

143
00:05:40,320 --> 00:05:42,000
i want to switch that a little bit and

144
00:05:42,000 --> 00:05:44,160
talk about you know have i actually

145
00:05:44,160 --> 00:05:47,520
enabled key log and for aws

146
00:05:47,520 --> 00:05:50,840
the most important log source from an ir

147
00:05:50,840 --> 00:05:54,240
perspective is cloudtrail

148
00:05:54,240 --> 00:05:56,479
and with cloudtrail i mean not just the

149
00:05:56,479 --> 00:05:58,479
management events but the data events as

150
00:05:58,479 --> 00:06:00,240
well and then we can start thinking

151
00:06:00,240 --> 00:06:02,639
about things like do we have vpc flow

152
00:06:02,639 --> 00:06:05,680
logs so i get an idea about the network

153
00:06:05,680 --> 00:06:07,120
flows

154
00:06:07,120 --> 00:06:10,400
if i'm using s3 do i have s3 access logs

155
00:06:10,400 --> 00:06:11,919
enabled

156
00:06:11,919 --> 00:06:13,680
if i'm using

157
00:06:13,680 --> 00:06:16,560
aws config to keep track of changes

158
00:06:16,560 --> 00:06:19,520
between my different assets am i using

159
00:06:19,520 --> 00:06:22,240
cloudfront am i using elb

160
00:06:22,240 --> 00:06:24,160
and again for all of these do i have

161
00:06:24,160 --> 00:06:25,440
logs

162
00:06:25,440 --> 00:06:27,360
it's always important to bear in mind

163
00:06:27,360 --> 00:06:29,360
that there is a cost associated with it

164
00:06:29,360 --> 00:06:31,840
and to be thinking of what are critical

165
00:06:31,840 --> 00:06:34,960
logs that are absolutely required

166
00:06:34,960 --> 00:06:38,000
in my case i always say it depends but

167
00:06:38,000 --> 00:06:40,240
you know bare minimum have cloud trails

168
00:06:40,240 --> 00:06:43,039
management and data events

169
00:06:43,039 --> 00:06:45,520
the second part of this is right we have

170
00:06:45,520 --> 00:06:47,840
all these logs but you know how do we

171
00:06:47,840 --> 00:06:50,800
actually store these because if you send

172
00:06:50,800 --> 00:06:53,120
everything to the sim the costs are high

173
00:06:53,120 --> 00:06:55,680
because often licenses for

174
00:06:55,680 --> 00:06:58,240
sim are linked to the storage or data

175
00:06:58,240 --> 00:06:59,520
ingestion

176
00:06:59,520 --> 00:07:00,639
um

177
00:07:00,639 --> 00:07:04,319
within aws s3 if you're using that as

178
00:07:04,319 --> 00:07:06,479
the store for all your logs it provides

179
00:07:06,479 --> 00:07:09,599
a lot of capability that allows us to

180
00:07:09,599 --> 00:07:12,000
use let's say different tiers of storage

181
00:07:12,000 --> 00:07:15,360
depending on the criticality of the logs

182
00:07:15,360 --> 00:07:18,000
i can use lifecycle so that i don't have

183
00:07:18,000 --> 00:07:20,319
to manually manage all of those and

184
00:07:20,319 --> 00:07:22,960
let's say that you know i just want logs

185
00:07:22,960 --> 00:07:25,680
for three months afterwards it's very

186
00:07:25,680 --> 00:07:27,599
rare for me to go and look for these

187
00:07:27,599 --> 00:07:29,599
logs so they can go into

188
00:07:29,599 --> 00:07:32,240
a different tier within frequent access

189
00:07:32,240 --> 00:07:34,000
and maybe after a year

190
00:07:34,000 --> 00:07:36,880
i'm only going to keep them for the ad

191
00:07:36,880 --> 00:07:39,199
hoc situation where you know there's a

192
00:07:39,199 --> 00:07:41,120
regulatory question

193
00:07:41,120 --> 00:07:43,360
or there's this obscure incident that

194
00:07:43,360 --> 00:07:45,120
i've been talking about so i need to go

195
00:07:45,120 --> 00:07:47,360
fetch it for those

196
00:07:47,360 --> 00:07:50,879
so i can use native s3 services again to

197
00:07:50,879 --> 00:07:53,039
manage all of these and keep my costs

198
00:07:53,039 --> 00:07:54,560
low

199
00:07:54,560 --> 00:07:57,280
without incurring too much cost

200
00:07:57,280 --> 00:08:00,080
but the other part of storage itself is

201
00:08:00,080 --> 00:08:02,240
for example if i take cloudtrail i'll

202
00:08:02,240 --> 00:08:05,199
use cloudtrail as an example quite a bit

203
00:08:05,199 --> 00:08:07,520
if i take cloudtrail as an example

204
00:08:07,520 --> 00:08:09,360
everything's in json

205
00:08:09,360 --> 00:08:11,120
compressed obviously

206
00:08:11,120 --> 00:08:14,560
and stored in s3 however when i query

207
00:08:14,560 --> 00:08:18,479
these logs i don't really do

208
00:08:18,479 --> 00:08:20,720
a query that needs to go through each

209
00:08:20,720 --> 00:08:23,120
individual row i'm interested in

210
00:08:23,120 --> 00:08:26,160
particular fields within the table let's

211
00:08:26,160 --> 00:08:28,960
say a particular ip address

212
00:08:28,960 --> 00:08:32,080
or a particular event type etc

213
00:08:32,080 --> 00:08:35,360
it raises questions around whether

214
00:08:35,360 --> 00:08:38,479
json or robay storage is actually the

215
00:08:38,479 --> 00:08:41,279
best way to store cloudtrail

216
00:08:41,279 --> 00:08:44,399
or purely from an efficiency perspective

217
00:08:44,399 --> 00:08:46,880
if something like parquet or a columnar

218
00:08:46,880 --> 00:08:49,600
database would be sorry columnar data

219
00:08:49,600 --> 00:08:51,440
format would be more

220
00:08:51,440 --> 00:08:54,560
suitable for our use cases and in this

221
00:08:54,560 --> 00:08:56,480
presentation i'm going to be focused on

222
00:08:56,480 --> 00:08:58,880
columnar data formats and how it's

223
00:08:58,880 --> 00:09:02,080
improved query efficiency for me

224
00:09:02,080 --> 00:09:04,320
at a very high level

225
00:09:04,320 --> 00:09:06,399
i'm not going to go into a lot of data

226
00:09:06,399 --> 00:09:08,959
science or analytics

227
00:09:08,959 --> 00:09:11,680
columnar data storage is more efficient

228
00:09:11,680 --> 00:09:13,440
just because

229
00:09:13,440 --> 00:09:15,760
we are reading data per column as

230
00:09:15,760 --> 00:09:18,320
opposed to each row and there are

231
00:09:18,320 --> 00:09:20,800
efficiencies to be gained from that

232
00:09:20,800 --> 00:09:23,920
and the last question is you know

233
00:09:23,920 --> 00:09:26,880
senior management wants a dashboard or

234
00:09:26,880 --> 00:09:30,000
even i want to be able to highlight

235
00:09:30,000 --> 00:09:32,800
anomalies or see what's happening

236
00:09:32,800 --> 00:09:34,560
and be able to visualize some of these

237
00:09:34,560 --> 00:09:36,480
logs easily

238
00:09:36,480 --> 00:09:39,519
so do i really need to transfer logs off

239
00:09:39,519 --> 00:09:42,640
platform by a platform i mean aws to

240
00:09:42,640 --> 00:09:45,360
create these dashboards

241
00:09:45,360 --> 00:09:48,480
or can i keep my logs effectively just

242
00:09:48,480 --> 00:09:52,080
in s3 and query them using athena and

243
00:09:52,080 --> 00:09:52,959
then

244
00:09:52,959 --> 00:09:55,519
create my visualizations using some

245
00:09:55,519 --> 00:09:57,600
other aws service

246
00:09:57,600 --> 00:09:59,760
so for this use case i'm going to be

247
00:09:59,760 --> 00:10:01,760
talking a little bit about quicksite and

248
00:10:01,760 --> 00:10:03,680
that's what i've used

249
00:10:03,680 --> 00:10:06,640
and it's pretty neat because it doesn't

250
00:10:06,640 --> 00:10:09,760
really need me to create multiple copies

251
00:10:09,760 --> 00:10:11,600
of the same data

252
00:10:11,600 --> 00:10:14,000
the fewer copies we have the easier they

253
00:10:14,000 --> 00:10:16,000
are to maintain obviously from an

254
00:10:16,000 --> 00:10:18,000
operational perspective but also there

255
00:10:18,000 --> 00:10:20,320
is less storage cost associated with

256
00:10:20,320 --> 00:10:21,519
that

257
00:10:21,519 --> 00:10:24,000
so these are the caveats to sort of bear

258
00:10:24,000 --> 00:10:25,360
in mind

259
00:10:25,360 --> 00:10:27,360
in terms of how we want to wrangle the

260
00:10:27,360 --> 00:10:30,399
data a do we have the right log

261
00:10:30,399 --> 00:10:32,640
b how do i store them so we'll talk a

262
00:10:32,640 --> 00:10:35,279
little bit about columnar format and see

263
00:10:35,279 --> 00:10:37,600
just how do i visualize the logs without

264
00:10:37,600 --> 00:10:39,440
having to create another copy of the

265
00:10:39,440 --> 00:10:42,079
data set

266
00:10:42,640 --> 00:10:44,079
all right

267
00:10:44,079 --> 00:10:46,959
if i switch and focus on data wrangling

268
00:10:46,959 --> 00:10:50,160
purely i've created a

269
00:10:50,160 --> 00:10:52,000
very simple

270
00:10:52,000 --> 00:10:54,079
pipeline or flow

271
00:10:54,079 --> 00:10:56,720
of how i would take logs

272
00:10:56,720 --> 00:10:58,720
as i mentioned i'll be focusing a bit on

273
00:10:58,720 --> 00:11:02,320
cloudtrail log and use just native aws

274
00:11:02,320 --> 00:11:05,680
services to convert the data from the

275
00:11:05,680 --> 00:11:08,240
json compressed format to park a which

276
00:11:08,240 --> 00:11:10,079
is a columnar format

277
00:11:10,079 --> 00:11:13,200
and then use athena to create these logs

278
00:11:13,200 --> 00:11:15,760
once they've been converted into part k

279
00:11:15,760 --> 00:11:18,480
and also use quick site on top of athena

280
00:11:18,480 --> 00:11:21,440
to create visualizations

281
00:11:21,440 --> 00:11:24,480
so here on the left hand side i'm

282
00:11:24,480 --> 00:11:26,640
assuming that all the logs are already

283
00:11:26,640 --> 00:11:28,399
stored in s3

284
00:11:28,399 --> 00:11:31,120
and then i switch to sort of steps two

285
00:11:31,120 --> 00:11:32,480
and three here

286
00:11:32,480 --> 00:11:35,519
where i'm using aws glue effectively

287
00:11:35,519 --> 00:11:38,560
glue is a serverless etl engine that aws

288
00:11:38,560 --> 00:11:40,079
provides

289
00:11:40,079 --> 00:11:43,120
so the first step is really to allow

290
00:11:43,120 --> 00:11:46,160
glue to create a metadata table for our

291
00:11:46,160 --> 00:11:47,760
lore rocks

292
00:11:47,760 --> 00:11:50,160
so you create what is known as a crawler

293
00:11:50,160 --> 00:11:52,720
and you point it to the s3 bucket it

294
00:11:52,720 --> 00:11:55,040
goes reads all the data and creates the

295
00:11:55,040 --> 00:11:57,920
metadata table so pretty neat all you

296
00:11:57,920 --> 00:12:00,959
need to do is create this job

297
00:12:00,959 --> 00:12:03,440
one thing i've learned through painful

298
00:12:03,440 --> 00:12:04,639
experience

299
00:12:04,639 --> 00:12:07,519
dealing with cloudtrail logs and glue

300
00:12:07,519 --> 00:12:11,279
is that they are often created with a

301
00:12:11,279 --> 00:12:13,760
specific type of

302
00:12:13,760 --> 00:12:15,600
data types for the different fields in

303
00:12:15,600 --> 00:12:17,200
cloudtrail

304
00:12:17,200 --> 00:12:20,399
however for two particular fields

305
00:12:20,399 --> 00:12:23,760
request parameters and response elements

306
00:12:23,760 --> 00:12:27,519
blue creates it with struct as a type

307
00:12:27,519 --> 00:12:30,240
however the data definition language

308
00:12:30,240 --> 00:12:32,399
that is provided by

309
00:12:32,399 --> 00:12:35,040
aws for cloudtrail logs are just using

310
00:12:35,040 --> 00:12:38,399
string so i had to modify it from

311
00:12:38,399 --> 00:12:40,720
struct to string to make this whole

312
00:12:40,720 --> 00:12:43,440
pipeline work

313
00:12:43,440 --> 00:12:46,399
so great we now have our metadata table

314
00:12:46,399 --> 00:12:47,680
in aws

315
00:12:47,680 --> 00:12:50,399
glue the next step is i want to convert

316
00:12:50,399 --> 00:12:52,639
all my data from the json compressed

317
00:12:52,639 --> 00:12:55,120
format into parquet snappy compressed

318
00:12:55,120 --> 00:12:58,160
format so i just use blue etl and then

319
00:12:58,160 --> 00:13:00,959
again create a job where it takes all

320
00:13:00,959 --> 00:13:03,519
this data which is in json format and

321
00:13:03,519 --> 00:13:05,920
converts it into park a which is

322
00:13:05,920 --> 00:13:08,720
columnar format and stores it in an s3

323
00:13:08,720 --> 00:13:11,440
bucket that i've pointed it to

324
00:13:11,440 --> 00:13:14,160
and then again i have to let blue run a

325
00:13:14,160 --> 00:13:16,639
crawler against this s3 bucket with the

326
00:13:16,639 --> 00:13:19,920
parquet data table sorry parquet data

327
00:13:19,920 --> 00:13:21,200
files

328
00:13:21,200 --> 00:13:23,600
and create the metadata table

329
00:13:23,600 --> 00:13:25,839
and then once that's created i can see

330
00:13:25,839 --> 00:13:27,920
the table in athena so i can run all my

331
00:13:27,920 --> 00:13:30,000
queries against this parquet

332
00:13:30,000 --> 00:13:32,800
table in athena and finally i can use

333
00:13:32,800 --> 00:13:35,519
quickside to visualize all this data

334
00:13:35,519 --> 00:13:38,720
so all i've used really is assuming my

335
00:13:38,720 --> 00:13:40,800
logs are in s3

336
00:13:40,800 --> 00:13:42,880
glue works with other data stores as

337
00:13:42,880 --> 00:13:45,760
well is use three different services

338
00:13:45,760 --> 00:13:49,440
plus s3 aws glue to do the atl create

339
00:13:49,440 --> 00:13:52,079
the metadata tables athena to run my

340
00:13:52,079 --> 00:13:54,480
queries and then finally quicksite to

341
00:13:54,480 --> 00:13:57,920
visualize all these logs

342
00:13:58,000 --> 00:14:00,399
i'm going to do a short demo but i just

343
00:14:00,399 --> 00:14:02,480
wanted to highlight their prerequisite

344
00:14:02,480 --> 00:14:04,880
before i switch to the demo

345
00:14:04,880 --> 00:14:07,760
i'm assuming that you have an iem role

346
00:14:07,760 --> 00:14:09,920
with the right permissions as with

347
00:14:09,920 --> 00:14:12,320
everything in aws there's always a role

348
00:14:12,320 --> 00:14:14,800
there's always a policy

349
00:14:14,800 --> 00:14:17,760
in this case you need access to a role

350
00:14:17,760 --> 00:14:19,199
which has the

351
00:14:19,199 --> 00:14:22,079
aws glue service role policy attached to

352
00:14:22,079 --> 00:14:23,040
the role

353
00:14:23,040 --> 00:14:25,279
um there will also need other things

354
00:14:25,279 --> 00:14:27,279
like you know being able to access the

355
00:14:27,279 --> 00:14:29,680
s3 bucket where the logs are

356
00:14:29,680 --> 00:14:31,920
right access to the s3 bucket where it's

357
00:14:31,920 --> 00:14:34,880
copying the parquet files to etc

358
00:14:34,880 --> 00:14:36,880
um i've also

359
00:14:36,880 --> 00:14:39,199
mentioned that you know in the previous

360
00:14:39,199 --> 00:14:41,519
slide that i have all the logs already

361
00:14:41,519 --> 00:14:44,560
in s3 i'm just mentioning this again

362
00:14:44,560 --> 00:14:46,880
but most important is especially if

363
00:14:46,880 --> 00:14:48,800
you're doing it for the first time is to

364
00:14:48,800 --> 00:14:50,560
be patient

365
00:14:50,560 --> 00:14:53,600
it was definitely not easy for me as

366
00:14:53,600 --> 00:14:56,160
someone with no data background to run

367
00:14:56,160 --> 00:14:57,279
glue

368
00:14:57,279 --> 00:14:58,880
but just going through the tutorials

369
00:14:58,880 --> 00:15:01,199
with aws it was fairly straightforward

370
00:15:01,199 --> 00:15:02,880
to replicate

371
00:15:02,880 --> 00:15:04,800
i was able to create the crawlers i was

372
00:15:04,800 --> 00:15:07,600
able to create the etl job and then you

373
00:15:07,600 --> 00:15:10,000
know create the table in athena

374
00:15:10,000 --> 00:15:12,480
so it's fairly straightforward uh quite

375
00:15:12,480 --> 00:15:15,600
good and easy to do

376
00:15:15,600 --> 00:15:18,320
um i'm gonna just now switch over to the

377
00:15:18,320 --> 00:15:20,320
demo

378
00:15:20,320 --> 00:15:23,760
here i have my aws account

379
00:15:23,760 --> 00:15:27,279
i have created an s3 bucket i have added

380
00:15:27,279 --> 00:15:28,800
a sample

381
00:15:28,800 --> 00:15:31,839
cloudtrail file here

382
00:15:31,839 --> 00:15:32,800
so

383
00:15:32,800 --> 00:15:34,720
i am

384
00:15:34,720 --> 00:15:39,519
going to duplicate this i can go to blue

385
00:15:39,519 --> 00:15:42,160
i have all my prerequisites already set

386
00:15:42,160 --> 00:15:44,480
up so that's pretty good i just wanted

387
00:15:44,480 --> 00:15:46,560
to save a bit of time

388
00:15:46,560 --> 00:15:49,040
um these are all old tables that i've

389
00:15:49,040 --> 00:15:51,360
created

390
00:15:51,360 --> 00:15:52,320
so

391
00:15:52,320 --> 00:15:55,199
i'm going to add a crawler

392
00:15:55,199 --> 00:15:57,839
i'm just going to call it b sides

393
00:15:57,839 --> 00:15:59,839
budapest

394
00:15:59,839 --> 00:16:02,639
called trail

395
00:16:03,839 --> 00:16:06,000
um i'm just going to say look at all the

396
00:16:06,000 --> 00:16:09,199
data stores crawl all folders within ns3

397
00:16:09,199 --> 00:16:11,199
bucket

398
00:16:11,199 --> 00:16:12,560
s3

399
00:16:12,560 --> 00:16:15,519
i am going to select the bucket that i

400
00:16:15,519 --> 00:16:18,320
showed you with one file

401
00:16:18,320 --> 00:16:20,720
and then next i don't want to add

402
00:16:20,720 --> 00:16:24,079
anything else next um

403
00:16:24,079 --> 00:16:26,639
i already have my i am roll so i'm just

404
00:16:26,639 --> 00:16:29,120
attaching that to this particular

405
00:16:29,120 --> 00:16:30,880
crawler

406
00:16:30,880 --> 00:16:33,920
and you can run the crawler on demand or

407
00:16:33,920 --> 00:16:35,759
you can set up sort of like a cron

408
00:16:35,759 --> 00:16:38,160
schedule for it so that it automatically

409
00:16:38,160 --> 00:16:40,240
picks up whenever a new file is added to

410
00:16:40,240 --> 00:16:42,639
the s3 bucket which is what i would be

411
00:16:42,639 --> 00:16:44,800
doing when i run this in a more

412
00:16:44,800 --> 00:16:47,519
production sort of environment

413
00:16:47,519 --> 00:16:49,759
i've configured it

414
00:16:49,759 --> 00:16:51,360
i'm just going to create all these

415
00:16:51,360 --> 00:16:55,360
tables in the default database

416
00:16:55,759 --> 00:16:58,160
and

417
00:16:58,959 --> 00:17:00,959
i have created this

418
00:17:00,959 --> 00:17:04,480
so now i have to run this

419
00:17:04,480 --> 00:17:06,799
crawler it refreshes it shows me a

420
00:17:06,799 --> 00:17:08,000
status

421
00:17:08,000 --> 00:17:10,720
hopefully it doesn't take too long

422
00:17:10,720 --> 00:17:15,160
but i'm going to let that run

423
00:17:28,400 --> 00:17:30,960
probably come back to this

424
00:17:30,960 --> 00:17:33,520
by switching between the presentation so

425
00:17:33,520 --> 00:17:37,520
that i'm not holding you guys up

426
00:17:37,520 --> 00:17:39,840
um

427
00:17:40,160 --> 00:17:43,840
i wanted to share why i felt this was

428
00:17:43,840 --> 00:17:46,720
quite interesting for me and

429
00:17:46,720 --> 00:17:48,400
important

430
00:17:48,400 --> 00:17:50,880
i've just added a few queries in here

431
00:17:50,880 --> 00:17:53,520
and i'm comparing sort of the runtime

432
00:17:53,520 --> 00:17:55,280
for each of these queries and the amount

433
00:17:55,280 --> 00:17:57,679
of data that was scanned for each of

434
00:17:57,679 --> 00:18:00,480
these queries if i run the query just

435
00:18:00,480 --> 00:18:03,120
against the json compressed file on

436
00:18:03,120 --> 00:18:04,400
partitioned

437
00:18:04,400 --> 00:18:06,559
and the power k file

438
00:18:06,559 --> 00:18:10,080
in terms of data set i used the summit

439
00:18:10,080 --> 00:18:12,799
route floss cloudtrail database i

440
00:18:12,799 --> 00:18:14,320
thought you know that's available to

441
00:18:14,320 --> 00:18:15,679
anyone

442
00:18:15,679 --> 00:18:18,240
who wants to go and test it with glue so

443
00:18:18,240 --> 00:18:20,880
it's probably an easy enough data set

444
00:18:20,880 --> 00:18:22,720
for everyone to use

445
00:18:22,720 --> 00:18:25,760
i know that the data scan here says 1.53

446
00:18:25,760 --> 00:18:27,520
gb

447
00:18:27,520 --> 00:18:30,240
it's usually not that big

448
00:18:30,240 --> 00:18:32,640
it's i think around

449
00:18:32,640 --> 00:18:36,559
240 mb or so the actual tar file which

450
00:18:36,559 --> 00:18:39,039
has the different culture log i just

451
00:18:39,039 --> 00:18:41,280
created multiple copies of it because i

452
00:18:41,280 --> 00:18:43,360
wanted to have roughly

453
00:18:43,360 --> 00:18:46,000
more than a gigabyte at least to

454
00:18:46,000 --> 00:18:47,919
do the comparison between running my

455
00:18:47,919 --> 00:18:50,000
queries against just the

456
00:18:50,000 --> 00:18:53,600
json data and the parquet data

457
00:18:53,600 --> 00:18:56,000
the first query i have here is just

458
00:18:56,000 --> 00:18:58,160
running through some of the api errors

459
00:18:58,160 --> 00:19:00,960
so you know just select if you feel from

460
00:19:00,960 --> 00:19:02,799
cloudtrail

461
00:19:02,799 --> 00:19:05,120
and i'm looking for particular

462
00:19:05,120 --> 00:19:06,880
error codes which indicate that there

463
00:19:06,880 --> 00:19:08,559
was a failure

464
00:19:08,559 --> 00:19:11,280
things like client invalid permission

465
00:19:11,280 --> 00:19:14,160
client not permitted access denied

466
00:19:14,160 --> 00:19:15,200
um

467
00:19:15,200 --> 00:19:17,200
just for simplicity i restricted

468
00:19:17,200 --> 00:19:20,799
findings to 25 so you can see that you

469
00:19:20,799 --> 00:19:23,679
know with json the run times around 17

470
00:19:23,679 --> 00:19:26,240
seconds it's count all the data whereas

471
00:19:26,240 --> 00:19:29,280
with parquet it's around 5.37 seconds

472
00:19:29,280 --> 00:19:30,960
and it scanned

473
00:19:30,960 --> 00:19:33,520
still not that great but you know

474
00:19:33,520 --> 00:19:37,600
still less than whatever it is with json

475
00:19:37,600 --> 00:19:40,000
now that i start moving to the second

476
00:19:40,000 --> 00:19:42,400
query which is looking at activity from

477
00:19:42,400 --> 00:19:44,559
a particular ip address which i know is

478
00:19:44,559 --> 00:19:45,840
malicious

479
00:19:45,840 --> 00:19:48,400
um so i've put the query i said you know

480
00:19:48,400 --> 00:19:50,960
give me the event time source name and

481
00:19:50,960 --> 00:19:53,280
the user identity

482
00:19:53,280 --> 00:19:56,080
for a particular source ip address so

483
00:19:56,080 --> 00:19:58,880
we're starting to narrow down

484
00:19:58,880 --> 00:20:01,520
again with json around 19 seconds

485
00:20:01,520 --> 00:20:02,960
because it has to go through all the

486
00:20:02,960 --> 00:20:04,799
data row by row

487
00:20:04,799 --> 00:20:07,280
scan all the data whereas with part k

488
00:20:07,280 --> 00:20:09,280
the run times you know just under three

489
00:20:09,280 --> 00:20:11,760
seconds that's quite a big difference

490
00:20:11,760 --> 00:20:15,600
um and the data scanned is 9 mb

491
00:20:15,600 --> 00:20:18,320
similarly another query ec2 instance

492
00:20:18,320 --> 00:20:20,559
enumerating an s3

493
00:20:20,559 --> 00:20:21,760
bucket

494
00:20:21,760 --> 00:20:24,880
again with json around 13 seconds

495
00:20:24,880 --> 00:20:27,600
scanned all the data whereas with 4k is

496
00:20:27,600 --> 00:20:30,240
taking you know less than two seconds

497
00:20:30,240 --> 00:20:34,880
and it's scanned 38 megabytes or 39

498
00:20:34,880 --> 00:20:38,240
so you can see that there is a savings

499
00:20:38,240 --> 00:20:40,559
not in terms not just in terms of how

500
00:20:40,559 --> 00:20:42,720
much time the query takes to run but

501
00:20:42,720 --> 00:20:44,640
also in terms of how much

502
00:20:44,640 --> 00:20:46,880
data is actually scanned by athena when

503
00:20:46,880 --> 00:20:49,520
these different queries are run

504
00:20:49,520 --> 00:20:52,000
so just to sort of give you an overview

505
00:20:52,000 --> 00:20:53,360
of how much

506
00:20:53,360 --> 00:20:55,679
savings we can have

507
00:20:55,679 --> 00:20:57,520
sorry i'm going to switch back to blue

508
00:20:57,520 --> 00:21:00,240
and see if this has run

509
00:21:00,240 --> 00:21:02,240
yeah as you can see they have finished

510
00:21:02,240 --> 00:21:03,600
running

511
00:21:03,600 --> 00:21:07,840
so now i can go back to my table

512
00:21:07,840 --> 00:21:11,440
and here i'm just going to sort this um

513
00:21:11,440 --> 00:21:15,039
here's the table that was just created

514
00:21:15,039 --> 00:21:18,320
i am going to go to the table

515
00:21:18,320 --> 00:21:20,640
as i mentioned earlier the request

516
00:21:20,640 --> 00:21:22,880
parameters and the response elements are

517
00:21:22,880 --> 00:21:24,559
instruct

518
00:21:24,559 --> 00:21:25,360
so

519
00:21:25,360 --> 00:21:29,280
what i'll do is i'll go to edit schema

520
00:21:29,280 --> 00:21:30,720
click here

521
00:21:30,720 --> 00:21:33,679
switch that to string

522
00:21:33,679 --> 00:21:36,000
click

523
00:21:36,159 --> 00:21:39,679
response element switch that to string

524
00:21:39,679 --> 00:21:42,080
save

525
00:21:42,240 --> 00:21:43,440
and

526
00:21:43,440 --> 00:21:46,159
great so now we have

527
00:21:46,159 --> 00:21:47,760
our table

528
00:21:47,760 --> 00:21:50,960
uh with the json file

529
00:21:50,960 --> 00:21:52,799
so

530
00:21:52,799 --> 00:21:53,840
all right

531
00:21:53,840 --> 00:21:56,000
just to quickly refresh this is what we

532
00:21:56,000 --> 00:21:58,559
have done i have my logs in s3 i've

533
00:21:58,559 --> 00:22:00,400
moved to the crawler i've created the

534
00:22:00,400 --> 00:22:02,640
metadata table

535
00:22:02,640 --> 00:22:06,000
now i'm going to create a job to convert

536
00:22:06,000 --> 00:22:08,000
data

537
00:22:08,000 --> 00:22:10,840
call it b size budapest

538
00:22:10,840 --> 00:22:14,240
json part k

539
00:22:14,240 --> 00:22:17,200
i already have my role

540
00:22:17,200 --> 00:22:19,840
i'm just going to let aws glue propose a

541
00:22:19,840 --> 00:22:22,480
script to me

542
00:22:22,480 --> 00:22:23,919
and

543
00:22:23,919 --> 00:22:25,440
here's where the

544
00:22:25,440 --> 00:22:27,360
script's going to be shared i have some

545
00:22:27,360 --> 00:22:29,280
of these directories already so it's

546
00:22:29,280 --> 00:22:31,840
just going to store it there

547
00:22:31,840 --> 00:22:34,399
next

548
00:22:36,159 --> 00:22:38,559
and here's the table that we just

549
00:22:38,559 --> 00:22:42,159
created cultural logs json1

550
00:22:42,159 --> 00:22:45,760
the classification is cloudtrail

551
00:22:45,760 --> 00:22:47,120
i'm going to go

552
00:22:47,120 --> 00:22:50,080
i want to change the schema

553
00:22:50,080 --> 00:22:50,960
and

554
00:22:50,960 --> 00:22:53,440
i want to create

555
00:22:53,440 --> 00:22:55,200
my

556
00:22:55,200 --> 00:22:58,640
target format is 4k and i want to save

557
00:22:58,640 --> 00:23:02,000
all of that in s3

558
00:23:02,000 --> 00:23:04,640
and i'll quickly show you that this

559
00:23:04,640 --> 00:23:06,720
pocket is empty

560
00:23:06,720 --> 00:23:10,240
cultural logs part k2

561
00:23:10,240 --> 00:23:12,400
select

562
00:23:12,400 --> 00:23:13,440
next

563
00:23:13,440 --> 00:23:16,559
this is what blue's going to map

564
00:23:16,559 --> 00:23:19,840
from json to parquet

565
00:23:19,840 --> 00:23:21,600
i'm not changing anything so i just

566
00:23:21,600 --> 00:23:25,039
click save job and edit script

567
00:23:25,039 --> 00:23:25,919
and

568
00:23:25,919 --> 00:23:29,520
i'm just going to close out of here

569
00:23:29,760 --> 00:23:33,919
and now run this job

570
00:23:34,400 --> 00:23:37,200
if i just click here it shows you the

571
00:23:37,200 --> 00:23:39,840
status um all the logs going to

572
00:23:39,840 --> 00:23:41,919
cloudwatch so if you're interested in

573
00:23:41,919 --> 00:23:43,760
finding out what it's doing you can go

574
00:23:43,760 --> 00:23:45,600
there and take a look

575
00:23:45,600 --> 00:23:48,000
um it also gives you an indication of

576
00:23:48,000 --> 00:23:50,240
how much time it's taking to run the

577
00:23:50,240 --> 00:23:52,720
script

578
00:23:52,720 --> 00:23:55,840
well so far let's take in 10 seconds

579
00:23:55,840 --> 00:23:58,400
so this just going back to my diagram

580
00:23:58,400 --> 00:24:00,960
here this is what we are currently doing

581
00:24:00,960 --> 00:24:03,760
is we have written an etl job we are

582
00:24:03,760 --> 00:24:05,919
running it and at the end of the whole

583
00:24:05,919 --> 00:24:09,279
process it should technically be in s3

584
00:24:09,279 --> 00:24:11,360
the parquet file the converted parquet

585
00:24:11,360 --> 00:24:12,720
file

586
00:24:12,720 --> 00:24:13,840
um

587
00:24:13,840 --> 00:24:16,400
let's see if this has completed i chose

588
00:24:16,400 --> 00:24:19,520
a very small file so that

589
00:24:19,520 --> 00:24:23,279
none of this takes minutes to run

590
00:24:23,279 --> 00:24:27,440
her we can come back to it in a bit

591
00:24:27,440 --> 00:24:29,679
um

592
00:24:29,679 --> 00:24:32,000
here are some metrics

593
00:24:32,000 --> 00:24:33,200
um

594
00:24:33,200 --> 00:24:35,760
in terms of savings

595
00:24:35,760 --> 00:24:38,080
coaching code not from a cost savings

596
00:24:38,080 --> 00:24:40,400
perspective per se but

597
00:24:40,400 --> 00:24:42,720
in terms of amount of data that is

598
00:24:42,720 --> 00:24:44,640
scanned and the

599
00:24:44,640 --> 00:24:46,960
effectiveness of running queries against

600
00:24:46,960 --> 00:24:49,039
parquet data

601
00:24:49,039 --> 00:24:50,480
my learnings

602
00:24:50,480 --> 00:24:52,559
using the flaw of cloudtrail database

603
00:24:52,559 --> 00:24:55,200
from summit route has been that

604
00:24:55,200 --> 00:24:58,720
um roughly running 15 queries

605
00:24:58,720 --> 00:25:02,000
typical ir craze against uh cloudtrail

606
00:25:02,000 --> 00:25:05,520
roughly 74 percent less data scanned and

607
00:25:05,520 --> 00:25:07,840
you know athena charges based on the

608
00:25:07,840 --> 00:25:09,840
amount of data that scans so that the

609
00:25:09,840 --> 00:25:11,919
lesser the data that scan the better it

610
00:25:11,919 --> 00:25:12,720
is

611
00:25:12,720 --> 00:25:14,640
um and it's seven to seven percent

612
00:25:14,640 --> 00:25:16,720
quicker as well because less than a

613
00:25:16,720 --> 00:25:18,880
scanned it has a more effective way of

614
00:25:18,880 --> 00:25:20,880
being able to go to the table and the

615
00:25:20,880 --> 00:25:23,760
particular field that you're after

616
00:25:23,760 --> 00:25:26,480
admittedly this is a very small data set

617
00:25:26,480 --> 00:25:29,200
but you know i've used it on cases and

618
00:25:29,200 --> 00:25:31,760
it's been remarkable because um

619
00:25:31,760 --> 00:25:34,080
especially as data size keeps going up

620
00:25:34,080 --> 00:25:36,000
you can use other services obviously

621
00:25:36,000 --> 00:25:38,880
like redshift etc but if you're using

622
00:25:38,880 --> 00:25:41,360
athena for doing ir

623
00:25:41,360 --> 00:25:44,080
instead of taking 10 minutes 20 minutes

624
00:25:44,080 --> 00:25:46,799
or timing out with parquet i can

625
00:25:46,799 --> 00:25:50,159
actually run the clothes much faster

626
00:25:50,159 --> 00:25:53,360
and it helps me triage quicker as an

627
00:25:53,360 --> 00:25:56,400
incident response professional

628
00:25:56,400 --> 00:25:59,360
so that's the savings part let's go back

629
00:25:59,360 --> 00:26:00,880
and see

630
00:26:00,880 --> 00:26:02,960
if this has finished

631
00:26:02,960 --> 00:26:05,679
so this job has finished

632
00:26:05,679 --> 00:26:08,320
it took roughly a minute

633
00:26:08,320 --> 00:26:10,559
this is the timeout period

634
00:26:10,559 --> 00:26:13,279
you can get the job to run quicker based

635
00:26:13,279 --> 00:26:16,480
on the parameters you use um but i just

636
00:26:16,480 --> 00:26:18,720
use the default settings

637
00:26:18,720 --> 00:26:19,520
so

638
00:26:19,520 --> 00:26:22,559
if i go back

639
00:26:22,640 --> 00:26:28,320
add a crawler again besides go to past

640
00:26:28,320 --> 00:26:30,960
okay

641
00:26:31,279 --> 00:26:35,360
data store scroller folders

642
00:26:35,360 --> 00:26:38,480
my data should be in this bucket let's

643
00:26:38,480 --> 00:26:41,919
refresh this and see

644
00:26:44,320 --> 00:26:46,640
here's the parquet files that have been

645
00:26:46,640 --> 00:26:49,200
created snappy is just a compression

646
00:26:49,200 --> 00:26:51,120
format

647
00:26:51,120 --> 00:26:53,279
so i select

648
00:26:53,279 --> 00:26:54,960
i click next

649
00:26:54,960 --> 00:26:56,960
this is similar to what we did before i

650
00:26:56,960 --> 00:26:59,440
don't want another data store

651
00:26:59,440 --> 00:27:02,960
i already have my iam role i'm going to

652
00:27:02,960 --> 00:27:04,880
run this on demand

653
00:27:04,880 --> 00:27:06,559
create it in default

654
00:27:06,559 --> 00:27:09,200
um i'm just going to add b sides

655
00:27:09,200 --> 00:27:12,880
budapest say prefix

656
00:27:12,880 --> 00:27:14,080
next

657
00:27:14,080 --> 00:27:15,360
finish

658
00:27:15,360 --> 00:27:17,039
um

659
00:27:17,039 --> 00:27:20,960
that's my crawler i'm going to

660
00:27:21,360 --> 00:27:23,600
it should take similar amount of time

661
00:27:23,600 --> 00:27:25,039
less than a minute

662
00:27:25,039 --> 00:27:29,200
um so i'm just going to let that run

663
00:27:29,440 --> 00:27:34,200
and switch to athena

664
00:27:37,520 --> 00:27:38,720
these are

665
00:27:38,720 --> 00:27:41,120
some of the older logs that i have but

666
00:27:41,120 --> 00:27:44,559
this is the particular data set that we

667
00:27:44,559 --> 00:27:47,279
asked bluetooth crawl and create a table

668
00:27:47,279 --> 00:27:49,760
for so you see it this these are just

669
00:27:49,760 --> 00:27:51,760
all tables i have

670
00:27:51,760 --> 00:27:54,240
so let's go back and see how

671
00:27:54,240 --> 00:27:56,320
it's just like real time how much time

672
00:27:56,320 --> 00:27:58,960
is taking

673
00:28:00,080 --> 00:28:03,840
um just to go back to the pipeline so

674
00:28:03,840 --> 00:28:06,080
you know just to refresh memory we took

675
00:28:06,080 --> 00:28:09,120
the raw logs from cloudtrail in json

676
00:28:09,120 --> 00:28:11,600
compressed format created the metadata

677
00:28:11,600 --> 00:28:13,440
table using glue

678
00:28:13,440 --> 00:28:16,640
uh crawlers then we ran an etl job to

679
00:28:16,640 --> 00:28:19,840
convert it from json to parquet format

680
00:28:19,840 --> 00:28:22,720
then i showed you the files in f3 in dot

681
00:28:22,720 --> 00:28:25,200
snappy dot prk format

682
00:28:25,200 --> 00:28:27,279
and i'm running the crawler again to

683
00:28:27,279 --> 00:28:30,080
create the metadata table

684
00:28:30,080 --> 00:28:33,440
so let's see that's topping so almost

685
00:28:33,440 --> 00:28:34,559
there

686
00:28:34,559 --> 00:28:39,000
just give it a couple of seconds

687
00:28:47,039 --> 00:28:48,960
sorry about that

688
00:28:48,960 --> 00:28:52,399
um i just wanted to pause and not move

689
00:28:52,399 --> 00:28:54,480
to the next slide because that's moving

690
00:28:54,480 --> 00:28:58,760
into another aws service

691
00:29:07,279 --> 00:29:08,240
right

692
00:29:08,240 --> 00:29:11,200
so this job has finished running

693
00:29:11,200 --> 00:29:16,880
if i go back to athena and click refresh

694
00:29:16,880 --> 00:29:19,760
here's the table and i've added the

695
00:29:19,760 --> 00:29:22,080
prefix of b sites budapest which is why

696
00:29:22,080 --> 00:29:25,360
it's showing us our way

697
00:29:25,440 --> 00:29:27,360
all right um

698
00:29:27,360 --> 00:29:31,918
so if i preview the table

699
00:29:32,240 --> 00:29:34,880
there you go here's the information you

700
00:29:34,880 --> 00:29:39,120
can see the user agent the event id user

701
00:29:39,120 --> 00:29:42,080
identity again with

702
00:29:42,080 --> 00:29:43,200
aws

703
00:29:43,200 --> 00:29:44,960
it's complicated because you have a

704
00:29:44,960 --> 00:29:47,440
principal id it has information on

705
00:29:47,440 --> 00:29:49,520
whether you use a session whether you

706
00:29:49,520 --> 00:29:51,679
use an access key etc so all that

707
00:29:51,679 --> 00:29:53,279
information is there

708
00:29:53,279 --> 00:29:57,200
event name source recipient account id

709
00:29:57,200 --> 00:29:58,399
region

710
00:29:58,399 --> 00:30:00,320
and

711
00:30:00,320 --> 00:30:01,919
here it is

712
00:30:01,919 --> 00:30:04,799
i also quickly wanted to show you the

713
00:30:04,799 --> 00:30:06,840
properties

714
00:30:06,840 --> 00:30:11,279
um this is all based on what we selected

715
00:30:11,279 --> 00:30:13,200
when we set up the crawler so we just

716
00:30:13,200 --> 00:30:15,679
said you know created it in the database

717
00:30:15,679 --> 00:30:17,760
sorry default database so that's what it

718
00:30:17,760 --> 00:30:19,840
is the create time

719
00:30:19,840 --> 00:30:22,480
you can also look at the

720
00:30:22,480 --> 00:30:23,840
table

721
00:30:23,840 --> 00:30:26,640
data definition language

722
00:30:26,640 --> 00:30:29,120
remember i mentioned that we need to use

723
00:30:29,120 --> 00:30:30,960
string for request parameters and

724
00:30:30,960 --> 00:30:33,039
response elements so this is reflected

725
00:30:33,039 --> 00:30:35,440
here but here's the entire data

726
00:30:35,440 --> 00:30:37,600
definition table

727
00:30:37,600 --> 00:30:39,600
it also has information that you know

728
00:30:39,600 --> 00:30:41,679
part k is being used

729
00:30:41,679 --> 00:30:43,840
and then just like properties and the

730
00:30:43,840 --> 00:30:46,159
number of records

731
00:30:46,159 --> 00:30:48,880
so that that was a very very quick

732
00:30:48,880 --> 00:30:52,559
introduction to blue and how to use it

733
00:30:52,559 --> 00:30:54,240
and just to show you that you know it's

734
00:30:54,240 --> 00:30:56,399
fairly simple to use

735
00:30:56,399 --> 00:30:58,000
um

736
00:30:58,000 --> 00:31:00,000
so we've spoken about

737
00:31:00,000 --> 00:31:02,320
generally getting logs moving it into a

738
00:31:02,320 --> 00:31:04,080
peanut querying it some of the

739
00:31:04,080 --> 00:31:06,080
efficiencies that we have seen in terms

740
00:31:06,080 --> 00:31:08,480
of data the amount of data scan but also

741
00:31:08,480 --> 00:31:11,919
how much time queries run

742
00:31:11,919 --> 00:31:14,559
finally i'm going to move to quick site

743
00:31:14,559 --> 00:31:16,840
so i have this open

744
00:31:16,840 --> 00:31:18,480
here

745
00:31:18,480 --> 00:31:19,760
and

746
00:31:19,760 --> 00:31:23,200
i am going to create a new data set

747
00:31:23,200 --> 00:31:25,360
here's athena

748
00:31:25,360 --> 00:31:28,720
i'm just going to call it again boost

749
00:31:28,720 --> 00:31:32,440
these slides budapest

750
00:31:34,880 --> 00:31:38,320
default database

751
00:31:41,200 --> 00:31:44,000
that's weird

752
00:31:44,240 --> 00:31:46,480
apologies about that i just realized

753
00:31:46,480 --> 00:31:48,320
what um

754
00:31:48,320 --> 00:31:52,159
i've done wrong i have used ireland as a

755
00:31:52,159 --> 00:31:54,320
region uh whereas when i set up

756
00:31:54,320 --> 00:31:59,440
quickside i used us east one and not eu

757
00:31:59,440 --> 00:32:04,240
which is why it can't see the data set

758
00:32:06,399 --> 00:32:09,760
so unfortunately i can't show you how to

759
00:32:09,760 --> 00:32:11,840
create it but effectively once you

760
00:32:11,840 --> 00:32:13,039
create the

761
00:32:13,039 --> 00:32:14,720
data set

762
00:32:14,720 --> 00:32:17,200
you just connect to athena and it looks

763
00:32:17,200 --> 00:32:19,600
very similar to

764
00:32:19,600 --> 00:32:21,600
elastic or tableau or one of the many

765
00:32:21,600 --> 00:32:24,959
visualization tools

766
00:32:25,200 --> 00:32:27,519
you know hopefully unfortunately all

767
00:32:27,519 --> 00:32:30,480
these things happen during presentations

768
00:32:30,480 --> 00:32:34,159
so if i switch back i do have an example

769
00:32:34,159 --> 00:32:36,480
dashboard that i created with quickside

770
00:32:36,480 --> 00:32:41,039
so hopefully this helps give an overview

771
00:32:41,279 --> 00:32:43,679
on the left hand side once you select

772
00:32:43,679 --> 00:32:46,960
the table so when i was doing my testing

773
00:32:46,960 --> 00:32:48,960
i used

774
00:32:48,960 --> 00:32:51,440
a table called flaws final test parquet

775
00:32:51,440 --> 00:32:53,279
analysis

776
00:32:53,279 --> 00:32:56,080
you see this tag here which says spice

777
00:32:56,080 --> 00:32:58,480
if you want quick side to be quicker you

778
00:32:58,480 --> 00:33:01,120
can use spice which basically just means

779
00:33:01,120 --> 00:33:03,360
it's doing all the analysis in memory

780
00:33:03,360 --> 00:33:04,960
rather than directly querying from

781
00:33:04,960 --> 00:33:07,120
athena and s3

782
00:33:07,120 --> 00:33:09,919
so again depends on your use case

783
00:33:09,919 --> 00:33:11,679
all the types of visuals that you can

784
00:33:11,679 --> 00:33:12,720
create

785
00:33:12,720 --> 00:33:14,080
are at the bottom

786
00:33:14,080 --> 00:33:17,120
the thunderbolt if you select it athena

787
00:33:17,120 --> 00:33:19,200
sorry not athena click site comes up

788
00:33:19,200 --> 00:33:21,200
with a

789
00:33:21,200 --> 00:33:23,760
suggested chart uh you just need to

790
00:33:23,760 --> 00:33:26,559
propose what fields you want

791
00:33:26,559 --> 00:33:29,760
here i've just added a few um they're

792
00:33:29,760 --> 00:33:32,480
not indicative of anything as such but

793
00:33:32,480 --> 00:33:35,120
you know on the first chart here it's

794
00:33:35,120 --> 00:33:37,039
just showing the top

795
00:33:37,039 --> 00:33:39,200
source ip addresses and the number of

796
00:33:39,200 --> 00:33:41,919
records um so on the right hand side we

797
00:33:41,919 --> 00:33:44,880
see this huge uptick um you know like

798
00:33:44,880 --> 00:33:46,320
almost

799
00:33:46,320 --> 00:33:48,159
5 million

800
00:33:48,159 --> 00:33:50,640
so probably that's normal

801
00:33:50,640 --> 00:33:52,240
or a lot of people are connecting from

802
00:33:52,240 --> 00:33:53,279
the same

803
00:33:53,279 --> 00:33:56,110
um whereas all these little

804
00:33:56,110 --> 00:33:57,840
[Music]

805
00:33:57,840 --> 00:34:00,880
lines here are probably in only so

806
00:34:00,880 --> 00:34:02,640
that's what i would go and look at if i

807
00:34:02,640 --> 00:34:04,559
was investigating

808
00:34:04,559 --> 00:34:07,120
but i would also check with the client

809
00:34:07,120 --> 00:34:09,040
um especially if there's objects like

810
00:34:09,040 --> 00:34:11,040
this if it's an ip address that belongs

811
00:34:11,040 --> 00:34:12,560
to them or not

812
00:34:12,560 --> 00:34:15,520
um also i created a bunch of pie charts

813
00:34:15,520 --> 00:34:16,879
it's just showing

814
00:34:16,879 --> 00:34:18,879
different types of events different

815
00:34:18,879 --> 00:34:22,239
types of user identities

816
00:34:22,239 --> 00:34:24,960
user agents you can also create

817
00:34:24,960 --> 00:34:27,918
pivot tables

818
00:34:28,560 --> 00:34:29,440
so

819
00:34:29,440 --> 00:34:31,359
you know i wish i could have shown you a

820
00:34:31,359 --> 00:34:33,839
quick side but i realized that i i

821
00:34:33,839 --> 00:34:36,079
messed up with regions

822
00:34:36,079 --> 00:34:38,239
so that's an important learning uh to

823
00:34:38,239 --> 00:34:40,560
keep in mind with regions always that it

824
00:34:40,560 --> 00:34:42,800
needs to be in the same region

825
00:34:42,800 --> 00:34:44,560
but this is what quickside looks like

826
00:34:44,560 --> 00:34:47,119
all you need to do is point it to the

827
00:34:47,119 --> 00:34:49,760
athena table with your parquet data and

828
00:34:49,760 --> 00:34:52,399
it it exposes all the fields to you and

829
00:34:52,399 --> 00:34:54,639
you can create the dashboard

830
00:34:54,639 --> 00:34:56,320
and it's pretty cool because i'm not

831
00:34:56,320 --> 00:34:58,880
creating a duplicate copy of the data

832
00:34:58,880 --> 00:35:01,280
i'm not having to migrate the data into

833
00:35:01,280 --> 00:35:03,680
another data store i'm not having to

834
00:35:03,680 --> 00:35:06,000
extend my pipeline

835
00:35:06,000 --> 00:35:08,720
so i quite like using this just because

836
00:35:08,720 --> 00:35:12,079
it's simple and it makes my life easier

837
00:35:12,079 --> 00:35:13,920
and it provides the functionality that

838
00:35:13,920 --> 00:35:15,520
i'm after from a dashboarding

839
00:35:15,520 --> 00:35:17,839
perspective

840
00:35:18,079 --> 00:35:19,280
finally

841
00:35:19,280 --> 00:35:21,680
just going back to takeaways and this

842
00:35:21,680 --> 00:35:23,680
slide links back to

843
00:35:23,680 --> 00:35:25,440
the caveat slide that i was talking

844
00:35:25,440 --> 00:35:26,960
about

845
00:35:26,960 --> 00:35:29,680
first is always do you have the right

846
00:35:29,680 --> 00:35:30,640
logs

847
00:35:30,640 --> 00:35:32,560
whilst it would be ideal to have

848
00:35:32,560 --> 00:35:35,040
everything and all logs unfortunately

849
00:35:35,040 --> 00:35:37,119
it's not always possible for various

850
00:35:37,119 --> 00:35:40,240
reasons cost is a major driver of this

851
00:35:40,240 --> 00:35:42,160
so be thinking about what are the

852
00:35:42,160 --> 00:35:44,160
important blogs

853
00:35:44,160 --> 00:35:46,880
within aws we spoke quite a bit about

854
00:35:46,880 --> 00:35:49,200
cloudtrail that's absolutely the most

855
00:35:49,200 --> 00:35:50,720
important log source

856
00:35:50,720 --> 00:35:53,599
especially data events

857
00:35:53,599 --> 00:35:55,839
to give you that level of detail the

858
00:35:55,839 --> 00:35:58,400
second aspect we spoke about

859
00:35:58,400 --> 00:36:00,560
as part of logs is how do you store

860
00:36:00,560 --> 00:36:02,160
these logs we

861
00:36:02,160 --> 00:36:04,880
spoke about using columnar format and i

862
00:36:04,880 --> 00:36:07,040
showed you some metrics

863
00:36:07,040 --> 00:36:08,880
you don't need to take my word for it

864
00:36:08,880 --> 00:36:11,680
you know the log sources that i used

865
00:36:11,680 --> 00:36:13,920
are open source and the reason i used it

866
00:36:13,920 --> 00:36:16,400
was so that anyone can take the exact

867
00:36:16,400 --> 00:36:18,960
same log sources use glue

868
00:36:18,960 --> 00:36:21,119
and athena and quickside to be able to

869
00:36:21,119 --> 00:36:23,520
replicate whatever i've done

870
00:36:23,520 --> 00:36:25,520
but you know roughly

871
00:36:25,520 --> 00:36:29,839
almost 75 percent to 80 say not 80 75

872
00:36:29,839 --> 00:36:32,079
percent let's call it in terms of uh

873
00:36:32,079 --> 00:36:34,240
better running of craze and also less

874
00:36:34,240 --> 00:36:37,839
data are scanned when using parquet um

875
00:36:37,839 --> 00:36:39,839
but also i showed you a demo hopefully

876
00:36:39,839 --> 00:36:42,560
give you an overview of how simple it is

877
00:36:42,560 --> 00:36:44,560
to use something like glue to convert

878
00:36:44,560 --> 00:36:45,520
data

879
00:36:45,520 --> 00:36:47,760
um and it can actually be set up as a

880
00:36:47,760 --> 00:36:49,839
job so you can you don't need to do it

881
00:36:49,839 --> 00:36:51,520
every time

882
00:36:51,520 --> 00:36:53,920
and then finally we spoke a little bit

883
00:36:53,920 --> 00:36:55,920
about visualizing logs

884
00:36:55,920 --> 00:36:56,800
i

885
00:36:56,800 --> 00:36:58,240
wanted to show you a quick side

886
00:36:58,240 --> 00:37:00,960
unfortunately i messed up

887
00:37:00,960 --> 00:37:03,280
but hopefully the dashboard screenshot

888
00:37:03,280 --> 00:37:05,599
that i showed gives you an overview of

889
00:37:05,599 --> 00:37:08,560
how easy it is to use clickside

890
00:37:08,560 --> 00:37:11,240
and that it provides the sort of

891
00:37:11,240 --> 00:37:13,440
visualizations that are available in

892
00:37:13,440 --> 00:37:14,240
other

893
00:37:14,240 --> 00:37:15,760
tools as well

894
00:37:15,760 --> 00:37:18,800
thank you for your time

