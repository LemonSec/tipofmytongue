1
00:00:03,030 --> 00:00:08,400
thank you Mark for giving this

2
00:00:05,220 --> 00:00:10,530
opportunity and as Mark mentioned I am a

3
00:00:08,400 --> 00:00:12,150
professor computer science and chair in

4
00:00:10,530 --> 00:00:14,428
cyber security at King's College College

5
00:00:12,150 --> 00:00:16,680
London where I lead the system security

6
00:00:14,429 --> 00:00:18,720
research lab and we work at intersection

7
00:00:16,680 --> 00:00:21,448
of program analysis and machine learning

8
00:00:18,720 --> 00:00:25,529
for systems security so pretty broad

9
00:00:21,449 --> 00:00:27,240
remit another confession to make okay so

10
00:00:25,529 --> 00:00:29,310
my confession is because I've been

11
00:00:27,240 --> 00:00:30,900
working in this at this intersection for

12
00:00:29,310 --> 00:00:32,519
quite a little time and that my

13
00:00:30,900 --> 00:00:33,930
confession is that I believe we haven't

14
00:00:32,520 --> 00:00:37,020
been using machine learning the right

15
00:00:33,930 --> 00:00:39,629
way okay so this is a bit of a bold

16
00:00:37,020 --> 00:00:41,399
statement I understand so let me just

17
00:00:39,629 --> 00:00:45,360
rephrase it as smooth it down a little

18
00:00:41,399 --> 00:00:48,059
bit but also just point you guys out

19
00:00:45,360 --> 00:00:50,340
that we're kind of getting to a brink of

20
00:00:48,059 --> 00:00:52,980
a machine learning crisis like this is

21
00:00:50,340 --> 00:00:55,079
what people are advocating all over and

22
00:00:52,980 --> 00:00:56,819
this is something that just was recently

23
00:00:55,079 --> 00:00:58,980
presented at the American Association

24
00:00:56,820 --> 00:01:03,149
Association for American Advancement of

25
00:00:58,980 --> 00:01:06,210
Sciences from Duke University

26
00:01:03,149 --> 00:01:08,640
where whether having mullen is machine

27
00:01:06,210 --> 00:01:12,000
learning causing science crisis because

28
00:01:08,640 --> 00:01:13,979
it's a bit unclear what's happening most

29
00:01:12,000 --> 00:01:17,070
of the times where we actually deploy

30
00:01:13,979 --> 00:01:19,009
these algorithms so there's a there's a

31
00:01:17,070 --> 00:01:22,679
fear that we're not able to reproduce

32
00:01:19,009 --> 00:01:24,720
properly experiments there's a fear that

33
00:01:22,680 --> 00:01:28,560
the data set that we're using our bias

34
00:01:24,720 --> 00:01:31,320
in one way or another and I'm not even

35
00:01:28,560 --> 00:01:33,420
touching on the social aspect of

36
00:01:31,320 --> 00:01:35,520
fairness but it's also a problem there

37
00:01:33,420 --> 00:01:38,220
there's a fear that although we might

38
00:01:35,520 --> 00:01:41,490
have high predictions we're not able to

39
00:01:38,220 --> 00:01:43,229
interpret or explain why prediction

40
00:01:41,490 --> 00:01:44,820
actually happened and this is all

41
00:01:43,229 --> 00:01:47,159
important because at the end of the day

42
00:01:44,820 --> 00:01:49,470
we're not just going to trust a number

43
00:01:47,159 --> 00:01:51,509
we just want to answer these sort of

44
00:01:49,470 --> 00:01:55,530
questions there are surrounding this

45
00:01:51,509 --> 00:01:57,299
number right but if you look at our

46
00:01:55,530 --> 00:01:59,930
history as a community a security

47
00:01:57,299 --> 00:02:02,189
community a my perspective my my

48
00:01:59,930 --> 00:02:05,700
perspective is really from a little bit

49
00:02:02,189 --> 00:02:07,169
more the academic side but we've been

50
00:02:05,700 --> 00:02:09,179
actually using and relying on machine

51
00:02:07,170 --> 00:02:11,459
learning for a number of years of course

52
00:02:09,179 --> 00:02:12,240
you know since 2012 where machine gun

53
00:02:11,459 --> 00:02:14,160
has been demo

54
00:02:12,240 --> 00:02:16,530
is that so there's been a surge even in

55
00:02:14,160 --> 00:02:18,359
academic conferences of papers even

56
00:02:16,530 --> 00:02:19,950
top-tier conferences that accept and

57
00:02:18,360 --> 00:02:21,540
work in machine learning and apply

58
00:02:19,950 --> 00:02:23,850
machining directly for cybersecurity

59
00:02:21,540 --> 00:02:25,590
from program analysis natural language

60
00:02:23,850 --> 00:02:26,280
processing and so for and so on don't

61
00:02:25,590 --> 00:02:28,800
get me wrong

62
00:02:26,280 --> 00:02:32,400
machine learning is called AI whatever

63
00:02:28,800 --> 00:02:34,680
you guys want to has been really

64
00:02:32,400 --> 00:02:36,690
promising on a number of tasks think

65
00:02:34,680 --> 00:02:38,670
about our relation systems image

66
00:02:36,690 --> 00:02:40,650
classification detection recognition

67
00:02:38,670 --> 00:02:46,049
speech recognitions and so forth and so

68
00:02:40,650 --> 00:02:47,840
on if we look at security in our context

69
00:02:46,050 --> 00:02:50,460
we've been using machine learning for

70
00:02:47,840 --> 00:02:51,870
quite some time in a number of different

71
00:02:50,460 --> 00:02:53,730
context I believe that we sort of

72
00:02:51,870 --> 00:02:56,250
started off with doing anomaly detection

73
00:02:53,730 --> 00:02:57,420
we're still doing it but then we try to

74
00:02:56,250 --> 00:02:59,310
look at machine learning in

75
00:02:57,420 --> 00:03:01,320
classification settings so supervised

76
00:02:59,310 --> 00:03:03,900
learning unsupervised learning so

77
00:03:01,320 --> 00:03:06,030
clustering and so for and so on but the

78
00:03:03,900 --> 00:03:08,130
bottom line is on all sort of different

79
00:03:06,030 --> 00:03:09,300
tasks we can imagine doing binary

80
00:03:08,130 --> 00:03:11,040
classification to detect whether

81
00:03:09,300 --> 00:03:13,410
something's militias are not try to

82
00:03:11,040 --> 00:03:15,390
identify malicious network flows to

83
00:03:13,410 --> 00:03:17,640
identify anomalies in the network

84
00:03:15,390 --> 00:03:21,839
behavior and so for and so on okay now

85
00:03:17,640 --> 00:03:23,790
if you have a look at recent approaches

86
00:03:21,840 --> 00:03:25,800
I'm an academic so we really work and

87
00:03:23,790 --> 00:03:28,200
try to explain everything and how the

88
00:03:25,800 --> 00:03:31,170
flow works so if you try to look even at

89
00:03:28,200 --> 00:03:33,420
recent publications over the past years

90
00:03:31,170 --> 00:03:37,130
in top-tier conferences where you can

91
00:03:33,420 --> 00:03:41,459
see that everyone is gonna you know

92
00:03:37,130 --> 00:03:44,490
boost boasting very high accuracy

93
00:03:41,460 --> 00:03:47,430
results so it's not uncommon to find

94
00:03:44,490 --> 00:03:51,120
papers that with tantalizing result with

95
00:03:47,430 --> 00:03:51,960
0.99 of f1 score or whatever score

96
00:03:51,120 --> 00:03:54,600
you're interested in it could be

97
00:03:51,960 --> 00:03:56,640
precision could be recall but with that

98
00:03:54,600 --> 00:03:58,440
high number so the question is quite

99
00:03:56,640 --> 00:04:01,380
natural so it seems that the problem is

100
00:03:58,440 --> 00:04:03,780
mostly solved after all we're dealing

101
00:04:01,380 --> 00:04:05,960
with statistical approaches so it is

102
00:04:03,780 --> 00:04:09,420
unlikely we're gonna get to 100 percent

103
00:04:05,960 --> 00:04:12,180
0.99 in most of the cases it's pretty

104
00:04:09,420 --> 00:04:17,120
good because it already gives us some

105
00:04:12,180 --> 00:04:19,709
performance that outperform humans at

106
00:04:17,120 --> 00:04:20,970
performing this type of tasks so the

107
00:04:19,709 --> 00:04:23,130
problem there is a there's a problem

108
00:04:20,970 --> 00:04:26,790
however and there is something that we

109
00:04:23,130 --> 00:04:29,700
as a community a security community

110
00:04:26,790 --> 00:04:31,290
I have been quite neglected and the fact

111
00:04:29,700 --> 00:04:33,390
is that usually we've been borrowing

112
00:04:31,290 --> 00:04:35,010
these practices algorithms from the

113
00:04:33,390 --> 00:04:36,240
machine learning community and most of

114
00:04:35,010 --> 00:04:37,680
the times we'll be using which we've

115
00:04:36,240 --> 00:04:40,500
been applying and deploying these

116
00:04:37,680 --> 00:04:42,330
approaches as a black box so we're

117
00:04:40,500 --> 00:04:43,830
starting now to have a pretty good

118
00:04:42,330 --> 00:04:46,380
understanding and how these approaches

119
00:04:43,830 --> 00:04:49,140
work and what are the assumptions that

120
00:04:46,380 --> 00:04:51,450
these algorithms require and one

121
00:04:49,140 --> 00:04:53,520
assumptions that most of the algorithms

122
00:04:51,450 --> 00:04:55,380
actually requires the assumptions of iid

123
00:04:53,520 --> 00:04:56,729
so I a distance for identically

124
00:04:55,380 --> 00:04:59,490
independent and identically distributed

125
00:04:56,730 --> 00:05:02,160
datasets which means that if you are in

126
00:04:59,490 --> 00:05:04,230
a supervised setting so a labeled where

127
00:05:02,160 --> 00:05:06,900
you have labels for your data and you

128
00:05:04,230 --> 00:05:09,000
want to train a system the idea is that

129
00:05:06,900 --> 00:05:10,560
the objects that belong to the training

130
00:05:09,000 --> 00:05:13,020
data set and the testing data set are

131
00:05:10,560 --> 00:05:14,790
drawn from the same distribution okay

132
00:05:13,020 --> 00:05:16,740
from the same underlying probability

133
00:05:14,790 --> 00:05:18,360
solution we might not know what is this

134
00:05:16,740 --> 00:05:21,150
distribution and it doesn't matter as

135
00:05:18,360 --> 00:05:22,680
long as this source generate objects

136
00:05:21,150 --> 00:05:24,870
from the same distribution and we're

137
00:05:22,680 --> 00:05:26,880
using this for training testing then

138
00:05:24,870 --> 00:05:28,680
these algorithms work pretty well so

139
00:05:26,880 --> 00:05:31,919
they work well in what is known as a

140
00:05:28,680 --> 00:05:34,350
stationary context in security however

141
00:05:31,919 --> 00:05:36,990
we don't have often the luxury of

142
00:05:34,350 --> 00:05:41,070
stationarity so most of the time we work

143
00:05:36,990 --> 00:05:42,900
in a non stationarity domain where the

144
00:05:41,070 --> 00:05:45,570
statistical properties of the data that

145
00:05:42,900 --> 00:05:47,609
we're trying to represent and to deal

146
00:05:45,570 --> 00:05:49,770
with change over time new threats

147
00:05:47,610 --> 00:05:51,840
variance of malware variants of

148
00:05:49,770 --> 00:05:53,250
vulnerability variance of attacks that

149
00:05:51,840 --> 00:05:55,349
you observe in a network and so for and

150
00:05:53,250 --> 00:05:57,510
so on so there is a point where a model

151
00:05:55,350 --> 00:05:59,280
will start decaying so the performance

152
00:05:57,510 --> 00:06:01,860
of a model would start decane over time

153
00:05:59,280 --> 00:06:05,820
and that's you know just life we need to

154
00:06:01,860 --> 00:06:08,520
now evaluate algorithms with this new

155
00:06:05,820 --> 00:06:11,669
mindset in mind because doing the

156
00:06:08,520 --> 00:06:14,430
typical value evaluation that would just

157
00:06:11,669 --> 00:06:16,710
allow online a couple slides doesn't

158
00:06:14,430 --> 00:06:19,770
give you the whole picture it just

159
00:06:16,710 --> 00:06:22,859
provides you just one answer of how the

160
00:06:19,770 --> 00:06:24,690
algorithm would perform in the absence

161
00:06:22,860 --> 00:06:26,040
of what is known as concentrate so

162
00:06:24,690 --> 00:06:28,500
constant drifting whenever you have a

163
00:06:26,040 --> 00:06:30,720
drifting so the testing data set drifts

164
00:06:28,500 --> 00:06:33,780
so the properties of the orbits of these

165
00:06:30,720 --> 00:06:38,780
objects drift from the training data set

166
00:06:33,780 --> 00:06:40,169
and this is a pretty endemic problem

167
00:06:38,780 --> 00:06:42,479
that

168
00:06:40,169 --> 00:06:44,849
our community now the extent to which

169
00:06:42,479 --> 00:06:47,969
this problem affects a specific security

170
00:06:44,849 --> 00:06:50,248
domain is unknown so we need to

171
00:06:47,969 --> 00:06:52,439
basically measure what is the effect of

172
00:06:50,249 --> 00:06:54,150
concentrate in a specific domain we

173
00:06:52,439 --> 00:06:56,029
might might find out that you know when

174
00:06:54,150 --> 00:06:58,739
you do malware classification on the

175
00:06:56,029 --> 00:07:00,330
Windows domain constant drift doesn't

176
00:06:58,740 --> 00:07:01,860
affect at all we might find out that

177
00:07:00,330 --> 00:07:03,508
when you do it on Android on the other

178
00:07:01,860 --> 00:07:04,740
hand it affects a lot the model

179
00:07:03,509 --> 00:07:06,840
performance and the model performance

180
00:07:04,740 --> 00:07:08,909
decay over time or you might find out

181
00:07:06,840 --> 00:07:11,719
other domains where you need to pay

182
00:07:08,909 --> 00:07:14,460
actually attention to it one thing that

183
00:07:11,719 --> 00:07:16,800
we don't usually pay attention to is

184
00:07:14,460 --> 00:07:19,799
that constant drift is really

185
00:07:16,800 --> 00:07:22,710
intertwined to the type of abstractions

186
00:07:19,800 --> 00:07:25,620
that you use to describe an object okay

187
00:07:22,710 --> 00:07:27,180
so take the example of a binary

188
00:07:25,620 --> 00:07:30,029
classification problem and you have

189
00:07:27,180 --> 00:07:31,439
malware and good we're okay and you

190
00:07:30,029 --> 00:07:33,360
train a classifier what it means to

191
00:07:31,439 --> 00:07:35,339
train a classifier it means to minimize

192
00:07:33,360 --> 00:07:36,449
an optimization problem so most of the

193
00:07:35,339 --> 00:07:38,849
times you want to minimize a loss

194
00:07:36,449 --> 00:07:40,830
function okay and you have a labor data

195
00:07:38,849 --> 00:07:44,399
set so you know how you good you're

196
00:07:40,830 --> 00:07:47,339
performing with respect to your optimal

197
00:07:44,399 --> 00:07:49,860
scenario until you minimize this loss

198
00:07:47,339 --> 00:07:52,620
function and what you want to do in this

199
00:07:49,860 --> 00:07:55,460
case you need however to represent an

200
00:07:52,620 --> 00:07:57,509
object with a proper abstraction and

201
00:07:55,460 --> 00:08:00,149
therefore you need to represent the

202
00:07:57,509 --> 00:08:02,189
object in a proper feature space okay a

203
00:08:00,149 --> 00:08:04,020
feature space you have to imagine that

204
00:08:02,189 --> 00:08:07,229
you can represent an object as a vector

205
00:08:04,020 --> 00:08:08,698
so it's a n-dimensional numbers they

206
00:08:07,229 --> 00:08:09,990
represent the object itself because a

207
00:08:08,699 --> 00:08:11,550
computer a machine an irregular it

208
00:08:09,990 --> 00:08:13,439
doesn't understand that how to represent

209
00:08:11,550 --> 00:08:16,469
a piece of software right you need to

210
00:08:13,439 --> 00:08:18,569
give them the abstraction that it makes

211
00:08:16,469 --> 00:08:20,610
sense from a mathematical perspective on

212
00:08:18,569 --> 00:08:22,499
how to reasoning about this and this

213
00:08:20,610 --> 00:08:25,319
abstraction can very could be sequences

214
00:08:22,499 --> 00:08:27,839
of bytes sequences of system calls some

215
00:08:25,319 --> 00:08:30,270
more complex a graph that captures

216
00:08:27,839 --> 00:08:32,039
dependencies between arguments in system

217
00:08:30,270 --> 00:08:34,348
causes of poor and so on all these

218
00:08:32,039 --> 00:08:37,289
different abstractions will give you a

219
00:08:34,349 --> 00:08:40,229
different implication in terms of

220
00:08:37,289 --> 00:08:42,149
accuracy of the algorithm the presence

221
00:08:40,229 --> 00:08:44,519
or in the absence of concentrate and

222
00:08:42,149 --> 00:08:45,810
support and so on so and this is

223
00:08:44,519 --> 00:08:47,550
something that we haven't been really

224
00:08:45,810 --> 00:08:49,469
neglected so we have been neglecting as

225
00:08:47,550 --> 00:08:51,599
a community so what I would like to be

226
00:08:49,470 --> 00:08:54,029
doing and the rest of the time is try to

227
00:08:51,600 --> 00:08:56,790
show you

228
00:08:54,029 --> 00:08:59,579
what are the effect of constant drift in

229
00:08:56,790 --> 00:09:01,620
this setting so in settings where there

230
00:08:59,579 --> 00:09:03,899
is non stationarity which affects

231
00:09:01,620 --> 00:09:06,810
basically most of the security

232
00:09:03,899 --> 00:09:09,509
application domains that we know of and

233
00:09:06,810 --> 00:09:12,029
also how can we perform a sound

234
00:09:09,509 --> 00:09:13,470
evaluation so it turns out that a lot of

235
00:09:12,029 --> 00:09:14,939
the time when you have a training

236
00:09:13,470 --> 00:09:16,709
testing data set people kind of you know

237
00:09:14,939 --> 00:09:18,870
use it and combine it in more creative

238
00:09:16,709 --> 00:09:20,550
ways and you end up by having good

239
00:09:18,870 --> 00:09:21,750
results not because the other is

240
00:09:20,550 --> 00:09:24,389
actually perform very well but because

241
00:09:21,750 --> 00:09:27,329
you are playing with the data set a bit

242
00:09:24,389 --> 00:09:30,240
too much and that doesn't represent

243
00:09:27,329 --> 00:09:33,089
anymore a realistic scenario so how can

244
00:09:30,240 --> 00:09:35,850
we perform a sound evaluation to remove

245
00:09:33,089 --> 00:09:37,829
any experimental bias that will actually

246
00:09:35,850 --> 00:09:40,410
affect and inflate the performance of

247
00:09:37,829 --> 00:09:42,660
our classifier so the problem I believe

248
00:09:40,410 --> 00:09:44,850
is endemic so we identify these

249
00:09:42,660 --> 00:09:46,649
experimental bias in the temporal in a

250
00:09:44,850 --> 00:09:48,569
spatial domain and I'll let you know in

251
00:09:46,649 --> 00:09:50,220
a second what it means by this but I

252
00:09:48,569 --> 00:09:53,550
believe that the problem is endemic to

253
00:09:50,220 --> 00:09:55,230
the community however as I mentioned to

254
00:09:53,550 --> 00:09:58,019
understand the extent to which this

255
00:09:55,230 --> 00:09:59,759
problem a specific application domain we

256
00:09:58,019 --> 00:10:02,339
have to measure it therefore it's hard

257
00:09:59,759 --> 00:10:04,319
for me to generalize to borrow some

258
00:10:02,339 --> 00:10:06,300
machine terminology to generalize to any

259
00:10:04,319 --> 00:10:07,500
security domains is the matter the

260
00:10:06,300 --> 00:10:11,040
methodology I'm going to be talking to

261
00:10:07,500 --> 00:10:13,470
you about is generic but the effect of

262
00:10:11,040 --> 00:10:15,870
this problem needs to be measured on a

263
00:10:13,470 --> 00:10:18,089
specific application domain and for this

264
00:10:15,870 --> 00:10:20,069
reason I focus on droid security so

265
00:10:18,089 --> 00:10:21,329
binary classification and the use case

266
00:10:20,069 --> 00:10:23,430
is just the Android binary

267
00:10:21,329 --> 00:10:25,378
classification malware detection okay

268
00:10:23,430 --> 00:10:29,040
the reason to if we're doing this is

269
00:10:25,379 --> 00:10:32,819
because there is an abundance of data

270
00:10:29,040 --> 00:10:34,319
that is available to any researcher and

271
00:10:32,819 --> 00:10:36,660
therefore we can perform sound

272
00:10:34,319 --> 00:10:38,519
experiments in particular the data set

273
00:10:36,660 --> 00:10:40,199
I'm referring to is called Andrew Zhu

274
00:10:38,519 --> 00:10:42,449
and it's maintained by the University of

275
00:10:40,199 --> 00:10:44,550
Luxembourg there are references down

276
00:10:42,449 --> 00:10:46,399
here in the slide deck if you're

277
00:10:44,550 --> 00:10:49,889
interested to get access to the data set

278
00:10:46,399 --> 00:10:51,870
and it comprises roughly up to now eight

279
00:10:49,889 --> 00:10:53,610
millions applications including

280
00:10:51,870 --> 00:10:55,470
malicious software as well so benign and

281
00:10:53,610 --> 00:10:56,790
malicious software so the data set might

282
00:10:55,470 --> 00:10:58,980
be a little bit biased because it's

283
00:10:56,790 --> 00:11:00,569
mostly West centric so there's an

284
00:10:58,980 --> 00:11:01,259
abundance of application from Google

285
00:11:00,569 --> 00:11:02,939
Play

286
00:11:01,259 --> 00:11:04,380
rather than third party markets there

287
00:11:02,939 --> 00:11:05,699
are also application from third body

288
00:11:04,380 --> 00:11:07,350
mark tech markets but just in the

289
00:11:05,699 --> 00:11:09,569
minority so this is the only by

290
00:11:07,350 --> 00:11:12,450
but as long as we are aware of a

291
00:11:09,570 --> 00:11:14,190
potential bias then it's fine so we can

292
00:11:12,450 --> 00:11:18,030
actually just take an informed decision

293
00:11:14,190 --> 00:11:23,190
and the data said that we work with

294
00:11:18,030 --> 00:11:25,110
covers from 2014 to 2016 and out of at

295
00:11:23,190 --> 00:11:27,750
that time we were talking about 5

296
00:11:25,110 --> 00:11:29,940
million of applications we didn't have

297
00:11:27,750 --> 00:11:31,860
the resource to analyze and I tell you

298
00:11:29,940 --> 00:11:33,780
in a second what it means analyze five

299
00:11:31,860 --> 00:11:35,700
million of applications in a reasonable

300
00:11:33,780 --> 00:11:37,860
amount of time so we decided to down

301
00:11:35,700 --> 00:11:40,260
sample this data set to 130,000

302
00:11:37,860 --> 00:11:43,410
applications that are scattered pretty

303
00:11:40,260 --> 00:11:46,640
much evenly throughout the timeline with

304
00:11:43,410 --> 00:11:50,160
a with an average of a 10% of malware

305
00:11:46,640 --> 00:11:52,710
throughout this entire timeline and I

306
00:11:50,160 --> 00:11:54,060
believe this is still a pretty large and

307
00:11:52,710 --> 00:11:57,450
reasonable that I said to work with and

308
00:11:54,060 --> 00:11:59,640
the idea of having 10% of malware I'll

309
00:11:57,450 --> 00:12:01,860
tell you a bit a bit later on why 10

310
00:11:59,640 --> 00:12:04,920
percent another number but basically

311
00:12:01,860 --> 00:12:06,420
this comes out from what vendors report

312
00:12:04,920 --> 00:12:07,740
and some sort of your private

313
00:12:06,420 --> 00:12:11,400
communication we had with other

314
00:12:07,740 --> 00:12:15,150
researchers believe this is the ratio of

315
00:12:11,400 --> 00:12:16,770
malware in Android that you observe and

316
00:12:15,150 --> 00:12:19,140
this is an average so there's a bit of a

317
00:12:16,770 --> 00:12:20,850
spike that it's an average if you this

318
00:12:19,140 --> 00:12:22,680
is an important number and however so if

319
00:12:20,850 --> 00:12:25,260
you want to identify and measure the

320
00:12:22,680 --> 00:12:27,839
effect of concert drift and how the

321
00:12:25,260 --> 00:12:30,210
classifier decays over time and you want

322
00:12:27,840 --> 00:12:33,180
to perform a sound evaluation you need

323
00:12:30,210 --> 00:12:34,350
to have this class ratio right because

324
00:12:33,180 --> 00:12:36,030
if you don't have the class ratio right

325
00:12:34,350 --> 00:12:37,350
you end up by inflating the results and

326
00:12:36,030 --> 00:12:41,069
I motivate you this with some

327
00:12:37,350 --> 00:12:42,900
experiments later on now once we

328
00:12:41,070 --> 00:12:45,750
identified a data set that we want to

329
00:12:42,900 --> 00:12:47,100
work with well we need to understand

330
00:12:45,750 --> 00:12:48,630
what approaches would like to evaluate

331
00:12:47,100 --> 00:12:50,550
so what are the approaches we would like

332
00:12:48,630 --> 00:12:53,040
to reason about to understand what the

333
00:12:50,550 --> 00:12:55,319
constant is a problem and how much is a

334
00:12:53,040 --> 00:12:58,260
problem and so for and so on so for this

335
00:12:55,320 --> 00:13:00,660
reason again it's a bit of a delicate

336
00:12:58,260 --> 00:13:02,400
question because one way would be to try

337
00:13:00,660 --> 00:13:04,920
to evaluate as many approaches as

338
00:13:02,400 --> 00:13:07,680
possible of course you know there's

339
00:13:04,920 --> 00:13:10,709
always a trade-off that you have to they

340
00:13:07,680 --> 00:13:11,969
have to pay attention to when it comes

341
00:13:10,710 --> 00:13:13,560
about doing this type of activities

342
00:13:11,970 --> 00:13:16,050
because you have limited resources a

343
00:13:13,560 --> 00:13:20,219
limited time basically so what we

344
00:13:16,050 --> 00:13:20,849
decided to do is just to select machine

345
00:13:20,220 --> 00:13:22,319
learning

346
00:13:20,850 --> 00:13:25,560
rhythms that were representative of

347
00:13:22,319 --> 00:13:27,269
their own category and program analysis

348
00:13:25,560 --> 00:13:29,310
to create these abstractions that I was

349
00:13:27,269 --> 00:13:31,949
telling to you about before to represent

350
00:13:29,310 --> 00:13:36,859
programs that are again representative

351
00:13:31,949 --> 00:13:40,019
of their own categories in particular I

352
00:13:36,860 --> 00:13:40,920
we focus on on two main algorithms at

353
00:13:40,019 --> 00:13:41,610
the end of the talk there's going to be

354
00:13:40,920 --> 00:13:45,689
a third one

355
00:13:41,610 --> 00:13:49,319
so both approaches and be published at

356
00:13:45,690 --> 00:13:50,880
NDS s1 in 2014 and 2017 for those of you

357
00:13:49,319 --> 00:13:52,469
are not familiar with academic

358
00:13:50,880 --> 00:13:54,959
conferences and this is one of the top

359
00:13:52,470 --> 00:13:56,790
four conferences I mean security there

360
00:13:54,959 --> 00:13:58,979
are a bunch of conferences and there are

361
00:13:56,790 --> 00:14:00,269
for compasses there are top four like we

362
00:13:58,980 --> 00:14:02,550
consider the community considered top

363
00:14:00,269 --> 00:14:04,949
four and this is a user neck security IT

364
00:14:02,550 --> 00:14:07,620
security and privacy and this says NAC

365
00:14:04,949 --> 00:14:11,729
MCCS these are the top four so this top

366
00:14:07,620 --> 00:14:13,949
work has been peer reviewed and that

367
00:14:11,730 --> 00:14:16,319
reports results that are pretty good in

368
00:14:13,949 --> 00:14:19,109
binary classification tasks on Android

369
00:14:16,319 --> 00:14:19,349
and in particular the first one and

370
00:14:19,110 --> 00:14:23,850
again

371
00:14:19,350 --> 00:14:25,139
the talk doesn't aim I don't really the

372
00:14:23,850 --> 00:14:26,699
intention my intention is not ready to

373
00:14:25,139 --> 00:14:28,110
point the finger against this piece of

374
00:14:26,699 --> 00:14:29,579
work it's just I took them as a

375
00:14:28,110 --> 00:14:32,130
representative of the state of the art

376
00:14:29,579 --> 00:14:34,620
to show what is the effect of concept

377
00:14:32,130 --> 00:14:37,199
briefed and what might happen if you

378
00:14:34,620 --> 00:14:39,449
don't enforce a proper sound evaluation

379
00:14:37,199 --> 00:14:41,459
to your classifier so the first approach

380
00:14:39,449 --> 00:14:43,339
are called Raven probably actually

381
00:14:41,459 --> 00:14:47,729
algorithm one for the rest of the talk

382
00:14:43,339 --> 00:14:49,709
it's a very simple it relies on a very

383
00:14:47,730 --> 00:14:51,269
simple automated static analysis you

384
00:14:49,709 --> 00:14:53,758
take Android application and the

385
00:14:51,269 --> 00:14:56,490
approach extracts basically strings that

386
00:14:53,759 --> 00:14:57,660
are meaningful there it could be URLs it

387
00:14:56,490 --> 00:15:00,380
could be strings are embedded in the

388
00:14:57,660 --> 00:15:02,430
bytecode or it could be API is that are

389
00:15:00,380 --> 00:15:04,980
in the bytecode of the Android

390
00:15:02,430 --> 00:15:06,989
application okay and then you encode

391
00:15:04,980 --> 00:15:09,300
this information as a big vector so

392
00:15:06,990 --> 00:15:12,449
let's say that we identify a hundred

393
00:15:09,300 --> 00:15:14,699
different of such strings then the bit

394
00:15:12,449 --> 00:15:16,170
vector for this specific application the

395
00:15:14,699 --> 00:15:17,399
way that I represent this application

396
00:15:16,170 --> 00:15:19,079
this abstraction is just a vector of a

397
00:15:17,399 --> 00:15:21,029
hundred dimensions and you have a one if

398
00:15:19,079 --> 00:15:22,258
you have that specific string or a zero

399
00:15:21,029 --> 00:15:23,730
if you don't have it now you have to do

400
00:15:22,259 --> 00:15:25,470
this for a hundred thirty thousand

401
00:15:23,730 --> 00:15:27,839
applications and you end up with a very

402
00:15:25,470 --> 00:15:32,279
large feature space of a hundred thirty

403
00:15:27,839 --> 00:15:34,620
thousand features that is very sparse so

404
00:15:32,279 --> 00:15:37,290
most of the

405
00:15:34,620 --> 00:15:39,030
our zero and just you know you have a

406
00:15:37,290 --> 00:15:41,910
few ones here and there this is a very

407
00:15:39,030 --> 00:15:43,170
large future space okay but it's a very

408
00:15:41,910 --> 00:15:46,530
simple static analysis so very

409
00:15:43,170 --> 00:15:49,319
lightweight it doesn't aim to capture

410
00:15:46,530 --> 00:15:51,390
any dependency between any of the

411
00:15:49,320 --> 00:15:53,910
actions so if you see that there is

412
00:15:51,390 --> 00:15:55,410
access to the IMEI so personal

413
00:15:53,910 --> 00:15:56,579
identifying information and you see

414
00:15:55,410 --> 00:15:59,490
there's a natural communication after

415
00:15:56,580 --> 00:16:02,130
after words you cannot really say that

416
00:15:59,490 --> 00:16:03,510
that information has been leaked through

417
00:16:02,130 --> 00:16:05,100
the network communication because

418
00:16:03,510 --> 00:16:06,870
there's no data flow analysis that tells

419
00:16:05,100 --> 00:16:08,460
you this this information so this could

420
00:16:06,870 --> 00:16:10,380
be just completely coincidental or there

421
00:16:08,460 --> 00:16:12,270
could be a leak okay but it's a very

422
00:16:10,380 --> 00:16:13,950
lightweight static analysis that you can

423
00:16:12,270 --> 00:16:15,329
do and this gives you an abstraction the

424
00:16:13,950 --> 00:16:18,540
algorithm the machine learning algorithm

425
00:16:15,330 --> 00:16:20,730
relies on is a simple linear SVM so it's

426
00:16:18,540 --> 00:16:21,900
something your classifier okay and the

427
00:16:20,730 --> 00:16:26,090
results are pretty good I believe that

428
00:16:21,900 --> 00:16:30,900
they report roughly 95% of precision at

429
00:16:26,090 --> 00:16:32,460
1% of positive rate I remember you know

430
00:16:30,900 --> 00:16:34,800
just by heart but I will see a couple of

431
00:16:32,460 --> 00:16:37,190
graphs later on second approach is

432
00:16:34,800 --> 00:16:39,329
different so it's three years after and

433
00:16:37,190 --> 00:16:42,150
it relies on a more sophisticated

434
00:16:39,330 --> 00:16:44,400
program analysis in particular it builds

435
00:16:42,150 --> 00:16:47,160
a cool graph out of every application

436
00:16:44,400 --> 00:16:49,500
and it encodes the Skoll graph in a

437
00:16:47,160 --> 00:16:51,030
Markov chain we're features so these

438
00:16:49,500 --> 00:16:54,030
dimensions that I mentioned to you about

439
00:16:51,030 --> 00:16:55,650
to represent specific programs are the

440
00:16:54,030 --> 00:16:58,439
probability of transition probabilities

441
00:16:55,650 --> 00:17:00,180
in this graph okay and you end up with

442
00:16:58,440 --> 00:17:01,860
again another vector with some numbers

443
00:17:00,180 --> 00:17:05,040
and this is a way that you represent

444
00:17:01,860 --> 00:17:06,300
this application and you do this for you

445
00:17:05,040 --> 00:17:09,200
know all the application you get up with

446
00:17:06,300 --> 00:17:14,940
your feature space the algorithm that

447
00:17:09,200 --> 00:17:16,890
algorithm Android relies on is a random

448
00:17:14,940 --> 00:17:18,690
forest so it's a it's a it's a tree

449
00:17:16,890 --> 00:17:19,940
based algorithms the decision tree based

450
00:17:18,690 --> 00:17:23,160
argument so completely different from

451
00:17:19,940 --> 00:17:25,290
SVM okay and at the end of the talk I

452
00:17:23,160 --> 00:17:27,209
also show you something else that relies

453
00:17:25,290 --> 00:17:29,730
on deep learning because it's the third

454
00:17:27,209 --> 00:17:32,010
category in a way of algorithm that we

455
00:17:29,730 --> 00:17:33,660
can actually rely on to identify these

456
00:17:32,010 --> 00:17:35,670
things of course there are others but

457
00:17:33,660 --> 00:17:36,990
they haven't been really explored in the

458
00:17:35,670 --> 00:17:40,230
security community so we didn't really

459
00:17:36,990 --> 00:17:42,630
want to add something that I wasn't an

460
00:17:40,230 --> 00:17:45,320
approach that hasn't been published yet

461
00:17:42,630 --> 00:17:47,610
or hasn't been vetted by the community

462
00:17:45,320 --> 00:17:48,480
so when it comes about temporal bias

463
00:17:47,610 --> 00:17:52,409
it's pretty simple

464
00:17:48,480 --> 00:17:54,480
but I'm sure that everyone that has ever

465
00:17:52,410 --> 00:17:56,309
used machine learning in any context

466
00:17:54,480 --> 00:17:57,900
that relies on so it needs to evaluate

467
00:17:56,309 --> 00:18:00,330
the model and we're talking about in

468
00:17:57,900 --> 00:18:02,490
this talk about supervised learning so

469
00:18:00,330 --> 00:18:04,710
we have a label data set a label train

470
00:18:02,490 --> 00:18:06,030
data set and a label testing data set of

471
00:18:04,710 --> 00:18:07,290
course you know you have to train the

472
00:18:06,030 --> 00:18:08,820
model without looking at the testing

473
00:18:07,290 --> 00:18:10,860
that I said but you evaluate the model

474
00:18:08,820 --> 00:18:13,020
on the testing data set and you have the

475
00:18:10,860 --> 00:18:15,840
label so you you know how good your

476
00:18:13,020 --> 00:18:18,080
performance basically so a common way to

477
00:18:15,840 --> 00:18:20,399
evaluate this algorithm is just to use a

478
00:18:18,080 --> 00:18:24,418
best practice that's known as a k-fold

479
00:18:20,400 --> 00:18:26,250
cross-validation and it works as in this

480
00:18:24,419 --> 00:18:28,860
way it's very simple so you take your

481
00:18:26,250 --> 00:18:32,370
whole data set you partition it in k

482
00:18:28,860 --> 00:18:35,250
folds in k chunks you train your model

483
00:18:32,370 --> 00:18:37,350
on K minus 1 chunk in all but one and

484
00:18:35,250 --> 00:18:39,809
you test your model on the cake chunk

485
00:18:37,350 --> 00:18:43,020
and you repeat this up this this process

486
00:18:39,809 --> 00:18:45,540
for K times so the reason is that and

487
00:18:43,020 --> 00:18:47,309
you average the result so what is the

488
00:18:45,540 --> 00:18:50,250
reason to do this the reason is that you

489
00:18:47,309 --> 00:18:52,049
would like to hinder the possibility

490
00:18:50,250 --> 00:18:53,520
that whenever you have your entire data

491
00:18:52,049 --> 00:18:55,260
set and you have to split it in training

492
00:18:53,520 --> 00:18:59,309
and testing you have a particularly

493
00:18:55,260 --> 00:19:01,470
lucky split where you classify performs

494
00:18:59,309 --> 00:19:03,809
really well because you're doing this

495
00:19:01,470 --> 00:19:05,549
all the time and every point is used as

496
00:19:03,809 --> 00:19:07,410
a training and as a testing just not at

497
00:19:05,549 --> 00:19:09,960
the same time you're just lowering this

498
00:19:07,410 --> 00:19:13,620
possibility of overfitting the model and

499
00:19:09,960 --> 00:19:15,750
of having a very lucky partition the

500
00:19:13,620 --> 00:19:17,939
problem is that this works pretty well

501
00:19:15,750 --> 00:19:20,370
and I'm not advocating of not using it

502
00:19:17,940 --> 00:19:22,320
so we have absolutely use k-fold

503
00:19:20,370 --> 00:19:25,080
cross-validation or holdout validation

504
00:19:22,320 --> 00:19:28,168
but the problem is that this works very

505
00:19:25,080 --> 00:19:30,178
well assuming that your data set is

506
00:19:28,169 --> 00:19:32,190
representative of the population and it

507
00:19:30,179 --> 00:19:34,380
doesn't change over time so again that

508
00:19:32,190 --> 00:19:36,150
the entire data set is stationary if

509
00:19:34,380 --> 00:19:38,250
you're working in the context where

510
00:19:36,150 --> 00:19:40,350
there's known stationarity well

511
00:19:38,250 --> 00:19:42,030
eventually whatever results you have out

512
00:19:40,350 --> 00:19:43,949
of this careful cross-validation is not

513
00:19:42,030 --> 00:19:46,350
realistic anymore why it is not

514
00:19:43,950 --> 00:19:48,990
realistic because if you do unroll your

515
00:19:46,350 --> 00:19:50,939
data set on a time line okay you have a

516
00:19:48,990 --> 00:19:53,100
timestamp data set every object is time

517
00:19:50,940 --> 00:19:55,770
step with with a with some information

518
00:19:53,100 --> 00:19:57,840
if you unroll this on a time line I

519
00:19:55,770 --> 00:20:00,929
mentioned that you partition the data

520
00:19:57,840 --> 00:20:02,159
set in folds and you train on K minus 1

521
00:20:00,929 --> 00:20:04,440
and came on the test one

522
00:20:02,160 --> 00:20:06,960
it might happen that you actually train

523
00:20:04,440 --> 00:20:09,750
with knowledge from the future in this

524
00:20:06,960 --> 00:20:10,850
timeline and you test from objects that

525
00:20:09,750 --> 00:20:14,910
are coming from the past

526
00:20:10,850 --> 00:20:17,070
that's cheating if you're deploying the

527
00:20:14,910 --> 00:20:18,480
model right if you are at the point that

528
00:20:17,070 --> 00:20:20,100
you deploy the model and you work in

529
00:20:18,480 --> 00:20:22,169
this non stationarity so that things

530
00:20:20,100 --> 00:20:23,189
might change over time well you cannot

531
00:20:22,169 --> 00:20:25,679
train the model with things that you

532
00:20:23,190 --> 00:20:26,160
haven't even seen yet you have just to

533
00:20:25,679 --> 00:20:27,840
reason about

534
00:20:26,160 --> 00:20:28,830
okay this is my entire data set I train

535
00:20:27,840 --> 00:20:32,010
the model with this piece of information

536
00:20:28,830 --> 00:20:34,350
and I deploy it from here onwards I see

537
00:20:32,010 --> 00:20:36,690
new objects now some of these objects

538
00:20:34,350 --> 00:20:38,370
might actually be drawn from the same

539
00:20:36,690 --> 00:20:40,590
distribution that I train my model with

540
00:20:38,370 --> 00:20:43,289
and that's fine so this will give you

541
00:20:40,590 --> 00:20:45,570
the average performance of the model in

542
00:20:43,289 --> 00:20:47,070
the absence of concept drift but for the

543
00:20:45,570 --> 00:20:49,980
model for the object that will start

544
00:20:47,070 --> 00:20:51,270
drifting away so will start being

545
00:20:49,980 --> 00:20:52,770
represented with properties that are

546
00:20:51,270 --> 00:20:56,400
different from the distribution that you

547
00:20:52,770 --> 00:20:58,980
train your model with then this objects

548
00:20:56,400 --> 00:21:01,350
will start being misclassified by the

549
00:20:58,980 --> 00:21:04,380
classifier because they just belong to a

550
00:21:01,350 --> 00:21:06,330
different distribution but if you do

551
00:21:04,380 --> 00:21:08,429
k-fold cross-validation only then you

552
00:21:06,330 --> 00:21:10,139
are mixing up the time line so you're

553
00:21:08,429 --> 00:21:11,940
using knowledge from the future and

554
00:21:10,140 --> 00:21:12,990
testing on the past now there is an

555
00:21:11,940 --> 00:21:14,429
interesting question about whether we

556
00:21:12,990 --> 00:21:16,289
should be forgetting about the past but

557
00:21:14,429 --> 00:21:20,190
I'll keep this as a maybe follow-up

558
00:21:16,289 --> 00:21:25,100
question or for online conversation so

559
00:21:20,190 --> 00:21:27,510
if you are basically relying only on

560
00:21:25,100 --> 00:21:28,980
k-fold cross-validation this is what

561
00:21:27,510 --> 00:21:31,679
happens so you're going to be using

562
00:21:28,980 --> 00:21:34,200
knowledge from the future to test

563
00:21:31,679 --> 00:21:36,000
potentially from the past and this is of

564
00:21:34,200 --> 00:21:38,429
course you know provides an inflated

565
00:21:36,000 --> 00:21:40,500
result on your evaluation because it's

566
00:21:38,429 --> 00:21:42,270
not realistic of whenever you be

567
00:21:40,500 --> 00:21:44,520
deploying the algorithm so because it

568
00:21:42,270 --> 00:21:46,200
doesn't capture the trend the natural

569
00:21:44,520 --> 00:21:49,080
train of concept drift that you will be

570
00:21:46,200 --> 00:21:51,059
observing over time and the question is

571
00:21:49,080 --> 00:21:53,428
understanding given an algorithm and a

572
00:21:51,059 --> 00:21:56,549
specific program analysis so feature

573
00:21:53,429 --> 00:21:58,710
space obstruction what is the rapidity

574
00:21:56,549 --> 00:22:00,179
that you decay over time so this is what

575
00:21:58,710 --> 00:22:02,100
we're trying to measure so if you

576
00:22:00,179 --> 00:22:05,309
violate this we call this constraint we

577
00:22:02,100 --> 00:22:06,750
call C 1 and violation use future

578
00:22:05,309 --> 00:22:09,780
knowledge in training so results might

579
00:22:06,750 --> 00:22:11,610
be inflated there is a second violation

580
00:22:09,780 --> 00:22:15,960
in the temporal domain there is a little

581
00:22:11,610 --> 00:22:20,219
bit more subtle the second the second

582
00:22:15,960 --> 00:22:22,170
violation basically suggests that you

583
00:22:20,220 --> 00:22:23,550
should be when you look at the class

584
00:22:22,170 --> 00:22:24,930
ratio so we're working with a binary

585
00:22:23,550 --> 00:22:28,649
classification case so we have good

586
00:22:24,930 --> 00:22:30,030
where and malware when we do extract you

587
00:22:28,650 --> 00:22:31,860
split the time and try data setting

588
00:22:30,030 --> 00:22:35,160
training and testing that's fine but

589
00:22:31,860 --> 00:22:37,139
once one you actually use the data to

590
00:22:35,160 --> 00:22:40,920
train your model you should actually be

591
00:22:37,140 --> 00:22:43,260
using benign and malware that are coming

592
00:22:40,920 --> 00:22:46,710
from the same time slice so there might

593
00:22:43,260 --> 00:22:49,890
be a situation in which you train with

594
00:22:46,710 --> 00:22:53,070
benign software that is from 2013 and

595
00:22:49,890 --> 00:22:56,880
then you train the model with malicious

596
00:22:53,070 --> 00:22:59,220
softer that is from 2015 okay so in that

597
00:22:56,880 --> 00:23:01,500
case and maybe you test your model with

598
00:22:59,220 --> 00:23:06,330
software that comes from 2016 onwards

599
00:23:01,500 --> 00:23:08,190
okay so c1 is c1 holds in this in this

600
00:23:06,330 --> 00:23:09,780
scenario but the problem is that when I

601
00:23:08,190 --> 00:23:11,970
classify it when you train a classifier

602
00:23:09,780 --> 00:23:13,980
the co-ceo train tries to solve an

603
00:23:11,970 --> 00:23:17,520
optimization problem and because you're

604
00:23:13,980 --> 00:23:19,710
giving the the two different classes

605
00:23:17,520 --> 00:23:21,540
benign and malware there are far away in

606
00:23:19,710 --> 00:23:23,220
the time line there's no guarantee that

607
00:23:21,540 --> 00:23:26,010
you're actually having a machine

608
00:23:23,220 --> 00:23:28,380
learning algorithm the learns good and

609
00:23:26,010 --> 00:23:30,390
bad you might have a machine learning

610
00:23:28,380 --> 00:23:34,110
algorithm the picks up artifact and

611
00:23:30,390 --> 00:23:37,680
learns old from new so there are api's

612
00:23:34,110 --> 00:23:40,199
that didn't exist in 2013 but there are

613
00:23:37,680 --> 00:23:42,090
in 2015 some of them are deprecated and

614
00:23:40,200 --> 00:23:44,430
so forth on the classifier might

615
00:23:42,090 --> 00:23:47,820
actually be picking up those indicator

616
00:23:44,430 --> 00:23:49,830
as the main one to derive and draw the

617
00:23:47,820 --> 00:23:51,659
decision boundary and actually separate

618
00:23:49,830 --> 00:23:53,699
the two classes but once you deploy the

619
00:23:51,660 --> 00:23:55,920
model again you observe objects that

620
00:23:53,700 --> 00:23:59,280
might fit into that time line or not so

621
00:23:55,920 --> 00:24:02,790
you add again a little bit of bias but

622
00:23:59,280 --> 00:24:05,870
it's an artificial bias that you add in

623
00:24:02,790 --> 00:24:09,300
the evaluation so ideally you should be

624
00:24:05,870 --> 00:24:10,679
drawing the objects of the different

625
00:24:09,300 --> 00:24:13,860
classes you train your classifier with

626
00:24:10,680 --> 00:24:15,840
from the same time slice okay and this

627
00:24:13,860 --> 00:24:17,639
is a way to sort of you know visually

628
00:24:15,840 --> 00:24:19,980
represent what I just mentioned in the

629
00:24:17,640 --> 00:24:21,540
KFOR cross-validation everything all the

630
00:24:19,980 --> 00:24:23,310
data set is used for training and for

631
00:24:21,540 --> 00:24:24,750
testing not at the same time but

632
00:24:23,310 --> 00:24:26,040
eventually everything so you end up by

633
00:24:24,750 --> 00:24:27,620
using knowledge from the future and

634
00:24:26,040 --> 00:24:29,668
therefore you might inflate your result

635
00:24:27,620 --> 00:24:31,918
when you have

636
00:24:29,669 --> 00:24:33,690
so there is a first form of temporary

637
00:24:31,919 --> 00:24:35,039
consistency that of course this

638
00:24:33,690 --> 00:24:37,289
shouldn't be happening because this is

639
00:24:35,039 --> 00:24:40,469
even even worse so you're only using

640
00:24:37,289 --> 00:24:43,469
data from the future and you test it in

641
00:24:40,469 --> 00:24:47,429
the past so this is of course a scenario

642
00:24:43,469 --> 00:24:50,669
that this knot is not possible the

643
00:24:47,429 --> 00:24:52,769
question here would really be is this

644
00:24:50,669 --> 00:24:55,320
data from the past something that I can

645
00:24:52,769 --> 00:24:58,529
still observe once I deploy the model if

646
00:24:55,320 --> 00:24:59,820
there is the answer then of course you

647
00:24:58,529 --> 00:25:01,979
know it makes sense to perform that kind

648
00:24:59,820 --> 00:25:03,689
of evaluation but if you don't know the

649
00:25:01,979 --> 00:25:05,429
likelihood of that to happen then of

650
00:25:03,690 --> 00:25:08,759
course you know this evaluation is again

651
00:25:05,429 --> 00:25:10,649
a bit biased the temporal inconsistent

652
00:25:08,759 --> 00:25:13,379
class ratio good or malware this shows

653
00:25:10,649 --> 00:25:16,129
you that although all the training

654
00:25:13,379 --> 00:25:18,718
points are on the left-hand side are

655
00:25:16,129 --> 00:25:20,609
antecedent to the testing point or at

656
00:25:18,719 --> 00:25:22,950
least overlapping with those on the

657
00:25:20,609 --> 00:25:24,178
right hand side you have malware and

658
00:25:22,950 --> 00:25:25,950
good where there are drawn from two

659
00:25:24,179 --> 00:25:27,599
different time slices so again this

660
00:25:25,950 --> 00:25:29,609
might actually induce the classifier to

661
00:25:27,599 --> 00:25:31,168
pick up artifacts and the promise that

662
00:25:29,609 --> 00:25:32,549
again we don't really know so most of

663
00:25:31,169 --> 00:25:34,469
the times what is the classification

664
00:25:32,549 --> 00:25:36,389
doing underneath depending on the

665
00:25:34,469 --> 00:25:40,999
abstraction that you are using so to try

666
00:25:36,389 --> 00:25:43,559
to remove bias you should actually be

667
00:25:40,999 --> 00:25:46,559
training a classifier with a data set

668
00:25:43,559 --> 00:25:48,269
that is split in this way so Google and

669
00:25:46,559 --> 00:25:51,418
malware are drawn from the same time

670
00:25:48,269 --> 00:25:55,200
slice and all the training is antecedent

671
00:25:51,419 --> 00:25:57,149
to all the testing again I'm not saying

672
00:25:55,200 --> 00:25:58,349
that we should get rid of k-fold

673
00:25:57,149 --> 00:26:00,178
cross-validation careful

674
00:25:58,349 --> 00:26:02,249
cross-validation provides you provides

675
00:26:00,179 --> 00:26:04,709
an approximation of the performance of

676
00:26:02,249 --> 00:26:07,709
the classifier in the absence of concept

677
00:26:04,709 --> 00:26:09,719
drift so whenever you deploy your

678
00:26:07,709 --> 00:26:12,869
classifier and the objects that you

679
00:26:09,719 --> 00:26:14,219
observe in real life in your feed still

680
00:26:12,869 --> 00:26:16,019
fall within the distribution that you

681
00:26:14,219 --> 00:26:17,369
train the classifier with that's good

682
00:26:16,019 --> 00:26:18,959
the performance that you should be

683
00:26:17,369 --> 00:26:21,228
expecting is the one that you were

684
00:26:18,959 --> 00:26:23,339
training with k-fold cross-validation

685
00:26:21,229 --> 00:26:25,679
but whenever these options will start

686
00:26:23,339 --> 00:26:27,479
drifting off so falling in another

687
00:26:25,679 --> 00:26:29,429
distribution different from the one is

688
00:26:27,479 --> 00:26:31,019
trying to classify with then the

689
00:26:29,429 --> 00:26:33,509
question is okay how quickly do I the

690
00:26:31,019 --> 00:26:35,099
Chi decay over time and how quickly do I

691
00:26:33,509 --> 00:26:36,749
need to do something like retrain a

692
00:26:35,099 --> 00:26:38,099
classifier for instance or using other

693
00:26:36,749 --> 00:26:43,619
techniques that I'll be mentioning in a

694
00:26:38,099 --> 00:26:47,009
second so this is actually what happens

695
00:26:43,619 --> 00:26:50,549
in terms of decay of performance of the

696
00:26:47,009 --> 00:26:55,769
classifier in this context so the graph

697
00:26:50,549 --> 00:26:59,908
shows a plot where precision recall and

698
00:26:55,769 --> 00:27:01,799
f1 are shown and the decay over time so

699
00:26:59,909 --> 00:27:04,200
the setting is this one we train the

700
00:27:01,799 --> 00:27:05,809
model on one year worth of data and then

701
00:27:04,200 --> 00:27:09,119
we test it on the subsequent two years

702
00:27:05,809 --> 00:27:10,559
this is just an example you can from an

703
00:27:09,119 --> 00:27:12,570
operational perspective you might don't

704
00:27:10,559 --> 00:27:13,918
don't want to train it on a year worth

705
00:27:12,570 --> 00:27:15,299
of data you might just want to train it

706
00:27:13,919 --> 00:27:17,759
on a month worth of data on three months

707
00:27:15,299 --> 00:27:20,070
worth of data this is just an example an

708
00:27:17,759 --> 00:27:22,769
illustrative example so we train with

709
00:27:20,070 --> 00:27:25,830
one year worth of data test in two years

710
00:27:22,769 --> 00:27:29,190
over two years subsequent two years and

711
00:27:25,830 --> 00:27:32,279
this is algorithm one so linear SVM very

712
00:27:29,190 --> 00:27:34,230
light with static analysis again so the

713
00:27:32,279 --> 00:27:36,059
two are intertwined so we tend to reason

714
00:27:34,230 --> 00:27:38,489
only by looking at the machine learning

715
00:27:36,059 --> 00:27:41,129
but the two are intertwined and you see

716
00:27:38,489 --> 00:27:43,619
how you have a different graph for the

717
00:27:41,129 --> 00:27:46,949
other algorithm in fact so and here it

718
00:27:43,619 --> 00:27:50,158
shows that you know some if you pick the

719
00:27:46,950 --> 00:27:53,429
blue line there's a harmonized procedure

720
00:27:50,159 --> 00:27:54,809
and recall so the f1 score you see that

721
00:27:53,429 --> 00:27:56,639
you know of course it goes a bit up and

722
00:27:54,809 --> 00:27:58,249
down and Y up and down because again

723
00:27:56,639 --> 00:28:00,570
some of the objects in that time slice

724
00:27:58,249 --> 00:28:02,340
do not drift too much from the training

725
00:28:00,570 --> 00:28:04,259
model while other eventually will start

726
00:28:02,340 --> 00:28:06,899
drifting a lot you can see that there is

727
00:28:04,259 --> 00:28:09,720
anyway a downtrend so the performance of

728
00:28:06,899 --> 00:28:11,428
the classifier over time will decay and

729
00:28:09,720 --> 00:28:14,159
this is what you would be expecting

730
00:28:11,429 --> 00:28:16,799
because it's a non stationary context if

731
00:28:14,159 --> 00:28:18,149
you compare that anyway and and you know

732
00:28:16,799 --> 00:28:20,580
we have we started with something a

733
00:28:18,149 --> 00:28:25,918
little bit more below 0.8 and it goes

734
00:28:20,580 --> 00:28:28,139
down to 0.4 something like that if you

735
00:28:25,919 --> 00:28:30,629
look at the dashed red lines however the

736
00:28:28,139 --> 00:28:31,799
dashed red line is the performance of

737
00:28:30,629 --> 00:28:33,480
the classifier in a k-fold

738
00:28:31,799 --> 00:28:38,129
cross-validation setting so something

739
00:28:33,480 --> 00:28:41,129
like 0.94 of f1 score and this is again

740
00:28:38,129 --> 00:28:43,799
the performance that you will obtain in

741
00:28:41,129 --> 00:28:45,779
the absence of concept drift so if the

742
00:28:43,799 --> 00:28:47,070
points that you have observed do not

743
00:28:45,779 --> 00:28:49,049
drift from the model you train a

744
00:28:47,070 --> 00:28:50,668
classifier with that is the performance

745
00:28:49,049 --> 00:28:52,769
that you should be expecting if the

746
00:28:50,669 --> 00:28:54,029
evaluation is if there are no other bias

747
00:28:52,769 --> 00:28:56,640
in the evaluation and the other bias

748
00:28:54,029 --> 00:28:58,860
that we have to look at is spatial bias

749
00:28:56,640 --> 00:29:01,320
but if you then start encountering

750
00:28:58,860 --> 00:29:03,719
consider if this is how quickly the

751
00:29:01,320 --> 00:29:05,610
classifier decays over time and this is

752
00:29:03,720 --> 00:29:07,740
informative because at that point you

753
00:29:05,610 --> 00:29:10,678
can understand you know I cannot accept

754
00:29:07,740 --> 00:29:12,690
a performance that goes below zero point

755
00:29:10,679 --> 00:29:14,760
seven and if that decay is depending on

756
00:29:12,690 --> 00:29:16,320
your operational situation well at that

757
00:29:14,760 --> 00:29:19,500
point you know that you have just a

758
00:29:16,320 --> 00:29:21,780
couple of months that you can live with

759
00:29:19,500 --> 00:29:23,460
this model after which you need to do

760
00:29:21,780 --> 00:29:25,320
something you need other to retrain

761
00:29:23,460 --> 00:29:27,360
incrementally but there's a cost

762
00:29:25,320 --> 00:29:29,250
associated to real Abel the object you

763
00:29:27,360 --> 00:29:30,959
might rely on classification with

764
00:29:29,250 --> 00:29:32,490
rejection or active learning there's all

765
00:29:30,960 --> 00:29:34,799
techniques that you can use to actually

766
00:29:32,490 --> 00:29:37,260
try to reduce the performance of the

767
00:29:34,799 --> 00:29:38,400
classifier over time and I'm not sure

768
00:29:37,260 --> 00:29:43,320
whether I have time to talk about these

769
00:29:38,400 --> 00:29:45,350
but if presentations are given it's in

770
00:29:43,320 --> 00:29:47,850
the slides but there is a paper that

771
00:29:45,350 --> 00:29:49,559
describes at length all of our and

772
00:29:47,850 --> 00:29:51,510
saying and there is a lot of stuff there

773
00:29:49,559 --> 00:29:56,178
about active learning mental learning

774
00:29:51,510 --> 00:29:58,200
classification of rejection so okay so

775
00:29:56,179 --> 00:30:00,540
these are the two temporal constraints

776
00:29:58,200 --> 00:30:02,460
that we must enforce if we want to

777
00:30:00,540 --> 00:30:04,620
understand how our classifier would

778
00:30:02,460 --> 00:30:06,150
perform in the presence of concert drift

779
00:30:04,620 --> 00:30:09,418
okay if you're not interested about that

780
00:30:06,150 --> 00:30:11,400
that is fine however even if you are not

781
00:30:09,419 --> 00:30:13,140
interested in concept drift and how the

782
00:30:11,400 --> 00:30:15,960
Crucifier performs the case over time

783
00:30:13,140 --> 00:30:18,840
there's still another dimension that you

784
00:30:15,960 --> 00:30:21,450
have to be looking at to make sure that

785
00:30:18,840 --> 00:30:23,549
your evaluation is not biased from an

786
00:30:21,450 --> 00:30:25,830
experimental perspective and this is

787
00:30:23,549 --> 00:30:28,889
something that is called splash special

788
00:30:25,830 --> 00:30:31,168
values that we propose in this work the

789
00:30:28,890 --> 00:30:33,870
reason is very simple so when you have

790
00:30:31,169 --> 00:30:35,490
your data set you split in training and

791
00:30:33,870 --> 00:30:37,168
testing and let's say that again is a

792
00:30:35,490 --> 00:30:39,390
binary classification problem so you

793
00:30:37,169 --> 00:30:41,820
have to decide what is the class ratio

794
00:30:39,390 --> 00:30:43,710
so some in some cases you cannot decide

795
00:30:41,820 --> 00:30:46,320
it really but in other cases by easy to

796
00:30:43,710 --> 00:30:49,830
the side so if you have a data set that

797
00:30:46,320 --> 00:30:51,149
you're available like under zoo you can

798
00:30:49,830 --> 00:30:54,059
download application and you have to

799
00:30:51,150 --> 00:30:55,470
decide okay how many benign I want in my

800
00:30:54,059 --> 00:30:57,870
in my training that I said how many

801
00:30:55,470 --> 00:30:59,490
malicious malware I want in my training

802
00:30:57,870 --> 00:31:02,760
data set so you have to decide a class

803
00:30:59,490 --> 00:31:04,679
ratio okay this is fine for the training

804
00:31:02,760 --> 00:31:06,300
for training purposes you can play with

805
00:31:04,679 --> 00:31:07,860
the class ratio as you wish there are

806
00:31:06,300 --> 00:31:09,418
implications because you're making the

807
00:31:07,860 --> 00:31:10,539
algorithm more sensitive towards one

808
00:31:09,419 --> 00:31:13,580
class or not

809
00:31:10,539 --> 00:31:15,950
but the question is can you do the same

810
00:31:13,580 --> 00:31:18,500
in the testing data set can you actually

811
00:31:15,950 --> 00:31:23,000
play with the class ratio in the testing

812
00:31:18,500 --> 00:31:24,529
data set and the problem is that if you

813
00:31:23,000 --> 00:31:27,139
play with the class ratio in the testing

814
00:31:24,529 --> 00:31:30,320
data set well you're just playing with

815
00:31:27,139 --> 00:31:32,979
reality so the class eration the testing

816
00:31:30,320 --> 00:31:36,350
that that's the testing data set is

817
00:31:32,980 --> 00:31:40,100
representative of what you observe the

818
00:31:36,350 --> 00:31:43,340
feed that you get real-life okay so new

819
00:31:40,100 --> 00:31:45,740
attacks that you receive new network

820
00:31:43,340 --> 00:31:48,439
flows that you receive so those cannot

821
00:31:45,740 --> 00:31:51,289
be artificially manipulated when you

822
00:31:48,440 --> 00:31:53,240
perform this experiments in lab so you

823
00:31:51,289 --> 00:31:55,460
need to understand in a way what is the

824
00:31:53,240 --> 00:31:58,279
natural class ratio of the problem that

825
00:31:55,460 --> 00:32:01,250
you're working with and again for

826
00:31:58,279 --> 00:32:03,409
Android malware classification it seems

827
00:32:01,250 --> 00:32:06,409
that it's reasonable to consider 10

828
00:32:03,409 --> 00:32:08,299
percent of malware and 90% of benign

829
00:32:06,409 --> 00:32:10,730
software for other domains Windows might

830
00:32:08,299 --> 00:32:12,470
be the opposite wait for malicious URL

831
00:32:10,730 --> 00:32:14,450
it could be a different ratio as well

832
00:32:12,470 --> 00:32:15,590
but you need to understand what is it

833
00:32:14,450 --> 00:32:17,870
the phenomenon you're dealing with

834
00:32:15,590 --> 00:32:19,850
because if you play with the class

835
00:32:17,870 --> 00:32:21,889
eration the test analysis you inflict

836
00:32:19,850 --> 00:32:25,399
the result and the example that I have

837
00:32:21,889 --> 00:32:27,789
here it's quiet it's quite simple to

838
00:32:25,399 --> 00:32:32,059
understand let's assume that you have a

839
00:32:27,789 --> 00:32:34,370
classifier that it's reasonably good at

840
00:32:32,059 --> 00:32:38,480
detecting malware but it raises a lot of

841
00:32:34,370 --> 00:32:40,070
false positives okay which means that in

842
00:32:38,480 --> 00:32:42,110
this case if you look at the precision

843
00:32:40,070 --> 00:32:44,269
metrics precision is the finest true

844
00:32:42,110 --> 00:32:47,299
positive over true positive plus plus

845
00:32:44,269 --> 00:32:49,850
false positives okay so if you have too

846
00:32:47,299 --> 00:32:53,720
many false positives then precision goes

847
00:32:49,850 --> 00:32:58,039
down now what I can do in my evaluation

848
00:32:53,720 --> 00:33:01,519
is to craft a testing data set with a

849
00:32:58,039 --> 00:33:03,259
class ratio of a lot more malware so I

850
00:33:01,519 --> 00:33:05,389
know that my classifier makes mistakes

851
00:33:03,259 --> 00:33:09,019
in misclassifying benign software as

852
00:33:05,389 --> 00:33:11,959
malware so what I can do in my testing

853
00:33:09,019 --> 00:33:13,549
data set is just give less benign

854
00:33:11,960 --> 00:33:15,679
software with respect to malware so I

855
00:33:13,549 --> 00:33:18,830
give more malware and I provide less

856
00:33:15,679 --> 00:33:22,070
benign software in that case the class

857
00:33:18,830 --> 00:33:24,050
ratio changes and as a matter of fact

858
00:33:22,070 --> 00:33:26,210
the false positives

859
00:33:24,050 --> 00:33:28,100
do not change because I'm not actually

860
00:33:26,210 --> 00:33:29,840
the first wasn't the first positive

861
00:33:28,100 --> 00:33:31,580
lowers because I'm not feeding the

862
00:33:29,840 --> 00:33:33,800
classifier with more benign software and

863
00:33:31,580 --> 00:33:35,740
the true positive increases because I'm

864
00:33:33,800 --> 00:33:38,480
feeding the classifier with more malware

865
00:33:35,740 --> 00:33:41,900
so at that point you see if you look at

866
00:33:38,480 --> 00:33:44,570
the was the orange II yellow orange

867
00:33:41,900 --> 00:33:47,180
align that you have a boost in the

868
00:33:44,570 --> 00:33:49,879
precision just because you play with the

869
00:33:47,180 --> 00:33:53,600
cluster ratio in the testing data set so

870
00:33:49,880 --> 00:33:55,550
the point is that we cannot do that so

871
00:33:53,600 --> 00:33:57,889
the testing data set should be is

872
00:33:55,550 --> 00:33:59,840
representative of real life so we need

873
00:33:57,890 --> 00:34:01,490
to understand what is the class duration

874
00:33:59,840 --> 00:34:03,830
of the problem we're dealing with and we

875
00:34:01,490 --> 00:34:07,460
need to enforce strictly that close

876
00:34:03,830 --> 00:34:09,080
ratio in some of the tasks that we have

877
00:34:07,460 --> 00:34:10,429
to deal with the class ratio is natural

878
00:34:09,080 --> 00:34:11,719
so if you just monitor network traffic

879
00:34:10,429 --> 00:34:13,070
well that's kind of you know what you

880
00:34:11,719 --> 00:34:14,959
observe over there but if you're

881
00:34:13,070 --> 00:34:17,210
building a data set and you can play

882
00:34:14,960 --> 00:34:20,480
with the data set because it's hard to

883
00:34:17,210 --> 00:34:22,460
to harvest that I said another way well

884
00:34:20,480 --> 00:34:24,260
you need to be sure that you enforce the

885
00:34:22,460 --> 00:34:27,290
right class ratio otherwise the results

886
00:34:24,260 --> 00:34:28,970
are just inflated and this is something

887
00:34:27,290 --> 00:34:35,469
that happened so let me just keep this

888
00:34:28,969 --> 00:34:39,739
this a couple of slides for a second but

889
00:34:35,469 --> 00:34:42,109
so the problem is that so the question

890
00:34:39,739 --> 00:34:43,339
is can we do this however can we play

891
00:34:42,110 --> 00:34:46,340
with the frustration in the training

892
00:34:43,340 --> 00:34:47,870
data set yes we can do that because

893
00:34:46,340 --> 00:34:49,100
playing with the consolation and

894
00:34:47,870 --> 00:34:51,650
training data set just makes the

895
00:34:49,100 --> 00:34:53,719
classifier more sensitive towards one

896
00:34:51,650 --> 00:34:56,750
class or the other of course you have to

897
00:34:53,719 --> 00:34:57,830
be careful because by making the

898
00:34:56,750 --> 00:34:59,150
customer more sensitive towards one

899
00:34:57,830 --> 00:35:02,210
class it means that you improve the

900
00:34:59,150 --> 00:35:04,280
detection of those objects but of course

901
00:35:02,210 --> 00:35:06,740
you as a catch you might miss class end

902
00:35:04,280 --> 00:35:09,020
up by misclassifying many objects of the

903
00:35:06,740 --> 00:35:11,350
other class so what you have to do is

904
00:35:09,020 --> 00:35:14,150
just to find a sort of a sweet spot

905
00:35:11,350 --> 00:35:15,710
where the classify just would by playing

906
00:35:14,150 --> 00:35:17,750
with the class eration the training data

907
00:35:15,710 --> 00:35:19,040
set you can improve a little bit more

908
00:35:17,750 --> 00:35:22,060
the performance of the classifier

909
00:35:19,040 --> 00:35:25,220
because you use your data set in a more

910
00:35:22,060 --> 00:35:27,380
intelligent way okay now of course you

911
00:35:25,220 --> 00:35:30,950
don't have to try at random because I

912
00:35:27,380 --> 00:35:32,570
you know this is just case so we

913
00:35:30,950 --> 00:35:34,759
designed an algorithm there's it's an

914
00:35:32,570 --> 00:35:37,400
empirical algorithm so there's no

915
00:35:34,760 --> 00:35:38,000
mathematical proof but basically the

916
00:35:37,400 --> 00:35:40,850
algorithm

917
00:35:38,000 --> 00:35:44,480
he finds this sort of sweet spot so

918
00:35:40,850 --> 00:35:46,100
there is an error that you have to be

919
00:35:44,480 --> 00:35:49,010
willing to accept in a classification

920
00:35:46,100 --> 00:35:50,750
and once you set the sort of error then

921
00:35:49,010 --> 00:35:53,240
the classifier the algorithm proposed

922
00:35:50,750 --> 00:35:56,360
you the class duration that better suite

923
00:35:53,240 --> 00:35:58,279
the task at hand and again this is

924
00:35:56,360 --> 00:35:59,780
really intertwined with the machine

925
00:35:58,280 --> 00:36:02,630
learning and the program analysis you're

926
00:35:59,780 --> 00:36:05,630
working with so the algorithm is generic

927
00:36:02,630 --> 00:36:07,610
but a class ratio for one algorithm is

928
00:36:05,630 --> 00:36:09,710
different from the class eration from

929
00:36:07,610 --> 00:36:11,870
another algorithm so you can see that in

930
00:36:09,710 --> 00:36:14,420
these two plots from algorithm one the

931
00:36:11,870 --> 00:36:17,359
class ratio the sweet spot is around 25%

932
00:36:14,420 --> 00:36:19,730
of malicious software in the training

933
00:36:17,360 --> 00:36:21,950
data set okay test analysis 10% is fixed

934
00:36:19,730 --> 00:36:23,120
it's life we can forget about it with

935
00:36:21,950 --> 00:36:25,819
the training I complete a little bit

936
00:36:23,120 --> 00:36:27,980
more and here I can have a class ratio

937
00:36:25,820 --> 00:36:30,170
of 25% for malicious software and that

938
00:36:27,980 --> 00:36:32,660
gives me the best performance that I'm

939
00:36:30,170 --> 00:36:35,660
interested in but this other algorithm

940
00:36:32,660 --> 00:36:39,730
we find that the best class ratio is at

941
00:36:35,660 --> 00:36:42,290
50% okay so it's a balanced situation

942
00:36:39,730 --> 00:36:45,940
50% malware 50% benign software and

943
00:36:42,290 --> 00:36:48,130
again the argument finds the sweet spot

944
00:36:45,940 --> 00:36:50,300
there is tailored to that specific

945
00:36:48,130 --> 00:36:52,160
machine learning algorithm and that

946
00:36:50,300 --> 00:36:54,590
specific program analysis that the

947
00:36:52,160 --> 00:37:00,620
machine learning algorithm relies on to

948
00:36:54,590 --> 00:37:04,160
create the feature space okay so this is

949
00:37:00,620 --> 00:37:05,569
just to show you I plotted here in this

950
00:37:04,160 --> 00:37:09,520
plus just look at the one at the bottom

951
00:37:05,570 --> 00:37:12,950
you can see that the shadowed blue line

952
00:37:09,520 --> 00:37:14,750
where that was the first the first

953
00:37:12,950 --> 00:37:16,819
performance if you look at the proceed

954
00:37:14,750 --> 00:37:18,440
if you look at the f1 of the graph that

955
00:37:16,820 --> 00:37:22,370
I showed you the very beginning of the

956
00:37:18,440 --> 00:37:27,110
decay there is the decay of that graph

957
00:37:22,370 --> 00:37:29,630
in both algorithms here the the solid

958
00:37:27,110 --> 00:37:31,130
blue line shows you how you can already

959
00:37:29,630 --> 00:37:33,620
improve the performance of the

960
00:37:31,130 --> 00:37:35,840
algorithms over time just by playing

961
00:37:33,620 --> 00:37:39,100
with the class ratio in the training

962
00:37:35,840 --> 00:37:40,280
data set okay so same algorithm same

963
00:37:39,100 --> 00:37:42,230
optimizations

964
00:37:40,280 --> 00:37:44,330
same program analysis you're not doing

965
00:37:42,230 --> 00:37:45,590
anything you're just using the data set

966
00:37:44,330 --> 00:37:47,750
in a little bit more intelligent way

967
00:37:45,590 --> 00:37:51,559
okay on the training cannot do anything

968
00:37:47,750 --> 00:37:52,910
on the testing and this is you know add

969
00:37:51,559 --> 00:37:56,749
I would say like you know a very good

970
00:37:52,910 --> 00:37:59,379
improvement in performance so to sum up

971
00:37:56,749 --> 00:38:02,598
you know summarize a little bit more

972
00:37:59,380 --> 00:38:04,160
about the constraint we need to be sure

973
00:38:02,599 --> 00:38:06,589
if we're interested in understanding the

974
00:38:04,160 --> 00:38:09,859
performance of our algorithm in a

975
00:38:06,589 --> 00:38:12,288
non-stationary scenario so wherever

976
00:38:09,859 --> 00:38:14,328
there is constant drift then we can no

977
00:38:12,289 --> 00:38:17,089
longer just rely on a k-fold

978
00:38:14,329 --> 00:38:20,749
cross-validation type of evaluation we

979
00:38:17,089 --> 00:38:23,599
need to enforce temporal constraint to

980
00:38:20,749 --> 00:38:27,379
be sure that we evaluate the algorithm

981
00:38:23,599 --> 00:38:29,509
over a time line in a proper way and at

982
00:38:27,380 --> 00:38:34,099
the same time we need to be sure that

983
00:38:29,509 --> 00:38:36,949
also we don't have any bias by playing

984
00:38:34,099 --> 00:38:38,989
with the class ratio in the testing data

985
00:38:36,949 --> 00:38:41,869
set because otherwise that represent an

986
00:38:38,989 --> 00:38:45,109
unrealistic scenario okay if we enforce

987
00:38:41,869 --> 00:38:47,930
these constraints then we can just show

988
00:38:45,109 --> 00:38:49,819
and capture the performance of this

989
00:38:47,930 --> 00:38:53,328
algorithms or the others that we have

990
00:38:49,819 --> 00:38:55,160
over time okay and this is this is what

991
00:38:53,329 --> 00:38:58,789
it looks like once we we enforce

992
00:38:55,160 --> 00:39:00,618
everything okay here I just wanted to

993
00:38:58,789 --> 00:39:02,630
introduce something else that for those

994
00:39:00,619 --> 00:39:04,969
that are familiar with a you rock it's

995
00:39:02,630 --> 00:39:06,499
just the air under the ROC curve that

996
00:39:04,969 --> 00:39:07,849
captures with a number how good you

997
00:39:06,499 --> 00:39:09,919
really performance over different

998
00:39:07,849 --> 00:39:11,689
thresholds so we borrowed a kind of you

999
00:39:09,920 --> 00:39:14,839
know reasoning buoy it's done a little

1000
00:39:11,689 --> 00:39:16,848
bit more so it's nice of a plot but I

1001
00:39:14,839 --> 00:39:19,009
will be better to have a number that I

1002
00:39:16,849 --> 00:39:21,769
can use in a program to compare things

1003
00:39:19,009 --> 00:39:25,999
so and this number is what we call aut

1004
00:39:21,769 --> 00:39:27,468
which is the area under the time of the

1005
00:39:25,999 --> 00:39:29,299
performance that you are interested in

1006
00:39:27,469 --> 00:39:34,489
so say that you are interested in the f1

1007
00:39:29,299 --> 00:39:38,479
score so the aut captures the area so

1008
00:39:34,489 --> 00:39:40,219
the best part of the f1 score over the

1009
00:39:38,479 --> 00:39:42,379
time line that you are interested in and

1010
00:39:40,219 --> 00:39:44,719
again here as an example we train on one

1011
00:39:42,380 --> 00:39:47,900
year with test on 12 months sorry I'm 24

1012
00:39:44,719 --> 00:39:50,089
months algorithm one are going to we

1013
00:39:47,900 --> 00:39:52,069
show over those two years how the

1014
00:39:50,089 --> 00:39:53,328
performance decay from a practical

1015
00:39:52,069 --> 00:39:55,729
perspective you might not be interested

1016
00:39:53,329 --> 00:39:57,650
in the whole 24 months you just

1017
00:39:55,729 --> 00:39:59,058
interested in 3-4 months and then you

1018
00:39:57,650 --> 00:40:01,390
just limit the aut

1019
00:39:59,059 --> 00:40:05,150
over just those three four months okay

1020
00:40:01,390 --> 00:40:05,480
and then here you have a number so 0.58

1021
00:40:05,150 --> 00:40:07,670
zero

1022
00:40:05,480 --> 00:40:09,200
1:32 there already you can use to sort

1023
00:40:07,670 --> 00:40:11,960
of compare so you say that you have a

1024
00:40:09,200 --> 00:40:14,299
couple of approaches that you don't know

1025
00:40:11,960 --> 00:40:17,540
which one really performs better in your

1026
00:40:14,300 --> 00:40:20,119
context in the presence of constant

1027
00:40:17,540 --> 00:40:21,529
drift then you can use these numbers you

1028
00:40:20,119 --> 00:40:23,090
can evaluate your approach you can try

1029
00:40:21,530 --> 00:40:25,250
to understand okay this approach works a

1030
00:40:23,090 --> 00:40:26,420
little better than this other one but

1031
00:40:25,250 --> 00:40:28,040
keep in mind it will only look at the

1032
00:40:26,420 --> 00:40:29,750
performance here we're not trying to

1033
00:40:28,040 --> 00:40:31,100
answer any question about how does the

1034
00:40:29,750 --> 00:40:31,790
argument performs in the absence of

1035
00:40:31,100 --> 00:40:34,310
constant drift

1036
00:40:31,790 --> 00:40:37,340
can i is this abstraction that I'm using

1037
00:40:34,310 --> 00:40:38,900
that gives me that curve useful to

1038
00:40:37,340 --> 00:40:41,630
explain the prediction these are all

1039
00:40:38,900 --> 00:40:44,000
open questions okay so this is just one

1040
00:40:41,630 --> 00:40:50,630
point one answer but don't take this as

1041
00:40:44,000 --> 00:40:51,650
a wholly grade okay so now I wanted to

1042
00:40:50,630 --> 00:40:55,160
show you something

1043
00:40:51,650 --> 00:40:58,130
so really the effect of experimental

1044
00:40:55,160 --> 00:41:00,440
bias and how this could really tricked

1045
00:40:58,130 --> 00:41:01,910
us in believing that we're working with

1046
00:41:00,440 --> 00:41:04,790
something that's really good well in

1047
00:41:01,910 --> 00:41:06,710
fact it is not so here I'll show you the

1048
00:41:04,790 --> 00:41:08,090
first two plots are about the two

1049
00:41:06,710 --> 00:41:12,020
ignorant that I just really mentioned

1050
00:41:08,090 --> 00:41:13,910
before algorithm one linear SVM so very

1051
00:41:12,020 --> 00:41:15,800
simple linear classifier very

1052
00:41:13,910 --> 00:41:17,810
lightweight static analyses that extract

1053
00:41:15,800 --> 00:41:20,060
strings so I'm oversimplifying but you

1054
00:41:17,810 --> 00:41:21,890
know just going with me with that so

1055
00:41:20,060 --> 00:41:23,869
extract strings representing the big

1056
00:41:21,890 --> 00:41:25,520
vector 1 or 0 1 if you have it 0 if you

1057
00:41:23,869 --> 00:41:28,550
don't have it simple is that very quick

1058
00:41:25,520 --> 00:41:32,270
linear layer classifier in k-fold

1059
00:41:28,550 --> 00:41:36,710
cross-validation the classifier performs

1060
00:41:32,270 --> 00:41:38,840
around 0.94 and something like there are

1061
00:41:36,710 --> 00:41:41,780
two straight lines

1062
00:41:38,840 --> 00:41:43,490
- horizontal lines one is black and the

1063
00:41:41,780 --> 00:41:45,920
other one is red the black is the one

1064
00:41:43,490 --> 00:41:48,259
that is reported in the paper and the

1065
00:41:45,920 --> 00:41:49,940
red is the one that we have so in the

1066
00:41:48,260 --> 00:41:53,240
paper but on a different data set on a

1067
00:41:49,940 --> 00:41:55,369
smaller data set and the red is the one

1068
00:41:53,240 --> 00:41:57,020
that we have reproduced on our own data

1069
00:41:55,369 --> 00:41:58,670
set which is much larger you can see

1070
00:41:57,020 --> 00:42:00,320
that the two lines are pretty close

1071
00:41:58,670 --> 00:42:02,000
there is a bit of flux on there because

1072
00:42:00,320 --> 00:42:07,480
it's a statistical approach so it's

1073
00:42:02,000 --> 00:42:07,480
roughly the same okay it's not too bad

1074
00:42:08,530 --> 00:42:15,050
the other lines capture what happens if

1075
00:42:12,500 --> 00:42:17,270
you have constant drift okay so what is

1076
00:42:15,050 --> 00:42:19,220
how the performance decays over time how

1077
00:42:17,270 --> 00:42:20,780
quickly does it decay over

1078
00:42:19,220 --> 00:42:22,810
in the presence of concentrate and this

1079
00:42:20,780 --> 00:42:27,170
is already what we just mentioned before

1080
00:42:22,810 --> 00:42:28,730
now interestingly let me just look at

1081
00:42:27,170 --> 00:42:29,930
the third plot for a second and I'll go

1082
00:42:28,730 --> 00:42:32,270
back to the second one the third one

1083
00:42:29,930 --> 00:42:34,609
instead it's something that is based on

1084
00:42:32,270 --> 00:42:37,580
deep learning okay so we wanted to also

1085
00:42:34,610 --> 00:42:39,590
evaluate how deep learning would affect

1086
00:42:37,580 --> 00:42:44,720
the performance of the classifier in the

1087
00:42:39,590 --> 00:42:46,550
presence of concentrate and so I'm not

1088
00:42:44,720 --> 00:42:48,589
giving much details on how that works

1089
00:42:46,550 --> 00:42:50,120
but it's the same in terms of program

1090
00:42:48,590 --> 00:42:51,860
analysis the same program analysis of

1091
00:42:50,120 --> 00:42:53,900
the first one it's just a different

1092
00:42:51,860 --> 00:42:56,840
machine learning algorithm so in this

1093
00:42:53,900 --> 00:43:01,220
case same abstraction but different ml

1094
00:42:56,840 --> 00:43:03,110
algorithm and you can see that in the

1095
00:43:01,220 --> 00:43:06,770
absence of concept rift so in a careful

1096
00:43:03,110 --> 00:43:10,040
cross-validation settings our so the the

1097
00:43:06,770 --> 00:43:15,290
the paper performance f1 score is a bit

1098
00:43:10,040 --> 00:43:18,500
above at 0.8 so 0.18 182 okay compared

1099
00:43:15,290 --> 00:43:24,500
to the first one it's not a winner right

1100
00:43:18,500 --> 00:43:27,320
now our performance on our data set when

1101
00:43:24,500 --> 00:43:29,990
we reproduce this those experiments is a

1102
00:43:27,320 --> 00:43:33,110
bit below 0.9 and it's a bit higher

1103
00:43:29,990 --> 00:43:34,459
because they didn't perform hyper

1104
00:43:33,110 --> 00:43:35,720
parameter optimization with the

1105
00:43:34,460 --> 00:43:38,870
performance so there's some

1106
00:43:35,720 --> 00:43:40,730
technicalities but it's it's alright you

1107
00:43:38,870 --> 00:43:42,319
can see that now it's it's much more

1108
00:43:40,730 --> 00:43:44,270
closer to the first argument so I

1109
00:43:42,320 --> 00:43:47,330
wouldn't say that is you know I would I

1110
00:43:44,270 --> 00:43:49,400
would discard it since they won because

1111
00:43:47,330 --> 00:43:51,860
the performance in the absence concept

1112
00:43:49,400 --> 00:43:53,780
is actually pretty close but

1113
00:43:51,860 --> 00:43:56,540
interestingly interestingly you can

1114
00:43:53,780 --> 00:43:57,980
actually observe that the third author

1115
00:43:56,540 --> 00:44:00,500
is the one that is based on concept

1116
00:43:57,980 --> 00:44:05,510
sorry on deep learning seems to resist

1117
00:44:00,500 --> 00:44:08,090
ooh time decay better than compared to

1118
00:44:05,510 --> 00:44:10,700
the other one okay so the reason is that

1119
00:44:08,090 --> 00:44:12,170
it's still a bit unclear because we

1120
00:44:10,700 --> 00:44:13,609
don't really know I mean we know deep

1121
00:44:12,170 --> 00:44:15,380
learning but we don't really know what

1122
00:44:13,610 --> 00:44:18,110
the different layer in layers captures

1123
00:44:15,380 --> 00:44:21,230
really but you can see that there is a

1124
00:44:18,110 --> 00:44:24,620
smoother decay over time the catch to

1125
00:44:21,230 --> 00:44:28,490
pay the catch is that on average there

1126
00:44:24,620 --> 00:44:30,620
is a much lower performance compared to

1127
00:44:28,490 --> 00:44:31,970
the first one so again I'm not trying to

1128
00:44:30,620 --> 00:44:32,779
say that the first one is better than

1129
00:44:31,970 --> 00:44:34,759
the third one

1130
00:44:32,780 --> 00:44:35,840
or vice-versa I'm just saying that you

1131
00:44:34,760 --> 00:44:39,740
know if you have the right methodology

1132
00:44:35,840 --> 00:44:42,440
you have information that you can use to

1133
00:44:39,740 --> 00:44:46,549
take decisions and you might depending

1134
00:44:42,440 --> 00:44:48,680
on your case you might say look 0.9 or

1135
00:44:46,550 --> 00:44:50,960
whatever it is there I don't really care

1136
00:44:48,680 --> 00:44:54,169
about that one but I do care that I have

1137
00:44:50,960 --> 00:44:56,330
a more stable performance over time if

1138
00:44:54,170 --> 00:44:57,680
that is what you're looking for then

1139
00:44:56,330 --> 00:44:59,299
maybe you should be going for this one

1140
00:44:57,680 --> 00:45:01,910
but again it might be harder to explain

1141
00:44:59,300 --> 00:45:03,890
the reason of a classification if on the

1142
00:45:01,910 --> 00:45:05,779
other hand you're not interested in time

1143
00:45:03,890 --> 00:45:07,339
decay or you are interested only in a

1144
00:45:05,780 --> 00:45:09,260
few months and then after a few months

1145
00:45:07,340 --> 00:45:11,060
you have a chance to return your

1146
00:45:09,260 --> 00:45:13,130
classify or things like that you can

1147
00:45:11,060 --> 00:45:15,890
actually probably use the first one but

1148
00:45:13,130 --> 00:45:17,930
I wanted to point you now your attention

1149
00:45:15,890 --> 00:45:20,779
to drag your attention to the second one

1150
00:45:17,930 --> 00:45:22,129
second one is the most recent algorithm

1151
00:45:20,780 --> 00:45:24,190
so it's been published pretty much in

1152
00:45:22,130 --> 00:45:26,900
the same timeframe of the third one

1153
00:45:24,190 --> 00:45:28,760
second water again more complex program

1154
00:45:26,900 --> 00:45:31,760
analysis there's a random force as a

1155
00:45:28,760 --> 00:45:32,540
machine learning algorithm but in the

1156
00:45:31,760 --> 00:45:34,940
episode of the paper

1157
00:45:32,540 --> 00:45:40,220
they claimed that they're arguing

1158
00:45:34,940 --> 00:45:43,190
performs at 0.99 of f1 score it's pretty

1159
00:45:40,220 --> 00:45:45,669
high right and that's the inner k-fold

1160
00:45:43,190 --> 00:45:49,160
cross-validation and that's the black

1161
00:45:45,670 --> 00:45:52,490
bashed and dot lying at the top okay of

1162
00:45:49,160 --> 00:45:54,649
the second plot now of course because

1163
00:45:52,490 --> 00:45:56,359
that's the KFOR cross-validation so the

1164
00:45:54,650 --> 00:45:57,950
first and the second constraint in the

1165
00:45:56,360 --> 00:46:00,050
temporal domain are violated but that's

1166
00:45:57,950 --> 00:46:02,180
fine because you are interested in the

1167
00:46:00,050 --> 00:46:05,360
kafir cross-validation so in the absence

1168
00:46:02,180 --> 00:46:08,029
of concentrate but you have also to

1169
00:46:05,360 --> 00:46:10,580
consider whether there is a violation of

1170
00:46:08,030 --> 00:46:13,070
the third constraint so whether they had

1171
00:46:10,580 --> 00:46:15,440
played with a class ratio in the testing

1172
00:46:13,070 --> 00:46:17,150
data set turns out that they played a

1173
00:46:15,440 --> 00:46:19,070
lot with that but I'm not saying that

1174
00:46:17,150 --> 00:46:21,080
they done the Dunnan on purpose just

1175
00:46:19,070 --> 00:46:23,210
happened that they corrupted their data

1176
00:46:21,080 --> 00:46:24,770
said their testing data set in a way

1177
00:46:23,210 --> 00:46:26,540
that is completely unrealistic so I

1178
00:46:24,770 --> 00:46:29,810
believe that the class ratio instead of

1179
00:46:26,540 --> 00:46:32,840
being 10 to 90 it was 90 to 10 so 90% of

1180
00:46:29,810 --> 00:46:35,000
malware and 10% of B'nai which basically

1181
00:46:32,840 --> 00:46:37,370
boosts the classifier performance to

1182
00:46:35,000 --> 00:46:39,830
0.99 but it inflates the result now if

1183
00:46:37,370 --> 00:46:41,750
you enforce the right constraint of

1184
00:46:39,830 --> 00:46:43,700
having a class duration of 10% of

1185
00:46:41,750 --> 00:46:45,410
malware in the testing data set the

1186
00:46:43,700 --> 00:46:46,470
performance in a careful classification

1187
00:46:45,410 --> 00:46:50,190
actually drops

1188
00:46:46,470 --> 00:46:54,078
to 0.83 or 84 okay

1189
00:46:50,190 --> 00:46:55,920
so now if that would have been the case

1190
00:46:54,079 --> 00:46:58,980
probably this paper wouldn't have been

1191
00:46:55,920 --> 00:47:02,069
accepted but I'm not saying that you

1192
00:46:58,980 --> 00:47:03,300
know this is bad I'm saying that we have

1193
00:47:02,069 --> 00:47:04,980
to be careful when we use machine

1194
00:47:03,300 --> 00:47:06,630
learning not just machine learning three

1195
00:47:04,980 --> 00:47:08,579
line of sicut learn and then we are all

1196
00:47:06,630 --> 00:47:09,839
machine learning specialists right so we

1197
00:47:08,579 --> 00:47:12,450
need to try to understand what are the

1198
00:47:09,839 --> 00:47:14,400
implications of using the algorithm

1199
00:47:12,450 --> 00:47:17,509
especially when it's combined and

1200
00:47:14,400 --> 00:47:19,859
coupled with program analysis as well

1201
00:47:17,510 --> 00:47:23,930
and then it's the time decay over time

1202
00:47:19,859 --> 00:47:26,670
okay so this is what I just mentioned

1203
00:47:23,930 --> 00:47:30,078
yes so this is what I just mentioned so

1204
00:47:26,670 --> 00:47:34,740
a couple of points that I can probably

1205
00:47:30,079 --> 00:47:41,280
conclude with is that we can use the aut

1206
00:47:34,740 --> 00:47:43,348
as a valid metric to to to measure a

1207
00:47:41,280 --> 00:47:45,030
baseline performance of an algorithm and

1208
00:47:43,349 --> 00:47:46,829
again this is intertwine with the

1209
00:47:45,030 --> 00:47:50,310
program analysis that we were lying on

1210
00:47:46,829 --> 00:47:53,579
and then use that to explore different

1211
00:47:50,310 --> 00:47:56,549
strategies on that I can engineer to

1212
00:47:53,579 --> 00:47:58,770
address concert drift being this

1213
00:47:56,550 --> 00:48:00,900
incremental retraining online learning

1214
00:47:58,770 --> 00:48:03,300
active learning classification rejection

1215
00:48:00,900 --> 00:48:05,339
so the aut will give me a number that

1216
00:48:03,300 --> 00:48:08,339
quantifies the effort the performance

1217
00:48:05,339 --> 00:48:10,619
and the cost within each of these

1218
00:48:08,339 --> 00:48:13,560
strategies and I have a data for a

1219
00:48:10,619 --> 00:48:18,450
methodology that I can use to take more

1220
00:48:13,560 --> 00:48:20,640
informed decisions there are a bunch of

1221
00:48:18,450 --> 00:48:23,009
other things that we could do so we can

1222
00:48:20,640 --> 00:48:25,109
have a class eration that is actually

1223
00:48:23,010 --> 00:48:26,849
changing over time as well because now

1224
00:48:25,109 --> 00:48:28,770
we have a class ratio that is on average

1225
00:48:26,849 --> 00:48:30,540
10% over the whole timeline of course

1226
00:48:28,770 --> 00:48:32,579
that is might not be actually very

1227
00:48:30,540 --> 00:48:35,069
accurate and realistic as well so we

1228
00:48:32,579 --> 00:48:36,180
have a variable class ratio that changes

1229
00:48:35,069 --> 00:48:37,680
over time but for that we need

1230
00:48:36,180 --> 00:48:40,740
measurement paper so we need to

1231
00:48:37,680 --> 00:48:43,919
understand what happens in real life we

1232
00:48:40,740 --> 00:48:46,200
might actually try to understand whether

1233
00:48:43,920 --> 00:48:50,849
what we're using to train our classifier

1234
00:48:46,200 --> 00:48:52,259
with enables us to the tech objects that

1235
00:48:50,849 --> 00:48:54,089
we haven't seen but the four would in

1236
00:48:52,260 --> 00:48:56,550
the same distribution but what is the

1237
00:48:54,089 --> 00:48:59,430
catch to pay how are we forgetting about

1238
00:48:56,550 --> 00:49:00,300
the past because ideally we should be

1239
00:48:59,430 --> 00:49:01,950
using the whole data set

1240
00:49:00,300 --> 00:49:03,750
but not all the data is alike so there

1241
00:49:01,950 --> 00:49:06,359
are a bunch of question that we can

1242
00:49:03,750 --> 00:49:07,980
actually just look at and in all of this

1243
00:49:06,360 --> 00:49:11,490
I haven't mentioned the buzzword

1244
00:49:07,980 --> 00:49:15,450
adversary machine learning okay so other

1245
00:49:11,490 --> 00:49:18,569
cemetry learning on purpose manipulates

1246
00:49:15,450 --> 00:49:22,109
the input space to create objects that

1247
00:49:18,570 --> 00:49:23,940
are closed in the put space to each

1248
00:49:22,110 --> 00:49:25,500
other but in the feature space they are

1249
00:49:23,940 --> 00:49:26,910
very far away so they can be

1250
00:49:25,500 --> 00:49:28,080
misclassified but it depends on what

1251
00:49:26,910 --> 00:49:30,540
type of attack you do see if it's a

1252
00:49:28,080 --> 00:49:31,980
targeted or not so how does this concept

1253
00:49:30,540 --> 00:49:34,529
briefly relate with other machine

1254
00:49:31,980 --> 00:49:36,240
learning and how what is the effect of

1255
00:49:34,530 --> 00:49:37,760
other machine learning not on images

1256
00:49:36,240 --> 00:49:43,830
because other semester learning has been

1257
00:49:37,760 --> 00:49:46,710
quite prolific in the image domain or in

1258
00:49:43,830 --> 00:49:48,360
in the in the speech or audio domain but

1259
00:49:46,710 --> 00:49:50,190
what happens when we move away from that

1260
00:49:48,360 --> 00:49:51,990
abstraction so from things that human

1261
00:49:50,190 --> 00:49:54,510
beings can relate to and we look at

1262
00:49:51,990 --> 00:49:55,979
software so we need to sort of start

1263
00:49:54,510 --> 00:49:57,960
thinking about adversary program

1264
00:49:55,980 --> 00:50:00,660
generation so programs that can be

1265
00:49:57,960 --> 00:50:02,880
automatically generated to create

1266
00:50:00,660 --> 00:50:07,680
software that can miss classify a

1267
00:50:02,880 --> 00:50:09,270
classifier so these are all football

1268
00:50:07,680 --> 00:50:12,810
costs and things that we are also

1269
00:50:09,270 --> 00:50:17,250
working on in my lab so to summarize

1270
00:50:12,810 --> 00:50:19,590
this is the paper that is on my lab web

1271
00:50:17,250 --> 00:50:20,880
page and if you want you can read the

1272
00:50:19,590 --> 00:50:22,800
full details of what I just mentioned

1273
00:50:20,880 --> 00:50:25,220
plus more because there's all part about

1274
00:50:22,800 --> 00:50:27,360
incremental retraining active learning

1275
00:50:25,220 --> 00:50:30,689
classification projection I didn't have

1276
00:50:27,360 --> 00:50:32,340
time to mention to you guys about it's

1277
00:50:30,690 --> 00:50:34,050
quite thorough so their content details

1278
00:50:32,340 --> 00:50:37,440
so I'll be more than happy for you to

1279
00:50:34,050 --> 00:50:39,180
follow up with me and so my final points

1280
00:50:37,440 --> 00:50:41,460
I believe is just you know we need to

1281
00:50:39,180 --> 00:50:42,600
use machine learning not just as a black

1282
00:50:41,460 --> 00:50:44,010
box but we need to understand the

1283
00:50:42,600 --> 00:50:46,350
implications and the assumptions that

1284
00:50:44,010 --> 00:50:49,440
are required to do it properly insecure

1285
00:50:46,350 --> 00:50:52,200
in the security domain we need to be

1286
00:50:49,440 --> 00:50:54,990
sure that when we evaluate an approach

1287
00:50:52,200 --> 00:50:57,390
there's no temporal or spatial bias and

1288
00:50:54,990 --> 00:50:58,830
if there is that we are aware of it and

1289
00:50:57,390 --> 00:51:02,490
what it means okay

1290
00:50:58,830 --> 00:51:03,120
and we can rely on some other metrics

1291
00:51:02,490 --> 00:51:06,779
like ie

1292
00:51:03,120 --> 00:51:08,640
aut in a gene in addition to other

1293
00:51:06,780 --> 00:51:10,050
metrics to sort of evaluate now the

1294
00:51:08,640 --> 00:51:12,390
performance of the classifier in this

1295
00:51:10,050 --> 00:51:14,070
context in the context of concert drift

1296
00:51:12,390 --> 00:51:15,509
and how quickly

1297
00:51:14,070 --> 00:51:19,440
model the case of a time so foreign soil

1298
00:51:15,510 --> 00:51:21,870
and so I haven't mentioned that all has

1299
00:51:19,440 --> 00:51:24,000
been released as open source so if

1300
00:51:21,870 --> 00:51:28,109
you're interested in this you can just

1301
00:51:24,000 --> 00:51:29,940
drop a line to me and I'll give you

1302
00:51:28,110 --> 00:51:33,540
access to the bitbucket repository it's

1303
00:51:29,940 --> 00:51:35,700
it's semi public in the sense that we

1304
00:51:33,540 --> 00:51:37,140
love as impact you know keeping track of

1305
00:51:35,700 --> 00:51:39,629
who's getting access to the code and

1306
00:51:37,140 --> 00:51:41,160
whatnot so it's easy for me if you drop

1307
00:51:39,630 --> 00:51:42,660
a line and said who you are and then

1308
00:51:41,160 --> 00:51:45,060
without finishing you're working with

1309
00:51:42,660 --> 00:51:47,160
and so you got just the line for private

1310
00:51:45,060 --> 00:51:48,920
you know personal record and I'll give

1311
00:51:47,160 --> 00:51:51,420
you access to the bitbucket repository

1312
00:51:48,920 --> 00:51:54,660
all the code has been released open

1313
00:51:51,420 --> 00:51:56,850
source all the data set of the

1314
00:51:54,660 --> 00:51:59,850
experiments so we cannot release the

1315
00:51:56,850 --> 00:52:01,710
data set directly you need to obtain the

1316
00:51:59,850 --> 00:52:03,930
data set from under Zoo but we released

1317
00:52:01,710 --> 00:52:05,850
all the ashes of the application that we

1318
00:52:03,930 --> 00:52:08,250
used in all the experiments and we

1319
00:52:05,850 --> 00:52:10,020
release also the feature space so the

1320
00:52:08,250 --> 00:52:12,090
program analysis that we ran to create

1321
00:52:10,020 --> 00:52:14,160
that feature space which is it takes a

1322
00:52:12,090 --> 00:52:15,930
bit of time to do that if you want to

1323
00:52:14,160 --> 00:52:17,520
reproduce the expect the whole thing

1324
00:52:15,930 --> 00:52:19,080
that we just mentioned about so it's all

1325
00:52:17,520 --> 00:52:21,890
there you can for repress it a bit of

1326
00:52:19,080 --> 00:52:25,410
reproducibility you can actually do it

1327
00:52:21,890 --> 00:52:28,259
the code is a secret learn compatible so

1328
00:52:25,410 --> 00:52:30,000
it follows the same API there's only a

1329
00:52:28,260 --> 00:52:32,280
couple of api's have an addition that

1330
00:52:30,000 --> 00:52:34,500
time line so the time split that you

1331
00:52:32,280 --> 00:52:36,120
want to work with and there is the aut

1332
00:52:34,500 --> 00:52:37,830
as well in there plus the other things

1333
00:52:36,120 --> 00:52:40,109
that I mentioned about incremental

1334
00:52:37,830 --> 00:52:44,759
retraining and super and so I'm almost

1335
00:52:40,110 --> 00:52:46,950
done thank you all but shameless plug so

1336
00:52:44,760 --> 00:52:48,120
I'm the general co-chair for ACM CCS is

1337
00:52:46,950 --> 00:52:50,299
one of the top four conferences I

1338
00:52:48,120 --> 00:52:52,799
mentioned you before at the beginning

1339
00:52:50,300 --> 00:52:55,170
the conference this year is usually held

1340
00:52:52,800 --> 00:52:57,090
in the US and has been at four years is

1341
00:52:55,170 --> 00:53:00,590
going back and forth US Europe US Europe

1342
00:52:57,090 --> 00:53:02,580
and it's good before November 11 to 15

1343
00:53:00,590 --> 00:53:04,680
go to the website if you're interested

1344
00:53:02,580 --> 00:53:07,049
in it it's a one on top for academic

1345
00:53:04,680 --> 00:53:09,060
conference so it's a very good quality

1346
00:53:07,050 --> 00:53:11,070
talk it's a different vibe so it's a

1347
00:53:09,060 --> 00:53:12,210
academic conference so it's good so that

1348
00:53:11,070 --> 00:53:14,700
you understand what is that I kind of

1349
00:53:12,210 --> 00:53:16,770
converse if you're interested there's or

1350
00:53:14,700 --> 00:53:17,730
drop me a line or go on a web site be

1351
00:53:16,770 --> 00:53:21,180
more than happy to talk to you more

1352
00:53:17,730 --> 00:53:23,130
about it and without further ado I

1353
00:53:21,180 --> 00:53:26,310
believe that I almost used up all my

1354
00:53:23,130 --> 00:53:28,260
time slice if we have a few more minutes

1355
00:53:26,310 --> 00:53:29,339
probably this time for you

1356
00:53:28,260 --> 00:53:31,110
I'll be more than happy to take any

1357
00:53:29,339 --> 00:53:41,490
question thanks

1358
00:53:31,110 --> 00:53:52,090
[Applause]

1359
00:53:41,490 --> 00:54:00,339
questions on the back yep that's a very

1360
00:53:52,090 --> 00:54:02,680
good question so yeah yeah yeah so so

1361
00:54:00,340 --> 00:54:05,440
the question is about who labels the

1362
00:54:02,680 --> 00:54:08,200
apps right that's a very good question

1363
00:54:05,440 --> 00:54:11,290
it's an open prop so the short answer is

1364
00:54:08,200 --> 00:54:13,569
a very open problem because that is

1365
00:54:11,290 --> 00:54:16,480
about the ground truth so all that I've

1366
00:54:13,570 --> 00:54:18,490
shown is good modulo the ground truth

1367
00:54:16,480 --> 00:54:19,960
okay so if something is wrong of course

1368
00:54:18,490 --> 00:54:21,609
the curves will look different that my

1369
00:54:19,960 --> 00:54:24,220
technology is still valid it's just that

1370
00:54:21,610 --> 00:54:27,010
the ground truth was not good enough so

1371
00:54:24,220 --> 00:54:29,740
for this work we just relied on this and

1372
00:54:27,010 --> 00:54:31,510
Roseau that asset and for the purposes

1373
00:54:29,740 --> 00:54:35,950
labels are passed

1374
00:54:31,510 --> 00:54:40,330
sorry apps are fed to buyer startup with

1375
00:54:35,950 --> 00:54:43,720
just one constraint so we tend to we

1376
00:54:40,330 --> 00:54:46,390
didn't use data up until a year ago so

1377
00:54:43,720 --> 00:54:48,759
now the study finishes at 2016 we're

1378
00:54:46,390 --> 00:54:51,100
expanding it up to 2018 we were not

1379
00:54:48,760 --> 00:54:53,619
using the last year of worth of data

1380
00:54:51,100 --> 00:54:55,900
because there's been a recent piece of

1381
00:54:53,619 --> 00:54:58,720
work that shows that within a year

1382
00:54:55,900 --> 00:55:00,790
labels shouldn't be trusted because they

1383
00:54:58,720 --> 00:55:03,368
can actually change so there is a bit of

1384
00:55:00,790 --> 00:55:05,170
an uncertainty and unstability but after

1385
00:55:03,369 --> 00:55:07,660
a year that an app has been flagged as

1386
00:55:05,170 --> 00:55:11,320
been good there is a good likelihood

1387
00:55:07,660 --> 00:55:13,569
that it is good there is a whole sort of

1388
00:55:11,320 --> 00:55:14,680
modulo reasoning because now we talk

1389
00:55:13,570 --> 00:55:17,320
about Android apps so there is

1390
00:55:14,680 --> 00:55:19,060
dynamically loaded code or library there

1391
00:55:17,320 --> 00:55:21,580
are you know add libraries and stuff so

1392
00:55:19,060 --> 00:55:22,869
it's a very complicated more space but

1393
00:55:21,580 --> 00:55:26,520
given these assumptions

1394
00:55:22,869 --> 00:55:29,290
I believe we try to limit the risk of

1395
00:55:26,520 --> 00:55:37,080
dealing with them with a bad ground

1396
00:55:29,290 --> 00:55:37,080
truth it it can still be there yeah

1397
00:55:38,820 --> 00:55:41,909
[Music]

1398
00:55:45,390 --> 00:55:50,799
and I think questions so the question is

1399
00:55:48,340 --> 00:55:53,500
about whether the apk so whether we

1400
00:55:50,800 --> 00:55:56,560
consider a situation in which you could

1401
00:55:53,500 --> 00:55:59,170
inject things at runtime not in this

1402
00:55:56,560 --> 00:56:02,290
case so all the program analysis that we

1403
00:55:59,170 --> 00:56:04,030
dealt with it's a static analysis it's

1404
00:56:02,290 --> 00:56:07,090
already very complicated this way

1405
00:56:04,030 --> 00:56:10,570
but of course you know I'm working a lot

1406
00:56:07,090 --> 00:56:12,100
on this but the conversation is also

1407
00:56:10,570 --> 00:56:13,119
about adversary program generation so

1408
00:56:12,100 --> 00:56:14,830
I'm working on a bracero program

1409
00:56:13,119 --> 00:56:16,510
generation where we do adversarial

1410
00:56:14,830 --> 00:56:18,910
attacks but just not in the feature

1411
00:56:16,510 --> 00:56:20,050
space because that is not realistic so

1412
00:56:18,910 --> 00:56:22,600
from the feature space then we

1413
00:56:20,050 --> 00:56:24,820
automatically generate a program that

1414
00:56:22,600 --> 00:56:26,950
whenever is analyzed by a classifier it

1415
00:56:24,820 --> 00:56:29,050
just generates features that will cause

1416
00:56:26,950 --> 00:56:30,100
the MIS classification there's all bunch

1417
00:56:29,050 --> 00:56:32,020
of issues there because you have

1418
00:56:30,100 --> 00:56:34,000
side-effects that you you bring in when

1419
00:56:32,020 --> 00:56:35,619
you do the program analysis you have to

1420
00:56:34,000 --> 00:56:37,240
be careful that what in j is not

1421
00:56:35,619 --> 00:56:38,859
executed at one time because you have to

1422
00:56:37,240 --> 00:56:40,930
guarantee semantics equivalence so there

1423
00:56:38,859 --> 00:56:43,600
are all sort of huge problems with

1424
00:56:40,930 --> 00:56:47,319
dynamic analysis it even is even worse

1425
00:56:43,600 --> 00:56:49,390
so one step at a time at the back over

1426
00:56:47,320 --> 00:56:51,790
there then I'm going to suggest one more

1427
00:56:49,390 --> 00:57:07,629
and then we'll break for that back over

1428
00:56:51,790 --> 00:57:08,800
there yeah yeah yes yes yeah so the

1429
00:57:07,630 --> 00:57:11,290
question is how do we differentiate

1430
00:57:08,800 --> 00:57:13,090
between behaviors that is malware or

1431
00:57:11,290 --> 00:57:15,640
behaviors from legitimate tools that

1432
00:57:13,090 --> 00:57:18,040
look like malware the answer is that we

1433
00:57:15,640 --> 00:57:20,770
don't so we just take the data set as

1434
00:57:18,040 --> 00:57:22,240
these somebody told us after this virus

1435
00:57:20,770 --> 00:57:23,740
total thing that has been vetted for a

1436
00:57:22,240 --> 00:57:27,399
stable for more than a year that this is

1437
00:57:23,740 --> 00:57:31,020
good and this is bad there is a another

1438
00:57:27,400 --> 00:57:34,090
longer answer to your question and the

1439
00:57:31,020 --> 00:57:36,430
the answer is that it's still quite

1440
00:57:34,090 --> 00:57:38,410
unclear what the classifier learns so

1441
00:57:36,430 --> 00:57:41,680
this very on topic with with this year

1442
00:57:38,410 --> 00:57:44,350
besides so it really varies with the

1443
00:57:41,680 --> 00:57:46,509
abstraction that you're using but most

1444
00:57:44,350 --> 00:57:49,540
of the times it doesn't picks up on it

1445
00:57:46,510 --> 00:57:50,740
doesn't pick up on what you believe will

1446
00:57:49,540 --> 00:57:51,779
be the behavior that you're interested

1447
00:57:50,740 --> 00:57:55,680
in

1448
00:57:51,780 --> 00:57:59,670
it picks up on artifacts that would just

1449
00:57:55,680 --> 00:58:02,330
make a nice decision boundary there is a

1450
00:57:59,670 --> 00:58:04,859
lot of research that is needed there and

1451
00:58:02,330 --> 00:58:07,049
it ties back with the program analysis

1452
00:58:04,859 --> 00:58:08,700
it ties back with explain ability ties

1453
00:58:07,050 --> 00:58:10,980
back with adversary machine learning

1454
00:58:08,700 --> 00:58:12,598
because the idea is if you if you

1455
00:58:10,980 --> 00:58:13,890
constrain also if you spend more time on

1456
00:58:12,599 --> 00:58:16,260
the engineering on the feature and

1457
00:58:13,890 --> 00:58:19,170
you're using features that are also hard

1458
00:58:16,260 --> 00:58:20,910
to manipulate you constrain a bunch of

1459
00:58:19,170 --> 00:58:23,520
things you might actually help explain

1460
00:58:20,910 --> 00:58:26,640
ability but you might not have a very

1461
00:58:23,520 --> 00:58:28,770
good performance so it's it's unclear

1462
00:58:26,640 --> 00:58:41,310
yet final question and then we'll have

1463
00:58:28,770 --> 00:58:44,940
to attack Lorenzo and Jason we we are we

1464
00:58:41,310 --> 00:58:47,070
are we are we are were not here with so

1465
00:58:44,940 --> 00:58:49,500
in fact this work so sorry the question

1466
00:58:47,070 --> 00:58:51,000
was whether we looked at the hospital

1467
00:58:49,500 --> 00:58:54,720
feature engineering and now that effect

1468
00:58:51,000 --> 00:58:57,660
all of this I am so with my lab we are

1469
00:58:54,720 --> 00:59:01,799
working on this as just a small lab and

1470
00:58:57,660 --> 00:59:04,279
far too many things to work on but hey

1471
00:59:01,800 --> 00:59:09,390
if you guys are from industry want to

1472
00:59:04,280 --> 00:59:11,040
donate funds for my research I I use

1473
00:59:09,390 --> 00:59:16,069
this for nonprofits so everything is

1474
00:59:11,040 --> 00:59:20,810
really is open so we all do research so

1475
00:59:16,070 --> 00:59:25,770
it's a it's a very complex problem

1476
00:59:20,810 --> 00:59:29,009
because of what I mentioned that there

1477
00:59:25,770 --> 00:59:32,670
are implication so performance is not

1478
00:59:29,010 --> 00:59:35,070
just we were we're only we've been

1479
00:59:32,670 --> 00:59:38,460
trained to look at performance as the

1480
00:59:35,070 --> 00:59:40,950
answer for the problem it's no longer

1481
00:59:38,460 --> 00:59:42,060
that and because there are a bunch of

1482
00:59:40,950 --> 00:59:44,700
other issues so you care about

1483
00:59:42,060 --> 00:59:46,049
performance sure but in many situations

1484
00:59:44,700 --> 00:59:48,839
so I had a very interesting conversation

1485
00:59:46,050 --> 00:59:51,869
with a vendor that I'm not sharing

1486
00:59:48,839 --> 00:59:53,400
anything about well you might guess so

1487
00:59:51,869 --> 00:59:54,839
the way the word so they have very good

1488
00:59:53,400 --> 00:59:56,400
detection rates and whatever but they're

1489
00:59:54,839 --> 00:59:59,400
not really they don't really care about

1490
00:59:56,400 --> 01:00:00,690
that of course they do care but they

1491
00:59:59,400 --> 01:00:03,510
want to be able to understand whether

1492
01:00:00,690 --> 01:00:05,609
what the classifier spits out can be

1493
01:00:03,510 --> 01:00:08,130
trusted so the cancer cells

1494
01:00:05,609 --> 01:00:10,170
malicious software how do I verify it

1495
01:00:08,130 --> 01:00:12,989
without having a human being to reverse

1496
01:00:10,170 --> 01:00:15,930
engineer in the app so they want really

1497
01:00:12,989 --> 01:00:18,180
to have something that support analysts

1498
01:00:15,930 --> 01:00:19,680
in understanding oh the content is bad

1499
01:00:18,180 --> 01:00:21,598
oh and because I have this automated

1500
01:00:19,680 --> 01:00:23,910
security centric generator description

1501
01:00:21,599 --> 01:00:26,069
of the behavior of the program oh yeah I

1502
01:00:23,910 --> 01:00:27,538
can read in a few 10 seconds say oh yeah

1503
01:00:26,069 --> 01:00:31,319
now I like it you know I trust it so

1504
01:00:27,539 --> 01:00:34,680
that's fine that abstraction however has

1505
01:00:31,319 --> 01:00:36,960
a cost so when you just extract strings

1506
01:00:34,680 --> 01:00:40,259
it's a very lightweight cost when you do

1507
01:00:36,960 --> 01:00:42,239
build a data dependency graph if you're

1508
01:00:40,259 --> 01:00:46,289
familiar with that it's it's a very

1509
01:00:42,239 --> 01:00:47,880
different type of cost so there's I

1510
01:00:46,289 --> 01:00:49,710
don't have an answer so we are really

1511
01:00:47,880 --> 01:00:52,680
working on it quite heavily I don't have

1512
01:00:49,710 --> 01:00:56,069
an answer right I'm going to just record

1513
01:00:52,680 --> 01:00:58,348
it Lorenza I'm here now and all

1514
01:00:56,069 --> 01:00:59,788
afternoon so please feel free to find

1515
01:00:58,349 --> 01:01:00,440
him and ask questions thank you very

1516
01:00:59,789 --> 01:01:05,019
much

1517
01:01:00,440 --> 01:01:05,019
[Applause]

