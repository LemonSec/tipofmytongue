1
00:00:01,760 --> 00:00:07,920
Okay. I think we're live now. Good morning and 
good afternoon everyone. A very warm welcome and

2
00:00:07,920 --> 00:00:13,920
thank you very much for joining us in our virtual 
conversation today marking the 10th anniversary of

3
00:00:13,920 --> 00:00:19,040
the Arab Spring. Today's event is one of many 
we're hosting as part of the 10th edition of

4
00:00:19,040 --> 00:00:27,040
RightsCon set to launch online from June 7th till 
June 11th 2021. This year, so, since January we've

5
00:00:27,040 --> 00:00:32,640
hit the 10-year mark for the revolutions in 
Tunisia, Egypt, and most recently, three days ago,

6
00:00:32,640 --> 00:00:39,600
on Valentine's Day to be exact, the revolution 
in Bahrain. As many in our audience know, in late

7
00:00:40,240 --> 00:00:46,800
2010, early 2011, millions of citizens in the region 
took to the streets demanding justice, dignity, and

8
00:00:46,800 --> 00:00:52,720
freedom. And whilst we can commemorate the Arab 
uprisings for various reasons, we'll delve today

9
00:00:52,720 --> 00:00:58,560
into one of its outstanding legacies, namely how 
activists and citizens utilize the internet and

10
00:00:58,560 --> 00:01:03,840
digital tools for political organizing and 
mobilization, and how that looks like today.

11
00:01:05,280 --> 00:01:10,080
So, the role of the internet, particularly Social 
Media companies, like Twitter and Facebook,

12
00:01:10,720 --> 00:01:17,520
has often been exaggerated in those uprisings. 
However, it is undeniable that the Arab Spring

13
00:01:17,520 --> 00:01:22,240
has ushered in new discussions over the 
role of technology and its impact on

14
00:01:23,040 --> 00:01:28,080
our fundamental rights, from content moderation 
issues, to internet shutdowns, and surveillance, and

15
00:01:28,080 --> 00:01:34,880
spyware; many of those discussions and issues we're 
still grappling with until this moment in time.

16
00:01:35,680 --> 00:01:42,000
So, today, I'm extremely honored to be joined by 
fantastic speakers, colleagues, and friends, some

17
00:01:42,000 --> 00:01:48,480
of whom were on the front lines of those protests, 
to reflect on the past decade. We have a special

18
00:01:48,480 --> 00:01:54,640
session today in terms of format. So, we won't have 
the conventional webinar or panel. Instead, in this

19
00:01:54,640 --> 00:02:02,560
one hour and a half, we'll cover a range of topics 
 in smaller intersessions. So, we'll first

20
00:02:02,560 --> 00:02:08,880
start with Mohamad Najem, the executive director 
and co-founder of SMEX, to reflect on the status

21
00:02:08,880 --> 00:02:15,040
of internet freedoms and digital rights in the 
region since 2011, and especially in light of

22
00:02:16,400 --> 00:02:22,560
Arab governments or regimes relentlessly 
moving to close down the last pockets of

23
00:02:24,000 --> 00:02:29,680
online freedoms enjoyed by citizens to speak up 
their minds, to communicate, and share, and access

24
00:02:29,680 --> 00:02:37,600
information openly and freely. Then, we'll show two 
videos from activists from Libya and Tunisia.

25
00:02:37,600 --> 00:02:42,960
We'll then move into a conversation on content 
moderation issues, and specifically the role of

26
00:02:42,960 --> 00:02:48,320
big tech companies in shaping online space, 
and how people exercise their rights in

27
00:02:48,320 --> 00:02:53,600
the region, particularly the right to freedom of 
expression and opinion; we'll have with us Dia Kayyali

28
00:02:53,600 --> 00:03:00,880
to have that conversation. We'll also  discuss 
the issue of online surveillance and the lived

29
00:03:00,880 --> 00:03:06,880
experience of human rights activists who have 
been the targets of malicious and sophisticated

30
00:03:06,880 --> 00:03:14,560
surveillance campaigns  together with  Maryam Al-Khawaja, 
Fouad Abdelmoumni from Morocco, and our very

31
00:03:14,560 --> 00:03:23,440
own Natalia from our tech and legal team. 
And finally, away from over celebrating the

32
00:03:23,440 --> 00:03:29,520
legacy of the Arab revolutions or lamenting 
on the calamitous trajectory they have taken,

33
00:03:29,520 --> 00:03:36,000
we hope to use the time today for us to 
think collectively, and as activists and civil

34
00:03:36,000 --> 00:03:44,160
society organizations, to inspire action and think 
forward in ways we can reclaim the internet

35
00:03:44,160 --> 00:03:49,840
as a free and open space for citizens, and for us 
to exercise our rights, especially as many continue

36
00:03:49,840 --> 00:03:55,920
to struggle today for justice and freedom 
under dictatorships and repressive regimes,

37
00:03:55,920 --> 00:04:03,200
in Egypt, in Saudi Arabia, in Morocco, in Bahrain, 
and as we have seen most recently, in Tunisia.

38
00:04:04,240 --> 00:04:10,080
Towards the end of the session, we'll have time 
to answer your questions, so please ask away during

39
00:04:10,080 --> 00:04:15,520
the session. We look forward to engaging 
with the comments and questions you raise.

40
00:04:16,640 --> 00:04:20,880
One quick housekeeping rule, 
this event is being recorded

41
00:04:20,880 --> 00:04:25,200
and we'll provide the recording on 
YouTube later with Arabic transcripts.

42
00:04:26,240 --> 00:04:32,960
And before we kick  off the discussion, I want 
to say something and to get it out of my system,

43
00:04:32,960 --> 00:04:39,680
and that is we acknowledge and appreciate  the 
complexity of the Arab revolutions and their

44
00:04:39,680 --> 00:04:46,800
aftermath, and as well as the diversity of voices 
from the region. So, we do not claim that this

45
00:04:46,800 --> 00:04:52,640
conversation is conclusive, nor inclusive of all 
the voices and issues we're facing in our region.

46
00:04:52,640 --> 00:05:00,400
But, we hope that this conversation will be start 
of many, and we look forward to engaging

47
00:05:00,400 --> 00:05:06,080
with you especially during RightsCon, the 
biggest convening on the issues related to

48
00:05:06,080 --> 00:05:15,200
human rights and  technology. So, diving 
right in, Mohamad, thank you so much for joining

49
00:05:15,200 --> 00:05:21,760
us. Just to introduce you to our audience, Mohamad 
Najem is the co-founder and executive director of

50
00:05:21,760 --> 00:05:28,880
SMEX, a digital rights organization based 
in Lebanon. And Mohamad, I'm gonna jump

51
00:05:28,880 --> 00:05:35,680
straight into  the conversation. I noticed that 
in the Times magazine, this year, you were listed as

52
00:05:35,680 --> 00:05:41,120
one of those activists still fighting towards 
progress in our region, since you founded

53
00:05:41,120 --> 00:05:49,840
or co-founded SMEX in 2008. So, you are an old 
veteran. You've been around before the Arab Spring,

54
00:05:49,840 --> 00:05:57,119
during, and after, and back in the days,  
admittingly, I was one of those people who thought

55
00:05:57,120 --> 00:06:02,960
that, you know, digital tools are, quote and quote, 
liberation tools. They definitely can help us

56
00:06:04,880 --> 00:06:12,000
democratize our societies, communicate freely, and 
they would usher us into a new era of political

57
00:06:12,000 --> 00:06:17,840
movement and organization. Of course, fast 
forward, we're to the point where we are

58
00:06:17,840 --> 00:06:23,760
now, the situation is a bit different. So, can 
you share with us, from your experience, how

59
00:06:25,120 --> 00:06:30,480
the internet as a landscape has 
changed, as well as the ways we

60
00:06:30,480 --> 00:06:36,320
can exercise our rights and organize on 
the internet, as well as on the ground?

61
00:06:39,680 --> 00:06:43,520
Sure, thank you Marwa, thank you 
Access Now for organizing this session.

62
00:06:44,400 --> 00:06:49,599
And I'm very happy and honored to be with all 
colleagues and friends, as well to be on this panel.

63
00:06:50,480 --> 00:06:58,320
I think Marwa that this feeling that 
the internet was a liberation space, it was

64
00:06:58,320 --> 00:07:03,120
a common feeling for everyone. I mean, even, 
and I want to go back a little bit

65
00:07:03,120 --> 00:07:08,800
to the 1996, when the internet, there was like a 
declaration of the independence of cyberspace.

66
00:07:09,520 --> 00:07:16,000
I mean, if we look at it right now, I mean, 
it's kind of cute you know. But like,

67
00:07:16,000 --> 00:07:23,680
there was this movement of, 
we got to a point in this region where

68
00:07:24,640 --> 00:07:31,039
the normal activism is not working anymore, 
the security apparatus is really strong,

69
00:07:31,760 --> 00:07:36,640
many people are in jail, and the internet 
came as this new tool to play with

70
00:07:37,280 --> 00:07:44,000
and to dream with as well. So, I was witnessing 
what was happening during this period of time,

71
00:07:44,000 --> 00:07:49,760
and actually managed to know how to play 
these games around these internet. I mean, of

72
00:07:49,760 --> 00:07:55,599
course, now in 2021, we look at these, when we 
look at this era, we want to look at it

73
00:07:55,600 --> 00:08:03,120
in a critical eye. But, we also want to look at it 
in a way that we need to understand that,

74
00:08:03,120 --> 00:08:10,160
that was normal to people look at it this way, and 
that feeling that existed during this time,

75
00:08:10,880 --> 00:08:17,680
it came naturally, because it 
came as as a saver from from something. So,

76
00:08:19,280 --> 00:08:24,559
back in 2008, when we started using the Social 
Media tools, it was all about: Okay, Social Media, how we

77
00:08:24,560 --> 00:08:30,320
can use these tools more strategically; how we can 
really get to the goals we want to get; how we can

78
00:08:30,320 --> 00:08:40,080
achieve what we want to achieve. I mean, everyone 
was jumping on these tools and,

79
00:08:40,080 --> 00:08:46,640
I mean, I would say, between 2008 and 2011, or 12, 
that was like four or five years, until we started

80
00:08:46,640 --> 00:08:51,360
thinking about; okay, there's content moderation 
here that we need to talk about, that we need to

81
00:08:52,000 --> 00:08:59,440
understand how these tech companies work, we 
need to understand how we can negotiate the space

82
00:08:59,440 --> 00:09:04,720
and what's accepted to be online and what's not 
accepted to be online, and freedom of expression

83
00:09:04,720 --> 00:09:13,760
is not total to everyone. There's 
all these obstacles, and unfortunately, also,

84
00:09:13,760 --> 00:09:20,720
at the same time, and also, that was normal. A 
lot of states, authoritarian regimes, have

85
00:09:20,720 --> 00:09:25,920
also jumped on the technology game,
and they have a lot of more resources than

86
00:09:27,920 --> 00:09:36,479
what everyone does, and they started hiring other 
technologists to play the game, and they start

87
00:09:37,040 --> 00:09:46,959
manipulating the technology, and here we are 
in 2021, there's this struggle and 

88
00:09:48,320 --> 00:09:51,120
and also, the elephant in 
the room are the tech companies.

89
00:09:52,000 --> 00:09:58,880
So, there's different dynamics; 
now there's the authoritarian regimes

90
00:09:58,880 --> 00:10:05,600
which they are very strong. But, tech companies 
seems to be stronger as well. And there's

91
00:10:05,600 --> 00:10:12,320
all these activists, all these different 
communities who are fighting for basic human

92
00:10:12,320 --> 00:10:23,440
rights, or just to be listened and to expect to 
achieve living social justice, basic democracy, I

93
00:10:23,440 --> 00:10:32,080
mean many things, and unfortunately in 2021, many 
of our friends are either in jail, or have been

94
00:10:33,760 --> 00:10:41,520
in exile for many years, and some of them have 
died as well. They paid the price so, this is

95
00:10:41,520 --> 00:10:51,760
the situation. Back to the tech 
companies, the debate, I mean, many look at it as

96
00:10:52,640 --> 00:10:57,360
content moderation issues, others 
want to challenge the business model.

97
00:10:59,520 --> 00:11:03,439
Also, there's a deep look 
into the the laws in the U.S,

98
00:11:04,160 --> 00:11:08,800
and how the tech companies have all 
the freedoms they have to act the way

99
00:11:08,800 --> 00:11:13,839
they act. So, there's some limitation they 
might need to happen to these companies.

100
00:11:16,000 --> 00:11:24,320
So, we're here. And, unfortunately, now a lot of Gulf 
companies are investing a lot in the new tools and

101
00:11:24,320 --> 00:11:35,120
technologies. They put a lot of money in 
new apps. Tech companies offices are in the Gulf.

102
00:11:36,000 --> 00:11:45,520
So, the challenge is bigger. But, we also 
still have hope, because we have some

103
00:11:45,520 --> 00:11:51,920
opportunities, we have some pressure 
points we need to keep doing. We need to be more

104
00:11:51,920 --> 00:11:58,240
organized. We need to talk to each other 
more, and I'm hopeful about the coming years.

105
00:12:00,000 --> 00:12:06,560
Thank you Mohamad. I think you've wrapped 
it quite nicely. We have been, actually over the

106
00:12:06,560 --> 00:12:13,520
past decade, in a state of negotiations 
between us as users, and also those tech companies.

107
00:12:13,520 --> 00:12:21,600
And speaking of which, I would like to show a 
video from our friend, imprisoned human rights

108
00:12:21,600 --> 00:12:28,880
defender, Alaa Abd El-Fattah, who quite fittingly, 
to the theme of our event, was a 

109
00:12:29,520 --> 00:12:36,400
keynote speaker for the first RightsCon in 
2011. So, if we can show the video from Alaa

110
00:12:36,960 --> 00:12:43,280
and what he said in regards to tech companies. I 
guess I'm here as an activist, as a food soldier

111
00:12:43,280 --> 00:12:55,600
in the revolution, to talk about how tech companies 
can find ways to maintain, and promote, and protect,

112
00:12:57,600 --> 00:13:04,800
and respect the human rights of their users. Now, 
that's a topic I'm quite cynical about. Companies

113
00:13:04,800 --> 00:13:09,040
are not really likely to do any of that. 
Corporations are not really likely to

114
00:13:09,040 --> 00:13:17,040
do any of that. It comes, the conflicts 
with you know. It's not exactly that there is

115
00:13:17,040 --> 00:13:22,959
a conflict of interest; I mean, I think we're all 
here because we know that it's actually possible

116
00:13:22,960 --> 00:13:28,480
to go about our business without infringing of 
people's rights, and without allowing and being

117
00:13:29,680 --> 00:13:35,439
tools that are being used to infringe on people's 
people's rights. But the relationship, the structure

118
00:13:35,440 --> 00:13:41,120
of relationships between power is such that even 
if it's possible, even if it doesn't cost much,

119
00:13:41,120 --> 00:13:45,280
even if it's not going to affect the profit 
margins, it's probably not going to happen.

120
00:13:45,840 --> 00:13:50,400
But, it also, sometimes, conflicts with 
the profit margin in very funny ways.

121
00:13:50,400 --> 00:13:54,800
So, you know, if you need, if 
from the perspective of an activist,

122
00:13:55,440 --> 00:14:04,480
some very normal features can be quite annoying,
can be quite problematic: real-name policies on Facebook,

123
00:14:05,600 --> 00:14:11,360
rate limits on Twitter, 
or anything like that. That is actually

124
00:14:11,360 --> 00:14:15,520
problematic. If you're trying to mobilize people 
the way mobile companies are trying to monetize

125
00:14:15,520 --> 00:14:22,240
every single transaction, that limits what we can 
do. But this is the business model, and, you know, I

126
00:14:22,240 --> 00:14:27,120
don't expect neither Twitter, nor Facebook, nor 
the mobile companies to change their business

127
00:14:27,120 --> 00:14:31,840
models just for activists. So, that is not going 
to happen. But here's something that could happen,

128
00:14:33,520 --> 00:14:40,400
you can, you know, companies, if governments are 
trying to pass legislation, or you know, change

129
00:14:40,400 --> 00:14:46,160
regulation, and it's going to affect their profit, 
then companies do stand up, do make a noise, do try

130
00:14:46,160 --> 00:14:52,240
and change things. But, if the same governments are 
doing something sinister that's going to affect

131
00:14:52,240 --> 00:14:57,040
humans, that's going to affect their users, they're 
not likely to talk about it. So, we've all heard

132
00:14:57,040 --> 00:15:03,360
about the kill switch, how Egypt was completely cut 
off from the internet for a few days during the

133
00:15:03,360 --> 00:15:10,720
first uprising in the Revolution. Vodafone and co, 
their defense, their constant defense is that this was

134
00:15:10,720 --> 00:15:15,920
by the law. There was nothing that they could do. 
But, they knew about that law two years in advance,

135
00:15:16,960 --> 00:15:23,040
and they never made a noise. We, in Egypt, had 
ways of fighting unjust laws: we could take it

136
00:15:23,040 --> 00:15:30,160
to the Constitutional Court, we could do a campaign 
against it. It might have been possible for us to

137
00:15:30,160 --> 00:15:35,680
get rid of that law before the revolution happened, 
if the companies had chosen to actually expose

138
00:15:35,680 --> 00:15:42,160
the fact that it happened to us, that law was 
almost secret. I think it's quite incredible how

139
00:15:42,160 --> 00:15:48,719
he predicted. Of course, I mean, stated 
back in 2011 that tech companies are

140
00:15:48,720 --> 00:15:53,200
not doing what they're supposed to do, and 
unfortunately that status quo has not changed.

141
00:15:53,200 --> 00:16:00,560
Mohamad, do you have any reactions or comments in 
regards to what Alaa had mentioned back in 2011?

142
00:16:03,600 --> 00:16:07,760
I mean, I remember that 
trip because I was also there,

143
00:16:07,760 --> 00:16:15,280
and I was just didn't know if it was 2011 
or 2012, honestly. Yeah, that was 2011. 

144
00:16:17,040 --> 00:16:23,520
I mean yes, he was totally right. We were 
also talking about these issues outside

145
00:16:23,520 --> 00:16:30,880
in the garden; it was sunny back then in San 
Francisco. I mean, I know Jillian was there,

146
00:16:30,880 --> 00:16:35,840
Castro was there, Castro Maher also our friend. 
And I'm not sure if Dia was there as well.

147
00:16:38,000 --> 00:16:41,280
But yeah, I mean he was very hopeful, and I

148
00:16:43,520 --> 00:16:47,760
also remember he went to the protest. 
There was protest back then somewhere

149
00:16:47,760 --> 00:16:54,800
near San Francisco, so, he went there in the bus 
or something. I mean, yeah he has a point,

150
00:16:57,280 --> 00:17:01,280
there's a prediction,
and unfortunately we are here now.

151
00:17:02,160 --> 00:17:12,240
We also want to wish Alaa that we can see 
him again soon, so he'll be released, so #FreeAlaa.

152
00:17:14,319 --> 00:17:24,000
Indeed, #FreeAlaa and all the political prisoners and 
activists who are behind bars as we convene

153
00:17:24,000 --> 00:17:31,120
and speak today in many of those oppressive 
regimes. Thank you so much Mohamad, and we will

154
00:17:31,120 --> 00:17:36,639
hold on to that conversation, and then discuss 
later what we can do to change the status quo.

155
00:17:37,840 --> 00:17:44,159
I will hand it over now to my 
colleague Dima, Dima Samaro, over to you.

156
00:17:45,040 --> 00:17:51,040
Thanks Marwa, and thanks also Mohamad 
for your input. All right, so just

157
00:17:51,040 --> 00:17:55,360
a quick intro: my name is Dima Samaro and 
I'm the Policy Analyst for the Middle East and

158
00:17:56,080 --> 00:18:04,080
North Africa at Access Now. So, for our next session, 
now, we will tackle the issue of ongoing and

159
00:18:04,080 --> 00:18:09,120
increase in crackdown on human rights activists, 
journalists, and opponents in the Middle East and

160
00:18:09,120 --> 00:18:15,120
North Africa to shut down digital activism 
and political activism in the region, as well.

161
00:18:15,920 --> 00:18:21,680
We have asked grassroots and journalists and 
activists from the MENA region to provide their

162
00:18:21,680 --> 00:18:28,480
testimonies, on their experience, and what has it 
changed for them, in terms of digital activism. So,

163
00:18:28,480 --> 00:18:34,640
since 2011, what has changed for digital activism, 
and social and political activism in general, and

164
00:18:34,640 --> 00:18:40,800
if they are still even affected by the 
repression of the governments in the region.

165
00:18:41,360 --> 00:18:48,639
So, we have two videos from two activists in the 
region. We will have a video from Libya Idres. 

166
00:18:48,640 --> 00:18:54,640
And Libya is editor at BBC Media Action. She's a 
journalist and a storyteller about human rights

167
00:18:54,640 --> 00:19:04,240
in Libya, and she also served, she covered 
different media and news during the revolution

168
00:19:04,240 --> 00:19:10,720
in 2011, in Libya and across the Middle East 
and North Africa. And the second speaker, we will

169
00:19:10,720 --> 00:19:18,480
have also with us a video from Emna Mizouni, and 
Emna is a human rights activist, and a human

170
00:19:18,480 --> 00:19:24,960
rights defender as well from Tunisia. She has also 
participated in these political movements and she

171
00:19:24,960 --> 00:19:32,320
will also provide her testimony. So now, we will 
have the videos and we can comment afterwards.

172
00:19:33,360 --> 00:19:38,879
Ten years ago, the night of 15th of February,  
I remember I was checking the Facebook page called:

173
00:19:41,360 --> 00:19:46,800
"Feb. 17, 2011: Day of Rage in Libya” when the old regime forces broke into our 
house to arrest my father. And I remember

174
00:19:46,800 --> 00:19:54,399
the first thing I did before I even ran to the 
door to see what's happening, was to send Facebook

175
00:19:54,400 --> 00:20:01,360
status and tweets to say they're breaking into 
our house, trying to stop us, the revolution have

176
00:20:01,360 --> 00:20:09,840
started, and so on. Today, I look back and I'm 
like, I really don't believe that 17 years old me

177
00:20:09,840 --> 00:20:17,120
did that, even though it was really dangerous. 
Back then, I grew up on stories about people that

178
00:20:17,120 --> 00:20:22,560
would lose their lives doing that. They broke 
into our house at that day because of a similar

179
00:20:22,560 --> 00:20:29,360
action, because my dad went out to news channels 
to report on the first protest that took place

180
00:20:30,480 --> 00:20:39,600
in Benghazi. But also, I believe I've done that 
because 17 years old me thought that I'm not

181
00:20:39,600 --> 00:20:46,000
really heard online, and that I've never 
experienced using Social Media for any other

182
00:20:46,000 --> 00:20:51,760
purpose rather than to connect with family and 
friends based elsewhere in the world, or to follow

183
00:20:51,760 --> 00:20:59,920
up with trends, and so on. But since then, I've 
became the active digital user that I am today,

184
00:21:00,640 --> 00:21:09,520
and I've been using the the digital 
space to practice my civil activism.

185
00:21:10,640 --> 00:21:17,120
Today, ten years after, I am the editor for  
social media platforms that promote social

186
00:21:17,120 --> 00:21:24,959
cohesion, and that are trying to 
create safe platforms for young Libyans,

187
00:21:24,960 --> 00:21:34,080
to exchange experiences, and to speak up their 
views, and to connect, and to spot the light on

188
00:21:34,640 --> 00:21:40,880
the richness of the Libyan culture, and 
to spot the light on the fact that 

189
00:21:40,880 --> 00:21:48,800
none of us is benefiting from the conflict. 
So, in the past 10 years, I have

190
00:21:48,800 --> 00:21:57,840
developed a lot of knowledge, and I've 
developed great faith in digital activism. 

191
00:21:59,040 --> 00:22:04,480
Another story that I would like to 
share with you today, to spot the light at too, 

192
00:22:04,480 --> 00:22:14,880
to say that not all stories are are positive and 
and promising. In 2014, I had a break

193
00:22:15,680 --> 00:22:23,360
from civil life. I was going through my 
personal battle, I was going through a depression

194
00:22:23,360 --> 00:22:31,360
due to the conflict, and due to myself and my 
family having to flee, being forced to flee

195
00:22:31,360 --> 00:22:40,639
the country due to the civil conflict. 
So, at that time, I disconnected. I

196
00:22:41,280 --> 00:22:46,960
didn't follow news or a reflection of news, and 
I didn't want to know anything that's happening on

197
00:22:46,960 --> 00:22:54,080
the ground, until that night in September 2014, 
when I got phone calls from people asking me

198
00:22:54,080 --> 00:23:00,399
to go online, to see a piece of news that I 
need to learn about myself. And when I did so,

199
00:23:00,960 --> 00:23:10,880
it was the news that my dearest friend, the late 
Tawfik Bensaud's assassination in Benghazi. And, 

200
00:23:10,880 --> 00:23:18,800
it was some sort of a  call, a wake-up 
call for me to figure out that disconnecting

201
00:23:18,800 --> 00:23:28,159
from news is not gonna stop news from 
happening, and not practicing my activism is not

202
00:23:28,160 --> 00:23:38,160
gonna stop building barriers, 
it's just gonna allow them to build more.

203
00:23:39,200 --> 00:23:48,880
Libya lost our dear Tawfik 
Bensaud due to his civil and political activism,

204
00:23:49,440 --> 00:23:54,480
and digital activism, and really digital 
activism back then and until today,

205
00:23:54,480 --> 00:23:59,920
is about the tool that this 
generation mastered, how to use and

206
00:24:02,080 --> 00:24:10,000
as it is in real life, as it is online, and on the 
internet, when you stop, it's going

207
00:24:10,000 --> 00:24:16,400
to be harder for you to expand, and it's going to 
be easier for others to build more barriers and to

208
00:24:16,400 --> 00:24:25,520
limit you. So, therefore I know this generation 
is not gonna stop their civil activism, and

209
00:24:26,560 --> 00:24:32,560
the internet is gonna continue 
to be the tool for us to connect,

210
00:24:33,360 --> 00:24:44,320
to open, widen our knowledge and our eyes 
on what this generation

211
00:24:44,320 --> 00:24:50,879
is going through, no matter where we are, no 
matter what we believe in, our backgrounds.

212
00:24:51,600 --> 00:24:57,040
It's easier for you to read, and to watch, and 
to learn about what's going on with others

213
00:24:58,000 --> 00:25:04,160
elsewhere, to be able to accept them the way they 
are, and to be able to learn from their experiences.

214
00:25:06,000 --> 00:25:12,960
Ten years after the Arab Spring, Tunisia is still 
facing the same issues, or even some people they

215
00:25:12,960 --> 00:25:20,960
would say, more issues than before. The reality 
is, we are so unfortunate to be celebrating the

216
00:25:20,960 --> 00:25:27,920
10th anniversary of our uprising in Tunisia by 
a huge and massive crackdown on the freedoms

217
00:25:27,920 --> 00:25:33,360
that we gained from that revolution: the freedom of 
protest; people they are arresting for the sake of

218
00:25:33,360 --> 00:25:40,000
protesting, the social and economic protest, or the 
political protest also. People they are arrested

219
00:25:40,000 --> 00:25:46,720
for their blog posts, and here blogging changed 
a lot in the last 10 years. Before the revolution,

220
00:25:46,720 --> 00:25:53,440
we were talking about people using blogs as 
platforms. Now, they're using Social Media platforms

221
00:25:53,440 --> 00:25:59,840
to express themselves. And so, many people they were 
arrested in Tunisia, unfortunately based on their

222
00:25:59,840 --> 00:26:07,199
posts on Social Media, and specifically on Facebook. 
Facebook was a very important tool. It was not a

223
00:26:07,200 --> 00:26:11,520
reason of the revolution, as some people they 
would pretend, but it was one of the tools that

224
00:26:12,720 --> 00:26:19,200
Tunisians used for their uprising. We used 
Facebook to rely the information of what

225
00:26:19,200 --> 00:26:29,120
was happening, specifically in December 2010 and 
January 2011. That crucial period,

226
00:26:29,120 --> 00:26:35,199
a lot of information was used on Facebook 
and a lot of information. And as I remember,

227
00:26:35,200 --> 00:26:42,480
a lot of, even profiles they were censored from 
Facebook. Censorship was a thing, was very very

228
00:26:43,440 --> 00:26:50,960
obvious by the Ben Ali regime, now it's not 
censorship as much as its threats on everybody's

229
00:26:50,960 --> 00:26:57,280
freedom of expression. Media was targeted at 
many points over the last 10 years. So, 

230
00:26:57,280 --> 00:27:04,960
if we continue saying that our assets: we're  
democracy, we're a successful democracy, I would say

231
00:27:04,960 --> 00:27:10,320
we're lying on ourselves, because, unfortunately,
there is a huge crackdown right now on the

232
00:27:10,320 --> 00:27:18,879
civic space. We've seen a lot of counter 
protests, and those counter protests, also, they are

233
00:27:19,840 --> 00:27:26,800
a big issue. If people who are officers, 
security officers, protesting with their

234
00:27:26,800 --> 00:27:33,360
weapons, this is a threat to the democracy, and 
this is a threat to the right to protest, peaceful

235
00:27:33,360 --> 00:27:40,240
protests are welcome, and they should be welcomed 
by everybody. As a an activist from Tunisia,

236
00:27:40,800 --> 00:27:48,000
and I'm seeing what's happening, at some 
point, I would feel like we're losing our

237
00:27:48,000 --> 00:27:55,280
revolution, we're losing the assets of this 
revolution. But in the same time, I would say,

238
00:27:55,280 --> 00:28:01,200
let's keep the hope in ourselves, let's keep 
thinking that what's happening right now is 

239
00:28:01,760 --> 00:28:09,040
a test for this democracy, is a test for the 
Tunisian people. This crisis should get us stronger.

240
00:28:09,040 --> 00:28:16,080
I think the new generation that is taking over 
the streets by protesting and continuing to

241
00:28:16,080 --> 00:28:24,639
protest has showed a massive change in the way 
of protesting. Their way of activism online and

242
00:28:24,640 --> 00:28:31,440
offline is different; it's very different because 
they learned how to protest while they have

243
00:28:31,440 --> 00:28:38,000
the freedom of protest, and freedom of expression, 
and freedom of internet as well, information is

244
00:28:38,000 --> 00:28:48,240
available. We have the access of information as an 
asset from this uprising. And so, those little tools

245
00:28:48,240 --> 00:28:55,680
are the tools that we're doing. It's very 
important now that many bodies and many activists

246
00:28:56,240 --> 00:29:03,120
would hang in and try to understand how to 
navigate the situation and to move forward,

247
00:29:03,120 --> 00:29:09,600
after 10 years, things are changing, things 
things change drastically in many ways, and so I

248
00:29:09,600 --> 00:29:16,879
think it's very important that we understand 
activism has changed. Access or the assets

249
00:29:18,160 --> 00:29:26,400
of the revolution are tools to continue forward, 
and not to just take them for granted, because as

250
00:29:26,400 --> 00:29:32,240
we are seeing, civic space, it's not taken for 
granted, freedoms in Tunisia are not taken for

251
00:29:32,240 --> 00:29:40,320
granted, or should not really be taken for granted, 
and then revolution is still ongoing, absolutely.

252
00:29:40,320 --> 00:29:47,919
I would like to thank our speakers, 
Emna and Libya, for the amazing testimonies

253
00:29:48,640 --> 00:29:54,800
that they have given, to to be able 
to understand also what's the future of

254
00:29:54,800 --> 00:30:00,159
peaceful assemblies, if the crackdown is 
going to continue or take place in our region.

255
00:30:00,720 --> 00:30:07,040
So, these were amazing 
testimonies, and again thank you so

256
00:30:07,040 --> 00:30:12,639
much. Okay, so now we are heading to 
our next session, which is also very

257
00:30:14,800 --> 00:30:21,120
relevant to the testimonies that we   
shed light on, and it's mainly related to a

258
00:30:21,120 --> 00:30:27,600
content moderation policies, and different actors 
also involved of the crackdown that is taking

259
00:30:27,600 --> 00:30:36,879
place in the region. And for this session,
mainly, we will highlight the issue of censorship

260
00:30:36,880 --> 00:30:43,760
by Social Media platforms, and how they even 
play their role in the repression, as well,

261
00:30:43,760 --> 00:30:50,240
along governments in the region. So we're going to 
explore more, to understand the policies and how

262
00:30:50,240 --> 00:30:56,960
there are some biased content moderation policies 
of Social Media platforms, and as well as unfair

263
00:30:56,960 --> 00:31:02,560
and discrimination in some of their community 
standards or terms of service, for example. And as

264
00:31:02,560 --> 00:31:08,879
a result, many activists and key activists have 
been silenced, and their voices, they couldn't

265
00:31:08,880 --> 00:31:15,200
physically raise up their voice or speak up on social 
media, because of these biased policies.

266
00:31:15,760 --> 00:31:25,040
So, now we will have in this session, I'll 
have with me Dia Kayyali, and so we

267
00:31:25,040 --> 00:31:31,040
will be having a discussion about this; it will 
be like a fireside chat, so we can ask

268
00:31:31,040 --> 00:31:38,080
questions, and just speak together 
about this issue, and understand more  

269
00:31:38,080 --> 00:31:44,720
their point of view. So, Dia is 
Associate Director of Advocacy at Mnemonic

270
00:31:46,160 --> 00:31:50,720
the umbrella organization for 
Syria archives, Yemeni archives, and

271
00:31:50,720 --> 00:31:56,960
Sudanese archives. In their role, Dia focuses 
on the real life impact of policy decisions

272
00:31:56,960 --> 00:32:02,160
made by lawmakers, and technology companies 
about content moderation, and related topics.

273
00:32:02,880 --> 00:32:09,680
Previously, Dia served as a Program Manager for 
tech + advocacy at WITNESS and they got their

274
00:32:09,680 --> 00:32:14,240
start in the digital rights as an activist  
at the Electronic Frontier Foundation.

275
00:32:15,600 --> 00:32:20,719
So, I'm really glad to have you here with us Dia, 
and thank you so much for joining the discussion.

276
00:32:21,600 --> 00:32:30,000
Okay, so we can start by asking a question 
about content moderation policies, and also

277
00:32:30,880 --> 00:32:36,480
how do you see this, have been shift in the 
region, what do you see the role of Facebook

278
00:32:36,480 --> 00:32:41,840
for example, and Twitter, YouTube, and different 
platforms, how would they, to some extent, have also

279
00:32:42,880 --> 00:32:48,400
been part of the repression, and silencing human 
rights defenders and activists in the MENA region?

280
00:32:50,720 --> 00:32:56,160
Yeah, so, thanks for having me and, 
I have to say I'm just really honored to

281
00:32:56,160 --> 00:33:04,720
be a part of this discussion. You know, on 
on my end, I was in the U.S in 2010 and 2011,

282
00:33:04,720 --> 00:33:11,040
and at the same time, as you know, we were seeing 
all of this happening on Social Media, but

283
00:33:11,040 --> 00:33:15,760
it was also happening in the U.S. I think it's an 
interesting reminder that, you know the 

284
00:33:15,760 --> 00:33:20,879
Oscar Grant protests were happening in Oakland, and 
and I think there was just a lot of excitement

285
00:33:20,880 --> 00:33:25,680
around the world about Social Media, and people 
were looking at what was happening, and they were

286
00:33:26,240 --> 00:33:32,240
saying wow, like people can use Facebook to 
tell their story outside of their country, 

287
00:33:32,240 --> 00:33:37,840
and to organize. Unfortunately, you know, and 
I'm so glad that we saw that clip from Alaa,

288
00:33:38,880 --> 00:33:44,880
you know he was pretty psychic, exactly what he 
predicted is what we've seen, that it's really

289
00:33:44,880 --> 00:33:49,920
not in the business model of these companies 
to ensure free expression in our region,

290
00:33:50,720 --> 00:33:56,400
and in fact, unfortunately, we've seen that a lot of 
the times when they use new tools or they have new

291
00:33:56,400 --> 00:34:01,520
policies, they want to you know test, trying to 
get a certain type of content off very quickly

292
00:34:01,520 --> 00:34:08,560
by using AI or something like that, they 
do it in our region. So, 10 years later, and

293
00:34:08,560 --> 00:34:14,639
especially you know as you mentioned, I work 
with Mnemonic, and you know, we've seen just a

294
00:34:14,639 --> 00:34:21,040
huge quantity of content deleted, and for a lot of 
us, it feels like our history is getting erased. So,

295
00:34:21,600 --> 00:34:25,839
there's still a lot of process, and you know 
we also had a ray of hope for Mohamad saying

296
00:34:26,719 --> 00:34:34,159
we do still have hope, but I think there's 
been a lot of cold awakenings around, really, how

297
00:34:34,159 --> 00:34:40,000
these policies work, and how important we are to 
these companies, you know and how important our

298
00:34:40,000 --> 00:34:45,920
voices are, and and the content coming from 
our region, and oftentimes it kind of seems like

299
00:34:45,920 --> 00:34:50,880
we're really the last thing on their minds when 
they're making their policies, or creating new

300
00:34:50,880 --> 00:34:58,240
tools for content moderation. Yes, thank you 
Dia. So, I think you touch on important points,

301
00:34:59,520 --> 00:35:04,960
also that is relevant to the events that we 
are having today, because also activists as

302
00:35:04,960 --> 00:35:10,080
you mentioned, use these platforms to organize 
and mobilize, so they were seeking these

303
00:35:10,080 --> 00:35:17,040
platforms, as you know, like, a an opportunity  
to participate, to organize themselves, to be

304
00:35:17,040 --> 00:35:24,800
more active. So, of course, this has a place,
like has played a huge role in shaping 

305
00:35:24,800 --> 00:35:30,320
the internet and shaping the online space. But, 
can you mention, based on your expertise, 

306
00:35:30,320 --> 00:35:36,240
like what sort of biased policies, where do you 
think, or like if you can give examples on where

307
00:35:36,240 --> 00:35:41,839
do you think there are bias policies of these  
social media platforms? So, for example, we can

308
00:35:41,840 --> 00:35:48,720
mention a takedown of content, or unequal 
enforcement of terms of service or community

309
00:35:48,720 --> 00:35:54,160
standards of these platforms. So what do you think, 
from your experience, what do you think are

310
00:35:54,160 --> 00:36:00,640
like, some key examples, or hot topics related 
to this that has happened in the region?

311
00:36:02,320 --> 00:36:08,880
Yeah, so I want to mention a few things. So,
it's the policies, and I'm gonna talk about that

312
00:36:08,880 --> 00:36:16,080
a little bit, it's the tools, but third, and this is 
very important, it's also, as I said, how we fit into

313
00:36:16,080 --> 00:36:22,080
the business model and how important our rights 
are compared to kind of the rest of the world. What

314
00:36:22,080 --> 00:36:28,799
I mean by that, is you know how much are companies 
willing to fight for users, to challenge

315
00:36:28,800 --> 00:36:35,040
government requests for data, as opposed to 
potentially other places. So let me explain a

316
00:36:35,040 --> 00:36:42,640
little bit of what I mean. So, I'll start by the 
policies themselves. So, one of the areas, and

317
00:36:42,640 --> 00:36:47,600
this is of course, not in any way comprehensive, 
but just especially because of what I focus on.

318
00:36:48,240 --> 00:36:54,879
One of the areas where we see a huge amount of 
bias is the removal of so-called graphic content,

319
00:36:54,880 --> 00:37:00,640
or so-called terrorist and violent extremist 
content. So, we see a lot of takedowns in our region,

320
00:37:01,680 --> 00:37:08,000
just simply because the policies themselves 
are based on biased information. So, for example,

321
00:37:08,000 --> 00:37:14,320
Facebook has a dangerous organizations list. That 
list is not public, but we know that that list

322
00:37:14,320 --> 00:37:21,120
is greatly influenced by two things: one is 
U.S designations of foreign terrorist

323
00:37:21,120 --> 00:37:27,279
organizations, and the other is the U.N sanctions 
list. And these lists are almost exclusively

324
00:37:27,840 --> 00:37:34,080
groups that are either coming from 
Arabic speaking or muslim majority countries,

325
00:37:34,800 --> 00:37:41,920
and they are they have, for example, very few 
far right or white supremacist groups. So, even

326
00:37:41,920 --> 00:37:47,680
though companies talk in this neutral language of 
terrorist and violent extremist content,  what it

327
00:37:47,680 --> 00:37:53,600
ultimately comes down to is, it's a lot of content, 
including counter speech, so this is one of the

328
00:37:53,600 --> 00:38:00,000
things that we see you know when people speak out 
against, for example, Hezbollah or even Hamas,

329
00:38:00,000 --> 00:38:04,320
that content gets taken down in the same way 
as content that might actually be coming from

330
00:38:04,320 --> 00:38:09,840
those groups. So, the policies themselves, 
the organizations that are on the list,

331
00:38:10,720 --> 00:38:17,600
also the way that the policies are rolled out, 
policies and the new tools, as well. So, I mentioned

332
00:38:17,600 --> 00:38:23,360
artificial intelligence, and machine learning 
algorithms, so these tools are being trained on

333
00:38:23,360 --> 00:38:28,160
biased data. They're being trained on data that 
is being collected by people who are sitting

334
00:38:28,160 --> 00:38:35,520
in Menlo Park. They're not really clear on what  
the political situation is. The

335
00:38:35,520 --> 00:38:41,840
tools themselves have a very hard time taking into 
context. So, for example, something we just saw,

336
00:38:42,640 --> 00:38:49,279
we've seen content get taken down from protests 
in Lebanon, but again it was actually critical of

337
00:38:49,280 --> 00:38:55,680
Hezbollah, for example, but because maybe it had 
just the word in there, it got taken down

338
00:38:55,680 --> 00:39:01,600
by the automated moderation. And also the automated 
moderation itself and natural language processing,

339
00:39:01,600 --> 00:39:07,520
it does very poorly with Arabic. We've all seen 
how ridiculously sometimes Google Translate

340
00:39:07,520 --> 00:39:14,080
can translate things from from Arabic to English. 
Now imagine those kind of problems, but every day

341
00:39:14,080 --> 00:39:18,640
shifting through all the content from our region, 
all of this content that might be on the line of

342
00:39:18,640 --> 00:39:24,160
the policies, you know it's graphic, but 
because it's showing human rights violations,

343
00:39:24,800 --> 00:39:30,240
but it actually falls under a newsworthy exception, 
or a human rights exception, but then you have this

344
00:39:30,800 --> 00:39:36,400
this automated content moderation that really is 
just not equipped to understand that very delicate

345
00:39:36,400 --> 00:39:42,640
context. So, the other thing I also want to mention 
is, just, you know how much are companies willing

346
00:39:42,640 --> 00:39:49,279
to fight for users, how much are they willing, 
for example, to push back on bad policies. We are

347
00:39:49,280 --> 00:39:53,360
seeing more and more indications that actually 
companies are just going to do whatever it is that

348
00:39:53,360 --> 00:39:59,120
governments ask them to do, and of course, we have 
a very bad example of companies relationships with,

349
00:39:59,120 --> 00:40:05,759
for example, with the, you know, the Israeli 
government. But also, this isn't our region

350
00:40:05,760 --> 00:40:11,440
but it's very close, you know we just saw that 
companies agreed to comply with this Turkish

351
00:40:11,440 --> 00:40:17,520
law now that requires them to have someone in 
the country in order to continue functioning.

352
00:40:17,520 --> 00:40:21,840
You know, what other governments are they going to 
comply with? And at what point are they going to

353
00:40:21,840 --> 00:40:25,840
say, no we're not going to hand over that data, 
no we're not going to institute this policy?

354
00:40:27,200 --> 00:40:32,240
So, you know it's automation, it's how much 
companies are actually willing to engage with us,

355
00:40:32,240 --> 00:40:36,720
engage with civil society, particularly when 
civil society doesn't agree with government,

356
00:40:37,680 --> 00:40:41,680
and it's the way the policies are written 
themselves, they're just really not written

357
00:40:42,320 --> 00:40:48,000
to make these platforms, you know, great tools for 
activists. They are, as Alaa said, they're really

358
00:40:48,960 --> 00:40:53,600
made to make money ultimately, so you know, 
that's really quite a bit at the heart of

359
00:40:53,600 --> 00:41:01,120
the problem. Yes, thanks so much Dia, I 
think these are great answers. So, basically

360
00:41:01,120 --> 00:41:06,880
could we say that these platforms have 
failed to develop content context based like on

361
00:41:06,880 --> 00:41:14,240
the MENA region, that is reflected 
or like you know, because some of the context

362
00:41:14,240 --> 00:41:18,720
is more like a global, but it couldn't fit 
sometimes when we talk about the MENA region,

363
00:41:18,720 --> 00:41:23,759
that couldn't be the case. You know, we have 
seen, for example, and you mentioned something

364
00:41:23,760 --> 00:41:28,960
about  documenting violations, we are 
speaking, for example, about war crimes in

365
00:41:32,640 --> 00:41:40,720
Syria, and also, when they took many videos 
down. So, this obviously has a

366
00:41:40,720 --> 00:41:45,439
long term also impact on documentation 
and reporting on human rights violations.

367
00:41:46,160 --> 00:41:52,879
So, do you think also because usually they lack  
providing like transparency about their policies,

368
00:41:52,880 --> 00:41:58,240
and even when they take some content down they 
don't like; sometimes in some cases that we

369
00:41:58,240 --> 00:42:06,080
have received even in Access Now, we don't receive 
like a clear explanation or clarification

370
00:42:06,080 --> 00:42:11,920
on why that content has been taken down. 
So, do you think this also, you know, it's part

371
00:42:11,920 --> 00:42:18,160
of the repression, and how they also are silencing 
activists and opponents in the region.

372
00:42:19,760 --> 00:42:26,400
Absolutely, just to give you a sense of what 
this looks like, for example, the last time that

373
00:42:26,400 --> 00:42:34,640
we ran a check on our collection of content from 
Syria, we found that at least 23 percent of the

374
00:42:34,640 --> 00:42:42,240
content that we have found verified and archived 
off of the platform, this is YouTube, at least 23

375
00:42:42,240 --> 00:42:46,640
of that content is no longer available. Sometimes, 
it's no longer available because somebody made

376
00:42:46,640 --> 00:42:51,520
the video private, but a lot of times, it's because 
it got taken down, and I can tell you, and I'm sure

377
00:42:51,520 --> 00:42:56,960
that  anybody who's, you know, run a hotline 
or who talks to activists on a regular basis,

378
00:42:57,520 --> 00:43:03,280
people oftentimes get notifications, and 
they have no idea what the notification means,

379
00:43:03,280 --> 00:43:08,560
because of the way that it's written, or they just 
have no idea why their account gets taken down,

380
00:43:08,560 --> 00:43:15,840
you know, they they reach out to us, and 
they say: yeah, suddenly my account was suspended. 

381
00:43:15,840 --> 00:43:20,400
The notifications, and it depends on 
the company; some give better information

382
00:43:20,400 --> 00:43:27,280
than others, but a lot of times, it's just 
you violated our policies, and you can no longer

383
00:43:27,280 --> 00:43:34,240
post, and unfortunately, sometimes this happens 
in the middle of some big event, so we've seen

384
00:43:34,240 --> 00:43:40,799
a lot of times that there's a big surge. We saw 
a ton of accounts get shut down in Sudan

385
00:43:40,800 --> 00:43:46,240
when there was sit-ins happening there. We saw, 
as I mentioned, a lot of accounts get shut down

386
00:43:46,240 --> 00:43:53,120
just recently in Beirut, you know, and the 
companies oftentimes don't have enough staffing

387
00:43:53,120 --> 00:43:59,200
to very rapidly respond then, when we say you know 
this is this is an issue these these accounts

388
00:43:59,200 --> 00:44:04,720
need to be available now because they're giving 
really important information. And you know,

389
00:44:04,720 --> 00:44:08,959
know that we're not the only ones who've been 
tracking that kind of removal, 7amleh has done a

390
00:44:08,960 --> 00:44:14,320
lot of work just showing how massive amounts 
of content that's documenting human rights

391
00:44:14,320 --> 00:44:21,840
violations there is getting taken down as well. 
Yes, thanks so much Dia. Yeah, so I think,

392
00:44:23,840 --> 00:44:30,640
well, basically, I think these platforms  
sometimes benefit or profit from human rights

393
00:44:30,640 --> 00:44:40,319
violations. We saw also that, for example with the 
advertisements, for example,

394
00:44:40,320 --> 00:44:47,680
during the elections in 2019 in Tunisia, 
there was also like a lack of transparency about

395
00:44:48,960 --> 00:44:54,080
the democratic transition in Tunisia in 
general. So, I would say this is also you know

396
00:44:54,080 --> 00:44:58,560
like  also could apply to other countries 
in the region, because we are talking about you

397
00:44:58,560 --> 00:45:05,440
know how we can democratize our countries, and or 
like, at least transparent when it comes to these

398
00:45:05,440 --> 00:45:10,800
policies and how they are. So, what do you 
think, like, what would be part of the solution

399
00:45:10,800 --> 00:45:14,880
in your point of view? I know that you mentioned 
that engaging with civil society, having a

400
00:45:14,880 --> 00:45:22,000
discussion  could be effective. I know that we 
also have engaged different times with

401
00:45:22,000 --> 00:45:29,120
these platforms, but I don't feel like, you 
know, like a sufficient solution by the end of

402
00:45:29,120 --> 00:45:34,319
the day. I don't see many actions, you know, like are 
implemented after these discussions and meetings.

403
00:45:34,320 --> 00:45:38,960
So, what do you think could be like the solution 
to give activists and human rights defenders

404
00:45:38,960 --> 00:45:44,800
This space, and be part of the change 
like, as we talk about Arab Spring now,

405
00:45:45,360 --> 00:45:48,960
but of course how we can also have a 
concrete actions from these companies,

406
00:45:48,960 --> 00:45:54,160
instead of saying, that we will try our best 
this is what we will do, but again you don't

407
00:45:54,160 --> 00:45:58,160
have like a concrete action by the end of the day,
or eventually. So, what do you think in your point

408
00:45:58,160 --> 00:46:04,640
of view could be part of the solution to give 
activist this space again to be able to speak up?

409
00:46:07,920 --> 00:46:14,080
Yeah, so I think that this feeling of you know 
something really big happens, and then we all sort

410
00:46:14,080 --> 00:46:20,160
of do a public letter, or you know, we push 
on the companies they they respond and say okay

411
00:46:20,720 --> 00:46:25,120
we see that that's a problem, and then nothing 
ever happens, this is very common for people

412
00:46:25,120 --> 00:46:30,240
who work on content moderation and free 
expression, and it is incredibly frustrating.

413
00:46:30,240 --> 00:46:36,879
So, one of the things is  that companies do 
need to engage with civil society, and they have

414
00:46:36,880 --> 00:46:43,760
increased their amounts of engagement, but it 
needs to not just be PR. So, you know companies

415
00:46:43,760 --> 00:46:50,080
need to actually engage with civil society in 
co-design of new policies and tools, and what I

416
00:46:50,080 --> 00:46:58,160
mean by that is, for example, you know there's a 
huge global push to get rid of, as I mentioned

417
00:46:58,160 --> 00:47:02,960
you know, so-called I say so-called terrorist 
violent and extremist content because there is no

418
00:47:02,960 --> 00:47:09,120
actually agreed upon definition, but there is 
a huge push around this globally, right? So,

419
00:47:09,120 --> 00:47:14,960
if companies do want to, for example, use natural 
language processing or they do want to use

420
00:47:14,960 --> 00:47:19,360
machine learning algorithms to try to detect and 
take down content, then they need to sit down with

421
00:47:19,360 --> 00:47:25,760
us and talk to us about: here's the 
training data that I'm using for this algorithm,

422
00:47:25,760 --> 00:47:31,200
here are some of the you know outputs that 
I get from this training data when I feed

423
00:47:31,200 --> 00:47:37,600
it into the machining algorithm. So, actually at the 
very ground level, sitting down with civil society

424
00:47:37,600 --> 00:47:43,200
and designing tools and policies together instead 
of, once they're already designed and we have these

425
00:47:43,200 --> 00:47:49,359
huge problems when they kind of get released into 
the platform ecosystem, and then you know we're all

426
00:47:49,360 --> 00:47:55,760
struggling to try to address it after the fact.
So, actually engaging at the ground level with

427
00:47:55,760 --> 00:48:01,600
civil society in the way that they're functioning, 
being much clearer about their relationships with

428
00:48:01,600 --> 00:48:07,759
government, you know, I think increasingly 
these companies have so much power over our

429
00:48:07,760 --> 00:48:12,720
lives that they should be clear about, you know, 
this is when we talk to a government, this is

430
00:48:12,720 --> 00:48:17,120
when we comply with the government, and of course, 
they're doing transparency reports that talk about

431
00:48:17,120 --> 00:48:22,240
legal requests, but that's not enough. All of 
their transparency reports need to have more

432
00:48:22,240 --> 00:48:28,959
granular information, so much more specific 
information; and also when they give notices to

433
00:48:28,960 --> 00:48:34,960
users, those notices need to actually explain 
why content was taken down, what was the

434
00:48:34,960 --> 00:48:41,600
policy under which the content was taken down, and 
how can you appeal it. So, you know, making sure

435
00:48:41,600 --> 00:48:47,839
that people are clear on their rights of 
appeal, and also just having enough staff in our

436
00:48:48,560 --> 00:48:53,759
region. They struggled, you know, I 
know at least one company has really struggled

437
00:48:53,760 --> 00:49:02,240
to maintain staff, to talk to civil society; so 
making sure that they are hiring enough people,

438
00:49:02,240 --> 00:49:08,319
that they are also taking into account the biases 
of their own employees, as well a lot of times

439
00:49:08,320 --> 00:49:13,440
we've heard, for example, like LGBTQ related content 
gets taken down a lot, and people feel like that

440
00:49:13,440 --> 00:49:18,800
might be because of the the content moderators 
or the employees themselves. So just looking at

441
00:49:18,800 --> 00:49:25,360
things like that. And you know, all of these 
things ultimately are aimed at pulling the company

442
00:49:25,360 --> 00:49:32,240
as much as possible from their profit motive, of 
just making money, which is this is why it's so

443
00:49:32,240 --> 00:49:37,439
hard for us, you know, to actually being accountable 
to the people who are using the platform,

444
00:49:37,440 --> 00:49:43,040
and to human rights, and I think that's really 
at the heart of our work, is pulling them

445
00:49:43,040 --> 00:49:49,520
towards that. And, sometimes we're seeing success,  
a lot of times it's just, it's quite frustrating.

446
00:49:49,520 --> 00:49:54,400
I also want to echo what Mohamad said 
earlier, of it's really important as much as

447
00:49:54,400 --> 00:49:59,840
possible for us to be talking to each other, and 
not just talking to each other in our region,

448
00:49:59,840 --> 00:50:05,440
but also having ties with people in other places, 
you know. I've seen some really interesting

449
00:50:05,440 --> 00:50:12,800
examples of groups from Beirut talking to 
groups from Chile, or you know there's a lot

450
00:50:12,800 --> 00:50:17,760
happening with Social Media platforms in India 
right now, and how activists are getting kicked

451
00:50:17,760 --> 00:50:23,680
off there. What does it look like for us, for 
example, to be forming solidarity with farmers,

452
00:50:24,240 --> 00:50:30,319
in the farmers protests in India, or with all 
of these other movements and activists around

453
00:50:30,320 --> 00:50:36,000
the world. So, I know you're asking what the 
companies should do, but also what we have our

454
00:50:36,000 --> 00:50:41,600
own power to do is also to be doing more of 
this cross-border solidarity with each other,

455
00:50:41,600 --> 00:50:46,240
and making sure that they can't ignore us, 
because it's really easy for them to ignore

456
00:50:46,240 --> 00:50:52,720
voices that are outside of the U.S and Europe, and 
in particular western Europe. So, we can raise our

457
00:50:52,720 --> 00:50:57,279
voices together, and I think we are seeing more 
of that, and that's something that gives me hope.

458
00:51:00,720 --> 00:51:05,600
Thank you so much Dia, that's a really powerful 
statement and thank you again for joining

459
00:51:06,160 --> 00:51:11,279
this discussion. Thanks for sharing 
your insights and for your reflection.

460
00:51:11,840 --> 00:51:17,600
So, now, we will move to the next session 
so I'll hand it over to my colleague

461
00:51:17,600 --> 00:51:28,400
Marwa. Thanks Dima, and thanks Dia again 
for all the work you do, and I know for

462
00:51:28,400 --> 00:51:32,640
those of us who work on content moderation 
issues sometimes it feels like Sisyphus work,

463
00:51:33,280 --> 00:51:40,320
but, we need to continue doing what we're doing. 
Now, moving on to the issue of surveillance,

464
00:51:40,320 --> 00:51:46,480
it is no secret that many Arab governments have 
invested millions of dollars to acquire the

465
00:51:46,480 --> 00:51:52,880
latest digital surveillance technologies, to spy 
on their citizens, to target human rights defenders,

466
00:51:52,880 --> 00:52:01,280
and also to commit human rights violations. 
Probably, in this discussion, the first

467
00:52:01,280 --> 00:52:07,680
example that comes to mind is the brutal murder of 
the Saudi journalist Jamal Khashoggi, and the role

468
00:52:08,880 --> 00:52:17,600
the Israeli company, NSO group, has played into 
facilitating that crime through spying 

469
00:52:17,600 --> 00:52:24,960
on his colleagues up to the point of his murder. 
Another example, is the case of the Emirati human

470
00:52:24,960 --> 00:52:31,200
rights defender Ahmed Mansoor, who's serving 
a 10-year prison sentence, also spied on by

471
00:52:31,200 --> 00:52:40,720
the government using the infamous NSO pegasus 
spyware. And so, we've been documenting many

472
00:52:40,720 --> 00:52:46,720
examples and cases of activists who have been the 
target of such malicious surveillance campaigns

473
00:52:47,920 --> 00:52:57,120
together with other organizations, such as Citizen Lab, 
and many others in our space. So, we will talk

474
00:52:57,120 --> 00:53:04,319
today, first, I would love to  start with Fouad  
Abdelmoumni, a human rights activist joining

475
00:53:04,320 --> 00:53:10,720
us from Morocco, and also a great ex-colleague of 
mine. Thank you so much Fouad for joining us.

476
00:53:10,720 --> 00:53:17,520
You've recently shared with 
us your courageous testimony being the target of

477
00:53:19,840 --> 00:53:27,680
surveillance, especially the case of basically 
the NSO hacking into WhatsApp, there have been at

478
00:53:27,680 --> 00:53:38,640
least 1400 users, activists, lawyers, journalists, who 
were the targets of this exploitation. So, could you

479
00:53:38,640 --> 00:53:45,359
share with us your experience being 
the target of this campaign, and

480
00:53:46,480 --> 00:53:55,600
what that means for your work as a human 
rights activist? Over to you Fouad. Thank you

481
00:53:55,600 --> 00:54:04,640
very much for having me with you in this session, 
and for all the work you are doing.  

482
00:54:08,160 --> 00:54:24,799
I was informed by the Citizen Lab, on 2019, that 
my phone had been targeted with this spyware, and

483
00:54:27,600 --> 00:54:30,000
it simply

484
00:54:32,720 --> 00:54:38,959
confirmed that I was targeted, 
through different means, 

485
00:54:40,400 --> 00:54:54,080
in my intimacy, by services that wanted to have 
anything about, not any dangerous armored violent

486
00:54:54,080 --> 00:55:05,279
action, but on my civil society activities, as 
promoter of human rights and transparency. And,

487
00:55:06,480 --> 00:55:10,800
at that time, we agreed we were 
eight persons who agreed to

488
00:55:12,160 --> 00:55:16,640
go public on this intrusion in our intimacy and

489
00:55:18,800 --> 00:55:20,960
violation of our

490
00:55:21,920 --> 00:55:31,200
rights. There was no official reaction by 
the state of Morocco, but we had different

491
00:55:33,760 --> 00:55:41,200
exacerbations of the action of the state, 
and its different apparatus against

492
00:55:42,320 --> 00:55:58,160
against us, and one of the main promoters of this 
campaign of publicity about the spyware, Maati Monjib,

493
00:55:58,960 --> 00:56:10,560
was afterwards attacked on different kinds of 
accusations that were not directly political.

494
00:56:10,560 --> 00:56:25,040
He was accused  of having receiving 
money from abroad, laundering money,

495
00:56:25,040 --> 00:56:35,040
things like that, while it was simply for the 
his civil activities. And then, he was

496
00:56:36,560 --> 00:56:42,560
incarcerated and on another trial,
he was condemned for two one-year

497
00:56:43,440 --> 00:56:54,560
prison, and other of his companions were 
also condemned to jail. For myself,

498
00:56:56,720 --> 00:57:02,640
I saw tens of people receiving

499
00:57:05,760 --> 00:57:19,200
videos on myself and my partner having sex, in 
our home. So, it meant that they didn't have only

500
00:57:20,640 --> 00:57:31,600
the spyware on the phone, and probably on
all other computer or technical means, but also

501
00:57:31,600 --> 00:57:44,400
they had cameras, very sophisticated ones inside 
different parts of our homes. And they had no fear,

502
00:57:44,960 --> 00:57:58,480
no complex, giving these elements to the public. So, 
of course, we can understand that spyware could

503
00:57:58,480 --> 00:58:09,280
be used because you have sometimes against some 
situations the risks of for the life, and the

504
00:58:10,720 --> 00:58:20,959
civil security. But, the problem in our countries 
is that it's not used mainly against any terrorist

505
00:58:20,960 --> 00:58:32,800
risk, but it's basically against civil society, 
pacific activists who simply are promoting

506
00:58:32,800 --> 00:58:47,120
the basic rules of good governance,
of freedom, and stopping impunity of our rulers.

507
00:58:49,200 --> 00:59:02,240
Now, we see that in different situations,  
the rulers are getting harsher, are getting

508
00:59:02,240 --> 00:59:11,680
ruthless with this kind of means, but still, 
I think we have to remain confident, to

509
00:59:12,240 --> 00:59:24,640
maintain our faith that we are still on 
a good path, because I can testify, when 

510
00:59:24,640 --> 00:59:34,960
I was a young man in the 70s and 80s of the 
last century, we could never enjoy or think of

511
00:59:34,960 --> 00:59:41,520
this kind of discussions we are having now, and 
all the strength it can give, and friends that

512
00:59:44,560 --> 00:59:54,160
don't allow our rulers to go to the 
tools they used to use against any

513
00:59:54,720 --> 01:00:02,319
opponent, I mean, disappearance, incommunicado detention 
for years, things that I

514
01:00:02,960 --> 01:00:14,000
I had to live myself when I was in 
my teenage and my twenties. Thank you.

515
01:00:14,000 --> 01:00:19,840
Thank you so much Fouad, first of all for 
sharing with us your lived experience.

516
01:00:19,840 --> 01:00:27,760
I can only imagine you know how hard it must 
be for you. I also wanted to follow up

517
01:00:27,760 --> 01:00:35,280
quickly, because this is not the first case 
that we hear of the Moroccan authorities

518
01:00:35,280 --> 01:00:42,000
using sophisticated spyware to spy on Moroccan 
activists. The last case we've heard of is the

519
01:00:43,040 --> 01:00:48,720
the targeting of the journalist Omar Radi, 
who's also facing bogus charges related to 

520
01:00:49,520 --> 01:00:57,200
sexual assault, among other charges. So, how are 
moroccan civil society organizations fighting

521
01:00:57,200 --> 01:01:05,759
against this surveillance that obviously is 
taking place in an environment of impunity?

522
01:01:09,040 --> 01:01:18,720
I would say that we don't feel that we 
we are in the state of law where you could

523
01:01:19,280 --> 01:01:27,040
have this kind of illegitimate 
behavior by state apparatus

524
01:01:29,360 --> 01:01:39,360
investigated and condemned. So basically, nobody 
is going to the justice apparatus to ask

525
01:01:39,920 --> 01:01:50,240
for any questioning of this kind of behaviors. I 
would say that the main thing is to get it on the

526
01:01:50,240 --> 01:02:00,640
public scene in a clear way, in a strong way, in 
a collective way, in a way that is shared by all

527
01:02:02,400 --> 01:02:09,840
serious actors of civil society, 
and I think that this is

528
01:02:11,920 --> 01:02:22,400
making the political cost of this rogue 
state behavior getting worse, and hopefully

529
01:02:23,520 --> 01:02:31,360
come to a moment to stop it, or at least 
make it become again marginal, and not

530
01:02:32,400 --> 01:02:39,520
mainstream. Yeah, thank you Fouad, indeed. 
I think one of the reasons why

531
01:02:40,480 --> 01:02:46,720
this problem continues to happen is the 
international ecosystem, so to speak, in which

532
01:02:47,440 --> 01:02:53,600
private companies and governments continue 
to operate and trade these surveillance

533
01:02:53,600 --> 01:03:00,480
technologies without a shred of transparency or 
accountability, and we'll speak on this

534
01:03:00,480 --> 01:03:07,520
issue in a bit with the Natalia. But for now, 
I'm looking at you Maryam Al-Khawaja. Maryam,

535
01:03:07,520 --> 01:03:12,960
a dear friend and also a prominent 
Bahraini human rights defender.

536
01:03:15,680 --> 01:03:23,600
So, I think, Najem had kind of alluded to 
the Gulf countries, one of the wealthiest in

537
01:03:23,600 --> 01:03:30,000
our region, and how they've used spyware 
to not only spy on citizens who live in

538
01:03:30,000 --> 01:03:35,120
those countries, but also those who managed 
to escape the oppression and live

539
01:03:35,120 --> 01:03:41,440
in exile, and now we're hearing more reports 
about the UAE and other countries trying to

540
01:03:41,440 --> 01:03:46,800
develop their own home ground surveillance 
technologies, so they are ambitious in

541
01:03:46,800 --> 01:03:53,760
their pursuit, so can you speak to us about your 
personal experience as well as experiences from

542
01:03:53,760 --> 01:04:00,640
from the Gulf region, from Bahrain of course and 
from the Gulf region? Thank you Marwa, and

543
01:04:00,640 --> 01:04:04,480
thank you for having me here, I appreciate it, 
and for organizing this amazing panel, it's

544
01:04:04,480 --> 01:04:09,600
been really interesting listening to all of my 
colleagues speak on these issues. Yes, I think

545
01:04:09,600 --> 01:04:14,560
you know, it's it's only natural that when talking 
about surveillance technology and human rights,

546
01:04:14,560 --> 01:04:19,440
that the Gulf is going to come up over and over 
again, given the kind of resources that they

547
01:04:19,440 --> 01:04:25,840
have, and their ability in both purchasing, but also 
making sure that there is a lack of accountability

548
01:04:26,560 --> 01:04:34,560
around the use of spyware inspired technology, 
pretty much in the region. I think, you

549
01:04:34,560 --> 01:04:42,799
know, for me personally, I've been the target of 
potential attempts to take over my email, to

550
01:04:42,800 --> 01:04:49,440
hack my devices, and so on. Thankfully  we've had 
some great colleagues like Bill Marczak at Citizen Lab,

551
01:04:49,440 --> 01:04:55,200
like Mohammed Al-Maskati from Front Line Defenders 
and others, who have really made a huge effort 

552
01:04:55,200 --> 01:05:00,399
in making sure that we're aware of how to protect 
ourselves online, and looking for the red flags

553
01:05:00,400 --> 01:05:06,480
once that sort of thing does happen. And so, even 
though there were attempts where we actually were

554
01:05:06,480 --> 01:05:12,880
able to verify that the attachment that was sent 
to me did actually have spyware in it, and had I

555
01:05:12,880 --> 01:05:17,680
downloaded it and opened it, it would 
have downloaded the spyware into my device, but

556
01:05:17,680 --> 01:05:24,160
because I knew to look for the red flags, 
I was able to avoid that. That being said, I think

557
01:05:24,160 --> 01:05:30,799
especially now with the normalization deal that 
the Gulf has entered into, we're looking at a

558
01:05:30,800 --> 01:05:35,120
situation where things are going to become, and 
have become more difficult for Gulf activists,

559
01:05:35,120 --> 01:05:39,920
because we're looking at a situation where the 
exchange of spyware and surveillance technology

560
01:05:39,920 --> 01:05:46,000
is going to happen in an even more smooth 
transaction, and this is something that I

561
01:05:46,000 --> 01:05:51,120
think for most of civil society in the Gulf, as 
soon as we heard about the normalization deals,

562
01:05:51,120 --> 01:05:55,839
that was one of the first alarms to go off for 
us, is that we already knew that NSO was being

563
01:05:55,840 --> 01:06:00,720
used against us, we already knew that surveillance 
technology beyond NSO, as well was being used by

564
01:06:00,720 --> 01:06:05,839
our regimes and targeting us, and now with the 
normalizations deals, we can only expect it to get

565
01:06:05,840 --> 01:06:11,440
worse, especially looking at that kind of  
surveillance and technology that's being used

566
01:06:11,440 --> 01:06:18,720
against Palestinian activists, and Palestinian 
civil society in the same frame. Beyond that,

567
01:06:18,720 --> 01:06:24,000
I think, you know, when you talk about the 
the occupation, and how they use surveillance

568
01:06:24,000 --> 01:06:27,600
technology against Palestinians, the way when 
you talk about the Gulf states and how they use

569
01:06:27,600 --> 01:06:30,799
surveillance technology, that doesn't necessarily 
come as a surprise, that's something that's more

570
01:06:30,800 --> 01:06:34,240
expected we know these are oppressive governments,
we know these are governments that break

571
01:06:34,240 --> 01:06:41,200
you know violate human rights, and regularly 
get away with it. I think part of the conversation

572
01:06:41,200 --> 01:06:46,319
that really needs to be shifted and focused on is 
the double standards within places like the EU, the

573
01:06:46,320 --> 01:06:53,920
European union, and the lack of real accountability 
and regulation around the selling of

574
01:06:53,920 --> 01:07:00,000
spyware technology and surveillance technology to 
oppressive governments from the EU, from EU-based

575
01:07:00,000 --> 01:07:06,320
companies. I'll give you a very quick example,
we found out that the Danish government had

576
01:07:06,320 --> 01:07:12,320
approved of a sale of mass surveillance technology 
to Saudi Arabia, that even the British had denied.

577
01:07:12,960 --> 01:07:17,360
The Danes then went ahead and approved it,
and this mass surveillance technology

578
01:07:17,360 --> 01:07:22,720
was sold to Saudi Arabia. We don't know if it's 
been used beyond the Saudi borders because as we

579
01:07:22,720 --> 01:07:27,279
know in the GCC, the security agreement between 
the different countries means if one country

580
01:07:27,280 --> 01:07:31,120
receives a tool it's very likely the rest of 
the countries receive it and use it as well.

581
01:07:31,680 --> 01:07:37,120
And so, what we know about the mass surveillance 
technology that was approved for sale by the

582
01:07:37,120 --> 01:07:40,720
Danish government, you know, a government that 
presents itself is very much on the forefront

583
01:07:40,720 --> 01:07:47,040
of human rights  fighting for especially, when 
it comes to gender equality and so on, this mass

584
01:07:47,040 --> 01:07:52,080
surveillance technology may have actually been 
used in the targeting of the women human rights

585
01:07:52,080 --> 01:07:56,880
defenders who were imprisoned and tortured 
like Loujain al-Hathloul, like others. It may have

586
01:07:56,880 --> 01:08:01,360
also, if it was shared with the Bahrainis, it may 
have also been used in the targeting of myself,

587
01:08:01,360 --> 01:08:07,120
my sister, and my father, who are Danish citizens. 
And so, you can see like the the link and how

588
01:08:07,680 --> 01:08:14,480
these double standards actually create this, 
you know, series of violations that don't stop in

589
01:08:14,480 --> 01:08:19,520
in one place, but they actually have a ripple 
effect in different areas as well. And so, I think

590
01:08:19,520 --> 01:08:23,680
that's one of the examples 
of where, you know, there are these double standards

591
01:08:23,680 --> 01:08:27,520
where governments say, you know we care about, 
for example, gender equality and women's rights,

592
01:08:27,520 --> 01:08:31,760
but then when it comes to the women the rights of 
women in Saudi Arabia maybe not so much, you know.

593
01:08:31,760 --> 01:08:37,120
And then, the other example is the surveillance 
that's been done under COVID, you know, where we're

594
01:08:37,120 --> 01:08:43,040
seeing the bracelets and other things and 
apps that have been developed where, as a citizen

595
01:08:43,040 --> 01:08:49,040
of that country again, mass surveillance where as a 
citizen of that country, to access, you know, the

596
01:08:49,040 --> 01:08:53,760
services that you need, you can't live without 
it, you have to access those services, you are

597
01:08:53,760 --> 01:08:57,520
permitting and you are actually downloading 
the very app that is going to be used to

598
01:08:57,520 --> 01:09:03,600
to survey you. And so, we're in these positions 
where even situations like a global pandemic

599
01:09:03,600 --> 01:09:08,160
is being used as a prime opportunity to do 
mass surveillance against these populations.

600
01:09:10,960 --> 01:09:14,000
Thank you Maryam, and I 
think it is very important

601
01:09:14,000 --> 01:09:19,279
to highlight what you already mentioned, that 
oppression in our region has become transnational

602
01:09:19,279 --> 01:09:24,639
over the years. It's no longer 
about the UAE surveilling its own citizens,

603
01:09:24,640 --> 01:09:30,160
but also surveilling those activists that  
live in Egypt, or live in in Syria, and

604
01:09:30,160 --> 01:09:36,639
other places, not only the UAE, but it's shared, and 
the new geopolitics of the region also reinforce

605
01:09:37,359 --> 01:09:44,000
that alliance of human rights 
abusers coming together. I guess the

606
01:09:44,000 --> 01:09:50,560
last  point we want to address here in this 
conversation regarding surveillance is how can we

607
01:09:52,720 --> 01:09:56,160
bring this problem to an end 
by exposing companies that

608
01:09:56,160 --> 01:10:03,280
that operate pretty much in the dark, and also 
those surveillance technologies that come from

609
01:10:03,280 --> 01:10:11,599
the EU, or from the U.S, or Canada, and being  
exported to dictatorships in

610
01:10:11,600 --> 01:10:17,360
different parts of the world, not only the MENA 
region. So, I am turning to my colleague Natalia

611
01:10:18,000 --> 01:10:27,920
Krapiva, who is our legal tech council to 
share our bit on the work we and other people

612
01:10:27,920 --> 01:10:32,880
or organizations in the space doing on this 
from the legal and the regulation front.

613
01:10:34,400 --> 01:10:39,360
Thank you Marwa, and thanks so much for all the 
speakers, and Maryam also highlighting the

614
01:10:39,360 --> 01:10:45,440
issue of EU export controls which I'm gonna 
touch upon. So, yeah I would like to focus

615
01:10:45,440 --> 01:10:51,040
on the attempts to stop proliferation of this 
spyware technologies across the MENA

616
01:10:51,040 --> 01:10:58,960
region and beyond, focusing particular on export 
control regulations and litigation. So for the EU

617
01:10:58,960 --> 01:11:05,760
export control regime, if you know back in 
2011, the EU decided to officially review

618
01:11:05,760 --> 01:11:10,880
their  rules for the so-called dual 
use surveillance items; dual use being

619
01:11:11,760 --> 01:11:17,040
software technology, or other goods that are 
meant for both civilian and military purposes.

620
01:11:17,760 --> 01:11:23,280
So, that review was in fact driven in part by 
the Arab Spring and the human rights violations

621
01:11:23,280 --> 01:11:31,840
that were uncovered during that event, and also the 
revelations of European technologies being sold

622
01:11:31,840 --> 01:11:41,280
to the Arab dictators, and that contributed 
to the abuses that happened, and so and. Then, also

623
01:11:41,280 --> 01:11:46,880
throughout the years, as the negotiators were 
sort of discussing these rules, there were also

624
01:11:46,880 --> 01:11:52,560
a number of surveillance abuses that have been 
revealed, which have already been mentioned, like

625
01:11:52,560 --> 01:12:00,240
targeting of Bahraini activists that was 
done by FinFisher Gamma International, which is a

626
01:12:00,240 --> 01:12:08,960
UK and and German  company. There was also NSO 
group targeting of Ahmed Mansoor, the killing of

627
01:12:08,960 --> 01:12:17,600
Jamal Khashoggi, and the the famous WhatsApp 
hacking scandal which also Fouad spoke about,

628
01:12:17,600 --> 01:12:26,320
and unfortunately been a victim of. So, the EU 
proposed like early drafts of this recast of

629
01:12:26,320 --> 01:12:32,480
the existing rules, and they were very positive 
in the beginning, and we, Access Now, and other civil

630
01:12:32,480 --> 01:12:38,559
society organizations, we who are working  
on this, we were very happy about those rules,

631
01:12:38,560 --> 01:12:42,800
they included a lot of provisions that we asked 
for in order to ensure human rights protections.

632
01:12:43,360 --> 01:12:52,000
But, the process got stuck, unfortunately, over 
many years because of some of the EU member states

633
01:12:52,000 --> 01:12:59,360
were actually, as it was revealed, they were caring 
more about profit than human rights and were

634
01:12:59,360 --> 01:13:06,240
listening more to the companies than the civil 
society. So, for example, UK, Sweden, and Finland,

635
01:13:06,240 --> 01:13:11,120
there was leaked documents that revealed that 
they were very critical of this efforts and that

636
01:13:11,120 --> 01:13:17,440
they were just pushing for weaker human rights 
protections. And so, that, of course, had an effect on

637
01:13:17,440 --> 01:13:25,679
the resulting compromise draft that we saw in 2020,
that was agreed upon. It was really a far cry of

638
01:13:25,680 --> 01:13:33,840
from what we as civil society asked for. For 
example, the final draft doesn't have the

639
01:13:34,720 --> 01:13:39,840
the appropriate human rights standards that 
we want it to be included, there's no mandatory

640
01:13:39,840 --> 01:13:47,840
human rights impact assessment, the mechanism for 
emergency break provisions is only partial,

641
01:13:47,840 --> 01:13:53,920
it's inadequate, there's other things like 
stand-alone EU control list that would allow

642
01:13:53,920 --> 01:13:58,880
EU institutions and not companies to decide 
which technologies to include on this list,

643
01:13:59,520 --> 01:14:05,360
so that wasn't met, and there was some transparency 
provisions that were included, but they were

644
01:14:05,360 --> 01:14:13,839
again only partial and still allowed leeway to 
companies to not disclose data that's required.

645
01:14:14,800 --> 01:14:20,000
And so, even though, we do welcome the
several positive provisions that were included,

646
01:14:20,000 --> 01:14:26,560
but, overall, it's just been a disappointment 
that after 10 years after the Arab Spring, and

647
01:14:26,560 --> 01:14:32,160
so many egregious violations facilitated by 
these technologies in the MENA region and beyond,

648
01:14:33,280 --> 01:14:36,639
EU member states are just 
unwilling to stick to their

649
01:14:37,200 --> 01:14:42,000
original commitments of protecting human rights 
and limiting the sales of these technologies.

650
01:14:42,640 --> 01:14:48,240
However, I just want to highlight,
it's not EU is not the only example. Of course,

651
01:14:48,240 --> 01:14:54,960
there's also efforts in U.S and Canada currently, 
and looks like there might be more room there

652
01:14:54,960 --> 01:15:00,160
to get more, sort of in terms of human rights 
protections, and with the new Biden administration,

653
01:15:00,160 --> 01:15:04,800
hopefully we'll see where, you know, they 
will listen to the civil society voices more.

654
01:15:05,600 --> 01:15:12,000
So that's on the regulation side, and then 
on the litigation, so there's been also a

655
01:15:12,000 --> 01:15:20,240
number of efforts: the 2019, there was a file 
criminal complaint filed against the FinFisher, 

656
01:15:20,240 --> 01:15:27,440
a company that was revealed they were spying on 
activists in Turkey. So, I mean that

657
01:15:27,440 --> 01:15:32,559
with that company's technologies were used to 
spy on activists in Turkey, so there is

658
01:15:33,120 --> 01:15:39,519
some criminal investigation happening in 
Germany, and then NSO, of course, faced multiple

659
01:15:39,520 --> 01:15:47,600
lawsuits around the world starting from 2018 
there are multiple lawsuits in Israel, Cyprus.

660
01:15:48,240 --> 01:15:57,040
Omar Abdulaziz filed a case in Israel against NSO. 
There's also Amnesty International tried to, they led

661
01:15:57,040 --> 01:16:02,880
a campaign to revoke NSO's expert license 
in Israel. Unfortunately, that was not successful.

662
01:16:04,080 --> 01:16:07,440
And then, there were a number of cases that 
are still being brought against the

663
01:16:07,440 --> 01:16:14,000
governments that use that technology to target 
activists and journalists, such as, for example

664
01:16:14,000 --> 01:16:21,440
Jamal Khashoggi's fiancee filed a case in the U.S. 
But, I wanted to highlight briefly the,

665
01:16:21,440 --> 01:16:26,000
another recent case which is 
interesting because it's filed by another

666
01:16:26,000 --> 01:16:32,640
tech company, which is WhatsApp and 
Facebook against NSO group. So, in California,

667
01:16:33,600 --> 01:16:40,560
we saw a case filed by WhatsApp against 
NSO for targeting their servers actually

668
01:16:40,560 --> 01:16:46,960
in California to deliver the pegasus spyware to 
the devices of the victims. We know it's over a

669
01:16:46,960 --> 01:16:52,480
thousand customers overall and about, and more than 
100 civil society members that were targeted.

670
01:16:53,440 --> 01:17:00,719
And so, that case is currently on the 
appeal, at the Ninth Circuit, the appealing

671
01:17:00,720 --> 01:17:07,200
jurisdiction, so basically NSO is arguing that they 
should just evade all accountability because they

672
01:17:07,200 --> 01:17:14,080
were following government orders, 
that the government's targeted the individuals

673
01:17:14,080 --> 01:17:21,200
and not NSO group. And so, we are challenging them 
on that, and Access Now and other organizations 

674
01:17:21,200 --> 01:17:28,320
intervened  with amicus brief highlighting 
the stories of the victims of this NSO hack,

675
01:17:29,440 --> 01:17:35,839
and it's from India, Rwanda, Togo, and Morocco 
and Fouad who spoke earlier, he is one of those

676
01:17:35,840 --> 01:17:42,000
brave individuals who shared his experiences with 
us in the court and we included his testimonial

677
01:17:42,000 --> 01:17:48,560
in our brief, so we do hope that the court will 
consider that the impact that it had

678
01:17:48,560 --> 01:17:56,640
on the victims of the of the hacking. And 
so, yeah, so just to sum up, so there's been,

679
01:17:56,640 --> 01:18:02,320
the progress has been very slow, both on 
regulation side and the litigation side, but

680
01:18:02,320 --> 01:18:08,719
I think we should as civil society we should keep 
fighting and highlighting the voices of victims

681
01:18:08,720 --> 01:18:15,040
of these abuses, and insist on accountability, 
and eventually putting an end to this abuses.

682
01:18:17,600 --> 01:18:24,800
Thank you so much Natalia. I'm conscious of time, so 
I will invite all of our panelists to join us now

683
01:18:24,800 --> 01:18:31,920
for final discussion of today. We've received 
a couple of questions from our audience, so

684
01:18:32,640 --> 01:18:40,560
we can, let me see the questions that 
we have, and then we can jump into

685
01:18:40,560 --> 01:18:43,680
concluding remarks, if you guys would like to

686
01:18:45,760 --> 01:18:52,320
share final thoughts with us. So, 
I guess maybe I'll start with you Natalia,

687
01:18:52,320 --> 01:18:58,080
given that you're the lawyer in the room. So, 
are there examples of strategic litigation that

688
01:18:58,080 --> 01:19:07,840
has succeeded to pressure tech companies into 
better behavior as it relates to the MENA region?

689
01:19:09,120 --> 01:19:14,640
I can't think of examples in the MENA region, 
but there are examples in other countries like

690
01:19:14,640 --> 01:19:20,560
with internet shutdowns, for example, we saw some 
companies being sued, for example, for facilitating

691
01:19:20,560 --> 01:19:28,880
shutdowns and that being successful, and 
like shutdowns ending, and then

692
01:19:28,880 --> 01:19:34,800
companies also revealing the information that 
necessary evidence that shows like , sort of,

693
01:19:34,800 --> 01:19:40,640
who ordered the shutdowns and what happened. So, we 
saw some of those examples in the Africa region

694
01:19:40,640 --> 01:19:47,520
and in Southeast Asia, so there 
is some hope I think, you know, and that's why

695
01:19:47,520 --> 01:19:52,160
these efforts are important. It could be very 
discouraging because the process is very slow,

696
01:19:52,880 --> 01:19:59,280
it happens over the years, but I think we should 
keep pressuring from multiple sides, from both like

697
01:19:59,280 --> 01:20:02,400
the litigation, regulation, and 
also like overall advocacy

698
01:20:03,520 --> 01:20:08,240
to make it, you know to push companies 
to to change their behavior and not only

699
01:20:08,240 --> 01:20:12,240
again the companies that are sort of 
the violators, but also other companies

700
01:20:12,240 --> 01:20:18,480
like Facebook, Google, Amazon, who also have their 
infrastructure targeted, and the users targeted

701
01:20:19,040 --> 01:20:25,440
with this spyware technology, so they they could be 
also our allies, and as we saw in WhatsApp case, and

702
01:20:25,440 --> 01:20:31,679
we should also hold them sort of accountable and 
say: okay, what are you doing about your users being

703
01:20:31,680 --> 01:20:35,920
targeted in such a way, and how can you stand 
with the civil society to fight against this.

704
01:20:38,400 --> 01:20:44,160
Yeah, thank you Natalia, and if I give one minute 
to all of our speakers, because we mentioned in

705
01:20:44,160 --> 01:20:48,960
the beginning of the conversation, that we don't 
want to just cry over what's happening in our

706
01:20:48,960 --> 01:20:55,760
region because the issues are complex and tragic, 
but to actually, you know, think forward and

707
01:20:55,760 --> 01:21:01,760
think of ways we can still fight back and fight 
against the different forms of oppression, whether

708
01:21:01,760 --> 01:21:07,520
it be coming from governments or from private 
tech companies, and so, maybe I will start with

709
01:21:08,080 --> 01:21:14,400
Foaud. So, if you can just 
share with us one final reflection or thought

710
01:21:15,280 --> 01:21:21,519
from you on how we can move forward in this 
space? And then we'll move to the other speakers.

711
01:21:26,320 --> 01:21:37,920
My impression is that in our 
countries, it's very difficult to

712
01:21:40,880 --> 01:21:55,840
to be alone facing the rogue states we have. 
So, I think that it's very important that

713
01:21:57,360 --> 01:22:10,400
we encourage as many people as possible 
to get public on these situations, and,

714
01:22:10,400 --> 01:22:20,400
in the meantime, have the most 
publicity and international involvement

715
01:22:22,560 --> 01:22:35,280
about these situations, and the risk 
is that if we go homeopathic, it's easy to just

716
01:22:36,000 --> 01:22:44,160
let it get down, or even be very 
harsh against the people who are

717
01:22:47,040 --> 01:22:54,240
denouncing the situations. So, I think 
we have today an opportunity

718
01:22:55,360 --> 01:23:05,200
to go with so many cases of people, 
in so many different places, and to 

719
01:23:07,120 --> 01:23:19,840
to impose global reference about 
the right to intimacy, and 

720
01:23:20,640 --> 01:23:30,320
stopping impunity of these behaviors. But, it 
calls for a real strong international coalition,

721
01:23:31,680 --> 01:23:51,760
and being quite bold on these elements. 
Thank you. Excuse me, I didn't hear you. 

722
01:23:52,960 --> 01:24:01,840
Marwa, did you speak? Oh, I'm on mute. I made that 
mistake. Okay, I said thank you, and then I said Dia.

723
01:24:04,320 --> 01:24:10,880
Thank you, I guess I'll just, sorry 
to sort of repeat what I said earlier,

724
01:24:10,880 --> 01:24:18,080
but I think that also, as Fouad said, 
really want a second, that I think there's a

725
01:24:18,080 --> 01:24:25,600
big opportunity in people, particularly people 
outside of the EU and the U.S, coming together

726
01:24:25,600 --> 01:24:33,040
to share our voices, and share our advocacy, 
and make our voices heard. You know, people

727
01:24:33,040 --> 01:24:40,480
in South America, people in Southeast Asia, 
people in South Asia, they understand much better

728
01:24:40,480 --> 01:24:46,400
what our needs are, and also some of the things 
that we've gone through. Our governments are

729
01:24:46,400 --> 01:24:53,200
using the same tools and tactics against us, 
legislation around misinformation and around

730
01:24:53,200 --> 01:25:01,599
regulating the internet. Also, the tactics of how 
they target activists. They're all very similar.

731
01:25:01,600 --> 01:25:05,520
And so, I think there's a lot of opportunity 
right now, while we still have the internet,

732
01:25:05,520 --> 01:25:10,880
we still have access to it, and that's one thing 
that really can't be taken away, you know building

733
01:25:10,880 --> 01:25:16,400
relationships with each other, and seeing 
what others are going through; that's something

734
01:25:16,400 --> 01:25:21,599
that no amounts of content moderation and 
no amounts of legislation can take away,

735
01:25:21,600 --> 01:25:25,840
as long as we still have access to the internet, 
as long as we're not under an internet shutdown,

736
01:25:26,400 --> 01:25:31,200
which is something that we also need to be 
aware, of as long as we have that access,

737
01:25:31,200 --> 01:25:35,440
let's use it, and let's use it to build 
those connections, to raise our voices,

738
01:25:35,440 --> 01:25:40,879
and to be stronger together. I really think that 
that's the next step in the future, we've

739
01:25:40,880 --> 01:25:45,680
learned how to use these tools for ourselves, the 
governments are quicker than us, they respond,

740
01:25:46,320 --> 01:25:52,000
they take the tools away from us, so now, we have 
each other, and so, I'm very hopeful in that way.

741
01:25:53,600 --> 01:26:01,920
Indeed, I'll move to Maryam, and then 
Mohamad, and then Natalia. Thank you Marwa.

742
01:26:01,920 --> 01:26:06,800
I would say you know very quickly on these,  
on what we can do moving forward, obviously like

743
01:26:06,800 --> 01:26:11,200
I'm not going to repeat what my colleagues have 
said, because yes accountability, yes ceasing to

744
01:26:11,200 --> 01:26:15,599
exist in silos and working together, and all 
of those things. What I would add to that is

745
01:26:16,560 --> 01:26:20,880
we really need to talk about discourse and 
terminology, and who gets to decide and set

746
01:26:20,880 --> 01:26:25,520
the discourse, and the terminologies, like who gets 
to decide who's a terrorist right? Like, what is the

747
01:26:25,520 --> 01:26:30,480
difference between the Saudi government beheading 
people, and ISIS beheading people? Why is it that

748
01:26:30,480 --> 01:26:35,440
one of them is not allowed access to Social Media 
platforms and the other is? And so, I think the

749
01:26:35,440 --> 01:26:41,040
conversation around who decides what the 
terminology means and who it's applied to, I think

750
01:26:41,040 --> 01:26:46,320
is really important moving forward. The second 
thing is, and this is something that I've been

751
01:26:46,320 --> 01:26:50,960
hoping, I've been saying for several years, and I'm 
hoping someone has already or will pick up on it,

752
01:26:50,960 --> 01:26:55,280
but, I've been saying that one of the things I 
really hope to see is some sort of universal

753
01:26:55,280 --> 01:26:59,840
declaration on digital rights. So, the same way 
that we have a universal declaration on human

754
01:26:59,840 --> 01:27:04,000
rights, I think we really need one that speaks 
specifically about our digital rights, because

755
01:27:04,000 --> 01:27:09,280
we exist in the digital space almost in the same 
ways that we exist in the physical space now;

756
01:27:09,280 --> 01:27:13,280
granted there is a large percentage 
of the world that still don't have access to

757
01:27:13,280 --> 01:27:18,080
the internet the way that we do, and that's a 
whole other issue that we also need to talk about.

758
01:27:18,080 --> 01:27:23,760
But, I do think that we need a foundation, like 
the part of the reason why we're able to have

759
01:27:24,320 --> 01:27:30,480
laws and regulations about everyday human rights 
is also because we have a foundation to start from.

760
01:27:30,480 --> 01:27:34,879
And I think for digital rights, the same thing 
is needed. We need a universal declaration that

761
01:27:34,880 --> 01:27:39,680
sets out what those rights are, and what those 
responsibilities are that need to be protected;

762
01:27:39,680 --> 01:27:43,120
and from there, talk about, okay what 
are the regulations that we need

763
01:27:43,120 --> 01:27:46,960
in place, what are the laws that we need in place 
to make sure that those rights are protected.

764
01:27:46,960 --> 01:27:52,720
I think is extremely important as well. 
Indeed, thank you so much Maryam, Najem.

765
01:27:58,720 --> 01:28:08,080
Okay I noticed earlier that I muted. So, I 
I just want to add to what's been said is that, I

766
01:28:08,080 --> 01:28:14,720
mean, for our region, the online space is by default 
the civic space, so we just need to treat it as

767
01:28:14,720 --> 01:28:21,440
such, and we need to make sure that we look 
at all the issues, because all of them intersect.

768
01:28:22,240 --> 01:28:28,000
And also, I totally agree with Dia, that all the 
regions as well intersect, and all the problems

769
01:28:28,640 --> 01:28:34,480
are kind of the same from different region. So, 
we need to be aware on all what's happening, we

770
01:28:34,480 --> 01:28:39,919
need to keep putting the 
puzzle together, and keep advocating for the

771
01:28:39,920 --> 01:28:45,600
change, whether on companies or governments,
and there's a lot to learn from each other.

772
01:28:47,680 --> 01:28:53,440
We just need to be more nice as well, and 
appreciate what we have for now, and I'm

773
01:28:53,440 --> 01:28:58,639
also hopeful, I mean, I don't know why, I'm also 
hopeful and I would remind everyone that we

774
01:28:58,640 --> 01:29:07,200
hope to see Alaa free soon, so #FreeAlaa. Thank you. 
Thank you Najem. And the last speaker, Natalia.

775
01:29:08,240 --> 01:29:12,639
Thank you, just be very brief, I think it's 
important for all of us to have events like

776
01:29:12,640 --> 01:29:18,240
this, to keep talking about these issues and abuses 
that are still happening in the MENA region, and we

777
01:29:18,240 --> 01:29:23,599
saw sort of the original flow of interest, and 
funding, and resources during and immediately

778
01:29:23,600 --> 01:29:29,680
after the Arab Spring, and now it's sort, of the 
world has sort of moved on in many ways, and I

779
01:29:29,680 --> 01:29:34,480
think that's very unfortunate, and we need to 
keep the attention of the media, of lawmakers,

780
01:29:34,480 --> 01:29:39,280
and international community on this issue, so 
that people in the region are not forgotten.

781
01:29:41,120 --> 01:29:46,880
Yeah, thank you so much. I think every time we 
have this this kind of conversation, I feel that

782
01:29:46,880 --> 01:29:52,800
we need years and years to just continue 
discussing and strategizing on ways forward.

783
01:29:53,600 --> 01:29:59,520
But, something I take away from this conversation, 
and it will inform pretty much all the work

784
01:29:59,520 --> 01:30:06,480
that we do at Access Now, solidarity and reclaiming 
narrative, and reclaiming the internet as 

785
01:30:06,480 --> 01:30:13,120
a space, and that's the the title of the session. 
We've come to an end, so I want to thank you,

786
01:30:13,120 --> 01:30:18,240
thank our speakers for all of their 
insights and expertise, and having

787
01:30:18,240 --> 01:30:24,639
the courage to share their individual stories, 
so thank you so much. And also, for our audience

788
01:30:25,280 --> 01:30:28,880
for sticking around with us in 
the last hour and a half.

789
01:30:29,920 --> 01:30:38,080
Before we conclude and we go and say goodbye, I 
just want to remind everyone that the registration

790
01:30:38,080 --> 01:30:45,600
for RightsCon 2021, will open next week, so keep an 
eye on the RightsCon page and we hope to continue

791
01:30:45,600 --> 01:30:54,880
this conversation then in June 2021. Thank you,
and take care everyone. Thank you. Bye-bye. Bye.

