1
00:00:04,000 --> 00:00:06,319
welcome back to the studio and we are in

2
00:00:06,319 --> 00:00:08,559
conversation the next 30 minutes with

3
00:00:08,559 --> 00:00:12,559
esha bhandari of the aclu asha welcome

4
00:00:12,559 --> 00:00:14,559
to the studio

5
00:00:14,559 --> 00:00:16,480
thank you so much melissa

6
00:00:16,480 --> 00:00:19,119
ashley i wonder if we can start uh by

7
00:00:19,119 --> 00:00:21,520
you having um introduced yourself and

8
00:00:21,520 --> 00:00:23,039
also tell us a little bit about the work

9
00:00:23,039 --> 00:00:24,880
that you do

10
00:00:24,880 --> 00:00:27,439
sure thanks very much uh i'm a deputy

11
00:00:27,439 --> 00:00:29,679
project director at the aclu speech

12
00:00:29,679 --> 00:00:31,920
privacy and technology project

13
00:00:31,920 --> 00:00:34,399
where i work on litigation and advocacy

14
00:00:34,399 --> 00:00:36,640
to protect freedom of expression and

15
00:00:36,640 --> 00:00:38,640
digital privacy

16
00:00:38,640 --> 00:00:41,120
the focus is on first amendment fourth

17
00:00:41,120 --> 00:00:42,800
amendment issues freedom of expression

18
00:00:42,800 --> 00:00:45,200
issues in the digital age the ways that

19
00:00:45,200 --> 00:00:46,640
these intersect

20
00:00:46,640 --> 00:00:48,239
and in the last couple of years i've

21
00:00:48,239 --> 00:00:50,719
really focused my work on the impact of

22
00:00:50,719 --> 00:00:53,199
artificial intelligence and big data on

23
00:00:53,199 --> 00:00:55,280
civil liberties and civil rights and i

24
00:00:55,280 --> 00:00:57,360
try to bring together the aclu's work on

25
00:00:57,360 --> 00:00:59,920
surveillance privacy racial justice

26
00:00:59,920 --> 00:01:03,199
gender justice and disability rights

27
00:01:03,199 --> 00:01:04,879
particularly because the rise of

28
00:01:04,879 --> 00:01:06,400
artificial intel intelligence is

29
00:01:06,400 --> 00:01:08,799
affecting all of these areas

30
00:01:08,799 --> 00:01:10,320
now esha um

31
00:01:10,320 --> 00:01:12,240
before we go further i want to remind

32
00:01:12,240 --> 00:01:14,960
the audience that this is a q a session

33
00:01:14,960 --> 00:01:17,439
so start inputting your questions now

34
00:01:17,439 --> 00:01:19,600
the earlier you get it in the more

35
00:01:19,600 --> 00:01:21,040
likely we'll have a chance to get

36
00:01:21,040 --> 00:01:23,520
through it but we are keen to answer as

37
00:01:23,520 --> 00:01:27,040
many questions as possible so esha uh

38
00:01:27,040 --> 00:01:28,560
thank you so much for the introduction

39
00:01:28,560 --> 00:01:31,280
can you also can you explain then your

40
00:01:31,280 --> 00:01:33,119
your sort of how you think about these

41
00:01:33,119 --> 00:01:35,520
things and your strategy uh in terms of

42
00:01:35,520 --> 00:01:38,079
dealing with all this stuff

43
00:01:38,079 --> 00:01:38,960
yes

44
00:01:38,960 --> 00:01:42,399
so privacy in the digital age um there's

45
00:01:42,399 --> 00:01:44,560
a lot of work that's encompassed within

46
00:01:44,560 --> 00:01:45,600
that

47
00:01:45,600 --> 00:01:47,680
you know it covers everything from

48
00:01:47,680 --> 00:01:49,520
facial recognition

49
00:01:49,520 --> 00:01:52,320
biometric surveillance other forms of

50
00:01:52,320 --> 00:01:54,079
surveillance and technology that's used

51
00:01:54,079 --> 00:01:57,200
by law enforcement it covers big data

52
00:01:57,200 --> 00:01:59,520
and collection of consumer data and how

53
00:01:59,520 --> 00:02:01,119
that's used

54
00:02:01,119 --> 00:02:02,719
it covers you know

55
00:02:02,719 --> 00:02:06,000
dna and the use of dna and dna data

56
00:02:06,000 --> 00:02:07,280
banks

57
00:02:07,280 --> 00:02:09,840
the one unifying thread in our work at

58
00:02:09,840 --> 00:02:12,640
the aclu and the work of my colleagues

59
00:02:12,640 --> 00:02:14,640
in civil society is that we want to

60
00:02:14,640 --> 00:02:17,200
ensure that the rise of technology and

61
00:02:17,200 --> 00:02:19,599
the advances of the digital age don't

62
00:02:19,599 --> 00:02:21,360
diminish our civil liberties and our

63
00:02:21,360 --> 00:02:23,120
civil rights we want to make sure that

64
00:02:23,120 --> 00:02:25,760
we preserve those privacy protections

65
00:02:25,760 --> 00:02:27,840
that we've always had that are necessary

66
00:02:27,840 --> 00:02:29,360
to a functioning democracy that are

67
00:02:29,360 --> 00:02:30,879
basic human rights

68
00:02:30,879 --> 00:02:33,519
so while technology you know the advance

69
00:02:33,519 --> 00:02:35,040
of technology can bring many wonderful

70
00:02:35,040 --> 00:02:37,280
things for human society we don't want

71
00:02:37,280 --> 00:02:40,080
it to come at the expense of our rights

72
00:02:40,080 --> 00:02:42,319
and that's really a unifying theme and

73
00:02:42,319 --> 00:02:44,400
and the way that ties into the racial

74
00:02:44,400 --> 00:02:46,239
justice focus of this conversation is

75
00:02:46,239 --> 00:02:47,120
that

76
00:02:47,120 --> 00:02:48,959
privacy and racial justice we don't see

77
00:02:48,959 --> 00:02:51,840
them as separate areas of work they're

78
00:02:51,840 --> 00:02:54,800
completely intertwined of course we all

79
00:02:54,800 --> 00:02:56,879
um you know i think we all should care

80
00:02:56,879 --> 00:02:58,640
about privacy we're all affected by

81
00:02:58,640 --> 00:03:00,879
diminishments of privacy but the

82
00:03:00,879 --> 00:03:03,200
disproportionate impact of

83
00:03:03,200 --> 00:03:04,879
violations of privacy the rise of

84
00:03:04,879 --> 00:03:07,040
surveillance technology the use of big

85
00:03:07,040 --> 00:03:08,239
data

86
00:03:08,239 --> 00:03:10,080
it is falling disproportionately on

87
00:03:10,080 --> 00:03:11,840
people of color and communities of color

88
00:03:11,840 --> 00:03:13,519
and we always try to foreground that in

89
00:03:13,519 --> 00:03:14,720
our work

90
00:03:14,720 --> 00:03:16,640
and so you know we see

91
00:03:16,640 --> 00:03:18,560
privacy work as a fundamental racial

92
00:03:18,560 --> 00:03:21,280
justice work in the 21st century

93
00:03:21,280 --> 00:03:23,040
and so i wonder if we can approach the

94
00:03:23,040 --> 00:03:25,440
next few minutes by putting them in

95
00:03:25,440 --> 00:03:27,360
buckets in terms of the advocacy work

96
00:03:27,360 --> 00:03:30,560
right uh you have litigation

97
00:03:30,560 --> 00:03:33,120
that the aclu is so well known for and

98
00:03:33,120 --> 00:03:35,200
legislation as well let's start with

99
00:03:35,200 --> 00:03:38,239
litigation can you give us a few of the

100
00:03:38,239 --> 00:03:41,680
cases that the aclu has worked on and um

101
00:03:41,680 --> 00:03:43,200
they can be at the state or federal

102
00:03:43,200 --> 00:03:45,200
level both it'd be awesome to hear a

103
00:03:45,200 --> 00:03:47,040
little bit more

104
00:03:47,040 --> 00:03:48,640
sure i'd be happy to so i want to

105
00:03:48,640 --> 00:03:50,400
highlight a couple of cases that we've

106
00:03:50,400 --> 00:03:52,480
brought recently and successfully

107
00:03:52,480 --> 00:03:55,120
litigated the first is a case called

108
00:03:55,120 --> 00:03:56,959
leaders of a beautiful struggle versus

109
00:03:56,959 --> 00:03:59,280
baltimore police department

110
00:03:59,280 --> 00:04:01,200
leaders of a beautiful struggle is a

111
00:04:01,200 --> 00:04:03,040
grassroots black led baltimore

112
00:04:03,040 --> 00:04:05,439
organization great name great name for

113
00:04:05,439 --> 00:04:06,560
the case

114
00:04:06,560 --> 00:04:08,879
which was about challenging persistent

115
00:04:08,879 --> 00:04:12,720
aerial surveillance in baltimore so uh

116
00:04:12,720 --> 00:04:14,480
you know what what happened is that the

117
00:04:14,480 --> 00:04:16,238
baltimore police department contracted

118
00:04:16,238 --> 00:04:18,798
with a private company to run

119
00:04:18,798 --> 00:04:21,600
uh surveillance planes over the city and

120
00:04:21,600 --> 00:04:24,160
these planes would fly um up to 40 hours

121
00:04:24,160 --> 00:04:26,720
a week capturing hours upon hours of

122
00:04:26,720 --> 00:04:30,000
footage covering 90 of baltimore

123
00:04:30,000 --> 00:04:30,960
and

124
00:04:30,960 --> 00:04:32,560
you know these photographs that were

125
00:04:32,560 --> 00:04:35,600
captured could um could track people's

126
00:04:35,600 --> 00:04:37,600
movements so even though you're taking

127
00:04:37,600 --> 00:04:39,120
these photographs from the sky there was

128
00:04:39,120 --> 00:04:40,800
sufficient detail that you'd be able to

129
00:04:40,800 --> 00:04:42,800
track individual people going from place

130
00:04:42,800 --> 00:04:43,840
to place

131
00:04:43,840 --> 00:04:45,520
that kind of persistent aerial

132
00:04:45,520 --> 00:04:47,840
surveillance raised major concerns for

133
00:04:47,840 --> 00:04:49,520
people in baltimore

134
00:04:49,520 --> 00:04:51,120
um you know you could see someone going

135
00:04:51,120 --> 00:04:52,639
from their home to their place of

136
00:04:52,639 --> 00:04:55,040
worship maybe to a medical clinic so

137
00:04:55,040 --> 00:04:58,080
this is totally totally like a total

138
00:04:58,080 --> 00:04:59,759
surveillance i mean this is the kind of

139
00:04:59,759 --> 00:05:01,520
thing that you hear about the kind of

140
00:05:01,520 --> 00:05:03,360
thing that i've reported on

141
00:05:03,360 --> 00:05:05,600
in authoritarian countries

142
00:05:05,600 --> 00:05:07,440
it's it's it's exactly you're being

143
00:05:07,440 --> 00:05:09,759
tracked from the sky 90 of baltimore

144
00:05:09,759 --> 00:05:12,160
would be uh covered with these planes

145
00:05:12,160 --> 00:05:14,639
taking photographs so i agree it sort of

146
00:05:14,639 --> 00:05:16,479
gives you this dystopian feeling right

147
00:05:16,479 --> 00:05:18,320
every time you leave the house

148
00:05:18,320 --> 00:05:20,080
even if you're in public we don't expect

149
00:05:20,080 --> 00:05:22,160
that our movements are being captured by

150
00:05:22,160 --> 00:05:25,759
a plane in the sky um so

151
00:05:25,759 --> 00:05:26,880
you know leaders of a beautiful

152
00:05:26,880 --> 00:05:28,240
beautiful struggle as i mentioned is one

153
00:05:28,240 --> 00:05:30,240
of the grassroots groups in baltimore

154
00:05:30,240 --> 00:05:32,080
that wanted to push back

155
00:05:32,080 --> 00:05:34,080
now the baltimore police department said

156
00:05:34,080 --> 00:05:36,960
we're doing this to combat serious crime

157
00:05:36,960 --> 00:05:38,560
but they took this

158
00:05:38,560 --> 00:05:41,039
dragnet approach where everyone is

159
00:05:41,039 --> 00:05:43,039
treated as a potential subject everyone

160
00:05:43,039 --> 00:05:45,360
living and working in baltimore city

161
00:05:45,360 --> 00:05:47,520
um and and we challenged that under the

162
00:05:47,520 --> 00:05:50,000
fourth amendment and we said that even

163
00:05:50,000 --> 00:05:52,880
if you're in public you still have a

164
00:05:52,880 --> 00:05:55,039
reasonable expectation of privacy in

165
00:05:55,039 --> 00:05:57,360
your movements long term in that level

166
00:05:57,360 --> 00:05:59,600
of detail so you know the government

167
00:05:59,600 --> 00:06:01,600
will often say look if you're in public

168
00:06:01,600 --> 00:06:03,199
what's what's the big deal the fourth

169
00:06:03,199 --> 00:06:05,039
amendment doesn't protect you we can

170
00:06:05,039 --> 00:06:06,560
take photographs

171
00:06:06,560 --> 00:06:07,280
but

172
00:06:07,280 --> 00:06:10,479
we won that case um we successfully uh

173
00:06:10,479 --> 00:06:12,000
litigated that and and the fourth

174
00:06:12,000 --> 00:06:13,600
circuit which is a federal appellate

175
00:06:13,600 --> 00:06:14,800
court

176
00:06:14,800 --> 00:06:16,560
uh held that

177
00:06:16,560 --> 00:06:18,000
this program that the baltimore police

178
00:06:18,000 --> 00:06:19,360
department was running did violate

179
00:06:19,360 --> 00:06:21,360
people's expectations of privacy it did

180
00:06:21,360 --> 00:06:23,600
violate the fourth amendment um that

181
00:06:23,600 --> 00:06:25,360
persistent aerial surveillance program

182
00:06:25,360 --> 00:06:28,160
is no more so that was a real victory

183
00:06:28,160 --> 00:06:29,840
for the people of baltimore now i

184
00:06:29,840 --> 00:06:32,160
presume then that other cities that were

185
00:06:32,160 --> 00:06:33,440
considering

186
00:06:33,440 --> 00:06:34,960
uh doing this

187
00:06:34,960 --> 00:06:37,039
looks at that case and and what are the

188
00:06:37,039 --> 00:06:39,039
consequences they can't really act on it

189
00:06:39,039 --> 00:06:42,240
now as a result of this ruling or

190
00:06:42,240 --> 00:06:44,000
that's the hope yes that even if there

191
00:06:44,000 --> 00:06:45,680
are cities outside of the fourth

192
00:06:45,680 --> 00:06:47,440
circuit's jurisdiction that they'll look

193
00:06:47,440 --> 00:06:49,599
at this ruling and say why you know why

194
00:06:49,599 --> 00:06:51,199
would we launch an unconstitutional

195
00:06:51,199 --> 00:06:53,520
program um so i hope that certainly

196
00:06:53,520 --> 00:06:55,199
every municipality in the country is on

197
00:06:55,199 --> 00:06:57,280
notice now that this kind of persistent

198
00:06:57,280 --> 00:06:58,720
aerial surveillance is not

199
00:06:58,720 --> 00:07:01,120
constitutional it's it's not okay and

200
00:07:01,120 --> 00:07:02,960
and again i think this case is a perfect

201
00:07:02,960 --> 00:07:05,280
example of the racial justice

202
00:07:05,280 --> 00:07:09,599
implications of this kind of work um so

203
00:07:09,599 --> 00:07:11,599
that's one example

204
00:07:11,599 --> 00:07:13,360
the other example i want to highlight

205
00:07:13,360 --> 00:07:16,720
for you is um our case challenging

206
00:07:16,720 --> 00:07:20,479
clearview ai's collection of face prints

207
00:07:20,479 --> 00:07:23,199
and building a database of face prints

208
00:07:23,199 --> 00:07:25,759
so in this case we also represented

209
00:07:25,759 --> 00:07:29,039
grassroots organizations in illinois

210
00:07:29,039 --> 00:07:32,479
including um mujeres latinas on axion

211
00:07:32,479 --> 00:07:34,880
which is a local grassroots organization

212
00:07:34,880 --> 00:07:37,360
that repre uh that it consists of people

213
00:07:37,360 --> 00:07:39,039
who survive domestic violence and

214
00:07:39,039 --> 00:07:40,720
advocate against uh gender-based

215
00:07:40,720 --> 00:07:41,840
violence

216
00:07:41,840 --> 00:07:43,919
and they highlighted the fact that you

217
00:07:43,919 --> 00:07:46,080
know clearview was building a database

218
00:07:46,080 --> 00:07:48,960
of face prints of millions of people

219
00:07:48,960 --> 00:07:51,440
and that has real implications for

220
00:07:51,440 --> 00:07:52,720
survivors

221
00:07:52,720 --> 00:07:54,240
for other people who are marginalized

222
00:07:54,240 --> 00:07:55,759
who are very concerned that if their

223
00:07:55,759 --> 00:07:57,919
face prints are in a database they can

224
00:07:57,919 --> 00:08:01,120
be tracked surveilled um and they don't

225
00:08:01,120 --> 00:08:03,120
really have control now keep in mind our

226
00:08:03,120 --> 00:08:05,360
face prints are immutable

227
00:08:05,360 --> 00:08:07,599
characteristics so if a company like

228
00:08:07,599 --> 00:08:09,520
clearvue captures your face print from a

229
00:08:09,520 --> 00:08:12,319
photograph and sells that or shares it

230
00:08:12,319 --> 00:08:14,080
with law enforcement

231
00:08:14,080 --> 00:08:16,080
we can't we can't change that it's not

232
00:08:16,080 --> 00:08:18,400
like losing your credit card or even a

233
00:08:18,400 --> 00:08:19,599
social security number where you can

234
00:08:19,599 --> 00:08:21,039
change that so

235
00:08:21,039 --> 00:08:22,560
once our face prints which are these

236
00:08:22,560 --> 00:08:24,160
immutable um

237
00:08:24,160 --> 00:08:26,400
you know identifiers are are gone

238
00:08:26,400 --> 00:08:29,759
it you know we really lose control so

239
00:08:29,759 --> 00:08:32,320
we once again we sued under an illinois

240
00:08:32,320 --> 00:08:33,440
state law

241
00:08:33,440 --> 00:08:35,839
uh the illinois illinois bipa biometric

242
00:08:35,839 --> 00:08:37,839
information privacy act

243
00:08:37,839 --> 00:08:39,279
which is

244
00:08:39,279 --> 00:08:41,200
that state law is probably one of the

245
00:08:41,200 --> 00:08:43,440
from my understanding one of the more um

246
00:08:43,440 --> 00:08:45,760
robust laws out there or one of the few

247
00:08:45,760 --> 00:08:48,000
that exist in in the states

248
00:08:48,000 --> 00:08:49,839
that's correct and i know we'll talk

249
00:08:49,839 --> 00:08:52,480
about legislation later but um certainly

250
00:08:52,480 --> 00:08:54,080
illinois

251
00:08:54,080 --> 00:08:55,920
was one of the earlier states to come

252
00:08:55,920 --> 00:08:57,600
out the gate and protect

253
00:08:57,600 --> 00:09:00,160
biometric information so under illinois

254
00:09:00,160 --> 00:09:02,320
bipa you need consent of people to

255
00:09:02,320 --> 00:09:04,320
capture their biometrics and clearview

256
00:09:04,320 --> 00:09:06,320
did not get consent of the millions of

257
00:09:06,320 --> 00:09:08,160
people including illinois residents that

258
00:09:08,160 --> 00:09:10,959
it um you know had in its database

259
00:09:10,959 --> 00:09:13,760
so we we settled that case very recently

260
00:09:13,760 --> 00:09:16,080
and um you know per the terms of our

261
00:09:16,080 --> 00:09:17,200
settlement

262
00:09:17,200 --> 00:09:19,760
clearview is now permanently

263
00:09:19,760 --> 00:09:21,519
banned from

264
00:09:21,519 --> 00:09:24,000
selling the the biometric information in

265
00:09:24,000 --> 00:09:26,640
its database nationwide to corporate

266
00:09:26,640 --> 00:09:29,360
parties and uh it can no longer share

267
00:09:29,360 --> 00:09:31,200
within illinois for the next five years

268
00:09:31,200 --> 00:09:33,440
with law enforcement uh even though the

269
00:09:33,440 --> 00:09:35,200
the illinois bipa you know has

270
00:09:35,200 --> 00:09:36,640
exceptions for

271
00:09:36,640 --> 00:09:39,120
uh law enforcement use of biometrics but

272
00:09:39,120 --> 00:09:40,160
clear view under the terms of the

273
00:09:40,160 --> 00:09:41,279
settlement is not going to share with

274
00:09:41,279 --> 00:09:42,720
law enforcement in illinois for five

275
00:09:42,720 --> 00:09:44,080
years

276
00:09:44,080 --> 00:09:45,760
again you know the goal of this

277
00:09:45,760 --> 00:09:48,240
litigation is all you know as we do at

278
00:09:48,240 --> 00:09:50,560
the aclu is always to

279
00:09:50,560 --> 00:09:52,160
set the precedent and then put other

280
00:09:52,160 --> 00:09:53,839
actors on notice

281
00:09:53,839 --> 00:09:55,279
that if they're going to do the same

282
00:09:55,279 --> 00:09:56,959
thing elsewhere you know you're going to

283
00:09:56,959 --> 00:09:58,640
run into problems so

284
00:09:58,640 --> 00:10:00,000
you know we would hope that other

285
00:10:00,000 --> 00:10:01,360
companies looking at the clearview

286
00:10:01,360 --> 00:10:03,120
settlement and the lawsuit

287
00:10:03,120 --> 00:10:04,959
will think twice before

288
00:10:04,959 --> 00:10:06,560
uh you know building these massive

289
00:10:06,560 --> 00:10:08,720
databases of biometric information and

290
00:10:08,720 --> 00:10:10,640
so you mentioned legislation let's talk

291
00:10:10,640 --> 00:10:13,279
a little bit about legislation

292
00:10:13,279 --> 00:10:15,360
yes so there are a lot of efforts there

293
00:10:15,360 --> 00:10:16,959
you know the federal and the state

294
00:10:16,959 --> 00:10:18,640
levels um

295
00:10:18,640 --> 00:10:20,800
operate in tandem sometimes but you know

296
00:10:20,800 --> 00:10:22,079
obviously the

297
00:10:22,079 --> 00:10:24,800
at the federal level um if legislation

298
00:10:24,800 --> 00:10:27,440
is slow to move the states can often act

299
00:10:27,440 --> 00:10:30,000
faster so we we try to you know work at

300
00:10:30,000 --> 00:10:32,560
both levels and encourage states that

301
00:10:32,560 --> 00:10:34,640
want to pass robust privacy laws to do

302
00:10:34,640 --> 00:10:37,440
so and not to wait for federal action

303
00:10:37,440 --> 00:10:39,600
on the state level i want to highlight a

304
00:10:39,600 --> 00:10:44,000
couple of initiatives one is in maine

305
00:10:44,000 --> 00:10:45,600
there would be you know there's a

306
00:10:45,600 --> 00:10:47,200
there's a potential bill

307
00:10:47,200 --> 00:10:49,200
to protect biometric identifiers

308
00:10:49,200 --> 00:10:51,279
biometric information so you know we

309
00:10:51,279 --> 00:10:53,120
talked about how illinois was early up

310
00:10:53,120 --> 00:10:56,480
the gate um with with the illinois bipa

311
00:10:56,480 --> 00:10:58,399
um but we're you know we're we're

312
00:10:58,399 --> 00:11:00,480
supporting states like maine passing

313
00:11:00,480 --> 00:11:03,680
their own biometric information privacy

314
00:11:03,680 --> 00:11:05,680
laws um you know i think that the

315
00:11:05,680 --> 00:11:08,000
illinois bipa you know was passed years

316
00:11:08,000 --> 00:11:09,760
ago now we have more information about

317
00:11:09,760 --> 00:11:12,079
how biometrics are captured and used so

318
00:11:12,079 --> 00:11:13,360
we should you know have the most

319
00:11:13,360 --> 00:11:15,519
up-to-date legislative language that we

320
00:11:15,519 --> 00:11:17,360
encourage states to adopt

321
00:11:17,360 --> 00:11:19,440
um and then on a slightly different tack

322
00:11:19,440 --> 00:11:22,000
moving away from biometrics in new york

323
00:11:22,000 --> 00:11:24,560
there's um a digital fairness act that's

324
00:11:24,560 --> 00:11:28,160
been proposed and this is a bill where

325
00:11:28,160 --> 00:11:30,480
private entities um

326
00:11:30,480 --> 00:11:32,079
you know would be regulated in terms of

327
00:11:32,079 --> 00:11:33,839
the information they can collect from

328
00:11:33,839 --> 00:11:35,040
consumers

329
00:11:35,040 --> 00:11:37,120
uh the algorithmic

330
00:11:37,120 --> 00:11:39,519
uses they can put that information to

331
00:11:39,519 --> 00:11:41,360
uh so you know one of the one of the

332
00:11:41,360 --> 00:11:42,880
issues that's happened with big data is

333
00:11:42,880 --> 00:11:43,920
it's not just that we're being

334
00:11:43,920 --> 00:11:45,920
surveilled and having you know our

335
00:11:45,920 --> 00:11:47,680
movements captured or our biometrics

336
00:11:47,680 --> 00:11:49,440
captured but

337
00:11:49,440 --> 00:11:51,200
the the data that's collected on us is

338
00:11:51,200 --> 00:11:53,120
then used in turn to give us economic

339
00:11:53,120 --> 00:11:55,279
opportunities right housing employment

340
00:11:55,279 --> 00:11:57,360
credit so there's a big gap in

341
00:11:57,360 --> 00:11:59,440
regulation there because

342
00:11:59,440 --> 00:12:02,079
you know if data is used to identify our

343
00:12:02,079 --> 00:12:04,560
race our gender our age and give us

344
00:12:04,560 --> 00:12:06,480
different opportunities on the basis of

345
00:12:06,480 --> 00:12:08,639
that then that's violating civil rights

346
00:12:08,639 --> 00:12:10,320
laws that's violating civil rights

347
00:12:10,320 --> 00:12:11,920
protections that we would expect in the

348
00:12:11,920 --> 00:12:13,760
offline world right so the digital

349
00:12:13,760 --> 00:12:15,440
fairness act is you know as new york

350
00:12:15,440 --> 00:12:18,240
state's um way of addressing that

351
00:12:18,240 --> 00:12:19,839
neither of these two bills have passed

352
00:12:19,839 --> 00:12:20,880
yet so

353
00:12:20,880 --> 00:12:22,399
um these are proposals that are out

354
00:12:22,399 --> 00:12:23,760
there there are many others in the

355
00:12:23,760 --> 00:12:25,279
states but you know we're trying to

356
00:12:25,279 --> 00:12:27,440
focus our efforts on getting the states

357
00:12:27,440 --> 00:12:29,279
where passage is possible to really do

358
00:12:29,279 --> 00:12:30,320
something

359
00:12:30,320 --> 00:12:31,760
uh and so you we've talked about

360
00:12:31,760 --> 00:12:33,519
litigation legislation can you quickly

361
00:12:33,519 --> 00:12:35,200
talk a little bit more about the broader

362
00:12:35,200 --> 00:12:37,200
advocacy work that you guys are doing

363
00:12:37,200 --> 00:12:39,760
and then i do want to actually talk to

364
00:12:39,760 --> 00:12:41,440
uh talk about roe v wade because there

365
00:12:41,440 --> 00:12:43,360
are certain things it's a it's a big

366
00:12:43,360 --> 00:12:45,200
thing right now um

367
00:12:45,200 --> 00:12:47,279
but very very quickly broader advocacy

368
00:12:47,279 --> 00:12:48,639
efforts

369
00:12:48,639 --> 00:12:52,399
yes we also focus advocacy on public

370
00:12:52,399 --> 00:12:56,160
education but also on um you know

371
00:12:56,160 --> 00:12:58,160
pushing companies to do better or do

372
00:12:58,160 --> 00:13:00,880
differently now we're not naive that

373
00:13:00,880 --> 00:13:02,320
you know we'll convince businesses to

374
00:13:02,320 --> 00:13:03,600
always act against their business

375
00:13:03,600 --> 00:13:05,440
interests but sometimes raising public

376
00:13:05,440 --> 00:13:07,600
attention to a problem can change things

377
00:13:07,600 --> 00:13:09,920
particularly for companies that are

378
00:13:09,920 --> 00:13:12,320
sensitive to public opinion or consumer

379
00:13:12,320 --> 00:13:14,800
opinion so for example we led an

380
00:13:14,800 --> 00:13:17,120
advocacy campaign against amazon's

381
00:13:17,120 --> 00:13:18,839
facial recognition tool known as

382
00:13:18,839 --> 00:13:21,760
recognition um you know we

383
00:13:21,760 --> 00:13:24,639
highlighted how members of congress were

384
00:13:24,639 --> 00:13:27,680
included um in the database uh you know

385
00:13:27,680 --> 00:13:29,200
and there were errors around people who

386
00:13:29,200 --> 00:13:30,800
had arrest records and really

387
00:13:30,800 --> 00:13:33,279
highlighting just how flawed facial

388
00:13:33,279 --> 00:13:35,440
recognition tools like this are and

389
00:13:35,440 --> 00:13:37,360
um that advocacy campaign put a lot of

390
00:13:37,360 --> 00:13:39,360
pressure on amazon to change what it was

391
00:13:39,360 --> 00:13:42,399
doing so we'll often engage in

392
00:13:42,399 --> 00:13:45,360
uh public campaigns like this when we

393
00:13:45,360 --> 00:13:47,279
you know there's a clear problem and and

394
00:13:47,279 --> 00:13:50,399
we could uh raise media and and public

395
00:13:50,399 --> 00:13:52,560
and consumer outrage about it

396
00:13:52,560 --> 00:13:55,199
and also just public education generally

397
00:13:55,199 --> 00:13:56,639
because

398
00:13:56,639 --> 00:13:58,320
i think a lot of times

399
00:13:58,320 --> 00:13:59,199
um

400
00:13:59,199 --> 00:14:00,880
you know people assume especially for

401
00:14:00,880 --> 00:14:02,079
younger generations but everyone's

402
00:14:02,079 --> 00:14:03,279
assumed

403
00:14:03,279 --> 00:14:04,800
everyone's used to being tracked right

404
00:14:04,800 --> 00:14:06,560
and nobody cares about privacy and

405
00:14:06,560 --> 00:14:09,279
nobody minds that everybody use might be

406
00:14:09,279 --> 00:14:11,199
selling their data but they do yeah

407
00:14:11,199 --> 00:14:13,199
exactly so sometimes it's really just

408
00:14:13,199 --> 00:14:14,800
highlighting look here's here's a

409
00:14:14,800 --> 00:14:16,399
practice that's out there we've learned

410
00:14:16,399 --> 00:14:17,839
about it through the course of our work

411
00:14:17,839 --> 00:14:20,240
we want the public to know and and then

412
00:14:20,240 --> 00:14:22,800
you know we as a society we as as the

413
00:14:22,800 --> 00:14:25,199
public can uh can take it from there

414
00:14:25,199 --> 00:14:27,040
once we're aware and as you know that's

415
00:14:27,040 --> 00:14:29,519
part of the work of journalists as well

416
00:14:29,519 --> 00:14:31,680
is often making people aware of things

417
00:14:31,680 --> 00:14:33,519
that they would care about if they knew

418
00:14:33,519 --> 00:14:35,120
yeah and i think one of the things with

419
00:14:35,120 --> 00:14:37,199
privacy is a theme that's coming out is

420
00:14:37,199 --> 00:14:38,959
facial recognition it's such a concern

421
00:14:38,959 --> 00:14:40,800
right now i mean you're talking about it

422
00:14:40,800 --> 00:14:43,360
um in terms of um you know the situation

423
00:14:43,360 --> 00:14:45,920
in the united states but uh earlier

424
00:14:45,920 --> 00:14:48,000
today on day one of rights con uh we

425
00:14:48,000 --> 00:14:49,680
also talked about how facial recognition

426
00:14:49,680 --> 00:14:52,800
is being used in conflict right uh with

427
00:14:52,800 --> 00:14:55,600
uh ukraine and russia and uh the

428
00:14:55,600 --> 00:14:56,880
ukrainian

429
00:14:56,880 --> 00:14:59,440
government using facial recognition and

430
00:14:59,440 --> 00:15:00,800
ai to

431
00:15:00,800 --> 00:15:02,880
id dead russian soldiers so there's all

432
00:15:02,880 --> 00:15:04,880
these applications and a lot of ethical

433
00:15:04,880 --> 00:15:07,440
questions um so asha let's talk about

434
00:15:07,440 --> 00:15:09,680
roe v wade um

435
00:15:09,680 --> 00:15:11,600
there's lots of concern about this and

436
00:15:11,600 --> 00:15:13,440
and and there's a digital concern too

437
00:15:13,440 --> 00:15:16,160
can you talk to us about that

438
00:15:16,160 --> 00:15:18,079
there is a lot of concern about this and

439
00:15:18,079 --> 00:15:20,480
one of the concerns that people have is

440
00:15:20,480 --> 00:15:22,399
we live in a very different world now

441
00:15:22,399 --> 00:15:24,880
than we did before roe was decided so

442
00:15:24,880 --> 00:15:27,199
the post-world world won't necessarily

443
00:15:27,199 --> 00:15:29,600
look like a pre-real world

444
00:15:29,600 --> 00:15:31,120
in terms of the amount of information

445
00:15:31,120 --> 00:15:32,720
there is about out there about us and

446
00:15:32,720 --> 00:15:34,000
the amount of information available to

447
00:15:34,000 --> 00:15:35,199
authorities

448
00:15:35,199 --> 00:15:37,199
one thing i want to emphasize is

449
00:15:37,199 --> 00:15:40,399
even if roe were overturned the majority

450
00:15:40,399 --> 00:15:41,680
of states

451
00:15:41,680 --> 00:15:44,160
have not yet criminalized people who

452
00:15:44,160 --> 00:15:46,399
self-manage abortions that doesn't mean

453
00:15:46,399 --> 00:15:48,079
that there wouldn't be rogue prosecutors

454
00:15:48,079 --> 00:15:49,920
for example who charged people

455
00:15:49,920 --> 00:15:52,000
criminally uh even in those states we've

456
00:15:52,000 --> 00:15:54,240
seen examples of those recently but i

457
00:15:54,240 --> 00:15:55,519
think that is an important caveat to

458
00:15:55,519 --> 00:15:56,880
have out there that right now the

459
00:15:56,880 --> 00:15:58,480
majority of states have not criminalized

460
00:15:58,480 --> 00:16:00,880
folks who self-manage abortion

461
00:16:00,880 --> 00:16:03,120
but of course that you know

462
00:16:03,120 --> 00:16:06,000
that that landscape where location data

463
00:16:06,000 --> 00:16:08,720
might be available uh internet search

464
00:16:08,720 --> 00:16:10,399
history might be available

465
00:16:10,399 --> 00:16:12,240
that's a real concern and i think it

466
00:16:12,240 --> 00:16:14,639
just highlights the need to regulate

467
00:16:14,639 --> 00:16:17,839
consumer privacy uh writ large

468
00:16:17,839 --> 00:16:19,120
it shouldn't be the case that if you're

469
00:16:19,120 --> 00:16:21,279
someone who you know googles where to

470
00:16:21,279 --> 00:16:24,000
find medication abortion um google's

471
00:16:24,000 --> 00:16:27,120
anything related to abortion pregnancy

472
00:16:27,120 --> 00:16:28,720
uh reproduction

473
00:16:28,720 --> 00:16:30,959
that that you know those histories are

474
00:16:30,959 --> 00:16:33,279
available for law enforcement should we

475
00:16:33,279 --> 00:16:35,120
end up in a world where that's the case

476
00:16:35,120 --> 00:16:37,839
um

477
00:16:38,560 --> 00:16:40,240
yes and i i would hope that there would

478
00:16:40,240 --> 00:16:42,399
be um you know more

479
00:16:42,399 --> 00:16:45,040
sort of across-the-board concern about

480
00:16:45,040 --> 00:16:46,639
what kind of a society we'll live in

481
00:16:46,639 --> 00:16:49,839
where anyone of reproductive age might

482
00:16:49,839 --> 00:16:53,040
be a potential criminal suspect whose

483
00:16:53,040 --> 00:16:55,680
search history and digital trail are

484
00:16:55,680 --> 00:16:57,440
fair game for law enforcement i think

485
00:16:57,440 --> 00:16:58,240
that

486
00:16:58,240 --> 00:17:00,320
is something to think about

487
00:17:00,320 --> 00:17:02,480
yeah exactly exactly it's a surveillance

488
00:17:02,480 --> 00:17:04,319
state that i think we haven't really

489
00:17:04,319 --> 00:17:05,599
experienced and i hope we don't

490
00:17:05,599 --> 00:17:08,480
experience unbelievable well um esha at

491
00:17:08,480 --> 00:17:09,919
this point i want to get to some

492
00:17:09,919 --> 00:17:11,919
questions um there there have been a few

493
00:17:11,919 --> 00:17:14,640
so i'm gonna check the monitor um so we

494
00:17:14,640 --> 00:17:16,319
have somebody from the friedrich naumann

495
00:17:16,319 --> 00:17:18,559
foundation asking um they're following

496
00:17:18,559 --> 00:17:20,720
the conversation and trying to

497
00:17:20,720 --> 00:17:23,119
figure out whether uh

498
00:17:23,119 --> 00:17:25,199
this there is um

499
00:17:25,199 --> 00:17:27,439
these are prevalent concerns in africa

500
00:17:27,439 --> 00:17:31,440
in countries in africa and if so um who

501
00:17:31,440 --> 00:17:33,520
is at the center of the technology

502
00:17:33,520 --> 00:17:35,200
behind it um actually i know you focus

503
00:17:35,200 --> 00:17:36,880
on the united states i'm not quite sure

504
00:17:36,880 --> 00:17:39,120
how much you know about things happening

505
00:17:39,120 --> 00:17:41,200
um elsewhere but if you can comment on

506
00:17:41,200 --> 00:17:43,039
that that'd be fantastic

507
00:17:43,039 --> 00:17:44,880
that's correct my focus is on the united

508
00:17:44,880 --> 00:17:46,799
states so i don't want to speak to you

509
00:17:46,799 --> 00:17:48,480
know specifics of what's happening in

510
00:17:48,480 --> 00:17:49,840
other countries but

511
00:17:49,840 --> 00:17:52,799
it is true that a lot of technology that

512
00:17:52,799 --> 00:17:55,200
enables surveillance is developed by

513
00:17:55,200 --> 00:17:57,679
american companies and i you know and

514
00:17:57,679 --> 00:18:00,640
they may be exported abroad you know

515
00:18:00,640 --> 00:18:02,160
when we have american companies that

516
00:18:02,160 --> 00:18:04,880
sell facial recognition tools or sell

517
00:18:04,880 --> 00:18:07,200
us you know uh other technology that

518
00:18:07,200 --> 00:18:08,160
enables

519
00:18:08,160 --> 00:18:10,559
device searches searches of laptops for

520
00:18:10,559 --> 00:18:11,520
example

521
00:18:11,520 --> 00:18:13,120
uh so much of this technology is

522
00:18:13,120 --> 00:18:14,480
developed in the united states which is

523
00:18:14,480 --> 00:18:16,240
why i think the united states does have

524
00:18:16,240 --> 00:18:20,160
a responsibility to regulate and to um

525
00:18:20,160 --> 00:18:22,400
you know to concern itself with that

526
00:18:22,400 --> 00:18:25,679
dynamic um certainly you know i think

527
00:18:25,679 --> 00:18:26,799
it's it's

528
00:18:26,799 --> 00:18:28,799
it's no solution to say well it's fine

529
00:18:28,799 --> 00:18:30,720
if an american company develops you know

530
00:18:30,720 --> 00:18:32,720
an ai surveillance tool as long as it's

531
00:18:32,720 --> 00:18:34,080
not used in the united states because

532
00:18:34,080 --> 00:18:36,000
it's just being used to sell to other

533
00:18:36,000 --> 00:18:37,520
countries and use on other people in

534
00:18:37,520 --> 00:18:39,840
other countries so i i think it's

535
00:18:39,840 --> 00:18:42,000
certainly an issue that we

536
00:18:42,000 --> 00:18:44,080
were aware of and partly why the public

537
00:18:44,080 --> 00:18:45,520
education campaigns and the public

538
00:18:45,520 --> 00:18:48,240
advocacy campaigns aimed at us-based

539
00:18:48,240 --> 00:18:50,080
companies i think are so important yeah

540
00:18:50,080 --> 00:18:52,480
i think a lot of focus has been on

541
00:18:52,480 --> 00:18:53,760
companies

542
00:18:53,760 --> 00:18:55,760
in in authoritarian states particularly

543
00:18:55,760 --> 00:18:58,080
china because the state there is very

544
00:18:58,080 --> 00:19:01,120
keen on building up its uh technology

545
00:19:01,120 --> 00:19:03,200
industry and its artificial intelligence

546
00:19:03,200 --> 00:19:05,360
industry and so you see a lot of stories

547
00:19:05,360 --> 00:19:06,799
about that but one of the things that a

548
00:19:06,799 --> 00:19:08,640
lot of people don't know necessarily or

549
00:19:08,640 --> 00:19:10,640
don't assume is that some of the top

550
00:19:10,640 --> 00:19:13,360
surveillance companies and in fact the

551
00:19:13,360 --> 00:19:15,840
the surveillance market in general is

552
00:19:15,840 --> 00:19:17,760
still dominated by companies in the

553
00:19:17,760 --> 00:19:20,799
united states and in europe so it's good

554
00:19:20,799 --> 00:19:22,880
to focus on what china is doing and what

555
00:19:22,880 --> 00:19:25,440
russia might be doing but it's also very

556
00:19:25,440 --> 00:19:26,880
important to look at what's happening in

557
00:19:26,880 --> 00:19:28,640
the united states and and the eu and

558
00:19:28,640 --> 00:19:30,400
frankly because these are democracies

559
00:19:30,400 --> 00:19:32,160
there's actually more recourse for

560
00:19:32,160 --> 00:19:33,840
people like you for people in civil

561
00:19:33,840 --> 00:19:37,440
society uh to really um you know put

562
00:19:37,440 --> 00:19:38,880
these uh

563
00:19:38,880 --> 00:19:40,720
companies on notice and to use

564
00:19:40,720 --> 00:19:43,679
legislation and and so on to try to

565
00:19:43,679 --> 00:19:46,640
effect change um i'm going to go and

566
00:19:46,640 --> 00:19:49,200
check another question joanna booth asks

567
00:19:49,200 --> 00:19:51,679
uh how much resistance or help to local

568
00:19:51,679 --> 00:19:53,679
authorities provide for these legal

569
00:19:53,679 --> 00:19:54,960
actions that you were talking about

570
00:19:54,960 --> 00:19:58,080
earlier and do you find that these

571
00:19:58,080 --> 00:20:01,120
corporations surveillance companies are

572
00:20:01,120 --> 00:20:03,600
working in partnership with authorities

573
00:20:03,600 --> 00:20:04,960
i think you answered that one a little

574
00:20:04,960 --> 00:20:07,200
bit but i guess um yeah tell us talk to

575
00:20:07,200 --> 00:20:09,039
us about the resistance from local

576
00:20:09,039 --> 00:20:10,799
authorities do they end up coming around

577
00:20:10,799 --> 00:20:12,640
do some actually partner and work with

578
00:20:12,640 --> 00:20:13,600
you

579
00:20:13,600 --> 00:20:14,960
what's going on there

580
00:20:14,960 --> 00:20:16,960
it really depends on what we're talking

581
00:20:16,960 --> 00:20:19,200
about one of the dynamics that

582
00:20:19,200 --> 00:20:21,039
we see over and over is that you have

583
00:20:21,039 --> 00:20:22,880
private companies that have a given

584
00:20:22,880 --> 00:20:24,320
surveillance technology that they've

585
00:20:24,320 --> 00:20:26,400
developed and they want to sell it and

586
00:20:26,400 --> 00:20:29,120
and a good customer for them are our

587
00:20:29,120 --> 00:20:32,000
state and local law enforcement agencies

588
00:20:32,000 --> 00:20:33,360
they're not just selling it to you know

589
00:20:33,360 --> 00:20:35,039
the federal government or what have you

590
00:20:35,039 --> 00:20:37,679
that there are um hundreds of state and

591
00:20:37,679 --> 00:20:39,120
local law enforcement agencies that they

592
00:20:39,120 --> 00:20:41,440
see as customers and so often times

593
00:20:41,440 --> 00:20:44,640
you'll see that uh local law enforcement

594
00:20:44,640 --> 00:20:46,799
in particular will be invested in the

595
00:20:46,799 --> 00:20:48,880
use of these technologies that they've

596
00:20:48,880 --> 00:20:51,440
purchased acquired um you know certainly

597
00:20:51,440 --> 00:20:52,559
in in the leaders of a beautiful

598
00:20:52,559 --> 00:20:53,919
struggle versus baltimore police

599
00:20:53,919 --> 00:20:55,039
department

600
00:20:55,039 --> 00:20:56,400
we were suing the baltimore police

601
00:20:56,400 --> 00:20:58,159
department that had bought the

602
00:20:58,159 --> 00:21:00,720
persistent aerial surveillance um you

603
00:21:00,720 --> 00:21:01,919
know had a contract with a private

604
00:21:01,919 --> 00:21:04,080
company but but it was the it was the

605
00:21:04,080 --> 00:21:07,360
police department that we were suing so

606
00:21:07,360 --> 00:21:09,520
it it really depends on the other hand

607
00:21:09,520 --> 00:21:11,520
there are often um

608
00:21:11,520 --> 00:21:13,120
you know local

609
00:21:13,120 --> 00:21:15,280
agencies and authorities that work with

610
00:21:15,280 --> 00:21:18,240
us um places that have human rights uh

611
00:21:18,240 --> 00:21:21,280
commissions for example um you know

612
00:21:21,280 --> 00:21:23,280
there might be other um you know state

613
00:21:23,280 --> 00:21:24,320
and local

614
00:21:24,320 --> 00:21:26,720
actors that really want to

615
00:21:26,720 --> 00:21:29,520
you know be privacy protecting a privacy

616
00:21:29,520 --> 00:21:31,760
preserving so um

617
00:21:31,760 --> 00:21:34,000
and also in local legislatures so for

618
00:21:34,000 --> 00:21:36,320
example we may often work with

619
00:21:36,320 --> 00:21:38,799
uh municipal legislators who want to get

620
00:21:38,799 --> 00:21:40,640
more democratic oversight of

621
00:21:40,640 --> 00:21:42,400
surveillance that's being done by local

622
00:21:42,400 --> 00:21:44,240
law enforcement so they may you know put

623
00:21:44,240 --> 00:21:45,840
forward information requests they may

624
00:21:45,840 --> 00:21:48,720
hold hearings locally so um we can often

625
00:21:48,720 --> 00:21:50,559
find you know local actors to partner

626
00:21:50,559 --> 00:21:52,799
with in that sense to you know bring

627
00:21:52,799 --> 00:21:54,960
transparency possibly change things and

628
00:21:54,960 --> 00:21:56,320
regulate

629
00:21:56,320 --> 00:21:58,400
got it and then greg has a question

630
00:21:58,400 --> 00:22:00,799
asking besides deleting data such as

631
00:22:00,799 --> 00:22:03,760
search history and location history what

632
00:22:03,760 --> 00:22:05,600
could communications service providers

633
00:22:05,600 --> 00:22:07,760
do to protect people seeking and

634
00:22:07,760 --> 00:22:10,159
providing abortions against overzealous

635
00:22:10,159 --> 00:22:12,320
prosecutors in anti-abortion states so

636
00:22:12,320 --> 00:22:14,559
this is uh regarding roe v wade in our

637
00:22:14,559 --> 00:22:16,880
conversation just now and he also adds

638
00:22:16,880 --> 00:22:18,960
and could pro-abortion states enact

639
00:22:18,960 --> 00:22:22,080
legislation that would be helpful

640
00:22:22,080 --> 00:22:23,600
great question

641
00:22:23,600 --> 00:22:26,080
one of the things that we always say to

642
00:22:26,080 --> 00:22:27,679
you know any any entities that are

643
00:22:27,679 --> 00:22:29,679
collecting data is just think really

644
00:22:29,679 --> 00:22:31,840
hard about the data that you do collect

645
00:22:31,840 --> 00:22:34,080
in the first instance deleting and

646
00:22:34,080 --> 00:22:35,840
having limited retention periods is

647
00:22:35,840 --> 00:22:38,240
great but anytime you're collecting data

648
00:22:38,240 --> 00:22:39,679
and you're holding it for any period of

649
00:22:39,679 --> 00:22:41,600
time that's potentially available to law

650
00:22:41,600 --> 00:22:43,120
enforcement it's potentially available

651
00:22:43,120 --> 00:22:45,200
to you know uh

652
00:22:45,200 --> 00:22:47,440
anyone really if it's hacked if anything

653
00:22:47,440 --> 00:22:49,280
happens so i think the

654
00:22:49,280 --> 00:22:51,840
um the mentality of collect more and

655
00:22:51,840 --> 00:22:53,440
deal with the consequences later really

656
00:22:53,440 --> 00:22:55,280
needs to change and

657
00:22:55,280 --> 00:22:56,960
thinking of minimization of what's

658
00:22:56,960 --> 00:22:58,799
collected in the first place

659
00:22:58,799 --> 00:22:59,760
um

660
00:22:59,760 --> 00:23:01,600
certainly i think that

661
00:23:01,600 --> 00:23:03,679
a lot of states we will see

662
00:23:03,679 --> 00:23:05,840
that um you know want to do more to

663
00:23:05,840 --> 00:23:08,080
protect people seeking abortions will

664
00:23:08,080 --> 00:23:10,240
pass legislation if if roe v wade is

665
00:23:10,240 --> 00:23:11,360
overturned

666
00:23:11,360 --> 00:23:13,440
i think that there are a lot of

667
00:23:13,440 --> 00:23:15,360
things that states can do including

668
00:23:15,360 --> 00:23:17,200
examining their own state data

669
00:23:17,200 --> 00:23:18,799
collection practices

670
00:23:18,799 --> 00:23:20,880
what information can be shared with out

671
00:23:20,880 --> 00:23:23,360
of state authorities um how they treat

672
00:23:23,360 --> 00:23:25,120
warrants for example or other legal

673
00:23:25,120 --> 00:23:27,200
requests from other states um there's a

674
00:23:27,200 --> 00:23:29,840
lot of nuance and very special very

675
00:23:29,840 --> 00:23:31,360
complex legal issues that may be

676
00:23:31,360 --> 00:23:33,039
involved with these cross-state issues

677
00:23:33,039 --> 00:23:34,240
particularly if you have people

678
00:23:34,240 --> 00:23:35,840
traveling from one state where abortion

679
00:23:35,840 --> 00:23:37,280
is not permitted to a state where it is

680
00:23:37,280 --> 00:23:39,600
permitted but i would expect and hope to

681
00:23:39,600 --> 00:23:42,320
see that states do look really hard at

682
00:23:42,320 --> 00:23:44,240
those issues and again really think

683
00:23:44,240 --> 00:23:46,799
about what data has been collected and

684
00:23:46,799 --> 00:23:48,799
shared as a matter of course

685
00:23:48,799 --> 00:23:50,799
i think one of the challenges for for

686
00:23:50,799 --> 00:23:52,880
people like you is is the technology is

687
00:23:52,880 --> 00:23:55,679
also uh moving at such a pace that it's

688
00:23:55,679 --> 00:23:57,360
hard to be preventive which is what what

689
00:23:57,360 --> 00:23:59,279
your ultimate aim is right uh rather

690
00:23:59,279 --> 00:24:01,679
than in post fact but uh

691
00:24:01,679 --> 00:24:03,600
technology is moving at such a pace and

692
00:24:03,600 --> 00:24:05,600
trying to keep pace when frankly

693
00:24:05,600 --> 00:24:07,200
litigation legislation are slower

694
00:24:07,200 --> 00:24:10,080
processes it must be very frustrating um

695
00:24:10,080 --> 00:24:12,960
another question from michaela mantegna

696
00:24:12,960 --> 00:24:15,360
um affiliated with the bachmann klein

697
00:24:15,360 --> 00:24:16,400
center

698
00:24:16,400 --> 00:24:18,720
how can the president on clearview ai

699
00:24:18,720 --> 00:24:20,720
extend to other companies entering the

700
00:24:20,720 --> 00:24:24,080
space um i we talked a little bit about

701
00:24:24,080 --> 00:24:25,840
that is there any public information

702
00:24:25,840 --> 00:24:28,240
action taking a look into integrated

703
00:24:28,240 --> 00:24:31,279
databases for border patrol uh

704
00:24:31,279 --> 00:24:35,840
and other things used in airports asia

705
00:24:35,840 --> 00:24:37,840
so maybe i'll get to the second question

706
00:24:37,840 --> 00:24:38,880
um

707
00:24:38,880 --> 00:24:40,960
we have done a fair amount of work and

708
00:24:40,960 --> 00:24:43,760
other partner organizations and other

709
00:24:43,760 --> 00:24:45,200
civil society groups have done a lot of

710
00:24:45,200 --> 00:24:47,360
work looking at border data collection

711
00:24:47,360 --> 00:24:48,720
of border practices i didn't talk about

712
00:24:48,720 --> 00:24:50,960
that as much in this conversation right

713
00:24:50,960 --> 00:24:53,279
as you allude to there are so many

714
00:24:53,279 --> 00:24:54,640
databases that the department of

715
00:24:54,640 --> 00:24:56,960
homeland security has which collect

716
00:24:56,960 --> 00:24:59,440
information and they're interrelated

717
00:24:59,440 --> 00:25:01,200
often in ways that are opaque to the

718
00:25:01,200 --> 00:25:04,000
public um we know certain things about

719
00:25:04,000 --> 00:25:06,000
you know immigration databases about

720
00:25:06,000 --> 00:25:08,159
databases that local law enforcement

721
00:25:08,159 --> 00:25:09,919
have and share with

722
00:25:09,919 --> 00:25:11,600
border officials but there's one in

723
00:25:11,600 --> 00:25:13,679
particular you know i want to highlight

724
00:25:13,679 --> 00:25:15,679
which is um during the trump

725
00:25:15,679 --> 00:25:17,120
administration

726
00:25:17,120 --> 00:25:17,919
um

727
00:25:17,919 --> 00:25:20,480
there were you know moves to collect dna

728
00:25:20,480 --> 00:25:22,559
from people held in immigration custody

729
00:25:22,559 --> 00:25:23,760
certain categories that people held in

730
00:25:23,760 --> 00:25:25,600
immigration custody and then those dna

731
00:25:25,600 --> 00:25:28,640
profiles would be added to um you know

732
00:25:28,640 --> 00:25:31,600
the fbi's codis database which already

733
00:25:31,600 --> 00:25:33,279
has dna profiles from people who are

734
00:25:33,279 --> 00:25:34,720
arrested

735
00:25:34,720 --> 00:25:36,400
and so we had you know big concerns

736
00:25:36,400 --> 00:25:38,240
about this because you're expanding a

737
00:25:38,240 --> 00:25:39,840
database that started off in a law

738
00:25:39,840 --> 00:25:41,440
enforcement context

739
00:25:41,440 --> 00:25:43,039
moving it to an immigration border

740
00:25:43,039 --> 00:25:44,880
context and of course

741
00:25:44,880 --> 00:25:47,120
you know disproportionately the the dna

742
00:25:47,120 --> 00:25:48,320
samples are going to be collected from

743
00:25:48,320 --> 00:25:49,679
people of color

744
00:25:49,679 --> 00:25:51,120
disproportionately people who are held

745
00:25:51,120 --> 00:25:54,080
in immigration detention so

746
00:25:54,080 --> 00:25:56,880
that was one area where we really i

747
00:25:56,880 --> 00:25:58,400
think we still have a lot more work to

748
00:25:58,400 --> 00:26:00,880
do to understand um

749
00:26:00,880 --> 00:26:03,279
what dna collection

750
00:26:03,279 --> 00:26:04,880
um you know

751
00:26:04,880 --> 00:26:07,360
how that those databases are interlinked

752
00:26:07,360 --> 00:26:09,039
and going to be used with other existing

753
00:26:09,039 --> 00:26:10,880
databases because i think the expansion

754
00:26:10,880 --> 00:26:12,799
of dna collection in the us is of a big

755
00:26:12,799 --> 00:26:15,279
concern and particularly at the border

756
00:26:15,279 --> 00:26:17,840
asha thank you so much um i think uh we

757
00:26:17,840 --> 00:26:20,799
just have a few minutes left so maybe we

758
00:26:20,799 --> 00:26:22,720
allow you to wrap up with some parting

759
00:26:22,720 --> 00:26:24,880
thoughts a couple minutes so whatever

760
00:26:24,880 --> 00:26:26,880
you'd like to

761
00:26:26,880 --> 00:26:28,960
convey to people and make sure that they

762
00:26:28,960 --> 00:26:31,440
take away

763
00:26:31,679 --> 00:26:33,200
one big picture takeaway i want to

764
00:26:33,200 --> 00:26:34,720
emphasize is there's a lot of

765
00:26:34,720 --> 00:26:36,320
information out there about self-help we

766
00:26:36,320 --> 00:26:38,400
can take individual actions to protect

767
00:26:38,400 --> 00:26:39,840
our privacy

768
00:26:39,840 --> 00:26:41,600
a lot of guidelines out there and i

769
00:26:41,600 --> 00:26:42,960
think those are really important and

770
00:26:42,960 --> 00:26:45,279
people should take whatever steps they

771
00:26:45,279 --> 00:26:47,919
can to preserve individual privacy but i

772
00:26:47,919 --> 00:26:49,840
do want to emphasize the fact that the

773
00:26:49,840 --> 00:26:51,679
onus shouldn't be on us as individuals

774
00:26:51,679 --> 00:26:53,279
it is

775
00:26:53,279 --> 00:26:54,960
practically impossible for all of us to

776
00:26:54,960 --> 00:26:57,360
live in the modern world and and hide

777
00:26:57,360 --> 00:26:59,200
all of the data that that is out there

778
00:26:59,200 --> 00:27:01,039
and to prevent our data from being

779
00:27:01,039 --> 00:27:03,279
collected so we really have to push for

780
00:27:03,279 --> 00:27:05,520
regulation and we have to push for

781
00:27:05,520 --> 00:27:07,039
understandings of constitutional

782
00:27:07,039 --> 00:27:08,640
protections that take into account

783
00:27:08,640 --> 00:27:10,480
technological change

784
00:27:10,480 --> 00:27:12,240
this shouldn't be seen as an individual

785
00:27:12,240 --> 00:27:13,919
level problem if we just cared about our

786
00:27:13,919 --> 00:27:15,279
privacy enough and stopped using our

787
00:27:15,279 --> 00:27:16,720
smartphones because that's not the

788
00:27:16,720 --> 00:27:18,240
answer

789
00:27:18,240 --> 00:27:20,159
esha bhandari thank you so much for

790
00:27:20,159 --> 00:27:21,679
joining us

791
00:27:21,679 --> 00:27:23,120
thank you

792
00:27:23,120 --> 00:27:25,600
and that's it from writescon and the

793
00:27:25,600 --> 00:27:28,480
studio here we'll see you later tune in

794
00:27:28,480 --> 00:27:30,880
and of course stay engaged check out

795
00:27:30,880 --> 00:27:34,200
those sessions

796
00:27:37,840 --> 00:27:39,918
you

