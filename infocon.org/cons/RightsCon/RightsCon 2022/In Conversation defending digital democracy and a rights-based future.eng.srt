1
00:00:03,679 --> 00:00:05,440
welcome back to this studio we're in

2
00:00:05,440 --> 00:00:07,600
conversation the next 30 minutes to

3
00:00:07,600 --> 00:00:10,639
discuss defending digital democracy and

4
00:00:10,639 --> 00:00:12,880
this is a conversation that will include

5
00:00:12,880 --> 00:00:15,360
anne-marie entof larson she is the

6
00:00:15,360 --> 00:00:18,160
technology ambassador of denmark and

7
00:00:18,160 --> 00:00:21,680
also eileen donohue at stanford over to

8
00:00:21,680 --> 00:00:25,000
both of you

9
00:00:43,440 --> 00:00:45,680
uh eileen and

10
00:00:45,680 --> 00:00:48,800
uh ambassador can you hear me

11
00:00:48,800 --> 00:00:52,239
now i can hear you thank you so much my

12
00:00:52,239 --> 00:00:53,280
well

13
00:00:53,280 --> 00:00:54,879
good to see now the tech is working well

14
00:00:54,879 --> 00:00:58,239
good afternoon uh good morning good day

15
00:00:58,239 --> 00:00:59,680
to all of you thank you so much for

16
00:00:59,680 --> 00:01:02,320
tuning in and uh thank you melissa

17
00:01:02,320 --> 00:01:04,159
my name is emery larsen and it's my

18
00:01:04,159 --> 00:01:06,240
great pleasure to be here with um my

19
00:01:06,240 --> 00:01:09,040
good friend and a strong defender of

20
00:01:09,040 --> 00:01:10,880
democracy in a digital age eileen

21
00:01:10,880 --> 00:01:12,400
donahoe

22
00:01:12,400 --> 00:01:15,439
today we will be talking about how

23
00:01:15,439 --> 00:01:17,920
authoritarians those who do not believe

24
00:01:17,920 --> 00:01:20,880
in democracy are increasingly taking up

25
00:01:20,880 --> 00:01:23,119
the tools that we thought would bring us

26
00:01:23,119 --> 00:01:25,119
more democracy new digital tools they

27
00:01:25,119 --> 00:01:27,280
are taking up and using them

28
00:01:27,280 --> 00:01:29,280
in their fight for

29
00:01:29,280 --> 00:01:32,000
limiting freedoms for limiting human

30
00:01:32,000 --> 00:01:34,400
rights for limiting democracies whether

31
00:01:34,400 --> 00:01:37,040
it be civilians in data gathering

32
00:01:37,040 --> 00:01:38,880
whether it's the use of disinformation

33
00:01:38,880 --> 00:01:40,799
or cyber attack

34
00:01:40,799 --> 00:01:43,600
we have seen an intensity in increasing

35
00:01:43,600 --> 00:01:45,759
and solidifying higher power around

36
00:01:45,759 --> 00:01:47,920
digital technologies

37
00:01:47,920 --> 00:01:49,759
that i think is a dark outlook the

38
00:01:49,759 --> 00:01:51,280
positive outlook is also that

39
00:01:51,280 --> 00:01:53,280
governments civil society and national

40
00:01:53,280 --> 00:01:56,079
organizations have increasingly

41
00:01:56,079 --> 00:01:58,399
raised to the fore and understand now

42
00:01:58,399 --> 00:02:00,880
that this is effect about values and if

43
00:02:00,880 --> 00:02:03,600
we are to uphold democracy in a digital

44
00:02:03,600 --> 00:02:04,479
age

45
00:02:04,479 --> 00:02:06,159
and expand it and make it more

46
00:02:06,159 --> 00:02:08,399
meaningful inclusive and transparency we

47
00:02:08,399 --> 00:02:10,399
need to harness the moment of

48
00:02:10,399 --> 00:02:13,040
opportunity that we are in right now so

49
00:02:13,040 --> 00:02:14,640
that's why i'm looking forward to this

50
00:02:14,640 --> 00:02:18,160
conversation um we will dive into

51
00:02:18,160 --> 00:02:20,239
sort of two big segments one is first

52
00:02:20,239 --> 00:02:21,360
looking at what is digital

53
00:02:21,360 --> 00:02:22,959
authoritarianism what are the challenges

54
00:02:22,959 --> 00:02:24,640
around it and then secondly we'll look

55
00:02:24,640 --> 00:02:25,360
at

56
00:02:25,360 --> 00:02:27,040
the more positive side what are some of

57
00:02:27,040 --> 00:02:29,200
the solutions and hopefully we'll see

58
00:02:29,200 --> 00:02:31,440
whether we leave those conversations as

59
00:02:31,440 --> 00:02:33,840
optimists or pessimists or maybe

60
00:02:33,840 --> 00:02:36,640
realists on behalf of the future

61
00:02:36,640 --> 00:02:38,879
um i just want to say you can also ask

62
00:02:38,879 --> 00:02:41,200
questions in the chat box please do so

63
00:02:41,200 --> 00:02:44,000
and we'll we'll take them as we go

64
00:02:44,000 --> 00:02:47,280
but let me start eileen um

65
00:02:47,280 --> 00:02:48,239
you know

66
00:02:48,239 --> 00:02:50,319
digital authoritarianism i know it's

67
00:02:50,319 --> 00:02:52,879
hard to do in 30 seconds but you know

68
00:02:52,879 --> 00:02:55,440
in brief why is it and why is it that we

69
00:02:55,440 --> 00:02:57,680
should care about it

70
00:02:57,680 --> 00:02:59,599
so let me first say thank you for

71
00:02:59,599 --> 00:03:01,599
including me i love having conversation

72
00:03:01,599 --> 00:03:04,239
with you i always learn something new so

73
00:03:04,239 --> 00:03:06,640
from my vantage point i think we have to

74
00:03:06,640 --> 00:03:09,480
see digital authoritarianism as really a

75
00:03:09,480 --> 00:03:12,000
turbocharged version of authoritarian

76
00:03:12,000 --> 00:03:13,840
governance

77
00:03:13,840 --> 00:03:15,840
where repressive governments are

78
00:03:15,840 --> 00:03:18,879
capitalizing on data and digital tools

79
00:03:18,879 --> 00:03:21,200
for surveillance censorship

80
00:03:21,200 --> 00:03:23,840
spread of disinformation and basically

81
00:03:23,840 --> 00:03:26,959
to solidify state control

82
00:03:26,959 --> 00:03:29,280
i would note ai

83
00:03:29,280 --> 00:03:32,799
has enabled a whole new level of digital

84
00:03:32,799 --> 00:03:34,159
repression

85
00:03:34,159 --> 00:03:37,040
and given authoritarians a previously

86
00:03:37,040 --> 00:03:40,319
unimaginable level of state control over

87
00:03:40,319 --> 00:03:43,280
the information realm and also very

88
00:03:43,280 --> 00:03:45,040
concerningly new

89
00:03:45,040 --> 00:03:47,360
social engineering tools to shape

90
00:03:47,360 --> 00:03:50,239
citizen motivation and behaviors

91
00:03:50,239 --> 00:03:51,360
to your

92
00:03:51,360 --> 00:03:53,280
pointed question why should we be

93
00:03:53,280 --> 00:03:54,400
concerned

94
00:03:54,400 --> 00:03:56,959
we have to recognize this is not just a

95
00:03:56,959 --> 00:04:00,560
problem of repressive use of technology

96
00:04:00,560 --> 00:04:02,480
the key point for everyone is to

97
00:04:02,480 --> 00:04:04,959
understand digital authoritarianism is

98
00:04:04,959 --> 00:04:08,239
really an alternative form of governance

99
00:04:08,239 --> 00:04:11,200
an entire technosocial system

100
00:04:11,200 --> 00:04:14,400
that promises security and control for

101
00:04:14,400 --> 00:04:16,959
the state as opposed to liberty and

102
00:04:16,959 --> 00:04:19,600
security for individuals so it is really

103
00:04:19,600 --> 00:04:22,479
antithetical to democracy and the

104
00:04:22,479 --> 00:04:24,560
original vision we had of an open

105
00:04:24,560 --> 00:04:25,759
internet

106
00:04:25,759 --> 00:04:28,320
and my concern as we've discussed is

107
00:04:28,320 --> 00:04:30,320
that it could become the dominant model

108
00:04:30,320 --> 00:04:34,240
of governance in the 21st century

109
00:04:34,880 --> 00:04:37,199
that was indeed in 30 seconds thank you

110
00:04:37,199 --> 00:04:38,560
eileen

111
00:04:38,560 --> 00:04:39,360
so

112
00:04:39,360 --> 00:04:41,120
you know as an alternative form of

113
00:04:41,120 --> 00:04:43,199
governance we've seen that alternative

114
00:04:43,199 --> 00:04:45,600
form really

115
00:04:45,600 --> 00:04:47,840
winning on behalf of democracy if we

116
00:04:47,840 --> 00:04:50,240
look at all the numbers we're seeing you

117
00:04:50,240 --> 00:04:52,240
know a decline in digital rights a

118
00:04:52,240 --> 00:04:55,040
decline in freedom online a decline in

119
00:04:55,040 --> 00:04:57,280
human rights we see a rise in people

120
00:04:57,280 --> 00:04:59,440
individuals living in authoritarian

121
00:04:59,440 --> 00:05:01,600
regimes that rather than democracies so

122
00:05:01,600 --> 00:05:03,919
on the statistical side it looks like

123
00:05:03,919 --> 00:05:06,400
the numbers are against us

124
00:05:06,400 --> 00:05:08,560
on the positive side and a lot of the

125
00:05:08,560 --> 00:05:10,479
work i know both that you do at stanford

126
00:05:10,479 --> 00:05:12,560
your engagement in the freedom online

127
00:05:12,560 --> 00:05:14,800
coalition and the number of other places

128
00:05:14,800 --> 00:05:16,880
we're sort of you know seeing

129
00:05:16,880 --> 00:05:18,400
the baton being kicked out you know

130
00:05:18,400 --> 00:05:20,800
being taken up this uh renewed

131
00:05:20,800 --> 00:05:23,360
commitment to you know multi-stakeholder

132
00:05:23,360 --> 00:05:25,440
push for ensuring democracy and digital

133
00:05:25,440 --> 00:05:26,800
age so

134
00:05:26,800 --> 00:05:28,160
you know is this the moment of

135
00:05:28,160 --> 00:05:31,280
opportunity is this where things turn or

136
00:05:31,280 --> 00:05:33,600
is this the sort of moment the point of

137
00:05:33,600 --> 00:05:35,440
no return of which we realize there's

138
00:05:35,440 --> 00:05:37,520
nothing to be done

139
00:05:37,520 --> 00:05:40,240
so it's both but i have to start on the

140
00:05:40,240 --> 00:05:42,160
negative side of the equation and just

141
00:05:42,160 --> 00:05:44,960
really emphasize that and in part

142
00:05:44,960 --> 00:05:46,000
because

143
00:05:46,000 --> 00:05:47,680
what we have seen

144
00:05:47,680 --> 00:05:50,560
is an unconscious drift within

145
00:05:50,560 --> 00:05:52,479
democracies themselves

146
00:05:52,479 --> 00:05:54,800
in terms of utilization of data and

147
00:05:54,800 --> 00:05:57,360
technology in ways that are inconsistent

148
00:05:57,360 --> 00:05:59,680
with human rights commitments and so

149
00:05:59,680 --> 00:06:02,479
many repressive practices have kind of

150
00:06:02,479 --> 00:06:04,720
been normalized around the world like

151
00:06:04,720 --> 00:06:06,400
internet shutdowns

152
00:06:06,400 --> 00:06:08,960
or use of spyware and hacking tools

153
00:06:08,960 --> 00:06:11,680
against human rights defenders lawyers

154
00:06:11,680 --> 00:06:14,080
opposition figures

155
00:06:14,080 --> 00:06:16,800
there's also a lot of repressive cyber

156
00:06:16,800 --> 00:06:19,919
regulation and digital regulation

157
00:06:19,919 --> 00:06:21,919
again inconsistent with the in

158
00:06:21,919 --> 00:06:24,160
international human rights law framework

159
00:06:24,160 --> 00:06:27,600
so i i have to emphasize there is an

160
00:06:27,600 --> 00:06:29,680
intentional aspect to this in the

161
00:06:29,680 --> 00:06:31,840
authoritarian world where there's an

162
00:06:31,840 --> 00:06:35,280
embrace of this entire model

163
00:06:35,280 --> 00:06:38,000
to achieve repressive aims but on the

164
00:06:38,000 --> 00:06:40,960
democratic side there we also see

165
00:06:40,960 --> 00:06:42,639
pretty bad trends

166
00:06:42,639 --> 00:06:45,759
on the positive side there are a couple

167
00:06:45,759 --> 00:06:47,280
of

168
00:06:47,280 --> 00:06:48,880
indicators that

169
00:06:48,880 --> 00:06:51,039
the democratic world has woken up to

170
00:06:51,039 --> 00:06:52,319
some extent

171
00:06:52,319 --> 00:06:54,560
and i will mention too the first of

172
00:06:54,560 --> 00:06:57,520
which was the denmark tech for democracy

173
00:06:57,520 --> 00:06:58,479
summit

174
00:06:58,479 --> 00:07:01,280
which was one of the most comprehensive

175
00:07:01,280 --> 00:07:02,479
positive

176
00:07:02,479 --> 00:07:05,039
visions of how we can bring the

177
00:07:05,039 --> 00:07:08,319
democratic world together and utilize

178
00:07:08,319 --> 00:07:11,039
technology to support democracy

179
00:07:11,039 --> 00:07:14,000
and get our acts together in terms of

180
00:07:14,000 --> 00:07:16,160
a democratic approach to use and

181
00:07:16,160 --> 00:07:18,319
regulation of tech and i will also

182
00:07:18,319 --> 00:07:21,680
mention the u.s summit for democracy

183
00:07:21,680 --> 00:07:24,639
um which was the biden administration's

184
00:07:24,639 --> 00:07:27,680
big initiative but the concern about

185
00:07:27,680 --> 00:07:30,400
digital authoritarianism was one of the

186
00:07:30,400 --> 00:07:32,800
big pillars authoritarianism and then

187
00:07:32,800 --> 00:07:35,199
recognition that technology has become

188
00:07:35,199 --> 00:07:36,400
the vehicle

189
00:07:36,400 --> 00:07:38,800
for authoritarians to assert their

190
00:07:38,800 --> 00:07:41,759
control so those two signals give me

191
00:07:41,759 --> 00:07:44,080
some optimism about the seriousness with

192
00:07:44,080 --> 00:07:45,759
which democratic governments are going

193
00:07:45,759 --> 00:07:48,479
to take this threat

194
00:07:48,479 --> 00:07:52,160
so let me let me flip it back to you um

195
00:07:52,160 --> 00:07:54,560
and let's shift our focus even more

196
00:07:54,560 --> 00:07:57,440
explicitly and further on the democratic

197
00:07:57,440 --> 00:08:00,000
side of the equation and i want to hear

198
00:08:00,000 --> 00:08:02,560
your assessment of how democratic

199
00:08:02,560 --> 00:08:03,759
governments have

200
00:08:03,759 --> 00:08:06,639
have really responded if at all

201
00:08:06,639 --> 00:08:08,879
to the authoritarian model

202
00:08:08,879 --> 00:08:11,599
or do you see them kind of sleepwalking

203
00:08:11,599 --> 00:08:13,199
into a world where digital

204
00:08:13,199 --> 00:08:16,000
authoritarianism becomes the dominant

205
00:08:16,000 --> 00:08:17,520
model

206
00:08:17,520 --> 00:08:19,520
where are we from your vantage point

207
00:08:19,520 --> 00:08:21,520
from where you sit in terms of advancing

208
00:08:21,520 --> 00:08:24,719
this democratic vision

209
00:08:25,360 --> 00:08:28,000
i think we are at a pivotal moment in

210
00:08:28,000 --> 00:08:30,240
the sense that

211
00:08:30,240 --> 00:08:31,840
when i came

212
00:08:31,840 --> 00:08:33,679
you know came of age and grew up over

213
00:08:33,679 --> 00:08:37,519
the past two decades uh it was a period

214
00:08:37,519 --> 00:08:39,599
of renewed optimism

215
00:08:39,599 --> 00:08:41,039
that we would see more and more

216
00:08:41,039 --> 00:08:43,599
democracies thriving around the world

217
00:08:43,599 --> 00:08:45,440
and it's sort of going from from the end

218
00:08:45,440 --> 00:08:48,800
of history but into this amazing time

219
00:08:48,800 --> 00:08:49,519
when

220
00:08:49,519 --> 00:08:52,000
you know twitter uh was a platform that

221
00:08:52,000 --> 00:08:53,839
would leave the arab spring routine more

222
00:08:53,839 --> 00:08:56,240
democracies in the middle east it was

223
00:08:56,240 --> 00:08:58,320
the facebook would connect us to the

224
00:08:58,320 --> 00:09:00,080
entire world and

225
00:09:00,080 --> 00:09:02,880
uh sort of across languages and cultures

226
00:09:02,880 --> 00:09:03,600
and

227
00:09:03,600 --> 00:09:05,760
um different barriers we would sort of

228
00:09:05,760 --> 00:09:08,720
be reunited in this beautiful digital

229
00:09:08,720 --> 00:09:09,600
way

230
00:09:09,600 --> 00:09:10,560
um

231
00:09:10,560 --> 00:09:11,839
and i think in that sense we did

232
00:09:11,839 --> 00:09:13,839
sleepwalk we were looking at these

233
00:09:13,839 --> 00:09:16,560
technology companies as sort of modern

234
00:09:16,560 --> 00:09:17,519
savers

235
00:09:17,519 --> 00:09:20,080
as opportunities for us to

236
00:09:20,080 --> 00:09:22,240
finally address the sort of the last

237
00:09:22,240 --> 00:09:24,399
strong men of the 20th century and

238
00:09:24,399 --> 00:09:25,839
ensure that democracy became the

239
00:09:25,839 --> 00:09:27,360
dominance

240
00:09:27,360 --> 00:09:30,080
well the thing is that we were naive i

241
00:09:30,080 --> 00:09:31,360
think we've been sleepwalking and we

242
00:09:31,360 --> 00:09:32,880
were somewhat may even thinking that

243
00:09:32,880 --> 00:09:35,519
that would all happen by itself um we

244
00:09:35,519 --> 00:09:37,360
were dependent on the technology that we

245
00:09:37,360 --> 00:09:39,519
didn't ask the right requirements to we

246
00:09:39,519 --> 00:09:41,839
did not demand enough of that to be

247
00:09:41,839 --> 00:09:43,360
actually supporting democracy and

248
00:09:43,360 --> 00:09:44,959
looking at the long sort of long

249
00:09:44,959 --> 00:09:47,279
perspective and i think more importantly

250
00:09:47,279 --> 00:09:49,360
we saw that the same tools that can be

251
00:09:49,360 --> 00:09:51,120
liberating they are phenomenal for

252
00:09:51,120 --> 00:09:53,360
suppression they are phenomenal for

253
00:09:53,360 --> 00:09:55,279
suppressing your own you know your own

254
00:09:55,279 --> 00:09:57,120
people and as you said sort of an

255
00:09:57,120 --> 00:09:58,959
intentional approach that is

256
00:09:58,959 --> 00:10:00,800
inconsistent with human rights law and

257
00:10:00,800 --> 00:10:01,600
that

258
00:10:01,600 --> 00:10:03,360
fundamentally is focusing on other

259
00:10:03,360 --> 00:10:04,880
responsibilities rather than those of

260
00:10:04,880 --> 00:10:06,640
the citizens so

261
00:10:06,640 --> 00:10:09,279
i think in short yes there has been

262
00:10:09,279 --> 00:10:11,360
sleepwalking or at least a naive

263
00:10:11,360 --> 00:10:12,800
approach to this

264
00:10:12,800 --> 00:10:15,040
the good thing is that that is no longer

265
00:10:15,040 --> 00:10:18,399
the case um the amount of you know

266
00:10:18,399 --> 00:10:19,839
governments around the world i think

267
00:10:19,839 --> 00:10:21,440
that really took the bible summit as you

268
00:10:21,440 --> 00:10:23,279
mentioned as a call to action for saying

269
00:10:23,279 --> 00:10:24,959
this is a time for us to do something

270
00:10:24,959 --> 00:10:26,079
different

271
00:10:26,079 --> 00:10:28,000
and most importantly it's a time for us

272
00:10:28,000 --> 00:10:30,000
to do things not only abroad and far

273
00:10:30,000 --> 00:10:32,959
away but look at home look at how the

274
00:10:32,959 --> 00:10:34,880
technologies that we are developing and

275
00:10:34,880 --> 00:10:36,640
using and integrating into our

276
00:10:36,640 --> 00:10:38,800
governments using inner citizens that is

277
00:10:38,800 --> 00:10:40,720
the backbone of our infrastructure and

278
00:10:40,720 --> 00:10:42,160
the ones that we are supporting other

279
00:10:42,160 --> 00:10:43,519
countries with

280
00:10:43,519 --> 00:10:45,600
how are they actually becoming

281
00:10:45,600 --> 00:10:47,839
democracy affirmative and how do they

282
00:10:47,839 --> 00:10:49,920
support the values that you know frankly

283
00:10:49,920 --> 00:10:51,120
we've probably been taking a bit for

284
00:10:51,120 --> 00:10:52,720
granted

285
00:10:52,720 --> 00:10:56,000
so um staying with this theme of um

286
00:10:56,000 --> 00:10:58,800
democracies waking up and the need for

287
00:10:58,800 --> 00:11:01,120
real democratic unity if we're going to

288
00:11:01,120 --> 00:11:02,800
confront this threat

289
00:11:02,800 --> 00:11:05,120
i want you to focus a little bit

290
00:11:05,120 --> 00:11:07,200
more specifically on what's going on in

291
00:11:07,200 --> 00:11:08,560
europe

292
00:11:08,560 --> 00:11:11,279
um and help us understand the different

293
00:11:11,279 --> 00:11:13,519
strands of thought in europe with

294
00:11:13,519 --> 00:11:15,680
respect to digital technology more

295
00:11:15,680 --> 00:11:18,880
generally and also with respect to the

296
00:11:18,880 --> 00:11:20,959
the digital authoritarian threat by

297
00:11:20,959 --> 00:11:24,000
government and i will note that the

298
00:11:24,000 --> 00:11:25,839
impression from the united states is

299
00:11:25,839 --> 00:11:29,120
that within europe the primary focus has

300
00:11:29,120 --> 00:11:30,880
been on reigning in

301
00:11:30,880 --> 00:11:32,800
u.s big tech

302
00:11:32,800 --> 00:11:33,600
and

303
00:11:33,600 --> 00:11:36,320
less focus on the digital authoritarian

304
00:11:36,320 --> 00:11:38,160
geopolitical threat

305
00:11:38,160 --> 00:11:40,959
so do you think that's a fair assessment

306
00:11:40,959 --> 00:11:42,880
and i will also say you know we could

307
00:11:42,880 --> 00:11:44,279
almost look at this

308
00:11:44,279 --> 00:11:46,959
pre-invasion of ukraine and post

309
00:11:46,959 --> 00:11:49,120
invasion of ukraine because

310
00:11:49,120 --> 00:11:51,839
there's been a dramatic shift in the in

311
00:11:51,839 --> 00:11:54,839
terms of democratic unity since then but

312
00:11:54,839 --> 00:11:57,600
pre-invasion of ukraine let's take

313
00:11:57,600 --> 00:11:59,519
where was europe and how much have

314
00:11:59,519 --> 00:12:01,440
things changed

315
00:12:01,440 --> 00:12:03,279
let's uh i definitely agree with you

316
00:12:03,279 --> 00:12:05,200
there's sort of february 24 sort of

317
00:12:05,200 --> 00:12:07,040
marks another you know there's a before

318
00:12:07,040 --> 00:12:09,680
and after looking at before

319
00:12:09,680 --> 00:12:10,480
i

320
00:12:10,480 --> 00:12:12,800
absolutely agree like looking as i'm you

321
00:12:12,800 --> 00:12:14,399
know somewhat as an outsider sitting

322
00:12:14,399 --> 00:12:16,560
over here in silicon valley europe has

323
00:12:16,560 --> 00:12:18,560
been focusing largely on reigning in big

324
00:12:18,560 --> 00:12:20,880
tech um some of the most you know

325
00:12:20,880 --> 00:12:22,800
significant pieces of legislation i

326
00:12:22,800 --> 00:12:25,600
think over the past 10 years and some of

327
00:12:25,600 --> 00:12:27,120
the pieces of legislation that will

328
00:12:27,120 --> 00:12:29,440
really change um

329
00:12:29,440 --> 00:12:31,680
how big big tech is operating has come

330
00:12:31,680 --> 00:12:33,680
the digital services act looking at

331
00:12:33,680 --> 00:12:35,600
online content moderation the digital

332
00:12:35,600 --> 00:12:37,360
markets act looking at entry trust

333
00:12:37,360 --> 00:12:40,560
measures um gdpr a lot of the work being

334
00:12:40,560 --> 00:12:43,360
done on ai now

335
00:12:43,360 --> 00:12:45,120
that being said i think it's because

336
00:12:45,120 --> 00:12:47,120
that was the immediate challenge that we

337
00:12:47,120 --> 00:12:49,519
knew how to tackle we know how to do

338
00:12:49,519 --> 00:12:51,920
antitrust and how to maybe take the

339
00:12:51,920 --> 00:12:53,440
toolbox of the 20th century of

340
00:12:53,440 --> 00:12:55,600
anti-trust and you know brief consider

341
00:12:55,600 --> 00:12:57,600
and reconfigure it for the for the 21st

342
00:12:57,600 --> 00:12:59,440
century um

343
00:12:59,440 --> 00:13:00,880
content moderation we've been discussing

344
00:13:00,880 --> 00:13:03,120
that for so long so not to say that the

345
00:13:03,120 --> 00:13:04,720
process of the digital services act was

346
00:13:04,720 --> 00:13:08,160
certainly uh not easy

347
00:13:08,160 --> 00:13:10,240
but we know the problem at hand when it

348
00:13:10,240 --> 00:13:11,920
comes to digital authoritarianism it's

349
00:13:11,920 --> 00:13:14,160
much more abstract it's um some of the

350
00:13:14,160 --> 00:13:15,760
geopolitical implications are much more

351
00:13:15,760 --> 00:13:16,880
challenging

352
00:13:16,880 --> 00:13:17,760
um

353
00:13:17,760 --> 00:13:19,120
and so going to a point i think there

354
00:13:19,120 --> 00:13:21,200
was a before you know

355
00:13:21,200 --> 00:13:22,880
before february 24th but with the

356
00:13:22,880 --> 00:13:24,639
invasion of ukraine i think we've seen

357
00:13:24,639 --> 00:13:26,320
sort of a new opening and saying this is

358
00:13:26,320 --> 00:13:27,519
this not

359
00:13:27,519 --> 00:13:30,399
this is not europe against american tech

360
00:13:30,399 --> 00:13:31,839
there's much more that unites and

361
00:13:31,839 --> 00:13:34,240
divides us

362
00:13:34,240 --> 00:13:36,880
yes there are some critical you know

363
00:13:36,880 --> 00:13:38,800
societal implications of big tech we

364
00:13:38,800 --> 00:13:40,160
need to address them i think we're doing

365
00:13:40,160 --> 00:13:42,800
that adequately with dna and dsa

366
00:13:42,800 --> 00:13:44,480
but these technologies they were founded

367
00:13:44,480 --> 00:13:46,720
in democratic countries they operate

368
00:13:46,720 --> 00:13:49,519
within democratic jurisdictions um we

369
00:13:49,519 --> 00:13:51,519
now have room for managing the downsides

370
00:13:51,519 --> 00:13:53,839
but we still believe ultimately that

371
00:13:53,839 --> 00:13:55,760
they are part of the solution you know

372
00:13:55,760 --> 00:13:57,360
that's the reason why denmark has a tech

373
00:13:57,360 --> 00:13:58,959
embassy in silicon valley that's for

374
00:13:58,959 --> 00:14:00,560
engaging diplomatically with them

375
00:14:00,560 --> 00:14:02,480
because we don't think it's about

376
00:14:02,480 --> 00:14:03,839
banning their products and services on

377
00:14:03,839 --> 00:14:05,040
the contrary we think they're actually

378
00:14:05,040 --> 00:14:06,800
some of the reasons for for hope in this

379
00:14:06,800 --> 00:14:08,880
world so you know just just a short

380
00:14:08,880 --> 00:14:11,199
point on you know after what happened

381
00:14:11,199 --> 00:14:13,839
you know on february 24th europe has

382
00:14:13,839 --> 00:14:16,800
been reunited i think we found out what

383
00:14:16,800 --> 00:14:19,120
is it worth fighting for what are the

384
00:14:19,120 --> 00:14:21,199
european i want to say transatlantic

385
00:14:21,199 --> 00:14:23,760
ideals of human rights of freedom of

386
00:14:23,760 --> 00:14:25,920
democracy that we've been taking for

387
00:14:25,920 --> 00:14:27,839
granted and all of a sudden there are

388
00:14:27,839 --> 00:14:29,120
boots on the ground and there are

389
00:14:29,120 --> 00:14:31,680
attacks in cyberspace that are not only

390
00:14:31,680 --> 00:14:33,760
threatening but directly attacking those

391
00:14:33,760 --> 00:14:34,720
values

392
00:14:34,720 --> 00:14:35,920
and so now

393
00:14:35,920 --> 00:14:37,600
i think we're about yeah i think we're a

394
00:14:37,600 --> 00:14:40,160
new moment also for technology companies

395
00:14:40,160 --> 00:14:41,279
um

396
00:14:41,279 --> 00:14:43,199
to actually you know work together on

397
00:14:43,199 --> 00:14:44,800
this front and that's looking at digital

398
00:14:44,800 --> 00:14:47,040
authoritarianism

399
00:14:47,040 --> 00:14:48,560
i think in that there's still a big

400
00:14:48,560 --> 00:14:51,199
question on on one of our one of the big

401
00:14:51,199 --> 00:14:53,519
countries to the east and and eileen i

402
00:14:53,519 --> 00:14:55,040
know you've been super engaged in this

403
00:14:55,040 --> 00:14:57,040
question around china digital

404
00:14:57,040 --> 00:14:58,560
authoritarianism obviously right now a

405
00:14:58,560 --> 00:15:00,560
lot of our eyes are on russia

406
00:15:00,560 --> 00:15:03,519
but i'll be curious you know where does

407
00:15:03,519 --> 00:15:07,519
we currently stand with regards to china

408
00:15:07,600 --> 00:15:10,000
oh so you know we do have a lot to worry

409
00:15:10,000 --> 00:15:12,000
about when it comes to russia and i

410
00:15:12,000 --> 00:15:14,399
think there has been a you know

411
00:15:14,399 --> 00:15:16,880
renewed understanding that the russia

412
00:15:16,880 --> 00:15:19,360
threat never really went away

413
00:15:19,360 --> 00:15:22,320
that said in the digital context to me

414
00:15:22,320 --> 00:15:24,399
china really is the thousand pound

415
00:15:24,399 --> 00:15:27,920
gorilla that we're up against and

416
00:15:27,920 --> 00:15:30,639
you know my my first point is we need to

417
00:15:30,639 --> 00:15:34,079
understand they have gigantic ambition

418
00:15:34,079 --> 00:15:36,240
to remake the international order

419
00:15:36,240 --> 00:15:38,639
according to authoritarian values and

420
00:15:38,639 --> 00:15:41,680
their vision of digital society

421
00:15:41,680 --> 00:15:44,720
and they are working really proactively

422
00:15:44,720 --> 00:15:47,199
to spread their model of digital

423
00:15:47,199 --> 00:15:50,480
authoritarianism on multiple layers

424
00:15:50,480 --> 00:15:52,959
you know it starts just by showcasing

425
00:15:52,959 --> 00:15:55,920
how effective they can be at control

426
00:15:55,920 --> 00:15:58,880
with digital tools at home and then they

427
00:15:58,880 --> 00:16:01,440
export and normalize those tools and

428
00:16:01,440 --> 00:16:03,440
practices abroad

429
00:16:03,440 --> 00:16:06,000
even more concerning in the export realm

430
00:16:06,000 --> 00:16:08,399
is that entire digital information

431
00:16:08,399 --> 00:16:10,880
infrastructure systems are being

432
00:16:10,880 --> 00:16:13,440
built and maintained in the developing

433
00:16:13,440 --> 00:16:14,480
world

434
00:16:14,480 --> 00:16:17,279
and china is really gaining leverage

435
00:16:17,279 --> 00:16:21,120
over those weaker importing states for

436
00:16:21,120 --> 00:16:24,320
decades to come by virtue of importing

437
00:16:24,320 --> 00:16:26,000
that infrastructure and they're also

438
00:16:26,000 --> 00:16:29,519
getting more new sources of data

439
00:16:29,519 --> 00:16:33,279
but beyond the technology diffusion

440
00:16:33,279 --> 00:16:34,959
there's also

441
00:16:34,959 --> 00:16:38,160
diffusion of ideas and norms we see them

442
00:16:38,160 --> 00:16:39,839
flooding the zone

443
00:16:39,839 --> 00:16:42,880
of international diplomacy related to

444
00:16:42,880 --> 00:16:44,639
cyber security

445
00:16:44,639 --> 00:16:46,639
and digital policy

446
00:16:46,639 --> 00:16:50,240
international standard organizations

447
00:16:50,240 --> 00:16:51,920
like the itu

448
00:16:51,920 --> 00:16:55,279
you know where they are trying to

449
00:16:55,279 --> 00:16:56,199
shape

450
00:16:56,199 --> 00:16:58,160
interoperability protocols for the

451
00:16:58,160 --> 00:16:59,199
future

452
00:16:59,199 --> 00:17:01,600
and that will have very big consequences

453
00:17:01,600 --> 00:17:04,640
if we do not wake up to those those

454
00:17:04,640 --> 00:17:06,319
other points of leverage that they've

455
00:17:06,319 --> 00:17:07,119
got

456
00:17:07,119 --> 00:17:09,720
and i'll also just mention besides the

457
00:17:09,720 --> 00:17:11,760
disinformation front and the

458
00:17:11,760 --> 00:17:14,799
surreptitious malign activity that goes

459
00:17:14,799 --> 00:17:17,119
on across borders

460
00:17:17,119 --> 00:17:20,079
we see really aggressive what they call

461
00:17:20,079 --> 00:17:23,119
wolf warrior diplomacy where they are

462
00:17:23,119 --> 00:17:24,400
openly

463
00:17:24,400 --> 00:17:26,959
using propaganda and their form of

464
00:17:26,959 --> 00:17:28,799
strategic communication

465
00:17:28,799 --> 00:17:30,000
to

466
00:17:30,000 --> 00:17:32,559
try to underscore their perception of

467
00:17:32,559 --> 00:17:35,039
democracy being a weaker form of

468
00:17:35,039 --> 00:17:36,320
governance

469
00:17:36,320 --> 00:17:37,120
um

470
00:17:37,120 --> 00:17:39,679
last point though i really want to make

471
00:17:39,679 --> 00:17:40,559
two

472
00:17:40,559 --> 00:17:43,440
democratic stakeholders on this is that

473
00:17:43,440 --> 00:17:46,720
china's international influence

474
00:17:46,720 --> 00:17:48,240
happening through all those layers

475
00:17:48,240 --> 00:17:49,600
really has

476
00:17:49,600 --> 00:17:50,720
started

477
00:17:50,720 --> 00:17:54,240
with massive investment in technology r

478
00:17:54,240 --> 00:17:55,360
d

479
00:17:55,360 --> 00:17:57,520
uh innovation and

480
00:17:57,520 --> 00:17:58,559
um

481
00:17:58,559 --> 00:18:01,360
they know the ccp knows that power

482
00:18:01,360 --> 00:18:03,679
derives from technology in a digital

483
00:18:03,679 --> 00:18:04,559
world

484
00:18:04,559 --> 00:18:06,640
and that through technological

485
00:18:06,640 --> 00:18:09,760
superiority they can gain military

486
00:18:09,760 --> 00:18:13,760
dominance economic dominance

487
00:18:13,760 --> 00:18:16,320
dominance and even have much bigger

488
00:18:16,320 --> 00:18:18,880
normative influence through the

489
00:18:18,880 --> 00:18:20,160
through the fact that they have

490
00:18:20,160 --> 00:18:23,360
technological power and so we can't

491
00:18:23,360 --> 00:18:26,480
afford to see technology as the problem

492
00:18:26,480 --> 00:18:28,640
we have to remember it's about

493
00:18:28,640 --> 00:18:31,039
leading in technology and then leading

494
00:18:31,039 --> 00:18:32,960
in governance of technology with

495
00:18:32,960 --> 00:18:36,400
values-based frameworks

496
00:18:36,799 --> 00:18:38,480
maybe that's a great segment i mean

497
00:18:38,480 --> 00:18:40,840
leading in technology leading in

498
00:18:40,840 --> 00:18:43,280
governance um i think you pointed out

499
00:18:43,280 --> 00:18:44,880
very well

500
00:18:44,880 --> 00:18:46,559
you know russia and what's happening

501
00:18:46,559 --> 00:18:48,160
right now might be the immediate

502
00:18:48,160 --> 00:18:49,760
challenge but i think you really laid

503
00:18:49,760 --> 00:18:51,360
out what are some of the much more

504
00:18:51,360 --> 00:18:53,440
structural substantial systemic

505
00:18:53,440 --> 00:18:54,799
challenges in

506
00:18:54,799 --> 00:18:58,240
two three five ten years from now

507
00:18:58,240 --> 00:19:00,480
let's go to the positive well let's go

508
00:19:00,480 --> 00:19:03,840
to the optimists um perspective on this

509
00:19:03,840 --> 00:19:05,679
i know you have a sort of a quite

510
00:19:05,679 --> 00:19:07,280
elaborate framework for them what is the

511
00:19:07,280 --> 00:19:10,000
solution how do we tackle and address

512
00:19:10,000 --> 00:19:12,799
this um maybe you want to share with us

513
00:19:12,799 --> 00:19:16,160
um i think it's a five-point plan

514
00:19:16,160 --> 00:19:18,400
well it was interesting having been part

515
00:19:18,400 --> 00:19:20,559
of your the denmark summit it really

516
00:19:20,559 --> 00:19:22,559
forced me to get very concrete and

517
00:19:22,559 --> 00:19:25,760
thinking okay what do we do about it um

518
00:19:25,760 --> 00:19:28,960
i i i like to put things in frameworks

519
00:19:28,960 --> 00:19:31,840
and i tend to see this as at least a

520
00:19:31,840 --> 00:19:34,240
three level three layer challenge

521
00:19:34,240 --> 00:19:35,679
different kinds of issues one is

522
00:19:35,679 --> 00:19:39,600
literally investment in technology

523
00:19:39,600 --> 00:19:42,480
to in where there is both a defensive

524
00:19:42,480 --> 00:19:44,880
part and an offensive part on the on the

525
00:19:44,880 --> 00:19:47,280
defense we we need to protect our

526
00:19:47,280 --> 00:19:48,880
critical supply chains like

527
00:19:48,880 --> 00:19:50,559
semiconductors

528
00:19:50,559 --> 00:19:54,160
um we need to make sure that there's

529
00:19:54,160 --> 00:19:56,240
adequate export controls on

530
00:19:56,240 --> 00:19:59,120
semiconductor manufacturing equipment

531
00:19:59,120 --> 00:20:01,520
which is if we hand that over we are

532
00:20:01,520 --> 00:20:03,600
handing over an area where we have

533
00:20:03,600 --> 00:20:06,159
superiority and there are certain

534
00:20:06,159 --> 00:20:08,400
critical technologies where we are right

535
00:20:08,400 --> 00:20:11,120
to protect and defend our advantage

536
00:20:11,120 --> 00:20:13,360
but on the offensive side when it comes

537
00:20:13,360 --> 00:20:16,480
to technology we really need to invest

538
00:20:16,480 --> 00:20:21,360
dollars in research development talent

539
00:20:21,360 --> 00:20:23,360
building skills

540
00:20:23,360 --> 00:20:26,080
leading on horizon technologies you know

541
00:20:26,080 --> 00:20:29,520
furthering um our advances in ai looking

542
00:20:29,520 --> 00:20:32,799
at quantum computing which really could

543
00:20:32,799 --> 00:20:36,960
be a game-changing technology and we you

544
00:20:36,960 --> 00:20:38,960
know there is a theory that

545
00:20:38,960 --> 00:20:41,919
quantum could be kind of winner take all

546
00:20:41,919 --> 00:20:44,559
if somebody gets a real leap ahead so we

547
00:20:44,559 --> 00:20:47,840
can't afford not to uh be superior in

548
00:20:47,840 --> 00:20:49,520
that category

549
00:20:49,520 --> 00:20:51,360
and then i think there are other

550
00:20:51,360 --> 00:20:54,959
technologies where

551
00:20:55,200 --> 00:20:58,000
the technology itself is supporting

552
00:20:58,000 --> 00:21:00,960
democracy sort of privacy enhancing

553
00:21:00,960 --> 00:21:04,159
technology so we can capitalize on data

554
00:21:04,159 --> 00:21:07,280
without giving up privacy

555
00:21:07,280 --> 00:21:10,559
and also my my big pet peeve is we you

556
00:21:10,559 --> 00:21:13,039
know the need to focus on quantum

557
00:21:13,039 --> 00:21:15,039
resilient encryption

558
00:21:15,039 --> 00:21:17,280
that is a technology that i would put at

559
00:21:17,280 --> 00:21:20,080
the top of the list we must lead there

560
00:21:20,080 --> 00:21:22,400
because if we don't if if we don't

561
00:21:22,400 --> 00:21:25,520
figure out how to protect our encryption

562
00:21:25,520 --> 00:21:26,880
encryption

563
00:21:26,880 --> 00:21:28,559
models and systems

564
00:21:28,559 --> 00:21:30,799
once somebody has that power

565
00:21:30,799 --> 00:21:33,840
we are our sense of radical cyber

566
00:21:33,840 --> 00:21:36,000
insecurity will go

567
00:21:36,000 --> 00:21:37,919
on steroids um

568
00:21:37,919 --> 00:21:40,320
so that's the tech layer underneath that

569
00:21:40,320 --> 00:21:42,000
i think there is a

570
00:21:42,000 --> 00:21:42,720
the

571
00:21:42,720 --> 00:21:45,039
governance layer tech governance that's

572
00:21:45,039 --> 00:21:47,919
got to be values based human rights

573
00:21:47,919 --> 00:21:50,080
based and there we need to do a much

574
00:21:50,080 --> 00:21:53,919
better job of articulating how would we

575
00:21:53,919 --> 00:21:55,600
demonstrate

576
00:21:55,600 --> 00:21:56,960
democratic

577
00:21:56,960 --> 00:22:00,400
human rights-based use and regulation of

578
00:22:00,400 --> 00:22:02,400
data and technology

579
00:22:02,400 --> 00:22:04,240
there has been a lot of progress in that

580
00:22:04,240 --> 00:22:06,480
regarding europe in particular

581
00:22:06,480 --> 00:22:08,480
we need much more work these two things

582
00:22:08,480 --> 00:22:10,799
have to go together the the tech

583
00:22:10,799 --> 00:22:12,960
superiority and the tech governance

584
00:22:12,960 --> 00:22:15,840
leadership and then the last thing

585
00:22:15,840 --> 00:22:18,880
is if we want to lead globally we can't

586
00:22:18,880 --> 00:22:22,240
retreat into sort of our own countries

587
00:22:22,240 --> 00:22:23,600
our own

588
00:22:23,600 --> 00:22:26,880
digital sovereignty or even just into

589
00:22:26,880 --> 00:22:29,440
the transatlantic alliance

590
00:22:29,440 --> 00:22:32,000
we have to have the bigger tent around

591
00:22:32,000 --> 00:22:34,640
the globe and so we have to invest much

592
00:22:34,640 --> 00:22:37,679
more in international diplomacy with

593
00:22:37,679 --> 00:22:40,720
respect to these norms cyber norms

594
00:22:40,720 --> 00:22:43,520
use of tech digital policy

595
00:22:43,520 --> 00:22:45,760
as i said earlier international standard

596
00:22:45,760 --> 00:22:49,280
setting we we need um to make sure the

597
00:22:49,280 --> 00:22:51,440
democratic world is bigger than the

598
00:22:51,440 --> 00:22:54,799
authoritarian world in digital society

599
00:22:54,799 --> 00:22:57,600
that's my grid um

600
00:22:57,600 --> 00:23:00,159
i'm curious we should go right to you on

601
00:23:00,159 --> 00:23:02,240
this exact front though because denmark

602
00:23:02,240 --> 00:23:04,960
has been the leader in getting people to

603
00:23:04,960 --> 00:23:07,200
focus on this affirmative vision not

604
00:23:07,200 --> 00:23:08,720
just you know

605
00:23:08,720 --> 00:23:11,200
ringing our hands and admiring the

606
00:23:11,200 --> 00:23:13,320
problem of digital

607
00:23:13,320 --> 00:23:16,559
authoritarianism or malign uses of text

608
00:23:16,559 --> 00:23:18,559
tell me a little bit tell us all a

609
00:23:18,559 --> 00:23:20,320
little bit about how is denmark

610
00:23:20,320 --> 00:23:22,000
approaching the positive side of the

611
00:23:22,000 --> 00:23:24,320
equation

612
00:23:24,320 --> 00:23:26,320
i think actually going to your layer two

613
00:23:26,320 --> 00:23:28,320
and layer three in the model so first on

614
00:23:28,320 --> 00:23:30,520
the tech governance

615
00:23:30,520 --> 00:23:32,400
recognizing and it goes a little bit to

616
00:23:32,400 --> 00:23:33,919
the what we discussed earlier about sort

617
00:23:33,919 --> 00:23:36,400
of a naive way of how text and regulate

618
00:23:36,400 --> 00:23:38,240
them say we we can't do that anymore

619
00:23:38,240 --> 00:23:40,480
because that sort of assertion and

620
00:23:40,480 --> 00:23:42,640
diffusion of norms and ideas from from

621
00:23:42,640 --> 00:23:45,200
our adversaries we need to do the same

622
00:23:45,200 --> 00:23:46,799
so it's much more thinking about what is

623
00:23:46,799 --> 00:23:49,520
a positive vision for where a world in

624
00:23:49,520 --> 00:23:52,000
which technology is increasing

625
00:23:52,000 --> 00:23:54,159
accountability increasing transparency

626
00:23:54,159 --> 00:23:56,000
giving more people voice

627
00:23:56,000 --> 00:23:58,960
it is supporting both on the innovation

628
00:23:58,960 --> 00:24:00,880
economic opportunity because that is big

629
00:24:00,880 --> 00:24:03,760
part of this it is a lever for national

630
00:24:03,760 --> 00:24:06,720
security for protection of your citizens

631
00:24:06,720 --> 00:24:08,880
as well as an opportunity for promoting

632
00:24:08,880 --> 00:24:10,559
human rights and i think it's important

633
00:24:10,559 --> 00:24:13,200
to say there is not a trade-off between

634
00:24:13,200 --> 00:24:15,679
national security economic opportunity

635
00:24:15,679 --> 00:24:16,880
and development

636
00:24:16,880 --> 00:24:18,720
and the promotion and securement of

637
00:24:18,720 --> 00:24:20,720
human rights fundamental values of

638
00:24:20,720 --> 00:24:22,960
liberties on the contrary we believe in

639
00:24:22,960 --> 00:24:25,600
it you know as as as strong democrats in

640
00:24:25,600 --> 00:24:27,279
this space that

641
00:24:27,279 --> 00:24:29,360
democracy ultimately

642
00:24:29,360 --> 00:24:31,440
sets us free as individuals but it also

643
00:24:31,440 --> 00:24:33,039
brings more opportunities and that's the

644
00:24:33,039 --> 00:24:34,880
positivism with the tech for democracy

645
00:24:34,880 --> 00:24:36,640
that we really put forward

646
00:24:36,640 --> 00:24:39,520
the copenhagen tech for democracy pledge

647
00:24:39,520 --> 00:24:42,159
is a really for me it serves as a

648
00:24:42,159 --> 00:24:45,039
reminder for those of us in governments

649
00:24:45,039 --> 00:24:47,039
for those people in civil society for

650
00:24:47,039 --> 00:24:50,720
the tech companies about recommitting to

651
00:24:50,720 --> 00:24:52,000
the reason why we're engaged in this

652
00:24:52,000 --> 00:24:53,279
topic is because we have some

653
00:24:53,279 --> 00:24:54,799
fundamental beliefs about a world that

654
00:24:54,799 --> 00:24:56,640
could be better and it's going back to

655
00:24:56,640 --> 00:24:58,960
some of the originals ideas of the

656
00:24:58,960 --> 00:25:01,120
people who invented the internet

657
00:25:01,120 --> 00:25:03,600
you know then the early inventors of ai

658
00:25:03,600 --> 00:25:05,600
the sci-fi writers who envisioned a

659
00:25:05,600 --> 00:25:08,000
world that was much kinder much more

660
00:25:08,000 --> 00:25:10,000
meaningful much more transparent much

661
00:25:10,000 --> 00:25:11,679
more accountable

662
00:25:11,679 --> 00:25:13,600
simply a better place to be in and i

663
00:25:13,600 --> 00:25:15,279
think that is some of the fundamental

664
00:25:15,279 --> 00:25:17,039
values to your point it's its values

665
00:25:17,039 --> 00:25:18,960
diplomacy and that is a question of

666
00:25:18,960 --> 00:25:20,640
interest

667
00:25:20,640 --> 00:25:22,640
two on the big tent approach we believe

668
00:25:22,640 --> 00:25:24,559
in the multi-stakeholder approach yes

669
00:25:24,559 --> 00:25:26,159
there is a lot that has to be done

670
00:25:26,159 --> 00:25:27,760
through legislation whether you know

671
00:25:27,760 --> 00:25:29,520
when it comes to facial recognition or

672
00:25:29,520 --> 00:25:30,799
spyware

673
00:25:30,799 --> 00:25:32,480
we probably need to do have some proper

674
00:25:32,480 --> 00:25:34,000
legislation around it

675
00:25:34,000 --> 00:25:36,080
but tackling this information that is

676
00:25:36,080 --> 00:25:38,000
also about equipping our citizens it's

677
00:25:38,000 --> 00:25:40,159
about digital literacy um it's about

678
00:25:40,159 --> 00:25:42,240
working with the tech sector when it

679
00:25:42,240 --> 00:25:44,400
comes to um i think you said it very

680
00:25:44,400 --> 00:25:46,799
well sort of export controls and secure

681
00:25:46,799 --> 00:25:49,440
value chains that's that's diplomacy we

682
00:25:49,440 --> 00:25:50,880
need to do that with other governments

683
00:25:50,880 --> 00:25:52,480
we need to do with the tech sector we

684
00:25:52,480 --> 00:25:54,159
need to work with academics with civil

685
00:25:54,159 --> 00:25:55,760
society

686
00:25:55,760 --> 00:25:57,760
so that's a tech for democracy not i

687
00:25:57,760 --> 00:25:59,440
think in a nutshell the second piece of

688
00:25:59,440 --> 00:26:01,120
attack for democracy is these action

689
00:26:01,120 --> 00:26:02,640
coalitions

690
00:26:02,640 --> 00:26:06,000
um i love writescon because we meet here

691
00:26:06,000 --> 00:26:07,679
you know whether we're you know from all

692
00:26:07,679 --> 00:26:08,960
sorts of different backgrounds from all

693
00:26:08,960 --> 00:26:11,279
over the world

694
00:26:11,279 --> 00:26:12,960
what do we do between the meetings what

695
00:26:12,960 --> 00:26:14,400
do we do between the the tech for

696
00:26:14,400 --> 00:26:16,240
democracy meeting the rights con how do

697
00:26:16,240 --> 00:26:18,240
we get into the dirty work together and

698
00:26:18,240 --> 00:26:19,520
that's why the multi-stakeholder

699
00:26:19,520 --> 00:26:21,440
coalition so both engaging much more

700
00:26:21,440 --> 00:26:23,279
with the freedom online coalition where

701
00:26:23,279 --> 00:26:24,960
so many of the people that are part of

702
00:26:24,960 --> 00:26:27,279
rightscon is also taking part

703
00:26:27,279 --> 00:26:28,559
um

704
00:26:28,559 --> 00:26:30,720
how do we engage on the un agenda on

705
00:26:30,720 --> 00:26:32,400
these issues so that's the other piece

706
00:26:32,400 --> 00:26:34,240
and then the action coalition so we have

707
00:26:34,240 --> 00:26:36,240
the action coalition on trustworthy

708
00:26:36,240 --> 00:26:38,400
information online that's working with

709
00:26:38,400 --> 00:26:41,440
um tech companies and civil society and

710
00:26:41,440 --> 00:26:42,559
and a number of governments and

711
00:26:42,559 --> 00:26:44,240
advancing more trustworthy information

712
00:26:44,240 --> 00:26:45,679
particularly in jurisdictions and

713
00:26:45,679 --> 00:26:48,480
countries that do not speak english

714
00:26:48,480 --> 00:26:50,480
we're looking at the uh the sort of

715
00:26:50,480 --> 00:26:52,640
gender-based harassment online

716
00:26:52,640 --> 00:26:54,880
we are practically keeping about half of

717
00:26:54,880 --> 00:26:56,559
the world's population out of meaningful

718
00:26:56,559 --> 00:26:58,080
engagement online

719
00:26:58,080 --> 00:26:59,360
so just to say these are some of the

720
00:26:59,360 --> 00:27:00,880
ways in which we're seeing an

721
00:27:00,880 --> 00:27:03,600
opportunity for engaging specifically in

722
00:27:03,600 --> 00:27:05,120
these issues and it's taking both the

723
00:27:05,120 --> 00:27:07,039
sort of long-term mission getting

724
00:27:07,039 --> 00:27:08,960
governments companies civil society

725
00:27:08,960 --> 00:27:10,799
international organizations on board the

726
00:27:10,799 --> 00:27:12,400
fundamental principles of what we're

727
00:27:12,400 --> 00:27:15,039
fighting for the shaping of an inclusive

728
00:27:15,039 --> 00:27:17,919
narrative and then it's doing the work

729
00:27:17,919 --> 00:27:19,520
and making sure that those words are

730
00:27:19,520 --> 00:27:22,640
actually translated into action

731
00:27:22,640 --> 00:27:23,520
um

732
00:27:23,520 --> 00:27:25,520
and with i know we only have a couple of

733
00:27:25,520 --> 00:27:28,480
minutes left and um i mean

734
00:27:28,480 --> 00:27:29,679
you and i could continue having this

735
00:27:29,679 --> 00:27:31,679
conversation because it is a rich and

736
00:27:31,679 --> 00:27:33,200
important one

737
00:27:33,200 --> 00:27:34,960
um i love what you said about the

738
00:27:34,960 --> 00:27:37,120
technology you know how do we you know

739
00:27:37,120 --> 00:27:39,200
it's about winning the race for for

740
00:27:39,200 --> 00:27:41,120
quantum secure encryption

741
00:27:41,120 --> 00:27:42,640
it's also about the opportunity for

742
00:27:42,640 --> 00:27:44,799
using ai for what we really hope for but

743
00:27:44,799 --> 00:27:46,399
maybe we can table that for another

744
00:27:46,399 --> 00:27:47,520
discussion

745
00:27:47,520 --> 00:27:48,880
so maybe i lean

746
00:27:48,880 --> 00:27:51,679
if we end up no we promise to tell

747
00:27:51,679 --> 00:27:53,520
people where whether we're optimists

748
00:27:53,520 --> 00:27:55,440
passing mr realist on behalf of this so

749
00:27:55,440 --> 00:27:58,000
maybe you want to close the va close us

750
00:27:58,000 --> 00:27:59,360
off let's just think a little bit about

751
00:27:59,360 --> 00:28:01,600
what what's what's in store

752
00:28:01,600 --> 00:28:04,559
and should we leave this conversation in

753
00:28:04,559 --> 00:28:06,000
this right corner as optimists or

754
00:28:06,000 --> 00:28:07,520
pessimists

755
00:28:07,520 --> 00:28:08,240
well

756
00:28:08,240 --> 00:28:11,360
one of the biggest opportunities that i

757
00:28:11,360 --> 00:28:14,240
see out there is this theme of digital

758
00:28:14,240 --> 00:28:16,640
inclusion which uh

759
00:28:16,640 --> 00:28:19,200
the chair current chair of the foc

760
00:28:19,200 --> 00:28:22,640
canada is making a ginormous push on

761
00:28:22,640 --> 00:28:24,080
this even today

762
00:28:24,080 --> 00:28:27,200
um this entire campaign to get people to

763
00:28:27,200 --> 00:28:28,720
wake up to the value of digital

764
00:28:28,720 --> 00:28:31,600
inclusion and the multiple dimensions of

765
00:28:31,600 --> 00:28:33,600
digital inclusion

766
00:28:33,600 --> 00:28:36,080
as you said it's like you know it's 40

767
00:28:36,080 --> 00:28:37,760
of the world is not yet digitally

768
00:28:37,760 --> 00:28:39,520
connected and all it's going to do is

769
00:28:39,520 --> 00:28:42,000
exacerbate inequality

770
00:28:42,000 --> 00:28:45,120
i believe export of information

771
00:28:45,120 --> 00:28:48,159
infrastructure is a way we

772
00:28:48,159 --> 00:28:51,360
contribute to connecting the world and

773
00:28:51,360 --> 00:28:53,760
spread our values which are embedded in

774
00:28:53,760 --> 00:28:56,799
our technology and that ironically is

775
00:28:56,799 --> 00:28:59,600
the best way for us to fight the spread

776
00:28:59,600 --> 00:29:02,080
of digital authoritarianism so i'm very

777
00:29:02,080 --> 00:29:05,919
optimistic about the potential there

778
00:29:05,919 --> 00:29:08,480
well and i you know i was born an

779
00:29:08,480 --> 00:29:11,120
optimist and despite all this sort of

780
00:29:11,120 --> 00:29:12,960
challenging things with you i think this

781
00:29:12,960 --> 00:29:15,120
right con is a great testimony to why we

782
00:29:15,120 --> 00:29:17,200
should remain optimist

783
00:29:17,200 --> 00:29:18,960
thank you so much for the conversation

784
00:29:18,960 --> 00:29:21,440
eileen um to all of those of you who

785
00:29:21,440 --> 00:29:23,120
joined in thank you so much for joining

786
00:29:23,120 --> 00:29:25,200
us you can read more about the tech for

787
00:29:25,200 --> 00:29:28,640
democracy and tech for democracy dot com

788
00:29:28,640 --> 00:29:30,480
we look forward to engaging with you

789
00:29:30,480 --> 00:29:32,880
thank you

790
00:29:33,039 --> 00:29:35,120
an excellent conversation with the

791
00:29:35,120 --> 00:29:37,360
ambassador and eileen at stanford

792
00:29:37,360 --> 00:29:40,399
university big topics and echoing uh

793
00:29:40,399 --> 00:29:43,120
some of the comments from maria raisa

794
00:29:43,120 --> 00:29:44,799
the journalist and nobel peace prize

795
00:29:44,799 --> 00:29:47,520
winner from earlier a few hours ago

796
00:29:47,520 --> 00:29:49,360
talking about the need for democracies

797
00:29:49,360 --> 00:29:52,399
to really think about technology's

798
00:29:52,399 --> 00:29:54,559
position in the 21st century and the

799
00:29:54,559 --> 00:29:57,600
relationship uh to democracies in a way

800
00:29:57,600 --> 00:30:00,159
that frankly some authoritarian states

801
00:30:00,159 --> 00:30:01,039
are

802
00:30:01,039 --> 00:30:02,720
well ahead on

803
00:30:02,720 --> 00:30:04,799
thank you so much for joining and speak

804
00:30:04,799 --> 00:30:08,760
to you later stay engaged

