1
00:01:01,150 --> 00:01:06,720
[Music]

2
00:01:12,320 --> 00:01:14,960
good evening

3
00:01:16,799 --> 00:01:20,640
i wanted to uh great just wanted to get

4
00:01:19,920 --> 00:01:24,159
us started

5
00:01:20,640 --> 00:01:26,080
um uh and welcome everyone tonight to

6
00:01:24,159 --> 00:01:29,439
our fireside chat with commissioner

7
00:01:26,080 --> 00:01:31,920
rohit chopra uh i know it's probably

8
00:01:29,439 --> 00:01:35,360
inappropriate to say this is my favorite

9
00:01:31,920 --> 00:01:38,320
ftc commissioner but here we are

10
00:01:35,360 --> 00:01:39,520
commissioner chopra i'm so so thrilled

11
00:01:38,320 --> 00:01:42,960
to be here with you

12
00:01:39,520 --> 00:01:44,158
um for those of you who are joining

13
00:01:42,960 --> 00:01:46,240
tonight we're gonna be

14
00:01:44,159 --> 00:01:48,720
in this great conversation for the next

15
00:01:46,240 --> 00:01:51,039
hour to talk about

16
00:01:48,720 --> 00:01:53,280
uh a variety of things i mean how could

17
00:01:51,040 --> 00:01:53,840
we have ever planned that we would get

18
00:01:53,280 --> 00:01:55,920
to

19
00:01:53,840 --> 00:01:57,360
interview you or have a conversation

20
00:01:55,920 --> 00:02:00,640
during the week of the

21
00:01:57,360 --> 00:02:02,960
anti-trust hearings that

22
00:02:00,640 --> 00:02:05,119
are facing the big tech industry and i

23
00:02:02,960 --> 00:02:05,919
know a number of people who are watching

24
00:02:05,119 --> 00:02:08,560
tonight

25
00:02:05,920 --> 00:02:10,560
are going to be excited to hear what you

26
00:02:08,560 --> 00:02:13,840
have to say about that so

27
00:02:10,560 --> 00:02:17,280
maybe before we jump in to um

28
00:02:13,840 --> 00:02:19,840
your insights from yesterday's hearings

29
00:02:17,280 --> 00:02:20,560
it seems like you know there's been

30
00:02:19,840 --> 00:02:23,760
quite a bit

31
00:02:20,560 --> 00:02:26,879
of attention over the last few

32
00:02:23,760 --> 00:02:29,519
years about the potential

33
00:02:26,879 --> 00:02:30,720
harms of big tech i mean we've the

34
00:02:29,520 --> 00:02:33,120
financial times

35
00:02:30,720 --> 00:02:34,640
declared tech lash i think is their

36
00:02:33,120 --> 00:02:37,760
official word of the year

37
00:02:34,640 --> 00:02:40,879
in 2019 um

38
00:02:37,760 --> 00:02:43,280
we were starting to see the

39
00:02:40,879 --> 00:02:43,920
consumer scrutiny and voter scrutiny

40
00:02:43,280 --> 00:02:45,440
around

41
00:02:43,920 --> 00:02:47,440
a variety of kind of harmful

42
00:02:45,440 --> 00:02:50,400
technologies when the story

43
00:02:47,440 --> 00:02:52,319
of cambridge analytica broke and people

44
00:02:50,400 --> 00:02:54,560
started to understand that

45
00:02:52,319 --> 00:02:55,920
the social media platforms for example

46
00:02:54,560 --> 00:02:59,840
that they're accustomed to

47
00:02:55,920 --> 00:03:01,920
working in um and with everyday

48
00:02:59,840 --> 00:03:03,280
uh might in fact be harvesting quite a

49
00:03:01,920 --> 00:03:07,679
bit of data about them

50
00:03:03,280 --> 00:03:11,599
and limiting the field

51
00:03:07,680 --> 00:03:12,239
of fair consideration of many points of

52
00:03:11,599 --> 00:03:15,200
view

53
00:03:12,239 --> 00:03:16,720
let's say so i wanted to see if we could

54
00:03:15,200 --> 00:03:19,359
open up this conversation

55
00:03:16,720 --> 00:03:20,319
with a little bit about your thoughts

56
00:03:19,360 --> 00:03:23,840
about

57
00:03:20,319 --> 00:03:25,839
um an older policy that we had in the

58
00:03:23,840 --> 00:03:27,360
united states the fairness doctrine

59
00:03:25,840 --> 00:03:29,840
which really was about

60
00:03:27,360 --> 00:03:30,959
kind of giving a broader landscape to

61
00:03:29,840 --> 00:03:33,440
opinions

62
00:03:30,959 --> 00:03:34,319
and points of view in our media

63
00:03:33,440 --> 00:03:37,040
landscape

64
00:03:34,319 --> 00:03:38,720
and maybe that would be a place to start

65
00:03:37,040 --> 00:03:40,560
the conversation to kind of set the

66
00:03:38,720 --> 00:03:42,000
stage or the landscape

67
00:03:40,560 --> 00:03:44,319
of our current communications

68
00:03:42,000 --> 00:03:47,120
environment

69
00:03:44,319 --> 00:03:47,839
yeah well and thanks professor noble for

70
00:03:47,120 --> 00:03:51,519
doing this

71
00:03:47,840 --> 00:03:52,000
we are definitely in a really unique

72
00:03:51,519 --> 00:03:55,680
time

73
00:03:52,000 --> 00:03:59,360
in history where there is

74
00:03:55,680 --> 00:04:03,040
a very small set of firms who have

75
00:03:59,360 --> 00:04:05,519
so much influence over our daily lives

76
00:04:03,040 --> 00:04:07,040
and in ways that more of us are

77
00:04:05,519 --> 00:04:10,080
questioning

78
00:04:07,040 --> 00:04:12,720
and fighting back to determine

79
00:04:10,080 --> 00:04:15,680
are all of these the wonderful fruits of

80
00:04:12,720 --> 00:04:18,959
technology can they also be weaponized

81
00:04:15,680 --> 00:04:23,040
to divide us to disrupt our democracy

82
00:04:18,959 --> 00:04:24,880
to impede our economy and i think

83
00:04:23,040 --> 00:04:26,880
you have raised something that is so

84
00:04:24,880 --> 00:04:29,520
important which is

85
00:04:26,880 --> 00:04:32,479
information and communication is such a

86
00:04:29,520 --> 00:04:36,159
core part of an open society

87
00:04:32,479 --> 00:04:40,560
and it's the lifeblood of

88
00:04:36,160 --> 00:04:43,040
democracy uh books and newspapers

89
00:04:40,560 --> 00:04:44,160
and music it is really what gives us the

90
00:04:43,040 --> 00:04:46,800
richness

91
00:04:44,160 --> 00:04:47,280
of life it allows us to fight with our

92
00:04:46,800 --> 00:04:50,960
word

93
00:04:47,280 --> 00:04:54,159
so that the best ideas can emerge and

94
00:04:50,960 --> 00:04:57,120
many of us are concerned that the

95
00:04:54,160 --> 00:04:58,000
openness of the internet and the

96
00:04:57,120 --> 00:05:00,720
openness

97
00:04:58,000 --> 00:05:02,240
of communications are increasingly being

98
00:05:00,720 --> 00:05:05,600
distorted

99
00:05:02,240 --> 00:05:08,639
by the business models of a handful of

100
00:05:05,600 --> 00:05:11,280
companies and so you talk about

101
00:05:08,639 --> 00:05:13,440
fairness doctrine which is really part

102
00:05:11,280 --> 00:05:14,960
of the thinking around communications

103
00:05:13,440 --> 00:05:17,320
regulation

104
00:05:14,960 --> 00:05:18,638
i really think quite a bit about

105
00:05:17,320 --> 00:05:21,759
unfairness

106
00:05:18,639 --> 00:05:25,120
doctrine which is the core

107
00:05:21,759 --> 00:05:28,479
of the ftc act it is the core of

108
00:05:25,120 --> 00:05:28,800
state law and lots of other federal laws

109
00:05:28,479 --> 00:05:31,599
that

110
00:05:28,800 --> 00:05:33,840
regulate commerce you know it's been a

111
00:05:31,600 --> 00:05:36,880
hundred years since we started

112
00:05:33,840 --> 00:05:40,960
this concept in the u.s of

113
00:05:36,880 --> 00:05:42,400
preventing and halting unfair methods of

114
00:05:40,960 --> 00:05:45,919
competition

115
00:05:42,400 --> 00:05:48,799
and unfair and deceptive practices

116
00:05:45,919 --> 00:05:51,599
because to protect a market that

117
00:05:48,800 --> 00:05:54,960
delivers a wide diversity of views a

118
00:05:51,600 --> 00:05:58,800
wide diversity of products and services

119
00:05:54,960 --> 00:06:02,318
we need to root out those practices

120
00:05:58,800 --> 00:06:05,440
and i am concerned that

121
00:06:02,319 --> 00:06:09,039
there is a myth that a hands-off

122
00:06:05,440 --> 00:06:12,880
approach is what made the us

123
00:06:09,039 --> 00:06:14,880
and other western societies um

124
00:06:12,880 --> 00:06:17,039
innovative but in fact there's a darker

125
00:06:14,880 --> 00:06:20,960
story and a much different story

126
00:06:17,039 --> 00:06:24,479
that i think we also have to confront

127
00:06:20,960 --> 00:06:27,520
yeah it was actually the antitrust um

128
00:06:24,479 --> 00:06:30,639
legislation that created the conditions

129
00:06:27,520 --> 00:06:31,758
in many ways for the current internet

130
00:06:30,639 --> 00:06:34,240
landscape

131
00:06:31,759 --> 00:06:35,520
that we're all experiencing and i think

132
00:06:34,240 --> 00:06:37,360
a lot of people

133
00:06:35,520 --> 00:06:39,280
don't necessarily understand that

134
00:06:37,360 --> 00:06:41,680
history that um

135
00:06:39,280 --> 00:06:42,318
the very industry that benefited the

136
00:06:41,680 --> 00:06:46,720
most

137
00:06:42,319 --> 00:06:48,319
from two key things in my mind which are

138
00:06:46,720 --> 00:06:50,560
first of all not only kind of the

139
00:06:48,319 --> 00:06:54,160
antitrust environment

140
00:06:50,560 --> 00:06:57,680
um breaking up for example att

141
00:06:54,160 --> 00:07:00,840
and making uh possible

142
00:06:57,680 --> 00:07:02,080
um startups to be able to enter the

143
00:07:00,840 --> 00:07:04,799
marketplace

144
00:07:02,080 --> 00:07:05,840
but there's also the offloading of the

145
00:07:04,800 --> 00:07:09,199
risk

146
00:07:05,840 --> 00:07:12,638
for small companies that

147
00:07:09,199 --> 00:07:14,560
have taken that let's say public funding

148
00:07:12,639 --> 00:07:15,759
public resources from a variety of

149
00:07:14,560 --> 00:07:18,319
different federal

150
00:07:15,759 --> 00:07:19,680
agencies the national science foundation

151
00:07:18,319 --> 00:07:23,759
for example being one

152
00:07:19,680 --> 00:07:26,639
that has deeply funded the tech industry

153
00:07:23,759 --> 00:07:27,520
many people i think think that all the

154
00:07:26,639 --> 00:07:30,560
risk

155
00:07:27,520 --> 00:07:33,359
for the tech sector lives

156
00:07:30,560 --> 00:07:34,560
on sandhill road right or among venture

157
00:07:33,360 --> 00:07:37,759
capitalists

158
00:07:34,560 --> 00:07:39,599
but in fact what we know is that much of

159
00:07:37,759 --> 00:07:40,479
the risk has been offloaded on to the

160
00:07:39,599 --> 00:07:44,240
public

161
00:07:40,479 --> 00:07:45,520
but the dividends or the return on those

162
00:07:44,240 --> 00:07:48,000
investments

163
00:07:45,520 --> 00:07:50,159
doesn't go back into the public coffers

164
00:07:48,000 --> 00:07:50,639
so i'm interested in kind of your take

165
00:07:50,160 --> 00:07:53,360
on

166
00:07:50,639 --> 00:07:54,400
um you know disrupting or rethinking

167
00:07:53,360 --> 00:07:57,440
this narrative

168
00:07:54,400 --> 00:08:00,479
about um antitrust

169
00:07:57,440 --> 00:08:02,319
and of course um you know that that's

170
00:08:00,479 --> 00:08:02,800
the conversation of the day that led us

171
00:08:02,319 --> 00:08:06,479
up to

172
00:08:02,800 --> 00:08:09,360
yesterday's uh hearings

173
00:08:06,479 --> 00:08:11,360
yeah and it's something that i think we

174
00:08:09,360 --> 00:08:13,840
all have to think about

175
00:08:11,360 --> 00:08:17,759
uh increasingly in a different way

176
00:08:13,840 --> 00:08:22,080
particularly as we reflect upon

177
00:08:17,759 --> 00:08:24,560
our own history uh in the us

178
00:08:22,080 --> 00:08:27,359
in europe and in around the world and

179
00:08:24,560 --> 00:08:30,319
really what set apart

180
00:08:27,360 --> 00:08:31,919
some of the benefits of democracy and

181
00:08:30,319 --> 00:08:34,959
open markets

182
00:08:31,919 --> 00:08:36,399
particularly with the rise of the

183
00:08:34,958 --> 00:08:38,478
chinese tech sector

184
00:08:36,399 --> 00:08:40,000
these questions are so important for the

185
00:08:38,479 --> 00:08:43,039
day and maybe i'll

186
00:08:40,000 --> 00:08:47,600
respond to you by um you

187
00:08:43,039 --> 00:08:51,279
raised this point of are the dividends

188
00:08:47,600 --> 00:08:54,320
of technological progress um

189
00:08:51,279 --> 00:08:56,640
are they accruing to the public given

190
00:08:54,320 --> 00:08:57,440
the risks and investment the public has

191
00:08:56,640 --> 00:08:59,439
made

192
00:08:57,440 --> 00:09:01,120
and one of the things that i'm quite

193
00:08:59,440 --> 00:09:05,200
troubled by

194
00:09:01,120 --> 00:09:08,240
uh is i think a distorted history

195
00:09:05,200 --> 00:09:10,720
that the those who created these

196
00:09:08,240 --> 00:09:13,279
trillion dollar companies or companies

197
00:09:10,720 --> 00:09:15,240
worth hundreds of billions of dollars

198
00:09:13,279 --> 00:09:16,880
that they sort of just did it by

199
00:09:15,240 --> 00:09:19,680
themselves

200
00:09:16,880 --> 00:09:20,320
um and that's just fundamentally wrong i

201
00:09:19,680 --> 00:09:23,359
i

202
00:09:20,320 --> 00:09:26,800
would there be a facebook

203
00:09:23,360 --> 00:09:30,480
or a google or an amazon had there not

204
00:09:26,800 --> 00:09:33,920
been the microsoft antitrust actions

205
00:09:30,480 --> 00:09:37,120
would there be an iphone

206
00:09:33,920 --> 00:09:40,399
had the us taxpayers

207
00:09:37,120 --> 00:09:44,320
not invested in some of the core

208
00:09:40,399 --> 00:09:48,120
research and science that powers

209
00:09:44,320 --> 00:09:49,680
so much of our information industry and

210
00:09:48,120 --> 00:09:52,000
telecommunications

211
00:09:49,680 --> 00:09:53,519
there's a wonderful scholar mariana

212
00:09:52,000 --> 00:09:56,000
matsukato

213
00:09:53,519 --> 00:09:58,080
who writes about how so much of

214
00:09:56,000 --> 00:10:01,839
technology today

215
00:09:58,080 --> 00:10:02,240
actually derives from the risk taking

216
00:10:01,839 --> 00:10:06,320
that

217
00:10:02,240 --> 00:10:10,560
we all took um as a society

218
00:10:06,320 --> 00:10:11,120
and as taxpayers but much of the fruits

219
00:10:10,560 --> 00:10:13,680
of that

220
00:10:11,120 --> 00:10:15,200
don't necessarily that they're

221
00:10:13,680 --> 00:10:18,319
privatized

222
00:10:15,200 --> 00:10:18,720
and so we need to start asking ourselves

223
00:10:18,320 --> 00:10:21,680
again

224
00:10:18,720 --> 00:10:24,000
what was it that especially made the

225
00:10:21,680 --> 00:10:26,959
united states such a leader

226
00:10:24,000 --> 00:10:28,959
in communications and technology and i

227
00:10:26,959 --> 00:10:29,920
think there were several ingredients to

228
00:10:28,959 --> 00:10:33,439
that

229
00:10:29,920 --> 00:10:36,800
one was as you mentioned real

230
00:10:33,440 --> 00:10:39,320
investments in people in science

231
00:10:36,800 --> 00:10:41,199
technology engineering and

232
00:10:39,320 --> 00:10:44,720
communications

233
00:10:41,200 --> 00:10:48,240
we made sure that no companies

234
00:10:44,720 --> 00:10:52,079
could own the pipes of

235
00:10:48,240 --> 00:10:55,440
our information flows we made sure

236
00:10:52,079 --> 00:10:57,359
that when it came to the ats of the

237
00:10:55,440 --> 00:10:58,959
world we we broke them up and then

238
00:10:57,360 --> 00:11:01,440
sometimes required

239
00:10:58,959 --> 00:11:02,479
opening up their patent vaults so our

240
00:11:01,440 --> 00:11:06,000
entire

241
00:11:02,480 --> 00:11:06,880
economy could benefit from it we made

242
00:11:06,000 --> 00:11:11,040
sure that we

243
00:11:06,880 --> 00:11:13,279
eliminated certain conflicts of interest

244
00:11:11,040 --> 00:11:14,560
such as in the communications act where

245
00:11:13,279 --> 00:11:17,600
we don't allow

246
00:11:14,560 --> 00:11:18,319
certain commingling of activities that

247
00:11:17,600 --> 00:11:21,519
allow

248
00:11:18,320 --> 00:11:23,600
dominant players to bully out

249
00:11:21,519 --> 00:11:25,279
smaller players that want to get in the

250
00:11:23,600 --> 00:11:28,720
game or

251
00:11:25,279 --> 00:11:29,680
take advantage of their unique ability

252
00:11:28,720 --> 00:11:32,000
to exploit

253
00:11:29,680 --> 00:11:35,040
data and i think some of those

254
00:11:32,000 --> 00:11:38,560
principles we have been forgetting about

255
00:11:35,040 --> 00:11:41,199
you're right sandhill road

256
00:11:38,560 --> 00:11:43,920
plays a role but it's not just sandhill

257
00:11:41,200 --> 00:11:46,959
road that is fueling these innovations

258
00:11:43,920 --> 00:11:49,519
wall street certainly does intermediate

259
00:11:46,959 --> 00:11:51,920
capital but wall street also distorts

260
00:11:49,519 --> 00:11:52,959
the incentives that lead to big payoffs

261
00:11:51,920 --> 00:11:55,120
for all of us

262
00:11:52,959 --> 00:11:56,800
and i think the net effect of that can

263
00:11:55,120 --> 00:12:00,079
sometimes be

264
00:11:56,800 --> 00:12:01,680
a distortionary to the entire economy

265
00:12:00,079 --> 00:12:05,279
and society

266
00:12:01,680 --> 00:12:08,800
one example of this is we now live

267
00:12:05,279 --> 00:12:11,480
in a world that is very much curated

268
00:12:08,800 --> 00:12:13,120
by the incentives of behavioral

269
00:12:11,480 --> 00:12:15,360
advertising

270
00:12:13,120 --> 00:12:16,160
so behavioral advertising is a

271
00:12:15,360 --> 00:12:19,920
fundamental

272
00:12:16,160 --> 00:12:20,959
shift in how media and communications um

273
00:12:19,920 --> 00:12:25,360
work

274
00:12:20,959 --> 00:12:26,959
instead of advertising to an audience or

275
00:12:25,360 --> 00:12:29,760
a demographic

276
00:12:26,959 --> 00:12:32,000
advertising targets not just a group of

277
00:12:29,760 --> 00:12:35,040
people but an individual

278
00:12:32,000 --> 00:12:38,320
a professor noble or myself that can

279
00:12:35,040 --> 00:12:40,959
track us across the internet and that's

280
00:12:38,320 --> 00:12:41,440
the business model that i think can lead

281
00:12:40,959 --> 00:12:44,560
to

282
00:12:41,440 --> 00:12:47,279
many of the harms um that

283
00:12:44,560 --> 00:12:48,638
we're talking about today in in the

284
00:12:47,279 --> 00:12:50,800
public debates

285
00:12:48,639 --> 00:12:52,800
what is the role of that business model

286
00:12:50,800 --> 00:12:55,519
in elevating

287
00:12:52,800 --> 00:12:57,920
extreme and hateful content what's the

288
00:12:55,519 --> 00:13:01,440
role of that business model

289
00:12:57,920 --> 00:13:03,599
when it comes to foreign interference in

290
00:13:01,440 --> 00:13:06,000
our democratic process so

291
00:13:03,600 --> 00:13:06,800
i think we've got to really think hard

292
00:13:06,000 --> 00:13:10,240
about

293
00:13:06,800 --> 00:13:14,240
if we want to create you know 20 or

294
00:13:10,240 --> 00:13:16,399
30 big tech companies

295
00:13:14,240 --> 00:13:18,320
maybe we need to make sure that we're

296
00:13:16,399 --> 00:13:21,440
thinking about what was it that gave

297
00:13:18,320 --> 00:13:23,040
us success and what was the way in which

298
00:13:21,440 --> 00:13:26,160
we cultivate it

299
00:13:23,040 --> 00:13:28,800
and i don't think uh we want to

300
00:13:26,160 --> 00:13:30,319
follow what we hear from some in silicon

301
00:13:28,800 --> 00:13:33,519
valley which is that

302
00:13:30,320 --> 00:13:34,160
well in order to stay competitive with

303
00:13:33,519 --> 00:13:36,880
chinese

304
00:13:34,160 --> 00:13:37,439
tech we got to make sure ours are you

305
00:13:36,880 --> 00:13:38,959
know

306
00:13:37,440 --> 00:13:42,079
they don't maybe they don't even have to

307
00:13:38,959 --> 00:13:43,760
follow the law maybe we can just support

308
00:13:42,079 --> 00:13:45,920
them for our own

309
00:13:43,760 --> 00:13:47,279
goals and i just think that's wrong what

310
00:13:45,920 --> 00:13:50,160
really made

311
00:13:47,279 --> 00:13:50,560
the american technology ecosystem thrive

312
00:13:50,160 --> 00:13:53,839
was

313
00:13:50,560 --> 00:13:56,959
a diverse set of firms

314
00:13:53,839 --> 00:14:00,320
big medium small lots of

315
00:13:56,959 --> 00:14:04,160
openness and new market entry and i'm

316
00:14:00,320 --> 00:14:04,160
worried that we're really losing that

317
00:14:04,720 --> 00:14:08,079
it's really interesting there's a lot of

318
00:14:06,800 --> 00:14:10,079
things to unpack here

319
00:14:08,079 --> 00:14:12,000
in the things that you just mentioned so

320
00:14:10,079 --> 00:14:16,880
first i want to get to

321
00:14:12,000 --> 00:14:19,600
wall street and the financialization of

322
00:14:16,880 --> 00:14:20,800
the markets that's fueled by digital

323
00:14:19,600 --> 00:14:24,560
technologies

324
00:14:20,800 --> 00:14:28,319
you talked about kind of our um

325
00:14:24,560 --> 00:14:31,359
loss of some core values

326
00:14:28,320 --> 00:14:35,040
you know in my own work i think of

327
00:14:31,360 --> 00:14:37,440
the lack of realization still

328
00:14:35,040 --> 00:14:38,639
for many people in the united states and

329
00:14:37,440 --> 00:14:42,320
around the world

330
00:14:38,639 --> 00:14:45,839
of democracy of justice

331
00:14:42,320 --> 00:14:49,440
of social equality economic equality

332
00:14:45,839 --> 00:14:54,160
um i was thinking as you were speaking

333
00:14:49,440 --> 00:14:57,360
about the mortgage crisis of 2008

334
00:14:54,160 --> 00:14:58,399
and how in that crisis one of the things

335
00:14:57,360 --> 00:15:01,839
that people

336
00:14:58,399 --> 00:15:02,720
may not realize is that people were

337
00:15:01,839 --> 00:15:05,680
using

338
00:15:02,720 --> 00:15:06,639
these very sophisticated predictive

339
00:15:05,680 --> 00:15:10,160
analytics

340
00:15:06,639 --> 00:15:13,600
for example to bet against americans

341
00:15:10,160 --> 00:15:16,959
to figure out who would lose in order to

342
00:15:13,600 --> 00:15:18,240
capitalize and in that process

343
00:15:16,959 --> 00:15:20,160
it wasn't just the kind of the

344
00:15:18,240 --> 00:15:21,920
gamification of the markets that

345
00:15:20,160 --> 00:15:24,480
happened through algorithms

346
00:15:21,920 --> 00:15:24,959
it was also kind of the net effect of

347
00:15:24,480 --> 00:15:28,720
that

348
00:15:24,959 --> 00:15:30,638
was the largest wipeout wipeout of

349
00:15:28,720 --> 00:15:32,240
african-american wealth in the history

350
00:15:30,639 --> 00:15:34,880
of the united states

351
00:15:32,240 --> 00:15:36,320
so when you think about all of the gains

352
00:15:34,880 --> 00:15:39,519
post

353
00:15:36,320 --> 00:15:42,079
emancipation through reconstruction

354
00:15:39,519 --> 00:15:42,959
through the uh on the other side of the

355
00:15:42,079 --> 00:15:44,638
depression

356
00:15:42,959 --> 00:15:46,399
through the civil rights movement and

357
00:15:44,639 --> 00:15:50,079
then through the

358
00:15:46,399 --> 00:15:52,160
uh you know modicum of gains

359
00:15:50,079 --> 00:15:53,599
uh through the 1980s and 90s with

360
00:15:52,160 --> 00:15:58,240
affirmative action

361
00:15:53,600 --> 00:15:58,240
you essentially could um

362
00:15:58,320 --> 00:16:02,320
you know predict people right back into

363
00:16:01,040 --> 00:16:04,480
despair

364
00:16:02,320 --> 00:16:05,920
um and so i think these things and you

365
00:16:04,480 --> 00:16:08,480
know i thought about this i was reading

366
00:16:05,920 --> 00:16:11,040
your recent opinion on

367
00:16:08,480 --> 00:16:12,560
bronx honda and one of the things that i

368
00:16:11,040 --> 00:16:14,079
thought was interesting about your

369
00:16:12,560 --> 00:16:17,680
opinion where you

370
00:16:14,079 --> 00:16:21,439
wrote about predatory uh

371
00:16:17,680 --> 00:16:24,800
car loan buying and the ways in which

372
00:16:21,440 --> 00:16:26,079
in this particular case a car dealership

373
00:16:24,800 --> 00:16:28,560
had not only

374
00:16:26,079 --> 00:16:30,239
blatantly discriminated told its

375
00:16:28,560 --> 00:16:31,439
employees when black and latino

376
00:16:30,240 --> 00:16:34,000
consumers show up

377
00:16:31,440 --> 00:16:35,680
make sure you charge them more and give

378
00:16:34,000 --> 00:16:38,800
them a higher interest rate

379
00:16:35,680 --> 00:16:39,519
but that the software and the financial

380
00:16:38,800 --> 00:16:41,920
tools

381
00:16:39,519 --> 00:16:43,839
also that get used kind of with this

382
00:16:41,920 --> 00:16:46,959
machine learning

383
00:16:43,839 --> 00:16:49,600
processes that train systems on

384
00:16:46,959 --> 00:16:51,359
old discriminatory data those things

385
00:16:49,600 --> 00:16:54,639
actually become rather opaque

386
00:16:51,360 --> 00:16:57,120
and very difficult to intervene upon so

387
00:16:54,639 --> 00:16:57,759
is this a place where the federal trade

388
00:16:57,120 --> 00:17:01,120
commission

389
00:16:57,759 --> 00:17:04,400
is looking at harm and um

390
00:17:01,120 --> 00:17:06,640
if so kind of how how do you see uh

391
00:17:04,400 --> 00:17:08,720
regulating or protecting from

392
00:17:06,640 --> 00:17:09,280
discrimination that happens in these

393
00:17:08,720 --> 00:17:12,079
kind of

394
00:17:09,280 --> 00:17:14,559
machine learning and gamified financial

395
00:17:12,079 --> 00:17:18,399
systems

396
00:17:14,559 --> 00:17:21,520
yeah um there's so much i think to learn

397
00:17:18,400 --> 00:17:24,319
from this subprime mortgage crisis

398
00:17:21,520 --> 00:17:25,760
in the united states that really just

399
00:17:24,319 --> 00:17:28,159
accelerated

400
00:17:25,760 --> 00:17:30,799
over a decade ago into how we can think

401
00:17:28,160 --> 00:17:34,160
about some of the problems today

402
00:17:30,799 --> 00:17:34,960
one of the things that was very very

403
00:17:34,160 --> 00:17:38,400
clear

404
00:17:34,960 --> 00:17:39,679
in many of our financial crises but

405
00:17:38,400 --> 00:17:43,840
particularly

406
00:17:39,679 --> 00:17:46,880
the one in 2008 was

407
00:17:43,840 --> 00:17:52,080
the sheer devastation

408
00:17:46,880 --> 00:17:54,640
of wealth in so many neighborhoods

409
00:17:52,080 --> 00:17:55,199
across america that actually really

410
00:17:54,640 --> 00:17:58,440
haven't

411
00:17:55,200 --> 00:18:03,679
recovered and the huge

412
00:17:58,440 --> 00:18:06,240
magnification of economic inequality

413
00:18:03,679 --> 00:18:07,280
it's something that we need to reflect

414
00:18:06,240 --> 00:18:10,320
on because

415
00:18:07,280 --> 00:18:13,879
let's just remember how it all happened

416
00:18:10,320 --> 00:18:16,520
we had a system of

417
00:18:13,880 --> 00:18:18,400
securitization i mean as you say

418
00:18:16,520 --> 00:18:22,000
gamification where

419
00:18:18,400 --> 00:18:24,720
one investment bank is buying up

420
00:18:22,000 --> 00:18:26,400
lots of subprime mortgages issuing

421
00:18:24,720 --> 00:18:29,120
securities for them

422
00:18:26,400 --> 00:18:30,480
and on the other hand um engaged in

423
00:18:29,120 --> 00:18:33,840
derivatives trading

424
00:18:30,480 --> 00:18:36,960
betting against those borrowers and

425
00:18:33,840 --> 00:18:40,080
it's almost like heads i win

426
00:18:36,960 --> 00:18:41,760
tails you lose and that's just not a

427
00:18:40,080 --> 00:18:42,000
system that's going to work and we saw

428
00:18:41,760 --> 00:18:45,440
the

429
00:18:42,000 --> 00:18:49,600
consequences of that and so when we now

430
00:18:45,440 --> 00:18:51,760
reflect on this new type of

431
00:18:49,600 --> 00:18:55,039
and as you have written about very

432
00:18:51,760 --> 00:18:57,760
eloquently the algorithms of oppression

433
00:18:55,039 --> 00:18:58,799
i think we also need to be honest with

434
00:18:57,760 --> 00:19:01,919
ourselves

435
00:18:58,799 --> 00:19:02,320
you know i hear a lot of well it's just

436
00:19:01,919 --> 00:19:05,679
so

437
00:19:02,320 --> 00:19:10,080
good that this is all you know done

438
00:19:05,679 --> 00:19:13,919
by uh algorithms and robots

439
00:19:10,080 --> 00:19:18,080
because that just sort of takes out uh

440
00:19:13,919 --> 00:19:21,760
the the human bias and and racism

441
00:19:18,080 --> 00:19:23,120
and this is such a false construct which

442
00:19:21,760 --> 00:19:26,240
is that

443
00:19:23,120 --> 00:19:27,760
algorithms i mean these automated

444
00:19:26,240 --> 00:19:30,880
decision tools

445
00:19:27,760 --> 00:19:34,320
they are reflections of

446
00:19:30,880 --> 00:19:38,000
um real human life

447
00:19:34,320 --> 00:19:41,360
they can build the racism into them they

448
00:19:38,000 --> 00:19:45,280
can reinforce the biases that

449
00:19:41,360 --> 00:19:48,479
already exist while also essentially

450
00:19:45,280 --> 00:19:50,000
evading some of the accountability and i

451
00:19:48,480 --> 00:19:51,200
think that's something that should be a

452
00:19:50,000 --> 00:19:54,480
huge concern

453
00:19:51,200 --> 00:19:55,600
across our society and sectors of the

454
00:19:54,480 --> 00:19:57,679
economy

455
00:19:55,600 --> 00:19:58,719
and one of the things that i would like

456
00:19:57,679 --> 00:20:01,400
to be

457
00:19:58,720 --> 00:20:03,280
to see being done is to use the

458
00:20:01,400 --> 00:20:06,159
century-old

459
00:20:03,280 --> 00:20:06,879
unfairness doctrine and unfairness

460
00:20:06,159 --> 00:20:10,480
doctrine

461
00:20:06,880 --> 00:20:11,840
is the prohibition on unfair acts or

462
00:20:10,480 --> 00:20:14,159
practices

463
00:20:11,840 --> 00:20:16,000
and it essentially says the following if

464
00:20:14,159 --> 00:20:19,280
a practice

465
00:20:16,000 --> 00:20:20,320
you know causes injury to you know

466
00:20:19,280 --> 00:20:23,520
individuals

467
00:20:20,320 --> 00:20:25,360
that they can't reasonably avoid um and

468
00:20:23,520 --> 00:20:28,559
that there's no countervailing

469
00:20:25,360 --> 00:20:31,399
benefits to consumers or competition

470
00:20:28,559 --> 00:20:33,039
it should be prohibited and

471
00:20:31,400 --> 00:20:36,400
discrimination

472
00:20:33,039 --> 00:20:38,320
even by algorithm really

473
00:20:36,400 --> 00:20:40,559
should be condemned by some of this and

474
00:20:38,320 --> 00:20:43,918
i'm really concerned

475
00:20:40,559 --> 00:20:46,960
that it's not just discrimination

476
00:20:43,919 --> 00:20:50,559
in employment or housing

477
00:20:46,960 --> 00:20:54,000
or what have you it's really the broad

478
00:20:50,559 --> 00:20:56,080
way in which we are participating online

479
00:20:54,000 --> 00:20:58,799
when some of the business models

480
00:20:56,080 --> 00:21:01,199
completely foreclose

481
00:20:58,799 --> 00:21:02,480
some individuals from even seeing an

482
00:21:01,200 --> 00:21:05,280
opportunity

483
00:21:02,480 --> 00:21:07,679
i mean i i i always think about these

484
00:21:05,280 --> 00:21:12,960
algorithms that analyze

485
00:21:07,679 --> 00:21:16,159
uh resumes and you know if there is a

486
00:21:12,960 --> 00:21:16,880
black fraternity or sorority's name on

487
00:21:16,159 --> 00:21:19,120
it

488
00:21:16,880 --> 00:21:21,039
um you know it doesn't necessarily get

489
00:21:19,120 --> 00:21:21,600
prioritized that's just sort of the

490
00:21:21,039 --> 00:21:24,320
machine

491
00:21:21,600 --> 00:21:26,719
learning it's drawing correlations and

492
00:21:24,320 --> 00:21:30,399
in reinforcing biases

493
00:21:26,720 --> 00:21:32,880
if it says uh the the word women

494
00:21:30,400 --> 00:21:33,600
in it does that actually distort it even

495
00:21:32,880 --> 00:21:36,799
if it says

496
00:21:33,600 --> 00:21:39,199
women's you know robotics team

497
00:21:36,799 --> 00:21:40,720
captain you know these types of

498
00:21:39,200 --> 00:21:44,559
inferences

499
00:21:40,720 --> 00:21:46,000
that decide what is working and who is

500
00:21:44,559 --> 00:21:49,120
successful

501
00:21:46,000 --> 00:21:52,000
based on our existing experience that

502
00:21:49,120 --> 00:21:54,559
that actually just reinforces

503
00:21:52,000 --> 00:21:55,200
um a lot of the inequities that we have

504
00:21:54,559 --> 00:21:58,639
so

505
00:21:55,200 --> 00:22:01,120
i'm interested in using some of our core

506
00:21:58,640 --> 00:22:02,320
legal doctrines to advance how we

507
00:22:01,120 --> 00:22:05,520
condemn that

508
00:22:02,320 --> 00:22:06,158
and to think about ways that we can

509
00:22:05,520 --> 00:22:09,679
attack

510
00:22:06,159 --> 00:22:13,840
some of these harms before they even

511
00:22:09,679 --> 00:22:13,840
proliferate all over the world

512
00:22:14,240 --> 00:22:17,760
i really appreciate you bringing that

513
00:22:16,400 --> 00:22:21,200
forward because

514
00:22:17,760 --> 00:22:23,280
we know that this relationship between

515
00:22:21,200 --> 00:22:26,320
humans and technology

516
00:22:23,280 --> 00:22:29,520
has been narrated in a very narrow

517
00:22:26,320 --> 00:22:31,840
way uh you bring this out

518
00:22:29,520 --> 00:22:33,600
that it's uh that technology will

519
00:22:31,840 --> 00:22:36,720
somehow be more fair

520
00:22:33,600 --> 00:22:39,360
rather than less that we can manage

521
00:22:36,720 --> 00:22:41,280
discrimination or we can write better

522
00:22:39,360 --> 00:22:42,639
algorithms of course this is one of the

523
00:22:41,280 --> 00:22:45,678
challenges

524
00:22:42,640 --> 00:22:47,280
that we face at ucla in our center for

525
00:22:45,679 --> 00:22:49,840
critical internet inquiry

526
00:22:47,280 --> 00:22:51,600
is we're always trying to think about

527
00:22:49,840 --> 00:22:54,639
these social dimensions

528
00:22:51,600 --> 00:22:56,480
of technology rather than

529
00:22:54,640 --> 00:22:58,000
what others might be doing which is

530
00:22:56,480 --> 00:23:00,520
trying to figure out how to make a more

531
00:22:58,000 --> 00:23:03,200
perfect technology

532
00:23:00,520 --> 00:23:04,879
decontextualized in many ways from these

533
00:23:03,200 --> 00:23:05,520
kinds of histories that you're talking

534
00:23:04,880 --> 00:23:08,720
about

535
00:23:05,520 --> 00:23:11,679
that really indeed do foreclose

536
00:23:08,720 --> 00:23:13,280
a lot of opportunities we see for

537
00:23:11,679 --> 00:23:18,080
example

538
00:23:13,280 --> 00:23:20,158
babies being pulled into biometric

539
00:23:18,080 --> 00:23:22,639
facial recognition systems so that they

540
00:23:20,159 --> 00:23:26,240
can get on an airplane and fly

541
00:23:22,640 --> 00:23:29,440
i i remember watching a major airline

542
00:23:26,240 --> 00:23:31,520
roll that out recently and um

543
00:23:29,440 --> 00:23:33,360
and being horrified and yet the

544
00:23:31,520 --> 00:23:34,320
narrative is that this is so much more

545
00:23:33,360 --> 00:23:38,080
secure

546
00:23:34,320 --> 00:23:41,279
and convenient for us to use uh

547
00:23:38,080 --> 00:23:44,399
technologies like facial recognition uh

548
00:23:41,279 --> 00:23:46,880
we saw a few months ago

549
00:23:44,400 --> 00:23:47,760
the administration was looking at a

550
00:23:46,880 --> 00:23:51,279
technology

551
00:23:47,760 --> 00:23:55,600
that would allegedly help predict

552
00:23:51,279 --> 00:23:58,640
who would be uh the next mass shooter

553
00:23:55,600 --> 00:24:01,918
as if that weren't even possible i mean

554
00:23:58,640 --> 00:24:04,240
it's like kind of a snake oil um

555
00:24:01,919 --> 00:24:06,320
environment that we're in right now

556
00:24:04,240 --> 00:24:08,559
where so many of these different kinds

557
00:24:06,320 --> 00:24:11,439
of predictive technologies

558
00:24:08,559 --> 00:24:13,918
are uh indeed being weaponized against

559
00:24:11,440 --> 00:24:16,320
all kinds of people and you know

560
00:24:13,919 --> 00:24:19,120
although my work is focused on people

561
00:24:16,320 --> 00:24:22,000
who are traditionally um

562
00:24:19,120 --> 00:24:23,760
uh you know vulnerable most vulnerable

563
00:24:22,000 --> 00:24:26,799
in systems people of color

564
00:24:23,760 --> 00:24:29,120
children women and girls uh

565
00:24:26,799 --> 00:24:30,080
you know i think about who the database

566
00:24:29,120 --> 00:24:31,918
of

567
00:24:30,080 --> 00:24:34,158
potential mass shooters might be in the

568
00:24:31,919 --> 00:24:35,360
united states right and so this might be

569
00:24:34,159 --> 00:24:38,240
a place where

570
00:24:35,360 --> 00:24:39,120
people who don't see themselves as

571
00:24:38,240 --> 00:24:41,840
necessarily

572
00:24:39,120 --> 00:24:43,039
ensnared in harmful technologies might

573
00:24:41,840 --> 00:24:45,918
want to care

574
00:24:43,039 --> 00:24:48,080
about that because of course um these

575
00:24:45,919 --> 00:24:50,559
profiling tools are going to reach

576
00:24:48,080 --> 00:24:51,678
everyone so that's my that's my aside i

577
00:24:50,559 --> 00:24:54,158
i will say

578
00:24:51,679 --> 00:24:55,360
that though as a segway that you've

579
00:24:54,159 --> 00:24:58,400
talked about

580
00:24:55,360 --> 00:25:01,120
um kind of big technology being a threat

581
00:24:58,400 --> 00:25:02,880
in three major ways i've heard you talk

582
00:25:01,120 --> 00:25:06,000
about it being a threat to

583
00:25:02,880 --> 00:25:07,440
fair economic competition

584
00:25:06,000 --> 00:25:09,120
i've heard you talk about it being a

585
00:25:07,440 --> 00:25:12,960
threat to civil rights

586
00:25:09,120 --> 00:25:14,559
and a threat to democracy so yesterday

587
00:25:12,960 --> 00:25:17,520
we had um

588
00:25:14,559 --> 00:25:18,559
the ceos from some of the major tech

589
00:25:17,520 --> 00:25:21,360
companies

590
00:25:18,559 --> 00:25:22,240
uh who were in front of the judiciary uh

591
00:25:21,360 --> 00:25:25,760
committee

592
00:25:22,240 --> 00:25:30,080
um apple amazon facebook google

593
00:25:25,760 --> 00:25:31,840
and um uh microsoft was missing so

594
00:25:30,080 --> 00:25:33,678
i guess they'll they'll be around on the

595
00:25:31,840 --> 00:25:36,320
next round um

596
00:25:33,679 --> 00:25:36,880
and i wanna i wanna know what you

597
00:25:36,320 --> 00:25:39,840
thought

598
00:25:36,880 --> 00:25:40,720
about the hearing what jumped out to you

599
00:25:39,840 --> 00:25:43,520
what

600
00:25:40,720 --> 00:25:44,720
wasn't said what is it that policymakers

601
00:25:43,520 --> 00:25:47,600
should have asked

602
00:25:44,720 --> 00:25:49,039
um just what's your like you were the

603
00:25:47,600 --> 00:25:50,639
first person i wanted to call

604
00:25:49,039 --> 00:25:52,240
after the hearings were to be like what

605
00:25:50,640 --> 00:25:54,640
did you think so i want to know what did

606
00:25:52,240 --> 00:25:57,919
you think

607
00:25:54,640 --> 00:26:00,720
yeah sure and i want to be careful as um

608
00:25:57,919 --> 00:26:02,159
you know in some cases we have uh an

609
00:26:00,720 --> 00:26:04,400
open investigation

610
00:26:02,159 --> 00:26:05,919
these all four of those firms are

611
00:26:04,400 --> 00:26:10,080
subject to

612
00:26:05,919 --> 00:26:12,640
current um ftc orders um but i can speak

613
00:26:10,080 --> 00:26:15,600
big picture which is that

614
00:26:12,640 --> 00:26:18,159
i think this was important for the

615
00:26:15,600 --> 00:26:21,840
public to remember

616
00:26:18,159 --> 00:26:26,000
that we are the ones that collectively

617
00:26:21,840 --> 00:26:30,000
control our markets our laws

618
00:26:26,000 --> 00:26:30,799
um and our democracy and i think there

619
00:26:30,000 --> 00:26:33,760
has been

620
00:26:30,799 --> 00:26:35,120
a mindset that's quite troubling which

621
00:26:33,760 --> 00:26:38,720
is that

622
00:26:35,120 --> 00:26:41,760
um some of our largest firms uh

623
00:26:38,720 --> 00:26:42,159
it feels that very unaccountable i mean

624
00:26:41,760 --> 00:26:45,360
there

625
00:26:42,159 --> 00:26:48,640
last year there was the ftc

626
00:26:45,360 --> 00:26:50,879
settlement with facebook that was

627
00:26:48,640 --> 00:26:51,360
essentially five billion dollars and

628
00:26:50,880 --> 00:26:53,600
some

629
00:26:51,360 --> 00:26:54,639
some paperwork i was very opposed to

630
00:26:53,600 --> 00:26:58,320
that settlement

631
00:26:54,640 --> 00:27:02,159
and one of many reasons was that i was

632
00:26:58,320 --> 00:27:05,200
deeply troubled that the agency did

633
00:27:02,159 --> 00:27:08,720
not uh subject

634
00:27:05,200 --> 00:27:10,000
mr zuckerberg or miss sandberg to sworn

635
00:27:08,720 --> 00:27:13,919
testimony

636
00:27:10,000 --> 00:27:17,520
did not seek mr zuckerberg's documents

637
00:27:13,919 --> 00:27:19,520
and in some ways it felt like based on

638
00:27:17,520 --> 00:27:22,320
some of the statements that were made

639
00:27:19,520 --> 00:27:22,720
after the announcement that the um that

640
00:27:22,320 --> 00:27:25,678
the

641
00:27:22,720 --> 00:27:26,080
government essentially traded getting

642
00:27:25,679 --> 00:27:29,600
more

643
00:27:26,080 --> 00:27:32,720
money um so that an individual

644
00:27:29,600 --> 00:27:35,279
did not have to uh

645
00:27:32,720 --> 00:27:37,120
submit to sworn testimony and and i just

646
00:27:35,279 --> 00:27:38,399
think that's fundamentally wrong that's

647
00:27:37,120 --> 00:27:41,439
not the way

648
00:27:38,399 --> 00:27:44,080
to ensure rule of law

649
00:27:41,440 --> 00:27:44,559
um i'll tell you this when the ftc goes

650
00:27:44,080 --> 00:27:48,158
after

651
00:27:44,559 --> 00:27:52,000
small firms for privacy violations

652
00:27:48,159 --> 00:27:54,960
boy do we take it all in some cases we

653
00:27:52,000 --> 00:27:55,760
we name the executives and we round them

654
00:27:54,960 --> 00:27:58,640
up and

655
00:27:55,760 --> 00:27:59,840
we can't have this sort of two-tiered

656
00:27:58,640 --> 00:28:03,279
system

657
00:27:59,840 --> 00:28:04,959
so having uh some of the most powerful

658
00:28:03,279 --> 00:28:08,640
men in the world

659
00:28:04,960 --> 00:28:12,880
uh be s subject to public questioning

660
00:28:08,640 --> 00:28:15,440
by uh elected members of congress i

661
00:28:12,880 --> 00:28:19,039
think is a good reminder that

662
00:28:15,440 --> 00:28:20,960
it is we as citizens uh

663
00:28:19,039 --> 00:28:22,320
through our elected representatives

664
00:28:20,960 --> 00:28:25,200
through our

665
00:28:22,320 --> 00:28:27,520
uh law enforcement through our policy

666
00:28:25,200 --> 00:28:29,200
makers this is the process we use to

667
00:28:27,520 --> 00:28:32,559
structure markets

668
00:28:29,200 --> 00:28:35,279
to ensure fairness and accountability uh

669
00:28:32,559 --> 00:28:36,480
and they don't have veto power over that

670
00:28:35,279 --> 00:28:39,760
and i think

671
00:28:36,480 --> 00:28:42,799
what you heard and what i understand i

672
00:28:39,760 --> 00:28:46,158
did not watch the full six hours

673
00:28:42,799 --> 00:28:49,279
is that many people uh

674
00:28:46,159 --> 00:28:50,559
they were asked questions about are they

675
00:28:49,279 --> 00:28:54,000
in some ways

676
00:28:50,559 --> 00:28:56,840
um possessing sovereign power

677
00:28:54,000 --> 00:28:58,880
the ability to tax the ability to

678
00:28:56,840 --> 00:29:00,799
exclude

679
00:28:58,880 --> 00:29:03,520
things that are typically done through

680
00:29:00,799 --> 00:29:07,200
the democratic process

681
00:29:03,520 --> 00:29:10,158
not dictated um you know by

682
00:29:07,200 --> 00:29:12,000
by the largest firms so i think at a at

683
00:29:10,159 --> 00:29:15,039
a base level that was very very

684
00:29:12,000 --> 00:29:18,880
important i also think

685
00:29:15,039 --> 00:29:22,158
you got a little bit of a window into

686
00:29:18,880 --> 00:29:25,360
the intersection between how

687
00:29:22,159 --> 00:29:27,840
some of these firms do create massive

688
00:29:25,360 --> 00:29:29,199
valuations you know all of these firms

689
00:29:27,840 --> 00:29:31,439
are roughly

690
00:29:29,200 --> 00:29:33,120
you know with with a few hundred billion

691
00:29:31,440 --> 00:29:36,399
in each direction

692
00:29:33,120 --> 00:29:37,600
trillion dollar firms and one of the

693
00:29:36,399 --> 00:29:39,279
things

694
00:29:37,600 --> 00:29:41,360
to continue off something you had

695
00:29:39,279 --> 00:29:43,919
previously said is that

696
00:29:41,360 --> 00:29:46,639
the incentives that are driven from our

697
00:29:43,919 --> 00:29:48,240
capital markets and wall street and sand

698
00:29:46,640 --> 00:29:51,919
hill road

699
00:29:48,240 --> 00:29:55,200
are one to create extremely

700
00:29:51,919 --> 00:29:59,039
strong network effects such

701
00:29:55,200 --> 00:30:02,240
that everyone has to essentially use you

702
00:29:59,039 --> 00:30:05,200
and then use that power

703
00:30:02,240 --> 00:30:06,640
to continue to create more and more cash

704
00:30:05,200 --> 00:30:09,039
flow streams

705
00:30:06,640 --> 00:30:10,720
in ways that are very very difficult to

706
00:30:09,039 --> 00:30:12,960
challenge and whether or not that is

707
00:30:10,720 --> 00:30:14,399
unlawful that will be an inquiry that is

708
00:30:12,960 --> 00:30:18,399
pursued

709
00:30:14,399 --> 00:30:21,360
but that's it it's not necessarily

710
00:30:18,399 --> 00:30:23,360
always competing on who has the best

711
00:30:21,360 --> 00:30:25,120
product or the best service sometimes

712
00:30:23,360 --> 00:30:29,279
it's competing with

713
00:30:25,120 --> 00:30:32,399
who has the capital to flood the market

714
00:30:29,279 --> 00:30:35,919
and gain share and dominance and then

715
00:30:32,399 --> 00:30:36,639
ratchet up and monetize that power and i

716
00:30:35,919 --> 00:30:39,120
think

717
00:30:36,640 --> 00:30:40,640
we will see the extent to which any of

718
00:30:39,120 --> 00:30:44,320
that violates u.s

719
00:30:40,640 --> 00:30:47,120
law obviously some of the data practices

720
00:30:44,320 --> 00:30:49,360
have we have found violations of law

721
00:30:47,120 --> 00:30:50,158
the antitrust investigations are

722
00:30:49,360 --> 00:30:52,879
continuing

723
00:30:50,159 --> 00:30:53,279
at the federal level in the state level

724
00:30:52,880 --> 00:30:57,039
our

725
00:30:53,279 --> 00:30:59,279
state attorneys general uh in the united

726
00:30:57,039 --> 00:31:01,120
states i try and stay in constant

727
00:30:59,279 --> 00:31:02,559
communication with them i think are

728
00:31:01,120 --> 00:31:04,799
going to play an outsized

729
00:31:02,559 --> 00:31:06,320
role in in thinking through these

730
00:31:04,799 --> 00:31:10,000
inquiries

731
00:31:06,320 --> 00:31:14,720
um but no i i i'm glad to see that

732
00:31:10,000 --> 00:31:17,840
uh we're moving more toward a sense that

733
00:31:14,720 --> 00:31:20,080
uh you can't hide just behind your

734
00:31:17,840 --> 00:31:21,678
corporate banner that individuals

735
00:31:20,080 --> 00:31:24,960
sometimes have to answer

736
00:31:21,679 --> 00:31:27,440
questions and face accountability

737
00:31:24,960 --> 00:31:30,480
because i'll tell you when

738
00:31:27,440 --> 00:31:34,080
all of everyone who

739
00:31:30,480 --> 00:31:35,519
is a normal average citizen that doesn't

740
00:31:34,080 --> 00:31:37,039
you know is not a millionaire or a

741
00:31:35,519 --> 00:31:39,840
billionaire

742
00:31:37,039 --> 00:31:40,640
when they break the law you know they

743
00:31:39,840 --> 00:31:44,240
don't get

744
00:31:40,640 --> 00:31:44,880
to you know have an army of lawyers kind

745
00:31:44,240 --> 00:31:48,000
of

746
00:31:44,880 --> 00:31:49,440
help them evade even be questioning um

747
00:31:48,000 --> 00:31:50,480
and and that's what i think we have to

748
00:31:49,440 --> 00:31:53,519
think about with

749
00:31:50,480 --> 00:31:55,760
with fair administration of justice

750
00:31:53,519 --> 00:31:58,640
even when it comes to the largest firms

751
00:31:55,760 --> 00:31:58,640
in our economy

752
00:31:58,840 --> 00:32:04,959
yeah you know as you were

753
00:32:01,519 --> 00:32:07,600
saying this i was thinking about how

754
00:32:04,960 --> 00:32:08,159
profoundly intertwined these companies

755
00:32:07,600 --> 00:32:12,399
are

756
00:32:08,159 --> 00:32:13,440
in the erosion of a variety of different

757
00:32:12,399 --> 00:32:16,479
democratic

758
00:32:13,440 --> 00:32:17,919
kinds of institutions so you know the

759
00:32:16,480 --> 00:32:20,480
the questioning yesterday and the

760
00:32:17,919 --> 00:32:21,519
hearing was getting at

761
00:32:20,480 --> 00:32:23,840
you know different kinds of

762
00:32:21,519 --> 00:32:25,200
anti-democratic practices whether it's

763
00:32:23,840 --> 00:32:29,519
the proliferation

764
00:32:25,200 --> 00:32:32,240
of um hate speech within their platforms

765
00:32:29,519 --> 00:32:33,360
um whether it was direct violation of

766
00:32:32,240 --> 00:32:37,360
civil rights law

767
00:32:33,360 --> 00:32:40,559
for example in um housing discrimination

768
00:32:37,360 --> 00:32:43,840
practices or other types of um

769
00:32:40,559 --> 00:32:46,399
you know violations of the law

770
00:32:43,840 --> 00:32:47,279
quite frankly that one of course should

771
00:32:46,399 --> 00:32:50,399
be held account

772
00:32:47,279 --> 00:32:52,320
to account for um

773
00:32:50,399 --> 00:32:53,840
but i was thinking you know it also at a

774
00:32:52,320 --> 00:32:57,039
kind of a local

775
00:32:53,840 --> 00:32:59,199
level you know here i am i'm a professor

776
00:32:57,039 --> 00:33:01,039
in the university of california system

777
00:32:59,200 --> 00:33:03,600
this is one of the flagship

778
00:33:01,039 --> 00:33:06,080
research public research university

779
00:33:03,600 --> 00:33:10,799
systems in the country

780
00:33:06,080 --> 00:33:14,158
um like many other public democratic

781
00:33:10,799 --> 00:33:18,158
institutions um we see

782
00:33:14,159 --> 00:33:21,039
the shrinking um coffers

783
00:33:18,159 --> 00:33:23,519
we have seen we are down in the uc

784
00:33:21,039 --> 00:33:26,559
system now to about a four percent

785
00:33:23,519 --> 00:33:28,080
contribution from taxpayers if you can

786
00:33:26,559 --> 00:33:31,039
imagine that

787
00:33:28,080 --> 00:33:32,799
and the conditions that that creates are

788
00:33:31,039 --> 00:33:35,120
precisely what you said and i want to

789
00:33:32,799 --> 00:33:37,360
give this example because i think people

790
00:33:35,120 --> 00:33:39,199
sometimes can't really conceptualize how

791
00:33:37,360 --> 00:33:42,320
this matters to them

792
00:33:39,200 --> 00:33:42,880
so if you have a public university

793
00:33:42,320 --> 00:33:45,519
system

794
00:33:42,880 --> 00:33:46,720
or a public education system k through

795
00:33:45,519 --> 00:33:50,080
12

796
00:33:46,720 --> 00:33:54,000
where no tax dollars from big companies

797
00:33:50,080 --> 00:33:58,399
for the most part go into the system

798
00:33:54,000 --> 00:34:02,960
then you have a very extractive model

799
00:33:58,399 --> 00:34:06,320
on one hand it happens by way of

800
00:34:02,960 --> 00:34:08,639
the public having to pay higher prices

801
00:34:06,320 --> 00:34:09,440
quite frankly make deeper personal

802
00:34:08,639 --> 00:34:12,480
investments

803
00:34:09,440 --> 00:34:13,918
in their themselves and their

804
00:34:12,480 --> 00:34:16,399
the people they care about getting an

805
00:34:13,918 --> 00:34:19,839
education and that happens through

806
00:34:16,399 --> 00:34:20,960
tuition increases right because the tax

807
00:34:19,839 --> 00:34:23,279
base isn't there

808
00:34:20,960 --> 00:34:25,040
and of course here we are in california

809
00:34:23,280 --> 00:34:26,960
uh you know

810
00:34:25,040 --> 00:34:28,960
home of silicon beach right up the

811
00:34:26,960 --> 00:34:33,119
street is silicon valley

812
00:34:28,960 --> 00:34:35,280
and we see almost no tax dollars in

813
00:34:33,119 --> 00:34:37,280
a meaningful way coming into the system

814
00:34:35,280 --> 00:34:38,720
so this is a this is interesting to me

815
00:34:37,280 --> 00:34:42,320
in terms of

816
00:34:38,719 --> 00:34:44,719
if we think of democratic institutions

817
00:34:42,320 --> 00:34:46,159
like educational systems k through

818
00:34:44,719 --> 00:34:50,638
higher education

819
00:34:46,159 --> 00:34:53,599
libraries um public media organizations

820
00:34:50,639 --> 00:34:54,079
public health organizations being the

821
00:34:53,599 --> 00:34:57,200
kind of

822
00:34:54,079 --> 00:35:00,880
public counterweight

823
00:34:57,200 --> 00:35:03,759
to big tech but big tech doesn't

824
00:35:00,880 --> 00:35:04,560
actually it actually actively works to

825
00:35:03,760 --> 00:35:07,760
undermine

826
00:35:04,560 --> 00:35:11,040
those systems by offshoring for example

827
00:35:07,760 --> 00:35:13,280
its profits um

828
00:35:11,040 --> 00:35:15,359
also taking the cream of the crop the

829
00:35:13,280 --> 00:35:17,359
best students right the best workers to

830
00:35:15,359 --> 00:35:20,880
go and work in the sector

831
00:35:17,359 --> 00:35:21,440
um you know this also really contributes

832
00:35:20,880 --> 00:35:24,079
to

833
00:35:21,440 --> 00:35:25,119
the kind of fundamental to me breakdowns

834
00:35:24,079 --> 00:35:27,760
in

835
00:35:25,119 --> 00:35:29,119
democracy and democratic institutions

836
00:35:27,760 --> 00:35:32,320
and things like

837
00:35:29,119 --> 00:35:33,280
um accessible education affordable

838
00:35:32,320 --> 00:35:35,200
education

839
00:35:33,280 --> 00:35:36,880
for the public and then what happens is

840
00:35:35,200 --> 00:35:39,839
the new narratives come about

841
00:35:36,880 --> 00:35:40,960
that well you know shame on you public

842
00:35:39,839 --> 00:35:42,560
for not knowing better

843
00:35:40,960 --> 00:35:44,960
how to tell the difference between

844
00:35:42,560 --> 00:35:46,160
knowledge and propaganda or

845
00:35:44,960 --> 00:35:49,200
disinformation

846
00:35:46,160 --> 00:35:51,598
right it's kind of a public shaming of

847
00:35:49,200 --> 00:35:53,359
of the users of the platforms while at

848
00:35:51,599 --> 00:35:57,280
the same time the very

849
00:35:53,359 --> 00:36:00,078
conditions of not educating the public

850
00:35:57,280 --> 00:36:01,119
are made in part by this sector i mean

851
00:36:00,079 --> 00:36:04,800
to me this is very

852
00:36:01,119 --> 00:36:08,160
interesting these are complex important

853
00:36:04,800 --> 00:36:10,240
questions we need to be asking um

854
00:36:08,160 --> 00:36:12,240
it also creates the conditions to come

855
00:36:10,240 --> 00:36:13,200
in and provide for example in the uc

856
00:36:12,240 --> 00:36:16,640
system

857
00:36:13,200 --> 00:36:17,040
the entire it backbone which google has

858
00:36:16,640 --> 00:36:19,920
done

859
00:36:17,040 --> 00:36:21,520
right so now all of the all of our email

860
00:36:19,920 --> 00:36:23,760
and all of our suite of services for the

861
00:36:21,520 --> 00:36:25,280
most part is in microsoft and google

862
00:36:23,760 --> 00:36:27,200
so i find these things to be very

863
00:36:25,280 --> 00:36:30,320
fascinating and i guess you know

864
00:36:27,200 --> 00:36:32,560
at the you know in our group

865
00:36:30,320 --> 00:36:34,400
of researchers at ucla we're always

866
00:36:32,560 --> 00:36:36,640
trying to think about the paradigm

867
00:36:34,400 --> 00:36:38,839
that we're in and how do we shift the

868
00:36:36,640 --> 00:36:41,359
paradigm thinking

869
00:36:38,839 --> 00:36:42,720
um and that of course has to take us to

870
00:36:41,359 --> 00:36:45,759
a place of thinking about

871
00:36:42,720 --> 00:36:46,640
remedies how do we fix it what are those

872
00:36:45,760 --> 00:36:49,520
solutions

873
00:36:46,640 --> 00:36:52,000
so i guess i want to ask you from your

874
00:36:49,520 --> 00:36:52,560
vantage point the ftc having so much

875
00:36:52,000 --> 00:36:55,440
power

876
00:36:52,560 --> 00:36:56,720
over thinking about consumer harm and

877
00:36:55,440 --> 00:36:59,839
public harm

878
00:36:56,720 --> 00:37:03,520
and anti-democratic

879
00:36:59,839 --> 00:37:06,078
practices you know

880
00:37:03,520 --> 00:37:07,040
anti-trust practices all of these things

881
00:37:06,079 --> 00:37:10,079
being very

882
00:37:07,040 --> 00:37:12,240
complicated but related um

883
00:37:10,079 --> 00:37:16,000
what do you see as maybe some of the

884
00:37:12,240 --> 00:37:16,000
remedies that are on your mind right now

885
00:37:16,320 --> 00:37:23,040
it's a great great um

886
00:37:19,440 --> 00:37:25,280
question if if you if i may

887
00:37:23,040 --> 00:37:26,960
i you you've sort of raised something

888
00:37:25,280 --> 00:37:30,720
that is close to my heart

889
00:37:26,960 --> 00:37:33,520
which is the role of higher education

890
00:37:30,720 --> 00:37:35,200
uh institution public institutions in

891
00:37:33,520 --> 00:37:37,520
our society um

892
00:37:35,200 --> 00:37:38,799
some of you may know that i spent

893
00:37:37,520 --> 00:37:41,920
several years

894
00:37:38,800 --> 00:37:43,359
uh at the consumer financial protection

895
00:37:41,920 --> 00:37:45,680
bureau as

896
00:37:43,359 --> 00:37:48,000
uh leading the student loan regulation

897
00:37:45,680 --> 00:37:50,480
and really butting heads

898
00:37:48,000 --> 00:37:51,200
with some of the bad actors in the

899
00:37:50,480 --> 00:37:54,000
industry

900
00:37:51,200 --> 00:37:57,439
shutting down some of the publicly

901
00:37:54,000 --> 00:38:00,160
traded for-profit college chains

902
00:37:57,440 --> 00:38:01,760
you know major lawsuits again sallie mae

903
00:38:00,160 --> 00:38:04,480
navient

904
00:38:01,760 --> 00:38:06,000
a lot of them wells fargo and so it's

905
00:38:04,480 --> 00:38:08,720
really a reflection

906
00:38:06,000 --> 00:38:10,040
in some ways our student debt crisis of

907
00:38:08,720 --> 00:38:13,359
the fundamental

908
00:38:10,040 --> 00:38:16,800
disinvestment in how

909
00:38:13,359 --> 00:38:20,319
the public thinks about elevating

910
00:38:16,800 --> 00:38:24,400
the society as a whole and its people

911
00:38:20,320 --> 00:38:25,040
and so i i i a few years ago i wrote a

912
00:38:24,400 --> 00:38:28,160
paper

913
00:38:25,040 --> 00:38:31,200
about soft corruption

914
00:38:28,160 --> 00:38:32,319
um you know in washington but but in

915
00:38:31,200 --> 00:38:35,680
capitals across

916
00:38:32,320 --> 00:38:36,560
the world and one of the things that has

917
00:38:35,680 --> 00:38:40,000
been it

918
00:38:36,560 --> 00:38:43,599
was a trend i noticed in my analysis is

919
00:38:40,000 --> 00:38:46,720
the increasing privatization

920
00:38:43,599 --> 00:38:48,079
of and weaponization of independent

921
00:38:46,720 --> 00:38:51,118
research

922
00:38:48,079 --> 00:38:52,480
so it's now not uncommon in policy

923
00:38:51,119 --> 00:38:56,079
fights

924
00:38:52,480 --> 00:38:59,760
for incumbent players

925
00:38:56,079 --> 00:39:02,800
to finance research

926
00:38:59,760 --> 00:39:05,359
including at public universities to

927
00:39:02,800 --> 00:39:07,119
advance a certain type of view in order

928
00:39:05,359 --> 00:39:10,319
to shape the debate

929
00:39:07,119 --> 00:39:12,720
in order to shape uh public policy

930
00:39:10,320 --> 00:39:14,000
it is not necessarily a campaign

931
00:39:12,720 --> 00:39:16,480
contribution it's

932
00:39:14,000 --> 00:39:17,440
not a bribe in the traditional sense but

933
00:39:16,480 --> 00:39:20,640
it is a form of

934
00:39:17,440 --> 00:39:24,079
soft corruption that i think is

935
00:39:20,640 --> 00:39:26,720
a result of under investment

936
00:39:24,079 --> 00:39:28,720
in the public sector whether it be our

937
00:39:26,720 --> 00:39:30,799
public universities whether it be in

938
00:39:28,720 --> 00:39:31,520
public of scientific research and the

939
00:39:30,800 --> 00:39:34,160
net effect

940
00:39:31,520 --> 00:39:35,680
is actually a a privatization of a lot

941
00:39:34,160 --> 00:39:37,680
of the fruits of that

942
00:39:35,680 --> 00:39:39,919
um i think which does not necessarily

943
00:39:37,680 --> 00:39:41,440
serve our economy well it doesn't serve

944
00:39:39,920 --> 00:39:42,160
our society well and does serve our

945
00:39:41,440 --> 00:39:45,440
democracy

946
00:39:42,160 --> 00:39:48,720
well so you know to answer

947
00:39:45,440 --> 00:39:52,400
your question i think really

948
00:39:48,720 --> 00:39:53,919
the remedies depend on the problems

949
00:39:52,400 --> 00:39:54,400
we're trying to solve and one of the

950
00:39:53,920 --> 00:39:57,839
things

951
00:39:54,400 --> 00:40:01,520
i've tried to facilitate is a lot

952
00:39:57,839 --> 00:40:04,640
more engagement by regulators

953
00:40:01,520 --> 00:40:07,119
across the globe and and i think

954
00:40:04,640 --> 00:40:09,680
the europeans have their point of view

955
00:40:07,119 --> 00:40:12,319
there's there's of course from

956
00:40:09,680 --> 00:40:14,160
asia latin america and africa are also

957
00:40:12,319 --> 00:40:15,119
increasingly worried about some of these

958
00:40:14,160 --> 00:40:16,319
issues

959
00:40:15,119 --> 00:40:18,960
and i think we're going to have to

960
00:40:16,319 --> 00:40:20,960
really come together to figure out what

961
00:40:18,960 --> 00:40:22,720
those solutions are and even if we don't

962
00:40:20,960 --> 00:40:25,040
we may have our country specific

963
00:40:22,720 --> 00:40:29,200
solutions but i think of them

964
00:40:25,040 --> 00:40:32,800
you know really trying to move away

965
00:40:29,200 --> 00:40:35,520
from the concept of monetary

966
00:40:32,800 --> 00:40:38,560
fines you know we've seen some very

967
00:40:35,520 --> 00:40:40,640
large fines in europe and the u.s and

968
00:40:38,560 --> 00:40:42,640
i think it's becoming clear that that

969
00:40:40,640 --> 00:40:43,920
has come it's not effective in in

970
00:40:42,640 --> 00:40:47,359
addressing

971
00:40:43,920 --> 00:40:49,920
the underlying incentives that motivated

972
00:40:47,359 --> 00:40:54,000
the misconduct or made the misconduct

973
00:40:49,920 --> 00:40:56,880
profitable so you know if we look to

974
00:40:54,000 --> 00:40:57,200
remedies in in us history i think there

975
00:40:56,880 --> 00:41:00,319
is

976
00:40:57,200 --> 00:41:03,520
a lot we can be thinking about that have

977
00:41:00,319 --> 00:41:04,079
correlations to today so one of them i

978
00:41:03,520 --> 00:41:08,240
think

979
00:41:04,079 --> 00:41:11,760
is the concept of structural separations

980
00:41:08,240 --> 00:41:14,399
uh the idea of figuring out

981
00:41:11,760 --> 00:41:15,359
where in certain business models there

982
00:41:14,400 --> 00:41:18,960
are inherent

983
00:41:15,359 --> 00:41:21,520
conflicts of interest we have addressed

984
00:41:18,960 --> 00:41:23,200
that in telecommunications we have

985
00:41:21,520 --> 00:41:26,079
addressed that

986
00:41:23,200 --> 00:41:27,759
in some aspects of banking we have

987
00:41:26,079 --> 00:41:30,480
addressed that in a lot of

988
00:41:27,760 --> 00:41:32,000
sectors that are that are sometimes

989
00:41:30,480 --> 00:41:35,760
succumbed to these

990
00:41:32,000 --> 00:41:37,920
abuses of dominance and market power

991
00:41:35,760 --> 00:41:40,160
and you know you're seeing in some

992
00:41:37,920 --> 00:41:43,359
jurisdictions around the world

993
00:41:40,160 --> 00:41:46,799
rather than creating a highly complex

994
00:41:43,359 --> 00:41:48,960
regulatory regime that you know in some

995
00:41:46,800 --> 00:41:52,560
ways the big companies have an easier

996
00:41:48,960 --> 00:41:54,960
time complying with simple and clear

997
00:41:52,560 --> 00:41:56,880
rules of the road and bands i think are

998
00:41:54,960 --> 00:41:57,760
the most appropriate and let the market

999
00:41:56,880 --> 00:41:59,920
innovate

1000
00:41:57,760 --> 00:42:01,440
within those boundaries i mean we've

1001
00:41:59,920 --> 00:42:04,880
seen examples

1002
00:42:01,440 --> 00:42:07,520
outside of the u.s i believe in india

1003
00:42:04,880 --> 00:42:10,160
they have started to put into place

1004
00:42:07,520 --> 00:42:13,359
rules that say if you're a dominant

1005
00:42:10,160 --> 00:42:16,078
e-commerce platform you either

1006
00:42:13,359 --> 00:42:17,759
sell your own goods or you sell other

1007
00:42:16,079 --> 00:42:20,960
people's goods you don't do

1008
00:42:17,760 --> 00:42:23,119
both the idea being to eliminate the

1009
00:42:20,960 --> 00:42:26,319
conflict of interest

1010
00:42:23,119 --> 00:42:29,920
of using the data and

1011
00:42:26,319 --> 00:42:31,759
from third party market participants and

1012
00:42:29,920 --> 00:42:34,480
platform participants

1013
00:42:31,760 --> 00:42:37,119
in order to benefit your own business or

1014
00:42:34,480 --> 00:42:39,760
to use it in a way to get an advantage

1015
00:42:37,119 --> 00:42:42,640
you obviously see this um you know in

1016
00:42:39,760 --> 00:42:45,359
other sectors and heavy industry

1017
00:42:42,640 --> 00:42:45,680
you know whether railroads can also own

1018
00:42:45,359 --> 00:42:48,319
coal

1019
00:42:45,680 --> 00:42:49,839
companies so i think part of it is

1020
00:42:48,319 --> 00:42:51,520
thinking through these structural

1021
00:42:49,839 --> 00:42:55,359
separations

1022
00:42:51,520 --> 00:42:57,359
um what should you be able to engage in

1023
00:42:55,359 --> 00:42:58,000
two types of activities that might

1024
00:42:57,359 --> 00:43:00,799
actually be

1025
00:42:58,000 --> 00:43:01,119
a conflict um should you be able you

1026
00:43:00,800 --> 00:43:04,000
know

1027
00:43:01,119 --> 00:43:05,599
a common analogy that is used is should

1028
00:43:04,000 --> 00:43:08,720
you be able to be the

1029
00:43:05,599 --> 00:43:11,280
player and also the referee so

1030
00:43:08,720 --> 00:43:14,560
thinking about those structural

1031
00:43:11,280 --> 00:43:18,319
separations i think is part of a way of

1032
00:43:14,560 --> 00:43:21,680
driving growth innovation and preventing

1033
00:43:18,319 --> 00:43:24,160
incumbents from essentially taxing um

1034
00:43:21,680 --> 00:43:26,399
you know the plumbing of our economy

1035
00:43:24,160 --> 00:43:30,399
another thing that i feel really

1036
00:43:26,400 --> 00:43:33,680
um is an important part of all of this

1037
00:43:30,400 --> 00:43:35,839
is the role of intellectual property

1038
00:43:33,680 --> 00:43:36,960
and you know there's a lot of debates

1039
00:43:35,839 --> 00:43:40,000
right now uh

1040
00:43:36,960 --> 00:43:41,680
in the u.s and europe uh with respect to

1041
00:43:40,000 --> 00:43:43,920
the chinese system

1042
00:43:41,680 --> 00:43:46,399
and in some ways the chinese system has

1043
00:43:43,920 --> 00:43:49,280
a very close affiliation

1044
00:43:46,400 --> 00:43:49,760
with its tech companies you know we do

1045
00:43:49,280 --> 00:43:52,880
have

1046
00:43:49,760 --> 00:43:56,000
insights about relationships and

1047
00:43:52,880 --> 00:43:57,680
between the ccp and some of it the the

1048
00:43:56,000 --> 00:44:00,319
large players

1049
00:43:57,680 --> 00:44:00,720
and we also know that the way in which

1050
00:44:00,319 --> 00:44:03,520
they

1051
00:44:00,720 --> 00:44:05,520
circulate um intellectual property

1052
00:44:03,520 --> 00:44:08,160
throughout the economy and and

1053
00:44:05,520 --> 00:44:10,480
in the west a lot of our intellectual

1054
00:44:08,160 --> 00:44:14,560
property is vaulted up

1055
00:44:10,480 --> 00:44:17,440
and intellectual property trade secrets

1056
00:44:14,560 --> 00:44:18,160
copyrights trademarks patents all of

1057
00:44:17,440 --> 00:44:21,520
that

1058
00:44:18,160 --> 00:44:24,720
that's really a decision for the public

1059
00:44:21,520 --> 00:44:25,520
about what kind of privileges and

1060
00:44:24,720 --> 00:44:30,480
special

1061
00:44:25,520 --> 00:44:31,920
benefits do we want to grant companies

1062
00:44:30,480 --> 00:44:33,839
in order to promote growth and

1063
00:44:31,920 --> 00:44:35,040
innovation and in some ways if they're

1064
00:44:33,839 --> 00:44:38,078
able to lock that

1065
00:44:35,040 --> 00:44:39,920
up and only use it for their themselves

1066
00:44:38,079 --> 00:44:42,640
without really creating

1067
00:44:39,920 --> 00:44:45,599
um broad benefits then maybe we need to

1068
00:44:42,640 --> 00:44:47,598
rethink that or if they are abusing

1069
00:44:45,599 --> 00:44:49,920
that intellectual property then maybe

1070
00:44:47,599 --> 00:44:51,119
they no longer should have exclusivity

1071
00:44:49,920 --> 00:44:54,880
to it

1072
00:44:51,119 --> 00:44:58,160
if you are we we remember that after

1073
00:44:54,880 --> 00:45:01,359
opening up the licensing to

1074
00:44:58,160 --> 00:45:04,480
bell labs generations ago incubated

1075
00:45:01,359 --> 00:45:05,680
a massive run of innovation in the

1076
00:45:04,480 --> 00:45:09,200
united states

1077
00:45:05,680 --> 00:45:12,240
to use all of that open technology

1078
00:45:09,200 --> 00:45:14,960
um and and bring those benefits to all

1079
00:45:12,240 --> 00:45:16,560
sorts of creators and innovators

1080
00:45:14,960 --> 00:45:18,319
the other remedy i think we're going to

1081
00:45:16,560 --> 00:45:21,839
need to think about

1082
00:45:18,319 --> 00:45:24,720
is open internet

1083
00:45:21,839 --> 00:45:25,040
open digital technologies so you know if

1084
00:45:24,720 --> 00:45:28,399
you

1085
00:45:25,040 --> 00:45:31,119
think about the origins of the best of

1086
00:45:28,400 --> 00:45:35,599
the internet is that no company really

1087
00:45:31,119 --> 00:45:38,720
owns it or at least didn't own it before

1088
00:45:35,599 --> 00:45:42,160
and it had low barriers to entry it had

1089
00:45:38,720 --> 00:45:44,879
open protocols in ways that

1090
00:45:42,160 --> 00:45:46,078
created more information flows and more

1091
00:45:44,880 --> 00:45:49,040
activity

1092
00:45:46,079 --> 00:45:50,480
and i think the more we can remember

1093
00:45:49,040 --> 00:45:53,680
that spirit of open

1094
00:45:50,480 --> 00:45:55,599
protocols open technology and open

1095
00:45:53,680 --> 00:45:58,240
licensing and standards

1096
00:45:55,599 --> 00:45:59,040
i think that's another type of remedy

1097
00:45:58,240 --> 00:46:02,319
that is

1098
00:45:59,040 --> 00:46:04,400
how we might restructure and fix if we

1099
00:46:02,319 --> 00:46:06,560
find unlawful conduct

1100
00:46:04,400 --> 00:46:07,520
and i i don't wan i think there's a

1101
00:46:06,560 --> 00:46:11,279
another really

1102
00:46:07,520 --> 00:46:14,480
important one to me which is the

1103
00:46:11,280 --> 00:46:17,760
issue of individual

1104
00:46:14,480 --> 00:46:21,040
accountability i mentioned this up top

1105
00:46:17,760 --> 00:46:25,520
which is that if individuals

1106
00:46:21,040 --> 00:46:27,680
were calling the shots to break the law

1107
00:46:25,520 --> 00:46:29,520
i think they individually need to be

1108
00:46:27,680 --> 00:46:32,078
held accountable

1109
00:46:29,520 --> 00:46:34,160
and there's lots of things that we have

1110
00:46:32,079 --> 00:46:36,240
learned over time about

1111
00:46:34,160 --> 00:46:37,520
the role of individual accountability in

1112
00:46:36,240 --> 00:46:39,919
the board room

1113
00:46:37,520 --> 00:46:42,800
whether it's related to the financial

1114
00:46:39,920 --> 00:46:46,000
fraud of enron or um

1115
00:46:42,800 --> 00:46:49,119
engaged in certain price fixing or

1116
00:46:46,000 --> 00:46:51,680
or overseas bribery i'm holding

1117
00:46:49,119 --> 00:46:53,520
individuals that called the shots to

1118
00:46:51,680 --> 00:46:55,919
break the law

1119
00:46:53,520 --> 00:46:56,560
that just feels like an important part

1120
00:46:55,920 --> 00:47:00,000
of

1121
00:46:56,560 --> 00:47:01,920
regulating commerce and justice and i i

1122
00:47:00,000 --> 00:47:04,400
also want us to

1123
00:47:01,920 --> 00:47:06,560
you ask about consumer harms i want us

1124
00:47:04,400 --> 00:47:09,680
to think about

1125
00:47:06,560 --> 00:47:13,839
the role of take it or

1126
00:47:09,680 --> 00:47:15,598
leave it contracts you know the ftc has

1127
00:47:13,839 --> 00:47:19,279
a pretty storied history

1128
00:47:15,599 --> 00:47:22,839
of banning certain terms

1129
00:47:19,280 --> 00:47:25,599
that are inherently one-sided or

1130
00:47:22,839 --> 00:47:27,599
abused you know market power or

1131
00:47:25,599 --> 00:47:30,160
information asymmetry

1132
00:47:27,599 --> 00:47:33,359
and we now live in a digital world that

1133
00:47:30,160 --> 00:47:34,879
is full of these terms and conditions

1134
00:47:33,359 --> 00:47:38,558
i apologize someone's ringing my

1135
00:47:34,880 --> 00:47:41,359
doorbell uh terms and conditions that

1136
00:47:38,559 --> 00:47:42,400
uh really just are one-sided they can

1137
00:47:41,359 --> 00:47:45,200
change at

1138
00:47:42,400 --> 00:47:46,720
any time um and that just can be

1139
00:47:45,200 --> 00:47:49,839
extractive

1140
00:47:46,720 --> 00:47:52,640
and particularly harm on the least

1141
00:47:49,839 --> 00:47:53,119
fortunate or the least representative in

1142
00:47:52,640 --> 00:47:55,920
our

1143
00:47:53,119 --> 00:47:56,640
represented in our society and so that's

1144
00:47:55,920 --> 00:47:59,440
what i think

1145
00:47:56,640 --> 00:48:01,598
is in some ways a short list of the

1146
00:47:59,440 --> 00:48:04,240
suite of remedies

1147
00:48:01,599 --> 00:48:04,640
we will need to think about and really

1148
00:48:04,240 --> 00:48:08,479
turn

1149
00:48:04,640 --> 00:48:09,839
the page on monetary fines that are a

1150
00:48:08,480 --> 00:48:11,920
big headline

1151
00:48:09,839 --> 00:48:14,558
and that look good but really don't fix

1152
00:48:11,920 --> 00:48:16,960
the problem

1153
00:48:14,559 --> 00:48:18,240
yeah i really appreciate the way that

1154
00:48:16,960 --> 00:48:20,319
you kind of opened up

1155
00:48:18,240 --> 00:48:22,319
those thoughts with your time at the

1156
00:48:20,319 --> 00:48:23,119
consumer finance protection board and

1157
00:48:22,319 --> 00:48:25,759
working on

1158
00:48:23,119 --> 00:48:26,319
student loan debt because it is true

1159
00:48:25,760 --> 00:48:30,000
that

1160
00:48:26,319 --> 00:48:32,000
um seeing facebook get slapped with a

1161
00:48:30,000 --> 00:48:34,640
minor fine that was probably pocket

1162
00:48:32,000 --> 00:48:37,680
change is extremely unsatisfying

1163
00:48:34,640 --> 00:48:38,319
i think to the public and it doesn't

1164
00:48:37,680 --> 00:48:42,000
actually

1165
00:48:38,319 --> 00:48:42,640
change much of the material everyday

1166
00:48:42,000 --> 00:48:44,720
life

1167
00:48:42,640 --> 00:48:46,319
for people who are interacting with

1168
00:48:44,720 --> 00:48:48,720
these systems

1169
00:48:46,319 --> 00:48:50,480
so i guess one of the i'm not going to

1170
00:48:48,720 --> 00:48:51,279
put you on the spot with this i'm just

1171
00:48:50,480 --> 00:48:53,599
going to offer

1172
00:48:51,280 --> 00:48:54,800
this as a provocation or a thing to

1173
00:48:53,599 --> 00:48:58,000
think about

1174
00:48:54,800 --> 00:49:02,720
which is that on one hand

1175
00:48:58,000 --> 00:49:05,920
regulators can do things like

1176
00:49:02,720 --> 00:49:09,040
punish predatory loan providers

1177
00:49:05,920 --> 00:49:12,319
and um create fines

1178
00:49:09,040 --> 00:49:15,119
for uh discriminatory

1179
00:49:12,319 --> 00:49:16,400
practices my colleague chris gilliard

1180
00:49:15,119 --> 00:49:18,240
calls that digital

1181
00:49:16,400 --> 00:49:20,000
redlining right so these kinds of

1182
00:49:18,240 --> 00:49:23,759
digital redlining practices that can

1183
00:49:20,000 --> 00:49:26,400
happen that get encoded into the systems

1184
00:49:23,760 --> 00:49:27,200
you know another set of remedies look

1185
00:49:26,400 --> 00:49:29,359
like

1186
00:49:27,200 --> 00:49:30,558
some type of restorative practice so

1187
00:49:29,359 --> 00:49:34,558
this is where you see

1188
00:49:30,559 --> 00:49:36,240
people who are for example calling upon

1189
00:49:34,559 --> 00:49:38,160
the federal government to wipe out

1190
00:49:36,240 --> 00:49:41,759
student loan debt

1191
00:49:38,160 --> 00:49:43,598
because it is a little bit insufficient

1192
00:49:41,760 --> 00:49:46,000
to have a generation of people who've

1193
00:49:43,599 --> 00:49:47,920
had to pay a hundred thousand dollars

1194
00:49:46,000 --> 00:49:50,319
for a bachelor's degree from a public

1195
00:49:47,920 --> 00:49:51,920
university and then turn around and say

1196
00:49:50,319 --> 00:49:53,680
oh we you know well we're gonna

1197
00:49:51,920 --> 00:49:54,960
lower the interest rate in the future

1198
00:49:53,680 --> 00:49:58,960
for future people

1199
00:49:54,960 --> 00:50:01,280
but your future is ruined sorry

1200
00:49:58,960 --> 00:50:02,960
so i think that there's like a tension

1201
00:50:01,280 --> 00:50:06,319
between

1202
00:50:02,960 --> 00:50:08,960
these issues of um restoration

1203
00:50:06,319 --> 00:50:10,319
reparation that needs to be considered

1204
00:50:08,960 --> 00:50:12,559
by regulators um

1205
00:50:10,319 --> 00:50:14,160
i don't know what your thoughts are

1206
00:50:12,559 --> 00:50:16,079
about that i'm going to give you a

1207
00:50:14,160 --> 00:50:19,040
chance

1208
00:50:16,079 --> 00:50:20,880
yeah no i i think it's a incredibly

1209
00:50:19,040 --> 00:50:22,240
important point and one of the things

1210
00:50:20,880 --> 00:50:24,880
that

1211
00:50:22,240 --> 00:50:27,439
both our law and our system of

1212
00:50:24,880 --> 00:50:32,640
commercial regulations struggle with

1213
00:50:27,440 --> 00:50:32,640
is how do you redress

1214
00:50:32,880 --> 00:50:39,760
the those who clearly were disadvantaged

1215
00:50:37,040 --> 00:50:43,359
by a practice but you can't necessarily

1216
00:50:39,760 --> 00:50:44,800
pinpoint specifically how an individual

1217
00:50:43,359 --> 00:50:47,040
practice hurt them

1218
00:50:44,800 --> 00:50:48,079
you know this is obviously one of the

1219
00:50:47,040 --> 00:50:49,599
things i i

1220
00:50:48,079 --> 00:50:52,160
think about it you you talk about

1221
00:50:49,599 --> 00:50:55,040
student debt and how the broken system

1222
00:50:52,160 --> 00:50:56,078
led to that you know think about just

1223
00:50:55,040 --> 00:50:59,279
how much has

1224
00:50:56,079 --> 00:51:02,559
changed in something you know a

1225
00:50:59,280 --> 00:51:03,040
relatively uh thing that we're now

1226
00:51:02,559 --> 00:51:06,000
getting

1227
00:51:03,040 --> 00:51:07,200
so used to in our daily lives is uh data

1228
00:51:06,000 --> 00:51:09,359
breaches

1229
00:51:07,200 --> 00:51:12,319
you know we used to think about this in

1230
00:51:09,359 --> 00:51:14,720
terms of identity theft

1231
00:51:12,319 --> 00:51:17,359
that you know someone was kind of

1232
00:51:14,720 --> 00:51:19,520
capturing this data so that they could

1233
00:51:17,359 --> 00:51:21,200
you know go to a department store to

1234
00:51:19,520 --> 00:51:23,680
open a credit card

1235
00:51:21,200 --> 00:51:25,200
but now we're actually in such a

1236
00:51:23,680 --> 00:51:27,680
different world where

1237
00:51:25,200 --> 00:51:29,759
in the united states where the biggest

1238
00:51:27,680 --> 00:51:33,839
intrusions of data

1239
00:51:29,760 --> 00:51:36,480
equifax marriott anthem

1240
00:51:33,839 --> 00:51:39,040
these were not done so that people could

1241
00:51:36,480 --> 00:51:42,079
get some money out of your bank account

1242
00:51:39,040 --> 00:51:44,800
this was done for the purpose of state

1243
00:51:42,079 --> 00:51:46,160
craft in true democratic intrusion

1244
00:51:44,800 --> 00:51:49,200
disruption

1245
00:51:46,160 --> 00:51:51,759
cataloging dossiers about

1246
00:51:49,200 --> 00:51:54,319
each of us for the purpose of frankly

1247
00:51:51,760 --> 00:51:57,440
manipulation and doing us harm

1248
00:51:54,319 --> 00:52:00,960
how do you redress that

1249
00:51:57,440 --> 00:52:03,040
uh credit monitoring doesn't do that uh

1250
00:52:00,960 --> 00:52:05,280
and and that's just a minor piece think

1251
00:52:03,040 --> 00:52:06,480
about the much broader harms i think

1252
00:52:05,280 --> 00:52:09,119
also to

1253
00:52:06,480 --> 00:52:09,599
economic opportunity in businesses how

1254
00:52:09,119 --> 00:52:13,440
do

1255
00:52:09,599 --> 00:52:16,480
you redress the business

1256
00:52:13,440 --> 00:52:18,640
that got choked off before it even got

1257
00:52:16,480 --> 00:52:19,839
off the ground because of these market

1258
00:52:18,640 --> 00:52:23,040
structures

1259
00:52:19,839 --> 00:52:27,119
how do you redress the

1260
00:52:23,040 --> 00:52:29,599
firm that lost out on so much business

1261
00:52:27,119 --> 00:52:31,839
uh and because they were playing

1262
00:52:29,599 --> 00:52:33,760
honestly and there was one actor that

1263
00:52:31,839 --> 00:52:36,000
was lying and cheating

1264
00:52:33,760 --> 00:52:36,800
um we have not figured out how to do

1265
00:52:36,000 --> 00:52:39,760
that and we

1266
00:52:36,800 --> 00:52:40,160
i think we need to we need to figure out

1267
00:52:39,760 --> 00:52:44,240
how

1268
00:52:40,160 --> 00:52:44,799
they might be uh in in terms of remedies

1269
00:52:44,240 --> 00:52:47,118
and and

1270
00:52:44,800 --> 00:52:48,480
the the specific laws we have they're

1271
00:52:47,119 --> 00:52:52,000
mixed on this

1272
00:52:48,480 --> 00:52:55,280
to what extent should they then have to

1273
00:52:52,000 --> 00:52:56,480
license uh you know their discoveries

1274
00:52:55,280 --> 00:52:58,960
and technologies

1275
00:52:56,480 --> 00:53:00,480
to what extent should they have to

1276
00:52:58,960 --> 00:53:03,119
forfeit

1277
00:53:00,480 --> 00:53:04,480
a lot of their illegal profits to the

1278
00:53:03,119 --> 00:53:06,720
public

1279
00:53:04,480 --> 00:53:09,359
and to what extent do we use some of

1280
00:53:06,720 --> 00:53:12,000
that to rebuild

1281
00:53:09,359 --> 00:53:14,000
some of the public's capabilities in

1282
00:53:12,000 --> 00:53:17,040
delivering those services and

1283
00:53:14,000 --> 00:53:19,839
and frankly research and insights

1284
00:53:17,040 --> 00:53:20,880
to the broader society so you're right

1285
00:53:19,839 --> 00:53:24,160
this is

1286
00:53:20,880 --> 00:53:26,640
something that is a tough nut to crack

1287
00:53:24,160 --> 00:53:28,799
um in the law enforcement context but

1288
00:53:26,640 --> 00:53:30,640
it's something that it's a real struggle

1289
00:53:28,800 --> 00:53:33,359
for me every day

1290
00:53:30,640 --> 00:53:35,040
when i see us doing sometimes these no

1291
00:53:33,359 --> 00:53:37,279
money settlements

1292
00:53:35,040 --> 00:53:38,720
even though clearly every honest

1293
00:53:37,280 --> 00:53:43,599
competitor

1294
00:53:38,720 --> 00:53:45,759
um you know lost out

1295
00:53:43,599 --> 00:53:47,359
yeah i mean this is the challenge and i

1296
00:53:45,760 --> 00:53:49,280
think again when we think about our

1297
00:53:47,359 --> 00:53:52,799
relationship between

1298
00:53:49,280 --> 00:53:56,000
um the tech sector and the public

1299
00:53:52,800 --> 00:53:58,559
it's really um difficult because you

1300
00:53:56,000 --> 00:54:00,720
know we are in a place where for example

1301
00:53:58,559 --> 00:54:02,000
people have no idea what their data

1302
00:54:00,720 --> 00:54:03,520
profile is

1303
00:54:02,000 --> 00:54:05,280
and they don't even know what it is

1304
00:54:03,520 --> 00:54:08,480
entirely that's been breached

1305
00:54:05,280 --> 00:54:10,400
i heard a high-level um federal law

1306
00:54:08,480 --> 00:54:12,720
enforcement person say once

1307
00:54:10,400 --> 00:54:13,760
at a conference that in the eyes of the

1308
00:54:12,720 --> 00:54:16,879
government

1309
00:54:13,760 --> 00:54:18,000
the public is we are as individuals our

1310
00:54:16,880 --> 00:54:21,040
data profile

1311
00:54:18,000 --> 00:54:23,520
and the question is what is that

1312
00:54:21,040 --> 00:54:24,800
and you know of course i love seeing um

1313
00:54:23,520 --> 00:54:28,240
you know john oliver

1314
00:54:24,800 --> 00:54:29,599
i think um did the great uh uh comedic

1315
00:54:28,240 --> 00:54:31,680
sketch on

1316
00:54:29,599 --> 00:54:33,520
trying to fix our credit scores and how

1317
00:54:31,680 --> 00:54:36,078
impossible that is right

1318
00:54:33,520 --> 00:54:37,920
it's it's fine if you've got over an 800

1319
00:54:36,079 --> 00:54:38,559
score if you're anybody who doesn't have

1320
00:54:37,920 --> 00:54:42,960
that

1321
00:54:38,559 --> 00:54:46,160
uh what so you think about

1322
00:54:42,960 --> 00:54:47,920
projects like uh you know that really

1323
00:54:46,160 --> 00:54:49,359
drive the internet the thousands and

1324
00:54:47,920 --> 00:54:51,680
thousands and thousands of

1325
00:54:49,359 --> 00:54:53,200
data brokers and sellers that are

1326
00:54:51,680 --> 00:54:55,359
remixing and remaking

1327
00:54:53,200 --> 00:54:57,359
all kinds of data about us every place

1328
00:54:55,359 --> 00:54:59,759
we've been the searches we do

1329
00:54:57,359 --> 00:55:00,400
the things we like um our medical

1330
00:54:59,760 --> 00:55:03,520
records

1331
00:55:00,400 --> 00:55:07,200
all kind you know our our um gps

1332
00:55:03,520 --> 00:55:10,319
coordinates um our ovulation

1333
00:55:07,200 --> 00:55:12,640
um all the things right that go into uh

1334
00:55:10,319 --> 00:55:13,920
making a data profile and i wonder if

1335
00:55:12,640 --> 00:55:17,118
what your thoughts might be

1336
00:55:13,920 --> 00:55:18,079
about whether the public might be able

1337
00:55:17,119 --> 00:55:21,200
to

1338
00:55:18,079 --> 00:55:22,640
um have some kind of digital amnesty

1339
00:55:21,200 --> 00:55:23,680
these are things that kind of kind of

1340
00:55:22,640 --> 00:55:24,720
things i'm thinking about and

1341
00:55:23,680 --> 00:55:27,520
researching right now

1342
00:55:24,720 --> 00:55:28,959
right like what if your like your

1343
00:55:27,520 --> 00:55:31,280
juvenile record

1344
00:55:28,960 --> 00:55:33,359
might be sealed because the things you

1345
00:55:31,280 --> 00:55:35,040
did as a kid shouldn't follow you into

1346
00:55:33,359 --> 00:55:37,598
your adulthood

1347
00:55:35,040 --> 00:55:39,680
is it plausible that we should have a

1348
00:55:37,599 --> 00:55:42,799
kind of digital amnesty or

1349
00:55:39,680 --> 00:55:44,879
a way to extract all of these profiles

1350
00:55:42,799 --> 00:55:46,960
that are being made out of these systems

1351
00:55:44,880 --> 00:55:48,079
um because you know of course i think

1352
00:55:46,960 --> 00:55:50,319
about this i've got a

1353
00:55:48,079 --> 00:55:51,280
young son and his profile is already

1354
00:55:50,319 --> 00:55:53,040
being set and

1355
00:55:51,280 --> 00:55:55,200
will he be predicted into the kind of

1356
00:55:53,040 --> 00:55:58,720
future we hope for him

1357
00:55:55,200 --> 00:56:02,078
we don't know yeah you know i

1358
00:55:58,720 --> 00:56:04,799
i i want to start by

1359
00:56:02,079 --> 00:56:06,880
i'm so glad you mentioned the credit

1360
00:56:04,799 --> 00:56:09,520
reporting system

1361
00:56:06,880 --> 00:56:10,640
so we're at the 50th i think 50th

1362
00:56:09,520 --> 00:56:12,880
anniversary

1363
00:56:10,640 --> 00:56:13,680
of the fair credit reporting act which

1364
00:56:12,880 --> 00:56:16,319
is

1365
00:56:13,680 --> 00:56:17,200
essentially the u.s law that's that

1366
00:56:16,319 --> 00:56:21,040
attempted

1367
00:56:17,200 --> 00:56:24,160
to um really clean up a very

1368
00:56:21,040 --> 00:56:26,720
very underhanded industry i mean i think

1369
00:56:24,160 --> 00:56:27,839
the origins of companies like equifax

1370
00:56:26,720 --> 00:56:29,839
was actually

1371
00:56:27,839 --> 00:56:31,920
you know some investigators that would

1372
00:56:29,839 --> 00:56:32,960
collect rumors on you and put it in a

1373
00:56:31,920 --> 00:56:36,480
file

1374
00:56:32,960 --> 00:56:39,200
um and the concept of

1375
00:56:36,480 --> 00:56:39,599
the fair credit reporting act was well

1376
00:56:39,200 --> 00:56:42,720
let's

1377
00:56:39,599 --> 00:56:45,760
let people look at what

1378
00:56:42,720 --> 00:56:49,359
is being collected on them let's let

1379
00:56:45,760 --> 00:56:50,960
people dispute uh things that they don't

1380
00:56:49,359 --> 00:56:53,200
think are right

1381
00:56:50,960 --> 00:56:56,240
let's make sure that the companies use

1382
00:56:53,200 --> 00:56:58,160
some sort of accuracy standards

1383
00:56:56,240 --> 00:57:00,160
i think one of the lessons of the fair

1384
00:56:58,160 --> 00:57:03,359
credit reporting act is that

1385
00:57:00,160 --> 00:57:06,520
it never really fundamentally addressed

1386
00:57:03,359 --> 00:57:10,558
the business model which is that

1387
00:57:06,520 --> 00:57:14,480
consumers are not the customer

1388
00:57:10,559 --> 00:57:16,000
they are the product credit reporting

1389
00:57:14,480 --> 00:57:18,720
agencies

1390
00:57:16,000 --> 00:57:20,559
were not really selling things to

1391
00:57:18,720 --> 00:57:23,759
consumers and families

1392
00:57:20,559 --> 00:57:26,799
they were selling you know

1393
00:57:23,760 --> 00:57:29,839
secrets about families

1394
00:57:26,799 --> 00:57:33,359
to businesses and in some ways that's

1395
00:57:29,839 --> 00:57:35,520
also the model of today's

1396
00:57:33,359 --> 00:57:36,960
technology industry in many ways as a

1397
00:57:35,520 --> 00:57:40,000
huge amount

1398
00:57:36,960 --> 00:57:43,280
of the valuation is driven by

1399
00:57:40,000 --> 00:57:45,760
not selling things directly to consumers

1400
00:57:43,280 --> 00:57:48,240
but charging them for services in the

1401
00:57:45,760 --> 00:57:51,119
forms of their data

1402
00:57:48,240 --> 00:57:52,479
but really their main business is to

1403
00:57:51,119 --> 00:57:56,000
sell

1404
00:57:52,480 --> 00:57:59,680
consumer data and also importantly

1405
00:57:56,000 --> 00:58:02,000
consumer behavior

1406
00:57:59,680 --> 00:58:03,118
to commercial entity entities their p

1407
00:58:02,000 --> 00:58:05,440
commercial entities

1408
00:58:03,119 --> 00:58:07,040
purchase consumer behavior in the form

1409
00:58:05,440 --> 00:58:09,680
of clicking on things

1410
00:58:07,040 --> 00:58:11,440
engaging on things so that means that

1411
00:58:09,680 --> 00:58:12,799
they have just like the credit reporting

1412
00:58:11,440 --> 00:58:14,880
agencies

1413
00:58:12,799 --> 00:58:17,440
these firms also have a business

1414
00:58:14,880 --> 00:58:18,880
incentive to engage in these digital

1415
00:58:17,440 --> 00:58:21,520
drag nets

1416
00:58:18,880 --> 00:58:22,000
to collect as much information about you

1417
00:58:21,520 --> 00:58:26,240
as they

1418
00:58:22,000 --> 00:58:28,240
can so that they can monetize you more

1419
00:58:26,240 --> 00:58:29,520
and i think there's some very serious

1420
00:58:28,240 --> 00:58:33,118
questions

1421
00:58:29,520 --> 00:58:35,440
about how that is weaponized

1422
00:58:33,119 --> 00:58:37,760
you know we do know that foreign

1423
00:58:35,440 --> 00:58:41,440
interference and manipulation

1424
00:58:37,760 --> 00:58:44,880
is used to create racial division

1425
00:58:41,440 --> 00:58:45,760
um disrupt kind of core democratic

1426
00:58:44,880 --> 00:58:47,920
processes

1427
00:58:45,760 --> 00:58:49,680
and so i think you're right that we're

1428
00:58:47,920 --> 00:58:50,240
going to have to ask some of those hard

1429
00:58:49,680 --> 00:58:53,359
questions

1430
00:58:50,240 --> 00:58:54,959
but i hope we also do it by not just

1431
00:58:53,359 --> 00:58:55,759
saying you get to look at what people

1432
00:58:54,960 --> 00:58:58,720
have on you

1433
00:58:55,760 --> 00:59:00,559
and maybe you get to delete it i'm a

1434
00:58:58,720 --> 00:59:02,879
little skeptical this stuff can even be

1435
00:59:00,559 --> 00:59:06,400
deleted in the first place

1436
00:59:02,880 --> 00:59:08,480
but i think looking at the core business

1437
00:59:06,400 --> 00:59:11,920
incentives that drive the harms

1438
00:59:08,480 --> 00:59:13,839
is just so fundamentally critical

1439
00:59:11,920 --> 00:59:15,920
you're absolutely right i mean i could

1440
00:59:13,839 --> 00:59:18,160
talk to you for at least another hour

1441
00:59:15,920 --> 00:59:21,359
about this and i know we're we're

1442
00:59:18,160 --> 00:59:24,399
out of time but i want to say that

1443
00:59:21,359 --> 00:59:27,759
you know commissioner chopra you've been

1444
00:59:24,400 --> 00:59:30,559
such a fierce advocate for so many

1445
00:59:27,760 --> 00:59:32,160
of the important issues in the federal

1446
00:59:30,559 --> 00:59:36,240
trade commission i'm really thrilled

1447
00:59:32,160 --> 00:59:36,799
that the ftc is is thinking seriously

1448
00:59:36,240 --> 00:59:39,359
about

1449
00:59:36,799 --> 00:59:40,880
these kinds of issues you know i wrote

1450
00:59:39,359 --> 00:59:43,839
in my book that i think that

1451
00:59:40,880 --> 00:59:45,520
um algorithms and artificial

1452
00:59:43,839 --> 00:59:47,359
intelligence really will become the

1453
00:59:45,520 --> 00:59:50,079
major human rights issue

1454
00:59:47,359 --> 00:59:51,839
of the 21st century and we see of course

1455
00:59:50,079 --> 00:59:53,119
all the civil and human rights

1456
00:59:51,839 --> 00:59:54,960
implications

1457
00:59:53,119 --> 00:59:56,240
um that stem from the things that we've

1458
00:59:54,960 --> 00:59:58,799
been talking about today

1459
00:59:56,240 --> 01:00:00,000
so i just want to say thank you for

1460
00:59:58,799 --> 01:00:01,759
giving me an opportunity

1461
01:00:00,000 --> 01:00:03,200
to have a conversation with you today

1462
01:00:01,760 --> 01:00:06,000
it's really a pleasure

1463
01:00:03,200 --> 01:00:07,759
and um i hope that your work will

1464
01:00:06,000 --> 01:00:08,480
continue to have the kind of important

1465
01:00:07,760 --> 01:00:11,520
impact

1466
01:00:08,480 --> 01:00:14,799
in the world that we need well thank you

1467
01:00:11,520 --> 01:00:18,160
so much professor noble and i i admire

1468
01:00:14,799 --> 01:00:21,040
so many of you who are trying to rethink

1469
01:00:18,160 --> 01:00:22,319
and challenge our assumptions about our

1470
01:00:21,040 --> 01:00:24,720
relationship

1471
01:00:22,319 --> 01:00:26,319
with large technology firms in

1472
01:00:24,720 --> 01:00:28,319
particular and how

1473
01:00:26,319 --> 01:00:30,880
how we might think about that kind of

1474
01:00:28,319 --> 01:00:31,200
future to protect ourselves our families

1475
01:00:30,880 --> 01:00:34,000
our

1476
01:00:31,200 --> 01:00:35,439
our communities our our our nations our

1477
01:00:34,000 --> 01:00:38,720
societies um

1478
01:00:35,440 --> 01:00:41,440
it it the time is now to do it um

1479
01:00:38,720 --> 01:00:43,279
and a global emergency like this has to

1480
01:00:41,440 --> 01:00:44,559
make us rethink so much of the

1481
01:00:43,280 --> 01:00:46,559
assumptions

1482
01:00:44,559 --> 01:00:47,920
of our markets and and i encourage all

1483
01:00:46,559 --> 01:00:50,799
of you

1484
01:00:47,920 --> 01:00:53,040
whether you work in some of these firms

1485
01:00:50,799 --> 01:00:55,119
whether you are trying to start a firm

1486
01:00:53,040 --> 01:00:56,319
that you want to go big or you are

1487
01:00:55,119 --> 01:00:58,640
advocating

1488
01:00:56,319 --> 01:01:00,720
um we need everyone's voices in this

1489
01:00:58,640 --> 01:01:02,240
debate we need there to be higher levels

1490
01:01:00,720 --> 01:01:05,279
of accountability

1491
01:01:02,240 --> 01:01:08,240
and ultimately build toward changing

1492
01:01:05,280 --> 01:01:10,160
um our markets in ways that are really

1493
01:01:08,240 --> 01:01:10,479
working for people so thank you to you

1494
01:01:10,160 --> 01:01:12,799
and

1495
01:01:10,480 --> 01:01:14,000
thank you to access now and and to

1496
01:01:12,799 --> 01:01:16,640
everyone who

1497
01:01:14,000 --> 01:01:19,280
who is engaging and thinking listening

1498
01:01:16,640 --> 01:01:19,279
and speaking

1499
01:01:19,760 --> 01:01:25,839
great great thanks everyone for coming

1500
01:01:22,960 --> 01:01:25,839
we appreciate you

1501
01:03:14,400 --> 01:03:16,480
you

