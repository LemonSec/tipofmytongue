1
00:00:08,639 --> 00:00:09,760
hello everyone

2
00:00:09,760 --> 00:00:11,759
great pleasure to be here my name is

3
00:00:11,759 --> 00:00:14,320
sang hong and i'm a phd candidate at the

4
00:00:14,320 --> 00:00:16,480
university of maryland college pop

5
00:00:16,480 --> 00:00:18,560
today i'm here to introduce one of the

6
00:00:18,560 --> 00:00:20,160
emerging security challenges that

7
00:00:20,160 --> 00:00:21,920
machine learning faces

8
00:00:21,920 --> 00:00:24,080
particularly on how an adversary can

9
00:00:24,080 --> 00:00:25,599
exploit weak hardware level

10
00:00:25,599 --> 00:00:26,560
vulnerabilities

11
00:00:26,560 --> 00:00:31,840
to jeopardize machine learning systems

12
00:00:31,840 --> 00:00:34,640
so all of us learn from our ancestors

13
00:00:34,640 --> 00:00:35,600
and so do i

14
00:00:35,600 --> 00:00:38,480
because oftentimes we know our ancestors

15
00:00:38,480 --> 00:00:40,239
provide the wisdom that can solve many

16
00:00:40,239 --> 00:00:41,680
contemporary problems

17
00:00:41,680 --> 00:00:44,239
like this quote from dr tallis a sound

18
00:00:44,239 --> 00:00:45,840
mind in a sound body

19
00:00:45,840 --> 00:00:47,600
that emphasizes the importance of the

20
00:00:47,600 --> 00:00:49,840
physical well-being for psychological

21
00:00:49,840 --> 00:00:51,199
well-being

22
00:00:51,199 --> 00:00:53,760
and in recent years machine learning has

23
00:00:53,760 --> 00:00:56,399
moved from research labs to production

24
00:00:56,399 --> 00:00:58,160
therefore the security of systems that

25
00:00:58,160 --> 00:00:59,840
include machine learning models

26
00:00:59,840 --> 00:01:02,320
increasingly become important and this

27
00:01:02,320 --> 00:01:04,080
makes many researchers and engineers

28
00:01:04,080 --> 00:01:04,799
think about

29
00:01:04,799 --> 00:01:06,640
what are potential tech services in

30
00:01:06,640 --> 00:01:08,159
machine learning

31
00:01:08,159 --> 00:01:10,560
so to answer this question research in

32
00:01:10,560 --> 00:01:11,920
the field of adversarial machine

33
00:01:11,920 --> 00:01:14,400
learning studies the attack services

34
00:01:14,400 --> 00:01:17,360
such as training data an adversary can

35
00:01:17,360 --> 00:01:19,200
manipulate the machine learning models

36
00:01:19,200 --> 00:01:20,159
and behaviors

37
00:01:20,159 --> 00:01:21,920
by feeding malicious samples during its

38
00:01:21,920 --> 00:01:23,360
training and

39
00:01:23,360 --> 00:01:25,360
once attacked the model becomes like

40
00:01:25,360 --> 00:01:27,280
this microsoft to check of tay

41
00:01:27,280 --> 00:01:29,600
that turned into a racist within 24

42
00:01:29,600 --> 00:01:31,680
hours of his release by learning bad

43
00:01:31,680 --> 00:01:33,040
tweets

44
00:01:33,040 --> 00:01:35,119
and another threat is the adversary of

45
00:01:35,119 --> 00:01:36,240
examples

46
00:01:36,240 --> 00:01:37,920
that aims to manipulate the model's

47
00:01:37,920 --> 00:01:39,759
prediction by adding imperceptible

48
00:01:39,759 --> 00:01:41,840
perturbations to the inputs

49
00:01:41,840 --> 00:01:43,840
and you can see the most famous panda in

50
00:01:43,840 --> 00:01:45,680
the world that turned into gibbon

51
00:01:45,680 --> 00:01:47,680
when we add small amount of noise

52
00:01:47,680 --> 00:01:49,920
crafted for this misprediction

53
00:01:49,920 --> 00:01:52,399
so by studying those tech surfaces we

54
00:01:52,399 --> 00:01:54,320
come up with the certified defenses

55
00:01:54,320 --> 00:01:56,079
which provide the mathematically

56
00:01:56,079 --> 00:01:57,680
provable guarantees

57
00:01:57,680 --> 00:02:01,439
such as data sanitization or robust

58
00:02:01,439 --> 00:02:03,840
static sticks against the poisoning or

59
00:02:03,840 --> 00:02:05,360
adversary training

60
00:02:05,360 --> 00:02:06,880
or randomized smoothing against the

61
00:02:06,880 --> 00:02:09,598
adversary samples

62
00:02:09,598 --> 00:02:12,080
so now can we say that our models are

63
00:02:12,080 --> 00:02:13,440
safe

64
00:02:13,440 --> 00:02:15,840
let's revisit what dr charles said a

65
00:02:15,840 --> 00:02:16,720
sound mind

66
00:02:16,720 --> 00:02:19,360
and a sound body and we soon realized

67
00:02:19,360 --> 00:02:19,840
there is

68
00:02:19,840 --> 00:02:22,560
still a question remains unanswered do

69
00:02:22,560 --> 00:02:24,000
we place a sound mind

70
00:02:24,000 --> 00:02:27,680
in a sound body answering this question

71
00:02:27,680 --> 00:02:28,959
requires to break

72
00:02:28,959 --> 00:02:30,720
convention and breaking convention

73
00:02:30,720 --> 00:02:32,560
requires a change in our perspective of

74
00:02:32,560 --> 00:02:34,400
looking at the problem

75
00:02:34,400 --> 00:02:36,480
so the prior work mostly studied the

76
00:02:36,480 --> 00:02:38,400
security of a machine learning model

77
00:02:38,400 --> 00:02:40,239
considering that as a standalone

78
00:02:40,239 --> 00:02:41,680
mathematical concept

79
00:02:41,680 --> 00:02:44,319
and in consequence we study the model's

80
00:02:44,319 --> 00:02:46,720
robustness in an isolated manner

81
00:02:46,720 --> 00:02:49,200
and more importantly this perspective

82
00:02:49,200 --> 00:02:50,000
often overlooks

83
00:02:50,000 --> 00:02:51,920
the impact of the hardware or system

84
00:02:51,920 --> 00:02:54,239
level vulnerabilities

85
00:02:54,239 --> 00:02:57,360
so let's think about this problem once

86
00:02:57,360 --> 00:02:59,200
we train machine learning models

87
00:02:59,200 --> 00:03:01,200
we want to deploy them to the real-world

88
00:03:01,200 --> 00:03:03,360
systems such as self-driving cars

89
00:03:03,360 --> 00:03:06,400
iot devices or cloud services

90
00:03:06,400 --> 00:03:09,599
and for that we require hardware and

91
00:03:09,599 --> 00:03:11,920
supporting infrastructure such as cpus

92
00:03:11,920 --> 00:03:14,959
gpus and fpgas and machine learning

93
00:03:14,959 --> 00:03:17,680
frameworks such as pytorch tensorflow or

94
00:03:17,680 --> 00:03:18,480
objects

95
00:03:18,480 --> 00:03:20,560
or databases and many other software

96
00:03:20,560 --> 00:03:21,519
components

97
00:03:21,519 --> 00:03:23,599
and each of those components can be

98
00:03:23,599 --> 00:03:26,000
subject to absorb pressure

99
00:03:26,000 --> 00:03:28,239
so we must consider those new attack

100
00:03:28,239 --> 00:03:30,000
surfaces and examine how machine

101
00:03:30,000 --> 00:03:31,040
learning can become

102
00:03:31,040 --> 00:03:33,760
vulnerable by exploiting them and for

103
00:03:33,760 --> 00:03:34,560
that

104
00:03:34,560 --> 00:03:36,560
here i propose a new perspective of

105
00:03:36,560 --> 00:03:39,519
looking at machine learning models

106
00:03:39,519 --> 00:03:41,519
i encourage the community to look at

107
00:03:41,519 --> 00:03:42,799
machine learning models

108
00:03:42,799 --> 00:03:44,879
as another computational tool or

109
00:03:44,879 --> 00:03:46,560
software running in many computer

110
00:03:46,560 --> 00:03:47,519
systems

111
00:03:47,519 --> 00:03:50,239
and from this unique perspective we can

112
00:03:50,239 --> 00:03:51,519
easily imagine that

113
00:03:51,519 --> 00:03:53,120
machine learning models may become

114
00:03:53,120 --> 00:03:54,959
vulnerable to existing hardware or

115
00:03:54,959 --> 00:03:56,959
software level vulnerabilities

116
00:03:56,959 --> 00:03:59,680
and more importantly those models may

117
00:03:59,680 --> 00:04:01,599
have computational properties

118
00:04:01,599 --> 00:04:04,159
that traditional tools do not have which

119
00:04:04,159 --> 00:04:06,080
makes them particularly more vulnerable

120
00:04:06,080 --> 00:04:07,519
to hardware or system level

121
00:04:07,519 --> 00:04:09,280
vulnerabilities

122
00:04:09,280 --> 00:04:12,480
so in this talk we focus our scope on

123
00:04:12,480 --> 00:04:15,360
hardware level vulnerabilities

124
00:04:15,360 --> 00:04:17,600
and what makes hardware attacks

125
00:04:17,600 --> 00:04:18,639
interesting

126
00:04:18,639 --> 00:04:20,320
is that those attacks can break

127
00:04:20,320 --> 00:04:21,918
mathematically proven security

128
00:04:21,918 --> 00:04:23,040
guarantees

129
00:04:23,040 --> 00:04:25,360
and in fact prior work shows the

130
00:04:25,360 --> 00:04:26,400
exploitation of

131
00:04:26,400 --> 00:04:27,680
hardware attacks against the

132
00:04:27,680 --> 00:04:30,000
cryptography such as faulty injection

133
00:04:30,000 --> 00:04:30,560
attacks

134
00:04:30,560 --> 00:04:33,040
or side channel attacks but it is

135
00:04:33,040 --> 00:04:34,720
important to know that this is not

136
00:04:34,720 --> 00:04:36,880
because crypto algorithms are we

137
00:04:36,880 --> 00:04:39,360
is because hardware attacks were outside

138
00:04:39,360 --> 00:04:40,840
of their threat

139
00:04:40,840 --> 00:04:43,919
models but one might say

140
00:04:43,919 --> 00:04:45,759
isn't it difficult to perform hardware

141
00:04:45,759 --> 00:04:47,520
attacks on local machines

142
00:04:47,520 --> 00:04:49,759
you need to break network defenses and

143
00:04:49,759 --> 00:04:51,600
to let the victim download and run your

144
00:04:51,600 --> 00:04:52,880
malicious software when you're

145
00:04:52,880 --> 00:04:54,479
exploiting it

146
00:04:54,479 --> 00:04:56,800
then there's an environment where an

147
00:04:56,800 --> 00:04:59,360
adversary may not face the challenges

148
00:04:59,360 --> 00:05:02,880
the cloud so because of this reason many

149
00:05:02,880 --> 00:05:05,199
exploitations of hardware attacks has

150
00:05:05,199 --> 00:05:06,880
shown in the cloud

151
00:05:06,880 --> 00:05:08,400
and there are clear benefits the

152
00:05:08,400 --> 00:05:10,560
attacker can achieve in the cloud

153
00:05:10,560 --> 00:05:12,800
the first is the collocation in the

154
00:05:12,800 --> 00:05:15,199
cloud there is a chance that an attacker

155
00:05:15,199 --> 00:05:16,880
and a victim's virtual machines

156
00:05:16,880 --> 00:05:19,759
share the same hardware such as cpus or

157
00:05:19,759 --> 00:05:20,560
memories

158
00:05:20,560 --> 00:05:23,360
and second with the collocated machine

159
00:05:23,360 --> 00:05:25,199
the attacker can remotely exploit

160
00:05:25,199 --> 00:05:27,199
hardware attacks without the access to

161
00:05:27,199 --> 00:05:29,360
the virtual machines

162
00:05:29,360 --> 00:05:31,600
however in the meantime we know that the

163
00:05:31,600 --> 00:05:33,440
cloud providers try to make their

164
00:05:33,440 --> 00:05:35,600
services secure against those hardware

165
00:05:35,600 --> 00:05:36,320
threats

166
00:05:36,320 --> 00:05:38,720
so the strong attackers who have

167
00:05:38,720 --> 00:05:40,479
fine-grained control over their

168
00:05:40,479 --> 00:05:41,360
behaviors

169
00:05:41,360 --> 00:05:43,759
becomes more and more infeasible and

170
00:05:43,759 --> 00:05:44,400
indeed

171
00:05:44,400 --> 00:05:46,320
it will be challenging for raw hammer

172
00:05:46,320 --> 00:05:47,840
attacker for example

173
00:05:47,840 --> 00:05:50,080
to flip a specific bit in the victim's

174
00:05:50,080 --> 00:05:52,400
memory because of the defense primitives

175
00:05:52,400 --> 00:05:54,880
deployed by the providers

176
00:05:54,880 --> 00:05:57,680
so in the end weak hardware attacks

177
00:05:57,680 --> 00:05:58,479
remain

178
00:05:58,479 --> 00:06:01,440
which is usually considered to useless

179
00:06:01,440 --> 00:06:04,400
to cause serious damage to a system

180
00:06:04,400 --> 00:06:07,440
however my question is is it true

181
00:06:07,440 --> 00:06:09,280
is it true that the weak attacks are

182
00:06:09,280 --> 00:06:11,440
always remain weak

183
00:06:11,440 --> 00:06:13,440
in the rest of my talk i will answer

184
00:06:13,440 --> 00:06:15,600
this question how vulnerable are machine

185
00:06:15,600 --> 00:06:16,639
learning models

186
00:06:16,639 --> 00:06:18,800
the weak hardware attacks i will

187
00:06:18,800 --> 00:06:21,120
introduce our research on the emerging

188
00:06:21,120 --> 00:06:23,199
practical threat caused by weak hardware

189
00:06:23,199 --> 00:06:23,919
attacks

190
00:06:23,919 --> 00:06:26,080
and in particular i'd like to show that

191
00:06:26,080 --> 00:06:27,280
the models have

192
00:06:27,280 --> 00:06:29,039
properties that conventional software

193
00:06:29,039 --> 00:06:31,280
does not have which can turn weak

194
00:06:31,280 --> 00:06:31,759
attacks

195
00:06:31,759 --> 00:06:33,600
into significant threats to machine

196
00:06:33,600 --> 00:06:35,199
learning systems

197
00:06:35,199 --> 00:06:37,199
so let's first look at roe hammer a

198
00:06:37,199 --> 00:06:40,639
well-studied faulty injection attack

199
00:06:40,639 --> 00:06:43,520
so let's start from this in 1990 a

200
00:06:43,520 --> 00:06:45,759
computer scientist that dr yan lecon

201
00:06:45,759 --> 00:06:47,520
published the seminal paper

202
00:06:47,520 --> 00:06:50,240
titled optimal brain damage and this is

203
00:06:50,240 --> 00:06:52,240
the first work that reported the grease

204
00:06:52,240 --> 00:06:52,560
the

205
00:06:52,560 --> 00:06:55,280
graceful degradation property by showing

206
00:06:55,280 --> 00:06:55,840
that

207
00:06:55,840 --> 00:06:58,160
one can remove sixty percent of model

208
00:06:58,160 --> 00:07:00,560
parameters without the accuracy drop

209
00:07:00,560 --> 00:07:02,720
and immediately after that researchers

210
00:07:02,720 --> 00:07:04,479
came up with a bunch of techniques based

211
00:07:04,479 --> 00:07:05,440
on this property

212
00:07:05,440 --> 00:07:07,280
such as pruning to reduce the

213
00:07:07,280 --> 00:07:08,639
competition of cost

214
00:07:08,639 --> 00:07:11,199
quantization to reduce the metric size

215
00:07:11,199 --> 00:07:13,280
and even blending noises to improve the

216
00:07:13,280 --> 00:07:14,319
robustness

217
00:07:14,319 --> 00:07:16,240
and all these techniques are possible

218
00:07:16,240 --> 00:07:18,400
with the negligible accuracy drop within

219
00:07:18,400 --> 00:07:20,160
one percent

220
00:07:20,160 --> 00:07:23,120
and in security prior work also show

221
00:07:23,120 --> 00:07:23,520
that

222
00:07:23,520 --> 00:07:26,240
even if you try to degrade the accuracy

223
00:07:26,240 --> 00:07:27,840
it is very challenging

224
00:07:27,840 --> 00:07:29,680
they blend a lot of poisoning samples

225
00:07:29,680 --> 00:07:31,120
into the training data

226
00:07:31,120 --> 00:07:33,520
cause a lot of random bit errors or

227
00:07:33,520 --> 00:07:35,440
cause a hardware for randomly

228
00:07:35,440 --> 00:07:37,360
but the maximum accuracy drop that we

229
00:07:37,360 --> 00:07:38,560
can observe was

230
00:07:38,560 --> 00:07:42,080
11 and all these practices gives us a

231
00:07:42,080 --> 00:07:42,639
false

232
00:07:42,639 --> 00:07:45,199
sense of security about the neural

233
00:07:45,199 --> 00:07:47,039
network resilience against the

234
00:07:47,039 --> 00:07:49,919
perturbations

235
00:07:50,160 --> 00:07:52,479
it's because they focused on the best

236
00:07:52,479 --> 00:07:54,560
case or the average case perturbation

237
00:07:54,560 --> 00:07:55,520
scenarios

238
00:07:55,520 --> 00:07:57,840
not the worst case one then that an

239
00:07:57,840 --> 00:07:59,919
adversary can cause

240
00:07:59,919 --> 00:08:01,599
so let's look at what's happening in

241
00:08:01,599 --> 00:08:04,319
detail so here we display the standard

242
00:08:04,319 --> 00:08:06,560
four layer convolutional neural network

243
00:08:06,560 --> 00:08:08,479
which has two convolutional layers and

244
00:08:08,479 --> 00:08:10,400
two fully connected layers trained on

245
00:08:10,400 --> 00:08:11,360
amnest

246
00:08:11,360 --> 00:08:13,599
and the network has model parameters

247
00:08:13,599 --> 00:08:15,440
described as blue boxes

248
00:08:15,440 --> 00:08:18,160
optimized in training and are used in

249
00:08:18,160 --> 00:08:19,759
inferences to classify the data

250
00:08:19,759 --> 00:08:20,639
correctly

251
00:08:20,639 --> 00:08:23,919
of course it has an accuracy of 99

252
00:08:23,919 --> 00:08:26,400
so once the model is loaded this is how

253
00:08:26,400 --> 00:08:28,479
the parameters are located in the memory

254
00:08:28,479 --> 00:08:30,160
while the network in use

255
00:08:30,160 --> 00:08:32,080
so you can see each of weights and

256
00:08:32,080 --> 00:08:34,080
biases take off the continuous address

257
00:08:34,080 --> 00:08:34,719
space

258
00:08:34,719 --> 00:08:36,479
and the size of those parameters in

259
00:08:36,479 --> 00:08:38,958
memory is larger than that of the code

260
00:08:38,958 --> 00:08:42,559
for running for learning the model

261
00:08:42,559 --> 00:08:45,040
so here i illustrated first an example

262
00:08:45,040 --> 00:08:46,880
of prior works perspective

263
00:08:46,880 --> 00:08:48,560
on a model's resilience against the

264
00:08:48,560 --> 00:08:50,720
parameter perturbations

265
00:08:50,720 --> 00:08:52,480
suppose that you manipulated five

266
00:08:52,480 --> 00:08:54,480
parameters in the last layer

267
00:08:54,480 --> 00:08:56,160
and from the conventional perspective

268
00:08:56,160 --> 00:08:58,160
the error is a small amount of change

269
00:08:58,160 --> 00:09:00,160
in the numerical representation of a

270
00:09:00,160 --> 00:09:03,839
value from 0.3 to 0.02

271
00:09:03,839 --> 00:09:06,160
which has a negligible impact on the

272
00:09:06,160 --> 00:09:07,600
model's accuracy

273
00:09:07,600 --> 00:09:10,000
however what about the worst case speed

274
00:09:10,000 --> 00:09:11,600
flip

275
00:09:11,600 --> 00:09:14,320
from the same parameter an adversary can

276
00:09:14,320 --> 00:09:15,120
flip the

277
00:09:15,120 --> 00:09:17,600
most significant bit in the exponent

278
00:09:17,600 --> 00:09:19,600
from 0 to 1.

279
00:09:19,600 --> 00:09:22,640
then the value changes exponentially and

280
00:09:22,640 --> 00:09:24,080
the accuracy of the melt

281
00:09:24,080 --> 00:09:26,480
model dropped significantly from 99

282
00:09:26,480 --> 00:09:28,480
percent to 57

283
00:09:28,480 --> 00:09:31,839
with decimal change so this illustration

284
00:09:31,839 --> 00:09:32,560
shows that

285
00:09:32,560 --> 00:09:34,959
the worst case perturbation a single bit

286
00:09:34,959 --> 00:09:37,200
flip can lead to a significant amount of

287
00:09:37,200 --> 00:09:41,120
damage to the neural network model

288
00:09:41,440 --> 00:09:44,320
so now but this is not sufficient to

289
00:09:44,320 --> 00:09:45,200
answer how much

290
00:09:45,200 --> 00:09:48,480
models could be vulnerable to bit flips

291
00:09:48,480 --> 00:09:51,120
so we need to answer how much damage can

292
00:09:51,120 --> 00:09:52,959
a single beat liberian fleet

293
00:09:52,959 --> 00:09:54,800
and how many model parameters can

294
00:09:54,800 --> 00:09:57,200
inflict the worst case

295
00:09:57,200 --> 00:09:59,120
so to quantify how many such a worst

296
00:09:59,120 --> 00:10:00,720
case existed we flip

297
00:10:00,720 --> 00:10:03,120
all the bits in deep neural network

298
00:10:03,120 --> 00:10:04,160
models

299
00:10:04,160 --> 00:10:07,040
we take a model flip each bit in all the

300
00:10:07,040 --> 00:10:07,839
parameters

301
00:10:07,839 --> 00:10:09,440
and measure the accuracy drop over the

302
00:10:09,440 --> 00:10:11,040
entire test set

303
00:10:11,040 --> 00:10:13,600
here we define the term on a kilo speed

304
00:10:13,600 --> 00:10:15,519
for referring to a bit whose flipping

305
00:10:15,519 --> 00:10:17,519
can cause the accuracy drop over 10

306
00:10:17,519 --> 00:10:18,320
percent

307
00:10:18,320 --> 00:10:20,160
which is the empirical upper bound

308
00:10:20,160 --> 00:10:23,040
presented by prior one

309
00:10:23,040 --> 00:10:26,240
then we also look at two things a we

310
00:10:26,240 --> 00:10:28,000
look at the maximum accuracy drop

311
00:10:28,000 --> 00:10:31,040
can be caused by a bit flip b we measure

312
00:10:31,040 --> 00:10:32,079
how many parameters

313
00:10:32,079 --> 00:10:34,800
are vulnerable in a model here we say a

314
00:10:34,800 --> 00:10:36,399
parameter is vulnerable

315
00:10:36,399 --> 00:10:39,839
when it contains at least one kill speed

316
00:10:39,839 --> 00:10:42,000
so we first examine the eight

317
00:10:42,000 --> 00:10:43,760
convolutional neural network model

318
00:10:43,760 --> 00:10:44,320
trained on

319
00:10:44,320 --> 00:10:46,600
amnes they have different architecture

320
00:10:46,600 --> 00:10:47,839
configurations

321
00:10:47,839 --> 00:10:50,079
and we flip each of the 32-bit in

322
00:10:50,079 --> 00:10:53,519
thousands of parameters in a model

323
00:10:53,519 --> 00:10:56,160
so in each of all eight models there is

324
00:10:56,160 --> 00:10:58,240
at least one achilles beat that causes

325
00:10:58,240 --> 00:11:00,560
an accuracy drop more than 98

326
00:11:00,560 --> 00:11:03,519
once the bit is flipped and we also

327
00:11:03,519 --> 00:11:04,320
found that

328
00:11:04,320 --> 00:11:06,480
almost half of the parameters contain at

329
00:11:06,480 --> 00:11:08,560
least one achilles bit

330
00:11:08,560 --> 00:11:10,320
that can cause an accuracy drop more

331
00:11:10,320 --> 00:11:12,000
than the empirical upper bound

332
00:11:12,000 --> 00:11:16,800
10 percent so we also examined 11 larger

333
00:11:16,800 --> 00:11:18,720
models with different architectures

334
00:11:18,720 --> 00:11:21,279
and the analysis outcome was consistent

335
00:11:21,279 --> 00:11:22,800
in all the architectures and the

336
00:11:22,800 --> 00:11:24,399
parameters that we examined

337
00:11:24,399 --> 00:11:26,720
each model has at least one bit whose

338
00:11:26,720 --> 00:11:27,519
flipping

339
00:11:27,519 --> 00:11:30,000
can inflict the accuracy drop up to 100

340
00:11:30,000 --> 00:11:31,600
percent

341
00:11:31,600 --> 00:11:33,680
and almost half of the parameters that

342
00:11:33,680 --> 00:11:36,000
we examine are vulnerable

343
00:11:36,000 --> 00:11:39,600
so what does it mean it means that

344
00:11:39,600 --> 00:11:42,000
the neural network has this property of

345
00:11:42,000 --> 00:11:44,320
the greaseless degradation

346
00:11:44,320 --> 00:11:46,720
and also this vulnerability is general

347
00:11:46,720 --> 00:11:48,160
of course many data set

348
00:11:48,160 --> 00:11:51,279
and architecture configurations

349
00:11:51,279 --> 00:11:53,440
so now the question is can a weak

350
00:11:53,440 --> 00:11:54,880
attacker exploit this general

351
00:11:54,880 --> 00:11:57,360
vulnerability right

352
00:11:57,360 --> 00:11:59,440
so we start from the space of the single

353
00:11:59,440 --> 00:12:00,560
bit adversary

354
00:12:00,560 --> 00:12:02,079
with the two axes based on the

355
00:12:02,079 --> 00:12:04,160
attacker's capability and knowledge

356
00:12:04,160 --> 00:12:06,000
and in the vertical axis the attacker

357
00:12:06,000 --> 00:12:08,079
becomes surgical when she can sleep a

358
00:12:08,079 --> 00:12:10,720
bit in an intended location in memory

359
00:12:10,720 --> 00:12:12,560
whereas there is the attacker with no

360
00:12:12,560 --> 00:12:14,800
control over the bit flip locations

361
00:12:14,800 --> 00:12:18,160
on the other side so in the horizontal

362
00:12:18,160 --> 00:12:20,399
axis the white box attacker knows the

363
00:12:20,399 --> 00:12:22,720
victim models architectures

364
00:12:22,720 --> 00:12:25,360
and knowing which parameters are

365
00:12:25,360 --> 00:12:26,320
vulnerable

366
00:12:26,320 --> 00:12:28,480
whereas black box or blind attacker is

367
00:12:28,480 --> 00:12:29,680
not

368
00:12:29,680 --> 00:12:32,240
so stronger attacker is in the upper

369
00:12:32,240 --> 00:12:33,440
right quadrant

370
00:12:33,440 --> 00:12:35,600
who knows which bit to flip in a model

371
00:12:35,600 --> 00:12:37,680
and can flip only that b

372
00:12:37,680 --> 00:12:39,360
but on the opposite side there is a

373
00:12:39,360 --> 00:12:41,519
weakest attacker who doesn't know which

374
00:12:41,519 --> 00:12:42,320
bit to flip

375
00:12:42,320 --> 00:12:44,639
and cannot even control over the beat

376
00:12:44,639 --> 00:12:46,000
flip locations

377
00:12:46,000 --> 00:12:47,680
so in this case the probability of

378
00:12:47,680 --> 00:12:49,440
hitting on a kill speed would be much

379
00:12:49,440 --> 00:12:50,959
much lower

380
00:12:50,959 --> 00:12:53,279
however suppose that the weakest

381
00:12:53,279 --> 00:12:54,399
attacker flips

382
00:12:54,399 --> 00:12:58,480
multiple bits randomly in the same model

383
00:12:58,480 --> 00:13:00,320
the probability of hitting an achilles

384
00:13:00,320 --> 00:13:02,320
speed increases of course

385
00:13:02,320 --> 00:13:03,920
because half of the model parameters

386
00:13:03,920 --> 00:13:06,160
contain at least one achilles speed

387
00:13:06,160 --> 00:13:08,639
and in consequence the attacker can turn

388
00:13:08,639 --> 00:13:12,240
into the stronger attacker

389
00:13:12,320 --> 00:13:14,480
now we evaluate the weakest attacker

390
00:13:14,480 --> 00:13:16,480
with multiple bit flips

391
00:13:16,480 --> 00:13:18,959
we consider the typical machine learning

392
00:13:18,959 --> 00:13:20,320
deployment scenarios

393
00:13:20,320 --> 00:13:22,320
in the cloud machine learning as a

394
00:13:22,320 --> 00:13:24,480
service where a dedicated virtual

395
00:13:24,480 --> 00:13:24,959
machine

396
00:13:24,959 --> 00:13:27,839
is running under the raw hammer pressure

397
00:13:27,839 --> 00:13:28,320
so

398
00:13:28,320 --> 00:13:30,560
we run our raw hammer attack with this

399
00:13:30,560 --> 00:13:31,440
setup

400
00:13:31,440 --> 00:13:33,200
i will not talk about the details but

401
00:13:33,200 --> 00:13:35,120
you can check them in our paper

402
00:13:35,120 --> 00:13:37,519
but in summary the attacker randomly

403
00:13:37,519 --> 00:13:38,240
flips a bit

404
00:13:38,240 --> 00:13:40,399
in the process memory we consider

405
00:13:40,399 --> 00:13:42,000
multiple dram chips

406
00:13:42,000 --> 00:13:44,160
and sufficiently run many bit flip

407
00:13:44,160 --> 00:13:45,839
attempts

408
00:13:45,839 --> 00:13:49,040
so our expected consequences are a a

409
00:13:49,040 --> 00:13:50,399
process crash

410
00:13:50,399 --> 00:13:52,639
b hitting any bit that doesn't lead to

411
00:13:52,639 --> 00:13:54,399
any serious damage

412
00:13:54,399 --> 00:13:57,360
c or causing an accuracy drop over 10

413
00:13:57,360 --> 00:13:57,920
percent

414
00:13:57,920 --> 00:14:00,800
by hitting on the kill speed and here is

415
00:14:00,800 --> 00:14:02,880
the summary of our result

416
00:14:02,880 --> 00:14:06,320
in 62 of our text the attacker causes an

417
00:14:06,320 --> 00:14:08,720
accuracy drop over 10

418
00:14:08,720 --> 00:14:10,480
and also the time it takes to cause the

419
00:14:10,480 --> 00:14:12,240
damage is at most within

420
00:14:12,240 --> 00:14:15,120
a few minutes and further our attacker

421
00:14:15,120 --> 00:14:16,160
can remain

422
00:14:16,160 --> 00:14:18,800
inconspicuous we observe only six

423
00:14:18,800 --> 00:14:19,920
program crashes

424
00:14:19,920 --> 00:14:24,639
over the entire 7.5 bit flip attempt

425
00:14:24,639 --> 00:14:26,800
so based on this research now we can say

426
00:14:26,800 --> 00:14:28,800
that a weak row hammer attack is

427
00:14:28,800 --> 00:14:29,519
sufficient

428
00:14:29,519 --> 00:14:31,440
to cause serious threat to neural

429
00:14:31,440 --> 00:14:34,399
network models

430
00:14:34,720 --> 00:14:38,720
so then we then move our focus on

431
00:14:38,720 --> 00:14:40,560
the side channel attacks especially

432
00:14:40,560 --> 00:14:42,880
casper cache based side channel tags

433
00:14:42,880 --> 00:14:46,720
exploitable in the cloud we ask here

434
00:14:46,720 --> 00:14:48,639
how can an attacker steal deep neural

435
00:14:48,639 --> 00:14:50,000
network architecture

436
00:14:50,000 --> 00:14:54,160
by leveraging only side channel leakage

437
00:14:54,160 --> 00:14:56,320
but someone will say that i agree we

438
00:14:56,320 --> 00:14:57,839
already know that neural network

439
00:14:57,839 --> 00:14:59,120
architectures such as

440
00:14:59,120 --> 00:15:01,920
vgg resnet etc then why does the

441
00:15:01,920 --> 00:15:04,720
attacker need to steal the architecture

442
00:15:04,720 --> 00:15:07,120
well then think about this architect

443
00:15:07,120 --> 00:15:09,199
architecture for example

444
00:15:09,199 --> 00:15:11,199
this network is automatically designed

445
00:15:11,199 --> 00:15:12,800
by an algorithm for searching

446
00:15:12,800 --> 00:15:13,839
architectures

447
00:15:13,839 --> 00:15:16,079
called a neural architecture search and

448
00:15:16,079 --> 00:15:18,399
no one tried to steal this architecture

449
00:15:18,399 --> 00:15:21,440
of via side channel before us

450
00:15:21,440 --> 00:15:24,160
so those architectures carry a lot of

451
00:15:24,160 --> 00:15:24,959
incentives

452
00:15:24,959 --> 00:15:28,079
to steal first unique architecture

453
00:15:28,079 --> 00:15:29,680
outperformed the off-the-shelf

454
00:15:29,680 --> 00:15:30,639
architectures

455
00:15:30,639 --> 00:15:32,320
for instance efficientnet is the

456
00:15:32,320 --> 00:15:33,680
currently state of the art

457
00:15:33,680 --> 00:15:36,240
on imagenet and transformers are known

458
00:15:36,240 --> 00:15:37,680
to be the state-of-the-art

459
00:15:37,680 --> 00:15:40,720
on language tasks and also

460
00:15:40,720 --> 00:15:43,440
constructing a unique architecture takes

461
00:15:43,440 --> 00:15:45,279
a lot of time and effort

462
00:15:45,279 --> 00:15:47,199
running the search algorithm like nas

463
00:15:47,199 --> 00:15:49,440
takes thousands of gpu hours

464
00:15:49,440 --> 00:15:51,040
and even handcrafting unique

465
00:15:51,040 --> 00:15:54,240
architectures remain 10 to 15 expert

466
00:15:54,240 --> 00:15:56,399
so in consequence once unique

467
00:15:56,399 --> 00:15:57,839
architecture is found

468
00:15:57,839 --> 00:16:00,000
they become intellectual property or

469
00:16:00,000 --> 00:16:02,000
treaty secrets

470
00:16:02,000 --> 00:16:04,639
so now our initial question becomes how

471
00:16:04,639 --> 00:16:06,720
can an attacker reconstruct a unique

472
00:16:06,720 --> 00:16:07,519
neural network

473
00:16:07,519 --> 00:16:10,160
architecture by leveraging only the side

474
00:16:10,160 --> 00:16:12,880
channel leakage

475
00:16:12,959 --> 00:16:16,399
let me show how we did that so to

476
00:16:16,399 --> 00:16:17,519
illustrate how

477
00:16:17,519 --> 00:16:20,079
how our algorithm works i prepare a

478
00:16:20,079 --> 00:16:22,639
simple neural network on the left side

479
00:16:22,639 --> 00:16:25,120
we assume our attacker utilizes flush

480
00:16:25,120 --> 00:16:26,240
and reload

481
00:16:26,240 --> 00:16:28,240
a remote cache side channel attack that

482
00:16:28,240 --> 00:16:30,639
is exploitable in the cloud

483
00:16:30,639 --> 00:16:33,360
if our attacker runs flush reroad the

484
00:16:33,360 --> 00:16:35,040
computational trace that she will

485
00:16:35,040 --> 00:16:35,759
observe

486
00:16:35,759 --> 00:16:37,759
while the network is processing only a

487
00:16:37,759 --> 00:16:38,800
single input

488
00:16:38,800 --> 00:16:41,839
looks like this now what attacker needs

489
00:16:41,839 --> 00:16:43,120
to do here

490
00:16:43,120 --> 00:16:44,639
is to reconstruct the victim's

491
00:16:44,639 --> 00:16:46,720
architecture and configurations from

492
00:16:46,720 --> 00:16:49,040
this tree

493
00:16:49,040 --> 00:16:51,759
for that we need to identify a the

494
00:16:51,759 --> 00:16:53,759
connections between layers

495
00:16:53,759 --> 00:16:55,759
b the layer configurations such as

496
00:16:55,759 --> 00:16:57,360
kernel sizes tries

497
00:16:57,360 --> 00:17:00,399
etc so this is challenging task because

498
00:17:00,399 --> 00:17:01,920
the search space is almost

499
00:17:01,920 --> 00:17:04,400
infinite and therefore prior makes

500
00:17:04,400 --> 00:17:06,720
unrealistic assumption that the victim

501
00:17:06,720 --> 00:17:09,599
uses the off-the-shelf architecture such

502
00:17:09,599 --> 00:17:10,640
as vgg

503
00:17:10,640 --> 00:17:14,319
resnet or etc etc however

504
00:17:14,319 --> 00:17:16,480
our intuition is that perhaps one can

505
00:17:16,480 --> 00:17:18,079
reconstruct the architecture by

506
00:17:18,079 --> 00:17:21,520
exploiting deep learning computations

507
00:17:21,520 --> 00:17:23,760
so let's see how it did that we first

508
00:17:23,760 --> 00:17:25,760
identify the sequence of layer axis

509
00:17:25,760 --> 00:17:28,319
while the network is processing an input

510
00:17:28,319 --> 00:17:30,559
then we can connect them one by one

511
00:17:30,559 --> 00:17:33,280
until we face the add operation which is

512
00:17:33,280 --> 00:17:35,440
a binary operator

513
00:17:35,440 --> 00:17:38,160
so since the ad can be connected to any

514
00:17:38,160 --> 00:17:39,440
preceding layers

515
00:17:39,440 --> 00:17:42,000
the problem here is that multiple

516
00:17:42,000 --> 00:17:44,840
architectures correspond to the observed

517
00:17:44,840 --> 00:17:46,160
trees

518
00:17:46,160 --> 00:17:49,520
but the solution is quite simple we just

519
00:17:49,520 --> 00:17:51,360
populate all the candidate architecture

520
00:17:51,360 --> 00:17:52,080
feasible

521
00:17:52,080 --> 00:17:53,600
and then just pruning comparable

522
00:17:53,600 --> 00:17:55,679
candidates later on

523
00:17:55,679 --> 00:17:58,480
so to eliminate we reconstructed two

524
00:17:58,480 --> 00:18:00,080
types of information

525
00:18:00,080 --> 00:18:02,480
first the layer configurations such as

526
00:18:02,480 --> 00:18:03,520
corner sizes

527
00:18:03,520 --> 00:18:06,880
tries and so on and the input and output

528
00:18:06,880 --> 00:18:09,520
dimensions of each layer

529
00:18:09,520 --> 00:18:11,520
so we extract them from the time it

530
00:18:11,520 --> 00:18:13,600
takes to process each layers

531
00:18:13,600 --> 00:18:15,039
we know that the neural network

532
00:18:15,039 --> 00:18:17,120
computations are implemented as

533
00:18:17,120 --> 00:18:19,600
matrix multiplications thus the time

534
00:18:19,600 --> 00:18:21,120
information is useful

535
00:18:21,120 --> 00:18:23,200
to estimate the size of the matrices

536
00:18:23,200 --> 00:18:26,320
considered in each multiplication

537
00:18:26,320 --> 00:18:28,559
and moreover we can identify the

538
00:18:28,559 --> 00:18:30,000
function calls in a matrix

539
00:18:30,000 --> 00:18:31,919
multiplication library such as

540
00:18:31,919 --> 00:18:34,400
gmm calls and the number of those

541
00:18:34,400 --> 00:18:36,240
function calls are proportional to the

542
00:18:36,240 --> 00:18:38,160
block size of the matrices

543
00:18:38,160 --> 00:18:40,400
and from this information we can

544
00:18:40,400 --> 00:18:41,360
estimate

545
00:18:41,360 --> 00:18:44,160
input output or filter dimensions in

546
00:18:44,160 --> 00:18:46,960
convolutional layer

547
00:18:46,960 --> 00:18:49,600
now we need to combine so combining

548
00:18:49,600 --> 00:18:51,200
those timing information

549
00:18:51,200 --> 00:18:53,440
our attacker can identify the input and

550
00:18:53,440 --> 00:18:54,640
output dimensions

551
00:18:54,640 --> 00:18:57,200
along with the layer configuration slide

552
00:18:57,200 --> 00:18:57,919
right on

553
00:18:57,919 --> 00:19:01,039
like on the right side so we then

554
00:19:01,039 --> 00:19:03,200
finally eliminate the incompatible

555
00:19:03,200 --> 00:19:04,000
candidate

556
00:19:04,000 --> 00:19:07,039
with this information

557
00:19:07,039 --> 00:19:10,080
so the reason why our reconstruction is

558
00:19:10,080 --> 00:19:10,880
possible

559
00:19:10,880 --> 00:19:13,280
because of the regularities in deem

560
00:19:13,280 --> 00:19:15,840
learning computations

561
00:19:15,840 --> 00:19:19,280
so now the evaluation we evaluated our

562
00:19:19,280 --> 00:19:21,280
reconstruction attack with two unique

563
00:19:21,280 --> 00:19:22,880
neural network architectures

564
00:19:22,880 --> 00:19:26,080
malcolm and proxylisnas cpu and we

565
00:19:26,080 --> 00:19:27,840
consider machine learning as a service

566
00:19:27,840 --> 00:19:30,640
scenarios in the cloud

567
00:19:30,640 --> 00:19:34,480
so in summary we a reconstructed both

568
00:19:34,480 --> 00:19:35,360
the unique

569
00:19:35,360 --> 00:19:38,640
architectures with zero error b

570
00:19:38,640 --> 00:19:40,640
and the time it takes to reconstruct is

571
00:19:40,640 --> 00:19:42,880
much less than the time to construct

572
00:19:42,880 --> 00:19:46,000
those architectures

573
00:19:46,400 --> 00:19:49,520
so finally from this noisy information

574
00:19:49,520 --> 00:19:52,160
which is considered weak extracted from

575
00:19:52,160 --> 00:19:53,919
the cache side channel attack

576
00:19:53,919 --> 00:19:55,840
our attacker can steal unique

577
00:19:55,840 --> 00:19:59,520
architectures with zero error

578
00:19:59,679 --> 00:20:02,880
now is your time to contribute

579
00:20:02,880 --> 00:20:05,600
this research is still an understudied

580
00:20:05,600 --> 00:20:06,320
topic

581
00:20:06,320 --> 00:20:08,400
so we encourage contributions from both

582
00:20:08,400 --> 00:20:10,400
academia and industry

583
00:20:10,400 --> 00:20:12,320
so now we know that the neural network

584
00:20:12,320 --> 00:20:14,240
is not a standalone concept

585
00:20:14,240 --> 00:20:16,240
but it's just a component that consists

586
00:20:16,240 --> 00:20:17,840
of computer systems

587
00:20:17,840 --> 00:20:20,000
so we need to re-examine the interplay

588
00:20:20,000 --> 00:20:22,000
between the models and other system

589
00:20:22,000 --> 00:20:22,960
components

590
00:20:22,960 --> 00:20:26,000
with the security mindset and with the

591
00:20:26,000 --> 00:20:27,280
security mindset

592
00:20:27,280 --> 00:20:29,600
the vulnerability of neural networks to

593
00:20:29,600 --> 00:20:31,120
microarchitecture attack

594
00:20:31,120 --> 00:20:33,919
is still understudied topic so we first

595
00:20:33,919 --> 00:20:35,120
need to understand

596
00:20:35,120 --> 00:20:37,120
what are the worst case behaviors when

597
00:20:37,120 --> 00:20:38,799
we design machine learning systems

598
00:20:38,799 --> 00:20:40,720
against those hardware thread

599
00:20:40,720 --> 00:20:42,480
and the worst case could be different

600
00:20:42,480 --> 00:20:44,880
from what we have known so far

601
00:20:44,880 --> 00:20:47,120
so moreover we also need to examine

602
00:20:47,120 --> 00:20:49,200
whether computational properties that

603
00:20:49,200 --> 00:20:49,760
makes

604
00:20:49,760 --> 00:20:52,159
that team neural networks have makes it

605
00:20:52,159 --> 00:20:54,640
possible for a weak adversary to inflict

606
00:20:54,640 --> 00:20:57,280
those worst cases

607
00:20:57,280 --> 00:21:00,240
and lastly system level defenses for

608
00:21:00,240 --> 00:21:02,159
machine learning models provide an

609
00:21:02,159 --> 00:21:05,039
incomplete view of security because weak

610
00:21:05,039 --> 00:21:07,200
hardware attacks are still

611
00:21:07,200 --> 00:21:09,520
causes a significant threat to the

612
00:21:09,520 --> 00:21:10,400
system

613
00:21:10,400 --> 00:21:12,720
thus we urge the community to consider a

614
00:21:12,720 --> 00:21:15,120
hardware software co-design approach

615
00:21:15,120 --> 00:21:17,039
for closing the attack surfaces in

616
00:21:17,039 --> 00:21:20,080
machine learning systems

617
00:21:20,080 --> 00:21:22,799
so considering our work and the future

618
00:21:22,799 --> 00:21:23,360
research

619
00:21:23,360 --> 00:21:25,679
directions that you will do we believe

620
00:21:25,679 --> 00:21:27,760
this is the best moment to pursue

621
00:21:27,760 --> 00:21:30,720
the ancient wisdom a sound mind in a

622
00:21:30,720 --> 00:21:32,559
sound body

623
00:21:32,559 --> 00:21:35,200
thank you and if you have any questions

624
00:21:35,200 --> 00:21:36,720
don't hesitate to shoot me an

625
00:21:36,720 --> 00:21:39,520
email and we also have a nice write-up

626
00:21:39,520 --> 00:21:40,559
about our research

627
00:21:40,559 --> 00:21:43,840
on this website hardwarefield.ml please

628
00:21:43,840 --> 00:21:44,720
check that out

629
00:21:44,720 --> 00:21:57,120
thank you

