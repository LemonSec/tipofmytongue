1
00:00:08,559 --> 00:00:10,000
hi everyone

2
00:00:10,000 --> 00:00:12,320
i'm patrick gage kelly a privacy

3
00:00:12,320 --> 00:00:13,040
security

4
00:00:13,040 --> 00:00:16,079
and anti-abuse researcher at google and

5
00:00:16,079 --> 00:00:17,520
a person in my line of work

6
00:00:17,520 --> 00:00:20,960
hears all the time that privacy is

7
00:00:20,960 --> 00:00:25,199
dead you too have probably heard privacy

8
00:00:25,199 --> 00:00:26,080
is dead

9
00:00:26,080 --> 00:00:28,800
or at least you've heard this question

10
00:00:28,800 --> 00:00:29,199
is

11
00:00:29,199 --> 00:00:32,238
privacy dead in mainstream publications

12
00:00:32,238 --> 00:00:34,800
and advice from ceos from venture

13
00:00:34,800 --> 00:00:38,000
funders even in the academic press

14
00:00:38,000 --> 00:00:41,440
but you know it isn't every time you

15
00:00:41,440 --> 00:00:42,960
tell someone a secret

16
00:00:42,960 --> 00:00:45,280
particularly if you expect them not to

17
00:00:45,280 --> 00:00:47,200
repeat it or use it in a way that might

18
00:00:47,200 --> 00:00:48,559
embarrass you

19
00:00:48,559 --> 00:00:50,719
every time you write something down in a

20
00:00:50,719 --> 00:00:52,000
diary or a journal

21
00:00:52,000 --> 00:00:53,600
which you know a family member or a

22
00:00:53,600 --> 00:00:55,280
partner could read

23
00:00:55,280 --> 00:00:56,719
every time you have a thought inside

24
00:00:56,719 --> 00:00:58,800
your own mind and then catch yourself

25
00:00:58,800 --> 00:01:01,760
and say don't think that these are

26
00:01:01,760 --> 00:01:02,879
moments

27
00:01:02,879 --> 00:01:05,920
moments of privacy is trust or secrecy

28
00:01:05,920 --> 00:01:08,720
or freedom and we all have them these

29
00:01:08,720 --> 00:01:10,560
moments are part of being human

30
00:01:10,560 --> 00:01:12,640
and the protection of these moments

31
00:01:12,640 --> 00:01:14,479
where we express privacy

32
00:01:14,479 --> 00:01:18,159
is part of what allows us to flourish

33
00:01:18,560 --> 00:01:21,200
and yet we hear too often about how

34
00:01:21,200 --> 00:01:22,479
privacy is dead

35
00:01:22,479 --> 00:01:25,439
or dying enough that an idea i don't

36
00:01:25,439 --> 00:01:26,799
give credence to

37
00:01:26,799 --> 00:01:28,560
is something that we're going to spend

38
00:01:28,560 --> 00:01:31,119
today working to understand

39
00:01:31,119 --> 00:01:33,439
to explore what it would mean for

40
00:01:33,439 --> 00:01:35,680
privacy to actually be dead

41
00:01:35,680 --> 00:01:38,079
and deconstruct this specifically in a

42
00:01:38,079 --> 00:01:39,520
quantitative way

43
00:01:39,520 --> 00:01:42,399
using data from over 90 000 respondents

44
00:01:42,399 --> 00:01:44,000
that we at google

45
00:01:44,000 --> 00:01:45,840
have been working to understand over the

46
00:01:45,840 --> 00:01:48,399
last few years

47
00:01:48,399 --> 00:01:50,799
specifically we've been working with a

48
00:01:50,799 --> 00:01:53,119
series of public opinion polling firms

49
00:01:53,119 --> 00:01:56,159
including gfk ipsos and most recently

50
00:01:56,159 --> 00:01:57,280
gallup

51
00:01:57,280 --> 00:01:59,200
we did this to get the highest gold

52
00:01:59,200 --> 00:02:00,719
standard sets of responses

53
00:02:00,719 --> 00:02:03,280
we could from people around the world we

54
00:02:03,280 --> 00:02:06,240
began in 2015 with just a few countries

55
00:02:06,240 --> 00:02:08,479
and we now have responses from 29

56
00:02:08,479 --> 00:02:10,479
countries on six continents

57
00:02:10,479 --> 00:02:12,720
and are beginning to see often very

58
00:02:12,720 --> 00:02:13,840
stable trends

59
00:02:13,840 --> 00:02:17,200
year over year our survey instrument

60
00:02:17,200 --> 00:02:19,680
that is the specific questions we ask

61
00:02:19,680 --> 00:02:20,800
our participants

62
00:02:20,800 --> 00:02:23,440
has also evolved over time with a core

63
00:02:23,440 --> 00:02:26,959
of 60 items about privacy and technology

64
00:02:26,959 --> 00:02:29,200
these questions range from far-reaching

65
00:02:29,200 --> 00:02:31,360
opinions to their current attitudes and

66
00:02:31,360 --> 00:02:33,200
to self-reported behaviors on their

67
00:02:33,200 --> 00:02:34,840
engagement with specific privacy

68
00:02:34,840 --> 00:02:37,599
technologies

69
00:02:37,599 --> 00:02:39,840
for example let's start with a question

70
00:02:39,840 --> 00:02:42,080
that we asked our respondents

71
00:02:42,080 --> 00:02:43,920
i want you to think to yourself and

72
00:02:43,920 --> 00:02:45,440
answer this

73
00:02:45,440 --> 00:02:47,440
to think about the amount of privacy

74
00:02:47,440 --> 00:02:49,680
you'll have 10 years from now

75
00:02:49,680 --> 00:02:52,879
in 2031 will it be

76
00:02:52,879 --> 00:02:56,560
more less or the same amount of privacy

77
00:02:56,560 --> 00:03:04,000
that you have today

78
00:03:04,000 --> 00:03:07,840
from the data we have we think that 49

79
00:03:07,840 --> 00:03:10,319
of people expect to have more privacy in

80
00:03:10,319 --> 00:03:11,360
10 years

81
00:03:11,360 --> 00:03:15,200
18 expect about the same and 33

82
00:03:15,200 --> 00:03:18,319
expect less privacy than they have today

83
00:03:18,319 --> 00:03:20,879
that's half of the population expecting

84
00:03:20,879 --> 00:03:22,000
more privacy

85
00:03:22,000 --> 00:03:24,159
about one in five expecting the same and

86
00:03:24,159 --> 00:03:25,680
just a third

87
00:03:25,680 --> 00:03:28,080
expecting less maybe the third that

88
00:03:28,080 --> 00:03:30,560
might think privacy is dead

89
00:03:30,560 --> 00:03:33,599
now assuming you all the attendees of

90
00:03:33,599 --> 00:03:34,480
enigma are

91
00:03:34,480 --> 00:03:36,640
similar to the populations across the

92
00:03:36,640 --> 00:03:37,599
couple of dozen

93
00:03:37,599 --> 00:03:39,519
countries we studied you might have

94
00:03:39,519 --> 00:03:41,120
thought similar responses

95
00:03:41,120 --> 00:03:43,200
but we'll come back to why the enigma

96
00:03:43,200 --> 00:03:44,720
audience might be a little bit different

97
00:03:44,720 --> 00:03:48,319
a little bit later now

98
00:03:48,319 --> 00:03:52,319
these results might seem optimistic

99
00:03:52,319 --> 00:03:55,280
it might be hope that privacy regulation

100
00:03:55,280 --> 00:03:57,200
can provide more data protection

101
00:03:57,200 --> 00:03:59,040
it might be hope that new technologies

102
00:03:59,040 --> 00:04:00,239
will keep our information

103
00:04:00,239 --> 00:04:02,400
safe from unwanted and inappropriate

104
00:04:02,400 --> 00:04:03,360
data use

105
00:04:03,360 --> 00:04:08,080
it might just be pure optimism

106
00:04:08,400 --> 00:04:10,480
even if you considered that privacy

107
00:04:10,480 --> 00:04:12,720
could be more protected in some ways but

108
00:04:12,720 --> 00:04:13,840
at greater risk in

109
00:04:13,840 --> 00:04:16,399
others even if you tried to balance

110
00:04:16,399 --> 00:04:18,000
these concerns

111
00:04:18,000 --> 00:04:20,560
many of our participants data that

112
00:04:20,560 --> 00:04:22,720
implies millions or billions of people

113
00:04:22,720 --> 00:04:23,840
around the world

114
00:04:23,840 --> 00:04:26,240
are choosing to believe that they will

115
00:04:26,240 --> 00:04:28,560
have a future with privacy

116
00:04:28,560 --> 00:04:30,560
this along with much of the rest of our

117
00:04:30,560 --> 00:04:32,400
data confirms that people

118
00:04:32,400 --> 00:04:36,400
haven't yet given up on privacy

119
00:04:36,400 --> 00:04:38,880
so that's our first point today that

120
00:04:38,880 --> 00:04:39,840
people are

121
00:04:39,840 --> 00:04:41,680
optimistic about the privacy they will

122
00:04:41,680 --> 00:04:43,360
have in 10 years

123
00:04:43,360 --> 00:04:45,440
and for the rest of today's talk i'd

124
00:04:45,440 --> 00:04:46,800
like to take us on a journey

125
00:04:46,800 --> 00:04:49,040
through five more interesting findings

126
00:04:49,040 --> 00:04:51,680
in our data

127
00:04:52,400 --> 00:04:55,520
the second is that people just straight

128
00:04:55,520 --> 00:04:56,320
up say

129
00:04:56,320 --> 00:04:59,520
privacy is important while asking people

130
00:04:59,520 --> 00:05:01,600
to forecast how much privacy

131
00:05:01,600 --> 00:05:03,520
they expect to have in a decade is

132
00:05:03,520 --> 00:05:06,000
interesting and a fun thought experiment

133
00:05:06,000 --> 00:05:09,120
it's not the most direct measure of

134
00:05:09,120 --> 00:05:12,240
how people feel about privacy today

135
00:05:12,240 --> 00:05:15,440
so our first measure here is doing just

136
00:05:15,440 --> 00:05:16,560
that

137
00:05:16,560 --> 00:05:18,560
when we asked participants directly

138
00:05:18,560 --> 00:05:20,720
saying how important is the following to

139
00:05:20,720 --> 00:05:21,520
you

140
00:05:21,520 --> 00:05:23,600
people told us that privacy is important

141
00:05:23,600 --> 00:05:24,720
to them

142
00:05:24,720 --> 00:05:27,759
very important actually we asked our

143
00:05:27,759 --> 00:05:29,120
respondents this question

144
00:05:29,120 --> 00:05:31,840
on a five point likert scale that means

145
00:05:31,840 --> 00:05:34,080
our participants had five options

146
00:05:34,080 --> 00:05:36,880
in this case extremely important very

147
00:05:36,880 --> 00:05:37,600
important

148
00:05:37,600 --> 00:05:39,680
moderately important slightly important

149
00:05:39,680 --> 00:05:41,600
or not at all important

150
00:05:41,600 --> 00:05:44,560
those last two slightly and not at all

151
00:05:44,560 --> 00:05:45,199
important

152
00:05:45,199 --> 00:05:47,520
are those two thin gray lines on the

153
00:05:47,520 --> 00:05:49,120
right side of the bar graph

154
00:05:49,120 --> 00:05:51,039
totaling just about three percent of our

155
00:05:51,039 --> 00:05:52,800
global respondents

156
00:05:52,800 --> 00:05:54,720
what this means is almost no one thinks

157
00:05:54,720 --> 00:05:57,680
privacy isn't important to them

158
00:05:57,680 --> 00:05:59,759
and across the last six years we

159
00:05:59,759 --> 00:06:01,680
regularly see that these top two

160
00:06:01,680 --> 00:06:02,639
categories

161
00:06:02,639 --> 00:06:04,880
very and extremely important are the

162
00:06:04,880 --> 00:06:05,759
choices an

163
00:06:05,759 --> 00:06:08,080
overwhelming majority select that's

164
00:06:08,080 --> 00:06:10,800
about 9 in 10 of our respondents

165
00:06:10,800 --> 00:06:13,199
near universal support the type of

166
00:06:13,199 --> 00:06:14,400
support across

167
00:06:14,400 --> 00:06:16,880
countries demographics and time that we

168
00:06:16,880 --> 00:06:17,440
see for

169
00:06:17,440 --> 00:06:20,639
very few of our measures simply when we

170
00:06:20,639 --> 00:06:21,280
ask people

171
00:06:21,280 --> 00:06:24,560
if privacy is important they say it is

172
00:06:24,560 --> 00:06:27,919
it's very important but of course

173
00:06:27,919 --> 00:06:29,280
they're just telling us that they

174
00:06:29,280 --> 00:06:31,120
believe it's important

175
00:06:31,120 --> 00:06:33,280
this brings us into our first set of

176
00:06:33,280 --> 00:06:35,280
sort of methodological notes about how

177
00:06:35,280 --> 00:06:36,000
we do this

178
00:06:36,000 --> 00:06:39,120
large survey work and these results come

179
00:06:39,120 --> 00:06:39,440
with

180
00:06:39,440 --> 00:06:41,360
all the caveats that are contained in

181
00:06:41,360 --> 00:06:42,720
doing survey work

182
00:06:42,720 --> 00:06:44,479
that means the data we're covering today

183
00:06:44,479 --> 00:06:45,840
is self-reported

184
00:06:45,840 --> 00:06:48,319
just like how people tell us how privacy

185
00:06:48,319 --> 00:06:50,240
is important to them

186
00:06:50,240 --> 00:06:52,800
to execute these global surveys means we

187
00:06:52,800 --> 00:06:53,759
wrote the questions

188
00:06:53,759 --> 00:06:55,680
we provided the answers and people had

189
00:06:55,680 --> 00:06:57,199
to answer the questions within the

190
00:06:57,199 --> 00:06:59,840
bounds of the responses we offered them

191
00:06:59,840 --> 00:07:02,160
they couldn't tell us that privacy was

192
00:07:02,160 --> 00:07:03,520
important in some

193
00:07:03,520 --> 00:07:05,520
different or nuanced way they had to

194
00:07:05,520 --> 00:07:07,520
answer it on our scale

195
00:07:07,520 --> 00:07:10,160
similarly there was no way to specify

196
00:07:10,160 --> 00:07:12,560
that if you think you will have privacy

197
00:07:12,560 --> 00:07:14,800
more in some ways in 10 years and less

198
00:07:14,800 --> 00:07:16,000
in others

199
00:07:16,000 --> 00:07:18,720
you can't tell us that do you average it

200
00:07:18,720 --> 00:07:20,560
and say it will be the same do you weigh

201
00:07:20,560 --> 00:07:21,680
these differently

202
00:07:21,680 --> 00:07:23,599
we aren't giving participants a way to

203
00:07:23,599 --> 00:07:25,919
tell us more

204
00:07:25,919 --> 00:07:27,680
well we built our survey instrument

205
00:07:27,680 --> 00:07:29,759
iteratively over multiple years and

206
00:07:29,759 --> 00:07:32,160
began with a selection of items from pew

207
00:07:32,160 --> 00:07:34,800
euro barometer gallup ipsos and other

208
00:07:34,800 --> 00:07:36,880
public opinion polling tools

209
00:07:36,880 --> 00:07:39,199
no survey question is perfect in terms

210
00:07:39,199 --> 00:07:41,199
of how respondents will read

211
00:07:41,199 --> 00:07:43,680
understand and answer it or how stable

212
00:07:43,680 --> 00:07:46,240
it is over time

213
00:07:46,240 --> 00:07:48,240
additionally we've asked our

214
00:07:48,240 --> 00:07:50,240
participants to fill out a 20-minute

215
00:07:50,240 --> 00:07:52,160
survey of privacy attitudes

216
00:07:52,160 --> 00:07:54,000
that may have been more time than they

217
00:07:54,000 --> 00:07:55,840
normally spend thinking about privacy in

218
00:07:55,840 --> 00:07:57,280
the course of a month

219
00:07:57,280 --> 00:07:58,879
this may have framed their answers to

220
00:07:58,879 --> 00:08:00,479
questions as they worked their way

221
00:08:00,479 --> 00:08:03,520
through the survey

222
00:08:05,599 --> 00:08:08,639
but even with these caveats in mind

223
00:08:08,639 --> 00:08:11,840
we think these results really show how

224
00:08:11,840 --> 00:08:12,319
people

225
00:08:12,319 --> 00:08:14,960
are engaging with privacy our third

226
00:08:14,960 --> 00:08:15,919
highlight today

227
00:08:15,919 --> 00:08:17,759
is that people have a range of negative

228
00:08:17,759 --> 00:08:19,759
experiences online

229
00:08:19,759 --> 00:08:22,080
thinking back to our last two questions

230
00:08:22,080 --> 00:08:23,039
the future

231
00:08:23,039 --> 00:08:25,280
or the importance of privacy these can

232
00:08:25,280 --> 00:08:26,080
feel very

233
00:08:26,080 --> 00:08:29,199
abstract very opinion based but we've

234
00:08:29,199 --> 00:08:30,720
also included items

235
00:08:30,720 --> 00:08:32,479
that attempt to measure people's lived

236
00:08:32,479 --> 00:08:35,039
experiences and the impacts of privacy

237
00:08:35,039 --> 00:08:38,000
directly on people's lives one type of

238
00:08:38,000 --> 00:08:39,279
impact that privacy

239
00:08:39,279 --> 00:08:42,240
or most often a lack or compromise of

240
00:08:42,240 --> 00:08:42,958
privacy

241
00:08:42,958 --> 00:08:46,000
can have are negative experiences we've

242
00:08:46,000 --> 00:08:47,440
asked our participants if they've

243
00:08:47,440 --> 00:08:49,519
experienced a wide variety of negative

244
00:08:49,519 --> 00:08:51,200
experiences while online

245
00:08:51,200 --> 00:08:54,080
with 7 in 10 respondents across all of

246
00:08:54,080 --> 00:08:55,519
the countries we surveyed

247
00:08:55,519 --> 00:08:57,839
telling us they'd experienced viruses

248
00:08:57,839 --> 00:08:59,920
malware or some other form of hacking

249
00:08:59,920 --> 00:09:01,519
that they'd been exposed to unwanted

250
00:09:01,519 --> 00:09:03,200
explicit content that they'd been the

251
00:09:03,200 --> 00:09:04,640
target of a scam

252
00:09:04,640 --> 00:09:06,720
phishing or some other form of identity

253
00:09:06,720 --> 00:09:08,320
theft and more

254
00:09:08,320 --> 00:09:10,320
on this chart we're showing a selection

255
00:09:10,320 --> 00:09:12,240
of the negative experiences that people

256
00:09:12,240 --> 00:09:13,839
globally have faced

257
00:09:13,839 --> 00:09:16,240
and while some are relatively infrequent

258
00:09:16,240 --> 00:09:18,560
i don't want to minimize the potentially

259
00:09:18,560 --> 00:09:20,399
severe impacts that some of these events

260
00:09:20,399 --> 00:09:22,560
can have

261
00:09:22,560 --> 00:09:25,120
while not all of these may seem directly

262
00:09:25,120 --> 00:09:26,800
related to privacy

263
00:09:26,800 --> 00:09:28,480
in the ways we think about it within

264
00:09:28,480 --> 00:09:31,040
this community our participants often

265
00:09:31,040 --> 00:09:32,320
think of these events

266
00:09:32,320 --> 00:09:34,560
and other events online in relationship

267
00:09:34,560 --> 00:09:36,240
with how they consider the use of their

268
00:09:36,240 --> 00:09:36,880
data

269
00:09:36,880 --> 00:09:40,880
and their ideas of safety and privacy

270
00:09:41,360 --> 00:09:44,160
also the range and forms of the negative

271
00:09:44,160 --> 00:09:45,040
experiences

272
00:09:45,040 --> 00:09:47,200
change based on the country as well as

273
00:09:47,200 --> 00:09:48,080
demographics

274
00:09:48,080 --> 00:09:51,360
particularly the participants gender

275
00:09:51,360 --> 00:09:53,920
but again i just want to emphasize that

276
00:09:53,920 --> 00:09:56,240
seven in ten of our respondents globally

277
00:09:56,240 --> 00:09:58,720
have directly and personally felt how

278
00:09:58,720 --> 00:10:00,720
their privacy and safety

279
00:10:00,720 --> 00:10:04,560
have been threatened online

280
00:10:04,560 --> 00:10:07,360
now i just mentioned that the negative

281
00:10:07,360 --> 00:10:09,120
experiences that people have online

282
00:10:09,120 --> 00:10:10,880
change based on the country

283
00:10:10,880 --> 00:10:12,560
and we see that difference on several

284
00:10:12,560 --> 00:10:15,200
metrics i've glossed over a lot of the

285
00:10:15,200 --> 00:10:16,399
details here

286
00:10:16,399 --> 00:10:18,160
and while i'm going to keep doing that

287
00:10:18,160 --> 00:10:20,240
to keep this talk at a very high level

288
00:10:20,240 --> 00:10:23,760
i've shown various global summaries and

289
00:10:23,760 --> 00:10:26,640
the story isn't that simple even the

290
00:10:26,640 --> 00:10:28,480
method of taking a global

291
00:10:28,480 --> 00:10:30,399
average or reporting a single global

292
00:10:30,399 --> 00:10:32,560
metric is not straightforward

293
00:10:32,560 --> 00:10:34,720
we can't take this data and encapsulate

294
00:10:34,720 --> 00:10:36,399
it down into a single number

295
00:10:36,399 --> 00:10:38,320
that represents how people feel about

296
00:10:38,320 --> 00:10:40,880
privacy around the entire world

297
00:10:40,880 --> 00:10:42,640
with privacy as with most

298
00:10:42,640 --> 00:10:44,079
socio-technical problems

299
00:10:44,079 --> 00:10:46,560
this is a complex and nuanced space and

300
00:10:46,560 --> 00:10:48,000
i don't want you to leave

301
00:10:48,000 --> 00:10:50,079
this talk believing that every person or

302
00:10:50,079 --> 00:10:51,839
that every group or community think of

303
00:10:51,839 --> 00:10:53,680
privacy similarly

304
00:10:53,680 --> 00:10:56,079
we see some very big divides between

305
00:10:56,079 --> 00:10:57,440
some countries and on

306
00:10:57,440 --> 00:11:00,399
certain metrics and some questions and

307
00:11:00,399 --> 00:11:02,160
so now we're going to revisit the

308
00:11:02,160 --> 00:11:03,760
a few of the questions we've already

309
00:11:03,760 --> 00:11:05,760
explored today to see how much some of

310
00:11:05,760 --> 00:11:08,720
these regions differ

311
00:11:09,519 --> 00:11:12,160
thinking back to how our global summary

312
00:11:12,160 --> 00:11:14,079
of people thinking about privacy in the

313
00:11:14,079 --> 00:11:15,600
next 10 years

314
00:11:15,600 --> 00:11:18,320
this first the top bar here is the same

315
00:11:18,320 --> 00:11:19,839
data i showed earlier

316
00:11:19,839 --> 00:11:21,279
where again about half of our

317
00:11:21,279 --> 00:11:23,519
respondents globally think there will be

318
00:11:23,519 --> 00:11:26,640
more privacy in 10 years

319
00:11:26,640 --> 00:11:28,480
when we separate this out by country

320
00:11:28,480 --> 00:11:30,560
though you can see that the us

321
00:11:30,560 --> 00:11:33,440
ireland and france have far fewer people

322
00:11:33,440 --> 00:11:34,959
responding that they think they will

323
00:11:34,959 --> 00:11:37,279
have more privacy in the next 10 years

324
00:11:37,279 --> 00:11:39,120
you can also see a country like thailand

325
00:11:39,120 --> 00:11:40,880
which matches the global totals

326
00:11:40,880 --> 00:11:43,440
almost exactly and countries like

327
00:11:43,440 --> 00:11:44,720
nigeria and china

328
00:11:44,720 --> 00:11:46,399
where a greater percentage of people

329
00:11:46,399 --> 00:11:48,160
expect to have more privacy in the next

330
00:11:48,160 --> 00:11:49,200
10 years

331
00:11:49,200 --> 00:11:51,839
here we just see a lot more optimism for

332
00:11:51,839 --> 00:11:53,360
privacy

333
00:11:53,360 --> 00:11:55,360
and as we continue through this tour of

334
00:11:55,360 --> 00:11:57,600
insights we will continue to see this

335
00:11:57,600 --> 00:11:59,120
particular divergence

336
00:11:59,120 --> 00:12:01,760
with a lot of optimism for privacy and

337
00:12:01,760 --> 00:12:03,040
for technology

338
00:12:03,040 --> 00:12:05,519
in countries that are still what we

339
00:12:05,519 --> 00:12:08,240
think of as the developing world

340
00:12:08,240 --> 00:12:10,079
in the fixed effect size analysis we

341
00:12:10,079 --> 00:12:11,360
conducted we found

342
00:12:11,360 --> 00:12:13,440
country level differences matter more

343
00:12:13,440 --> 00:12:15,839
than any demographics

344
00:12:15,839 --> 00:12:20,399
including age gender urbanicity and more

345
00:12:20,399 --> 00:12:23,040
so again the country level and regional

346
00:12:23,040 --> 00:12:25,279
differences really do have an impact on

347
00:12:25,279 --> 00:12:28,160
the results that we're looking at today

348
00:12:28,160 --> 00:12:30,480
however there are questions where this

349
00:12:30,480 --> 00:12:31,920
isn't true as well

350
00:12:31,920 --> 00:12:34,639
and so again we cannot just oversimplify

351
00:12:34,639 --> 00:12:35,040
this

352
00:12:35,040 --> 00:12:37,279
to where certain countries will always

353
00:12:37,279 --> 00:12:39,760
behave in tandem or in unison

354
00:12:39,760 --> 00:12:42,079
we also have questions like the

355
00:12:42,079 --> 00:12:43,839
importance of privacy

356
00:12:43,839 --> 00:12:46,560
where we see some differences here

357
00:12:46,560 --> 00:12:48,160
between what percentage of

358
00:12:48,160 --> 00:12:49,680
our participants and respondents

359
00:12:49,680 --> 00:12:51,360
reported privacy being

360
00:12:51,360 --> 00:12:54,320
very important or extremely important

361
00:12:54,320 --> 00:12:56,399
but there really aren't sharp divides

362
00:12:56,399 --> 00:12:57,040
here

363
00:12:57,040 --> 00:12:59,120
and importantly there's no country in

364
00:12:59,120 --> 00:13:00,480
our data set where we see

365
00:13:00,480 --> 00:13:02,399
20 percent or even 10 percent of our

366
00:13:02,399 --> 00:13:03,839
respondents saying

367
00:13:03,839 --> 00:13:06,560
privacy isn't important those gray bars

368
00:13:06,560 --> 00:13:10,479
never get outside of our margin of error

369
00:13:10,800 --> 00:13:13,680
but this brings us back into our second

370
00:13:13,680 --> 00:13:15,440
methodological break

371
00:13:15,440 --> 00:13:17,279
although i just showed a couple of

372
00:13:17,279 --> 00:13:18,959
graphs where i was just putting

373
00:13:18,959 --> 00:13:20,720
bar charts with countries and you could

374
00:13:20,720 --> 00:13:23,120
make easy visual comparisons

375
00:13:23,120 --> 00:13:25,279
i want us to be really careful about

376
00:13:25,279 --> 00:13:26,720
making direct between country

377
00:13:26,720 --> 00:13:28,399
comparisons

378
00:13:28,399 --> 00:13:30,800
apart from the general concerns of

379
00:13:30,800 --> 00:13:32,320
comparing different communities

380
00:13:32,320 --> 00:13:34,720
cultures and societies there are two

381
00:13:34,720 --> 00:13:36,639
specific limitations that make country

382
00:13:36,639 --> 00:13:38,560
comparisons a challenge

383
00:13:38,560 --> 00:13:41,040
first we wrote all of the items in

384
00:13:41,040 --> 00:13:42,240
english originally

385
00:13:42,240 --> 00:13:44,240
and while we worked with local teams and

386
00:13:44,240 --> 00:13:45,440
experts to maintain

387
00:13:45,440 --> 00:13:48,160
very high levels of translation accuracy

388
00:13:48,160 --> 00:13:49,440
concepts and languages

389
00:13:49,440 --> 00:13:51,600
won't always directly map our

390
00:13:51,600 --> 00:13:52,480
respondents

391
00:13:52,480 --> 00:13:56,079
are answering different questions

392
00:13:56,079 --> 00:13:58,959
secondly the panels are also different

393
00:13:58,959 --> 00:13:59,600
that is

394
00:13:59,600 --> 00:14:01,600
the ways we get participants to take the

395
00:14:01,600 --> 00:14:03,440
survey and what we expect those

396
00:14:03,440 --> 00:14:06,320
participants to be representative of

397
00:14:06,320 --> 00:14:09,040
for example in nigeria which we looked

398
00:14:09,040 --> 00:14:10,639
at earlier as being one of the most

399
00:14:10,639 --> 00:14:13,440
optimistic places about privacy

400
00:14:13,440 --> 00:14:15,839
our sample is most representative of the

401
00:14:15,839 --> 00:14:17,360
online population

402
00:14:17,360 --> 00:14:19,279
almost entirely consolidated in urban

403
00:14:19,279 --> 00:14:21,040
areas often younger than our global

404
00:14:21,040 --> 00:14:21,920
averages

405
00:14:21,920 --> 00:14:23,680
we're looking at a different population

406
00:14:23,680 --> 00:14:25,279
there similarly in

407
00:14:25,279 --> 00:14:26,959
india and china our sample is

408
00:14:26,959 --> 00:14:29,360
specifically recruited to cover a mix

409
00:14:29,360 --> 00:14:31,519
based on a preset list of tier 1 and

410
00:14:31,519 --> 00:14:33,279
tier 2 cities

411
00:14:33,279 --> 00:14:35,120
these are both quite different ways of

412
00:14:35,120 --> 00:14:36,720
recruiting participants

413
00:14:36,720 --> 00:14:39,199
than for example a country like the us

414
00:14:39,199 --> 00:14:40,240
where we have

415
00:14:40,240 --> 00:14:43,199
almost true national representation or

416
00:14:43,199 --> 00:14:45,040
in europe where we have really broad

417
00:14:45,040 --> 00:14:46,880
representation in our panels

418
00:14:46,880 --> 00:14:48,880
and that broad representation allows us

419
00:14:48,880 --> 00:14:50,399
to do additional waiting

420
00:14:50,399 --> 00:14:52,079
for example we can wait on some of the

421
00:14:52,079 --> 00:14:54,079
demographic categories that we know very

422
00:14:54,079 --> 00:14:55,279
well at a national level

423
00:14:55,279 --> 00:14:58,560
and from our survey respondents

424
00:14:58,560 --> 00:15:00,720
we take this work very seriously but

425
00:15:00,720 --> 00:15:02,480
myself and the core researchers on this

426
00:15:02,480 --> 00:15:04,000
project are coming from a western

427
00:15:04,000 --> 00:15:04,720
viewpoint

428
00:15:04,720 --> 00:15:06,560
we are all currently based in the united

429
00:15:06,560 --> 00:15:07,920
states and while we worked with

430
00:15:07,920 --> 00:15:09,440
colleagues in other countries

431
00:15:09,440 --> 00:15:11,600
survey experts translators and local

432
00:15:11,600 --> 00:15:14,000
teams we cannot fully separate out that

433
00:15:14,000 --> 00:15:15,920
gaze from this work

434
00:15:15,920 --> 00:15:18,000
okay now while i'd be happy to talk

435
00:15:18,000 --> 00:15:19,839
about our methodological choices here

436
00:15:19,839 --> 00:15:20,399
all day

437
00:15:20,399 --> 00:15:22,160
and the trade-offs behind them follow up

438
00:15:22,160 --> 00:15:23,760
with me after because i want to get back

439
00:15:23,760 --> 00:15:26,959
to the last two insights from our data

440
00:15:26,959 --> 00:15:30,399
so for the fifth feature i want to tell

441
00:15:30,399 --> 00:15:32,480
a story about how our participants have

442
00:15:32,480 --> 00:15:34,800
strong opinions about data use

443
00:15:34,800 --> 00:15:36,560
here we asked our participants a series

444
00:15:36,560 --> 00:15:38,160
of three questions

445
00:15:38,160 --> 00:15:40,079
all three of these questions are about

446
00:15:40,079 --> 00:15:42,160
when governments and law enforcement

447
00:15:42,160 --> 00:15:42,639
should have

448
00:15:42,639 --> 00:15:45,839
access to people's data the first

449
00:15:45,839 --> 00:15:47,759
was if they should have access to

450
00:15:47,759 --> 00:15:49,120
examine specific

451
00:15:49,120 --> 00:15:52,240
data related to a specific investigation

452
00:15:52,240 --> 00:15:54,240
we might think of this as a warrant or a

453
00:15:54,240 --> 00:15:56,160
very focused data request

454
00:15:56,160 --> 00:15:58,880
and globally we saw a split with about 6

455
00:15:58,880 --> 00:16:00,800
in 10 respondents saying yes

456
00:16:00,800 --> 00:16:03,279
and 4 and 10 saying no that this type of

457
00:16:03,279 --> 00:16:06,560
access shouldn't be allowed

458
00:16:07,440 --> 00:16:10,240
we also asked about monitoring specific

459
00:16:10,240 --> 00:16:12,480
individuals suspected of wrongdoing

460
00:16:12,480 --> 00:16:14,800
and again we see a very similar split

461
00:16:14,800 --> 00:16:16,480
with about 6 and 10 saying

462
00:16:16,480 --> 00:16:18,639
yes this type of data access can be

463
00:16:18,639 --> 00:16:19,600
acceptable

464
00:16:19,600 --> 00:16:22,079
even it's a little higher here and so

465
00:16:22,079 --> 00:16:23,440
you might start to think

466
00:16:23,440 --> 00:16:25,920
okay well about 60 percent of people

467
00:16:25,920 --> 00:16:27,600
think governments and law enforcement

468
00:16:27,600 --> 00:16:29,279
should have access to data

469
00:16:29,279 --> 00:16:31,759
and the other 40 of participants think

470
00:16:31,759 --> 00:16:34,079
that they should have none of it

471
00:16:34,079 --> 00:16:36,880
but this actually isn't what we see we

472
00:16:36,880 --> 00:16:37,920
asked one more

473
00:16:37,920 --> 00:16:41,199
item here if monitoring all individuals

474
00:16:41,199 --> 00:16:42,800
including those not suspected of

475
00:16:42,800 --> 00:16:44,560
wrongdoing was acceptable

476
00:16:44,560 --> 00:16:46,959
and here we see that acceptability

477
00:16:46,959 --> 00:16:48,240
dropped sharply

478
00:16:48,240 --> 00:16:51,519
down to about one in six globally

479
00:16:51,519 --> 00:16:53,680
now i don't bring this up to focus on

480
00:16:53,680 --> 00:16:55,680
the questions of government data use

481
00:16:55,680 --> 00:16:58,880
but rather to show that participants and

482
00:16:58,880 --> 00:17:00,959
and the people who are responding to our

483
00:17:00,959 --> 00:17:01,759
surveys

484
00:17:01,759 --> 00:17:04,720
are savvy enough to understand some uses

485
00:17:04,720 --> 00:17:06,079
of data are okay

486
00:17:06,079 --> 00:17:07,199
and there are others they aren't

487
00:17:07,199 --> 00:17:09,199
comfortable with even if it's

488
00:17:09,199 --> 00:17:12,559
the same types of data this again

489
00:17:12,559 --> 00:17:14,559
flies directly in the face of the common

490
00:17:14,559 --> 00:17:16,959
argument there's no hope for privacy

491
00:17:16,959 --> 00:17:19,359
privacy is dead because all the data is

492
00:17:19,359 --> 00:17:20,240
out there

493
00:17:20,240 --> 00:17:22,400
what we see here is many people

494
00:17:22,400 --> 00:17:24,319
reporting they want some limits and

495
00:17:24,319 --> 00:17:25,119
constraints

496
00:17:25,119 --> 00:17:26,720
on how data should be able to be

497
00:17:26,720 --> 00:17:29,679
collected and used

498
00:17:31,520 --> 00:17:33,919
the final insight i want to share today

499
00:17:33,919 --> 00:17:35,440
is an exploration into how our

500
00:17:35,440 --> 00:17:36,960
participants considered

501
00:17:36,960 --> 00:17:39,280
the possibility of being private in

502
00:17:39,280 --> 00:17:41,280
today's world

503
00:17:41,280 --> 00:17:43,520
unlike the result we showed very early

504
00:17:43,520 --> 00:17:45,679
on where nearly everyone thought privacy

505
00:17:45,679 --> 00:17:46,880
was important

506
00:17:46,880 --> 00:17:49,360
here we don't see that same broad

507
00:17:49,360 --> 00:17:50,799
agreement

508
00:17:50,799 --> 00:17:53,200
we see a substantial number of people

509
00:17:53,200 --> 00:17:53,840
saying

510
00:17:53,840 --> 00:17:56,400
privacy is possible and another group

511
00:17:56,400 --> 00:17:56,960
saying

512
00:17:56,960 --> 00:17:59,600
privacy isn't possible at all we see a

513
00:17:59,600 --> 00:18:01,440
plurality of respondents more than one

514
00:18:01,440 --> 00:18:02,000
in three

515
00:18:02,000 --> 00:18:04,400
right in the middle this means the same

516
00:18:04,400 --> 00:18:05,360
people who

517
00:18:05,360 --> 00:18:07,600
told us earlier that privacy was very or

518
00:18:07,600 --> 00:18:09,360
extremely important to them

519
00:18:09,360 --> 00:18:12,320
think it's also not necessarily simple

520
00:18:12,320 --> 00:18:13,600
to achieve

521
00:18:13,600 --> 00:18:15,600
and maybe some of them think it's

522
00:18:15,600 --> 00:18:18,080
impossible

523
00:18:18,799 --> 00:18:22,240
i don't bring this up to end with the

524
00:18:22,240 --> 00:18:23,600
difficulty of achieving

525
00:18:23,600 --> 00:18:26,799
privacy on this pessimistic point but

526
00:18:26,799 --> 00:18:28,000
rather as a challenge we

527
00:18:28,000 --> 00:18:30,640
must face as we look back over

528
00:18:30,640 --> 00:18:32,240
everything we've seen today

529
00:18:32,240 --> 00:18:34,320
we see that people aren't sure if

530
00:18:34,320 --> 00:18:36,080
privacy is possible online

531
00:18:36,080 --> 00:18:38,880
but it's important even as many see

532
00:18:38,880 --> 00:18:40,960
privacy is challenging to achieve

533
00:18:40,960 --> 00:18:42,960
as having limits in some cases and

534
00:18:42,960 --> 00:18:45,200
having gaps where attacks and harms can

535
00:18:45,200 --> 00:18:46,080
occur

536
00:18:46,080 --> 00:18:48,240
we see across the world there are people

537
00:18:48,240 --> 00:18:50,400
concerned about the future of privacy

538
00:18:50,400 --> 00:18:54,720
and many who remain optimistic

539
00:18:54,960 --> 00:18:57,280
as we reflect back on this and all of

540
00:18:57,280 --> 00:18:58,559
the data that we have

541
00:18:58,559 --> 00:19:00,400
i hope we've shown that people do not

542
00:19:00,400 --> 00:19:02,799
want or believe that privacy is dead

543
00:19:02,799 --> 00:19:04,160
but i don't think the audience here at

544
00:19:04,160 --> 00:19:06,240
enigma needed me to tell you that

545
00:19:06,240 --> 00:19:08,320
when we think about privacy we need to

546
00:19:08,320 --> 00:19:10,240
think about both the individual moments

547
00:19:10,240 --> 00:19:11,840
these privacy decisions

548
00:19:11,840 --> 00:19:13,760
data accesses and challenging events

549
00:19:13,760 --> 00:19:15,679
that actually impact people's lives

550
00:19:15,679 --> 00:19:17,600
but also their overall attitudes and

551
00:19:17,600 --> 00:19:18,720
sentiments

552
00:19:18,720 --> 00:19:19,919
and we've seen that these will be

553
00:19:19,919 --> 00:19:21,840
interpreted differently by people and

554
00:19:21,840 --> 00:19:23,520
groups around the world

555
00:19:23,520 --> 00:19:24,960
and also that people are able to

556
00:19:24,960 --> 00:19:27,039
consider and opine and are willing to

557
00:19:27,039 --> 00:19:29,840
engage with the subtleties of privacy

558
00:19:29,840 --> 00:19:32,320
to close i believe it's our role not

559
00:19:32,320 --> 00:19:34,080
only to study privacy views but to

560
00:19:34,080 --> 00:19:35,919
protect people's privacy

561
00:19:35,919 --> 00:19:37,760
and to do this i believe we need to

562
00:19:37,760 --> 00:19:40,000
invest in three areas

563
00:19:40,000 --> 00:19:42,240
one as a researcher i'm convinced we

564
00:19:42,240 --> 00:19:43,360
need more deep

565
00:19:43,360 --> 00:19:45,760
foundational research into both specific

566
00:19:45,760 --> 00:19:47,520
privacy moments the decisions people

567
00:19:47,520 --> 00:19:49,600
make and how they navigate these moments

568
00:19:49,600 --> 00:19:51,440
but also research that respects and

569
00:19:51,440 --> 00:19:53,520
seeks to understand the cultural

570
00:19:53,520 --> 00:19:55,520
and social ways privacy manifests for

571
00:19:55,520 --> 00:19:56,960
different communities

572
00:19:56,960 --> 00:19:58,799
particularly those that differ from our

573
00:19:58,799 --> 00:20:00,080
own often western

574
00:20:00,080 --> 00:20:01,679
white or silicon valley focused

575
00:20:01,679 --> 00:20:03,840
conceptions of privacy and data

576
00:20:03,840 --> 00:20:06,320
we hope this talk and this data helps to

577
00:20:06,320 --> 00:20:07,200
motivate

578
00:20:07,200 --> 00:20:10,480
more research in this vein

579
00:20:10,480 --> 00:20:13,200
second we need to invest in protections

580
00:20:13,200 --> 00:20:15,280
and solutions whether technological

581
00:20:15,280 --> 00:20:17,280
regulatory or simply by making our

582
00:20:17,280 --> 00:20:18,880
existing systems

583
00:20:18,880 --> 00:20:21,440
more usable and accessible to give

584
00:20:21,440 --> 00:20:23,200
people control in some cases

585
00:20:23,200 --> 00:20:25,039
but even more importantly to help them

586
00:20:25,039 --> 00:20:26,880
feel there are some systems in this

587
00:20:26,880 --> 00:20:27,520
world

588
00:20:27,520 --> 00:20:29,360
that are designed to make the privacy

589
00:20:29,360 --> 00:20:32,240
they desire feel possible

590
00:20:32,240 --> 00:20:34,080
and finally we need to shift the

591
00:20:34,080 --> 00:20:35,280
narrative

592
00:20:35,280 --> 00:20:37,280
privacy may be struggling but we must

593
00:20:37,280 --> 00:20:38,720
maintain some hope

594
00:20:38,720 --> 00:20:41,120
some who claim privacy will die are

595
00:20:41,120 --> 00:20:42,559
saying this to make a claim of

596
00:20:42,559 --> 00:20:43,760
inevitability

597
00:20:43,760 --> 00:20:45,440
because they want to own a future where

598
00:20:45,440 --> 00:20:47,360
privacy is gone and they can use data

599
00:20:47,360 --> 00:20:49,200
however they see fit

600
00:20:49,200 --> 00:20:51,280
this isn't the only way and we must

601
00:20:51,280 --> 00:20:52,640
provide alternate

602
00:20:52,640 --> 00:20:54,880
more optimistic visions of how privacy

603
00:20:54,880 --> 00:20:57,600
protective futures can be built

604
00:20:57,600 --> 00:20:59,520
thank you all so much and i'm happy to

605
00:20:59,520 --> 00:21:03,840
take questions

