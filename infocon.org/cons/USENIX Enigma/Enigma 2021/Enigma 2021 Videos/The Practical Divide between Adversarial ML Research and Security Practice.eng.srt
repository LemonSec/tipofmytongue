1
00:00:09,120 --> 00:00:10,240
greetings

2
00:00:10,240 --> 00:00:12,400
my name is hayer anderson and my talk

3
00:00:12,400 --> 00:00:14,559
today addresses the

4
00:00:14,559 --> 00:00:16,480
tangible gap between researchers and

5
00:00:16,480 --> 00:00:18,160
practitioners when it comes

6
00:00:18,160 --> 00:00:20,960
to securing machine learning and today i

7
00:00:20,960 --> 00:00:22,320
hope that you will come away with

8
00:00:22,320 --> 00:00:25,599
two main takeaways one that

9
00:00:25,599 --> 00:00:28,080
security of machine learning has become

10
00:00:28,080 --> 00:00:30,880
an information security problem

11
00:00:30,880 --> 00:00:33,280
but number two that there are simple

12
00:00:33,280 --> 00:00:34,160
things that

13
00:00:34,160 --> 00:00:37,200
you and i can do today to secure ml that

14
00:00:37,200 --> 00:00:37,520
have

15
00:00:37,520 --> 00:00:39,840
much more in common with traditional

16
00:00:39,840 --> 00:00:41,600
security practice

17
00:00:41,600 --> 00:00:44,320
than ingesting and implementing papers

18
00:00:44,320 --> 00:00:45,760
important as they are

19
00:00:45,760 --> 00:00:47,200
from the latest to machine learning

20
00:00:47,200 --> 00:00:49,760
conferences

21
00:00:51,199 --> 00:00:52,719
let me begin by demonstrating what i

22
00:00:52,719 --> 00:00:55,039
think is a fundamental paradigm

23
00:00:55,039 --> 00:00:58,480
mismatch many encapsulate their

24
00:00:58,480 --> 00:00:59,920
understanding of machine learning

25
00:00:59,920 --> 00:01:01,039
security

26
00:01:01,039 --> 00:01:03,920
by adversarial examples illustrated here

27
00:01:03,920 --> 00:01:05,680
by these two now cliche

28
00:01:05,680 --> 00:01:08,880
pictures of a tabby cat

29
00:01:08,880 --> 00:01:10,799
subtle changes to the image on the right

30
00:01:10,799 --> 00:01:12,960
cause a machine learning model to

31
00:01:12,960 --> 00:01:13,840
classify

32
00:01:13,840 --> 00:01:17,119
the tabby cat as guacamole this has come

33
00:01:17,119 --> 00:01:20,320
to iconify intentional

34
00:01:20,320 --> 00:01:22,880
or adversarial failure modes of modern

35
00:01:22,880 --> 00:01:26,080
machine learning whether an attacker

36
00:01:26,080 --> 00:01:28,560
when an attacker presents specific worst

37
00:01:28,560 --> 00:01:30,720
case conditions to render

38
00:01:30,720 --> 00:01:33,119
sometimes silly but sometimes critical

39
00:01:33,119 --> 00:01:33,920
machine

40
00:01:33,920 --> 00:01:36,640
learning model failures the research

41
00:01:36,640 --> 00:01:38,000
field has been around for more than a

42
00:01:38,000 --> 00:01:39,680
decade but just in the last

43
00:01:39,680 --> 00:01:42,159
few years researchers have written more

44
00:01:42,159 --> 00:01:42,720
than 2

45
00:01:42,720 --> 00:01:45,840
000 papers about the weaknesses

46
00:01:45,840 --> 00:01:48,640
and proposed defenses for machine

47
00:01:48,640 --> 00:01:51,520
learning under attack

48
00:01:52,840 --> 00:01:55,840
meanwhile security professionals

49
00:01:55,840 --> 00:01:58,159
are dealing with things like solarwinds

50
00:01:58,159 --> 00:02:01,119
software updates and ssl patches

51
00:02:01,119 --> 00:02:04,240
phishing and education ransomware

52
00:02:04,240 --> 00:02:05,759
and cloud credentials that you just

53
00:02:05,759 --> 00:02:07,280
checked into github

54
00:02:07,280 --> 00:02:09,038
and they're left to wonder what in the

55
00:02:09,038 --> 00:02:11,120
world does

56
00:02:11,120 --> 00:02:13,920
the security of a tabby cat have to do

57
00:02:13,920 --> 00:02:15,520
with the security problems that i'm

58
00:02:15,520 --> 00:02:16,080
facing

59
00:02:16,080 --> 00:02:18,400
today

60
00:02:19,120 --> 00:02:21,440
so it should come as no surprise that

61
00:02:21,440 --> 00:02:22,400
the state

62
00:02:22,400 --> 00:02:25,599
of security of ml is relatively low

63
00:02:25,599 --> 00:02:27,440
as my colleagues at microsoft discovered

64
00:02:27,440 --> 00:02:29,840
in a recent survey of organizations

65
00:02:29,840 --> 00:02:32,480
awareness or at least the priority of

66
00:02:32,480 --> 00:02:34,640
the security risks to ml

67
00:02:34,640 --> 00:02:38,800
is low it was judged as too futuristic

68
00:02:38,800 --> 00:02:40,480
and when especially when compared with

69
00:02:40,480 --> 00:02:42,239
the pressing traditional security

70
00:02:42,239 --> 00:02:44,400
threats

71
00:02:44,400 --> 00:02:46,319
number two even as ml researchers

72
00:02:46,319 --> 00:02:48,239
advanced the state of our understanding

73
00:02:48,239 --> 00:02:51,120
there are still fundamental theory gaps

74
00:02:51,120 --> 00:02:52,400
about what makes

75
00:02:52,400 --> 00:02:56,160
ml secure as a result our collective

76
00:02:56,160 --> 00:02:57,599
security posture

77
00:02:57,599 --> 00:03:01,120
is close to zero in the microsoft survey

78
00:03:01,120 --> 00:03:04,400
i referenced it was found that almost 90

79
00:03:04,400 --> 00:03:08,720
25 of 20 organizations spoken to did not

80
00:03:08,720 --> 00:03:12,400
know how to secure their ml systems

81
00:03:12,400 --> 00:03:14,879
that is a risk that's especially

82
00:03:14,879 --> 00:03:17,360
applicable to non-security applications

83
00:03:17,360 --> 00:03:18,640
of ml

84
00:03:18,640 --> 00:03:20,560
for which adversaries can cause

85
00:03:20,560 --> 00:03:22,560
magnified security risks

86
00:03:22,560 --> 00:03:24,799
uh in in domains especially where people

87
00:03:24,799 --> 00:03:28,159
aren't thinking about security first

88
00:03:28,159 --> 00:03:30,400
indeed as a recent study pointed out

89
00:03:30,400 --> 00:03:32,720
machine learning presents a new attack

90
00:03:32,720 --> 00:03:33,680
surface

91
00:03:33,680 --> 00:03:36,319
that application leaders must anticipate

92
00:03:36,319 --> 00:03:39,518
to mitigate those risks

93
00:03:41,040 --> 00:03:42,879
to demonstrate some of the lowest

94
00:03:42,879 --> 00:03:45,360
hanging ml security risks

95
00:03:45,360 --> 00:03:47,519
today i will walk you through parts of a

96
00:03:47,519 --> 00:03:48,879
red team engagement

97
00:03:48,879 --> 00:03:51,200
that my team and i participated in at

98
00:03:51,200 --> 00:03:52,400
microsoft

99
00:03:52,400 --> 00:03:54,560
i hope you welcome the fact that mature

100
00:03:54,560 --> 00:03:56,720
companies like microsoft

101
00:03:56,720 --> 00:04:00,000
should and do conduct red team exercises

102
00:04:00,000 --> 00:04:02,080
against their own products and services

103
00:04:02,080 --> 00:04:04,480
including ml

104
00:04:04,480 --> 00:04:06,640
before i begin it's important to make a

105
00:04:06,640 --> 00:04:07,840
distinction between an

106
00:04:07,840 --> 00:04:10,720
adversarial ml attack tree in which

107
00:04:10,720 --> 00:04:11,680
subverting

108
00:04:11,680 --> 00:04:14,799
the ml model is the end objective

109
00:04:14,799 --> 00:04:17,279
and a red teaming attack tree where the

110
00:04:17,279 --> 00:04:19,040
ml model is merely a cog

111
00:04:19,040 --> 00:04:22,160
in a system and that system has

112
00:04:22,160 --> 00:04:24,400
far greater ramifications than that ml

113
00:04:24,400 --> 00:04:26,240
model alone

114
00:04:26,240 --> 00:04:28,240
for example if an adversary wanted to

115
00:04:28,240 --> 00:04:30,080
commit expense fraud

116
00:04:30,080 --> 00:04:33,199
she could do this by digitally altering

117
00:04:33,199 --> 00:04:35,600
real receipts to full and automated

118
00:04:35,600 --> 00:04:36,960
systems similar

119
00:04:36,960 --> 00:04:40,080
to the tabby cat and guacamole example

120
00:04:40,080 --> 00:04:42,320
however a much easier thing to do is to

121
00:04:42,320 --> 00:04:43,520
simply submit

122
00:04:43,520 --> 00:04:46,160
valid receipts to the automated system

123
00:04:46,160 --> 00:04:47,759
that do not represent legitimate

124
00:04:47,759 --> 00:04:49,919
business expenses

125
00:04:49,919 --> 00:04:52,960
so in some cases an email model or the

126
00:04:52,960 --> 00:04:54,000
data

127
00:04:54,000 --> 00:04:56,639
may indeed be the crown jewels but as

128
00:04:56,639 --> 00:04:57,759
you can see in this

129
00:04:57,759 --> 00:05:00,880
example securing an ml model is often

130
00:05:00,880 --> 00:05:02,080
only one part

131
00:05:02,080 --> 00:05:04,560
of securing against a broader attack and

132
00:05:04,560 --> 00:05:05,759
in my view

133
00:05:05,759 --> 00:05:08,479
it makes the most sense to think about

134
00:05:08,479 --> 00:05:10,639
ml security as part of

135
00:05:10,639 --> 00:05:13,280
not separate from the broader security

136
00:05:13,280 --> 00:05:15,840
concerns

137
00:05:16,000 --> 00:05:18,880
when it comes to red teaming when an ml

138
00:05:18,880 --> 00:05:20,080
model is involved

139
00:05:20,080 --> 00:05:22,080
there are a number of touch points where

140
00:05:22,080 --> 00:05:24,880
we now taking an attacker perspective

141
00:05:24,880 --> 00:05:28,080
can cause confidentiality integrity or

142
00:05:28,080 --> 00:05:29,440
availability concerns

143
00:05:29,440 --> 00:05:32,720
violations during any

144
00:05:32,720 --> 00:05:36,400
phase before model deployment

145
00:05:36,400 --> 00:05:38,639
data curation labeling feature

146
00:05:38,639 --> 00:05:39,520
extraction

147
00:05:39,520 --> 00:05:41,840
model training and model validation

148
00:05:41,840 --> 00:05:43,280
there's at least an opportunity

149
00:05:43,280 --> 00:05:46,960
for both access to private data

150
00:05:46,960 --> 00:05:50,800
and to to make causative effect

151
00:05:50,800 --> 00:05:52,880
on the model causes of attacker

152
00:05:52,880 --> 00:05:55,360
influence such as model poisoning

153
00:05:55,360 --> 00:05:57,919
or model back dooring those would

154
00:05:57,919 --> 00:05:59,039
require of course

155
00:05:59,039 --> 00:06:01,120
kind of write permissions to the model

156
00:06:01,120 --> 00:06:03,199
data or training code

157
00:06:03,199 --> 00:06:05,840
when a model is deployed after that

158
00:06:05,840 --> 00:06:06,639
deployment

159
00:06:06,639 --> 00:06:08,720
it is still subject to integrity

160
00:06:08,720 --> 00:06:10,800
violations

161
00:06:10,800 --> 00:06:12,720
through model evasion like the tabby cat

162
00:06:12,720 --> 00:06:15,680
example or confidentiality violations

163
00:06:15,680 --> 00:06:16,080
through

164
00:06:16,080 --> 00:06:19,120
through model model stealing and model

165
00:06:19,120 --> 00:06:20,400
inversion

166
00:06:20,400 --> 00:06:23,440
interestingly if a model pipeline

167
00:06:23,440 --> 00:06:25,600
leverages feedback as part of its data

168
00:06:25,600 --> 00:06:27,440
collection process

169
00:06:27,440 --> 00:06:30,479
an attack can affect a causative

170
00:06:30,479 --> 00:06:32,800
influence without those so-called write

171
00:06:32,800 --> 00:06:34,160
permissions to the model

172
00:06:34,160 --> 00:06:37,759
development cycle as you'll see

173
00:06:37,759 --> 00:06:40,479
in our red team engagement case study in

174
00:06:40,479 --> 00:06:41,919
the following slides

175
00:06:41,919 --> 00:06:44,319
being familiar with the ml development

176
00:06:44,319 --> 00:06:45,520
life cycle presents

177
00:06:45,520 --> 00:06:48,639
opportunities to cause change changes to

178
00:06:48,639 --> 00:06:49,759
the system

179
00:06:49,759 --> 00:06:51,919
and from a defender's perspective you

180
00:06:51,919 --> 00:06:52,800
should think about

181
00:06:52,800 --> 00:06:56,960
securing these separate distinct phases

182
00:06:57,440 --> 00:06:59,520
for the case study let's look at an

183
00:06:59,520 --> 00:07:00,479
exercise

184
00:07:00,479 --> 00:07:03,280
against an internal microsoft cloud

185
00:07:03,280 --> 00:07:04,800
provisioning service

186
00:07:04,800 --> 00:07:07,840
that uses ml to take

187
00:07:07,840 --> 00:07:10,560
automated actions whether or not cloud

188
00:07:10,560 --> 00:07:11,440
security

189
00:07:11,440 --> 00:07:14,560
uh cloud provision uh club provisioning

190
00:07:14,560 --> 00:07:16,319
is your cup of tea i think this example

191
00:07:16,319 --> 00:07:18,160
will demonstrate

192
00:07:18,160 --> 00:07:20,400
similar security principles of a system

193
00:07:20,400 --> 00:07:22,560
that you might know more intimately

194
00:07:22,560 --> 00:07:25,120
in this system a web portal request for

195
00:07:25,120 --> 00:07:26,000
an internal

196
00:07:26,000 --> 00:07:29,280
software resource like a database of a

197
00:07:29,280 --> 00:07:31,199
certain size or a virtual machine with

198
00:07:31,199 --> 00:07:33,520
specified memory and cpu

199
00:07:33,520 --> 00:07:37,199
is processed by this large system

200
00:07:37,199 --> 00:07:39,520
that decides which physical hardware the

201
00:07:39,520 --> 00:07:40,319
resources

202
00:07:40,319 --> 00:07:43,520
should occupy as you can imagine due

203
00:07:43,520 --> 00:07:44,879
to the number the sheer number of

204
00:07:44,879 --> 00:07:46,560
employee employees at a large company

205
00:07:46,560 --> 00:07:47,599
like microsoft

206
00:07:47,599 --> 00:07:50,720
using these logical resources and their

207
00:07:50,720 --> 00:07:52,960
underlying physical hardware

208
00:07:52,960 --> 00:07:55,280
the the savings achieved by efficiently

209
00:07:55,280 --> 00:07:56,240
mapping

210
00:07:56,240 --> 00:07:58,800
resources to hardware is is not

211
00:07:58,800 --> 00:08:01,039
insignificant

212
00:08:01,039 --> 00:08:03,199
in our red team engagement we took the

213
00:08:03,199 --> 00:08:06,639
role of an adversary who wished to cause

214
00:08:06,639 --> 00:08:10,080
uh an indiscriminate denial of service

215
00:08:10,080 --> 00:08:11,120
attack

216
00:08:11,120 --> 00:08:12,639
through a so-called noisy neighbor

217
00:08:12,639 --> 00:08:14,720
attack by tricking

218
00:08:14,720 --> 00:08:18,160
the system to deploy hungry

219
00:08:18,160 --> 00:08:21,599
resources on physical hardware

220
00:08:21,599 --> 00:08:24,840
that contained high availability service

221
00:08:24,840 --> 00:08:27,199
containers as you'll see

222
00:08:27,199 --> 00:08:30,479
evading the ml model part of this

223
00:08:30,479 --> 00:08:34,240
is a linchpin for this exercise so that

224
00:08:34,240 --> 00:08:36,479
the question is by asking for just the

225
00:08:36,479 --> 00:08:37,360
right

226
00:08:37,360 --> 00:08:40,479
services could we cause

227
00:08:40,479 --> 00:08:43,679
a system availability violation

228
00:08:43,679 --> 00:08:47,040
by causing an ml integrity violation

229
00:08:47,040 --> 00:08:48,800
for an inner machine learning service

230
00:08:48,800 --> 00:08:51,040
project

231
00:08:51,040 --> 00:08:55,120
an inter ml service

232
00:08:55,120 --> 00:08:57,680
and importantly at no point do we have

233
00:08:57,680 --> 00:08:58,240
direct

234
00:08:58,240 --> 00:09:01,680
access to the model it's only accessible

235
00:09:01,680 --> 00:09:02,320
through

236
00:09:02,320 --> 00:09:05,839
internal private api

237
00:09:06,880 --> 00:09:10,560
so let's walk through uh the details

238
00:09:10,560 --> 00:09:13,519
of uh the red team attack on this

239
00:09:13,519 --> 00:09:14,560
internal service

240
00:09:14,560 --> 00:09:17,120
and in this exercise we begin with

241
00:09:17,120 --> 00:09:19,120
insider access

242
00:09:19,120 --> 00:09:23,279
through a valid account

243
00:09:23,279 --> 00:09:26,399
as an attacker now on the inside we kind

244
00:09:26,399 --> 00:09:26,720
of

245
00:09:26,720 --> 00:09:28,880
knew from a reconnaissance phase what we

246
00:09:28,880 --> 00:09:30,480
were looking for

247
00:09:30,480 --> 00:09:32,720
from external publications we had we

248
00:09:32,720 --> 00:09:34,240
knew that there was a resource

249
00:09:34,240 --> 00:09:36,000
provisioning service that used machine

250
00:09:36,000 --> 00:09:38,959
learning and we also knew that an ml

251
00:09:38,959 --> 00:09:40,480
model would have artifacts

252
00:09:40,480 --> 00:09:44,160
like training data a model file

253
00:09:44,160 --> 00:09:46,480
and as a red team we were we found we

254
00:09:46,480 --> 00:09:47,920
find ourselves extremely lucky to find

255
00:09:47,920 --> 00:09:49,600
that our credentials

256
00:09:49,600 --> 00:09:53,839
gave us access to two critical pieces of

257
00:09:53,839 --> 00:09:54,480
this

258
00:09:54,480 --> 00:09:57,200
internal ml model first we discovered

259
00:09:57,200 --> 00:09:57,839
that we had

260
00:09:57,839 --> 00:10:00,880
read-only access to some data

261
00:10:00,880 --> 00:10:03,360
some of which turned out to be training

262
00:10:03,360 --> 00:10:06,079
data to the model but cared about

263
00:10:06,079 --> 00:10:09,360
a second though we didn't find any

264
00:10:09,360 --> 00:10:10,959
detailed code for the entire model

265
00:10:10,959 --> 00:10:13,680
pipeline we did discover key pieces

266
00:10:13,680 --> 00:10:16,959
for the data collection featurization

267
00:10:16,959 --> 00:10:21,120
and that alone was enough for us

268
00:10:21,120 --> 00:10:24,720
to create our own model even before

269
00:10:24,720 --> 00:10:25,760
discovering

270
00:10:25,760 --> 00:10:28,800
uh any internal api access to that part

271
00:10:28,800 --> 00:10:30,720
of the system

272
00:10:30,720 --> 00:10:32,320
so this was much easier and more

273
00:10:32,320 --> 00:10:34,320
reliable as a process

274
00:10:34,320 --> 00:10:37,279
than um maybe using a sophisticated

275
00:10:37,279 --> 00:10:38,160
black box

276
00:10:38,160 --> 00:10:40,880
model stealing algorithm against the

277
00:10:40,880 --> 00:10:42,079
internal service

278
00:10:42,079 --> 00:10:44,560
instead we could get access to data and

279
00:10:44,560 --> 00:10:46,880
code and build our own model

280
00:10:46,880 --> 00:10:50,720
as it turns out anyway direct access to

281
00:10:50,720 --> 00:10:51,600
that model

282
00:10:51,600 --> 00:10:55,440
would have proved to be more difficult

283
00:10:57,200 --> 00:10:59,519
so with that even though even though

284
00:10:59,519 --> 00:11:00,800
we've built a now

285
00:11:00,800 --> 00:11:04,399
a poor man a straw man replica model um

286
00:11:04,399 --> 00:11:06,240
that's likely not identical to the

287
00:11:06,240 --> 00:11:08,079
production model it did allow us

288
00:11:08,079 --> 00:11:11,440
to study as a straw man

289
00:11:11,440 --> 00:11:13,440
and formulate and test an attack

290
00:11:13,440 --> 00:11:15,279
strategy offline

291
00:11:15,279 --> 00:11:17,839
this is important because we didn't know

292
00:11:17,839 --> 00:11:18,959
what kind

293
00:11:18,959 --> 00:11:21,839
of logging and monitoring and auditing

294
00:11:21,839 --> 00:11:23,360
would have been attached

295
00:11:23,360 --> 00:11:26,560
to the deployed model service even even

296
00:11:26,560 --> 00:11:27,519
if we had

297
00:11:27,519 --> 00:11:30,560
direct access to it so we could do do

298
00:11:30,560 --> 00:11:31,839
all that in a way

299
00:11:31,839 --> 00:11:34,480
that was we thought could be blind to

300
00:11:34,480 --> 00:11:37,200
any observers

301
00:11:37,920 --> 00:11:39,760
so by conducting the offline attack of

302
00:11:39,760 --> 00:11:41,760
the replica model we collected

303
00:11:41,760 --> 00:11:44,399
a number of evasive variants that is

304
00:11:44,399 --> 00:11:45,760
inputs

305
00:11:45,760 --> 00:11:49,120
to the inner machine learning model

306
00:11:49,120 --> 00:11:52,800
that would guarantee an over-subscribe

307
00:11:52,800 --> 00:11:55,279
prediction in particular we discovered

308
00:11:55,279 --> 00:11:57,440
what services we could provision

309
00:11:57,440 --> 00:12:00,560
uh vms with what specs database types

310
00:12:00,560 --> 00:12:03,600
of what size and replication factors at

311
00:12:03,600 --> 00:12:05,680
what times of day and days of the week

312
00:12:05,680 --> 00:12:07,360
and under what conditions that this

313
00:12:07,360 --> 00:12:08,800
model would predict

314
00:12:08,800 --> 00:12:12,639
our model to be a friendly low resource

315
00:12:12,639 --> 00:12:13,760
container

316
00:12:13,760 --> 00:12:15,360
to be oversubscribed with high

317
00:12:15,360 --> 00:12:17,920
confidence

318
00:12:20,639 --> 00:12:23,839
so those inputs that we collected

319
00:12:23,839 --> 00:12:25,760
turned out to be uh that you know the

320
00:12:25,760 --> 00:12:28,000
linchpin for the entire engagement

321
00:12:28,000 --> 00:12:30,480
and we collected them for later use so

322
00:12:30,480 --> 00:12:31,519
that we could

323
00:12:31,519 --> 00:12:34,560
create a new account and then with that

324
00:12:34,560 --> 00:12:35,440
account

325
00:12:35,440 --> 00:12:37,680
we could use inputs previously

326
00:12:37,680 --> 00:12:39,040
discovered to request

327
00:12:39,040 --> 00:12:42,480
resources and deploy these so-called

328
00:12:42,480 --> 00:12:46,240
noisy neighbors with a payload

329
00:12:46,240 --> 00:12:48,079
and as is often the case in machine

330
00:12:48,079 --> 00:12:49,760
learning even though we attack

331
00:12:49,760 --> 00:12:51,519
a straw man there's a good chance that

332
00:12:51,519 --> 00:12:52,959
those

333
00:12:52,959 --> 00:12:55,680
the the inputs to that straw man model

334
00:12:55,680 --> 00:12:57,200
are also effective

335
00:12:57,200 --> 00:12:59,519
to the live deployed model at least for

336
00:12:59,519 --> 00:13:00,560
a window of time

337
00:13:00,560 --> 00:13:03,920
before retraining so

338
00:13:03,920 --> 00:13:06,720
with having those resource requests that

339
00:13:06,720 --> 00:13:08,639
would guarantee and over subscribe

340
00:13:08,639 --> 00:13:09,440
condition

341
00:13:09,440 --> 00:13:11,680
we could an instrument a virtual machine

342
00:13:11,680 --> 00:13:12,800
for example

343
00:13:12,800 --> 00:13:16,480
with um with actually uh hungry resource

344
00:13:16,480 --> 00:13:18,639
payloads high cpu utilization

345
00:13:18,639 --> 00:13:21,200
and memory usage which would then be

346
00:13:21,200 --> 00:13:22,880
over provisioned and caused into the

347
00:13:22,880 --> 00:13:23,680
life service

348
00:13:23,680 --> 00:13:25,839
to the other containers on the same

349
00:13:25,839 --> 00:13:28,800
physical host

350
00:13:30,480 --> 00:13:32,959
i'll note that this exercise was

351
00:13:32,959 --> 00:13:34,079
recently summarized

352
00:13:34,079 --> 00:13:36,480
and published by colleagues in an

353
00:13:36,480 --> 00:13:38,639
adversarial ml threat matrix

354
00:13:38,639 --> 00:13:40,480
which i am citing here at the top that i

355
00:13:40,480 --> 00:13:41,920
encourage you to go look at

356
00:13:41,920 --> 00:13:44,720
check out to summarize again to recap

357
00:13:44,720 --> 00:13:45,519
like any

358
00:13:45,519 --> 00:13:47,120
red team operation it included

359
00:13:47,120 --> 00:13:50,000
reconnaissance initial access

360
00:13:50,000 --> 00:13:52,639
and had striking similarities to

361
00:13:52,639 --> 00:13:55,519
adversary activity on any it system

362
00:13:55,519 --> 00:13:58,160
there was exfiltration evasion and

363
00:13:58,160 --> 00:13:59,920
execution finally ending up

364
00:13:59,920 --> 00:14:03,440
with the impact of service availability

365
00:14:03,440 --> 00:14:05,360
there are several lessons to glean that

366
00:14:05,360 --> 00:14:08,320
can be extrapolated from this exercise

367
00:14:08,320 --> 00:14:11,760
number one internal models are not

368
00:14:11,760 --> 00:14:14,959
safe by default that is an argument that

369
00:14:14,959 --> 00:14:16,160
is simply

370
00:14:16,160 --> 00:14:19,440
again security by obscurity in disguise

371
00:14:19,440 --> 00:14:21,120
even though a model may not be directly

372
00:14:21,120 --> 00:14:22,639
accessible

373
00:14:22,639 --> 00:14:25,519
to the outside world there are paths by

374
00:14:25,519 --> 00:14:27,519
which an attacker can exploit them

375
00:14:27,519 --> 00:14:30,480
to cause cascading downstream effects in

376
00:14:30,480 --> 00:14:32,639
an overall system

377
00:14:32,639 --> 00:14:35,440
number two permissive access to data or

378
00:14:35,440 --> 00:14:38,079
code can lead to simple model data theft

379
00:14:38,079 --> 00:14:39,920
and this seems really simple simple but

380
00:14:39,920 --> 00:14:41,839
um ask your data science team

381
00:14:41,839 --> 00:14:44,959
uh if they are uh you know how they how

382
00:14:44,959 --> 00:14:45,360
they

383
00:14:45,360 --> 00:14:47,040
set up permissions around their data and

384
00:14:47,040 --> 00:14:48,560
their code

385
00:14:48,560 --> 00:14:50,639
who cares about algorithmic model

386
00:14:50,639 --> 00:14:52,160
inversion when an attacker

387
00:14:52,160 --> 00:14:54,480
could more easily just replicate your

388
00:14:54,480 --> 00:14:55,279
model

389
00:14:55,279 --> 00:14:58,720
and do it more precisely by exploiting

390
00:14:58,720 --> 00:15:00,959
the lack of simple security hygiene like

391
00:15:00,959 --> 00:15:02,639
setting up access controls that are not

392
00:15:02,639 --> 00:15:05,040
over permissive

393
00:15:05,040 --> 00:15:07,120
number three in a system design

394
00:15:07,120 --> 00:15:09,199
carefully check the model output before

395
00:15:09,199 --> 00:15:10,000
taking

396
00:15:10,000 --> 00:15:13,279
prescriptive actions um

397
00:15:13,279 --> 00:15:16,000
what what is a what is that um that

398
00:15:16,000 --> 00:15:17,600
sanity check it could be a human in a

399
00:15:17,600 --> 00:15:17,920
loop

400
00:15:17,920 --> 00:15:20,320
in some cases maybe auditing one out of

401
00:15:20,320 --> 00:15:21,040
every thousand

402
00:15:21,040 --> 00:15:24,160
outputs or it could be um in a system

403
00:15:24,160 --> 00:15:26,720
like mid for automation like the one uh

404
00:15:26,720 --> 00:15:27,440
that uh

405
00:15:27,440 --> 00:15:30,480
we we described it could be uh simple

406
00:15:30,480 --> 00:15:31,120
sanity

407
00:15:31,120 --> 00:15:33,839
checks and guard rails like you should

408
00:15:33,839 --> 00:15:34,320
never

409
00:15:34,320 --> 00:15:37,680
over subscribe a 24 core vm no matter

410
00:15:37,680 --> 00:15:40,079
what the model predicts

411
00:15:40,079 --> 00:15:42,399
it's important also to as number four

412
00:15:42,399 --> 00:15:44,639
important to log

413
00:15:44,639 --> 00:15:48,639
model behavior and development during

414
00:15:48,639 --> 00:15:51,440
training and deployment even if there's

415
00:15:51,440 --> 00:15:51,839
no

416
00:15:51,839 --> 00:15:54,800
active program or person monitoring

417
00:15:54,800 --> 00:15:56,320
those logs in real time

418
00:15:56,320 --> 00:15:58,079
we should always have an assumed breach

419
00:15:58,079 --> 00:16:00,240
mindset where having

420
00:16:00,240 --> 00:16:02,560
access to those logs could be critical

421
00:16:02,560 --> 00:16:05,120
in a post-mortem breach analysis

422
00:16:05,120 --> 00:16:08,320
so that one could ask you know

423
00:16:08,320 --> 00:16:10,720
one could ask are there any unusual

424
00:16:10,720 --> 00:16:11,839
volume spikes

425
00:16:11,839 --> 00:16:13,920
in my request activity are there any new

426
00:16:13,920 --> 00:16:15,839
users or new ip addresses

427
00:16:15,839 --> 00:16:18,639
interact with my model at this time are

428
00:16:18,639 --> 00:16:20,320
there any interesting inputs or

429
00:16:20,320 --> 00:16:22,399
interesting outputs

430
00:16:22,399 --> 00:16:24,160
just to dive a little bit deeper on that

431
00:16:24,160 --> 00:16:26,079
point um

432
00:16:26,079 --> 00:16:28,079
no matter what you may be logging right

433
00:16:28,079 --> 00:16:30,000
now health monitoring checks to see if

434
00:16:30,000 --> 00:16:32,320
your ml service is alive

435
00:16:32,320 --> 00:16:34,880
add to that some simple things and add

436
00:16:34,880 --> 00:16:36,880
to it at all phases of this

437
00:16:36,880 --> 00:16:39,279
machine learning development pipeline

438
00:16:39,279 --> 00:16:40,959
during the training phases

439
00:16:40,959 --> 00:16:42,880
you can use data set versioning that

440
00:16:42,880 --> 00:16:45,600
includes a hash of your entire data set

441
00:16:45,600 --> 00:16:47,680
so you can easily check to see if it's

442
00:16:47,680 --> 00:16:50,720
been changed since last time you trained

443
00:16:50,720 --> 00:16:53,839
you can include hashes in your in the

444
00:16:53,839 --> 00:16:55,519
the binary of your model the model

445
00:16:55,519 --> 00:16:57,360
weights to know if that has changed

446
00:16:57,360 --> 00:16:59,120
substantially

447
00:16:59,120 --> 00:17:02,880
you can you can check with hashes also

448
00:17:02,880 --> 00:17:04,880
whether your model prediction

449
00:17:04,880 --> 00:17:08,319
or invalidation or training if those

450
00:17:08,319 --> 00:17:11,520
predictions have changed

451
00:17:11,599 --> 00:17:14,319
at telemetry time inference telemetry

452
00:17:14,319 --> 00:17:15,039
logs are

453
00:17:15,039 --> 00:17:17,280
incredibly useful as i'll demonstrate

454
00:17:17,280 --> 00:17:18,959
but you can log simple things like a

455
00:17:18,959 --> 00:17:20,160
time stamp

456
00:17:20,160 --> 00:17:22,640
the model idea what what model is being

457
00:17:22,640 --> 00:17:23,520
logged here

458
00:17:23,520 --> 00:17:25,280
who's interacting with the model if

459
00:17:25,280 --> 00:17:28,000
there's a user id or a client ip address

460
00:17:28,000 --> 00:17:29,520
and then again what what are sort of the

461
00:17:29,520 --> 00:17:31,039
inputs and outputs and

462
00:17:31,039 --> 00:17:32,799
for preserving privacy those can be

463
00:17:32,799 --> 00:17:34,640
hashes or simple

464
00:17:34,640 --> 00:17:36,640
simple score outputs that just give you

465
00:17:36,640 --> 00:17:37,840
a rough feel

466
00:17:37,840 --> 00:17:41,280
of what's happening with your model

467
00:17:41,679 --> 00:17:43,919
because data can actually tell you a lot

468
00:17:43,919 --> 00:17:45,039
without knowing

469
00:17:45,039 --> 00:17:48,559
hardly anything about this plot

470
00:17:48,559 --> 00:17:51,120
you can tell that there's something

471
00:17:51,120 --> 00:17:52,480
different that happens

472
00:17:52,480 --> 00:17:55,840
at query number i think 1000. so

473
00:17:55,840 --> 00:17:59,200
um this the x-axis here represents uh

474
00:17:59,200 --> 00:18:01,200
time the query number and the y-axis

475
00:18:01,200 --> 00:18:04,080
is simply the output of a model that is

476
00:18:04,080 --> 00:18:06,480
giving a binary decision a one

477
00:18:06,480 --> 00:18:09,840
or a zero and a a score for that numbers

478
00:18:09,840 --> 00:18:11,600
between zero and one

479
00:18:11,600 --> 00:18:14,240
and um if you if you see nothing else

480
00:18:14,240 --> 00:18:15,360
you see that once

481
00:18:15,360 --> 00:18:18,400
a an algorithmic attack begins you can

482
00:18:18,400 --> 00:18:18,880
see

483
00:18:18,880 --> 00:18:20,559
characteristics of an algorithm

484
00:18:20,559 --> 00:18:21,840
interacting with your model and that

485
00:18:21,840 --> 00:18:22,880
already could be

486
00:18:22,880 --> 00:18:26,000
um uh interesting data in a post

487
00:18:26,000 --> 00:18:29,840
more in breach analysis

488
00:18:30,720 --> 00:18:34,400
to conclude ml risk is outpacing

489
00:18:34,400 --> 00:18:37,280
ml security and while researchers are

490
00:18:37,280 --> 00:18:37,679
doing

491
00:18:37,679 --> 00:18:39,679
important work on that front there are

492
00:18:39,679 --> 00:18:42,080
important bare minimum things that we

493
00:18:42,080 --> 00:18:44,000
should all do today

494
00:18:44,000 --> 00:18:46,480
it starts with robust traditional

495
00:18:46,480 --> 00:18:47,440
security

496
00:18:47,440 --> 00:18:50,720
postures that include access controls

497
00:18:50,720 --> 00:18:53,520
permissions logging and auditing but it

498
00:18:53,520 --> 00:18:54,880
doesn't in there

499
00:18:54,880 --> 00:18:57,600
we should have an active mindset how

500
00:18:57,600 --> 00:18:58,960
will you respond

501
00:18:58,960 --> 00:19:01,280
when your machine learning model is

502
00:19:01,280 --> 00:19:03,200
duped first how will you know

503
00:19:03,200 --> 00:19:05,360
when it is attacked how will you

504
00:19:05,360 --> 00:19:07,760
ascertain a root cause

505
00:19:07,760 --> 00:19:09,760
of an adversarial failure mode was it

506
00:19:09,760 --> 00:19:10,799
due to poisoning

507
00:19:10,799 --> 00:19:15,200
was it due to a run time evasion attack

508
00:19:15,200 --> 00:19:17,520
when your model is exploited what is the

509
00:19:17,520 --> 00:19:19,520
blast radius of that

510
00:19:19,520 --> 00:19:21,600
how many other systems does does it

511
00:19:21,600 --> 00:19:23,600
affect and how can you reduce that

512
00:19:23,600 --> 00:19:27,280
footprint and if nothing else if you do

513
00:19:27,280 --> 00:19:27,840
nothing but

514
00:19:27,840 --> 00:19:30,240
think more like an adversary based on

515
00:19:30,240 --> 00:19:31,520
what we've talked about today i think

516
00:19:31,520 --> 00:19:31,919
you're

517
00:19:31,919 --> 00:19:34,799
well on your way

518
00:19:36,240 --> 00:19:39,360
i'll conclude where i began ml

519
00:19:39,360 --> 00:19:42,000
security is an information security

520
00:19:42,000 --> 00:19:43,200
problem

521
00:19:43,200 --> 00:19:46,960
in simple infosec hygiene at least today

522
00:19:46,960 --> 00:19:50,320
is at least half the battle

523
00:19:50,320 --> 00:19:52,320
so the same infosec hygiene and

524
00:19:52,320 --> 00:19:54,080
precautions you would take with

525
00:19:54,080 --> 00:19:57,200
any other software system you would keep

526
00:19:57,200 --> 00:19:58,240
your users data

527
00:19:58,240 --> 00:20:00,799
private and protected you would audit

528
00:20:00,799 --> 00:20:02,080
software dependencies

529
00:20:02,080 --> 00:20:04,880
and vulnerabilities for vulnerabilities

530
00:20:04,880 --> 00:20:05,840
you would validate

531
00:20:05,840 --> 00:20:08,240
inputs to your system and you would make

532
00:20:08,240 --> 00:20:08,880
sure

533
00:20:08,880 --> 00:20:10,720
you'd make every effort to keep your

534
00:20:10,720 --> 00:20:12,320
source code and your other intellectual

535
00:20:12,320 --> 00:20:13,679
property secure

536
00:20:13,679 --> 00:20:16,320
let's make apple absolutely sure that at

537
00:20:16,320 --> 00:20:17,039
each step

538
00:20:17,039 --> 00:20:18,480
of that machine learning development

539
00:20:18,480 --> 00:20:20,799
life cycle there were securing machine

540
00:20:20,799 --> 00:20:21,840
learning models

541
00:20:21,840 --> 00:20:26,159
using at least those same precautions

542
00:20:26,159 --> 00:20:35,840
thank you

543
00:20:36,640 --> 00:20:38,720
you

