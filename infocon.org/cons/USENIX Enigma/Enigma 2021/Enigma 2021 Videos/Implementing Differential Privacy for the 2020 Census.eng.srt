1
00:00:09,280 --> 00:00:09,760
hi

2
00:00:09,760 --> 00:00:12,160
i'm simpson garfinkel i'm the senior

3
00:00:12,160 --> 00:00:14,799
computer scientist for confidentiality

4
00:00:14,799 --> 00:00:17,520
and data access at the us census bureau

5
00:00:17,520 --> 00:00:19,520
i'm going to be talking to you today

6
00:00:19,520 --> 00:00:21,680
about our experiences implementing

7
00:00:21,680 --> 00:00:24,560
differential privacy for the 2020 census

8
00:00:24,560 --> 00:00:26,880
this is the abstract of the talk that is

9
00:00:26,880 --> 00:00:28,800
here so that google will find the slides

10
00:00:28,800 --> 00:00:30,880
and index them

11
00:00:30,880 --> 00:00:32,800
and these are the acknowledgements this

12
00:00:32,800 --> 00:00:35,040
is to remind you that this talk is

13
00:00:35,040 --> 00:00:37,840
the product of the work of many people

14
00:00:37,840 --> 00:00:40,480
working at the u.s census bureau

15
00:00:40,480 --> 00:00:42,879
so it's important to remember that

16
00:00:42,879 --> 00:00:44,719
differential privacy

17
00:00:44,719 --> 00:00:48,800
was invented in 2006 and that was 14

18
00:00:48,800 --> 00:00:50,000
years ago

19
00:00:50,000 --> 00:00:53,920
now 14 years this seems like a long time

20
00:00:53,920 --> 00:00:56,840
but it's not a long time since a

21
00:00:56,840 --> 00:00:59,440
fundamental scientific invention

22
00:00:59,440 --> 00:01:01,760
and for comparison i want you to think

23
00:01:01,760 --> 00:01:03,520
about the invention of public key

24
00:01:03,520 --> 00:01:04,799
cryptography

25
00:01:04,799 --> 00:01:07,720
which was invented between 1976 and

26
00:01:07,720 --> 00:01:09,360
1978.

27
00:01:09,360 --> 00:01:11,920
we are 14 years into differential

28
00:01:11,920 --> 00:01:12,960
privacy

29
00:01:12,960 --> 00:01:15,759
but 14 years after the invention of

30
00:01:15,759 --> 00:01:18,000
public key cryptography

31
00:01:18,000 --> 00:01:22,080
was two years before netscape navigator

32
00:01:22,080 --> 00:01:25,600
came out with ssl it was three years

33
00:01:25,600 --> 00:01:26,080
before

34
00:01:26,080 --> 00:01:29,200
ssh was invented and four years before

35
00:01:29,200 --> 00:01:31,439
pgp 3.0 came out

36
00:01:31,439 --> 00:01:34,960
and going into the future

37
00:01:34,960 --> 00:01:39,040
it wasn't until 2011 that the eff pushed

38
00:01:39,040 --> 00:01:41,439
its https now campaign

39
00:01:41,439 --> 00:01:44,159
and putting https everywhere and that

40
00:01:44,159 --> 00:01:44,479
was

41
00:01:44,479 --> 00:01:48,000
34 years after the invention of public

42
00:01:48,000 --> 00:01:49,439
key cryptography

43
00:01:49,439 --> 00:01:52,320
so that's still 20 years in our future

44
00:01:52,320 --> 00:01:54,720
as far as differential privacy goes

45
00:01:54,720 --> 00:01:57,280
now with that said the census bureau

46
00:01:57,280 --> 00:01:59,280
deployed the first implementation of

47
00:01:59,280 --> 00:02:01,520
differential privacy in the world

48
00:02:01,520 --> 00:02:04,560
just two years after dp was invented

49
00:02:04,560 --> 00:02:07,200
this is because we were able to adopt a

50
00:02:07,200 --> 00:02:08,000
system that we

51
00:02:08,000 --> 00:02:10,000
already had under development that used

52
00:02:10,000 --> 00:02:11,120
noise infusion

53
00:02:11,120 --> 00:02:13,040
to the new mathematics of differential

54
00:02:13,040 --> 00:02:14,800
privacy

55
00:02:14,800 --> 00:02:16,480
but there were a lot more technical

56
00:02:16,480 --> 00:02:18,239
problems that we had to solve for the

57
00:02:18,239 --> 00:02:19,599
2020 census

58
00:02:19,599 --> 00:02:21,599
than we had to solve for on the map

59
00:02:21,599 --> 00:02:23,599
here's the most important slide

60
00:02:23,599 --> 00:02:26,160
despite all the work we've done we still

61
00:02:26,160 --> 00:02:27,599
haven't met our users

62
00:02:27,599 --> 00:02:30,160
expectations that's because data

63
00:02:30,160 --> 00:02:31,760
scientists are being trained to work

64
00:02:31,760 --> 00:02:33,360
with raw data

65
00:02:33,360 --> 00:02:34,800
and they would probably get better

66
00:02:34,800 --> 00:02:37,120
results if they learned how to work with

67
00:02:37,120 --> 00:02:38,959
the direct results of differential

68
00:02:38,959 --> 00:02:40,160
privacy

69
00:02:40,160 --> 00:02:42,879
data users expect consistent data

70
00:02:42,879 --> 00:02:44,000
releases

71
00:02:44,000 --> 00:02:46,879
we can make our data consistent but that

72
00:02:46,879 --> 00:02:47,840
introduces

73
00:02:47,840 --> 00:02:51,200
much more error and degrades accuracy

74
00:02:51,200 --> 00:02:53,280
than if we were to release just the raw

75
00:02:53,280 --> 00:02:56,400
differential privacy measurements

76
00:02:56,400 --> 00:02:58,560
some users are calling the data that we

77
00:02:58,560 --> 00:02:59,440
release

78
00:02:59,440 --> 00:03:02,879
fake data like fake news that's not an

79
00:03:02,879 --> 00:03:04,959
accurate characterization of what we're

80
00:03:04,959 --> 00:03:07,680
doing we're releasing synthetic data

81
00:03:07,680 --> 00:03:08,959
that is based

82
00:03:08,959 --> 00:03:11,760
on real data and which has had privacy

83
00:03:11,760 --> 00:03:13,200
protection mechanisms

84
00:03:13,200 --> 00:03:16,480
added and our final problem has been

85
00:03:16,480 --> 00:03:18,640
it's really not clear how to do quality

86
00:03:18,640 --> 00:03:19,519
assurance

87
00:03:19,519 --> 00:03:22,239
and data exploration in a differential

88
00:03:22,239 --> 00:03:23,760
privacy framework

89
00:03:23,760 --> 00:03:25,760
and so we're really looking to engage

90
00:03:25,760 --> 00:03:27,040
with the community on that

91
00:03:27,040 --> 00:03:29,040
issue and that's really important for

92
00:03:29,040 --> 00:03:32,159
differential privacy moving forward so

93
00:03:32,159 --> 00:03:33,760
because we all just went through this

94
00:03:33,760 --> 00:03:36,159
we're familiar with the decennial census

95
00:03:36,159 --> 00:03:38,239
it's the longest running statistical

96
00:03:38,239 --> 00:03:40,000
program in the world

97
00:03:40,000 --> 00:03:42,560
it's the largest peacetime mobilization

98
00:03:42,560 --> 00:03:44,480
of labor in the united states

99
00:03:44,480 --> 00:03:47,280
we just completed it last year and the

100
00:03:47,280 --> 00:03:48,480
data will be released

101
00:03:48,480 --> 00:03:50,480
this year and that whose data will be

102
00:03:50,480 --> 00:03:52,159
used to apportion the house of

103
00:03:52,159 --> 00:03:53,280
representatives

104
00:03:53,280 --> 00:03:57,120
and to distribute somewhere between 675

105
00:03:57,120 --> 00:03:58,159
billion dollars

106
00:03:58,159 --> 00:04:01,360
and 1.5 trillion dollars a year

107
00:04:01,360 --> 00:04:03,920
according to various estimates now our

108
00:04:03,920 --> 00:04:04,879
problem

109
00:04:04,879 --> 00:04:07,439
is that the well our challenge is that

110
00:04:07,439 --> 00:04:07,920
the

111
00:04:07,920 --> 00:04:09,920
constitution requires that we collect

112
00:04:09,920 --> 00:04:11,040
this data

113
00:04:11,040 --> 00:04:14,560
and title 13 section 9 of the us code

114
00:04:14,560 --> 00:04:15,360
requires that we

115
00:04:15,360 --> 00:04:18,000
maintain data confidentiality for the

116
00:04:18,000 --> 00:04:18,959
respondents

117
00:04:18,959 --> 00:04:21,680
whose information we collect so to

118
00:04:21,680 --> 00:04:23,919
understand that requirement

119
00:04:23,919 --> 00:04:27,440
in terms of a statistical agency imagine

120
00:04:27,440 --> 00:04:28,800
if there was a block that had

121
00:04:28,800 --> 00:04:31,280
10 people living on it and they were all

122
00:04:31,280 --> 00:04:32,320
the same sex

123
00:04:32,320 --> 00:04:35,440
and all the same age we could not

124
00:04:35,440 --> 00:04:38,160
publish an exact statistic that there

125
00:04:38,160 --> 00:04:40,080
were 10 people on the block

126
00:04:40,080 --> 00:04:43,040
with offering some sort of demonstration

127
00:04:43,040 --> 00:04:44,400
that the statistic

128
00:04:44,400 --> 00:04:47,280
protected confidentiality if we didn't

129
00:04:47,280 --> 00:04:48,960
have that protection there

130
00:04:48,960 --> 00:04:51,440
and you knew that allison lived on that

131
00:04:51,440 --> 00:04:52,080
block

132
00:04:52,080 --> 00:04:54,479
then you'd immediately know that allison

133
00:04:54,479 --> 00:04:57,600
was a woman and that allison is 24.

134
00:04:57,600 --> 00:04:59,759
so at the census bureau we use the term

135
00:04:59,759 --> 00:05:01,360
disclosure avoidance

136
00:05:01,360 --> 00:05:03,759
to describe this process of protecting

137
00:05:03,759 --> 00:05:05,199
confidential data

138
00:05:05,199 --> 00:05:07,600
and we use that because the process is

139
00:05:07,600 --> 00:05:08,880
designed to prevent an

140
00:05:08,880 --> 00:05:11,759
improper disclosure under title 13

141
00:05:11,759 --> 00:05:13,919
section 9.

142
00:05:13,919 --> 00:05:16,639
so previously we used the technique

143
00:05:16,639 --> 00:05:18,479
called swapping to protect

144
00:05:18,479 --> 00:05:21,199
the information that we collected when

145
00:05:21,199 --> 00:05:22,400
we published it

146
00:05:22,400 --> 00:05:24,240
and the problem with swapping was we

147
00:05:24,240 --> 00:05:26,320
couldn't release the actual details of

148
00:05:26,320 --> 00:05:27,440
the mechanism

149
00:05:27,440 --> 00:05:29,600
and we couldn't release the parameters

150
00:05:29,600 --> 00:05:31,600
uh to tell people how much privacy

151
00:05:31,600 --> 00:05:33,360
protection it provided

152
00:05:33,360 --> 00:05:36,080
with differential privacy we can control

153
00:05:36,080 --> 00:05:39,039
that trade-off between privacy on the

154
00:05:39,039 --> 00:05:42,160
one side and very accurate data

155
00:05:42,160 --> 00:05:45,120
that we release on the other side and

156
00:05:45,120 --> 00:05:46,080
with that

157
00:05:46,080 --> 00:05:48,880
change we can now involve policy makers

158
00:05:48,880 --> 00:05:49,919
in the decision

159
00:05:49,919 --> 00:05:51,440
as to how much we should protect

160
00:05:51,440 --> 00:05:53,199
people's data versus

161
00:05:53,199 --> 00:05:56,080
how accurate should the data products be

162
00:05:56,080 --> 00:05:56,400
and

163
00:05:56,400 --> 00:05:58,639
differential privacy is also future

164
00:05:58,639 --> 00:06:01,680
proof our previous mechanism assumed

165
00:06:01,680 --> 00:06:04,080
that attackers that might want to

166
00:06:04,080 --> 00:06:06,160
re-identify people in the data had

167
00:06:06,160 --> 00:06:08,800
access to certain data sets and certain

168
00:06:08,800 --> 00:06:10,479
amounts of computer power

169
00:06:10,479 --> 00:06:13,280
differential privacy assu doesn't make

170
00:06:13,280 --> 00:06:14,560
any of those assumptions

171
00:06:14,560 --> 00:06:16,560
and its guarantees don't depend upon

172
00:06:16,560 --> 00:06:19,600
limitations of the attackers

173
00:06:19,600 --> 00:06:21,600
but each of these limitations ended up

174
00:06:21,600 --> 00:06:23,520
causing us problems

175
00:06:23,520 --> 00:06:25,600
many people within the census bureau

176
00:06:25,600 --> 00:06:27,759
thought that secrecy should remain an

177
00:06:27,759 --> 00:06:29,759
important part of our privacy protection

178
00:06:29,759 --> 00:06:30,960
mechanism

179
00:06:30,960 --> 00:06:33,520
so when we moved to differential privacy

180
00:06:33,520 --> 00:06:35,280
there were many concerns about

181
00:06:35,280 --> 00:06:38,240
actually publishing the source code

182
00:06:38,240 --> 00:06:39,120
likewise

183
00:06:39,120 --> 00:06:41,680
differential privacy lets us control the

184
00:06:41,680 --> 00:06:44,400
privacy loss accuracy trade-off

185
00:06:44,400 --> 00:06:46,880
but that meant we needed to develop

186
00:06:46,880 --> 00:06:49,280
tools to help policy makers make that

187
00:06:49,280 --> 00:06:50,400
decision

188
00:06:50,400 --> 00:06:52,720
and we also needed to provide the policy

189
00:06:52,720 --> 00:06:55,120
makers with context and training

190
00:06:55,120 --> 00:06:56,960
to understand the decision that they

191
00:06:56,960 --> 00:06:59,520
were making we needed to design

192
00:06:59,520 --> 00:07:01,919
our algorithms so that the accuracy

193
00:07:01,919 --> 00:07:03,759
would be where it was needed

194
00:07:03,759 --> 00:07:06,000
and not anywhere else so that the

195
00:07:06,000 --> 00:07:08,479
privacy loss using differential privacy

196
00:07:08,479 --> 00:07:10,319
could be minimized

197
00:07:10,319 --> 00:07:12,319
and then there's this issue of future

198
00:07:12,319 --> 00:07:14,960
proof we say that differential privacy

199
00:07:14,960 --> 00:07:16,240
is future proof

200
00:07:16,240 --> 00:07:18,880
because the privacy definition

201
00:07:18,880 --> 00:07:21,280
guarantees that it makes a relative

202
00:07:21,280 --> 00:07:24,800
not absolute and this change in the

203
00:07:24,800 --> 00:07:27,520
sense of what it means to offer privacy

204
00:07:27,520 --> 00:07:30,400
also requires significant explanations

205
00:07:30,400 --> 00:07:33,120
and additional training

206
00:07:33,120 --> 00:07:36,560
so the need to develop and tune the

207
00:07:36,560 --> 00:07:38,479
differential privacy algorithm

208
00:07:38,479 --> 00:07:41,039
was unlike anything that decennial

209
00:07:41,039 --> 00:07:42,400
response processing

210
00:07:42,400 --> 00:07:45,840
had ever done before

211
00:07:47,360 --> 00:07:50,479
and that was before the pandemic by 2017

212
00:07:50,479 --> 00:07:52,479
we thought we had a good understanding

213
00:07:52,479 --> 00:07:54,960
of how differential privacy would fit

214
00:07:54,960 --> 00:07:58,080
into the census data processing pipeline

215
00:07:58,080 --> 00:08:00,560
the plan was to simply replace the old

216
00:08:00,560 --> 00:08:03,199
privacy mechanism with the new one

217
00:08:03,199 --> 00:08:06,639
for the 2010 census we took the census

218
00:08:06,639 --> 00:08:07,919
edited file

219
00:08:07,919 --> 00:08:10,879
applied our swapping mechanism produced

220
00:08:10,879 --> 00:08:13,440
a set of micro data called the 100

221
00:08:13,440 --> 00:08:16,639
file and it was the confidential hdf

222
00:08:16,639 --> 00:08:18,479
that was tabulated for census

223
00:08:18,479 --> 00:08:19,919
publications

224
00:08:19,919 --> 00:08:22,960
for the 2020 census our plan was to use

225
00:08:22,960 --> 00:08:25,840
differential privacy to produce a newly

226
00:08:25,840 --> 00:08:26,560
named

227
00:08:26,560 --> 00:08:31,759
privacy protected micro data detail file

228
00:08:31,919 --> 00:08:35,440
one of our first big surprises was that

229
00:08:35,440 --> 00:08:37,679
different groups at the census bureau

230
00:08:37,679 --> 00:08:39,519
had different assumptions about what

231
00:08:39,519 --> 00:08:40,000
that would

232
00:08:40,000 --> 00:08:43,919
mean previously every state was

233
00:08:43,919 --> 00:08:45,040
processed

234
00:08:45,040 --> 00:08:48,080
on a continuous flow basis with the big

235
00:08:48,080 --> 00:08:49,120
states moving through

236
00:08:49,120 --> 00:08:51,680
and the small states moving through all

237
00:08:51,680 --> 00:08:53,200
in their own way

238
00:08:53,200 --> 00:08:56,800
but when we move to differential privacy

239
00:08:56,800 --> 00:08:59,839
the top-down mechanism that we designed

240
00:08:59,839 --> 00:09:02,480
required that all the states be computed

241
00:09:02,480 --> 00:09:03,839
upon at once

242
00:09:03,839 --> 00:09:06,240
and this created a bottleneck in the

243
00:09:06,240 --> 00:09:08,640
decennial response processing

244
00:09:08,640 --> 00:09:12,160
because of the mechanism that we chose

245
00:09:12,160 --> 00:09:14,240
for differential privacy

246
00:09:14,240 --> 00:09:17,360
and another impact of our mechanism

247
00:09:17,360 --> 00:09:19,680
is that it required significantly more

248
00:09:19,680 --> 00:09:21,120
computing power

249
00:09:21,120 --> 00:09:23,680
than the traditional tabulations and

250
00:09:23,680 --> 00:09:26,240
this is because our mechanism required

251
00:09:26,240 --> 00:09:30,800
800 000 privacy optimizations

252
00:09:30,800 --> 00:09:33,360
to be performed another challenge that

253
00:09:33,360 --> 00:09:34,959
we faced was that our differential

254
00:09:34,959 --> 00:09:36,240
privacy system

255
00:09:36,240 --> 00:09:39,279
had to be developed with real title 13

256
00:09:39,279 --> 00:09:40,800
protected data

257
00:09:40,800 --> 00:09:43,600
most of the systems for the 2020 census

258
00:09:43,600 --> 00:09:44,959
were developed using

259
00:09:44,959 --> 00:09:48,320
simulated data and simulated data

260
00:09:48,320 --> 00:09:51,279
by that we mean not based on protected

261
00:09:51,279 --> 00:09:52,880
respondent data

262
00:09:52,880 --> 00:09:55,360
so the problem is that little work in

263
00:09:55,360 --> 00:09:55,920
the

264
00:09:55,920 --> 00:09:58,720
academic literature had been done using

265
00:09:58,720 --> 00:09:59,519
differential

266
00:09:59,519 --> 00:10:02,560
privacy to protect complex confidential

267
00:10:02,560 --> 00:10:03,440
data

268
00:10:03,440 --> 00:10:06,480
so we needed to develop new algorithms

269
00:10:06,480 --> 00:10:08,640
that would work with data as complex as

270
00:10:08,640 --> 00:10:10,160
the united states

271
00:10:10,160 --> 00:10:12,240
and in the u.s there are clusters by age

272
00:10:12,240 --> 00:10:13,760
and race there are multi-racial

273
00:10:13,760 --> 00:10:14,640
households

274
00:10:14,640 --> 00:10:17,279
there are same-sex unions there's small

275
00:10:17,279 --> 00:10:18,000
group quarters

276
00:10:18,000 --> 00:10:20,240
and large group quarters and they're all

277
00:10:20,240 --> 00:10:22,160
geographically diverse

278
00:10:22,160 --> 00:10:24,800
so we we needed to do this with real

279
00:10:24,800 --> 00:10:25,600
data

280
00:10:25,600 --> 00:10:27,800
and that created privacy and

281
00:10:27,800 --> 00:10:28,959
confidentiality

282
00:10:28,959 --> 00:10:31,360
issues because we needed to do it on

283
00:10:31,360 --> 00:10:33,360
computers that were approved to hold

284
00:10:33,360 --> 00:10:35,120
that real data

285
00:10:35,120 --> 00:10:37,120
this graph illustrates another problem

286
00:10:37,120 --> 00:10:38,399
that we encountered

287
00:10:38,399 --> 00:10:40,079
if you're familiar with differential

288
00:10:40,079 --> 00:10:42,399
privacy you know that this is a graph

289
00:10:42,399 --> 00:10:43,360
that shows how

290
00:10:43,360 --> 00:10:46,959
accuracy and privacy loss coexist with

291
00:10:46,959 --> 00:10:49,040
different values of epsilon

292
00:10:49,040 --> 00:10:52,320
but to produce this graph we need five

293
00:10:52,320 --> 00:10:54,720
to ten runs of our system at each of

294
00:10:54,720 --> 00:10:56,160
those data points

295
00:10:56,160 --> 00:10:58,560
so this graph is about a hundred runs of

296
00:10:58,560 --> 00:10:59,600
the algorithm

297
00:10:59,600 --> 00:11:02,880
and each of those runs requires 800 000

298
00:11:02,880 --> 00:11:05,680
solves as i previously mentioned so this

299
00:11:05,680 --> 00:11:06,079
is a

300
00:11:06,079 --> 00:11:08,640
huge amount of computer time required to

301
00:11:08,640 --> 00:11:11,839
produce this relatively simple graph

302
00:11:11,839 --> 00:11:15,839
so how did we do it

303
00:11:16,399 --> 00:11:18,079
well the most important thing is that

304
00:11:18,079 --> 00:11:20,880
the census bureau leadership was 100

305
00:11:20,880 --> 00:11:23,600
behind the move to differential privacy

306
00:11:23,600 --> 00:11:24,480
john thompson

307
00:11:24,480 --> 00:11:27,680
director from 2013 to 2017 was

308
00:11:27,680 --> 00:11:29,360
instrumental in the move

309
00:11:29,360 --> 00:11:32,079
he brought on board john about as chief

310
00:11:32,079 --> 00:11:32,959
scientist

311
00:11:32,959 --> 00:11:35,279
for the purpose of implementing

312
00:11:35,279 --> 00:11:36,720
differential privacy

313
00:11:36,720 --> 00:11:38,880
and john about had tremendous support

314
00:11:38,880 --> 00:11:40,160
from ron jarmon

315
00:11:40,160 --> 00:11:42,320
who was performing the non-exclusive

316
00:11:42,320 --> 00:11:44,399
functions and duties of the director

317
00:11:44,399 --> 00:11:47,600
from 2017 to 2019 and who is now the

318
00:11:47,600 --> 00:11:49,440
deputy director and chief operating

319
00:11:49,440 --> 00:11:50,800
officer

320
00:11:50,800 --> 00:11:54,000
the initial implementation was done by

321
00:11:54,000 --> 00:11:56,800
penn state professor daniel kiefer who

322
00:11:56,800 --> 00:11:59,120
took a sabbatical at the u.s census

323
00:11:59,120 --> 00:11:59,760
bureau

324
00:11:59,760 --> 00:12:02,000
and built the first implementation of

325
00:12:02,000 --> 00:12:03,920
our top-down algorithm

326
00:12:03,920 --> 00:12:07,519
in 2017 i joined the census bureau

327
00:12:07,519 --> 00:12:11,360
and i engineered a framework from dan's

328
00:12:11,360 --> 00:12:12,800
original ideas

329
00:12:12,800 --> 00:12:15,279
and that framework is actually a system

330
00:12:15,279 --> 00:12:17,360
for building disclosure avoidance

331
00:12:17,360 --> 00:12:18,240
systems

332
00:12:18,240 --> 00:12:20,480
it's a system for performing science

333
00:12:20,480 --> 00:12:22,880
experiments with plug-ins for the reader

334
00:12:22,880 --> 00:12:25,360
and the engine and the validator and

335
00:12:25,360 --> 00:12:27,680
it's driven by a configuration file that

336
00:12:27,680 --> 00:12:28,160
allows

337
00:12:28,160 --> 00:12:30,880
us to easily experiment with parameters

338
00:12:30,880 --> 00:12:31,279
and

339
00:12:31,279 --> 00:12:34,079
plug in and plug out new code and then

340
00:12:34,079 --> 00:12:35,200
in the 2018

341
00:12:35,200 --> 00:12:38,079
end-to-end test we put the source code

342
00:12:38,079 --> 00:12:38,959
release

343
00:12:38,959 --> 00:12:41,680
as specification and specifications as

344
00:12:41,680 --> 00:12:43,600
an official data product

345
00:12:43,600 --> 00:12:46,000
and this meant that that the fact that

346
00:12:46,000 --> 00:12:48,079
we were releasing that data went through

347
00:12:48,079 --> 00:12:49,120
the process

348
00:12:49,120 --> 00:12:51,600
of approvals the source code that we

349
00:12:51,600 --> 00:12:52,560
developed

350
00:12:52,560 --> 00:12:56,399
also could run from the 1940 census

351
00:12:56,399 --> 00:12:58,959
that data has been publicly released so

352
00:12:58,959 --> 00:13:00,959
we provided the general public with a

353
00:13:00,959 --> 00:13:02,160
way that they could run

354
00:13:02,160 --> 00:13:04,240
our software even though they didn't

355
00:13:04,240 --> 00:13:07,279
have access to our confidential data

356
00:13:07,279 --> 00:13:09,600
so initially we were not approved for

357
00:13:09,600 --> 00:13:11,680
using title 13 data for

358
00:13:11,680 --> 00:13:14,720
software development in the amazon web

359
00:13:14,720 --> 00:13:16,320
services environment

360
00:13:16,320 --> 00:13:19,120
that took work internally and until we

361
00:13:19,120 --> 00:13:20,000
got there

362
00:13:20,000 --> 00:13:22,399
we had to do our development with an

363
00:13:22,399 --> 00:13:24,800
on-premise linux cluster

364
00:13:24,800 --> 00:13:26,880
eventually we were able to move to

365
00:13:26,880 --> 00:13:28,480
amazon web services

366
00:13:28,480 --> 00:13:30,800
but we discovered that the software for

367
00:13:30,800 --> 00:13:32,399
monitoring

368
00:13:32,399 --> 00:13:34,880
execution of elastic mapreduce jobs in

369
00:13:34,880 --> 00:13:35,600
amazon

370
00:13:35,600 --> 00:13:37,519
was not the didn't provide the

371
00:13:37,519 --> 00:13:40,079
sufficient fidelity that we needed to

372
00:13:40,079 --> 00:13:42,320
develop and tune the algorithm so we

373
00:13:42,320 --> 00:13:44,880
ended up creating our own dashboard

374
00:13:44,880 --> 00:13:46,880
and each dashboard tracks all the

375
00:13:46,880 --> 00:13:48,720
running clusters that we have

376
00:13:48,720 --> 00:13:50,880
and each execution of the top down

377
00:13:50,880 --> 00:13:51,839
algorithm

378
00:13:51,839 --> 00:13:53,920
and during development we have executed

379
00:13:53,920 --> 00:13:54,880
the algorithm

380
00:13:54,880 --> 00:13:58,240
tens of thousands of times each of the

381
00:13:58,240 --> 00:14:00,639
rectangles in the graph at the right

382
00:14:00,639 --> 00:14:03,920
represents an elastic mapreduce cluster

383
00:14:03,920 --> 00:14:06,240
and as new clusters are created they

384
00:14:06,240 --> 00:14:07,680
appear on this graph

385
00:14:07,680 --> 00:14:11,040
and as they are deleted they go away

386
00:14:11,040 --> 00:14:14,320
so this graph shows the

387
00:14:14,320 --> 00:14:17,680
cpu utilization of the nodes in one of

388
00:14:17,680 --> 00:14:19,360
our emr clusters

389
00:14:19,360 --> 00:14:22,160
each line represents the average cpu

390
00:14:22,160 --> 00:14:24,160
load of one of the nodes

391
00:14:24,160 --> 00:14:27,760
there are 21 nodes and each one has 96

392
00:14:27,760 --> 00:14:28,639
cores

393
00:14:28,639 --> 00:14:31,199
when the lines are over 96 those

394
00:14:31,199 --> 00:14:32,000
machines are

395
00:14:32,000 --> 00:14:35,040
over utilized and when they're under 90

396
00:14:35,040 --> 00:14:37,360
under 100 those machines are largely

397
00:14:37,360 --> 00:14:39,440
idle and so you can see that for that

398
00:14:39,440 --> 00:14:41,360
section in the middle and the section on

399
00:14:41,360 --> 00:14:42,240
the right

400
00:14:42,240 --> 00:14:45,040
we're well utilizing our cluster but on

401
00:14:45,040 --> 00:14:45,600
the

402
00:14:45,600 --> 00:14:47,839
the leftmost side and the rightmost side

403
00:14:47,839 --> 00:14:50,320
the cluster is not being well utilized

404
00:14:50,320 --> 00:14:52,639
and graphs like this allow us to tune

405
00:14:52,639 --> 00:14:53,760
the algorithm

406
00:14:53,760 --> 00:14:56,880
to get better machine utilization and

407
00:14:56,880 --> 00:14:58,959
lower clock times for the algorithm's

408
00:14:58,959 --> 00:15:00,480
execution

409
00:15:00,480 --> 00:15:02,880
to help our data users understand the

410
00:15:02,880 --> 00:15:05,040
impact of differential privacy

411
00:15:05,040 --> 00:15:08,399
we took the data from the 2010 census

412
00:15:08,399 --> 00:15:10,880
processed it with a top-down algorithm

413
00:15:10,880 --> 00:15:12,240
and published it

414
00:15:12,240 --> 00:15:14,800
we then had a special workshop at the

415
00:15:14,800 --> 00:15:16,800
national academies of science

416
00:15:16,800 --> 00:15:20,399
in december 2019 to discuss

417
00:15:20,399 --> 00:15:23,120
the uh accuracy and the errors that were

418
00:15:23,120 --> 00:15:24,160
in the data that

419
00:15:24,160 --> 00:15:26,800
was part of that release now a lot of

420
00:15:26,800 --> 00:15:28,160
the data users

421
00:15:28,160 --> 00:15:30,720
voice concerns and that was the subject

422
00:15:30,720 --> 00:15:31,199
of some

423
00:15:31,199 --> 00:15:34,240
articles in the popular media in general

424
00:15:34,240 --> 00:15:34,720
the

425
00:15:34,720 --> 00:15:37,600
2010 demonstration data products were

426
00:15:37,600 --> 00:15:39,120
not well received

427
00:15:39,120 --> 00:15:41,199
our data users thought that differential

428
00:15:41,199 --> 00:15:44,000
privacy was introducing too much error

429
00:15:44,000 --> 00:15:46,480
into the data products

430
00:15:46,480 --> 00:15:50,000
so the root of our problem is that

431
00:15:50,000 --> 00:15:53,360
we are adding random values which might

432
00:15:53,360 --> 00:15:53,680
be

433
00:15:53,680 --> 00:15:56,320
positive or they might be negative so if

434
00:15:56,320 --> 00:15:57,839
we take a child's age

435
00:15:57,839 --> 00:16:00,560
and we add a random value we might end

436
00:16:00,560 --> 00:16:02,399
up with a negative age

437
00:16:02,399 --> 00:16:04,399
if we take an adult's age and we add a

438
00:16:04,399 --> 00:16:06,880
random value we probably won't end up

439
00:16:06,880 --> 00:16:08,480
with a negative number

440
00:16:08,480 --> 00:16:11,920
and this means that if you take ages

441
00:16:11,920 --> 00:16:14,320
and make sure that they're all going to

442
00:16:14,320 --> 00:16:15,440
be positive

443
00:16:15,440 --> 00:16:18,560
you're introducing systematic bias the

444
00:16:18,560 --> 00:16:20,639
there's no way around that the real

445
00:16:20,639 --> 00:16:21,600
solution here

446
00:16:21,600 --> 00:16:23,759
is to let the data users work with the

447
00:16:23,759 --> 00:16:25,120
noisy measurements

448
00:16:25,120 --> 00:16:27,360
these are the clean data from the ceph

449
00:16:27,360 --> 00:16:28,560
that have then had

450
00:16:28,560 --> 00:16:31,279
privacy protecting noise added but

451
00:16:31,279 --> 00:16:33,839
haven't yet gone through the consistency

452
00:16:33,839 --> 00:16:37,120
and integerization process we are now

453
00:16:37,120 --> 00:16:39,279
working with data users to give them

454
00:16:39,279 --> 00:16:40,079
access

455
00:16:40,079 --> 00:16:42,320
to these noisy measurement files in

456
00:16:42,320 --> 00:16:45,839
addition to the processed micro data

457
00:16:45,839 --> 00:16:48,399
so in summary differential privacy is

458
00:16:48,399 --> 00:16:50,320
now 14 years old

459
00:16:50,320 --> 00:16:52,160
and even though the census bureau

460
00:16:52,160 --> 00:16:54,480
deployed the first differential privacy

461
00:16:54,480 --> 00:16:55,279
system

462
00:16:55,279 --> 00:16:57,519
just two years later that was a new

463
00:16:57,519 --> 00:16:59,360
system that was designed from the

464
00:16:59,360 --> 00:17:01,680
beginning to use noise infusion

465
00:17:01,680 --> 00:17:04,079
so it wasn't that heavy of a lift for

466
00:17:04,079 --> 00:17:06,079
the 2020 census

467
00:17:06,079 --> 00:17:08,720
we've had to retrofit the longest

468
00:17:08,720 --> 00:17:09,439
running

469
00:17:09,439 --> 00:17:12,480
statistical program in the world and to

470
00:17:12,480 --> 00:17:13,119
do that

471
00:17:13,119 --> 00:17:15,839
has required support from the very top

472
00:17:15,839 --> 00:17:17,439
levels of the census bureau

473
00:17:17,439 --> 00:17:19,919
it's required senior leadership and a

474
00:17:19,919 --> 00:17:22,319
significant scientific effort

475
00:17:22,319 --> 00:17:25,199
on the part of the entire research and

476
00:17:25,199 --> 00:17:26,559
methodology directorate

477
00:17:26,559 --> 00:17:28,319
with the cooperation of the rest of the

478
00:17:28,319 --> 00:17:30,480
entire 2020 census team

479
00:17:30,480 --> 00:17:33,039
it's been a lot of work and we've worked

480
00:17:33,039 --> 00:17:35,200
eagerly and actively to involve data

481
00:17:35,200 --> 00:17:36,720
users as well

482
00:17:36,720 --> 00:17:39,679
data users have had some concerns but we

483
00:17:39,679 --> 00:17:41,360
think that we will address them and we

484
00:17:41,360 --> 00:17:42,880
think that people will be very pleased

485
00:17:42,880 --> 00:17:44,880
with the final product

486
00:17:44,880 --> 00:17:47,600
so this was necessarily a very short

487
00:17:47,600 --> 00:17:48,240
talk

488
00:17:48,240 --> 00:17:50,400
but you can get access to our current

489
00:17:50,400 --> 00:17:52,000
source code on github

490
00:17:52,000 --> 00:17:54,160
and you can also download our system

491
00:17:54,160 --> 00:17:55,440
specification

492
00:17:55,440 --> 00:17:58,320
and our scientific papers and there's

493
00:17:58,320 --> 00:18:00,000
been a lot of public engagement

494
00:18:00,000 --> 00:18:01,600
so here are some other talks that you

495
00:18:01,600 --> 00:18:03,520
might want to look at

496
00:18:03,520 --> 00:18:05,440
so i really want to thank yousnick's

497
00:18:05,440 --> 00:18:07,440
enigma for allowing me to speak

498
00:18:07,440 --> 00:18:15,840
and i'm ready to take questions

499
00:18:18,320 --> 00:18:20,399
you

