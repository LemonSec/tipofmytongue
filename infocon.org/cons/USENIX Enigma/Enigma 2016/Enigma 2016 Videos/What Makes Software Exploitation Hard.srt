1
00:00:00,033 --> 00:00:05,330


2
00:00:05,330 --> 00:00:06,693
HAWKES: Hi.

3
00:00:06,693 --> 00:00:09,065
My name is Ben Hawkes.

4
00:00:09,066 --> 00:00:11,693
And I'm a member
of Project Zero.

5
00:00:11,693 --> 00:00:13,792
This talk is
about software exploitation,

6
00:00:13,792 --> 00:00:15,626
what makes software
exploitation hard,

7
00:00:15,627 --> 00:00:17,726
and why does it even matter?

8
00:00:19,759 --> 00:00:22,396
About 18 months ago,
a group of security engineers

9
00:00:22,396 --> 00:00:26,132
at Google formed Project Zero
with the stated mission

10
00:00:26,132 --> 00:00:27,858
to make 0day hard,

11
00:00:27,858 --> 00:00:29,957
to make software exploitation

12
00:00:29,957 --> 00:00:33,660
using 0day vulnerabilities
harder than it is today.

13
00:00:33,660 --> 00:00:36,099
And behind that sort of
encapsulates the idea

14
00:00:36,099 --> 00:00:39,626
that we think
that the cost of an exploit

15
00:00:39,627 --> 00:00:42,396
is not in balance
with the capabilities

16
00:00:42,396 --> 00:00:45,660
that an attacker gains
from using that exploit.

17
00:00:45,660 --> 00:00:49,726
And also, we knew that
there was a private market

18
00:00:49,726 --> 00:00:52,396
for exploit purchase and sale,

19
00:00:52,396 --> 00:00:55,858
a global market
for exploit purchases,

20
00:00:55,858 --> 00:00:59,099
and the exploits were being used
to harm our users

21
00:00:59,099 --> 00:01:01,660
and to harm companies
like Google.

22
00:01:01,660 --> 00:01:03,693
And we thought as
security engineers

23
00:01:03,693 --> 00:01:07,792
that we could work
on this problem.

24
00:01:07,792 --> 00:01:09,363
Here's a concrete example.

25
00:01:09,363 --> 00:01:12,264
$130,000.

26
00:01:12,264 --> 00:01:14,857
This was the purchase price
recently

27
00:01:14,858 --> 00:01:16,297
for what we might think of

28
00:01:16,297 --> 00:01:19,726
as the canonical exploit chain
of recent years, at least,

29
00:01:19,726 --> 00:01:22,099
which is an Adobe Flash exploit,

30
00:01:22,099 --> 00:01:25,264
0day exploit, combined
with a Windows Kernel exploit.

31
00:01:25,264 --> 00:01:28,098
$40,000 for a Flash exploit

32
00:01:28,099 --> 00:01:31,924
and about $90,000
for a Windows Kernel exploit.

33
00:01:31,924 --> 00:01:34,495
And at first glance,
you might look at that number

34
00:01:34,495 --> 00:01:36,297
and think that's a lot of money

35
00:01:36,297 --> 00:01:37,693
if you compare the cost

36
00:01:37,693 --> 00:01:39,099
of setting up
a phishing operation

37
00:01:39,099 --> 00:01:40,462
or something like this,

38
00:01:40,462 --> 00:01:42,231
this is much more expensive.

39
00:01:42,231 --> 00:01:47,198
But then if you think about
what this exploit is,

40
00:01:47,198 --> 00:01:49,627
it's a full system compromise

41
00:01:49,627 --> 00:01:52,231
on hundreds of millions
of machines,

42
00:01:52,231 --> 00:01:55,957
just by having a user visit
a URL control,

43
00:01:55,957 --> 00:01:59,726
you control, or if you have
network access and the ability

44
00:01:59,726 --> 00:02:01,627
to inject into network traffic,

45
00:02:01,627 --> 00:02:03,033
not even that.

46
00:02:03,033 --> 00:02:05,825
So all of the classical
targeted attacks

47
00:02:05,825 --> 00:02:08,396
that we've heard about, system
administrators being compromised

48
00:02:08,395 --> 00:02:10,032
and network access being stolen,

49
00:02:10,032 --> 00:02:11,957
diplomats losing documents,

50
00:02:11,957 --> 00:02:14,132
company executives
losing e-mails,

51
00:02:14,132 --> 00:02:16,099
cryptographic key material
being stolen,

52
00:02:16,099 --> 00:02:17,792
back doors being inserted,

53
00:02:17,792 --> 00:02:20,099
all of this is open to you

54
00:02:20,099 --> 00:02:23,825
by this exploit's capability.

55
00:02:23,825 --> 00:02:27,528
So, in practice,
we think exploits are too cheap

56
00:02:27,528 --> 00:02:29,891
and too numerous.

57
00:02:29,891 --> 00:02:34,726
So this talk is, in essence,
about our technical strategy

58
00:02:34,726 --> 00:02:37,297
to try to address this problem.

59
00:02:37,297 --> 00:02:39,957
There are other
nontechnical facets

60
00:02:39,957 --> 00:02:43,759
about how you might approach
this, policy issues,

61
00:02:43,759 --> 00:02:45,099
outreach education.

62
00:02:45,099 --> 00:02:48,165
But today I'm focusing
on our technical strategy.

63
00:02:48,165 --> 00:02:49,429
I want to sort of explain

64
00:02:49,429 --> 00:02:51,528
the observations and principles

65
00:02:51,528 --> 00:02:53,263
behind our technical strategy,

66
00:02:53,264 --> 00:02:54,726
why we think it's effective,

67
00:02:54,726 --> 00:02:56,693
and then obviously
get feedback from you all

68
00:02:56,693 --> 00:02:59,759
about whether you agree
with that assessment

69
00:02:59,759 --> 00:03:02,857
and ideas of other
sort of technical facets

70
00:03:02,858 --> 00:03:07,000
that we can introduce
into our work on this problem.

71
00:03:07,000 --> 00:03:09,627
In essence, our answer
to the question,

72
00:03:09,627 --> 00:03:12,627
"What makes software
exploitation hard?" is twofold.

73
00:03:12,627 --> 00:03:16,363
It's strategically targeted
vulnerability research

74
00:03:16,363 --> 00:03:18,825
and incrementally increasing
the state-of-the-art

75
00:03:18,825 --> 00:03:20,626
of exploit mitigations.

76
00:03:20,627 --> 00:03:22,495
But, in essence, behind that
is the idea

77
00:03:22,495 --> 00:03:25,132
that Project Zero is
an attack-research team

78
00:03:25,132 --> 00:03:28,462
in the same mold
that a private attacker,

79
00:03:28,462 --> 00:03:31,396
a private actor
might want to set up,

80
00:03:31,396 --> 00:03:34,065
except we do our work
transparently in the public

81
00:03:34,066 --> 00:03:36,330
for defensive purposes.

82
00:03:40,330 --> 00:03:44,627
In 18 months, we've reported
600 vulnerabilities,

83
00:03:44,627 --> 00:03:46,594
most of these
in non-Google software.

84
00:03:46,594 --> 00:03:47,957
So, importantly our scope

85
00:03:47,957 --> 00:03:52,396
is beyond just what Google
is producing as software.

86
00:03:52,396 --> 00:03:55,891
We look holistically
at all the types of software

87
00:03:55,891 --> 00:03:59,297
that are used
in the ecosystem of our users.

88
00:03:59,297 --> 00:04:01,858
600 vulnerabilities
in about 18 months.

89
00:04:01,858 --> 00:04:05,066
We have a blog
that's focused heavily

90
00:04:05,066 --> 00:04:07,726
on technical reports
about how you discover

91
00:04:07,726 --> 00:04:10,561
vulnerabilities, exploitation,

92
00:04:10,561 --> 00:04:12,792
how exploitation
actually happens,

93
00:04:12,792 --> 00:04:19,065
and mitigation design and
how we can break mitigations.

94
00:04:19,065 --> 00:04:21,263
There's a new blog post

95
00:04:21,264 --> 00:04:24,759
comes out
about every second week.

96
00:04:24,759 --> 00:04:27,165
And we've also been involved

97
00:04:27,165 --> 00:04:28,891
in shipping
some exploit mitigations.

98
00:04:28,891 --> 00:04:33,099
I think of Chrome, Linux,

99
00:04:33,099 --> 00:04:35,693
Flash, Windows, Android.

100
00:04:35,693 --> 00:04:37,429
All ship mitigations

101
00:04:37,429 --> 00:04:39,395
that are either influenced
by our work

102
00:04:39,396 --> 00:04:41,528
or are directly designed
by our team.

103
00:04:41,528 --> 00:04:42,957
It's a start.

104
00:04:42,957 --> 00:04:45,495
I would love to do more
at this final point,

105
00:04:45,495 --> 00:04:48,594
and we'll address that shortly.

106
00:04:48,594 --> 00:04:50,330
But here's another number.

107
00:04:50,330 --> 00:04:52,363
26.

108
00:04:52,363 --> 00:04:57,462
So, 26 is the number of cases of
exploits

109
00:04:57,462 --> 00:04:59,726
discovered in the wild in 2015

110
00:04:59,726 --> 00:05:02,297
that affected
the set of software targets

111
00:05:02,297 --> 00:05:03,561
that we're monitoring.

112
00:05:03,561 --> 00:05:06,527
This includes Internet Explorer,
the Windows Kernel,

113
00:05:06,528 --> 00:05:08,891
Adobe Flash, Microsoft Office.

114
00:05:08,891 --> 00:05:14,198
26 cases of discovered
0day exploits in the wild.

115
00:05:14,198 --> 00:05:16,032
And at first glance again,

116
00:05:16,033 --> 00:05:17,957
you would look at that number
and we're very used to

117
00:05:17,957 --> 00:05:21,033
in the security community
hearing very high numbers

118
00:05:21,033 --> 00:05:24,693
of thousands of attacks per day
against systems, and so on,

119
00:05:24,693 --> 00:05:27,032
and this is relatively
a small number.

120
00:05:27,033 --> 00:05:30,066
But we have to decompress that

121
00:05:30,066 --> 00:05:32,561
and look at what's behind it.

122
00:05:32,561 --> 00:05:34,132
This number, 26, represents

123
00:05:34,132 --> 00:05:36,165
the failure case
for the attacker.

124
00:05:36,165 --> 00:05:38,099
It's where something
has gone wrong,

125
00:05:38,099 --> 00:05:39,363
some technical process,

126
00:05:39,363 --> 00:05:42,198
some operational-security
procedure has gone wrong,

127
00:05:42,198 --> 00:05:44,032
or the attacker
just gets very unlucky

128
00:05:44,033 --> 00:05:46,099
in order to allow
the security community

129
00:05:46,099 --> 00:05:50,462
to detect and discover
these instances of 0day attacks.

130
00:05:50,462 --> 00:05:52,164
But there's an unknown here,

131
00:05:52,165 --> 00:05:57,066
and the unknown is the ratio
of discovery to nondiscovery.

132
00:05:57,066 --> 00:05:59,330
We don't know the rates

133
00:05:59,330 --> 00:06:02,099
that 0day exploits
have been used in the wild

134
00:06:02,099 --> 00:06:05,462
for which we've not seen,
for which we're not discovering.

135
00:06:05,462 --> 00:06:09,198
So, we need to be very careful
about looking at this number,

136
00:06:09,198 --> 00:06:11,527
this failure case,

137
00:06:11,528 --> 00:06:13,858
and taking it too far,

138
00:06:13,858 --> 00:06:16,825
taking too much insight
from just this number alone.

139
00:06:16,825 --> 00:06:18,825
And the idea
behind this is something

140
00:06:18,825 --> 00:06:22,198
that is, I think, very central

141
00:06:22,198 --> 00:06:23,594
to my approach to security,

142
00:06:23,594 --> 00:06:26,066
which is this idea
that it's the attacker's job

143
00:06:26,066 --> 00:06:27,494
to deprive us of the data

144
00:06:27,495 --> 00:06:32,099
that we need to make
optimal decisions about defense.

145
00:06:32,099 --> 00:06:34,693
This is, I think,
a significant mind-set

146
00:06:34,693 --> 00:06:37,231
and it's stating the obvious,
of course.

147
00:06:37,231 --> 00:06:38,924
Security is unique in the sense

148
00:06:38,924 --> 00:06:40,330
that we have an adversary,

149
00:06:40,330 --> 00:06:41,924
and as engineers, our desire

150
00:06:41,924 --> 00:06:45,263
to be data-driven
in all things is very strong.

151
00:06:45,264 --> 00:06:46,792
It's natural, in fact.

152
00:06:46,792 --> 00:06:49,231
But security is a little bit
different in the sense

153
00:06:49,231 --> 00:06:50,924
that in many cases --

154
00:06:50,924 --> 00:06:52,527
not all cases,
but in many cases --

155
00:06:52,528 --> 00:06:55,726
just taking the data alone
at face value,

156
00:06:55,726 --> 00:06:58,891
will lead to suboptimal decision
making in security.

157
00:06:58,891 --> 00:07:02,363
So we try to think
about a little bit deeper,

158
00:07:02,363 --> 00:07:04,165
what is going on
behind the scenes,

159
00:07:04,165 --> 00:07:07,561
what is the model

160
00:07:07,561 --> 00:07:10,495
that we can build
about attacker behavior?

161
00:07:14,396 --> 00:07:17,000
One approach is to start
to imagine yourself

162
00:07:17,000 --> 00:07:19,594
as in individual
attack researcher.

163
00:07:19,594 --> 00:07:22,264
On a day-to-day basis, Project
Zero is performing this work,

164
00:07:22,264 --> 00:07:25,594
and we know also
the individual anxieties,

165
00:07:25,594 --> 00:07:27,825
the things that a researcher

166
00:07:27,825 --> 00:07:30,000
worries about on
the attack research side.

167
00:07:30,000 --> 00:07:31,495
It's questions like this.

168
00:07:31,495 --> 00:07:35,858
"Will I be able to find
a good vulnerability?"

169
00:07:35,858 --> 00:07:38,495
We know now that not
all vulnerabilities are equal.

170
00:07:38,495 --> 00:07:40,429
Some are more useful
than others.

171
00:07:40,429 --> 00:07:42,561
"What are the chances
that this particular bug

172
00:07:42,561 --> 00:07:44,462
I've just found will be fixed?"

173
00:07:44,462 --> 00:07:47,363
"What might be discovered
by the vendor itself

174
00:07:47,363 --> 00:07:48,957
or an open-source project

175
00:07:48,957 --> 00:07:50,693
and might be discovered
by another

176
00:07:50,693 --> 00:07:52,462
defensive-security researcher

177
00:07:52,462 --> 00:07:54,659
that's very disruptive
to me as an attacker

178
00:07:54,660 --> 00:07:57,231
if I want to use this bug
as an exploit?"

179
00:07:57,231 --> 00:07:59,099
And "how long will it take me
to write an exploit?"

180
00:07:59,099 --> 00:08:02,198
We know that attackers
are also resource constrained

181
00:08:02,198 --> 00:08:04,726
and that even though

182
00:08:04,726 --> 00:08:07,231
theoretically a bug
might be exploitable,

183
00:08:07,231 --> 00:08:11,627
there are prioritizations
that have to take place.

184
00:08:11,627 --> 00:08:13,363
And "how can I make
this exploit reliable?"

185
00:08:13,363 --> 00:08:16,330
Reliability is something
that I think defense

186
00:08:16,330 --> 00:08:19,825
understands very poorly
in the context of exploitation.

187
00:08:19,825 --> 00:08:24,495
When you're intending
to use an exploit

188
00:08:24,495 --> 00:08:26,429
in the wild in operation,

189
00:08:26,429 --> 00:08:27,659
if the exploit fails,

190
00:08:27,660 --> 00:08:31,165
this is a very costly thing
to occur.

191
00:08:31,165 --> 00:08:35,164
So we need to, as attackers,
make those exploits reliable.

192
00:08:35,164 --> 00:08:38,890
Now, at Project Zero,
we think about these questions

193
00:08:38,890 --> 00:08:41,527
and we think about how best
to approach each of these

194
00:08:41,528 --> 00:08:46,693
in a way to heighten the concern
of the attack researcher,

195
00:08:46,693 --> 00:08:50,264
the anxiety
of the attack researcher.

196
00:08:50,264 --> 00:08:53,396
So I wanted to dive into our
technical strategy a little bit.

197
00:08:53,396 --> 00:08:55,165
There's two sides,
the vulnerability research,

198
00:08:55,165 --> 00:08:57,198
and there's
the exploit-mitigation side.

199
00:08:57,198 --> 00:09:00,924
And I'll start with
our vulnerability research.

200
00:09:00,924 --> 00:09:04,033
And our strategy
really is based around one idea,

201
00:09:04,033 --> 00:09:07,396
which is what we call
contention.

202
00:09:07,396 --> 00:09:09,824
And contention is the occurrence

203
00:09:09,825 --> 00:09:14,726
of multiple researchers
running into the same output

204
00:09:14,726 --> 00:09:16,825
as a researcher --

205
00:09:16,825 --> 00:09:19,462
bug collision
or research rediscovery,

206
00:09:19,462 --> 00:09:20,759
in essence.

207
00:09:20,759 --> 00:09:23,396
Our strategy is built around
trying to increase

208
00:09:23,396 --> 00:09:25,495
the rate of bug collision
between ourselves

209
00:09:25,495 --> 00:09:27,924
and attack researchers

210
00:09:27,924 --> 00:09:29,824
in the private market.

211
00:09:29,825 --> 00:09:33,396
We do this in sort
of two main threads.

212
00:09:33,396 --> 00:09:36,462
Not exclusively, but primarily,
these are our two approaches.

213
00:09:36,462 --> 00:09:38,759
The pincer strategy that we use.

214
00:09:38,759 --> 00:09:41,495
The first is to eliminate
low-hanging fruits --

215
00:09:41,495 --> 00:09:45,033
to use Google's immense
machine resources

216
00:09:45,033 --> 00:09:46,858
and incredible access

217
00:09:46,858 --> 00:09:48,791
to corpus data,
file-format data,

218
00:09:48,792 --> 00:09:50,792
from our web crawler
and other sources

219
00:09:50,792 --> 00:09:53,759
to build some of
the world's best fuzzers.

220
00:09:53,759 --> 00:09:56,132
We know historically
that fuzzing is

221
00:09:56,132 --> 00:10:00,627
a method that a subset
of attackers are willing to use.

222
00:10:00,627 --> 00:10:02,495
Perhaps they're not
the most advanced attackers,

223
00:10:02,495 --> 00:10:04,033
but there are attackers
out there

224
00:10:04,033 --> 00:10:07,066
who are willing to use
fuzzing as a methodology

225
00:10:07,066 --> 00:10:09,429
to discover bugs
that are then exploited.

226
00:10:09,429 --> 00:10:11,231
And we think
that's just unacceptable.

227
00:10:11,231 --> 00:10:13,297
It's too easy. It's too cheap.

228
00:10:13,297 --> 00:10:15,759
And we believe
that we can actually

229
00:10:15,759 --> 00:10:18,000
substantially affect
the ability of attackers

230
00:10:18,000 --> 00:10:22,693
to use fuzzing as
a viable research methodology.

231
00:10:22,693 --> 00:10:24,495
On the other side --
And here I would say

232
00:10:24,495 --> 00:10:25,858
that the contention comes

233
00:10:25,858 --> 00:10:27,957
from a shared methodology,
a shared approach.

234
00:10:27,957 --> 00:10:30,066
There isn't such a wide variety

235
00:10:30,066 --> 00:10:31,528
of fuzzing techniques

236
00:10:31,528 --> 00:10:32,858
at this current moment,

237
00:10:32,858 --> 00:10:35,429
that different fuzzers
are always gonna

238
00:10:35,429 --> 00:10:36,759
find different sets of bugs.

239
00:10:36,759 --> 00:10:38,957
There's gonna be some level
of overlap in the output

240
00:10:38,957 --> 00:10:42,759
of independently written
fuzzers.

241
00:10:42,759 --> 00:10:46,264
The other approach is
the last step of the bug chain,

242
00:10:46,264 --> 00:10:48,363
and here what
I want to emphasize

243
00:10:48,363 --> 00:10:50,066
is that a modern exploit

244
00:10:50,066 --> 00:10:53,132
is not a single-shot
vulnerability anymore.

245
00:10:53,132 --> 00:10:56,462
They tend to be a chain
of vulnerabilities

246
00:10:56,462 --> 00:10:59,264
that add up
to a full-system compromise.

247
00:10:59,264 --> 00:11:03,297
A classic example of this is
when you attack a browser,

248
00:11:03,297 --> 00:11:05,363
you need a render a process bug,

249
00:11:05,363 --> 00:11:07,000
and you need a sandbox escape.

250
00:11:07,000 --> 00:11:10,132
Then you may also need
a kernel bug

251
00:11:10,132 --> 00:11:13,462
to get administrator access.

252
00:11:13,462 --> 00:11:16,726
So the observation here
is that the attacker

253
00:11:16,726 --> 00:11:18,165
requires the full chain,

254
00:11:18,165 --> 00:11:21,660
the complete chain in order
to have a complete capability.

255
00:11:21,660 --> 00:11:23,726
And along the way on that chain,

256
00:11:23,726 --> 00:11:26,231
there are some parts,
some links of the chain

257
00:11:26,231 --> 00:11:28,396
that are more fragile
than others,

258
00:11:28,396 --> 00:11:31,627
that are perhaps
constrained attack surfaces

259
00:11:31,627 --> 00:11:35,065
or that are less densely buggy
in some sense.

260
00:11:35,066 --> 00:11:38,363
And this is where we try
to focus much of our effort.

261
00:11:38,363 --> 00:11:41,132
The prime example
would be a sandbox escape

262
00:11:41,132 --> 00:11:44,824
or a kernel-privilege-escalation
bug.

263
00:11:44,825 --> 00:11:48,198
And here we're really willing
to use any methodology possible,

264
00:11:48,198 --> 00:11:52,726
a mix of automated methodologies

265
00:11:52,726 --> 00:11:56,165
and manual approaches,
manual analysis.

266
00:12:00,363 --> 00:12:01,627
But the question
still remains --

267
00:12:01,627 --> 00:12:03,957
How on earth do we know
what to look at?

268
00:12:03,957 --> 00:12:06,957
There's such a wide variety
of software targets out there

269
00:12:06,957 --> 00:12:09,792
in the world that
are potentially under attack.

270
00:12:09,792 --> 00:12:13,033
We are very limited
in our individual resources.

271
00:12:13,033 --> 00:12:17,462
Our team size is around
10 security researchers.

272
00:12:17,462 --> 00:12:20,264
So we need to prioritize
our own target selection.

273
00:12:20,264 --> 00:12:22,099
And again,
this comes back to the idea

274
00:12:22,099 --> 00:12:23,726
that we don't have
perfect insight

275
00:12:23,726 --> 00:12:26,165
into the target selection
of attackers.

276
00:12:26,165 --> 00:12:32,627
They don't share their lists
of exploits they have stored up.

277
00:12:32,627 --> 00:12:36,462
They don't share the lists
of software targets

278
00:12:36,462 --> 00:12:39,593
that they're willing to purchase
exploits against.

279
00:12:39,594 --> 00:12:42,924
So we need to model this again.

280
00:12:42,924 --> 00:12:44,560
And for us, it's a mix

281
00:12:44,561 --> 00:12:47,660
of different sources
of information.

282
00:12:47,660 --> 00:12:49,098
Firstly,
we look at that number --

283
00:12:49,099 --> 00:12:50,924
26 -- the observed attacks --

284
00:12:50,924 --> 00:12:53,693
and try and gain
some insights from that.

285
00:12:53,693 --> 00:12:56,924
But as I mentioned,
this is a little risky

286
00:12:56,924 --> 00:12:59,264
when you're studying the
failure case of the attacker.

287
00:12:59,264 --> 00:13:01,693
If you draw too much into that,

288
00:13:01,693 --> 00:13:04,660
you may run the risk
of drawing false conclusions.

289
00:13:04,660 --> 00:13:07,858
So we also try to gain
other insights

290
00:13:07,858 --> 00:13:10,957
about what the attackers
are going after.

291
00:13:10,957 --> 00:13:13,132
So a mix of external feedback

292
00:13:13,132 --> 00:13:15,231
from participants
within the private market,

293
00:13:15,231 --> 00:13:19,627
either directly, through
our relationships with them,

294
00:13:19,627 --> 00:13:23,165
or through other means
where public information --

295
00:13:23,165 --> 00:13:25,560
information becomes public
about that world,

296
00:13:25,561 --> 00:13:27,495
which happens from time to time.

297
00:13:27,495 --> 00:13:28,924
And the final way,
I think, perhaps,

298
00:13:28,924 --> 00:13:32,693
the most useful that I find,
is our own internal deduction.

299
00:13:32,693 --> 00:13:36,264
The premise of our team
is that we hire some of the top

300
00:13:36,264 --> 00:13:38,231
vulnerability researchers
in the world.

301
00:13:38,231 --> 00:13:41,561
We want to use their expertise
and their experience

302
00:13:41,561 --> 00:13:42,792
to predict --

303
00:13:42,792 --> 00:13:44,099
to have predictive value

304
00:13:44,099 --> 00:13:46,363
about what they think
will be the next big thing.

305
00:13:46,363 --> 00:13:48,429
More often than not, I believe,

306
00:13:48,429 --> 00:13:51,429
they are right on the money.

307
00:13:51,429 --> 00:13:53,593
So in practice,
what does that mean?

308
00:13:53,594 --> 00:13:56,264
We're focusing heavily
on endpoint attacks

309
00:13:56,264 --> 00:13:58,330
against ubiquitous
software systems.

310
00:13:58,330 --> 00:14:03,363
So mobile is a big thing.
We work on both Android and iOS.

311
00:14:03,363 --> 00:14:04,561
Desktop operating systems --

312
00:14:04,561 --> 00:14:08,297
The major kernels are a big part

313
00:14:08,297 --> 00:14:10,000
of what we work on.

314
00:14:10,000 --> 00:14:12,924
Almost every browser
is on our radar.

315
00:14:12,924 --> 00:14:14,593
And document readers

316
00:14:14,594 --> 00:14:16,198
and recently, also,

317
00:14:16,198 --> 00:14:19,528
antivirus endpoint
security systems.

318
00:14:24,627 --> 00:14:28,791
But vulnerability research
is only one side of the coin.

319
00:14:28,792 --> 00:14:31,660
We have a pipeline of sorts
where we find vulnerabilities.

320
00:14:31,660 --> 00:14:33,231
For some set
of those vulnerabilities,

321
00:14:33,231 --> 00:14:35,033
we write exploits.

322
00:14:35,033 --> 00:14:37,033
And then from those exploits,

323
00:14:37,033 --> 00:14:39,165
we gain insight about

324
00:14:39,165 --> 00:14:43,000
the state-of-the-art
of exploit mitigations.

325
00:14:43,000 --> 00:14:45,132
And this is an area

326
00:14:45,132 --> 00:14:48,363
where we think that
our practical insights

327
00:14:48,363 --> 00:14:50,099
about the realities
of exploitation

328
00:14:50,099 --> 00:14:52,198
can lead
to better exploit mitigations.

329
00:14:52,198 --> 00:14:53,957
There's kind
of two sides to that.

330
00:14:53,957 --> 00:14:55,924
In the process
of writing in exploits,

331
00:14:55,924 --> 00:14:57,957
we have to bypass
exploit mitigations

332
00:14:57,957 --> 00:14:59,627
that are already
shipped to users.

333
00:14:59,627 --> 00:15:03,593
So in essence,
we can find those edge cases --

334
00:15:03,594 --> 00:15:07,660
find the areas
where an implementation

335
00:15:07,660 --> 00:15:10,198
is not correct in relationship
to its design

336
00:15:10,198 --> 00:15:14,329
or that the design is not
sufficient in all cases.

337
00:15:14,330 --> 00:15:18,330
And on the other hand,
we can start to plan

338
00:15:18,330 --> 00:15:22,528
or suggest new ideas
for exploit mitigations.

339
00:15:22,528 --> 00:15:25,825
So using our insight
about our own exploits,

340
00:15:25,825 --> 00:15:27,891
we have a sense
of where in the exploit

341
00:15:27,891 --> 00:15:30,560
is there fragility --

342
00:15:30,561 --> 00:15:34,132
whether we get a little bit
lucky, in essence.

343
00:15:34,132 --> 00:15:37,858
Where might we incur
a lot more time

344
00:15:37,858 --> 00:15:41,527
or effort if we have
a new mitigation idea?

345
00:15:41,528 --> 00:15:42,792
And then we share that

346
00:15:42,792 --> 00:15:45,693
with the broader
software-development community

347
00:15:45,693 --> 00:15:48,231
and the broader
security community.

348
00:15:48,231 --> 00:15:50,066
In essence, I see ourselves

349
00:15:50,066 --> 00:15:53,891
as advocates for good
exploit-mitigation design.

350
00:15:53,891 --> 00:15:56,132
There's a lot of bad
exploit-mitigation design ideas

351
00:15:56,132 --> 00:15:57,363
out there, as well.

352
00:16:00,132 --> 00:16:02,791
But I've been talking
about exploit mitigations

353
00:16:02,792 --> 00:16:05,462
without actually really defining
what I mean,

354
00:16:05,462 --> 00:16:07,792
and this is something
that I think, in the past,

355
00:16:07,792 --> 00:16:09,231
as in the tech-research
community,

356
00:16:09,231 --> 00:16:10,627
we're being quite bad at,

357
00:16:10,627 --> 00:16:16,098
is just talking in generalities
and not in specificity.

358
00:16:16,099 --> 00:16:19,660
So, this is one approach
to one sort of --

359
00:16:19,660 --> 00:16:21,791
Well, we call it a taxonomy.

360
00:16:21,792 --> 00:16:23,264
It's a vocabulary,
if you will,

361
00:16:23,264 --> 00:16:25,297
about how to talk
about mitigations.

362
00:16:25,297 --> 00:16:26,726
It's not the perfect taxonomy.

363
00:16:26,726 --> 00:16:29,066
It's not the only one,
certainly.

364
00:16:29,066 --> 00:16:31,462
It is sufficient
as a starting point.

365
00:16:35,066 --> 00:16:37,594
So, to run through
some of these,

366
00:16:37,594 --> 00:16:39,429
a Strong Mitigation,

367
00:16:39,429 --> 00:16:41,065
these are ideas,

368
00:16:41,066 --> 00:16:44,198
like recently
Microsoft's Edge Browser

369
00:16:44,198 --> 00:16:47,066
has shipped a new allocator type
called MemGC.

370
00:16:47,066 --> 00:16:49,891
This may well be considered
a Strong Mitigation.

371
00:16:49,891 --> 00:16:51,462
Now, it's early days yet.

372
00:16:51,462 --> 00:16:53,066
It might end the bug class

373
00:16:53,066 --> 00:16:56,132
of use-after-frees
in Microsoft Edge,

374
00:16:56,132 --> 00:16:59,527
which would be a really amazing
accomplishment of Microsoft's.

375
00:16:59,528 --> 00:17:03,297
Another example
you might consider --

376
00:17:03,297 --> 00:17:04,858
/GS or DEP,

377
00:17:04,858 --> 00:17:08,231
stacked cookies making
stacked buffer overflows

378
00:17:08,231 --> 00:17:10,198
more or less unexploitable.

379
00:17:10,198 --> 00:17:12,693
Or, perhaps, other forms
of compiler technology --

380
00:17:12,693 --> 00:17:16,528
CFI, CFG.

381
00:17:16,528 --> 00:17:20,098
Now, Weak Mitigations

382
00:17:20,098 --> 00:17:22,231
is a little bit more
on the tactical level.

383
00:17:22,231 --> 00:17:25,000
So, we observe often
that there's commonalities

384
00:17:25,000 --> 00:17:27,627
between a sequence
of different exploits,

385
00:17:27,627 --> 00:17:31,792
where they're reliant
on the same ideas.

386
00:17:31,792 --> 00:17:36,890
And a recent example of this is
Project Zero's work with Adobe.

387
00:17:36,891 --> 00:17:38,660
We worked with Adobe

388
00:17:38,660 --> 00:17:40,330
to partition out

389
00:17:40,330 --> 00:17:43,000
a certain object type
called the vector of uint,

390
00:17:43,000 --> 00:17:45,528
which we had seen
in exploits time and time again

391
00:17:45,528 --> 00:17:47,693
over the last two, three years.

392
00:17:47,693 --> 00:17:50,792
And by partitioning out
that particular object type,

393
00:17:50,792 --> 00:17:54,759
it forced attackers to find
different objects to go after.

394
00:17:54,759 --> 00:17:57,792
Eventually, this leads
to a more generalized approach

395
00:17:57,792 --> 00:18:00,462
and a generalized solution.

396
00:18:00,462 --> 00:18:02,858
When saying that something
is a Weak Mitigation,

397
00:18:02,858 --> 00:18:06,495
it doesn't necessarily mean it's
worse than a Strong Mitigation.

398
00:18:06,495 --> 00:18:07,792
It's just the observation

399
00:18:07,792 --> 00:18:09,099
that the properties

400
00:18:09,099 --> 00:18:11,330
that are given by the mitigation

401
00:18:11,330 --> 00:18:14,131
is unlikely to stop
the exploit from occurring.

402
00:18:14,132 --> 00:18:17,099
It's about the overall cost,
the investment --

403
00:18:17,099 --> 00:18:19,495
the time and investment
going into the exploit.

404
00:18:21,693 --> 00:18:24,066
There's a couple of other types

405
00:18:24,066 --> 00:18:27,363
of exploit mitigations
that I'll briefly discuss,

406
00:18:27,363 --> 00:18:30,726
and they're on the verge.

407
00:18:30,726 --> 00:18:33,033
Some people wouldn't consider
these exploit mitigations.

408
00:18:33,033 --> 00:18:34,824
Personally, I do.

409
00:18:34,825 --> 00:18:37,066
And there are two types.

410
00:18:37,066 --> 00:18:38,528
Attack Surface Reduction --

411
00:18:38,528 --> 00:18:41,297
so, an example that
we're working on at the moment

412
00:18:41,297 --> 00:18:43,693
with Chrome is --

413
00:18:43,693 --> 00:18:45,825
as we mentioned,
the canonical exploit chain

414
00:18:45,825 --> 00:18:49,132
is a Flash exploit combined
with a kernel exploit.

415
00:18:49,132 --> 00:18:52,066
Now, the problem, in essence,

416
00:18:52,066 --> 00:18:54,033
is that Flash has
access to the kernel.

417
00:18:54,033 --> 00:18:56,462
Does it need it?
It turns out perhaps it doesn't.

418
00:18:56,462 --> 00:18:57,693
We're working on the project

419
00:18:57,693 --> 00:19:01,792
to remove access to Win32k

420
00:19:01,792 --> 00:19:03,659
from the Flash plug-in process,

421
00:19:03,660 --> 00:19:06,726
to effectively remove
the kernel attack surface

422
00:19:06,726 --> 00:19:09,132
from the purview of Flash.

423
00:19:09,132 --> 00:19:11,297
Other examples
are font transcoders,

424
00:19:11,297 --> 00:19:13,396
shader transcoders,
the types of ideas

425
00:19:13,396 --> 00:19:16,132
that we've had in
browser security in the past.

426
00:19:16,132 --> 00:19:21,000
And the final idea is perhaps
the rarest type of mitigation --

427
00:19:21,000 --> 00:19:22,231
Chain Extension.

428
00:19:22,231 --> 00:19:25,231
As I mentioned, exploit
mitigations are chains --

429
00:19:25,231 --> 00:19:26,957
Sorry, exploits are chains,

430
00:19:26,957 --> 00:19:29,231
and that the longer the chain
is means, effectively,

431
00:19:29,231 --> 00:19:32,792
that a new bug
has to be discovered.

432
00:19:32,792 --> 00:19:36,726
The chances to introduce
the chain extension

433
00:19:36,726 --> 00:19:37,924
are actually quite rare.

434
00:19:37,924 --> 00:19:40,099
Think of, perhaps,
something like ASLR,

435
00:19:40,099 --> 00:19:42,099
where you now leave
an information-leak bug,

436
00:19:42,099 --> 00:19:45,428
or introduce in a sandbox
or a virtualization

437
00:19:45,429 --> 00:19:47,198
ideas where you need
a VM escape.

438
00:19:47,198 --> 00:19:48,726
These are the types of ideas

439
00:19:48,726 --> 00:19:52,726
we would consider
a chain extension.

440
00:19:52,726 --> 00:19:54,528
So, to wrap things up
just quickly,

441
00:19:54,528 --> 00:19:55,825
here's a quick example

442
00:19:55,825 --> 00:19:58,627
of one of the projects
we've been working on.

443
00:19:58,627 --> 00:20:00,330
I've mentioned it. Flash.

444
00:20:00,330 --> 00:20:03,693
We've reported about 180
vulnerabilities in 18 months,

445
00:20:03,693 --> 00:20:05,858
and we've worked
with Adobe very closely.

446
00:20:05,858 --> 00:20:08,197
And Adobe have been
working very hard

447
00:20:08,198 --> 00:20:10,924
on exploit-mitigation
improvements.

448
00:20:10,924 --> 00:20:13,792
The observed price increase
for an exploit,

449
00:20:13,792 --> 00:20:15,824
based on just the public data,

450
00:20:15,825 --> 00:20:18,099
not with any sort
of market in sight,

451
00:20:18,099 --> 00:20:21,297
was 60% in 18 months.

452
00:20:21,297 --> 00:20:22,693
I'd like that to be higher.

453
00:20:22,693 --> 00:20:25,528
The reality, after some of
these mitigation improvements,

454
00:20:25,528 --> 00:20:30,858
we may well see an increase
in that price over time.

455
00:20:30,858 --> 00:20:32,066
Definitely on the ground

456
00:20:32,066 --> 00:20:34,297
as individual researchers
within our team,

457
00:20:34,297 --> 00:20:35,825
we have noticed the difficulty

458
00:20:35,825 --> 00:20:38,429
in performing exploitation work
against Flash

459
00:20:38,429 --> 00:20:40,891
in the last 18 months.

460
00:20:40,891 --> 00:20:43,594
So, what makes
software exploitation hard?

461
00:20:43,594 --> 00:20:47,198
For us, it's the combination
of vulnerability research

462
00:20:47,198 --> 00:20:48,858
and exploit mitigation.

463
00:20:48,858 --> 00:20:51,956
One without the other
is not quite as effective.

464
00:20:51,957 --> 00:20:53,891
If you just
perform vulnerability research,

465
00:20:53,891 --> 00:20:56,066
you are ceding
an advantage to attackers

466
00:20:56,066 --> 00:20:57,957
when they do eventually
find a bug.

467
00:20:57,957 --> 00:21:00,924
You are letting them build
reliable, reusable exploits

468
00:21:00,924 --> 00:21:04,627
for a very cheap investment.

469
00:21:04,627 --> 00:21:07,528
On the other hand, if you only
perform exploit mitigation --

470
00:21:07,528 --> 00:21:12,000
We know that most mitigations
have documented edge cases

471
00:21:12,000 --> 00:21:14,429
if you have a very densely
buggy attack.

472
00:21:14,429 --> 00:21:17,363
So, first, then, it's easier
to find edge-case bugs

473
00:21:17,363 --> 00:21:22,000
which aren't covered
by your mitigation approaches.

474
00:21:22,000 --> 00:21:24,033
So, we need to think
of exploits as chains.

475
00:21:24,033 --> 00:21:26,231
There's two approaches.

476
00:21:26,231 --> 00:21:28,363
You make
each link harder to achieve

477
00:21:28,363 --> 00:21:30,396
or you extend the chain.

478
00:21:30,396 --> 00:21:31,792
Thank you.

479
00:21:31,792 --> 00:21:34,462
[ Applause ]

480
00:21:34,462 --> 00:21:33,462


