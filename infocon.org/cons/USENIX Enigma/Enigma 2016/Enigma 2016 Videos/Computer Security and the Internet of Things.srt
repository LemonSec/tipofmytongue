1
00:00:00,033 --> 00:00:00,495


2
00:00:00,495 --> 00:00:04,726
♪♪

3
00:00:04,726 --> 00:00:07,032
-So, thank you.

4
00:00:07,033 --> 00:00:08,726
It's a great pleasure to be here today.

5
00:00:08,726 --> 00:00:11,561
As Daniella said, my name is Yoshi Kohno,

6
00:00:11,561 --> 00:00:13,363
and I wanted to spend a little bit of time

7
00:00:13,363 --> 00:00:16,065
talking with you about computer security

8
00:00:16,065 --> 00:00:19,230
and the Internet of Things.

9
00:00:19,231 --> 00:00:21,693
First off, what do we actually mean

10
00:00:21,693 --> 00:00:23,561
by "the Internet of Things"?

11
00:00:23,561 --> 00:00:25,759
We can give you a few examples --

12
00:00:25,759 --> 00:00:27,099
things you might have thought about,

13
00:00:27,099 --> 00:00:30,693
such as door locks, thermostats, furnaces, refrigerators,

14
00:00:30,693 --> 00:00:32,692
children's toys, automobiles.

15
00:00:32,692 --> 00:00:34,330
Essentially, I like to think about

16
00:00:34,330 --> 00:00:37,230
the Internet of Things being any connected device

17
00:00:37,231 --> 00:00:39,660
that a consumer might try to purchase.

18
00:00:39,660 --> 00:00:42,726
And so that's the definition that I'll use for the purposes

19
00:00:42,726 --> 00:00:45,891
of today's talk.

20
00:00:45,891 --> 00:00:47,759
Now, when we think about the Internet of Things,

21
00:00:47,759 --> 00:00:49,198
it brings numerous benefits.

22
00:00:49,198 --> 00:00:50,462
I mean, there's a reason why

23
00:00:50,462 --> 00:00:53,297
the Internet of Things is coming.

24
00:00:53,297 --> 00:00:55,165
Of course, when we also think about technology,

25
00:00:55,165 --> 00:00:56,726
we also have to realize that there might be

26
00:00:56,726 --> 00:00:58,693
some associated risks.

27
00:00:58,693 --> 00:01:00,626
And there's a wide variety of the types of risks

28
00:01:00,627 --> 00:01:02,660
that one might think about,

29
00:01:02,660 --> 00:01:04,363
but for today, I really want to talk

30
00:01:04,363 --> 00:01:07,000
about what could the security and privacy risks

31
00:01:07,000 --> 00:01:09,297
be with the Internet of Things?

32
00:01:11,528 --> 00:01:14,099
There are a number of topics that probably immediately

33
00:01:14,099 --> 00:01:15,627
come to mind.

34
00:01:15,627 --> 00:01:17,264
For example, does the Internet of Things

35
00:01:17,264 --> 00:01:19,759
affect people's safety?

36
00:01:19,759 --> 00:01:22,561
Or are there privacy risks with the Internet of Things?

37
00:01:22,561 --> 00:01:25,594
You know, what do these sensors actually reveal about us?

38
00:01:25,594 --> 00:01:27,363
And these are things that I'm gonna talk about,

39
00:01:27,363 --> 00:01:29,000
but I also want to broaden the definition

40
00:01:29,000 --> 00:01:31,099
and include other types of issues as well.

41
00:01:31,099 --> 00:01:33,891
So, for example, not just what are the direct privacy leakages,

42
00:01:33,891 --> 00:01:35,759
but what other information can be inferred

43
00:01:35,759 --> 00:01:36,858
about a person or people

44
00:01:36,858 --> 00:01:38,792
from these Internet of Things devices?

45
00:01:38,792 --> 00:01:41,561
What financial risks might arise?

46
00:01:41,561 --> 00:01:43,627
Can these devices be used as stepping stones?

47
00:01:43,627 --> 00:01:46,363
And, of course, zombies and the uncertain future.

48
00:01:46,363 --> 00:01:47,627
If you don't know what that means,

49
00:01:47,627 --> 00:01:50,494
you'll know by the end of this talk.

50
00:01:50,495 --> 00:01:53,330
And so this talk is a sequence of examples of research projects

51
00:01:53,330 --> 00:01:54,890
that have come out of The University of Washington

52
00:01:54,891 --> 00:01:57,429
as well as with collaborators.

53
00:01:57,429 --> 00:01:58,891
And my goal in presenting these things

54
00:01:58,891 --> 00:02:00,825
is try to help encourage a broad range

55
00:02:00,825 --> 00:02:02,099
of thinking about the issues

56
00:02:02,099 --> 00:02:05,429
that can arise in an IoT environment.

57
00:02:05,429 --> 00:02:07,396
And again, of course, all this work

58
00:02:07,396 --> 00:02:09,330
was made possible due to wonderful collaborations

59
00:02:09,330 --> 00:02:11,693
with students at The University of Washington, other faculty,

60
00:02:11,693 --> 00:02:14,561
students and faculty elsewhere, and our generous funders.

61
00:02:16,759 --> 00:02:18,165
The first example I wanted to give you

62
00:02:18,165 --> 00:02:20,329
is that of a modern automobile.

63
00:02:20,330 --> 00:02:21,429
The modern automobile

64
00:02:21,429 --> 00:02:24,363
is becoming increasingly computerized.

65
00:02:24,363 --> 00:02:26,429
There might be 10, 30, even 100 computers

66
00:02:26,429 --> 00:02:28,924
inside the modern automobile,

67
00:02:28,924 --> 00:02:31,330
and these computers are connected to each other,

68
00:02:31,330 --> 00:02:34,066
and this brings numerous safety and other benefits.

69
00:02:34,066 --> 00:02:35,396
For example, you might have a computer

70
00:02:35,396 --> 00:02:36,759
attached to each wheel

71
00:02:36,759 --> 00:02:38,462
that's detecting how fast each wheel is spinning,

72
00:02:38,462 --> 00:02:40,066
then it will send a message

73
00:02:40,066 --> 00:02:41,726
over the car's internal computer network

74
00:02:41,726 --> 00:02:44,330
to another computer that will determine

75
00:02:44,330 --> 00:02:46,099
whether one wheel is spinning faster than the other.

76
00:02:46,099 --> 00:02:48,594
If that's happening, you might be getting into a skid.

77
00:02:48,594 --> 00:02:50,231
And so that computer will send another message

78
00:02:50,231 --> 00:02:51,858
to the brake computer, saying, "Brake computer,

79
00:02:51,858 --> 00:02:53,693
slow down the back-right wheel."

80
00:02:53,693 --> 00:02:55,858
And it's through this process that we get increased safety.

81
00:02:55,858 --> 00:02:59,165
For example, traction control.

82
00:02:59,165 --> 00:03:03,263
So numerous benefits, but what are the risks?

83
00:03:03,264 --> 00:03:04,825
This is Karl Koscher, one of the PhD students

84
00:03:04,825 --> 00:03:07,363
on the project.

85
00:03:07,363 --> 00:03:09,330
To try to explore this question,

86
00:03:09,330 --> 00:03:11,363
we, in collaboration with UC San Diego --

87
00:03:11,363 --> 00:03:12,594
you'll hear from Stefan next --

88
00:03:12,594 --> 00:03:16,000
purchased two 2009-edition modern sedans,

89
00:03:16,000 --> 00:03:17,297
and we subjected them to

90
00:03:17,297 --> 00:03:18,825
basically computer-security analyses --

91
00:03:18,825 --> 00:03:21,099
tried to figure out what we could do.

92
00:03:23,132 --> 00:03:24,396
The first thing we needed to figure out

93
00:03:24,396 --> 00:03:25,825
is, how can we communicate

94
00:03:25,825 --> 00:03:27,923
on the car's internal computer network?

95
00:03:27,924 --> 00:03:29,231
That's the first thing an adversary would need

96
00:03:29,231 --> 00:03:30,528
to be able to do.

97
00:03:30,528 --> 00:03:32,495
We found numerous ways to do that.

98
00:03:32,495 --> 00:03:35,066
For example, our car has an exposed diagnostics port

99
00:03:35,066 --> 00:03:36,858
underneath the dash.

100
00:03:36,858 --> 00:03:38,825
So an adversary or unauthorized party

101
00:03:38,825 --> 00:03:40,429
could just connect his or her own equipment

102
00:03:40,429 --> 00:03:42,132
to that diagnostics port

103
00:03:42,132 --> 00:03:43,726
and communicate with other components

104
00:03:43,726 --> 00:03:45,924
within the vehicle.

105
00:03:45,924 --> 00:03:49,231
Our car also has a built-in telematics unit.

106
00:03:49,231 --> 00:03:52,792
A telematics unit allows the car to effectively call 911

107
00:03:52,792 --> 00:03:55,462
if it gets into an accident.

108
00:03:55,462 --> 00:03:57,165
What this means is that as an adversary,

109
00:03:57,165 --> 00:04:00,626
we are also able to call that car's phone number.

110
00:04:00,627 --> 00:04:02,693
We found that we could call that car's phone number,

111
00:04:02,693 --> 00:04:05,132
play the appropriate tones to switch to an in-band modem,

112
00:04:05,132 --> 00:04:06,594
play another modem tone

113
00:04:06,594 --> 00:04:08,528
to bypass an authentication vulnerability,

114
00:04:08,528 --> 00:04:10,594
and then some more modem tones to get our own code

115
00:04:10,594 --> 00:04:12,726
running on the car.

116
00:04:12,726 --> 00:04:14,231
Our car has 3G data,

117
00:04:14,231 --> 00:04:16,428
it has Internet off the lot,

118
00:04:16,428 --> 00:04:18,099
and so our little piece of code

119
00:04:18,099 --> 00:04:20,197
opens up an FTP connection to servers

120
00:04:20,197 --> 00:04:22,890
at The University of Washington, which downloads an IRC client.

121
00:04:22,891 --> 00:04:24,231
We then run our IRC client

122
00:04:24,231 --> 00:04:26,429
on the cars, actually both cars.

123
00:04:26,429 --> 00:04:27,957
They connect to servers at The University of Washington,

124
00:04:27,957 --> 00:04:29,561
and we have an automobile botnet.

125
00:04:32,462 --> 00:04:34,659
Once we could communicate on the car's internal computer

126
00:04:34,660 --> 00:04:37,033
network, we could start affecting other components

127
00:04:37,033 --> 00:04:38,462
within the vehicle.

128
00:04:38,462 --> 00:04:39,957
Since a picture's worth a thousand words,

129
00:04:39,957 --> 00:04:41,462
I want to show you a video.

130
00:04:41,462 --> 00:04:44,033
This is a video of us driving one of our test vehicles

131
00:04:44,033 --> 00:04:46,825
down a decommissioned airport runway.

132
00:04:46,825 --> 00:04:49,296
For this experiment, Alexei Czeskis is driving.

133
00:04:49,297 --> 00:04:51,165
Karl Koscher has his laptop connected

134
00:04:51,165 --> 00:04:52,627
to the diagnostics port underneath the dash.

135
00:04:52,627 --> 00:04:54,627
He'll press a button on his laptop

136
00:04:54,627 --> 00:04:56,264
that will forcibly apply the brakes

137
00:04:56,264 --> 00:04:57,858
as Alexei is driving,

138
00:04:57,858 --> 00:05:01,132
basically exploiting properties of the car's brake controller.

139
00:05:01,132 --> 00:05:02,495
-2, 1...

140
00:05:02,495 --> 00:05:04,924
-So you can hear, Alexei's talking on the radio.

141
00:05:04,924 --> 00:05:06,099
We're kind of coordinating all this.

142
00:05:06,099 --> 00:05:08,890
Numerous safety precautions in place.

143
00:05:08,891 --> 00:05:12,165
-Hit the brakes at 3, 2, 1, 0 -- now.

144
00:05:12,165 --> 00:05:14,264
-Karl presses a button on his computer...

145
00:05:14,264 --> 00:05:17,561
sends a message over the car's network, the brakes are applied.

146
00:05:17,561 --> 00:05:19,000
-That was cool.

147
00:05:19,000 --> 00:05:20,429
Really scary, but cool.

148
00:05:20,429 --> 00:05:21,726
-So we have a lot of fun, of course,

149
00:05:21,726 --> 00:05:23,165
doing our research.

150
00:05:23,165 --> 00:05:25,231
This is us disengaging the brakes,

151
00:05:25,231 --> 00:05:27,000
so Alexei's gonna get up to 20 miles an hour,

152
00:05:27,000 --> 00:05:30,528
Karl will send the message over the car's computer network

153
00:05:30,528 --> 00:05:31,759
that will disengage the brakes.

154
00:05:31,759 --> 00:05:33,660
And you can think about how this might arise

155
00:05:33,660 --> 00:05:36,066
in the sense that our car has anti-lock braking,

156
00:05:36,066 --> 00:05:37,263
which means that it has the ability

157
00:05:37,264 --> 00:05:39,759
to override the driver's brake pedal,

158
00:05:39,759 --> 00:05:41,198
and we just exploit that.

159
00:05:41,198 --> 00:05:43,231
-3, 2, 1, 0.

160
00:05:43,231 --> 00:05:47,363
-So Alexei's gonna be getting up to speed.

161
00:05:47,363 --> 00:05:49,165
This timer isn't working, by the way.

162
00:05:49,165 --> 00:05:50,231
It still says 30.

163
00:05:50,231 --> 00:05:52,132
-Delivering the packet at 3, 2, 1, 0.

164
00:05:52,132 --> 00:05:55,000
-Karl presses a button on his internal computer network.

165
00:05:55,000 --> 00:05:57,726
-I cannot brake. -Alexei can't brake.

166
00:06:01,000 --> 00:06:03,396
-Okay, now I'm at 20 and I still can't brake.

167
00:06:03,396 --> 00:06:06,561
-And so he's pushing full pressure on the brake pedal.

168
00:06:06,561 --> 00:06:08,164
You can see the end of the airport runway, by the way.

169
00:06:08,165 --> 00:06:09,429
-I'm gonna try the parking brake,

170
00:06:09,429 --> 00:06:11,957
in case someone actually might be in this situation.

171
00:06:11,957 --> 00:06:13,891
-For this particular vehicle, the parking brake

172
00:06:13,891 --> 00:06:15,132
was under mechanical control.

173
00:06:15,132 --> 00:06:16,528
It did stop the car.

174
00:06:16,528 --> 00:06:18,231
We didn't want to test faster than 20 miles an hour,

175
00:06:18,231 --> 00:06:20,264
and I would say that for other vehicles,

176
00:06:20,264 --> 00:06:24,429
the parking brake is under electronic control.

177
00:06:24,429 --> 00:06:28,066
So, the lesson from this is that there are potential safety risks

178
00:06:28,066 --> 00:06:31,000
with computer security issues with IoT devices.

179
00:06:31,000 --> 00:06:33,000
That doesn't mean these necessarily are gonna manifest

180
00:06:33,000 --> 00:06:34,330
in the real world.

181
00:06:34,330 --> 00:06:36,858
So there's a difference between, you know, the fact

182
00:06:36,858 --> 00:06:38,462
that someone could do this and someone will do this,

183
00:06:38,462 --> 00:06:40,890
but this is the type of lesson I wanted to extract.

184
00:06:44,066 --> 00:06:45,890
One of the other things that we tried to explore

185
00:06:45,891 --> 00:06:47,561
were are there other things

186
00:06:47,561 --> 00:06:49,296
an adversary might try to do with this vehicle?

187
00:06:49,297 --> 00:06:52,363
So I'm gonna turn to another example.

188
00:06:52,363 --> 00:06:55,231
Our car has Bluetooth hands-free calling in it.

189
00:06:55,231 --> 00:07:00,198
This means that there are microphones inside the cabin.

190
00:07:00,198 --> 00:07:01,858
In this video,

191
00:07:01,858 --> 00:07:03,363
I'll show you how we've called the car's phone number --

192
00:07:03,363 --> 00:07:05,000
again, an unmodified car off the lot.

193
00:07:05,000 --> 00:07:06,462
Called the car's phone number.

194
00:07:06,462 --> 00:07:08,561
play the modem tones to switch to the in-band modem,

195
00:07:08,561 --> 00:07:10,231
bypass authentication, get our own code

196
00:07:10,231 --> 00:07:12,462
running on the car, our car then connects to our servers

197
00:07:12,462 --> 00:07:15,858
at The University of Washington for further commands.

198
00:07:15,858 --> 00:07:17,198
In this particular video, we will show

199
00:07:17,198 --> 00:07:19,296
how we can basically,

200
00:07:19,297 --> 00:07:20,924
once we've connected to this car,

201
00:07:20,924 --> 00:07:23,296
we can enable a theft program.

202
00:07:23,297 --> 00:07:25,066
So we can learn the car's GPS coordinates, you know,

203
00:07:25,066 --> 00:07:27,164
where the car is, and then our code on the car

204
00:07:27,165 --> 00:07:28,627
can do things like unlock the car's door,

205
00:07:28,627 --> 00:07:31,198
start the engine, disable the shift lock solenoid,

206
00:07:31,198 --> 00:07:33,066
so that a thief could just go to those GPS coordinates,

207
00:07:33,066 --> 00:07:35,659
get in the car, and drive away.

208
00:07:35,660 --> 00:07:37,099
-We're going to steal this car over the Internet.

209
00:07:37,099 --> 00:07:38,495
-So, this is Franzi Roesner.

210
00:07:38,495 --> 00:07:39,792
She's now another professor at The University of Washington.

211
00:07:39,792 --> 00:07:41,231
-This car has an IRC button on it

212
00:07:41,231 --> 00:07:43,429
that will let us communicate with it.

213
00:07:43,429 --> 00:07:45,561
-She's connected to this unmodified vehicle

214
00:07:45,561 --> 00:07:46,890
over the Internet, again,

215
00:07:46,891 --> 00:07:49,330
with the car's built-in cellular connection.

216
00:07:51,462 --> 00:07:52,726
-And then it's just a simple matter

217
00:07:52,726 --> 00:07:53,891
of executing the theft program.

218
00:07:53,891 --> 00:07:55,033
-Running the theft program.

219
00:07:55,033 --> 00:07:57,627
In a moment, you'll hear the car start.

220
00:07:57,627 --> 00:07:59,264
[ Engine starts ] -And then my colleague

221
00:07:59,264 --> 00:08:02,495
can just drive the car away without a key.

222
00:08:02,495 --> 00:08:04,330
-And you'll soon see Karl Koscher, one of the other

223
00:08:04,330 --> 00:08:07,825
PhD students on the project... -[ Laughs evilly ]

224
00:08:07,825 --> 00:08:09,198
-...walking up to the car.

225
00:08:09,198 --> 00:08:11,924
The theft program unlocked the car's doors,

226
00:08:11,924 --> 00:08:14,726
already started the engine, as you heard.

227
00:08:14,726 --> 00:08:16,297
Like any good car thief,

228
00:08:16,297 --> 00:08:18,726
always remember to buckle up, which Karl will do next.

229
00:08:18,726 --> 00:08:21,759
-Do-de-do-de-do.

230
00:08:21,759 --> 00:08:23,330
-We've disabled the shift lock solenoid,

231
00:08:23,330 --> 00:08:24,527
so you can shift out of park.

232
00:08:24,528 --> 00:08:26,264
No key in the ignition,

233
00:08:26,264 --> 00:08:27,528
and we've bypassed the immobilizer,

234
00:08:27,528 --> 00:08:29,231
which keeps the car running. -Here we go.

235
00:08:32,495 --> 00:08:34,594
-And I was confused by the timer,

236
00:08:34,594 --> 00:08:37,165
so I gave some of the examples a little bit differently.

237
00:08:37,164 --> 00:08:38,693
So the next one -- so this is an example

238
00:08:38,693 --> 00:08:40,264
of the financial risks.

239
00:08:40,264 --> 00:08:42,395
The next one is one about Bluetooth hand-free calling.

240
00:08:42,395 --> 00:08:45,858
So our car does have microphones within the cabin.

241
00:08:45,858 --> 00:08:47,758
Adversary can turn on these microphones

242
00:08:47,759 --> 00:08:50,264
and listen in on anything happening inside the vehicle.

243
00:08:50,264 --> 00:08:52,198
So in this particular example, Steve Checkoway,

244
00:08:52,198 --> 00:08:53,495
one of the PhD students at the time

245
00:08:53,495 --> 00:08:55,495
at The University of California San Diego,

246
00:08:55,495 --> 00:08:57,297
has compromised the UW car.

247
00:08:57,297 --> 00:08:59,759
Alexei and Karl are gonna walk into the UW car

248
00:08:59,759 --> 00:09:03,363
and have a private conversation.

249
00:09:03,363 --> 00:09:06,726
So here they are walking into the car --

250
00:09:06,726 --> 00:09:08,000
again, unmodified car. This time,

251
00:09:08,000 --> 00:09:10,858
compromised by the UC San Diego folks.

252
00:09:10,858 --> 00:09:12,560
-[ Laughs evilly ]

253
00:09:12,561 --> 00:09:15,198
King Bowser's gonna be super excited

254
00:09:15,198 --> 00:09:17,066
that we finally kidnapped Yoshi.

255
00:09:17,066 --> 00:09:19,858
-They came up with this skit on their own.

256
00:09:19,858 --> 00:09:21,396
And so, meanwhile, you'll see what is happening

257
00:09:21,396 --> 00:09:22,824
at Steve Checkoway's office.

258
00:09:22,825 --> 00:09:24,165
In Steve Checkoway's office,

259
00:09:24,165 --> 00:09:26,627
you can see that he can observe the GPS coordinates

260
00:09:26,627 --> 00:09:28,924
of the vehicle,

261
00:09:28,924 --> 00:09:30,264
and pretty soon --

262
00:09:30,264 --> 00:09:32,066
Alexei and Karl are driving around --

263
00:09:32,066 --> 00:09:33,792
pretty soon you'll start hearing the real-time audio

264
00:09:33,792 --> 00:09:36,462
that is actually being recorded in Steve's office.

265
00:09:36,462 --> 00:09:38,165
So it is of this high quality.

266
00:09:41,792 --> 00:09:43,297
It starts just about now.

267
00:09:44,297 --> 00:09:46,759
-[ Laughs evilly ]

268
00:09:46,759 --> 00:09:49,396
King Bowser's gonna be super excited

269
00:09:49,396 --> 00:09:51,000
that we finally kidnapped Yoshi.

270
00:09:51,000 --> 00:09:52,561
-And so the lesson from this is

271
00:09:52,561 --> 00:09:55,033
that there are potential privacy risks

272
00:09:55,033 --> 00:09:57,825
with computer security issues in an IoT environment.

273
00:09:57,825 --> 00:10:00,396
This time, you know, there are microphones in the cabin.

274
00:10:00,396 --> 00:10:01,759
If an adversary can get access to that,

275
00:10:01,759 --> 00:10:04,825
they can learn private information.

276
00:10:04,825 --> 00:10:06,462
I want to move on to example number two,

277
00:10:06,462 --> 00:10:08,462
very briefly, on children's toys.

278
00:10:08,462 --> 00:10:09,792
One of the things that we're seeing

279
00:10:09,792 --> 00:10:11,924
is that there are becoming increasing computerization

280
00:10:11,924 --> 00:10:14,924
with things that we are giving to our children.

281
00:10:14,924 --> 00:10:16,495
In fact, these are some dated photos, now,

282
00:10:16,495 --> 00:10:18,330
from 2008 and 2009,

283
00:10:18,330 --> 00:10:20,528
of some toys that you might get for a child

284
00:10:20,528 --> 00:10:22,792
that have computation in them.

285
00:10:22,792 --> 00:10:24,660
These two particular ones, here,

286
00:10:24,660 --> 00:10:28,033
have both a built-in webcam and Wi-Fi.

287
00:10:28,033 --> 00:10:30,627
One example usage model for this might be --

288
00:10:30,627 --> 00:10:33,758
a friend gives, or a child receives one of these toys

289
00:10:33,759 --> 00:10:35,132
for their birthday,

290
00:10:35,132 --> 00:10:38,032
assembles it, connects it to their home wireless network,

291
00:10:38,033 --> 00:10:39,594
and then when they go to their friend's house,

292
00:10:39,594 --> 00:10:42,033
they can connect back over the Internet to the toy in the room

293
00:10:42,033 --> 00:10:44,759
and make sure their parents aren't in the room.

294
00:10:44,759 --> 00:10:46,363
Unfortunately, we found that these webcams

295
00:10:46,363 --> 00:10:48,627
are also accessible to other parties.

296
00:10:48,627 --> 00:10:51,726
So other adversaries over the Internet who can then also

297
00:10:51,726 --> 00:10:53,132
see what's happening inside the children's room

298
00:10:53,132 --> 00:10:56,858
or wherever the robot might be -- the toys might be.

299
00:10:56,858 --> 00:11:00,296
So one of the key lessons from this is that there, again,

300
00:11:00,297 --> 00:11:04,033
are privacy issues with these IoT technologies.

301
00:11:04,033 --> 00:11:05,924
We can also think about other issues that might arise.

302
00:11:05,924 --> 00:11:07,528
So, for example, if there's a bunch of webcams

303
00:11:07,528 --> 00:11:09,759
in our environment, could an adversary some day

304
00:11:09,759 --> 00:11:11,099
use those for financial gain?

305
00:11:11,099 --> 00:11:14,528
Maybe blackmail and so on.

306
00:11:14,528 --> 00:11:16,264
The toy example also brings up the question

307
00:11:16,264 --> 00:11:18,396
of who administers these devices.

308
00:11:18,396 --> 00:11:19,627
You know, in the future, we might have

309
00:11:19,627 --> 00:11:21,198
devices connected to our home networks

310
00:11:21,198 --> 00:11:22,957
that are being administered by children,

311
00:11:22,957 --> 00:11:25,660
maybe sometimes they can't even read yet.

312
00:11:25,660 --> 00:11:27,824
And, also, who is affected by these devices?

313
00:11:27,825 --> 00:11:29,858
Not just the user of the technology,

314
00:11:29,858 --> 00:11:32,198
but anyone else in the device's environment.

315
00:11:35,231 --> 00:11:36,825
For my third example, I want to come back

316
00:11:36,825 --> 00:11:38,792
to the modern automobile.

317
00:11:38,792 --> 00:11:40,165
I mentioned that there was a lot

318
00:11:40,165 --> 00:11:44,957
of communication already occurring within the car.

319
00:11:44,957 --> 00:11:46,593
And so, for example, we have messages being sent

320
00:11:46,594 --> 00:11:48,231
from the brake computer to other computers.

321
00:11:48,231 --> 00:11:50,099
We have messages being sent on the car's internal computer

322
00:11:50,099 --> 00:11:54,099
network, when someone turns on the lights, and so on.

323
00:11:54,099 --> 00:11:55,594
One of the questions we had is,

324
00:11:55,594 --> 00:11:58,561
what private information could someone learn

325
00:11:58,561 --> 00:12:01,132
about the drivers of these vehicles

326
00:12:01,132 --> 00:12:04,065
just by running analytics on top of the messages

327
00:12:04,066 --> 00:12:07,264
that are already being communicated within the car?

328
00:12:07,264 --> 00:12:10,825
So, for example, what if information

329
00:12:10,825 --> 00:12:13,000
and analytics of the messages going on within the car

330
00:12:13,000 --> 00:12:14,528
was being sent from the telematics unit

331
00:12:14,528 --> 00:12:17,825
to the telematics service provider?

332
00:12:17,825 --> 00:12:19,429
Or what if the car's owner

333
00:12:19,429 --> 00:12:21,000
bought one of these insurance dongles,

334
00:12:21,000 --> 00:12:22,495
connected it to the diagnostics port

335
00:12:22,495 --> 00:12:24,264
underneath the dash -- you might have heard, for example,

336
00:12:24,264 --> 00:12:26,891
of the Progressive Auto Insurance's dongle.

337
00:12:26,891 --> 00:12:29,065
Well, what if that dongle was sending information

338
00:12:29,066 --> 00:12:31,066
about the communications that were already occurring

339
00:12:31,066 --> 00:12:33,429
on the car's internal computer network?

340
00:12:33,429 --> 00:12:37,363
What private information might they be able to learn?

341
00:12:37,363 --> 00:12:38,858
To kind of push on this further,

342
00:12:38,858 --> 00:12:41,627
we found that someone monitoring the existing communications

343
00:12:41,627 --> 00:12:43,296
on the car's internal computer network

344
00:12:43,297 --> 00:12:45,165
could, for example, identify the driver

345
00:12:45,165 --> 00:12:47,363
from the set of small possible drivers in the vehicle.

346
00:12:47,363 --> 00:12:52,033
For example, which of the family members is driving this car?

347
00:12:52,033 --> 00:12:53,726
The reason I wanted to give this example

348
00:12:53,726 --> 00:12:55,000
is to show that there's information

349
00:12:55,000 --> 00:12:57,891
that can be inferred -- private information

350
00:12:57,891 --> 00:12:59,759
that can be inferred from these devices

351
00:12:59,759 --> 00:13:01,231
that might not be directly exposed.

352
00:13:01,231 --> 00:13:02,462
For example, not like the microphone

353
00:13:02,462 --> 00:13:04,363
and the video camera, but there's information

354
00:13:04,363 --> 00:13:07,363
that someone can learn by just running analytics

355
00:13:07,363 --> 00:13:10,462
on top of the information already there.

356
00:13:13,132 --> 00:13:14,396
For example number four,

357
00:13:14,396 --> 00:13:17,000
I want to talk about home power line monitoring.

358
00:13:17,000 --> 00:13:18,561
We're seeing more companies,

359
00:13:18,561 --> 00:13:20,066
and also academic efforts,

360
00:13:20,066 --> 00:13:23,165
focusing on how can we more effectively monitor

361
00:13:23,165 --> 00:13:24,957
the use of the home power line?

362
00:13:24,957 --> 00:13:26,462
So, for example, there are products out there

363
00:13:26,462 --> 00:13:29,264
that will give you essentially an itemized bill

364
00:13:29,264 --> 00:13:32,957
of how much power each device is using within your home.

365
00:13:32,957 --> 00:13:34,693
So, how much power is a toaster using?

366
00:13:34,693 --> 00:13:35,858
How much power is a stove using?

367
00:13:35,858 --> 00:13:37,263
How much power is a washing machine using?

368
00:13:37,264 --> 00:13:39,297
And so on.

369
00:13:39,297 --> 00:13:41,561
And there's numerous reasons why someone might want this,

370
00:13:41,561 --> 00:13:44,759
but the question we asked is, if someone wanted to,

371
00:13:44,759 --> 00:13:46,528
how much more private information

372
00:13:46,528 --> 00:13:48,363
could they learn about the activities

373
00:13:48,363 --> 00:13:50,693
within the home by monitoring the power line?

374
00:13:53,132 --> 00:13:54,791
What we found is that an adversary

375
00:13:54,792 --> 00:13:56,858
monitoring the home's power line could even go so far

376
00:13:56,858 --> 00:14:00,924
as inferring what TV show someone might be watching on TV.

377
00:14:00,924 --> 00:14:03,429
To give you an indication of how this might be possible,

378
00:14:03,429 --> 00:14:07,000
this figure on the right shows measurements

379
00:14:07,000 --> 00:14:08,792
that are being taken from a device connected

380
00:14:08,792 --> 00:14:11,462
to the home power line shown in the frequency domain,

381
00:14:11,462 --> 00:14:14,066
so monitoring the power line in the frequency domain.

382
00:14:14,066 --> 00:14:15,528
And the part on the left --

383
00:14:15,528 --> 00:14:16,957
you can see very visually

384
00:14:16,957 --> 00:14:18,627
when the washing machine turns on and off

385
00:14:18,627 --> 00:14:21,429
and when a CFL light bulb turns on or off.

386
00:14:21,429 --> 00:14:24,264
And you can see that little wavy line by the TV.

387
00:14:24,264 --> 00:14:25,528
It turns out that as the TV

388
00:14:25,528 --> 00:14:27,693
is going from light scenes to dark scenes,

389
00:14:27,693 --> 00:14:30,561
it affects this signal that appears on the power line.

390
00:14:30,561 --> 00:14:32,660
And since different TV shows have different characteristic

391
00:14:32,660 --> 00:14:34,791
patterns from light to dark and so on,

392
00:14:34,792 --> 00:14:36,297
we can use that to infer information

393
00:14:36,297 --> 00:14:40,000
about what show someone is watching on TV.

394
00:14:40,000 --> 00:14:41,660
So, again, the lesson from this is that information

395
00:14:41,660 --> 00:14:42,924
can be inferred

396
00:14:42,924 --> 00:14:46,000
that might not necessarily be directly collected.

397
00:14:48,198 --> 00:14:49,528
For my final example,

398
00:14:49,528 --> 00:14:54,000
I want to talk about home automation systems.

399
00:14:54,000 --> 00:14:56,561
Home automation systems are becoming increasingly popular.

400
00:14:56,561 --> 00:14:58,330
You might've seen advertisements for them.

401
00:14:58,330 --> 00:14:59,792
These are systems, for example,

402
00:14:59,792 --> 00:15:02,528
you might drive away from home and say, "Oh, shoot, I forgot.

403
00:15:02,528 --> 00:15:04,132
I don't remember if I locked my car door,

404
00:15:04,132 --> 00:15:05,462
and so you pull out your cellphone,

405
00:15:05,462 --> 00:15:07,660
connect to the home automation system over the Internet,

406
00:15:07,660 --> 00:15:09,560
and then, from that point, lock your door,

407
00:15:09,561 --> 00:15:12,429
make sure you turned off your lights, and so on.

408
00:15:12,429 --> 00:15:14,660
The question is, what happens if there might be an adversary

409
00:15:14,660 --> 00:15:16,231
wanting to exploit some properties

410
00:15:16,231 --> 00:15:17,792
of this home automation system?

411
00:15:17,792 --> 00:15:20,066
This is a picture of Tope, one of the PhD students

412
00:15:20,066 --> 00:15:22,759
on this particular project.

413
00:15:22,759 --> 00:15:24,561
It's well known that if we can compromise

414
00:15:24,561 --> 00:15:26,297
the home automation controller,

415
00:15:26,297 --> 00:15:28,495
then the adversary might be able to access things

416
00:15:28,495 --> 00:15:31,231
like control the door locks -- unlock or lock the door --

417
00:15:31,231 --> 00:15:33,066
affect the furnace, affect the dimmer,

418
00:15:33,066 --> 00:15:37,099
the light switches, and so on.

419
00:15:37,099 --> 00:15:39,231
But one of the things we wanted to know is, you know,

420
00:15:39,231 --> 00:15:42,231
how far could we push an adversary's capabilities?

421
00:15:42,231 --> 00:15:43,462
Like, are there other things

422
00:15:43,462 --> 00:15:44,759
that an adversary might be able to do

423
00:15:44,759 --> 00:15:47,858
besides just affecting those devices directly connected

424
00:15:47,858 --> 00:15:51,429
to the home automation system.

425
00:15:51,429 --> 00:15:52,726
And we found out the answer is yes,

426
00:15:52,726 --> 00:15:55,396
there actually can be things an adversary can do.

427
00:15:55,396 --> 00:15:56,891
For example, imagine that someone

428
00:15:56,891 --> 00:15:58,462
has purchased just a regular,

429
00:15:58,462 --> 00:16:01,593
from Home Depot, a CFL light bulb

430
00:16:01,594 --> 00:16:04,462
and plugged it into a light switch controlled by a dimmer.

431
00:16:04,462 --> 00:16:05,924
This is not something someone should do,

432
00:16:05,924 --> 00:16:07,891
because it's known that if you connect a CFL light bulb

433
00:16:07,891 --> 00:16:10,495
to a dimmer, there's a risk of a fire.

434
00:16:10,495 --> 00:16:13,330
Well, let's say that someone has done this.

435
00:16:13,330 --> 00:16:14,825
Well, then we found that an adversary

436
00:16:14,825 --> 00:16:16,858
who can control the dimmer can make it go back and forth,

437
00:16:16,858 --> 00:16:18,593
back and forth at a fast enough rate

438
00:16:18,594 --> 00:16:20,660
to actually cause a CFL light bulb to pop.

439
00:16:20,660 --> 00:16:21,759
And so this is a picture that we have

440
00:16:21,759 --> 00:16:23,132
of one of our light bulbs

441
00:16:23,132 --> 00:16:24,593
in our experiments with a little bit of charring.

442
00:16:27,198 --> 00:16:29,231
The reason I give this example is several-fold.

443
00:16:29,231 --> 00:16:30,891
One, to highlight the fact

444
00:16:30,891 --> 00:16:32,198
that when we're thinking about security

445
00:16:32,198 --> 00:16:35,231
in an IoT environment, we don't need to think about

446
00:16:35,231 --> 00:16:37,264
directly someone just immediately

447
00:16:37,264 --> 00:16:38,693
trying to compromise the car

448
00:16:38,693 --> 00:16:40,165
or the children's toy or so on,

449
00:16:40,165 --> 00:16:41,495
but using these devices

450
00:16:41,495 --> 00:16:43,396
within the environment as stepping stones.

451
00:16:43,396 --> 00:16:45,329
So first compromising the home automation controller

452
00:16:45,330 --> 00:16:48,396
to thereby access the dimmer, to thereby access another device.

453
00:16:48,396 --> 00:16:49,429
Now, you may not particularly care

454
00:16:49,429 --> 00:16:50,726
about this specific example,

455
00:16:50,726 --> 00:16:54,627
but I think the stepping-stones lesson is real.

456
00:16:54,627 --> 00:16:56,693
And, second, I think this is an example

457
00:16:56,693 --> 00:16:58,264
of thinking about the fact

458
00:16:58,264 --> 00:17:00,891
that when we're thinking about security for IoT environment,

459
00:17:00,891 --> 00:17:02,924
we might actually be affecting other devices

460
00:17:02,924 --> 00:17:05,825
that actually might not traditionally be called IoT.

461
00:17:05,825 --> 00:17:08,132
For example, when someone created the CFL light bulb,

462
00:17:08,132 --> 00:17:09,693
they were thinking they were just creating a light bulb

463
00:17:09,693 --> 00:17:11,594
that was gonna be plugged into a light socket.

464
00:17:11,594 --> 00:17:13,363
They never really thought about that light socket

465
00:17:13,363 --> 00:17:15,396
being controlled by an adversary.

466
00:17:15,396 --> 00:17:18,231
Similarly, when someone created this IoT dimmer,

467
00:17:18,231 --> 00:17:19,791
they probably didn't think about what happens

468
00:17:19,791 --> 00:17:24,659
when you connect a CFL light bulb to it.

469
00:17:24,660 --> 00:17:25,792
I'm gonna also use this figure

470
00:17:25,791 --> 00:17:28,693
to talk about the zombie problem.

471
00:17:28,693 --> 00:17:30,297
So, what is a zombie?

472
00:17:30,297 --> 00:17:33,099
If you think about purchasing some of these IoT devices,

473
00:17:33,099 --> 00:17:35,131
such as a furnace or refrigerator,

474
00:17:35,132 --> 00:17:36,693
those devices might live in your home

475
00:17:36,693 --> 00:17:41,132
for 5, 10, 15, 20, 30 years.

476
00:17:41,132 --> 00:17:44,165
But how long will those devices be maintained

477
00:17:44,165 --> 00:17:47,197
and receive security updates by the manufacturer?

478
00:17:47,198 --> 00:17:48,924
Maybe two years? Five years?

479
00:17:48,924 --> 00:17:51,363
Zero years, I see someone saying.

480
00:17:51,363 --> 00:17:54,099
So what we call a zombie is an IoT device

481
00:17:54,099 --> 00:17:55,297
that is living in this environment

482
00:17:55,297 --> 00:17:57,132
that is no longer being maintained,

483
00:17:57,132 --> 00:17:59,000
no longer receiving security patches.

484
00:17:59,000 --> 00:18:00,429
So I think it's a big challenge to think

485
00:18:00,429 --> 00:18:03,132
about what are we gonna do with these zombies

486
00:18:03,132 --> 00:18:05,924
for the remaining 28 years that they remain in service?

487
00:18:08,066 --> 00:18:09,561
And, finally, I want to highlight the fact

488
00:18:09,561 --> 00:18:12,693
that in an IoT environment, there is an uncertain future.

489
00:18:12,693 --> 00:18:16,099
We don't necessarily know what types of IoT devices

490
00:18:16,099 --> 00:18:18,956
will exist in the world in 5, 10, or 15 years.

491
00:18:18,957 --> 00:18:21,792
And so how do we start thinking about security for IoT now?

492
00:18:24,264 --> 00:18:25,726
I want to close with two things.

493
00:18:25,726 --> 00:18:27,693
First, I want to thank all the wonderful collaborators,

494
00:18:27,693 --> 00:18:29,198
some of whom I see in this audience,

495
00:18:29,198 --> 00:18:30,363
on these projects.

496
00:18:30,363 --> 00:18:31,561
And I know you'll hear from Stefan next

497
00:18:31,561 --> 00:18:34,264
about computer security for automobiles

498
00:18:34,264 --> 00:18:37,197
and a little bit more about the timeline there.

499
00:18:37,198 --> 00:18:38,792
Then close with this final slide.

500
00:18:38,792 --> 00:18:40,924
Again, I hope that this talk has encouraged you

501
00:18:40,924 --> 00:18:42,561
to think broadly about what computer security

502
00:18:42,561 --> 00:18:45,428
and privacy might be in an IoT environment.

503
00:18:45,429 --> 00:18:46,759
I've given you some examples.

504
00:18:46,759 --> 00:18:48,660
Of course, many, many other examples are out there,

505
00:18:48,660 --> 00:18:49,825
and if you pay attention to the news,

506
00:18:49,825 --> 00:18:52,231
I'm sure that within the next day or two days,

507
00:18:52,231 --> 00:18:55,165
you'll begin to see more and more examples of IoT issues.

508
00:18:55,165 --> 00:18:56,462
So, with that, I think I'll close.

509
00:18:56,462 --> 00:18:55,462


