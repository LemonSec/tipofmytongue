1
00:00:00,198 --> 00:00:02,264


2
00:00:02,264 --> 00:00:04,923
♪♪

3
00:00:12,363 --> 00:00:14,825
Why do you work in security?

4
00:00:14,825 --> 00:00:16,825
Why are you here today?

5
00:00:16,825 --> 00:00:18,330
I work in security

6
00:00:18,330 --> 00:00:21,264
because I like the idea of helping people.

7
00:00:21,264 --> 00:00:24,264
I think that, at its core, privacy and security

8
00:00:24,264 --> 00:00:28,726
are about helping people get control over technology.

9
00:00:28,726 --> 00:00:31,396
I have privilege and you have privilege

10
00:00:31,396 --> 00:00:33,528
because we're security experts.

11
00:00:33,528 --> 00:00:34,759
We know how to tell the difference

12
00:00:34,759 --> 00:00:37,693
between HTTP and HTTPS.

13
00:00:37,693 --> 00:00:39,924
We know how to tell where a file came from

14
00:00:39,924 --> 00:00:42,363
and that it's the right file.

15
00:00:42,363 --> 00:00:45,363
We know how to avoid being scammed.

16
00:00:45,363 --> 00:00:48,891
And we have that privilege because we are security experts.

17
00:00:48,891 --> 00:00:52,065
But not everyone can be a security expert.

18
00:00:52,066 --> 00:00:55,066
Doctors are busy helping their patients,

19
00:00:55,066 --> 00:00:57,561
parents are busy raising good kids,

20
00:00:57,561 --> 00:00:59,330
and artists are busy making the world

21
00:00:59,330 --> 00:01:01,065
a more beautiful place.

22
00:01:01,066 --> 00:01:03,231
But I still think that they deserve to have

23
00:01:03,231 --> 00:01:07,759
the same level of control over privacy and security settings

24
00:01:07,759 --> 00:01:11,396
and features that we in this room do.

25
00:01:11,396 --> 00:01:14,330
This is why I work on usable security for end users

26
00:01:14,330 --> 00:01:17,000
and why I think you should, too.

27
00:01:17,000 --> 00:01:19,627
Now, what is usable security for end users?

28
00:01:19,627 --> 00:01:22,890
There's many ways to define it, but, in my opinion,

29
00:01:22,891 --> 00:01:25,858
a security feature that a user has to interact with

30
00:01:25,858 --> 00:01:28,792
should be invisible when you don't need it

31
00:01:28,792 --> 00:01:33,726
and helpful, even to nonexperts, when you do.

32
00:01:33,726 --> 00:01:37,033
This is easier for some classes of security features.

33
00:01:37,033 --> 00:01:40,627
For example, if you're isolating

34
00:01:40,627 --> 00:01:42,231
different principles and different processes,

35
00:01:42,231 --> 00:01:44,824
that's easy, to always be invisible.

36
00:01:44,825 --> 00:01:46,132
But if you're working on something

37
00:01:46,132 --> 00:01:49,000
that has the user in the loop, like authentication,

38
00:01:49,000 --> 00:01:52,627
or anything with an exception or an alert,

39
00:01:52,627 --> 00:01:54,792
I believe this should hold true.

40
00:01:54,792 --> 00:01:56,495
But this is really difficult.

41
00:01:56,495 --> 00:01:58,594
This is really hard.

42
00:01:58,594 --> 00:02:00,693
To pick on my own work, for a minute,

43
00:02:00,693 --> 00:02:02,164
and the work that my team is doing,

44
00:02:02,165 --> 00:02:03,495
here are some recent tweets

45
00:02:03,495 --> 00:02:06,264
that I've seen about Chrome's HTTPS warning.

46
00:02:14,066 --> 00:02:16,825
Both of these people had the same problem.

47
00:02:16,825 --> 00:02:18,329
What's going on here is that they were trying

48
00:02:18,330 --> 00:02:22,396
to load a web page over HTTPS and something went wrong:

49
00:02:22,396 --> 00:02:24,791
the certificate chain failed to validate.

50
00:02:24,792 --> 00:02:26,660
And so, instead of being able to get to the website

51
00:02:26,660 --> 00:02:28,363
that they were trying to go to,

52
00:02:28,363 --> 00:02:31,000
they got an error message from Chrome, instead.

53
00:02:31,000 --> 00:02:33,594
This is not a good user experience.

54
00:02:33,594 --> 00:02:36,759
It's not helpful. It's not empowering.

55
00:02:36,759 --> 00:02:38,759
And honestly, in this situation,

56
00:02:38,759 --> 00:02:40,626
it wasn't even really protecting them from anything

57
00:02:40,627 --> 00:02:44,000
because there probably wasn't a real attack here.

58
00:02:44,000 --> 00:02:45,462
Now, I'm picking on myself here,

59
00:02:45,462 --> 00:02:47,660
but this is not just a Chrome problem;

60
00:02:47,660 --> 00:02:50,891
it's a problem that we see across the industry.

61
00:02:50,891 --> 00:02:54,594
We see these types of problems in other products and areas.

62
00:02:54,594 --> 00:02:57,626
People have trouble with the unlock on their phone;

63
00:02:57,627 --> 00:02:59,825
people have trouble with authentication;

64
00:02:59,825 --> 00:03:03,363
phishing is still a big problem, et cetera.

65
00:03:03,363 --> 00:03:05,792
Why is usable security so hard?

66
00:03:05,792 --> 00:03:08,495
Why are we still having difficulty with this idea

67
00:03:08,495 --> 00:03:11,066
of invisible but helpful?, which I think a lot of people

68
00:03:11,066 --> 00:03:15,132
agree is what usable security should be.

69
00:03:15,132 --> 00:03:17,198
Now, this is a very complex problem

70
00:03:17,198 --> 00:03:20,132
and I don't want to diminish its complexity

71
00:03:20,132 --> 00:03:21,792
by giving a glib response.

72
00:03:21,792 --> 00:03:24,000
But today I'm gonna talk about one thing

73
00:03:24,000 --> 00:03:27,264
that comes to mind when I think about this problem,

74
00:03:27,264 --> 00:03:30,198
which is that usable security is a science,

75
00:03:30,198 --> 00:03:34,693
but not enough people treat it like a science.

76
00:03:34,693 --> 00:03:37,495
At Google, a motley crew of people --

77
00:03:37,495 --> 00:03:40,660
a mix of engineers, designers, researchers --

78
00:03:40,660 --> 00:03:43,858
have been treating usable security like a science.

79
00:03:43,858 --> 00:03:46,792
And, today, I'm gonna tell you three different stories

80
00:03:46,792 --> 00:03:50,957
about how we used a scientific approach to security

81
00:03:50,957 --> 00:03:54,692
to tackle really hard, usable security problems in Chrome.

82
00:03:54,693 --> 00:03:57,561
Two of these stories are gonna be about successes

83
00:03:57,561 --> 00:04:00,594
and one of the stories is gonna be about a failure,

84
00:04:00,594 --> 00:04:04,957
but I hope that we can learn from all three of them.

85
00:04:04,957 --> 00:04:06,924
IF you don't already believe me,

86
00:04:06,924 --> 00:04:08,825
that usable security is a science,

87
00:04:08,825 --> 00:04:10,528
I'm hoping that these three stories

88
00:04:10,528 --> 00:04:13,594
will convince you that it is.

89
00:04:13,594 --> 00:04:16,099
If you already agree with me,

90
00:04:16,099 --> 00:04:17,462
I hope that you can use these stories

91
00:04:17,462 --> 00:04:19,231
as talking points to bring back

92
00:04:19,231 --> 00:04:21,692
to help convince other people who you work with,

93
00:04:21,692 --> 00:04:24,858
other engineers, other security experts.

94
00:04:24,858 --> 00:04:28,099
And, if you work in academia, I assume you probably

95
00:04:28,099 --> 00:04:31,330
already do science and maybe even science on usable security.

96
00:04:31,330 --> 00:04:33,066
I want to illustrate some of the gap

97
00:04:33,066 --> 00:04:36,296
between the science that we find useful in industry

98
00:04:36,297 --> 00:04:40,495
and what we're currently getting from academic research.

99
00:04:40,495 --> 00:04:45,726
Now, what do I mean by treating usable security like science?

100
00:04:45,726 --> 00:04:50,297
Well, first, I mean look for specific research.

101
00:04:50,297 --> 00:04:51,825
Look to see if there's existing research --

102
00:04:51,825 --> 00:04:54,527
at SOUPS, at CHI, at USENIX Security --

103
00:04:54,528 --> 00:04:56,132
that could guide

104
00:04:56,132 --> 00:04:59,561
how you're building a feature that puts the user in the loop.

105
00:04:59,561 --> 00:05:01,758
But if you can't find this research --

106
00:05:01,759 --> 00:05:04,693
which, honestly, we've found you might not be able to --

107
00:05:04,693 --> 00:05:07,099
you need to run your own experiments.

108
00:05:07,099 --> 00:05:10,296
You need to have defined controls and conditions

109
00:05:10,297 --> 00:05:13,495
and hypotheses that you can test.

110
00:05:13,495 --> 00:05:15,627
You need to take things that are nebulous,

111
00:05:15,627 --> 00:05:18,000
like the idea of users understanding a warning,

112
00:05:18,000 --> 00:05:22,495
and turn them into something that's concrete and testable.

113
00:05:22,495 --> 00:05:25,297
And there's also a few things that you should not do.

114
00:05:25,297 --> 00:05:28,264
you should not trust your gut.

115
00:05:28,264 --> 00:05:29,825
Your gut, my gut --

116
00:05:29,825 --> 00:05:32,462
we are not representative of regular users.

117
00:05:32,462 --> 00:05:33,957
Things that seem understandable to us

118
00:05:33,957 --> 00:05:36,594
or convincing to us will not generalize.

119
00:05:36,594 --> 00:05:39,296
And similarly, you should not trust common wisdom,

120
00:05:39,297 --> 00:05:41,858
unless it's backed by data and research.

121
00:05:44,495 --> 00:05:47,165
The first story that I'm going to tell you about

122
00:05:47,165 --> 00:05:50,396
started about a year and a half ago.

123
00:05:50,396 --> 00:05:53,429
An engineer came to me and started telling me

124
00:05:53,429 --> 00:05:59,792
about the problem of wanting to bring rich APIs to the web.

125
00:05:59,792 --> 00:06:02,066
So, currently, we see very rich APIs

126
00:06:02,066 --> 00:06:05,494
on platforms like Android and iOS,

127
00:06:05,495 --> 00:06:09,561
where apps can do lots of things that websites currently can't,

128
00:06:09,561 --> 00:06:12,066
but, in many cases, web developers want to.

129
00:06:12,066 --> 00:06:14,231
Take notifications, for example.

130
00:06:14,231 --> 00:06:17,759
Until fairly recently, websites were not able

131
00:06:17,759 --> 00:06:20,099
to send notifications, whereas an app could.

132
00:06:20,099 --> 00:06:22,957
So, if you were Gmail, you could build a Gmail app

133
00:06:22,957 --> 00:06:25,066
and the Gmail app, even when the users closed it,

134
00:06:25,066 --> 00:06:27,924
can still notify you about an incoming email message.

135
00:06:27,924 --> 00:06:30,132
But websites weren't able to do this.

136
00:06:30,132 --> 00:06:33,297
So, this is an example of one of these new APIs.

137
00:06:33,297 --> 00:06:36,099
And what do you think about that idea?

138
00:06:36,099 --> 00:06:38,693
Different nerds have differences of opinion

139
00:06:38,693 --> 00:06:41,462
over how cool this type of new API is.

140
00:06:41,462 --> 00:06:44,825
If you're a web developer, it probably seems pretty cool:

141
00:06:44,825 --> 00:06:47,264
you can drive user engagement back to your website;

142
00:06:47,264 --> 00:06:49,099
you can provide features don't require

143
00:06:49,099 --> 00:06:52,627
than to be looking at your website at that moment.

144
00:06:52,627 --> 00:06:55,396
So, from that perspective, it's great.

145
00:06:55,396 --> 00:06:58,396
But if, strictly hypothetically speaking,

146
00:06:58,396 --> 00:07:01,297
you are a paranoid security person --

147
00:07:01,297 --> 00:07:03,132
purely hypothetically, I'm sure that doesn't apply

148
00:07:03,132 --> 00:07:05,099
to anyone here --

149
00:07:05,099 --> 00:07:09,264
you see a lot of things to be afraid of in this setup.

150
00:07:09,264 --> 00:07:10,858
So, just looking at notifications,

151
00:07:10,858 --> 00:07:13,891
specifically, there's the concern

152
00:07:13,891 --> 00:07:16,726
that they'll be spammy and obnoxious.

153
00:07:16,726 --> 00:07:18,462
There's the concern that they could be used

154
00:07:18,462 --> 00:07:20,527
for better phishing attacks.

155
00:07:20,528 --> 00:07:24,297
There's the concern that it just sort of might confuse people

156
00:07:24,297 --> 00:07:26,231
about their relationship with a website,

157
00:07:26,231 --> 00:07:30,296
like "I closed it. Why is it opening again?"

158
00:07:30,297 --> 00:07:32,396
And one of the ways that comes to mind

159
00:07:32,396 --> 00:07:36,264
to fix this fear is the idea of having a permission request.

160
00:07:36,264 --> 00:07:39,000
So, what you can see here is an example

161
00:07:39,000 --> 00:07:40,792
of a permission request where Facebook is asking

162
00:07:40,792 --> 00:07:43,594
for access to the new notifications API

163
00:07:43,594 --> 00:07:46,000
and it tells the user what website it is --

164
00:07:46,000 --> 00:07:48,660
it's Facebook; it tells you what it's requesting,

165
00:07:48,660 --> 00:07:50,099
which is notifications;

166
00:07:50,099 --> 00:07:53,759
and then, you have a choice between allowing or blocking.

167
00:07:53,759 --> 00:07:56,297
Okay, so we've kind of solved this first problem.

168
00:07:56,297 --> 00:07:58,165
We've given users control over a website

169
00:07:58,165 --> 00:08:01,000
can actually do this potentially scary thing or not,

170
00:08:01,000 --> 00:08:03,132
but, now, we have a new concern,

171
00:08:03,132 --> 00:08:06,726
which is the fear of annoying users.

172
00:08:06,726 --> 00:08:09,330
And we've probably seen, from our own experiences,

173
00:08:09,330 --> 00:08:11,264
that like I don't really like it when stuff pops up

174
00:08:11,264 --> 00:08:12,792
on the bottom of the screen when I'm trying

175
00:08:12,792 --> 00:08:15,857
to use a website; that's kind of obnoxious.

176
00:08:15,858 --> 00:08:17,924
And there's also a good amount of prior literature

177
00:08:17,924 --> 00:08:21,000
telling us about a phenomenon called habituation,

178
00:08:21,000 --> 00:08:24,231
which is that when someone experiences the same stimuli

179
00:08:24,231 --> 00:08:25,924
over and over again,

180
00:08:25,924 --> 00:08:30,231
they pay less and less attention to each subsequent iteration.

181
00:08:30,231 --> 00:08:33,198
So this prior literature is telling us

182
00:08:33,198 --> 00:08:35,593
that there might be a negative side effect

183
00:08:35,594 --> 00:08:38,792
of asking people these questions over and over again.

184
00:08:38,792 --> 00:08:40,759
But it's not entirely clear.

185
00:08:40,759 --> 00:08:43,099
What does that actually mean in practice?

186
00:08:46,099 --> 00:08:49,693
How many permission requests is too many permission requests?

187
00:08:49,693 --> 00:08:50,957
What are the costs?

188
00:08:50,957 --> 00:08:53,132
Like are people gonna stop using Chrome,

189
00:08:53,132 --> 00:08:54,957
are people gonna stop using their phones,

190
00:08:54,957 --> 00:08:56,495
or are people just gonna be moderately annoyed

191
00:08:56,495 --> 00:08:59,363
for a split second before they move on?

192
00:08:59,363 --> 00:09:02,066
And does that prior literature even apply here, right?

193
00:09:02,066 --> 00:09:04,462
It's hard to say, because prior literature focuses

194
00:09:04,462 --> 00:09:07,495
on full-screen warnings and showing people them

195
00:09:07,495 --> 00:09:09,957
over and over again in a really short period of time,

196
00:09:09,957 --> 00:09:12,561
but in our actual scenario, people are seeing warnings

197
00:09:12,561 --> 00:09:15,396
over a long period of time.

198
00:09:15,396 --> 00:09:17,363
We're, luckily, in a good position

199
00:09:17,363 --> 00:09:19,000
to be able to actually test this,

200
00:09:19,000 --> 00:09:20,396
to answer some of these questions,

201
00:09:20,396 --> 00:09:24,824
although not all of them, but we can answer some of them.

202
00:09:24,825 --> 00:09:27,924
What this is showing you here is that we've taken all the users,

203
00:09:27,924 --> 00:09:30,066
in aggregate, on one specific day,

204
00:09:30,066 --> 00:09:31,528
and saw how they responded

205
00:09:31,528 --> 00:09:34,528
to a notification permission request.

206
00:09:34,528 --> 00:09:37,330
And then we looked back over the previous three months

207
00:09:37,330 --> 00:09:39,462
to see how many prior requests they'd seen

208
00:09:39,462 --> 00:09:42,098
in the previous three months.

209
00:09:42,099 --> 00:09:44,561
And what you can see here is that for people at the zero --

210
00:09:44,561 --> 00:09:46,759
so people who have never seen a permission request, at least,

211
00:09:46,759 --> 00:09:49,957
not in the past three months -- there's an almost 40% chance

212
00:09:49,957 --> 00:09:52,924
that they'll grant the permission request.

213
00:09:52,924 --> 00:09:57,132
But, by the time you get down to 5 or 6 permission requests

214
00:09:57,132 --> 00:09:58,858
over the last 3 months,

215
00:09:58,858 --> 00:10:03,363
the acceptance rate has dropped to about 10%.

216
00:10:03,363 --> 00:10:06,594
What this is telling us is that there is a very real cost

217
00:10:06,594 --> 00:10:08,330
of websites requesting permissions,

218
00:10:08,330 --> 00:10:10,264
perhaps unnecessarily, spuriously,

219
00:10:10,264 --> 00:10:12,627
at bad times, for bad reasons,

220
00:10:12,627 --> 00:10:16,132
because people are less and less likely to grant the permission

221
00:10:16,132 --> 00:10:18,693
as they see more and more permission requests.

222
00:10:18,693 --> 00:10:21,726
And this holds true across different types of APIs,

223
00:10:21,726 --> 00:10:23,528
not just notifications.

224
00:10:23,528 --> 00:10:25,924
We see the same thing happening with geolocation,

225
00:10:25,924 --> 00:10:29,198
where, as people see more requests over time,

226
00:10:29,198 --> 00:10:31,329
the likelihood that they'll accept drops

227
00:10:31,330 --> 00:10:34,528
from a little over 25% to well below 10%.

228
00:10:36,858 --> 00:10:38,791
In this situation, the common wisdom

229
00:10:38,792 --> 00:10:41,693
and our gut feeling that lots of permission requests

230
00:10:41,693 --> 00:10:46,165
were annoying and cause people to pay less attention to them

231
00:10:46,165 --> 00:10:47,296
turned out to be true.

232
00:10:47,297 --> 00:10:50,000
We substantiated that idea.

233
00:10:50,000 --> 00:10:52,165
But, if we hadn't actually tested this,

234
00:10:52,165 --> 00:10:53,824
if we hadn't actually looked at it,

235
00:10:53,825 --> 00:10:56,561
we wouldn't have known how rapidly that occurs,

236
00:10:56,561 --> 00:10:58,858
and this is information that we're now able to use

237
00:10:58,858 --> 00:11:01,263
as input to our engineering projects,

238
00:11:01,264 --> 00:11:04,429
to try to decrease the rate at which people see spurious

239
00:11:04,429 --> 00:11:06,264
or unnecessary permission requests,

240
00:11:06,264 --> 00:11:07,759
so we aren't spamming people with things

241
00:11:07,759 --> 00:11:10,726
that less than 10% of people are going to say yes to.

242
00:11:13,858 --> 00:11:16,891
The next story takes place in the land of malware warnings.

243
00:11:16,891 --> 00:11:20,363
Chrome relies on a service called Safe Browsing.

244
00:11:20,363 --> 00:11:23,627
Safe Browsing tells Chrome when a website is known

245
00:11:23,627 --> 00:11:26,627
for being malicious or deceptive,

246
00:11:26,627 --> 00:11:29,000
and, when that's the case, we show one of these warnings

247
00:11:29,000 --> 00:11:31,264
about the website.

248
00:11:31,264 --> 00:11:32,924
The warning tries to convince people

249
00:11:32,924 --> 00:11:35,462
not to proceed forward to the website.

250
00:11:35,462 --> 00:11:37,000
Safe Browsing is very accurate,

251
00:11:37,000 --> 00:11:39,693
so we know with fairly high likelihood

252
00:11:39,693 --> 00:11:41,033
that the user is gonna be at risk

253
00:11:41,033 --> 00:11:43,396
if the proceed through the website.

254
00:11:43,396 --> 00:11:46,032
We don't just block people. We could, right?

255
00:11:46,033 --> 00:11:48,627
We could make it so people can't proceed through the website,

256
00:11:48,627 --> 00:11:50,593
but we still want to give people the choice.

257
00:11:50,594 --> 00:11:53,957
However, we want to convince them to adhere to our message

258
00:11:53,957 --> 00:11:56,495
that you should actually turn back and do something else,

259
00:11:56,495 --> 00:11:59,330
instead of visiting the malicious website.

260
00:11:59,330 --> 00:12:03,330
About 2 years ago, the adherence rate was only about 70%,

261
00:12:03,330 --> 00:12:05,627
and we wanted it to be closer to 90%.

262
00:12:05,627 --> 00:12:07,429
A higher adherence rate, in this case, is better,

263
00:12:07,429 --> 00:12:12,891
because that means more people heeded our advice to go back.

264
00:12:12,891 --> 00:12:15,198
So, we wanted to test a few different things

265
00:12:15,198 --> 00:12:18,363
to see what the best way is to raise the adherence rate.

266
00:12:18,363 --> 00:12:19,891
And, today, I'm gonna talk about some experiments

267
00:12:19,891 --> 00:12:21,198
with the headline text --

268
00:12:21,198 --> 00:12:24,396
the headline text is the larger text on the top --

269
00:12:24,396 --> 00:12:27,098
and also experiments with the malware download warning,

270
00:12:27,099 --> 00:12:28,627
which occurs at the bottom of Chrome,

271
00:12:28,627 --> 00:12:29,924
when you've downloaded a warning.

272
00:12:29,924 --> 00:12:32,495
I'm sorry, when you've downloaded a malicious file.

273
00:12:34,726 --> 00:12:37,000
We started off with a literature review.

274
00:12:37,000 --> 00:12:39,594
We went to look to see what are the main recommendations

275
00:12:39,594 --> 00:12:41,066
for what you should do

276
00:12:41,066 --> 00:12:44,462
if you want to increase adherence to a warning.

277
00:12:44,462 --> 00:12:47,231
Some of the literature recommended really useful things

278
00:12:47,231 --> 00:12:49,198
like, for example, you should aim

279
00:12:49,198 --> 00:12:51,759
for a sixth-grade reading level.

280
00:12:51,759 --> 00:12:53,165
This is really great advice

281
00:12:53,165 --> 00:12:56,198
because it's extremely specific and it's measurable.

282
00:12:56,198 --> 00:12:59,660
We were able to take our text, we were able to reduce

283
00:12:59,660 --> 00:13:03,329
the warning level, using a tool, and then run with that.

284
00:13:03,330 --> 00:13:06,165
But other pieces of advice were a little harder to follow.

285
00:13:06,165 --> 00:13:09,759
For example, on one hand, be as brief as possible;

286
00:13:09,759 --> 00:13:11,099
and, on the other hand,

287
00:13:11,099 --> 00:13:15,033
be extremely specific about the risk.

288
00:13:15,033 --> 00:13:16,792
How do you do both at once?

289
00:13:16,792 --> 00:13:18,627
How brief is brief enough?

290
00:13:18,627 --> 00:13:21,824
How specific is specific?

291
00:13:21,825 --> 00:13:23,330
It was kind of difficult for us to follow,

292
00:13:23,330 --> 00:13:26,627
but, nonetheless, we still tried.

293
00:13:26,627 --> 00:13:29,098
We ran a series of A/B tests,

294
00:13:29,099 --> 00:13:31,561
also known as randomized control trials,

295
00:13:31,561 --> 00:13:33,231
to test out some of the different things

296
00:13:33,231 --> 00:13:34,693
that we gleaned from literature

297
00:13:34,693 --> 00:13:38,693
in an attempt to increase the adherence rates.

298
00:13:38,693 --> 00:13:41,396
Here are two examples of experiments.

299
00:13:41,396 --> 00:13:44,791
In these specific experiments, what we're testing is the idea

300
00:13:44,792 --> 00:13:48,066
that we should give people a command to follow our advice.

301
00:13:48,066 --> 00:13:49,396
So, for example, at the top,

302
00:13:49,396 --> 00:13:52,429
the control is "The website ahead contains malware!"

303
00:13:52,429 --> 00:13:54,924
It's a control because it's the existing text.

304
00:13:54,924 --> 00:13:57,924
And then we altered it slightly, so that the first condition

305
00:13:57,924 --> 00:14:00,462
adds "Go back" to the end of it.

306
00:14:00,462 --> 00:14:02,429
We did the same in the second set of experiments.

307
00:14:02,429 --> 00:14:04,891
We added commands like "Discard this file.

308
00:14:04,891 --> 00:14:08,660
Don't run it," to that control.

309
00:14:08,660 --> 00:14:12,329
And so, here, each set of experiments has a hypothesis,

310
00:14:12,330 --> 00:14:14,132
which is that the alternate condition

311
00:14:14,132 --> 00:14:17,495
will do better than the control, and we randomly assigned

312
00:14:17,495 --> 00:14:20,396
small groups of Chrome users into each of these buckets.

313
00:14:20,396 --> 00:14:22,098
And then, over time, we could look

314
00:14:22,099 --> 00:14:23,825
at their adherence rates for the people who fell

315
00:14:23,825 --> 00:14:26,825
in the different buckets and compare them to each other.

316
00:14:26,825 --> 00:14:29,792
I'm emphasizing the fact that we had a control

317
00:14:29,792 --> 00:14:31,528
and we had various conditions

318
00:14:31,528 --> 00:14:33,825
and we had random assignment, because these are key parts

319
00:14:33,825 --> 00:14:37,957
of scientifically running an A/B test.

320
00:14:37,957 --> 00:14:39,264
Unfortunately,

321
00:14:39,264 --> 00:14:42,330
these two particular experiments had negative results.

322
00:14:42,330 --> 00:14:44,066
We found that adding the command

323
00:14:44,066 --> 00:14:46,264
didn't actually make much of a difference.

324
00:14:46,264 --> 00:14:47,693
There are some small differences

325
00:14:47,693 --> 00:14:49,759
between the different conditions here,

326
00:14:49,759 --> 00:14:52,297
but they fall within the normal fluctuations

327
00:14:52,297 --> 00:14:56,594
that we see on a daily basis in this type of warning.

328
00:14:56,594 --> 00:15:00,066
I know that negative results aren't superexciting,

329
00:15:00,066 --> 00:15:02,165
but the reason I'm sharing them with you today

330
00:15:02,165 --> 00:15:03,858
is because I still think we learned a lot

331
00:15:03,858 --> 00:15:06,032
from this experiment.

332
00:15:06,033 --> 00:15:08,462
What we learned is that, even if you do your best

333
00:15:08,462 --> 00:15:12,462
to follow previous literature and even if, in your gut,

334
00:15:12,462 --> 00:15:14,593
you think "Oh, yeah! This is totally gonna work,"

335
00:15:14,594 --> 00:15:18,924
it might actually not work and you don't know that

336
00:15:18,924 --> 00:15:23,000
until you've run the experiment to see whether it works or not.

337
00:15:23,000 --> 00:15:24,957
If we had just gone ahead and made this change,

338
00:15:24,957 --> 00:15:27,891
added "Go back" and been like "Yeah! Mission accomplished"

339
00:15:27,891 --> 00:15:29,429
and rolled it out,

340
00:15:29,429 --> 00:15:31,297
well, a few months later, we would've been surprised

341
00:15:31,297 --> 00:15:34,594
when we discovered it didn't actually help.

342
00:15:34,594 --> 00:15:36,693
Instead, because we ran the experiment

343
00:15:36,693 --> 00:15:39,066
and because we got to find out that it didn't work,

344
00:15:39,066 --> 00:15:40,957
we were able to learn

345
00:15:40,957 --> 00:15:43,066
and, instead, we tried out other things.

346
00:15:43,066 --> 00:15:45,033
We tried changing the background color of the warning;

347
00:15:45,033 --> 00:15:48,693
we tried changing the icons; we tried changing the layout.

348
00:15:48,693 --> 00:15:50,924
And, in the end, it turned out that changing the layout

349
00:15:50,924 --> 00:15:53,098
did have a positive effect on adherence

350
00:15:53,099 --> 00:15:55,429
and if you'd like to learn more about how that worked,

351
00:15:55,429 --> 00:15:57,759
you can read our CHI paper from last year.

352
00:16:00,627 --> 00:16:03,858
For the third story, we're gonna talk briefly

353
00:16:03,858 --> 00:16:05,396
about internationalization.

354
00:16:08,495 --> 00:16:12,726
Traditionally, historically, most previous research

355
00:16:12,726 --> 00:16:15,759
on HTTPS warnings, like these up here

356
00:16:15,759 --> 00:16:17,957
and like the malware warnings I was just talking about,

357
00:16:17,957 --> 00:16:19,593
has focused specifically

358
00:16:19,594 --> 00:16:24,132
on English-speaking Americans from college towns.

359
00:16:24,132 --> 00:16:26,726
There have been other studies on, for example,

360
00:16:26,726 --> 00:16:28,957
some Canadians in college towns,

361
00:16:28,957 --> 00:16:32,363
and also some that have used Mechanical Turk

362
00:16:32,363 --> 00:16:34,000
to look more broadly at Americans,

363
00:16:34,000 --> 00:16:36,066
but, in general, it is still centered

364
00:16:36,066 --> 00:16:40,363
on English-speaking North Americans.

365
00:16:40,363 --> 00:16:42,693
And I can understand why, for practical reasons,

366
00:16:42,693 --> 00:16:46,924
it's much easier to run a study with the people around you.

367
00:16:46,924 --> 00:16:50,693
Most of my prior work has looked this way, too.

368
00:16:50,693 --> 00:16:53,462
And this work has yielded positive results.

369
00:16:53,462 --> 00:16:55,033
It steered us away --

370
00:16:55,033 --> 00:16:57,528
"us" as in the broader security community --

371
00:16:57,528 --> 00:17:00,297
it steered us away from using passive indicators

372
00:17:00,297 --> 00:17:02,429
that were really small and kind of just in the corner

373
00:17:02,429 --> 00:17:05,660
of the browser, to instead prefer these larger,

374
00:17:05,660 --> 00:17:08,858
full-page warnings that do, in fact, work much better.

375
00:17:08,858 --> 00:17:10,560
But we still see a real gap.

376
00:17:10,560 --> 00:17:13,362
On one hand, studying

377
00:17:13,363 --> 00:17:15,594
people in North America who speak English,

378
00:17:15,594 --> 00:17:18,891
and then, on the other hand, browsers serve global audiences.

379
00:17:21,462 --> 00:17:25,098
So we decided to take a look at this

380
00:17:25,098 --> 00:17:30,495
and see what regional differences we might find.

381
00:17:30,495 --> 00:17:31,824
We were able to slice

382
00:17:31,825 --> 00:17:35,099
the malware warning adherence rates by country.

383
00:17:35,099 --> 00:17:36,593
And one thing I want to point out, first,

384
00:17:36,594 --> 00:17:39,066
is that we're looking at Android and Windows

385
00:17:39,066 --> 00:17:40,561
separately here, and this is important

386
00:17:40,561 --> 00:17:42,824
because the type of device you use,

387
00:17:42,825 --> 00:17:45,066
your operating system, is actually a confound here,

388
00:17:45,066 --> 00:17:46,924
because we see that people on Android

389
00:17:46,924 --> 00:17:48,858
adhere to warnings consistently

390
00:17:48,858 --> 00:17:50,197
at a higher rate than Windows users,

391
00:17:50,198 --> 00:17:52,528
so we have to look at them separately.

392
00:17:52,528 --> 00:17:55,330
But our hypothesis here was

393
00:17:55,330 --> 00:17:58,627
that warning adherence rate would differ across countries,

394
00:17:58,627 --> 00:18:00,726
and it does turn out to be true; it does differ.

395
00:18:00,726 --> 00:18:02,462
In fact, you'll notice that we have a problem

396
00:18:02,462 --> 00:18:05,297
specifically in Japan, where Japanese users

397
00:18:05,297 --> 00:18:08,693
are not adhering to warnings, at least on Windows,

398
00:18:08,693 --> 00:18:11,561
at the same rate as other countries are.

399
00:18:11,561 --> 00:18:14,396
We took a look at the same thing for HTTPS error warnings

400
00:18:14,396 --> 00:18:17,297
and, again, we do see differences between countries,

401
00:18:17,297 --> 00:18:19,825
with Turkey having the highest adherence rates and,

402
00:18:19,825 --> 00:18:25,000
again, Japan on Windows having the lowest adherence rates.

403
00:18:25,000 --> 00:18:26,726
Why Turkey and Japan?

404
00:18:26,726 --> 00:18:30,594
I honestly don't know and I actually would love it

405
00:18:30,594 --> 00:18:32,561
if people in this room could help figure out why.

406
00:18:32,561 --> 00:18:34,396
If you speak another language or love to travel,

407
00:18:34,396 --> 00:18:36,363
I think this would be a great area to look at,

408
00:18:36,363 --> 00:18:38,264
an important area to look at.

409
00:18:38,264 --> 00:18:40,330
For example, I think that, looking at Turkey,

410
00:18:40,330 --> 00:18:43,066
we could learn maybe there's something

411
00:18:43,066 --> 00:18:44,561
in the way it's translated

412
00:18:44,561 --> 00:18:46,428
or something in how they're perceiving the warnings.

413
00:18:46,429 --> 00:18:48,462
Maybe we could learn from that and translate it

414
00:18:48,462 --> 00:18:50,330
to help the rest of the world.

415
00:18:50,330 --> 00:18:51,759
And also, if we look at Japan,

416
00:18:51,759 --> 00:18:53,297
maybe we'll find specific problems there

417
00:18:53,297 --> 00:18:56,033
that could improve and bring Japan up

418
00:18:56,033 --> 00:19:00,264
to the rest of the world, in terms of adherence rates.

419
00:19:00,264 --> 00:19:02,561
The reason why I told this last story

420
00:19:02,561 --> 00:19:05,593
was because I again wanted to highlight the importance

421
00:19:05,594 --> 00:19:08,033
of how your results can be surprising

422
00:19:08,033 --> 00:19:10,264
once you actually dig into data.

423
00:19:10,264 --> 00:19:12,264
If you had asked me before we started looking

424
00:19:12,264 --> 00:19:13,824
at statistics sliced by country,

425
00:19:13,825 --> 00:19:17,066
I would've guessed, "Well, based on stereotypes,

426
00:19:17,066 --> 00:19:18,891
Germans are very privacy-sensitive,

427
00:19:18,891 --> 00:19:21,660
so I would expect higher adherence rates for Germany,

428
00:19:21,660 --> 00:19:23,726
and that's not actually what we ended up finding,

429
00:19:23,726 --> 00:19:25,726
so I'm very glad that we actually took the time

430
00:19:25,726 --> 00:19:27,264
to dig into this data.

431
00:19:29,858 --> 00:19:33,264
Usable security is science.

432
00:19:33,264 --> 00:19:37,726
At Google, we test ideas with clearly defined hypotheses,

433
00:19:37,726 --> 00:19:40,561
with experiments, and we use the results

434
00:19:40,561 --> 00:19:42,659
to determine what we should launch.

435
00:19:42,660 --> 00:19:44,726
We use experiments to figure out

436
00:19:44,726 --> 00:19:46,231
how to settle technical arguments

437
00:19:46,231 --> 00:19:49,198
about the right way to build a feature

438
00:19:49,198 --> 00:19:51,627
and we look at our data over time, like in the case

439
00:19:51,627 --> 00:19:56,132
of those international charts I just showed you,

440
00:19:56,132 --> 00:20:00,330
to look at Chrome's strengths and weaknesses.

441
00:20:00,330 --> 00:20:03,231
If you're in industry, hopefully, I've convinced you

442
00:20:03,231 --> 00:20:05,825
that there is merit to taking this type of approach

443
00:20:05,825 --> 00:20:08,000
and given you some stories about things

444
00:20:08,000 --> 00:20:10,627
that are surprising, or negative results

445
00:20:10,627 --> 00:20:12,627
that you can take back to your colleagues

446
00:20:12,627 --> 00:20:14,033
to demonstrate why it's important

447
00:20:14,033 --> 00:20:17,033
to actually test things, instead of just making changes

448
00:20:17,033 --> 00:20:19,000
that we feel in our gut are good for users

449
00:20:19,000 --> 00:20:22,297
and then launching that, because it doesn't always work.

450
00:20:22,297 --> 00:20:24,528
And, if you're in academia, I hope I've convinced you

451
00:20:24,528 --> 00:20:26,396
that there's still a lot of ground to cover

452
00:20:26,396 --> 00:20:28,363
in usable security research.

453
00:20:28,363 --> 00:20:30,561
There are companies, like Google, that are hungry

454
00:20:30,561 --> 00:20:33,297
for specific and actionable recommendations

455
00:20:33,297 --> 00:20:34,627
backed by your research

456
00:20:34,627 --> 00:20:37,033
that we can use to improve our products.

457
00:20:37,033 --> 00:20:39,462
Matthew Smith, who'll be talking later today,

458
00:20:39,462 --> 00:20:41,363
is an excellent example of this.

459
00:20:41,363 --> 00:20:44,231
So I appeal to you: please respect end users

460
00:20:44,231 --> 00:20:46,264
by going and doing usable security

461
00:20:46,264 --> 00:20:49,000
as a scientific field. Thank you.

462
00:20:49,000 --> 00:20:50,561
[ Applause ]

463
00:20:50,561 --> 00:20:56,131
♪♪

464
00:20:56,132 --> 00:20:55,132


