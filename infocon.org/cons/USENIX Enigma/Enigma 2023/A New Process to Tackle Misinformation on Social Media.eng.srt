1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:14,960 --> 00:00:17,760
thank you so much for attending my

3
00:00:17,760 --> 00:00:20,580
presentation today I'm coming shekhar I

4
00:00:20,580 --> 00:00:22,680
lead uh the privacy and data governance

5
00:00:22,680 --> 00:00:25,560
vertical at the dialogue dialogue is a

6
00:00:25,560 --> 00:00:28,740
think tank based out of Delhi India uh

7
00:00:28,740 --> 00:00:31,439
today I will be presenting about

8
00:00:31,439 --> 00:00:33,300
um uh like

9
00:00:33,300 --> 00:00:35,640
a new process for tackling this

10
00:00:35,640 --> 00:00:37,380
information misinformation of the social

11
00:00:37,380 --> 00:00:39,719
media platform using a prevalence-based

12
00:00:39,719 --> 00:00:42,239
condition process so

13
00:00:42,239 --> 00:00:45,000
um today I I like I'll be doing a

14
00:00:45,000 --> 00:00:46,920
recording of this uh and like which you

15
00:00:46,920 --> 00:00:49,920
guys will be watching uh unfortunately I

16
00:00:49,920 --> 00:00:51,180
was not able to make it for the

17
00:00:51,180 --> 00:00:53,039
conference and I really appreciate the

18
00:00:53,039 --> 00:00:54,840
conference team for

19
00:00:54,840 --> 00:00:58,440
um having this option ready for me so

20
00:00:58,440 --> 00:01:01,559
um just moving on uh today like what uh

21
00:01:01,559 --> 00:01:03,120
what I will be like actually like

22
00:01:03,120 --> 00:01:05,939
covering in like couple of minutes is

23
00:01:05,939 --> 00:01:08,340
like start from what is the problem what

24
00:01:08,340 --> 00:01:10,560
is the process level intervention what

25
00:01:10,560 --> 00:01:12,180
is the privilege-based creation process

26
00:01:12,180 --> 00:01:14,760
that I am actually like suggesting and

27
00:01:14,760 --> 00:01:17,100
uh how after that like we should be

28
00:01:17,100 --> 00:01:18,540
evaluating the high prevalent

29
00:01:18,540 --> 00:01:20,520
information and like accordingly like

30
00:01:20,520 --> 00:01:22,380
you know enforcing

31
00:01:22,380 --> 00:01:25,680
um the calibrated Collective action so

32
00:01:25,680 --> 00:01:28,920
quickly moving to what is the problem so

33
00:01:28,920 --> 00:01:31,259
like information is power yet

34
00:01:31,259 --> 00:01:33,299
unfortunately the information we receive

35
00:01:33,299 --> 00:01:35,220
is heavily distorted due to

36
00:01:35,220 --> 00:01:37,500
disinformation and misinformation but

37
00:01:37,500 --> 00:01:39,240
let's take one step backward and first

38
00:01:39,240 --> 00:01:40,680
understand the difference between this

39
00:01:40,680 --> 00:01:43,500
information and misinformation when a

40
00:01:43,500 --> 00:01:45,479
person produce uses baseless and

41
00:01:45,479 --> 00:01:47,400
misleading information to deceit their

42
00:01:47,400 --> 00:01:49,340
followers it's called the disinformation

43
00:01:49,340 --> 00:01:52,140
when they're unsuspecting followers

44
00:01:52,140 --> 00:01:54,000
start sharing the same misleading

45
00:01:54,000 --> 00:01:56,100
information within their own network

46
00:01:56,100 --> 00:01:58,320
it's called misinformation in the case

47
00:01:58,320 --> 00:02:00,840
of the latter the intention is not to

48
00:02:00,840 --> 00:02:04,140
deceive instead the reader believes that

49
00:02:04,140 --> 00:02:05,719
the information they read is true

50
00:02:05,719 --> 00:02:08,098
confirming their pre-existing biases

51
00:02:08,098 --> 00:02:10,199
thus like this shows that disinformation

52
00:02:10,199 --> 00:02:11,760
starts a change reaction of

53
00:02:11,760 --> 00:02:14,280
misinformation which eventually becomes

54
00:02:14,280 --> 00:02:16,980
a wild forest fire

55
00:02:16,980 --> 00:02:18,420
um while this information and

56
00:02:18,420 --> 00:02:21,120
misinformation is not a new threat we do

57
00:02:21,120 --> 00:02:23,760
all agree that it has been aggregated or

58
00:02:23,760 --> 00:02:27,239
accelerated by social media platforms as

59
00:02:27,239 --> 00:02:29,099
as we saw in the case of like

60
00:02:29,099 --> 00:02:30,660
information War which happened between

61
00:02:30,660 --> 00:02:33,720
Ukraine and Russia during the war

62
00:02:33,720 --> 00:02:36,239
um like well like obviously like in

63
00:02:36,239 --> 00:02:38,340
background of this like you know quick

64
00:02:38,340 --> 00:02:40,560
access and like easy access and quick

65
00:02:40,560 --> 00:02:41,940
access to information through social

66
00:02:41,940 --> 00:02:43,800
media platform has definitely been

67
00:02:43,800 --> 00:02:46,860
helpful but at the same time lack of

68
00:02:46,860 --> 00:02:48,660
understanding of the accuracy of the

69
00:02:48,660 --> 00:02:50,700
information of the social media platform

70
00:02:50,700 --> 00:02:53,280
his like eventually is also becoming

71
00:02:53,280 --> 00:02:55,920
problematic this is especially true in

72
00:02:55,920 --> 00:02:57,540
case of high stake informations like

73
00:02:57,540 --> 00:02:59,700
election related information health

74
00:02:59,700 --> 00:03:00,900
related information

75
00:03:00,900 --> 00:03:03,180
Etc which has critical consequences on

76
00:03:03,180 --> 00:03:05,519
individuals and communities if they are

77
00:03:05,519 --> 00:03:06,780
actually muddled with this information

78
00:03:06,780 --> 00:03:09,120
and misinformation for instance like

79
00:03:09,120 --> 00:03:10,860
despite the efforts taken by social

80
00:03:10,860 --> 00:03:13,080
media platforms to flag evolve into

81
00:03:13,080 --> 00:03:14,940
false information on the you know

82
00:03:14,940 --> 00:03:17,760
election Integrity uh U.S capital was

83
00:03:17,760 --> 00:03:21,000
attacked on 6th January 2021. like

84
00:03:21,000 --> 00:03:22,620
partly due to the spread of

85
00:03:22,620 --> 00:03:25,739
disinformation and misinformation

86
00:03:25,739 --> 00:03:27,540
um

87
00:03:27,540 --> 00:03:29,459
presently

88
00:03:29,459 --> 00:03:32,340
um functions of establishing policies or

89
00:03:32,340 --> 00:03:34,200
like mechanisms for tackling this

90
00:03:34,200 --> 00:03:35,519
information and this information are

91
00:03:35,519 --> 00:03:38,099
distributed across various actors which

92
00:03:38,099 --> 00:03:39,959
starts and like includes social media

93
00:03:39,959 --> 00:03:43,440
platforms government users especially in

94
00:03:43,440 --> 00:03:45,799
the context of decentralized platforms

95
00:03:45,799 --> 00:03:47,099
multi-stakeholder groups

96
00:03:47,099 --> 00:03:49,019
intergovernmental organizations

97
00:03:49,019 --> 00:03:51,360
Etc however the role of social media

98
00:03:51,360 --> 00:03:53,099
platforms themselves themselves in

99
00:03:53,099 --> 00:03:55,500
tackling the information disorder needs

100
00:03:55,500 --> 00:03:57,720
a greater attention why I say this is

101
00:03:57,720 --> 00:03:59,459
that their Innovation for intervention

102
00:03:59,459 --> 00:04:01,920
actually pose both question of

103
00:04:01,920 --> 00:04:04,620
um competency as well as unintendent

104
00:04:04,620 --> 00:04:06,420
consequences which could potentially

105
00:04:06,420 --> 00:04:08,220
cause ill effects like infringing

106
00:04:08,220 --> 00:04:10,560
freedom of expression discretion over

107
00:04:10,560 --> 00:04:14,040
discard descent Etc many like

108
00:04:14,040 --> 00:04:16,199
centralized and decentralized platforms

109
00:04:16,199 --> 00:04:18,060
currently indulgent have like you know

110
00:04:18,060 --> 00:04:20,220
content level hard moderation that's

111
00:04:20,220 --> 00:04:22,500
nothing but the system that they use for

112
00:04:22,500 --> 00:04:24,600
making this decisions about content and

113
00:04:24,600 --> 00:04:26,759
accounts to reduce the spread of like

114
00:04:26,759 --> 00:04:29,880
misleading and fake information now

115
00:04:29,880 --> 00:04:31,560
um at that level like platforms use

116
00:04:31,560 --> 00:04:34,560
various like the technological measures

117
00:04:34,560 --> 00:04:37,280
like word filters automated hashtagging

118
00:04:37,280 --> 00:04:40,139
geoblocking content IDs and also like

119
00:04:40,139 --> 00:04:42,060
other predictive machine learning tools

120
00:04:42,060 --> 00:04:44,639
learning tools to like detect unlawful

121
00:04:44,639 --> 00:04:46,080
content like

122
00:04:46,080 --> 00:04:48,960
um uh child sexual images photography

123
00:04:48,960 --> 00:04:50,820
also includes disinformation and

124
00:04:50,820 --> 00:04:52,080
misinformation which is like a

125
00:04:52,080 --> 00:04:53,460
borderline

126
00:04:53,460 --> 00:04:55,979
um content uh borderline like content

127
00:04:55,979 --> 00:04:58,740
Etc platforms take decisions related to

128
00:04:58,740 --> 00:05:00,900
the like detected content and the

129
00:05:00,900 --> 00:05:02,639
account of individuals using sometimes

130
00:05:02,639 --> 00:05:04,979
human moderators or algorithms

131
00:05:04,979 --> 00:05:08,520
themselves like though we have to

132
00:05:08,520 --> 00:05:10,380
acknowledge that these technological

133
00:05:10,380 --> 00:05:12,900
measures have their own merits to extend

134
00:05:12,900 --> 00:05:15,120
especially where platforms can act

135
00:05:15,120 --> 00:05:17,639
faster and at scale at the same time we

136
00:05:17,639 --> 00:05:19,740
increasingly see content falling through

137
00:05:19,740 --> 00:05:21,600
the crack due to false negatives and

138
00:05:21,600 --> 00:05:23,820
getting struck down due to false

139
00:05:23,820 --> 00:05:26,400
positives false negatives yes like there

140
00:05:26,400 --> 00:05:28,380
has been multiple instances like what we

141
00:05:28,380 --> 00:05:30,360
saw on the U.S capital attack with the

142
00:05:30,360 --> 00:05:32,520
previous slide on the other hand like

143
00:05:32,520 --> 00:05:34,500
false positives like could hamper

144
00:05:34,500 --> 00:05:36,780
freedom of expression and opinion for

145
00:05:36,780 --> 00:05:38,940
instance like um

146
00:05:38,940 --> 00:05:41,280
a social media platform famously took

147
00:05:41,280 --> 00:05:43,500
down the award-winning image of a naked

148
00:05:43,500 --> 00:05:47,840
girl fleeing uh uh nalpam bomb like

149
00:05:47,840 --> 00:05:51,060
captured during the Vietnam war this

150
00:05:51,060 --> 00:05:53,340
like though had like some sort of you

151
00:05:53,340 --> 00:05:57,000
know uh connotations but like like the

152
00:05:57,000 --> 00:05:58,800
platform did take it down which was like

153
00:05:58,800 --> 00:06:01,440
reinstated later one of the critical

154
00:06:01,440 --> 00:06:04,020
reasons for like why the posts fall

155
00:06:04,020 --> 00:06:05,520
through the clock crack is that

156
00:06:05,520 --> 00:06:07,440
platforms are presently confined to a

157
00:06:07,440 --> 00:06:10,020
Content level intervention in absence of

158
00:06:10,020 --> 00:06:11,940
the process level Clarity and

159
00:06:11,940 --> 00:06:14,100
intervention within the personal clarity

160
00:06:14,100 --> 00:06:15,600
as well as the intervention within the

161
00:06:15,600 --> 00:06:18,180
content moderation pipeline this lack of

162
00:06:18,180 --> 00:06:19,800
process level intervention causes

163
00:06:19,800 --> 00:06:22,139
platforms to utilize resources and time

164
00:06:22,139 --> 00:06:25,440
inefficiently therefore like the status

165
00:06:25,440 --> 00:06:28,440
quo calls for a proactive and not just a

166
00:06:28,440 --> 00:06:31,020
reactive content moderation process and

167
00:06:31,020 --> 00:06:34,919
means to implement the same efficiently

168
00:06:34,919 --> 00:06:36,900
thank you

169
00:06:36,900 --> 00:06:39,419
against this backdrop I that's where

170
00:06:39,419 --> 00:06:41,220
like I come in and against this backdrop

171
00:06:41,220 --> 00:06:42,900
I propose one such process level

172
00:06:42,900 --> 00:06:44,639
intervention that could refine the

173
00:06:44,639 --> 00:06:46,620
content moderation Pipeline and enable

174
00:06:46,620 --> 00:06:48,780
efficient use of tools and resources of

175
00:06:48,780 --> 00:06:51,240
processes entirety so with the process

176
00:06:51,240 --> 00:06:53,039
level intervention discussed here which

177
00:06:53,039 --> 00:06:55,440
you could see on the slide

178
00:06:55,440 --> 00:06:57,720
um like can Elevate the false negatives

179
00:06:57,720 --> 00:06:59,759
and positives to more efficient use of

180
00:06:59,759 --> 00:07:02,819
resources but they cannot like perform

181
00:07:02,819 --> 00:07:05,100
any better at eradicating borderline

182
00:07:05,100 --> 00:07:06,300
content like disinformation

183
00:07:06,300 --> 00:07:08,460
misinformation that is highly contextual

184
00:07:08,460 --> 00:07:11,340
however I think like you know elevating

185
00:07:11,340 --> 00:07:13,560
is the first step forward towards like

186
00:07:13,560 --> 00:07:16,800
you know okay taking any action against

187
00:07:16,800 --> 00:07:19,440
um such contextual information sir uh

188
00:07:19,440 --> 00:07:21,180
with mounting pressure on the platforms

189
00:07:21,180 --> 00:07:23,039
from the government as well as

190
00:07:23,039 --> 00:07:25,199
individuals to you know tackle narrative

191
00:07:25,199 --> 00:07:27,180
harms there are sort through hard

192
00:07:27,180 --> 00:07:28,319
moderation

193
00:07:28,319 --> 00:07:30,720
um heart content moderation but yes they

194
00:07:30,720 --> 00:07:33,840
face a problem of scale but scale has

195
00:07:33,840 --> 00:07:35,880
never been a problem for soft moderation

196
00:07:35,880 --> 00:07:38,400
that is your recommendous systems design

197
00:07:38,400 --> 00:07:40,560
decisions and architectures due to one

198
00:07:40,560 --> 00:07:42,840
factor that is utilization of data

199
00:07:42,840 --> 00:07:44,460
points on the individuals to recommend

200
00:07:44,460 --> 00:07:46,860
prevalent content which aligns with the

201
00:07:46,860 --> 00:07:49,380
you know preferences of the X individual

202
00:07:49,380 --> 00:07:52,139
then platform can use prevalence based

203
00:07:52,139 --> 00:07:53,819
in systems to determine the real-time

204
00:07:53,819 --> 00:07:55,680
ranking and record like recommendations

205
00:07:55,680 --> 00:07:57,960
to individuals in form of soft

206
00:07:57,960 --> 00:08:00,360
moderation I propose a symbol of

207
00:08:00,360 --> 00:08:02,039
prevalence brace graduation process the

208
00:08:02,039 --> 00:08:04,139
system that gives us prevalence as

209
00:08:04,139 --> 00:08:06,960
integral element for hard moderation to

210
00:08:06,960 --> 00:08:08,099
tackle this information and

211
00:08:08,099 --> 00:08:09,419
misinformation

212
00:08:09,419 --> 00:08:12,000
okay what is the problem like um

213
00:08:12,000 --> 00:08:13,500
prevalence-based correlation process

214
00:08:13,500 --> 00:08:16,199
this is a process that would

215
00:08:16,199 --> 00:08:17,759
add through means through which like

216
00:08:17,759 --> 00:08:19,919
social media platforms can evaluate

217
00:08:19,919 --> 00:08:21,900
content using exact day measures

218
00:08:21,900 --> 00:08:23,940
according to the prevalence and exercise

219
00:08:23,940 --> 00:08:25,860
optimal corrective actions in a

220
00:08:25,860 --> 00:08:27,960
calibrated format are just remain

221
00:08:27,960 --> 00:08:29,819
according to the exposure level of the

222
00:08:29,819 --> 00:08:32,039
information here when we talk about

223
00:08:32,039 --> 00:08:33,979
exposure or prevalence of information

224
00:08:33,979 --> 00:08:37,200
again it's not just about the popularity

225
00:08:37,200 --> 00:08:39,360
but rather calibrated information

226
00:08:39,360 --> 00:08:41,760
buckated like the ones you see on your

227
00:08:41,760 --> 00:08:43,580
writer

228
00:08:43,580 --> 00:08:47,519
So currently platforms do follow like a

229
00:08:47,519 --> 00:08:49,740
plane and a simple hard moderation uh

230
00:08:49,740 --> 00:08:51,959
without any glandular

231
00:08:51,959 --> 00:08:53,880
um relation to efficiently utilize the

232
00:08:53,880 --> 00:08:56,279
limited resources of the disposal for

233
00:08:56,279 --> 00:08:58,440
seeing like serious consensus while

234
00:08:58,440 --> 00:09:00,600
platforms do like categorize like

235
00:09:00,600 --> 00:09:02,220
election related information health

236
00:09:02,220 --> 00:09:04,380
related information as like high stake

237
00:09:04,380 --> 00:09:06,360
informations that needs additional

238
00:09:06,360 --> 00:09:10,260
additional uh restrictions and scrutiny

239
00:09:10,260 --> 00:09:13,019
but within the high stake information

240
00:09:13,019 --> 00:09:14,940
platforms typically don't primarily

241
00:09:14,940 --> 00:09:16,740
prioritize content according to the

242
00:09:16,740 --> 00:09:19,620
reach and prevalence this to me sounds

243
00:09:19,620 --> 00:09:22,560
little sub-optimal um the sub-optimal As

244
00:09:22,560 --> 00:09:24,480
Nice take information with two different

245
00:09:24,480 --> 00:09:26,399
privileged levels should not be treated

246
00:09:26,399 --> 00:09:27,600
the same

247
00:09:27,600 --> 00:09:29,339
therefore platform need to start

248
00:09:29,339 --> 00:09:30,959
utilizing the data collected on

249
00:09:30,959 --> 00:09:33,060
individuals like follower levels like

250
00:09:33,060 --> 00:09:35,880
shares comments Etc on the content to

251
00:09:35,880 --> 00:09:37,380
bucket the information within the

252
00:09:37,380 --> 00:09:39,779
gradation metrics that you see

253
00:09:39,779 --> 00:09:41,279
um table on the right is a simple

254
00:09:41,279 --> 00:09:43,440
gradation you know prevalence based

255
00:09:43,440 --> 00:09:45,660
gradation metrics that uses data points

256
00:09:45,660 --> 00:09:48,360
such as as I said like followers like

257
00:09:48,360 --> 00:09:50,399
shares and Etc to bucket high stake

258
00:09:50,399 --> 00:09:52,500
information ranging from high to low

259
00:09:52,500 --> 00:09:54,839
level and chances of prevalence as you

260
00:09:54,839 --> 00:09:56,880
can see like it's also crawler coded for

261
00:09:56,880 --> 00:09:58,500
like better understanding where like

262
00:09:58,500 --> 00:10:00,360
bucket nine and eight is like you know

263
00:10:00,360 --> 00:10:02,760
not in the radar and like eventually

264
00:10:02,760 --> 00:10:04,860
like content goes to the Amber which is

265
00:10:04,860 --> 00:10:06,720
Your Bucket seven and then like

266
00:10:06,720 --> 00:10:09,060
eventually to Orange and then like move

267
00:10:09,060 --> 00:10:12,080
on to the red which is the high alert

268
00:10:12,080 --> 00:10:14,760
however like also like you know the

269
00:10:14,760 --> 00:10:16,620
information that you see is just a

270
00:10:16,620 --> 00:10:18,839
simple ingredient metrics and like

271
00:10:18,839 --> 00:10:20,820
platforms do collect like various um

272
00:10:20,820 --> 00:10:23,100
amount of data which we cup like we

273
00:10:23,100 --> 00:10:25,320
don't like uh extensive amount of data

274
00:10:25,320 --> 00:10:26,940
which we really don't come to know about

275
00:10:26,940 --> 00:10:29,640
so maybe platforms can come up with like

276
00:10:29,640 --> 00:10:31,620
more Nuance like you know gradation

277
00:10:31,620 --> 00:10:34,140
process itself

278
00:10:34,140 --> 00:10:35,459
um

279
00:10:35,459 --> 00:10:38,820
now when the you know the content is

280
00:10:38,820 --> 00:10:41,339
like is getting into the you know higher

281
00:10:41,339 --> 00:10:44,100
bucket of one two three or something

282
00:10:44,100 --> 00:10:46,620
um at that time like exante scrutinine

283
00:10:46,620 --> 00:10:49,560
should be up like you know applied uh on

284
00:10:49,560 --> 00:10:51,779
that such content again what is an

285
00:10:51,779 --> 00:10:54,120
excuse me for social media platform is

286
00:10:54,120 --> 00:10:56,459
to act on the inappropriate content

287
00:10:56,459 --> 00:10:59,519
before the you know individuals Flags it

288
00:10:59,519 --> 00:11:01,800
or screens content you know or strain

289
00:11:01,800 --> 00:11:03,480
the content before the individual is

290
00:11:03,480 --> 00:11:05,279
like information or that info

291
00:11:05,279 --> 00:11:07,440
individuals account has made public or

292
00:11:07,440 --> 00:11:10,500
shared this pre-screening of um almost

293
00:11:10,500 --> 00:11:12,600
every piece of information on the

294
00:11:12,600 --> 00:11:15,000
platform is not really pragmatic and

295
00:11:15,000 --> 00:11:16,800
disproportionate infringing on freedom

296
00:11:16,800 --> 00:11:19,800
of expression of individuals so that's

297
00:11:19,800 --> 00:11:21,600
where

298
00:11:21,600 --> 00:11:24,180
however like you know we do understand

299
00:11:24,180 --> 00:11:25,860
that like prevalence of information

300
00:11:25,860 --> 00:11:28,800
shared or made public increases the

301
00:11:28,800 --> 00:11:31,260
importance of the shared information and

302
00:11:31,260 --> 00:11:33,120
so does the responsibility to scrutinize

303
00:11:33,120 --> 00:11:35,339
the content proportionately therefore

304
00:11:35,339 --> 00:11:36,860
instead of like traditional

305
00:11:36,860 --> 00:11:39,000
pre-screening it may be ideal for

306
00:11:39,000 --> 00:11:41,100
platforms to act before the Harvest cost

307
00:11:41,100 --> 00:11:43,920
in a calibrated way depending upon the

308
00:11:43,920 --> 00:11:46,220
level of the information uh prevalence

309
00:11:46,220 --> 00:11:49,620
the proposed model does consider this as

310
00:11:49,620 --> 00:11:52,200
an example now for instance the

311
00:11:52,200 --> 00:11:54,180
information in bucket123 should be

312
00:11:54,180 --> 00:11:56,220
subjected to exantis scrutiny that is

313
00:11:56,220 --> 00:11:58,620
evaluating the prevalent high stake

314
00:11:58,620 --> 00:12:00,899
information to find this information in

315
00:12:00,899 --> 00:12:02,579
this information while the information

316
00:12:02,579 --> 00:12:05,220
and bucket to four to six stays on the

317
00:12:05,220 --> 00:12:07,820
high alert

318
00:12:08,160 --> 00:12:10,680
okay next in the pipeline is like

319
00:12:10,680 --> 00:12:12,480
suppose the exactly scrutiny of a

320
00:12:12,480 --> 00:12:13,740
prevalent high stake information

321
00:12:13,740 --> 00:12:16,860
provides that you know proves that like

322
00:12:16,860 --> 00:12:19,459
you know the the content that has been

323
00:12:19,459 --> 00:12:22,560
scrutinized is misleading and false in

324
00:12:22,560 --> 00:12:24,600
that case Platformers take a proper

325
00:12:24,600 --> 00:12:26,579
corrective action to control the spread

326
00:12:26,579 --> 00:12:30,540
and the harm uh however what like what

327
00:12:30,540 --> 00:12:32,339
happens currently is that platforms

328
00:12:32,339 --> 00:12:33,959
exercise a very limited range of

329
00:12:33,959 --> 00:12:35,820
corrective actions that don't

330
00:12:35,820 --> 00:12:37,440
necessarily like align with the hard

331
00:12:37,440 --> 00:12:39,839
moderation enforcement goals panning out

332
00:12:39,839 --> 00:12:42,000
excessively in some you know instances

333
00:12:42,000 --> 00:12:44,459
for instance platforms like like we have

334
00:12:44,459 --> 00:12:47,279
seen like extensively use content data

335
00:12:47,279 --> 00:12:49,560
on or like your own user content

336
00:12:49,560 --> 00:12:51,959
blocking or content flagging to achieve

337
00:12:51,959 --> 00:12:54,420
almost every hard moderation enforcement

338
00:12:54,420 --> 00:12:57,779
goals tefl platforms must have various

339
00:12:57,779 --> 00:12:59,940
corrective actions to align with you

340
00:12:59,940 --> 00:13:01,920
know different enforcement goals of the

341
00:13:01,920 --> 00:13:04,079
heart moderation maybe start with like

342
00:13:04,079 --> 00:13:06,180
moratorium for you know deterrence

343
00:13:06,180 --> 00:13:09,660
misleading information for deterrence

344
00:13:09,660 --> 00:13:12,720
misleading Etc uh

345
00:13:12,720 --> 00:13:15,779
the harm to be recognized for the

346
00:13:15,779 --> 00:13:17,519
corrective action should be tangible

347
00:13:17,519 --> 00:13:19,440
like Financial

348
00:13:19,440 --> 00:13:20,399
um

349
00:13:20,399 --> 00:13:23,760
uh Financial harms Health repercussions

350
00:13:23,760 --> 00:13:26,700
Etc and also intangible Harms in forms

351
00:13:26,700 --> 00:13:28,019
of like you know emotional or

352
00:13:28,019 --> 00:13:29,639
psychological or reputational

353
00:13:29,639 --> 00:13:32,700
repercussions Etc moreover the causation

354
00:13:32,700 --> 00:13:35,459
of harm should not the only factor for

355
00:13:35,459 --> 00:13:37,620
the platform to take action connective

356
00:13:37,620 --> 00:13:39,360
action must be exciting to whenever

357
00:13:39,360 --> 00:13:41,820
level where acting before the harm is

358
00:13:41,820 --> 00:13:44,100
caused by the prevalent or the

359
00:13:44,100 --> 00:13:45,720
misleading information and to extend

360
00:13:45,720 --> 00:13:48,480
like speculative also where concerns

361
00:13:48,480 --> 00:13:50,820
concerns should rise in the long term

362
00:13:50,820 --> 00:13:53,040
like keeping the information in the

363
00:13:53,040 --> 00:13:56,519
bucket four to six in the high alert

364
00:13:56,519 --> 00:13:58,320
um connective action must be calibrated

365
00:13:58,320 --> 00:14:00,360
you know such that the information of

366
00:14:00,360 --> 00:14:02,279
two different prevalence must be

367
00:14:02,279 --> 00:14:04,380
subjected to appropriate corrective

368
00:14:04,380 --> 00:14:07,139
action right like now information when

369
00:14:07,139 --> 00:14:08,700
the bucket one should not be actually

370
00:14:08,700 --> 00:14:10,920
like should be treated similar as the

371
00:14:10,920 --> 00:14:12,600
information in paketo and subsequent

372
00:14:12,600 --> 00:14:15,540
buckets for instance Bank

373
00:14:15,540 --> 00:14:17,820
with like a platforms flagging and

374
00:14:17,820 --> 00:14:19,920
masking the information to be misleading

375
00:14:19,920 --> 00:14:22,200
and fake and stopping people from

376
00:14:22,200 --> 00:14:24,839
sharing it further as the same

377
00:14:24,839 --> 00:14:26,940
information like you know elevates in

378
00:14:26,940 --> 00:14:29,579
the prevalence metrics uh and starts

379
00:14:29,579 --> 00:14:31,200
falling within the bucket two or one

380
00:14:31,200 --> 00:14:33,060
more severe actions can be like you know

381
00:14:33,060 --> 00:14:34,980
taken into consideration

382
00:14:34,980 --> 00:14:36,300
so

383
00:14:36,300 --> 00:14:39,420
like like final step within this process

384
00:14:39,420 --> 00:14:42,000
is nothing but like um you know

385
00:14:42,000 --> 00:14:44,000
conveying this information

386
00:14:44,000 --> 00:14:47,040
properly the decision information to the

387
00:14:47,040 --> 00:14:49,920
individuals itself such that this the

388
00:14:49,920 --> 00:14:53,399
entire process is like complete and like

389
00:14:53,399 --> 00:14:55,260
doesn't really

390
00:14:55,260 --> 00:14:57,300
um you know be one way and also like

391
00:14:57,300 --> 00:15:00,240
little bit transparent and accountable

392
00:15:00,240 --> 00:15:03,240
therefore

393
00:15:03,779 --> 00:15:06,000
um adopting a prevalence-based gradation

394
00:15:06,000 --> 00:15:07,740
process by social media platform would

395
00:15:07,740 --> 00:15:09,600
be very critical you have the process

396
00:15:09,600 --> 00:15:11,579
level things that we have like discussed

397
00:15:11,579 --> 00:15:13,320
just now which I have discussed right

398
00:15:13,320 --> 00:15:15,959
now will help tackle false negatives

399
00:15:15,959 --> 00:15:18,300
which like which cuts down the you know

400
00:15:18,300 --> 00:15:20,040
positive reinforcement for negative

401
00:15:20,040 --> 00:15:22,440
behaves like posting or sharing invest

402
00:15:22,440 --> 00:15:24,480
information less information what I'm

403
00:15:24,480 --> 00:15:26,160
saying is that like you know once like

404
00:15:26,160 --> 00:15:28,199
people have been like given strikes and

405
00:15:28,199 --> 00:15:31,199
like you know uh and like uh I'm like

406
00:15:31,199 --> 00:15:33,060
not let go because of the false

407
00:15:33,060 --> 00:15:35,220
negatives he eventually people will come

408
00:15:35,220 --> 00:15:36,660
to know that their actions have

409
00:15:36,660 --> 00:15:39,480
repercussions moreover it will create a

410
00:15:39,480 --> 00:15:41,760
more transparent environment where users

411
00:15:41,760 --> 00:15:43,139
start understanding the measures

412
00:15:43,139 --> 00:15:44,880
enforced by the platforms according to

413
00:15:44,880 --> 00:15:47,760
the prevalence which is not really the

414
00:15:47,760 --> 00:15:49,980
case at this moment this process will

415
00:15:49,980 --> 00:15:51,839
also indirectly Aid the Platforms in

416
00:15:51,839 --> 00:15:54,300
reducing false positives has it gained

417
00:15:54,300 --> 00:15:56,459
them more time and resources to

418
00:15:56,459 --> 00:15:57,540
understand the context of the

419
00:15:57,540 --> 00:15:59,880
information better and like also like

420
00:15:59,880 --> 00:16:02,040
inform which is more important to be

421
00:16:02,040 --> 00:16:04,320
tackled at that moment and not the other

422
00:16:04,320 --> 00:16:07,440
information sir and gradation metrics

423
00:16:07,440 --> 00:16:09,180
can also help platforms accomplish

424
00:16:09,180 --> 00:16:12,660
efficient utilization of resources

425
00:16:12,660 --> 00:16:13,740
um

426
00:16:13,740 --> 00:16:15,720
in terms of monitoring content for

427
00:16:15,720 --> 00:16:17,880
narrative arms they can also spend like

428
00:16:17,880 --> 00:16:21,000
as I was saying right like you know

429
00:16:21,000 --> 00:16:23,459
um various other like um they can also

430
00:16:23,459 --> 00:16:26,399
use this time for like efficiently

431
00:16:26,399 --> 00:16:28,620
um use for other purposes as well which

432
00:16:28,620 --> 00:16:31,139
is like beyond the content moderation

433
00:16:31,139 --> 00:16:33,199
and also think about Innovation and Etc

434
00:16:33,199 --> 00:16:35,279
however it's also essential to

435
00:16:35,279 --> 00:16:37,500
systematically confront the cons of this

436
00:16:37,500 --> 00:16:40,019
process like white listing that is like

437
00:16:40,019 --> 00:16:43,259
yeah like uh shadowing a particular set

438
00:16:43,259 --> 00:16:45,420
of individuals from any moderation or

439
00:16:45,420 --> 00:16:47,339
discrimination

440
00:16:47,339 --> 00:16:49,259
um leading to like um like

441
00:16:49,259 --> 00:16:50,940
discrimination or inferencing your

442
00:16:50,940 --> 00:16:52,740
voices understand of the

443
00:16:52,740 --> 00:16:55,440
underrepresented individuals Etc besides

444
00:16:55,440 --> 00:16:57,660
it's also essential to like I outline

445
00:16:57,660 --> 00:16:59,880
the extent to which the data points can

446
00:16:59,880 --> 00:17:02,220
be used for this process level of

447
00:17:02,220 --> 00:17:06,240
gradation like threshold boundaries of

448
00:17:06,240 --> 00:17:07,919
the process because like they should not

449
00:17:07,919 --> 00:17:10,079
be just let by the platforms to be done

450
00:17:10,079 --> 00:17:11,339
now

451
00:17:11,339 --> 00:17:12,419
um

452
00:17:12,419 --> 00:17:14,699
moreover it is also like critical to

453
00:17:14,699 --> 00:17:16,380
democratically expand the corrective

454
00:17:16,380 --> 00:17:18,359
action toolkit for the platforms right

455
00:17:18,359 --> 00:17:20,699
like as we discussed involving various

456
00:17:20,699 --> 00:17:22,799
policy actors in the process to achieve

457
00:17:22,799 --> 00:17:24,359
various hard moderational enforcement

458
00:17:24,359 --> 00:17:27,419
goals therefore as we move forward

459
00:17:27,419 --> 00:17:30,120
platforms must adopt this process or any

460
00:17:30,120 --> 00:17:32,340
other process level intervention to

461
00:17:32,340 --> 00:17:33,540
strengthen the foundation that

462
00:17:33,540 --> 00:17:35,460
Underpants the internet success that is

463
00:17:35,460 --> 00:17:37,320
trustworthiness by intervening in the

464
00:17:37,320 --> 00:17:38,840
application layer of the internet

465
00:17:38,840 --> 00:17:40,620
therefore this process level

466
00:17:40,620 --> 00:17:42,480
intervention and content moderation to

467
00:17:42,480 --> 00:17:44,039
tackle this information misinformation

468
00:17:44,039 --> 00:17:47,580
it shows that individuals have trust and

469
00:17:47,580 --> 00:17:49,980
secure internet experience thank you so

470
00:17:49,980 --> 00:17:52,500
much sir so as I'm not present there

471
00:17:52,500 --> 00:17:54,419
though I will be like available on the

472
00:17:54,419 --> 00:17:56,340
slack platforms and like I would be

473
00:17:56,340 --> 00:17:58,080
answering all the questions that comes

474
00:17:58,080 --> 00:18:02,100
from the audience at this moment and uh

475
00:18:02,100 --> 00:18:03,900
really looking forward to like

476
00:18:03,900 --> 00:18:06,299
connecting offline with everybody who

477
00:18:06,299 --> 00:18:07,980
are interested in this topic or like

478
00:18:07,980 --> 00:18:10,740
would love to know more about this

479
00:18:10,740 --> 00:18:13,799
um and like and uh love to do that and

480
00:18:13,799 --> 00:18:16,580
thank you so much

