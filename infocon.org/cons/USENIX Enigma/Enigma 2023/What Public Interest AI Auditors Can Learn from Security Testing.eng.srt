1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:16,699 --> 00:00:18,660
reports I'm going to be talking about

3
00:00:18,660 --> 00:00:20,100
the importance of public interest

4
00:00:20,100 --> 00:00:21,260
testing

5
00:00:21,260 --> 00:00:23,160
specifically what public interest

6
00:00:23,160 --> 00:00:25,019
testers can learn from the history of

7
00:00:25,019 --> 00:00:26,820
security researchers has been a long

8
00:00:26,820 --> 00:00:28,939
history of of security researchers

9
00:00:28,939 --> 00:00:31,080
encountering policy and practical

10
00:00:31,080 --> 00:00:33,239
difficulties in their work and as we

11
00:00:33,239 --> 00:00:34,920
think about testing Ai and other

12
00:00:34,920 --> 00:00:36,420
complicated systems how can we

13
00:00:36,420 --> 00:00:37,980
incorporate some of the learnings from

14
00:00:37,980 --> 00:00:41,460
that into how we proceed on that so

15
00:00:41,460 --> 00:00:44,100
first why does Consumer Reports care

16
00:00:44,100 --> 00:00:46,200
about testing testing is obviously who

17
00:00:46,200 --> 00:00:48,360
we are as an organization we've been

18
00:00:48,360 --> 00:00:51,600
around for 85 some years have a history

19
00:00:51,600 --> 00:00:54,800
of testing cars and dishwashers and and

20
00:00:54,800 --> 00:00:57,000
refrigerators and applications and all

21
00:00:57,000 --> 00:00:58,860
sorts of things to provide try to

22
00:00:58,860 --> 00:01:00,239
provide information to the marketplace

23
00:01:00,239 --> 00:01:02,640
about what works what is the most

24
00:01:02,640 --> 00:01:05,540
effective and fast and energy efficient

25
00:01:05,540 --> 00:01:07,619
and dangerous

26
00:01:07,619 --> 00:01:09,900
um a product in order to kind of help

27
00:01:09,900 --> 00:01:13,020
consumers to make informed choices for

28
00:01:13,020 --> 00:01:15,060
the last several years we've been

29
00:01:15,060 --> 00:01:17,159
testing products more and more for

30
00:01:17,159 --> 00:01:20,520
attributes like privacy and security

31
00:01:20,520 --> 00:01:23,040
um you know uh Smart TVs now other

32
00:01:23,040 --> 00:01:24,900
connected devices have you know they're

33
00:01:24,900 --> 00:01:26,460
connected to the internet they have the

34
00:01:26,460 --> 00:01:27,960
Run by software

35
00:01:27,960 --> 00:01:29,340
um they have they do have the potential

36
00:01:29,340 --> 00:01:31,740
to present privacy and security concerns

37
00:01:31,740 --> 00:01:34,860
to folks um we started back in 2015 we

38
00:01:34,860 --> 00:01:36,720
were reviewing fertility apps we found

39
00:01:36,720 --> 00:01:39,600
an app that was exposing a sensitive

40
00:01:39,600 --> 00:01:41,400
fertility data with a number of

41
00:01:41,400 --> 00:01:43,380
vulnerabilities tried to work with the

42
00:01:43,380 --> 00:01:45,900
company to to address them and since

43
00:01:45,900 --> 00:01:48,360
then have been trying to formalize our

44
00:01:48,360 --> 00:01:51,479
our our Research into ratings and so now

45
00:01:51,479 --> 00:01:53,520
when you look to buy a smart TV we have

46
00:01:53,520 --> 00:01:54,840
which has like the brightest screen

47
00:01:54,840 --> 00:01:57,240
which has the best sound and here's how

48
00:01:57,240 --> 00:01:59,880
to set up your the the the settings uh

49
00:01:59,880 --> 00:02:01,020
but we increase it now we have a score

50
00:02:01,020 --> 00:02:02,460
for privacy which ones are good on

51
00:02:02,460 --> 00:02:05,700
privacy which ones are good on security

52
00:02:05,700 --> 00:02:06,659
um and then we're trying to expand that

53
00:02:06,659 --> 00:02:07,799
into looking to other other things

54
00:02:07,799 --> 00:02:10,619
looking into auditing AI systems to see

55
00:02:10,619 --> 00:02:12,900
what works where there may be potential

56
00:02:12,900 --> 00:02:14,340
bias and so we've done a number of

57
00:02:14,340 --> 00:02:16,080
investigative journalism stories looking

58
00:02:16,080 --> 00:02:17,580
in uh things

59
00:02:17,580 --> 00:02:19,260
um like pulse oximeters and insurance

60
00:02:19,260 --> 00:02:21,000
pricing that shows like you know there

61
00:02:21,000 --> 00:02:22,860
are assist times when systems lead to

62
00:02:22,860 --> 00:02:25,379
unfair results for people

63
00:02:25,379 --> 00:02:26,940
um but as I think most the folks in this

64
00:02:26,940 --> 00:02:29,220
room know testing external systems or

65
00:02:29,220 --> 00:02:31,440
adversarial testing is really hard you

66
00:02:31,440 --> 00:02:32,819
often have a very used limited

67
00:02:32,819 --> 00:02:34,140
visibility into what you're looking into

68
00:02:34,140 --> 00:02:35,940
you can see this is what we see in the

69
00:02:35,940 --> 00:02:38,040
labs is this what the rest of the world

70
00:02:38,040 --> 00:02:40,800
is seeing um plus like obviously

71
00:02:40,800 --> 00:02:42,239
um everything's run by software now so

72
00:02:42,239 --> 00:02:44,340
like with a software update it can

73
00:02:44,340 --> 00:02:46,140
change radically and so traditionally

74
00:02:46,140 --> 00:02:48,300
Consumer Reports would put our ratings

75
00:02:48,300 --> 00:02:50,280
in a magazine but then if the next day

76
00:02:50,280 --> 00:02:52,620
like the the system got an update um

77
00:02:52,620 --> 00:02:54,360
think of Teslas Teslas are constantly

78
00:02:54,360 --> 00:02:57,480
pushing updates to the to the system may

79
00:02:57,480 --> 00:02:58,980
dratically change how the system works

80
00:02:58,980 --> 00:03:00,660
it may now have self-driving and they

81
00:03:00,660 --> 00:03:02,160
have dangerous self-driving it may have

82
00:03:02,160 --> 00:03:04,319
a governor to say only goes 55 miles per

83
00:03:04,319 --> 00:03:04,980
hour

84
00:03:04,980 --> 00:03:07,080
and so how do we design our systems to

85
00:03:07,080 --> 00:03:09,660
account for changes in software

86
00:03:09,660 --> 00:03:11,280
um this talk is derived largely from a

87
00:03:11,280 --> 00:03:13,680
paper called opening black boxes which

88
00:03:13,680 --> 00:03:15,360
is helpfully hyperlinked there for

89
00:03:15,360 --> 00:03:17,760
everybody maybe someone can put in the

90
00:03:17,760 --> 00:03:19,860
chat the the paper this is the paper my

91
00:03:19,860 --> 00:03:22,140
colleague Nandita sampath wrote for us

92
00:03:22,140 --> 00:03:24,599
last year after sending a a several

93
00:03:24,599 --> 00:03:26,159
months kind of looking at some like the

94
00:03:26,159 --> 00:03:29,400
the legal impediments to AI testing she

95
00:03:29,400 --> 00:03:30,420
was supposed to deliver this talk

96
00:03:30,420 --> 00:03:33,300
unfortunately she took a different job a

97
00:03:33,300 --> 00:03:35,220
couple weeks ago doing full-time AI

98
00:03:35,220 --> 00:03:36,900
testing so so good for her and it's good

99
00:03:36,900 --> 00:03:39,239
for the people she's testing but now I'm

100
00:03:39,239 --> 00:03:40,980
I'm backing up for her

101
00:03:40,980 --> 00:03:44,000
um one caveat is I'm not a technologist

102
00:03:44,000 --> 00:03:46,200
I have a legal and policy background

103
00:03:46,200 --> 00:03:47,940
I've done a lot of work on on privacy

104
00:03:47,940 --> 00:03:49,560
testing specifically but I know that I'm

105
00:03:49,560 --> 00:03:51,180
talking to a room full of people who've

106
00:03:51,180 --> 00:03:53,180
done more rigorous testing than I have

107
00:03:53,180 --> 00:03:55,680
but that's one of the cool things about

108
00:03:55,680 --> 00:03:56,940
Enigma conference that is

109
00:03:56,940 --> 00:03:59,519
multi-disciplinary so I spent a lot of

110
00:03:59,519 --> 00:04:01,200
time with the policy issues and the

111
00:04:01,200 --> 00:04:02,819
policy makers who are starting to think

112
00:04:02,819 --> 00:04:04,500
about this from a less sophisticated

113
00:04:04,500 --> 00:04:06,599
perspective so I say something that

114
00:04:06,599 --> 00:04:10,200
seems wrong or stupid or underinformed

115
00:04:10,200 --> 00:04:11,939
please let me know and tell you like no

116
00:04:11,939 --> 00:04:13,260
if not what we're worried about this is

117
00:04:13,260 --> 00:04:15,480
what we're actually worried about

118
00:04:15,480 --> 00:04:17,339
um and so like why why is public

119
00:04:17,339 --> 00:04:19,738
interest AI testing important obviously

120
00:04:19,738 --> 00:04:22,260
like AI affects more and more of our

121
00:04:22,260 --> 00:04:24,360
day-to-day lives things like credit

122
00:04:24,360 --> 00:04:26,280
scoring we're traditionally very stupid

123
00:04:26,280 --> 00:04:28,259
algorithms now we're seeing startups who

124
00:04:28,259 --> 00:04:29,699
are doing more and more sophisticated

125
00:04:29,699 --> 00:04:31,680
looking at more attributes and maybe

126
00:04:31,680 --> 00:04:33,900
looking into systems in less explainable

127
00:04:33,900 --> 00:04:36,180
ways there are tenant screening

128
00:04:36,180 --> 00:04:37,800
algorithms there are hiring algorithms

129
00:04:37,800 --> 00:04:39,240
all of which have an effect on our

130
00:04:39,240 --> 00:04:41,699
day-to-day lives the things that we're

131
00:04:41,699 --> 00:04:42,960
probably most interested in testing

132
00:04:42,960 --> 00:04:45,120
though they probably there's maybe um

133
00:04:45,120 --> 00:04:46,620
maybe there may be more things they're

134
00:04:46,620 --> 00:04:48,540
not thinking about with one you know

135
00:04:48,540 --> 00:04:50,460
bias are there bias and systems that are

136
00:04:50,460 --> 00:04:52,620
leading to biased results Amazon

137
00:04:52,620 --> 00:04:55,380
famously had a test a hiring algorithm

138
00:04:55,380 --> 00:04:57,960
that evaluated resumes and it turns out

139
00:04:57,960 --> 00:05:00,360
that guys named Chad who played lacrosse

140
00:05:00,360 --> 00:05:02,820
were ranked highest potentially like

141
00:05:02,820 --> 00:05:05,699
reflecting historical biases but then

142
00:05:05,699 --> 00:05:07,199
also Effectiveness do these things work

143
00:05:07,199 --> 00:05:09,540
there's a ton of AI snake oil out there

144
00:05:09,540 --> 00:05:11,340
companies like promising this this will

145
00:05:11,340 --> 00:05:13,020
this will do everything

146
00:05:13,020 --> 00:05:14,340
um you know I think if you're making a

147
00:05:14,340 --> 00:05:16,080
promise about cell driving does that

148
00:05:16,080 --> 00:05:17,580
really work how do we test for that and

149
00:05:17,580 --> 00:05:19,979
tell people no and maybe it works in

150
00:05:19,979 --> 00:05:21,660
this in the track that does not work in

151
00:05:21,660 --> 00:05:24,000
these other environments obviously in a

152
00:05:24,000 --> 00:05:25,199
suffering in a perfect world

153
00:05:25,199 --> 00:05:26,940
self-regulation would do it but I think

154
00:05:26,940 --> 00:05:28,440
obviously that people do not have

155
00:05:28,440 --> 00:05:29,940
sufficient motivations

156
00:05:29,940 --> 00:05:31,560
um even if they were

157
00:05:31,560 --> 00:05:34,139
um maybe they're hard to see again these

158
00:05:34,139 --> 00:05:35,580
are complicated systems where it's hard

159
00:05:35,580 --> 00:05:37,380
to tell what's going on all the time

160
00:05:37,380 --> 00:05:39,360
uh and then finally when I say auditing

161
00:05:39,360 --> 00:05:42,000
like I don't mean like KPMG I mean like

162
00:05:42,000 --> 00:05:44,340
all of us like anyone else outside the

163
00:05:44,340 --> 00:05:46,080
company you know we need people who are

164
00:05:46,080 --> 00:05:47,880
able to kick the tires

165
00:05:47,880 --> 00:05:50,940
um Regulators journalists testers like

166
00:05:50,940 --> 00:05:53,460
us researchers academics to kind of hold

167
00:05:53,460 --> 00:05:56,759
people to account for what they do

168
00:05:56,759 --> 00:05:58,139
um so quickly just a couple kind of

169
00:05:58,139 --> 00:06:00,600
famous examples of AI auditing systems

170
00:06:00,600 --> 00:06:03,720
NYU had a system called ad Observatory

171
00:06:03,720 --> 00:06:04,560
um it was something that you could

172
00:06:04,560 --> 00:06:06,300
install into your browser it would it

173
00:06:06,300 --> 00:06:07,919
would whenever you went to Facebook and

174
00:06:07,919 --> 00:06:10,080
I think YouTube it would you know see

175
00:06:10,080 --> 00:06:11,880
collect political content that you were

176
00:06:11,880 --> 00:06:13,199
collecting there as well as information

177
00:06:13,199 --> 00:06:14,880
about why it may have been targeted to

178
00:06:14,880 --> 00:06:16,440
you to try to get a sense of like what's

179
00:06:16,440 --> 00:06:18,120
going on like why are you getting this

180
00:06:18,120 --> 00:06:19,500
certain content

181
00:06:19,500 --> 00:06:21,000
um eventually Facebook actually

182
00:06:21,000 --> 00:06:24,000
deprecated access to to ad Observatory

183
00:06:24,000 --> 00:06:25,979
they were under a consent decree with

184
00:06:25,979 --> 00:06:27,479
the Federal Trade Commission for letting

185
00:06:27,479 --> 00:06:28,919
companies like Cambridge analytica

186
00:06:28,919 --> 00:06:31,080
third-party software access systems they

187
00:06:31,080 --> 00:06:33,479
said if we can't let Cambridge lydica do

188
00:06:33,479 --> 00:06:35,880
it can we let NYU Do It

189
00:06:35,880 --> 00:06:37,380
um but I think people recognize like the

190
00:06:37,380 --> 00:06:39,180
the value to knowing why we're getting

191
00:06:39,180 --> 00:06:41,759
targeted with certain political content

192
00:06:41,759 --> 00:06:43,380
and then another example and there are

193
00:06:43,380 --> 00:06:44,639
dozens of examples

194
00:06:44,639 --> 00:06:46,800
um pro-publica a few years ago did a

195
00:06:46,800 --> 00:06:49,199
story on on sentencing algorithms these

196
00:06:49,199 --> 00:06:52,020
are the real algorithms that judges were

197
00:06:52,020 --> 00:06:53,880
using to decide whether to send to them

198
00:06:53,880 --> 00:06:55,319
back to jail or to let them out early

199
00:06:55,319 --> 00:06:57,479
they looked at the scores that people

200
00:06:57,479 --> 00:06:59,340
were given and then looked at um later

201
00:06:59,340 --> 00:07:03,300
versus recidivism rates and saw that it

202
00:07:03,300 --> 00:07:05,039
was leading to biased results black

203
00:07:05,039 --> 00:07:07,020
defendants were incorrectly identified

204
00:07:07,020 --> 00:07:09,120
as more likely to to commit crimes going

205
00:07:09,120 --> 00:07:10,139
forward

206
00:07:10,139 --> 00:07:11,280
um white defendants were getting

207
00:07:11,280 --> 00:07:13,139
unfairly high scores or low scores

208
00:07:13,139 --> 00:07:14,400
depending on how you look at it saying

209
00:07:14,400 --> 00:07:15,720
they were less likely than they actually

210
00:07:15,720 --> 00:07:18,120
were to commit crime so just a couple of

211
00:07:18,120 --> 00:07:20,160
examples of important ways that AI

212
00:07:20,160 --> 00:07:23,819
systems are intermediating our lives

213
00:07:23,819 --> 00:07:25,139
um I want to say a couple of ways about

214
00:07:25,139 --> 00:07:27,000
different ways to test AI systems this

215
00:07:27,000 --> 00:07:28,740
comes from these are these um this

216
00:07:28,740 --> 00:07:30,000
formulation comes from the paper that

217
00:07:30,000 --> 00:07:31,440
Christian sandvig put out a couple years

218
00:07:31,440 --> 00:07:33,240
ago about about different ways to test

219
00:07:33,240 --> 00:07:34,860
systems

220
00:07:34,860 --> 00:07:36,599
um so code like you can get access to

221
00:07:36,599 --> 00:07:39,479
the code and and and be able to look at

222
00:07:39,479 --> 00:07:40,860
everything there

223
00:07:40,860 --> 00:07:42,900
um one example from recent years is

224
00:07:42,900 --> 00:07:43,860
Twitter

225
00:07:43,860 --> 00:07:45,660
um their face cropping algorithm was

226
00:07:45,660 --> 00:07:47,699
alleged to have bias results if you

227
00:07:47,699 --> 00:07:49,199
posted a picture of

228
00:07:49,199 --> 00:07:51,419
um one two different people the the

229
00:07:51,419 --> 00:07:52,979
cropping algorithm would tend to

230
00:07:52,979 --> 00:07:55,139
gravitate toward the lighter face

231
00:07:55,139 --> 00:07:56,639
um leading the the white people were

232
00:07:56,639 --> 00:07:59,220
more likely to be shown in feed um than

233
00:07:59,220 --> 00:08:01,080
darker skinned people and Twitter to

234
00:08:01,080 --> 00:08:02,400
their credit made the algorithm

235
00:08:02,400 --> 00:08:04,620
available and said okay find out find

236
00:08:04,620 --> 00:08:06,840
out what what run wrong here

237
00:08:06,840 --> 00:08:08,460
um can be helpful in some cases it may

238
00:08:08,460 --> 00:08:10,259
not be it may be too overwhelming if

239
00:08:10,259 --> 00:08:12,300
Consumer Reports had access to Google's

240
00:08:12,300 --> 00:08:13,979
all of Google's search algorithms I'm

241
00:08:13,979 --> 00:08:15,840
not sure we can make sense of it

242
00:08:15,840 --> 00:08:17,340
um and also even if you have the code

243
00:08:17,340 --> 00:08:18,960
like without the training data it may

244
00:08:18,960 --> 00:08:21,000
not be worthwhile right the code it just

245
00:08:21,000 --> 00:08:22,740
is if the code is learning from training

246
00:08:22,740 --> 00:08:25,160
data that does reflect historical biases

247
00:08:25,160 --> 00:08:27,180
you may not be able to see that unless

248
00:08:27,180 --> 00:08:29,840
you have access to that data as well

249
00:08:29,840 --> 00:08:31,860
crowdsourcing again if you can recruit

250
00:08:31,860 --> 00:08:33,240
people to kind of show this is my

251
00:08:33,240 --> 00:08:34,500
experience with this algorithm and they

252
00:08:34,500 --> 00:08:35,940
can kind of bring that all together to

253
00:08:35,940 --> 00:08:36,779
see

254
00:08:36,779 --> 00:08:38,159
um you know to maybe try to find some

255
00:08:38,159 --> 00:08:40,140
truth Consumer Reports has done a ton of

256
00:08:40,140 --> 00:08:41,640
this like I mentioned we've done studies

257
00:08:41,640 --> 00:08:43,799
on insurance insurance pricing that

258
00:08:43,799 --> 00:08:45,899
showed that in traditionally redlined

259
00:08:45,899 --> 00:08:47,820
areas people were getting artificially

260
00:08:47,820 --> 00:08:49,920
higher prices would even win accounting

261
00:08:49,920 --> 00:08:52,740
for other criteria but this can be hard

262
00:08:52,740 --> 00:08:54,480
to do at scale it's hard to collect and

263
00:08:54,480 --> 00:08:56,279
normalize the data if you're collecting

264
00:08:56,279 --> 00:08:58,260
data from 15 different companies their

265
00:08:58,260 --> 00:09:00,480
bills all look differently it's kind of

266
00:09:00,480 --> 00:09:02,279
challenging to do it in practice it may

267
00:09:02,279 --> 00:09:04,800
also if you're people who volunteer all

268
00:09:04,800 --> 00:09:06,300
look a certain way it may not be

269
00:09:06,300 --> 00:09:09,899
representative of the general population

270
00:09:09,899 --> 00:09:11,279
um scraping again there's a ton of

271
00:09:11,279 --> 00:09:13,200
publicly available out data out there

272
00:09:13,200 --> 00:09:15,480
and why you add Observatory is an

273
00:09:15,480 --> 00:09:17,820
example of a kind of combo crowdsourced

274
00:09:17,820 --> 00:09:19,019
you would install the software but then

275
00:09:19,019 --> 00:09:21,420
it would automatically scrape for you

276
00:09:21,420 --> 00:09:23,279
um again but are you allowed to scrape

277
00:09:23,279 --> 00:09:25,440
then you start to run into potential um

278
00:09:25,440 --> 00:09:27,660
uh uh legal issues I'm going to talk

279
00:09:27,660 --> 00:09:28,740
about in a second

280
00:09:28,740 --> 00:09:31,019
and then sock puppet accounts you can

281
00:09:31,019 --> 00:09:32,339
create a bunch of fake accounts and then

282
00:09:32,339 --> 00:09:33,779
you can make them look however you want

283
00:09:33,779 --> 00:09:35,880
they can reflect whatever demographics

284
00:09:35,880 --> 00:09:38,279
you're trying to trying to test this may

285
00:09:38,279 --> 00:09:40,620
be tough to do at scale can you create a

286
00:09:40,620 --> 00:09:42,540
hundred thousand ten thousand in order

287
00:09:42,540 --> 00:09:45,360
to get scientifically legitimate results

288
00:09:45,360 --> 00:09:47,519
and then will a company let you create

289
00:09:47,519 --> 00:09:49,920
10 000 fake accounts from one IP address

290
00:09:49,920 --> 00:09:51,959
right there they have legitimate reasons

291
00:09:51,959 --> 00:09:54,180
to stop inauthentic activity and so you

292
00:09:54,180 --> 00:09:55,860
may not be able to generate the the the

293
00:09:55,860 --> 00:09:58,500
the data you need or the inputs you need

294
00:09:58,500 --> 00:10:00,480
to test the system

295
00:10:00,480 --> 00:10:01,800
and so what does this all have to do

296
00:10:01,800 --> 00:10:03,600
with security testing and so like some

297
00:10:03,600 --> 00:10:05,339
of the laws that have like made us

298
00:10:05,339 --> 00:10:06,839
nervous as we're doing testing on

299
00:10:06,839 --> 00:10:08,040
security

300
00:10:08,040 --> 00:10:09,959
um you know have also like hindered us

301
00:10:09,959 --> 00:10:11,880
as we're thinking about how to test AI

302
00:10:11,880 --> 00:10:13,440
systems and so

303
00:10:13,440 --> 00:10:15,120
um the Computer Fraud and Abuse Act um

304
00:10:15,120 --> 00:10:17,100
famously you know limits a computer

305
00:10:17,100 --> 00:10:19,920
hacking if I'm accessing a system in

306
00:10:19,920 --> 00:10:21,540
ways that are slightly different from

307
00:10:21,540 --> 00:10:23,399
the way that the system is presented to

308
00:10:23,399 --> 00:10:25,380
the world is that hacking does that

309
00:10:25,380 --> 00:10:27,180
invite criminal liability

310
00:10:27,180 --> 00:10:28,440
um there's been some helpful cases on

311
00:10:28,440 --> 00:10:29,820
this in recent years I'm going to talk

312
00:10:29,820 --> 00:10:31,440
about in a second but there's also state

313
00:10:31,440 --> 00:10:33,839
laws that are often vagor and more

314
00:10:33,839 --> 00:10:35,339
confusing and there's not a lot of law

315
00:10:35,339 --> 00:10:37,800
interpreting what they do

316
00:10:37,800 --> 00:10:40,080
um contract law okay even if violating a

317
00:10:40,080 --> 00:10:43,019
contract is not hacking is there a legal

318
00:10:43,019 --> 00:10:44,880
recourse if the if the if the contract

319
00:10:44,880 --> 00:10:46,800
says if you violate this you owe me a

320
00:10:46,800 --> 00:10:48,120
hundred thousand dollars in liquidated

321
00:10:48,120 --> 00:10:50,279
damages is that going to scare me off as

322
00:10:50,279 --> 00:10:52,200
a researcher if they prohibit fake

323
00:10:52,200 --> 00:10:53,160
accounts

324
00:10:53,160 --> 00:10:55,920
um am I liable now or potential um can

325
00:10:55,920 --> 00:10:57,480
be sued for that

326
00:10:57,480 --> 00:10:58,980
is there a Prohibition on publishing

327
00:10:58,980 --> 00:11:01,200
testing results there's been a long

328
00:11:01,200 --> 00:11:03,600
history of software companies say no you

329
00:11:03,600 --> 00:11:05,519
may not publish results about us in in

330
00:11:05,519 --> 00:11:08,700
in their uh uh legal agreements they've

331
00:11:08,700 --> 00:11:10,620
traditionally been called Oracle Clauses

332
00:11:10,620 --> 00:11:12,899
uh oracles one of the first companies to

333
00:11:12,899 --> 00:11:14,519
innovate in this area

334
00:11:14,519 --> 00:11:17,760
um or uh or or you you can task but you

335
00:11:17,760 --> 00:11:20,040
have to come to us for permission let us

336
00:11:20,040 --> 00:11:21,660
complain and whine to you first about

337
00:11:21,660 --> 00:11:23,940
your about your results we've also seen

338
00:11:23,940 --> 00:11:26,279
companies um say you you assign us a

339
00:11:26,279 --> 00:11:28,200
copyright in any test results you put

340
00:11:28,200 --> 00:11:30,060
out about us and therefore if we don't

341
00:11:30,060 --> 00:11:33,120
like it we can take it down so a lot of

342
00:11:33,120 --> 00:11:34,200
different ways to kind of get to that

343
00:11:34,200 --> 00:11:36,360
and then um is an open legal question

344
00:11:36,360 --> 00:11:38,519
about robots thought text this is the

345
00:11:38,519 --> 00:11:40,560
signal a website can kind of publish the

346
00:11:40,560 --> 00:11:42,899
world saying don't scrape me if I

347
00:11:42,899 --> 00:11:44,820
violate that is that illegal I think

348
00:11:44,820 --> 00:11:46,500
it's an open question

349
00:11:46,500 --> 00:11:48,660
um the dmca uh had the provision like

350
00:11:48,660 --> 00:11:50,760
about circumventing anti-copying

351
00:11:50,760 --> 00:11:53,700
measures if I again may be useful for

352
00:11:53,700 --> 00:11:56,459
testing um again intended for limiting

353
00:11:56,459 --> 00:11:57,899
copyright abuse

354
00:11:57,899 --> 00:11:59,760
um but does on his face apply to a lot

355
00:11:59,760 --> 00:12:03,420
of basic security uh testing uh systems

356
00:12:03,420 --> 00:12:04,620
and then obviously there are practical

357
00:12:04,620 --> 00:12:06,240
challenges like access how do I get

358
00:12:06,240 --> 00:12:09,060
enough data to have informed informed uh

359
00:12:09,060 --> 00:12:11,060
results about how the system works for

360
00:12:11,060 --> 00:12:13,800
representative population

361
00:12:13,800 --> 00:12:15,180
and then this is one I'm really

362
00:12:15,180 --> 00:12:17,700
interested in like actions to evade

363
00:12:17,700 --> 00:12:19,560
testing if the system thinks it's being

364
00:12:19,560 --> 00:12:21,779
tested and behaves differently is that

365
00:12:21,779 --> 00:12:23,040
legal

366
00:12:23,040 --> 00:12:24,600
um famously like Volkswagen like when

367
00:12:24,600 --> 00:12:25,860
they're when they when Volkswagen

368
00:12:25,860 --> 00:12:27,420
thought they were in the EPA testing lab

369
00:12:27,420 --> 00:12:29,100
that they like shutter down their

370
00:12:29,100 --> 00:12:31,019
emissions like they put out like lower

371
00:12:31,019 --> 00:12:33,120
NO2 and SO2

372
00:12:33,120 --> 00:12:35,100
um and then they got in trouble for that

373
00:12:35,100 --> 00:12:36,300
but that's mostly because they promised

374
00:12:36,300 --> 00:12:37,440
there were low emissions and that was

375
00:12:37,440 --> 00:12:38,700
not true

376
00:12:38,700 --> 00:12:39,899
um there's also been like a long history

377
00:12:39,899 --> 00:12:42,600
of people thinking that cell phones when

378
00:12:42,600 --> 00:12:44,279
they are going undergoing common

379
00:12:44,279 --> 00:12:46,860
benchmarking tests might run a little

380
00:12:46,860 --> 00:12:47,940
bit hotter or do a little bit

381
00:12:47,940 --> 00:12:49,260
differently in order to make it look

382
00:12:49,260 --> 00:12:50,459
like they're more effective than they

383
00:12:50,459 --> 00:12:52,440
actually are in a production environment

384
00:12:52,440 --> 00:12:54,540
and then there's a story from a few

385
00:12:54,540 --> 00:12:56,100
years ago Uber

386
00:12:56,100 --> 00:12:58,560
um they configured allegedly configured

387
00:12:58,560 --> 00:13:00,060
their software that when they were being

388
00:13:00,060 --> 00:13:02,160
tested by San Francisco or California

389
00:13:02,160 --> 00:13:04,560
Regulators I'm not sure they would

390
00:13:04,560 --> 00:13:05,760
behave differently they would give them

391
00:13:05,760 --> 00:13:07,620
better rates or do something differently

392
00:13:07,620 --> 00:13:10,079
um but again the companies software

393
00:13:10,079 --> 00:13:12,839
smarter software again can adapt if it

394
00:13:12,839 --> 00:13:14,700
can adapt to testing environments they

395
00:13:14,700 --> 00:13:15,899
make it harder for us to provide

396
00:13:15,899 --> 00:13:18,600
detailed information to the public

397
00:13:18,600 --> 00:13:20,220
that said I'd like to think we have this

398
00:13:20,220 --> 00:13:22,019
history of security researching and like

399
00:13:22,019 --> 00:13:23,220
things have gotten like the policy

400
00:13:23,220 --> 00:13:24,720
environment have adapted in a number of

401
00:13:24,720 --> 00:13:27,120
ways to make things better to

402
00:13:27,120 --> 00:13:28,800
um again I think maybe 20 years ago like

403
00:13:28,800 --> 00:13:30,600
the anti-hacking statues they're they're

404
00:13:30,600 --> 00:13:32,100
a little bit more scary

405
00:13:32,100 --> 00:13:34,079
um now policymakers are adapting to try

406
00:13:34,079 --> 00:13:36,540
to allow for people to to hold these

407
00:13:36,540 --> 00:13:38,459
companies to account so there have been

408
00:13:38,459 --> 00:13:39,899
some really strong cases interpreting

409
00:13:39,899 --> 00:13:42,420
the Computer Fraud and Abuse Act

410
00:13:42,420 --> 00:13:44,040
um same with the bar was a case where

411
00:13:44,040 --> 00:13:46,139
research academics were creating stock

412
00:13:46,139 --> 00:13:48,300
puppet accounts in violation of terms of

413
00:13:48,300 --> 00:13:49,920
service the court said no that's not

414
00:13:49,920 --> 00:13:51,660
hacking that's just a violation of the

415
00:13:51,660 --> 00:13:54,000
contract maybe May violate other things

416
00:13:54,000 --> 00:13:57,480
but it isn't um criminal uh Van Buren

417
00:13:57,480 --> 00:13:59,160
versus the United States was it was a

418
00:13:59,160 --> 00:14:01,320
Supreme Court case from just last term

419
00:14:01,320 --> 00:14:03,959
uh in that case I think Van Buren was a

420
00:14:03,959 --> 00:14:06,600
police officer in Georgia and he had

421
00:14:06,600 --> 00:14:08,700
access to a police database that he was

422
00:14:08,700 --> 00:14:10,380
accessing it for super sketchy reasons

423
00:14:10,380 --> 00:14:12,899
to access like friends girlfriends

424
00:14:12,899 --> 00:14:14,100
um in that case the court said again

425
00:14:14,100 --> 00:14:15,540
it's not hacking he had access to the

426
00:14:15,540 --> 00:14:16,620
system

427
00:14:16,620 --> 00:14:18,959
um so um just exceeded it that's not

428
00:14:18,959 --> 00:14:20,880
criminal hacking so again like makes

429
00:14:20,880 --> 00:14:22,740
security researchers who may have access

430
00:14:22,740 --> 00:14:24,180
to a system but are doing slightly

431
00:14:24,180 --> 00:14:26,220
different things to it less likely to be

432
00:14:26,220 --> 00:14:28,560
charged with with criminal hacking and

433
00:14:28,560 --> 00:14:30,899
then last year the Department of Justice

434
00:14:30,899 --> 00:14:33,120
put uh put out a policy statement said

435
00:14:33,120 --> 00:14:35,100
they're not going to prosecute people

436
00:14:35,100 --> 00:14:36,839
for doing good faith security hacking

437
00:14:36,839 --> 00:14:38,519
under the Consumer Fraud and Abuse Act

438
00:14:38,519 --> 00:14:40,920
again like helpful could be rescinded

439
00:14:40,920 --> 00:14:42,600
doesn't apply to state laws but again

440
00:14:42,600 --> 00:14:44,519
this I think progress only applies to

441
00:14:44,519 --> 00:14:46,680
security hacking hopefully they would

442
00:14:46,680 --> 00:14:48,899
also interpret it as applying um to to

443
00:14:48,899 --> 00:14:50,880
Ai and other sorts of public interest

444
00:14:50,880 --> 00:14:53,579
good faith testing as well

445
00:14:53,579 --> 00:14:55,500
um the dmca like I said like the if you

446
00:14:55,500 --> 00:14:57,360
if you circumvent anti-copying

447
00:14:57,360 --> 00:14:59,040
Technologies you can get in trouble but

448
00:14:59,040 --> 00:15:01,380
but they do provide for exceptions

449
00:15:01,380 --> 00:15:03,000
um the Library of Congress I don't know

450
00:15:03,000 --> 00:15:04,260
why it's a Library of Congress every

451
00:15:04,260 --> 00:15:06,180
three years they put out exceptions that

452
00:15:06,180 --> 00:15:08,579
it's okay to do to circumvent for these

453
00:15:08,579 --> 00:15:10,500
purposes and for the last several rounds

454
00:15:10,500 --> 00:15:12,240
they have identified good faith security

455
00:15:12,240 --> 00:15:14,940
testing at the exception um and that

456
00:15:14,940 --> 00:15:17,880
doesn't violate the dmca

457
00:15:17,880 --> 00:15:19,440
um there's also been some good uh

458
00:15:19,440 --> 00:15:21,300
developments in Consumer Protection Law

459
00:15:21,300 --> 00:15:22,860
in the last few years so I mentioned

460
00:15:22,860 --> 00:15:25,079
Oracle Clauses Clauses that are limiting

461
00:15:25,079 --> 00:15:27,060
people's ability to test

462
00:15:27,060 --> 00:15:28,740
um uh State Attorneys General have gone

463
00:15:28,740 --> 00:15:29,940
after companies for putting those

464
00:15:29,940 --> 00:15:31,980
Clauses in their their contracts and so

465
00:15:31,980 --> 00:15:33,839
McAfee and blue coat are two examples

466
00:15:33,839 --> 00:15:36,420
where a state attorney general said no

467
00:15:36,420 --> 00:15:38,639
like that's not enforceable and not only

468
00:15:38,639 --> 00:15:40,560
is it not enforceable putting it in the

469
00:15:40,560 --> 00:15:42,480
contract is deceptive you're fooling

470
00:15:42,480 --> 00:15:43,860
people into think they're not allowed to

471
00:15:43,860 --> 00:15:45,600
test you but there's a public interest

472
00:15:45,600 --> 00:15:48,300
value to doing it rock labs was a

473
00:15:48,300 --> 00:15:49,560
similar case from the Federal Trade

474
00:15:49,560 --> 00:15:50,699
Commission that gives to that kind of

475
00:15:50,699 --> 00:15:52,980
copyright trick I talked about where you

476
00:15:52,980 --> 00:15:56,040
assign a copyright and any uh review or

477
00:15:56,040 --> 00:15:58,019
content about people

478
00:15:58,019 --> 00:15:59,519
um a few years ago Congress actually

479
00:15:59,519 --> 00:16:01,320
passed a law um the consumer review

480
00:16:01,320 --> 00:16:03,540
Fairness Act which says consumers have a

481
00:16:03,540 --> 00:16:07,199
right to publish a good faith reviews of

482
00:16:07,199 --> 00:16:09,480
com of companies uh it doesn't go so far

483
00:16:09,480 --> 00:16:11,040
as a right to test a right to

484
00:16:11,040 --> 00:16:12,660
necessarily kick the tires

485
00:16:12,660 --> 00:16:13,860
um but if you have an opinion you're

486
00:16:13,860 --> 00:16:15,779
allowed to say it in contractual Clauses

487
00:16:15,779 --> 00:16:18,240
can't stop you from doing that

488
00:16:18,240 --> 00:16:19,800
um I mentioned this question is testing

489
00:16:19,800 --> 00:16:23,160
a fooling a tester um deceptive or

490
00:16:23,160 --> 00:16:25,320
unfair if someone in consumer reports

491
00:16:25,320 --> 00:16:27,300
Labs operates in a different way to try

492
00:16:27,300 --> 00:16:29,339
to trick us into giving it a higher a

493
00:16:29,339 --> 00:16:31,680
better an unfair review this is an open

494
00:16:31,680 --> 00:16:33,000
question that I think is really

495
00:16:33,000 --> 00:16:35,279
important the law says you can't deceive

496
00:16:35,279 --> 00:16:36,839
the consumer it doesn't necessarily say

497
00:16:36,839 --> 00:16:38,940
you can't deceive a tester

498
00:16:38,940 --> 00:16:40,320
um the law I think the current law is

499
00:16:40,320 --> 00:16:42,120
definitely unclear on that

500
00:16:42,120 --> 00:16:44,100
um alternatively can argue is an unfair

501
00:16:44,100 --> 00:16:45,600
business practice that's the practice

502
00:16:45,600 --> 00:16:48,060
that causes harm it is not offset by

503
00:16:48,060 --> 00:16:49,680
benefits but this is an area that's not

504
00:16:49,680 --> 00:16:51,480
a lot of guidance out there for for

505
00:16:51,480 --> 00:16:53,579
testers

506
00:16:53,579 --> 00:16:55,139
um a couple other areas where I think

507
00:16:55,139 --> 00:16:56,519
we're seeing Regulators start to get

508
00:16:56,519 --> 00:16:58,139
this is not necessarily a victory yet

509
00:16:58,139 --> 00:16:59,060
but I think we're starting to see

510
00:16:59,060 --> 00:17:01,199
regulators and policy makers think about

511
00:17:01,199 --> 00:17:03,660
it is that maybe auditing mandates in

512
00:17:03,660 --> 00:17:05,760
certain cases maybe not requiring people

513
00:17:05,760 --> 00:17:08,359
to publish code but maybe for certain

514
00:17:08,359 --> 00:17:10,919
algorithms and AI systems that have like

515
00:17:10,919 --> 00:17:13,140
like important legal results you have an

516
00:17:13,140 --> 00:17:14,880
obligation to have a third party look at

517
00:17:14,880 --> 00:17:16,799
it that's something we're seeing in

518
00:17:16,799 --> 00:17:19,319
legislation the federal government and

519
00:17:19,319 --> 00:17:22,199
in DC D.C state government California

520
00:17:22,199 --> 00:17:23,880
and Washington

521
00:17:23,880 --> 00:17:25,799
um again more nascent but I think the

522
00:17:25,799 --> 00:17:26,880
things that people are starting to think

523
00:17:26,880 --> 00:17:28,079
about

524
00:17:28,079 --> 00:17:29,520
um and then whistleblower protections I

525
00:17:29,520 --> 00:17:30,540
know the next panel is talking about

526
00:17:30,540 --> 00:17:31,679
whistleblowers

527
00:17:31,679 --> 00:17:33,000
um but again like testing's got

528
00:17:33,000 --> 00:17:34,559
limitations you can't find everything

529
00:17:34,559 --> 00:17:36,840
and so having protections for insiders

530
00:17:36,840 --> 00:17:39,360
who see wrong wrongdoing

531
00:17:39,360 --> 00:17:41,580
um again a lot of the new laws that are

532
00:17:41,580 --> 00:17:43,039
thinking about AI thinking about

533
00:17:43,039 --> 00:17:45,480
transparency do have explicit

534
00:17:45,480 --> 00:17:47,460
protections for whistleblowers again not

535
00:17:47,460 --> 00:17:49,620
in effect yet but there is like a long

536
00:17:49,620 --> 00:17:51,720
history of like of of Regulation

537
00:17:51,720 --> 00:17:53,580
especially around like false claims at

538
00:17:53,580 --> 00:17:55,980
Government Contracting to protect um and

539
00:17:55,980 --> 00:17:57,840
even reward and encourage with the

540
00:17:57,840 --> 00:18:00,720
blowers in some cases

541
00:18:00,720 --> 00:18:01,919
um sorry just a couple more slides I

542
00:18:01,919 --> 00:18:03,600
know I started late apologies

543
00:18:03,600 --> 00:18:04,799
um so what can companies do in the

544
00:18:04,799 --> 00:18:05,940
meantime

545
00:18:05,940 --> 00:18:08,280
um like in some cases open sourcing uh

546
00:18:08,280 --> 00:18:10,860
again may be helpful

547
00:18:10,860 --> 00:18:12,600
um I've certainly heard Regulators talk

548
00:18:12,600 --> 00:18:14,220
about that if you do open source your

549
00:18:14,220 --> 00:18:16,020
coding it seems like a good show a show

550
00:18:16,020 --> 00:18:17,400
of good faith that you're trying to

551
00:18:17,400 --> 00:18:19,620
avoid bias or you're trying to to

552
00:18:19,620 --> 00:18:21,780
generate Fair results

553
00:18:21,780 --> 00:18:23,039
um again we've learned a lot from

554
00:18:23,039 --> 00:18:24,840
security bug Bounty programs over the

555
00:18:24,840 --> 00:18:27,600
last 10 more years

556
00:18:27,600 --> 00:18:29,340
um you know I mentioned the example of

557
00:18:29,340 --> 00:18:30,900
Twitter and the and the cropping

558
00:18:30,900 --> 00:18:33,059
algorithm as a algorithmic bug Bounty

559
00:18:33,059 --> 00:18:35,160
program could be useful to see more of

560
00:18:35,160 --> 00:18:37,679
that in the wild and then finally you

561
00:18:37,679 --> 00:18:39,720
know what can people who want to see a

562
00:18:39,720 --> 00:18:41,940
better policy environment for testers

563
00:18:41,940 --> 00:18:44,100
can do like I said like policy makers

564
00:18:44,100 --> 00:18:45,900
are a lot more open to this I think you

565
00:18:45,900 --> 00:18:48,299
know 10 years ago maybe 15 years ago

566
00:18:48,299 --> 00:18:50,820
like a hacker was still like a loaded

567
00:18:50,820 --> 00:18:53,700
term and people were worried about a bad

568
00:18:53,700 --> 00:18:55,500
faith breaking of systems and I think

569
00:18:55,500 --> 00:18:58,679
prosecutors often brought um unhelpful

570
00:18:58,679 --> 00:19:01,400
and in some cases asterisk prosecutions

571
00:19:01,400 --> 00:19:03,120
policy makers I think are different

572
00:19:03,120 --> 00:19:04,500
these days not all of them but I think

573
00:19:04,500 --> 00:19:05,940
there is progress and there is more of a

574
00:19:05,940 --> 00:19:07,679
recognition that things need to be done

575
00:19:07,679 --> 00:19:09,840
to allow for third-party testing to

576
00:19:09,840 --> 00:19:12,059
allow for accountability so there are

577
00:19:12,059 --> 00:19:13,679
like some policy vehicles that may be

578
00:19:13,679 --> 00:19:15,840
moving on this um most explicitly like

579
00:19:15,840 --> 00:19:16,980
short term

580
00:19:16,980 --> 00:19:18,720
um I mentioned the dmca every three

581
00:19:18,720 --> 00:19:21,419
years pushed out exceptions for the dmca

582
00:19:21,419 --> 00:19:23,880
where it's okay to circumvent anti-uh

583
00:19:23,880 --> 00:19:26,400
copying technology and it's up for

584
00:19:26,400 --> 00:19:28,860
review this summer so again it could be

585
00:19:28,860 --> 00:19:30,780
expanded to be from from just security

586
00:19:30,780 --> 00:19:33,179
testing to testing for other sorts of

587
00:19:33,179 --> 00:19:34,440
testing other sorts of research other

588
00:19:34,440 --> 00:19:36,660
sorts of things that can be done again

589
00:19:36,660 --> 00:19:39,000
to hold from these incrutable and

590
00:19:39,000 --> 00:19:41,460
difficult to parse black boxes

591
00:19:41,460 --> 00:19:44,340
accountable for what they do

592
00:19:44,340 --> 00:19:45,780
um with that um thank you for your

593
00:19:45,780 --> 00:19:47,820
patience and then technical delays I'm

594
00:19:47,820 --> 00:19:49,620
happy to answer uh any questions about

595
00:19:49,620 --> 00:19:50,820
this thanks

596
00:19:50,820 --> 00:19:53,909
[Applause]

