1
00:00:01,199 --> 00:00:04,199
foreign

2
00:00:16,940 --> 00:00:20,400
is widely being widely used in cyber

3
00:00:20,400 --> 00:00:22,920
security nowadays whoever is very common

4
00:00:22,920 --> 00:00:24,779
to notice some pitfalls and challenges

5
00:00:24,779 --> 00:00:26,820
that most of time are not covered in the

6
00:00:26,820 --> 00:00:27,960
literature

7
00:00:27,960 --> 00:00:30,300
I am fabricious King a PhD student at

8
00:00:30,300 --> 00:00:32,820
Federal University of Parana Brazil and

9
00:00:32,820 --> 00:00:34,440
I'm going to help you spot the

10
00:00:34,440 --> 00:00:36,660
differences and show you the quirks of

11
00:00:36,660 --> 00:00:38,700
machine learning in security of course

12
00:00:38,700 --> 00:00:40,680
we came to petite title however you want

13
00:00:40,680 --> 00:00:43,820
it's up to you

14
00:00:46,559 --> 00:00:49,020
so today a massive amount of data is

15
00:00:49,020 --> 00:00:51,120
produced and as a consequence new

16
00:00:51,120 --> 00:00:53,280
emerging threats are constantly being

17
00:00:53,280 --> 00:00:54,960
developed by attackers

18
00:00:54,960 --> 00:00:57,660
increasing the losses in the past years

19
00:00:57,660 --> 00:00:59,820
thus machine learning is considered the

20
00:00:59,820 --> 00:01:02,219
state of the art and the ideal Ally to

21
00:01:02,219 --> 00:01:04,199
build automated solutions to detect

22
00:01:04,199 --> 00:01:05,939
attack vectors

23
00:01:05,939 --> 00:01:07,500
however cyber security has many

24
00:01:07,500 --> 00:01:10,500
popularities that are not inherited from

25
00:01:10,500 --> 00:01:13,220
other fields

26
00:01:13,860 --> 00:01:16,260
okay but why is machine learning for

27
00:01:16,260 --> 00:01:18,420
security different I'll show you a

28
00:01:18,420 --> 00:01:21,540
classical example of why

29
00:01:21,540 --> 00:01:23,700
imagine a simple image classification

30
00:01:23,700 --> 00:01:26,759
problem we want to create a model to

31
00:01:26,759 --> 00:01:28,740
classify cats and logs

32
00:01:28,740 --> 00:01:30,659
this model will not change as time goes

33
00:01:30,659 --> 00:01:33,119
by cat will always be cats and so the

34
00:01:33,119 --> 00:01:34,140
dogs

35
00:01:34,140 --> 00:01:36,840
they always look the same so we do not

36
00:01:36,840 --> 00:01:38,460
need to update classification model

37
00:01:38,460 --> 00:01:41,820
every time a new dog or cat appears we

38
00:01:41,820 --> 00:01:43,380
need we just need a population that

39
00:01:43,380 --> 00:01:46,759
generalize both classes right

40
00:01:47,100 --> 00:01:49,799
in cyber security we have a very good

41
00:01:49,799 --> 00:01:51,840
example with phishing according to

42
00:01:51,840 --> 00:01:54,119
google 68 percent of fishing males

43
00:01:54,119 --> 00:01:56,460
blocked by Gmail are different from day

44
00:01:56,460 --> 00:01:58,200
to day which means that their

45
00:01:58,200 --> 00:02:00,840
distribution is non-stationary

46
00:02:00,840 --> 00:02:02,399
here you can see image from the very

47
00:02:02,399 --> 00:02:05,159
same Google Drive page updated by the

48
00:02:05,159 --> 00:02:07,320
attackers over time so they update the

49
00:02:07,320 --> 00:02:10,318
the phishing page over time to to evade

50
00:02:10,318 --> 00:02:12,239
classification to evade the the

51
00:02:12,239 --> 00:02:13,379
detection

52
00:02:13,379 --> 00:02:16,519
over time

53
00:02:17,160 --> 00:02:19,260
as a consequence of that the literature

54
00:02:19,260 --> 00:02:20,640
says that blindly applying machine

55
00:02:20,640 --> 00:02:23,340
learning evaluation best patches May

56
00:02:23,340 --> 00:02:26,720
backfire in security context

57
00:02:26,819 --> 00:02:29,700
okay but how do I know about it when I

58
00:02:29,700 --> 00:02:31,620
build my first smart detector with

59
00:02:31,620 --> 00:02:33,780
machine learning uh back in my masters

60
00:02:33,780 --> 00:02:35,640
the questions were the same as any

61
00:02:35,640 --> 00:02:38,099
machine learning task which data set to

62
00:02:38,099 --> 00:02:40,980
use what features which models what

63
00:02:40,980 --> 00:02:43,620
metrics after some research and

64
00:02:43,620 --> 00:02:46,019
experiment uh I ended up publishing a

65
00:02:46,019 --> 00:02:48,120
paper where I achieving 98 of currency

66
00:02:48,120 --> 00:02:51,260
seems good right

67
00:02:51,959 --> 00:02:54,180
years later there was a machine learning

68
00:02:54,180 --> 00:02:56,700
Envision competition with which

69
00:02:56,700 --> 00:02:59,280
objective was to submit the model that

70
00:02:59,280 --> 00:03:01,260
detects variants of more developed by

71
00:03:01,260 --> 00:03:03,599
the attackers I thought my model

72
00:03:03,599 --> 00:03:05,760
achieved 90 of accuracy of course it's

73
00:03:05,760 --> 00:03:07,019
going to attack everything in this

74
00:03:07,019 --> 00:03:10,739
challenge we are going to win right

75
00:03:10,739 --> 00:03:12,659
but it was not what happened

76
00:03:12,659 --> 00:03:14,760
my mother did not achieve the desired

77
00:03:14,760 --> 00:03:16,560
performance it did not detect the

78
00:03:16,560 --> 00:03:18,239
majority of samples developed by

79
00:03:18,239 --> 00:03:20,040
attackers and my team finished in the

80
00:03:20,040 --> 00:03:21,540
second position

81
00:03:21,540 --> 00:03:23,640
in the end it was very similar to a fast

82
00:03:23,640 --> 00:03:26,640
food ad we had an expectation and

83
00:03:26,640 --> 00:03:29,879
finished with another reality right and

84
00:03:29,879 --> 00:03:31,140
now I'm here to tell you what is

85
00:03:31,140 --> 00:03:33,060
different in cyber security and why we

86
00:03:33,060 --> 00:03:34,200
need to take care when using machine

87
00:03:34,200 --> 00:03:36,840
learning algorithms

88
00:03:36,840 --> 00:03:39,599
so let's start by talking about data set

89
00:03:39,599 --> 00:03:40,560
definition

90
00:03:40,560 --> 00:03:42,420
which is the first step to building a

91
00:03:42,420 --> 00:03:44,519
solution

92
00:03:44,519 --> 00:03:46,200
so defining a data set is a challenge

93
00:03:46,200 --> 00:03:47,940
that is normal in any machine learning

94
00:03:47,940 --> 00:03:50,280
domain but this can have severe

95
00:03:50,280 --> 00:03:52,920
implications for security so first it's

96
00:03:52,920 --> 00:03:54,860
very difficult in security because

97
00:03:54,860 --> 00:03:57,060
organizations generally do not share

98
00:03:57,060 --> 00:03:59,400
data in order to not review their own

99
00:03:59,400 --> 00:04:00,840
abilities

100
00:04:00,840 --> 00:04:04,739
on on one hand small data sets may not

101
00:04:04,739 --> 00:04:07,159
be representative of a real scenario

102
00:04:07,159 --> 00:04:09,299
which may evaluate the result of

103
00:04:09,299 --> 00:04:11,400
possible experiments using them and may

104
00:04:11,400 --> 00:04:14,040
not generalize enough the solution may

105
00:04:14,040 --> 00:04:16,019
not work as intended presenting a poor

106
00:04:16,019 --> 00:04:18,600
classification performance in practice

107
00:04:18,600 --> 00:04:21,899
and on the other hand a very large data

108
00:04:21,899 --> 00:04:23,600
set can have longer training times

109
00:04:23,600 --> 00:04:26,160
produce very complex models which can

110
00:04:26,160 --> 00:04:27,840
make the model not feasible in the real

111
00:04:27,840 --> 00:04:28,800
world

112
00:04:28,800 --> 00:04:30,300
this is very common in deep learning

113
00:04:30,300 --> 00:04:32,100
models which often need a large amount

114
00:04:32,100 --> 00:04:35,780
of data to show good results

115
00:04:36,720 --> 00:04:38,820
in the words of short box and mentioned

116
00:04:38,820 --> 00:04:40,320
that in the majority of machine learning

117
00:04:40,320 --> 00:04:42,660
books essentially all models are wrong

118
00:04:42,660 --> 00:04:44,699
but some are useful

119
00:04:44,699 --> 00:04:47,940
in our context models built using a data

120
00:04:47,940 --> 00:04:50,520
set can be useful for a given task it

121
00:04:50,520 --> 00:04:53,040
might not be perfect but we need to know

122
00:04:53,040 --> 00:04:55,380
how wrong it needs to be by selecting a

123
00:04:55,380 --> 00:04:57,840
false positive rate threshold for

124
00:04:57,840 --> 00:05:00,300
instance or any other metric

125
00:05:00,300 --> 00:05:02,580
finally it probably won't be good at

126
00:05:02,580 --> 00:05:04,919
other tasks we need to collect new data

127
00:05:04,919 --> 00:05:06,780
or select new model parameters if you

128
00:05:06,780 --> 00:05:10,400
want to use it for other tasks

129
00:05:10,740 --> 00:05:13,800
Alfred corzibiscus a very non-existing

130
00:05:13,800 --> 00:05:15,960
Theory also has an interesting code that

131
00:05:15,960 --> 00:05:19,440
says that the map is not a territory it

132
00:05:19,440 --> 00:05:22,320
is just a signal index or representation

133
00:05:22,320 --> 00:05:25,259
but it is not actually the place itself

134
00:05:25,259 --> 00:05:28,199
that is a map is subject to errors just

135
00:05:28,199 --> 00:05:29,639
like a data set

136
00:05:29,639 --> 00:05:32,220
it can represent a real scenario but it

137
00:05:32,220 --> 00:05:33,960
can only be a representation of it and

138
00:05:33,960 --> 00:05:36,919
may contain errors

139
00:05:37,080 --> 00:05:39,600
in addition we are often faced with the

140
00:05:39,600 --> 00:05:42,419
Encore bias which make us think that the

141
00:05:42,419 --> 00:05:45,300
labor data set is better making it a new

142
00:05:45,300 --> 00:05:47,759
Baseline for future comparisons as a

143
00:05:47,759 --> 00:05:49,979
good example is thinking that a data set

144
00:05:49,979 --> 00:05:52,139
with 1 million samples is better than a

145
00:05:52,139 --> 00:05:53,699
smaller data set

146
00:05:53,699 --> 00:05:56,460
whoever such a fallacy is not true

147
00:05:56,460 --> 00:05:58,680
it's not about size it's about the

148
00:05:58,680 --> 00:06:01,259
repositiveness of the real word for the

149
00:06:01,259 --> 00:06:04,860
task at hand just like a map

150
00:06:04,860 --> 00:06:08,220
so here we have the Enigma hotel map as

151
00:06:08,220 --> 00:06:11,100
an example let's say that our objective

152
00:06:11,100 --> 00:06:12,960
is to get to the converse hotel room

153
00:06:12,960 --> 00:06:15,240
from an airport nearby

154
00:06:15,240 --> 00:06:17,580
as we can note this map is great to know

155
00:06:17,580 --> 00:06:19,500
which airports are near

156
00:06:19,500 --> 00:06:21,660
so we can use this information to locate

157
00:06:21,660 --> 00:06:25,500
ourselves and get to the hotel right

158
00:06:25,500 --> 00:06:27,780
however if we use the Santa Clara

159
00:06:27,780 --> 00:06:29,900
restaurant maps for the very same

160
00:06:29,900 --> 00:06:32,520
objective it won't work and won't

161
00:06:32,520 --> 00:06:34,500
represent our problem this is very

162
00:06:34,500 --> 00:06:37,520
similar to a data set right

163
00:06:38,400 --> 00:06:40,199
to show that data set size is not

164
00:06:40,199 --> 00:06:42,840
relevant as we may think I performed two

165
00:06:42,840 --> 00:06:44,160
experiments that consider different

166
00:06:44,160 --> 00:06:46,080
proportions of the data set in both

167
00:06:46,080 --> 00:06:48,120
training and test sets

168
00:06:48,120 --> 00:06:50,940
here I use it a subset of under Zoo data

169
00:06:50,940 --> 00:06:54,120
set containing almost 300 000 samples

170
00:06:54,120 --> 00:06:56,100
the samples are randomly selected for

171
00:06:56,100 --> 00:06:57,360
each proportion

172
00:06:57,360 --> 00:06:59,639
and in the experiment one I compare

173
00:06:59,639 --> 00:07:01,680
different classifiers using a temporal

174
00:07:01,680 --> 00:07:03,020
classification

175
00:07:03,020 --> 00:07:05,400
splitting the data set into two sets

176
00:07:05,400 --> 00:07:07,560
order by time so the training set has

177
00:07:07,560 --> 00:07:09,780
the oldest samples other real solution

178
00:07:09,780 --> 00:07:11,160
should

179
00:07:11,160 --> 00:07:13,440
the results show a pattern for our

180
00:07:13,440 --> 00:07:16,560
classifiers which have an optimal

181
00:07:16,560 --> 00:07:19,380
results obtaining using only about 10

182
00:07:19,380 --> 00:07:21,479
and 20 of the data set

183
00:07:21,479 --> 00:07:24,060
an experiment too I tested different

184
00:07:24,060 --> 00:07:26,460
Evolution approach like cross validation

185
00:07:26,460 --> 00:07:28,919
temporal classification and data stream

186
00:07:28,919 --> 00:07:31,500
with and without this detection in the

187
00:07:31,500 --> 00:07:33,000
same classifier which is the random

188
00:07:33,000 --> 00:07:35,759
Forest uh all of them also show a

189
00:07:35,759 --> 00:07:38,280
pattern and stabilize the results uh

190
00:07:38,280 --> 00:07:41,699
using around 30 to 40 of the data set

191
00:07:41,699 --> 00:07:44,099
finally it's possible to say that a data

192
00:07:44,099 --> 00:07:47,520
set is not about size it's just about

193
00:07:47,520 --> 00:07:49,800
the representiveness of the real world

194
00:07:49,800 --> 00:07:52,020
for the task at hand just like a data

195
00:07:52,020 --> 00:07:56,240
just like a map once again right

196
00:07:56,819 --> 00:07:58,680
so another problem that we can face in

197
00:07:58,680 --> 00:08:02,300
cyber security is class imbalance

198
00:08:02,759 --> 00:08:05,580
by definition classes imbalance is when

199
00:08:05,580 --> 00:08:07,259
the class distribution of data set

200
00:08:07,259 --> 00:08:09,599
differs relatively by a substantial

201
00:08:09,599 --> 00:08:10,620
margin

202
00:08:10,620 --> 00:08:12,240
this problem is very common in real

203
00:08:12,240 --> 00:08:14,160
world research since it made difficult

204
00:08:14,160 --> 00:08:16,860
to find malicious samples a good example

205
00:08:16,860 --> 00:08:19,020
is the under Zoom our data set which has

206
00:08:19,020 --> 00:08:21,780
only 18 of Mars samples

207
00:08:21,780 --> 00:08:23,879
the literature has several methods to

208
00:08:23,879 --> 00:08:25,919
solve this problem but here we are going

209
00:08:25,919 --> 00:08:28,080
to focus on the preprocessing strategies

210
00:08:28,080 --> 00:08:30,120
based on resembly which consists

211
00:08:30,120 --> 00:08:32,159
basically in removing samples or

212
00:08:32,159 --> 00:08:35,179
creating new ones

213
00:08:36,000 --> 00:08:38,039
the under sampling method consists in

214
00:08:38,039 --> 00:08:39,779
removing samples from the majority class

215
00:08:39,779 --> 00:08:42,479
so consider a data set with two classes

216
00:08:42,479 --> 00:08:44,279
green and red

217
00:08:44,279 --> 00:08:46,140
with the second class containing two

218
00:08:46,140 --> 00:08:48,480
subclasses or two concepts for instance

219
00:08:48,480 --> 00:08:51,120
two Mara families that evolve over time

220
00:08:51,120 --> 00:08:52,740
the first image sold the original

221
00:08:52,740 --> 00:08:55,560
distribution of the data set without any

222
00:08:55,560 --> 00:08:57,600
resampling technique

223
00:08:57,600 --> 00:08:59,700
in the second figure a conventional

224
00:08:59,700 --> 00:09:01,620
resampling technique is used without any

225
00:09:01,620 --> 00:09:03,360
temporal information

226
00:09:03,360 --> 00:09:05,339
as we can notice the result is different

227
00:09:05,339 --> 00:09:07,500
from the reality the distribution of

228
00:09:07,500 --> 00:09:09,540
Mario families or subclasses has changed

229
00:09:09,540 --> 00:09:10,920
and no longer represents the original

230
00:09:10,920 --> 00:09:15,060
data set which can lead to a bicep model

231
00:09:15,060 --> 00:09:17,459
in an ideal scenario the under sampling

232
00:09:17,459 --> 00:09:19,080
technique should consider the temporal

233
00:09:19,080 --> 00:09:20,880
information of the data set

234
00:09:20,880 --> 00:09:22,620
keeping the very same proportion of

235
00:09:22,620 --> 00:09:24,360
subclasses from the majority class as

236
00:09:24,360 --> 00:09:26,580
time goes by resulting in a more

237
00:09:26,580 --> 00:09:28,019
realistic scenario as we can see in

238
00:09:28,019 --> 00:09:30,740
figure 3.

239
00:09:32,399 --> 00:09:34,680
the oversampling method consists in

240
00:09:34,680 --> 00:09:36,300
generating new samples from luminary

241
00:09:36,300 --> 00:09:37,260
class

242
00:09:37,260 --> 00:09:39,240
now let's consider data sets similar to

243
00:09:39,240 --> 00:09:41,519
the one presented previously but now if

244
00:09:41,519 --> 00:09:43,080
the majority class being the green one

245
00:09:43,080 --> 00:09:45,300
and the minority being read again we

246
00:09:45,300 --> 00:09:47,700
have two subclasses or concepts red and

247
00:09:47,700 --> 00:09:49,320
orange assuming the original

248
00:09:49,320 --> 00:09:51,779
distribution Universe figure

249
00:09:51,779 --> 00:09:53,700
and if you make sure we can observe a

250
00:09:53,700 --> 00:09:55,680
conventional oversampling technique that

251
00:09:55,680 --> 00:09:57,660
does not take temporal information into

252
00:09:57,660 --> 00:09:59,580
account creating new samples with

253
00:09:59,580 --> 00:10:02,160
features of both subclasses even in

254
00:10:02,160 --> 00:10:03,839
periods when the RH concept does not

255
00:10:03,839 --> 00:10:06,540
exist both orange or red white

256
00:10:06,540 --> 00:10:08,820
as notice the resulting data set no

257
00:10:08,820 --> 00:10:10,920
longer represents the original one which

258
00:10:10,920 --> 00:10:13,019
can lead again to a by the model

259
00:10:13,019 --> 00:10:15,000
so we have a problem in an ideal

260
00:10:15,000 --> 00:10:17,640
scenario where we consider the temporal

261
00:10:17,640 --> 00:10:19,640
information the oversampling technique

262
00:10:19,640 --> 00:10:21,899
should consider subclasses when

263
00:10:21,899 --> 00:10:24,300
generating new instance maintaining the

264
00:10:24,300 --> 00:10:26,880
original DSS proportion by using the

265
00:10:26,880 --> 00:10:28,320
temporary information as we can see

266
00:10:28,320 --> 00:10:31,519
again in figure 3.

267
00:10:33,540 --> 00:10:35,700
I could not present this work with optic

268
00:10:35,700 --> 00:10:39,360
talking about adversary machine learning

269
00:10:39,360 --> 00:10:41,700
so degeneration of ursar samples is a

270
00:10:41,700 --> 00:10:43,080
very common problem in machine learning

271
00:10:43,080 --> 00:10:45,839
for security because attackers are

272
00:10:45,839 --> 00:10:48,120
trying to generate a diversaries that

273
00:10:48,120 --> 00:10:50,519
looks similar to the nice samples

274
00:10:50,519 --> 00:10:52,320
in the smart detection example we can

275
00:10:52,320 --> 00:10:54,240
see that it's possible to create a fully

276
00:10:54,240 --> 00:10:56,100
fully functional malware that bypasses

277
00:10:56,100 --> 00:10:58,380
the detection of machine learning models

278
00:10:58,380 --> 00:11:01,740
for this given an original mower that it

279
00:11:01,740 --> 00:11:03,720
is detected with 91 percent of

280
00:11:03,720 --> 00:11:05,820
confidence an embedding feature on that

281
00:11:05,820 --> 00:11:08,579
embeds the mower inside another binary

282
00:11:08,579 --> 00:11:12,240
in a set of mini files it is possible to

283
00:11:12,240 --> 00:11:14,040
generate on the universal malware that

284
00:11:14,040 --> 00:11:15,959
has an even higher confidence than a

285
00:11:15,959 --> 00:11:18,240
Goodwill in this case 92 percent of

286
00:11:18,240 --> 00:11:20,600
confidence

287
00:11:21,000 --> 00:11:22,800
we can know the effect of the results

288
00:11:22,800 --> 00:11:24,720
created using this strategy in real

289
00:11:24,720 --> 00:11:26,100
antiviruses

290
00:11:26,100 --> 00:11:28,740
I submitted 50 Mario samples to virus

291
00:11:28,740 --> 00:11:30,839
total including the evasive variations

292
00:11:30,839 --> 00:11:33,120
and as we can note

293
00:11:33,120 --> 00:11:35,339
the evasive samples have lower detection

294
00:11:35,339 --> 00:11:37,140
rate than the originals the original

295
00:11:37,140 --> 00:11:40,200
ones indicating that AVS are also

296
00:11:40,200 --> 00:11:41,579
working with machine learning models

297
00:11:41,579 --> 00:11:43,500
under the hood and are also affected by

298
00:11:43,500 --> 00:11:46,880
this simple strategy

299
00:11:47,760 --> 00:11:49,440
possible mitigation to do with new

300
00:11:49,440 --> 00:11:51,120
attack vectors is to update the

301
00:11:51,120 --> 00:11:53,279
classification model after we have their

302
00:11:53,279 --> 00:11:54,360
labels

303
00:11:54,360 --> 00:11:57,060
to simulate this situation I trained two

304
00:11:57,060 --> 00:11:59,160
classifiers the first the first one with

305
00:11:59,160 --> 00:12:01,740
no adversar is in the training set using

306
00:12:01,740 --> 00:12:03,540
only the Ember data set

307
00:12:03,540 --> 00:12:05,279
and the second including universities

308
00:12:05,279 --> 00:12:09,000
generating in 2019 we tested uh both

309
00:12:09,000 --> 00:12:11,700
models with two test sets one with the

310
00:12:11,700 --> 00:12:15,120
2020 2015 original Mars samples and the

311
00:12:15,120 --> 00:12:16,800
other with the reserve variations of

312
00:12:16,800 --> 00:12:19,079
this mower from 2020

313
00:12:19,079 --> 00:12:21,480
uh both sets with Windows apps sb9

314
00:12:21,480 --> 00:12:22,860
applications

315
00:12:22,860 --> 00:12:25,320
as we can see the model training with

316
00:12:25,320 --> 00:12:27,060
node research is able to detect our

317
00:12:27,060 --> 00:12:28,980
original mower samples and the majority

318
00:12:28,980 --> 00:12:31,260
of good with a very low phospholitude

319
00:12:31,260 --> 00:12:32,339
rate

320
00:12:32,339 --> 00:12:34,380
in contrast is did not detected the

321
00:12:34,380 --> 00:12:36,360
reserve variations of the original 2020

322
00:12:36,360 --> 00:12:38,160
Mario samples so

323
00:12:38,160 --> 00:12:41,899
it's not detecting adversar

324
00:12:42,360 --> 00:12:44,639
in addition the model training with the

325
00:12:44,639 --> 00:12:47,579
user detected are more samples even the

326
00:12:47,579 --> 00:12:49,980
universal ones with the cost of labeling

327
00:12:49,980 --> 00:12:51,779
the majority of goodware samples are

328
00:12:51,779 --> 00:12:54,779
smarter which make its use invisible in

329
00:12:54,779 --> 00:12:56,760
practice because it's basically saying

330
00:12:56,760 --> 00:12:59,100
that all samples presented are mower

331
00:12:59,100 --> 00:13:01,820
so finally we can say that in practice

332
00:13:01,820 --> 00:13:04,320
uh just updating the model residual

333
00:13:04,320 --> 00:13:06,540
service is not enough we need new

334
00:13:06,540 --> 00:13:07,980
strategies to deal with this this

335
00:13:07,980 --> 00:13:10,220
problem

336
00:13:10,980 --> 00:13:13,200
the last step in creating a solution is

337
00:13:13,200 --> 00:13:14,880
the evaluation where we have the problem

338
00:13:14,880 --> 00:13:17,940
of the delay labels

339
00:13:17,940 --> 00:13:19,560
one of the challenges in evaluating

340
00:13:19,560 --> 00:13:21,060
Security Solutions that use machine

341
00:13:21,060 --> 00:13:23,100
learning is the delay in making labels

342
00:13:23,100 --> 00:13:24,240
available

343
00:13:24,240 --> 00:13:26,220
this directly affects how the violation

344
00:13:26,220 --> 00:13:28,980
is done since labels do not exist right

345
00:13:28,980 --> 00:13:31,019
after the data is collected

346
00:13:31,019 --> 00:13:32,820
this problem is not considered by many

347
00:13:32,820 --> 00:13:35,220
Works they consider that labels are

348
00:13:35,220 --> 00:13:37,260
available right after data collection

349
00:13:37,260 --> 00:13:39,839
some Works consider that the labeler

350
00:13:39,839 --> 00:13:42,180
data have the same label that the

351
00:13:42,180 --> 00:13:44,279
machine learning model predicted which

352
00:13:44,279 --> 00:13:45,839
can be a problem System model may be

353
00:13:45,839 --> 00:13:47,839
vulnerable to poisoning attacks

354
00:13:47,839 --> 00:13:51,060
decreasing its attacking rate over time

355
00:13:51,060 --> 00:13:53,700
in our detection for instance just a

356
00:13:53,700 --> 00:13:56,639
symbol snapshot of diverse total scan is

357
00:13:56,639 --> 00:13:58,620
used without considering the labels

358
00:13:58,620 --> 00:14:02,339
delay according to the literature mower

359
00:14:02,339 --> 00:14:04,260
labels can vary from 10 days to more

360
00:14:04,260 --> 00:14:05,579
than 2 years

361
00:14:05,579 --> 00:14:08,220
furthermore AV Solutions take about 19

362
00:14:08,220 --> 00:14:12,620
days to detect most threats

363
00:14:12,660 --> 00:14:15,000
to verify the impact of label delay I

364
00:14:15,000 --> 00:14:17,639
simulated the situation using a subset

365
00:14:17,639 --> 00:14:19,560
of the Android data set again

366
00:14:19,560 --> 00:14:21,720
using the Adaptive handle first with and

367
00:14:21,720 --> 00:14:25,380
without drift detectors as a data stream

368
00:14:25,380 --> 00:14:27,360
for this I'll make the labels available

369
00:14:27,360 --> 00:14:29,880
for classifier to be updated only after

370
00:14:29,880 --> 00:14:32,579
n days it was first seen only then the

371
00:14:32,579 --> 00:14:35,339
decision module is updated with the new

372
00:14:35,339 --> 00:14:36,420
data

373
00:14:36,420 --> 00:14:38,700
by observing the results when there is

374
00:14:38,700 --> 00:14:40,320
no delay there are Griffin with the

375
00:14:40,320 --> 00:14:43,019
drift detector improved detection with

376
00:14:43,019 --> 00:14:46,380
almost 100 recall and precision so since

377
00:14:46,380 --> 00:14:48,660
like the ideal scenario

378
00:14:48,660 --> 00:14:51,300
after a day of delay the model that does

379
00:14:51,300 --> 00:14:53,040
not consider drift turns out to be

380
00:14:53,040 --> 00:14:55,019
better overall indicating that the drift

381
00:14:55,019 --> 00:14:58,199
detectors are highly affected by delays

382
00:14:58,199 --> 00:15:01,079
there is in practice machine learning

383
00:15:01,079 --> 00:15:02,339
models do not perform as they are

384
00:15:02,339 --> 00:15:04,680
commonly evaluated without considering

385
00:15:04,680 --> 00:15:08,639
these delays in the the labels

386
00:15:08,639 --> 00:15:11,880
so now concluding the the talk the

387
00:15:11,880 --> 00:15:13,620
future of machine learning for security

388
00:15:13,620 --> 00:15:15,839
is promising machine learning has become

389
00:15:15,839 --> 00:15:18,240
so popular that is hard to imagine a

390
00:15:18,240 --> 00:15:20,760
future without it however there is still

391
00:15:20,760 --> 00:15:22,500
a lot of room for improvement such as

392
00:15:22,500 --> 00:15:25,019
the following recommendations

393
00:15:25,019 --> 00:15:27,120
we should stop looking on that metrics

394
00:15:27,120 --> 00:15:29,519
and start looking at effects many of the

395
00:15:29,519 --> 00:15:31,320
challenges present in this work remain

396
00:15:31,320 --> 00:15:33,600
as open research problems which would

397
00:15:33,600 --> 00:15:35,579
benefit both Academy and Industry if

398
00:15:35,579 --> 00:15:37,680
properly solve it

399
00:15:37,680 --> 00:15:39,420
in addition their mitigation would

400
00:15:39,420 --> 00:15:42,000
faster use of machine learning for cyber

401
00:15:42,000 --> 00:15:44,100
security problems and improve this cyber

402
00:15:44,100 --> 00:15:45,660
security review in the same way machine

403
00:15:45,660 --> 00:15:48,300
learning improver improved and advanced

404
00:15:48,300 --> 00:15:50,100
other research fields

405
00:15:50,100 --> 00:15:51,959
Community players have to take into

406
00:15:51,959 --> 00:15:53,699
consideration the peculiarities

407
00:15:53,699 --> 00:15:56,399
associated with cyber security data and

408
00:15:56,399 --> 00:15:57,779
its sources

409
00:15:57,779 --> 00:16:00,660
also commit yourself to the real world I

410
00:16:00,660 --> 00:16:02,639
advocate that future research Works

411
00:16:02,639 --> 00:16:04,500
always keep the model machine learning

412
00:16:04,500 --> 00:16:06,240
that matters in mind when developing new

413
00:16:06,240 --> 00:16:07,380
Solutions

414
00:16:07,380 --> 00:16:10,139
we need to stop trying to improve 0.01

415
00:16:10,139 --> 00:16:12,300
of accuracy why we are still evaluating

416
00:16:12,300 --> 00:16:14,880
our models or our Solutions in the wrong

417
00:16:14,880 --> 00:16:17,459
way and have many uh open research

418
00:16:17,459 --> 00:16:20,040
problems such as the ones mentioned here

419
00:16:20,040 --> 00:16:21,720
thus if your work is losing connection

420
00:16:21,720 --> 00:16:25,760
to the real world uh we have a problem

421
00:16:26,399 --> 00:16:28,260
finally check your work my

422
00:16:28,260 --> 00:16:30,480
recommendation is to carefully observe

423
00:16:30,480 --> 00:16:32,880
all the the items and to consider it of

424
00:16:32,880 --> 00:16:34,320
them during the design and

425
00:16:34,320 --> 00:16:36,060
implementation of Machinery models for

426
00:16:36,060 --> 00:16:37,380
cyber security

427
00:16:37,380 --> 00:16:40,560
to encourage that I present a checklist

428
00:16:40,560 --> 00:16:42,720
to serve as a reminder and prevent

429
00:16:42,720 --> 00:16:44,699
researchers and Patronus from committing

430
00:16:44,699 --> 00:16:47,160
these common mistakes or at least being

431
00:16:47,160 --> 00:16:49,620
aware of their existence you can assess

432
00:16:49,620 --> 00:16:51,600
these checklist through the URL or the

433
00:16:51,600 --> 00:16:54,259
this QR code

434
00:16:54,300 --> 00:16:56,160
I would like to finish my presentation

435
00:16:56,160 --> 00:16:58,500
with a quote from Ireland Cena which

436
00:16:58,500 --> 00:17:01,019
says that the past is just data I don't

437
00:17:01,019 --> 00:17:02,339
see the future

438
00:17:02,339 --> 00:17:04,559
so let's look to the Future and start

439
00:17:04,559 --> 00:17:05,880
working for a better machine learning

440
00:17:05,880 --> 00:17:09,140
and cyber security community

441
00:17:09,299 --> 00:17:10,980
thank you for your attention and for

442
00:17:10,980 --> 00:17:12,339
your time

443
00:17:12,339 --> 00:17:15,958
[Applause]

