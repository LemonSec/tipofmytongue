1
00:00:12,910 --> 00:00:16,869
so again my name is Steven boo genie and

2
00:00:15,070 --> 00:00:18,730
I spent this past summer at the Aspen

3
00:00:16,869 --> 00:00:20,529
tech policy hub which is a new

4
00:00:18,730 --> 00:00:22,599
fellowship for technologists and that's

5
00:00:20,529 --> 00:00:24,910
broadly defined so I think most people

6
00:00:22,599 --> 00:00:27,130
in this room qualify it's a fellowship

7
00:00:24,910 --> 00:00:28,810
to allow folks like us to apply our

8
00:00:27,130 --> 00:00:30,880
industry and technical expertise to

9
00:00:28,810 --> 00:00:33,250
public policy problems and at the

10
00:00:30,880 --> 00:00:35,170
fellowship I met my colleague brandy

11
00:00:33,250 --> 00:00:37,780
Nanaki who's the director of the citrus

12
00:00:35,170 --> 00:00:39,040
policy lab at UC Berkeley and she was

13
00:00:37,780 --> 00:00:40,420
telling me in those first few days of

14
00:00:39,040 --> 00:00:42,940
the fellowship about some of the

15
00:00:40,420 --> 00:00:45,270
research research she was doing at

16
00:00:42,940 --> 00:00:47,800
citrus policy lab which included

17
00:00:45,270 --> 00:00:49,890
investigating how bots were influencing

18
00:00:47,800 --> 00:00:52,180
discourse on social media platforms I

19
00:00:49,890 --> 00:00:54,160
thought this was fascinating because she

20
00:00:52,180 --> 00:00:57,399
wrote a scraper to interface with the

21
00:00:54,160 --> 00:00:59,288
public API to collect her dataset and I

22
00:00:57,399 --> 00:01:01,329
thought wow what a vast improvement from

23
00:00:59,289 --> 00:01:03,010
the way that I thought academic research

24
00:01:01,329 --> 00:01:05,050
was being conducted which is you put up

25
00:01:03,010 --> 00:01:07,690
some fliers in the hall pay some

26
00:01:05,050 --> 00:01:10,360
students to come by to participate in

27
00:01:07,690 --> 00:01:12,700
some crude facsimile of how humans are

28
00:01:10,360 --> 00:01:14,200
interacting online we actually just have

29
00:01:12,700 --> 00:01:17,250
these platforms that are collecting

30
00:01:14,200 --> 00:01:20,920
unique high-quality datasets that really

31
00:01:17,250 --> 00:01:23,160
granularly mimic and portray how humans

32
00:01:20,920 --> 00:01:25,390
are interacting online it seems like a

33
00:01:23,160 --> 00:01:28,110
inefficiency in the marketplace right it

34
00:01:25,390 --> 00:01:30,610
seems like Brandi who's a very qualified

35
00:01:28,110 --> 00:01:32,800
researcher with a large track record

36
00:01:30,610 --> 00:01:34,539
should just be able to query some portal

37
00:01:32,800 --> 00:01:35,770
and get the full data set she needs

38
00:01:34,539 --> 00:01:38,800
without having to do all this legwork

39
00:01:35,770 --> 00:01:40,539
especially as a non-technical person so

40
00:01:38,800 --> 00:01:42,220
Brandi and I resolved to spend our three

41
00:01:40,539 --> 00:01:44,560
months at the fellowship building a

42
00:01:42,220 --> 00:01:45,580
bridge between industry and academia we

43
00:01:44,560 --> 00:01:47,830
thought the bridge would look like this

44
00:01:45,580 --> 00:01:49,300
straight and narrow but it ended up

45
00:01:47,830 --> 00:01:51,009
looking a lot more like this by the time

46
00:01:49,300 --> 00:01:52,690
we were done there are a lot of planks

47
00:01:51,009 --> 00:01:54,399
missing from this bridge just like there

48
00:01:52,690 --> 00:01:56,800
are large gaps in the policy landscape

49
00:01:54,399 --> 00:01:59,619
and those can be categorized into three

50
00:01:56,800 --> 00:02:02,590
separate groups the first is legal

51
00:01:59,619 --> 00:02:05,770
the second is technological and the

52
00:02:02,590 --> 00:02:07,030
third is philosophical so I think one

53
00:02:05,770 --> 00:02:09,489
question that I want to answer right off

54
00:02:07,030 --> 00:02:11,620
the bat is why now why I'm on the stage

55
00:02:09,489 --> 00:02:12,569
now because the platform's have been

56
00:02:11,620 --> 00:02:14,739
around for a while

57
00:02:12,569 --> 00:02:16,929
academics have wanted these datasets for

58
00:02:14,739 --> 00:02:20,200
a while so why is now the opportunity do

59
00:02:16,930 --> 00:02:22,810
we have an opportunity to move and that

60
00:02:20,200 --> 00:02:24,760
has to go back to the 2016 United States

61
00:02:22,810 --> 00:02:25,900
presidential election where in the

62
00:02:24,760 --> 00:02:26,620
months following there was a lot of

63
00:02:25,900 --> 00:02:28,299
discussion

64
00:02:26,620 --> 00:02:30,159
what roles if any of these platforms

65
00:02:28,299 --> 00:02:33,569
played in influencing the outcome of the

66
00:02:30,159 --> 00:02:36,159
election and so for the first time

67
00:02:33,569 --> 00:02:38,018
academia government and industry were

68
00:02:36,159 --> 00:02:41,379
all aligned on a one research question

69
00:02:38,019 --> 00:02:43,690
so that provided an opportunity for us

70
00:02:41,379 --> 00:02:45,730
to collaborate in a way that was never

71
00:02:43,690 --> 00:02:47,379
done before because there were differing

72
00:02:45,730 --> 00:02:50,500
views about what was important what

73
00:02:47,379 --> 00:02:52,179
questions were important and before

74
00:02:50,500 --> 00:02:54,220
there was some tension between industry

75
00:02:52,180 --> 00:02:56,200
and academia in large part because of

76
00:02:54,220 --> 00:02:57,849
Cambridge analytic ah remember that data

77
00:02:56,200 --> 00:03:01,510
exfiltration was actually done by our

78
00:02:57,849 --> 00:03:03,608
academic and so the concept of sharing

79
00:03:01,510 --> 00:03:05,260
this data even in a formal arrangement a

80
00:03:03,609 --> 00:03:07,299
lot of industry folks were somewhat

81
00:03:05,260 --> 00:03:08,679
tepid about this because of what had

82
00:03:07,299 --> 00:03:10,680
happened previously in the flack that

83
00:03:08,680 --> 00:03:14,769
they received for something that really

84
00:03:10,680 --> 00:03:16,690
was not under their control so the new

85
00:03:14,769 --> 00:03:18,040
innovation that came about shortly after

86
00:03:16,690 --> 00:03:19,629
the election was something called social

87
00:03:18,040 --> 00:03:21,599
science one and this was a new

88
00:03:19,629 --> 00:03:25,480
partnership between Facebook and some

89
00:03:21,599 --> 00:03:27,909
academics and it's novel innovation was

90
00:03:25,480 --> 00:03:29,768
this introduction of a third party so

91
00:03:27,909 --> 00:03:31,480
the value that the third party provided

92
00:03:29,769 --> 00:03:33,459
to industry was that it solicited

93
00:03:31,480 --> 00:03:35,530
applications for research requests it

94
00:03:33,459 --> 00:03:38,829
vetted those applications made sure that

95
00:03:35,530 --> 00:03:40,209
it was the research questions were valid

96
00:03:38,829 --> 00:03:42,010
they weren't just hatched jobs by

97
00:03:40,209 --> 00:03:44,859
researchers who wanted to take down the

98
00:03:42,010 --> 00:03:46,569
man and they also made sure that the

99
00:03:44,859 --> 00:03:48,940
research had researchers had a track

100
00:03:46,569 --> 00:03:50,108
record right they weren't just someone

101
00:03:48,940 --> 00:03:51,879
who just wanted the data for other

102
00:03:50,109 --> 00:03:53,769
purposes whether it's to sell it on the

103
00:03:51,879 --> 00:03:55,448
black market or whatever else to

104
00:03:53,769 --> 00:03:57,519
academia the value that the third party

105
00:03:55,449 --> 00:03:59,470
provided was the third party had special

106
00:03:57,519 --> 00:04:01,030
access to go into the walled garden to

107
00:03:59,470 --> 00:04:03,359
see what sort of datasets were available

108
00:04:01,030 --> 00:04:05,829
and what sort of research questions

109
00:04:03,359 --> 00:04:07,389
academia could ask because previously

110
00:04:05,829 --> 00:04:10,660
they didn't have that level of very on

111
00:04:07,389 --> 00:04:11,859
that level of transparency into what was

112
00:04:10,660 --> 00:04:14,560
actually hiding inside these walled

113
00:04:11,859 --> 00:04:16,329
gardens so social science one has been

114
00:04:14,560 --> 00:04:19,690
around for like a year year and a half

115
00:04:16,329 --> 00:04:21,220
at this point but unfortunately we're no

116
00:04:19,690 --> 00:04:22,719
longer the three musketeers right in the

117
00:04:21,220 --> 00:04:25,090
standoff right now between government

118
00:04:22,720 --> 00:04:26,919
academia and Industry and what happens

119
00:04:25,090 --> 00:04:29,469
as a programmer when things go wrong a

120
00:04:26,919 --> 00:04:31,539
lot of case in a lot of cases we do a

121
00:04:29,470 --> 00:04:33,310
post-mortem right we analyzed the

122
00:04:31,539 --> 00:04:35,080
problem we figure out what went wrong

123
00:04:33,310 --> 00:04:36,010
and we change our process to make sure

124
00:04:35,080 --> 00:04:38,020
it never happens again

125
00:04:36,010 --> 00:04:39,529
so social science one is not dead so

126
00:04:38,020 --> 00:04:41,568
maybe calling it a post mortem is

127
00:04:39,529 --> 00:04:43,189
entirely accurate but I think it would

128
00:04:41,569 --> 00:04:45,289
be useful to actually look at some of

129
00:04:43,189 --> 00:04:48,769
the problems that it faced and then end

130
00:04:45,289 --> 00:04:50,449
with how we can move forward so going

131
00:04:48,769 --> 00:04:53,179
back to our three core problems I want

132
00:04:50,449 --> 00:04:55,429
to start with a legal context so first

133
00:04:53,179 --> 00:04:56,839
gdpr is the law of the land I'm sure

134
00:04:55,429 --> 00:04:58,818
many of you are familiar with this

135
00:04:56,839 --> 00:05:02,689
acronym but for those of you who are not

136
00:04:58,819 --> 00:05:06,289
you may recognize bars like this

137
00:05:02,689 --> 00:05:08,379
the Europeans general European Union's

138
00:05:06,289 --> 00:05:11,149
general data privacy regulation

139
00:05:08,379 --> 00:05:12,889
regulates a lot of different things it's

140
00:05:11,149 --> 00:05:14,209
very very broad and complex but today

141
00:05:12,889 --> 00:05:16,399
we're only going to be focusing on just

142
00:05:14,209 --> 00:05:19,129
one of those concepts and that is the

143
00:05:16,399 --> 00:05:21,259
concept of personal data so in the GDP

144
00:05:19,129 --> 00:05:22,789
our world personal data is any data

145
00:05:21,259 --> 00:05:26,419
point that can be used to uniquely

146
00:05:22,789 --> 00:05:28,759
identify an individual but it can also

147
00:05:26,419 --> 00:05:30,709
mean any unification or composition of

148
00:05:28,759 --> 00:05:32,329
identifiers that create a higher-order

149
00:05:30,709 --> 00:05:34,849
identifier that can uniquely identify

150
00:05:32,329 --> 00:05:36,469
someone and this percents are pretty

151
00:05:34,849 --> 00:05:38,419
interesting epistemological question

152
00:05:36,469 --> 00:05:39,799
because if you want to share your data

153
00:05:38,419 --> 00:05:41,808
it could be something that seems totally

154
00:05:39,799 --> 00:05:43,459
innocuous to you but how can you

155
00:05:41,809 --> 00:05:44,959
guarantee that I can never be unified

156
00:05:43,459 --> 00:05:47,269
with some data set that you may not even

157
00:05:44,959 --> 00:05:49,239
know about to uniquely identify someone

158
00:05:47,269 --> 00:05:51,529
thus violating gdpr

159
00:05:49,239 --> 00:05:52,609
so I kind of think of it as you know

160
00:05:51,529 --> 00:05:53,929
especially if some of these large

161
00:05:52,610 --> 00:05:56,689
platforms where you have tons of data

162
00:05:53,929 --> 00:05:57,948
many many years worth of all sorts of

163
00:05:56,689 --> 00:05:59,689
different interactions that could be

164
00:05:57,949 --> 00:06:02,389
used to uniquely identify someone I kind

165
00:05:59,689 --> 00:06:04,849
of think of it as a huge dam and it only

166
00:06:02,389 --> 00:06:07,579
takes one small crack to lead to

167
00:06:04,849 --> 00:06:11,179
catastrophic failure now what does

168
00:06:07,579 --> 00:06:15,139
catastrophic failure look like in GDP PR

169
00:06:11,179 --> 00:06:19,578
regulation world a cool 57 million

170
00:06:15,139 --> 00:06:22,189
dollars and if we think back to Anna

171
00:06:19,579 --> 00:06:24,259
Lisa's talk this regulation is

172
00:06:22,189 --> 00:06:27,559
relatively new it only went into effect

173
00:06:24,259 --> 00:06:29,509
in the spring of 2018 and it's very very

174
00:06:27,559 --> 00:06:31,849
complex as I said earlier so there are a

175
00:06:29,509 --> 00:06:33,969
lot of gray areas where there hasn't

176
00:06:31,849 --> 00:06:36,919
been regulatory guidance issued and

177
00:06:33,969 --> 00:06:39,319
there hasn't yet been clarification from

178
00:06:36,919 --> 00:06:40,609
the courts and as we know from analysis

179
00:06:39,319 --> 00:06:42,889
talk that the u.s. cares a lot about

180
00:06:40,609 --> 00:06:44,929
liability and a lot of these platforms

181
00:06:42,889 --> 00:06:46,189
are based in the United States so one of

182
00:06:44,929 --> 00:06:48,589
the first challenges that we solve a

183
00:06:46,189 --> 00:06:50,419
social science one was that a lot of

184
00:06:48,589 --> 00:06:52,249
lawyers that these big tech companies

185
00:06:50,419 --> 00:06:53,159
felt like they did not have sufficient

186
00:06:52,249 --> 00:06:56,250
regulates

187
00:06:53,160 --> 00:06:58,230
a clarification to enter into these

188
00:06:56,250 --> 00:07:03,060
sorts of partnerships while protecting

189
00:06:58,230 --> 00:07:04,590
their client from liability so next

190
00:07:03,060 --> 00:07:07,200
we're gonna move on to the technological

191
00:07:04,590 --> 00:07:08,669
gaps that exist and one thing that I

192
00:07:07,200 --> 00:07:11,039
think is pretty interesting that social

193
00:07:08,670 --> 00:07:13,200
science one did is they almost foresaw

194
00:07:11,040 --> 00:07:14,940
the complexity that gdpr was going to

195
00:07:13,200 --> 00:07:17,159
bring to the table and they came up with

196
00:07:14,940 --> 00:07:18,570
a clever solution and that solution was

197
00:07:17,160 --> 00:07:21,930
what if we didn't actually have to work

198
00:07:18,570 --> 00:07:24,360
under GPR at all remember GDP are only

199
00:07:21,930 --> 00:07:27,120
applies to personal data so if you can

200
00:07:24,360 --> 00:07:30,090
anonymize your data it actually does not

201
00:07:27,120 --> 00:07:32,250
is actually not covered by GDP are now

202
00:07:30,090 --> 00:07:33,719
as programmers we know that one of the

203
00:07:32,250 --> 00:07:35,910
easiest ways to anonymize data is

204
00:07:33,720 --> 00:07:37,320
hashing for those of you who aren't

205
00:07:35,910 --> 00:07:38,280
familiar with hashing you can think of

206
00:07:37,320 --> 00:07:40,140
it as a black box

207
00:07:38,280 --> 00:07:42,030
that takes some input and outputs a

208
00:07:40,140 --> 00:07:44,550
fingerprint and one really unique

209
00:07:42,030 --> 00:07:46,380
property of this is that it's not

210
00:07:44,550 --> 00:07:47,990
reversible you can't take the

211
00:07:46,380 --> 00:07:50,610
fingerprint and recover the source data

212
00:07:47,990 --> 00:07:52,200
so if you want to share personal data

213
00:07:50,610 --> 00:07:54,570
you can feed it through this black box

214
00:07:52,200 --> 00:07:56,909
get some fingerprints spread the

215
00:07:54,570 --> 00:07:58,200
fingerprints around and no adversary is

216
00:07:56,910 --> 00:07:59,880
gonna be able to take those fingerprints

217
00:07:58,200 --> 00:08:02,370
and recover the original personal data

218
00:07:59,880 --> 00:08:04,650
meaning that you've violated gdpr in

219
00:08:02,370 --> 00:08:06,000
some way but you'll also notice that the

220
00:08:04,650 --> 00:08:08,190
fingerprint looks nothing like the

221
00:08:06,000 --> 00:08:09,870
original so we lose all the interesting

222
00:08:08,190 --> 00:08:11,490
granularity of the data the trend lines

223
00:08:09,870 --> 00:08:13,560
in the data when we hash it that's

224
00:08:11,490 --> 00:08:16,230
useful for that academics need to

225
00:08:13,560 --> 00:08:18,330
conduct their research so there's a new

226
00:08:16,230 --> 00:08:20,700
concept out that some of you might have

227
00:08:18,330 --> 00:08:22,169
heard of called differential privacy so

228
00:08:20,700 --> 00:08:24,450
differential privacy is a new way of

229
00:08:22,169 --> 00:08:26,190
anonymizing data that preserves the

230
00:08:24,450 --> 00:08:28,020
trends in the data so this seems like

231
00:08:26,190 --> 00:08:29,550
the best of both worlds we can anonymize

232
00:08:28,020 --> 00:08:32,669
our data set meaning that we don't have

233
00:08:29,550 --> 00:08:34,919
to abide by GDP our while the academics

234
00:08:32,669 --> 00:08:38,000
still have access to the trends that

235
00:08:34,919 --> 00:08:40,079
they mean to come up with new insights

236
00:08:38,000 --> 00:08:41,969
but there's so a couple problems that we

237
00:08:40,080 --> 00:08:43,530
found with GDP our excuse me with

238
00:08:41,969 --> 00:08:46,290
differential privacy when it came to

239
00:08:43,530 --> 00:08:48,720
implementation so the first is a slider

240
00:08:46,290 --> 00:08:51,209
differential privacy allows for sort of

241
00:08:48,720 --> 00:08:54,060
a an adjustment of the level of privacy

242
00:08:51,210 --> 00:08:56,400
guarantees that you have but GDP are

243
00:08:54,060 --> 00:09:01,890
actually doesn't hasn't thought of

244
00:08:56,400 --> 00:09:03,390
anonymous data or an intimidation as it

245
00:09:01,890 --> 00:09:05,310
thinks okay that's black and white it

246
00:09:03,390 --> 00:09:06,780
doesn't think of it as a gradient so

247
00:09:05,310 --> 00:09:07,020
again the lawyers come into play and say

248
00:09:06,780 --> 00:09:09,350
well

249
00:09:07,020 --> 00:09:13,530
we can't be sure what GDP RS implement

250
00:09:09,350 --> 00:09:15,150
interpretation of an animation is what

251
00:09:13,530 --> 00:09:17,160
level it is what threshold we'll need to

252
00:09:15,150 --> 00:09:19,350
meet so automatically they were wary of

253
00:09:17,160 --> 00:09:22,800
this solution there are a couple other

254
00:09:19,350 --> 00:09:25,200
problems too so one of the key areas of

255
00:09:22,800 --> 00:09:28,890
research in this world of election

256
00:09:25,200 --> 00:09:32,850
research by and how social media

257
00:09:28,890 --> 00:09:34,970
platforms impact elections is the

258
00:09:32,850 --> 00:09:37,830
concept of the spread of disinformation

259
00:09:34,970 --> 00:09:39,570
so Facebook tried to put together a data

260
00:09:37,830 --> 00:09:40,410
set that will included a list of links

261
00:09:39,570 --> 00:09:42,060
in the number of times they've been

262
00:09:40,410 --> 00:09:43,949
shared to see how information was

263
00:09:42,060 --> 00:09:45,839
spreading through their platform but as

264
00:09:43,950 --> 00:09:48,990
we know from a lot of data sets there's

265
00:09:45,840 --> 00:09:50,730
always a long tail and the presence of

266
00:09:48,990 --> 00:09:52,680
outliers in your data set especially

267
00:09:50,730 --> 00:09:55,400
mins and maxes can actually weaken some

268
00:09:52,680 --> 00:09:58,800
of the guarantees that gdpr provides and

269
00:09:55,400 --> 00:10:00,449
so there's a solution to this the

270
00:09:58,800 --> 00:10:02,849
solution comes with trade-offs like most

271
00:10:00,450 --> 00:10:05,100
things in technology the solution is to

272
00:10:02,850 --> 00:10:07,470
establish thresholds above or below

273
00:10:05,100 --> 00:10:09,600
which you chop off data but that can

274
00:10:07,470 --> 00:10:12,690
bias your data set of course and not

275
00:10:09,600 --> 00:10:15,420
only that but that weakens some of the

276
00:10:12,690 --> 00:10:18,690
privacy guarantees brought about by GDP

277
00:10:15,420 --> 00:10:20,099
are the other problem is that to set the

278
00:10:18,690 --> 00:10:22,230
thresholds a lot of times you have to

279
00:10:20,100 --> 00:10:23,730
bring humans into the loop right to

280
00:10:22,230 --> 00:10:25,350
actually figure out where the thresholds

281
00:10:23,730 --> 00:10:28,380
should be set so you can't necessarily

282
00:10:25,350 --> 00:10:30,210
just slap gee differential privacy

283
00:10:28,380 --> 00:10:31,560
everywhere and sort of get the

284
00:10:30,210 --> 00:10:35,670
guarantees you actually need a human

285
00:10:31,560 --> 00:10:36,930
looking at the data trends to set these

286
00:10:35,670 --> 00:10:38,370
thresholds correctly and make sure the

287
00:10:36,930 --> 00:10:40,489
privacy guarantees are at an appropriate

288
00:10:38,370 --> 00:10:42,810
level which obviously slows things down

289
00:10:40,490 --> 00:10:44,910
humans can't work as fast as computers

290
00:10:42,810 --> 00:10:46,770
but also once you bring a human into the

291
00:10:44,910 --> 00:10:48,480
loop you introduce the opportunity for

292
00:10:46,770 --> 00:10:51,590
error and what happens when a human

293
00:10:48,480 --> 00:10:53,730
makes a mistake 57 million dollar fines

294
00:10:51,590 --> 00:10:55,650
the last thing I want to talk about is

295
00:10:53,730 --> 00:10:57,120
the philosophical caps and I will admit

296
00:10:55,650 --> 00:10:59,579
that right now I'm gonna edit Oreo lies

297
00:10:57,120 --> 00:11:00,990
a bit but please bear with me so the

298
00:10:59,580 --> 00:11:04,530
first thing that is definitely a problem

299
00:11:00,990 --> 00:11:06,870
is that it's not exactly clear that our

300
00:11:04,530 --> 00:11:08,819
sort of general concept of what

301
00:11:06,870 --> 00:11:10,200
businesses should be doing aligns with

302
00:11:08,820 --> 00:11:12,180
the sharing of personal data for

303
00:11:10,200 --> 00:11:14,370
academic Public Interest Research right

304
00:11:12,180 --> 00:11:16,380
very few of these social media platforms

305
00:11:14,370 --> 00:11:18,840
actually have buy-in to do this sort of

306
00:11:16,380 --> 00:11:20,520
work from the c-suite and if you think

307
00:11:18,840 --> 00:11:20,880
about the individual engineers and the

308
00:11:20,520 --> 00:11:22,350
low

309
00:11:20,880 --> 00:11:24,420
middle managers who are tasked with this

310
00:11:22,350 --> 00:11:26,610
people get promoted by hitting okay ours

311
00:11:24,420 --> 00:11:29,339
most okay ours that these businesses are

312
00:11:26,610 --> 00:11:32,220
actually to increase the shareholder

313
00:11:29,340 --> 00:11:34,740
return and providing these sorts of

314
00:11:32,220 --> 00:11:36,960
services is actually a cost center so we

315
00:11:34,740 --> 00:11:38,700
found that a lot of these resources are

316
00:11:36,960 --> 00:11:40,320
a lot of the groups that are actually

317
00:11:38,700 --> 00:11:42,000
working on these problems within these

318
00:11:40,320 --> 00:11:45,030
companies are severely under-resourced

319
00:11:42,000 --> 00:11:47,490
and unfortunately they haven't been able

320
00:11:45,030 --> 00:11:49,319
to hit the timelines that they initially

321
00:11:47,490 --> 00:11:52,440
set out to achieve because of the lack

322
00:11:49,320 --> 00:11:54,450
of resources that they have another

323
00:11:52,440 --> 00:11:55,950
question that's come about is that so

324
00:11:54,450 --> 00:11:57,750
far the focus with social science one

325
00:11:55,950 --> 00:11:59,280
has been on the differential privacy we

326
00:11:57,750 --> 00:12:02,460
think that's the key to sort of

327
00:11:59,280 --> 00:12:04,260
unlocking this logjam but the problem is

328
00:12:02,460 --> 00:12:06,030
that we've seen some folks at Facebook

329
00:12:04,260 --> 00:12:07,260
have difficulty with this and Facebook

330
00:12:06,030 --> 00:12:09,449
has some of the smartest engineers on

331
00:12:07,260 --> 00:12:11,370
the planet so if Facebook is having

332
00:12:09,450 --> 00:12:16,260
difficulty anonymizing their admittedly

333
00:12:11,370 --> 00:12:18,030
really gnarly data set startups that are

334
00:12:16,260 --> 00:12:20,460
focused on just making it to the next

335
00:12:18,030 --> 00:12:22,050
quarter achieving profitability whatever

336
00:12:20,460 --> 00:12:23,610
the case might be they're probably not

337
00:12:22,050 --> 00:12:25,650
going to have the engineering cycles and

338
00:12:23,610 --> 00:12:28,080
maybe not the engineering expertise to

339
00:12:25,650 --> 00:12:29,520
be able to apply differential privacy to

340
00:12:28,080 --> 00:12:32,220
their data sets in order to share them

341
00:12:29,520 --> 00:12:33,930
with researchers so if social science

342
00:12:32,220 --> 00:12:36,480
one sets differential privacy as the

343
00:12:33,930 --> 00:12:39,420
standard for enabling this sort of data

344
00:12:36,480 --> 00:12:41,130
sharing agreement we may win the battle

345
00:12:39,420 --> 00:12:43,140
right we may unlock this face these

346
00:12:41,130 --> 00:12:46,460
Facebook data sets but we might lose the

347
00:12:43,140 --> 00:12:48,870
war because we've set the floor too high

348
00:12:46,460 --> 00:12:50,430
this matrix has been really helpful for

349
00:12:48,870 --> 00:12:52,500
helping me understand the regulatory

350
00:12:50,430 --> 00:12:54,239
landscape so we kind of have a dichotomy

351
00:12:52,500 --> 00:12:55,740
between private info and public info

352
00:12:54,240 --> 00:12:57,510
that's being shared publicly or not

353
00:12:55,740 --> 00:12:59,280
shared publicly so in the upper right

354
00:12:57,510 --> 00:13:00,900
hand corner we have public info that's

355
00:12:59,280 --> 00:13:03,360
shared publicly this is the digital

356
00:13:00,900 --> 00:13:05,010
transparency movement this allows me to

357
00:13:03,360 --> 00:13:06,510
enter in a few keystrokes look up my

358
00:13:05,010 --> 00:13:08,520
great-grandfather's immigration record

359
00:13:06,510 --> 00:13:10,350
into this country that's awesome I think

360
00:13:08,520 --> 00:13:12,180
everybody's on board with that in the

361
00:13:10,350 --> 00:13:13,680
bottom right we have public info that's

362
00:13:12,180 --> 00:13:15,329
not yet shared publicly but we have

363
00:13:13,680 --> 00:13:17,579
tools to uncover that information and

364
00:13:15,330 --> 00:13:18,690
let it release to the public make sure

365
00:13:17,580 --> 00:13:21,020
that it's released to the public

366
00:13:18,690 --> 00:13:23,490
for example Freedom of Information Act

367
00:13:21,020 --> 00:13:25,410
and there are some obviously checks in

368
00:13:23,490 --> 00:13:27,740
place to make sure that you know the UFO

369
00:13:25,410 --> 00:13:29,790
FILES an area 51 aren't being leaked in

370
00:13:27,740 --> 00:13:30,930
the upper left-hand corner we have

371
00:13:29,790 --> 00:13:33,120
private info that's being shared

372
00:13:30,930 --> 00:13:35,250
publicly we just call that social media

373
00:13:33,120 --> 00:13:37,290
and in the bottom right hand corner

374
00:13:35,250 --> 00:13:38,160
that's where the gap is right that's

375
00:13:37,290 --> 00:13:40,110
where there are some really interesting

376
00:13:38,160 --> 00:13:43,949
data sets where there's really no

377
00:13:40,110 --> 00:13:45,330
regulation to guide us and so that's why

378
00:13:43,950 --> 00:13:46,860
today Brandi and I want to make a strong

379
00:13:45,330 --> 00:13:49,170
call for a HIPAA for Public Interest

380
00:13:46,860 --> 00:13:51,740
Research hip hop being the regulation

381
00:13:49,170 --> 00:13:53,729
that governs medical records and

382
00:13:51,740 --> 00:13:56,070
facilitates the use of medical records

383
00:13:53,730 --> 00:13:58,110
private health information for medical

384
00:13:56,070 --> 00:13:59,400
research that benefits us all now my

385
00:13:58,110 --> 00:14:01,440
father's been a physician for thirty

386
00:13:59,400 --> 00:14:04,079
years so I've seen up close and personal

387
00:14:01,440 --> 00:14:05,910
house caloric and help Yorick Radek that

388
00:14:04,080 --> 00:14:07,500
industry is but I think it's pretty

389
00:14:05,910 --> 00:14:10,469
telling that we think our industry is

390
00:14:07,500 --> 00:14:12,360
really innovative and nimble and we

391
00:14:10,470 --> 00:14:14,220
can't figure out how to share some

392
00:14:12,360 --> 00:14:16,260
things like phone numbers we can't come

393
00:14:14,220 --> 00:14:17,700
up with a framework where as the medical

394
00:14:16,260 --> 00:14:20,069
industry has figured out how to share

395
00:14:17,700 --> 00:14:22,650
really private data sets that some of

396
00:14:20,070 --> 00:14:25,170
which you know might be pre-existing

397
00:14:22,650 --> 00:14:26,640
conditions that could affect how you are

398
00:14:25,170 --> 00:14:28,319
treated or get insurance or whatever the

399
00:14:26,640 --> 00:14:30,060
case might be maybe you have a condition

400
00:14:28,320 --> 00:14:31,560
that has a lot of stigma attached to it

401
00:14:30,060 --> 00:14:33,150
they figured out how to share this

402
00:14:31,560 --> 00:14:35,130
information and immediately it's not

403
00:14:33,150 --> 00:14:37,949
perfect but at least they are able to do

404
00:14:35,130 --> 00:14:39,270
research in this manner so what would a

405
00:14:37,950 --> 00:14:42,360
HIPAA for Public Interest to research

406
00:14:39,270 --> 00:14:43,589
look like well brandy and I think it

407
00:14:42,360 --> 00:14:44,839
needs to include a couple of things that

408
00:14:43,589 --> 00:14:47,520
we don't have right now

409
00:14:44,839 --> 00:14:50,400
the first is safe harbors both for

410
00:14:47,520 --> 00:14:52,350
academia and for industry as we know in

411
00:14:50,400 --> 00:14:55,020
the United States these platforms care a

412
00:14:52,350 --> 00:14:56,250
lot about liability so we need to make

413
00:14:55,020 --> 00:14:57,390
sure there's a clear safe harbor for

414
00:14:56,250 --> 00:14:59,640
them if they're acting in the public

415
00:14:57,390 --> 00:15:01,680
interest to ensure that they are

416
00:14:59,640 --> 00:15:03,900
shielded from liability whatever lessons

417
00:15:01,680 --> 00:15:06,660
are learned from their data sets but we

418
00:15:03,900 --> 00:15:08,130
also need a safe harbor for academics as

419
00:15:06,660 --> 00:15:10,500
well so they can look at really granular

420
00:15:08,130 --> 00:15:12,689
user level data without necessarily

421
00:15:10,500 --> 00:15:14,790
being held to the same protections that

422
00:15:12,690 --> 00:15:16,080
I as a random citizen who's trying to

423
00:15:14,790 --> 00:15:18,510
look at different data sets might be

424
00:15:16,080 --> 00:15:20,339
held to but there also needs to be

425
00:15:18,510 --> 00:15:22,170
accountability as well again with

426
00:15:20,339 --> 00:15:24,600
Cambridge analytic huh we need to make

427
00:15:22,170 --> 00:15:26,880
sure that if people if another situation

428
00:15:24,600 --> 00:15:28,980
happens like that again we're an

429
00:15:26,880 --> 00:15:31,320
academic violates the the rules of the

430
00:15:28,980 --> 00:15:34,050
conduct within the safe harbor that

431
00:15:31,320 --> 00:15:37,070
criminal penalties are are leveraged in

432
00:15:34,050 --> 00:15:39,449
order to enforce that code of conduct

433
00:15:37,070 --> 00:15:41,670
the interesting thing about gdpr right

434
00:15:39,450 --> 00:15:43,530
now is that it regulates data right and

435
00:15:41,670 --> 00:15:45,780
have some criteria about whether or not

436
00:15:43,530 --> 00:15:46,480
data is personal or not and if it is

437
00:15:45,780 --> 00:15:48,160
then

438
00:15:46,480 --> 00:15:49,630
bunch of rules and regulations are

439
00:15:48,160 --> 00:15:53,769
applied to it and controlling how you

440
00:15:49,630 --> 00:15:55,269
collect store analyze etc but we think

441
00:15:53,769 --> 00:15:57,930
that the hippo for Public Interest

442
00:15:55,269 --> 00:15:59,889
research should regulate the process

443
00:15:57,930 --> 00:16:02,319
regulating the data is not flexible

444
00:15:59,889 --> 00:16:03,699
enough we've seen that regulating the

445
00:16:02,320 --> 00:16:06,160
process we think we should look to the

446
00:16:03,699 --> 00:16:07,990
financial industry think about insider

447
00:16:06,160 --> 00:16:09,880
trading regulating insider trading for

448
00:16:07,990 --> 00:16:11,350
example when you try to determine

449
00:16:09,880 --> 00:16:13,180
whether or not someone has committed

450
00:16:11,350 --> 00:16:14,829
insider trading using privileged

451
00:16:13,180 --> 00:16:16,750
information to buy stocks and make money

452
00:16:14,829 --> 00:16:18,219
you don't look at the trades themselves

453
00:16:16,750 --> 00:16:20,050
right you don't look at the metadata of

454
00:16:18,220 --> 00:16:22,720
the trade you look at the process that

455
00:16:20,050 --> 00:16:25,359
led to someone buying the stock and we

456
00:16:22,720 --> 00:16:27,100
think gdpr are the new regulation to

457
00:16:25,360 --> 00:16:30,279
facilitate this type of research should

458
00:16:27,100 --> 00:16:32,740
do the same finally we need to trust but

459
00:16:30,279 --> 00:16:33,880
we also need to verify it's been pretty

460
00:16:32,740 --> 00:16:36,910
clear through this partnership between

461
00:16:33,880 --> 00:16:38,490
industry and academia academia that it's

462
00:16:36,910 --> 00:16:41,319
a relatively toothless agreement and

463
00:16:38,490 --> 00:16:43,000
there's still really no way for us as

464
00:16:41,320 --> 00:16:44,310
external stakeholders to understand

465
00:16:43,000 --> 00:16:46,209
whether or not the datasets

466
00:16:44,310 --> 00:16:47,529
we're receiving are actually

467
00:16:46,209 --> 00:16:50,469
representative of the whole dataset

468
00:16:47,529 --> 00:16:51,279
right so we can trust them but there

469
00:16:50,470 --> 00:16:52,870
really should be some sort of

470
00:16:51,279 --> 00:16:55,000
third-party verification process just

471
00:16:52,870 --> 00:16:56,889
like there is in other industries to

472
00:16:55,000 --> 00:16:58,240
make sure that the data that's being

473
00:16:56,889 --> 00:17:00,220
revealed and being shared is actually

474
00:16:58,240 --> 00:17:04,359
representative and that we're really

475
00:17:00,220 --> 00:17:07,419
have full access to all that we need to

476
00:17:04,359 --> 00:17:11,409
conduct the research to facilitate and

477
00:17:07,419 --> 00:17:12,579
benefit the public interest so brandy

478
00:17:11,410 --> 00:17:14,079
and I created something that we are

479
00:17:12,579 --> 00:17:16,270
calling Pyrrha the Public Interest

480
00:17:14,079 --> 00:17:18,399
Research Alliance and this is a multi

481
00:17:16,270 --> 00:17:19,660
stakeholder collaboration group that's

482
00:17:18,400 --> 00:17:21,220
meant to get a lot more voices in the

483
00:17:19,660 --> 00:17:22,600
room because social science one one

484
00:17:21,220 --> 00:17:24,370
problem that we saw was that it was

485
00:17:22,599 --> 00:17:26,349
between Facebook and academics from

486
00:17:24,369 --> 00:17:27,849
Harvard and Stanford and there are

487
00:17:26,349 --> 00:17:31,809
obviously a lot of voices being left out

488
00:17:27,849 --> 00:17:33,520
of that room and we were learned in our

489
00:17:31,809 --> 00:17:36,010
just three short months working on this

490
00:17:33,520 --> 00:17:38,440
program how complex and nuanced this

491
00:17:36,010 --> 00:17:40,030
issue is and so that's one of the

492
00:17:38,440 --> 00:17:41,320
reasons that I'm really excited to be on

493
00:17:40,030 --> 00:17:43,330
the stage here today because I want to

494
00:17:41,320 --> 00:17:45,850
issue a strong call to action all of you

495
00:17:43,330 --> 00:17:47,549
have really unique perspectives global

496
00:17:45,850 --> 00:17:49,959
perspectives that need to be

497
00:17:47,549 --> 00:17:53,290
incorporated into the discussion so

498
00:17:49,960 --> 00:17:54,940
moving forward we're able to make sure

499
00:17:53,290 --> 00:17:56,350
that we have a flexible regulation that

500
00:17:54,940 --> 00:17:59,500
works for everybody while preserving

501
00:17:56,350 --> 00:18:01,090
user privacy and enabling the further

502
00:17:59,500 --> 00:18:04,000
of how humans are interacting with each

503
00:18:01,090 --> 00:18:06,100
other online now why is discussions so

504
00:18:04,000 --> 00:18:07,720
important if really nothing's getting

505
00:18:06,100 --> 00:18:10,600
done in the current situation is so poor

506
00:18:07,720 --> 00:18:12,310
well I have good news and that is

507
00:18:10,600 --> 00:18:15,219
because we are all pioneers on this

508
00:18:12,310 --> 00:18:17,050
frontier the California consumer Privacy

509
00:18:15,220 --> 00:18:19,120
Act which is similar to GDP are in some

510
00:18:17,050 --> 00:18:21,340
ways just went into effect a couple of

511
00:18:19,120 --> 00:18:23,679
weeks ago and that's gonna spur federal

512
00:18:21,340 --> 00:18:26,909
privacy regulation so this is our

513
00:18:23,680 --> 00:18:30,490
opportunity for a cohesive comprehensive

514
00:18:26,910 --> 00:18:32,110
United States privacy regulation is

515
00:18:30,490 --> 00:18:34,000
going to go into effect most likely and

516
00:18:32,110 --> 00:18:36,070
we have the opportunity right now to

517
00:18:34,000 --> 00:18:37,660
actually shake that and so that's

518
00:18:36,070 --> 00:18:40,510
another reason why I'm excited to be

519
00:18:37,660 --> 00:18:41,890
here in 2020 because we actually have

520
00:18:40,510 --> 00:18:44,140
the ability to do something about that

521
00:18:41,890 --> 00:18:46,000
and I can tell you firsthand that

522
00:18:44,140 --> 00:18:48,760
everybody in this room that's exactly

523
00:18:46,000 --> 00:18:50,530
the perspective possesses the

524
00:18:48,760 --> 00:18:52,810
perspectives that regulators want to

525
00:18:50,530 --> 00:18:55,710
hear because a lot of them have no

526
00:18:52,810 --> 00:18:57,399
understanding whatsoever of the complex

527
00:18:55,710 --> 00:19:01,270
philosophical discussions that we're

528
00:18:57,400 --> 00:19:03,250
having here or the complex security

529
00:19:01,270 --> 00:19:06,340
arrangements that you all work so hard

530
00:19:03,250 --> 00:19:08,280
to protect user data it's our job as

531
00:19:06,340 --> 00:19:11,230
citizens to actually inform our

532
00:19:08,280 --> 00:19:13,060
representatives or senators folks within

533
00:19:11,230 --> 00:19:14,470
the government establishment of exactly

534
00:19:13,060 --> 00:19:16,210
what we're seeing on the ground so that

535
00:19:14,470 --> 00:19:18,760
the regulation the regulation that

536
00:19:16,210 --> 00:19:21,040
results is flexible while maintaining

537
00:19:18,760 --> 00:19:23,530
user privacy and again helps us better

538
00:19:21,040 --> 00:19:25,360
understand ourselves so thank you so

539
00:19:23,530 --> 00:19:27,810
much I really appreciate your time today

540
00:19:25,360 --> 00:19:30,129
our contact information is on the slide

541
00:19:27,810 --> 00:19:31,330
I'll be around after we're really

542
00:19:30,130 --> 00:19:34,020
looking forward to hearing from you all

543
00:19:31,330 --> 00:19:34,020
thanks so much

