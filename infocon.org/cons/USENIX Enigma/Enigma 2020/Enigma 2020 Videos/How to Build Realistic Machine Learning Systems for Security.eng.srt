1
00:00:15,090 --> 00:00:21,180
thanks for the great introduction if you

2
00:00:19,380 --> 00:00:23,549
want to detect any kind of malicious

3
00:00:21,180 --> 00:00:27,599
content at scale machine learning is

4
00:00:23,550 --> 00:00:29,550
very useful but as you all know that

5
00:00:27,599 --> 00:00:32,070
machine learning has lots of problem

6
00:00:29,550 --> 00:00:34,460
it's inexplicable it can be easily

7
00:00:32,070 --> 00:00:36,540
fooled and most importantly though

8
00:00:34,460 --> 00:00:39,180
emission learning is designed to solve

9
00:00:36,540 --> 00:00:41,220
the average case whereas in security we

10
00:00:39,180 --> 00:00:44,670
are focused on detecting anomalies

11
00:00:41,220 --> 00:00:47,280
things that rarely happens so one of the

12
00:00:44,670 --> 00:00:49,230
questions I often worry about is that is

13
00:00:47,280 --> 00:00:51,870
machine learning useful for security at

14
00:00:49,230 --> 00:00:54,180
all so this is answering this question

15
00:00:51,870 --> 00:00:55,890
would be the main goal of my talk I will

16
00:00:54,180 --> 00:00:58,019
talk about some of the mistakes that we

17
00:00:55,890 --> 00:00:59,969
can avoid to make sure that machine

18
00:00:58,020 --> 00:01:03,780
learning is still useful in the security

19
00:00:59,969 --> 00:01:06,060
context so let's try to build a malware

20
00:01:03,780 --> 00:01:07,740
detector using machine learning so like

21
00:01:06,060 --> 00:01:10,770
any other machine learning system you

22
00:01:07,740 --> 00:01:12,600
will start with a label data set of

23
00:01:10,770 --> 00:01:14,700
malware and benign content and it will

24
00:01:12,600 --> 00:01:17,309
extract features and then train our

25
00:01:14,700 --> 00:01:20,069
model and once we have a model we can

26
00:01:17,310 --> 00:01:23,490
use it to detect any new file that we

27
00:01:20,069 --> 00:01:25,110
want to detect like any other system the

28
00:01:23,490 --> 00:01:27,568
quality of the model depends on the

29
00:01:25,110 --> 00:01:31,050
quality of the data but it seems that in

30
00:01:27,569 --> 00:01:33,360
the malicious content case getting a

31
00:01:31,050 --> 00:01:36,270
good label data source for malware is

32
00:01:33,360 --> 00:01:39,209
very difficult so let me give you an

33
00:01:36,270 --> 00:01:42,090
example let's suppose you have this code

34
00:01:39,209 --> 00:01:43,679
that connects to an external command and

35
00:01:42,090 --> 00:01:46,740
control server and does something

36
00:01:43,679 --> 00:01:49,410
malicious do you think this is malware I

37
00:01:46,740 --> 00:01:51,000
think most of us would consider this as

38
00:01:49,410 --> 00:01:54,330
malware if this is doing something

39
00:01:51,000 --> 00:01:56,550
malicious now let's suppose over our

40
00:01:54,330 --> 00:01:58,619
external period of time this bot master

41
00:01:56,550 --> 00:02:01,020
disappeared so now this piece of code

42
00:01:58,619 --> 00:02:04,649
tries to connect to an inactive IP

43
00:02:01,020 --> 00:02:05,940
address and does nothing because now no

44
00:02:04,649 --> 00:02:10,770
one is commanding it to do anything

45
00:02:05,940 --> 00:02:12,540
malicious so is this still malware as it

46
00:02:10,770 --> 00:02:16,170
turned out the answer depends on who you

47
00:02:12,540 --> 00:02:19,019
ask and when you ask it so one way to

48
00:02:16,170 --> 00:02:22,219
find the maliciousness of any file is to

49
00:02:19,020 --> 00:02:24,870
ask virus total virus tool shows the

50
00:02:22,219 --> 00:02:28,530
decision of many commercial antivirus

51
00:02:24,870 --> 00:02:32,130
systems in one place if you at

52
00:02:28,530 --> 00:02:34,560
we were able to find a specific malware

53
00:02:32,130 --> 00:02:37,380
that fits this definition for this piece

54
00:02:34,560 --> 00:02:39,900
of malware it was connecting to the

55
00:02:37,380 --> 00:02:42,900
command and control server last year and

56
00:02:39,900 --> 00:02:45,480
over time now the control server is

57
00:02:42,900 --> 00:02:49,370
inactive so now it does not do anything

58
00:02:45,480 --> 00:02:52,049
jakoli the virus total in September 2019

59
00:02:49,370 --> 00:02:54,870
42% of the antivirus is considered as

60
00:02:52,050 --> 00:02:58,530
malware but if you check virus total in

61
00:02:54,870 --> 00:03:01,170
January 2020 72% of the antivirus

62
00:02:58,530 --> 00:03:03,239
consider it as malware so there's still

63
00:03:01,170 --> 00:03:06,329
things to consider here first is that

64
00:03:03,239 --> 00:03:09,480
the levels of antivirus has changed over

65
00:03:06,330 --> 00:03:12,060
time so these labels are not constant

66
00:03:09,480 --> 00:03:14,549
static labels and the second thing is

67
00:03:12,060 --> 00:03:17,220
that all the antivirus companies are not

68
00:03:14,550 --> 00:03:20,790
always in consensus about what is

69
00:03:17,220 --> 00:03:23,190
malware so that brings me to the first

70
00:03:20,790 --> 00:03:25,829
question of this talk is that how can we

71
00:03:23,190 --> 00:03:27,840
expect to protect users from malware

72
00:03:25,830 --> 00:03:29,670
when people who are in the business of

73
00:03:27,840 --> 00:03:33,780
detecting malware are not always in

74
00:03:29,670 --> 00:03:35,940
consensus about what is malware so let's

75
00:03:33,780 --> 00:03:38,670
take a step back let's consider that you

76
00:03:35,940 --> 00:03:41,880
are a malware researcher and you want to

77
00:03:38,670 --> 00:03:44,820
figure out the best way the best source

78
00:03:41,880 --> 00:03:48,090
of malware by doing analysis by yourself

79
00:03:44,820 --> 00:03:50,790
so the best way to do this would be run

80
00:03:48,090 --> 00:03:53,940
a piece of code on an end users machine

81
00:03:50,790 --> 00:03:56,640
do static and dynamic analysis now this

82
00:03:53,940 --> 00:03:58,890
is obviously not realistic because

83
00:03:56,640 --> 00:04:01,828
people are not gonna lend me their

84
00:03:58,890 --> 00:04:03,540
machines to run malware's on so you can

85
00:04:01,829 --> 00:04:05,970
do the same kind of analysis on a

86
00:04:03,540 --> 00:04:07,980
virtual machines this kind of analysis

87
00:04:05,970 --> 00:04:10,019
will take hours to do just for one

88
00:04:07,980 --> 00:04:13,078
malware so this is not very realistic a

89
00:04:10,019 --> 00:04:15,299
better way to get dynamic and static

90
00:04:13,079 --> 00:04:18,000
analysis data is using an open source

91
00:04:15,299 --> 00:04:21,810
sandbox for example ku-ku-ku-ku

92
00:04:18,000 --> 00:04:24,090
performs dynamic and now a dynamic and

93
00:04:21,810 --> 00:04:27,900
static analysis of codes and provides a

94
00:04:24,090 --> 00:04:30,359
suspiciousness core and based on some

95
00:04:27,900 --> 00:04:33,000
threshold you can consider a piece of

96
00:04:30,360 --> 00:04:35,190
code as malicious if it has a certain

97
00:04:33,000 --> 00:04:37,169
level of suspiciousness score this

98
00:04:35,190 --> 00:04:39,570
usually takes one or two minutes to do

99
00:04:37,169 --> 00:04:41,760
profile which is still a really

100
00:04:39,570 --> 00:04:44,159
time-consuming if you want

101
00:04:41,760 --> 00:04:46,500
to do this analysis for the millions of

102
00:04:44,160 --> 00:04:50,670
data that you need to train a robust

103
00:04:46,500 --> 00:04:53,340
machine learning model so this is why

104
00:04:50,670 --> 00:04:55,200
research papers take a shortcut and they

105
00:04:53,340 --> 00:04:57,810
get labels from other sources

106
00:04:55,200 --> 00:05:00,810
we studied fully papers from 2001 to

107
00:04:57,810 --> 00:05:04,770
2009 teen to check how do they get their

108
00:05:00,810 --> 00:05:06,330
ground truth from so it seems that a 50%

109
00:05:04,770 --> 00:05:08,460
of these papers get their ground truth

110
00:05:06,330 --> 00:05:10,520
from a collection of data that someone

111
00:05:08,460 --> 00:05:13,280
else put out some researchers

112
00:05:10,520 --> 00:05:16,140
meticulously did some analysis and

113
00:05:13,280 --> 00:05:18,000
created a repository of malware so then

114
00:05:16,140 --> 00:05:20,340
other research papers use that label

115
00:05:18,000 --> 00:05:23,370
data to train their train and evaluate

116
00:05:20,340 --> 00:05:26,640
their system another popular way to get

117
00:05:23,370 --> 00:05:29,250
antiva labels for malware data is from

118
00:05:26,640 --> 00:05:32,490
virus to virus total shows the antivirus

119
00:05:29,250 --> 00:05:35,160
labels of many from many different anti

120
00:05:32,490 --> 00:05:36,720
viruses so you can use those labels to

121
00:05:35,160 --> 00:05:40,500
decide if something is malicious or not

122
00:05:36,720 --> 00:05:42,960
a small amount of small number of papers

123
00:05:40,500 --> 00:05:44,700
we found only three that did some manual

124
00:05:42,960 --> 00:05:48,719
analysis on a small amount of data and

125
00:05:44,700 --> 00:05:51,750
use that for training and evaluating the

126
00:05:48,720 --> 00:05:55,590
papers that use virustotal data to

127
00:05:51,750 --> 00:05:58,380
decide what is malicious are not in

128
00:05:55,590 --> 00:06:00,030
consensus about what is the right

129
00:05:58,380 --> 00:06:02,370
threshold to figure out the

130
00:06:00,030 --> 00:06:04,080
maliciousness of a file we found that

131
00:06:02,370 --> 00:06:06,600
there are some papers that use labels

132
00:06:04,080 --> 00:06:08,789
from one antivirus system some papers

133
00:06:06,600 --> 00:06:10,500
consider that if four antivirus system

134
00:06:08,790 --> 00:06:12,810
consider something as malware then it

135
00:06:10,500 --> 00:06:15,600
smelled some papers saying no the

136
00:06:12,810 --> 00:06:18,060
threshold should be 5 and maybe 10 is a

137
00:06:15,600 --> 00:06:19,560
better threshold or maybe all of the

138
00:06:18,060 --> 00:06:22,650
antivirus should consider something as

139
00:06:19,560 --> 00:06:24,180
malware for a file chip in our some

140
00:06:22,650 --> 00:06:26,159
papers think that maybe we should do

141
00:06:24,180 --> 00:06:28,230
some kind of majority voting other

142
00:06:26,160 --> 00:06:29,940
papers notice that all the anti viruses

143
00:06:28,230 --> 00:06:31,650
are not equally accurate so doing

144
00:06:29,940 --> 00:06:33,780
majority voting it's not a good idea so

145
00:06:31,650 --> 00:06:36,690
maybe we should do weighted majority

146
00:06:33,780 --> 00:06:38,880
voting so you can see here that research

147
00:06:36,690 --> 00:06:42,060
papers are also not in consensus about

148
00:06:38,880 --> 00:06:45,330
what what is the best threshold to use

149
00:06:42,060 --> 00:06:47,580
and when you're when you're building a

150
00:06:45,330 --> 00:06:49,650
data set for training and evaluating our

151
00:06:47,580 --> 00:06:53,430
system what is the basis of the data

152
00:06:49,650 --> 00:06:55,289
that is problematic because that suppose

153
00:06:53,430 --> 00:06:55,440
you are a malware researcher and trying

154
00:06:55,290 --> 00:06:57,180
to

155
00:06:55,440 --> 00:06:58,860
figure out the best way to detect

156
00:06:57,180 --> 00:07:00,690
malware and you were reading all these

157
00:06:58,860 --> 00:07:03,540
papers but how do you compare them

158
00:07:00,690 --> 00:07:05,460
because each one of them used a

159
00:07:03,540 --> 00:07:06,840
different data source and a different

160
00:07:05,460 --> 00:07:08,760
threshold and a different way of

161
00:07:06,840 --> 00:07:10,440
evaluating their approach

162
00:07:08,760 --> 00:07:12,450
so just by reading these papers and

163
00:07:10,440 --> 00:07:14,430
comparing their accuracies are not going

164
00:07:12,450 --> 00:07:20,580
to give you a much a much better idea

165
00:07:14,430 --> 00:07:24,900
about how to detect malware so we did a

166
00:07:20,580 --> 00:07:27,229
study of antivirus systems to figure out

167
00:07:24,900 --> 00:07:31,859
this problem we noticed that on

168
00:07:27,230 --> 00:07:36,870
virustotal top antivirus companies are

169
00:07:31,860 --> 00:07:38,820
sort of in our consensus they but we

170
00:07:36,870 --> 00:07:41,460
also noticed that the labels of the

171
00:07:38,820 --> 00:07:45,360
antivirus systems change over time in

172
00:07:41,460 --> 00:07:48,180
our data sets we found that 96% the anti

173
00:07:45,360 --> 00:07:51,780
viruses reach 96% agreement on our data

174
00:07:48,180 --> 00:07:55,440
set after three days and reached 99%

175
00:07:51,780 --> 00:07:57,659
agreement after three weeks so this

176
00:07:55,440 --> 00:08:01,280
gives us an idea about a simple

177
00:07:57,660 --> 00:08:04,350
heuristics that we can use to create a

178
00:08:01,280 --> 00:08:06,030
stable ground through data set so there

179
00:08:04,350 --> 00:08:08,490
are two things to consider here again

180
00:08:06,030 --> 00:08:11,549
one is that antivirus labels change over

181
00:08:08,490 --> 00:08:13,680
time so we shouldn't use the label that

182
00:08:11,550 --> 00:08:15,930
appeared today we should wait for a

183
00:08:13,680 --> 00:08:19,050
while to for the label to get stabilized

184
00:08:15,930 --> 00:08:22,020
and the second point is that anti

185
00:08:19,050 --> 00:08:24,510
viruses are not in consensus about every

186
00:08:22,020 --> 00:08:26,580
piece of file about the maliciousness of

187
00:08:24,510 --> 00:08:28,710
the file so instead of when we are

188
00:08:26,580 --> 00:08:31,200
building this round two data set instead

189
00:08:28,710 --> 00:08:33,329
of taking all every piece of malicious

190
00:08:31,200 --> 00:08:35,669
data we should focus on the piece of

191
00:08:33,330 --> 00:08:38,039
data that the majority of the antivirus

192
00:08:35,669 --> 00:08:39,890
companies current consensus about that

193
00:08:38,039 --> 00:08:43,049
they are malicious stuff

194
00:08:39,890 --> 00:08:44,970
so now let's suppose now that we figured

195
00:08:43,049 --> 00:08:47,910
out how to get the best ground two data

196
00:08:44,970 --> 00:08:51,300
to train and evaluate our data we build

197
00:08:47,910 --> 00:08:53,959
our classifier how would we evaluate the

198
00:08:51,300 --> 00:08:56,910
performance of this classifier in a

199
00:08:53,960 --> 00:08:58,530
general machine learning case we focus

200
00:08:56,910 --> 00:09:01,319
on the overall performance of the

201
00:08:58,530 --> 00:09:02,939
classifier but in security the overall

202
00:09:01,320 --> 00:09:05,310
performance of the classifier does not

203
00:09:02,940 --> 00:09:08,250
matter much because in security we are

204
00:09:05,310 --> 00:09:09,150
concerned about the special cases the

205
00:09:08,250 --> 00:09:11,460
rare events

206
00:09:09,150 --> 00:09:13,530
even though the number of malware today

207
00:09:11,460 --> 00:09:15,690
are way more than the number of malware

208
00:09:13,530 --> 00:09:18,920
we were still 10 years ago still on a

209
00:09:15,690 --> 00:09:21,630
regular basis and end-user has way more

210
00:09:18,920 --> 00:09:23,370
benign files than malware files so we

211
00:09:21,630 --> 00:09:25,140
need to make sure that the false

212
00:09:23,370 --> 00:09:28,860
positive rates of this classifiers are

213
00:09:25,140 --> 00:09:30,720
really small so instead of focusing on

214
00:09:28,860 --> 00:09:32,940
the entire performance of the pacifier

215
00:09:30,720 --> 00:09:35,940
we need to prepare classifiers in this

216
00:09:32,940 --> 00:09:37,500
in this zone of interest where the false

217
00:09:35,940 --> 00:09:40,200
positive rate of the classifier is

218
00:09:37,500 --> 00:09:45,380
significantly low less than 1% but it

219
00:09:40,200 --> 00:09:45,380
also has a high true positive over 90%

220
00:09:45,530 --> 00:09:51,180
whenever we are talking about using

221
00:09:48,900 --> 00:09:53,670
machine learning in security we have to

222
00:09:51,180 --> 00:09:56,219
think about adversarial attacks because

223
00:09:53,670 --> 00:09:58,199
at least in security we always have this

224
00:09:56,220 --> 00:10:01,290
active adversary who are always trying

225
00:09:58,200 --> 00:10:04,410
to evade our system adversarial attacks

226
00:10:01,290 --> 00:10:06,030
is one of the mostly research area in

227
00:10:04,410 --> 00:10:08,760
machine learning and security there's

228
00:10:06,030 --> 00:10:13,170
more than 1500 papers on adversarial

229
00:10:08,760 --> 00:10:15,750
machine learning since 2014 many of this

230
00:10:13,170 --> 00:10:17,849
attack papers many of these papers are

231
00:10:15,750 --> 00:10:20,150
also defense papers but they often get

232
00:10:17,850 --> 00:10:24,600
broken before the papers get published

233
00:10:20,150 --> 00:10:26,910
we wanted to see do these papers help us

234
00:10:24,600 --> 00:10:28,560
understand the security of machine

235
00:10:26,910 --> 00:10:32,010
learning do this paper help us

236
00:10:28,560 --> 00:10:34,140
understand how many of them many of

237
00:10:32,010 --> 00:10:37,650
these attacks can be used to evade a

238
00:10:34,140 --> 00:10:40,920
real-world antivirus systems we found

239
00:10:37,650 --> 00:10:44,040
that only 36 of these papers only 2% of

240
00:10:40,920 --> 00:10:47,189
these papers tried to focus on evading

241
00:10:44,040 --> 00:10:50,010
malware detectors another thing we

242
00:10:47,190 --> 00:10:51,690
notice is that all of these papers focus

243
00:10:50,010 --> 00:10:53,730
on answering this question can

244
00:10:51,690 --> 00:10:57,660
adversarial malware evade malware

245
00:10:53,730 --> 00:10:59,820
detectors but a better question to

246
00:10:57,660 --> 00:11:02,280
answer for the security audience would

247
00:10:59,820 --> 00:11:05,640
be our adversarial attacks harmful for

248
00:11:02,280 --> 00:11:07,380
users now if you look at this question

249
00:11:05,640 --> 00:11:09,810
it seems that these questions are the

250
00:11:07,380 --> 00:11:12,060
same question right that an adversarial

251
00:11:09,810 --> 00:11:14,849
malware that evades malware detectors

252
00:11:12,060 --> 00:11:18,510
are obviously going to be harmful for

253
00:11:14,850 --> 00:11:20,670
users but not quite it's because the way

254
00:11:18,510 --> 00:11:23,189
there there are more than one ways to do

255
00:11:20,670 --> 00:11:24,930
adversarial attacks one way to do

256
00:11:23,190 --> 00:11:26,910
adversary attacks is changing the

257
00:11:24,930 --> 00:11:30,089
malware file and create a real-world

258
00:11:26,910 --> 00:11:32,459
malware we call this kind of attack the

259
00:11:30,090 --> 00:11:35,160
problem space attack another way to do

260
00:11:32,460 --> 00:11:36,840
adversary attack is to do the attack on

261
00:11:35,160 --> 00:11:39,300
the feature vector that you extracted

262
00:11:36,840 --> 00:11:42,060
from the malware file so if you are

263
00:11:39,300 --> 00:11:45,390
concerned about how well your mission

264
00:11:42,060 --> 00:11:47,880
learning model do when you do do

265
00:11:45,390 --> 00:11:50,189
adversity attacks then doing the attack

266
00:11:47,880 --> 00:11:53,580
on the feature vector is good enough but

267
00:11:50,190 --> 00:11:55,260
if you want to figure out whether this

268
00:11:53,580 --> 00:11:58,200
adversarial attacks are harmful to users

269
00:11:55,260 --> 00:12:00,090
you have to be able to transfer your

270
00:11:58,200 --> 00:12:02,700
attack from the feature vector to a

271
00:12:00,090 --> 00:12:05,430
real-world malware and see is this

272
00:12:02,700 --> 00:12:09,090
realistic is this malware it can this

273
00:12:05,430 --> 00:12:11,430
malware existed the real world so now

274
00:12:09,090 --> 00:12:13,650
let me give you an example why this is

275
00:12:11,430 --> 00:12:15,930
not the feature vector and problem

276
00:12:13,650 --> 00:12:18,329
problem space are different for malware

277
00:12:15,930 --> 00:12:21,920
because in other domains it's not that

278
00:12:18,330 --> 00:12:25,230
different for example images one of the

279
00:12:21,920 --> 00:12:27,810
one of the most common attack that you

280
00:12:25,230 --> 00:12:29,940
can do what a common adversary technique

281
00:12:27,810 --> 00:12:32,280
and during the malware doing is adding a

282
00:12:29,940 --> 00:12:34,170
new section to the Miller since you are

283
00:12:32,280 --> 00:12:36,449
just adding a garbage section to the

284
00:12:34,170 --> 00:12:40,680
malware it should not affect the

285
00:12:36,450 --> 00:12:43,770
maliciousness of the malware but that's

286
00:12:40,680 --> 00:12:45,780
not what happens in reality when we try

287
00:12:43,770 --> 00:12:47,550
to do this attack on some of the Milberg

288
00:12:45,780 --> 00:12:50,520
we noticed that in many cases it

289
00:12:47,550 --> 00:12:52,979
produced a malware that that was not

290
00:12:50,520 --> 00:12:56,310
even a valid application and in some

291
00:12:52,980 --> 00:12:59,190
cases is produced a malware that was not

292
00:12:56,310 --> 00:13:01,829
malicious anymore this usually happens

293
00:12:59,190 --> 00:13:05,490
when the new section you added overrides

294
00:13:01,830 --> 00:13:08,400
an existing section now I will give you

295
00:13:05,490 --> 00:13:13,380
two examples why this this override

296
00:13:08,400 --> 00:13:15,660
action can happen so here's a primer on

297
00:13:13,380 --> 00:13:18,660
how Windows malware work so Windows

298
00:13:15,660 --> 00:13:21,120
malware have three sections the headers

299
00:13:18,660 --> 00:13:23,280
the executable sections and an overlay

300
00:13:21,120 --> 00:13:26,460
section that does not get loaded when

301
00:13:23,280 --> 00:13:27,770
you load the program in the memory so

302
00:13:26,460 --> 00:13:30,840
some malware

303
00:13:27,770 --> 00:13:33,000
keep some of their maliciousness in the

304
00:13:30,840 --> 00:13:34,670
executable section but keep many of

305
00:13:33,000 --> 00:13:36,800
their malicious

306
00:13:34,670 --> 00:13:38,689
in the overlay section and once they are

307
00:13:36,800 --> 00:13:40,880
loaded in the memory they load the

308
00:13:38,690 --> 00:13:44,000
malicious code from the overlay section

309
00:13:40,880 --> 00:13:45,740
to the memory so in some times when you

310
00:13:44,000 --> 00:13:48,470
try to add a new section you might

311
00:13:45,740 --> 00:13:50,930
override the overlay section and in that

312
00:13:48,470 --> 00:13:54,830
case the malicious functionality of the

313
00:13:50,930 --> 00:13:56,660
malware might get a factor another case

314
00:13:54,830 --> 00:13:59,420
when you are trying to add a new section

315
00:13:56,660 --> 00:14:03,620
you have to change the header of the

316
00:13:59,420 --> 00:14:05,540
file so that the file has an idea that

317
00:14:03,620 --> 00:14:07,220
the new section has been added but in

318
00:14:05,540 --> 00:14:09,170
some cases if you don't have enough

319
00:14:07,220 --> 00:14:11,060
space in the file header it might

320
00:14:09,170 --> 00:14:13,430
override some of the sections in your

321
00:14:11,060 --> 00:14:15,800
file and that could also result in a

322
00:14:13,430 --> 00:14:20,569
corrupted file or a file that just does

323
00:14:15,800 --> 00:14:22,640
not do anything so we wanted to see in

324
00:14:20,570 --> 00:14:24,530
all the research papers how many ask

325
00:14:22,640 --> 00:14:27,680
this question that our adversarial

326
00:14:24,530 --> 00:14:30,439
attacks harmful to users we found that

327
00:14:27,680 --> 00:14:32,359
only one-fourth of the papers changed

328
00:14:30,440 --> 00:14:34,130
the malware files the rest of the papers

329
00:14:32,360 --> 00:14:37,760
did their adversarial attacks only on

330
00:14:34,130 --> 00:14:40,790
the feature vectors and one of these

331
00:14:37,760 --> 00:14:45,260
papers only four papers try to execute

332
00:14:40,790 --> 00:14:47,030
the file now just trying to execute the

333
00:14:45,260 --> 00:14:49,040
file and see that the file works does

334
00:14:47,030 --> 00:14:51,770
not help us answer this question because

335
00:14:49,040 --> 00:14:54,170
the file has to be malicious for this

336
00:14:51,770 --> 00:14:55,910
attack to be harmful to users so do you

337
00:14:54,170 --> 00:15:01,910
wanna guess how many papers checked if

338
00:14:55,910 --> 00:15:04,240
the attack was harmful to users yes so

339
00:15:01,910 --> 00:15:06,530
this is probably one of the reasons why

340
00:15:04,240 --> 00:15:09,230
academics do this adversarial attacks

341
00:15:06,530 --> 00:15:11,420
and contact industry industry people are

342
00:15:09,230 --> 00:15:15,320
often like thanks for letting us know

343
00:15:11,420 --> 00:15:18,800
but we don't know what to do another

344
00:15:15,320 --> 00:15:22,100
reason could be that evading one

345
00:15:18,800 --> 00:15:24,229
classifier is often not enough to evade

346
00:15:22,100 --> 00:15:26,570
the entire pipeline because commercial

347
00:15:24,230 --> 00:15:30,080
AVS has to go through a series of

348
00:15:26,570 --> 00:15:32,060
detection before it it gets detected as

349
00:15:30,080 --> 00:15:34,070
benign or malware there are signature

350
00:15:32,060 --> 00:15:38,420
based detection there static detection

351
00:15:34,070 --> 00:15:41,120
and there's dynamic detection you can do

352
00:15:38,420 --> 00:15:43,400
an adversarial attack by evading the

353
00:15:41,120 --> 00:15:46,520
static detection but if you click and

354
00:15:43,400 --> 00:15:47,449
run the malware it we'll get detected by

355
00:15:46,520 --> 00:15:50,929
the

356
00:15:47,449 --> 00:15:54,469
dynamic analyzer so we in research case

357
00:15:50,929 --> 00:15:56,660
just show the static detection that that

358
00:15:54,470 --> 00:15:59,089
shows that we were able to evade the

359
00:15:56,660 --> 00:16:02,089
static detector but it wasn't able to

360
00:15:59,089 --> 00:16:04,459
show that this malware actually runs and

361
00:16:02,089 --> 00:16:06,499
it's harmful to the users so as far as

362
00:16:04,459 --> 00:16:07,969
anti viruses are concerned adversarial

363
00:16:06,499 --> 00:16:10,639
attacks are still not harmful to the

364
00:16:07,970 --> 00:16:13,129
users so that brings me to the last

365
00:16:10,639 --> 00:16:14,989
point of my talk whenever we are

366
00:16:13,129 --> 00:16:16,970
building our security applications we

367
00:16:14,989 --> 00:16:19,999
need to worry about we need to talk

368
00:16:16,970 --> 00:16:22,429
about who is the adversary in the

369
00:16:19,999 --> 00:16:24,109
cryptographic system we have a notion of

370
00:16:22,429 --> 00:16:26,179
an adversary which is bounded by

371
00:16:24,109 --> 00:16:28,189
computational complexity but we don't

372
00:16:26,179 --> 00:16:30,799
have such a notion of an adversary in

373
00:16:28,189 --> 00:16:32,779
the machine learning case papers usually

374
00:16:30,799 --> 00:16:34,759
talk about white-box adversaries that

375
00:16:32,779 --> 00:16:36,919
have full access to the system or

376
00:16:34,759 --> 00:16:39,259
blackbox adversary that has no access to

377
00:16:36,919 --> 00:16:41,329
the system but if you read those papers

378
00:16:39,259 --> 00:16:42,859
the fine prints and most of the papers

379
00:16:41,329 --> 00:16:45,378
are actually considering gray box

380
00:16:42,859 --> 00:16:49,790
attacks where adversaries have a wide

381
00:16:45,379 --> 00:16:51,470
range of incomparable capabilities so

382
00:16:49,790 --> 00:16:53,899
some papers consider that adversaries

383
00:16:51,470 --> 00:16:56,600
have full access to the features or the

384
00:16:53,899 --> 00:16:58,160
adversary's can do unlimited queries the

385
00:16:56,600 --> 00:17:01,730
adversaries have access to the training

386
00:16:58,160 --> 00:17:05,029
data the adversary can create a perfect

387
00:17:01,730 --> 00:17:07,220
substitute classifier now some of these

388
00:17:05,029 --> 00:17:09,769
are some of these capabilities are

389
00:17:07,220 --> 00:17:11,389
realistic for example the malware author

390
00:17:09,769 --> 00:17:13,039
and the malware defender are both

391
00:17:11,388 --> 00:17:15,138
working with malware so they probably

392
00:17:13,039 --> 00:17:17,959
have some ideas about the features and

393
00:17:15,138 --> 00:17:20,688
the training data but other capabilities

394
00:17:17,959 --> 00:17:22,669
are probably not that realistic for

395
00:17:20,689 --> 00:17:25,130
example if an adversary is sending

396
00:17:22,669 --> 00:17:27,730
unlimited queries to an antivirus system

397
00:17:25,130 --> 00:17:29,779
it will get blocked because most

398
00:17:27,730 --> 00:17:33,039
practical and diverse system will

399
00:17:29,779 --> 00:17:35,539
consider that as as a malicious action

400
00:17:33,039 --> 00:17:37,820
most importantly though there's just no

401
00:17:35,539 --> 00:17:39,200
way for us to compare the capabilities

402
00:17:37,820 --> 00:17:42,260
of all these different kinds of

403
00:17:39,200 --> 00:17:44,029
adversaries an adversary that has full

404
00:17:42,260 --> 00:17:46,190
access to the features or the adversary

405
00:17:44,029 --> 00:17:49,070
can do unlimited queries which one is

406
00:17:46,190 --> 00:17:51,200
which one is the strongest adversary we

407
00:17:49,070 --> 00:17:53,480
don't have such a notion to compare this

408
00:17:51,200 --> 00:17:56,960
with suppressors and this is problematic

409
00:17:53,480 --> 00:17:59,000
because if we don't have a notion of the

410
00:17:56,960 --> 00:17:59,789
hardness of the attack and defense we

411
00:17:59,000 --> 00:18:02,489
cannot track

412
00:17:59,789 --> 00:18:04,169
progress in this area even if we defend

413
00:18:02,489 --> 00:18:07,409
against some sort of attack we cannot

414
00:18:04,169 --> 00:18:11,759
say that this is better than what it was

415
00:18:07,409 --> 00:18:13,229
before so in summary to build a

416
00:18:11,759 --> 00:18:15,299
realistic machine learning model for

417
00:18:13,229 --> 00:18:18,450
security we need to come up with a

418
00:18:15,299 --> 00:18:21,149
notion of a consistent ground truth we

419
00:18:18,450 --> 00:18:23,249
need to evaluate our classifiers in the

420
00:18:21,149 --> 00:18:26,699
realistic bound in low false positive

421
00:18:23,249 --> 00:18:29,639
rates we need a notion of measurable

422
00:18:26,700 --> 00:18:32,309
adversary so far we have simple

423
00:18:29,639 --> 00:18:35,279
heuristics to build consistent ground

424
00:18:32,309 --> 00:18:37,499
truth and evaluation but to create a

425
00:18:35,279 --> 00:18:42,470
measurable adversary is still an open

426
00:18:37,499 --> 00:18:42,470
problem and that ends my time thank you

427
00:18:44,020 --> 00:18:47,210
[Applause]

