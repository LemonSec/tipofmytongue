1
00:00:12,610 --> 00:00:17,619
today I'm gonna talk about privacy by

2
00:00:15,010 --> 00:00:19,960
design and I'll do that by giving a very

3
00:00:17,619 --> 00:00:21,220
brief introduction and list the

4
00:00:19,960 --> 00:00:24,310
requirements from gdpr

5
00:00:21,220 --> 00:00:27,340
and and then I'll move on to some of the

6
00:00:24,310 --> 00:00:29,380
challenges on actually applying privacy

7
00:00:27,340 --> 00:00:31,720
by design at the large scale as some of

8
00:00:29,380 --> 00:00:36,220
the strategies we have come off it at

9
00:00:31,720 --> 00:00:39,220
uber so privacy by design model it is

10
00:00:36,220 --> 00:00:43,500
popularized by gdpr is actually a quite

11
00:00:39,220 --> 00:00:46,420
all the notion it exists since 1995 it

12
00:00:43,500 --> 00:00:49,120
when it was first introduced some

13
00:00:46,420 --> 00:00:52,449
high-level principles over there

14
00:00:49,120 --> 00:00:54,879
including user centricity transparency

15
00:00:52,449 --> 00:00:58,358
privacy as the default setting and

16
00:00:54,879 --> 00:01:00,428
positive-sum so it's not privacy versus

17
00:00:58,359 --> 00:01:03,030
security privacy versus safety privacy

18
00:01:00,429 --> 00:01:06,430
and security and privacy and safety

19
00:01:03,030 --> 00:01:09,070
these principles were applied in

20
00:01:06,430 --> 00:01:11,380
specific domains such as smart grids and

21
00:01:09,070 --> 00:01:13,960
online target advertising but because

22
00:01:11,380 --> 00:01:16,449
they're quite high level and did not

23
00:01:13,960 --> 00:01:18,669
supply specific methodologies they never

24
00:01:16,450 --> 00:01:21,580
become popular and and they were also

25
00:01:18,670 --> 00:01:23,260
not enforced by a legislation so gdpr is

26
00:01:21,580 --> 00:01:26,460
the first one that actually requires

27
00:01:23,260 --> 00:01:29,650
privacy by design and by default and

28
00:01:26,460 --> 00:01:32,110
when you look at GDP are the particular

29
00:01:29,650 --> 00:01:34,060
article you will not find specific

30
00:01:32,110 --> 00:01:37,120
requirements like encryption for health

31
00:01:34,060 --> 00:01:39,810
data or a specific control you will have

32
00:01:37,120 --> 00:01:42,340
to come up with the controls yourself

33
00:01:39,810 --> 00:01:44,530
but they need to satisfy certain

34
00:01:42,340 --> 00:01:47,050
requirements so the first the control

35
00:01:44,530 --> 00:01:50,020
you just have to be appropriate what

36
00:01:47,050 --> 00:01:51,910
that means is it has to actually reduce

37
00:01:50,020 --> 00:01:53,619
the privacy risk towards an individual

38
00:01:51,910 --> 00:01:56,050
so you cannot have this umbrella set of

39
00:01:53,620 --> 00:01:58,570
controls less encryption and say yes we

40
00:01:56,050 --> 00:02:00,550
are now gdpr compliant you have to

41
00:01:58,570 --> 00:02:02,589
actually reduce the risk it has to be

42
00:02:00,550 --> 00:02:04,660
appropriate for the problem you're

43
00:02:02,590 --> 00:02:06,550
trying to solve it has to be effective

44
00:02:04,660 --> 00:02:08,679
so you need to demonstrate that your

45
00:02:06,550 --> 00:02:10,800
controls are actually working you can do

46
00:02:08,679 --> 00:02:14,019
it with some quantitative metrics or

47
00:02:10,800 --> 00:02:16,150
also qualitatively for example having

48
00:02:14,019 --> 00:02:18,580
expert checks or as having some audits

49
00:02:16,150 --> 00:02:21,760
you have to think about state of the art

50
00:02:18,580 --> 00:02:24,460
so you do not need to worry about

51
00:02:21,760 --> 00:02:25,600
experimental technologies but you have

52
00:02:24,460 --> 00:02:27,850
to follow the

53
00:02:25,600 --> 00:02:31,359
those best practices and standards that

54
00:02:27,850 --> 00:02:34,660
are coming out recently course can never

55
00:02:31,360 --> 00:02:37,870
be a reason not to comply but if you

56
00:02:34,660 --> 00:02:39,820
have a cheaper control that is as

57
00:02:37,870 --> 00:02:41,890
effective as the more expensive control

58
00:02:39,820 --> 00:02:44,470
you can obtain for the ship you control

59
00:02:41,890 --> 00:02:46,709
an example is if you're tied to a vendor

60
00:02:44,470 --> 00:02:50,200
and the vendor is charging a lot on

61
00:02:46,710 --> 00:02:52,360
automated deletions you can do periodic

62
00:02:50,200 --> 00:02:54,820
removals if that is cheaper for you and

63
00:02:52,360 --> 00:02:57,580
finally you need to think about the

64
00:02:54,820 --> 00:02:59,650
context the purpose and the risk so for

65
00:02:57,580 --> 00:03:02,980
example if you have health data you want

66
00:02:59,650 --> 00:03:05,320
to do marketing for that the controls

67
00:03:02,980 --> 00:03:08,049
you have to be in place has to be much

68
00:03:05,320 --> 00:03:10,060
stronger than just name an address and

69
00:03:08,050 --> 00:03:12,400
an marketing for example

70
00:03:10,060 --> 00:03:14,230
if you want to send some product

71
00:03:12,400 --> 00:03:16,210
information for sleep apnea to an

72
00:03:14,230 --> 00:03:18,130
individual that can actually imply that

73
00:03:16,210 --> 00:03:20,020
the individual is suffering from sleep

74
00:03:18,130 --> 00:03:21,609
apnea so just sending it by email is

75
00:03:20,020 --> 00:03:25,900
probably not the wisest choice

76
00:03:21,610 --> 00:03:28,330
and finally gdpr requires privacy

77
00:03:25,900 --> 00:03:30,430
controls to be embedded in all software

78
00:03:28,330 --> 00:03:33,490
development lifecycle so not just on the

79
00:03:30,430 --> 00:03:35,590
design or just launch phase but you need

80
00:03:33,490 --> 00:03:37,450
to be already thinking about privacy

81
00:03:35,590 --> 00:03:39,550
where you make a decision on the tech

82
00:03:37,450 --> 00:03:41,829
stack on your vendor and you're thinking

83
00:03:39,550 --> 00:03:44,950
about prototypes and after you launch

84
00:03:41,830 --> 00:03:47,110
the job is not over you have to look

85
00:03:44,950 --> 00:03:48,640
into the effectivity of controls you

86
00:03:47,110 --> 00:03:51,700
have to deal with data breaches if there

87
00:03:48,640 --> 00:03:53,589
is any data has to be deleted when no

88
00:03:51,700 --> 00:03:55,839
longer needed and vendors have to be

89
00:03:53,590 --> 00:03:58,140
checked and the NIST privacy framework

90
00:03:55,840 --> 00:04:00,370
that is released this month is also

91
00:03:58,140 --> 00:04:02,970
mentioning this that privacy has to be

92
00:04:00,370 --> 00:04:06,550
taught about all development life cycles

93
00:04:02,970 --> 00:04:07,210
so now these are the requirements from

94
00:04:06,550 --> 00:04:09,580
gdpr

95
00:04:07,210 --> 00:04:12,520
and there are also some guidelines

96
00:04:09,580 --> 00:04:16,359
coming from the authorities one thing

97
00:04:12,520 --> 00:04:19,390
they have in common they assume that the

98
00:04:16,358 --> 00:04:22,330
Safa Delton life cycle is clearly six

99
00:04:19,390 --> 00:04:23,860
sequential and it's clearly separated so

100
00:04:22,330 --> 00:04:26,080
there's a waterfall model so you have

101
00:04:23,860 --> 00:04:28,480
your requirements you will look into it

102
00:04:26,080 --> 00:04:30,400
you will put your privacy controls into

103
00:04:28,480 --> 00:04:32,680
design and you just evaluate it during

104
00:04:30,400 --> 00:04:35,620
the testing and it also assumes you have

105
00:04:32,680 --> 00:04:38,409
a rather simple software using a

106
00:04:35,620 --> 00:04:41,199
monolithic architecture so that you know

107
00:04:38,409 --> 00:04:43,839
where to place your controls and it also

108
00:04:41,199 --> 00:04:46,569
lists the rather well-known privacy

109
00:04:43,839 --> 00:04:48,669
principles such as transparency data

110
00:04:46,569 --> 00:04:52,110
minimization and accuracy so the job is

111
00:04:48,669 --> 00:04:55,779
simply to include them into your design

112
00:04:52,110 --> 00:04:58,449
but in complex architectures such as

113
00:04:55,779 --> 00:05:01,149
uber there is another challenge so for

114
00:04:58,449 --> 00:05:04,269
example an uber we have an iOS app

115
00:05:01,149 --> 00:05:07,479
Android hub actually over 30 apps and we

116
00:05:04,269 --> 00:05:10,300
have web services we have many of these

117
00:05:07,479 --> 00:05:13,869
services that do one thing well and rely

118
00:05:10,300 --> 00:05:16,839
on others to fulfill the task and in the

119
00:05:13,869 --> 00:05:20,319
end it looks like a snowflake like this

120
00:05:16,839 --> 00:05:22,479
over 3,000 web services that are used

121
00:05:20,319 --> 00:05:24,399
that is not really visible to the

122
00:05:22,479 --> 00:05:27,068
end-user but it's all happening behind

123
00:05:24,399 --> 00:05:30,939
the scenes so this creates particular

124
00:05:27,069 --> 00:05:34,059
challenges and second development is

125
00:05:30,939 --> 00:05:36,309
agile is a quite popular methodology

126
00:05:34,059 --> 00:05:39,069
that is being adopted by many of the

127
00:05:36,309 --> 00:05:40,959
companies and the change here is that

128
00:05:39,069 --> 00:05:42,459
you do not have requirements that are

129
00:05:40,959 --> 00:05:45,839
fixed where you can just put your

130
00:05:42,459 --> 00:05:48,550
privacy controls you need to be able to

131
00:05:45,839 --> 00:05:53,740
expect the change and change your

132
00:05:48,550 --> 00:05:55,360
controls accordingly so particular

133
00:05:53,740 --> 00:05:57,729
challenge we see from these two

134
00:05:55,360 --> 00:05:59,740
developments is you have this video data

135
00:05:57,729 --> 00:06:02,889
your mix of structured and unstructured

136
00:05:59,740 --> 00:06:05,829
data by different technologies and to

137
00:06:02,889 --> 00:06:07,959
actually discover them off the shell

138
00:06:05,829 --> 00:06:10,659
tools won't always work won't always

139
00:06:07,959 --> 00:06:12,909
scale and another particular challenge

140
00:06:10,659 --> 00:06:14,558
here is that you do not have stable

141
00:06:12,909 --> 00:06:16,748
architectural documentation so

142
00:06:14,559 --> 00:06:19,329
implementation might not match the

143
00:06:16,749 --> 00:06:21,189
documentation the originals owners might

144
00:06:19,329 --> 00:06:23,800
not be with the company so you might not

145
00:06:21,189 --> 00:06:26,589
know why a particular decision was made

146
00:06:23,800 --> 00:06:29,199
in that particular architecture so again

147
00:06:26,589 --> 00:06:31,539
to understand the system do you do not

148
00:06:29,199 --> 00:06:32,519
have a very good documentation all the

149
00:06:31,539 --> 00:06:36,639
time

150
00:06:32,519 --> 00:06:39,459
to deal with this the first thing we

151
00:06:36,639 --> 00:06:42,689
have to do is actually classify the type

152
00:06:39,459 --> 00:06:45,969
of personal data we have at uber so the

153
00:06:42,689 --> 00:06:48,159
list of tiers that we have defined range

154
00:06:45,969 --> 00:06:49,730
from highly restricted to public so

155
00:06:48,159 --> 00:06:52,039
highly restricted would be

156
00:06:49,730 --> 00:06:54,650
such as driver's license that's highly

157
00:06:52,040 --> 00:06:56,930
linkable and has a big privacy impact

158
00:06:54,650 --> 00:06:58,700
but you also have confidential

159
00:06:56,930 --> 00:07:01,760
information such as make and model and

160
00:06:58,700 --> 00:07:03,740
color of a car for a driver that does

161
00:07:01,760 --> 00:07:06,349
not always link to a particular

162
00:07:03,740 --> 00:07:08,360
individual but given a data set could be

163
00:07:06,350 --> 00:07:10,490
linkable and you also have of course

164
00:07:08,360 --> 00:07:13,790
public information that is already

165
00:07:10,490 --> 00:07:19,280
available and release such as in product

166
00:07:13,790 --> 00:07:22,400
brochures so after defining these tiers

167
00:07:19,280 --> 00:07:24,679
the job is not over so often how this

168
00:07:22,400 --> 00:07:27,169
works is privacy legal will look into

169
00:07:24,680 --> 00:07:30,620
existing legislations and come up with a

170
00:07:27,170 --> 00:07:31,760
draft list of personal data

171
00:07:30,620 --> 00:07:34,040
categorizations

172
00:07:31,760 --> 00:07:35,870
and privacy engineering and information

173
00:07:34,040 --> 00:07:38,000
security will think of appropriate

174
00:07:35,870 --> 00:07:40,490
controls and we'll look into real-life

175
00:07:38,000 --> 00:07:42,680
cases with other stakeholders and this

176
00:07:40,490 --> 00:07:45,290
will be done in an iterative fashion so

177
00:07:42,680 --> 00:07:49,130
the classification and controls will be

178
00:07:45,290 --> 00:07:51,770
applied during privacy reviews and the

179
00:07:49,130 --> 00:07:54,130
policy is changed accordingly from

180
00:07:51,770 --> 00:07:58,510
real-life experiences

181
00:07:54,130 --> 00:08:01,400
so now we have a list of privacy

182
00:07:58,510 --> 00:08:03,710
personal data categorizations the next

183
00:08:01,400 --> 00:08:05,989
step is of course to have a beta

184
00:08:03,710 --> 00:08:08,450
inventory this is needed to understand

185
00:08:05,990 --> 00:08:10,850
where the personal data is what sort of

186
00:08:08,450 --> 00:08:13,789
personal data is there and to think of

187
00:08:10,850 --> 00:08:17,450
further controls this we do with

188
00:08:13,790 --> 00:08:20,720
automatic tagging of personal data after

189
00:08:17,450 --> 00:08:23,500
determining the owners and specific

190
00:08:20,720 --> 00:08:26,360
systems and then verifying of this

191
00:08:23,500 --> 00:08:29,300
tagging and of course your data

192
00:08:26,360 --> 00:08:31,160
inventory will mature as you go so at

193
00:08:29,300 --> 00:08:33,710
the first start you will be able to do

194
00:08:31,160 --> 00:08:35,180
tagging at database level then you have

195
00:08:33,710 --> 00:08:37,070
to move through a tagging at column

196
00:08:35,179 --> 00:08:39,079
level and finally you should be able to

197
00:08:37,070 --> 00:08:41,180
identify all the weight of an individual

198
00:08:39,080 --> 00:08:43,070
and actually monitor this so at this

199
00:08:41,179 --> 00:08:45,439
start you'll be able to know which

200
00:08:43,070 --> 00:08:47,960
systems do I have and who are the owners

201
00:08:45,440 --> 00:08:50,840
and later you should be able to detect

202
00:08:47,960 --> 00:08:53,180
detect some violations where the

203
00:08:50,840 --> 00:08:55,070
policies is not being used such as

204
00:08:53,180 --> 00:08:57,170
sensitive data being copied to a

205
00:08:55,070 --> 00:08:59,690
repository that is not appropriate for

206
00:08:57,170 --> 00:09:02,189
this purpose and the output of this is

207
00:08:59,690 --> 00:09:04,320
very important it will have to fit in

208
00:09:02,190 --> 00:09:07,140
to several processes such as privacy

209
00:09:04,320 --> 00:09:10,020
reviews privacy rights processes that

210
00:09:07,140 --> 00:09:13,680
often get improved with date elementary

211
00:09:10,020 --> 00:09:16,890
a good data inventory second challenge

212
00:09:13,680 --> 00:09:18,709
we see from this micro service

213
00:09:16,890 --> 00:09:22,439
environment is how do you actually

214
00:09:18,710 --> 00:09:25,410
determine the threat when you evaluate a

215
00:09:22,440 --> 00:09:27,600
particular micro service let's say you

216
00:09:25,410 --> 00:09:30,000
evaluate the the one with the red dot

217
00:09:27,600 --> 00:09:33,210
that particular service and you place

218
00:09:30,000 --> 00:09:35,640
necessary controls but some privacy risk

219
00:09:33,210 --> 00:09:38,100
can still remain if another service is

220
00:09:35,640 --> 00:09:40,530
reading data from it and it affects the

221
00:09:38,100 --> 00:09:42,930
another service that brings a question

222
00:09:40,530 --> 00:09:46,560
very you place to control so let's say

223
00:09:42,930 --> 00:09:48,390
the yellow dot is the service that is

224
00:09:46,560 --> 00:09:50,310
connected to multiple ones ideally you

225
00:09:48,390 --> 00:09:51,630
want to put your control there for

226
00:09:50,310 --> 00:09:54,089
example retention you don't want to

227
00:09:51,630 --> 00:09:56,010
solve it at every single initiative in

228
00:09:54,090 --> 00:09:58,170
every single project but you first need

229
00:09:56,010 --> 00:10:00,660
to know that the blue and red are

230
00:09:58,170 --> 00:10:03,000
connected to the yellow and they always

231
00:10:00,660 --> 00:10:06,390
get connected to a service that is

232
00:10:03,000 --> 00:10:09,090
privacy valid and privacy can be a slow

233
00:10:06,390 --> 00:10:10,590
activity compared to agile because you

234
00:10:09,090 --> 00:10:13,260
have multiple stakeholders you have a

235
00:10:10,590 --> 00:10:16,860
legal that has a specific perception of

236
00:10:13,260 --> 00:10:19,110
risk you have InfoSec you have product

237
00:10:16,860 --> 00:10:22,350
owners that have an idea and to

238
00:10:19,110 --> 00:10:25,490
determining privacy risk for a product

239
00:10:22,350 --> 00:10:28,920
that will exist later that can be slow

240
00:10:25,490 --> 00:10:31,020
you have legacy systems of course and

241
00:10:28,920 --> 00:10:33,689
they come with privacy depth these

242
00:10:31,020 --> 00:10:36,240
systems exist since many years and a lot

243
00:10:33,690 --> 00:10:38,640
of others rely on those so changing them

244
00:10:36,240 --> 00:10:41,340
and making improvements can be difficult

245
00:10:38,640 --> 00:10:44,460
and finally this cost a lot of resource

246
00:10:41,340 --> 00:10:46,710
and time you have a limited set of

247
00:10:44,460 --> 00:10:51,120
privacy experts to cover the whole

248
00:10:46,710 --> 00:10:53,760
domain actually take some effort so how

249
00:10:51,120 --> 00:10:55,740
we do solve this or at least make an

250
00:10:53,760 --> 00:10:58,500
attempt at it is having technical

251
00:10:55,740 --> 00:11:00,839
privacy reviews so we offer a service to

252
00:10:58,500 --> 00:11:03,030
engineers and when they're designing

253
00:11:00,839 --> 00:11:05,190
their system or having some ideas we

254
00:11:03,030 --> 00:11:07,170
will discuss with them and ask the

255
00:11:05,190 --> 00:11:09,990
questions such as which personal data

256
00:11:07,170 --> 00:11:11,939
are you collecting or and where are you

257
00:11:09,990 --> 00:11:13,380
going to get it from and where will you

258
00:11:11,940 --> 00:11:15,340
actually store it who will have access

259
00:11:13,380 --> 00:11:17,400
to it why do you actually

260
00:11:15,340 --> 00:11:20,590
need it and how long do you need it and

261
00:11:17,400 --> 00:11:22,449
finally we have services such as a beta

262
00:11:20,590 --> 00:11:25,900
deletion service which I'll discuss in a

263
00:11:22,450 --> 00:11:28,210
second a consent service export service

264
00:11:25,900 --> 00:11:30,400
so that the engineers can onboard with

265
00:11:28,210 --> 00:11:32,760
these services without an effort so this

266
00:11:30,400 --> 00:11:35,020
kind of pushes the question and

267
00:11:32,760 --> 00:11:37,720
socializes the beta handling

268
00:11:35,020 --> 00:11:40,840
classification we have and brings

269
00:11:37,720 --> 00:11:42,670
privacy into their minds the outcome is

270
00:11:40,840 --> 00:11:44,860
technical privacy requirements for this

271
00:11:42,670 --> 00:11:46,810
specific project there will be list of

272
00:11:44,860 --> 00:11:48,430
mitigations some will have to be

273
00:11:46,810 --> 00:11:50,349
prioritized have to be there before

274
00:11:48,430 --> 00:11:51,880
launch some have to be there in a

275
00:11:50,350 --> 00:11:54,190
reasonable amount of time but can be

276
00:11:51,880 --> 00:11:55,930
done later and we will give our input to

277
00:11:54,190 --> 00:11:58,660
privacy legal as well so that they can

278
00:11:55,930 --> 00:12:01,050
do their compliance assessment but as I

279
00:11:58,660 --> 00:12:04,829
said assessing one particular service

280
00:12:01,050 --> 00:12:07,870
doesn't mean the privacy review is done

281
00:12:04,830 --> 00:12:09,790
based on the discovered knowledge we

282
00:12:07,870 --> 00:12:11,650
will look into the platform's larger

283
00:12:09,790 --> 00:12:13,900
platforms and detect them and embed

284
00:12:11,650 --> 00:12:16,150
privacy into them and we will also

285
00:12:13,900 --> 00:12:18,520
update or standards based on the

286
00:12:16,150 --> 00:12:24,160
knowledge it's always a iterative

287
00:12:18,520 --> 00:12:26,170
process and one final thing to tackle

288
00:12:24,160 --> 00:12:28,360
the resource issue of course we have a

289
00:12:26,170 --> 00:12:30,660
limited set of privacy experts and those

290
00:12:28,360 --> 00:12:33,400
privacy experts often don't have the

291
00:12:30,660 --> 00:12:35,860
low-level technical knowledge of each

292
00:12:33,400 --> 00:12:38,470
and every system and what we have

293
00:12:35,860 --> 00:12:41,890
piloted is a privacy champions program

294
00:12:38,470 --> 00:12:44,200
so privacy champions are the main point

295
00:12:41,890 --> 00:12:46,630
of contact within an organization they

296
00:12:44,200 --> 00:12:48,430
are responsible for training and

297
00:12:46,630 --> 00:12:51,340
creating privacy awareness within their

298
00:12:48,430 --> 00:12:53,709
organization and at later stages we

299
00:12:51,340 --> 00:12:55,930
would expect them to have things like

300
00:12:53,710 --> 00:12:58,570
actually detecting potential privacy

301
00:12:55,930 --> 00:13:00,670
issues trigger a privacy review if it

302
00:12:58,570 --> 00:13:02,980
hasn't started yet and actually help

303
00:13:00,670 --> 00:13:07,959
with validation since they often have

304
00:13:02,980 --> 00:13:10,360
the low level knowledge the final

305
00:13:07,960 --> 00:13:12,700
challenge I want to discuss about doing

306
00:13:10,360 --> 00:13:14,860
privacy at scale and deletion as a

307
00:13:12,700 --> 00:13:17,440
particular case and over we have

308
00:13:14,860 --> 00:13:19,660
multiple use cases for deletion so this

309
00:13:17,440 --> 00:13:21,940
can be initiated by the user so they

310
00:13:19,660 --> 00:13:24,040
might be asking their account to be

311
00:13:21,940 --> 00:13:27,040
deleted and any associated data has to

312
00:13:24,040 --> 00:13:27,279
expire there might be inactive users and

313
00:13:27,040 --> 00:13:29,889
there

314
00:13:27,279 --> 00:13:31,870
accounts have to be deleted because

315
00:13:29,889 --> 00:13:35,019
they're no longer with the platform and

316
00:13:31,870 --> 00:13:36,730
finally there might be a specific period

317
00:13:35,019 --> 00:13:38,829
of time where we need to keep the state

318
00:13:36,730 --> 00:13:41,439
and that needs to be deleted when that

319
00:13:38,829 --> 00:13:43,508
time passes so again as I said there is

320
00:13:41,439 --> 00:13:46,149
a variety of data stores and how

321
00:13:43,509 --> 00:13:49,240
deletion can be implemented the strategy

322
00:13:46,149 --> 00:13:51,850
will differ and you have to do it in a

323
00:13:49,240 --> 00:13:54,610
scalable manner because there are over

324
00:13:51,850 --> 00:13:58,269
70 million active monthly users at uber

325
00:13:54,610 --> 00:14:00,220
7 million drivers over 700 countries so

326
00:13:58,269 --> 00:14:03,490
deletion has to account for these

327
00:14:00,220 --> 00:14:05,649
challenges it has to be reliable so even

328
00:14:03,490 --> 00:14:08,019
if a deletion attempt fails that has to

329
00:14:05,649 --> 00:14:10,059
be detected and later retried again it

330
00:14:08,019 --> 00:14:12,160
has to be adaptable if the requirements

331
00:14:10,059 --> 00:14:14,410
legal requirements change how system

332
00:14:12,160 --> 00:14:16,660
handles it has to adapt to it and it has

333
00:14:14,410 --> 00:14:18,430
to be demonstratable so there should be

334
00:14:16,660 --> 00:14:22,480
some other trail to show that deletion

335
00:14:18,430 --> 00:14:24,370
has actually occurred so how this is

336
00:14:22,480 --> 00:14:27,370
sold is that there is a deletion service

337
00:14:24,370 --> 00:14:30,370
at over that actually orchestrate the

338
00:14:27,370 --> 00:14:32,920
deletion and tracks whether deletion has

339
00:14:30,370 --> 00:14:34,779
occurred properly so it talks so data

340
00:14:32,920 --> 00:14:37,779
custodian services that are the main

341
00:14:34,779 --> 00:14:41,620
owners of data and tracks whether

342
00:14:37,779 --> 00:14:43,360
deletion has happened properly and also

343
00:14:41,620 --> 00:14:45,399
it ensures that there is a cool-off

344
00:14:43,360 --> 00:14:48,519
period so in case users change their

345
00:14:45,399 --> 00:14:50,199
mind or there's a account takeover so

346
00:14:48,519 --> 00:14:52,300
that we do not want there this

347
00:14:50,199 --> 00:14:55,389
particular users data to be deleted and

348
00:14:52,300 --> 00:14:58,449
that can also be implemented and this

349
00:14:55,389 --> 00:15:01,540
system supports this large-scale of date

350
00:14:58,449 --> 00:15:04,300
and data stores we need be consciously

351
00:15:01,540 --> 00:15:06,730
on board if services to this service for

352
00:15:04,300 --> 00:15:09,128
data to be deleted and there is always a

353
00:15:06,730 --> 00:15:11,709
discussion with legal to understand

354
00:15:09,129 --> 00:15:14,079
which data should be deleted and when it

355
00:15:11,709 --> 00:15:16,258
should be deleted and it also has to be

356
00:15:14,079 --> 00:15:19,599
rather frictionless so that is easy for

357
00:15:16,259 --> 00:15:25,089
engineers to onboard to this service so

358
00:15:19,600 --> 00:15:28,540
that the there is no real challenge so

359
00:15:25,089 --> 00:15:31,540
that conclusions I have in this talk is

360
00:15:28,540 --> 00:15:33,099
privacy controls need to be chosen by

361
00:15:31,540 --> 00:15:35,550
the organization and later be

362
00:15:33,100 --> 00:15:38,080
demonstratable so there are no

363
00:15:35,550 --> 00:15:41,319
requirements you will find in

364
00:15:38,080 --> 00:15:43,800
here and existing guidelines that we see

365
00:15:41,320 --> 00:15:46,300
do not always address the challenges of

366
00:15:43,800 --> 00:15:49,209
agile and micro service environments so

367
00:15:46,300 --> 00:15:51,699
what we do currently is fix what you can

368
00:15:49,210 --> 00:15:53,620
insert the right privacy controls into

369
00:15:51,700 --> 00:15:55,810
your project but then discover the

370
00:15:53,620 --> 00:15:58,090
bigger challenges and always keep

371
00:15:55,810 --> 00:15:59,949
iterating because the requirements and

372
00:15:58,090 --> 00:16:02,740
underlying systems can change and

373
00:15:59,950 --> 00:16:05,320
finally providing privacy tools and

374
00:16:02,740 --> 00:16:07,920
services that are easy to adapt that to

375
00:16:05,320 --> 00:16:10,980
engineers really makes a difference

376
00:16:07,920 --> 00:16:14,118
thank you

377
00:16:10,980 --> 00:16:14,119
[Applause]

