1
00:00:16,000 --> 00:00:21,430
so trustworthy machining is crucial for

2
00:00:18,369 --> 00:00:22,750
many reasons and I'm gonna start the

3
00:00:21,430 --> 00:00:27,160
presentation by giving you an overview

4
00:00:22,750 --> 00:00:28,869
of several of these and maybe from from

5
00:00:27,160 --> 00:00:33,040
this you'll get the impression that

6
00:00:28,869 --> 00:00:34,930
there is a possibility that attacking

7
00:00:33,040 --> 00:00:37,480
missionarying is easier than defending

8
00:00:34,930 --> 00:00:39,699
it and so that this would result in an

9
00:00:37,480 --> 00:00:42,459
arms race between Decker's and defenders

10
00:00:39,699 --> 00:00:45,160
and this is exactly the goal of this

11
00:00:42,460 --> 00:00:47,860
presentation is to think about how we

12
00:00:45,160 --> 00:00:50,468
can design machine learning algorithms

13
00:00:47,860 --> 00:00:54,790
that are trustworthy and so by

14
00:00:50,469 --> 00:00:57,129
trustworthy here I mean a notion a

15
00:00:54,790 --> 00:01:00,309
composition of properties that include

16
00:00:57,129 --> 00:01:02,860
things like security privacy but also

17
00:01:00,309 --> 00:01:06,220
safety fairness and ethics so this is

18
00:01:02,860 --> 00:01:09,640
what I'll start by discussing first as

19
00:01:06,220 --> 00:01:12,369
we look through the entire pipeline of

20
00:01:09,640 --> 00:01:15,280
machine learning both during training

21
00:01:12,369 --> 00:01:18,130
and test time we've already seen

22
00:01:15,280 --> 00:01:20,500
instances of adversaries actively

23
00:01:18,130 --> 00:01:25,298
manipulating the inputs that algorithm

24
00:01:20,500 --> 00:01:27,460
serve processing so if we look at what

25
00:01:25,299 --> 00:01:31,150
malicious individuals did on Twitter

26
00:01:27,460 --> 00:01:34,270
when Microsoft released their teach at

27
00:01:31,150 --> 00:01:36,909
BOTS we quickly saw that they had the

28
00:01:34,270 --> 00:01:39,820
ability to poison the training set that

29
00:01:36,909 --> 00:01:42,430
this chat bot was using to produce new

30
00:01:39,820 --> 00:01:44,710
content and so the chat bot inevitably

31
00:01:42,430 --> 00:01:47,229
learned how to produce things like

32
00:01:44,710 --> 00:01:49,960
racist content so this is one of the

33
00:01:47,229 --> 00:01:51,579
tweets that the bot produced one of the

34
00:01:49,960 --> 00:01:55,539
only ones that I'm comfortable putting

35
00:01:51,579 --> 00:01:57,520
on this slide but even if the model is

36
00:01:55,540 --> 00:02:00,939
already trained and deployed and is

37
00:01:57,520 --> 00:02:03,759
making predictions because very often

38
00:02:00,939 --> 00:02:05,710
the input domain is so large the model

39
00:02:03,759 --> 00:02:09,310
will not make correct predictions on all

40
00:02:05,710 --> 00:02:11,079
of the possible inputs and so we've seen

41
00:02:09,310 --> 00:02:12,970
things like when YouTube uses machine

42
00:02:11,079 --> 00:02:15,250
learning to filter videos that are not

43
00:02:12,970 --> 00:02:17,170
appropriate for children some of the

44
00:02:15,250 --> 00:02:19,390
videos will evade this detection

45
00:02:17,170 --> 00:02:23,619
mechanism again because the model will

46
00:02:19,390 --> 00:02:27,010
not always correctly predict even when

47
00:02:23,620 --> 00:02:28,880
there are no adversary's at play we may

48
00:02:27,010 --> 00:02:30,560
be concerned about safety

49
00:02:28,880 --> 00:02:32,930
and there has been demonstrations by

50
00:02:30,560 --> 00:02:34,820
researchers taking mission earning

51
00:02:32,930 --> 00:02:37,070
algorithms use for self-driving

52
00:02:34,820 --> 00:02:39,470
applications and showing that these

53
00:02:37,070 --> 00:02:42,890
algorithms are very sensitive to simple

54
00:02:39,470 --> 00:02:45,200
changes of lighting in the images that

55
00:02:42,890 --> 00:02:47,929
they're processing which sort of result

56
00:02:45,200 --> 00:02:50,000
and very drastically different actions

57
00:02:47,930 --> 00:02:54,320
being taken here you can see how the

58
00:02:50,000 --> 00:02:56,450
algorithm would steer the wheel other

59
00:02:54,320 --> 00:02:59,750
aspects of trustworthy machining relate

60
00:02:56,450 --> 00:03:03,470
more to societal components of

61
00:02:59,750 --> 00:03:05,750
trustworthiness for instance as we're

62
00:03:03,470 --> 00:03:08,330
deploying machine learning algorithms we

63
00:03:05,750 --> 00:03:10,010
have to be careful that they are

64
00:03:08,330 --> 00:03:14,120
respectful of the data that they're

65
00:03:10,010 --> 00:03:16,760
analyzing and one component of this is

66
00:03:14,120 --> 00:03:19,010
privacy as we're using data that is

67
00:03:16,760 --> 00:03:20,510
sometimes very sensitive we have to be

68
00:03:19,010 --> 00:03:23,959
concerned that machine learning models

69
00:03:20,510 --> 00:03:27,380
may be memorizing some of their training

70
00:03:23,960 --> 00:03:29,780
data and this is in fact what some

71
00:03:27,380 --> 00:03:33,370
researchers demonstrated where they took

72
00:03:29,780 --> 00:03:37,580
official facial recognition model and

73
00:03:33,370 --> 00:03:40,760
they were able to reconstruct a face

74
00:03:37,580 --> 00:03:42,410
from one of the persons that were used

75
00:03:40,760 --> 00:03:44,899
in the training set so here what you see

76
00:03:42,410 --> 00:03:46,460
on the left is the person and on the

77
00:03:44,900 --> 00:03:51,770
right is the image that they were able

78
00:03:46,460 --> 00:03:53,780
to reconstruct and finally there are a

79
00:03:51,770 --> 00:03:56,180
lot of components that relate to the

80
00:03:53,780 --> 00:03:59,060
more ethical aspects of mission earning

81
00:03:56,180 --> 00:04:02,060
in particular if we look at the

82
00:03:59,060 --> 00:04:04,010
predictions of models on populations

83
00:04:02,060 --> 00:04:07,459
that are underrepresented in the data

84
00:04:04,010 --> 00:04:10,970
sets they typically tend to be less

85
00:04:07,459 --> 00:04:13,310
accurate so for instance again facial

86
00:04:10,970 --> 00:04:17,600
recognition models have been shown to

87
00:04:13,310 --> 00:04:21,620
perform poorly on images of women or

88
00:04:17,600 --> 00:04:25,820
people of color but there are also other

89
00:04:21,620 --> 00:04:28,430
components related to the progress that

90
00:04:25,820 --> 00:04:29,840
we're seeing in generative modeling so

91
00:04:28,430 --> 00:04:32,900
as models are getting better at

92
00:04:29,840 --> 00:04:36,710
generating content that is realistic we

93
00:04:32,900 --> 00:04:38,630
have to be concerned as to the ability

94
00:04:36,710 --> 00:04:40,169
of machine learning to manipulate public

95
00:04:38,630 --> 00:04:43,169
discourse

96
00:04:40,169 --> 00:04:44,729
that we are all interacting with and so

97
00:04:43,169 --> 00:04:48,810
I'll come back to this in the conclusion

98
00:04:44,729 --> 00:04:51,360
of this talk so next I'd like to discuss

99
00:04:48,810 --> 00:04:54,029
how we can design training algorithms

100
00:04:51,360 --> 00:04:57,270
that will provide these notions of

101
00:04:54,029 --> 00:05:00,300
trustworthy machine learning and escape

102
00:04:57,270 --> 00:05:04,020
this arms race but I'll start with a

103
00:05:00,300 --> 00:05:06,840
failed attempt just to capture how

104
00:05:04,020 --> 00:05:09,180
difficult this problem is so when we

105
00:05:06,840 --> 00:05:11,068
tried first to build models that

106
00:05:09,180 --> 00:05:12,990
recognize handwritten digits so this is

107
00:05:11,069 --> 00:05:14,789
what happy R is like a very simple

108
00:05:12,990 --> 00:05:16,229
problem you have images like the one

109
00:05:14,789 --> 00:05:17,789
that I'm showing on the screen and

110
00:05:16,229 --> 00:05:20,610
you're trying to tell with confidence

111
00:05:17,789 --> 00:05:22,949
what digit is in the image it turns out

112
00:05:20,610 --> 00:05:25,979
these models like many missionarying

113
00:05:22,949 --> 00:05:28,289
classifiers are very vulnerable to small

114
00:05:25,979 --> 00:05:30,089
perturbations of their inputs called

115
00:05:28,289 --> 00:05:34,050
adversarial examples and we'll hear more

116
00:05:30,089 --> 00:05:36,449
about those in the next talk and so to

117
00:05:34,050 --> 00:05:39,960
make models more robust to these

118
00:05:36,449 --> 00:05:42,270
perturbations the community next looked

119
00:05:39,960 --> 00:05:44,878
at a definition of robustness which

120
00:05:42,270 --> 00:05:47,430
basically requires that the model be

121
00:05:44,879 --> 00:05:48,930
constant around its training inputs so

122
00:05:47,430 --> 00:05:51,029
what you're seeing here on the left is

123
00:05:48,930 --> 00:05:54,300
you have a training image for the class

124
00:05:51,029 --> 00:05:56,639
3 which is in the center of this disc

125
00:05:54,300 --> 00:05:59,550
which is defined by a certain distance

126
00:05:56,639 --> 00:06:01,020
metric and you can see that the model

127
00:05:59,550 --> 00:06:03,449
that we've trained which is the dotted

128
00:06:01,020 --> 00:06:05,310
red line does not match exactly the

129
00:06:03,449 --> 00:06:10,949
ideal black line that we'd like to

130
00:06:05,310 --> 00:06:12,930
achieve and so we're asking the model so

131
00:06:10,949 --> 00:06:15,749
that it's less vulnerable to small

132
00:06:12,930 --> 00:06:18,389
perturbations to instead be constant in

133
00:06:15,749 --> 00:06:19,710
the disc around the training image and

134
00:06:18,389 --> 00:06:21,810
so we'll achieve

135
00:06:19,710 --> 00:06:24,448
models like this here where we have the

136
00:06:21,810 --> 00:06:28,379
dotted black line which is now squeezed

137
00:06:24,449 --> 00:06:31,020
between the two disks and is now robust

138
00:06:28,379 --> 00:06:33,629
to these small perturbations but by

139
00:06:31,020 --> 00:06:36,568
doing that because the reality is not

140
00:06:33,629 --> 00:06:38,729
made of these small disks we've made the

141
00:06:36,569 --> 00:06:41,669
model more vulnerable to new forms of

142
00:06:38,729 --> 00:06:44,490
attacks and it now misclassifies the

143
00:06:41,669 --> 00:06:46,560
image of a 5 here that is shown because

144
00:06:44,490 --> 00:06:49,379
it is very close according to this

145
00:06:46,560 --> 00:06:51,779
distance to the image of the three but

146
00:06:49,379 --> 00:06:52,150
semantically it's very different because

147
00:06:51,779 --> 00:06:54,550
it's

148
00:06:52,150 --> 00:06:57,849
a different class and so because we have

149
00:06:54,550 --> 00:06:59,860
this conflict between the definition of

150
00:06:57,850 --> 00:07:02,170
security and what we're trying to

151
00:06:59,860 --> 00:07:03,820
achieve with machine owning we're not

152
00:07:02,170 --> 00:07:06,730
able to achieve both at the same time

153
00:07:03,820 --> 00:07:11,260
and because the airspace is so large

154
00:07:06,730 --> 00:07:13,210
we've basically resulted in an arms race

155
00:07:11,260 --> 00:07:15,730
where we come up with defenses against

156
00:07:13,210 --> 00:07:19,870
specific attacks and then attackers come

157
00:07:15,730 --> 00:07:21,760
up with new attacks so does that mean

158
00:07:19,870 --> 00:07:24,670
that achieving trustworthy machining is

159
00:07:21,760 --> 00:07:27,250
any different than what we've seen in

160
00:07:24,670 --> 00:07:30,430
more traditional computer systems were

161
00:07:27,250 --> 00:07:33,040
very often we end up in an arms race and

162
00:07:30,430 --> 00:07:36,070
we have to balance the cost of

163
00:07:33,040 --> 00:07:37,810
protection with the risk of loss I would

164
00:07:36,070 --> 00:07:40,210
like to argue that it's not the case

165
00:07:37,810 --> 00:07:43,000
that machine learning brings a paradigm

166
00:07:40,210 --> 00:07:44,859
that is sufficiently novel for us to

167
00:07:43,000 --> 00:07:47,410
reason differently and in particular in

168
00:07:44,860 --> 00:07:49,990
a principled way and that is because

169
00:07:47,410 --> 00:07:52,200
machine learning makes it a lot easier

170
00:07:49,990 --> 00:07:56,470
to describe the behavior of the system

171
00:07:52,200 --> 00:07:59,640
formally and to convince you that that's

172
00:07:56,470 --> 00:08:01,240
the case I'm going to walk you through

173
00:07:59,640 --> 00:08:03,250
progress that we've made in

174
00:08:01,240 --> 00:08:04,960
privacy-preserving mission earning so

175
00:08:03,250 --> 00:08:07,420
whereas we've made very little progress

176
00:08:04,960 --> 00:08:09,609
in achieving a robust machine learning

177
00:08:07,420 --> 00:08:11,470
we've made a lot more progress in

178
00:08:09,610 --> 00:08:13,570
achieving privacy-preserving mission

179
00:08:11,470 --> 00:08:14,770
earning so what do we mean by

180
00:08:13,570 --> 00:08:16,870
privacy-preserving

181
00:08:14,770 --> 00:08:19,479
we fortunately have a definition called

182
00:08:16,870 --> 00:08:24,000
differential privacy which requires that

183
00:08:19,480 --> 00:08:26,860
the algorithm that we're analyzing have

184
00:08:24,000 --> 00:08:30,490
behavior that is indistinguishable to an

185
00:08:26,860 --> 00:08:32,890
observer when the algorithm is operating

186
00:08:30,490 --> 00:08:35,289
on the first version of the data that

187
00:08:32,890 --> 00:08:38,110
contains the record corresponding to an

188
00:08:35,289 --> 00:08:40,030
individual or when the algorithm is

189
00:08:38,110 --> 00:08:42,400
operating on the second version of the

190
00:08:40,030 --> 00:08:45,550
data that does not contain this record

191
00:08:42,400 --> 00:08:48,730
what this means is the person observing

192
00:08:45,550 --> 00:08:51,310
our algorithm is unable to tell whether

193
00:08:48,730 --> 00:08:53,770
we included the record for that specific

194
00:08:51,310 --> 00:08:56,469
individual and the data set or not and

195
00:08:53,770 --> 00:08:59,439
so in turn this person cannot learn

196
00:08:56,470 --> 00:09:01,150
anything about the individual or any

197
00:08:59,440 --> 00:09:04,150
private information that would have been

198
00:09:01,150 --> 00:09:06,420
included in the data set and we can

199
00:09:04,150 --> 00:09:09,160
formalize this by looking at a problem

200
00:09:06,420 --> 00:09:12,160
that the algorithm make certain outputs

201
00:09:09,160 --> 00:09:13,719
so how do we do this in practice well if

202
00:09:12,160 --> 00:09:15,130
we train missionary models typically

203
00:09:13,720 --> 00:09:17,350
we'll use something like stochastic

204
00:09:15,130 --> 00:09:19,150
gradient descent where we take a batch

205
00:09:17,350 --> 00:09:21,460
of data we compute the error of the

206
00:09:19,150 --> 00:09:24,400
model in this batch of data and we

207
00:09:21,460 --> 00:09:25,839
derive from this error gradients of the

208
00:09:24,400 --> 00:09:28,150
error with respect to the model

209
00:09:25,839 --> 00:09:30,550
parameters and this allows us to tune

210
00:09:28,150 --> 00:09:33,459
the model parameters to decrease the

211
00:09:30,550 --> 00:09:34,930
error of the model well if we want to do

212
00:09:33,460 --> 00:09:37,330
this with privacy we're going to make

213
00:09:34,930 --> 00:09:39,670
very small changes first we're going to

214
00:09:37,330 --> 00:09:42,720
compute the error on a per example basis

215
00:09:39,670 --> 00:09:45,849
what this means is we can then compute

216
00:09:42,720 --> 00:09:48,430
gradients of these errors with respect

217
00:09:45,850 --> 00:09:51,100
to model parameters also on a per

218
00:09:48,430 --> 00:09:53,800
example basis and so we know how much

219
00:09:51,100 --> 00:09:56,860
the training algorithm is extracting

220
00:09:53,800 --> 00:09:59,770
from each individual training example to

221
00:09:56,860 --> 00:10:02,740
update the model parameters and so if we

222
00:09:59,770 --> 00:10:05,920
clip these gradients we control their

223
00:10:02,740 --> 00:10:08,170
magnitude and we can bound how much the

224
00:10:05,920 --> 00:10:10,540
algorithm is sensitive to these

225
00:10:08,170 --> 00:10:12,370
individual training examples and then we

226
00:10:10,540 --> 00:10:14,110
add a little bit of noise to get the

227
00:10:12,370 --> 00:10:17,680
notion of indistinguishability that I

228
00:10:14,110 --> 00:10:19,089
discussed previously what's very

229
00:10:17,680 --> 00:10:22,000
interesting is if we look at

230
00:10:19,089 --> 00:10:24,010
privacy-preserving models their behavior

231
00:10:22,000 --> 00:10:27,520
tells us a lot about the data that

232
00:10:24,010 --> 00:10:30,910
they're analyzing here I'm showing again

233
00:10:27,520 --> 00:10:32,800
my example of Hendra ten digits and I'm

234
00:10:30,910 --> 00:10:35,170
modeling this just as a very simple

235
00:10:32,800 --> 00:10:37,540
distribution and here you can see the

236
00:10:35,170 --> 00:10:42,219
tails of the distribution where you have

237
00:10:37,540 --> 00:10:44,560
very unusual examples of digits it turns

238
00:10:42,220 --> 00:10:46,170
out to be able to learn these digits you

239
00:10:44,560 --> 00:10:49,540
have to configure the privacy-preserving

240
00:10:46,170 --> 00:10:52,209
algorithm very loosely to provide very

241
00:10:49,540 --> 00:10:54,010
weak notions of privacy which means you

242
00:10:52,209 --> 00:10:55,719
basically don't clip the grade it's too

243
00:10:54,010 --> 00:10:58,270
much and you don't introduce that much

244
00:10:55,720 --> 00:11:00,310
noise and so the algorithm is able to

245
00:10:58,270 --> 00:11:03,910
extract information from these corner

246
00:11:00,310 --> 00:11:07,239
cases if instead we strengthen the

247
00:11:03,910 --> 00:11:10,060
privacy guarantee now we're going to be

248
00:11:07,240 --> 00:11:11,440
unable to learn about these tails but

249
00:11:10,060 --> 00:11:13,569
we're going to be able to learn about

250
00:11:11,440 --> 00:11:15,700
these new examples which are a little

251
00:11:13,570 --> 00:11:18,790
bit more prototypical right they look a

252
00:11:15,700 --> 00:11:20,110
lot more like digits to me and if we

253
00:11:18,790 --> 00:11:23,199
continue to strengthen

254
00:11:20,110 --> 00:11:25,450
the privacy guarantee then we are only

255
00:11:23,200 --> 00:11:28,839
able to learn about these very canonical

256
00:11:25,450 --> 00:11:30,550
examples from the data and so what this

257
00:11:28,839 --> 00:11:33,399
shows you is that when we learn with

258
00:11:30,550 --> 00:11:35,229
privacy we are doing exactly what we

259
00:11:33,399 --> 00:11:37,149
wanted to do in the first place when we

260
00:11:35,230 --> 00:11:40,240
started using machine learning which is

261
00:11:37,149 --> 00:11:42,279
to extract patterns that generalize well

262
00:11:40,240 --> 00:11:44,529
to other examples so we're really

263
00:11:42,279 --> 00:11:47,140
learning about general patterns rather

264
00:11:44,529 --> 00:11:51,100
than specific details from individual

265
00:11:47,140 --> 00:11:53,019
examples another cool thing about

266
00:11:51,100 --> 00:11:55,480
learning with privacy is that it tells

267
00:11:53,019 --> 00:11:58,060
us a lot about how we should design

268
00:11:55,480 --> 00:11:59,860
machine earning models so if you are in

269
00:11:58,060 --> 00:12:01,540
the business of designing machinery

270
00:11:59,860 --> 00:12:03,760
models and you know that you're going to

271
00:12:01,540 --> 00:12:05,349
learn with privacy you're going to end

272
00:12:03,760 --> 00:12:09,250
up making completely different design

273
00:12:05,350 --> 00:12:11,290
choices one very simple example is if

274
00:12:09,250 --> 00:12:13,540
without privacy typically the more

275
00:12:11,290 --> 00:12:16,810
parameters you add to your model the

276
00:12:13,540 --> 00:12:18,640
better the model will perform instead

277
00:12:16,810 --> 00:12:20,589
when you're learning with privacy there

278
00:12:18,640 --> 00:12:23,079
is an inflection point after which

279
00:12:20,589 --> 00:12:25,180
adding more parameters will simply

280
00:12:23,079 --> 00:12:27,040
degrade the performance of the model and

281
00:12:25,180 --> 00:12:28,359
this makes sense because if you remember

282
00:12:27,040 --> 00:12:29,260
the way that I described the

283
00:12:28,360 --> 00:12:32,170
privacy-preserving

284
00:12:29,260 --> 00:12:33,880
training algorithm it clips the

285
00:12:32,170 --> 00:12:36,729
information that you get from training

286
00:12:33,880 --> 00:12:39,040
examples to have a maximum magnitude and

287
00:12:36,730 --> 00:12:40,390
so if you have lots of parameters what

288
00:12:39,040 --> 00:12:42,670
this means is you're going to end up

289
00:12:40,390 --> 00:12:44,560
splitting this small amount of

290
00:12:42,670 --> 00:12:47,079
information across all of the parameters

291
00:12:44,560 --> 00:12:49,989
so the more parameters you have the less

292
00:12:47,079 --> 00:12:54,310
each parameter gets to learn from your

293
00:12:49,990 --> 00:12:55,959
training examples and so what this shows

294
00:12:54,310 --> 00:12:58,089
us in conclusion is we were to think

295
00:12:55,959 --> 00:13:01,000
about designing training algorithms that

296
00:12:58,089 --> 00:13:03,730
are trustworthy it really matters what

297
00:13:01,000 --> 00:13:06,130
definition we're working with whereas

298
00:13:03,730 --> 00:13:07,899
with robust s2 adversarial examples we

299
00:13:06,130 --> 00:13:09,790
haven't made much progress with

300
00:13:07,899 --> 00:13:11,770
differential privacy we don't have a

301
00:13:09,790 --> 00:13:14,589
necessary trade-off between privacy and

302
00:13:11,770 --> 00:13:16,300
utility because the notions that we're

303
00:13:14,589 --> 00:13:18,610
trying to achieve are very compatible

304
00:13:16,300 --> 00:13:20,769
and when we were unable to achieve

305
00:13:18,610 --> 00:13:23,019
privacy we have this smooth degradation

306
00:13:20,769 --> 00:13:26,079
that I visualized with the tails of the

307
00:13:23,019 --> 00:13:29,589
distribution does that mean that we're

308
00:13:26,079 --> 00:13:31,969
done not quite yet because we have only

309
00:13:29,589 --> 00:13:34,819
talked about the training phase

310
00:13:31,970 --> 00:13:37,639
the next question is what happens a test

311
00:13:34,819 --> 00:13:39,589
time and here I think there are two

312
00:13:37,639 --> 00:13:41,929
directions that we need to work on and

313
00:13:39,589 --> 00:13:43,849
there has been very little work so far

314
00:13:41,929 --> 00:13:45,409
in that direction I'll give you two

315
00:13:43,849 --> 00:13:47,720
examples of things that we've been

316
00:13:45,409 --> 00:13:48,169
looking at the first one is Mission

317
00:13:47,720 --> 00:13:50,659
Control

318
00:13:48,169 --> 00:13:54,019
and here the ideas that we want to have

319
00:13:50,659 --> 00:13:56,209
a mechanism to understand when our model

320
00:13:54,019 --> 00:13:58,220
is unable to specify some of these

321
00:13:56,209 --> 00:14:01,758
properties that we wanted to achieve a

322
00:13:58,220 --> 00:14:04,099
training and one very simple example of

323
00:14:01,759 --> 00:14:05,869
this is computing the uncertainty of

324
00:14:04,099 --> 00:14:08,509
predictions that the model is making

325
00:14:05,869 --> 00:14:11,239
it's in general very difficult to

326
00:14:08,509 --> 00:14:14,089
estimate how uncertain a model is about

327
00:14:11,239 --> 00:14:15,799
making a prediction and it's very

328
00:14:14,089 --> 00:14:18,589
difficult because these uncertainty

329
00:14:15,799 --> 00:14:21,139
estimations are not well calibrated so

330
00:14:18,589 --> 00:14:23,809
what we've done is we've decided to open

331
00:14:21,139 --> 00:14:25,729
up the box and look at the internal

332
00:14:23,809 --> 00:14:28,009
representations that missionary models

333
00:14:25,729 --> 00:14:29,449
extract from their data for instance

334
00:14:28,009 --> 00:14:31,279
when you have neural networks you can

335
00:14:29,449 --> 00:14:35,059
just look at the intermediate outputs

336
00:14:31,279 --> 00:14:37,909
that each layer is producing and using

337
00:14:35,059 --> 00:14:40,579
that we're able to look in the training

338
00:14:37,909 --> 00:14:43,099
data and to search for the support that

339
00:14:40,579 --> 00:14:45,498
we have in making a specific prediction

340
00:14:43,099 --> 00:14:48,559
and by looking at the geometry of this

341
00:14:45,499 --> 00:14:50,989
support we can estimate how certain or

342
00:14:48,559 --> 00:14:55,639
uncertain the algorithm is in making a

343
00:14:50,989 --> 00:14:57,699
specific prediction another aspect of

344
00:14:55,639 --> 00:15:00,529
trustworthy machining a test time is

345
00:14:57,699 --> 00:15:02,539
model governance and by this I mean the

346
00:15:00,529 --> 00:15:04,999
fact that we don't pay much attention to

347
00:15:02,539 --> 00:15:08,059
what happens to our models once they've

348
00:15:04,999 --> 00:15:11,929
been deployed and if you think about it

349
00:15:08,059 --> 00:15:13,759
with new legislation like the gdpr that

350
00:15:11,929 --> 00:15:16,189
are promoting things like the right to

351
00:15:13,759 --> 00:15:17,689
be forgotten you will have to pay a lot

352
00:15:16,189 --> 00:15:19,759
of attention to what happens to your

353
00:15:17,689 --> 00:15:22,009
model once it's deployed for instance

354
00:15:19,759 --> 00:15:25,039
users might come to you and request that

355
00:15:22,009 --> 00:15:26,839
they're you their data be deleted and so

356
00:15:25,039 --> 00:15:28,609
what do you do do you just delete all

357
00:15:26,839 --> 00:15:31,519
the models that you created using this

358
00:15:28,609 --> 00:15:33,109
data of course that's not very optimal

359
00:15:31,519 --> 00:15:37,849
and so what we've been looking at is

360
00:15:33,109 --> 00:15:40,519
ways to shard and slice datasets in a

361
00:15:37,849 --> 00:15:42,709
way that it makes it easier to save

362
00:15:40,519 --> 00:15:45,029
intermediate states of the model during

363
00:15:42,709 --> 00:15:47,189
training which means that at

364
00:15:45,029 --> 00:15:51,540
test time when someone comes to us and

365
00:15:47,189 --> 00:15:53,399
requests to unlearn their data we can

366
00:15:51,540 --> 00:15:56,459
revert the model back to an earlier

367
00:15:53,399 --> 00:15:59,790
state and continue training from that

368
00:15:56,459 --> 00:16:01,109
which saves a lot of computations so

369
00:15:59,790 --> 00:16:04,709
this is something we call machine

370
00:16:01,110 --> 00:16:06,420
learning so to conclude I think it's

371
00:16:04,709 --> 00:16:07,829
really important to first think about

372
00:16:06,420 --> 00:16:11,399
the policies that we're trying to

373
00:16:07,829 --> 00:16:13,800
achieve here I've talked about security

374
00:16:11,399 --> 00:16:15,240
and privacy but there are other notions

375
00:16:13,800 --> 00:16:17,579
that I mentioned earlier in the talk

376
00:16:15,240 --> 00:16:19,230
like fairness and in particular it's

377
00:16:17,579 --> 00:16:21,989
interesting to think about those in

378
00:16:19,230 --> 00:16:24,300
combination it's for instance very

379
00:16:21,990 --> 00:16:26,399
likely that achieving fairness in

380
00:16:24,300 --> 00:16:29,579
conjunction with privacy will be

381
00:16:26,399 --> 00:16:31,889
difficult because populations that will

382
00:16:29,579 --> 00:16:33,660
be affected by the lack of fairness will

383
00:16:31,889 --> 00:16:35,519
often be populations for which you don't

384
00:16:33,660 --> 00:16:37,529
have much data so it will be difficult

385
00:16:35,519 --> 00:16:40,589
to learn from them with strong privacy

386
00:16:37,529 --> 00:16:43,500
guarantees technology needs to evolve to

387
00:16:40,589 --> 00:16:47,519
provide a training time robustness

388
00:16:43,500 --> 00:16:49,829
guarantees that these policies are met

389
00:16:47,519 --> 00:16:52,920
and a test time we need to have emission

390
00:16:49,829 --> 00:16:54,959
control but also model governance but

391
00:16:52,920 --> 00:16:57,540
sometimes technology will not solve

392
00:16:54,959 --> 00:16:59,369
everything if you think about defects we

393
00:16:57,540 --> 00:17:02,579
wrote a recent think piece about it

394
00:16:59,370 --> 00:17:05,640
where we conclude that any progress made

395
00:17:02,579 --> 00:17:08,520
in detecting deep face will actually

396
00:17:05,640 --> 00:17:10,829
help us design better defects because

397
00:17:08,520 --> 00:17:14,339
the technology is such that generative

398
00:17:10,829 --> 00:17:17,299
models are trained by defeating what is

399
00:17:14,339 --> 00:17:20,428
essentially a detector for defects and

400
00:17:17,299 --> 00:17:22,470
so here it's clear that technology will

401
00:17:20,429 --> 00:17:24,449
not provide an answer to each fix and

402
00:17:22,470 --> 00:17:26,539
that we have to look at other two forms

403
00:17:24,449 --> 00:17:29,940
of solutions like legal frameworks or

404
00:17:26,539 --> 00:17:32,309
simply education that said I hope that

405
00:17:29,940 --> 00:17:34,740
in this presentation I conveyed my

406
00:17:32,309 --> 00:17:37,289
enthusiasm for the fact that trustworthy

407
00:17:34,740 --> 00:17:39,840
learning is an opportunity to make

408
00:17:37,289 --> 00:17:43,190
mission earning better and with that

409
00:17:39,840 --> 00:17:45,250
I'll take any questions you may have

410
00:17:43,190 --> 00:17:45,250
you

