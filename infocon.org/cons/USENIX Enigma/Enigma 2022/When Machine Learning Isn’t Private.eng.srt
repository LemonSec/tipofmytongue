1
00:00:06,960 --> 00:00:07,919
um

2
00:00:07,919 --> 00:00:10,480
i'm nicholas and i'm going to talk

3
00:00:10,480 --> 00:00:13,679
about what happens when machine learning

4
00:00:13,679 --> 00:00:17,840
isn't designed explicitly to be private

5
00:00:17,840 --> 00:00:19,840
so we're now using machine learning

6
00:00:19,840 --> 00:00:21,039
models

7
00:00:21,039 --> 00:00:24,240
on everything from our medical images

8
00:00:24,240 --> 00:00:27,119
to our text messages

9
00:00:27,119 --> 00:00:27,840
to

10
00:00:27,840 --> 00:00:29,439
our emails

11
00:00:29,439 --> 00:00:31,279
and when we do this

12
00:00:31,279 --> 00:00:33,040
the real question

13
00:00:33,040 --> 00:00:35,680
is what happens

14
00:00:35,680 --> 00:00:37,440
to privacy

15
00:00:37,440 --> 00:00:39,280
so

16
00:00:39,280 --> 00:00:40,960
do these models

17
00:00:40,960 --> 00:00:43,040
leak sensitive information

18
00:00:43,040 --> 00:00:45,280
about our training data

19
00:00:45,280 --> 00:00:48,079
and the reason why you can imagine that

20
00:00:48,079 --> 00:00:50,000
this wouldn't happen well the reason why

21
00:00:50,000 --> 00:00:51,520
you can imagine why maybe you get

22
00:00:51,520 --> 00:00:52,800
privacy

23
00:00:52,800 --> 00:00:54,480
is what we have

24
00:00:54,480 --> 00:00:56,840
is like maybe all of you have training

25
00:00:56,840 --> 00:00:59,840
data and you're going to contribute all

26
00:00:59,840 --> 00:01:01,680
your training data together and we're

27
00:01:01,680 --> 00:01:03,120
going to use this to collaboratively

28
00:01:03,120 --> 00:01:05,519
train a big giant model

29
00:01:05,519 --> 00:01:07,760
and now me i'm going to be allowed to

30
00:01:07,760 --> 00:01:10,479
query this model

31
00:01:10,479 --> 00:01:12,400
and so you can imagine that maybe it's

32
00:01:12,400 --> 00:01:15,200
hard for an adversary to do anything to

33
00:01:15,200 --> 00:01:17,360
be able to distinguish the case

34
00:01:17,360 --> 00:01:20,000
where we have all this data

35
00:01:20,000 --> 00:01:22,320
and all this data plus one individual

36
00:01:22,320 --> 00:01:23,759
record

37
00:01:23,759 --> 00:01:25,360
like it's maybe tricky to distinguish

38
00:01:25,360 --> 00:01:26,880
these two cases and so maybe there's

39
00:01:26,880 --> 00:01:29,200
privacy here just because of how much

40
00:01:29,200 --> 00:01:31,920
data there is

41
00:01:32,400 --> 00:01:34,799
now there's a problem

42
00:01:34,799 --> 00:01:36,320
in that

43
00:01:36,320 --> 00:01:38,320
at least theoretically

44
00:01:38,320 --> 00:01:40,960
we've known for a long time

45
00:01:40,960 --> 00:01:42,880
that it's possible to have privacy

46
00:01:42,880 --> 00:01:44,479
violations

47
00:01:44,479 --> 00:01:46,560
and we've known that

48
00:01:46,560 --> 00:01:49,520
academically you know the theory people

49
00:01:49,520 --> 00:01:51,119
tell each other ghost stories at night

50
00:01:51,119 --> 00:01:52,640
about how it's possible that training

51
00:01:52,640 --> 00:01:54,000
data could be leaked in these machine

52
00:01:54,000 --> 00:01:55,759
learning models

53
00:01:55,759 --> 00:01:57,920
the question is like

54
00:01:57,920 --> 00:02:00,479
yeah it's possible in theory

55
00:02:00,479 --> 00:02:02,479
but are privacy attacks actually

56
00:02:02,479 --> 00:02:04,640
something that we can really do

57
00:02:04,640 --> 00:02:06,799
that matter in practice like is this a

58
00:02:06,799 --> 00:02:08,080
real thing or is it something that we

59
00:02:08,080 --> 00:02:10,080
just like to be worried about

60
00:02:10,080 --> 00:02:11,200
and what i'm going to hope to convince

61
00:02:11,200 --> 00:02:12,560
you of in this talk

62
00:02:12,560 --> 00:02:14,640
is that the privacy attacks actually

63
00:02:14,640 --> 00:02:17,280
matter and we can do them and we can do

64
00:02:17,280 --> 00:02:20,800
quite strong attacks in practice

65
00:02:20,959 --> 00:02:23,120
okay and the way i'm going to do that is

66
00:02:23,120 --> 00:02:24,560
i'm going to first introduce to you

67
00:02:24,560 --> 00:02:26,800
what's called a training data extraction

68
00:02:26,800 --> 00:02:28,400
attack

69
00:02:28,400 --> 00:02:29,520
this is an attack that we've been

70
00:02:29,520 --> 00:02:31,200
working on for some time

71
00:02:31,200 --> 00:02:33,040
and it's going to be able to breach the

72
00:02:33,040 --> 00:02:34,720
privacy of individual people in a

73
00:02:34,720 --> 00:02:37,200
training data set

74
00:02:37,200 --> 00:02:38,720
and the type of model i'm going to

75
00:02:38,720 --> 00:02:39,760
attack

76
00:02:39,760 --> 00:02:41,680
is what's going to be called a language

77
00:02:41,680 --> 00:02:42,720
model

78
00:02:42,720 --> 00:02:44,800
and the way a language model works is

79
00:02:44,800 --> 00:02:46,239
fairly simple it's just like any other

80
00:02:46,239 --> 00:02:48,319
classification model

81
00:02:48,319 --> 00:02:49,360
and

82
00:02:49,360 --> 00:02:51,440
it takes some input text

83
00:02:51,440 --> 00:02:52,959
and then outputs a prediction of what it

84
00:02:52,959 --> 00:02:55,280
thinks the next word is going to be

85
00:02:55,280 --> 00:02:57,200
so maybe the text is you know hello my

86
00:02:57,200 --> 00:02:58,239
name is

87
00:02:58,239 --> 00:02:59,760
and it predicts the next word if it was

88
00:02:59,760 --> 00:03:03,200
trained on my data is nicholas

89
00:03:03,200 --> 00:03:04,400
and so then what you can do is the thing

90
00:03:04,400 --> 00:03:05,760
that makes language models nice is you

91
00:03:05,760 --> 00:03:07,920
can use them recursively you can copy

92
00:03:07,920 --> 00:03:09,360
the output

93
00:03:09,360 --> 00:03:11,200
back to the input and then get the next

94
00:03:11,200 --> 00:03:12,480
sequence to go

95
00:03:12,480 --> 00:03:13,840
and you can say what comes next and it

96
00:03:13,840 --> 00:03:16,319
will say you know this nicholas and

97
00:03:16,319 --> 00:03:17,840
and then you can sort of run them this

98
00:03:17,840 --> 00:03:19,280
way forward sort of recursively

99
00:03:19,280 --> 00:03:20,879
generating word after word until it

100
00:03:20,879 --> 00:03:22,400
gives you an entire sentence you know

101
00:03:22,400 --> 00:03:24,000
maybe hello my name is nicholas and this

102
00:03:24,000 --> 00:03:26,000
is my talk

103
00:03:26,000 --> 00:03:27,680
and it's not like they can do you know

104
00:03:27,680 --> 00:03:29,519
little small toy examples like this the

105
00:03:29,519 --> 00:03:31,200
best language models can do really high

106
00:03:31,200 --> 00:03:32,879
quality text

107
00:03:32,879 --> 00:03:35,040
so here's some example from

108
00:03:35,040 --> 00:03:38,480
the gpd2 language model from open ai

109
00:03:38,480 --> 00:03:40,879
where humans put in some text talking

110
00:03:40,879 --> 00:03:43,120
about unicorns humans said

111
00:03:43,120 --> 00:03:45,200
in a shocking finding scientists

112
00:03:45,200 --> 00:03:47,519
discovered a heard of unicorns living in

113
00:03:47,519 --> 00:03:49,200
a remote previously unexplored valley in

114
00:03:49,200 --> 00:03:50,720
the andes mountains

115
00:03:50,720 --> 00:03:52,319
and then the humans went on to describe

116
00:03:52,319 --> 00:03:54,640
one more sentence about these unicorns

117
00:03:54,640 --> 00:03:56,239
and then we asked the machine learning

118
00:03:56,239 --> 00:03:57,840
model okay this is the this is the

119
00:03:57,840 --> 00:04:00,400
prompt please complete what comes next

120
00:04:00,400 --> 00:04:03,200
and it sort of goes word by word

121
00:04:03,200 --> 00:04:05,840
the scientists named the population

122
00:04:05,840 --> 00:04:08,159
after their distinctive horn ovid's

123
00:04:08,159 --> 00:04:09,439
unicorn

124
00:04:09,439 --> 00:04:11,360
and then it you know goes on from there

125
00:04:11,360 --> 00:04:13,599
to talk about various properties of

126
00:04:13,599 --> 00:04:15,040
these unicorns and how they were

127
00:04:15,040 --> 00:04:15,920
discovered and where they were

128
00:04:15,920 --> 00:04:18,079
discovered and by all accounts it's very

129
00:04:18,079 --> 00:04:20,160
very nice text like this model is a very

130
00:04:20,160 --> 00:04:22,079
good language model and can write some

131
00:04:22,079 --> 00:04:24,080
high quality text and the reason why

132
00:04:24,080 --> 00:04:25,759
these models are interesting

133
00:04:25,759 --> 00:04:27,440
is that not only can they tell us fun

134
00:04:27,440 --> 00:04:30,080
stories they're also the kinds of models

135
00:04:30,080 --> 00:04:32,960
that we use when we train on our emails

136
00:04:32,960 --> 00:04:34,880
and our text messages

137
00:04:34,880 --> 00:04:36,080
and so it's really important to

138
00:04:36,080 --> 00:04:39,040
understand if they're private

139
00:04:39,040 --> 00:04:39,919
and so what i'm going to do is i'm going

140
00:04:39,919 --> 00:04:41,759
to show you an attack which we call a

141
00:04:41,759 --> 00:04:43,280
training data extraction attack which is

142
00:04:43,280 --> 00:04:45,040
allow us to violate the privacy of the

143
00:04:45,040 --> 00:04:47,199
training data here

144
00:04:47,199 --> 00:04:48,560
okay

145
00:04:48,560 --> 00:04:49,919
the way this attack is going to work is

146
00:04:49,919 --> 00:04:52,320
it's going to have two simple steps

147
00:04:52,320 --> 00:04:54,400
the first step is just going to be to

148
00:04:54,400 --> 00:04:57,440
generate a lot of training data

149
00:04:57,440 --> 00:04:59,120
or sorry a lot of data hopefully someone

150
00:04:59,120 --> 00:05:00,800
will be training data the reason why you

151
00:05:00,800 --> 00:05:02,639
expect that generating a lot of data

152
00:05:02,639 --> 00:05:03,919
should be easy is this what models are

153
00:05:03,919 --> 00:05:05,120
designed to do

154
00:05:05,120 --> 00:05:06,800
like the purpose of a language model is

155
00:05:06,800 --> 00:05:08,479
to spit out text

156
00:05:08,479 --> 00:05:10,080
and so all we have to do is to ask it to

157
00:05:10,080 --> 00:05:11,520
give us a lot of text and it will go and

158
00:05:11,520 --> 00:05:13,600
do that for us

159
00:05:13,600 --> 00:05:16,560
and now it turns out some of this text

160
00:05:16,560 --> 00:05:18,800
is going to be training data and the

161
00:05:18,800 --> 00:05:20,880
reason why is that when we train these

162
00:05:20,880 --> 00:05:23,120
models the objective of training

163
00:05:23,120 --> 00:05:26,160
is to minimize the loss on training data

164
00:05:26,160 --> 00:05:28,240
which explicitly encourages the model to

165
00:05:28,240 --> 00:05:30,800
memorize at least some of the data

166
00:05:30,800 --> 00:05:32,639
the thing is though that there's a lot

167
00:05:32,639 --> 00:05:34,560
of text here and it's really hard to

168
00:05:34,560 --> 00:05:36,400
filter the signal from the noise the

169
00:05:36,400 --> 00:05:38,160
training data from the non-training data

170
00:05:38,160 --> 00:05:40,080
and so when you just look at all of this

171
00:05:40,080 --> 00:05:41,840
most of it looks fine and so you can't

172
00:05:41,840 --> 00:05:43,360
actually pick out individual training

173
00:05:43,360 --> 00:05:44,639
sequences even though some of them are

174
00:05:44,639 --> 00:05:46,639
there

175
00:05:46,639 --> 00:05:47,840
and so that's where the second part of

176
00:05:47,840 --> 00:05:49,520
the attack comes in which is what's

177
00:05:49,520 --> 00:05:51,520
called a membership inference attack

178
00:05:51,520 --> 00:05:53,039
which you can think of as this magic

179
00:05:53,039 --> 00:05:55,680
black box that will highlight for you

180
00:05:55,680 --> 00:05:57,680
individual training examples

181
00:05:57,680 --> 00:05:59,680
from the data set so we'll say like of

182
00:05:59,680 --> 00:06:01,199
all the things that were generated most

183
00:06:01,199 --> 00:06:02,800
of that is just benign text that it came

184
00:06:02,800 --> 00:06:04,400
up with by itself

185
00:06:04,400 --> 00:06:06,400
but a couple of examples here

186
00:06:06,400 --> 00:06:09,840
are training examples

187
00:06:10,400 --> 00:06:11,680
okay so this question you know how does

188
00:06:11,680 --> 00:06:14,080
this magic black box work um what it

189
00:06:14,080 --> 00:06:16,479
relies on is this property of of machine

190
00:06:16,479 --> 00:06:17,919
learning models that they tend to have

191
00:06:17,919 --> 00:06:19,600
very high confidence on things that they

192
00:06:19,600 --> 00:06:20,880
were trained on

193
00:06:20,880 --> 00:06:23,120
so let's imagine that i had two

194
00:06:23,120 --> 00:06:24,319
sequences

195
00:06:24,319 --> 00:06:26,240
one of the sequences says my phone

196
00:06:26,240 --> 00:06:28,800
number is zero zero zero one two three

197
00:06:28,800 --> 00:06:31,120
four and the model says the probability

198
00:06:31,120 --> 00:06:32,840
that's occurring is one

199
00:06:32,840 --> 00:06:35,759
percent probably this isn't training

200
00:06:35,759 --> 00:06:37,440
data because one percent is a fairly low

201
00:06:37,440 --> 00:06:38,639
probability

202
00:06:38,639 --> 00:06:40,560
on the other hand if i said my phone

203
00:06:40,560 --> 00:06:42,960
number is five five five six seven eight

204
00:06:42,960 --> 00:06:44,639
nine and the probability of this

205
00:06:44,639 --> 00:06:46,560
occurring was eighty percent this

206
00:06:46,560 --> 00:06:48,000
probably is training data because that's

207
00:06:48,000 --> 00:06:49,919
an abnormally high probability for

208
00:06:49,919 --> 00:06:51,199
something to have if it was not already

209
00:06:51,199 --> 00:06:52,880
in the data set

210
00:06:52,880 --> 00:06:54,240
and again like this isn't something that

211
00:06:54,240 --> 00:06:55,680
just like you know i'm making up numbers

212
00:06:55,680 --> 00:06:58,400
here let me show you some actual data

213
00:06:58,400 --> 00:06:59,360
what i'm going to do here is i'm

214
00:06:59,360 --> 00:07:01,199
plotting i'm going to train a bunch of

215
00:07:01,199 --> 00:07:02,319
bottles

216
00:07:02,319 --> 00:07:04,479
either containing some example or not

217
00:07:04,479 --> 00:07:06,400
containing the example

218
00:07:06,400 --> 00:07:08,479
in orange are the distribution of

219
00:07:08,479 --> 00:07:10,080
confidence values

220
00:07:10,080 --> 00:07:12,400
when you do train on an example

221
00:07:12,400 --> 00:07:14,000
and in blue is the distribution of

222
00:07:14,000 --> 00:07:16,000
confidence values for different models

223
00:07:16,000 --> 00:07:19,039
when you don't train on an example

224
00:07:19,039 --> 00:07:20,400
and so what you'll find is that when you

225
00:07:20,400 --> 00:07:21,759
train models like i've trained like you

226
00:07:21,759 --> 00:07:23,919
know 10 000 models here lots of models

227
00:07:23,919 --> 00:07:26,319
when you train models on an example the

228
00:07:26,319 --> 00:07:29,039
confidence is like almost always

229
00:07:29,039 --> 00:07:31,199
big and when you don't train on the

230
00:07:31,199 --> 00:07:32,720
example the confidence is almost always

231
00:07:32,720 --> 00:07:33,759
small

232
00:07:33,759 --> 00:07:35,360
and so it's really easy to separate if

233
00:07:35,360 --> 00:07:37,199
you show me what the confidence value is

234
00:07:37,199 --> 00:07:38,639
whether or not the model trained on this

235
00:07:38,639 --> 00:07:40,240
example or not

236
00:07:40,240 --> 00:07:42,240
and as a result i can very easily tell

237
00:07:42,240 --> 00:07:43,919
if something was training data which

238
00:07:43,919 --> 00:07:46,720
allows me to do extraction because i've

239
00:07:46,720 --> 00:07:49,759
generated this sequence already

240
00:07:49,759 --> 00:07:52,080
okay

241
00:07:52,240 --> 00:07:55,039
so let's go and actually evaluate this

242
00:07:55,039 --> 00:07:56,960
and see how well this attack works in

243
00:07:56,960 --> 00:07:58,479
practice

244
00:07:58,479 --> 00:08:00,639
and the first thing we find which is

245
00:08:00,639 --> 00:08:02,800
maybe a surprising observation is that

246
00:08:02,800 --> 00:08:05,120
like you know up to five percent of the

247
00:08:05,120 --> 00:08:07,520
output of language models is actually

248
00:08:07,520 --> 00:08:10,479
memorized training data

249
00:08:10,479 --> 00:08:11,840
and this is a very big number it's maybe

250
00:08:11,840 --> 00:08:13,360
surprisingly large

251
00:08:13,360 --> 00:08:14,960
um and it's so surprisingly large

252
00:08:14,960 --> 00:08:16,639
actually it turns out it's not entirely

253
00:08:16,639 --> 00:08:17,840
interesting to actually look at this

254
00:08:17,840 --> 00:08:20,000
number uh a big reason why that's this

255
00:08:20,000 --> 00:08:21,840
big is because

256
00:08:21,840 --> 00:08:24,639
lots of re repetitive text is sort of

257
00:08:24,639 --> 00:08:25,840
contained on the internet and when you

258
00:08:25,840 --> 00:08:27,599
train models on repetitive text it will

259
00:08:27,599 --> 00:08:29,520
spit out repetitive answers

260
00:08:29,520 --> 00:08:31,520
so a lot of this five percent of data is

261
00:08:31,520 --> 00:08:34,559
just you know copying licenses of the

262
00:08:34,559 --> 00:08:37,120
mit license or copying various things

263
00:08:37,120 --> 00:08:38,159
that are repeated on the internet

264
00:08:38,159 --> 00:08:39,599
thousands of times and so it's not all

265
00:08:39,599 --> 00:08:41,039
that interesting so instead you know

266
00:08:41,039 --> 00:08:43,120
let's not look at the absolute number of

267
00:08:43,120 --> 00:08:45,600
amount of data that's memorized

268
00:08:45,600 --> 00:08:48,080
instead let's sort of look at individual

269
00:08:48,080 --> 00:08:50,399
examples of memorization

270
00:08:50,399 --> 00:08:51,760
and for this i'm going to look at

271
00:08:51,760 --> 00:08:56,000
memorization in the gpt2 language model

272
00:08:56,000 --> 00:08:57,200
which is the one i showed you the

273
00:08:57,200 --> 00:08:59,600
unicorn text from earlier

274
00:08:59,600 --> 00:09:00,640
this is the model that was trained by

275
00:09:00,640 --> 00:09:01,760
openai

276
00:09:01,760 --> 00:09:03,920
and we wanted to study this model

277
00:09:03,920 --> 00:09:05,760
because at the time we did the work this

278
00:09:05,760 --> 00:09:07,760
was the best language model anyone had

279
00:09:07,760 --> 00:09:09,920
ever trained and so we can be pretty

280
00:09:09,920 --> 00:09:11,839
sure that any privacy violations we find

281
00:09:11,839 --> 00:09:14,240
there are sort of general properties of

282
00:09:14,240 --> 00:09:16,000
machine learning and not

283
00:09:16,000 --> 00:09:17,680
accidental things that we find because

284
00:09:17,680 --> 00:09:19,760
we're attacking models we attack we

285
00:09:19,760 --> 00:09:23,839
trained ourselves as privacy researchers

286
00:09:23,920 --> 00:09:25,120
the other reason why this model is

287
00:09:25,120 --> 00:09:26,640
interesting is that it was trained

288
00:09:26,640 --> 00:09:28,000
exclusively on data from the public

289
00:09:28,000 --> 00:09:29,279
internet

290
00:09:29,279 --> 00:09:31,200
and so that means that even if we could

291
00:09:31,200 --> 00:09:33,120
succeed in our attacks like the best

292
00:09:33,120 --> 00:09:34,160
that we're going to do is actually

293
00:09:34,160 --> 00:09:36,080
extract data that was already public so

294
00:09:36,080 --> 00:09:37,279
now actually we're not going to harm

295
00:09:37,279 --> 00:09:39,200
anyone here

296
00:09:39,200 --> 00:09:40,800
okay

297
00:09:40,800 --> 00:09:43,040
so what do we find

298
00:09:43,040 --> 00:09:44,800
we find that we can get a fairly diverse

299
00:09:44,800 --> 00:09:47,279
amount of memorization out of this model

300
00:09:47,279 --> 00:09:49,040
we get things like us and international

301
00:09:49,040 --> 00:09:51,200
news we get lots of log files and error

302
00:09:51,200 --> 00:09:54,160
messages we get you know licenses we get

303
00:09:54,160 --> 00:09:56,240
urls

304
00:09:56,240 --> 00:09:58,000
we can find examples of personally

305
00:09:58,000 --> 00:10:00,080
identifiable information contact

306
00:10:00,080 --> 00:10:02,000
information these kinds of things

307
00:10:02,000 --> 00:10:03,360
the thing i want to take away from here

308
00:10:03,360 --> 00:10:04,800
is how diverse the amount of

309
00:10:04,800 --> 00:10:07,120
memorization is and again remember this

310
00:10:07,120 --> 00:10:10,000
is not sensitive but this is by design

311
00:10:10,000 --> 00:10:12,320
gpt2 was trained on data from the public

312
00:10:12,320 --> 00:10:14,320
internet and so you get is memorization

313
00:10:14,320 --> 00:10:15,600
that looks like things that appear from

314
00:10:15,600 --> 00:10:17,360
the public internet

315
00:10:17,360 --> 00:10:19,120
but let's imagine that instead of

316
00:10:19,120 --> 00:10:20,480
training on the internet you trained on

317
00:10:20,480 --> 00:10:23,200
some internal corporate data set

318
00:10:23,200 --> 00:10:25,279
then instead of having memorization of

319
00:10:25,279 --> 00:10:27,440
u.s and international news maybe you

320
00:10:27,440 --> 00:10:29,760
have memorization of internal company

321
00:10:29,760 --> 00:10:31,360
bulletins

322
00:10:31,360 --> 00:10:34,320
and instead of memorization of log files

323
00:10:34,320 --> 00:10:35,920
of people asking for help on stack

324
00:10:35,920 --> 00:10:39,040
overflow you get log files that contain

325
00:10:39,040 --> 00:10:40,560
user data

326
00:10:40,560 --> 00:10:42,480
and instead of personally identifiable

327
00:10:42,480 --> 00:10:44,399
information that people put online by

328
00:10:44,399 --> 00:10:45,360
choice

329
00:10:45,360 --> 00:10:47,200
you'll get personally identifiable

330
00:10:47,200 --> 00:10:49,040
information that was supposed to be

331
00:10:49,040 --> 00:10:51,440
private

332
00:10:51,440 --> 00:10:52,800
and so this is the kind of memorization

333
00:10:52,800 --> 00:10:55,279
we're able to get out of these models

334
00:10:55,279 --> 00:10:56,959
now let me tell you one experiment that

335
00:10:56,959 --> 00:10:58,399
hopefully gets at the question of why

336
00:10:58,399 --> 00:11:00,079
this happens why are we able to get this

337
00:11:00,079 --> 00:11:02,480
amount of memorization and it turns out

338
00:11:02,480 --> 00:11:04,320
a big reason why

339
00:11:04,320 --> 00:11:06,399
is because models tend to memorize

340
00:11:06,399 --> 00:11:08,320
things if they're repeated

341
00:11:08,320 --> 00:11:10,399
more than once

342
00:11:10,399 --> 00:11:12,839
and so i'm going to show you here

343
00:11:12,839 --> 00:11:16,160
is what happens when we we found a

344
00:11:16,160 --> 00:11:17,839
document that was repeat that was

345
00:11:17,839 --> 00:11:20,000
contained in the gpt2 training data set

346
00:11:20,000 --> 00:11:22,320
that repeated various urls

347
00:11:22,320 --> 00:11:24,079
between eight times

348
00:11:24,079 --> 00:11:26,640
and 359 times

349
00:11:26,640 --> 00:11:28,079
and then we can ask for each of these

350
00:11:28,079 --> 00:11:30,399
urls we can go one by one and say did

351
00:11:30,399 --> 00:11:32,240
our extraction attack find this one yes

352
00:11:32,240 --> 00:11:34,160
the tax extraction attack find this one

353
00:11:34,160 --> 00:11:36,880
yes walk away down until we find that

354
00:11:36,880 --> 00:11:38,800
our extraction attack starts failing at

355
00:11:38,800 --> 00:11:40,480
things that are repeated

356
00:11:40,480 --> 00:11:42,800
17 times or fewer

357
00:11:42,800 --> 00:11:44,320
and so we can say like you know it looks

358
00:11:44,320 --> 00:11:45,120
like

359
00:11:45,120 --> 00:11:46,560
there's roughly a threshold here the

360
00:11:46,560 --> 00:11:48,560
attack succeeds almost always

361
00:11:48,560 --> 00:11:50,000
when you have something inserted more

362
00:11:50,000 --> 00:11:54,240
than 17 times and never fewer

363
00:11:54,240 --> 00:11:55,519
okay so this is you know something a

364
00:11:55,519 --> 00:11:58,160
little interesting maybe you say um

365
00:11:58,160 --> 00:12:00,000
this is no longer a problem if you

366
00:12:00,000 --> 00:12:02,240
repeat something 17 or 30 times you know

367
00:12:02,240 --> 00:12:03,600
at this point it's fine for the model to

368
00:12:03,600 --> 00:12:05,200
memorize it

369
00:12:05,200 --> 00:12:07,680
you know i might disagree a little bit

370
00:12:07,680 --> 00:12:09,600
first because i probably told my family

371
00:12:09,600 --> 00:12:11,120
things 30 times that i wouldn't want

372
00:12:11,120 --> 00:12:12,560
made public

373
00:12:12,560 --> 00:12:15,440
but even if i agreed with you

374
00:12:15,440 --> 00:12:18,320
this this this 20 or 30 number is only

375
00:12:18,320 --> 00:12:20,000
one example

376
00:12:20,000 --> 00:12:22,560
and it's only one example for one model

377
00:12:22,560 --> 00:12:24,959
so we can do is we can you know take a

378
00:12:24,959 --> 00:12:28,079
plot of like x-axis how big the model is

379
00:12:28,079 --> 00:12:29,680
number of parameters

380
00:12:29,680 --> 00:12:31,760
this star is here corresponding to gpt-2

381
00:12:31,760 --> 00:12:33,839
it's roughly a billion parameters and on

382
00:12:33,839 --> 00:12:35,680
the y-axis we have how many times you

383
00:12:35,680 --> 00:12:36,959
need to insert something before it gets

384
00:12:36,959 --> 00:12:38,480
memorized

385
00:12:38,480 --> 00:12:40,800
and sort of i showed you one point but i

386
00:12:40,800 --> 00:12:42,720
can expand this by looking at a bunch of

387
00:12:42,720 --> 00:12:46,639
different model families into a line

388
00:12:46,639 --> 00:12:48,720
and what we find is that as you make

389
00:12:48,720 --> 00:12:50,399
models bigger

390
00:12:50,399 --> 00:12:52,079
they memorize things that are repeated

391
00:12:52,079 --> 00:12:55,040
less and less often

392
00:12:55,279 --> 00:12:56,880
up until the point where you know maybe

393
00:12:56,880 --> 00:12:58,880
you reach 100 billion parameters and

394
00:12:58,880 --> 00:13:00,800
models sort of look like they start like

395
00:13:00,800 --> 00:13:03,279
if the trend continues memorizing things

396
00:13:03,279 --> 00:13:04,639
you know repeated just a handful of

397
00:13:04,639 --> 00:13:06,320
times

398
00:13:06,320 --> 00:13:07,920
and maybe you say you know 100 billion

399
00:13:07,920 --> 00:13:08,959
that's a big number we're not going to

400
00:13:08,959 --> 00:13:10,959
train models that big

401
00:13:10,959 --> 00:13:13,360
sorry but you're already wrong

402
00:13:13,360 --> 00:13:16,000
the biggest models that exist as of last

403
00:13:16,000 --> 00:13:17,040
year

404
00:13:17,040 --> 00:13:19,200
are a trillion parameters like we

405
00:13:19,200 --> 00:13:20,560
already have

406
00:13:20,560 --> 00:13:23,440
trillion parameter neural networks

407
00:13:23,440 --> 00:13:25,120
and we're only getting bigger and bigger

408
00:13:25,120 --> 00:13:27,200
models as time goes on and so this is

409
00:13:27,200 --> 00:13:29,279
really concerning because it means that

410
00:13:29,279 --> 00:13:30,720
so the machine learning community is

411
00:13:30,720 --> 00:13:31,839
pushing the direction of bigger and

412
00:13:31,839 --> 00:13:33,360
bigger models

413
00:13:33,360 --> 00:13:34,720
and as we've seen bigger models means

414
00:13:34,720 --> 00:13:36,240
more memorization and this is going to

415
00:13:36,240 --> 00:13:37,680
cause more of a problem as we go

416
00:13:37,680 --> 00:13:40,160
forwards

417
00:13:40,560 --> 00:13:42,079
okay

418
00:13:42,079 --> 00:13:43,680
so what are we going to do about this

419
00:13:43,680 --> 00:13:44,800
well one thing you could do is you could

420
00:13:44,800 --> 00:13:47,440
try and add sort of privacy by coming up

421
00:13:47,440 --> 00:13:50,880
with some ad hoc defenses to prevent it

422
00:13:50,880 --> 00:13:52,399
let me tell you about just one example

423
00:13:52,399 --> 00:13:53,920
of what happens here that's sort of an

424
00:13:53,920 --> 00:13:56,160
example from the vast space of attacks

425
00:13:56,160 --> 00:13:58,320
on ad hoc defenses

426
00:13:58,320 --> 00:13:59,440
and the defense i'm going to tell you

427
00:13:59,440 --> 00:14:00,880
about very briefly

428
00:14:00,880 --> 00:14:02,720
is this paper that's called instahide

429
00:14:02,720 --> 00:14:04,720
which got a bunch of attention and

430
00:14:04,720 --> 00:14:06,880
received some awards for being a very

431
00:14:06,880 --> 00:14:08,560
good defense to prevent exactly these

432
00:14:08,560 --> 00:14:10,959
kinds of reconstruction attacks

433
00:14:10,959 --> 00:14:12,959
and this defense works not on text but

434
00:14:12,959 --> 00:14:14,399
on images

435
00:14:14,399 --> 00:14:15,519
and so what we're going to the way the

436
00:14:15,519 --> 00:14:17,199
way the defense works is you take a

437
00:14:17,199 --> 00:14:19,519
normal image and you encode the image

438
00:14:19,519 --> 00:14:22,079
into some noisy version of this image

439
00:14:22,079 --> 00:14:23,360
and then instead of training machine

440
00:14:23,360 --> 00:14:25,199
learning models on the original images

441
00:14:25,199 --> 00:14:26,800
you only ever train machine learning

442
00:14:26,800 --> 00:14:29,760
models on these noisy images

443
00:14:29,760 --> 00:14:31,920
and then use the models like this

444
00:14:31,920 --> 00:14:33,199
now first of all it's kind of weird that

445
00:14:33,199 --> 00:14:34,639
you can do this in coding and train

446
00:14:34,639 --> 00:14:36,160
models on noise and have them become

447
00:14:36,160 --> 00:14:38,880
good um you know that's kind of like

448
00:14:38,880 --> 00:14:40,560
surprising

449
00:14:40,560 --> 00:14:42,240
i'm not going to talk so much about that

450
00:14:42,240 --> 00:14:44,160
it turns out that you can do this you

451
00:14:44,160 --> 00:14:45,279
know in their paper they have some

452
00:14:45,279 --> 00:14:47,120
experiments that show tables like

453
00:14:47,120 --> 00:14:49,040
accuracy works

454
00:14:49,040 --> 00:14:50,320
i don't really care about the accuracy i

455
00:14:50,320 --> 00:14:53,600
want to ask the question privacy like if

456
00:14:53,600 --> 00:14:55,680
you were to do this and train models on

457
00:14:55,680 --> 00:14:58,320
these noisy images why should you expect

458
00:14:58,320 --> 00:15:00,399
you to get privacy

459
00:15:00,399 --> 00:15:02,399
and the argument in this paper is fairly

460
00:15:02,399 --> 00:15:04,639
nice and it's fairly clean

461
00:15:04,639 --> 00:15:07,199
the argument goes like this if you train

462
00:15:07,199 --> 00:15:11,760
a model on only these encoded images

463
00:15:11,760 --> 00:15:12,880
then

464
00:15:12,880 --> 00:15:14,800
if you could do an attack you should

465
00:15:14,800 --> 00:15:16,240
never be able to recover the original

466
00:15:16,240 --> 00:15:17,279
image

467
00:15:17,279 --> 00:15:19,440
the best you should be able to do is to

468
00:15:19,440 --> 00:15:23,040
recover the encoded version of the image

469
00:15:23,040 --> 00:15:25,839
and because you can't go from an encoded

470
00:15:25,839 --> 00:15:27,839
image to an original image

471
00:15:27,839 --> 00:15:29,600
you're fine

472
00:15:29,600 --> 00:15:30,399
like

473
00:15:30,399 --> 00:15:32,079
you can use whatever machine learning

474
00:15:32,079 --> 00:15:33,440
you want the best you'll be able to do

475
00:15:33,440 --> 00:15:35,120
is recover encoded images and so just

476
00:15:35,120 --> 00:15:36,560
construct an encoding function that's

477
00:15:36,560 --> 00:15:37,600
one way

478
00:15:37,600 --> 00:15:38,880
that you can go from an original image

479
00:15:38,880 --> 00:15:40,639
to an encoded image but not backwards

480
00:15:40,639 --> 00:15:43,120
and now you have privacy for free

481
00:15:43,120 --> 00:15:45,440
and the question is um does doing this

482
00:15:45,440 --> 00:15:47,120
actually give you privacy and and the

483
00:15:47,120 --> 00:15:48,720
problem is that most of these encoding

484
00:15:48,720 --> 00:15:50,160
schemes don't actually have a proof of

485
00:15:50,160 --> 00:15:52,560
security so if the authors write down

486
00:15:52,560 --> 00:15:54,720
that like they tried hard to break their

487
00:15:54,720 --> 00:15:56,000
encoding scheme and they couldn't but

488
00:15:56,000 --> 00:15:57,600
that doesn't mean that you can't break

489
00:15:57,600 --> 00:15:58,399
it

490
00:15:58,399 --> 00:16:00,639
and we actually show that in in the case

491
00:16:00,639 --> 00:16:01,759
of instahyde it's relatively

492
00:16:01,759 --> 00:16:03,199
straightforward and for some other

493
00:16:03,199 --> 00:16:05,600
encoding schemes you can do them as well

494
00:16:05,600 --> 00:16:08,720
and the general way that you do this

495
00:16:08,720 --> 00:16:11,839
is like if you squint really hard at the

496
00:16:11,839 --> 00:16:13,600
sort of noisy image you'll kind of see

497
00:16:13,600 --> 00:16:15,600
that it still has some resemblance of of

498
00:16:15,600 --> 00:16:17,040
the cat

499
00:16:17,040 --> 00:16:19,120
and you know i can do some fairly simple

500
00:16:19,120 --> 00:16:21,199
local sort of

501
00:16:21,199 --> 00:16:23,360
convolution stuff to actually you can

502
00:16:23,360 --> 00:16:24,800
like you can like almost completely

503
00:16:24,800 --> 00:16:27,040
recover the image just from the noise

504
00:16:27,040 --> 00:16:29,120
itself and if you work with pairs of

505
00:16:29,120 --> 00:16:30,959
images and do some fancy math and some

506
00:16:30,959 --> 00:16:32,560
graph theory then you can basically

507
00:16:32,560 --> 00:16:35,199
completely recover the original images

508
00:16:35,199 --> 00:16:37,120
the authors for these defenses put out

509
00:16:37,120 --> 00:16:39,199
challenges on github asking people to

510
00:16:39,199 --> 00:16:40,959
try and break them

511
00:16:40,959 --> 00:16:42,480
and what i'm showing on the top here are

512
00:16:42,480 --> 00:16:44,079
the original images that the authors

513
00:16:44,079 --> 00:16:45,839
included in their challenge

514
00:16:45,839 --> 00:16:47,600
they of course included encoded versions

515
00:16:47,600 --> 00:16:49,040
of these images

516
00:16:49,040 --> 00:16:51,759
and then we reverse engineered

517
00:16:51,759 --> 00:16:53,680
the original images from these encoded

518
00:16:53,680 --> 00:16:55,839
versions and our reconstructions aren't

519
00:16:55,839 --> 00:16:56,800
perfect

520
00:16:56,800 --> 00:16:58,160
but they're pretty close they're about

521
00:16:58,160 --> 00:17:00,480
as close as you could get hopefully for

522
00:17:00,480 --> 00:17:02,880
images

523
00:17:03,519 --> 00:17:04,640
okay

524
00:17:04,640 --> 00:17:05,679
um

525
00:17:05,679 --> 00:17:08,160
so this is not going to work uh to do

526
00:17:08,160 --> 00:17:10,160
some kind of ad hoc privacy

527
00:17:10,160 --> 00:17:11,599
which brings me to the final point in

528
00:17:11,599 --> 00:17:13,119
this talk like which is what what can we

529
00:17:13,119 --> 00:17:14,959
do if we can't rely on some kind of ad

530
00:17:14,959 --> 00:17:17,359
hoc privacy proposal

531
00:17:17,359 --> 00:17:19,280
and basically what we're going to do

532
00:17:19,280 --> 00:17:20,559
what i'm going to propose is you can do

533
00:17:20,559 --> 00:17:22,880
with one of four things

534
00:17:22,880 --> 00:17:24,319
the first thing that we can do is just

535
00:17:24,319 --> 00:17:26,559
not use sensitive data

536
00:17:26,559 --> 00:17:28,799
right if you only train your model on

537
00:17:28,799 --> 00:17:31,280
imagenet data then extraction attacks

538
00:17:31,280 --> 00:17:32,559
don't matter

539
00:17:32,559 --> 00:17:34,559
because anyone who wants imagenet

540
00:17:34,559 --> 00:17:36,960
already has a copy on their disk and so

541
00:17:36,960 --> 00:17:38,000
it doesn't even matter if you have to do

542
00:17:38,000 --> 00:17:39,360
an extraction attack here you can always

543
00:17:39,360 --> 00:17:41,600
just download the data

544
00:17:41,600 --> 00:17:43,360
but you know let's suppose that we're in

545
00:17:43,360 --> 00:17:44,480
a setting

546
00:17:44,480 --> 00:17:46,799
where you can't rely on public data

547
00:17:46,799 --> 00:17:48,720
because the public data doesn't exist

548
00:17:48,720 --> 00:17:50,559
and you have to rely on some sensitive

549
00:17:50,559 --> 00:17:51,600
data

550
00:17:51,600 --> 00:17:52,799
then

551
00:17:52,799 --> 00:17:54,320
you can try and at least just not

552
00:17:54,320 --> 00:17:56,320
release the models

553
00:17:56,320 --> 00:17:58,640
if you treat machine learning models as

554
00:17:58,640 --> 00:18:00,880
private as the data that they was used

555
00:18:00,880 --> 00:18:02,720
to train them everything's fine if

556
00:18:02,720 --> 00:18:04,400
everyone who has access to the machine

557
00:18:04,400 --> 00:18:06,160
learning model could have just gone to

558
00:18:06,160 --> 00:18:07,679
the data set

559
00:18:07,679 --> 00:18:09,200
then again an extraction attack doesn't

560
00:18:09,200 --> 00:18:11,600
matter because the best thing that they

561
00:18:11,600 --> 00:18:12,799
could do would be to look at the data

562
00:18:12,799 --> 00:18:14,000
but they already have permission to do

563
00:18:14,000 --> 00:18:15,600
this

564
00:18:15,600 --> 00:18:18,240
think for example if we had a data set

565
00:18:18,240 --> 00:18:20,000
which was medical images and the only

566
00:18:20,000 --> 00:18:21,200
people who were allowed to query the

567
00:18:21,200 --> 00:18:23,360
model was were doctors who had access to

568
00:18:23,360 --> 00:18:24,799
medical images

569
00:18:24,799 --> 00:18:26,640
this would be fine we would even if they

570
00:18:26,640 --> 00:18:28,240
could extract the images they could just

571
00:18:28,240 --> 00:18:31,039
have looked at them directly

572
00:18:31,039 --> 00:18:32,480
okay but you know there are settings

573
00:18:32,480 --> 00:18:34,559
still where you want to release models

574
00:18:34,559 --> 00:18:35,919
to everyone

575
00:18:35,919 --> 00:18:37,200
um but

576
00:18:37,200 --> 00:18:39,520
but have it be allowed that the people

577
00:18:39,520 --> 00:18:40,960
can query the model may not be allowed

578
00:18:40,960 --> 00:18:42,480
to access the data

579
00:18:42,480 --> 00:18:43,919
then what do you do

580
00:18:43,919 --> 00:18:46,240
and one thing you can do here

581
00:18:46,240 --> 00:18:48,000
is apply what's called differential

582
00:18:48,000 --> 00:18:49,760
privacy

583
00:18:49,760 --> 00:18:52,000
this is a defense technique which is

584
00:18:52,000 --> 00:18:54,080
provably correct

585
00:18:54,080 --> 00:18:56,160
i unfortunately don't have enough time

586
00:18:56,160 --> 00:18:58,799
to tell you anything about how it works

587
00:18:58,799 --> 00:19:01,440
but if you apply it correctly

588
00:19:01,440 --> 00:19:02,720
then you're guaranteed that the

589
00:19:02,720 --> 00:19:04,559
reconstruction attacks that i've shown

590
00:19:04,559 --> 00:19:06,960
here cannot be done ever

591
00:19:06,960 --> 00:19:07,919
um

592
00:19:07,919 --> 00:19:09,520
it's not like no attacker will ever be

593
00:19:09,520 --> 00:19:10,799
able to do it the math just sort of

594
00:19:10,799 --> 00:19:12,559
proves that you can't

595
00:19:12,559 --> 00:19:13,600
there are some problems with

596
00:19:13,600 --> 00:19:15,039
differential privacy though you know

597
00:19:15,039 --> 00:19:17,120
it's a little bit slower you lose a

598
00:19:17,120 --> 00:19:18,720
little bit of accuracy sometimes it's

599
00:19:18,720 --> 00:19:20,160
hard to define

600
00:19:20,160 --> 00:19:22,720
you know what sort of make the the

601
00:19:22,720 --> 00:19:24,320
theory actually match the practice of

602
00:19:24,320 --> 00:19:26,400
definitions but you know if you can set

603
00:19:26,400 --> 00:19:28,000
it up right you'll be fine

604
00:19:28,000 --> 00:19:29,360
but again let's suppose that you can't

605
00:19:29,360 --> 00:19:30,559
what if you can't use differential

606
00:19:30,559 --> 00:19:32,960
privacy for some reason then at the very

607
00:19:32,960 --> 00:19:35,120
least what you should do is what we call

608
00:19:35,120 --> 00:19:38,320
an auditing and audit the privacy of

609
00:19:38,320 --> 00:19:39,760
your models

610
00:19:39,760 --> 00:19:41,840
and try and understand how private they

611
00:19:41,840 --> 00:19:44,240
are by you know attacking them and

612
00:19:44,240 --> 00:19:45,760
investigating their privacy in various

613
00:19:45,760 --> 00:19:46,640
ways

614
00:19:46,640 --> 00:19:48,320
again this is something i can't go into

615
00:19:48,320 --> 00:19:49,520
very much now

616
00:19:49,520 --> 00:19:51,600
fortunately here though if you wait like

617
00:19:51,600 --> 00:19:53,039
10 minutes

618
00:19:53,039 --> 00:19:54,640
reza is going to follow me and he's

619
00:19:54,640 --> 00:19:56,240
going to give an entire talk on auditing

620
00:19:56,240 --> 00:19:58,000
machine learning models and so you'll

621
00:19:58,000 --> 00:20:01,039
get all of the answers for that then um

622
00:20:01,039 --> 00:20:03,520
so with that um you know this is

623
00:20:03,520 --> 00:20:04,880
basically everything that i wanted to

624
00:20:04,880 --> 00:20:07,280
convince you of the important takeaway

625
00:20:07,280 --> 00:20:10,000
here is that you know we understand that

626
00:20:10,000 --> 00:20:11,840
machine learning models aren't private

627
00:20:11,840 --> 00:20:14,400
by default and that we can do attacks

628
00:20:14,400 --> 00:20:16,159
and it's necessary to take some steps

629
00:20:16,159 --> 00:20:18,000
maybe these maybe others

630
00:20:18,000 --> 00:20:19,520
in order to prevent these attacks from

631
00:20:19,520 --> 00:20:21,760
happening in practice and so with that

632
00:20:21,760 --> 00:20:23,200
thank you very much and i'm happy to

633
00:20:23,200 --> 00:20:26,520
take any questions

634
00:20:26,760 --> 00:20:31,849
[Applause]

635
00:23:26,240 --> 00:23:28,320
you

