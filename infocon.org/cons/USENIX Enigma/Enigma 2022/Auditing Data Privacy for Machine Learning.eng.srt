1
00:00:07,680 --> 00:00:09,440
how can we audit

2
00:00:09,440 --> 00:00:11,040
data privacy for machine learning

3
00:00:11,040 --> 00:00:13,280
algorithms

4
00:00:13,280 --> 00:00:16,079
this is not an easy question to answer

5
00:00:16,079 --> 00:00:18,400
depends on what exactly we mean by

6
00:00:18,400 --> 00:00:19,840
privacy

7
00:00:19,840 --> 00:00:21,920
what we consider as

8
00:00:21,920 --> 00:00:23,359
privacy loss

9
00:00:23,359 --> 00:00:25,920
what are privacy risks against machine

10
00:00:25,920 --> 00:00:27,599
learning algorithms

11
00:00:27,599 --> 00:00:30,000
and what we expect from a privacy

12
00:00:30,000 --> 00:00:31,679
auditing process

13
00:00:31,679 --> 00:00:34,239
in the last few years i've been working

14
00:00:34,239 --> 00:00:35,120
on

15
00:00:35,120 --> 00:00:37,600
analyzing privacy for machine learning

16
00:00:37,600 --> 00:00:39,520
trying to find answers to these

17
00:00:39,520 --> 00:00:40,719
questions

18
00:00:40,719 --> 00:00:43,600
and the result of our research showed

19
00:00:43,600 --> 00:00:44,559
that

20
00:00:44,559 --> 00:00:47,039
they pose a serious threat to data

21
00:00:47,039 --> 00:00:50,000
privacy they are vulnerable to a wide

22
00:00:50,000 --> 00:00:52,399
range of inference attacks however in

23
00:00:52,399 --> 00:00:54,559
practice in most cases they are being

24
00:00:54,559 --> 00:00:56,840
used with all their

25
00:00:56,840 --> 00:00:59,760
vulnerabilities without being properly

26
00:00:59,760 --> 00:01:02,239
audited

27
00:01:02,239 --> 00:01:05,119
i try to allow elaborate on what the

28
00:01:05,119 --> 00:01:07,040
risks are

29
00:01:07,040 --> 00:01:09,840
what's the definition of privacy and how

30
00:01:09,840 --> 00:01:11,680
we can go about performing a

31
00:01:11,680 --> 00:01:14,159
quantitative privacy auditing for

32
00:01:14,159 --> 00:01:16,720
machine learning algorithms

33
00:01:16,720 --> 00:01:20,320
so privacy policies and regulations in

34
00:01:20,320 --> 00:01:23,360
particular gdpr do include suggestions

35
00:01:23,360 --> 00:01:26,320
for auditing as one of their main

36
00:01:26,320 --> 00:01:28,960
requirements however the focus is mostly

37
00:01:28,960 --> 00:01:32,640
on data collection data sharing

38
00:01:32,640 --> 00:01:34,400
access control and basically all

39
00:01:34,400 --> 00:01:37,439
different ways that privacy

40
00:01:37,439 --> 00:01:39,520
and private information could be

41
00:01:39,520 --> 00:01:43,600
directly leaked to unauthorized entities

42
00:01:43,600 --> 00:01:44,399
in

43
00:01:44,399 --> 00:01:46,320
a machine learning

44
00:01:46,320 --> 00:01:47,439
pipeline

45
00:01:47,439 --> 00:01:49,439
this is concerned with the entities that

46
00:01:49,439 --> 00:01:51,200
store the training data and run the

47
00:01:51,200 --> 00:01:54,320
training algorithm and the servers that

48
00:01:54,320 --> 00:01:55,520
host the

49
00:01:55,520 --> 00:01:57,600
the trained model and get the input

50
00:01:57,600 --> 00:02:00,079
query data and compute the predictions

51
00:02:00,079 --> 00:02:01,600
obviously the training data and the

52
00:02:01,600 --> 00:02:03,600
input course could be very sensitive and

53
00:02:03,600 --> 00:02:06,000
contain personal information and the

54
00:02:06,000 --> 00:02:08,160
access to such data needs to be limited

55
00:02:08,160 --> 00:02:08,959
to

56
00:02:08,959 --> 00:02:11,920
authorized users and and the algorithms

57
00:02:11,920 --> 00:02:13,840
and operations that we run on them need

58
00:02:13,840 --> 00:02:16,640
to preserve the confidentiality of data

59
00:02:16,640 --> 00:02:17,680
such that

60
00:02:17,680 --> 00:02:19,280
uh the

61
00:02:19,280 --> 00:02:20,879
the users that have access to those

62
00:02:20,879 --> 00:02:24,160
systems are not able to see the data in

63
00:02:24,160 --> 00:02:26,480
clear text

64
00:02:26,480 --> 00:02:28,319
but this is not

65
00:02:28,319 --> 00:02:29,520
all the

66
00:02:29,520 --> 00:02:32,480
privacy risk another

67
00:02:32,480 --> 00:02:34,560
significant yet

68
00:02:34,560 --> 00:02:37,200
subtle information leakage happens when

69
00:02:37,200 --> 00:02:38,480
the model

70
00:02:38,480 --> 00:02:41,440
indirectly leaks information about this

71
00:02:41,440 --> 00:02:43,599
training data

72
00:02:43,599 --> 00:02:47,280
to its users that are supposed to be a

73
00:02:47,280 --> 00:02:48,840
wide range of

74
00:02:48,840 --> 00:02:51,680
users and this leakage happens through

75
00:02:51,680 --> 00:02:54,080
the model parameters or

76
00:02:54,080 --> 00:02:56,640
their predictions

77
00:02:56,640 --> 00:02:58,879
this is not a violation of access

78
00:02:58,879 --> 00:03:01,280
control or data confidentiality measures

79
00:03:01,280 --> 00:03:02,720
which prevent

80
00:03:02,720 --> 00:03:04,560
only directly catch

81
00:03:04,560 --> 00:03:06,319
but it's an obvious violation of data

82
00:03:06,319 --> 00:03:07,680
privacy

83
00:03:07,680 --> 00:03:10,640
this time through indirect leakage

84
00:03:10,640 --> 00:03:11,920
and and this

85
00:03:11,920 --> 00:03:14,239
is a real threat

86
00:03:14,239 --> 00:03:16,400
and here are a few examples that it can

87
00:03:16,400 --> 00:03:18,480
provide

88
00:03:18,480 --> 00:03:20,239
consider machine learning as a service

89
00:03:20,239 --> 00:03:21,680
platforms

90
00:03:21,680 --> 00:03:23,599
provided by

91
00:03:23,599 --> 00:03:25,280
virtually all major cloud service

92
00:03:25,280 --> 00:03:26,640
providers

93
00:03:26,640 --> 00:03:29,519
an entity can upload data to the cloud

94
00:03:29,519 --> 00:03:32,560
train a model and provide access to

95
00:03:32,560 --> 00:03:34,959
users to do predictions

96
00:03:34,959 --> 00:03:37,440
through very well-defined prediction

97
00:03:37,440 --> 00:03:38,560
apis

98
00:03:38,560 --> 00:03:41,120
we showed that without having any

99
00:03:41,120 --> 00:03:43,599
information about which models are being

100
00:03:43,599 --> 00:03:46,080
trained then without having access to

101
00:03:46,080 --> 00:03:47,519
the intermediate steps of the

102
00:03:47,519 --> 00:03:49,360
computation

103
00:03:49,360 --> 00:03:50,480
as a

104
00:03:50,480 --> 00:03:53,280
user of this system an adversary can

105
00:03:53,280 --> 00:03:55,760
tell with high confidence and accuracy

106
00:03:55,760 --> 00:03:57,200
here

107
00:03:57,200 --> 00:03:59,439
data point has been included in the

108
00:03:59,439 --> 00:04:03,439
training set of of a model

109
00:04:03,439 --> 00:04:06,000
so an authorized user can infer

110
00:04:06,000 --> 00:04:09,200
sensitive information about data

111
00:04:09,200 --> 00:04:12,159
which it does not have access to

112
00:04:12,159 --> 00:04:15,200
right so this obviously violates data

113
00:04:15,200 --> 00:04:17,839
privacy of those who contributed data uh

114
00:04:17,839 --> 00:04:19,680
to the training set

115
00:04:19,680 --> 00:04:21,839
another example is the case of

116
00:04:21,839 --> 00:04:26,000
generative models notably that of

117
00:04:26,000 --> 00:04:29,520
language models that generate text

118
00:04:29,520 --> 00:04:31,680
an adversary can run similar inference

119
00:04:31,680 --> 00:04:33,840
attacks as in the ones that i mentioned

120
00:04:33,840 --> 00:04:36,960
for the uh ml as a service platforms but

121
00:04:36,960 --> 00:04:40,639
this time to partially reconstruct text

122
00:04:40,639 --> 00:04:42,560
from the training data

123
00:04:42,560 --> 00:04:44,720
and what is important is that usually

124
00:04:44,720 --> 00:04:47,360
the atypical data

125
00:04:47,360 --> 00:04:49,600
which are more sensitive are more

126
00:04:49,600 --> 00:04:52,960
vulnerable to these attacks

127
00:04:52,960 --> 00:04:55,280
last example that i want to

128
00:04:55,280 --> 00:04:56,800
give is

129
00:04:56,800 --> 00:04:58,960
the federated learning

130
00:04:58,960 --> 00:05:02,160
that is by the way wrongly advertised as

131
00:05:02,160 --> 00:05:05,600
a privacy enhancing technology

132
00:05:05,600 --> 00:05:07,919
multiple entities for example hospitals

133
00:05:07,919 --> 00:05:10,639
or mobile phones repeatedly share the

134
00:05:10,639 --> 00:05:12,720
models that they have locally trained on

135
00:05:12,720 --> 00:05:15,600
their private data with a server

136
00:05:15,600 --> 00:05:17,680
to be aggregated and shared back with

137
00:05:17,680 --> 00:05:19,440
them

138
00:05:19,440 --> 00:05:21,440
so they repeat this

139
00:05:21,440 --> 00:05:24,080
and after many iterations

140
00:05:24,080 --> 00:05:26,160
the model that they train converge to

141
00:05:26,160 --> 00:05:27,600
some

142
00:05:27,600 --> 00:05:28,639
set of

143
00:05:28,639 --> 00:05:30,400
parameter values

144
00:05:30,400 --> 00:05:33,520
which is hopefully more accurate than

145
00:05:33,520 --> 00:05:34,320
what

146
00:05:34,320 --> 00:05:36,720
they could have had by training only on

147
00:05:36,720 --> 00:05:38,960
their local data so they don't share the

148
00:05:38,960 --> 00:05:41,520
data across the network but they share

149
00:05:41,520 --> 00:05:43,680
their models repeatedly

150
00:05:43,680 --> 00:05:46,479
we show that a curious server or a

151
00:05:46,479 --> 00:05:49,039
participant in federated learning

152
00:05:49,039 --> 00:05:51,520
can run inference attacks to reconstruct

153
00:05:51,520 --> 00:05:54,160
sensitive information about participants

154
00:05:54,160 --> 00:05:55,840
local private data

155
00:05:55,840 --> 00:05:58,639
using various types of inference attacks

156
00:05:58,639 --> 00:06:00,880
we also showed that an attacker

157
00:06:00,880 --> 00:06:03,759
can also fool other participants by

158
00:06:03,759 --> 00:06:07,199
sharing adversarial models

159
00:06:07,199 --> 00:06:09,840
so that they confess even more

160
00:06:09,840 --> 00:06:13,280
information about their training data

161
00:06:13,280 --> 00:06:16,479
so going beyond running passive attacks

162
00:06:16,479 --> 00:06:18,400
the adversary can run

163
00:06:18,400 --> 00:06:20,960
active inference attacks to learn more

164
00:06:20,960 --> 00:06:22,479
information about

165
00:06:22,479 --> 00:06:24,960
the data of other participants

166
00:06:24,960 --> 00:06:28,720
in all these examples

167
00:06:28,720 --> 00:06:30,400
we showed that the anniversary can

168
00:06:30,400 --> 00:06:32,800
indirectly learn information about the

169
00:06:32,800 --> 00:06:36,000
data of other participants

170
00:06:36,000 --> 00:06:38,479
and all these examples show that for the

171
00:06:38,479 --> 00:06:40,880
case of high dimensional models trained

172
00:06:40,880 --> 00:06:44,639
on sensitive personal data sets

173
00:06:44,639 --> 00:06:45,919
the model

174
00:06:45,919 --> 00:06:47,600
is personal data

175
00:06:47,600 --> 00:06:49,599
models are as sensitive as the data on

176
00:06:49,599 --> 00:06:51,120
which they are trained

177
00:06:51,120 --> 00:06:53,840
they have a tendency to memorize

178
00:06:53,840 --> 00:06:57,120
information about their training data

179
00:06:57,120 --> 00:06:58,960
that can be extracted potentially by

180
00:06:58,960 --> 00:07:02,359
inference attacks

181
00:07:02,639 --> 00:07:04,960
so given that machine learning is being

182
00:07:04,960 --> 00:07:07,440
used widely on all sorts of personal

183
00:07:07,440 --> 00:07:09,919
data we need to extend

184
00:07:09,919 --> 00:07:12,479
privacy auditing to ml

185
00:07:12,479 --> 00:07:16,160
in order to analyze the privacy risks of

186
00:07:16,160 --> 00:07:19,520
indirect information leakage

187
00:07:19,520 --> 00:07:21,199
and in this talk i'm going to tell you

188
00:07:21,199 --> 00:07:25,360
how to do that in a systematic way

189
00:07:25,360 --> 00:07:27,599
reason i'm going to design a game that

190
00:07:27,599 --> 00:07:30,000
allows us to quantitatively measure such

191
00:07:30,000 --> 00:07:32,160
privacy risks

192
00:07:32,160 --> 00:07:34,960
assume that we have a data set

193
00:07:34,960 --> 00:07:38,240
and we use an algorithm to train a model

194
00:07:38,240 --> 00:07:40,560
we want to know how much information the

195
00:07:40,560 --> 00:07:42,240
algorithm leaks

196
00:07:42,240 --> 00:07:45,199
about the data records in its training

197
00:07:45,199 --> 00:07:47,440
set

198
00:07:47,440 --> 00:07:50,000
here is how we design the game

199
00:07:50,000 --> 00:07:52,960
let's add a new record to the training

200
00:07:52,960 --> 00:07:53,919
set

201
00:07:53,919 --> 00:07:58,000
and train a second model

202
00:07:58,000 --> 00:07:58,800
so

203
00:07:58,800 --> 00:08:00,879
we want to know

204
00:08:00,879 --> 00:08:03,039
if a powerful attacker

205
00:08:03,039 --> 00:08:05,680
that gets one of these models

206
00:08:05,680 --> 00:08:08,240
at random

207
00:08:08,240 --> 00:08:12,720
can distinguish between the two cases

208
00:08:12,720 --> 00:08:14,639
the case where

209
00:08:14,639 --> 00:08:15,919
data set

210
00:08:15,919 --> 00:08:18,879
does not include data record d

211
00:08:18,879 --> 00:08:20,960
and the case where the data set includes

212
00:08:20,960 --> 00:08:23,280
data d note that the adversary only

213
00:08:23,280 --> 00:08:25,120
observes one model

214
00:08:25,120 --> 00:08:27,599
which is randomly sampled from these two

215
00:08:27,599 --> 00:08:29,120
cases

216
00:08:29,120 --> 00:08:30,800
and the question is whether the

217
00:08:30,800 --> 00:08:32,080
adversary can tell the difference

218
00:08:32,080 --> 00:08:34,240
between the two

219
00:08:34,240 --> 00:08:35,120
if

220
00:08:35,120 --> 00:08:36,640
p succeeds

221
00:08:36,640 --> 00:08:38,880
it indicates that the algorithm

222
00:08:38,880 --> 00:08:41,279
leaks information about

223
00:08:41,279 --> 00:08:44,480
this additional data records

224
00:08:44,480 --> 00:08:46,240
and this information goes beyond the

225
00:08:46,240 --> 00:08:48,399
general information that any user of the

226
00:08:48,399 --> 00:08:49,279
model

227
00:08:49,279 --> 00:08:52,720
is expected to learn from the data

228
00:08:52,720 --> 00:08:54,959
and if the attacker succeeds

229
00:08:54,959 --> 00:08:57,920
many time in this game

230
00:08:57,920 --> 00:09:00,480
as we change data record d and as we

231
00:09:00,480 --> 00:09:02,480
change the training data and as we run

232
00:09:02,480 --> 00:09:05,440
the algorithm multiple times

233
00:09:05,440 --> 00:09:07,920
then we can confidently conclude that

234
00:09:07,920 --> 00:09:10,240
the ml algorithm is violating data

235
00:09:10,240 --> 00:09:12,320
privacy

236
00:09:12,320 --> 00:09:16,800
and it is too risky to be deployed

237
00:09:17,440 --> 00:09:21,360
we call this attack algorithm

238
00:09:21,360 --> 00:09:24,240
which the adversary uses in that game

239
00:09:24,240 --> 00:09:26,160
membership inference attack

240
00:09:26,160 --> 00:09:28,480
because given a model

241
00:09:28,480 --> 00:09:30,560
and a data point

242
00:09:30,560 --> 00:09:32,399
the adversary asks

243
00:09:32,399 --> 00:09:34,480
whether this data point was part of the

244
00:09:34,480 --> 00:09:36,560
training set

245
00:09:36,560 --> 00:09:39,279
we simulate this attack

246
00:09:39,279 --> 00:09:42,160
and use this method to audit privacy for

247
00:09:42,160 --> 00:09:44,640
machine learning

248
00:09:44,640 --> 00:09:46,720
for this we can use

249
00:09:46,720 --> 00:09:50,000
well-established statistical methods

250
00:09:50,000 --> 00:09:52,720
called hypothesis testing to solve the

251
00:09:52,720 --> 00:09:55,120
problem

252
00:09:56,640 --> 00:09:58,959
i'm not going to talk about the details

253
00:09:58,959 --> 00:10:00,880
of these tests

254
00:10:00,880 --> 00:10:01,920
but

255
00:10:01,920 --> 00:10:04,480
at a high level what the adversary does

256
00:10:04,480 --> 00:10:07,040
is to use

257
00:10:07,040 --> 00:10:09,680
a couple of statistical tests

258
00:10:09,680 --> 00:10:12,000
which compare the behavior of the model

259
00:10:12,000 --> 00:10:15,760
on the target data with respect to

260
00:10:15,760 --> 00:10:16,640
some

261
00:10:16,640 --> 00:10:19,839
reference baselines

262
00:10:20,240 --> 00:10:22,800
to enable him to distinguish the the

263
00:10:22,800 --> 00:10:24,880
behavior of the model on its

264
00:10:24,880 --> 00:10:26,800
training data versus the behavior of the

265
00:10:26,800 --> 00:10:28,079
model on

266
00:10:28,079 --> 00:10:31,600
on on the data outside his training set

267
00:10:31,600 --> 00:10:34,880
and here are a couple of tests

268
00:10:34,880 --> 00:10:35,920
in

269
00:10:35,920 --> 00:10:38,399
simplified form

270
00:10:38,399 --> 00:10:40,079
one test

271
00:10:40,079 --> 00:10:43,200
involves comparing the

272
00:10:43,200 --> 00:10:46,800
error of the model the target model on

273
00:10:46,800 --> 00:10:49,600
target data point d

274
00:10:49,600 --> 00:10:53,360
with the error of the the target model

275
00:10:53,360 --> 00:10:56,399
on some random samples from

276
00:10:56,399 --> 00:10:59,120
the universe

277
00:10:59,279 --> 00:11:02,160
if the error of the model on the target

278
00:11:02,160 --> 00:11:04,160
data and random samples from the

279
00:11:04,160 --> 00:11:06,320
universe are similar

280
00:11:06,320 --> 00:11:08,240
then maybe we can conclude that this

281
00:11:08,240 --> 00:11:10,480
data point has not been a member of the

282
00:11:10,480 --> 00:11:12,240
training set because the model does not

283
00:11:12,240 --> 00:11:15,120
distinguish between them

284
00:11:15,120 --> 00:11:16,800
and

285
00:11:16,800 --> 00:11:20,079
the the main idea behind this is that

286
00:11:20,079 --> 00:11:22,880
there is potentially difference between

287
00:11:22,880 --> 00:11:24,800
the error of the model on on its

288
00:11:24,800 --> 00:11:26,240
training set

289
00:11:26,240 --> 00:11:28,800
versus its test set

290
00:11:28,800 --> 00:11:31,200
so members versus non-members

291
00:11:31,200 --> 00:11:33,440
are distinguishable when we look at the

292
00:11:33,440 --> 00:11:36,320
error of the model

293
00:11:36,560 --> 00:11:38,560
another test

294
00:11:38,560 --> 00:11:41,839
again in simplified form

295
00:11:41,839 --> 00:11:44,240
is to compare the error

296
00:11:44,240 --> 00:11:46,560
of the target model on the target data

297
00:11:46,560 --> 00:11:48,720
point d

298
00:11:48,720 --> 00:11:50,800
with the error of some reference models

299
00:11:50,800 --> 00:11:52,000
which are

300
00:11:52,000 --> 00:11:54,079
not trained on d

301
00:11:54,079 --> 00:11:55,920
and are trained on some random samples

302
00:11:55,920 --> 00:11:58,639
from the population

303
00:11:58,639 --> 00:12:00,880
and see if they are similar again if the

304
00:12:00,880 --> 00:12:02,079
error of

305
00:12:02,079 --> 00:12:04,480
the target model on d and the error of

306
00:12:04,480 --> 00:12:07,440
the reference models on d are similar

307
00:12:07,440 --> 00:12:10,800
we can probably conclude that this data

308
00:12:10,800 --> 00:12:13,680
is a non-member

309
00:12:13,680 --> 00:12:17,680
but in either of these tests

310
00:12:18,079 --> 00:12:20,480
if we find some information

311
00:12:20,480 --> 00:12:24,399
that convinces us that there is some

312
00:12:24,880 --> 00:12:27,360
difference in behavior of the models

313
00:12:27,360 --> 00:12:29,040
with respect to

314
00:12:29,040 --> 00:12:31,200
data point d

315
00:12:31,200 --> 00:12:33,360
compared to our references

316
00:12:33,360 --> 00:12:38,000
then we can predict as member

317
00:12:38,000 --> 00:12:40,320
so these tests

318
00:12:40,320 --> 00:12:43,200
enable us to quantify different types of

319
00:12:43,200 --> 00:12:47,200
the error that the adversary makes

320
00:12:47,200 --> 00:12:50,639
but in a simplified way

321
00:12:50,639 --> 00:12:54,079
the the success rate of the attacker

322
00:12:54,079 --> 00:12:57,680
is a rough measure for privacy loss

323
00:12:57,680 --> 00:12:59,120
which is part of the output of the

324
00:12:59,120 --> 00:13:01,519
privacy auditing process

325
00:13:01,519 --> 00:13:04,240
the perfect for privacy would be when

326
00:13:04,240 --> 00:13:06,480
best membership influence attacks cannot

327
00:13:06,480 --> 00:13:09,600
beat a random guest strategy where the

328
00:13:09,600 --> 00:13:12,000
adversary has no idea whether the data

329
00:13:12,000 --> 00:13:13,040
point was

330
00:13:13,040 --> 00:13:15,360
a member or not a 50

331
00:13:15,360 --> 00:13:18,000
success rate

332
00:13:18,320 --> 00:13:20,000
so this methodology has already

333
00:13:20,000 --> 00:13:22,320
influenced the ai security and privacy

334
00:13:22,320 --> 00:13:24,160
guidelines

335
00:13:24,160 --> 00:13:26,560
and

336
00:13:27,120 --> 00:13:29,920
following this we have implemented

337
00:13:29,920 --> 00:13:31,839
the whole framework

338
00:13:31,839 --> 00:13:34,480
as an open source tool called machine

339
00:13:34,480 --> 00:13:35,440
learning

340
00:13:35,440 --> 00:13:36,880
privacy meter

341
00:13:36,880 --> 00:13:39,199
which is currently being used in

342
00:13:39,199 --> 00:13:41,920
academia and also being adopted in

343
00:13:41,920 --> 00:13:43,600
industry

344
00:13:43,600 --> 00:13:45,600
you simply provide data training

345
00:13:45,600 --> 00:13:47,920
algorithm and models and the tool

346
00:13:47,920 --> 00:13:50,720
performs the complete privacy oddity it

347
00:13:50,720 --> 00:13:53,360
provides numerical reports about privacy

348
00:13:53,360 --> 00:13:55,279
laws of the model

349
00:13:55,279 --> 00:13:56,560
as well as

350
00:13:56,560 --> 00:13:58,720
privacy scores for each record in the

351
00:13:58,720 --> 00:14:00,000
training set

352
00:14:00,000 --> 00:14:02,079
so to know which data points are more

353
00:14:02,079 --> 00:14:04,320
vulnerable than others this enables you

354
00:14:04,320 --> 00:14:07,839
to compare different training algorithms

355
00:14:07,839 --> 00:14:11,199
and also different models

356
00:14:11,199 --> 00:14:14,000
and decide whether it is safe to

357
00:14:14,000 --> 00:14:15,600
use a model or not

358
00:14:15,600 --> 00:14:18,000
this also enables us

359
00:14:18,000 --> 00:14:19,199
to

360
00:14:19,199 --> 00:14:22,240
evaluate the efficacy of different

361
00:14:22,240 --> 00:14:25,519
protection algorithms

362
00:14:26,320 --> 00:14:29,360
the evaluation methods and the the tool

363
00:14:29,360 --> 00:14:31,760
can apply on all different types of data

364
00:14:31,760 --> 00:14:34,560
from micro data to images and text

365
00:14:34,560 --> 00:14:37,839
and here i provide a few examples

366
00:14:37,839 --> 00:14:40,320
let's train a language model on speaker

367
00:14:40,320 --> 00:14:43,360
annotated ted talks and check whether we

368
00:14:43,360 --> 00:14:46,320
can identify which talks are used for

369
00:14:46,320 --> 00:14:47,360
training

370
00:14:47,360 --> 00:14:49,760
we can see that as the number of

371
00:14:49,760 --> 00:14:53,040
sentences that an adversary knows about

372
00:14:53,040 --> 00:14:55,760
the particular speaker increases

373
00:14:55,760 --> 00:14:57,600
the chance that

374
00:14:57,600 --> 00:15:00,480
you can correctly infer the membership

375
00:15:00,480 --> 00:15:01,440
of

376
00:15:01,440 --> 00:15:02,240
that

377
00:15:02,240 --> 00:15:04,399
speaker in the training set

378
00:15:04,399 --> 00:15:05,600
increases

379
00:15:05,600 --> 00:15:08,880
very rapidly to 100

380
00:15:09,600 --> 00:15:12,240
this is due to the large capacity of

381
00:15:12,240 --> 00:15:14,880
language models and their tendency for

382
00:15:14,880 --> 00:15:16,480
memorization

383
00:15:16,480 --> 00:15:18,959
basically there is not much privacy when

384
00:15:18,959 --> 00:15:21,839
using these kind of models

385
00:15:21,839 --> 00:15:24,240
the auditing reports also gives us the

386
00:15:24,240 --> 00:15:27,120
list of most vulnerable points

387
00:15:27,120 --> 00:15:29,360
which are the more atypical points in

388
00:15:29,360 --> 00:15:32,399
the data set these are some examples

389
00:15:32,399 --> 00:15:34,639
as you can see these are uh you know

390
00:15:34,639 --> 00:15:36,880
sensitive topics

391
00:15:36,880 --> 00:15:39,040
cracking stocks net or secrets of

392
00:15:39,040 --> 00:15:41,519
surveillance state which are different

393
00:15:41,519 --> 00:15:42,639
from the

394
00:15:42,639 --> 00:15:44,959
usual talks on

395
00:15:44,959 --> 00:15:47,279
content

396
00:15:47,279 --> 00:15:48,800
another example is on image

397
00:15:48,800 --> 00:15:51,440
classification models and we observed

398
00:15:51,440 --> 00:15:53,680
that as the number of model parameters

399
00:15:53,680 --> 00:15:55,040
increase

400
00:15:55,040 --> 00:15:58,079
the model becomes more vulnerable to

401
00:15:58,079 --> 00:15:59,120
attacks

402
00:15:59,120 --> 00:16:00,240
even

403
00:16:00,240 --> 00:16:02,800
though this large capacity

404
00:16:02,800 --> 00:16:05,040
could help the models to

405
00:16:05,040 --> 00:16:06,839
learn

406
00:16:06,839 --> 00:16:12,360
um models with high generalizability

407
00:16:14,639 --> 00:16:16,639
so to conclude

408
00:16:16,639 --> 00:16:19,920
uh given the privacy vulnerabilities of

409
00:16:19,920 --> 00:16:21,920
machine learning algorithms given the

410
00:16:21,920 --> 00:16:24,480
fact that their capacity to learn

411
00:16:24,480 --> 00:16:26,480
complex tasks

412
00:16:26,480 --> 00:16:29,120
increases

413
00:16:29,279 --> 00:16:31,680
i would say enabling access to models

414
00:16:31,680 --> 00:16:34,079
without auditing them and without

415
00:16:34,079 --> 00:16:36,480
mitigating the risks

416
00:16:36,480 --> 00:16:39,199
and their privacy risks

417
00:16:39,199 --> 00:16:41,040
would not be much worse than allowing

418
00:16:41,040 --> 00:16:43,839
unauthorized access to data so this is a

419
00:16:43,839 --> 00:16:45,120
serious

420
00:16:45,120 --> 00:16:46,399
privacy risk

421
00:16:46,399 --> 00:16:48,320
especially because we are training many

422
00:16:48,320 --> 00:16:50,880
many models on our sensitive data and

423
00:16:50,880 --> 00:16:55,199
it's not a matter of one single model

424
00:16:55,199 --> 00:16:57,519
being

425
00:16:58,000 --> 00:16:59,440
risky

426
00:16:59,440 --> 00:17:00,160
so

427
00:17:00,160 --> 00:17:02,839
ml privacy meter aids

428
00:17:02,839 --> 00:17:06,079
regulatory compliance through a

429
00:17:06,079 --> 00:17:08,880
systematic method to audit data privacy

430
00:17:08,880 --> 00:17:11,039
for a wide range of machine learning

431
00:17:11,039 --> 00:17:12,319
algorithms

432
00:17:12,319 --> 00:17:14,240
it's an open source tool

433
00:17:14,240 --> 00:17:16,559
and and if you are interested you can

434
00:17:16,559 --> 00:17:18,480
you can go and contribute to that

435
00:17:18,480 --> 00:17:20,319
use it um

436
00:17:20,319 --> 00:17:22,400
for your systems

437
00:17:22,400 --> 00:17:24,319
and uh here is also the list of

438
00:17:24,319 --> 00:17:27,599
references for the algorithm that we use

439
00:17:27,599 --> 00:17:29,679
for developing this tool

440
00:17:29,679 --> 00:17:31,360
i'd like to thank

441
00:17:31,360 --> 00:17:34,000
the audience for listening to this talk

442
00:17:34,000 --> 00:17:38,200
and i'd be happy to take questions

