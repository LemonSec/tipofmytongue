1
00:00:08,240 --> 00:00:10,719
hi everybody we are sasha and camille

2
00:00:10,719 --> 00:00:12,240
and we're here representing the team

3
00:00:12,240 --> 00:00:14,400
behind a report called bug bounties for

4
00:00:14,400 --> 00:00:16,239
algorithmic harms which was just

5
00:00:16,239 --> 00:00:18,160
published by the algorithmic justice

6
00:00:18,160 --> 00:00:20,560
league the report looks at how people

7
00:00:20,560 --> 00:00:23,119
who work to minimize algorithmic harm

8
00:00:23,119 --> 00:00:25,599
can learn from infosec practices

9
00:00:25,599 --> 00:00:28,000
especially from vulnerability reporting

10
00:00:28,000 --> 00:00:30,560
and disclosure practices you can read

11
00:00:30,560 --> 00:00:33,440
the full report on hl.org

12
00:00:33,440 --> 00:00:35,200
bugs and it's filled with great

13
00:00:35,200 --> 00:00:37,680
information

14
00:00:37,680 --> 00:00:40,160
hi everyone so i'm sasha i use day and

15
00:00:40,160 --> 00:00:41,520
she pronouns and i'm director of

16
00:00:41,520 --> 00:00:43,920
research and design at ajl let's start

17
00:00:43,920 --> 00:00:45,039
with a little bit about the history

18
00:00:45,039 --> 00:00:46,559
behind the report

19
00:00:46,559 --> 00:00:49,360
back in 2017 when dr joy bulowini

20
00:00:49,360 --> 00:00:51,840
founder of ajl exposed how facial

21
00:00:51,840 --> 00:00:54,719
recognition technologies or frts

22
00:00:54,719 --> 00:00:56,879
fail more on women and darker skinned

23
00:00:56,879 --> 00:01:00,000
people and most on dark-skinned women

24
00:01:00,000 --> 00:01:01,760
she and her collaborators triggered a

25
00:01:01,760 --> 00:01:05,040
very adversarial reaction from industry

26
00:01:05,040 --> 00:01:08,320
so frt vendors initially responded by

27
00:01:08,320 --> 00:01:10,799
attempting to discredit the research and

28
00:01:10,799 --> 00:01:13,119
the researchers but eventually they had

29
00:01:13,119 --> 00:01:15,600
to backtrack especially after a missed

30
00:01:15,600 --> 00:01:18,000
study confirmed the findings

31
00:01:18,000 --> 00:01:20,320
so this is an example of how what's

32
00:01:20,320 --> 00:01:22,240
happening currently in the fight against

33
00:01:22,240 --> 00:01:24,880
algorithmic harm has parallels in the

34
00:01:24,880 --> 00:01:27,439
early history of infosec when companies

35
00:01:27,439 --> 00:01:29,759
were constantly attempting to discredit

36
00:01:29,759 --> 00:01:32,159
sue and even file criminal charges

37
00:01:32,159 --> 00:01:34,400
against hackers just for finding and

38
00:01:34,400 --> 00:01:36,880
sharing security vulnes

39
00:01:36,880 --> 00:01:38,479
before we go any further though let's

40
00:01:38,479 --> 00:01:41,920
clarify what we mean by algorithmic harm

41
00:01:41,920 --> 00:01:44,240
so algorithmic harm occurs when an

42
00:01:44,240 --> 00:01:46,240
organization or individual

43
00:01:46,240 --> 00:01:48,880
uses an algorithmic system to automate

44
00:01:48,880 --> 00:01:50,880
classification prediction

45
00:01:50,880 --> 00:01:53,200
recommendations or scoring

46
00:01:53,200 --> 00:01:55,280
in a process that harms people in some

47
00:01:55,280 --> 00:01:56,399
way

48
00:01:56,399 --> 00:01:58,799
so algorithmic harm could involve loss

49
00:01:58,799 --> 00:02:01,040
of freedom or opportunity

50
00:02:01,040 --> 00:02:04,000
violation of rights or physical safety

51
00:02:04,000 --> 00:02:06,799
social stigma or affronts to dignity or

52
00:02:06,799 --> 00:02:10,160
even a loss of life

53
00:02:10,160 --> 00:02:12,560
now these days people often talk about

54
00:02:12,560 --> 00:02:15,360
racial or gender bias in training data

55
00:02:15,360 --> 00:02:17,520
and that is part of it but algorithmic

56
00:02:17,520 --> 00:02:20,319
harms are not only about biased data

57
00:02:20,319 --> 00:02:22,400
they may arise at any stage in the life

58
00:02:22,400 --> 00:02:24,800
cycle of an algorithmic system

59
00:02:24,800 --> 00:02:26,239
during data collection and

60
00:02:26,239 --> 00:02:27,599
classification

61
00:02:27,599 --> 00:02:30,160
in model development and testing or

62
00:02:30,160 --> 00:02:32,720
post-deployment in the context of use by

63
00:02:32,720 --> 00:02:34,720
humans

64
00:02:34,720 --> 00:02:38,000
and this is not just hypothetical people

65
00:02:38,000 --> 00:02:40,160
in the real world experience algorithmic

66
00:02:40,160 --> 00:02:42,080
harm every day

67
00:02:42,080 --> 00:02:45,280
for example in 2020 the aclu sued on

68
00:02:45,280 --> 00:02:47,519
behalf of robert williams who's pictured

69
00:02:47,519 --> 00:02:48,319
here

70
00:02:48,319 --> 00:02:50,080
he was falsely arrested in front of his

71
00:02:50,080 --> 00:02:52,160
wife and two daughters thanks to the

72
00:02:52,160 --> 00:02:54,720
failure of facial recognition technology

73
00:02:54,720 --> 00:02:56,800
used by the detroit police

74
00:02:56,800 --> 00:02:58,879
he was mistakenly identified as someone

75
00:02:58,879 --> 00:03:00,879
who had committed a theft

76
00:03:00,879 --> 00:03:04,319
so algorithmic harm can be life-changing

77
00:03:04,319 --> 00:03:06,480
and the idea of rewarding folks who

78
00:03:06,480 --> 00:03:08,000
might be able to help prevent

79
00:03:08,000 --> 00:03:10,959
algorithmic harm makes complete sense

80
00:03:10,959 --> 00:03:13,120
similar to how rewarding hackers for

81
00:03:13,120 --> 00:03:15,120
discovering bones make sense

82
00:03:15,120 --> 00:03:17,280
but in both cases the devil is in the

83
00:03:17,280 --> 00:03:18,640
details

84
00:03:18,640 --> 00:03:20,800
so to help us more fully understand the

85
00:03:20,800 --> 00:03:23,440
draw the draws and the drawbacks of bug

86
00:03:23,440 --> 00:03:24,480
bounties

87
00:03:24,480 --> 00:03:26,000
and whether they might really be useful

88
00:03:26,000 --> 00:03:27,760
for algorithmic harm

89
00:03:27,760 --> 00:03:29,599
we turned to fellow practitioners and

90
00:03:29,599 --> 00:03:31,120
researchers who were kind enough to

91
00:03:31,120 --> 00:03:33,440
share their wisdom experience and ideas

92
00:03:33,440 --> 00:03:36,159
with us for this report

93
00:03:36,159 --> 00:03:38,159
today we'll share our insights with you

94
00:03:38,159 --> 00:03:40,720
through a handful of mini vignettes from

95
00:03:40,720 --> 00:03:43,599
the short but colorful history of bug

96
00:03:43,599 --> 00:03:46,159
bounties first we will take a look at a

97
00:03:46,159 --> 00:03:48,239
wacky historical bounty as a way to

98
00:03:48,239 --> 00:03:50,239
highlight some central themes from our

99
00:03:50,239 --> 00:03:52,560
research then we'll jump ahead at the

100
00:03:52,560 --> 00:03:54,720
moment at which traditional infosec

101
00:03:54,720 --> 00:03:57,280
bounties start to encompass a greater

102
00:03:57,280 --> 00:03:59,680
range of social technical issues which

103
00:03:59,680 --> 00:04:02,159
happens around 2018

104
00:04:02,159 --> 00:04:03,760
then we will look at twitter's bias

105
00:04:03,760 --> 00:04:06,080
bounty challenge from last year before

106
00:04:06,080 --> 00:04:08,159
closing with a look ahead to what's

107
00:04:08,159 --> 00:04:09,760
happening right now with proctoring

108
00:04:09,760 --> 00:04:12,080
software and what this may suggest for

109
00:04:12,080 --> 00:04:14,879
the future of algorithmic harms bounty

110
00:04:14,879 --> 00:04:17,199
a key contribution of our work is to

111
00:04:17,199 --> 00:04:19,199
take a look at the verities of bounties

112
00:04:19,199 --> 00:04:21,680
and associated mechanisms for reporting

113
00:04:21,680 --> 00:04:24,160
and disclosing vulnerabilities and to

114
00:04:24,160 --> 00:04:26,000
abstract some key programmatic

115
00:04:26,000 --> 00:04:28,160
differences this is what we call our

116
00:04:28,160 --> 00:04:29,840
design lovers

117
00:04:29,840 --> 00:04:32,560
how these levers are configured really

118
00:04:32,560 --> 00:04:36,000
matters for these programs impact on

119
00:04:36,000 --> 00:04:38,720
transparency accountability community

120
00:04:38,720 --> 00:04:41,199
building and many other aspects that we

121
00:04:41,199 --> 00:04:42,800
care deeply about

122
00:04:42,800 --> 00:04:45,280
we are building on previous work noting

123
00:04:45,280 --> 00:04:47,600
that many of these bug bounty programs

124
00:04:47,600 --> 00:04:50,160
vary by how they define market access

125
00:04:50,160 --> 00:04:53,040
program duration and compensation and to

126
00:04:53,040 --> 00:04:55,600
those design we add whether public

127
00:04:55,600 --> 00:04:58,000
disclosure is guaranteed and on a

128
00:04:58,000 --> 00:05:00,479
pre-established time frame how a given

129
00:05:00,479 --> 00:05:02,720
program is managed is it fully in-house

130
00:05:02,720 --> 00:05:04,800
or is it outsourced to some degree

131
00:05:04,800 --> 00:05:07,360
perhaps to the platforms like hacker one

132
00:05:07,360 --> 00:05:08,639
or background

133
00:05:08,639 --> 00:05:10,639
what is officially considered in scope

134
00:05:10,639 --> 00:05:13,120
and what level of access are researchers

135
00:05:13,120 --> 00:05:14,639
actually given

136
00:05:14,639 --> 00:05:16,639
whether a program is voluntary or

137
00:05:16,639 --> 00:05:19,039
adversarial and in other words like has

138
00:05:19,039 --> 00:05:21,600
the target organization consented to

139
00:05:21,600 --> 00:05:22,800
receiving

140
00:05:22,800 --> 00:05:24,560
vulnerability report

141
00:05:24,560 --> 00:05:26,160
all right let's go into our first

142
00:05:26,160 --> 00:05:28,639
vignette

143
00:05:29,440 --> 00:05:32,720
okay so this is a challenge lock from

144
00:05:32,720 --> 00:05:34,560
the 18th century

145
00:05:34,560 --> 00:05:37,280
it was manufactured by joseph brahma and

146
00:05:37,280 --> 00:05:39,759
had almost 500 million possible

147
00:05:39,759 --> 00:05:41,520
combinations

148
00:05:41,520 --> 00:05:44,720
and you can see on the cover of the lock

149
00:05:44,720 --> 00:05:46,800
there's an inscription

150
00:05:46,800 --> 00:05:49,199
and most copies of this lock were not

151
00:05:49,199 --> 00:05:51,520
inscribed in this way this particular

152
00:05:51,520 --> 00:05:53,759
one was created to sit in brahma's shop

153
00:05:53,759 --> 00:05:56,479
front as a kind of advertising

154
00:05:56,479 --> 00:05:58,400
he's saying i'm so confident in the

155
00:05:58,400 --> 00:06:00,720
strength of this lock that i'll pay you

156
00:06:00,720 --> 00:06:02,560
if you can pick it

157
00:06:02,560 --> 00:06:04,800
well brahma's lock remained unbreakable

158
00:06:04,800 --> 00:06:07,360
and the bounty uncollected for decades

159
00:06:07,360 --> 00:06:11,600
61 years to be precise until 1851 when

160
00:06:11,600 --> 00:06:14,400
another locksmith alfred charles hobbs

161
00:06:14,400 --> 00:06:16,880
succeeded in picking brahma's lock after

162
00:06:16,880 --> 00:06:19,520
over 50 hours of tinkering during the

163
00:06:19,520 --> 00:06:22,639
course of two weeks

164
00:06:22,639 --> 00:06:24,960
considering this early security bounty

165
00:06:24,960 --> 00:06:27,840
using our design levers the challenge is

166
00:06:27,840 --> 00:06:30,240
voluntary rather than adversarial since

167
00:06:30,240 --> 00:06:31,680
the locksmith is the one who offered the

168
00:06:31,680 --> 00:06:32,880
challenge

169
00:06:32,880 --> 00:06:34,800
compensated in the form of a one-time

170
00:06:34,800 --> 00:06:36,000
bounty

171
00:06:36,000 --> 00:06:37,600
because only after he managed to pick

172
00:06:37,600 --> 00:06:39,520
the luck and since he was the first one

173
00:06:39,520 --> 00:06:41,440
to do it hobbes was paid for his

174
00:06:41,440 --> 00:06:42,639
discovery

175
00:06:42,639 --> 00:06:44,240
actually he was well compensated the

176
00:06:44,240 --> 00:06:47,120
bounty was 200 guineas which is about

177
00:06:47,120 --> 00:06:49,840
thousand dollars in today's currency

178
00:06:49,840 --> 00:06:52,319
in terms of the disclosure model hobbes

179
00:06:52,319 --> 00:06:54,400
reportedly performed the feat in front

180
00:06:54,400 --> 00:06:56,720
of journalists live which is about as

181
00:06:56,720 --> 00:06:59,440
full disclosure as it's possible to go

182
00:06:59,440 --> 00:07:01,440
around the same time hobbs was also

183
00:07:01,440 --> 00:07:03,120
making the case for publishing

184
00:07:03,120 --> 00:07:05,039
weaknesses in lock design

185
00:07:05,039 --> 00:07:07,120
he has a book from 1853 called

186
00:07:07,120 --> 00:07:09,360
construction of locks and safes and he

187
00:07:09,360 --> 00:07:11,919
wrote quote the spread of the knowledge

188
00:07:11,919 --> 00:07:14,400
is necessary to give fair play to those

189
00:07:14,400 --> 00:07:17,280
who might suffer by ignorance

190
00:07:17,280 --> 00:07:19,759
it was open participation because anyone

191
00:07:19,759 --> 00:07:21,520
was able to participate

192
00:07:21,520 --> 00:07:24,000
the duration was ongoing

193
00:07:24,000 --> 00:07:27,280
really ongoing 61 years ongoing

194
00:07:27,280 --> 00:07:30,080
and finally regarding scope and access

195
00:07:30,080 --> 00:07:31,759
this challenge focused on picking the

196
00:07:31,759 --> 00:07:32,639
lock

197
00:07:32,639 --> 00:07:34,639
so there was physical access and the

198
00:07:34,639 --> 00:07:36,080
details of how the lock worked were

199
00:07:36,080 --> 00:07:37,680
public

200
00:07:37,680 --> 00:07:39,360
all right so we learned that bug

201
00:07:39,360 --> 00:07:41,599
bounties are really not a new concept

202
00:07:41,599 --> 00:07:43,440
and that they can be applied to various

203
00:07:43,440 --> 00:07:44,800
systems

204
00:07:44,800 --> 00:07:46,639
and you know what happens next bug

205
00:07:46,639 --> 00:07:49,360
bounties come to infosec

206
00:07:49,360 --> 00:07:50,639
and we cover

207
00:07:50,639 --> 00:07:52,800
some of that historical trajectory of

208
00:07:52,800 --> 00:07:55,440
how early info segment bounties um

209
00:07:55,440 --> 00:07:57,039
really evolved we talked about this in

210
00:07:57,039 --> 00:07:59,280
our report but it has been well covered

211
00:07:59,280 --> 00:08:01,759
by others including here at enigma and

212
00:08:01,759 --> 00:08:02,879
particularly

213
00:08:02,879 --> 00:08:04,960
at the moment where our report came out

214
00:08:04,960 --> 00:08:07,120
two other reports by data and society

215
00:08:07,120 --> 00:08:09,520
researchers came out at the same time

216
00:08:09,520 --> 00:08:12,720
really exploring that history of hacking

217
00:08:12,720 --> 00:08:14,800
and in general

218
00:08:14,800 --> 00:08:17,759
the history of how big bounties came to

219
00:08:17,759 --> 00:08:20,560
their current form in infosec and the

220
00:08:20,560 --> 00:08:22,319
impact that this has specifically on the

221
00:08:22,319 --> 00:08:24,720
labor market so we really encourage you

222
00:08:24,720 --> 00:08:26,720
to check out those two recent works they

223
00:08:26,720 --> 00:08:28,960
are just there by our data and society

224
00:08:28,960 --> 00:08:31,120
colleagues and we want to fast forward

225
00:08:31,120 --> 00:08:35,120
you to the early 2010.

226
00:08:35,120 --> 00:08:37,440
at this point we're starting to see an

227
00:08:37,440 --> 00:08:40,240
already widespread use of bug bounty

228
00:08:40,240 --> 00:08:42,958
programs often in combination with other

229
00:08:42,958 --> 00:08:44,880
vulnerability disclosure programs and

230
00:08:44,880 --> 00:08:47,360
with pen testing and it's also a moment

231
00:08:47,360 --> 00:08:49,600
where we really see the rise of major

232
00:08:49,600 --> 00:08:51,839
book bounding platforms like hacker one

233
00:08:51,839 --> 00:08:54,240
background yes we hack and of course

234
00:08:54,240 --> 00:08:57,279
this is after the first uh big big

235
00:08:57,279 --> 00:08:59,519
bounty programs for the us government

236
00:08:59,519 --> 00:09:01,200
hack the pentagon

237
00:09:01,200 --> 00:09:02,880
and so at this point even some of the

238
00:09:02,880 --> 00:09:05,680
largest players in tech use uh bug

239
00:09:05,680 --> 00:09:07,680
bounties and use bug bounty platforms to

240
00:09:07,680 --> 00:09:10,480
solicit entry as reports and for hackers

241
00:09:10,480 --> 00:09:12,800
those platforms offer a more consistent

242
00:09:12,800 --> 00:09:14,959
user experience access to many programs

243
00:09:14,959 --> 00:09:17,120
in one place a repository of past

244
00:09:17,120 --> 00:09:19,760
reports to learn from and sometimes also

245
00:09:19,760 --> 00:09:21,360
a genuine community

246
00:09:21,360 --> 00:09:22,800
however

247
00:09:22,800 --> 00:09:25,920
even in those early heady days of the

248
00:09:25,920 --> 00:09:27,920
bounty everything hype people really

249
00:09:27,920 --> 00:09:29,600
caution that bounties will not work

250
00:09:29,600 --> 00:09:32,000
unless the organization offering them

251
00:09:32,000 --> 00:09:33,839
are deeply committed to secure

252
00:09:33,839 --> 00:09:36,160
development practices throughout the

253
00:09:36,160 --> 00:09:38,640
entire product life cycle and that's

254
00:09:38,640 --> 00:09:40,320
really a concept that we think

255
00:09:40,320 --> 00:09:42,399
translates well to the space of

256
00:09:42,399 --> 00:09:44,880
algorithmic harms which is this focus on

257
00:09:44,880 --> 00:09:47,279
not just one moment in the development

258
00:09:47,279 --> 00:09:50,560
but really on the full life cycle in

259
00:09:50,560 --> 00:09:52,720
short bounties have never been silver

260
00:09:52,720 --> 00:09:53,920
bullets

261
00:09:53,920 --> 00:09:55,760
so now let's fast forward again boom

262
00:09:55,760 --> 00:09:57,760
we're in 2018

263
00:09:57,760 --> 00:10:00,240
and this is when the cambridge analytica

264
00:10:00,240 --> 00:10:02,880
controversy makes headlines

265
00:10:02,880 --> 00:10:05,120
less than a month later facebook and

266
00:10:05,120 --> 00:10:07,360
google shortly after that announced book

267
00:10:07,360 --> 00:10:10,240
bounties for data and api abuse now

268
00:10:10,240 --> 00:10:11,760
that's similar to traditional bug

269
00:10:11,760 --> 00:10:14,399
bounties but it's also really different

270
00:10:14,399 --> 00:10:16,720
from a program perspective they have to

271
00:10:16,720 --> 00:10:19,279
be configured very differently to tackle

272
00:10:19,279 --> 00:10:21,839
this new type of issues this highlights

273
00:10:21,839 --> 00:10:24,560
the pr value of black bounties as band

274
00:10:24,560 --> 00:10:26,640
aids in a crisis

275
00:10:26,640 --> 00:10:30,000
also in october 2018 there's at least

276
00:10:30,000 --> 00:10:32,320
one other organization that stretches

277
00:10:32,320 --> 00:10:34,720
the bound of traditional bounties to

278
00:10:34,720 --> 00:10:37,120
surface algorithmic harms and that's

279
00:10:37,120 --> 00:10:38,720
rockstar game

280
00:10:38,720 --> 00:10:42,160
in response to claims of false positive

281
00:10:42,160 --> 00:10:44,320
band punishments from gamers who are

282
00:10:44,320 --> 00:10:46,880
facing bans from rockstar's cheat

283
00:10:46,880 --> 00:10:48,720
flagging algorithms

284
00:10:48,720 --> 00:10:51,040
the company sets up an add-on to its

285
00:10:51,040 --> 00:10:53,600
security bounty promising a ten thousand

286
00:10:53,600 --> 00:10:55,360
dollar reward for anyone who can

287
00:10:55,360 --> 00:10:58,399
successfully identify a reproducible

288
00:10:58,399 --> 00:11:01,519
incorrect ban either in grand theft auto

289
00:11:01,519 --> 00:11:03,279
or in red dead

290
00:11:03,279 --> 00:11:05,279
so what do we learn from this expansion

291
00:11:05,279 --> 00:11:08,240
of bug bounty programs to datum api and

292
00:11:08,240 --> 00:11:10,240
chief flagging algorithms that happens

293
00:11:10,240 --> 00:11:12,320
around 2018

294
00:11:12,320 --> 00:11:14,000
first we learned that bug bounty

295
00:11:14,000 --> 00:11:15,200
programs

296
00:11:15,200 --> 00:11:17,040
clearly can be applied to

297
00:11:17,040 --> 00:11:19,360
socio-technical challenges beyond

298
00:11:19,360 --> 00:11:21,519
traditional security vulnerabilities

299
00:11:21,519 --> 00:11:23,279
that's a trend that we document and that

300
00:11:23,279 --> 00:11:25,120
we think will accelerate

301
00:11:25,120 --> 00:11:26,560
second we learned that there are lessons

302
00:11:26,560 --> 00:11:28,399
from cyber security bug bounties that

303
00:11:28,399 --> 00:11:30,640
really apply into new domains we also

304
00:11:30,640 --> 00:11:32,000
think that there are lessons that need

305
00:11:32,000 --> 00:11:34,560
to be unlearned but here we see that

306
00:11:34,560 --> 00:11:37,440
google and facebook brought their legal

307
00:11:37,440 --> 00:11:39,519
safe harbor over from their security

308
00:11:39,519 --> 00:11:42,320
bounties to come into these new programs

309
00:11:42,320 --> 00:11:45,279
on data abuse these safe harbor clauses

310
00:11:45,279 --> 00:11:47,519
can help reassure researchers that they

311
00:11:47,519 --> 00:11:49,440
won't be sued by the targets for

312
00:11:49,440 --> 00:11:51,920
potential copyright drm infringement

313
00:11:51,920 --> 00:11:53,360
computer fraud and abuse after

314
00:11:53,360 --> 00:11:55,440
defamations etc

315
00:11:55,440 --> 00:11:57,120
we also learned that companies will set

316
00:11:57,120 --> 00:11:59,200
up bug bounties for algorithmic harms if

317
00:11:59,200 --> 00:12:01,600
it makes business sense to do so like

318
00:12:01,600 --> 00:12:04,240
address pr or customer concerns and if

319
00:12:04,240 --> 00:12:06,720
it feels safe for them to do so

320
00:12:06,720 --> 00:12:11,600
now let's take on to our next vignette

321
00:12:11,600 --> 00:12:13,040
okay so now we're going to talk about

322
00:12:13,040 --> 00:12:15,600
twitter's bias bounty so recently for

323
00:12:15,600 --> 00:12:19,040
defcon 2021 twitter announced a one-week

324
00:12:19,040 --> 00:12:21,600
algorithmic bias bounty challenge

325
00:12:21,600 --> 00:12:23,440
the program was created by twitter's

326
00:12:23,440 --> 00:12:26,079
machine learning ethics transparency and

327
00:12:26,079 --> 00:12:28,560
accountability or meta team in

328
00:12:28,560 --> 00:12:30,720
partnership with hackerone and yes they

329
00:12:30,720 --> 00:12:34,480
had the name meta before meta was meta

330
00:12:34,480 --> 00:12:37,040
anyway twitter's bias bounty focused on

331
00:12:37,040 --> 00:12:39,440
an image cropping algorithm that users

332
00:12:39,440 --> 00:12:42,160
felt was biased in ways that reinforce

333
00:12:42,160 --> 00:12:46,000
racism and sexism in 2020 twitter users

334
00:12:46,000 --> 00:12:49,040
had performed a participatory audit

335
00:12:49,040 --> 00:12:51,680
they shared screenshots of image crop

336
00:12:51,680 --> 00:12:54,480
fails on social media and we have a

337
00:12:54,480 --> 00:12:56,720
slide that sort of demonstrates that

338
00:12:56,720 --> 00:12:58,959
in-house researchers from twitter later

339
00:12:58,959 --> 00:13:00,800
published research confirming these

340
00:13:00,800 --> 00:13:02,880
users findings

341
00:13:02,880 --> 00:13:04,639
through the defcon challenge twitter

342
00:13:04,639 --> 00:13:06,320
offered an opportunity for third-party

343
00:13:06,320 --> 00:13:08,720
researchers to scrutinize their model

344
00:13:08,720 --> 00:13:10,160
with bounties for the top three

345
00:13:10,160 --> 00:13:11,600
submissions

346
00:13:11,600 --> 00:13:13,680
the company also produced a scoring

347
00:13:13,680 --> 00:13:17,519
rubric for algorithmic bias and harms

348
00:13:17,519 --> 00:13:19,600
our team was thrilled to see all this

349
00:13:19,600 --> 00:13:21,279
happen and we think they did a lot of

350
00:13:21,279 --> 00:13:22,720
things right

351
00:13:22,720 --> 00:13:24,720
we also think that this case study

352
00:13:24,720 --> 00:13:26,720
illustrates the difficulty of applying

353
00:13:26,720 --> 00:13:29,519
bug bounties to algorithmic harms

354
00:13:29,519 --> 00:13:30,720
for example

355
00:13:30,720 --> 00:13:32,880
their scoring rubric gave more points

356
00:13:32,880 --> 00:13:34,720
for problems that affected the most

357
00:13:34,720 --> 00:13:35,680
people

358
00:13:35,680 --> 00:13:38,560
even though that implies de-prioritizing

359
00:13:38,560 --> 00:13:41,120
small groups of people who might be at

360
00:13:41,120 --> 00:13:43,680
most risk of suffering the worst kinds

361
00:13:43,680 --> 00:13:45,519
of algorithmic harm

362
00:13:45,519 --> 00:13:48,240
so as a trans person for example any

363
00:13:48,240 --> 00:13:50,480
scoring rubric that says you get the

364
00:13:50,480 --> 00:13:51,760
most points if it applies to the most

365
00:13:51,760 --> 00:13:53,120
people in the world well if you're part

366
00:13:53,120 --> 00:13:54,880
of a tiny population

367
00:13:54,880 --> 00:13:56,560
then you're going to feel excluded like

368
00:13:56,560 --> 00:13:58,240
this isn't set up to support your

369
00:13:58,240 --> 00:13:59,279
community

370
00:13:59,279 --> 00:14:01,199
now twitter didn't provide any scores

371
00:14:01,199 --> 00:14:03,360
publicly so it's hard to assess how

372
00:14:03,360 --> 00:14:06,320
useful the rubric was in practice

373
00:14:06,320 --> 00:14:08,000
in addition while it's great to see

374
00:14:08,000 --> 00:14:09,600
these kinds of programs emerging in

375
00:14:09,600 --> 00:14:11,760
response to controversies in all the

376
00:14:11,760 --> 00:14:14,399
cases we looked at google facebook and

377
00:14:14,399 --> 00:14:17,199
twitter the original reporters of the

378
00:14:17,199 --> 00:14:19,040
issues the ones who first put in the

379
00:14:19,040 --> 00:14:21,920
work to document and expose the harms

380
00:14:21,920 --> 00:14:25,040
were not rewarded by the programs

381
00:14:25,040 --> 00:14:26,800
on the other hand there's also an

382
00:14:26,800 --> 00:14:29,120
important change management lesson here

383
00:14:29,120 --> 00:14:30,800
because we think that twitter saw an

384
00:14:30,800 --> 00:14:32,959
opportunity where stars were aligned in

385
00:14:32,959 --> 00:14:35,360
favor of doing something novel

386
00:14:35,360 --> 00:14:36,959
in our interviews we heard again and

387
00:14:36,959 --> 00:14:38,800
again the importance of finding the

388
00:14:38,800 --> 00:14:41,760
right pilot program so that was the case

389
00:14:41,760 --> 00:14:43,360
for example with the hack the pentagon

390
00:14:43,360 --> 00:14:44,480
program

391
00:14:44,480 --> 00:14:46,639
you do a pilot first so that then you

392
00:14:46,639 --> 00:14:50,000
can expand bounties um you know across

393
00:14:50,000 --> 00:14:51,360
the organization

394
00:14:51,360 --> 00:14:53,680
so we think that several factors were

395
00:14:53,680 --> 00:14:56,480
key to minimizing the risk to twitter

396
00:14:56,480 --> 00:14:59,199
for their for their um their program

397
00:14:59,199 --> 00:15:01,279
first of all harms from the image

398
00:15:01,279 --> 00:15:02,959
cropping algorithm had already been

399
00:15:02,959 --> 00:15:05,680
exposed by users so the reputational

400
00:15:05,680 --> 00:15:08,480
damage had already been incurred

401
00:15:08,480 --> 00:15:10,720
second twitter had already published an

402
00:15:10,720 --> 00:15:13,120
examination of the system's flaws and

403
00:15:13,120 --> 00:15:14,959
was already planning to decommission the

404
00:15:14,959 --> 00:15:17,839
algorithm so that mitigated further risk

405
00:15:17,839 --> 00:15:20,000
of public criticism

406
00:15:20,000 --> 00:15:23,279
and third the crop algorithm was open

407
00:15:23,279 --> 00:15:26,320
source rather than proprietary so even

408
00:15:26,320 --> 00:15:27,760
when they opened it up they weren't

409
00:15:27,760 --> 00:15:30,959
exposing any ip

410
00:15:30,959 --> 00:15:32,880
all right now let's pivot to who else

411
00:15:32,880 --> 00:15:34,639
could use big bounties for algorithmic

412
00:15:34,639 --> 00:15:37,839
harms in our last vignette

413
00:15:37,839 --> 00:15:39,759
during the pandemic many schools and

414
00:15:39,759 --> 00:15:41,759
many universities rapidly switch to

415
00:15:41,759 --> 00:15:43,759
remote learning and a lot of them

416
00:15:43,759 --> 00:15:45,839
started using e-proctoring systems that

417
00:15:45,839 --> 00:15:47,839
monitor students remotely

418
00:15:47,839 --> 00:15:49,279
there are a lot of problems with these

419
00:15:49,279 --> 00:15:51,680
systems for instance as we know a lot of

420
00:15:51,680 --> 00:15:53,440
these systems use facial recognition

421
00:15:53,440 --> 00:15:56,000
technologies that perform less well on

422
00:15:56,000 --> 00:15:58,000
students with darker skin

423
00:15:58,000 --> 00:16:00,880
at least one researcher auxiliary on

424
00:16:00,880 --> 00:16:03,199
their blog proctor ninja reversed

425
00:16:03,199 --> 00:16:05,120
engineered the widely used remote

426
00:16:05,120 --> 00:16:06,839
proctoring system

427
00:16:06,839 --> 00:16:09,519
proctor.io and found that it was using a

428
00:16:09,519 --> 00:16:11,839
facial recognition training library that

429
00:16:11,839 --> 00:16:13,759
is not meant for production environments

430
00:16:13,759 --> 00:16:15,680
and that is known to perform poorly on

431
00:16:15,680 --> 00:16:17,600
darker skin

432
00:16:17,600 --> 00:16:19,519
people subject to these technologies

433
00:16:19,519 --> 00:16:21,600
have been speaking up like the students

434
00:16:21,600 --> 00:16:24,160
activists at encode justice

435
00:16:24,160 --> 00:16:26,480
and if we take a step back

436
00:16:26,480 --> 00:16:28,560
another key trend that we observed

437
00:16:28,560 --> 00:16:30,480
throughout the history of big bounties

438
00:16:30,480 --> 00:16:32,639
is that the few attempts at truly

439
00:16:32,639 --> 00:16:35,680
adversarial programs did not last very

440
00:16:35,680 --> 00:16:37,839
long or succeed widely

441
00:16:37,839 --> 00:16:40,639
there are a few notable exceptions for

442
00:16:40,639 --> 00:16:43,199
programs that live within large and

443
00:16:43,199 --> 00:16:45,600
well-funded corporations like project

444
00:16:45,600 --> 00:16:48,560
zero at google but besides these

445
00:16:48,560 --> 00:16:51,360
adversarial bounties have not really

446
00:16:51,360 --> 00:16:54,160
found their final form

447
00:16:54,160 --> 00:16:56,079
we think that here there is a great

448
00:16:56,079 --> 00:16:58,959
opportunity to more meaningfully tackle

449
00:16:58,959 --> 00:17:01,120
algorithmic harms and our proposal is

450
00:17:01,120 --> 00:17:02,720
simple

451
00:17:02,720 --> 00:17:05,039
adversarial bounties for algorithmic

452
00:17:05,039 --> 00:17:06,559
harms

453
00:17:06,559 --> 00:17:09,199
so this slide shows the way that we

454
00:17:09,199 --> 00:17:10,559
might configure

455
00:17:10,559 --> 00:17:12,559
that type of adversarial bias and harm

456
00:17:12,559 --> 00:17:13,439
boundary

457
00:17:13,439 --> 00:17:14,880
the reporting

458
00:17:14,880 --> 00:17:16,959
would be adversarial

459
00:17:16,959 --> 00:17:18,400
the compensation

460
00:17:18,400 --> 00:17:20,319
would be a bounty

461
00:17:20,319 --> 00:17:22,400
disclosure would be delayed but full

462
00:17:22,400 --> 00:17:23,679
disclosure

463
00:17:23,679 --> 00:17:26,559
participation would be public

464
00:17:26,559 --> 00:17:29,600
program management would be third-party

465
00:17:29,600 --> 00:17:31,840
platforms so not a traditional platform

466
00:17:31,840 --> 00:17:33,840
where the target is paying a platform to

467
00:17:33,840 --> 00:17:36,880
host but a third party is doing so

468
00:17:36,880 --> 00:17:39,360
the duration would be time limited

469
00:17:39,360 --> 00:17:40,960
and the scope and access would be

470
00:17:40,960 --> 00:17:44,400
expansive and closed box

471
00:17:44,400 --> 00:17:45,360
so

472
00:17:45,360 --> 00:17:47,600
if this sounds interesting or exciting

473
00:17:47,600 --> 00:17:49,760
to you and you want to participate or

474
00:17:49,760 --> 00:17:51,440
you want to debate the details with us

475
00:17:51,440 --> 00:17:52,400
and you think the model should be

476
00:17:52,400 --> 00:17:53,600
tweaked a little bit

477
00:17:53,600 --> 00:17:55,120
get in touch with us

478
00:17:55,120 --> 00:17:57,600
sign up for ajl's mailing list

479
00:17:57,600 --> 00:18:00,160
and if you have ideas for targets of

480
00:18:00,160 --> 00:18:03,360
adversarial algorithmic harm bbps let us

481
00:18:03,360 --> 00:18:04,559
know

482
00:18:04,559 --> 00:18:06,320
and if you decide to run your own

483
00:18:06,320 --> 00:18:09,280
adversarial algorithmic harm bvp we'd

484
00:18:09,280 --> 00:18:10,960
love to hear about it

485
00:18:10,960 --> 00:18:14,200
so thank you so much and please do visit

486
00:18:14,200 --> 00:18:17,840
ajl.org bugs to learn more and to

487
00:18:17,840 --> 00:18:19,679
download the full report which is just

488
00:18:19,679 --> 00:18:22,799
full of lots of great information

489
00:18:22,799 --> 00:18:26,200
thanks everybody

490
00:18:33,600 --> 00:18:35,678
you

