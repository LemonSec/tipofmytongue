1
00:00:07,440 --> 00:00:08,960
thank you everybody again my name is

2
00:00:08,960 --> 00:00:10,719
kurt thomas i'm a research scientist at

3
00:00:10,719 --> 00:00:12,320
google and today i really want to talk

4
00:00:12,320 --> 00:00:14,799
about reasoning what we mean by security

5
00:00:14,799 --> 00:00:16,560
when it comes to online hate and

6
00:00:16,560 --> 00:00:18,080
harassment

7
00:00:18,080 --> 00:00:19,359
and the reason i'm interested in this

8
00:00:19,359 --> 00:00:21,680
research area is that emerging threats

9
00:00:21,680 --> 00:00:23,519
like hate and harassment really are

10
00:00:23,519 --> 00:00:25,119
transforming the day-to-day abuse

11
00:00:25,119 --> 00:00:27,039
experiences that people face on the

12
00:00:27,039 --> 00:00:29,199
internet before i would talk to you

13
00:00:29,199 --> 00:00:31,519
about spam phishing and malware but now

14
00:00:31,519 --> 00:00:33,280
there's a rash of new types of attacks

15
00:00:33,280 --> 00:00:35,200
that are occurring

16
00:00:35,200 --> 00:00:36,399
these are threats like the

17
00:00:36,399 --> 00:00:38,239
non-consensual leaking of intimate

18
00:00:38,239 --> 00:00:41,600
images say by a intimate partner or

19
00:00:41,600 --> 00:00:43,920
another person that's anonymous online

20
00:00:43,920 --> 00:00:45,680
this happened to former representative

21
00:00:45,680 --> 00:00:47,440
katie hill who

22
00:00:47,440 --> 00:00:49,360
previously was in congress and then had

23
00:00:49,360 --> 00:00:51,520
intimate images leaked allegedly by a

24
00:00:51,520 --> 00:00:53,520
former partner that led to her stepping

25
00:00:53,520 --> 00:00:55,680
down from her role and ultimately severe

26
00:00:55,680 --> 00:00:57,120
mental health and well-being

27
00:00:57,120 --> 00:00:59,520
consequences for her

28
00:00:59,520 --> 00:01:01,199
there are also attacks like coordinated

29
00:01:01,199 --> 00:01:03,440
campaigns of toxic comments on social

30
00:01:03,440 --> 00:01:06,479
media that attempt to silence voices for

31
00:01:06,479 --> 00:01:08,159
leslie jones when she released

32
00:01:08,159 --> 00:01:10,799
ghostbusters there's a wave of racists

33
00:01:10,799 --> 00:01:12,479
and other messages that occurred against

34
00:01:12,479 --> 00:01:14,799
her on social media that ultimately led

35
00:01:14,799 --> 00:01:17,119
her stepping back from the platforms and

36
00:01:17,119 --> 00:01:19,920
participating online

37
00:01:19,920 --> 00:01:21,840
there's also cases of people being

38
00:01:21,840 --> 00:01:24,080
falsely reported to authorities or

39
00:01:24,080 --> 00:01:26,000
platforms to either take action against

40
00:01:26,000 --> 00:01:28,320
their person or their accounts

41
00:01:28,320 --> 00:01:30,240
for nate hill while he was live

42
00:01:30,240 --> 00:01:32,240
streaming on twitch he had the swat team

43
00:01:32,240 --> 00:01:34,320
called to his home and cut off his

44
00:01:34,320 --> 00:01:36,479
stream while that was happening but it

45
00:01:36,479 --> 00:01:38,240
doesn't have to be a swat team it can be

46
00:01:38,240 --> 00:01:40,000
a copyright claim made against your

47
00:01:40,000 --> 00:01:42,000
content or a false claim that you're

48
00:01:42,000 --> 00:01:44,320
abusing a platform to try to remove your

49
00:01:44,320 --> 00:01:46,960
voice from online

50
00:01:46,960 --> 00:01:48,720
and there's also cases of surveilling

51
00:01:48,720 --> 00:01:50,799
intimate partners through their devices

52
00:01:50,799 --> 00:01:52,720
or through their accounts for many

53
00:01:52,720 --> 00:01:54,240
people this could be in the form of

54
00:01:54,240 --> 00:01:56,479
keystroke monitoring on their phones it

55
00:01:56,479 --> 00:01:58,719
could be hidden cameras could be gps

56
00:01:58,719 --> 00:02:00,719
tracking basically weaponizing

57
00:02:00,719 --> 00:02:02,640
technology in an intimate partner

58
00:02:02,640 --> 00:02:04,880
violence setting

59
00:02:04,880 --> 00:02:06,560
and the common thread that connects all

60
00:02:06,560 --> 00:02:08,800
these different threats together is that

61
00:02:08,800 --> 00:02:10,639
there's an intent to inflict emotional

62
00:02:10,639 --> 00:02:13,040
harm including coercive control or

63
00:02:13,040 --> 00:02:15,440
instilling a fear of sexual or physical

64
00:02:15,440 --> 00:02:17,200
violence

65
00:02:17,200 --> 00:02:18,640
and i think it's important to note that

66
00:02:18,640 --> 00:02:20,640
it's not just high-profile targets that

67
00:02:20,640 --> 00:02:22,959
are being affected by these attacks

68
00:02:22,959 --> 00:02:25,440
in 2017 pew did a survey and they found

69
00:02:25,440 --> 00:02:27,360
that 41 percent of people in the united

70
00:02:27,360 --> 00:02:29,760
states have personally experienced

71
00:02:29,760 --> 00:02:31,760
online hate and harassment

72
00:02:31,760 --> 00:02:33,519
and the same is true for about 40

73
00:02:33,519 --> 00:02:37,200
percent of people globally

74
00:02:37,200 --> 00:02:38,879
and this hasn't escaped the attention of

75
00:02:38,879 --> 00:02:40,800
many internet users if you ask people

76
00:02:40,800 --> 00:02:42,160
what are their top concerns of

77
00:02:42,160 --> 00:02:44,160
participating online what they feel

78
00:02:44,160 --> 00:02:46,480
technology or policy or law enforcement

79
00:02:46,480 --> 00:02:47,840
should be addressing

80
00:02:47,840 --> 00:02:49,440
yes identity theft and stolen

81
00:02:49,440 --> 00:02:51,360
information still ranks very highly

82
00:02:51,360 --> 00:02:53,760
amongst their concerns but harassment

83
00:02:53,760 --> 00:02:55,440
and bullying is up there in the top

84
00:02:55,440 --> 00:02:57,840
three of what they care about and

85
00:02:57,840 --> 00:02:59,519
there's increasing energy from these

86
00:02:59,519 --> 00:03:01,440
users to say that platforms and other

87
00:03:01,440 --> 00:03:02,959
individuals should be stepping in to

88
00:03:02,959 --> 00:03:04,800
help them

89
00:03:04,800 --> 00:03:06,400
the problem is that a lot of the

90
00:03:06,400 --> 00:03:08,239
existing protections that we have in

91
00:03:08,239 --> 00:03:10,800
this context really focus on for-profit

92
00:03:10,800 --> 00:03:13,360
cyber crime we've made immense strides

93
00:03:13,360 --> 00:03:14,080
in

94
00:03:14,080 --> 00:03:16,239
warning people about spam phishing and

95
00:03:16,239 --> 00:03:18,159
malware and getting them not to go to

96
00:03:18,159 --> 00:03:19,760
dangerous websites

97
00:03:19,760 --> 00:03:21,599
we've warned them against data breaches

98
00:03:21,599 --> 00:03:23,519
and password reuse and behaviors that

99
00:03:23,519 --> 00:03:24,720
put them at risk of having their

100
00:03:24,720 --> 00:03:27,519
accounts taken over and of course bots

101
00:03:27,519 --> 00:03:29,120
and automation

102
00:03:29,120 --> 00:03:31,680
but none of these map onto the security

103
00:03:31,680 --> 00:03:33,360
and privacy needs in a hate and

104
00:03:33,360 --> 00:03:35,360
harassment context

105
00:03:35,360 --> 00:03:37,280
and so really the thesis of this talk

106
00:03:37,280 --> 00:03:38,799
today and what i hope to motivate you

107
00:03:38,799 --> 00:03:41,280
about is that we need to expand our

108
00:03:41,280 --> 00:03:43,200
security threat models to address these

109
00:03:43,200 --> 00:03:45,360
attacks that lack the same scale or

110
00:03:45,360 --> 00:03:47,360
profit incentives of the way we

111
00:03:47,360 --> 00:03:48,959
currently think about security and

112
00:03:48,959 --> 00:03:51,280
privacy

113
00:03:51,280 --> 00:03:53,040
to that end one of the things that our

114
00:03:53,040 --> 00:03:54,480
research team has been working on is

115
00:03:54,480 --> 00:03:56,159
really building a framework to reason

116
00:03:56,159 --> 00:03:58,159
about what are hate and harassment

117
00:03:58,159 --> 00:04:00,480
attacks right now who are the attackers

118
00:04:00,480 --> 00:04:02,640
and what are the harms that result from

119
00:04:02,640 --> 00:04:04,319
it

120
00:04:04,319 --> 00:04:06,480
as part of this we teamed up with six

121
00:04:06,480 --> 00:04:08,560
different academic institutions across

122
00:04:08,560 --> 00:04:10,879
the united states as well as europe to

123
00:04:10,879 --> 00:04:13,599
do this meta-analysis of about 150

124
00:04:13,599 --> 00:04:15,920
articles that commented on here's an

125
00:04:15,920 --> 00:04:18,079
attack that happened to me or here's a

126
00:04:18,079 --> 00:04:20,238
deep dive into different things that are

127
00:04:20,238 --> 00:04:23,840
going online or across the internet

128
00:04:23,840 --> 00:04:24,960
and what we found is that there's a

129
00:04:24,960 --> 00:04:26,639
variety of types of attackers when it

130
00:04:26,639 --> 00:04:28,400
comes to hate and harassment

131
00:04:28,400 --> 00:04:29,919
it could be a spouse it could be a

132
00:04:29,919 --> 00:04:31,919
family member it could be a peer which

133
00:04:31,919 --> 00:04:34,080
fundamentally changes our assumptions

134
00:04:34,080 --> 00:04:36,080
about security these are people who have

135
00:04:36,080 --> 00:04:38,160
access to your device access to your

136
00:04:38,160 --> 00:04:40,080
account and even access to your physical

137
00:04:40,080 --> 00:04:41,120
person

138
00:04:41,120 --> 00:04:42,560
of course they're anonymous internet

139
00:04:42,560 --> 00:04:44,880
users but there's also attackers who are

140
00:04:44,880 --> 00:04:47,120
public personalities or even coordinated

141
00:04:47,120 --> 00:04:49,040
mobs where there's a huge power

142
00:04:49,040 --> 00:04:51,680
differential between the attackers and

143
00:04:51,680 --> 00:04:53,600
their capabilities and the individual

144
00:04:53,600 --> 00:04:57,360
that's being targeted for attack

145
00:04:57,360 --> 00:04:58,960
we're also seeing a variety of different

146
00:04:58,960 --> 00:05:00,800
types of harms that are different from

147
00:05:00,800 --> 00:05:02,639
what we think about in traditional say

148
00:05:02,639 --> 00:05:05,120
for-profit cyber crime the goal isn't to

149
00:05:05,120 --> 00:05:06,880
make money off an individual the goal

150
00:05:06,880 --> 00:05:08,960
here is to silence their voice it's to

151
00:05:08,960 --> 00:05:11,520
damage their reputation it's to reduce

152
00:05:11,520 --> 00:05:14,000
their sexual or physical safety or even

153
00:05:14,000 --> 00:05:16,080
reduce their financial security or

154
00:05:16,080 --> 00:05:19,120
ability to kind of operate independently

155
00:05:19,120 --> 00:05:21,039
and the list of harms goes on but these

156
00:05:21,039 --> 00:05:22,560
very different from the types of things

157
00:05:22,560 --> 00:05:24,560
we normally talk about when we say a

158
00:05:24,560 --> 00:05:28,240
computer system being broken into

159
00:05:28,240 --> 00:05:30,160
what we found is that in analyzing all

160
00:05:30,160 --> 00:05:31,520
these different attacks that people have

161
00:05:31,520 --> 00:05:33,759
been experiencing there's roughly seven

162
00:05:33,759 --> 00:05:35,360
categories and i'm not going to go into

163
00:05:35,360 --> 00:05:37,440
all of them today but we came up with as

164
00:05:37,440 --> 00:05:38,880
a framework to reason about what

165
00:05:38,880 --> 00:05:41,280
differentiates them so let's take toxic

166
00:05:41,280 --> 00:05:42,960
content and example

167
00:05:42,960 --> 00:05:45,039
this includes things like bullying

168
00:05:45,039 --> 00:05:48,560
trolling hate speech threats of violence

169
00:05:48,560 --> 00:05:50,400
sexual harassment

170
00:05:50,400 --> 00:05:52,479
and what connects all these together is

171
00:05:52,479 --> 00:05:55,280
that there's an intent to be seen by the

172
00:05:55,280 --> 00:05:57,199
individual being targeted as well as

173
00:05:57,199 --> 00:05:58,880
often intend to be seen by a wider

174
00:05:58,880 --> 00:06:00,960
audience and all it requires is a

175
00:06:00,960 --> 00:06:03,440
communication channel a messaging app a

176
00:06:03,440 --> 00:06:05,520
social media connection in which to

177
00:06:05,520 --> 00:06:07,600
interact with that individual

178
00:06:07,600 --> 00:06:08,960
and ultimately the intent here is to

179
00:06:08,960 --> 00:06:11,360
silence them or damage their reputation

180
00:06:11,360 --> 00:06:13,120
in most cases

181
00:06:13,120 --> 00:06:14,400
contrast that with something like

182
00:06:14,400 --> 00:06:15,840
surveillance where we're talking about

183
00:06:15,840 --> 00:06:17,120
somebody who's stalking you or

184
00:06:17,120 --> 00:06:19,440
geo-tracking your activities or

185
00:06:19,440 --> 00:06:21,520
monitoring your accounts

186
00:06:21,520 --> 00:06:22,800
it's fundamentally different in the

187
00:06:22,800 --> 00:06:24,880
sense that they don't want to be seen

188
00:06:24,880 --> 00:06:26,479
there's no audience involved here

189
00:06:26,479 --> 00:06:28,160
there's no communication channel that's

190
00:06:28,160 --> 00:06:30,319
necessary for this fundamentally what

191
00:06:30,319 --> 00:06:32,080
underpins all these attacks is a

192
00:06:32,080 --> 00:06:33,680
requirement for

193
00:06:33,680 --> 00:06:36,000
access to privileged information about

194
00:06:36,000 --> 00:06:38,560
the user or the devices

195
00:06:38,560 --> 00:06:40,000
and again the intent for harm is

196
00:06:40,000 --> 00:06:41,520
different here it's usually about

197
00:06:41,520 --> 00:06:43,919
coercion reducing sexual or physical

198
00:06:43,919 --> 00:06:46,160
safety rather than necessarily trying to

199
00:06:46,160 --> 00:06:48,479
silence them online and there's a

200
00:06:48,479 --> 00:06:49,759
variety of other texts that we go over

201
00:06:49,759 --> 00:06:50,960
in our framework

202
00:06:50,960 --> 00:06:52,639
it is available online and i'll link it

203
00:06:52,639 --> 00:06:55,199
to it in the end of the talk

204
00:06:55,199 --> 00:06:56,720
the other interesting thing in kind of

205
00:06:56,720 --> 00:06:58,160
analyzing all these different hate and

206
00:06:58,160 --> 00:06:59,680
harassment scenarios is that there's

207
00:06:59,680 --> 00:07:02,000
actually many parallels that we can draw

208
00:07:02,000 --> 00:07:04,160
to existing attacks that many of us deal

209
00:07:04,160 --> 00:07:05,520
with in this room

210
00:07:05,520 --> 00:07:07,440
for toxic content it's very similar to

211
00:07:07,440 --> 00:07:09,759
spam you know thousands of messages

212
00:07:09,759 --> 00:07:11,120
being sent to you that you don't want to

213
00:07:11,120 --> 00:07:12,479
receive whether it's about

214
00:07:12,479 --> 00:07:14,800
pharmaceutical products or about how you

215
00:07:14,800 --> 00:07:16,319
shouldn't belong on the internet you

216
00:07:16,319 --> 00:07:18,560
know is a vague differential

217
00:07:18,560 --> 00:07:20,639
also for things like impersonation it's

218
00:07:20,639 --> 00:07:22,240
very similar to the threats we face with

219
00:07:22,240 --> 00:07:24,880
phishing and representation of what an

220
00:07:24,880 --> 00:07:27,120
identity looks like online or for

221
00:07:27,120 --> 00:07:29,599
surveillance we see even tools like

222
00:07:29,599 --> 00:07:31,919
remote access trojans which historically

223
00:07:31,919 --> 00:07:33,919
come from say

224
00:07:33,919 --> 00:07:35,759
banking malware contacts you're trying

225
00:07:35,759 --> 00:07:37,599
to steal somebody's money is now being

226
00:07:37,599 --> 00:07:39,520
used in a surveillance context in an

227
00:07:39,520 --> 00:07:42,479
intimate partner violence scenario

228
00:07:42,479 --> 00:07:44,080
so there's lessons that we can take so

229
00:07:44,080 --> 00:07:45,840
that we don't have to start from ground

230
00:07:45,840 --> 00:07:50,319
zero in addressing many of these threats

231
00:07:50,319 --> 00:07:52,560
so in coming up with this taxonomy what

232
00:07:52,560 --> 00:07:54,560
we found is that really addressing each

233
00:07:54,560 --> 00:07:56,319
class of hate and harassment is going to

234
00:07:56,319 --> 00:07:58,720
require some unique combination

235
00:07:58,720 --> 00:08:02,080
of warnings nudges moderation automated

236
00:08:02,080 --> 00:08:04,879
detection or even just conscious design

237
00:08:04,879 --> 00:08:06,800
because how we're going to go about and

238
00:08:06,800 --> 00:08:08,800
solve something like toxic content is

239
00:08:08,800 --> 00:08:10,400
going to be fundamentally different from

240
00:08:10,400 --> 00:08:11,759
how we go about thinking about

241
00:08:11,759 --> 00:08:14,479
surveillance or impersonation or

242
00:08:14,479 --> 00:08:17,280
intimate content peaking online so we

243
00:08:17,280 --> 00:08:18,720
need to understand the nuances of each

244
00:08:18,720 --> 00:08:20,400
of those threats and their capabilities

245
00:08:20,400 --> 00:08:24,318
before we think about the solution space

246
00:08:24,479 --> 00:08:25,840
another thing we've been working on our

247
00:08:25,840 --> 00:08:27,680
research team is understanding the

248
00:08:27,680 --> 00:08:29,919
prevalence of online hate and harassment

249
00:08:29,919 --> 00:08:32,080
and who's uniquely at risk of attack

250
00:08:32,080 --> 00:08:33,839
right now

251
00:08:33,839 --> 00:08:35,839
as part of this we've been measuring the

252
00:08:35,839 --> 00:08:38,159
prevalence of harassment globally for

253
00:08:38,159 --> 00:08:40,799
many years now in 22 different countries

254
00:08:40,799 --> 00:08:43,519
across 50 000 different participants in

255
00:08:43,519 --> 00:08:45,279
different countries around the world

256
00:08:45,279 --> 00:08:47,600
and different interstitials of time

257
00:08:47,600 --> 00:08:49,440
and what we found is that

258
00:08:49,440 --> 00:08:51,120
hate and harassment really is a global

259
00:08:51,120 --> 00:08:53,680
phenomenon in kenya it affects as many

260
00:08:53,680 --> 00:08:55,360
as 72 percent of people that are

261
00:08:55,360 --> 00:08:57,680
participating online and in japan it's

262
00:08:57,680 --> 00:09:00,240
about 20 of individuals and the united

263
00:09:00,240 --> 00:09:01,680
states is somewhere in between at about

264
00:09:01,680 --> 00:09:04,240
36 percent of people

265
00:09:04,240 --> 00:09:05,760
what we also find is that these country

266
00:09:05,760 --> 00:09:08,080
granularities matter the threats in say

267
00:09:08,080 --> 00:09:10,480
brazil people often talk about sexual

268
00:09:10,480 --> 00:09:12,800
harassment as being one of their top

269
00:09:12,800 --> 00:09:14,880
concerns whereas in the united states

270
00:09:14,880 --> 00:09:17,360
its concerns around stockin and every

271
00:09:17,360 --> 00:09:22,000
other country has different variations

272
00:09:22,000 --> 00:09:23,360
but if we take it as a global

273
00:09:23,360 --> 00:09:25,279
perspective just averaging everything

274
00:09:25,279 --> 00:09:27,519
together we find that the top most

275
00:09:27,519 --> 00:09:29,760
concerns very often are about toxic

276
00:09:29,760 --> 00:09:32,080
content many people are experiencing

277
00:09:32,080 --> 00:09:34,320
being insulted and treated unkindly

278
00:09:34,320 --> 00:09:35,920
having someone make hateful comments

279
00:09:35,920 --> 00:09:39,040
about them being called offensive names

280
00:09:39,040 --> 00:09:41,680
and this counts for about 14 to 19 of

281
00:09:41,680 --> 00:09:43,839
people around the world

282
00:09:43,839 --> 00:09:46,480
but we also see a variety of more severe

283
00:09:46,480 --> 00:09:48,640
but less prevalent threats things like

284
00:09:48,640 --> 00:09:51,040
being stalked having your account hacked

285
00:09:51,040 --> 00:09:53,360
by somebody that you know having someone

286
00:09:53,360 --> 00:09:55,440
purposefully leak photos about you to

287
00:09:55,440 --> 00:09:57,519
embarrass you or having someone use

288
00:09:57,519 --> 00:10:00,000
spyware to monitor activities this

289
00:10:00,000 --> 00:10:01,680
affects people you know on the order of

290
00:10:01,680 --> 00:10:03,680
four to seven percent of individuals

291
00:10:03,680 --> 00:10:05,200
around the globe

292
00:10:05,200 --> 00:10:06,959
and so i think when we talk about how do

293
00:10:06,959 --> 00:10:08,959
we prioritize which threats we should be

294
00:10:08,959 --> 00:10:10,720
looking at you have to think about a

295
00:10:10,720 --> 00:10:12,880
dual spectrum here there's a prevalence

296
00:10:12,880 --> 00:10:15,120
component which would say toxic content

297
00:10:15,120 --> 00:10:17,200
is the biggest problem but there's also

298
00:10:17,200 --> 00:10:19,600
a severity component for how impactful

299
00:10:19,600 --> 00:10:22,320
these harms are to the end users in

300
00:10:22,320 --> 00:10:23,760
which case things like stocking and

301
00:10:23,760 --> 00:10:25,519
account hijacking may take higher

302
00:10:25,519 --> 00:10:28,720
prevalence or importance

303
00:10:29,200 --> 00:10:31,120
the other challenge here is that in

304
00:10:31,120 --> 00:10:32,800
measuring this year-over-year we're

305
00:10:32,800 --> 00:10:34,160
seeing that hate and harassment really

306
00:10:34,160 --> 00:10:37,120
is an increasing problem so in 2016 when

307
00:10:37,120 --> 00:10:38,160
we first started doing these

308
00:10:38,160 --> 00:10:40,800
measurements if we looked one year later

309
00:10:40,800 --> 00:10:42,240
the odds of experiencing hate and

310
00:10:42,240 --> 00:10:45,279
harassment had increased by 1.2 times

311
00:10:45,279 --> 00:10:46,959
and again if we looked in 2018 it

312
00:10:46,959 --> 00:10:49,200
increased by 1.3 times

313
00:10:49,200 --> 00:10:51,120
this is a growing and unabated problem

314
00:10:51,120 --> 00:10:52,720
it means that the solutions that we have

315
00:10:52,720 --> 00:10:54,800
right now aren't addressing the problem

316
00:10:54,800 --> 00:10:56,000
and more and more people are

317
00:10:56,000 --> 00:10:58,839
experiencing this type of abuse

318
00:10:58,839 --> 00:11:00,800
online but this abuse is

319
00:11:00,800 --> 00:11:02,560
disproportionately felt by certain

320
00:11:02,560 --> 00:11:04,800
populations so if we were to model the

321
00:11:04,800 --> 00:11:06,160
likelihood that you personally

322
00:11:06,160 --> 00:11:08,079
experience hate and harassment online

323
00:11:08,079 --> 00:11:09,120
while controlling for different

324
00:11:09,120 --> 00:11:11,519
variables like where you live how often

325
00:11:11,519 --> 00:11:14,160
you use social media your gender and

326
00:11:14,160 --> 00:11:16,720
other demographic attributes about you

327
00:11:16,720 --> 00:11:18,079
we come up with different modeling

328
00:11:18,079 --> 00:11:19,839
components so

329
00:11:19,839 --> 00:11:21,680
an interesting thing is men and women

330
00:11:21,680 --> 00:11:23,440
actually face similar levels of hate and

331
00:11:23,440 --> 00:11:25,839
harassment online but their experiences

332
00:11:25,839 --> 00:11:27,680
are fundamentally different

333
00:11:27,680 --> 00:11:31,200
men often face threats of violence and

334
00:11:31,200 --> 00:11:33,120
physical threats and name calling

335
00:11:33,120 --> 00:11:35,040
whereas women are far more often facing

336
00:11:35,040 --> 00:11:37,040
threats related to sexual harassment and

337
00:11:37,040 --> 00:11:38,399
stalking

338
00:11:38,399 --> 00:11:40,720
but for the lgbtq plus community they

339
00:11:40,720 --> 00:11:42,959
face two times the rate of online hate

340
00:11:42,959 --> 00:11:44,720
and harassment compared to other

341
00:11:44,720 --> 00:11:46,880
counterparts and the same is also true

342
00:11:46,880 --> 00:11:49,279
for very young people who identify as 18

343
00:11:49,279 --> 00:11:51,920
to 24 years of age they face four times

344
00:11:51,920 --> 00:11:53,680
the rate of hate and harassment online

345
00:11:53,680 --> 00:11:56,160
than older counterparts and again for

346
00:11:56,160 --> 00:11:58,000
people who use social media on a daily

347
00:11:58,000 --> 00:12:00,880
basis they face 2.5 times the rate of

348
00:12:00,880 --> 00:12:03,279
hate and harassment

349
00:12:03,279 --> 00:12:04,639
and so from all this modeling the

350
00:12:04,639 --> 00:12:06,639
important thing to understand is that we

351
00:12:06,639 --> 00:12:08,800
really need when considering solutions

352
00:12:08,800 --> 00:12:10,399
for hate and harassment

353
00:12:10,399 --> 00:12:12,000
to design for the experiences and

354
00:12:12,000 --> 00:12:13,519
threats faced by these at-risk

355
00:12:13,519 --> 00:12:15,760
populations not just the mythical

356
00:12:15,760 --> 00:12:17,120
average user

357
00:12:17,120 --> 00:12:19,040
because in many cases these individuals

358
00:12:19,040 --> 00:12:21,200
are at the forefront of the threats that

359
00:12:21,200 --> 00:12:23,920
are being uh that are occurring online

360
00:12:23,920 --> 00:12:25,440
and by learning from them there's

361
00:12:25,440 --> 00:12:27,200
actually downstream benefits that we can

362
00:12:27,200 --> 00:12:29,600
provide to the broader population by

363
00:12:29,600 --> 00:12:31,440
learning from them building more robust

364
00:12:31,440 --> 00:12:33,279
protections and scaling that to

365
00:12:33,279 --> 00:12:35,680
everybody

366
00:12:35,680 --> 00:12:37,360
the last point i want to make today is

367
00:12:37,360 --> 00:12:39,399
that there isn't going to be a

368
00:12:39,399 --> 00:12:41,600
one-size-fits-all solution to hate and

369
00:12:41,600 --> 00:12:43,839
harassment attacks i want to talk about

370
00:12:43,839 --> 00:12:46,079
toxic content as a way to illustrate

371
00:12:46,079 --> 00:12:47,600
this point

372
00:12:47,600 --> 00:12:49,920
right now there's a variety of tools and

373
00:12:49,920 --> 00:12:51,600
technologies that are being developed to

374
00:12:51,600 --> 00:12:53,839
potentially automatically classify and

375
00:12:53,839 --> 00:12:56,560
detect hate and harassment that occurs

376
00:12:56,560 --> 00:12:58,720
via messages and comments and things

377
00:12:58,720 --> 00:12:59,920
like that

378
00:12:59,920 --> 00:13:01,279
google has an offering called

379
00:13:01,279 --> 00:13:03,279
perspective that rates a comment on a

380
00:13:03,279 --> 00:13:05,600
rating of zero to one on how likely it's

381
00:13:05,600 --> 00:13:07,120
going to

382
00:13:07,120 --> 00:13:09,120
kind of degrade the discourse of that

383
00:13:09,120 --> 00:13:10,800
conversation

384
00:13:10,800 --> 00:13:12,480
instagram has also launched a feature

385
00:13:12,480 --> 00:13:14,800
recently where if you're about ready to

386
00:13:14,800 --> 00:13:17,200
post a comment to an image

387
00:13:17,200 --> 00:13:19,120
they will warn you whether it's similar

388
00:13:19,120 --> 00:13:20,880
to content that's been reported in the

389
00:13:20,880 --> 00:13:23,519
past as hate and harassment and ibm and

390
00:13:23,519 --> 00:13:25,040
others have kind of started to develop

391
00:13:25,040 --> 00:13:27,519
classifiers along this line

392
00:13:27,519 --> 00:13:29,279
but a fundamental problem underpinning

393
00:13:29,279 --> 00:13:31,360
all these different solutions is that

394
00:13:31,360 --> 00:13:34,160
what constitutes toxic content is highly

395
00:13:34,160 --> 00:13:36,079
subjective and it's going to vary from

396
00:13:36,079 --> 00:13:38,320
one individual to another based on their

397
00:13:38,320 --> 00:13:40,240
personal experience

398
00:13:40,240 --> 00:13:42,399
and illustrate this point

399
00:13:42,399 --> 00:13:44,320
we did a survey where we asked a number

400
00:13:44,320 --> 00:13:46,800
of people a simple question which was

401
00:13:46,800 --> 00:13:48,959
how toxic do you personally find this

402
00:13:48,959 --> 00:13:51,120
comment we showed them a different set

403
00:13:51,120 --> 00:13:53,360
of comments drawn from twitter reddit

404
00:13:53,360 --> 00:13:55,440
4chan with a sliding scale of what we

405
00:13:55,440 --> 00:13:57,680
interpreted as very likely to be toxic

406
00:13:57,680 --> 00:13:59,760
to this should be innocuous

407
00:13:59,760 --> 00:14:01,519
and we gave them a scale to rate

408
00:14:01,519 --> 00:14:03,279
themselves that said i think this is not

409
00:14:03,279 --> 00:14:07,279
at all toxic too it's extremely toxic

410
00:14:07,279 --> 00:14:08,959
and we ran this in the united states

411
00:14:08,959 --> 00:14:10,959
only across 20 000 different

412
00:14:10,959 --> 00:14:13,519
participants and we showed them 100 000

413
00:14:13,519 --> 00:14:15,040
different comments that were randomly

414
00:14:15,040 --> 00:14:18,160
sampled but we made sure to select a

415
00:14:18,160 --> 00:14:19,920
pool of participants with diverse

416
00:14:19,920 --> 00:14:21,680
backgrounds so people that were

417
00:14:21,680 --> 00:14:23,839
minorities people that were lgbtq plus

418
00:14:23,839 --> 00:14:25,279
people who were religious people who

419
00:14:25,279 --> 00:14:27,440
were parents all to understand how these

420
00:14:27,440 --> 00:14:29,760
various factors might inform how them

421
00:14:29,760 --> 00:14:32,000
reading a comment online their belief

422
00:14:32,000 --> 00:14:34,399
that yes you should be able to post this

423
00:14:34,399 --> 00:14:36,000
yes i'm comfortable being able to see

424
00:14:36,000 --> 00:14:37,680
this

425
00:14:37,680 --> 00:14:39,279
and we found is that

426
00:14:39,279 --> 00:14:41,600
the chance of you rating content is

427
00:14:41,600 --> 00:14:43,600
toxic is highly dependent on these

428
00:14:43,600 --> 00:14:45,839
different demographic factors so if

429
00:14:45,839 --> 00:14:48,880
you're a young individual who's 18 to 24

430
00:14:48,880 --> 00:14:51,279
you're 1.3 times more likely to see a

431
00:14:51,279 --> 00:14:53,440
random comment and say i think that's

432
00:14:53,440 --> 00:14:54,959
toxics and i don't want to see it and i

433
00:14:54,959 --> 00:14:56,079
don't think that should be able to be

434
00:14:56,079 --> 00:14:58,000
posted online compared to older

435
00:14:58,000 --> 00:14:59,199
individuals

436
00:14:59,199 --> 00:15:00,720
and we saw the same thing was true for

437
00:15:00,720 --> 00:15:03,360
minorities for the lgbtq plus and for

438
00:15:03,360 --> 00:15:05,360
parents so all these people are

439
00:15:05,360 --> 00:15:06,959
approaching content that's appearing

440
00:15:06,959 --> 00:15:09,199
online through the lens of maybe what do

441
00:15:09,199 --> 00:15:11,040
i want my kids to see what is my

442
00:15:11,040 --> 00:15:13,760
community experiencing right now

443
00:15:13,760 --> 00:15:16,800
that other people might not be aware of

444
00:15:16,800 --> 00:15:18,399
but a really fascinating component to

445
00:15:18,399 --> 00:15:20,399
this is also people who have personally

446
00:15:20,399 --> 00:15:22,800
experienced toxic content and hate and

447
00:15:22,800 --> 00:15:25,680
harassment are 1.4 times more likely to

448
00:15:25,680 --> 00:15:26,480
say

449
00:15:26,480 --> 00:15:28,000
this comment that you've randomly drawn

450
00:15:28,000 --> 00:15:29,839
from the internet is toxic and i don't

451
00:15:29,839 --> 00:15:31,120
think it should be

452
00:15:31,120 --> 00:15:33,120
posted what this means is that they're

453
00:15:33,120 --> 00:15:35,680
internalizing their own experience and

454
00:15:35,680 --> 00:15:37,519
interpreting what's being posted through

455
00:15:37,519 --> 00:15:39,600
that lens for what type of speech they

456
00:15:39,600 --> 00:15:40,959
feel should or shouldn't be allowed

457
00:15:40,959 --> 00:15:42,560
online

458
00:15:42,560 --> 00:15:45,199
so as technology platforms policymakers

459
00:15:45,199 --> 00:15:47,120
and lawmakers this poses a significant

460
00:15:47,120 --> 00:15:48,240
challenge

461
00:15:48,240 --> 00:15:49,839
really what we need is to enable

462
00:15:49,839 --> 00:15:52,079
personalization or community level

463
00:15:52,079 --> 00:15:54,320
protections to help us overcome this

464
00:15:54,320 --> 00:15:56,560
ambiguity that can't be tackled by

465
00:15:56,560 --> 00:15:59,120
platform right lines and this isn't to

466
00:15:59,120 --> 00:16:00,399
say there isn't such a thing as hate

467
00:16:00,399 --> 00:16:01,920
speech where platforms really should

468
00:16:01,920 --> 00:16:04,000
draw a line and say there is no context

469
00:16:04,000 --> 00:16:05,440
in which this should be allowed to be

470
00:16:05,440 --> 00:16:08,160
posted but that in the long tail of the

471
00:16:08,160 --> 00:16:09,839
internet and speech everybody is going

472
00:16:09,839 --> 00:16:12,399
to approach this with different opinions

473
00:16:12,399 --> 00:16:13,759
and so we need is really to think about

474
00:16:13,759 --> 00:16:16,160
different spheres of enforcement

475
00:16:16,160 --> 00:16:17,920
what is it that's acceptable for me to

476
00:16:17,920 --> 00:16:20,560
decide what i want to see or not to see

477
00:16:20,560 --> 00:16:22,320
how do you empower communities to make

478
00:16:22,320 --> 00:16:24,000
moderation decisions on other people's

479
00:16:24,000 --> 00:16:26,079
behalf so not everybody has to read

480
00:16:26,079 --> 00:16:28,079
every noxious comment on the internet to

481
00:16:28,079 --> 00:16:29,600
make a determination of what they do or

482
00:16:29,600 --> 00:16:31,759
don't want to see and likewise what are

483
00:16:31,759 --> 00:16:33,279
the decisions that we can say you know

484
00:16:33,279 --> 00:16:35,600
as a platform ready to make this call or

485
00:16:35,600 --> 00:16:37,199
as a society we're ready to make this

486
00:16:37,199 --> 00:16:38,959
call of what content should or shouldn't

487
00:16:38,959 --> 00:16:41,360
be allowed

488
00:16:41,360 --> 00:16:43,519
so to wrap up uh things that i want you

489
00:16:43,519 --> 00:16:45,519
to take away from this talk is really

490
00:16:45,519 --> 00:16:46,480
that

491
00:16:46,480 --> 00:16:48,639
as hate and harassment transforms the

492
00:16:48,639 --> 00:16:50,399
threats that people experience

493
00:16:50,399 --> 00:16:53,120
we as a security community need to adapt

494
00:16:53,120 --> 00:16:55,120
and meet these challenges head on this

495
00:16:55,120 --> 00:16:58,160
is not a sociology problem it is not a

496
00:16:58,160 --> 00:17:00,320
problem for another conference these are

497
00:17:00,320 --> 00:17:02,399
technical means that are being used to

498
00:17:02,399 --> 00:17:05,520
affect end users and we have a you know

499
00:17:05,520 --> 00:17:07,280
and the onus is on us to step in as

500
00:17:07,280 --> 00:17:09,439
security and privacy experts to help and

501
00:17:09,439 --> 00:17:10,799
address it

502
00:17:10,799 --> 00:17:13,039
so to that end what we've provided so

503
00:17:13,039 --> 00:17:15,520
far is one a framework for reasoning

504
00:17:15,520 --> 00:17:18,079
about what these threats are the harms

505
00:17:18,079 --> 00:17:19,520
and the types of protections and

506
00:17:19,520 --> 00:17:21,119
solutions that might work for each of

507
00:17:21,119 --> 00:17:23,679
the individual contexts because again

508
00:17:23,679 --> 00:17:25,359
how we solve toxic content is going to

509
00:17:25,359 --> 00:17:27,199
be fundamentally different from how we

510
00:17:27,199 --> 00:17:29,360
think about surveillance or

511
00:17:29,360 --> 00:17:31,120
overloading people with

512
00:17:31,120 --> 00:17:33,840
you know sms bombs or falsely reporting

513
00:17:33,840 --> 00:17:36,160
them for copyright strikes

514
00:17:36,160 --> 00:17:38,000
we've also provided metrics on the

515
00:17:38,000 --> 00:17:39,679
prevalence of the types of abuse that

516
00:17:39,679 --> 00:17:41,520
people are experiencing right now as

517
00:17:41,520 --> 00:17:43,600
well as identified at-risk populations

518
00:17:43,600 --> 00:17:45,280
that we really need to prioritize

519
00:17:45,280 --> 00:17:47,760
protections for and can also use for

520
00:17:47,760 --> 00:17:49,840
understanding what works for them and

521
00:17:49,840 --> 00:17:53,200
scaled it out to the broader population

522
00:17:53,200 --> 00:17:54,880
and lastly because of all the nuance

523
00:17:54,880 --> 00:17:56,160
that's here as that many people have

524
00:17:56,160 --> 00:17:57,520
already brought up as we talked about

525
00:17:57,520 --> 00:17:59,679
content moderation earlier you know

526
00:17:59,679 --> 00:18:01,600
there's a need for customization here to

527
00:18:01,600 --> 00:18:03,200
empower individuals and communities

528
00:18:03,200 --> 00:18:04,960
around the world to react to the threats

529
00:18:04,960 --> 00:18:06,240
that they're facing

530
00:18:06,240 --> 00:18:08,480
and i don't want to paint this as a it's

531
00:18:08,480 --> 00:18:11,039
on the user to solve this problem it's

532
00:18:11,039 --> 00:18:13,039
to say that that is one lever amongst

533
00:18:13,039 --> 00:18:15,840
many with policy makers and others to

534
00:18:15,840 --> 00:18:17,760
try to address this problem i'm a

535
00:18:17,760 --> 00:18:19,679
technologist so that's where i focus my

536
00:18:19,679 --> 00:18:21,840
energy right now

537
00:18:21,840 --> 00:18:23,360
if you want to learn more we've actually

538
00:18:23,360 --> 00:18:24,720
been doing a lot of research in the

539
00:18:24,720 --> 00:18:27,039
space the two papers listed at the top

540
00:18:27,039 --> 00:18:29,039
are ones that i talked about today we've

541
00:18:29,039 --> 00:18:31,280
also done research in

542
00:18:31,280 --> 00:18:32,799
south asia really to understand the

543
00:18:32,799 --> 00:18:34,799
context of how does hate and harassment

544
00:18:34,799 --> 00:18:36,799
differ from country to country and how

545
00:18:36,799 --> 00:18:39,919
do the design solutions that we adopt

546
00:18:39,919 --> 00:18:41,120
need to meet the needs of those

547
00:18:41,120 --> 00:18:43,120
individuals in those regions

548
00:18:43,120 --> 00:18:44,640
and we also have some research coming up

549
00:18:44,640 --> 00:18:45,919
at kai at the end of the year

550
00:18:45,919 --> 00:18:47,679
specifically talking to another arrest

551
00:18:47,679 --> 00:18:50,000
populations which is creators online

552
00:18:50,000 --> 00:18:51,360
people who are at the forefront of

553
00:18:51,360 --> 00:18:53,440
experiencing hate and harassment due to

554
00:18:53,440 --> 00:18:56,880
their prominence and their visibility

555
00:18:56,880 --> 00:18:58,640
so with that a shout out to many

556
00:18:58,640 --> 00:19:01,520
collaborators both at google and abroad

557
00:19:01,520 --> 00:19:03,039
for who helped on this research and

558
00:19:03,039 --> 00:19:05,920
thank you all for listening

559
00:19:05,920 --> 00:19:09,369
[Applause]

560
00:19:15,760 --> 00:19:17,840
you

