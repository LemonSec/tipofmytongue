1
00:00:08,720 --> 00:00:12,400
so uh as i said i'm dr gus andrews um

2
00:00:12,400 --> 00:00:14,480
this should be can the fight against

3
00:00:14,480 --> 00:00:15,839
disinformation really scale but i'm

4
00:00:15,839 --> 00:00:18,320
going to start by adjusting my title

5
00:00:18,320 --> 00:00:19,520
you'll notice i'm mostly going to say

6
00:00:19,520 --> 00:00:22,000
misinformation as a community we're

7
00:00:22,000 --> 00:00:24,880
mostly saying dis to mean deliberately

8
00:00:24,880 --> 00:00:26,480
deliberately manipulative bad

9
00:00:26,480 --> 00:00:28,480
information and miss to mean the larger

10
00:00:28,480 --> 00:00:30,800
category of bad information i'm mostly

11
00:00:30,800 --> 00:00:32,238
going to talk about the larger category

12
00:00:32,238 --> 00:00:34,399
but as you'll see i'm about to make the

13
00:00:34,399 --> 00:00:36,559
case that neither miss nor dis is

14
00:00:36,559 --> 00:00:40,480
actually useful to solving the problem

15
00:00:40,480 --> 00:00:42,480
how would we approach the problem

16
00:00:42,480 --> 00:00:44,480
differently if we started from the

17
00:00:44,480 --> 00:00:45,600
assumption

18
00:00:45,600 --> 00:00:47,440
that there is no such thing as

19
00:00:47,440 --> 00:00:49,920
misinformation or disinformation

20
00:00:49,920 --> 00:00:51,600
only information

21
00:00:51,600 --> 00:00:53,600
because there are two powerful forces in

22
00:00:53,600 --> 00:00:55,280
the world that already act as if this is

23
00:00:55,280 --> 00:00:56,399
the case

24
00:00:56,399 --> 00:00:58,800
one of these forces is the internet it's

25
00:00:58,800 --> 00:01:01,120
been built to offer information without

26
00:01:01,120 --> 00:01:02,800
gatekeepers right

27
00:01:02,800 --> 00:01:04,559
the other is the people using the

28
00:01:04,559 --> 00:01:06,000
internet

29
00:01:06,000 --> 00:01:07,680
as an anthropologist this is how i

30
00:01:07,680 --> 00:01:09,439
approach the question right

31
00:01:09,439 --> 00:01:11,360
i start from how people use information

32
00:01:11,360 --> 00:01:13,280
on their own terms and i

33
00:01:13,280 --> 00:01:15,200
empirically observe them in the field

34
00:01:15,200 --> 00:01:17,840
not in the lab

35
00:01:18,960 --> 00:01:20,880
i see good information and bad

36
00:01:20,880 --> 00:01:23,759
information as forming opposing teams

37
00:01:23,759 --> 00:01:25,680
the teams have colors they wear to show

38
00:01:25,680 --> 00:01:27,920
they belong to the club they have chance

39
00:01:27,920 --> 00:01:29,520
they repeat they have players they

40
00:01:29,520 --> 00:01:31,920
idolize and the way they chant and what

41
00:01:31,920 --> 00:01:33,439
they wear to show their pride

42
00:01:33,439 --> 00:01:35,200
contributes to their sense of who they

43
00:01:35,200 --> 00:01:37,360
are

44
00:01:37,360 --> 00:01:39,360
team western empirical science has its

45
00:01:39,360 --> 00:01:40,880
own ways to demonstrate you're a super

46
00:01:40,880 --> 00:01:43,360
fan like citing your sources or on a

47
00:01:43,360 --> 00:01:45,840
more superficial level giving unmasked

48
00:01:45,840 --> 00:01:47,360
people dirty looks

49
00:01:47,360 --> 00:01:49,520
so face it team science you ask people

50
00:01:49,520 --> 00:01:54,000
to demonstrate team loyalty this way too

51
00:01:54,000 --> 00:01:55,759
bring up the team metaphor

52
00:01:55,759 --> 00:01:57,360
because whether you want to admit it or

53
00:01:57,360 --> 00:02:00,960
not all information has a social life

54
00:02:00,960 --> 00:02:03,439
even scientific facts

55
00:02:03,439 --> 00:02:05,360
starting from this perspective let me

56
00:02:05,360 --> 00:02:07,840
explain why our dominant approaches to

57
00:02:07,840 --> 00:02:10,399
misinformation as they stand are deeply

58
00:02:10,399 --> 00:02:12,640
flawed

59
00:02:12,640 --> 00:02:14,800
right now two major approaches make up

60
00:02:14,800 --> 00:02:16,480
the lion's share of projects in the

61
00:02:16,480 --> 00:02:18,800
credibility coalition which is a central

62
00:02:18,800 --> 00:02:20,239
organizing space

63
00:02:20,239 --> 00:02:21,840
for the misinformation fighting efforts

64
00:02:21,840 --> 00:02:24,800
started by journalists and technologists

65
00:02:24,800 --> 00:02:26,319
these two approaches are first

66
00:02:26,319 --> 00:02:28,800
fact-checking and second large-scale

67
00:02:28,800 --> 00:02:31,200
technical solutions

68
00:02:31,200 --> 00:02:32,800
this graphic shows a little more than

69
00:02:32,800 --> 00:02:34,400
half of the projects in the credibility

70
00:02:34,400 --> 00:02:36,879
coalition's catalog that are listed as

71
00:02:36,879 --> 00:02:38,400
doing fact checking

72
00:02:38,400 --> 00:02:40,160
they include projects like profact

73
00:02:40,160 --> 00:02:43,120
moldova ecuador czechaea and voice of

74
00:02:43,120 --> 00:02:44,720
san diego which are all grounded in

75
00:02:44,720 --> 00:02:46,080
local news

76
00:02:46,080 --> 00:02:48,080
there are also newcomer nonprofits like

77
00:02:48,080 --> 00:02:49,760
full fact and meta fact that claim a

78
00:02:49,760 --> 00:02:51,840
broader scope

79
00:02:51,840 --> 00:02:53,599
here's credibility coalitions projects

80
00:02:53,599 --> 00:02:55,440
using the second approach algorithms

81
00:02:55,440 --> 00:02:57,840
artificial intelligence machine learning

82
00:02:57,840 --> 00:02:59,920
and the blockchain

83
00:02:59,920 --> 00:03:03,760
to identify misinformation at scale

84
00:03:03,840 --> 00:03:07,200
these approaches start from a few flawed

85
00:03:07,200 --> 00:03:10,159
a priori assumptions

86
00:03:10,159 --> 00:03:11,680
one of these is that facts should be at

87
00:03:11,680 --> 00:03:13,040
the center of our fight against

88
00:03:13,040 --> 00:03:15,599
misinformation

89
00:03:15,599 --> 00:03:17,200
another is that it's easy to convince

90
00:03:17,200 --> 00:03:19,599
people to abandon their team to trust

91
00:03:19,599 --> 00:03:23,519
our facts our good information

92
00:03:23,519 --> 00:03:25,280
and those two faulty assumptions

93
00:03:25,280 --> 00:03:27,280
underlie the third which is that we know

94
00:03:27,280 --> 00:03:28,879
what it looks like to fight information

95
00:03:28,879 --> 00:03:31,040
at scale

96
00:03:31,040 --> 00:03:32,959
to solve the problem we need to

97
00:03:32,959 --> 00:03:35,360
understand why people join team

98
00:03:35,360 --> 00:03:37,040
conspiracy theory

99
00:03:37,040 --> 00:03:38,640
we need to reckon with how that team

100
00:03:38,640 --> 00:03:40,080
identifies with certain kinds of

101
00:03:40,080 --> 00:03:41,440
information

102
00:03:41,440 --> 00:03:43,040
and if we do that

103
00:03:43,040 --> 00:03:44,879
then maybe we have a chance at winning

104
00:03:44,879 --> 00:03:46,640
those people over team evidence-based

105
00:03:46,640 --> 00:03:48,720
reality

106
00:03:48,720 --> 00:03:50,319
we need to start by recognizing that

107
00:03:50,319 --> 00:03:54,080
facts and information are social

108
00:03:54,080 --> 00:03:55,599
what do i mean am i some sort of

109
00:03:55,599 --> 00:03:57,439
loosey-goosey postmodernist making the

110
00:03:57,439 --> 00:03:58,879
argument the facts can be whatever you

111
00:03:58,879 --> 00:04:01,840
want them to be man no

112
00:04:01,840 --> 00:04:03,360
some of you team science fans are out

113
00:04:03,360 --> 00:04:04,799
there going that's preposterous you

114
00:04:04,799 --> 00:04:07,280
can't argue with the natural world

115
00:04:07,280 --> 00:04:09,680
but team sciences facts don't spring

116
00:04:09,680 --> 00:04:12,000
fully formed from the natural world

117
00:04:12,000 --> 00:04:15,120
in fact the word fact as we know it

118
00:04:15,120 --> 00:04:17,519
didn't even come into a usage until the

119
00:04:17,519 --> 00:04:19,358
17th century

120
00:04:19,358 --> 00:04:21,120
historian david wooten points out that

121
00:04:21,120 --> 00:04:22,479
there is in fact

122
00:04:22,479 --> 00:04:24,880
no classical greek or latin word for

123
00:04:24,880 --> 00:04:26,080
fact

124
00:04:26,080 --> 00:04:28,960
it was in the 1600s that experimental

125
00:04:28,960 --> 00:04:30,720
scientists started to call and appeal to

126
00:04:30,720 --> 00:04:33,199
evidence of fact

127
00:04:33,199 --> 00:04:35,919
notice that construction right a fact is

128
00:04:35,919 --> 00:04:38,720
not the same thing as evidence it is an

129
00:04:38,720 --> 00:04:41,280
appeal to evidence

130
00:04:41,280 --> 00:04:43,680
it's evidence plus a human being going

131
00:04:43,680 --> 00:04:45,919
here's my evidence this is why it's a

132
00:04:45,919 --> 00:04:47,919
fact this machine measured it that other

133
00:04:47,919 --> 00:04:50,400
guy i'm citing says so also and so forth

134
00:04:50,400 --> 00:04:52,560
it's evidence with peer review with

135
00:04:52,560 --> 00:04:55,360
citations and then some

136
00:04:55,360 --> 00:04:56,400
consider

137
00:04:56,400 --> 00:04:58,800
how many human arguments it took

138
00:04:58,800 --> 00:05:01,520
for people to accept facts from galileo

139
00:05:01,520 --> 00:05:03,520
pasteur darwin

140
00:05:03,520 --> 00:05:05,120
those arguments took place in royal

141
00:05:05,120 --> 00:05:06,080
courts

142
00:05:06,080 --> 00:05:08,240
legal courts churches and schools as

143
00:05:08,240 --> 00:05:09,840
well as scientific journals social

144
00:05:09,840 --> 00:05:11,280
spaces

145
00:05:11,280 --> 00:05:13,600
and facts aren't permanent

146
00:05:13,600 --> 00:05:15,600
new teams emerge to make new appeals to

147
00:05:15,600 --> 00:05:18,000
evidence and new facts

148
00:05:18,000 --> 00:05:19,919
for example darwin's rival lamarck

149
00:05:19,919 --> 00:05:21,759
regained some credibility since most of

150
00:05:21,759 --> 00:05:24,479
us learned in school that he was wrong

151
00:05:24,479 --> 00:05:26,000
lamarck was the guy who argued that

152
00:05:26,000 --> 00:05:27,840
giraffes stretched their necks and

153
00:05:27,840 --> 00:05:29,759
passed their trade on to their offspring

154
00:05:29,759 --> 00:05:31,840
so team epigenetics found new evidence

155
00:05:31,840 --> 00:05:34,000
here and started to score better against

156
00:05:34,000 --> 00:05:36,800
team evolution

157
00:05:37,120 --> 00:05:39,520
looking at the historical records

158
00:05:39,520 --> 00:05:42,000
there is no guarantee that team science

159
00:05:42,000 --> 00:05:43,759
wins

160
00:05:43,759 --> 00:05:45,759
if we want to win and i think most of us

161
00:05:45,759 --> 00:05:46,800
do

162
00:05:46,800 --> 00:05:48,960
we need to understand what kind of

163
00:05:48,960 --> 00:05:51,280
socio-political maneuvers it takes to

164
00:05:51,280 --> 00:05:52,960
win

165
00:05:52,960 --> 00:05:55,520
for example fact checking right

166
00:05:55,520 --> 00:05:57,039
this used to be a name for work that

167
00:05:57,039 --> 00:05:59,039
professional journalists did

168
00:05:59,039 --> 00:06:01,039
but the public hasn't always assumed

169
00:06:01,039 --> 00:06:03,680
that newspapers printed facts

170
00:06:03,680 --> 00:06:05,199
you may or may not be aware but in the

171
00:06:05,199 --> 00:06:07,280
1800s journalists had an even worse

172
00:06:07,280 --> 00:06:09,440
reputation than they do now

173
00:06:09,440 --> 00:06:11,520
yellow journalism was what people called

174
00:06:11,520 --> 00:06:14,720
sensationalist news they mistrusted

175
00:06:14,720 --> 00:06:16,240
what critics said about william randolph

176
00:06:16,240 --> 00:06:17,600
hearst is a little bit like what's said

177
00:06:17,600 --> 00:06:19,680
about the media today

178
00:06:19,680 --> 00:06:21,280
so how did team journalism turn it

179
00:06:21,280 --> 00:06:22,160
around

180
00:06:22,160 --> 00:06:23,440
how did they demonstrate that their

181
00:06:23,440 --> 00:06:26,240
facts were trustworthy

182
00:06:26,240 --> 00:06:28,400
they sought professional status the same

183
00:06:28,400 --> 00:06:29,600
kind of professional status that

184
00:06:29,600 --> 00:06:32,000
scientists had built up in the 1800s and

185
00:06:32,000 --> 00:06:34,400
that medicine has enjoyed since medieval

186
00:06:34,400 --> 00:06:35,840
times

187
00:06:35,840 --> 00:06:37,199
how did they do that

188
00:06:37,199 --> 00:06:39,600
guess what social maneuvers once again

189
00:06:39,600 --> 00:06:41,520
joseph pulitzer founded the colombian

190
00:06:41,520 --> 00:06:44,160
journalism school in 1892 he founded the

191
00:06:44,160 --> 00:06:46,560
prize bearing its name

192
00:06:46,560 --> 00:06:49,039
professionalization this process is when

193
00:06:49,039 --> 00:06:51,039
a group of people decides there's

194
00:06:51,039 --> 00:06:53,120
certain specialized knowledge that they

195
00:06:53,120 --> 00:06:54,800
and nobody else are in charge of like

196
00:06:54,800 --> 00:06:57,520
doctors saying not midwives anymore or

197
00:06:57,520 --> 00:07:00,000
psychologists saying hypnosis but not

198
00:07:00,000 --> 00:07:01,520
mesmerism

199
00:07:01,520 --> 00:07:03,440
they set up professional associations

200
00:07:03,440 --> 00:07:06,479
codes of ethics gates to keep people out

201
00:07:06,479 --> 00:07:08,880
diplomas licenses

202
00:07:08,880 --> 00:07:10,960
training that you need in order to call

203
00:07:10,960 --> 00:07:13,680
yourself a professional for journalists

204
00:07:13,680 --> 00:07:16,400
that includes rules for fact checking

205
00:07:16,400 --> 00:07:18,080
people training to be both scientists

206
00:07:18,080 --> 00:07:20,479
and journalists learn their team's

207
00:07:20,479 --> 00:07:23,680
approved methods to build facts

208
00:07:23,680 --> 00:07:25,759
so recap about facts they're an appeal

209
00:07:25,759 --> 00:07:27,919
to evidence by human beings teams of

210
00:07:27,919 --> 00:07:28,720
them

211
00:07:28,720 --> 00:07:30,400
supposedly playing by certain rules so

212
00:07:30,400 --> 00:07:33,679
we'll trust them and their facts

213
00:07:34,000 --> 00:07:37,440
so fans of team evidence-based reality

214
00:07:37,440 --> 00:07:38,560
pardon me

215
00:07:38,560 --> 00:07:40,319
don't get confused

216
00:07:40,319 --> 00:07:42,080
what you trust is not the evidence

217
00:07:42,080 --> 00:07:45,599
itself what you trust is the facts

218
00:07:45,599 --> 00:07:47,120
and the facts are always an appeal to

219
00:07:47,120 --> 00:07:49,360
evidence and what you trust is that

220
00:07:49,360 --> 00:07:51,759
professional scientists and journalists

221
00:07:51,759 --> 00:07:54,000
are using those rules to make a case for

222
00:07:54,000 --> 00:07:56,000
their facts

223
00:07:56,000 --> 00:07:57,280
i'm not here to tell you that you

224
00:07:57,280 --> 00:07:58,960
shouldn't trust facts

225
00:07:58,960 --> 00:08:00,639
what i'm here to clarify is why team

226
00:08:00,639 --> 00:08:02,400
misinformation identifies with other

227
00:08:02,400 --> 00:08:06,000
experts and trusts their facts

228
00:08:06,000 --> 00:08:09,520
so let's unpack what we know about trust

229
00:08:09,520 --> 00:08:11,120
because both fact checking and

230
00:08:11,120 --> 00:08:12,800
algorithmic approaches to misinformation

231
00:08:12,800 --> 00:08:16,960
make bad assumptions about trust as well

232
00:08:16,960 --> 00:08:18,400
trust in social research doesn't mean

233
00:08:18,400 --> 00:08:21,199
what it means in computer engineering

234
00:08:21,199 --> 00:08:22,960
to those of us who study human beings

235
00:08:22,960 --> 00:08:25,680
trust is cognitive it's your neurons

236
00:08:25,680 --> 00:08:27,840
expectation that someone will continue

237
00:08:27,840 --> 00:08:29,599
to behave as they have behaved

238
00:08:29,599 --> 00:08:30,960
previously

239
00:08:30,960 --> 00:08:33,519
so it has a historical aspect

240
00:08:33,519 --> 00:08:36,479
it relies on past interactions

241
00:08:36,479 --> 00:08:38,320
it's developmental it grows within an

242
00:08:38,320 --> 00:08:41,599
individual or it doesn't it's social it

243
00:08:41,599 --> 00:08:44,080
grows between individuals or it doesn't

244
00:08:44,080 --> 00:08:46,160
and it's limited

245
00:08:46,160 --> 00:08:48,080
dunbar's number which is the estimated

246
00:08:48,080 --> 00:08:51,040
limit to human group sizes is debated

247
00:08:51,040 --> 00:08:52,720
but historically humans have only

248
00:08:52,720 --> 00:08:54,800
interacted with a couple hundred people

249
00:08:54,800 --> 00:08:55,839
tops

250
00:08:55,839 --> 00:08:58,480
and this presents a challenge to scaling

251
00:08:58,480 --> 00:09:00,880
trust

252
00:09:01,200 --> 00:09:03,360
trust is contextual and relative if i've

253
00:09:03,360 --> 00:09:05,519
met this furry guy here before or others

254
00:09:05,519 --> 00:09:07,760
of a species i may or may not trust him

255
00:09:07,760 --> 00:09:10,399
not to bite me or children or poop on

256
00:09:10,399 --> 00:09:12,320
the rug i do not trust him to drive a

257
00:09:12,320 --> 00:09:13,200
car

258
00:09:13,200 --> 00:09:14,880
so tell me are your algorithms really

259
00:09:14,880 --> 00:09:18,000
smart enough to get trust right

260
00:09:18,000 --> 00:09:19,760
because facts are social we have to

261
00:09:19,760 --> 00:09:21,040
consider

262
00:09:21,040 --> 00:09:22,880
what personal experiences has mis

263
00:09:22,880 --> 00:09:24,240
experiences have misinformation

264
00:09:24,240 --> 00:09:26,320
spreaders had with team science or with

265
00:09:26,320 --> 00:09:28,560
team medicine with team journalism team

266
00:09:28,560 --> 00:09:30,560
media with government how do they feel

267
00:09:30,560 --> 00:09:33,519
about the professional class generally

268
00:09:33,519 --> 00:09:35,839
and why what did their parents say about

269
00:09:35,839 --> 00:09:38,399
doctors or journalists or scientists how

270
00:09:38,399 --> 00:09:40,720
were they presented in the media

271
00:09:40,720 --> 00:09:42,000
honestly

272
00:09:42,000 --> 00:09:44,000
i think it's unsurprising

273
00:09:44,000 --> 00:09:45,839
we see so little trust in medical facts

274
00:09:45,839 --> 00:09:48,399
at this moment in history

275
00:09:48,399 --> 00:09:50,959
consider the opioid epidemic

276
00:09:50,959 --> 00:09:52,880
how many people in the united states had

277
00:09:52,880 --> 00:09:55,040
their trust in medicine shaken when the

278
00:09:55,040 --> 00:09:56,800
entire health care system insisted it

279
00:09:56,800 --> 00:09:58,000
was a fact

280
00:09:58,000 --> 00:10:00,320
that opioids were not addictive

281
00:10:00,320 --> 00:10:01,760
that science said they could safely be

282
00:10:01,760 --> 00:10:03,680
used in higher and higher doses and then

283
00:10:03,680 --> 00:10:06,079
their loved ones grew addicted to drugs

284
00:10:06,079 --> 00:10:09,599
their doctors prescribed them

285
00:10:10,000 --> 00:10:12,160
think about what purdue pharma did to

286
00:10:12,160 --> 00:10:13,839
convince all of us that opioids were

287
00:10:13,839 --> 00:10:14,959
safe

288
00:10:14,959 --> 00:10:17,120
they used what appeared to all but the

289
00:10:17,120 --> 00:10:19,040
most attentive i

290
00:10:19,040 --> 00:10:20,720
to be science facts

291
00:10:20,720 --> 00:10:23,920
they used journal articles graphs

292
00:10:23,920 --> 00:10:25,600
pr pieces that got turned into news

293
00:10:25,600 --> 00:10:27,279
reports and so journalism gets a black

294
00:10:27,279 --> 00:10:28,720
eye there as well

295
00:10:28,720 --> 00:10:31,279
they co-opted trusted experts they

296
00:10:31,279 --> 00:10:33,200
whined and dined our doctors convincing

297
00:10:33,200 --> 00:10:35,600
them that higher doses were safe

298
00:10:35,600 --> 00:10:37,279
and that's not even deeper history like

299
00:10:37,279 --> 00:10:39,440
the tuskegee syphilis experiment or

300
00:10:39,440 --> 00:10:40,959
american eugenicists forcibly

301
00:10:40,959 --> 00:10:42,720
sterilizing people

302
00:10:42,720 --> 00:10:45,200
this 1945 article around the time my

303
00:10:45,200 --> 00:10:46,560
grandmother was young

304
00:10:46,560 --> 00:10:48,399
from a professional grand uh

305
00:10:48,399 --> 00:10:49,839
professional medical journal talked

306
00:10:49,839 --> 00:10:51,839
about forcibly sterilizing patients for

307
00:10:51,839 --> 00:10:54,000
being mentally defective

308
00:10:54,000 --> 00:10:55,839
which could mean as this article says

309
00:10:55,839 --> 00:10:57,279
for epilepsy

310
00:10:57,279 --> 00:11:00,640
forgery or skipping school

311
00:11:00,640 --> 00:11:03,120
or for not being white

312
00:11:03,120 --> 00:11:04,959
this was many one of many such articles

313
00:11:04,959 --> 00:11:07,040
written by mds so is it any wonder

314
00:11:07,040 --> 00:11:08,560
there's a large group of people out

315
00:11:08,560 --> 00:11:11,519
there who just don't just trust

316
00:11:11,519 --> 00:11:13,519
medical facts who might trust other

317
00:11:13,519 --> 00:11:16,560
sources of information

318
00:11:16,560 --> 00:11:19,120
i guarantee misinformation spreaders are

319
00:11:19,120 --> 00:11:20,880
building their own team from people they

320
00:11:20,880 --> 00:11:22,000
trust

321
00:11:22,000 --> 00:11:23,920
their friends social media influencers

322
00:11:23,920 --> 00:11:26,079
anyone who pure appeals to evidence they

323
00:11:26,079 --> 00:11:28,959
can see with their own eyes

324
00:11:28,959 --> 00:11:31,040
experts they find trustworthy because

325
00:11:31,040 --> 00:11:33,040
they're familiar to them because of

326
00:11:33,040 --> 00:11:34,880
experiences they have personally had

327
00:11:34,880 --> 00:11:36,880
starting in childhood

328
00:11:36,880 --> 00:11:38,560
we can probably trace it back to how

329
00:11:38,560 --> 00:11:40,560
they feel about school

330
00:11:40,560 --> 00:11:42,000
were they really given the opportunity

331
00:11:42,000 --> 00:11:44,000
to succeed in science class or did it

332
00:11:44,000 --> 00:11:46,000
seem like people like them were bad at

333
00:11:46,000 --> 00:11:47,040
science

334
00:11:47,040 --> 00:11:48,480
were they exposed to research methods

335
00:11:48,480 --> 00:11:52,160
that empowered them or exploited them

336
00:11:52,480 --> 00:11:54,720
team loyalties go back for generations

337
00:11:54,720 --> 00:11:56,720
and some religious leaders still reject

338
00:11:56,720 --> 00:11:58,240
darwin's appeals to evidence as in the

339
00:11:58,240 --> 00:12:00,480
scopes monkey trial they preach that

340
00:12:00,480 --> 00:12:03,120
evolution is a lie

341
00:12:03,120 --> 00:12:04,560
francesca trippidy wrote this great

342
00:12:04,560 --> 00:12:06,800
report on what we could call team

343
00:12:06,800 --> 00:12:08,399
scriptural inference

344
00:12:08,399 --> 00:12:09,839
which is basically conservative

345
00:12:09,839 --> 00:12:11,600
christians applying bible study reading

346
00:12:11,600 --> 00:12:13,440
practices to make their own facts from

347
00:12:13,440 --> 00:12:16,079
what they see in the media

348
00:12:16,079 --> 00:12:18,240
the idea that 5g causes covid as patrick

349
00:12:18,240 --> 00:12:21,200
talked about sounds me maybe to us

350
00:12:21,200 --> 00:12:22,560
but the idea that radio waves are

351
00:12:22,560 --> 00:12:23,760
hazardous to your health it's been

352
00:12:23,760 --> 00:12:25,200
around as long as we've had the ability

353
00:12:25,200 --> 00:12:27,440
to produce radio waves to those already

354
00:12:27,440 --> 00:12:29,360
convinced of this fact attributing

355
00:12:29,360 --> 00:12:30,720
covent to radio frequencies seems

356
00:12:30,720 --> 00:12:33,040
obvious

357
00:12:33,040 --> 00:12:34,320
team q anon

358
00:12:34,320 --> 00:12:37,279
makes their own appeals to evidence

359
00:12:37,279 --> 00:12:38,079
human

360
00:12:38,079 --> 00:12:40,480
enthusiasts and anti-vaxxers

361
00:12:40,480 --> 00:12:42,720
encourage people to do the research to

362
00:12:42,720 --> 00:12:44,240
find their own evidence make their own

363
00:12:44,240 --> 00:12:45,120
facts

364
00:12:45,120 --> 00:12:47,519
expand your thinking as the team

365
00:12:47,519 --> 00:12:48,959
chant

366
00:12:48,959 --> 00:12:51,519
this logo from a t-shirt shows off a

367
00:12:51,519 --> 00:12:53,440
number of ways team q and on appeals to

368
00:12:53,440 --> 00:12:55,680
evidence good versus evil again suggests

369
00:12:55,680 --> 00:12:57,200
a biblical reading

370
00:12:57,200 --> 00:12:59,519
they appeal to future evidence that

371
00:12:59,519 --> 00:13:02,240
proves past drops from cue

372
00:13:02,240 --> 00:13:03,920
so they're relying on evidence they just

373
00:13:03,920 --> 00:13:05,600
don't play by team sciences don't

374
00:13:05,600 --> 00:13:08,720
cherry-pick evidence rule

375
00:13:08,800 --> 00:13:10,240
they also use some of the same facts as

376
00:13:10,240 --> 00:13:11,519
the rest of us

377
00:13:11,519 --> 00:13:14,560
huber here is u.s attorney john huber

378
00:13:14,560 --> 00:13:16,079
who investigated hillary clinton while

379
00:13:16,079 --> 00:13:18,160
in trump's justice department

380
00:13:18,160 --> 00:13:20,240
so you might expect q anon followers

381
00:13:20,240 --> 00:13:21,519
think government facts are never

382
00:13:21,519 --> 00:13:23,600
trustworthy but at the time the shirt

383
00:13:23,600 --> 00:13:25,440
was printed they were still hoping that

384
00:13:25,440 --> 00:13:27,279
huber's legal documents would build

385
00:13:27,279 --> 00:13:28,880
their kind of facts

386
00:13:28,880 --> 00:13:31,040
the shirt also refers to the five eyes

387
00:13:31,040 --> 00:13:33,440
the international intelligence alliance

388
00:13:33,440 --> 00:13:34,959
edward snowden's leaks put out in the

389
00:13:34,959 --> 00:13:36,320
public eye

390
00:13:36,320 --> 00:13:37,440
so again

391
00:13:37,440 --> 00:13:40,000
evidence that most of us accept is fact

392
00:13:40,000 --> 00:13:42,240
team q anon may have their own logic but

393
00:13:42,240 --> 00:13:43,680
they also value documents that are

394
00:13:43,680 --> 00:13:45,839
considered factual by teams journalism

395
00:13:45,839 --> 00:13:47,600
and law

396
00:13:47,600 --> 00:13:49,680
so you can't just fix this by

397
00:13:49,680 --> 00:13:52,720
downranking conspiracy texts right

398
00:13:52,720 --> 00:13:55,040
it's not about what team conspiracy

399
00:13:55,040 --> 00:13:57,519
reads it's about how they read it

400
00:13:57,519 --> 00:14:02,199
and whose interpretation they trust

401
00:14:02,320 --> 00:14:04,000
if you're familiar with machine learning

402
00:14:04,000 --> 00:14:06,000
you know this is not the kind of nuance

403
00:14:06,000 --> 00:14:08,560
technical solutions can detect i

404
00:14:08,560 --> 00:14:09,680
challenge you

405
00:14:09,680 --> 00:14:11,760
to make me an ai that can distinguish

406
00:14:11,760 --> 00:14:13,360
between q anons and snowden's thoughts

407
00:14:13,360 --> 00:14:16,079
about five eyes good luck with that

408
00:14:16,079 --> 00:14:18,160
part of the problem is those a priori

409
00:14:18,160 --> 00:14:19,440
assumptions that we are bringing to the

410
00:14:19,440 --> 00:14:20,880
table

411
00:14:20,880 --> 00:14:23,440
as janelle shane demonstrates amply on

412
00:14:23,440 --> 00:14:25,519
her ai weirdness blog

413
00:14:25,519 --> 00:14:27,199
if you tell your ai it's looking for

414
00:14:27,199 --> 00:14:28,880
coveted lungs it will find them in

415
00:14:28,880 --> 00:14:30,639
stuffed giraffes

416
00:14:30,639 --> 00:14:32,800
so tell your ai that it has to find

417
00:14:32,800 --> 00:14:34,800
misinformation or it has to find facts

418
00:14:34,800 --> 00:14:36,240
and it will certainly try to please you

419
00:14:36,240 --> 00:14:37,760
if none of the above is actually in the

420
00:14:37,760 --> 00:14:40,079
picture

421
00:14:40,079 --> 00:14:42,480
tell me you clever developers how

422
00:14:42,480 --> 00:14:44,639
exactly do you build the human

423
00:14:44,639 --> 00:14:46,160
understanding of trust into your

424
00:14:46,160 --> 00:14:48,639
algorithm what indicators does it rely

425
00:14:48,639 --> 00:14:49,760
on

426
00:14:49,760 --> 00:14:51,519
retweets don't equal endorsement or

427
00:14:51,519 --> 00:14:54,079
trust neither are connections those of

428
00:14:54,079 --> 00:14:55,839
us on the security side of things know

429
00:14:55,839 --> 00:14:57,680
bad actors social engineer people into

430
00:14:57,680 --> 00:14:59,680
adding them as linkedin contacts all the

431
00:14:59,680 --> 00:15:01,920
time

432
00:15:02,560 --> 00:15:05,519
it's time to admit misinformation is not

433
00:15:05,519 --> 00:15:08,399
a problem we can solve with an algorithm

434
00:15:08,399 --> 00:15:10,639
it's a social problem

435
00:15:10,639 --> 00:15:12,880
it's a social problem that tech company

436
00:15:12,880 --> 00:15:17,439
business models have brought into being

437
00:15:17,519 --> 00:15:19,920
from web 2.0 onwards scalable online

438
00:15:19,920 --> 00:15:22,639
business models have always explicitly

439
00:15:22,639 --> 00:15:24,240
been built on the elimination of

440
00:15:24,240 --> 00:15:28,399
professionals trained to vet information

441
00:15:28,399 --> 00:15:30,000
craigslist and google started chipping

442
00:15:30,000 --> 00:15:31,839
away at the ad revenue of news at the

443
00:15:31,839 --> 00:15:33,440
turn of the century

444
00:15:33,440 --> 00:15:35,519
but the real damage was done in the year

445
00:15:35,519 --> 00:15:38,560
facebook introduced its news feed

446
00:15:38,560 --> 00:15:40,240
guess who gets laid off first when news

447
00:15:40,240 --> 00:15:41,519
organizations have to tighten their

448
00:15:41,519 --> 00:15:42,639
belts

449
00:15:42,639 --> 00:15:44,560
hint it's not reporters

450
00:15:44,560 --> 00:15:47,519
it's fact checkers

451
00:15:47,519 --> 00:15:49,199
the tech industry considers eliminating

452
00:15:49,199 --> 00:15:51,519
editors a feature not a bug

453
00:15:51,519 --> 00:15:53,360
as dave weiner one of the developers of

454
00:15:53,360 --> 00:15:55,759
rss summed up well

455
00:15:55,759 --> 00:15:57,120
he was excited that on the internet

456
00:15:57,120 --> 00:15:59,120
sources will go direct and prove that

457
00:15:59,120 --> 00:16:01,120
publishers editors and fact checkers do

458
00:16:01,120 --> 00:16:02,959
unnecessary work

459
00:16:02,959 --> 00:16:04,720
editing can sometimes improve the

460
00:16:04,720 --> 00:16:07,040
effectiveness of writing heroed

461
00:16:07,040 --> 00:16:09,279
and we see where that turned out

462
00:16:09,279 --> 00:16:11,040
let's also not forget that google's page

463
00:16:11,040 --> 00:16:13,120
rank was created to rank scientific

464
00:16:13,120 --> 00:16:16,240
citations for that matter

465
00:16:16,399 --> 00:16:18,160
team social media has effectively

466
00:16:18,160 --> 00:16:20,320
flattened teams journalism and science

467
00:16:20,320 --> 00:16:22,399
when it comes to fighting for facts and

468
00:16:22,399 --> 00:16:24,800
in light of this it's very very

469
00:16:24,800 --> 00:16:27,120
interesting that meta is pouring money

470
00:16:27,120 --> 00:16:29,680
into supporting third party third-party

471
00:16:29,680 --> 00:16:33,839
fact checkers all over the globe

472
00:16:34,639 --> 00:16:35,600
so

473
00:16:35,600 --> 00:16:37,120
i've dismantled your best hopes about

474
00:16:37,120 --> 00:16:39,279
information both good and miss facts and

475
00:16:39,279 --> 00:16:40,160
trust

476
00:16:40,160 --> 00:16:41,920
thanks you ivory tower jerk you may be

477
00:16:41,920 --> 00:16:43,360
saying that's cute and all but how the

478
00:16:43,360 --> 00:16:46,000
hell am i supposed to apply this

479
00:16:46,000 --> 00:16:47,920
uh chris pointed out earlier to me that

480
00:16:47,920 --> 00:16:50,079
most of the recommendations i'm about to

481
00:16:50,079 --> 00:16:51,839
make track with the aspen institute

482
00:16:51,839 --> 00:16:53,920
commission on information disorder i was

483
00:16:53,920 --> 00:16:55,360
not involved in producing that report

484
00:16:55,360 --> 00:16:56,639
and i do disagree with it on a few

485
00:16:56,639 --> 00:17:00,079
points but i refer you to it

486
00:17:00,079 --> 00:17:02,079
first i'd suggest that when investing in

487
00:17:02,079 --> 00:17:03,440
fact checking

488
00:17:03,440 --> 00:17:05,280
support solutions that are based in

489
00:17:05,280 --> 00:17:07,439
local institutions that people already

490
00:17:07,439 --> 00:17:08,559
trust

491
00:17:08,559 --> 00:17:10,160
local solutions are more likely to

492
00:17:10,160 --> 00:17:11,679
succeed

493
00:17:11,679 --> 00:17:13,119
random fact-checking outlets that are

494
00:17:13,119 --> 00:17:14,799
floating onto the scene on moored from

495
00:17:14,799 --> 00:17:16,880
local context by contrast are not going

496
00:17:16,880 --> 00:17:18,480
to have an impact

497
00:17:18,480 --> 00:17:20,640
and do note that local news institutions

498
00:17:20,640 --> 00:17:23,280
that people trust is a rocky field to

499
00:17:23,280 --> 00:17:24,880
navigate i've been doing it for about 20

500
00:17:24,880 --> 00:17:25,839
years

501
00:17:25,839 --> 00:17:27,599
good luck identifying local political

502
00:17:27,599 --> 00:17:29,600
fault lines or news outlets that may or

503
00:17:29,600 --> 00:17:31,200
may not be in the pocket of corrupt

504
00:17:31,200 --> 00:17:33,840
dictators

505
00:17:34,320 --> 00:17:35,600
if you're going to have fact checkers

506
00:17:35,600 --> 00:17:38,000
why not hire editors or curators

507
00:17:38,000 --> 00:17:39,679
only having fact checkers is fighting a

508
00:17:39,679 --> 00:17:42,559
rear guard and after the fact battle

509
00:17:42,559 --> 00:17:44,240
at the very least learn from older

510
00:17:44,240 --> 00:17:46,559
projects this is not our first content

511
00:17:46,559 --> 00:17:48,400
moderation goat rodeo

512
00:17:48,400 --> 00:17:50,000
we shouldn't invent trust and safety

513
00:17:50,000 --> 00:17:51,919
from whole cloth

514
00:17:51,919 --> 00:17:53,360
learn from veterans who have been

515
00:17:53,360 --> 00:17:54,799
weeding out conspiracy theories fraud

516
00:17:54,799 --> 00:17:56,559
and flame wars for years at places like

517
00:17:56,559 --> 00:17:58,160
reddit metafilter

518
00:17:58,160 --> 00:17:59,760
what would they put into algorithms what

519
00:17:59,760 --> 00:18:02,720
are their best practices

520
00:18:02,880 --> 00:18:04,960
regarding scalable technical solutions

521
00:18:04,960 --> 00:18:07,120
the public needs to know as chris said

522
00:18:07,120 --> 00:18:09,200
how platforms work before we can build

523
00:18:09,200 --> 00:18:10,480
policy on them

524
00:18:10,480 --> 00:18:11,919
algorithmic transparency and

525
00:18:11,919 --> 00:18:14,240
accountability will be vital tech

526
00:18:14,240 --> 00:18:16,080
companies need to hire qualitative

527
00:18:16,080 --> 00:18:18,000
social researchers as well to help them

528
00:18:18,000 --> 00:18:19,520
think through the implications of

529
00:18:19,520 --> 00:18:21,280
algorithms or at the very least

530
00:18:21,280 --> 00:18:23,760
make them available to the public

531
00:18:23,760 --> 00:18:25,679
expose algorithms to more perspectives

532
00:18:25,679 --> 00:18:28,960
hire women people of color queer folks

533
00:18:28,960 --> 00:18:30,240
and people from different class

534
00:18:30,240 --> 00:18:32,880
backgrounds especially and treat them

535
00:18:32,880 --> 00:18:34,880
fairly act on their advice and don't

536
00:18:34,880 --> 00:18:36,400
fire them for a change it hurts your

537
00:18:36,400 --> 00:18:37,600
bottom line

538
00:18:37,600 --> 00:18:39,600
if it hurts your bottom line that profit

539
00:18:39,600 --> 00:18:42,799
was not yours to begin with

540
00:18:42,799 --> 00:18:45,679
another modest swiftian proposal from me

541
00:18:45,679 --> 00:18:47,760
algorithms should not promote strong

542
00:18:47,760 --> 00:18:48,880
emotions

543
00:18:48,880 --> 00:18:51,840
any strong emotions positive or negative

544
00:18:51,840 --> 00:18:53,120
we learn from facebook's motion

545
00:18:53,120 --> 00:18:54,720
experiment that algorithms can drive us

546
00:18:54,720 --> 00:18:57,360
to extremes we must require them to do

547
00:18:57,360 --> 00:18:59,440
the opposite make social media news

548
00:18:59,440 --> 00:19:01,600
feeds unsticky again even if it hurts

549
00:19:01,600 --> 00:19:03,679
the bottom line

550
00:19:03,679 --> 00:19:04,880
did we actually miss the boat on

551
00:19:04,880 --> 00:19:07,360
saturational news in 1958 when edward r

552
00:19:07,360 --> 00:19:09,120
murrow had beef with commercializing tv

553
00:19:09,120 --> 00:19:11,200
news is this a multi-industry problem

554
00:19:11,200 --> 00:19:13,200
yes absolutely but that's a topic for

555
00:19:13,200 --> 00:19:15,200
another talk if you don't want to take

556
00:19:15,200 --> 00:19:17,120
my swiftine advice and eat the juicy

557
00:19:17,120 --> 00:19:18,559
babies of your business model there's

558
00:19:18,559 --> 00:19:20,160
always another way to scale the fight

559
00:19:20,160 --> 00:19:22,640
against misinformation

560
00:19:22,640 --> 00:19:24,480
we can argue that feed algorithms serve

561
00:19:24,480 --> 00:19:26,640
the same function as editors

562
00:19:26,640 --> 00:19:29,120
without them we drown in garbage section

563
00:19:29,120 --> 00:19:31,919
230 should not apply to algorithm

564
00:19:31,919 --> 00:19:33,679
curated social media

565
00:19:33,679 --> 00:19:35,600
the model profits from prioritizing

566
00:19:35,600 --> 00:19:38,640
certain information with harmful effects

567
00:19:38,640 --> 00:19:40,080
hold meta and anyone else with an

568
00:19:40,080 --> 00:19:43,440
algorithmic feed liable

569
00:19:43,440 --> 00:19:45,120
for that matter section 230 shouldn't

570
00:19:45,120 --> 00:19:46,799
apply to a company that is pouring money

571
00:19:46,799 --> 00:19:48,320
into fact checking and pretending it's

572
00:19:48,320 --> 00:19:50,840
somehow not a news

573
00:19:50,840 --> 00:19:53,280
outlet personally

574
00:19:53,280 --> 00:19:54,799
i hope to fight misinformation on a

575
00:19:54,799 --> 00:19:56,799
human scale i'm currently proposing a

576
00:19:56,799 --> 00:19:59,120
design research project to build trust

577
00:19:59,120 --> 00:20:00,799
on a community level and i'm hoping to

578
00:20:00,799 --> 00:20:01,840
convene

579
00:20:01,840 --> 00:20:04,000
conflict resolution specialists trauma

580
00:20:04,000 --> 00:20:05,840
experts cult member reprogrammers and

581
00:20:05,840 --> 00:20:07,840
gang interrupters to understand their

582
00:20:07,840 --> 00:20:09,280
best practices

583
00:20:09,280 --> 00:20:11,440
if you also personally want to join the

584
00:20:11,440 --> 00:20:13,120
fight at a human scale

585
00:20:13,120 --> 00:20:15,360
public schools are your local team

586
00:20:15,360 --> 00:20:17,440
empirical science fan club

587
00:20:17,440 --> 00:20:19,039
support them

588
00:20:19,039 --> 00:20:20,799
fund public schools so they can attract

589
00:20:20,799 --> 00:20:22,000
great teachers

590
00:20:22,000 --> 00:20:24,000
and by that i mean teachers who welcome

591
00:20:24,000 --> 00:20:26,480
kids born to all teams regardless of

592
00:20:26,480 --> 00:20:27,840
social class

593
00:20:27,840 --> 00:20:29,840
and gently encourage them towards the

594
00:20:29,840 --> 00:20:32,000
facts

595
00:20:32,000 --> 00:20:33,840
it sucks it's kafka-esque but richard

596
00:20:33,840 --> 00:20:35,679
feynman did it and so can you show up

597
00:20:35,679 --> 00:20:36,960
for those school board meetings that are

598
00:20:36,960 --> 00:20:39,200
melting down right now be a calming

599
00:20:39,200 --> 00:20:40,320
presence

600
00:20:40,320 --> 00:20:42,159
be on the textbook book review for

601
00:20:42,159 --> 00:20:44,720
boards like i said it sucks but feynman

602
00:20:44,720 --> 00:20:47,200
did it and you can too

603
00:20:47,200 --> 00:20:49,290
there's my talk thanks

604
00:20:49,290 --> 00:20:53,869
[Applause]

