1
00:00:07,759 --> 00:00:09,679
so the title of my talk is crypto for

2
00:00:09,679 --> 00:00:11,440
the people part 2 and this is a

3
00:00:11,440 --> 00:00:13,679
follow-up uh to a keynote i gave at

4
00:00:13,679 --> 00:00:14,719
crypto

5
00:00:14,719 --> 00:00:15,519
in

6
00:00:15,519 --> 00:00:17,199
2020

7
00:00:17,199 --> 00:00:19,359
so the original talk was motivated by

8
00:00:19,359 --> 00:00:21,279
protests uh that were going on at the

9
00:00:21,279 --> 00:00:22,320
time

10
00:00:22,320 --> 00:00:25,199
um due to the killings of george floyd

11
00:00:25,199 --> 00:00:28,320
briana taylor and ahmad albert

12
00:00:28,320 --> 00:00:30,480
and um

13
00:00:30,480 --> 00:00:31,920
the talk was really a critique about

14
00:00:31,920 --> 00:00:34,079
computer science as a whole but

15
00:00:34,079 --> 00:00:36,160
cryptography in particular and one of

16
00:00:36,160 --> 00:00:39,040
the points that i tried to make was that

17
00:00:39,040 --> 00:00:41,840
while it was clear to me that industry

18
00:00:41,840 --> 00:00:45,440
government and um venture capitalists

19
00:00:45,440 --> 00:00:47,680
benefit from our work

20
00:00:47,680 --> 00:00:49,600
including including my work

21
00:00:49,600 --> 00:00:52,239
what was less clear to me is whether

22
00:00:52,239 --> 00:00:53,760
regular people and in particular people

23
00:00:53,760 --> 00:00:55,920
from marginalized communities benefit

24
00:00:55,920 --> 00:00:58,320
from our work

25
00:00:58,320 --> 00:01:01,199
and so in the talk i i gave some

26
00:01:01,199 --> 00:01:04,239
examples of projects that i felt were in

27
00:01:04,239 --> 00:01:06,320
line with this right or projects that i

28
00:01:06,320 --> 00:01:07,200
felt

29
00:01:07,200 --> 00:01:08,400
benefited

30
00:01:08,400 --> 00:01:11,200
people from marginalized communities

31
00:01:11,200 --> 00:01:12,960
but i also thought it would be important

32
00:01:12,960 --> 00:01:14,560
to give examples of projects that i

33
00:01:14,560 --> 00:01:16,640
don't feel achieve this right and one of

34
00:01:16,640 --> 00:01:19,439
the examples that i gave

35
00:01:19,439 --> 00:01:21,840
um was a project of the forum you know

36
00:01:21,840 --> 00:01:24,080
like this blockchain is going to solve

37
00:01:24,080 --> 00:01:26,159
long-standing developmental issues and

38
00:01:26,159 --> 00:01:27,920
unlock much-needed economic growth in

39
00:01:27,920 --> 00:01:30,000
rural communities in africa right and

40
00:01:30,000 --> 00:01:31,680
this happens to be a real project

41
00:01:31,680 --> 00:01:33,759
actually like i didn't i didn't make it

42
00:01:33,759 --> 00:01:34,400
up

43
00:01:34,400 --> 00:01:35,439
um

44
00:01:35,439 --> 00:01:36,479
so

45
00:01:36,479 --> 00:01:38,079
yeah so so so

46
00:01:38,079 --> 00:01:41,200
i gave the talk and i got a um a lot of

47
00:01:41,200 --> 00:01:43,680
email after the talk which was great

48
00:01:43,680 --> 00:01:45,840
that started a lot of really interesting

49
00:01:45,840 --> 00:01:48,320
conversations um

50
00:01:48,320 --> 00:01:49,920
with a lot of folks

51
00:01:49,920 --> 00:01:53,040
but i also got emails

52
00:01:53,040 --> 00:01:56,320
about projects that i didn't find um

53
00:01:56,320 --> 00:01:57,520
you know

54
00:01:57,520 --> 00:02:00,079
as great and one example was was a a

55
00:02:00,079 --> 00:02:01,200
project

56
00:02:01,200 --> 00:02:02,399
uh where somebody was trying to explain

57
00:02:02,399 --> 00:02:06,159
to me how blockchain could be used to

58
00:02:06,159 --> 00:02:08,160
tackle the problem of police violence

59
00:02:08,160 --> 00:02:08,959
right

60
00:02:08,959 --> 00:02:12,800
and um you know and this sort of like

61
00:02:12,800 --> 00:02:16,480
you know was confusing because it was

62
00:02:16,480 --> 00:02:18,879
almost exactly what i said i didn't want

63
00:02:18,879 --> 00:02:20,840
to hear about

64
00:02:20,840 --> 00:02:22,959
um but

65
00:02:22,959 --> 00:02:24,400
you know of course like the way that

66
00:02:24,400 --> 00:02:27,120
this happens is that you know if if you

67
00:02:27,120 --> 00:02:28,080
take like

68
00:02:28,080 --> 00:02:30,000
imagine your favorite photographer bob

69
00:02:30,000 --> 00:02:32,640
right and bob likes fancy cryptography

70
00:02:32,640 --> 00:02:34,720
so bob likes blockchain and like

71
00:02:34,720 --> 00:02:36,160
encrypted search which is the area that

72
00:02:36,160 --> 00:02:38,560
i work in in distinguishable obfuscation

73
00:02:38,560 --> 00:02:40,879
and polymorphic encryption right so bob

74
00:02:40,879 --> 00:02:43,120
likes these these fancy

75
00:02:43,120 --> 00:02:44,879
you know these fancy crypto technologies

76
00:02:44,879 --> 00:02:46,319
but of course bob also notices that

77
00:02:46,319 --> 00:02:47,840
people are talking about police violence

78
00:02:47,840 --> 00:02:49,680
and like sexual assault and bias and

79
00:02:49,680 --> 00:02:51,120
discrimination

80
00:02:51,120 --> 00:02:53,040
you know misinformation

81
00:02:53,040 --> 00:02:54,160
and so

82
00:02:54,160 --> 00:02:55,040
right

83
00:02:55,040 --> 00:02:56,879
bob puts two and two together and thinks

84
00:02:56,879 --> 00:02:58,480
hey like i know what i'll do i'll use

85
00:02:58,480 --> 00:03:00,159
cryptography to solve police violence

86
00:03:00,159 --> 00:03:01,360
right

87
00:03:01,360 --> 00:03:03,599
um and like

88
00:03:03,599 --> 00:03:04,800
you know so

89
00:03:04,800 --> 00:03:06,400
so why is this a problem right this is a

90
00:03:06,400 --> 00:03:07,599
problem because bob isn't really

91
00:03:07,599 --> 00:03:09,519
interested in the social problem bob is

92
00:03:09,519 --> 00:03:11,120
interested in the crypto problem that he

93
00:03:11,120 --> 00:03:12,720
claims is motivated by this social

94
00:03:12,720 --> 00:03:13,920
problem

95
00:03:13,920 --> 00:03:16,239
um and well you know while the the

96
00:03:16,239 --> 00:03:17,840
technical problem well the crypto

97
00:03:17,840 --> 00:03:19,840
problem is maybe fun right it doesn't

98
00:03:19,840 --> 00:03:22,800
really address the actual social issue

99
00:03:22,800 --> 00:03:23,599
right

100
00:03:23,599 --> 00:03:25,200
um and so essentially what's happening

101
00:03:25,200 --> 00:03:26,959
is that bob has a hammer and he's

102
00:03:26,959 --> 00:03:29,280
looking for a social nail right and of

103
00:03:29,280 --> 00:03:30,879
course like bob isn't the only person

104
00:03:30,879 --> 00:03:32,720
doing this companies do this all the

105
00:03:32,720 --> 00:03:34,000
time

106
00:03:34,000 --> 00:03:35,440
so uh

107
00:03:35,440 --> 00:03:37,360
this is the the front page of the dm

108
00:03:37,360 --> 00:03:39,840
project so if you remember facebook uh

109
00:03:39,840 --> 00:03:41,440
started a cryptocurrency called libra a

110
00:03:41,440 --> 00:03:43,360
few years ago and this is like some

111
00:03:43,360 --> 00:03:45,440
version or iteration of

112
00:03:45,440 --> 00:03:46,720
of that project

113
00:03:46,720 --> 00:03:47,760
and

114
00:03:47,760 --> 00:03:48,799
you know

115
00:03:48,799 --> 00:03:50,480
when i saw this i was really surprised

116
00:03:50,480 --> 00:03:52,080
because you know presumably this is a

117
00:03:52,080 --> 00:03:54,959
woman from west africa and you know my

118
00:03:54,959 --> 00:03:57,840
my family is from west africa

119
00:03:57,840 --> 00:04:00,720
and i never knew that facebook was

120
00:04:00,720 --> 00:04:02,480
so interested in

121
00:04:02,480 --> 00:04:04,560
and or cared so much about west africa

122
00:04:04,560 --> 00:04:06,640
about its people its cultures

123
00:04:06,640 --> 00:04:07,680
um

124
00:04:07,680 --> 00:04:09,439
and its and its economies but apparently

125
00:04:09,439 --> 00:04:10,799
it is right

126
00:04:10,799 --> 00:04:14,080
and um of course i think most of us can

127
00:04:14,080 --> 00:04:15,519
see through this right most of us

128
00:04:15,519 --> 00:04:17,120
understand that like the libra project

129
00:04:17,120 --> 00:04:18,880
was probably not

130
00:04:18,880 --> 00:04:20,000
started

131
00:04:20,000 --> 00:04:21,759
like the reason that facebook started

132
00:04:21,759 --> 00:04:24,400
the libra project probably wasn't

133
00:04:24,400 --> 00:04:26,560
uh because of developing nations right

134
00:04:26,560 --> 00:04:28,639
like that was in the original motivation

135
00:04:28,639 --> 00:04:30,880
um but of course you know when you're

136
00:04:30,880 --> 00:04:32,720
trying to sell something that people

137
00:04:32,720 --> 00:04:34,400
don't want and that people view as a

138
00:04:34,400 --> 00:04:35,919
power grab

139
00:04:35,919 --> 00:04:37,120
then

140
00:04:37,120 --> 00:04:39,520
using marginalized groups and and

141
00:04:39,520 --> 00:04:42,400
developing nations as branding

142
00:04:42,400 --> 00:04:43,680
is a great way

143
00:04:43,680 --> 00:04:46,080
to seem um

144
00:04:46,080 --> 00:04:49,280
sincere right and to seem altruistic

145
00:04:49,280 --> 00:04:50,560
right

146
00:04:50,560 --> 00:04:51,840
so

147
00:04:51,840 --> 00:04:55,280
you know and this is basically

148
00:04:55,280 --> 00:04:57,199
what's happening right so like it's

149
00:04:57,199 --> 00:04:59,360
great to want to address social problems

150
00:04:59,360 --> 00:05:01,120
but often what people are actually doing

151
00:05:01,120 --> 00:05:02,400
is they're using marginalized

152
00:05:02,400 --> 00:05:04,000
communities where they're using

153
00:05:04,000 --> 00:05:05,840
social problems as branding right and

154
00:05:05,840 --> 00:05:06,960
this is why we get all of these

155
00:05:06,960 --> 00:05:09,280
narratives in tech like cryptocurrencies

156
00:05:09,280 --> 00:05:11,199
for developing countries and fintech for

157
00:05:11,199 --> 00:05:12,880
the unbanked and

158
00:05:12,880 --> 00:05:15,280
ai teachers and chatbots for poor and

159
00:05:15,280 --> 00:05:17,039
underserved areas right so instead of

160
00:05:17,039 --> 00:05:19,280
giving people actually

161
00:05:19,280 --> 00:05:21,520
more teachers right we're gonna just

162
00:05:21,520 --> 00:05:22,639
like

163
00:05:22,639 --> 00:05:26,160
have them educated by a chatbot

164
00:05:26,240 --> 00:05:28,000
so

165
00:05:28,000 --> 00:05:30,639
you know this is sort of what's

166
00:05:30,639 --> 00:05:32,960
this is pretty common um

167
00:05:32,960 --> 00:05:34,960
so what should bob do right if bob

168
00:05:34,960 --> 00:05:37,280
generally cares about a social problem

169
00:05:37,280 --> 00:05:39,919
then in my opinion bob should work with

170
00:05:39,919 --> 00:05:42,240
experts on that problem right

171
00:05:42,240 --> 00:05:43,520
this means experts in the social

172
00:05:43,520 --> 00:05:45,600
sciences and the humanities who have uh

173
00:05:45,600 --> 00:05:47,520
the technical kind of background to

174
00:05:47,520 --> 00:05:48,800
understand

175
00:05:48,800 --> 00:05:51,520
um the problem in its in its entirety

176
00:05:51,520 --> 00:05:52,400
but also

177
00:05:52,400 --> 00:05:54,240
you should work with experts who have

178
00:05:54,240 --> 00:05:55,840
lived experience right people who have

179
00:05:55,840 --> 00:05:57,440
actually been affected by these problems

180
00:05:57,440 --> 00:05:59,360
and who have experienced them

181
00:05:59,360 --> 00:06:01,039
and this is really really important

182
00:06:01,039 --> 00:06:02,560
right like and

183
00:06:02,560 --> 00:06:04,800
you know if if if you've ever

184
00:06:04,800 --> 00:06:07,120
been to something traumatic for example

185
00:06:07,120 --> 00:06:08,400
right

186
00:06:08,400 --> 00:06:09,840
you'll probably

187
00:06:09,840 --> 00:06:11,199
like what you probably notice is that

188
00:06:11,199 --> 00:06:12,800
there's a lot of details and other

189
00:06:12,800 --> 00:06:14,479
nuances

190
00:06:14,479 --> 00:06:16,800
that you never would have predicted and

191
00:06:16,800 --> 00:06:18,080
that you never would have anticipated

192
00:06:18,080 --> 00:06:19,520
that people who haven't been through it

193
00:06:19,520 --> 00:06:22,080
just don't know right

194
00:06:22,080 --> 00:06:24,639
and these little details and nuances are

195
00:06:24,639 --> 00:06:25,919
really really important they really

196
00:06:25,919 --> 00:06:27,759
matter and oftentimes they're actually

197
00:06:27,759 --> 00:06:28,560
like

198
00:06:28,560 --> 00:06:30,000
unfortunately they can be part of the

199
00:06:30,000 --> 00:06:32,479
worst part of that experience

200
00:06:32,479 --> 00:06:34,400
and so

201
00:06:34,400 --> 00:06:35,919
so people so working with people with

202
00:06:35,919 --> 00:06:37,440
experience is crucial because they know

203
00:06:37,440 --> 00:06:39,360
these details right the details that

204
00:06:39,360 --> 00:06:41,680
matter the nuances that matter they

205
00:06:41,680 --> 00:06:43,759
understand the psychological state of

206
00:06:43,759 --> 00:06:45,520
the people that are involved right in

207
00:06:45,520 --> 00:06:46,319
these

208
00:06:46,319 --> 00:06:47,039
like

209
00:06:47,039 --> 00:06:48,560
that are dealing with this they

210
00:06:48,560 --> 00:06:50,639
understand the broader context as well

211
00:06:50,639 --> 00:06:51,759
right

212
00:06:51,759 --> 00:06:54,000
so this is very very important

213
00:06:54,000 --> 00:06:55,039
um

214
00:06:55,039 --> 00:06:57,280
so okay so so the number one thing or

215
00:06:57,280 --> 00:06:58,720
the first thing that bob should do is

216
00:06:58,720 --> 00:07:00,319
bob should be collaborating right there

217
00:07:00,319 --> 00:07:03,520
should be some kind of teamwork um

218
00:07:03,520 --> 00:07:04,639
happening

219
00:07:04,639 --> 00:07:06,319
with experts and those experts are the

220
00:07:06,319 --> 00:07:07,599
ones that are going to know so both

221
00:07:07,599 --> 00:07:09,039
academic and

222
00:07:09,039 --> 00:07:11,039
uh experts with lived experience and

223
00:07:11,039 --> 00:07:12,560
those experts are going to know which

224
00:07:12,560 --> 00:07:14,080
assumptions make sense and which don't

225
00:07:14,080 --> 00:07:15,840
right they're going to know of the real

226
00:07:15,840 --> 00:07:18,319
world constraints that bob just can't

227
00:07:18,319 --> 00:07:19,680
really

228
00:07:19,680 --> 00:07:21,280
fathom right they're going to understand

229
00:07:21,280 --> 00:07:22,479
the human and the psychological

230
00:07:22,479 --> 00:07:23,599
dimensions of the problem they're going

231
00:07:23,599 --> 00:07:25,120
to know which risks are tolerable and

232
00:07:25,120 --> 00:07:26,639
which risks aren't

233
00:07:26,639 --> 00:07:27,919
and crucially they're also going to be

234
00:07:27,919 --> 00:07:29,919
able to see potential harms that bob

235
00:07:29,919 --> 00:07:31,759
isn't able to see or bob isn't equipped

236
00:07:31,759 --> 00:07:33,199
to see

237
00:07:33,199 --> 00:07:34,479
okay

238
00:07:34,479 --> 00:07:35,520
so

239
00:07:35,520 --> 00:07:37,840
um so yeah so bob should work with

240
00:07:37,840 --> 00:07:38,880
experts

241
00:07:38,880 --> 00:07:41,840
but also it's also important that bob

242
00:07:41,840 --> 00:07:44,240
focuses on designing what the experts

243
00:07:44,240 --> 00:07:46,080
believe is useful not what he thinks is

244
00:07:46,080 --> 00:07:48,639
useful okay and that should be the case

245
00:07:48,639 --> 00:07:50,720
even if the technology or the crypto in

246
00:07:50,720 --> 00:07:53,199
this case is boring right the point of

247
00:07:53,199 --> 00:07:56,000
this the point of this work isn't

248
00:07:56,000 --> 00:07:58,240
like technical innovation right that's

249
00:07:58,240 --> 00:07:59,599
not the goal that's not the point the

250
00:07:59,599 --> 00:08:02,160
point is hopefully to

251
00:08:02,160 --> 00:08:03,759
play some part

252
00:08:03,759 --> 00:08:05,199
in addressing

253
00:08:05,199 --> 00:08:07,440
a problem right that affects people

254
00:08:07,440 --> 00:08:10,639
right like that's the goal

255
00:08:10,800 --> 00:08:13,759
so um okay now i want to talk about a

256
00:08:13,759 --> 00:08:15,039
little bit about a project that we did

257
00:08:15,039 --> 00:08:18,639
at brown um that was uh related to to a

258
00:08:18,639 --> 00:08:20,400
to a social problem

259
00:08:20,400 --> 00:08:22,160
and use that to discuss some of the

260
00:08:22,160 --> 00:08:24,840
challenges in doing this kind of

261
00:08:24,840 --> 00:08:28,639
work so unfortunately gun violence is a

262
00:08:28,639 --> 00:08:29,919
problem

263
00:08:29,919 --> 00:08:31,759
that affects

264
00:08:31,759 --> 00:08:34,240
everybody

265
00:08:35,279 --> 00:08:38,799
affects people all around the world and

266
00:08:38,799 --> 00:08:41,519
it affects americans um as well right so

267
00:08:41,519 --> 00:08:44,000
36 000 americans are killed by guns

268
00:08:44,000 --> 00:08:47,120
every year 100 000 are injured um 600

269
00:08:47,120 --> 00:08:48,399
women are shot and killed by intimate

270
00:08:48,399 --> 00:08:50,080
partner by an intimate partner every

271
00:08:50,080 --> 00:08:52,080
year four and a half million women are

272
00:08:52,080 --> 00:08:53,839
threatened with a gun every year black

273
00:08:53,839 --> 00:08:55,279
people are 10 times more likely to be

274
00:08:55,279 --> 00:08:57,519
killed with a gun and

275
00:08:57,519 --> 00:09:00,959
black men account for 52 of gun debts

276
00:09:00,959 --> 00:09:01,920
okay

277
00:09:01,920 --> 00:09:03,760
and we've also seen how there's just you

278
00:09:03,760 --> 00:09:05,680
know there's been an epidemic of mass

279
00:09:05,680 --> 00:09:07,920
shootings in the u.s and these occur

280
00:09:07,920 --> 00:09:09,440
everywhere right they've happened at

281
00:09:09,440 --> 00:09:11,760
movie theaters and concerts universities

282
00:09:11,760 --> 00:09:13,600
elementary schools

283
00:09:13,600 --> 00:09:14,480
and

284
00:09:14,480 --> 00:09:16,399
grocery stores right

285
00:09:16,399 --> 00:09:18,160
um

286
00:09:18,160 --> 00:09:19,760
now the problem of gun violence is a

287
00:09:19,760 --> 00:09:22,320
really complicated one right like i

288
00:09:22,320 --> 00:09:23,600
obviously that's not going to be solved

289
00:09:23,600 --> 00:09:25,440
by technology that that's just that

290
00:09:25,440 --> 00:09:26,640
would be absurd

291
00:09:26,640 --> 00:09:29,120
right the like gun violence is a huge

292
00:09:29,120 --> 00:09:31,040
hugely complex issues

293
00:09:31,040 --> 00:09:32,160
um

294
00:09:32,160 --> 00:09:33,360
and

295
00:09:33,360 --> 00:09:35,360
and one of the things that make it one

296
00:09:35,360 --> 00:09:37,040
of the many many many things that make

297
00:09:37,040 --> 00:09:38,880
it so complicated

298
00:09:38,880 --> 00:09:40,560
is gun control laws and so i'm not going

299
00:09:40,560 --> 00:09:42,560
to go over gun control laws in the u.s

300
00:09:42,560 --> 00:09:44,399
because i just don't have time but i

301
00:09:44,399 --> 00:09:47,519
will just um highlight the firearm owner

302
00:09:47,519 --> 00:09:50,399
protection act of 1986

303
00:09:50,399 --> 00:09:51,920
which one of the things that it does is

304
00:09:51,920 --> 00:09:53,440
that it prohibits the federal government

305
00:09:53,440 --> 00:09:55,360
and state governments from requiring gun

306
00:09:55,360 --> 00:09:58,560
registration okay

307
00:09:59,600 --> 00:10:01,680
so okay so how did this project start so

308
00:10:01,680 --> 00:10:04,079
basically in early 2019 senator wyden

309
00:10:04,079 --> 00:10:05,279
staff

310
00:10:05,279 --> 00:10:07,920
reached out to us and um

311
00:10:07,920 --> 00:10:10,000
they were working on a bill

312
00:10:10,000 --> 00:10:11,360
that would

313
00:10:11,360 --> 00:10:13,760
allow for

314
00:10:13,760 --> 00:10:16,959
a form of of some approximation of a

315
00:10:16,959 --> 00:10:18,800
national gun registry

316
00:10:18,800 --> 00:10:19,600
but

317
00:10:19,600 --> 00:10:21,440
it would be a voluntary system and it

318
00:10:21,440 --> 00:10:22,959
would be completely decentralized and

319
00:10:22,959 --> 00:10:24,079
the fact that it's voluntary and

320
00:10:24,079 --> 00:10:25,519
decentralized

321
00:10:25,519 --> 00:10:27,600
is very important here right there's a

322
00:10:27,600 --> 00:10:29,760
lot of like legal design happening in

323
00:10:29,760 --> 00:10:31,440
this bill

324
00:10:31,440 --> 00:10:33,040
and so they were working with very like

325
00:10:33,040 --> 00:10:34,800
specific constraints

326
00:10:34,800 --> 00:10:36,560
and so they came to us

327
00:10:36,560 --> 00:10:38,000
because one of the things that was

328
00:10:38,000 --> 00:10:40,480
important for for what they had in mind

329
00:10:40,480 --> 00:10:43,200
was that these uh that the the

330
00:10:43,200 --> 00:10:45,440
registrations would be stored in

331
00:10:45,440 --> 00:10:47,839
database that were antenna encrypted and

332
00:10:47,839 --> 00:10:50,160
controlled by a local official right and

333
00:10:50,160 --> 00:10:51,279
that in particular the federal

334
00:10:51,279 --> 00:10:52,480
government and state governments would

335
00:10:52,480 --> 00:10:53,920
never be able to see this data because

336
00:10:53,920 --> 00:10:56,320
it was encrypted

337
00:10:56,320 --> 00:10:57,600
um

338
00:10:57,600 --> 00:10:58,640
and another

339
00:10:58,640 --> 00:11:01,360
important constraints was that the local

340
00:11:01,360 --> 00:11:02,720
officials should be able to pull their

341
00:11:02,720 --> 00:11:04,959
data at any time right

342
00:11:04,959 --> 00:11:06,079
and so like

343
00:11:06,079 --> 00:11:08,480
these two constraints here that i

344
00:11:08,480 --> 00:11:10,800
highlighted with the the red the red

345
00:11:10,800 --> 00:11:12,000
symbol

346
00:11:12,000 --> 00:11:15,040
these are like subtle things that we

347
00:11:15,040 --> 00:11:16,640
would have had no idea about right and

348
00:11:16,640 --> 00:11:19,600
these were part and the only reason

349
00:11:19,600 --> 00:11:21,200
um why we knew about him is because

350
00:11:21,200 --> 00:11:23,360
these were like very very specific

351
00:11:23,360 --> 00:11:26,480
things that the senator's staff put in

352
00:11:26,480 --> 00:11:29,040
the design right in their legal sort of

353
00:11:29,040 --> 00:11:30,480
design

354
00:11:30,480 --> 00:11:32,480
um

355
00:11:32,480 --> 00:11:34,399
so after talking with them what we did

356
00:11:34,399 --> 00:11:36,480
is you know we took the legislation and

357
00:11:36,480 --> 00:11:38,720
we we generated a set of technical

358
00:11:38,720 --> 00:11:40,320
requirements from that and then we

359
00:11:40,320 --> 00:11:42,240
designed a cryptographic protocol to

360
00:11:42,240 --> 00:11:45,440
support um what what they needed or what

361
00:11:45,440 --> 00:11:46,720
they were asking for and then we

362
00:11:46,720 --> 00:11:48,800
implemented a prototype

363
00:11:48,800 --> 00:11:50,800
and we evaluated it and you know i'm not

364
00:11:50,800 --> 00:11:52,480
going to bore you with the technical

365
00:11:52,480 --> 00:11:54,160
details of the project you can read

366
00:11:54,160 --> 00:11:55,200
about them in the paper if you're

367
00:11:55,200 --> 00:11:57,040
interested but the point i do want to

368
00:11:57,040 --> 00:11:58,160
make is that

369
00:11:58,160 --> 00:12:00,160
at the end you know what we were able to

370
00:12:00,160 --> 00:12:02,240
demonstrate is that it was possible to

371
00:12:02,240 --> 00:12:04,399
design such a thing and that it was

372
00:12:04,399 --> 00:12:06,720
possible to run at the scale of the us

373
00:12:06,720 --> 00:12:08,480
right and another question that they had

374
00:12:08,480 --> 00:12:09,680
is how much would something like this

375
00:12:09,680 --> 00:12:11,360
cost roughly right and so we were able

376
00:12:11,360 --> 00:12:12,399
to estimate that it would cost about a

377
00:12:12,399 --> 00:12:15,440
hundred thousand dollars a year

378
00:12:15,519 --> 00:12:17,200
so um

379
00:12:17,200 --> 00:12:18,160
right so one of the things i want to

380
00:12:18,160 --> 00:12:19,200
highlight about this project is that

381
00:12:19,200 --> 00:12:21,200
this really was a collaboration right

382
00:12:21,200 --> 00:12:22,720
like we worked with the senator staff

383
00:12:22,720 --> 00:12:25,040
from beginning to end right and really

384
00:12:25,040 --> 00:12:27,519
like the design of you know the

385
00:12:27,519 --> 00:12:29,279
the technical requirements were really

386
00:12:29,279 --> 00:12:30,880
derived from the legal requirements

387
00:12:30,880 --> 00:12:32,959
which were designed by senator white and

388
00:12:32,959 --> 00:12:35,360
staff right so everything was based on

389
00:12:35,360 --> 00:12:37,279
the legislation and we worked according

390
00:12:37,279 --> 00:12:38,560
to their constraints

391
00:12:38,560 --> 00:12:41,200
and we prioritized their needs and the

392
00:12:41,200 --> 00:12:42,880
trade-offs that they felt were

393
00:12:42,880 --> 00:12:45,760
reasonable right and anytime we made an

394
00:12:45,760 --> 00:12:47,839
assumption we went back to them and had

395
00:12:47,839 --> 00:12:49,680
them vet the assumption right and this

396
00:12:49,680 --> 00:12:51,519
is sort of the style of work or the way

397
00:12:51,519 --> 00:12:52,800
of collaborating that i think is

398
00:12:52,800 --> 00:12:56,319
necessary for these kinds of projects

399
00:12:56,399 --> 00:13:00,639
okay so next i want to talk about um

400
00:13:00,639 --> 00:13:02,639
some of the challenges involved

401
00:13:02,639 --> 00:13:04,880
in this work so

402
00:13:04,880 --> 00:13:06,399
there are several challenges

403
00:13:06,399 --> 00:13:07,680
and one of them is the peer review

404
00:13:07,680 --> 00:13:09,440
process there's just no way around this

405
00:13:09,440 --> 00:13:10,320
right

406
00:13:10,320 --> 00:13:11,200
um

407
00:13:11,200 --> 00:13:13,600
so one of the reviews that we got is is

408
00:13:13,600 --> 00:13:15,760
the phone so it said the use of npc npc

409
00:13:15,760 --> 00:13:17,200
is a particular crypto technology that

410
00:13:17,200 --> 00:13:19,920
we used so the use of npc demanded by

411
00:13:19,920 --> 00:13:21,200
the functionality of the system as a

412
00:13:21,200 --> 00:13:22,959
whole is not very novel the author is

413
00:13:22,959 --> 00:13:24,560
using crypto databases in a rather black

414
00:13:24,560 --> 00:13:26,800
box way i wonder if there is some scope

415
00:13:26,800 --> 00:13:28,720
to significantly strengthen the paper to

416
00:13:28,720 --> 00:13:30,240
consider an hour implementation of some

417
00:13:30,240 --> 00:13:32,240
of these primitives that are scoped only

418
00:13:32,240 --> 00:13:34,800
to this particular problem right so the

419
00:13:34,800 --> 00:13:36,800
main keywords here are like lack of

420
00:13:36,800 --> 00:13:37,920
novelty

421
00:13:37,920 --> 00:13:40,399
uh the fact that we reuse other work or

422
00:13:40,399 --> 00:13:42,240
that we reuse crypto databases in a

423
00:13:42,240 --> 00:13:44,079
black box way right et cetera et cetera

424
00:13:44,079 --> 00:13:45,839
now if you've ever submitted a paper for

425
00:13:45,839 --> 00:13:47,120
publication you get reviews you know

426
00:13:47,120 --> 00:13:48,399
that this is like

427
00:13:48,399 --> 00:13:49,839
every

428
00:13:49,839 --> 00:13:52,079
paper gets this review roughly right

429
00:13:52,079 --> 00:13:55,760
like nothing is novel to reviewers

430
00:13:55,760 --> 00:13:58,480
but obviously we disagree right we felt

431
00:13:58,480 --> 00:14:00,880
like there was a ton of novelty in the

432
00:14:00,880 --> 00:14:02,639
technical novelty in the paper but it

433
00:14:02,639 --> 00:14:03,839
doesn't even matter right like that

434
00:14:03,839 --> 00:14:05,760
wasn't even the point like even if there

435
00:14:05,760 --> 00:14:06,639
was

436
00:14:06,639 --> 00:14:07,519
like

437
00:14:07,519 --> 00:14:09,519
no novelty technical novelty in the

438
00:14:09,519 --> 00:14:11,519
paper like who cares right the goal of

439
00:14:11,519 --> 00:14:12,800
the paper

440
00:14:12,800 --> 00:14:16,000
was to use our expertise to solve a new

441
00:14:16,000 --> 00:14:18,720
technical problem motivated by a social

442
00:14:18,720 --> 00:14:21,600
problem right that experts asked us to

443
00:14:21,600 --> 00:14:23,680
solve under the constraints that they

444
00:14:23,680 --> 00:14:25,600
laid out and based on assumptions that

445
00:14:25,600 --> 00:14:27,440
they vetted right

446
00:14:27,440 --> 00:14:28,959
the goal of the project is not to

447
00:14:28,959 --> 00:14:30,800
impress reviewer number two or anybody

448
00:14:30,800 --> 00:14:32,399
else with how smart we are

449
00:14:32,399 --> 00:14:36,000
right so this is sort of an important

450
00:14:36,000 --> 00:14:37,199
distinction

451
00:14:37,199 --> 00:14:38,880
um but if you're gonna do this kind of

452
00:14:38,880 --> 00:14:40,079
work unfortunately you're just gonna

453
00:14:40,079 --> 00:14:42,000
have to deal with this kind of review um

454
00:14:42,000 --> 00:14:44,639
but you know hopefully with time in time

455
00:14:44,639 --> 00:14:47,839
reviewers will be able to distinguish um

456
00:14:47,839 --> 00:14:49,519
you know

457
00:14:49,519 --> 00:14:52,079
what's important right

458
00:14:52,079 --> 00:14:54,000
so another issue was related to

459
00:14:54,000 --> 00:14:55,680
incentives so

460
00:14:55,680 --> 00:14:58,399
this project had no funding right like

461
00:14:58,399 --> 00:15:00,800
senators don't have funding uh for

462
00:15:00,800 --> 00:15:02,000
research

463
00:15:02,000 --> 00:15:03,920
and um but this was actually a

464
00:15:03,920 --> 00:15:05,120
relatively large team there was one

465
00:15:05,120 --> 00:15:06,800
associate professor which was myself a

466
00:15:06,800 --> 00:15:08,639
post-doc a phd student two master's

467
00:15:08,639 --> 00:15:10,240
students and it took about two years

468
00:15:10,240 --> 00:15:11,279
right

469
00:15:11,279 --> 00:15:13,120
um and the traditional funding sources

470
00:15:13,120 --> 00:15:14,800
you know aren't going to fund this work

471
00:15:14,800 --> 00:15:16,320
like the national science foundation

472
00:15:16,320 --> 00:15:18,240
right the turnaround time for the nsf is

473
00:15:18,240 --> 00:15:19,760
much larger than what we had we had to

474
00:15:19,760 --> 00:15:21,680
start working on the project right away

475
00:15:21,680 --> 00:15:23,360
and honestly like you know

476
00:15:23,360 --> 00:15:26,720
nsf funding like a few people get it and

477
00:15:26,720 --> 00:15:28,079
it's usually focused on kind of more

478
00:15:28,079 --> 00:15:30,639
trendy problems in trendy areas right

479
00:15:30,639 --> 00:15:32,800
darpa and iorpa funding is focused on

480
00:15:32,800 --> 00:15:34,240
government and military needs and

481
00:15:34,240 --> 00:15:35,920
industry funding is focused on industry

482
00:15:35,920 --> 00:15:37,839
needs and trends right

483
00:15:37,839 --> 00:15:40,480
so this is a real challenge because like

484
00:15:40,480 --> 00:15:42,000
you know you may not know this but if

485
00:15:42,000 --> 00:15:43,759
you're a faculty member you don't get

486
00:15:43,759 --> 00:15:45,360
paid for 12 months out of the year you

487
00:15:45,360 --> 00:15:47,440
get paid a nine month salary that's it

488
00:15:47,440 --> 00:15:49,440
for the for the rest of your salary you

489
00:15:49,440 --> 00:15:51,680
need to get funding right for you to

490
00:15:51,680 --> 00:15:53,680
receive your

491
00:15:53,680 --> 00:15:55,839
the remainder of your salary and you

492
00:15:55,839 --> 00:15:57,040
need that funding also to support your

493
00:15:57,040 --> 00:16:00,639
phd students um and postdocs and so if

494
00:16:00,639 --> 00:16:01,759
you're going to take on one of these

495
00:16:01,759 --> 00:16:04,160
projects which has no funding and you're

496
00:16:04,160 --> 00:16:05,920
going to spend two years with a full

497
00:16:05,920 --> 00:16:08,560
team on it right you like you're going

498
00:16:08,560 --> 00:16:10,399
to take a hit right your salary your

499
00:16:10,399 --> 00:16:12,800
personal salary is going to take a hit

500
00:16:12,800 --> 00:16:14,240
um

501
00:16:14,240 --> 00:16:16,240
and that's just unfortunately that's

502
00:16:16,240 --> 00:16:18,480
part of the this

503
00:16:18,480 --> 00:16:20,000
part of the current system right unless

504
00:16:20,000 --> 00:16:22,160
we have better funding sources for this

505
00:16:22,160 --> 00:16:24,160
kind of research which

506
00:16:24,160 --> 00:16:26,560
uh which we need

507
00:16:26,560 --> 00:16:29,279
so another issue is that when we started

508
00:16:29,279 --> 00:16:31,040
the project we

509
00:16:31,040 --> 00:16:32,720
knew or we you know assumed that the

510
00:16:32,720 --> 00:16:34,000
protocol on the system that we were

511
00:16:34,000 --> 00:16:35,360
going to design and build will likely

512
00:16:35,360 --> 00:16:37,040
never be used right the idea of a

513
00:16:37,040 --> 00:16:38,560
national guard registry is a political

514
00:16:38,560 --> 00:16:41,040
third rail in the u.s we knew that the

515
00:16:41,040 --> 00:16:43,120
legislation may not might

516
00:16:43,120 --> 00:16:45,279
not even ever come out and we knew that

517
00:16:45,279 --> 00:16:47,519
the timing of the legislation and also

518
00:16:47,519 --> 00:16:49,040
the timing

519
00:16:49,040 --> 00:16:50,480
of our work when we were going to be

520
00:16:50,480 --> 00:16:52,079
able to talk about it

521
00:16:52,079 --> 00:16:53,360
um

522
00:16:53,360 --> 00:16:54,639
was going to be subject to the political

523
00:16:54,639 --> 00:16:56,480
landscape right it's going to be it was

524
00:16:56,480 --> 00:16:58,240
going to be dependent on what was going

525
00:16:58,240 --> 00:16:59,839
on right like

526
00:16:59,839 --> 00:17:01,600
in the news cycles what was going to be

527
00:17:01,600 --> 00:17:03,440
going on politically all of those things

528
00:17:03,440 --> 00:17:05,520
or all those things could have an impact

529
00:17:05,520 --> 00:17:08,160
on whether our research would ever would

530
00:17:08,160 --> 00:17:09,760
ever come out

531
00:17:09,760 --> 00:17:11,280
we also knew there was

532
00:17:11,280 --> 00:17:13,520
basically no opportunity to claim real

533
00:17:13,520 --> 00:17:15,119
world impact which for people who are

534
00:17:15,119 --> 00:17:17,520
doing applied work is is important right

535
00:17:17,520 --> 00:17:18,880
it helps for tenure and it helps with

536
00:17:18,880 --> 00:17:21,119
funding

537
00:17:21,119 --> 00:17:21,839
so

538
00:17:21,839 --> 00:17:25,039
it was clear to us that um our work

539
00:17:25,039 --> 00:17:26,000
would have

540
00:17:26,000 --> 00:17:27,520
basically no industry impact no

541
00:17:27,520 --> 00:17:30,000
financial impact and would it would be

542
00:17:30,000 --> 00:17:32,160
very unlikely to have any practical real

543
00:17:32,160 --> 00:17:33,280
world impact right that it would

544
00:17:33,280 --> 00:17:35,600
actually be running

545
00:17:35,600 --> 00:17:38,000
um so why did we do it right we still

546
00:17:38,000 --> 00:17:40,080
felt it was worthwhile because

547
00:17:40,080 --> 00:17:41,840
our hope is that it would have baby

548
00:17:41,840 --> 00:17:43,760
policy impact right like we know that

549
00:17:43,760 --> 00:17:44,720
gun violence is one of the most

550
00:17:44,720 --> 00:17:46,640
important social problems in the us that

551
00:17:46,640 --> 00:17:47,679
gun control is one of the most

552
00:17:47,679 --> 00:17:50,160
intractable policy problems in the u.s

553
00:17:50,160 --> 00:17:51,919
and the privacy of gun owners is a

554
00:17:51,919 --> 00:17:54,080
crucial element of this debate and so

555
00:17:54,080 --> 00:17:55,679
our hope is that maybe our work in some

556
00:17:55,679 --> 00:17:57,200
small part would help remove this

557
00:17:57,200 --> 00:17:58,880
concern and then maybe help change the

558
00:17:58,880 --> 00:18:01,200
debate even a little bit right but the

559
00:18:01,200 --> 00:18:02,960
reality is that like even though this

560
00:18:02,960 --> 00:18:04,799
was important to us

561
00:18:04,799 --> 00:18:06,960
right there was really no

562
00:18:06,960 --> 00:18:09,760
like there weren't really any like

563
00:18:09,760 --> 00:18:11,840
practical incentives to do it

564
00:18:11,840 --> 00:18:13,360
right not the way that the current

565
00:18:13,360 --> 00:18:15,039
incentives are structured

566
00:18:15,039 --> 00:18:17,919
for um for for

567
00:18:17,919 --> 00:18:19,200
professors

568
00:18:19,200 --> 00:18:22,400
phd students and postdocs

569
00:18:22,400 --> 00:18:23,840
okay so the last thing i want to talk

570
00:18:23,840 --> 00:18:25,520
about is education because i do think

571
00:18:25,520 --> 00:18:28,799
that it plays its part here so

572
00:18:28,799 --> 00:18:30,559
one of the things that i hope is clear

573
00:18:30,559 --> 00:18:32,559
is that you know to most people most

574
00:18:32,559 --> 00:18:34,320
people would agree with is that since

575
00:18:34,320 --> 00:18:36,720
the 2000s or the 2010s technology is

576
00:18:36,720 --> 00:18:39,520
impacting people directly right

577
00:18:39,520 --> 00:18:40,960
things like of course machine learning

578
00:18:40,960 --> 00:18:43,120
and automated decision making social

579
00:18:43,120 --> 00:18:44,480
networks the fact that all our

580
00:18:44,480 --> 00:18:46,000
communications are digital the erosion

581
00:18:46,000 --> 00:18:47,600
of privacy et cetera et cetera right

582
00:18:47,600 --> 00:18:49,760
clearly technology is impacting people's

583
00:18:49,760 --> 00:18:50,880
lives like

584
00:18:50,880 --> 00:18:53,280
directly okay this is very different

585
00:18:53,280 --> 00:18:55,200
than what happened in the past right up

586
00:18:55,200 --> 00:18:58,000
until the 90s or maybe the early 2000s

587
00:18:58,000 --> 00:18:59,919
which i'd probably say the 90s

588
00:18:59,919 --> 00:19:01,280
right it was

589
00:19:01,280 --> 00:19:03,120
the the general thought was that

590
00:19:03,120 --> 00:19:04,880
technology and computer science didn't

591
00:19:04,880 --> 00:19:06,720
really impact people right it had some

592
00:19:06,720 --> 00:19:09,200
impact but it was an indirect impact

593
00:19:09,200 --> 00:19:11,679
right and so because of this computer

594
00:19:11,679 --> 00:19:14,799
science education right was designed

595
00:19:14,799 --> 00:19:16,559
really to or i wouldn't say it was

596
00:19:16,559 --> 00:19:18,080
designed but most computer science

597
00:19:18,080 --> 00:19:20,559
curriculums right completely ignored

598
00:19:20,559 --> 00:19:22,480
people right most computer science

599
00:19:22,480 --> 00:19:24,720
curriculums were just like

600
00:19:24,720 --> 00:19:29,280
90 percent or 100 stem right um

601
00:19:29,280 --> 00:19:30,880
and that was it right and if you were

602
00:19:30,880 --> 00:19:32,799
educated in the 90s like i was right

603
00:19:32,799 --> 00:19:33,919
like

604
00:19:33,919 --> 00:19:36,080
hopefully you you you

605
00:19:36,080 --> 00:19:38,799
this you agree to this right

606
00:19:38,799 --> 00:19:40,160
um

607
00:19:40,160 --> 00:19:41,520
and so the computer science education

608
00:19:41,520 --> 00:19:43,280
that we currently have right it values

609
00:19:43,280 --> 00:19:45,120
stem over everything else it prices

610
00:19:45,120 --> 00:19:47,760
technical prowess over critical thinking

611
00:19:47,760 --> 00:19:49,679
and it trains students exclusively to

612
00:19:49,679 --> 00:19:52,160
solve quantitative problems right but

613
00:19:52,160 --> 00:19:54,080
the problem is that social problems are

614
00:19:54,080 --> 00:19:55,840
not well posed problems right they don't

615
00:19:55,840 --> 00:19:58,160
have an optimal solution and they span

616
00:19:58,160 --> 00:20:00,799
many fields have nothing to do with stem

617
00:20:00,799 --> 00:20:01,919
and

618
00:20:01,919 --> 00:20:04,400
in particular like stem education

619
00:20:04,400 --> 00:20:06,960
right it teaches you how to solve

620
00:20:06,960 --> 00:20:08,240
technical problems it teaches you how to

621
00:20:08,240 --> 00:20:10,960
solve well-posed problems right on the

622
00:20:10,960 --> 00:20:13,520
other hand educate like social science

623
00:20:13,520 --> 00:20:15,840
and and humanities classes

624
00:20:15,840 --> 00:20:17,440
they teach you how to solve and how to

625
00:20:17,440 --> 00:20:19,039
think about fuzzy problems right

626
00:20:19,039 --> 00:20:20,320
problems that don't necessarily have a

627
00:20:20,320 --> 00:20:21,840
solution problems of a more

628
00:20:21,840 --> 00:20:24,159
philosophical nature right and this is a

629
00:20:24,159 --> 00:20:26,559
really crucial and important skill set

630
00:20:26,559 --> 00:20:28,799
to have

631
00:20:28,799 --> 00:20:30,159
and so if we look at like the

632
00:20:30,159 --> 00:20:31,840
traditional the classical computer

633
00:20:31,840 --> 00:20:33,440
science student right the one that like

634
00:20:33,440 --> 00:20:36,640
we've put on a pedestal like since

635
00:20:36,640 --> 00:20:38,640
forever right it's the student that is

636
00:20:38,640 --> 00:20:40,640
going it takes like programming

637
00:20:40,640 --> 00:20:43,200
operating systems compilers networking

638
00:20:43,200 --> 00:20:45,360
machine learning ai and computer vision

639
00:20:45,360 --> 00:20:46,880
right it takes electrical engineering

640
00:20:46,880 --> 00:20:48,559
and probability theory and learning

641
00:20:48,559 --> 00:20:50,799
algebra and complexity theory

642
00:20:50,799 --> 00:20:53,360
and algorithms bio chemistry et cetera

643
00:20:53,360 --> 00:20:54,559
et cetera right

644
00:20:54,559 --> 00:20:56,080
and that built their own operating

645
00:20:56,080 --> 00:20:57,600
system like by the time they were a

646
00:20:57,600 --> 00:21:00,559
third year undergrad

647
00:21:01,520 --> 00:21:04,480
the okay is that like this like as i

648
00:21:04,480 --> 00:21:06,559
said computer science and technology

649
00:21:06,559 --> 00:21:08,400
right have undercon i've undergone a

650
00:21:08,400 --> 00:21:09,679
massive shift where now they're

651
00:21:09,679 --> 00:21:12,000
impacting people directly and so the

652
00:21:12,000 --> 00:21:14,080
students that are being produced with

653
00:21:14,080 --> 00:21:16,320
this sort of classical view of computer

654
00:21:16,320 --> 00:21:18,080
science are not really equipped to

655
00:21:18,080 --> 00:21:20,400
tackle the new problems that are coming

656
00:21:20,400 --> 00:21:21,360
up now

657
00:21:21,360 --> 00:21:23,600
and so we need a different kind of

658
00:21:23,600 --> 00:21:26,559
student right we need a student that has

659
00:21:26,559 --> 00:21:27,440
like

660
00:21:27,440 --> 00:21:29,520
you know in addition to the stem skills

661
00:21:29,520 --> 00:21:31,760
also has really like

662
00:21:31,760 --> 00:21:33,679
real critical thinking skills

663
00:21:33,679 --> 00:21:34,720
right

664
00:21:34,720 --> 00:21:36,320
so these are just some examples of

665
00:21:36,320 --> 00:21:38,159
classes that are offered at brown that i

666
00:21:38,159 --> 00:21:40,400
think are actually relevant right to a

667
00:21:40,400 --> 00:21:42,159
computer science student

668
00:21:42,159 --> 00:21:44,720
um that is graduating today right

669
00:21:44,720 --> 00:21:46,640
classes like ethnographic research

670
00:21:46,640 --> 00:21:48,000
methods introduction to social

671
00:21:48,000 --> 00:21:49,840
psychology methods of social research

672
00:21:49,840 --> 00:21:52,320
history of capitalism right our students

673
00:21:52,320 --> 00:21:54,320
are are graduating and going into

674
00:21:54,320 --> 00:21:56,159
industry right they should understand

675
00:21:56,159 --> 00:21:58,400
like how capitalism works and what's and

676
00:21:58,400 --> 00:22:00,799
what's going on right like modern

677
00:22:00,799 --> 00:22:02,559
genocide right we know that like for

678
00:22:02,559 --> 00:22:05,039
example facebook has played its part in

679
00:22:05,039 --> 00:22:08,000
the genocide in myanmar right

680
00:22:08,000 --> 00:22:10,880
um from freud to q anon is another class

681
00:22:10,880 --> 00:22:13,280
right we we know that social networks

682
00:22:13,280 --> 00:22:15,440
and technology are you know increasing

683
00:22:15,440 --> 00:22:17,440
misinformation right

684
00:22:17,440 --> 00:22:19,600
so these are just some examples

685
00:22:19,600 --> 00:22:21,440
um but i do think it's really important

686
00:22:21,440 --> 00:22:23,360
right i think it's crucial that the

687
00:22:23,360 --> 00:22:25,679
students that we produce

688
00:22:25,679 --> 00:22:28,080
start to have a more well-rounded and

689
00:22:28,080 --> 00:22:30,720
balanced education like beyond stem and

690
00:22:30,720 --> 00:22:33,280
i think if we're able to do that then i

691
00:22:33,280 --> 00:22:34,799
think a lot of the projects a lot of

692
00:22:34,799 --> 00:22:35,760
this

693
00:22:35,760 --> 00:22:37,440
a lot of the projects

694
00:22:37,440 --> 00:22:40,559
um that are socially relevant right that

695
00:22:40,559 --> 00:22:42,640
that we try to tackle

696
00:22:42,640 --> 00:22:44,720
um you know i'm not necessarily saying

697
00:22:44,720 --> 00:22:46,159
that like we'll be able to solve those

698
00:22:46,159 --> 00:22:48,559
problems but i think we'll have

699
00:22:48,559 --> 00:22:50,080
technologists and computer scientists

700
00:22:50,080 --> 00:22:52,880
that are better equipped to help

701
00:22:52,880 --> 00:22:54,880
solve at least some small

702
00:22:54,880 --> 00:22:56,960
fraction of those problems

703
00:22:56,960 --> 00:22:57,840
okay

704
00:22:57,840 --> 00:23:00,960
um and yeah that's pretty much all i had

705
00:23:00,960 --> 00:23:01,679
so

706
00:23:01,679 --> 00:23:04,919
thank you

707
00:23:11,440 --> 00:23:13,520
you

