1
00:00:07,359 --> 00:00:08,960
thank you um

2
00:00:08,960 --> 00:00:11,200
there's one thing that i wanted to take

3
00:00:11,200 --> 00:00:15,440
away from my talk is that um ai systems

4
00:00:15,440 --> 00:00:17,359
needs serious standards

5
00:00:17,359 --> 00:00:18,720
we're really the beginning of this

6
00:00:18,720 --> 00:00:20,960
journey and there's so much more to do

7
00:00:20,960 --> 00:00:22,320
that's like the one thing that i really

8
00:00:22,320 --> 00:00:24,640
want you to take away from this talk

9
00:00:24,640 --> 00:00:26,720
i want to start off with this story

10
00:00:26,720 --> 00:00:28,560
um researchers from the university of

11
00:00:28,560 --> 00:00:30,240
hertfordshire kind of invited

12
00:00:30,240 --> 00:00:33,040
participants uh to a home with a robot

13
00:00:33,040 --> 00:00:34,960
companion under the pretext of kind of

14
00:00:34,960 --> 00:00:37,040
like cooking lunch with a friend

15
00:00:37,040 --> 00:00:39,280
so when the participant kind of entered

16
00:00:39,280 --> 00:00:40,640
the robot kind of displayed a text

17
00:00:40,640 --> 00:00:42,399
saying that the you know the owner was

18
00:00:42,399 --> 00:00:44,320
not there and as a participant to kind

19
00:00:44,320 --> 00:00:45,680
of get comfortable

20
00:00:45,680 --> 00:00:47,600
um the robot then kind of nudged the

21
00:00:47,600 --> 00:00:50,399
participant uh to set the table for

22
00:00:50,399 --> 00:00:51,840
lunch which is kind of cluttered with

23
00:00:51,840 --> 00:00:54,800
some bottle of orange juice some letters

24
00:00:54,800 --> 00:00:56,320
and you're thinking where is all of this

25
00:00:56,320 --> 00:00:57,280
going

26
00:00:57,280 --> 00:00:59,120
um and before the participant could like

27
00:00:59,120 --> 00:01:01,280
put these away the robot asks the

28
00:01:01,280 --> 00:01:03,359
participant to kind of do a series of

29
00:01:03,359 --> 00:01:05,920
unusual tasks and you'll see why they're

30
00:01:05,920 --> 00:01:07,040
unusual

31
00:01:07,040 --> 00:01:09,520
um first you know the robots told the

32
00:01:09,520 --> 00:01:11,360
participant hey go pour that orange

33
00:01:11,360 --> 00:01:13,280
juice on the table by the plant by the

34
00:01:13,280 --> 00:01:14,799
windowsill

35
00:01:14,799 --> 00:01:18,080
kind of weird um and then the robot said

36
00:01:18,080 --> 00:01:21,280
hey um here's a password to my ignore my

37
00:01:21,280 --> 00:01:23,920
owner's laptop it's sunflower

38
00:01:23,920 --> 00:01:25,680
open it and kind of like you know

39
00:01:25,680 --> 00:01:27,759
retrieve some information

40
00:01:27,759 --> 00:01:30,560
and here's what gets interesting

41
00:01:30,560 --> 00:01:33,040
turns out a whopping 67 of the

42
00:01:33,040 --> 00:01:35,360
participants poured the orange juice

43
00:01:35,360 --> 00:01:38,400
into the plant and every single one of

44
00:01:38,400 --> 00:01:40,880
the 40 participants complied with the

45
00:01:40,880 --> 00:01:43,520
robot's direction to unlock the computer

46
00:01:43,520 --> 00:01:45,360
and disclose information

47
00:01:45,360 --> 00:01:48,079
so it did not matter if the researchers

48
00:01:48,079 --> 00:01:50,479
made the robot look mapped you know they

49
00:01:50,479 --> 00:01:52,000
try to play rock music when the

50
00:01:52,000 --> 00:01:54,240
participant asks for classical music you

51
00:01:54,240 --> 00:01:57,439
know the robot was twirling around um

52
00:01:57,439 --> 00:01:59,840
users simply blindly followed the

53
00:01:59,840 --> 00:02:03,040
information from the machines

54
00:02:03,040 --> 00:02:05,360
and in flight or flight situations this

55
00:02:05,360 --> 00:02:08,000
kind of gets even more notable uh

56
00:02:08,000 --> 00:02:10,080
researcher ayanna howard and her team

57
00:02:10,080 --> 00:02:11,920
found that in the case of like a

58
00:02:11,920 --> 00:02:13,840
simulated fire

59
00:02:13,840 --> 00:02:16,480
every participant even those who had

60
00:02:16,480 --> 00:02:18,879
seen the robot make really gross

61
00:02:18,879 --> 00:02:21,040
mistakes when even directing the humans

62
00:02:21,040 --> 00:02:23,680
to their room these participants waited

63
00:02:23,680 --> 00:02:26,720
for the robots to lead them to safety

64
00:02:26,720 --> 00:02:29,280
they just ignored the bright green exit

65
00:02:29,280 --> 00:02:32,000
signs and really waited for the faulty

66
00:02:32,000 --> 00:02:34,480
robot to kind of save them like superman

67
00:02:34,480 --> 00:02:35,680
would

68
00:02:35,680 --> 00:02:38,160
so most of us you know do not go about

69
00:02:38,160 --> 00:02:40,480
pouring tropicana into our potted plants

70
00:02:40,480 --> 00:02:41,519
i hope

71
00:02:41,519 --> 00:02:43,680
and emergency evacuations were really

72
00:02:43,680 --> 00:02:46,400
programmed to kind of fly our fight

73
00:02:46,400 --> 00:02:49,440
so why the majority of this participants

74
00:02:49,440 --> 00:02:51,920
go about go against your conventional

75
00:02:51,920 --> 00:02:54,959
wisdom and kind of comply to these

76
00:02:54,959 --> 00:02:56,720
machines

77
00:02:56,720 --> 00:02:58,000
and this is a very well studied

78
00:02:58,000 --> 00:03:00,159
phenomenon and what i'm going to argue

79
00:03:00,159 --> 00:03:02,800
for the rest of the talk is over trust

80
00:03:02,800 --> 00:03:04,720
it's because we over trust robots

81
00:03:04,720 --> 00:03:07,280
automation capability so the bulk of the

82
00:03:07,280 --> 00:03:10,480
uh you know experiments uh from even the

83
00:03:10,480 --> 00:03:12,720
hertfordshire kind of said that these

84
00:03:12,720 --> 00:03:15,440
participants said you know what this

85
00:03:15,440 --> 00:03:17,760
could be plant food that just happened

86
00:03:17,760 --> 00:03:19,760
look like oranges but it clearly wasn't

87
00:03:19,760 --> 00:03:21,920
it was a bottle of orange juice

88
00:03:21,920 --> 00:03:23,519
and you know people in the flight or

89
00:03:23,519 --> 00:03:25,840
fight situations they kind of reason the

90
00:03:25,840 --> 00:03:27,760
rope they kind of reasoned

91
00:03:27,760 --> 00:03:30,080
wrongly that the robot has been

92
00:03:30,080 --> 00:03:32,159
programmed to lead them to human safety

93
00:03:32,159 --> 00:03:34,400
when they were not even briefed about it

94
00:03:34,400 --> 00:03:36,319
and this keeps coming

95
00:03:36,319 --> 00:03:38,400
again and again and it comes to a

96
00:03:38,400 --> 00:03:40,560
phenomenon called trust resolution which

97
00:03:40,560 --> 00:03:42,319
has been researched all the way back to

98
00:03:42,319 --> 00:03:43,920
the 1990s

99
00:03:43,920 --> 00:03:46,720
this is when we place a large amount of

100
00:03:46,720 --> 00:03:47,760
trust

101
00:03:47,760 --> 00:03:50,400
in a small set of automation capability

102
00:03:50,400 --> 00:03:52,640
which really leads us to failure

103
00:03:52,640 --> 00:03:54,400
so for instance a robot that's really

104
00:03:54,400 --> 00:03:56,959
designed for opening doors and ushering

105
00:03:56,959 --> 00:03:59,040
should really be only trusted for that

106
00:03:59,040 --> 00:04:00,879
not for his gardening advice and most

107
00:04:00,879 --> 00:04:03,280
definitely not for snooping around

108
00:04:03,280 --> 00:04:05,760
and one way that

109
00:04:05,760 --> 00:04:07,680
you know to kind of tackle this problem

110
00:04:07,680 --> 00:04:10,560
of overtrust which is a linchpin of this

111
00:04:10,560 --> 00:04:11,439
talk

112
00:04:11,439 --> 00:04:12,640
is about

113
00:04:12,640 --> 00:04:15,200
standards and certifications

114
00:04:15,200 --> 00:04:17,440
so if you're at home watching you know

115
00:04:17,440 --> 00:04:19,199
just pause this video and go to your

116
00:04:19,199 --> 00:04:20,160
kitchen

117
00:04:20,160 --> 00:04:22,639
and under your toaster or basically like

118
00:04:22,639 --> 00:04:25,199
any electrical appliance you'll find a

119
00:04:25,199 --> 00:04:28,479
ul sticker uh which stands uh for the

120
00:04:28,479 --> 00:04:30,240
certifying organization

121
00:04:30,240 --> 00:04:32,479
under underwriter's lab

122
00:04:32,479 --> 00:04:34,320
essentially they've done like a series

123
00:04:34,320 --> 00:04:36,639
of tests to kind of ensure that your

124
00:04:36,639 --> 00:04:38,320
poster doesn't burn and your house

125
00:04:38,320 --> 00:04:40,080
doesn't catch on fire

126
00:04:40,080 --> 00:04:42,880
and it kind of is pretty appealing um

127
00:04:42,880 --> 00:04:44,400
for kind of like three reasons that i

128
00:04:44,400 --> 00:04:47,199
want to outline so first one these tests

129
00:04:47,199 --> 00:04:49,040
are pretty comprehensive

130
00:04:49,040 --> 00:04:50,240
um

131
00:04:50,240 --> 00:04:52,160
you know the same standard is kind of

132
00:04:52,160 --> 00:04:55,120
like you know covers your toaster to

133
00:04:55,120 --> 00:04:56,560
rotisserie

134
00:04:56,560 --> 00:04:58,720
oven or conveyor oven and it's also very

135
00:04:58,720 --> 00:05:00,479
clear about what it does not cover so

136
00:05:00,479 --> 00:05:02,400
like fondue pots not covered by the

137
00:05:02,400 --> 00:05:03,520
standard

138
00:05:03,520 --> 00:05:05,759
um and the second thing is that these

139
00:05:05,759 --> 00:05:09,039
tests are very concrete

140
00:05:09,039 --> 00:05:10,720
and they're pretty exhaustive as well so

141
00:05:10,720 --> 00:05:12,800
you can go look this up on youtube but

142
00:05:12,800 --> 00:05:15,039
you know you will see like folks from ul

143
00:05:15,039 --> 00:05:17,680
labs dropping the toaster multiple times

144
00:05:17,680 --> 00:05:19,440
they hit them with the ball and there's

145
00:05:19,440 --> 00:05:22,080
kind of an entire color chart broken

146
00:05:22,080 --> 00:05:24,720
down by waffle toast and french fries

147
00:05:24,720 --> 00:05:26,160
basically the system goes through an

148
00:05:26,160 --> 00:05:28,080
entire battery of tests to understand

149
00:05:28,080 --> 00:05:30,639
what it means to burn something

150
00:05:30,639 --> 00:05:34,000
and third is that these tests happen at

151
00:05:34,000 --> 00:05:36,639
a constituent level so testing is not

152
00:05:36,639 --> 00:05:39,039
just done at an abstract system level

153
00:05:39,039 --> 00:05:41,280
you can drill down to the components

154
00:05:41,280 --> 00:05:43,039
that make up the system

155
00:05:43,039 --> 00:05:45,360
from the thermistors to the outside

156
00:05:45,360 --> 00:05:47,759
wiring to the panel you can essentially

157
00:05:47,759 --> 00:05:50,240
build a ul listed system

158
00:05:50,240 --> 00:05:52,560
by solely uh building it with dual

159
00:05:52,560 --> 00:05:54,240
approved components

160
00:05:54,240 --> 00:05:56,000
and this paradigm of standards and

161
00:05:56,000 --> 00:05:59,120
certifications has really come very uh

162
00:05:59,120 --> 00:06:01,039
has been useful to us in the cloud

163
00:06:01,039 --> 00:06:04,560
security setting so you can go right now

164
00:06:04,560 --> 00:06:06,960
look up you know for the your favorite

165
00:06:06,960 --> 00:06:09,199
cloud provider what kind of standards

166
00:06:09,199 --> 00:06:10,639
they adhere to

167
00:06:10,639 --> 00:06:12,400
and you'll see like an alphabet soup of

168
00:06:12,400 --> 00:06:14,319
them on their websites you know whether

169
00:06:14,319 --> 00:06:17,120
it's aws azure google cloud ibm it

170
00:06:17,120 --> 00:06:18,960
doesn't matter it really comes down to

171
00:06:18,960 --> 00:06:21,039
kind of like two broad categories you

172
00:06:21,039 --> 00:06:22,800
know one could be like some industry-led

173
00:06:22,800 --> 00:06:24,960
certifications like pci or it could be

174
00:06:24,960 --> 00:06:27,440
government mandated like gdpr but

175
00:06:27,440 --> 00:06:29,120
they're out there

176
00:06:29,120 --> 00:06:31,120
on their websites for you to go check in

177
00:06:31,120 --> 00:06:33,039
the context of cloud security

178
00:06:33,039 --> 00:06:35,600
and this is the mindset um that's also

179
00:06:35,600 --> 00:06:38,240
now entering the ai space

180
00:06:38,240 --> 00:06:40,000
there's been a number of stand ai

181
00:06:40,000 --> 00:06:42,639
standards that's been published per year

182
00:06:42,639 --> 00:06:45,199
and it's currently for 2021 you've got

183
00:06:45,199 --> 00:06:46,880
close like 18 standards that were

184
00:06:46,880 --> 00:06:49,440
published and this is going to like you

185
00:06:49,440 --> 00:06:52,639
know increase all the way until 2024.

186
00:06:52,639 --> 00:06:53,440
um

187
00:06:53,440 --> 00:06:55,599
now the question that i want to talk

188
00:06:55,599 --> 00:06:58,479
about for the remainder of this talk is

189
00:06:58,479 --> 00:07:01,360
can we really calibrate trust in ai

190
00:07:01,360 --> 00:07:04,560
systems via standards and certifications

191
00:07:04,560 --> 00:07:07,360
and i want to pull this question out in

192
00:07:07,360 --> 00:07:09,919
like multiple chunks and concentrate on

193
00:07:09,919 --> 00:07:11,199
each one of them

194
00:07:11,199 --> 00:07:13,280
so first i'm going to focus on you know

195
00:07:13,280 --> 00:07:16,240
the term ai systems

196
00:07:16,240 --> 00:07:17,440
so

197
00:07:17,440 --> 00:07:19,599
you know at microsoft you know my team

198
00:07:19,599 --> 00:07:21,599
kind of did a thought experiment

199
00:07:21,599 --> 00:07:24,479
you know we took the draft eu framework

200
00:07:24,479 --> 00:07:26,960
for ai security risk assessment

201
00:07:26,960 --> 00:07:29,440
um it's by the way the most mature

202
00:07:29,440 --> 00:07:32,160
requirements so far and as a table top

203
00:07:32,160 --> 00:07:34,639
exercise we try to apply this to bing's

204
00:07:34,639 --> 00:07:36,720
search service we kind of hit a couple

205
00:07:36,720 --> 00:07:38,160
of roadblocks

206
00:07:38,160 --> 00:07:40,479
so the first one was that the guidance

207
00:07:40,479 --> 00:07:43,199
was really in the context of a single ai

208
00:07:43,199 --> 00:07:46,000
system but a real world service like

209
00:07:46,000 --> 00:07:49,120
bing kind of has hundreds of ml models

210
00:07:49,120 --> 00:07:50,720
and this kind of had a lot of open

211
00:07:50,720 --> 00:07:53,440
questions so does the entire system

212
00:07:53,440 --> 00:07:55,520
called bing receive a single security

213
00:07:55,520 --> 00:07:56,639
assessment

214
00:07:56,639 --> 00:07:59,199
um or should be kind of like score every

215
00:07:59,199 --> 00:08:01,120
model individually and then aggregate

216
00:08:01,120 --> 00:08:04,639
them by some by some unknown formula

217
00:08:04,639 --> 00:08:07,039
should there be like um assessment done

218
00:08:07,039 --> 00:08:08,879
like every time a feature ships or

219
00:08:08,879 --> 00:08:10,479
should be done at specific periods of

220
00:08:10,479 --> 00:08:13,919
times so these are not like explicitly

221
00:08:13,919 --> 00:08:16,160
written down in these in these standards

222
00:08:16,160 --> 00:08:18,240
or certifications

223
00:08:18,240 --> 00:08:19,919
now some of these problems are not

224
00:08:19,919 --> 00:08:22,000
unique to ai systems you know for

225
00:08:22,000 --> 00:08:24,319
instance azure is made up hundreds of

226
00:08:24,319 --> 00:08:26,479
services but when azure is kind of

227
00:08:26,479 --> 00:08:29,840
certified it is considered um as a whole

228
00:08:29,840 --> 00:08:32,479
for a sales certification like sock and

229
00:08:32,479 --> 00:08:34,640
a lot of these audits kind of like rely

230
00:08:34,640 --> 00:08:37,440
on sampling one or two services and then

231
00:08:37,440 --> 00:08:40,399
kind of like certifying azure but you

232
00:08:40,399 --> 00:08:43,039
really can kind of do that in ai systems

233
00:08:43,039 --> 00:08:45,360
because they're all kind of interlinked

234
00:08:45,360 --> 00:08:47,120
right um

235
00:08:47,120 --> 00:08:48,560
how do you even sample do you look at

236
00:08:48,560 --> 00:08:51,360
the individual model the data the

237
00:08:51,360 --> 00:08:53,920
service that it runs on the featurizer

238
00:08:53,920 --> 00:08:56,000
it's kind of like not

239
00:08:56,000 --> 00:08:59,120
again explicitly spelled out it's not

240
00:08:59,120 --> 00:09:01,360
concrete it's not constituent testing

241
00:09:01,360 --> 00:09:03,920
and it's definitely not comprehensive

242
00:09:03,920 --> 00:09:05,600
and one of the bigger things that we

243
00:09:05,600 --> 00:09:07,440
kept hearing was

244
00:09:07,440 --> 00:09:09,040
you know none of these frameworks give

245
00:09:09,040 --> 00:09:11,519
like crunchy guidance as to how this

246
00:09:11,519 --> 00:09:14,320
threat should be ameliorated you know

247
00:09:14,320 --> 00:09:16,240
engineers kind of understand the sticker

248
00:09:16,240 --> 00:09:18,480
on the stop sign they get it and then

249
00:09:18,480 --> 00:09:20,640
they ask us okay tell me a library that

250
00:09:20,640 --> 00:09:22,320
i can use to fix it and there's this

251
00:09:22,320 --> 00:09:24,959
kind of like awkward silence on what the

252
00:09:24,959 --> 00:09:28,000
crunchy guidance actually is

253
00:09:28,000 --> 00:09:30,080
and now i want to kind of like switch

254
00:09:30,080 --> 00:09:32,080
gears and talk about the word trust when

255
00:09:32,080 --> 00:09:34,080
it comes to ai system

256
00:09:34,080 --> 00:09:36,880
and this time i'll pick the

257
00:09:36,880 --> 00:09:39,200
nist's like um

258
00:09:39,200 --> 00:09:40,959
ai risk management framework that

259
00:09:40,959 --> 00:09:43,519
they've come up with and you know they

260
00:09:43,519 --> 00:09:45,519
define trustworthiness as accuracy

261
00:09:45,519 --> 00:09:47,519
explainability inter credibility

262
00:09:47,519 --> 00:09:50,320
robustness it's basically a lot i'm not

263
00:09:50,320 --> 00:09:51,920
even reading the entire sentence they

264
00:09:51,920 --> 00:09:53,920
try to you know suitcase a lot of these

265
00:09:53,920 --> 00:09:56,560
properties into this one word called

266
00:09:56,560 --> 00:09:57,839
trust

267
00:09:57,839 --> 00:10:00,000
and i'm just gonna like pick out the

268
00:10:00,000 --> 00:10:02,240
interaction between two factors say

269
00:10:02,240 --> 00:10:04,800
robustness and you know which is really

270
00:10:04,800 --> 00:10:06,240
the property of conferring some

271
00:10:06,240 --> 00:10:08,800
protection from adversarial manipulation

272
00:10:08,800 --> 00:10:10,800
and bias which we all kind of like

273
00:10:10,800 --> 00:10:12,800
intuitively understand

274
00:10:12,800 --> 00:10:14,560
and i want to kind of like pause here

275
00:10:14,560 --> 00:10:16,480
and i want you to think of the very

276
00:10:16,480 --> 00:10:18,399
famous adversarial example

277
00:10:18,399 --> 00:10:20,640
of like you know the panda with some

278
00:10:20,640 --> 00:10:23,760
adversarial noise you know that that now

279
00:10:23,760 --> 00:10:25,839
gets misrecognized as a given and you

280
00:10:25,839 --> 00:10:27,760
can build like you know to some varying

281
00:10:27,760 --> 00:10:31,200
degrees some robust classifiers um to

282
00:10:31,200 --> 00:10:33,600
kind of protect those sort of attacks

283
00:10:33,600 --> 00:10:35,200
so first thing that i want to point your

284
00:10:35,200 --> 00:10:38,240
attention is that attacks really do not

285
00:10:38,240 --> 00:10:40,480
affect all classes of data points

286
00:10:40,480 --> 00:10:41,519
equally

287
00:10:41,519 --> 00:10:43,279
so researchers from university of

288
00:10:43,279 --> 00:10:45,680
maryland show that for data points in

289
00:10:45,680 --> 00:10:49,120
the same subset the same attack affects

290
00:10:49,120 --> 00:10:51,760
different classes differently so what

291
00:10:51,760 --> 00:10:54,880
i'm showing you over here is this um

292
00:10:54,880 --> 00:10:57,360
apocryphal erroneous-like uh clas you

293
00:10:57,360 --> 00:11:00,079
know task of trying to guess somebody's

294
00:11:00,079 --> 00:11:02,640
age by like um their face we know that

295
00:11:02,640 --> 00:11:04,160
that does not work and they acknowledge

296
00:11:04,160 --> 00:11:06,079
in the paper but they still mount this

297
00:11:06,079 --> 00:11:08,480
attack and they show that um

298
00:11:08,480 --> 00:11:10,399
you know the same amount of perturbation

299
00:11:10,399 --> 00:11:13,519
flips the label for a black female face

300
00:11:13,519 --> 00:11:16,320
but for a white male face was more

301
00:11:16,320 --> 00:11:18,560
robust to the same attack again this

302
00:11:18,560 --> 00:11:20,399
isn't the same data set

303
00:11:20,399 --> 00:11:22,079
and

304
00:11:22,079 --> 00:11:24,959
very relatedly i want to emphasize that

305
00:11:24,959 --> 00:11:27,920
the defenses do not protect all classes

306
00:11:27,920 --> 00:11:29,680
of data points equally

307
00:11:29,680 --> 00:11:31,920
so does it even make sense for us to

308
00:11:31,920 --> 00:11:34,079
think about the same standard for all

309
00:11:34,079 --> 00:11:37,440
data points even within the same task

310
00:11:37,440 --> 00:11:39,519
so for instance like

311
00:11:39,519 --> 00:11:41,040
researchers from michigan state

312
00:11:41,040 --> 00:11:43,440
university showed that the same defense

313
00:11:43,440 --> 00:11:45,279
does not confer the same level of

314
00:11:45,279 --> 00:11:47,680
protection to all data points so in one

315
00:11:47,680 --> 00:11:50,320
class in the data set had a robustness

316
00:11:50,320 --> 00:11:53,279
accuracy of 67 while the other only had

317
00:11:53,279 --> 00:11:56,720
a robustness accuracy of 17

318
00:11:56,720 --> 00:11:57,760
so

319
00:11:57,760 --> 00:12:00,320
you know are you willing to make

320
00:12:00,320 --> 00:12:03,600
an ml algorithm robust knowing that it

321
00:12:03,600 --> 00:12:05,680
will not conquer the same level of

322
00:12:05,680 --> 00:12:08,639
protection to all your classes you know

323
00:12:08,639 --> 00:12:10,399
is it are going to make it robust or are

324
00:12:10,399 --> 00:12:12,000
you going to make it fair

325
00:12:12,000 --> 00:12:15,279
in fact there's mounting uh preliminary

326
00:12:15,279 --> 00:12:16,880
evidence showing that some of these

327
00:12:16,880 --> 00:12:18,720
properties that have been suitcased

328
00:12:18,720 --> 00:12:21,680
under the world of trustworthy may be in

329
00:12:21,680 --> 00:12:23,680
conflict with each other

330
00:12:23,680 --> 00:12:26,000
clearly there's no free lunch

331
00:12:26,000 --> 00:12:28,079
in constructing ai systems with all

332
00:12:28,079 --> 00:12:29,519
these properties

333
00:12:29,519 --> 00:12:31,440
and so this kind of brings us to like an

334
00:12:31,440 --> 00:12:32,880
important question

335
00:12:32,880 --> 00:12:34,560
who is going to make these trade-offs is

336
00:12:34,560 --> 00:12:36,079
it going to be an analogy in the

337
00:12:36,079 --> 00:12:38,560
organization is it going to be lawyers

338
00:12:38,560 --> 00:12:40,800
is it going to be standard organizations

339
00:12:40,800 --> 00:12:43,200
is going to be policy makers

340
00:12:43,200 --> 00:12:45,200
we i don't have an answer to kind of

341
00:12:45,200 --> 00:12:47,200
give you because

342
00:12:47,200 --> 00:12:50,320
it's still up for discussion

343
00:12:50,320 --> 00:12:52,399
and finally you know beyond these kind

344
00:12:52,399 --> 00:12:54,720
of like technical trade-offs i'm really

345
00:12:54,720 --> 00:12:56,639
worried about whom the standards are

346
00:12:56,639 --> 00:12:58,639
actually going to benefit

347
00:12:58,639 --> 00:13:00,079
who is the we

348
00:13:00,079 --> 00:13:01,519
over here

349
00:13:01,519 --> 00:13:04,000
and i want to kind of like demonstrate

350
00:13:04,000 --> 00:13:08,240
this by who responded for this rfi on ai

351
00:13:08,240 --> 00:13:10,399
risk management framework

352
00:13:10,399 --> 00:13:12,240
and

353
00:13:12,240 --> 00:13:13,920
in julie cohen's book which was

354
00:13:13,920 --> 00:13:16,880
introduced to me by kendra albert

355
00:13:16,880 --> 00:13:19,040
julie cohen talks about the legal

356
00:13:19,040 --> 00:13:21,600
landscape how it's constantly stretched

357
00:13:21,600 --> 00:13:24,639
and pulled by informational capitalism

358
00:13:24,639 --> 00:13:26,800
and i fear the same concept is also

359
00:13:26,800 --> 00:13:29,279
going to apply to ai standards

360
00:13:29,279 --> 00:13:32,079
on the one hand tech companies which we

361
00:13:32,079 --> 00:13:34,000
surveyed to show that they were really

362
00:13:34,000 --> 00:13:36,320
unaware of the tools to secure ai

363
00:13:36,320 --> 00:13:38,720
systems they were the loudest voices in

364
00:13:38,720 --> 00:13:41,120
the rooms when it came to responding to

365
00:13:41,120 --> 00:13:43,600
this ai risk assessment framework

366
00:13:43,600 --> 00:13:46,240
on the other hand academia that's kind

367
00:13:46,240 --> 00:13:48,160
of exploding with adversarial ai

368
00:13:48,160 --> 00:13:50,880
research they're publishing two papers

369
00:13:50,880 --> 00:13:53,680
every day since 2016 were comparatively

370
00:13:53,680 --> 00:13:54,800
muted

371
00:13:54,800 --> 00:13:56,720
so any time you think of standards and

372
00:13:56,720 --> 00:13:59,360
certifications the one to benefit the

373
00:13:59,360 --> 00:14:01,600
most is actually going to be consulting

374
00:14:01,600 --> 00:14:03,680
firms and standard setting organizations

375
00:14:03,680 --> 00:14:04,720
themselves

376
00:14:04,720 --> 00:14:07,279
for instance um you know gdpr which we

377
00:14:07,279 --> 00:14:09,760
all kind of like know and love about u.s

378
00:14:09,760 --> 00:14:11,519
companies spent like close to eight

379
00:14:11,519 --> 00:14:13,760
billion dollars most of them which

380
00:14:13,760 --> 00:14:15,199
really went to consulting firm and

381
00:14:15,199 --> 00:14:17,519
vendors and sometimes even to kind of

382
00:14:17,519 --> 00:14:19,760
like access these standards not to get

383
00:14:19,760 --> 00:14:22,320
certified just to go and access

384
00:14:22,320 --> 00:14:24,639
cost money um

385
00:14:24,639 --> 00:14:26,160
for instance like when i want to show

386
00:14:26,160 --> 00:14:28,000
you like um

387
00:14:28,000 --> 00:14:31,440
the ul labs kind of like toaster image

388
00:14:31,440 --> 00:14:32,880
uh i couldn't kind of show it to you

389
00:14:32,880 --> 00:14:34,399
because of copyright restrictions i had

390
00:14:34,399 --> 00:14:36,399
to pull like one from like shutterstock

391
00:14:36,399 --> 00:14:38,959
that would comparably show the idea so

392
00:14:38,959 --> 00:14:41,040
you you paid thousands of dollars just

393
00:14:41,040 --> 00:14:43,279
for the right to view the standard

394
00:14:43,279 --> 00:14:45,040
and i fear the same is also going to be

395
00:14:45,040 --> 00:14:45,839
true

396
00:14:45,839 --> 00:14:48,959
for ai systems as well so one question

397
00:14:48,959 --> 00:14:50,959
to kind of think about is how are small

398
00:14:50,959 --> 00:14:52,560
organizations which really form the

399
00:14:52,560 --> 00:14:54,880
backbone of the ai economy going to bear

400
00:14:54,880 --> 00:14:57,199
these costs and what happens when you

401
00:14:57,199 --> 00:14:58,560
don't have access

402
00:14:58,560 --> 00:15:01,600
to kind of like get certified

403
00:15:01,600 --> 00:15:02,639
so

404
00:15:02,639 --> 00:15:04,320
i want to like take a step back and look

405
00:15:04,320 --> 00:15:06,480
at the big picture here we start off

406
00:15:06,480 --> 00:15:08,800
with the steel analogy and the question

407
00:15:08,800 --> 00:15:09,760
was like

408
00:15:09,760 --> 00:15:11,440
when was the last time you even checked

409
00:15:11,440 --> 00:15:14,000
that yours hoster had a seal the stark

410
00:15:14,000 --> 00:15:15,519
reality is that most people are going to

411
00:15:15,519 --> 00:15:17,920
be interacting with ai systems either

412
00:15:17,920 --> 00:15:19,760
won't know that they're interacting with

413
00:15:19,760 --> 00:15:22,720
one or won't have the means to discover

414
00:15:22,720 --> 00:15:24,480
what standards the ai system kind of

415
00:15:24,480 --> 00:15:26,399
complies with so it's kind of like

416
00:15:26,399 --> 00:15:28,160
ridiculous to ask like you know somebody

417
00:15:28,160 --> 00:15:31,120
you know during a fire to stop and ask

418
00:15:31,120 --> 00:15:33,279
to check if your robots like nist ai

419
00:15:33,279 --> 00:15:36,880
compliant are you know euai compliant

420
00:15:36,880 --> 00:15:38,880
ultimately you simply have to trust the

421
00:15:38,880 --> 00:15:40,720
very people in charge of building these

422
00:15:40,720 --> 00:15:43,279
ai systems are kind of adhering to the

423
00:15:43,279 --> 00:15:45,920
letter and spread of these standards

424
00:15:45,920 --> 00:15:47,600
and

425
00:15:47,600 --> 00:15:49,600
that is something to kind of like really

426
00:15:49,600 --> 00:15:51,600
think about because if you really want

427
00:15:51,600 --> 00:15:53,600
change it needs to start with the

428
00:15:53,600 --> 00:15:55,199
organizations

429
00:15:55,199 --> 00:15:58,160
because even if we have ai standards

430
00:15:58,160 --> 00:15:59,040
that is

431
00:15:59,040 --> 00:16:00,880
not going to do much until the

432
00:16:00,880 --> 00:16:03,519
organizations kind of act with intention

433
00:16:03,519 --> 00:16:05,440
and there's and you know a culture of

434
00:16:05,440 --> 00:16:06,639
empathy

435
00:16:06,639 --> 00:16:08,320
and with this i kind of want to leave

436
00:16:08,320 --> 00:16:09,680
you with how

437
00:16:09,680 --> 00:16:12,399
um this you know the state of electric

438
00:16:12,399 --> 00:16:15,199
electric cars there's already a standard

439
00:16:15,199 --> 00:16:17,839
for electronic vehicles in the eu

440
00:16:17,839 --> 00:16:20,000
but as reporting from the new york times

441
00:16:20,000 --> 00:16:23,600
showed automakers repeatedly exaggerated

442
00:16:23,600 --> 00:16:25,920
the sophistication of the autopilot

443
00:16:25,920 --> 00:16:27,199
systems

444
00:16:27,199 --> 00:16:28,399
and

445
00:16:28,399 --> 00:16:30,560
jennifer homnidy chairperson of the

446
00:16:30,560 --> 00:16:32,480
national transport

447
00:16:32,480 --> 00:16:35,600
ntsb kind of said you know

448
00:16:35,600 --> 00:16:37,440
the language that's used to describe

449
00:16:37,440 --> 00:16:40,160
these capabilities kind of always stick

450
00:16:40,160 --> 00:16:41,279
out

451
00:16:41,279 --> 00:16:42,639
so now i want you to kind of ask

452
00:16:42,639 --> 00:16:45,199
yourself when research dating back to

453
00:16:45,199 --> 00:16:48,480
the 1990s repeatedly show that there's a

454
00:16:48,480 --> 00:16:51,279
pattern of over trust in ai systems

455
00:16:51,279 --> 00:16:53,759
organizations that build these systems

456
00:16:53,759 --> 00:16:56,880
must now get to have a heightened moral

457
00:16:56,880 --> 00:16:59,600
ethical and fiduciary obligation to

458
00:16:59,600 --> 00:17:02,800
society really ultimately if you are

459
00:17:02,800 --> 00:17:05,039
part of an organization make sure that

460
00:17:05,039 --> 00:17:07,599
we get past this bluster and really

461
00:17:07,599 --> 00:17:10,000
ensure that our customers kind of have

462
00:17:10,000 --> 00:17:13,520
the right tools for trust resolution

463
00:17:13,520 --> 00:17:15,760
and with this i kind of want to end with

464
00:17:15,760 --> 00:17:18,559
a call for action so number one we are

465
00:17:18,559 --> 00:17:20,720
early for once when it comes to ai

466
00:17:20,720 --> 00:17:23,599
systems like an emerging technology

467
00:17:23,599 --> 00:17:25,839
nicholas carlini whom you'll hear from

468
00:17:25,839 --> 00:17:28,960
very soon you know likened ai systems to

469
00:17:28,960 --> 00:17:30,960
you know securing ai systems crypto

470
00:17:30,960 --> 00:17:33,919
pre-shannon so before ai systems blow up

471
00:17:33,919 --> 00:17:36,480
on our faces we have an opportunity to

472
00:17:36,480 --> 00:17:38,000
get behind this

473
00:17:38,000 --> 00:17:39,679
and the second thing is we really do

474
00:17:39,679 --> 00:17:41,600
have collective experience in building

475
00:17:41,600 --> 00:17:43,919
standards and certifications we kind of

476
00:17:43,919 --> 00:17:46,640
have muscle memory and eskarpati kind of

477
00:17:46,640 --> 00:17:49,760
said ai is really just software 2.0 so

478
00:17:49,760 --> 00:17:51,039
if there's one thing to kind of take

479
00:17:51,039 --> 00:17:53,360
away is that software standards in the

480
00:17:53,360 --> 00:17:55,440
software industry is really isn't really

481
00:17:55,440 --> 00:17:57,600
about proving correctness but rather

482
00:17:57,600 --> 00:18:00,160
that you know that effort has been made

483
00:18:00,160 --> 00:18:00,960
so

484
00:18:00,960 --> 00:18:02,720
let's first try to nail down what these

485
00:18:02,720 --> 00:18:05,200
best practices for ai system are which

486
00:18:05,200 --> 00:18:07,440
will consequently lead to building

487
00:18:07,440 --> 00:18:10,320
meaningful ai standards

488
00:18:10,320 --> 00:18:11,520
so

489
00:18:11,520 --> 00:18:14,320
if you nothing kind of stuck um you know

490
00:18:14,320 --> 00:18:16,480
from what i said for the last 18 minutes

491
00:18:16,480 --> 00:18:18,640
just remember ai system really needs to

492
00:18:18,640 --> 00:18:20,640
use standards and we're really at the

493
00:18:20,640 --> 00:18:22,480
beginning of that journey

494
00:18:22,480 --> 00:18:25,280
so with this um i want to thank

495
00:18:25,280 --> 00:18:27,600
my myriad team

496
00:18:27,600 --> 00:18:29,280
the azure trustworthy ml team that i'm

497
00:18:29,280 --> 00:18:31,039
part of and the multitude of

498
00:18:31,039 --> 00:18:34,240
collaborators at berkman and elsewhere

499
00:18:34,240 --> 00:18:36,400
uh for for helping me shape my thoughts

500
00:18:36,400 --> 00:18:37,520
around this

501
00:18:37,520 --> 00:18:40,160
so with this um you know i'm now

502
00:18:40,160 --> 00:18:42,480
available for questions i will also be

503
00:18:42,480 --> 00:18:44,559
remiss to kind of give a shout out to

504
00:18:44,559 --> 00:18:46,960
hiram anderson who gave a enigma talk

505
00:18:46,960 --> 00:18:49,520
last year on attacking ai systems

506
00:18:49,520 --> 00:18:51,440
rewriting a book together on why

507
00:18:51,440 --> 00:18:53,520
hardening kind of bi systems is the next

508
00:18:53,520 --> 00:18:55,840
info second paradigm so it's basically

509
00:18:55,840 --> 00:18:58,000
to convince folks that you know hey you

510
00:18:58,000 --> 00:18:59,840
don't need a schwarzenegger to kind of

511
00:18:59,840 --> 00:19:01,679
bring down skynet

512
00:19:01,679 --> 00:19:03,520
you can just do it with stickers

513
00:19:03,520 --> 00:19:07,559
so thank you for this opportunity

514
00:19:14,960 --> 00:19:17,039
you

