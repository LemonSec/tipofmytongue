1
00:00:00,000 --> 00:00:04,140
they're still awake Jesus is freaking

2
00:00:02,310 --> 00:00:07,170
amazing it's three o'clock you guys have

3
00:00:04,140 --> 00:00:09,870
still got energy cool you are in

4
00:00:07,170 --> 00:00:11,639
ballroom see this is the anything Trek I

5
00:00:09,870 --> 00:00:13,889
really hope that's why you came here

6
00:00:11,639 --> 00:00:15,750
it's been two days if you're still

7
00:00:13,889 --> 00:00:18,779
confused I'm sorry

8
00:00:15,750 --> 00:00:21,300
map reading lessons are next week

9
00:00:18,779 --> 00:00:23,400
couple things don't stand in my fire

10
00:00:21,300 --> 00:00:25,170
exits I will have people drag you out of

11
00:00:23,400 --> 00:00:27,959
there come on in we got plenty of

12
00:00:25,170 --> 00:00:29,460
seating second thing the phones silence

13
00:00:27,960 --> 00:00:30,990
turn them off whatever I will come out

14
00:00:29,460 --> 00:00:32,850
there I'm wearing boots I will stomp

15
00:00:30,990 --> 00:00:36,559
them we had a bloody incident yesterday

16
00:00:32,850 --> 00:00:40,140
we don't need to repeat fair enough

17
00:00:36,559 --> 00:00:41,550
awesome alright so what's it like when

18
00:00:40,140 --> 00:00:43,920
you can't trust the things you're seeing

19
00:00:41,550 --> 00:00:51,718
well next speakers gonna tell you all

20
00:00:43,920 --> 00:00:55,010
about it give it up for April it's not

21
00:00:51,719 --> 00:00:57,750
really me this is a hologram don't worry

22
00:00:55,010 --> 00:01:03,870
so let me fix today we're gonna talk

23
00:00:57,750 --> 00:01:06,090
about torn privacy and politics we're

24
00:01:03,870 --> 00:01:07,679
entering an era in which our enemies can

25
00:01:06,090 --> 00:01:10,320
make it look like anyone is saying

26
00:01:07,680 --> 00:01:14,280
anything at any point in time even if

27
00:01:10,320 --> 00:01:16,199
they would never say those things so for

28
00:01:14,280 --> 00:01:21,470
instance they could have me say things

29
00:01:16,200 --> 00:01:25,290
like I don't know kill monger was right

30
00:01:21,470 --> 00:01:27,840
Ben Carson is in the sunken place or how

31
00:01:25,290 --> 00:01:32,040
about this simply president Trump is a

32
00:01:27,840 --> 00:01:34,829
total and complete dick now you see I

33
00:01:32,040 --> 00:01:37,770
would never say these things at least

34
00:01:34,829 --> 00:01:43,320
not in a public address but someone else

35
00:01:37,770 --> 00:01:47,280
would someone like Jordan Peele this is

36
00:01:43,320 --> 00:01:49,320
a dangerous time moving forward we need

37
00:01:47,280 --> 00:01:51,720
to be more vigilant with what we trust

38
00:01:49,320 --> 00:01:55,369
from the internet it's a time when we

39
00:01:51,720 --> 00:01:59,719
need to rely on trusted news sources

40
00:01:55,369 --> 00:02:04,049
they sound basic but how we move forward

41
00:01:59,719 --> 00:02:06,419
age of information is gonna be the

42
00:02:04,049 --> 00:02:09,119
difference between whether we survive or

43
00:02:06,420 --> 00:02:11,068
whether we become some kind of dystopia

44
00:02:09,119 --> 00:02:24,390
thank you

45
00:02:11,068 --> 00:02:26,609
stay wolf bitches okay so for 200,000

46
00:02:24,390 --> 00:02:29,069
years humans have relied on our eyes and

47
00:02:26,610 --> 00:02:31,860
ears to separate truth from lies in fact

48
00:02:29,069 --> 00:02:34,379
from fiction even if we ignore the rise

49
00:02:31,860 --> 00:02:36,269
of fake news technology built using deep

50
00:02:34,379 --> 00:02:37,980
learning is on the verge of making it

51
00:02:36,269 --> 00:02:41,849
impossible to know if what we're seeing

52
00:02:37,980 --> 00:02:44,190
and hearing is real or fake video has

53
00:02:41,849 --> 00:02:46,319
historically been a way to validate and

54
00:02:44,190 --> 00:02:48,510
attribute an action to a person the

55
00:02:46,319 --> 00:02:50,909
surveillance still just from just before

56
00:02:48,510 --> 00:02:52,739
the Boston Marathon bombing helped

57
00:02:50,909 --> 00:02:55,798
identify the suspect that who was later

58
00:02:52,739 --> 00:02:57,690
apprehended from news to CCTV images

59
00:02:55,799 --> 00:02:59,720
photos and videos have played an

60
00:02:57,690 --> 00:03:01,909
important role in informing educating

61
00:02:59,720 --> 00:03:05,670
entertaining and helping us to deliver

62
00:03:01,909 --> 00:03:07,470
justice since their invention lately the

63
00:03:05,670 --> 00:03:11,369
power of the image to show us the truth

64
00:03:07,470 --> 00:03:13,290
has been increasingly challenged it was

65
00:03:11,370 --> 00:03:15,599
just a single year after the invention

66
00:03:13,290 --> 00:03:18,690
of the photograph until the very first

67
00:03:15,599 --> 00:03:20,849
faked photo was taken it was snapped by

68
00:03:18,690 --> 00:03:23,459
a French photography pioneer named I

69
00:03:20,849 --> 00:03:25,530
believe I Yard who was - Louis Daguerre

70
00:03:23,459 --> 00:03:29,040
the inventor of the did Garrett type

71
00:03:25,530 --> 00:03:32,010
what Tesla was Edison - Guerin by are

72
00:03:29,040 --> 00:03:34,769
both invented photographic processes the

73
00:03:32,010 --> 00:03:37,858
first ever but de Guerre released his

74
00:03:34,769 --> 00:03:40,049
process first because some drama the

75
00:03:37,859 --> 00:03:42,840
French Academy of Sciences so out of

76
00:03:40,049 --> 00:03:45,480
protest and or spite by Yard created the

77
00:03:42,840 --> 00:03:48,359
first hoax photo ever it was titled

78
00:03:45,480 --> 00:03:50,399
self-portrait as a drowned man he was of

79
00:03:48,359 --> 00:03:52,139
course alive but the photo along with

80
00:03:50,400 --> 00:03:54,299
the caption on the back meant to

81
00:03:52,139 --> 00:03:56,699
convince people he had committed suicide

82
00:03:54,299 --> 00:03:57,359
over the whole ordeal a little

83
00:03:56,699 --> 00:04:00,120
overdramatic

84
00:03:57,359 --> 00:04:02,069
he asked me does the first known example

85
00:04:00,120 --> 00:04:03,810
of a fake photo it Illustrated two

86
00:04:02,069 --> 00:04:05,429
qualities of images first they can

87
00:04:03,810 --> 00:04:07,169
depict the world in a manner that

88
00:04:05,430 --> 00:04:09,269
closely mimics the way we see it

89
00:04:07,169 --> 00:04:11,849
second that since their invention

90
00:04:09,269 --> 00:04:13,530
they've been staged or altered in ways

91
00:04:11,849 --> 00:04:16,469
that remain consistent with the ways

92
00:04:13,530 --> 00:04:19,909
that we state this consistency is what

93
00:04:16,470 --> 00:04:22,919
makes them all the more believable a

94
00:04:19,909 --> 00:04:24,280
portmanteau of deep learning and fake

95
00:04:22,919 --> 00:04:26,349
the concept of a deep

96
00:04:24,280 --> 00:04:28,270
refers to the digital manipulation of

97
00:04:26,350 --> 00:04:30,190
sound images or video to impersonate

98
00:04:28,270 --> 00:04:32,469
someone or make it appear that a person

99
00:04:30,190 --> 00:04:34,390
did something and to do so in a manner

100
00:04:32,470 --> 00:04:36,310
that is increasingly realistic to the

101
00:04:34,390 --> 00:04:39,250
point that the unaided observer cannot

102
00:04:36,310 --> 00:04:41,410
detect the fake defects or the scariest

103
00:04:39,250 --> 00:04:44,380
possible fake news of the present near

104
00:04:41,410 --> 00:04:46,660
future apps for creating content often

105
00:04:44,380 --> 00:04:49,510
leverage Google's publicly available

106
00:04:46,660 --> 00:04:51,400
tensorflow AI framework but can deep

107
00:04:49,510 --> 00:04:55,090
fakes provide any actual benefit to

108
00:04:51,400 --> 00:04:59,979
society see fix go far beyond

109
00:04:55,090 --> 00:05:02,020
photoshopping they're moving speaking

110
00:04:59,980 --> 00:05:04,270
realist a rendition of a person that can

111
00:05:02,020 --> 00:05:06,760
be recorded for later use or performed

112
00:05:04,270 --> 00:05:09,460
live even they spread virally they're

113
00:05:06,760 --> 00:05:12,310
entertaining and controversial in

114
00:05:09,460 --> 00:05:14,979
December 2017 a user named defects

115
00:05:12,310 --> 00:05:18,160
posted realistic-looking explicit videos

116
00:05:14,980 --> 00:05:20,169
of famous celebrities on reddit this was

117
00:05:18,160 --> 00:05:22,270
the first known use of the new type of

118
00:05:20,169 --> 00:05:24,880
deep learning face tattoo swab

119
00:05:22,270 --> 00:05:27,159
technology these are fakes found a way

120
00:05:24,880 --> 00:05:29,680
to insert celebrities faces into adult

121
00:05:27,160 --> 00:05:31,870
movies lawmakers and intelligence

122
00:05:29,680 --> 00:05:35,040
officials they worry more about bogus

123
00:05:31,870 --> 00:05:37,900
videos that go beyond adult films though

124
00:05:35,040 --> 00:05:38,950
that go beyond parody and starts to be

125
00:05:37,900 --> 00:05:41,169
used to threaten national security

126
00:05:38,950 --> 00:05:45,490
interfere in elections and perform other

127
00:05:41,169 --> 00:05:47,380
attacks against the public early CGI in

128
00:05:45,490 --> 00:05:50,080
movies was often clumsy and obviously

129
00:05:47,380 --> 00:05:52,469
fake CGI also required skilled video

130
00:05:50,080 --> 00:05:56,409
editors that could spend countless hours

131
00:05:52,470 --> 00:05:57,850
trying to achieve the decent results the

132
00:05:56,410 --> 00:06:00,880
concern is concerning breakthrough with

133
00:05:57,850 --> 00:06:03,190
the fakes versus CGI though is that with

134
00:06:00,880 --> 00:06:05,290
deep learning anybody with a GPU and

135
00:06:03,190 --> 00:06:07,630
some AI training can create believable

136
00:06:05,290 --> 00:06:10,360
fake videos using input derived from

137
00:06:07,630 --> 00:06:13,050
social media it was only a matter of

138
00:06:10,360 --> 00:06:16,330
time before image l turing technology

139
00:06:13,050 --> 00:06:17,200
became so sophisticated that it would be

140
00:06:16,330 --> 00:06:19,710
difficult to tell if the

141
00:06:17,200 --> 00:06:24,760
computer-generated image is not real

142
00:06:19,710 --> 00:06:27,070
in 1970 japanese roboticists Masahiro

143
00:06:24,760 --> 00:06:29,020
mori there's a lot of names in this that

144
00:06:27,070 --> 00:06:30,550
aren't i gotta pronounce here he

145
00:06:29,020 --> 00:06:31,780
proposed a theory and an essay called

146
00:06:30,550 --> 00:06:33,430
the uncanny valley

147
00:06:31,780 --> 00:06:35,590
the theory was the more

148
00:06:33,430 --> 00:06:37,930
human a robot acted or looked more

149
00:06:35,590 --> 00:06:39,909
endearing it would be to a human however

150
00:06:37,930 --> 00:06:42,130
at some point the likeness to human

151
00:06:39,910 --> 00:06:44,860
seems too strong and yet somehow to

152
00:06:42,130 --> 00:06:47,170
different it comes across as a very

153
00:06:44,860 --> 00:06:50,680
strange human being at which time the

154
00:06:47,170 --> 00:06:52,450
acceptance of that it drops suddenly and

155
00:06:50,680 --> 00:06:56,050
it changes into a really powerful

156
00:06:52,450 --> 00:06:57,960
negative reaction the valley evokes a

157
00:06:56,050 --> 00:07:00,910
number of primal responses in humans

158
00:06:57,960 --> 00:07:02,680
particularly revulsion and these

159
00:07:00,910 --> 00:07:04,840
responses may apply the sound as well

160
00:07:02,680 --> 00:07:06,640
for some moving objects trigger a

161
00:07:04,840 --> 00:07:08,320
stronger response which is why a corpse

162
00:07:06,640 --> 00:07:10,599
is creepy but a moving corpse is even

163
00:07:08,320 --> 00:07:12,010
creepier but the algorithms providing

164
00:07:10,600 --> 00:07:14,340
deep fakes are gaining precision

165
00:07:12,010 --> 00:07:17,140
signaling the end of the valley and

166
00:07:14,340 --> 00:07:18,820
overcoming the valley is ultimately one

167
00:07:17,140 --> 00:07:23,020
of the requirements for believability of

168
00:07:18,820 --> 00:07:25,630
the deep fake videos a new technique

169
00:07:23,020 --> 00:07:27,609
called retiming can synthesize extremely

170
00:07:25,630 --> 00:07:30,040
high-quality deep fakes using a

171
00:07:27,610 --> 00:07:31,750
recurrent neural network it learns of

172
00:07:30,040 --> 00:07:33,550
the mapping from the raw audio features

173
00:07:31,750 --> 00:07:35,950
of the mouth shapes and given the mouth

174
00:07:33,550 --> 00:07:38,320
shape at each time instant it creates

175
00:07:35,950 --> 00:07:41,320
high quality mouth texture it composites

176
00:07:38,320 --> 00:07:43,659
that with proper 3d posing and it

177
00:07:41,320 --> 00:07:45,640
matches it to change what the subject

178
00:07:43,660 --> 00:07:47,410
appears to be saying the approach

179
00:07:45,640 --> 00:07:49,150
produces photo realistic results which

180
00:07:47,410 --> 00:07:53,410
may help deep fix reach beyond the

181
00:07:49,150 --> 00:07:55,179
valley defects rely heavily on facial

182
00:07:53,410 --> 00:07:57,400
detection but this technology is not new

183
00:07:55,180 --> 00:07:59,110
it's called the Viola Jones algorithm

184
00:07:57,400 --> 00:08:01,299
and it's something you've probably seen

185
00:07:59,110 --> 00:08:03,010
in point-and-shoot cameras machines have

186
00:08:01,300 --> 00:08:05,140
been doing this for years but what's new

187
00:08:03,010 --> 00:08:07,480
is the ability to map two faces and

188
00:08:05,140 --> 00:08:09,490
create a composite of both and while a

189
00:08:07,480 --> 00:08:11,440
deep fake can be created from hundreds

190
00:08:09,490 --> 00:08:13,000
or even thousands of still images it's

191
00:08:11,440 --> 00:08:16,120
much easier to pick a handful of videos

192
00:08:13,000 --> 00:08:17,770
than it is to find hundreds of images at

193
00:08:16,120 --> 00:08:21,340
just the right angle and just the right

194
00:08:17,770 --> 00:08:23,080
lighting we've seen similar real-time

195
00:08:21,340 --> 00:08:24,700
technology implemented in apps like

196
00:08:23,080 --> 00:08:27,039
snapchat in the form of augmented

197
00:08:24,700 --> 00:08:28,659
reality filters and real-time the app

198
00:08:27,040 --> 00:08:28,990
Maps her face and overlays something

199
00:08:28,660 --> 00:08:31,060
else

200
00:08:28,990 --> 00:08:32,890
this is very similar to the tech behind

201
00:08:31,060 --> 00:08:35,409
a deep fake except the outputs not very

202
00:08:32,890 --> 00:08:39,760
realistic and it's not believable and it

203
00:08:35,409 --> 00:08:41,469
quite often shows glitches this is what

204
00:08:39,760 --> 00:08:43,059
it looks like to a machine when it's

205
00:08:41,470 --> 00:08:44,830
learning how to create the faces of John

206
00:08:43,059 --> 00:08:46,689
Oliver and Jimmy Kim both

207
00:08:44,830 --> 00:08:49,150
have been the subject of numerous teeth

208
00:08:46,690 --> 00:08:51,610
fake examples both of them are of course

209
00:08:49,150 --> 00:08:53,560
purveyors of and fans of parody but TV

210
00:08:51,610 --> 00:08:55,510
studios use consistent lighting and

211
00:08:53,560 --> 00:08:58,060
there are thousands of hours of both

212
00:08:55,510 --> 00:09:01,290
Oliver and Kimmel all with perfectly lit

213
00:08:58,060 --> 00:09:04,390
environments for training and algorithm

214
00:09:01,290 --> 00:09:05,980
so at this point no one's probably

215
00:09:04,390 --> 00:09:09,100
surprised that this all started on

216
00:09:05,980 --> 00:09:11,830
reddit like VHS and other technology

217
00:09:09,100 --> 00:09:13,870
just became popular through porn Nic

218
00:09:11,830 --> 00:09:16,480
Cage and Steve Buscemi are popular indie

219
00:09:13,870 --> 00:09:22,360
pics and world leaders are frequently

220
00:09:16,480 --> 00:09:25,090
featured in parody deep fake videos so

221
00:09:22,360 --> 00:09:26,770
some technical stuff the deep pits are

222
00:09:25,090 --> 00:09:28,090
generally made from still photos and

223
00:09:26,770 --> 00:09:29,680
they take hours days or weeks of

224
00:09:28,090 --> 00:09:31,600
processing to become better they tend to

225
00:09:29,680 --> 00:09:35,050
superimpose the impose a pace of one

226
00:09:31,600 --> 00:09:37,090
person on to someone else now deep video

227
00:09:35,050 --> 00:09:38,650
portraits are a little different they're

228
00:09:37,090 --> 00:09:40,630
made using a source video that features

229
00:09:38,650 --> 00:09:42,850
an actor this is what we saw in the

230
00:09:40,630 --> 00:09:44,860
Obama video at the beginning this is

231
00:09:42,850 --> 00:09:46,450
sometimes known as video puppetry the

232
00:09:44,860 --> 00:09:49,210
main differences of the source data type

233
00:09:46,450 --> 00:09:52,300
is images or video and whether you have

234
00:09:49,210 --> 00:09:55,120
intend to preserve the person or replace

235
00:09:52,300 --> 00:09:57,790
the person in the output using videos as

236
00:09:55,120 --> 00:09:59,530
a source rather than photos is much more

237
00:09:57,790 --> 00:10:01,089
realistic because you're not changing

238
00:09:59,530 --> 00:10:03,400
the face of the target you're just

239
00:10:01,090 --> 00:10:04,570
manipulating the original face for the

240
00:10:03,400 --> 00:10:07,350
purposes of this talk we're just going

241
00:10:04,570 --> 00:10:11,020
to call all of this deep face though

242
00:10:07,350 --> 00:10:12,760
some famous examples Peter Cushing the

243
00:10:11,020 --> 00:10:15,280
actor that played grand moff tarkin and

244
00:10:12,760 --> 00:10:18,670
the original Star Wars from 1977 died in

245
00:10:15,280 --> 00:10:20,230
1994 in Star Wars rogue one Tarkin is

246
00:10:18,670 --> 00:10:22,569
played by actor guy Henry who were

247
00:10:20,230 --> 00:10:25,630
facial performance rig to map his face

248
00:10:22,570 --> 00:10:27,550
onto a digital 3d model face the 3d

249
00:10:25,630 --> 00:10:29,320
model was made of a scan from a mold of

250
00:10:27,550 --> 00:10:31,870
Cushing his face that was made to create

251
00:10:29,320 --> 00:10:35,590
a special effects these prosthesis for a

252
00:10:31,870 --> 00:10:37,360
1984 movie called top-secret rogue one

253
00:10:35,590 --> 00:10:40,420
required frame by frame

254
00:10:37,360 --> 00:10:42,820
editing by ILM it took 18 months to

255
00:10:40,420 --> 00:10:44,920
complete all that timing work resulted

256
00:10:42,820 --> 00:10:47,470
in a hyper realistic rendition of target

257
00:10:44,920 --> 00:10:49,510
for the film but the effort to create

258
00:10:47,470 --> 00:10:51,870
adequate realism for a high in the

259
00:10:49,510 --> 00:10:57,380
Definition film versus a 640 by 480

260
00:10:51,870 --> 00:11:00,860
pixel compressed video is very different

261
00:10:57,380 --> 00:11:02,510
German Chancellor Angela Merkel was face

262
00:11:00,860 --> 00:11:04,160
flopped with Donald Trump in this low

263
00:11:02,510 --> 00:11:06,830
resolution video it's rather poor

264
00:11:04,160 --> 00:11:08,510
quality for a deep fake but it's an

265
00:11:06,830 --> 00:11:23,270
example of world leaders being

266
00:11:08,510 --> 00:11:26,260
manipulated in video following Godwin's

267
00:11:23,270 --> 00:11:29,240
law argentine president Mauricio Macri

268
00:11:26,260 --> 00:11:30,980
was face flat with Hitler another

269
00:11:29,240 --> 00:11:35,270
example of political commentary and

270
00:11:30,980 --> 00:11:37,670
manipulation tools exist today let you

271
00:11:35,270 --> 00:11:39,699
type in text and generate a video of an

272
00:11:37,670 --> 00:11:41,689
actual person saying the typed words

273
00:11:39,700 --> 00:11:43,730
while the techniques used to achieve

274
00:11:41,690 --> 00:11:46,160
this are complex using the tool is

275
00:11:43,730 --> 00:11:48,830
frightening ly simple videos generated

276
00:11:46,160 --> 00:11:52,040
by one tool were shunned a group of 138

277
00:11:48,830 --> 00:11:54,080
people 60% believed the fake videos were

278
00:11:52,040 --> 00:11:57,589
real and the same group was able to

279
00:11:54,080 --> 00:12:01,280
identify real videos only 80% of the

280
00:11:57,590 --> 00:12:03,050
time look at how seamlessly of the video

281
00:12:01,280 --> 00:12:05,449
we're about to see changes from Bill

282
00:12:03,050 --> 00:12:08,240
Hader with Bill hader's face it's a Bill

283
00:12:05,450 --> 00:12:10,490
Hader with Tom Cruise's Vegas guys and

284
00:12:08,240 --> 00:12:13,130
then and then Tom Cruise walks in and

285
00:12:10,490 --> 00:12:26,390
even those guys like whoa and he's super

286
00:12:13,130 --> 00:12:29,960
stoked immediately excited when he walks

287
00:12:26,390 --> 00:12:31,220
into a room and and so he comes over he

288
00:12:29,960 --> 00:12:33,260
sits next to me and I think he had been

289
00:12:31,220 --> 00:12:36,530
briefed on some of the supporting guys

290
00:12:33,260 --> 00:12:38,780
but he was like trying to place me yeah

291
00:12:36,530 --> 00:12:41,170
you know so he sat down next to me he's

292
00:12:38,780 --> 00:12:41,170
like I

293
00:12:43,050 --> 00:13:00,149
I love your work oh thanks I love your

294
00:12:49,500 --> 00:13:01,589
work you so with so many possibility

295
00:13:00,149 --> 00:13:02,100
that the rise of the fix has only just

296
00:13:01,589 --> 00:13:04,350
begun

297
00:13:02,100 --> 00:13:06,149
well bulliest torment fellow students

298
00:13:04,350 --> 00:13:08,399
well political dissidents impact

299
00:13:06,149 --> 00:13:11,550
elections or start a war let's talk

300
00:13:08,399 --> 00:13:15,170
about the implications good and bad is

301
00:13:11,550 --> 00:13:17,430
this an attack yes and no it can be

302
00:13:15,170 --> 00:13:20,550
defects can also be protected free

303
00:13:17,430 --> 00:13:22,170
speech and or parody your likeness is

304
00:13:20,550 --> 00:13:24,029
vulnerable if you've ever posted on

305
00:13:22,170 --> 00:13:27,420
social media or visited a public

306
00:13:24,029 --> 00:13:29,189
publicly monitored place like any weapon

307
00:13:27,420 --> 00:13:31,189
as we know in the hacker community a

308
00:13:29,190 --> 00:13:34,170
tool can be used for good or evil

309
00:13:31,190 --> 00:13:36,839
bloggers Alan Zaccone said it well every

310
00:13:34,170 --> 00:13:38,760
time a new technology appears we need to

311
00:13:36,839 --> 00:13:41,250
question the impact it will have on us

312
00:13:38,760 --> 00:13:43,080
and on the people around us we cannot

313
00:13:41,250 --> 00:13:44,790
rely on governments and laws for

314
00:13:43,080 --> 00:13:46,740
guidance as they are notoriously

315
00:13:44,790 --> 00:13:49,050
ineffective and inefficient at keeping

316
00:13:46,740 --> 00:13:50,520
up with the pace of progress well some

317
00:13:49,050 --> 00:13:52,589
people would like to see any face

318
00:13:50,520 --> 00:13:56,160
whopping technology banned that's not

319
00:13:52,589 --> 00:13:57,779
going to resolve the issue virtually

320
00:13:56,160 --> 00:13:59,819
everyone is vulnerable but public

321
00:13:57,779 --> 00:14:03,240
figures and celebrities especially are

322
00:13:59,820 --> 00:14:04,920
no one is immune even if the target does

323
00:14:03,240 --> 00:14:07,140
not have a social media presence on

324
00:14:04,920 --> 00:14:08,339
platform acts there's a reasonable

325
00:14:07,140 --> 00:14:10,770
chance that someone else may have

326
00:14:08,339 --> 00:14:13,050
uploaded a photo that has their face in

327
00:14:10,770 --> 00:14:14,819
it they will reveal the needed input or

328
00:14:13,050 --> 00:14:18,270
if you've been in a public place of

329
00:14:14,820 --> 00:14:20,910
course the internet never forgets this

330
00:14:18,270 --> 00:14:23,850
brings us to consent on this topic

331
00:14:20,910 --> 00:14:26,399
adult-film perform enough please grace

332
00:14:23,850 --> 00:14:27,779
evangeline said about deep fix one

333
00:14:26,399 --> 00:14:29,940
important thing that always needs to

334
00:14:27,779 --> 00:14:32,550
happen is consent consent in private

335
00:14:29,940 --> 00:14:34,260
life as well as consent on film creating

336
00:14:32,550 --> 00:14:37,349
fake scenes of celebrities takes away

337
00:14:34,260 --> 00:14:38,939
their consent it's wrong conversely the

338
00:14:37,350 --> 00:14:41,490
original redditor known I was deep fakes

339
00:14:38,940 --> 00:14:43,170
said every technology can be used with

340
00:14:41,490 --> 00:14:44,880
bad motivations and it's impossible to

341
00:14:43,170 --> 00:14:47,490
stop that the main difference is how

342
00:14:44,880 --> 00:14:49,410
easy it is to do that by everyone it's

343
00:14:47,490 --> 00:14:51,440
not a bad thing for more average people

344
00:14:49,410 --> 00:14:54,120
to engage in machine learning research

345
00:14:51,440 --> 00:14:56,100
two very valid arguments

346
00:14:54,120 --> 00:14:59,100
regardless of consent this can't really

347
00:14:56,100 --> 00:15:01,079
be stopped Business Insider said that

348
00:14:59,100 --> 00:15:03,449
deep fake videos exist in a legal limbo

349
00:15:01,079 --> 00:15:05,180
making it difficult for victims to track

350
00:15:03,449 --> 00:15:07,559
down perpetrators and remove content

351
00:15:05,180 --> 00:15:09,930
some experts think that it's a First

352
00:15:07,559 --> 00:15:12,110
Amendment issue some think it might fall

353
00:15:09,930 --> 00:15:15,508
under defamation identity theft a fraud

354
00:15:12,110 --> 00:15:17,279
another view from Scarlett Johansson

355
00:15:15,509 --> 00:15:19,529
she said that trying to stop people

356
00:15:17,279 --> 00:15:20,430
making deep fake videos of her is a lost

357
00:15:19,529 --> 00:15:22,680
cause

358
00:15:20,430 --> 00:15:24,599
quote nothing can stop someone from

359
00:15:22,680 --> 00:15:26,189
cutting and pasting my image or anyone

360
00:15:24,600 --> 00:15:29,040
else's onto a different body and making

361
00:15:26,189 --> 00:15:31,349
it look as eerily realistic as desired

362
00:15:29,040 --> 00:15:32,610
the fact is that trying to protect

363
00:15:31,350 --> 00:15:34,589
yourself from the internet and it's

364
00:15:32,610 --> 00:15:37,019
deprave idli drift depravity is

365
00:15:34,589 --> 00:15:39,480
basically a lost cause she added one of

366
00:15:37,019 --> 00:15:41,759
my favorite quotes over the Internet is

367
00:15:39,480 --> 00:15:44,180
a vast worm hole of darkness that eats

368
00:15:41,759 --> 00:15:44,180
itself

369
00:15:45,649 --> 00:15:50,550
platforms like Facebook Twitter even

370
00:15:47,999 --> 00:15:51,990
read it and pornhub have attempted to

371
00:15:50,550 --> 00:15:54,329
thwart misinformation and

372
00:15:51,990 --> 00:15:57,509
personalization abuse when found however

373
00:15:54,329 --> 00:15:59,370
defects or live deep video broadcasts

374
00:15:57,509 --> 00:16:01,620
can evade both human and machine

375
00:15:59,370 --> 00:16:03,509
detection motherboard has made a bold

376
00:16:01,620 --> 00:16:05,579
claim that there is no tech solution to

377
00:16:03,509 --> 00:16:09,629
deep fix but as we'll discuss later

378
00:16:05,579 --> 00:16:10,979
researchers don't agree support platform

379
00:16:09,629 --> 00:16:14,100
sharing of deep fakes you must first

380
00:16:10,980 --> 00:16:16,079
detect it fake video in RAW format has

381
00:16:14,100 --> 00:16:18,240
telltale signs of alteration within the

382
00:16:16,079 --> 00:16:20,998
pixels but sites like YouTube don't

383
00:16:18,240 --> 00:16:22,470
store every pixel when its quality is

384
00:16:20,999 --> 00:16:24,540
lowered during upload and conversion

385
00:16:22,470 --> 00:16:26,910
some evidences erase but some artifacts

386
00:16:24,540 --> 00:16:29,969
remain like glitches head movement

387
00:16:26,910 --> 00:16:31,319
errors or unusual blinking patterns the

388
00:16:29,970 --> 00:16:32,850
more times a photo is uploaded and

389
00:16:31,319 --> 00:16:34,709
downloaded the processing it goes

390
00:16:32,850 --> 00:16:37,740
through makes altered pixels harder to

391
00:16:34,709 --> 00:16:39,449
detect detection is such a critical

392
00:16:37,740 --> 00:16:41,730
issue that DARPA is halfway through a

393
00:16:39,449 --> 00:16:44,160
four-year effort to create deep fake

394
00:16:41,730 --> 00:16:48,379
image detection tools via the media

395
00:16:44,160 --> 00:16:50,610
forensics project to identify deep fakes

396
00:16:48,379 --> 00:16:52,290
recently Facebook Microsoft's and

397
00:16:50,610 --> 00:16:54,540
multiple top universities have launched

398
00:16:52,290 --> 00:16:56,309
a challenge that will begin in late 2019

399
00:16:54,540 --> 00:16:59,129
with the release of a data set of faces

400
00:16:56,309 --> 00:17:00,660
to accept the competitors 10 million

401
00:16:59,129 --> 00:17:02,819
dollars and grants and awards are

402
00:17:00,660 --> 00:17:04,709
available to participants the test

403
00:17:02,819 --> 00:17:05,970
mechanisms enables teams to score their

404
00:17:04,709 --> 00:17:08,490
models effectiveness against

405
00:17:05,970 --> 00:17:10,079
one or more black box test sites this is

406
00:17:08,490 --> 00:17:11,929
being done to create new ways of

407
00:17:10,079 --> 00:17:15,990
detecting and preventing media

408
00:17:11,929 --> 00:17:17,640
manipulated via AI researchers believe

409
00:17:15,990 --> 00:17:19,079
that since realistic deep fakes are

410
00:17:17,640 --> 00:17:21,059
ultimately generated by an algorithm

411
00:17:19,079 --> 00:17:25,829
they can be detected in their provenance

412
00:17:21,059 --> 00:17:27,689
verified biometric detection is an

413
00:17:25,829 --> 00:17:30,690
interesting way of looking at this it

414
00:17:27,689 --> 00:17:32,400
can be analyzed by algorithms to look at

415
00:17:30,690 --> 00:17:34,530
the characteristics of people and videos

416
00:17:32,400 --> 00:17:36,090
are they breathing at the right rate is

417
00:17:34,530 --> 00:17:37,980
the pulse in the forehead the same as

418
00:17:36,090 --> 00:17:39,299
the pulse in the neck does the pitch and

419
00:17:37,980 --> 00:17:43,830
frequency of the voice seem appropriate

420
00:17:39,299 --> 00:17:46,289
and is the person blinking normally what

421
00:17:43,830 --> 00:17:48,720
does this mean machines may have the

422
00:17:46,289 --> 00:17:51,179
samples from machines basically we may

423
00:17:48,720 --> 00:17:54,059
have to provide training data labeled as

424
00:17:51,179 --> 00:17:56,159
fake or real to AI because algorithms

425
00:17:54,059 --> 00:17:59,549
can in theory learn to detect fakes more

426
00:17:56,159 --> 00:18:01,320
easily than we can however if you can

427
00:17:59,549 --> 00:18:02,730
create a neural network that will be

428
00:18:01,320 --> 00:18:04,379
able to tell a real image from a fake

429
00:18:02,730 --> 00:18:06,360
one you can train another network to

430
00:18:04,380 --> 00:18:10,289
defeat it this is like running a virus

431
00:18:06,360 --> 00:18:13,789
through antivirus this leads us to

432
00:18:10,289 --> 00:18:16,260
generative adversarial networks or gans

433
00:18:13,789 --> 00:18:18,690
the creators of deep fakes use the same

434
00:18:16,260 --> 00:18:20,340
glands that create deep that create deep

435
00:18:18,690 --> 00:18:22,580
fix to discover how to beat the deep

436
00:18:20,340 --> 00:18:22,580
fakes

437
00:18:24,140 --> 00:18:29,400
here's another attack scenario easy to

438
00:18:27,690 --> 00:18:31,200
use tools for manipulating perception

439
00:18:29,400 --> 00:18:34,250
and falsifying reality are being used

440
00:18:31,200 --> 00:18:37,200
right now for automated laser fishing

441
00:18:34,250 --> 00:18:40,020
its latest social engineering tactic

442
00:18:37,200 --> 00:18:42,630
that makes manual spearfishing and uses

443
00:18:40,020 --> 00:18:45,059
AI to scan open source intelligence or

444
00:18:42,630 --> 00:18:47,280
assent data points like social media and

445
00:18:45,059 --> 00:18:51,840
craft false but believable messages from

446
00:18:47,280 --> 00:18:53,190
people we know AI based impersonation

447
00:18:51,840 --> 00:18:55,289
attacks are just the beginning of what

448
00:18:53,190 --> 00:18:56,870
could be major headaches for businesses

449
00:18:55,289 --> 00:18:58,980
and organizations in the future

450
00:18:56,870 --> 00:19:01,049
criminals used commercially available

451
00:18:58,980 --> 00:19:03,090
voice generating AI software to

452
00:19:01,049 --> 00:19:05,929
impersonate the boss of a German parent

453
00:19:03,090 --> 00:19:09,209
company that owns a uk-based energy firm

454
00:19:05,929 --> 00:19:11,669
using phishing or voice phishing the

455
00:19:09,210 --> 00:19:13,770
British CEO hearing the familiar slight

456
00:19:11,669 --> 00:19:16,350
German accent of his boss suspected

457
00:19:13,770 --> 00:19:18,639
nothing the fraudsters boldly posed as

458
00:19:16,350 --> 00:19:20,918
the German CEO twice

459
00:19:18,640 --> 00:19:23,140
the second time the British CEO refused

460
00:19:20,919 --> 00:19:26,049
to make the payment now imagine if the

461
00:19:23,140 --> 00:19:27,580
request had come from a video a deep

462
00:19:26,049 --> 00:19:30,399
fake perhaps in the form of video

463
00:19:27,580 --> 00:19:32,289
puppetry over FaceTime or some other

464
00:19:30,399 --> 00:19:36,158
type of video chat do you think he would

465
00:19:32,289 --> 00:19:37,960
have made the second transfer another

466
00:19:36,159 --> 00:19:39,850
real-world scenario where realistic

467
00:19:37,960 --> 00:19:42,940
fakes of impacted politics is called

468
00:19:39,850 --> 00:19:45,100
polity simulation increasingly

469
00:19:42,940 --> 00:19:47,169
believable AI power bots are able to

470
00:19:45,100 --> 00:19:49,178
effectively compete with real humans for

471
00:19:47,169 --> 00:19:50,710
legislature attention because it's too

472
00:19:49,179 --> 00:19:52,779
difficult to tell the difference

473
00:19:50,710 --> 00:19:55,090
stitched together content culled from

474
00:19:52,779 --> 00:19:56,649
text audio and social media profiles are

475
00:19:55,090 --> 00:19:58,539
used to steal identities and make phone

476
00:19:56,649 --> 00:20:00,789
calls or post videos to effect change

477
00:19:58,539 --> 00:20:02,919
more the two million baht accounts

478
00:20:00,789 --> 00:20:04,600
flooded the FCC's open common systems to

479
00:20:02,919 --> 00:20:06,940
amplify the call to repeal net

480
00:20:04,600 --> 00:20:08,289
neutrality researchers concluded that

481
00:20:06,940 --> 00:20:11,470
automation use natural language

482
00:20:08,289 --> 00:20:13,419
processing or NLP these obscured

483
00:20:11,470 --> 00:20:15,070
legitimate comments and undermined the

484
00:20:13,419 --> 00:20:17,760
authenticity of the entire comment

485
00:20:15,070 --> 00:20:20,080
system and we all know where that lid

486
00:20:17,760 --> 00:20:22,360
the washington post bound users on

487
00:20:20,080 --> 00:20:24,210
discussion boards and private chats that

488
00:20:22,360 --> 00:20:28,149
are claiming to make videos by request

489
00:20:24,210 --> 00:20:30,490
friends or enemies or pest lovers etc

490
00:20:28,149 --> 00:20:32,289
one user in a discord chatroom said they

491
00:20:30,490 --> 00:20:33,940
can make a pretty good video of a girl

492
00:20:32,289 --> 00:20:36,250
they went to high school with using

493
00:20:33,940 --> 00:20:43,539
around 380 pictures scraped from her

494
00:20:36,250 --> 00:20:45,610
Instagram and Facebook accounts the u.s.

495
00:20:43,539 --> 00:20:47,110
has no right to be forgotten but

496
00:20:45,610 --> 00:20:49,418
companies like Google have agreed to

497
00:20:47,110 --> 00:20:50,879
delink such videos it's essential for

498
00:20:49,419 --> 00:20:53,200
targets to respond with lightning speed

499
00:20:50,880 --> 00:20:55,480
individuals should take screenshots to

500
00:20:53,200 --> 00:20:57,010
download the fake video call an attorney

501
00:20:55,480 --> 00:20:59,980
and the police and then begin the

502
00:20:57,010 --> 00:21:01,600
process of removal there are civil laws

503
00:20:59,980 --> 00:21:04,299
that apply like intentional infliction

504
00:21:01,600 --> 00:21:06,070
of emotional distress and defamation but

505
00:21:04,299 --> 00:21:08,408
generally it's hard to prove intentional

506
00:21:06,070 --> 00:21:10,299
harm from videos like this the Maine law

507
00:21:08,409 --> 00:21:12,279
that protects sharing platforms is

508
00:21:10,299 --> 00:21:15,158
called the Communications Decency Act or

509
00:21:12,279 --> 00:21:17,080
CDA passed in 1996 is one of the first

510
00:21:15,159 --> 00:21:20,200
attempts to regulate explicit content on

511
00:21:17,080 --> 00:21:21,908
the internet CD a section 230 says

512
00:21:20,200 --> 00:21:23,710
platforms like Facebook and we've seen

513
00:21:21,909 --> 00:21:25,960
this happen a number of times since then

514
00:21:23,710 --> 00:21:27,820
are immune from liability for the

515
00:21:25,960 --> 00:21:32,050
conduct of their users and are able to

516
00:21:27,820 --> 00:21:33,909
profit off of users behavior and posts

517
00:21:32,050 --> 00:21:35,678
of course criminals they'll follow laws

518
00:21:33,910 --> 00:21:39,490
and US laws don't apply to anyone

519
00:21:35,679 --> 00:21:41,170
outside the United States so the e FF

520
00:21:39,490 --> 00:21:42,970
our electronic freedom foundation

521
00:21:41,170 --> 00:21:45,640
purports we don't need new laws for

522
00:21:42,970 --> 00:21:47,110
faked videos we already have them the

523
00:21:45,640 --> 00:21:49,240
theory is if deep fake is used for

524
00:21:47,110 --> 00:21:52,330
criminal purposes then criminal laws

525
00:21:49,240 --> 00:21:54,190
apply such as extortion if defects are

526
00:21:52,330 --> 00:21:56,439
used to harass harassment laws apply

527
00:21:54,190 --> 00:21:58,210
there may be no need to make new

528
00:21:56,440 --> 00:22:00,220
specific laws about toothpicks in any

529
00:21:58,210 --> 00:22:02,080
either of these situations and if

530
00:22:00,220 --> 00:22:06,250
there's copyright infringement copyright

531
00:22:02,080 --> 00:22:07,960
laws apply face swapping by itself is

532
00:22:06,250 --> 00:22:09,820
not illegal although you could be

533
00:22:07,960 --> 00:22:11,380
dabbling in copyright infringement if

534
00:22:09,820 --> 00:22:13,030
you're using videos made by others

535
00:22:11,380 --> 00:22:15,100
the use of someone else's face without

536
00:22:13,030 --> 00:22:16,870
their consent violates personal personal

537
00:22:15,100 --> 00:22:19,209
right of publicity which covers the use

538
00:22:16,870 --> 00:22:20,800
of their image or identity and using

539
00:22:19,210 --> 00:22:23,050
this technology to create non-consensual

540
00:22:20,800 --> 00:22:24,970
adult content a section technically not

541
00:22:23,050 --> 00:22:27,399
a crime yet but it could fall into a

542
00:22:24,970 --> 00:22:29,650
category of revenge porn and of course

543
00:22:27,400 --> 00:22:31,059
base whopping underage people into those

544
00:22:29,650 --> 00:22:34,750
kinds of scenes is definitely a crime

545
00:22:31,059 --> 00:22:38,139
for obvious reasons so who owns the

546
00:22:34,750 --> 00:22:39,250
copyrights for a creative deep it's kind

547
00:22:38,140 --> 00:22:41,050
of hard to answer this question because

548
00:22:39,250 --> 00:22:43,300
it falls into that legal grade a gray

549
00:22:41,050 --> 00:22:45,129
area it's reasonable leave that face

550
00:22:43,300 --> 00:22:47,470
popping could be treated like any other

551
00:22:45,130 --> 00:22:50,230
video editing process under the law it

552
00:22:47,470 --> 00:22:52,330
means that if you if you own all those

553
00:22:50,230 --> 00:22:53,710
copyrights for the original footage then

554
00:22:52,330 --> 00:22:59,530
you'll also own the copyrights for the

555
00:22:53,710 --> 00:23:00,880
newly faceswap video most countries

556
00:22:59,530 --> 00:23:03,129
allow copyright enforcement to be

557
00:23:00,880 --> 00:23:05,350
avoided for the purpose of caricature

558
00:23:03,130 --> 00:23:06,670
parody or imitation there's no reason to

559
00:23:05,350 --> 00:23:10,750
believe that this will not be the case

560
00:23:06,670 --> 00:23:20,770
for Face Swap videos let's talk about

561
00:23:10,750 --> 00:23:23,050
ethics okay so Carrie Fisher not only

562
00:23:20,770 --> 00:23:24,940
gave her ilm her blessing to digitally

563
00:23:23,050 --> 00:23:26,590
insert her younger self in rogue one but

564
00:23:24,940 --> 00:23:28,900
she was actually able to see the result

565
00:23:26,590 --> 00:23:31,449
prior to her death and she quote loved

566
00:23:28,900 --> 00:23:33,520
it i alum wanted to make sure they did

567
00:23:31,450 --> 00:23:37,120
this right both in terms of Leia and

568
00:23:33,520 --> 00:23:39,309
grand moff tarkin ripped from recent

569
00:23:37,120 --> 00:23:40,870
headlines even obviously faked videos

570
00:23:39,309 --> 00:23:43,928
can potentially reach millions of people

571
00:23:40,870 --> 00:23:45,219
and pursue the 8 to 30 Facebook let how

572
00:23:43,929 --> 00:23:47,470
video of house

573
00:23:45,220 --> 00:23:48,730
your Nancy Pelosi that was manipulated

574
00:23:47,470 --> 00:23:51,460
to make it appear as though she was

575
00:23:48,730 --> 00:23:53,679
drunk to stay on its platform YouTube on

576
00:23:51,460 --> 00:23:56,169
the other hand opted to remove it it was

577
00:23:53,679 --> 00:23:58,630
legal to do so in both cases but was

578
00:23:56,169 --> 00:24:01,059
either action ethical did Facebook

579
00:23:58,630 --> 00:24:05,380
interfere in reality or did YouTube it

580
00:24:01,059 --> 00:24:07,510
infringed on free speech the combination

581
00:24:05,380 --> 00:24:09,460
of powerful open-source tools are

582
00:24:07,510 --> 00:24:11,408
rapidly evolving ability to discern

583
00:24:09,460 --> 00:24:13,240
truth from fake news and the way social

584
00:24:11,409 --> 00:24:15,490
media can turn viral has set us up for

585
00:24:13,240 --> 00:24:17,380
serious consequences in the last two

586
00:24:15,490 --> 00:24:19,240
years Reuters had doubled the number of

587
00:24:17,380 --> 00:24:20,980
people who verified video content and

588
00:24:19,240 --> 00:24:23,500
spends time training them how on how to

589
00:24:20,980 --> 00:24:25,900
spot deep fakes all content writers

590
00:24:23,500 --> 00:24:27,820
published is verified by humans and it

591
00:24:25,900 --> 00:24:29,470
uses tech to help like cross-reference

592
00:24:27,820 --> 00:24:31,629
in Google Maps locations or reverse

593
00:24:29,470 --> 00:24:33,370
image searching writer says that in

594
00:24:31,630 --> 00:24:35,110
terms of fake videos it's not enough to

595
00:24:33,370 --> 00:24:40,080
be bunk it you'd have to authenticate it

596
00:24:35,110 --> 00:24:40,080
as well so what could go wrong

597
00:24:40,650 --> 00:24:45,130
advanced in well resource adversaries

598
00:24:42,909 --> 00:24:46,960
will freely develop defects tools so

599
00:24:45,130 --> 00:24:49,299
they can launch character assassinations

600
00:24:46,960 --> 00:24:52,090
against key figures blackmail critical

601
00:24:49,299 --> 00:24:54,490
infrastructure personnel distract and

602
00:24:52,090 --> 00:24:57,070
divert media attention discredit

603
00:24:54,490 --> 00:24:59,169
ideological opponents force conflict

604
00:24:57,070 --> 00:25:01,899
between communities cast doubt on

605
00:24:59,169 --> 00:25:03,760
legitimate evidence taint meaningful

606
00:25:01,900 --> 00:25:05,890
discussions waste investigators

607
00:25:03,760 --> 00:25:08,230
resources or otherwise seize control of

608
00:25:05,890 --> 00:25:11,770
the narrative by replacing reality with

609
00:25:08,230 --> 00:25:13,510
a tailored illusion imagine a fake

610
00:25:11,770 --> 00:25:15,549
parent who calls a child and says

611
00:25:13,510 --> 00:25:17,650
they'll be picked up today describes the

612
00:25:15,549 --> 00:25:20,559
person and the child isn't kidnapped by

613
00:25:17,650 --> 00:25:21,730
the person a bully fakes a video of one

614
00:25:20,559 --> 00:25:24,460
of their victims doing something

615
00:25:21,730 --> 00:25:26,140
embarrassing undermining a rivals

616
00:25:24,460 --> 00:25:29,130
relationship with fake evidence of an

617
00:25:26,140 --> 00:25:32,110
affair so-called revenge porn of course

618
00:25:29,130 --> 00:25:34,390
ransomware or blackmail using fake

619
00:25:32,110 --> 00:25:35,889
videos to extract money or confidential

620
00:25:34,390 --> 00:25:38,020
information from individuals who have

621
00:25:35,890 --> 00:25:40,720
reasonably of the disproving the video

622
00:25:38,020 --> 00:25:43,658
will be hard regulations could be

623
00:25:40,720 --> 00:25:45,850
decimated even if the videos are

624
00:25:43,659 --> 00:25:47,530
ultimately exposed as fakes harmless

625
00:25:45,850 --> 00:25:49,870
spread rapidly but the technical

626
00:25:47,530 --> 00:25:53,620
rebuttals and corrections do not spread

627
00:25:49,870 --> 00:25:55,989
as quickly fake videos could feature

628
00:25:53,620 --> 00:25:58,059
public officials taking bribes uttering

629
00:25:55,990 --> 00:26:00,669
racial slurs or engaging in lascivious

630
00:25:58,059 --> 00:26:04,359
and they could be released the night

631
00:26:00,669 --> 00:26:06,159
before an election soldiers could be

632
00:26:04,359 --> 00:26:08,228
shown murdering innocent civilians in a

633
00:26:06,159 --> 00:26:10,059
war zone precipitating waves of violence

634
00:26:08,229 --> 00:26:12,639
or strategically hampering a war effort

635
00:26:10,059 --> 00:26:14,320
a police officer could be shown shooting

636
00:26:12,639 --> 00:26:17,709
an unarmed man while shouting racial

637
00:26:14,320 --> 00:26:20,499
slurs emergency officials could be

638
00:26:17,710 --> 00:26:23,499
announcing an impending missile strike

639
00:26:20,499 --> 00:26:26,049
on la or an emergent pandemic in New

640
00:26:23,499 --> 00:26:27,729
York provoking panic an audio clip

641
00:26:26,049 --> 00:26:31,629
revealing criminal behavior by a

642
00:26:27,729 --> 00:26:35,859
candidate could be released and not

643
00:26:31,629 --> 00:26:38,168
discredited before it's the night of an

644
00:26:35,859 --> 00:26:40,389
election for example these examples all

645
00:26:38,169 --> 00:26:42,190
emphasize how well executed and well

646
00:26:40,389 --> 00:26:43,689
timed you fake might generate

647
00:26:42,190 --> 00:26:45,999
significant harm in a particular

648
00:26:43,690 --> 00:26:47,889
instance whether the damages to physical

649
00:26:45,999 --> 00:26:50,349
property in life or to the integrity of

650
00:26:47,889 --> 00:26:51,839
an election in government the spread of

651
00:26:50,349 --> 00:26:54,519
the effects could erode the trust

652
00:26:51,839 --> 00:26:56,918
necessary for democracy to function

653
00:26:54,519 --> 00:27:01,059
effectively as a public trust it's

654
00:26:56,919 --> 00:27:02,619
government less and less there's another

655
00:27:01,059 --> 00:27:05,019
consequence of educating the public

656
00:27:02,619 --> 00:27:07,928
about deep fakes research researchers

657
00:27:05,019 --> 00:27:09,969
call it the Liars dividend people will

658
00:27:07,929 --> 00:27:12,249
become skeptical of all videos including

659
00:27:09,969 --> 00:27:13,929
real ones this will allow people who

660
00:27:12,249 --> 00:27:15,700
actually do bad things to claim that

661
00:27:13,929 --> 00:27:18,070
they didn't and escaped accountability

662
00:27:15,700 --> 00:27:20,169
by blaming one a deep fake what does

663
00:27:18,070 --> 00:27:20,710
this mean for court cases and digital

664
00:27:20,169 --> 00:27:22,659
evidence

665
00:27:20,710 --> 00:27:25,629
this means reasonable doubt in virtually

666
00:27:22,659 --> 00:27:28,899
every place where a video and audio is

667
00:27:25,629 --> 00:27:32,699
concerned therefore does video presented

668
00:27:28,899 --> 00:27:35,498
in court eventually equate to hearsay

669
00:27:32,700 --> 00:27:37,899
it's not hard to imagine the rise of a

670
00:27:35,499 --> 00:27:41,139
profitable new dystopian service and

671
00:27:37,899 --> 00:27:43,149
mutable authentication trails the idea

672
00:27:41,139 --> 00:27:44,320
is a person who is sufficiently

673
00:27:43,149 --> 00:27:46,359
interested in protecting themselves

674
00:27:44,320 --> 00:27:48,849
against deep fakes or whose employer

675
00:27:46,359 --> 00:27:50,529
feels this way may be willing to pay for

676
00:27:48,849 --> 00:27:53,080
a service that comprehensively tracks

677
00:27:50,529 --> 00:27:55,299
some or all of their the following their

678
00:27:53,080 --> 00:27:57,609
movements electronic communications

679
00:27:55,299 --> 00:28:00,580
in-person communications and surrounding

680
00:27:57,609 --> 00:28:02,199
viz visuals circumstances the vendor

681
00:28:00,580 --> 00:28:04,299
providing the service to be successful

682
00:28:02,200 --> 00:28:06,039
would have to develop a significant and

683
00:28:04,299 --> 00:28:07,809
sufficient reputation for the

684
00:28:06,039 --> 00:28:09,940
immutability and comprehensiveness of

685
00:28:07,809 --> 00:28:11,680
its data it might then have it to own

686
00:28:09,940 --> 00:28:14,020
arrangements with media platforms

687
00:28:11,680 --> 00:28:16,300
it's beyond to debunk perhaps right very

688
00:28:14,020 --> 00:28:18,600
rapidly get emergency fix impacting its

689
00:28:16,300 --> 00:28:21,730
clients this vendor would have a

690
00:28:18,600 --> 00:28:23,379
significant amount of power whatever the

691
00:28:21,730 --> 00:28:25,720
benefits the social cost of such a

692
00:28:23,380 --> 00:28:28,210
service would be extremely profound it

693
00:28:25,720 --> 00:28:30,520
risks a complete voluntary unraveling

694
00:28:28,210 --> 00:28:32,020
and privacy pressure from friends

695
00:28:30,520 --> 00:28:34,450
employers the community and the

696
00:28:32,020 --> 00:28:35,800
government may exacerbate the trend the

697
00:28:34,450 --> 00:28:38,770
service would require a database of

698
00:28:35,800 --> 00:28:40,870
human behavior of unprecedented depth

699
00:28:38,770 --> 00:28:45,670
and breadth a breach could be the most

700
00:28:40,870 --> 00:28:47,199
significant of such an event ever our

701
00:28:45,670 --> 00:28:50,230
brains do not work like a computer

702
00:28:47,200 --> 00:28:51,250
retrieving data from a disk our memories

703
00:28:50,230 --> 00:28:53,530
can be corrupted

704
00:28:51,250 --> 00:28:55,480
so if images can create false memories

705
00:28:53,530 --> 00:28:57,129
it's not just our present and future

706
00:28:55,480 --> 00:28:57,880
reality that could collapse it's also

707
00:28:57,130 --> 00:29:00,100
our past

708
00:28:57,880 --> 00:29:01,840
Figg media could manipulate what we

709
00:29:00,100 --> 00:29:03,730
remember effectively altering the past

710
00:29:01,840 --> 00:29:07,060
by seeding the population with false

711
00:29:03,730 --> 00:29:10,090
memories Thank You 1984 for calling us

712
00:29:07,060 --> 00:29:12,100
out way ahead of them damaged by a

713
00:29:10,090 --> 00:29:14,379
torrent of constant misinformation

714
00:29:12,100 --> 00:29:15,969
people simply start to give up this is

715
00:29:14,380 --> 00:29:18,070
common in areas where information is

716
00:29:15,970 --> 00:29:20,290
poor and thus assumed to be incorrect

717
00:29:18,070 --> 00:29:22,149
however the adoption of apathy who had

718
00:29:20,290 --> 00:29:23,710
developed society like ours is not a

719
00:29:22,150 --> 00:29:26,080
good path if people stopped paying

720
00:29:23,710 --> 00:29:28,540
attention to news democracy becomes

721
00:29:26,080 --> 00:29:29,679
unstable we have to remain critical and

722
00:29:28,540 --> 00:29:32,800
vigilant when it comes to the

723
00:29:29,680 --> 00:29:34,330
information we consume if you're like me

724
00:29:32,800 --> 00:29:37,460
you're probably wondering at this point

725
00:29:34,330 --> 00:29:40,980
well what about internet cats

726
00:29:37,460 --> 00:29:51,210
well we can't worship false cats people

727
00:29:40,980 --> 00:29:54,180
this has just gone too far so some

728
00:29:51,210 --> 00:29:56,010
possible good case scenarios death is

729
00:29:54,180 --> 00:29:58,920
inevitable and universal but the way to

730
00:29:56,010 --> 00:30:01,620
mourn our dear ones is not in Japan a

731
00:29:58,920 --> 00:30:03,450
robot may create a new way to mourn this

732
00:30:01,620 --> 00:30:05,459
robot is supposed to look and sound like

733
00:30:03,450 --> 00:30:08,610
a loved one so imagine the robot having

734
00:30:05,460 --> 00:30:10,470
a 3d mask of their face your loved one

735
00:30:08,610 --> 00:30:13,139
would have had a hand or at least a face

736
00:30:10,470 --> 00:30:15,930
sorry and creating content for it before

737
00:30:13,140 --> 00:30:19,200
their passing imagine it saying I'm

738
00:30:15,930 --> 00:30:23,900
proud of you or you're such a smart

739
00:30:19,200 --> 00:30:23,900
child as it follows you around the house

740
00:30:26,450 --> 00:30:30,780
Hollywood has had this technology of its

741
00:30:28,650 --> 00:30:32,490
fingertips but not at this low cost so

742
00:30:30,780 --> 00:30:33,840
if they can create a great-looking video

743
00:30:32,490 --> 00:30:35,790
with this technique it'll change the

744
00:30:33,840 --> 00:30:37,949
demand for skilled editors over time

745
00:30:35,790 --> 00:30:39,810
actors are also having their likenesses

746
00:30:37,950 --> 00:30:42,510
digitally preserved so they can continue

747
00:30:39,810 --> 00:30:44,250
working long after they're dead it could

748
00:30:42,510 --> 00:30:46,230
open up new opportunities for instance

749
00:30:44,250 --> 00:30:47,790
making movies with unknown actors and

750
00:30:46,230 --> 00:30:51,150
then superimposing famous celebrities

751
00:30:47,790 --> 00:30:54,600
onto them or including yourself in a

752
00:30:51,150 --> 00:30:56,340
video that Netflix offers a menu that

753
00:30:54,600 --> 00:30:58,620
says who would you like to see as the

754
00:30:56,340 --> 00:31:00,600
star of this movie this could work for

755
00:30:58,620 --> 00:31:02,699
YouTube videos or even news channels

756
00:31:00,600 --> 00:31:03,899
studios could change actors based on

757
00:31:02,700 --> 00:31:07,020
their target market like more

758
00:31:03,900 --> 00:31:09,090
Schwarzenegger for Austrians or just

759
00:31:07,020 --> 00:31:12,240
being able to select anything and

760
00:31:09,090 --> 00:31:14,070
everything in the movie most likely this

761
00:31:12,240 --> 00:31:15,720
will just generate revenue from for the

762
00:31:14,070 --> 00:31:20,040
estates of long-dead actors by bringing

763
00:31:15,720 --> 00:31:23,030
them back to life one day you'll look up

764
00:31:20,040 --> 00:31:25,320
at yourself and see you on a billboard

765
00:31:23,030 --> 00:31:26,910
imagine a world where the ads you see as

766
00:31:25,320 --> 00:31:31,100
you surf the web include your friends

767
00:31:26,910 --> 00:31:33,600
and your family while hyper-realistic

768
00:31:31,100 --> 00:31:35,399
our hyper personalized advertising may

769
00:31:33,600 --> 00:31:37,560
come across as creepy today it could be

770
00:31:35,400 --> 00:31:40,290
the norm in a few years your favorite

771
00:31:37,560 --> 00:31:43,050
clothing company could superimpose your

772
00:31:40,290 --> 00:31:44,460
face on a body that matches yours and

773
00:31:43,050 --> 00:31:47,460
convince you that it's worth trying out

774
00:31:44,460 --> 00:31:49,690
their new line of jackets sizer dot me

775
00:31:47,460 --> 00:31:51,549
is a tool that scans your body

776
00:31:49,690 --> 00:31:53,440
seen in this video to analyze the

777
00:31:51,549 --> 00:31:57,330
perfect fit for clothes it can allow you

778
00:31:53,440 --> 00:31:59,740
to virtually try on garments but image

779
00:31:57,330 --> 00:32:01,960
imagine your actual body showing up in a

780
00:31:59,740 --> 00:32:04,740
targeted ad this is not the future this

781
00:32:01,960 --> 00:32:07,120
is actually happening right now

782
00:32:04,740 --> 00:32:09,039
what about put putting yourself into a

783
00:32:07,120 --> 00:32:11,408
video game some people like to portray

784
00:32:09,039 --> 00:32:14,110
alter egos but others want to see

785
00:32:11,409 --> 00:32:15,970
themselves in the game with 3d avatars

786
00:32:14,110 --> 00:32:18,519
you can kill yourself over and over or

787
00:32:15,970 --> 00:32:20,289
your boss in the Sims or you could watch

788
00:32:18,519 --> 00:32:22,200
yourself slap people with a fish and hit

789
00:32:20,289 --> 00:32:24,879
man

790
00:32:22,200 --> 00:32:27,039
Sarah proc is a new text-to-speech tool

791
00:32:24,879 --> 00:32:29,379
that can synthesize voices it's being

792
00:32:27,039 --> 00:32:31,450
licensed for a number of uses notably

793
00:32:29,379 --> 00:32:34,000
for a radio journalist named Jamie

794
00:32:31,450 --> 00:32:36,549
Dupree who lost his voice after losing

795
00:32:34,000 --> 00:32:38,649
it from a very severe illness not only

796
00:32:36,549 --> 00:32:40,779
does it make the capability to talk and

797
00:32:38,649 --> 00:32:42,549
sing accessible to all individuals this

798
00:32:40,779 --> 00:32:46,750
technology has the potential to change

799
00:32:42,549 --> 00:32:49,059
many industries I'm Jani Dupree in

800
00:32:46,750 --> 00:32:51,039
Washington returning from his European

801
00:32:49,059 --> 00:32:52,870
trip President Trump tweeted from Air

802
00:32:51,039 --> 00:32:55,299
Force One that there is a quote the

803
00:32:52,870 --> 00:33:01,029
chance of a deal with Mexico related to

804
00:32:55,299 --> 00:33:04,029
illegal immigration of course not

805
00:33:01,029 --> 00:33:06,639
everyone likes the synthesized voices so

806
00:33:04,029 --> 00:33:08,500
much some people said that it was weird

807
00:33:06,639 --> 00:33:10,779
and some people said that they got used

808
00:33:08,500 --> 00:33:16,750
to it there's that uncanny valley making

809
00:33:10,779 --> 00:33:19,240
an appearance here some other

810
00:33:16,750 --> 00:33:20,649
interesting ideas anonymity for example

811
00:33:19,240 --> 00:33:21,190
removing the stigma with going to

812
00:33:20,649 --> 00:33:22,570
therapy

813
00:33:21,190 --> 00:33:25,870
where you wouldn't have to show your own

814
00:33:22,570 --> 00:33:27,879
face one idea from youtube user derp fix

815
00:33:25,870 --> 00:33:30,340
who i contacted during when i was

816
00:33:27,879 --> 00:33:32,620
putting this together said that using

817
00:33:30,340 --> 00:33:34,689
models and video interviews to remove

818
00:33:32,620 --> 00:33:38,469
gender or racial bias when hiring is

819
00:33:34,690 --> 00:33:41,799
another good idea and i agree in

820
00:33:38,470 --> 00:33:43,450
addition to zebra fication researchers

821
00:33:41,799 --> 00:33:45,009
have used ganz to modify the weather

822
00:33:43,450 --> 00:33:47,230
conditions in a video converting a

823
00:33:45,009 --> 00:33:49,389
breeze will stay to a windy day they've

824
00:33:47,230 --> 00:33:51,279
aligned blooming and dyeing flowers and

825
00:33:49,389 --> 00:33:54,158
they've sent synthesized of convincing

826
00:33:51,279 --> 00:33:56,409
sun runners from videos on the web the

827
00:33:54,159 --> 00:33:58,389
biggest game that we'll probably see in

828
00:33:56,409 --> 00:34:01,629
the near term is a deep fake learning

829
00:33:58,389 --> 00:34:02,750
will lead to better ganz ganz create

830
00:34:01,629 --> 00:34:05,600
realistic images for

831
00:34:02,750 --> 00:34:08,239
learn from so ganz can be used to

832
00:34:05,600 --> 00:34:11,509
actually generate fake content to train

833
00:34:08,239 --> 00:34:16,339
better ganz this is AI learning for me

834
00:34:11,510 --> 00:34:17,810
AI learning for me I output so ganz a

835
00:34:16,340 --> 00:34:19,609
frequently used to compete against each

836
00:34:17,810 --> 00:34:22,699
other in one instance this was done to

837
00:34:19,609 --> 00:34:24,770
create synthetic MRIs of brands trying

838
00:34:22,699 --> 00:34:26,839
to murrs these tumor images can help

839
00:34:24,770 --> 00:34:28,580
create training data for algorithms to

840
00:34:26,840 --> 00:34:33,440
learn how to spot tumors and actually

841
00:34:28,580 --> 00:34:35,509
get better at doing so in terms of

842
00:34:33,440 --> 00:34:38,000
content validation defects are a total

843
00:34:35,510 --> 00:34:40,849
arms race in person communication is

844
00:34:38,000 --> 00:34:42,530
still reliable for now digital

845
00:34:40,849 --> 00:34:44,869
watermarks could be used potentially

846
00:34:42,530 --> 00:34:46,869
existing digital signature tech could be

847
00:34:44,869 --> 00:34:51,139
used to authenticated videos like emails

848
00:34:46,869 --> 00:34:53,149
Holograms in place of video what about

849
00:34:51,139 --> 00:34:56,450
up an occasion via crypto hashing and

850
00:34:53,149 --> 00:34:58,670
blockchain digital it's at least signing

851
00:34:56,449 --> 00:35:00,259
using asymmetric cryptography still

852
00:34:58,670 --> 00:35:01,910
relies on protecting the private key

853
00:35:00,260 --> 00:35:04,310
which is the same problem we have today

854
00:35:01,910 --> 00:35:06,259
adding digital signatures to cell phone

855
00:35:04,310 --> 00:35:08,450
cameras would also do nothing to address

856
00:35:06,260 --> 00:35:10,310
common sources of false video narrative

857
00:35:08,450 --> 00:35:12,020
since the issue is not whether footage

858
00:35:10,310 --> 00:35:14,420
is real or fake or whether the footage

859
00:35:12,020 --> 00:35:16,609
captures the entire situation and

860
00:35:14,420 --> 00:35:17,750
whether the description assigned to it

861
00:35:16,609 --> 00:35:20,779
represents with the video actually

862
00:35:17,750 --> 00:35:22,760
depicts digital signatures also would

863
00:35:20,780 --> 00:35:24,530
not prevent staged videos which look

864
00:35:22,760 --> 00:35:26,060
like actors or real locations and

865
00:35:24,530 --> 00:35:27,530
materials are used to create false

866
00:35:26,060 --> 00:35:30,730
scenes that are difficult to distinguish

867
00:35:27,530 --> 00:35:32,750
in the poor quality of a phone camera

868
00:35:30,730 --> 00:35:34,670
the truth could be sitting right in

869
00:35:32,750 --> 00:35:37,070
front of us and cryptographically can

870
00:35:34,670 --> 00:35:38,990
secure from the public as it could be

871
00:35:37,070 --> 00:35:40,339
and we'd all be stubbornly refused to

872
00:35:38,990 --> 00:35:44,000
see it because of a buzzword like

873
00:35:40,339 --> 00:35:46,279
blockchain true pic is an interesting

874
00:35:44,000 --> 00:35:47,869
example of validation it verifies an

875
00:35:46,280 --> 00:35:49,940
image hasn't been altered yet and

876
00:35:47,869 --> 00:35:53,210
watermarks with the timestamp geo code

877
00:35:49,940 --> 00:35:54,710
URL and other metadata true pic service

878
00:35:53,210 --> 00:35:56,720
for a version of the photo is signed

879
00:35:54,710 --> 00:35:59,599
with a six digit code and its URL plus a

880
00:35:56,720 --> 00:36:01,549
spot on the blockchain users can post

881
00:35:59,599 --> 00:36:03,200
their Trouper Kimmage ins in apps to

882
00:36:01,550 --> 00:36:04,820
prove they're not catfishing someone on

883
00:36:03,200 --> 00:36:06,700
a dating site selling something broken

884
00:36:04,820 --> 00:36:09,230
on an e-commerce site etc

885
00:36:06,700 --> 00:36:10,549
viewers can visit the URL watermark on

886
00:36:09,230 --> 00:36:12,770
the photo to compare it to the vault

887
00:36:10,550 --> 00:36:14,579
save version to ensure it hasn't been

888
00:36:12,770 --> 00:36:17,349
modified after the fact

889
00:36:14,579 --> 00:36:19,450
like spawning phishing emails education

890
00:36:17,349 --> 00:36:23,099
and awareness is ultimately our best

891
00:36:19,450 --> 00:36:25,180
defense right now how about a story in

892
00:36:23,099 --> 00:36:27,849
1983 at the peak of the Cold War

893
00:36:25,180 --> 00:36:30,009
stanislav petrov was a soviet lieutenant

894
00:36:27,849 --> 00:36:32,260
colonel stationed good of bunker close

895
00:36:30,010 --> 00:36:33,880
to the Soviet capital he was monitoring

896
00:36:32,260 --> 00:36:36,570
the early warning system for detecting

897
00:36:33,880 --> 00:36:39,640
nuclear missile strikes from the US

898
00:36:36,570 --> 00:36:41,500
September 26 1983 the bunker systems

899
00:36:39,640 --> 00:36:44,348
alerted Petrov a missile missile

900
00:36:41,500 --> 00:36:46,119
launched from Montana the USS and USSR

901
00:36:44,349 --> 00:36:48,849
were following the mutually assured

902
00:36:46,119 --> 00:36:50,980
destruction doctrine if the Americans

903
00:36:48,849 --> 00:36:52,359
were to launch a nuclear attack the

904
00:36:50,980 --> 00:36:54,010
Soviets would have retaliated with a

905
00:36:52,359 --> 00:36:56,170
massive counter-attack ensuring the

906
00:36:54,010 --> 00:36:58,119
annihilation of both countries had

907
00:36:56,170 --> 00:37:00,400
Petrov alerted the superiors he would

908
00:36:58,119 --> 00:37:02,920
have started a World War 3 but instead

909
00:37:00,400 --> 00:37:05,140
he Petrov correctly recognized that it

910
00:37:02,920 --> 00:37:07,869
was unlikely for the u.s. to actually

911
00:37:05,140 --> 00:37:10,750
lock launch only five missiles as he was

912
00:37:07,869 --> 00:37:12,250
seeing it was a bug he reported it as a

913
00:37:10,750 --> 00:37:14,500
computer malfunction and instead of an

914
00:37:12,250 --> 00:37:16,300
attack he was right and World War three

915
00:37:14,500 --> 00:37:19,869
was avoided thanks to the critical

916
00:37:16,300 --> 00:37:22,089
judgement of a man another scenario

917
00:37:19,869 --> 00:37:23,950
known as diplomacy manipulation is when

918
00:37:22,089 --> 00:37:25,390
a malicious enemy uses defects who

919
00:37:23,950 --> 00:37:28,660
create the belief that an event has

920
00:37:25,390 --> 00:37:31,328
occurred to influence geopolitics a few

921
00:37:28,660 --> 00:37:32,859
years in the future say 2025

922
00:37:31,329 --> 00:37:35,680
the situation between North Korea and

923
00:37:32,859 --> 00:37:37,720
the u.s. is still tense CNN receives a

924
00:37:35,680 --> 00:37:39,910
video from an anonymous source kim

925
00:37:37,720 --> 00:37:42,669
jeong-hoon appears to be in a secured

926
00:37:39,910 --> 00:37:44,710
facility with his generals this footage

927
00:37:42,670 --> 00:37:46,510
has never since seen before korean

928
00:37:44,710 --> 00:37:47,829
interpreters are called the supreme

929
00:37:46,510 --> 00:37:49,660
leader is demanding the launch of a

930
00:37:47,829 --> 00:37:53,410
nuclear missile strike in a matter of

931
00:37:49,660 --> 00:37:55,810
hours the video gets the Oval Office and

932
00:37:53,410 --> 00:37:57,609
intelligence can't defer Myr deny the

933
00:37:55,810 --> 00:38:00,009
authenticity even by consulting

934
00:37:57,609 --> 00:38:02,140
additional sources the US President must

935
00:38:00,010 --> 00:38:04,420
act he orders a preventive attack the

936
00:38:02,140 --> 00:38:07,000
war starts but was the video evidence

937
00:38:04,420 --> 00:38:08,230
enough to justify such a decision this

938
00:38:07,000 --> 00:38:11,410
is information warfare

939
00:38:08,230 --> 00:38:13,119
how will we fight state-sponsored and

940
00:38:11,410 --> 00:38:17,348
targeted disinformation and propaganda

941
00:38:13,119 --> 00:38:19,060
in the era of deep fix if society wants

942
00:38:17,349 --> 00:38:20,710
to avoid the worst of these consequences

943
00:38:19,060 --> 00:38:23,440
we have to take action now to protect

944
00:38:20,710 --> 00:38:26,319
the misuse of these technologies so a

945
00:38:23,440 --> 00:38:28,269
multi-prong technology is required

946
00:38:26,319 --> 00:38:29,769
clarifying existing laws and their

947
00:38:28,269 --> 00:38:31,868
impact on creation of faking

948
00:38:29,769 --> 00:38:34,118
applications or the production and

949
00:38:31,869 --> 00:38:35,589
distribution of fake images platforms

950
00:38:34,119 --> 00:38:37,359
will have to decide whether to be

951
00:38:35,589 --> 00:38:40,479
accountable for their users content and

952
00:38:37,359 --> 00:38:42,249
when improving counter technology to

953
00:38:40,479 --> 00:38:44,589
look deeper at images to identify

954
00:38:42,249 --> 00:38:47,189
signals that the image is fake strong

955
00:38:44,589 --> 00:38:49,900
anti defect policies and all media in

956
00:38:47,189 --> 00:38:51,399
agencies better education of young

957
00:38:49,900 --> 00:38:53,919
people about the consequences of

958
00:38:51,400 --> 00:38:55,599
misusing this technology and of course

959
00:38:53,919 --> 00:39:01,348
being skeptical of everything we see in

960
00:38:55,599 --> 00:39:03,669
here media occasionally gets it wrong

961
00:39:01,349 --> 00:39:05,559
the AAP deleted the tweet about a

962
00:39:03,669 --> 00:39:08,650
giraffe being born because it included

963
00:39:05,559 --> 00:39:09,969
the photo of the wrong dress but you

964
00:39:08,650 --> 00:39:11,979
won't be able to trust any media

965
00:39:09,969 --> 00:39:13,900
broadcaster you consider reputable if

966
00:39:11,979 --> 00:39:15,788
you cannot rely on the judgement for

967
00:39:13,900 --> 00:39:20,079
discerning discerning real and fake

968
00:39:15,789 --> 00:39:24,249
sources of information so I leave you

969
00:39:20,079 --> 00:39:26,650
with this video sorry does anything

970
00:39:24,249 --> 00:39:28,509
matter if nothing is real by remaining

971
00:39:26,650 --> 00:39:31,150
skeptical as a society we will likely

972
00:39:28,509 --> 00:39:35,769
ultimately reach agreement collectively

973
00:39:31,150 --> 00:39:37,479
about the authenticity of content if we

974
00:39:35,769 --> 00:39:39,189
have to take the blue pill instead the

975
00:39:37,479 --> 00:39:40,198
story ends you wake up in your bed and

976
00:39:39,189 --> 00:39:42,339
believe whatever you want to believe

977
00:39:40,199 --> 00:39:44,109
that's why we need to take the red pill

978
00:39:42,339 --> 00:39:46,659
to stay in Wonderland and find out how

979
00:39:44,109 --> 00:39:48,279
deep this rabbit hole goes all we hope

980
00:39:46,659 --> 00:39:56,910
for all we can hope for is the truth

981
00:39:48,279 --> 00:40:05,640
nothing more thank you

982
00:39:56,910 --> 00:40:07,980
[Applause]

983
00:40:05,640 --> 00:40:18,069
so I think we have time for questions

984
00:40:07,980 --> 00:40:19,870
yeah okay yes so the question is about

985
00:40:18,070 --> 00:40:22,090
live-streaming is there any kind of

986
00:40:19,870 --> 00:40:25,509
concern about that yeah I think there is

987
00:40:22,090 --> 00:40:27,400
if if you can do the deep video puppetry

988
00:40:25,510 --> 00:40:29,500
which is not changing the original face

989
00:40:27,400 --> 00:40:32,140
and just manipulating the facial

990
00:40:29,500 --> 00:40:34,480
features like the Obama video if you can

991
00:40:32,140 --> 00:40:37,500
do that in real time you can do that

992
00:40:34,480 --> 00:40:41,020
with somebody sitting at a computer

993
00:40:37,500 --> 00:40:44,140
either using voice synthesizers to

994
00:40:41,020 --> 00:40:46,150
create the voice via text or you could

995
00:40:44,140 --> 00:40:48,970
have somebody doing an impression so

996
00:40:46,150 --> 00:40:51,930
there's no reason to believe that any

997
00:40:48,970 --> 00:40:55,319
video whether recorded or real-time is

998
00:40:51,930 --> 00:40:56,669
not being faked unless you can detect it

999
00:40:55,320 --> 00:41:07,900
thank you

1000
00:40:56,670 --> 00:41:11,850
did you a question yeah so the question

1001
00:41:07,900 --> 00:41:14,050
is about stock manipulation I see you

1002
00:41:11,850 --> 00:41:16,810
how can this be used to manipulate

1003
00:41:14,050 --> 00:41:19,240
stocks I mean I think that defamation of

1004
00:41:16,810 --> 00:41:22,360
a CEO something like that showing them

1005
00:41:19,240 --> 00:41:24,100
you know cheating on somebody or shaking

1006
00:41:22,360 --> 00:41:25,330
hands with you know Kim jong-un or

1007
00:41:24,100 --> 00:41:27,850
something like that kind of stuff can

1008
00:41:25,330 --> 00:41:30,810
can lead to real consequences for people

1009
00:41:27,850 --> 00:41:30,810
yes

1010
00:42:12,849 --> 00:42:19,789
right so the the comment was about if if

1011
00:42:18,470 --> 00:42:22,700
there was an incoming missile we'd have

1012
00:42:19,789 --> 00:42:24,680
other things that would detect it rather

1013
00:42:22,700 --> 00:42:26,390
than one individual system and that's

1014
00:42:24,680 --> 00:42:28,399
true that's that's why I think that the

1015
00:42:26,390 --> 00:42:29,779
multi-pronged approach of like having

1016
00:42:28,400 --> 00:42:31,940
somebody think about what's happening

1017
00:42:29,779 --> 00:42:37,160
not just having computers react is

1018
00:42:31,940 --> 00:42:39,049
important and but being able to

1019
00:42:37,160 --> 00:42:41,180
detective in the first place is still a

1020
00:42:39,049 --> 00:42:43,910
real challenge so in terms of this

1021
00:42:41,180 --> 00:42:45,799
technology with missiles yeah we have

1022
00:42:43,910 --> 00:42:47,808
some other things so yeah I mean World

1023
00:42:45,799 --> 00:42:53,559
War II may not start because of a deep

1024
00:42:47,809 --> 00:42:53,559
fake but it's possible yes

1025
00:43:17,940 --> 00:43:24,630
yeah the question is about tabloids and

1026
00:43:21,340 --> 00:43:26,830
if this will further their income and

1027
00:43:24,630 --> 00:43:29,650
make it harder for the people that are

1028
00:43:26,830 --> 00:43:33,700
fighting the tabloids already I I mean I

1029
00:43:29,650 --> 00:43:36,640
think that the tabloids people already

1030
00:43:33,700 --> 00:43:38,560
kind of know not to trust them but that

1031
00:43:36,640 --> 00:43:41,950
there is there is a bit of mistrust

1032
00:43:38,560 --> 00:43:45,310
there already definitely not enough and

1033
00:43:41,950 --> 00:43:48,580
definitely not everyone I'm confident

1034
00:43:45,310 --> 00:43:53,170
that they are already producing fake

1035
00:43:48,580 --> 00:43:56,080
photos if not videos and I mean there's

1036
00:43:53,170 --> 00:43:57,370
all kinds of things online I don't know

1037
00:43:56,080 --> 00:43:58,870
but we don't know who's producing them

1038
00:43:57,370 --> 00:44:01,060
we don't know if it's the tabloids it's

1039
00:43:58,870 --> 00:44:03,130
producing it's probably somebody in a

1040
00:44:01,060 --> 00:44:05,410
basement somewhere that's that's making

1041
00:44:03,130 --> 00:44:07,330
these and trying to sell them and yeah

1042
00:44:05,410 --> 00:44:09,700
they're gonna make a quick buck off of

1043
00:44:07,330 --> 00:44:11,890
it if they put it up in the first 24

1044
00:44:09,700 --> 00:44:15,100
hours there they're gonna get a lot of

1045
00:44:11,890 --> 00:44:16,900
hits on that because we don't have a

1046
00:44:15,100 --> 00:44:20,529
good way to disprove it how do you just

1047
00:44:16,900 --> 00:44:21,730
prove that somebody wasn't you know

1048
00:44:20,530 --> 00:44:24,160
doing something completely inappropriate

1049
00:44:21,730 --> 00:44:26,950
in their bedroom how do you say that

1050
00:44:24,160 --> 00:44:29,830
wasn't me if there's a video of it we

1051
00:44:26,950 --> 00:44:32,230
just don't have which don't have that

1052
00:44:29,830 --> 00:44:33,370
technology or capability and people

1053
00:44:32,230 --> 00:44:35,020
aren't going to believe you they're

1054
00:44:33,370 --> 00:44:37,359
gonna believe the first thing they see

1055
00:44:35,020 --> 00:44:41,820
and then everything after that doesn't

1056
00:44:37,360 --> 00:44:41,820
matter that's what I think yes

1057
00:44:51,300 --> 00:44:58,210
yeah the comment was about having a

1058
00:44:55,330 --> 00:44:59,740
public and transparent effort to build

1059
00:44:58,210 --> 00:45:01,720
detection technologies I think that's

1060
00:44:59,740 --> 00:45:03,720
true I think that there's been a lot of

1061
00:45:01,720 --> 00:45:06,368
media coverage about the fakes and

1062
00:45:03,720 --> 00:45:07,569
talking about a lot of the bad things

1063
00:45:06,369 --> 00:45:09,460
that can happen I tried to talk about

1064
00:45:07,570 --> 00:45:11,380
some of the good things we could do but

1065
00:45:09,460 --> 00:45:13,420
the media has been covering it really

1066
00:45:11,380 --> 00:45:16,180
well because they have to deal with it

1067
00:45:13,420 --> 00:45:18,520
on it probably a daily basis I think

1068
00:45:16,180 --> 00:45:20,529
that the government getting talking

1069
00:45:18,520 --> 00:45:24,099
about it a little bit more they put out

1070
00:45:20,530 --> 00:45:27,130
some like the Israeli military put out a

1071
00:45:24,099 --> 00:45:33,940
warning and you know I don't know how

1072
00:45:27,130 --> 00:45:35,320
useful that is right yeah before if

1073
00:45:33,940 --> 00:45:37,089
you're gonna be designing something

1074
00:45:35,320 --> 00:45:40,020
that's cryptographically going to track

1075
00:45:37,089 --> 00:45:43,270
this stuff it should be an open forum of

1076
00:45:40,020 --> 00:45:46,330
you know well-known researchers and and

1077
00:45:43,270 --> 00:45:48,550
cryptographic specialists that can add

1078
00:45:46,330 --> 00:45:50,290
to that and make sure that it's going to

1079
00:45:48,550 --> 00:45:53,380
be the best that it can be absolutely

1080
00:45:50,290 --> 00:45:56,700
yeah and then as an open system

1081
00:45:53,380 --> 00:45:56,700
are you angry yes

1082
00:46:59,170 --> 00:47:02,280
[Music]

1083
00:47:06,320 --> 00:47:09,320
right

1084
00:47:43,450 --> 00:47:48,848
so the question is about accountability

1085
00:47:46,359 --> 00:47:50,558
for people that are reporting false

1086
00:47:48,849 --> 00:47:53,020
things or that have reported false

1087
00:47:50,559 --> 00:47:55,990
things I think that really falls to the

1088
00:47:53,020 --> 00:47:59,799
policy of the media organization we

1089
00:47:55,990 --> 00:48:01,598
already have lots of okay we already

1090
00:47:59,799 --> 00:48:06,400
have I don't think that that's going to

1091
00:48:01,599 --> 00:48:08,470
be unless we change see the 80 30 I

1092
00:48:06,400 --> 00:48:09,849
don't think that government's going to

1093
00:48:08,470 --> 00:48:11,049
be very effective in this and I don't

1094
00:48:09,849 --> 00:48:14,770
even know if I want government to be

1095
00:48:11,049 --> 00:48:16,809
involved in this frankly but I I think

1096
00:48:14,770 --> 00:48:19,750
that it comes down to the individual

1097
00:48:16,809 --> 00:48:21,609
sources like do you trust CNN versus the

1098
00:48:19,750 --> 00:48:23,530
onion like which one like we know that

1099
00:48:21,609 --> 00:48:25,720
one of them is gonna have some false

1100
00:48:23,530 --> 00:48:27,130
stuff or a lot of false stuff whereas

1101
00:48:25,720 --> 00:48:28,569
the other one might only have false

1102
00:48:27,130 --> 00:48:30,549
stuff once in a while and in terms of

1103
00:48:28,569 --> 00:48:32,740
accountability you can print a

1104
00:48:30,549 --> 00:48:36,309
retraction but if it happens enough

1105
00:48:32,740 --> 00:48:38,049
times eventually that that journalists

1106
00:48:36,309 --> 00:48:40,690
or that reporter or whoever is doing the

1107
00:48:38,049 --> 00:48:41,859
work of putting that out there is going

1108
00:48:40,690 --> 00:48:44,230
to get a bad name in the industry

1109
00:48:41,859 --> 00:48:46,180
they're gonna ruin their careers and it

1110
00:48:44,230 --> 00:48:48,069
will almost take care of itself but

1111
00:48:46,180 --> 00:48:49,240
that's that's that takes some time it's

1112
00:48:48,069 --> 00:48:51,819
not gonna be happen the first time

1113
00:48:49,240 --> 00:48:53,890
unless it's a huge thing that you know

1114
00:48:51,819 --> 00:48:57,579
goes crazy but like the Nancy Pelosi

1115
00:48:53,890 --> 00:48:59,078
thing that that started on you know it

1116
00:48:57,579 --> 00:49:03,190
was uploaded to one site and it just

1117
00:48:59,079 --> 00:49:05,440
went everywhere because blogs and news

1118
00:49:03,190 --> 00:49:07,569
outlets will repost things from other

1119
00:49:05,440 --> 00:49:10,210
blogs and news outlets without actually

1120
00:49:07,569 --> 00:49:13,240
checking into it and I can and hopefully

1121
00:49:10,210 --> 00:49:16,240
the public will start to become more

1122
00:49:13,240 --> 00:49:19,779
aware as that you know like who's

1123
00:49:16,240 --> 00:49:21,279
trustable it's just like in our industry

1124
00:49:19,780 --> 00:49:24,130
we know that there are certain people

1125
00:49:21,280 --> 00:49:27,130
who put up good content they do things

1126
00:49:24,130 --> 00:49:28,599
there trustable there you know we trust

1127
00:49:27,130 --> 00:49:30,609
them to do tests on our stuff without

1128
00:49:28,599 --> 00:49:32,710
stealing our information what kind of

1129
00:49:30,609 --> 00:49:36,900
thing so I think it really comes down to

1130
00:49:32,710 --> 00:49:39,900
reputation and policy of the news outlet

1131
00:49:36,900 --> 00:49:39,900
yes

1132
00:49:59,450 --> 00:50:05,490
so the question is about the success of

1133
00:50:02,880 --> 00:50:07,349
Star Wars leading to eventually not

1134
00:50:05,490 --> 00:50:09,569
needing actors and actresses and what

1135
00:50:07,349 --> 00:50:11,130
the concern is there I have no concern I

1136
00:50:09,569 --> 00:50:16,230
don't make any money from those movies

1137
00:50:11,130 --> 00:50:18,869
so you know I think that anytime there

1138
00:50:16,230 --> 00:50:20,490
is a new technology like toll booth like

1139
00:50:18,869 --> 00:50:23,130
automated toll booths there's going to

1140
00:50:20,490 --> 00:50:25,288
be people who bear the brunt of that new

1141
00:50:23,130 --> 00:50:27,210
technology and they're know they're

1142
00:50:25,289 --> 00:50:29,789
absolute their jobs are obsolete so toll

1143
00:50:27,210 --> 00:50:32,579
booth operators for example or key grips

1144
00:50:29,789 --> 00:50:34,109
or you know whatever the thing is yeah I

1145
00:50:32,579 --> 00:50:36,869
think eventually we'll be able to

1146
00:50:34,109 --> 00:50:38,848
completely put out movies and content

1147
00:50:36,869 --> 00:50:43,769
and games and other things that have no

1148
00:50:38,849 --> 00:50:45,839
actors but will we want to see them will

1149
00:50:43,769 --> 00:50:48,089
we enjoy them as much as seeing an

1150
00:50:45,839 --> 00:50:49,650
actual person that you know like like

1151
00:50:48,089 --> 00:50:51,359
everybody loves you know Tom Cruise

1152
00:50:49,650 --> 00:50:53,009
movies or whatever right like people go

1153
00:50:51,359 --> 00:50:55,470
to see Tom Cruise they don't go to see

1154
00:50:53,009 --> 00:50:57,839
an image generator but you know who

1155
00:50:55,470 --> 00:50:59,819
knows in like a 50 or 100 years people

1156
00:50:57,839 --> 00:51:05,069
may not care about that anymore they may

1157
00:50:59,819 --> 00:51:07,259
not have ever been exposed to it so yeah

1158
00:51:05,069 --> 00:51:10,410
I mean you could you could make anything

1159
00:51:07,259 --> 00:51:12,359
anytime anywhere at very cheaply all

1160
00:51:10,410 --> 00:51:13,410
right April I hate to interrupt you I

1161
00:51:12,359 --> 00:51:16,200
believe you'll probably be available

1162
00:51:13,410 --> 00:51:17,700
after for more questions okay so if you

1163
00:51:16,200 --> 00:51:19,288
guys have more questions or comments for

1164
00:51:17,700 --> 00:51:21,490
April catch her afterwards thank you

1165
00:51:19,289 --> 00:51:27,650
very much one more time for April right

1166
00:51:21,490 --> 00:51:27,649
[Applause]

