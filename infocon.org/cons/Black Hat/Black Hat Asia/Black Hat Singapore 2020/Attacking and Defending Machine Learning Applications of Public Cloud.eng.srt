1
00:00:01,599 --> 00:00:03,919
hello everyone i'm very glad to have

2
00:00:03,919 --> 00:00:06,080
this opportunity to participate in the

3
00:00:06,080 --> 00:00:08,080
black house security meeting

4
00:00:08,080 --> 00:00:10,240
and it is also a great honor to share

5
00:00:10,240 --> 00:00:12,240
our achievement here

6
00:00:12,240 --> 00:00:14,240
today my topic is attacking and

7
00:00:14,240 --> 00:00:16,079
defending machine learning application

8
00:00:16,079 --> 00:00:18,960
of public cloud my presentation is going

9
00:00:18,960 --> 00:00:21,279
to focus on three topics

10
00:00:21,279 --> 00:00:23,439
first the attack method for the

11
00:00:23,439 --> 00:00:26,800
classification service of public cloud

12
00:00:26,800 --> 00:00:29,119
next the combination of adversarial

13
00:00:29,119 --> 00:00:30,080
attacks and

14
00:00:30,080 --> 00:00:33,120
characteristics of cloud service

15
00:00:33,120 --> 00:00:35,680
and the last introduce the procedure and

16
00:00:35,680 --> 00:00:37,840
method that required for a secure

17
00:00:37,840 --> 00:00:39,520
development of machine learning

18
00:00:39,520 --> 00:00:42,640
application this content above

19
00:00:42,640 --> 00:00:45,920
can reduce the serious attack problem ai

20
00:00:45,920 --> 00:00:49,440
service currently facing

21
00:00:50,160 --> 00:00:52,399
briefly introduce our team's field of

22
00:00:52,399 --> 00:00:53,199
work

23
00:00:53,199 --> 00:00:55,680
the direction of our work as air model

24
00:00:55,680 --> 00:00:56,719
security

25
00:00:56,719 --> 00:00:59,039
including research on the model attack

26
00:00:59,039 --> 00:01:00,160
and defense

27
00:01:00,160 --> 00:01:03,280
methods and also provides solutions to

28
00:01:03,280 --> 00:01:06,640
security risk of various ai applications

29
00:01:06,640 --> 00:01:08,960
like fake face detection and model

30
00:01:08,960 --> 00:01:10,840
robustness

31
00:01:10,840 --> 00:01:13,119
evaluation they are included

32
00:01:13,119 --> 00:01:16,960
in the ai security toolkit adv box

33
00:01:16,960 --> 00:01:19,200
we also have several heavyweight

34
00:01:19,200 --> 00:01:20,960
security projects

35
00:01:20,960 --> 00:01:24,159
such as massage links open rasp

36
00:01:24,159 --> 00:01:26,799
and most of them are placed on github as

37
00:01:26,799 --> 00:01:29,280
open source

38
00:01:29,280 --> 00:01:31,280
the following is the main topic of

39
00:01:31,280 --> 00:01:33,119
today's discussion

40
00:01:33,119 --> 00:01:35,439
first we will show the attack against

41
00:01:35,439 --> 00:01:37,520
the machine learning application

42
00:01:37,520 --> 00:01:39,439
and then introduce defense and

43
00:01:39,439 --> 00:01:41,119
mitigation measures

44
00:01:41,119 --> 00:01:43,759
these two aspects will comprehensively

45
00:01:43,759 --> 00:01:46,079
illustrate the security development

46
00:01:46,079 --> 00:01:48,240
process of machine learning service on

47
00:01:48,240 --> 00:01:50,880
public cloud

48
00:01:50,880 --> 00:01:53,840
at present when ai brings us convenience

49
00:01:53,840 --> 00:01:55,600
and a surprise

50
00:01:55,600 --> 00:01:58,240
because of the existence of adversarial

51
00:01:58,240 --> 00:01:59,119
example

52
00:01:59,119 --> 00:02:01,759
the safety of ai has to be mentioned and

53
00:02:01,759 --> 00:02:03,360
discussed

54
00:02:03,360 --> 00:02:06,000
while ai application ranges more widely

55
00:02:06,000 --> 00:02:06,880
nowadays

56
00:02:06,880 --> 00:02:09,598
ai safety will become the first need of

57
00:02:09,598 --> 00:02:11,120
consumers

58
00:02:11,120 --> 00:02:14,319
various image analysis method emerged

59
00:02:14,319 --> 00:02:15,440
from the earliest

60
00:02:15,440 --> 00:02:18,480
stage of image classification

61
00:02:18,480 --> 00:02:21,440
to object detection 3d detection

62
00:02:21,440 --> 00:02:23,920
semantic segmentation

63
00:02:23,920 --> 00:02:27,040
fake face and etc

64
00:02:27,040 --> 00:02:29,599
among all applications image

65
00:02:29,599 --> 00:02:31,280
classification is the most

66
00:02:31,280 --> 00:02:35,440
essential part of image analysis

67
00:02:35,440 --> 00:02:37,680
most id companies provide the image

68
00:02:37,680 --> 00:02:39,760
classification service

69
00:02:39,760 --> 00:02:42,239
and most of them can be displayed

70
00:02:42,239 --> 00:02:44,239
through cloud api

71
00:02:44,239 --> 00:02:46,800
for example you can upload an image to

72
00:02:46,800 --> 00:02:47,680
ask google

73
00:02:47,680 --> 00:02:50,239
what is it and it will give you a write

74
00:02:50,239 --> 00:02:52,640
description of the image you sent

75
00:02:52,640 --> 00:02:56,239
in this case a cat however at the same

76
00:02:56,239 --> 00:02:56,720
time

77
00:02:56,720 --> 00:02:59,200
an attacker can make the cloud believe

78
00:02:59,200 --> 00:03:01,280
that it is the dog

79
00:03:01,280 --> 00:03:03,280
by simply adding some carefully

80
00:03:03,280 --> 00:03:04,640
constructed noises

81
00:03:04,640 --> 00:03:08,400
to the original image such image sample

82
00:03:08,400 --> 00:03:11,040
is called the adversarial example which

83
00:03:11,040 --> 00:03:12,159
aims to for

84
00:03:12,159 --> 00:03:16,800
ai classifiers generating adversarial

85
00:03:16,800 --> 00:03:20,239
example is not as easy as it sounds like

86
00:03:20,239 --> 00:03:23,200
it is a difficult task and it becomes

87
00:03:23,200 --> 00:03:24,959
more complicated if you want to

88
00:03:24,959 --> 00:03:27,200
influence multiple cloud classification

89
00:03:27,200 --> 00:03:27,760
services

90
00:03:27,760 --> 00:03:31,280
at the same time but our method

91
00:03:31,280 --> 00:03:33,440
is able to create a one adversarial

92
00:03:33,440 --> 00:03:36,080
image that is misclassified on multiple

93
00:03:36,080 --> 00:03:38,959
cloud service

94
00:03:39,280 --> 00:03:42,080
the security of ai model comes from the

95
00:03:42,080 --> 00:03:43,680
transparency of its

96
00:03:43,680 --> 00:03:47,040
important information such as network

97
00:03:47,040 --> 00:03:47,920
structure

98
00:03:47,920 --> 00:03:51,280
parameters and the input if all this

99
00:03:51,280 --> 00:03:54,239
information is open it will be an open

100
00:03:54,239 --> 00:03:56,720
interest for a detector

101
00:03:56,720 --> 00:03:59,040
they can easily generate a serial

102
00:03:59,040 --> 00:03:59,760
example

103
00:03:59,760 --> 00:04:02,640
against the model in that case we call

104
00:04:02,640 --> 00:04:04,720
it the wet box

105
00:04:04,720 --> 00:04:08,000
when the original image is modified

106
00:04:08,000 --> 00:04:10,400
the cost of image distortion is very

107
00:04:10,400 --> 00:04:11,280
small

108
00:04:11,280 --> 00:04:13,840
and it is almost impossible to find out

109
00:04:13,840 --> 00:04:14,640
any attack

110
00:04:14,640 --> 00:04:19,040
traits so unless the ai model is for

111
00:04:19,040 --> 00:04:20,399
education use

112
00:04:20,399 --> 00:04:22,880
wet box should be avoided especially

113
00:04:22,880 --> 00:04:25,199
model for commercial uses

114
00:04:25,199 --> 00:04:27,280
using a white box model is very

115
00:04:27,280 --> 00:04:28,400
dangerous even

116
00:04:28,400 --> 00:04:32,400
for transferable training

117
00:04:33,600 --> 00:04:36,080
the opposite of the white box is the

118
00:04:36,080 --> 00:04:37,520
black box model

119
00:04:37,520 --> 00:04:40,160
which attackers don't have the access to

120
00:04:40,160 --> 00:04:42,000
critical information of the model

121
00:04:42,000 --> 00:04:45,120
directly but he can repeatedly try to

122
00:04:45,120 --> 00:04:47,280
verify the effect of attacks

123
00:04:47,280 --> 00:04:49,440
and guess the important information of

124
00:04:49,440 --> 00:04:50,639
the model

125
00:04:50,639 --> 00:04:53,680
that is also very difficult unlike

126
00:04:53,680 --> 00:04:55,919
password which can be used repeatedly

127
00:04:55,919 --> 00:04:57,840
once it is cracked

128
00:04:57,840 --> 00:05:00,560
the value of a single adversarial image

129
00:05:00,560 --> 00:05:02,080
is negligible

130
00:05:02,080 --> 00:05:04,240
it is like cracking one encrypted

131
00:05:04,240 --> 00:05:06,160
message by brute force

132
00:05:06,160 --> 00:05:09,680
the significance is low but the cost is

133
00:05:09,680 --> 00:05:12,639
extremely high

134
00:05:12,639 --> 00:05:15,280
the ai service on cloud are also a type

135
00:05:15,280 --> 00:05:16,240
of black box

136
00:05:16,240 --> 00:05:18,960
model but they have less information and

137
00:05:18,960 --> 00:05:21,280
more limitations than the black box

138
00:05:21,280 --> 00:05:23,280
model we talked about

139
00:05:23,280 --> 00:05:26,320
besides unknown structure and parameters

140
00:05:26,320 --> 00:05:29,280
the pre-modification applied on input is

141
00:05:29,280 --> 00:05:30,639
also sealed

142
00:05:30,639 --> 00:05:33,120
like clipping filtering and a fine

143
00:05:33,120 --> 00:05:34,720
transformation

144
00:05:34,720 --> 00:05:36,720
there are many bottlenecks for using

145
00:05:36,720 --> 00:05:38,639
repeated attack to obtain

146
00:05:38,639 --> 00:05:41,280
extra information of the model because

147
00:05:41,280 --> 00:05:43,600
the model can set a restriction on the

148
00:05:43,600 --> 00:05:45,600
requested frequency

149
00:05:45,600 --> 00:05:48,000
the cost and difficulty of attack will

150
00:05:48,000 --> 00:05:52,400
be enormously high in this in that case

151
00:05:52,400 --> 00:05:55,600
understanding the difficulty of attack

152
00:05:55,600 --> 00:05:57,680
let's look at some common method of

153
00:05:57,680 --> 00:05:59,680
cloud-based attack

154
00:05:59,680 --> 00:06:02,560
first attacks based on query which

155
00:06:02,560 --> 00:06:04,319
repeatedly requests the service

156
00:06:04,319 --> 00:06:06,639
thousands of times

157
00:06:06,639 --> 00:06:08,800
although we can't get the gradient of

158
00:06:08,800 --> 00:06:10,319
the model directly

159
00:06:10,319 --> 00:06:12,960
we still are able to obtain some import

160
00:06:12,960 --> 00:06:15,199
important information of the black box

161
00:06:15,199 --> 00:06:16,400
model

162
00:06:16,400 --> 00:06:19,520
however it takes more than 1000 requests

163
00:06:19,520 --> 00:06:22,400
to reveal a portion of a model using low

164
00:06:22,400 --> 00:06:24,240
resolution images

165
00:06:24,240 --> 00:06:26,720
the cause of using high resolution image

166
00:06:26,720 --> 00:06:27,360
is

167
00:06:27,360 --> 00:06:30,639
several times higher the second method

168
00:06:30,639 --> 00:06:32,560
transfer learning attacks

169
00:06:32,560 --> 00:06:35,520
the adversarial example is transferable

170
00:06:35,520 --> 00:06:37,120
under similar dns

171
00:06:37,120 --> 00:06:40,240
structure the work of don son has

172
00:06:40,240 --> 00:06:41,600
verified this statement

173
00:06:41,600 --> 00:06:45,520
in 2016. the comparison chart in the

174
00:06:45,520 --> 00:06:46,160
paper

175
00:06:46,160 --> 00:06:49,599
is cited here it shows that a serial

176
00:06:49,599 --> 00:06:50,080
effect

177
00:06:50,080 --> 00:06:52,880
can be transferred to some degree by

178
00:06:52,880 --> 00:06:55,360
using similar method to train the model

179
00:06:55,360 --> 00:06:57,599
with a similar function

180
00:06:57,599 --> 00:06:59,680
but the challenge is how to find the

181
00:06:59,680 --> 00:07:00,960
open source model

182
00:07:00,960 --> 00:07:02,800
that are similar to the model on the

183
00:07:02,800 --> 00:07:05,360
cloud because parameters and the

184
00:07:05,360 --> 00:07:08,240
structure of the model are hidden

185
00:07:08,240 --> 00:07:11,039
attacking on cloud-based ai service has

186
00:07:11,039 --> 00:07:12,800
certain conditions

187
00:07:12,800 --> 00:07:14,960
some of them are very difficult to

188
00:07:14,960 --> 00:07:17,120
achieve is that saying that

189
00:07:17,120 --> 00:07:19,840
cloud service are safe enough i would

190
00:07:19,840 --> 00:07:22,719
say probably no

191
00:07:24,720 --> 00:07:27,360
here is an example of using our method

192
00:07:27,360 --> 00:07:29,199
against the cloud classification

193
00:07:29,199 --> 00:07:30,639
services

194
00:07:30,639 --> 00:07:34,000
this is a british long-haired cat google

195
00:07:34,000 --> 00:07:36,160
image classification gives the right

196
00:07:36,160 --> 00:07:38,639
answer

197
00:07:38,880 --> 00:07:41,599
this is an adversary example generated

198
00:07:41,599 --> 00:07:43,199
by our method

199
00:07:43,199 --> 00:07:45,440
although it is impossible to get the

200
00:07:45,440 --> 00:07:47,919
model information from google

201
00:07:47,919 --> 00:07:50,960
the attack is still successful even

202
00:07:50,960 --> 00:07:52,720
though there are some participation

203
00:07:52,720 --> 00:07:55,440
and distortion the difference between

204
00:07:55,440 --> 00:07:58,000
the adversary example and the original

205
00:07:58,000 --> 00:08:00,800
image is not very large how do we do

206
00:08:00,800 --> 00:08:02,400
that

207
00:08:02,400 --> 00:08:05,360
here is some feature of our method first

208
00:08:05,360 --> 00:08:06,000
we use

209
00:08:06,000 --> 00:08:08,080
a loss function to compute the feature

210
00:08:08,080 --> 00:08:09,280
similarity

211
00:08:09,280 --> 00:08:12,800
and define it as ffl fyi

212
00:08:12,800 --> 00:08:15,039
it can increase the transferability of

213
00:08:15,039 --> 00:08:16,479
attacks

214
00:08:16,479 --> 00:08:19,120
next we build a small data set to train

215
00:08:19,120 --> 00:08:19,840
the model

216
00:08:19,840 --> 00:08:22,319
and for each image in the data set we

217
00:08:22,319 --> 00:08:24,400
send it to the cloud-based service

218
00:08:24,400 --> 00:08:27,680
no more than two times finally we use a

219
00:08:27,680 --> 00:08:29,680
white box model generated by

220
00:08:29,680 --> 00:08:32,958
auto dl to replace the cloud-based

221
00:08:32,958 --> 00:08:35,279
service

222
00:08:35,279 --> 00:08:38,799
generally our attack involves two steps

223
00:08:38,799 --> 00:08:41,599
first we use the result of cloud api to

224
00:08:41,599 --> 00:08:42,320
construct

225
00:08:42,320 --> 00:08:45,360
a small amount of data set and use it to

226
00:08:45,360 --> 00:08:48,240
training the substitution model

227
00:08:48,240 --> 00:08:50,720
second we use a substitution model to

228
00:08:50,720 --> 00:08:53,480
generate a serial example with better

229
00:08:53,480 --> 00:08:55,760
transferability

230
00:08:55,760 --> 00:08:57,920
now i will introduce the process of

231
00:08:57,920 --> 00:08:58,800
these steps

232
00:08:58,800 --> 00:09:01,439
in detail

233
00:09:02,080 --> 00:09:04,560
notice that the substitution model here

234
00:09:04,560 --> 00:09:07,200
is trained to produce a result that are

235
00:09:07,200 --> 00:09:08,640
similar to the result

236
00:09:08,640 --> 00:09:11,519
from the real one but only in certain

237
00:09:11,519 --> 00:09:12,959
categories

238
00:09:12,959 --> 00:09:15,440
the substitution model is only retrieved

239
00:09:15,440 --> 00:09:17,360
for the categories that need to be

240
00:09:17,360 --> 00:09:18,480
attacked

241
00:09:18,480 --> 00:09:20,800
its feature in this category are

242
00:09:20,800 --> 00:09:23,279
expected to be more similar to the

243
00:09:23,279 --> 00:09:24,959
target model

244
00:09:24,959 --> 00:09:27,760
so when we use non-targeted attacks if

245
00:09:27,760 --> 00:09:28,640
we want to use

246
00:09:28,640 --> 00:09:31,519
cats to attack then we only need a

247
00:09:31,519 --> 00:09:33,519
binary classification

248
00:09:33,519 --> 00:09:37,040
a cat or not a cat we fix the parameter

249
00:09:37,040 --> 00:09:38,240
of the feature layer

250
00:09:38,240 --> 00:09:40,399
and train only the full connection of

251
00:09:40,399 --> 00:09:43,120
the last layer

252
00:09:43,120 --> 00:09:45,519
the training process can be described as

253
00:09:45,519 --> 00:09:46,959
a follow

254
00:09:46,959 --> 00:09:49,519
we determine the category of the text

255
00:09:49,519 --> 00:09:50,000
select

256
00:09:50,000 --> 00:09:52,720
a text image and integrate those images

257
00:09:52,720 --> 00:09:54,959
into our training set

258
00:09:54,959 --> 00:09:58,000
which can have dozens or fewer

259
00:09:58,000 --> 00:10:00,160
those images are labeled by the cloud

260
00:10:00,160 --> 00:10:01,040
service

261
00:10:01,040 --> 00:10:04,079
then the small data set is successfully

262
00:10:04,079 --> 00:10:07,279
constructed to reach the output of

263
00:10:07,279 --> 00:10:08,880
binary classification

264
00:10:08,880 --> 00:10:11,519
we also need image of other animals in

265
00:10:11,519 --> 00:10:13,200
the training set

266
00:10:13,200 --> 00:10:15,519
training substitution model takes time

267
00:10:15,519 --> 00:10:16,560
of course

268
00:10:16,560 --> 00:10:18,959
but it is independent from the cloud

269
00:10:18,959 --> 00:10:19,760
service

270
00:10:19,760 --> 00:10:21,920
which is saying that there is no way for

271
00:10:21,920 --> 00:10:23,040
defenses to

272
00:10:23,040 --> 00:10:26,560
interfere this step

273
00:10:26,560 --> 00:10:29,279
because the data set is very small and

274
00:10:29,279 --> 00:10:30,839
there are only two target

275
00:10:30,839 --> 00:10:33,440
categories too many parameters will

276
00:10:33,440 --> 00:10:34,640
increase the risk of

277
00:10:34,640 --> 00:10:37,200
overfitting and the features are not

278
00:10:37,200 --> 00:10:39,120
likely to be robust

279
00:10:39,120 --> 00:10:41,839
therefore to make the feature extract

280
00:10:41,839 --> 00:10:44,399
the substitution model more robust

281
00:10:44,399 --> 00:10:47,519
we decided to reduce a certain number of

282
00:10:47,519 --> 00:10:50,320
parameters

283
00:10:50,640 --> 00:10:53,440
as for creating a serial examples we

284
00:10:53,440 --> 00:10:55,200
propose a loss function called

285
00:10:55,200 --> 00:10:58,240
ffl for pgd attacks it

286
00:10:58,240 --> 00:11:01,040
is used to improve the success rate of

287
00:11:01,040 --> 00:11:03,200
transferable attacks between different

288
00:11:03,200 --> 00:11:04,720
models

289
00:11:04,720 --> 00:11:07,040
the traditional pgd attacks only use

290
00:11:07,040 --> 00:11:08,800
classification laws

291
00:11:08,800 --> 00:11:12,160
focusing on model classification errors

292
00:11:12,160 --> 00:11:14,480
we increase the loss of multi-layered

293
00:11:14,480 --> 00:11:16,800
features to ensure that these features

294
00:11:16,800 --> 00:11:17,839
are more different

295
00:11:17,839 --> 00:11:19,920
than the one generated by a traditional

296
00:11:19,920 --> 00:11:22,880
pgd attack

297
00:11:23,360 --> 00:11:26,240
in addition to classification laws we

298
00:11:26,240 --> 00:11:28,079
will choose the output of multiple

299
00:11:28,079 --> 00:11:29,760
convolution layers

300
00:11:29,760 --> 00:11:31,680
the features of different convolution

301
00:11:31,680 --> 00:11:34,000
layers represent the semantic content of

302
00:11:34,000 --> 00:11:35,600
different dimensions

303
00:11:35,600 --> 00:11:37,519
we choose several different convolution

304
00:11:37,519 --> 00:11:40,079
layer features to propagate the features

305
00:11:40,079 --> 00:11:40,560
with

306
00:11:40,560 --> 00:11:43,920
at different levels in different

307
00:11:43,920 --> 00:11:46,320
convolution layers the first one is to

308
00:11:46,320 --> 00:11:47,680
recognize the edges

309
00:11:47,680 --> 00:11:49,839
and the lines mainly in the lasso

310
00:11:49,839 --> 00:11:50,959
convolution layer

311
00:11:50,959 --> 00:11:53,040
it recognizes high dimensional features

312
00:11:53,040 --> 00:11:54,839
such as eyes and

313
00:11:54,839 --> 00:11:57,760
noses lastly data set under

314
00:11:57,760 --> 00:11:59,279
pre-processing

315
00:11:59,279 --> 00:12:02,560
since the category to attack is the cat

316
00:12:02,560 --> 00:12:05,920
we select the 100 image of cats and 100

317
00:12:05,920 --> 00:12:06,720
image of

318
00:12:06,720 --> 00:12:09,839
other animals from the imagenet dataset

319
00:12:09,839 --> 00:12:11,600
all images are clipped to

320
00:12:11,600 --> 00:12:14,560
fix the proportion and format we don't

321
00:12:14,560 --> 00:12:15,920
have a validation set

322
00:12:15,920 --> 00:12:18,480
here the images for attack are also

323
00:12:18,480 --> 00:12:19,120
selected

324
00:12:19,120 --> 00:12:23,279
from the training set in our experiment

325
00:12:23,279 --> 00:12:26,240
the resonant 152 has a higher accuracy

326
00:12:26,240 --> 00:12:26,880
rate

327
00:12:26,880 --> 00:12:29,680
so we choose resonant 152 as our

328
00:12:29,680 --> 00:12:31,360
substitution model

329
00:12:31,360 --> 00:12:35,360
using traditional pgd and flpgd method

330
00:12:35,360 --> 00:12:38,399
generate adversarial examples compare

331
00:12:38,399 --> 00:12:39,200
them to find

332
00:12:39,200 --> 00:12:42,880
which one has better transferability

333
00:12:42,880 --> 00:12:44,519
in addition to compare the

334
00:12:44,519 --> 00:12:46,000
transferability

335
00:12:46,000 --> 00:12:48,079
we should we should also pay attention

336
00:12:48,079 --> 00:12:49,920
to the gap between them

337
00:12:49,920 --> 00:12:51,600
we need to measure the degree of

338
00:12:51,600 --> 00:12:53,680
distortion of adversarial

339
00:12:53,680 --> 00:12:56,320
examples if the image are quite

340
00:12:56,320 --> 00:12:58,399
different from the original one

341
00:12:58,399 --> 00:13:01,680
it will lose the meaning of the attack

342
00:13:01,680 --> 00:13:05,279
so we use the peak signal to noise ratio

343
00:13:05,279 --> 00:13:08,639
psnr to measure the quality of image

344
00:13:08,639 --> 00:13:11,760
and ssim to measure the similarity

345
00:13:11,760 --> 00:13:13,600
between the original image and the

346
00:13:13,600 --> 00:13:16,800
adversarial examples

347
00:13:16,800 --> 00:13:19,519
first look at the escape rate of them we

348
00:13:19,519 --> 00:13:20,560
choose amazon

349
00:13:20,560 --> 00:13:23,120
google microsoft and collaborate as the

350
00:13:23,120 --> 00:13:23,839
target of

351
00:13:23,839 --> 00:13:27,279
attack the average success rate of ffl

352
00:13:27,279 --> 00:13:29,120
pgd in this four different

353
00:13:29,120 --> 00:13:31,360
classification services

354
00:13:31,360 --> 00:13:33,519
is more than 90 percent the

355
00:13:33,519 --> 00:13:35,839
classification service of each platform

356
00:13:35,839 --> 00:13:38,399
has a higher success rate with ffl

357
00:13:38,399 --> 00:13:42,480
pgd method it can be shown that fflpgd

358
00:13:42,480 --> 00:13:46,000
has better transferability

359
00:13:46,000 --> 00:13:48,639
finally we conduct a set of comparison

360
00:13:48,639 --> 00:13:51,120
to verify the transformability of using

361
00:13:51,120 --> 00:13:54,079
assembled model attack method

362
00:13:54,079 --> 00:13:56,240
the method assembles many different

363
00:13:56,240 --> 00:13:59,360
models to generate adversarial examples

364
00:13:59,360 --> 00:14:02,320
and we use it to compare our method the

365
00:14:02,320 --> 00:14:04,320
baselines here are vgg

366
00:14:04,320 --> 00:14:07,920
resnet inception and other models

367
00:14:07,920 --> 00:14:10,079
we can see that the basic success rate

368
00:14:10,079 --> 00:14:10,959
of this cloud

369
00:14:10,959 --> 00:14:14,639
classification service is less than 50

370
00:14:14,639 --> 00:14:17,600
which is much worse than our method it

371
00:14:17,600 --> 00:14:18,320
is still

372
00:14:18,320 --> 00:14:20,199
very difficult for the traditional

373
00:14:20,199 --> 00:14:22,560
transferability method to attack the

374
00:14:22,560 --> 00:14:24,160
cloud classification service

375
00:14:24,160 --> 00:14:26,240
successfully

376
00:14:26,240 --> 00:14:28,079
the previous introduction of cloud

377
00:14:28,079 --> 00:14:29,839
attack using

378
00:14:29,839 --> 00:14:32,240
pgd shows that the cloud servers are

379
00:14:32,240 --> 00:14:32,880
still not

380
00:14:32,880 --> 00:14:36,160
robust under the attack later

381
00:14:36,160 --> 00:14:38,240
we will introduce the defense and

382
00:14:38,240 --> 00:14:40,959
mitigation method of the model

383
00:14:40,959 --> 00:14:43,360
the serial attack mitigation has two

384
00:14:43,360 --> 00:14:44,000
types of

385
00:14:44,000 --> 00:14:46,800
defense strategies the first one is

386
00:14:46,800 --> 00:14:47,680
reactive

387
00:14:47,680 --> 00:14:50,480
which is a detector of serial examples

388
00:14:50,480 --> 00:14:53,360
after deep neural network are built

389
00:14:53,360 --> 00:14:56,399
for example adobe serial detection input

390
00:14:56,399 --> 00:14:57,839
reconstruction

391
00:14:57,839 --> 00:15:00,880
and network verification

392
00:15:00,880 --> 00:15:03,279
the other is a pre-active which is

393
00:15:03,279 --> 00:15:05,680
making deep neural network more robust

394
00:15:05,680 --> 00:15:07,600
before adobe series generates

395
00:15:07,600 --> 00:15:08,880
adversarial

396
00:15:08,880 --> 00:15:12,720
examples like network destination

397
00:15:12,720 --> 00:15:15,199
with serial training and a classic for

398
00:15:15,199 --> 00:15:17,360
robustness

399
00:15:17,360 --> 00:15:19,680
the above methods are all based on the

400
00:15:19,680 --> 00:15:21,760
white box security defense

401
00:15:21,760 --> 00:15:23,839
some of them use a sophisticated

402
00:15:23,839 --> 00:15:26,160
ingredient gradient masking

403
00:15:26,160 --> 00:15:29,279
or input reconstruction method

404
00:15:29,279 --> 00:15:32,079
but this gives a false sense of security

405
00:15:32,079 --> 00:15:33,199
in defenses

406
00:15:33,199 --> 00:15:36,639
against our serial examples and they can

407
00:15:36,639 --> 00:15:37,600
be destroyed

408
00:15:37,600 --> 00:15:41,120
by improving attack techniques

409
00:15:41,120 --> 00:15:43,680
the only defense significantly increased

410
00:15:43,680 --> 00:15:46,720
robustness to other serial examples

411
00:15:46,720 --> 00:15:50,000
within the threat model proposed is over

412
00:15:50,000 --> 00:15:50,959
the serial

413
00:15:50,959 --> 00:15:54,320
training considering the realization

414
00:15:54,320 --> 00:15:55,360
difficulties

415
00:15:55,360 --> 00:15:58,399
and actual effect we recommend using

416
00:15:58,399 --> 00:16:00,639
input reconstruction in the data

417
00:16:00,639 --> 00:16:02,800
preprocessing stage

418
00:16:02,800 --> 00:16:05,199
and i will thoroughly train the model in

419
00:16:05,199 --> 00:16:08,560
the model prediction stage at the same

420
00:16:08,560 --> 00:16:09,120
time

421
00:16:09,120 --> 00:16:11,279
adversarial training included

422
00:16:11,279 --> 00:16:13,040
adversarial examples

423
00:16:13,040 --> 00:16:15,920
in the training stage and generated

424
00:16:15,920 --> 00:16:17,680
adversarial examples

425
00:16:17,680 --> 00:16:20,720
in every step of training and inject

426
00:16:20,720 --> 00:16:23,120
them into the training set

427
00:16:23,120 --> 00:16:26,000
on the other hand we can also generate

428
00:16:26,000 --> 00:16:27,040
adversarial

429
00:16:27,040 --> 00:16:30,320
samples offline the size of the serial

430
00:16:30,320 --> 00:16:30,959
samples

431
00:16:30,959 --> 00:16:34,079
is equal to the original data set and

432
00:16:34,079 --> 00:16:34,480
then

433
00:16:34,480 --> 00:16:38,240
retrieve the model using our defense

434
00:16:38,240 --> 00:16:40,480
technique can effectively resist

435
00:16:40,480 --> 00:16:43,199
knowing spatial attacks such as gaussian

436
00:16:43,199 --> 00:16:44,160
noise

437
00:16:44,160 --> 00:16:47,279
salt and pepper noise rotation and

438
00:16:47,279 --> 00:16:50,639
moral componentization

439
00:16:50,639 --> 00:16:53,279
robustness evaluation is used to

440
00:16:53,279 --> 00:16:55,920
understand the robustness characteristic

441
00:16:55,920 --> 00:16:58,399
of the model in the actual environment

442
00:16:58,399 --> 00:17:02,160
after the model training is completed

443
00:17:02,160 --> 00:17:04,640
using multiple methods according to

444
00:17:04,640 --> 00:17:06,480
certain conditions to generate

445
00:17:06,480 --> 00:17:08,480
examples with a limited virtual

446
00:17:08,480 --> 00:17:11,839
difference from the original image

447
00:17:11,839 --> 00:17:14,480
but it is possible to make the model

448
00:17:14,480 --> 00:17:16,160
predict the wrong labels

449
00:17:16,160 --> 00:17:19,039
to evaluate the robustness of the model

450
00:17:19,039 --> 00:17:21,520
in this environment

451
00:17:21,520 --> 00:17:24,640
it can be of two types the first is

452
00:17:24,640 --> 00:17:25,919
safety related

453
00:17:25,919 --> 00:17:28,559
using adversarial examples formed by

454
00:17:28,559 --> 00:17:30,559
spatial transformations

455
00:17:30,559 --> 00:17:34,080
or image corruptions such as scaling

456
00:17:34,080 --> 00:17:37,919
light transformation weather blur

457
00:17:37,919 --> 00:17:41,679
shake et cetera the second is security

458
00:17:41,679 --> 00:17:42,480
related

459
00:17:42,480 --> 00:17:45,039
which uses the model gradient to stack a

460
00:17:45,039 --> 00:17:46,240
perturbation to

461
00:17:46,240 --> 00:17:49,760
attack such as fgsm

462
00:17:49,760 --> 00:17:54,080
pgd sw etc

463
00:17:54,080 --> 00:17:56,240
the first is more general and more

464
00:17:56,240 --> 00:17:57,120
common and

465
00:17:57,120 --> 00:18:00,640
also supports black box testing

466
00:18:00,640 --> 00:18:03,840
the second is more targeted the human

467
00:18:03,840 --> 00:18:04,160
eye

468
00:18:04,160 --> 00:18:07,440
is less likely to detect it and it

469
00:18:07,440 --> 00:18:10,640
relies more on white box attacks

470
00:18:10,640 --> 00:18:13,440
it is very difficult for an attacker to

471
00:18:13,440 --> 00:18:15,760
obtain model parameters

472
00:18:15,760 --> 00:18:18,559
so the safety related robustness test is

473
00:18:18,559 --> 00:18:20,400
more practical

474
00:18:20,400 --> 00:18:23,360
currently we provide open source version

475
00:18:23,360 --> 00:18:23,679
of

476
00:18:23,679 --> 00:18:26,880
robust tools for such evaluation

477
00:18:26,880 --> 00:18:29,840
perceptron benchmark 10 the tool

478
00:18:29,840 --> 00:18:32,400
supports the testing of local models and

479
00:18:32,400 --> 00:18:34,400
the cloud apis

480
00:18:34,400 --> 00:18:37,120
it use 15 evaluation methods such as

481
00:18:37,120 --> 00:18:38,000
brightness

482
00:18:38,000 --> 00:18:41,360
construction rotation noise shake

483
00:18:41,360 --> 00:18:46,000
occlusion frost rain snow etc etc

484
00:18:46,000 --> 00:18:48,400
because each method can set different

485
00:18:48,400 --> 00:18:49,520
threshold

486
00:18:49,520 --> 00:18:51,760
and the degree of image corruption and

487
00:18:51,760 --> 00:18:54,080
attack event are also different

488
00:18:54,080 --> 00:18:57,120
we use psnr and ssim as

489
00:18:57,120 --> 00:18:59,679
accelerary evaluations evaluation

490
00:18:59,679 --> 00:19:01,840
standards

491
00:19:01,840 --> 00:19:05,679
take the imagenet dataset as an example

492
00:19:05,679 --> 00:19:07,679
in the angle rotation performance

493
00:19:07,679 --> 00:19:10,880
especially the positive and negative 135

494
00:19:10,880 --> 00:19:11,840
degrees

495
00:19:11,840 --> 00:19:14,799
has a attack the success rate of close

496
00:19:14,799 --> 00:19:15,120
to

497
00:19:15,120 --> 00:19:18,320
70 percent which indicate that the

498
00:19:18,320 --> 00:19:20,559
confrontation at this angle is the

499
00:19:20,559 --> 00:19:21,520
weakest

500
00:19:21,520 --> 00:19:24,080
in the performance of noise most of the

501
00:19:24,080 --> 00:19:24,559
label

502
00:19:24,559 --> 00:19:27,200
after the attack focus on several

503
00:19:27,200 --> 00:19:28,720
categories

504
00:19:28,720 --> 00:19:31,200
which is particularly obvious in the

505
00:19:31,200 --> 00:19:32,960
salt and pepper noise

506
00:19:32,960 --> 00:19:35,600
this weakness of the model that has been

507
00:19:35,600 --> 00:19:37,919
demonstrated through robustness test can

508
00:19:37,919 --> 00:19:39,360
be targeted

509
00:19:39,360 --> 00:19:41,679
to use the adversarial training as

510
00:19:41,679 --> 00:19:44,640
mentioned before to companies for it

511
00:19:44,640 --> 00:19:47,280
cloud service may still be subjective to

512
00:19:47,280 --> 00:19:49,280
adversarial attacks

513
00:19:49,280 --> 00:19:51,919
combined with the characteristic of

514
00:19:51,919 --> 00:19:53,360
serial attacks

515
00:19:53,360 --> 00:19:55,679
and the cloud service the security

516
00:19:55,679 --> 00:19:56,480
development

517
00:19:56,480 --> 00:19:59,039
cycle for machine learning application

518
00:19:59,039 --> 00:20:00,720
is introduced

519
00:20:00,720 --> 00:20:03,600
including about serial detection model

520
00:20:03,600 --> 00:20:04,559
defense

521
00:20:04,559 --> 00:20:07,679
and robustness evaluation the

522
00:20:07,679 --> 00:20:09,679
application of this method in the

523
00:20:09,679 --> 00:20:10,960
development

524
00:20:10,960 --> 00:20:13,520
will greatly improve the security of the

525
00:20:13,520 --> 00:20:14,240
model

526
00:20:14,240 --> 00:20:16,799
and help developers build more secure

527
00:20:16,799 --> 00:20:18,559
software

528
00:20:18,559 --> 00:20:21,120
this is the end of my speech we hope to

529
00:20:21,120 --> 00:20:23,120
have more communicate with friends

530
00:20:23,120 --> 00:20:26,080
who are interested in ai security thank

531
00:20:26,080 --> 00:20:29,520
you very much

