1
00:00:01,920 --> 00:00:03,280
hello everyone

2
00:00:03,280 --> 00:00:06,319
my name is epin lee and i'm a researcher

3
00:00:06,319 --> 00:00:08,800
from baidu security

4
00:00:08,800 --> 00:00:11,040
today i'm here to introduce a new

5
00:00:11,040 --> 00:00:13,040
technique to track the anonymous

6
00:00:13,040 --> 00:00:17,119
author of online disinformation

7
00:00:17,119 --> 00:00:19,279
so first my presentation has five

8
00:00:19,279 --> 00:00:21,279
sections are showing

9
00:00:21,279 --> 00:00:23,359
the introduction talks about what

10
00:00:23,359 --> 00:00:25,359
problem did we solve

11
00:00:25,359 --> 00:00:27,519
the second part is about some previous

12
00:00:27,519 --> 00:00:30,000
work we referred to

13
00:00:30,000 --> 00:00:32,719
method and experiment discuss how we

14
00:00:32,719 --> 00:00:34,559
implement our model

15
00:00:34,559 --> 00:00:36,800
and that is comparison result with other

16
00:00:36,800 --> 00:00:38,800
baselines

17
00:00:38,800 --> 00:00:42,480
finally a short summary

18
00:00:43,280 --> 00:00:46,000
so before we get into it i want to start

19
00:00:46,000 --> 00:00:48,399
with a fun fact

20
00:00:48,399 --> 00:00:51,520
did you know we actually use 100 of our

21
00:00:51,520 --> 00:00:53,680
brain power

22
00:00:53,680 --> 00:00:55,840
i thought it was interesting because i

23
00:00:55,840 --> 00:00:58,079
have heard many people saying that we

24
00:00:58,079 --> 00:01:01,600
only use 10 percent of our brain

25
00:01:01,600 --> 00:01:04,159
now back to the same so what is the

26
00:01:04,159 --> 00:01:04,799
point of

27
00:01:04,799 --> 00:01:07,040
this is that i want to show you the

28
00:01:07,040 --> 00:01:10,720
existence of false information

29
00:01:10,720 --> 00:01:13,280
from printing to the development of the

30
00:01:13,280 --> 00:01:14,560
internet

31
00:01:14,560 --> 00:01:17,119
the information we generated has grown

32
00:01:17,119 --> 00:01:19,920
exponentially

33
00:01:19,920 --> 00:01:22,799
well we enjoy the convenience it brings

34
00:01:22,799 --> 00:01:23,680
to us

35
00:01:23,680 --> 00:01:27,840
we also have to face the dark side

36
00:01:29,360 --> 00:01:32,400
here a study shows that about 3.8

37
00:01:32,400 --> 00:01:33,520
million people

38
00:01:33,520 --> 00:01:36,880
spend 144 minutes per day

39
00:01:36,880 --> 00:01:40,880
on social media on average

40
00:01:40,880 --> 00:01:44,079
notice that the data is from 2019

41
00:01:44,079 --> 00:01:47,680
and the status will only go up in 2020

42
00:01:47,680 --> 00:01:50,799
due to the quarantine

43
00:01:51,520 --> 00:01:54,079
from the status we can see that social

44
00:01:54,079 --> 00:01:56,719
media gradually becomes the largest

45
00:01:56,719 --> 00:02:00,399
information source but it also leads to

46
00:02:00,399 --> 00:02:01,520
the rise of this

47
00:02:01,520 --> 00:02:04,320
information the false information intend

48
00:02:04,320 --> 00:02:06,479
to mislead

49
00:02:06,479 --> 00:02:08,878
with the aid of social media false

50
00:02:08,878 --> 00:02:12,560
information has become a serious problem

51
00:02:12,560 --> 00:02:15,360
there is an old saying that a light will

52
00:02:15,360 --> 00:02:15,920
travel

53
00:02:15,920 --> 00:02:18,239
around the world while the truth is

54
00:02:18,239 --> 00:02:20,879
pulling on its boots

55
00:02:20,879 --> 00:02:23,280
it is a perfect illustration of why

56
00:02:23,280 --> 00:02:27,280
false information is so problematic

57
00:02:27,280 --> 00:02:29,520
it spreads out so quickly that you don't

58
00:02:29,520 --> 00:02:32,640
even get a chance to stop it

59
00:02:32,640 --> 00:02:35,200
another study showed that fake news and

60
00:02:35,200 --> 00:02:36,000
stories

61
00:02:36,000 --> 00:02:38,400
are 70 percent more likely to be

62
00:02:38,400 --> 00:02:41,760
reposted on twitter

63
00:02:41,840 --> 00:02:44,560
besides false information tends to be

64
00:02:44,560 --> 00:02:46,400
trustworthy for users

65
00:02:46,400 --> 00:02:49,440
because users want it to be true

66
00:02:49,440 --> 00:02:51,599
it is like people saying that you are

67
00:02:51,599 --> 00:02:53,360
promoted

68
00:02:53,360 --> 00:02:55,440
you don't know it for sure but you hope

69
00:02:55,440 --> 00:02:58,080
it is true

70
00:02:58,080 --> 00:03:00,159
people are more likely to trust things

71
00:03:00,159 --> 00:03:02,560
they want to believe

72
00:03:02,560 --> 00:03:05,040
on the other hand the purpose of fake

73
00:03:05,040 --> 00:03:06,080
news is to

74
00:03:06,080 --> 00:03:09,519
attract the visitors therefore readers

75
00:03:09,519 --> 00:03:11,760
tend to be driven by the emotional

76
00:03:11,760 --> 00:03:12,640
impulse

77
00:03:12,640 --> 00:03:16,480
ignoring its authenticity

78
00:03:16,480 --> 00:03:19,120
it is worse to point out that features

79
00:03:19,120 --> 00:03:20,480
like top trend

80
00:03:20,480 --> 00:03:23,280
a news feed allows fake news to be

81
00:03:23,280 --> 00:03:26,799
exposed more frequently

82
00:03:27,920 --> 00:03:30,480
now you may ask why would people just

83
00:03:30,480 --> 00:03:33,200
make stuff up what is the purpose of

84
00:03:33,200 --> 00:03:34,879
doing this

85
00:03:34,879 --> 00:03:37,920
actually they can get a lot

86
00:03:37,920 --> 00:03:40,640
first fabricating rumors is a public

87
00:03:40,640 --> 00:03:41,680
relation move

88
00:03:41,680 --> 00:03:45,120
that changes public opinions

89
00:03:45,120 --> 00:03:47,440
usually they hire a group of ghost

90
00:03:47,440 --> 00:03:48,640
writers to post

91
00:03:48,640 --> 00:03:51,840
online comments or reviews

92
00:03:51,840 --> 00:03:54,799
we call it online water army in china

93
00:03:54,799 --> 00:03:58,080
since their comments are like afloat

94
00:03:58,080 --> 00:04:00,480
with those water armies they can do a

95
00:04:00,480 --> 00:04:01,519
lot of stuff

96
00:04:01,519 --> 00:04:05,040
like advertising for example

97
00:04:05,040 --> 00:04:07,200
if you want to buy your shoes and then

98
00:04:07,200 --> 00:04:08,239
you look for some

99
00:04:08,239 --> 00:04:11,280
recommendations from others

100
00:04:11,280 --> 00:04:14,080
a review pops up and show you the top 10

101
00:04:14,080 --> 00:04:15,760
shoes they ranked

102
00:04:15,760 --> 00:04:18,880
a coup sounds like to the point but it

103
00:04:18,880 --> 00:04:20,798
actually can be a fake review that

104
00:04:20,798 --> 00:04:24,639
tricked you into buying their product

105
00:04:24,639 --> 00:04:27,440
slender and deformation which used to

106
00:04:27,440 --> 00:04:28,880
happen on celebrity

107
00:04:28,880 --> 00:04:33,280
and idols shifting public attention

108
00:04:33,280 --> 00:04:36,720
which is a classic pr move

109
00:04:36,720 --> 00:04:40,080
reshaping public opinions reshaping

110
00:04:40,080 --> 00:04:41,199
public opinion

111
00:04:41,199 --> 00:04:44,400
is mostly used in the political aspect

112
00:04:44,400 --> 00:04:48,160
like propaganda gaining public attention

113
00:04:48,160 --> 00:04:50,720
which draws attention to themselves so

114
00:04:50,720 --> 00:04:52,479
that they can have higher commercial

115
00:04:52,479 --> 00:04:54,479
value

116
00:04:54,479 --> 00:04:56,639
and the last counseling marketing

117
00:04:56,639 --> 00:04:58,160
turbulence

118
00:04:58,160 --> 00:05:01,840
which will be introduced later

119
00:05:01,840 --> 00:05:05,039
in fact there is one ultimate goal

120
00:05:05,039 --> 00:05:06,800
behind six purpose

121
00:05:06,800 --> 00:05:10,080
and that is making money and now

122
00:05:10,080 --> 00:05:13,280
let's talk about potential victims

123
00:05:13,280 --> 00:05:15,919
you know fake news actually has a wide

124
00:05:15,919 --> 00:05:17,520
impact on us

125
00:05:17,520 --> 00:05:21,600
anyone can be the victim politicians

126
00:05:21,600 --> 00:05:24,720
celebrities and companies those three

127
00:05:24,720 --> 00:05:25,520
suffers from

128
00:05:25,520 --> 00:05:28,720
deformation and slander mostly

129
00:05:28,720 --> 00:05:31,360
and we consumers will be traded into a

130
00:05:31,360 --> 00:05:32,000
pursuit

131
00:05:32,000 --> 00:05:36,720
because of some fake review or fake news

132
00:05:37,360 --> 00:05:40,320
furthermore fake news can also bring

133
00:05:40,320 --> 00:05:42,560
serious negative consequences

134
00:05:42,560 --> 00:05:46,160
like stock market turbulence in the past

135
00:05:46,160 --> 00:05:48,880
few years some well-known company has

136
00:05:48,880 --> 00:05:51,680
suffered from online short attacks

137
00:05:51,680 --> 00:05:54,400
and you can see from the graph the stock

138
00:05:54,400 --> 00:05:55,360
price dropped

139
00:05:55,360 --> 00:05:59,520
sharply due to those short attacks

140
00:05:59,759 --> 00:06:02,880
moreover some attackers fabricate fake

141
00:06:02,880 --> 00:06:06,080
news about the social influencers so

142
00:06:06,080 --> 00:06:09,280
that they can ask for money

143
00:06:09,280 --> 00:06:11,520
therefore to protect and maintain

144
00:06:11,520 --> 00:06:13,039
personal reputation

145
00:06:13,039 --> 00:06:16,560
brand images and a healthy cyberspace

146
00:06:16,560 --> 00:06:18,960
it is necessary to track the source of

147
00:06:18,960 --> 00:06:21,360
the fake news

148
00:06:21,360 --> 00:06:24,319
however most fake news and articles are

149
00:06:24,319 --> 00:06:24,720
posed

150
00:06:24,720 --> 00:06:27,360
anonymously and it lacks valid

151
00:06:27,360 --> 00:06:30,960
information to identify the author

152
00:06:30,960 --> 00:06:34,000
so tracking anonymous article is also a

153
00:06:34,000 --> 00:06:37,039
challenging problem

154
00:06:37,039 --> 00:06:39,919
fortunately it is not impossible

155
00:06:39,919 --> 00:06:41,600
different people have different

156
00:06:41,600 --> 00:06:44,319
writing styles so we are able to

157
00:06:44,319 --> 00:06:45,360
identify some

158
00:06:45,360 --> 00:06:49,840
writers by their distinct habits

159
00:06:49,840 --> 00:06:53,199
as shown on the top right there is two

160
00:06:53,199 --> 00:06:56,400
twitters with two writing styles

161
00:06:56,400 --> 00:06:59,120
ron paul is the only candidate who

162
00:06:59,120 --> 00:06:59,759
offers

163
00:06:59,759 --> 00:07:02,800
us a real choice that sounds like a

164
00:07:02,800 --> 00:07:04,479
senator

165
00:07:04,479 --> 00:07:07,199
it's getting late so i will be here for

166
00:07:07,199 --> 00:07:07,759
probably

167
00:07:07,759 --> 00:07:11,199
two more hours tops actually i shouldn't

168
00:07:11,199 --> 00:07:13,199
read this way

169
00:07:13,199 --> 00:07:16,560
it's getting l number eight so

170
00:07:16,560 --> 00:07:19,759
aob here number four

171
00:07:19,759 --> 00:07:23,360
pop two more hurst tops

172
00:07:23,360 --> 00:07:25,520
that sounds like a tweet written by a

173
00:07:25,520 --> 00:07:28,479
teenager right

174
00:07:28,560 --> 00:07:31,360
or it could be an other way around but

175
00:07:31,360 --> 00:07:33,360
they are definitely two distinct

176
00:07:33,360 --> 00:07:36,639
writing styles and what you see here is

177
00:07:36,639 --> 00:07:38,479
that we can kind of guess at their

178
00:07:38,479 --> 00:07:41,759
identity based on that

179
00:07:41,759 --> 00:07:43,840
the table on the bottom shows the

180
00:07:43,840 --> 00:07:44,879
multiple level

181
00:07:44,879 --> 00:07:48,479
of writing styles for example

182
00:07:48,479 --> 00:07:50,479
different authors may have different

183
00:07:50,479 --> 00:07:52,639
preference of word choice

184
00:07:52,639 --> 00:07:54,720
some people like to use certain

185
00:07:54,720 --> 00:07:57,680
adjectives but others don't

186
00:07:57,680 --> 00:08:00,160
like my friend who prefers to use

187
00:08:00,160 --> 00:08:01,120
hilarious

188
00:08:01,120 --> 00:08:04,960
instead of funny as for the anonymous

189
00:08:04,960 --> 00:08:05,840
articles

190
00:08:05,840 --> 00:08:08,720
the more features we collect in writing

191
00:08:08,720 --> 00:08:09,520
the clear

192
00:08:09,520 --> 00:08:13,120
fingerprint we will get

193
00:08:13,120 --> 00:08:16,479
well in short we have discussed the

194
00:08:16,479 --> 00:08:18,960
effect of this information and some

195
00:08:18,960 --> 00:08:22,000
insight on tracking anonymous articles

196
00:08:22,000 --> 00:08:26,479
let's look at some previous works

197
00:08:26,479 --> 00:08:28,800
so first the problem of tracking

198
00:08:28,800 --> 00:08:30,160
anonymous articles

199
00:08:30,160 --> 00:08:33,519
is defined as authorship analysis

200
00:08:33,519 --> 00:08:36,000
and it contains three sub problems as

201
00:08:36,000 --> 00:08:38,240
shown on the screen

202
00:08:38,240 --> 00:08:41,279
authorship attribution takes one text

203
00:08:41,279 --> 00:08:43,679
and the final is author from a list of

204
00:08:43,679 --> 00:08:45,600
candidates

205
00:08:45,600 --> 00:08:49,040
authorship verification takes two taxes

206
00:08:49,040 --> 00:08:51,519
that tells whether the author of the two

207
00:08:51,519 --> 00:08:54,640
articles are the same or not

208
00:08:54,640 --> 00:08:57,600
authorship clustering takes a list of

209
00:08:57,600 --> 00:08:58,800
text inputs

210
00:08:58,800 --> 00:09:01,519
then organize the article based on their

211
00:09:01,519 --> 00:09:03,279
authors

212
00:09:03,279 --> 00:09:06,000
lastly the combination of approach to

213
00:09:06,000 --> 00:09:07,200
these three tasks

214
00:09:07,200 --> 00:09:09,120
is applied to tracking anonymous

215
00:09:09,120 --> 00:09:11,680
articles

216
00:09:12,320 --> 00:09:15,440
surprisingly authorship analysis

217
00:09:15,440 --> 00:09:18,240
is a long-standing problem and it was

218
00:09:18,240 --> 00:09:19,839
initiated by the work of

219
00:09:19,839 --> 00:09:23,839
most terror and the wellness in 1964.

220
00:09:23,839 --> 00:09:26,560
the development of authorship analysis

221
00:09:26,560 --> 00:09:29,120
has three major stages

222
00:09:29,120 --> 00:09:31,839
in early 2000 a method based on

223
00:09:31,839 --> 00:09:32,720
stylistic

224
00:09:32,720 --> 00:09:36,160
and the content feature was introduced

225
00:09:36,160 --> 00:09:38,560
it requires them to manually extract the

226
00:09:38,560 --> 00:09:39,600
trades

227
00:09:39,600 --> 00:09:42,480
so it not only take a lot of effort but

228
00:09:42,480 --> 00:09:43,440
also performed

229
00:09:43,440 --> 00:09:46,880
poorly on the test character

230
00:09:46,880 --> 00:09:50,000
and gram was introduced 10 years later

231
00:09:50,000 --> 00:09:52,399
and it has been proved to be the most

232
00:09:52,399 --> 00:09:54,800
effective features for authorship and

233
00:09:54,800 --> 00:09:55,120
not

234
00:09:55,120 --> 00:09:58,880
attribution at that time but still

235
00:09:58,880 --> 00:10:01,200
the performance is limited since it

236
00:10:01,200 --> 00:10:02,079
involves many

237
00:10:02,079 --> 00:10:05,279
high dimensional matrix manipulation

238
00:10:05,279 --> 00:10:08,079
in recent years with a breakthrough of

239
00:10:08,079 --> 00:10:09,200
deep learning

240
00:10:09,200 --> 00:10:11,440
some researchers proposed a method based

241
00:10:11,440 --> 00:10:13,760
on deep learning networks

242
00:10:13,760 --> 00:10:16,480
it turns out to be effective but only

243
00:10:16,480 --> 00:10:17,279
for authorship

244
00:10:17,279 --> 00:10:20,640
attribution those methods do not apply

245
00:10:20,640 --> 00:10:24,240
to authorship verification or clustering

246
00:10:24,240 --> 00:10:27,519
the story does not end there although

247
00:10:27,519 --> 00:10:29,680
our method is also based on deep

248
00:10:29,680 --> 00:10:31,600
learning networks

249
00:10:31,600 --> 00:10:34,560
what's new about it is that it satisfies

250
00:10:34,560 --> 00:10:38,399
all three needs in authorship analysis

251
00:10:38,399 --> 00:10:41,920
let me show you how we did it first

252
00:10:41,920 --> 00:10:44,880
we were inspired by physnet which is a

253
00:10:44,880 --> 00:10:47,680
unified method for face recognition

254
00:10:47,680 --> 00:10:50,880
verification and clustering

255
00:10:50,880 --> 00:10:53,600
well it has achieved a big success in

256
00:10:53,600 --> 00:10:56,399
computer vision

257
00:10:56,399 --> 00:10:59,120
similarly we propose a unified method

258
00:10:59,120 --> 00:11:00,000
based on deep

259
00:11:00,000 --> 00:11:02,720
learning and our model learns a mapping

260
00:11:02,720 --> 00:11:04,560
from texas to compact

261
00:11:04,560 --> 00:11:07,760
n-dimensional euclidean space this

262
00:11:07,760 --> 00:11:08,880
mapping embeds

263
00:11:08,880 --> 00:11:12,000
attacks into the surface of a sphere

264
00:11:12,000 --> 00:11:14,800
with a radius of 1 and the center of the

265
00:11:14,800 --> 00:11:16,880
origin

266
00:11:16,880 --> 00:11:19,680
basically our model is like a function

267
00:11:19,680 --> 00:11:21,760
which takes text as an input

268
00:11:21,760 --> 00:11:24,959
and turning into a point on a graph

269
00:11:24,959 --> 00:11:27,360
in euclidean space the distance

270
00:11:27,360 --> 00:11:28,320
indicates text

271
00:11:28,320 --> 00:11:31,360
similarity you can see here there are

272
00:11:31,360 --> 00:11:32,560
four authors and

273
00:11:32,560 --> 00:11:36,000
each of them has four articles

274
00:11:36,000 --> 00:11:39,120
text of same author has shorter distance

275
00:11:39,120 --> 00:11:42,240
vice versa in the graph

276
00:11:42,240 --> 00:11:46,000
16 articles are projected to the space

277
00:11:46,000 --> 00:11:50,800
x1 and x2 is embedded to those red dots

278
00:11:50,800 --> 00:11:52,959
since they are located together we

279
00:11:52,959 --> 00:11:54,240
suppose they are

280
00:11:54,240 --> 00:11:57,440
written by the same author moreover

281
00:11:57,440 --> 00:12:00,880
to change the model we define a triplet

282
00:12:00,880 --> 00:12:03,120
a small data set that contains three

283
00:12:03,120 --> 00:12:04,480
samples

284
00:12:04,480 --> 00:12:06,720
as shown on the screen a triplet

285
00:12:06,720 --> 00:12:07,600
consists of

286
00:12:07,600 --> 00:12:12,079
an anchor a positive and a negative

287
00:12:12,079 --> 00:12:15,200
the anchor is the target text the one we

288
00:12:15,200 --> 00:12:17,279
use to compare

289
00:12:17,279 --> 00:12:19,839
the positive is a text from the same

290
00:12:19,839 --> 00:12:20,959
author

291
00:12:20,959 --> 00:12:23,680
and the negative is a text from another

292
00:12:23,680 --> 00:12:25,839
author

293
00:12:25,839 --> 00:12:28,480
the model will embed these three inputs

294
00:12:28,480 --> 00:12:31,120
into the euclidean space

295
00:12:31,120 --> 00:12:33,839
initially the three taxes are mapped

296
00:12:33,839 --> 00:12:36,480
into three random locations

297
00:12:36,480 --> 00:12:39,120
and the positional relations among three

298
00:12:39,120 --> 00:12:39,680
points

299
00:12:39,680 --> 00:12:42,320
can be categorized into those three

300
00:12:42,320 --> 00:12:43,360
cases

301
00:12:43,360 --> 00:12:46,720
which is shown on the left remember

302
00:12:46,720 --> 00:12:50,000
the distance indicates a text similarity

303
00:12:50,000 --> 00:12:52,800
the first one is when positive is closer

304
00:12:52,800 --> 00:12:53,839
to the anchor

305
00:12:53,839 --> 00:12:56,480
than negative which is a case we like to

306
00:12:56,480 --> 00:12:58,079
see

307
00:12:58,079 --> 00:13:00,320
the articles from the same author has

308
00:13:00,320 --> 00:13:02,320
shorter distance

309
00:13:02,320 --> 00:13:04,399
the second one is when negative is

310
00:13:04,399 --> 00:13:07,200
closer to the anchor than positive

311
00:13:07,200 --> 00:13:10,399
we don't want to see that

312
00:13:10,399 --> 00:13:12,480
the third one is when positive and the

313
00:13:12,480 --> 00:13:14,720
negative is equally away from

314
00:13:14,720 --> 00:13:20,560
anchor we don't want to see that either

315
00:13:20,560 --> 00:13:23,360
to separate n curve and positive from

316
00:13:23,360 --> 00:13:24,560
the negative

317
00:13:24,560 --> 00:13:27,760
we define a triplet loss function

318
00:13:27,760 --> 00:13:31,120
as shown on the left this loss function

319
00:13:31,120 --> 00:13:32,240
aims to let

320
00:13:32,240 --> 00:13:35,760
a the squared euclidean distance

321
00:13:35,760 --> 00:13:38,399
between the anchor and the negative to

322
00:13:38,399 --> 00:13:40,560
be greater than dp

323
00:13:40,560 --> 00:13:42,639
the square the euclidean distance

324
00:13:42,639 --> 00:13:45,600
between the anchor and the positive

325
00:13:45,600 --> 00:13:48,480
for better discrimination ability we

326
00:13:48,480 --> 00:13:49,199
introduced

327
00:13:49,199 --> 00:13:52,800
a margin just like the margin of support

328
00:13:52,800 --> 00:13:55,839
vector machine in other words

329
00:13:55,839 --> 00:13:58,880
when the margin is 0.5 that means we

330
00:13:58,880 --> 00:14:00,800
want negative to be at least

331
00:14:00,800 --> 00:14:04,160
0.5 units further than positive from the

332
00:14:04,160 --> 00:14:06,160
anchor

333
00:14:06,160 --> 00:14:08,800
the triple a loss function which is

334
00:14:08,800 --> 00:14:09,600
defined as

335
00:14:09,600 --> 00:14:13,199
l is simply measuring how much further

336
00:14:13,199 --> 00:14:16,560
is negative than positive taking margin

337
00:14:16,560 --> 00:14:18,880
into account

338
00:14:18,880 --> 00:14:22,480
you see here is that dp plus margin

339
00:14:22,480 --> 00:14:27,040
minus dn when the loss is a positive

340
00:14:27,040 --> 00:14:29,600
that means that the difference is not

341
00:14:29,600 --> 00:14:30,880
enough

342
00:14:30,880 --> 00:14:32,800
otherwise it means the difference

343
00:14:32,800 --> 00:14:35,040
already meets our goal

344
00:14:35,040 --> 00:14:38,240
therefore we want the loss to be 0

345
00:14:38,240 --> 00:14:41,760
updating nothing so there is a max

346
00:14:41,760 --> 00:14:44,000
function to set the minimum loss to

347
00:14:44,000 --> 00:14:47,120
zero the sum of the loss

348
00:14:47,120 --> 00:14:51,120
of all triplets gives us the total loss

349
00:14:51,120 --> 00:14:53,519
based on the triplet loss function we

350
00:14:53,519 --> 00:14:56,720
propose a new model architecture

351
00:14:56,720 --> 00:14:59,680
as shown in the figure the input is a

352
00:14:59,680 --> 00:15:00,959
triplet

353
00:15:00,959 --> 00:15:04,240
which has an anchor a positive and a

354
00:15:04,240 --> 00:15:05,839
negative

355
00:15:05,839 --> 00:15:09,360
the output is three euclidean embeddings

356
00:15:09,360 --> 00:15:12,480
and they are possessed by l2 norms so

357
00:15:12,480 --> 00:15:13,760
that they will be

358
00:15:13,760 --> 00:15:17,120
mapped to the surface of a sphere

359
00:15:17,120 --> 00:15:19,760
the loss function is a triplet loss

360
00:15:19,760 --> 00:15:22,079
which is able to separate the anchor and

361
00:15:22,079 --> 00:15:22,959
the positive

362
00:15:22,959 --> 00:15:26,000
from the negative in euclidean space

363
00:15:26,000 --> 00:15:30,399
as we talked about in particular

364
00:15:30,399 --> 00:15:33,440
the deep network is flexible available

365
00:15:33,440 --> 00:15:35,279
models are faster text

366
00:15:35,279 --> 00:15:39,040
in gram cn and the syntax cn

367
00:15:39,040 --> 00:15:40,959
considering the trade-off between

368
00:15:40,959 --> 00:15:43,600
performance and the model complexity

369
00:15:43,600 --> 00:15:46,000
we choose the engram cnn as our deep

370
00:15:46,000 --> 00:15:46,639
network

371
00:15:46,639 --> 00:15:51,040
for medium performance and complexity

372
00:15:51,040 --> 00:15:54,079
besides we also optimize our triplet

373
00:15:54,079 --> 00:15:57,680
selection method suppose we have 100

374
00:15:57,680 --> 00:15:58,160
authors

375
00:15:58,160 --> 00:16:01,519
in our data set and each author has 100

376
00:16:01,519 --> 00:16:03,120
articles

377
00:16:03,120 --> 00:16:05,759
so for each author there will be 100

378
00:16:05,759 --> 00:16:06,880
anchor

379
00:16:06,880 --> 00:16:09,920
and each anchor has 99 positives to

380
00:16:09,920 --> 00:16:13,839
pair since there are 99 different

381
00:16:13,839 --> 00:16:16,880
authors we multiply it by 100 article

382
00:16:16,880 --> 00:16:18,079
per person

383
00:16:18,079 --> 00:16:21,839
which is 9900 negatives

384
00:16:21,839 --> 00:16:24,720
product of those number gave us about 10

385
00:16:24,720 --> 00:16:28,079
billion kinds of triplet combinations

386
00:16:28,079 --> 00:16:30,240
data is a huge number even though we

387
00:16:30,240 --> 00:16:31,120
only have 10

388
00:16:31,120 --> 00:16:34,000
thousand articles in our data set as for

389
00:16:34,000 --> 00:16:35,040
the result

390
00:16:35,040 --> 00:16:38,000
we implemented two selection method and

391
00:16:38,000 --> 00:16:39,120
the first one is

392
00:16:39,120 --> 00:16:42,000
random selection simple but not

393
00:16:42,000 --> 00:16:43,519
efficient

394
00:16:43,519 --> 00:16:46,320
basically for every possible anchor we

395
00:16:46,320 --> 00:16:48,399
set a top setting for the number of

396
00:16:48,399 --> 00:16:51,440
positive and the negative we select v

397
00:16:51,440 --> 00:16:55,519
and q respectively therefore the amount

398
00:16:55,519 --> 00:16:56,880
of the triplets set

399
00:16:56,880 --> 00:17:00,000
will reduce to a manageable size

400
00:17:00,000 --> 00:17:02,320
the randomization ensures that our

401
00:17:02,320 --> 00:17:03,040
selection

402
00:17:03,040 --> 00:17:05,599
is a representative of the overall data

403
00:17:05,599 --> 00:17:07,839
set

404
00:17:08,160 --> 00:17:11,760
but it is not enough for us imagine you

405
00:17:11,760 --> 00:17:13,599
are climbing over a hill

406
00:17:13,599 --> 00:17:16,480
if you choose a flat road it might take

407
00:17:16,480 --> 00:17:19,119
a long time to reach the top

408
00:17:19,119 --> 00:17:22,079
but if you choose a steep path the time

409
00:17:22,079 --> 00:17:23,520
it takes you to the top

410
00:17:23,520 --> 00:17:26,400
is much shorter even though it requires

411
00:17:26,400 --> 00:17:28,160
more work

412
00:17:28,160 --> 00:17:30,400
similarly we want the model to learn

413
00:17:30,400 --> 00:17:32,000
faster even it takes more

414
00:17:32,000 --> 00:17:35,120
time each epic to accelerate

415
00:17:35,120 --> 00:17:38,080
model training we implemented a method

416
00:17:38,080 --> 00:17:38,400
called

417
00:17:38,400 --> 00:17:42,080
dynamics selection firstly we usually

418
00:17:42,080 --> 00:17:44,080
and randomly divide the data set

419
00:17:44,080 --> 00:17:47,360
into several partitions for each

420
00:17:47,360 --> 00:17:48,640
partition

421
00:17:48,640 --> 00:17:51,520
we select the anchor positive pair like

422
00:17:51,520 --> 00:17:54,000
the random selection strategy

423
00:17:54,000 --> 00:17:56,799
and for each pair we pick the negative

424
00:17:56,799 --> 00:17:57,679
candidate

425
00:17:57,679 --> 00:18:00,880
that satisfy the following equation to

426
00:18:00,880 --> 00:18:02,080
put it simple

427
00:18:02,080 --> 00:18:04,480
we filter out those triplets that

428
00:18:04,480 --> 00:18:05,280
already meet

429
00:18:05,280 --> 00:18:08,240
our goal which is the first case in the

430
00:18:08,240 --> 00:18:09,760
graph

431
00:18:09,760 --> 00:18:12,400
lastly we randomly select one negative

432
00:18:12,400 --> 00:18:14,080
from candidate

433
00:18:14,080 --> 00:18:16,559
to compose the triplet and threw them

434
00:18:16,559 --> 00:18:19,840
into the next epic

435
00:18:19,840 --> 00:18:22,240
this selection triplet has a positive

436
00:18:22,240 --> 00:18:24,960
impact on the overall loss value

437
00:18:24,960 --> 00:18:28,240
which improves efficiency

438
00:18:28,240 --> 00:18:31,520
the first part is our experiment which

439
00:18:31,520 --> 00:18:32,000
includes

440
00:18:32,000 --> 00:18:36,559
our experiment procedures and results

441
00:18:36,559 --> 00:18:38,799
firstly we recorded the article from

442
00:18:38,799 --> 00:18:40,640
eight websites

443
00:18:40,640 --> 00:18:44,000
then we organize and clean the raw data

444
00:18:44,000 --> 00:18:46,720
filtering out unwanted information like

445
00:18:46,720 --> 00:18:49,039
incomplete or duplicate articles

446
00:18:49,039 --> 00:18:52,559
and author info our data set

447
00:18:52,559 --> 00:18:56,000
include about 130 articles

448
00:18:56,000 --> 00:18:59,679
from 3600 authors

449
00:18:59,679 --> 00:19:02,720
we performed comparison comp experiment

450
00:19:02,720 --> 00:19:05,840
on those three major tasks in authorship

451
00:19:05,840 --> 00:19:06,559
analysis

452
00:19:06,559 --> 00:19:10,080
as mentioned before

453
00:19:10,080 --> 00:19:12,799
here is some basic information about our

454
00:19:12,799 --> 00:19:13,520
experiment

455
00:19:13,520 --> 00:19:16,880
environment our gpu is a tesla

456
00:19:16,880 --> 00:19:20,400
p40 and we use the tension flow and the

457
00:19:20,400 --> 00:19:21,200
kiris

458
00:19:21,200 --> 00:19:24,400
as the platform for deep learning in

459
00:19:24,400 --> 00:19:25,120
order to

460
00:19:25,120 --> 00:19:27,840
explore the effect of the sample size on

461
00:19:27,840 --> 00:19:29,200
each model

462
00:19:29,200 --> 00:19:31,200
we write the author based on their

463
00:19:31,200 --> 00:19:32,480
number of words

464
00:19:32,480 --> 00:19:34,960
and create seven data set that contains

465
00:19:34,960 --> 00:19:36,080
answers from top

466
00:19:36,080 --> 00:19:39,360
five to top 2000

467
00:19:39,360 --> 00:19:43,360
also we select no more than 120 articles

468
00:19:43,360 --> 00:19:44,640
per author

469
00:19:44,640 --> 00:19:50,240
and the trend test ratio is 822 as usual

470
00:19:50,880 --> 00:19:53,600
for authorship attribution we compared

471
00:19:53,600 --> 00:19:54,480
our model with

472
00:19:54,480 --> 00:19:58,640
cn and gram cn word and engram svn

473
00:19:58,640 --> 00:20:01,919
by measuring the f1 score

474
00:20:01,919 --> 00:20:05,600
ds here stands for triplet as in a model

475
00:20:05,600 --> 00:20:08,799
that trained by the dynamic selection

476
00:20:08,799 --> 00:20:11,520
and rs means trained by random selection

477
00:20:11,520 --> 00:20:13,840
of course

478
00:20:13,840 --> 00:20:16,720
you can see here is that our model is

479
00:20:16,720 --> 00:20:17,120
over

480
00:20:17,120 --> 00:20:20,320
cm word and the cn cnn grant

481
00:20:20,320 --> 00:20:23,440
among all the data sets

482
00:20:23,440 --> 00:20:26,400
even though engram svm has performed

483
00:20:26,400 --> 00:20:28,480
better when the number of authors

484
00:20:28,480 --> 00:20:32,240
is 50 and 100. our method has

485
00:20:32,240 --> 00:20:34,960
obvious advantage that other baselines

486
00:20:34,960 --> 00:20:38,159
when the number of author is large

487
00:20:38,159 --> 00:20:41,679
next for authorship verification again

488
00:20:41,679 --> 00:20:43,919
it is determining whether the author of

489
00:20:43,919 --> 00:20:47,280
two articles are the same or not

490
00:20:47,280 --> 00:20:50,320
we use the val a matrices introduce the

491
00:20:50,320 --> 00:20:51,919
interface net

492
00:20:51,919 --> 00:20:55,039
to evaluate its performance you can see

493
00:20:55,039 --> 00:20:56,000
our model is

494
00:20:56,000 --> 00:20:59,120
over the baseline under all data set

495
00:20:59,120 --> 00:21:01,200
and provides higher accuracy on

496
00:21:01,200 --> 00:21:03,200
calculating the similarity between

497
00:21:03,200 --> 00:21:05,200
articles

498
00:21:05,200 --> 00:21:08,320
lastly we use the fb cubed score to

499
00:21:08,320 --> 00:21:09,919
compare our model with

500
00:21:09,919 --> 00:21:13,360
login again triplet snn

501
00:21:13,360 --> 00:21:17,120
ds scored higher in all data sets

502
00:21:17,120 --> 00:21:19,919
the advantage is more significant as the

503
00:21:19,919 --> 00:21:21,120
number of authors

504
00:21:21,120 --> 00:21:24,480
increase here is a graph virtualize the

505
00:21:24,480 --> 00:21:25,840
clustering result

506
00:21:25,840 --> 00:21:30,240
on 500 articles of the top two authors

507
00:21:30,240 --> 00:21:32,559
blue dots and red dots represent a

508
00:21:32,559 --> 00:21:34,400
different author

509
00:21:34,400 --> 00:21:36,960
you can see our method produce a

510
00:21:36,960 --> 00:21:37,679
distinct

511
00:21:37,679 --> 00:21:40,799
spatial separation we'll log in to

512
00:21:40,799 --> 00:21:45,280
generate a more chaotic image

513
00:21:45,280 --> 00:21:48,320
and that's the end of our experiment now

514
00:21:48,320 --> 00:21:50,400
let's wrap up our presentation with a

515
00:21:50,400 --> 00:21:53,200
short summary

516
00:21:53,200 --> 00:21:55,919
first we design and implement the first

517
00:21:55,919 --> 00:21:56,640
unified

518
00:21:56,640 --> 00:22:00,240
embedding method for authorship analysis

519
00:22:00,240 --> 00:22:03,200
we also designed an effective triplet

520
00:22:03,200 --> 00:22:05,840
selection technique

521
00:22:05,840 --> 00:22:08,480
in the experiment we collect the first

522
00:22:08,480 --> 00:22:09,840
chinese data set

523
00:22:09,840 --> 00:22:13,200
for authorship analysis and our method

524
00:22:13,200 --> 00:22:15,919
outperformed other baselines

525
00:22:15,919 --> 00:22:19,360
especially when the dataset gets large

526
00:22:19,360 --> 00:22:21,919
in the future we will continue to test

527
00:22:21,919 --> 00:22:23,120
our model

528
00:22:23,120 --> 00:22:26,080
and optimize the deep learning network

529
00:22:26,080 --> 00:22:30,080
and triple h selection strategy

530
00:22:30,159 --> 00:22:32,480
and that's all i have to say thank you

531
00:22:32,480 --> 00:22:33,679
for your patience

532
00:22:33,679 --> 00:22:37,600
hope you all have a great day guys

