1
00:00:01,920 --> 00:00:14,709
[Music]

2
00:00:16,800 --> 00:00:17,840
hi

3
00:00:17,840 --> 00:00:20,560
this is the bmat approach to threat

4
00:00:20,560 --> 00:00:24,720
modeling at black hat asia 2021 i'm adam

5
00:00:24,720 --> 00:00:26,320
shostak

6
00:00:26,320 --> 00:00:28,160
and this is a talk

7
00:00:28,160 --> 00:00:29,279
today

8
00:00:29,279 --> 00:00:30,640
about

9
00:00:30,640 --> 00:00:33,920
a particular problem which i see over

10
00:00:33,920 --> 00:00:35,840
and over again when i'm talking to

11
00:00:35,840 --> 00:00:37,920
people about how they're adapting threat

12
00:00:37,920 --> 00:00:39,120
modeling

13
00:00:39,120 --> 00:00:41,680
to their organization

14
00:00:41,680 --> 00:00:43,600
and

15
00:00:43,600 --> 00:00:45,520
so i'm looking forward to sharing it

16
00:00:45,520 --> 00:00:47,039
with you

17
00:00:47,039 --> 00:00:48,719
tell you a little bit about myself

18
00:00:48,719 --> 00:00:51,600
before i get started so

19
00:00:51,600 --> 00:00:54,239
i do a lot of work in threat modeling we

20
00:00:54,239 --> 00:00:56,320
wrote a book on the subject created the

21
00:00:56,320 --> 00:00:58,399
elevation of privileged card game to

22
00:00:58,399 --> 00:01:00,719
help people learn to threat model

23
00:01:00,719 --> 00:01:03,760
i advise areas risk a company that

24
00:01:03,760 --> 00:01:06,240
builds software earlier in my career i

25
00:01:06,240 --> 00:01:08,960
built what is now the microsoft sdl

26
00:01:08,960 --> 00:01:10,560
threat modeling tool

27
00:01:10,560 --> 00:01:12,400
i'm on the review board here at black

28
00:01:12,400 --> 00:01:15,280
hat i helped to create the cve

29
00:01:15,280 --> 00:01:17,360
and i run a consulting company show

30
00:01:17,360 --> 00:01:19,600
stack and associates that helps people

31
00:01:19,600 --> 00:01:21,600
choose threat model

32
00:01:21,600 --> 00:01:24,000
and today

33
00:01:24,000 --> 00:01:26,960
this is really focused on conflict

34
00:01:26,960 --> 00:01:31,200
between security and other teams and

35
00:01:31,200 --> 00:01:32,720
i'm going to give you

36
00:01:32,720 --> 00:01:34,799
a quick introduction to threat modeling

37
00:01:34,799 --> 00:01:37,280
in case you're not familiar with how we

38
00:01:37,280 --> 00:01:40,960
do it today there's been tremendous

39
00:01:40,960 --> 00:01:42,320
tremendous

40
00:01:42,320 --> 00:01:45,119
progress made in how we structure the

41
00:01:45,119 --> 00:01:47,280
way we threaten model over the last few

42
00:01:47,280 --> 00:01:49,119
years and i'm going to tell you about

43
00:01:49,119 --> 00:01:51,280
the bmat approach and i want to tell you

44
00:01:51,280 --> 00:01:54,479
don't bother googling bmad it's not

45
00:01:54,479 --> 00:01:56,479
something that people have heard about

46
00:01:56,479 --> 00:01:59,200
in this form before today

47
00:01:59,200 --> 00:02:02,159
and so i will explain it later on in the

48
00:02:02,159 --> 00:02:05,759
top where the name comes from

49
00:02:05,759 --> 00:02:07,439
let's get going

50
00:02:07,439 --> 00:02:08,720
so

51
00:02:08,720 --> 00:02:11,200
i want to start out by talking about

52
00:02:11,200 --> 00:02:14,720
security teams and conflict

53
00:02:14,720 --> 00:02:17,360
this is something i hear over and over

54
00:02:17,360 --> 00:02:20,000
again is that

55
00:02:20,000 --> 00:02:22,640
the security team is in the midst of yet

56
00:02:22,640 --> 00:02:25,040
another escalation with

57
00:02:25,040 --> 00:02:27,599
development with operations i was

58
00:02:27,599 --> 00:02:29,280
talking to a buddy of mine and he said

59
00:02:29,280 --> 00:02:30,480
adam

60
00:02:30,480 --> 00:02:32,800
how do i get these folks to stop

61
00:02:32,800 --> 00:02:34,000
fighting

62
00:02:34,000 --> 00:02:36,080
but there's really this pattern where

63
00:02:36,080 --> 00:02:38,239
security is fighting with ops security

64
00:02:38,239 --> 00:02:40,160
is fighting with software development

65
00:02:40,160 --> 00:02:42,319
security needs to learn to talk to the

66
00:02:42,319 --> 00:02:44,720
business

67
00:02:46,000 --> 00:02:48,000
and you know

68
00:02:48,000 --> 00:02:50,080
something something i need to say here

69
00:02:50,080 --> 00:02:52,000
which is that when we're talking to our

70
00:02:52,000 --> 00:02:54,239
friends something that there's a sort of

71
00:02:54,239 --> 00:02:56,080
an old chestnut

72
00:02:56,080 --> 00:02:58,159
that what we say is the one thing that

73
00:02:58,159 --> 00:03:00,720
your failed relationships have in common

74
00:03:00,720 --> 00:03:02,840
is you

75
00:03:02,840 --> 00:03:05,680
and there's three things we can do about

76
00:03:05,680 --> 00:03:07,440
this

77
00:03:07,440 --> 00:03:10,159
one we can keep fighting

78
00:03:10,159 --> 00:03:11,280
two

79
00:03:11,280 --> 00:03:14,080
we can go find a new relationship and

80
00:03:14,080 --> 00:03:16,000
hope it works better

81
00:03:16,000 --> 00:03:18,959
or three we can think about how our

82
00:03:18,959 --> 00:03:20,959
behavior might be contributing to the

83
00:03:20,959 --> 00:03:22,080
problem

84
00:03:22,080 --> 00:03:25,040
and spoiler alert that's what this talk

85
00:03:25,040 --> 00:03:26,319
is about

86
00:03:26,319 --> 00:03:29,519
is the way in which a particular pattern

87
00:03:29,519 --> 00:03:32,400
of behavior by security teams

88
00:03:32,400 --> 00:03:35,519
leads to needless conflict and reduces

89
00:03:35,519 --> 00:03:38,560
our ability to protect the businesses in

90
00:03:38,560 --> 00:03:42,080
the way in which we hope to protect them

91
00:03:42,080 --> 00:03:44,080
so

92
00:03:44,080 --> 00:03:45,120
let me

93
00:03:45,120 --> 00:03:47,360
let me give you a quick intro to threat

94
00:03:47,360 --> 00:03:50,640
modeling this is adam's two-minute intro

95
00:03:50,640 --> 00:03:53,360
for those of you who are not familiar

96
00:03:53,360 --> 00:03:55,840
bear with me if this is review

97
00:03:55,840 --> 00:03:58,720
so let me start what is threat modeling

98
00:03:58,720 --> 00:04:00,959
scrap modeling is very simple it's the

99
00:04:00,959 --> 00:04:03,599
use of models to help us think about

100
00:04:03,599 --> 00:04:06,319
threats and a model is an abstraction we

101
00:04:06,319 --> 00:04:08,560
get rid of some details so we can see

102
00:04:08,560 --> 00:04:10,640
the forest not the trees

103
00:04:10,640 --> 00:04:13,200
and a threat the way i'm using it we use

104
00:04:13,200 --> 00:04:15,519
it in a lot of ways in security the way

105
00:04:15,519 --> 00:04:18,238
i use it is the promise of a future

106
00:04:18,238 --> 00:04:20,320
problem especially if we don't do

107
00:04:20,320 --> 00:04:21,839
something to fix it

108
00:04:21,839 --> 00:04:23,440
he threatened to beat me up if i didn't

109
00:04:23,440 --> 00:04:26,560
give him my lunch money that's a threat

110
00:04:26,560 --> 00:04:29,520
and in computer security

111
00:04:29,520 --> 00:04:32,160
there might be a threat that someone's

112
00:04:32,160 --> 00:04:34,639
going to brute force logins on this

113
00:04:34,639 --> 00:04:37,680
particular api or they're going to reuse

114
00:04:37,680 --> 00:04:39,600
stolen credentials

115
00:04:39,600 --> 00:04:41,680
to get in over this api and then we

116
00:04:41,680 --> 00:04:44,720
might build defenses to deal with that

117
00:04:44,720 --> 00:04:47,440
anticipated problem

118
00:04:47,440 --> 00:04:50,000
and so threat modeling is a powerful set

119
00:04:50,000 --> 00:04:52,320
of techniques that we use and sometimes

120
00:04:52,320 --> 00:04:54,560
people confuse it with threat

121
00:04:54,560 --> 00:04:58,160
intelligence let me compare and contrast

122
00:04:58,160 --> 00:05:01,039
threat intelligence is collecting data

123
00:05:01,039 --> 00:05:04,000
often indicators of indicators of

124
00:05:04,000 --> 00:05:06,160
compromise

125
00:05:06,160 --> 00:05:09,680
ip addresses domains hashes and using

126
00:05:09,680 --> 00:05:12,080
them to find attacker activity it is

127
00:05:12,080 --> 00:05:13,600
something we do

128
00:05:13,600 --> 00:05:15,520
when a system has been built and when

129
00:05:15,520 --> 00:05:18,080
it's operational in contrast threat

130
00:05:18,080 --> 00:05:19,440
modeling

131
00:05:19,440 --> 00:05:20,800
is about

132
00:05:20,800 --> 00:05:23,440
anticipating these problems

133
00:05:23,440 --> 00:05:25,680
perhaps including realizing that we

134
00:05:25,680 --> 00:05:27,520
might have problems and that we might

135
00:05:27,520 --> 00:05:30,479
need to react to them and so designing

136
00:05:30,479 --> 00:05:34,320
our systems to deal with them

137
00:05:34,320 --> 00:05:36,560
and

138
00:05:36,800 --> 00:05:38,880
you know i

139
00:05:38,880 --> 00:05:40,560
really want to emphasize that threat

140
00:05:40,560 --> 00:05:43,280
modeling is actually pretty easy

141
00:05:43,280 --> 00:05:46,000
many of us picked it up by osmosis we

142
00:05:46,000 --> 00:05:47,680
learned it

143
00:05:47,680 --> 00:05:49,680
without having to have a huge amount of

144
00:05:49,680 --> 00:05:52,880
structure and today it's helpful to have

145
00:05:52,880 --> 00:05:54,880
some structure and the structure i like

146
00:05:54,880 --> 00:05:56,479
to use the most

147
00:05:56,479 --> 00:05:58,080
is what i call the four question

148
00:05:58,080 --> 00:06:00,639
framework

149
00:06:00,880 --> 00:06:02,639
the four question framework is four

150
00:06:02,639 --> 00:06:05,039
simple questions what are we working on

151
00:06:05,039 --> 00:06:06,880
what can go wrong what are we going to

152
00:06:06,880 --> 00:06:10,160
do about it did we do a good job

153
00:06:10,160 --> 00:06:12,800
and if you simply ask these questions

154
00:06:12,800 --> 00:06:14,880
you're doing threat modeling and we can

155
00:06:14,880 --> 00:06:17,039
get very specific in the ways in which

156
00:06:17,039 --> 00:06:21,400
we answer them excuse me

157
00:06:26,160 --> 00:06:28,080
we get very specific in the ways we

158
00:06:28,080 --> 00:06:30,319
answer these questions so for example as

159
00:06:30,319 --> 00:06:32,479
we ask what can go wrong

160
00:06:32,479 --> 00:06:34,720
we might use stride strides and

161
00:06:34,720 --> 00:06:36,639
mnemonics stands for spoofing tampering

162
00:06:36,639 --> 00:06:38,319
repudiation information disclosure

163
00:06:38,319 --> 00:06:40,000
denial of service and elevation of

164
00:06:40,000 --> 00:06:41,120
privilege

165
00:06:41,120 --> 00:06:44,000
and this helps us bring structure

166
00:06:44,000 --> 00:06:47,440
to how we think about what can go wrong

167
00:06:47,440 --> 00:06:50,880
so that is my two minute

168
00:06:50,880 --> 00:06:52,960
that is my two minute introduction to

169
00:06:52,960 --> 00:06:55,039
threat modeling and let me tell you

170
00:06:55,039 --> 00:06:56,880
threat modeling is full of traps that

171
00:06:56,880 --> 00:06:58,720
might be pushing the forward button on

172
00:06:58,720 --> 00:07:01,199
your remote control slightly too early

173
00:07:01,199 --> 00:07:03,599
but there are much more important traps

174
00:07:03,599 --> 00:07:06,080
which you can fall into

175
00:07:06,080 --> 00:07:08,880
and it's a trap because it's easy to

176
00:07:08,880 --> 00:07:10,880
fall into these things we're going along

177
00:07:10,880 --> 00:07:14,080
with good intent and we make a mistake

178
00:07:14,080 --> 00:07:16,080
or we don't even make a mistake we just

179
00:07:16,080 --> 00:07:18,400
fall into a pit and

180
00:07:18,400 --> 00:07:21,919
cat modeling is so full of traps because

181
00:07:21,919 --> 00:07:23,440
it's easy

182
00:07:23,440 --> 00:07:26,960
because we learn it from osmosis

183
00:07:26,960 --> 00:07:29,520
the ways in which we fall into traps our

184
00:07:29,520 --> 00:07:32,479
legion and this talk is about one

185
00:07:32,479 --> 00:07:35,199
particular trap that keeps getting us

186
00:07:35,199 --> 00:07:37,520
the be mad approach so let's talk about

187
00:07:37,520 --> 00:07:39,840
what that is

188
00:07:39,840 --> 00:07:41,599
as we do

189
00:07:41,599 --> 00:07:43,759
i want to put this in a little bit more

190
00:07:43,759 --> 00:07:46,080
context

191
00:07:46,080 --> 00:07:48,400
i created the jenga model of threat

192
00:07:48,400 --> 00:07:52,000
modeling the idea is we have at the top

193
00:07:52,000 --> 00:07:55,680
a secure product delivery system and we

194
00:07:55,680 --> 00:07:57,440
support that

195
00:07:57,440 --> 00:08:00,639
with a set of building blocks and unlike

196
00:08:00,639 --> 00:08:02,639
in django where the goal is to have the

197
00:08:02,639 --> 00:08:05,919
tower not fall down our goal is to build

198
00:08:05,919 --> 00:08:08,960
up a tower that's stable and we do so

199
00:08:08,960 --> 00:08:11,280
with three types of blocks we do so with

200
00:08:11,280 --> 00:08:13,120
technical building blocks things like

201
00:08:13,120 --> 00:08:15,199
stride and data flow diagrams the

202
00:08:15,199 --> 00:08:17,759
software we use to threat model

203
00:08:17,759 --> 00:08:20,800
the mental tools we use to structure our

204
00:08:20,800 --> 00:08:22,319
thinking

205
00:08:22,319 --> 00:08:26,479
there are organizational building blocks

206
00:08:26,479 --> 00:08:28,240
things like metrics and executive

207
00:08:28,240 --> 00:08:30,639
support and how we file bugs

208
00:08:30,639 --> 00:08:32,880
and there are interpersonal skills

209
00:08:32,880 --> 00:08:35,039
things like humility and active

210
00:08:35,039 --> 00:08:36,240
listening

211
00:08:36,240 --> 00:08:38,479
and a constructive approach to

212
00:08:38,479 --> 00:08:40,000
engineering

213
00:08:40,000 --> 00:08:41,120
and so

214
00:08:41,120 --> 00:08:43,599
this particular talk is organizational

215
00:08:43,599 --> 00:08:45,600
it's about conflict

216
00:08:45,600 --> 00:08:48,959
that happens in the way organizations

217
00:08:48,959 --> 00:08:51,360
interact in the way they structure their

218
00:08:51,360 --> 00:08:53,279
threat model and work

219
00:08:53,279 --> 00:08:55,200
and i want to be really clear

220
00:08:55,200 --> 00:08:57,279
this is a conflict that emerges from

221
00:08:57,279 --> 00:09:00,640
good intent nobody here is saying

222
00:09:00,640 --> 00:09:03,279
i'm trying to make a problem but this

223
00:09:03,279 --> 00:09:05,920
conflict happens over and over

224
00:09:05,920 --> 00:09:08,000
and so

225
00:09:08,000 --> 00:09:11,680
the tooling we use in threat modeling is

226
00:09:11,680 --> 00:09:14,640
a little bit different

227
00:09:14,640 --> 00:09:17,200
so i i think about this in terms of how

228
00:09:17,200 --> 00:09:19,600
much engagement does each engineer need

229
00:09:19,600 --> 00:09:20,880
to have

230
00:09:20,880 --> 00:09:24,560
and how much attention is required

231
00:09:24,560 --> 00:09:26,720
so over in one corner over in this

232
00:09:26,720 --> 00:09:29,360
corner is the secure development life

233
00:09:29,360 --> 00:09:31,760
cycle the framework and most engineers

234
00:09:31,760 --> 00:09:35,600
don't need to pay attention to it at all

235
00:09:35,600 --> 00:09:37,760
then in the middle is a set of tools if

236
00:09:37,760 --> 00:09:39,600
you're not familiar with these tools

237
00:09:39,600 --> 00:09:42,399
don't worry about it these tools are all

238
00:09:42,399 --> 00:09:45,040
the software we use in threat modeling

239
00:09:45,040 --> 00:09:47,600
the static analysis the fuzzers

240
00:09:47,600 --> 00:09:49,519
the testing tools

241
00:09:49,519 --> 00:09:51,760
and most engineers don't need to pay

242
00:09:51,760 --> 00:09:53,519
close attention they don't need to be

243
00:09:53,519 --> 00:09:56,160
really involved in how these work they

244
00:09:56,160 --> 00:09:58,160
get a bug they deal with the bug and

245
00:09:58,160 --> 00:10:00,160
they might need to know

246
00:10:00,160 --> 00:10:02,800
something about how the tool works

247
00:10:02,800 --> 00:10:05,200
but if they don't really understand it

248
00:10:05,200 --> 00:10:07,440
it's okay that they're managing one

249
00:10:07,440 --> 00:10:10,000
discrete little issue

250
00:10:10,000 --> 00:10:11,920
and then threat modeling is a little

251
00:10:11,920 --> 00:10:14,320
different in that it often requires a

252
00:10:14,320 --> 00:10:16,640
little bit more thinking

253
00:10:16,640 --> 00:10:19,920
on the part of the participants

254
00:10:19,920 --> 00:10:23,040
and so that brings me to how be mad

255
00:10:23,040 --> 00:10:25,440
causes problems and again you do not

256
00:10:25,440 --> 00:10:27,440
know what b-man stands for yet and

257
00:10:27,440 --> 00:10:28,880
that's okay

258
00:10:28,880 --> 00:10:32,320
don't be mad

259
00:10:32,320 --> 00:10:33,040
so

260
00:10:33,040 --> 00:10:36,240
the basic scenario is this

261
00:10:36,240 --> 00:10:38,480
the developer comes along and says hey

262
00:10:38,480 --> 00:10:41,040
can you help us secure this system

263
00:10:41,040 --> 00:10:42,079
and

264
00:10:42,079 --> 00:10:44,640
the security team represented here by an

265
00:10:44,640 --> 00:10:46,720
awesome little ninja dude

266
00:10:46,720 --> 00:10:47,680
says

267
00:10:47,680 --> 00:10:50,160
sure bring me a diagram and the ninja

268
00:10:50,160 --> 00:10:53,200
person is the security team not the

269
00:10:53,200 --> 00:10:55,519
attacker here

270
00:10:55,519 --> 00:10:56,959
anyway

271
00:10:56,959 --> 00:10:57,920
the

272
00:10:57,920 --> 00:11:00,640
security person says bring me a diagram

273
00:11:00,640 --> 00:11:03,120
and the developer says okay sure i can

274
00:11:03,120 --> 00:11:05,200
bring you a diagram now what they're

275
00:11:05,200 --> 00:11:06,880
really thinking

276
00:11:06,880 --> 00:11:08,800
what the heck is a diagram i'm a

277
00:11:08,800 --> 00:11:11,440
developer i'm not an artist

278
00:11:11,440 --> 00:11:13,600
what does the diagram look like and so

279
00:11:13,600 --> 00:11:16,399
they go off and they diligently create a

280
00:11:16,399 --> 00:11:18,880
diagram like a blueprint of everything

281
00:11:18,880 --> 00:11:21,440
they're doing and they hone it and they

282
00:11:21,440 --> 00:11:24,000
put it through a review cycle and they

283
00:11:24,000 --> 00:11:26,720
bring back this great diagram and the

284
00:11:26,720 --> 00:11:29,440
security team says oh my gosh let me

285
00:11:29,440 --> 00:11:30,640
tell you about all the problems we've

286
00:11:30,640 --> 00:11:31,839
got here

287
00:11:31,839 --> 00:11:34,959
the security team says

288
00:11:35,360 --> 00:11:38,480
i did what you told me

289
00:11:40,480 --> 00:11:43,279
so it's there's some things that happen

290
00:11:43,279 --> 00:11:46,079
here that are important to understand

291
00:11:46,079 --> 00:11:48,399
one is that the security team's view of

292
00:11:48,399 --> 00:11:50,639
what's happening is that all these folks

293
00:11:50,639 --> 00:11:52,399
are showing up saying hey can you help

294
00:11:52,399 --> 00:11:53,839
us secure the system can you help us

295
00:11:53,839 --> 00:11:55,440
secure this system

296
00:11:55,440 --> 00:11:57,760
the security team says bring me diagrams

297
00:11:57,760 --> 00:11:59,519
and then they wait

298
00:11:59,519 --> 00:12:01,839
and as they wait there's so many things

299
00:12:01,839 --> 00:12:03,839
for them to go off in poem there's so

300
00:12:03,839 --> 00:12:05,519
many systems to break there's so many

301
00:12:05,519 --> 00:12:08,079
security issues to find

302
00:12:08,079 --> 00:12:10,320
that they're they're off doing that

303
00:12:10,320 --> 00:12:12,880
other work waiting for the team to come

304
00:12:12,880 --> 00:12:14,880
back with a diagram and what they're

305
00:12:14,880 --> 00:12:16,079
thinking

306
00:12:16,079 --> 00:12:17,760
is that that diagram is going to be like

307
00:12:17,760 --> 00:12:20,079
a whiteboard diagram it's going to be an

308
00:12:20,079 --> 00:12:21,920
early thing where we can have a

309
00:12:21,920 --> 00:12:23,760
conversation

310
00:12:23,760 --> 00:12:27,519
about the security problems that exist

311
00:12:27,519 --> 00:12:28,720
and this

312
00:12:28,720 --> 00:12:31,040
what we have here is a failure to

313
00:12:31,040 --> 00:12:32,320
communicate

314
00:12:32,320 --> 00:12:34,480
this is common and you've probably seen

315
00:12:34,480 --> 00:12:36,639
diagrams of these swings where the

316
00:12:36,639 --> 00:12:38,800
customer wants a tire swing

317
00:12:38,800 --> 00:12:40,639
and they don't get it and i picked this

318
00:12:40,639 --> 00:12:43,200
particular one

319
00:12:43,200 --> 00:12:45,279
because security doesn't even show up

320
00:12:45,279 --> 00:12:48,480
this is not really a security specific

321
00:12:48,480 --> 00:12:50,639
failure to commute the failure to

322
00:12:50,639 --> 00:12:52,320
communicate excuse me the failure to

323
00:12:52,320 --> 00:12:54,720
communicate is not a problem that's

324
00:12:54,720 --> 00:12:56,880
unique to security it happens all over

325
00:12:56,880 --> 00:12:58,720
the place in business

326
00:12:58,720 --> 00:13:00,720
but this is a talk about a particular

327
00:13:00,720 --> 00:13:03,839
security failure

328
00:13:04,000 --> 00:13:06,320
so what's happening here is

329
00:13:06,320 --> 00:13:09,120
miscommunication about expectations do i

330
00:13:09,120 --> 00:13:10,800
want that blueprint or do i want a

331
00:13:10,800 --> 00:13:13,440
whiteboard it's miscommunication about

332
00:13:13,440 --> 00:13:16,480
roles and responsibilities whose job is

333
00:13:16,480 --> 00:13:18,800
creating the diagram

334
00:13:18,800 --> 00:13:20,720
what is the thing that happens is it a

335
00:13:20,720 --> 00:13:23,519
review or a dialogue about what we're

336
00:13:23,519 --> 00:13:25,279
gonna go do

337
00:13:25,279 --> 00:13:27,760
and you know you may be saying oh god

338
00:13:27,760 --> 00:13:30,800
this is so boring adam i just wanna pop

339
00:13:30,800 --> 00:13:33,120
systems there's so many things out there

340
00:13:33,120 --> 00:13:35,279
there's so many cross-site scripting

341
00:13:35,279 --> 00:13:37,120
issues in my system there's so much sql

342
00:13:37,120 --> 00:13:40,240
injection why are we talking about this

343
00:13:40,240 --> 00:13:43,360
and the answer is because threat

344
00:13:43,360 --> 00:13:45,680
modeling gives you an opportunity to be

345
00:13:45,680 --> 00:13:48,880
structured systematic and comprehensive

346
00:13:48,880 --> 00:13:52,959
in how you deal with security issues

347
00:13:52,959 --> 00:13:53,920
and

348
00:13:53,920 --> 00:13:56,880
instead of engaging in those good

349
00:13:56,880 --> 00:14:02,000
conversations we're having these fights

350
00:14:02,000 --> 00:14:05,360
so b-man stands for bring me a diagram

351
00:14:05,360 --> 00:14:09,199
those four simple words are at the core

352
00:14:09,199 --> 00:14:11,839
of this problem

353
00:14:11,839 --> 00:14:13,760
because when i tell someone to bring me

354
00:14:13,760 --> 00:14:15,199
a diagram

355
00:14:15,199 --> 00:14:17,279
and i don't communicate clearly about

356
00:14:17,279 --> 00:14:19,680
what we're going to do next

357
00:14:19,680 --> 00:14:22,720
all of these problems just roll out of

358
00:14:22,720 --> 00:14:26,639
those four little words

359
00:14:26,639 --> 00:14:28,160
so there are some contributors to the

360
00:14:28,160 --> 00:14:29,760
problem again

361
00:14:29,760 --> 00:14:32,160
security teams are highly loaded and

362
00:14:32,160 --> 00:14:35,120
especially security teams

363
00:14:35,120 --> 00:14:38,800
cannot hire enough of those application

364
00:14:38,800 --> 00:14:41,680
security folks who have real software

365
00:14:41,680 --> 00:14:44,959
design experience you can talk fluently

366
00:14:44,959 --> 00:14:48,240
about design trade-offs

367
00:14:48,240 --> 00:14:50,399
security teams are learning about threat

368
00:14:50,399 --> 00:14:52,800
modeling today and i'm watching a lot of

369
00:14:52,800 --> 00:14:56,320
really exciting progress in the field

370
00:14:56,320 --> 00:14:58,399
but one of the things that's happening

371
00:14:58,399 --> 00:15:02,160
is we don't have enough experience

372
00:15:02,160 --> 00:15:04,560
with great threat modeling rollouts to

373
00:15:04,560 --> 00:15:07,199
see that this diverges from the good

374
00:15:07,199 --> 00:15:08,720
pattern

375
00:15:08,720 --> 00:15:10,480
and

376
00:15:10,480 --> 00:15:12,240
when i talk about threat modeling i talk

377
00:15:12,240 --> 00:15:14,000
about four questions what are we working

378
00:15:14,000 --> 00:15:15,519
on what can go wrong what are we going

379
00:15:15,519 --> 00:15:18,720
to do about it and did we do a good job

380
00:15:18,720 --> 00:15:21,199
and there's some folks who say hey

381
00:15:21,199 --> 00:15:23,279
teams should be focused on doing

382
00:15:23,279 --> 00:15:26,079
retrospectives anyway we don't need to

383
00:15:26,079 --> 00:15:27,839
ask if we do a good job and there's some

384
00:15:27,839 --> 00:15:29,519
popular advice i don't want to point

385
00:15:29,519 --> 00:15:31,120
fingers we've had the conversation in

386
00:15:31,120 --> 00:15:32,639
private

387
00:15:32,639 --> 00:15:33,440
but

388
00:15:33,440 --> 00:15:35,519
there's some good advice out there that

389
00:15:35,519 --> 00:15:38,240
eliminates the fourth question ended

390
00:15:38,240 --> 00:15:40,959
eliminating it in not making the time

391
00:15:40,959 --> 00:15:42,880
and the space

392
00:15:42,880 --> 00:15:43,920
for

393
00:15:43,920 --> 00:15:45,120
the

394
00:15:45,120 --> 00:15:47,920
question of did we do a good job in

395
00:15:47,920 --> 00:15:50,639
threat modeling in particular

396
00:15:50,639 --> 00:15:52,959
the problem occurs because there's no

397
00:15:52,959 --> 00:15:55,120
time to focus on it and so i think it's

398
00:15:55,120 --> 00:15:58,800
really important for you to understand

399
00:15:58,800 --> 00:16:00,880
that as you get started threat modeling

400
00:16:00,880 --> 00:16:02,320
you should spend as much time on

401
00:16:02,320 --> 00:16:04,560
retrospectives as you do on doing the

402
00:16:04,560 --> 00:16:06,160
technical work you don't have to do it

403
00:16:06,160 --> 00:16:09,519
for very long but doing it as you get

404
00:16:09,519 --> 00:16:11,839
started really helps you identify

405
00:16:11,839 --> 00:16:14,079
problems including but not limited to

406
00:16:14,079 --> 00:16:16,560
this one

407
00:16:16,639 --> 00:16:17,839
the other thing that's happening is

408
00:16:17,839 --> 00:16:19,839
development teams are getting more and

409
00:16:19,839 --> 00:16:21,600
more agile

410
00:16:21,600 --> 00:16:24,320
and so

411
00:16:24,880 --> 00:16:27,040
instead of doing two week sprints we're

412
00:16:27,040 --> 00:16:29,120
seeing cicd

413
00:16:29,120 --> 00:16:31,519
when is their time to create a diagram

414
00:16:31,519 --> 00:16:33,759
the diagram gets created after the

415
00:16:33,759 --> 00:16:35,759
software gets shipped

416
00:16:35,759 --> 00:16:39,040
someone will do a technical debt retro

417
00:16:39,040 --> 00:16:40,240
we're going to pay down the debt we're

418
00:16:40,240 --> 00:16:42,000
going to draw what we've just dropped

419
00:16:42,000 --> 00:16:42,959
built

420
00:16:42,959 --> 00:16:45,279
we're going to draw what we've shipped

421
00:16:45,279 --> 00:16:46,320
you can't

422
00:16:46,320 --> 00:16:48,720
you can threat model what you've just

423
00:16:48,720 --> 00:16:51,279
shipped it's much more useful to threat

424
00:16:51,279 --> 00:16:53,600
model well it's in that early stage that

425
00:16:53,600 --> 00:16:57,199
whiteboard stage that sketch stage

426
00:16:57,199 --> 00:16:59,680
and so let's talk about how to avoid be

427
00:16:59,680 --> 00:17:01,839
mad in specific

428
00:17:01,839 --> 00:17:03,920
and i'm going to use the jenga model

429
00:17:03,920 --> 00:17:05,720
again the model of technical

430
00:17:05,720 --> 00:17:09,520
organizational and interpersonal skills

431
00:17:09,520 --> 00:17:11,439
so technical

432
00:17:11,439 --> 00:17:13,520
as we talk about

433
00:17:13,520 --> 00:17:16,400
what are we working on

434
00:17:16,400 --> 00:17:20,400
moving from bring me a diagram to

435
00:17:20,400 --> 00:17:23,119
whiteboards is super valuable

436
00:17:23,119 --> 00:17:24,319
and there are

437
00:17:24,319 --> 00:17:26,640
lots of collaborative drawing tools that

438
00:17:26,640 --> 00:17:29,600
work today during the pandemic

439
00:17:29,600 --> 00:17:31,039
for

440
00:17:31,039 --> 00:17:34,240
great collaborative analysis of this is

441
00:17:34,240 --> 00:17:36,960
what we're working on right now

442
00:17:36,960 --> 00:17:39,679
there's a series of talks is our tyrone

443
00:17:39,679 --> 00:17:42,559
doc has done a set of talks on threat

444
00:17:42,559 --> 00:17:44,640
model every story

445
00:17:44,640 --> 00:17:46,480
back when he was at autodesk they

446
00:17:46,480 --> 00:17:48,640
released a bunch of the training

447
00:17:48,640 --> 00:17:52,320
material for that it's good stuff

448
00:17:52,320 --> 00:17:53,919
and

449
00:17:53,919 --> 00:17:55,919
then there's big wall maps i'll talk

450
00:17:55,919 --> 00:17:58,000
about those on the next slide

451
00:17:58,000 --> 00:18:00,720
as we talk about our deliverables

452
00:18:00,720 --> 00:18:03,440
having a sample of this is the sort of

453
00:18:03,440 --> 00:18:07,520
diagram we expect to see at this stage

454
00:18:07,520 --> 00:18:10,559
really helps set people's expectations

455
00:18:10,559 --> 00:18:13,120
about the level of quality

456
00:18:13,120 --> 00:18:14,720
that you're looking for the level of

457
00:18:14,720 --> 00:18:17,360
refinement the level of review

458
00:18:17,360 --> 00:18:19,679
which should be low you want these

459
00:18:19,679 --> 00:18:22,240
things to come in early on while there's

460
00:18:22,240 --> 00:18:24,160
time to make changes

461
00:18:24,160 --> 00:18:28,679
to the systems that are being developed

462
00:18:29,120 --> 00:18:31,440
you want to align the deliverables to

463
00:18:31,440 --> 00:18:34,080
the four questions of threat modeling so

464
00:18:34,080 --> 00:18:36,880
a deliverable is what are we working on

465
00:18:36,880 --> 00:18:37,760
here

466
00:18:37,760 --> 00:18:39,919
a deliverable is a set of answers to

467
00:18:39,919 --> 00:18:43,120
what are the questions or what can

468
00:18:43,120 --> 00:18:45,679
excuse me a deliverable is a set of

469
00:18:45,679 --> 00:18:47,760
answers to the question

470
00:18:47,760 --> 00:18:50,080
what can go wrong that might just be a

471
00:18:50,080 --> 00:18:52,160
list um

472
00:18:52,160 --> 00:18:54,000
and then later on as we figure out what

473
00:18:54,000 --> 00:18:55,520
are we going to do about it we write

474
00:18:55,520 --> 00:18:58,720
bugs for each of those things

475
00:18:58,720 --> 00:19:01,039
and you want to think very explicitly

476
00:19:01,039 --> 00:19:03,679
about what skills do people need to

477
00:19:03,679 --> 00:19:06,160
answer these questions well

478
00:19:06,160 --> 00:19:08,720
so does the security team have design

479
00:19:08,720 --> 00:19:11,360
skills or penetration testing skills i

480
00:19:11,360 --> 00:19:13,919
know where that falls today

481
00:19:13,919 --> 00:19:16,160
does the development the ops team the

482
00:19:16,160 --> 00:19:19,840
sre team have security skills if you

483
00:19:19,840 --> 00:19:22,559
know the answers to these questions

484
00:19:22,559 --> 00:19:25,039
you can think about appropriate focus

485
00:19:25,039 --> 00:19:27,280
training to help people deliver the

486
00:19:27,280 --> 00:19:31,440
skills they need to do these things

487
00:19:33,760 --> 00:19:36,240
let me speak about big wall maps

488
00:19:36,240 --> 00:19:38,000
which is another part of the technical

489
00:19:38,000 --> 00:19:41,280
approach to threat modeling

490
00:19:41,600 --> 00:19:43,760
when we're in offices

491
00:19:43,760 --> 00:19:46,720
the space on the walls is a super

492
00:19:46,720 --> 00:19:48,960
limited resource that we can think about

493
00:19:48,960 --> 00:19:50,880
and we think about how do we budget it

494
00:19:50,880 --> 00:19:53,440
and we use that wall space

495
00:19:53,440 --> 00:19:56,400
to show what's important to us we put

496
00:19:56,400 --> 00:19:59,280
kanban boards on it we put scrum boards

497
00:19:59,280 --> 00:20:02,240
on it to generate shared awareness

498
00:20:02,240 --> 00:20:04,960
and in a shared office space we can also

499
00:20:04,960 --> 00:20:08,000
create a big wall map of the software

500
00:20:08,000 --> 00:20:10,880
and as we threat model

501
00:20:10,880 --> 00:20:13,200
we can go up to that wall map and we can

502
00:20:13,200 --> 00:20:15,120
say hey

503
00:20:15,120 --> 00:20:18,000
my story adds a line between these two

504
00:20:18,000 --> 00:20:19,520
components

505
00:20:19,520 --> 00:20:21,280
i need to think about how that could be

506
00:20:21,280 --> 00:20:24,720
spoof tampered repudiated etc or

507
00:20:24,720 --> 00:20:26,720
my story doesn't change this i'm just

508
00:20:26,720 --> 00:20:28,880
changing an algorithm we're using on the

509
00:20:28,880 --> 00:20:30,960
same data ooh but i should think about

510
00:20:30,960 --> 00:20:32,880
how i'm parsing the data that i get from

511
00:20:32,880 --> 00:20:35,120
this database because i see that it's on

512
00:20:35,120 --> 00:20:37,679
the far side of a trust boundary

513
00:20:37,679 --> 00:20:39,120
and so

514
00:20:39,120 --> 00:20:41,760
this is easier frankly when we're all in

515
00:20:41,760 --> 00:20:43,919
a shared space and that's not the case

516
00:20:43,919 --> 00:20:45,200
today

517
00:20:45,200 --> 00:20:47,919
but we can emulate it we can build these

518
00:20:47,919 --> 00:20:50,880
maps we can show what the system

519
00:20:50,880 --> 00:20:53,520
overall looks like and in doing so we

520
00:20:53,520 --> 00:20:56,320
reduce redesign work

521
00:20:56,320 --> 00:20:58,720
and we keep everyone focused on there's

522
00:20:58,720 --> 00:21:00,880
a method to the madness you should know

523
00:21:00,880 --> 00:21:02,960
how your system fits in

524
00:21:02,960 --> 00:21:06,159
to the big picture

525
00:21:07,760 --> 00:21:11,120
the organizational fixes to be mad

526
00:21:11,120 --> 00:21:14,080
first know who delivers what to whom and

527
00:21:14,080 --> 00:21:15,919
some of these deliverables like the

528
00:21:15,919 --> 00:21:18,480
models of a system can be collaborative

529
00:21:18,480 --> 00:21:20,720
it can be the security team

530
00:21:20,720 --> 00:21:23,840
works with the developer to build a

531
00:21:23,840 --> 00:21:26,559
quick model of the system

532
00:21:26,559 --> 00:21:28,640
you should know

533
00:21:28,640 --> 00:21:31,600
if your team if your security team if

534
00:21:31,600 --> 00:21:33,760
your organization

535
00:21:33,760 --> 00:21:37,679
thinks about early and often as a win we

536
00:21:37,679 --> 00:21:39,679
want to invest the energy in it or it's

537
00:21:39,679 --> 00:21:40,880
a waste now

538
00:21:40,880 --> 00:21:42,720
bring us a diagram we're okay with the

539
00:21:42,720 --> 00:21:45,039
conflict we're okay with

540
00:21:45,039 --> 00:21:47,120
treating this as a review and i'll talk

541
00:21:47,120 --> 00:21:49,360
about that in a minute but you also have

542
00:21:49,360 --> 00:21:52,000
to think about how much design work

543
00:21:52,000 --> 00:21:54,480
can the security team really do how many

544
00:21:54,480 --> 00:21:56,640
engagements do you have

545
00:21:56,640 --> 00:21:59,520
what's the time per engagement

546
00:21:59,520 --> 00:22:03,120
are we staffed to meet these goals

547
00:22:03,120 --> 00:22:04,960
and how do we balance this with other

548
00:22:04,960 --> 00:22:07,840
tasks i believe that threat modeling is

549
00:22:07,840 --> 00:22:10,640
the most important work i do

550
00:22:10,640 --> 00:22:12,960
and i say that as someone who helped

551
00:22:12,960 --> 00:22:15,120
create the cva and i'm on the review

552
00:22:15,120 --> 00:22:17,760
board for black hat and i think threat

553
00:22:17,760 --> 00:22:20,720
modeling is the work that

554
00:22:20,720 --> 00:22:24,159
generates all of the other

555
00:22:24,159 --> 00:22:27,200
investments i make in securing products

556
00:22:27,200 --> 00:22:30,320
systems and designs and so i believe

557
00:22:30,320 --> 00:22:32,159
that this works i believe that it can be

558
00:22:32,159 --> 00:22:36,400
taught and that you ought to be doing it

559
00:22:36,720 --> 00:22:38,320
then there's the interpersonal jenga

560
00:22:38,320 --> 00:22:41,280
building blocks as we respond to the be

561
00:22:41,280 --> 00:22:44,320
mad problem the bing bring me a diagram

562
00:22:44,320 --> 00:22:45,360
problem

563
00:22:45,360 --> 00:22:47,039
so are we going to have a design

564
00:22:47,039 --> 00:22:49,919
collaboration or a design review that's

565
00:22:49,919 --> 00:22:52,559
a choice that your organization can i

566
00:22:52,559 --> 00:22:55,280
can make is making

567
00:22:55,280 --> 00:22:57,360
and either you're making it

568
00:22:57,360 --> 00:23:01,439
consciously or accidentally

569
00:23:04,320 --> 00:23:06,679
communicating expectations both

570
00:23:06,679 --> 00:23:08,880
organizationally and as part of every

571
00:23:08,880 --> 00:23:10,960
meeting is really important saying

572
00:23:10,960 --> 00:23:12,799
things like this is going to be a

573
00:23:12,799 --> 00:23:14,320
collaborative meeting we're going to

574
00:23:14,320 --> 00:23:16,159
spend about an hour

575
00:23:16,159 --> 00:23:17,360
or maybe we're going to spend five

576
00:23:17,360 --> 00:23:20,480
minutes developing a system model

577
00:23:20,480 --> 00:23:22,400
we'll work through what can go wrong as

578
00:23:22,400 --> 00:23:23,760
it's natural but we're going to come

579
00:23:23,760 --> 00:23:26,159
back and do that later our goal here is

580
00:23:26,159 --> 00:23:28,480
to have a good system model

581
00:23:28,480 --> 00:23:30,320
or maybe you're going to say this is a

582
00:23:30,320 --> 00:23:32,640
design review meeting

583
00:23:32,640 --> 00:23:34,960
1 out of 10 of these have no stop issues

584
00:23:34,960 --> 00:23:37,520
or eight out of ten of them that number

585
00:23:37,520 --> 00:23:39,600
is really important for setting

586
00:23:39,600 --> 00:23:41,520
expectations and having people

587
00:23:41,520 --> 00:23:43,120
anticipate

588
00:23:43,120 --> 00:23:44,960
what's going to happen

589
00:23:44,960 --> 00:23:47,279
and so with that

590
00:23:47,279 --> 00:23:49,039
i want to say thank you for your time

591
00:23:49,039 --> 00:23:51,520
for your attention

592
00:23:51,520 --> 00:23:52,720
and

593
00:23:52,720 --> 00:23:54,880
i want to encourage you to avoid the

594
00:23:54,880 --> 00:23:58,000
work to avoid naively saying the words

595
00:23:58,000 --> 00:24:00,080
bring me a diagram it's going to lead to

596
00:24:00,080 --> 00:24:01,120
problems

597
00:24:01,120 --> 00:24:03,760
a couple of resources for you as i take

598
00:24:03,760 --> 00:24:06,320
questions a group of 15 experts in

599
00:24:06,320 --> 00:24:07,840
threat modeling

600
00:24:07,840 --> 00:24:10,240
recently released a manifesto for threat

601
00:24:10,240 --> 00:24:11,360
modeling

602
00:24:11,360 --> 00:24:14,240
the thinking in this talk really ties to

603
00:24:14,240 --> 00:24:16,559
that we thought about

604
00:24:16,559 --> 00:24:18,799
ways to reduce conflict to make threat

605
00:24:18,799 --> 00:24:21,200
modeling effective it's short it's easy

606
00:24:21,200 --> 00:24:22,799
to read it's creative commons at

607
00:24:22,799 --> 00:24:25,760
threatmodelingmanifesto.org

608
00:24:25,760 --> 00:24:29,679
and there's associates.showstack.org

609
00:24:29,679 --> 00:24:32,080
whitepapers where you can see the django

610
00:24:32,080 --> 00:24:33,279
white paper

611
00:24:33,279 --> 00:24:36,640
with that if you're here at black hat

612
00:24:36,640 --> 00:24:39,679
asia i am happy to take questions

613
00:24:39,679 --> 00:24:42,600
if you're not here's my email adam at

614
00:24:42,600 --> 00:24:45,520
showstack.org shoot me an email

615
00:24:45,520 --> 00:24:47,279
talk to me on linkedin i'm always happy

616
00:24:47,279 --> 00:24:48,799
to take questions and talk about threat

617
00:24:48,799 --> 00:24:52,760
modeling thank you

