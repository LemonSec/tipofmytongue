1
00:00:01,130 --> 00:00:14,690
[Music]

2
00:00:16,480 --> 00:00:18,640
okay thank you so much for joining our

3
00:00:18,640 --> 00:00:20,160
presentations today

4
00:00:20,160 --> 00:00:22,320
uh today we will be presenting hiding

5
00:00:22,320 --> 00:00:24,640
objects from computer vision by

6
00:00:24,640 --> 00:00:27,439
exploiting the correlation biases

7
00:00:27,439 --> 00:00:29,519
and in our group uh

8
00:00:29,519 --> 00:00:32,159
we have three researchers

9
00:00:32,159 --> 00:00:35,280
and paul and and masaki kamizuno and but

10
00:00:35,280 --> 00:00:38,000
today we're presenting uh me and paul

11
00:00:38,000 --> 00:00:40,399
will be presenting today

12
00:00:40,399 --> 00:00:41,680
so before

13
00:00:41,680 --> 00:00:44,640
it let me introduce myself i am jimmy

14
00:00:44,640 --> 00:00:46,960
papar i am originally from myanmar and

15
00:00:46,960 --> 00:00:48,719
currently working as a senior researcher

16
00:00:48,719 --> 00:00:51,360
and manager at satellite japan and my

17
00:00:51,360 --> 00:00:53,199
research interests include network

18
00:00:53,199 --> 00:00:56,239
security malware analysis id security

19
00:00:56,239 --> 00:00:58,640
web security and ai security

20
00:00:58,640 --> 00:01:01,280
i did several international research

21
00:01:01,280 --> 00:01:02,960
collaboration with universities in

22
00:01:02,960 --> 00:01:05,119
myanmar u.s and in netherlands and

23
00:01:05,119 --> 00:01:06,799
germany and you can find

24
00:01:06,799 --> 00:01:10,799
more about me on my website

25
00:01:10,799 --> 00:01:12,799
and let me

26
00:01:12,799 --> 00:01:15,439
pause to paul paul could you please

27
00:01:15,439 --> 00:01:17,680
uh introduce yourself

28
00:01:17,680 --> 00:01:18,720
of course

29
00:01:18,720 --> 00:01:21,680
so hi my name is paul ziegler um i'm the

30
00:01:21,680 --> 00:01:23,280
founder and ceo of a company called

31
00:01:23,280 --> 00:01:25,280
reflare and i'm originally from germany

32
00:01:25,280 --> 00:01:26,799
but i've been living in japan for the

33
00:01:26,799 --> 00:01:29,439
last 16 years and working here as well

34
00:01:29,439 --> 00:01:31,200
i'm a consultant and advisor on

35
00:01:31,200 --> 00:01:33,840
information security issues to large

36
00:01:33,840 --> 00:01:36,000
companies and other consultancies

37
00:01:36,000 --> 00:01:37,759
mostly with a focus on research and

38
00:01:37,759 --> 00:01:40,240
development i'm completely self-taught

39
00:01:40,240 --> 00:01:42,079
and that has given me a relatively

40
00:01:42,079 --> 00:01:44,079
shallow but in turn very wide

41
00:01:44,079 --> 00:01:46,320
understanding of computer security which

42
00:01:46,320 --> 00:01:48,640
in turn really helps me and my car and

43
00:01:48,640 --> 00:01:51,040
my customers do research

44
00:01:51,040 --> 00:01:52,880
back to you papa so

45
00:01:52,880 --> 00:01:55,040
so let me introduce

46
00:01:55,040 --> 00:01:58,320
our another member masaki kami zuno

47
00:01:58,320 --> 00:02:00,880
he is the cto and partner of that light

48
00:02:00,880 --> 00:02:03,680
japan and he engages in server research

49
00:02:03,680 --> 00:02:06,479
and new service development and he also

50
00:02:06,479 --> 00:02:09,038
manages a lot of large-scale projects

51
00:02:09,038 --> 00:02:11,200
and design socially supportive

52
00:02:11,200 --> 00:02:13,360
demonstrational experiments that utilize

53
00:02:13,360 --> 00:02:15,200
these research results and he

54
00:02:15,200 --> 00:02:17,200
contributes a lot to human resource

55
00:02:17,200 --> 00:02:19,360
development through the publications of

56
00:02:19,360 --> 00:02:23,080
papers and ledgers

57
00:02:23,440 --> 00:02:25,760
so before we get into the details of the

58
00:02:25,760 --> 00:02:28,400
studies let me give you the exceptive

59
00:02:28,400 --> 00:02:31,200
summaries of our result

60
00:02:31,200 --> 00:02:33,840
so in this study we found out that the

61
00:02:33,840 --> 00:02:36,879
computer vision systems have an implicit

62
00:02:36,879 --> 00:02:39,519
correlation bias that must be accounted

63
00:02:39,519 --> 00:02:42,319
for so if you take a look at the figure

64
00:02:42,319 --> 00:02:44,720
on the right side you will see that the

65
00:02:44,720 --> 00:02:46,879
stop sign in the figure is totally

66
00:02:46,879 --> 00:02:50,480
hidden by major cloud vision system

67
00:02:50,480 --> 00:02:54,319
and this happens uh because of the

68
00:02:54,319 --> 00:02:57,920
uh the correlation bias we call uh the

69
00:02:57,920 --> 00:02:59,120
stop sign

70
00:02:59,120 --> 00:03:01,040
and the food

71
00:03:01,040 --> 00:03:03,680
really exists together in nature

72
00:03:03,680 --> 00:03:04,560
so

73
00:03:04,560 --> 00:03:07,280
with this correlation bias feature

74
00:03:07,280 --> 00:03:11,519
existing in nature uh we we can easily

75
00:03:11,519 --> 00:03:14,640
hide the stop sign in the in the figure

76
00:03:14,640 --> 00:03:17,360
so it's it's easy it's com it's

77
00:03:17,360 --> 00:03:20,720
is possible to completely hide the stop

78
00:03:20,720 --> 00:03:21,599
sign

79
00:03:21,599 --> 00:03:25,440
uh by contextualizing it with other

80
00:03:25,440 --> 00:03:27,440
non-correlating objects like here in

81
00:03:27,440 --> 00:03:29,920
this figure the food so if the detection

82
00:03:29,920 --> 00:03:32,239
rate of the objects through the computer

83
00:03:32,239 --> 00:03:34,480
vision system is critical

84
00:03:34,480 --> 00:03:37,360
uh we need to train on specialized data

85
00:03:37,360 --> 00:03:41,599
set that equalize the training bias

86
00:03:41,599 --> 00:03:45,120
and and this can mitigate the risk of of

87
00:03:45,120 --> 00:03:48,159
this kind of attack

88
00:03:48,959 --> 00:03:50,720
so let me

89
00:03:50,720 --> 00:03:53,200
give some important notes so in

90
00:03:53,200 --> 00:03:55,439
throughout these presentations unless we

91
00:03:55,439 --> 00:03:58,080
stated otherwise all evaluations are

92
00:03:58,080 --> 00:04:00,319
performed against yellow version streets

93
00:04:00,319 --> 00:04:03,840
train on cuckoo 2017 data set

94
00:04:03,840 --> 00:04:06,720
and here in our presentation major cloud

95
00:04:06,720 --> 00:04:10,400
fusion ais refer to computer vision apis

96
00:04:10,400 --> 00:04:13,360
provided by google and microsoft

97
00:04:13,360 --> 00:04:16,399
and in order not to single out uh which

98
00:04:16,399 --> 00:04:18,000
one is better or

99
00:04:18,000 --> 00:04:20,880
good or we will only represent this

100
00:04:20,880 --> 00:04:23,840
category as a total detection percentage

101
00:04:23,840 --> 00:04:24,720
uh

102
00:04:24,720 --> 00:04:27,520
all evaluation against the club vision

103
00:04:27,520 --> 00:04:32,000
ais were performed on march 8 2021

104
00:04:32,000 --> 00:04:34,240
so it's likely that you know the

105
00:04:34,240 --> 00:04:36,240
features updates to these systems will

106
00:04:36,240 --> 00:04:38,240
lead to changes in in the detection

107
00:04:38,240 --> 00:04:40,960
behavior

108
00:04:41,120 --> 00:04:43,040
so let me give you a very much

109
00:04:43,040 --> 00:04:46,240
simplified recap of the computer vision

110
00:04:46,240 --> 00:04:49,360
technology the image classification

111
00:04:49,360 --> 00:04:52,000
is like it say like there is a stop sign

112
00:04:52,000 --> 00:04:56,320
in this picture and object detection say

113
00:04:56,320 --> 00:04:59,840
there is a stop sign at this location of

114
00:04:59,840 --> 00:05:01,840
this location in this picture

115
00:05:01,840 --> 00:05:04,240
and the object segmentation

116
00:05:04,240 --> 00:05:05,199
say

117
00:05:05,199 --> 00:05:07,440
this is the outline of the stop sign in

118
00:05:07,440 --> 00:05:10,560
the picture and the certainty say i am x

119
00:05:10,560 --> 00:05:11,600
percent

120
00:05:11,600 --> 00:05:13,919
certain it's just saying the certainty

121
00:05:13,919 --> 00:05:16,080
uh expression certain that there is a

122
00:05:16,080 --> 00:05:18,080
stop sign in this in this picture for

123
00:05:18,080 --> 00:05:20,720
example like a 80 percent sure this is a

124
00:05:20,720 --> 00:05:23,600
stop sign and the threshold means

125
00:05:23,600 --> 00:05:26,000
i will detect the stop sign in this

126
00:05:26,000 --> 00:05:27,039
picture

127
00:05:27,039 --> 00:05:30,080
if my certainty is over x percent or for

128
00:05:30,080 --> 00:05:33,758
example like 50 or 60

129
00:05:35,280 --> 00:05:36,639
so

130
00:05:36,639 --> 00:05:38,960
let me uh give you

131
00:05:38,960 --> 00:05:43,360
how we we find out the correlation bias

132
00:05:43,360 --> 00:05:46,400
uh features uh at the beginning of our

133
00:05:46,400 --> 00:05:48,000
study so

134
00:05:48,000 --> 00:05:49,840
in the beginning of our study we

135
00:05:49,840 --> 00:05:52,800
automatically generate thousands of

136
00:05:52,800 --> 00:05:55,440
composite images we just combine each

137
00:05:55,440 --> 00:05:58,479
other i from the public data set we we

138
00:05:58,479 --> 00:06:01,199
take out a lot of

139
00:06:01,199 --> 00:06:03,840
graphics and then we com we do a lot of

140
00:06:03,840 --> 00:06:06,720
composite images and then we find out

141
00:06:06,720 --> 00:06:09,120
that the certain classes of identity

142
00:06:09,120 --> 00:06:11,199
identification errors

143
00:06:11,199 --> 00:06:13,039
are exist

144
00:06:13,039 --> 00:06:16,080
and so let me uh and give you

145
00:06:16,080 --> 00:06:18,319
some examples of these identification

146
00:06:18,319 --> 00:06:21,199
errors we find out so the first one is

147
00:06:21,199 --> 00:06:22,960
like we found out that the

148
00:06:22,960 --> 00:06:24,880
misclassification

149
00:06:24,880 --> 00:06:28,080
uh happens in the same general category

150
00:06:28,080 --> 00:06:30,479
for example if you take a look at the

151
00:06:30,479 --> 00:06:32,560
figure you will see that the dogs and

152
00:06:32,560 --> 00:06:35,680
cats are seen as the combination of dog

153
00:06:35,680 --> 00:06:38,240
phase and catastrophe as horses

154
00:06:38,240 --> 00:06:41,680
and it's caused by by mixing or features

155
00:06:41,680 --> 00:06:44,240
used to identify each class

156
00:06:44,240 --> 00:06:47,039
so in most computer vision system

157
00:06:47,039 --> 00:06:49,599
detection is done through future

158
00:06:49,599 --> 00:06:51,360
clusters extracted through the

159
00:06:51,360 --> 00:06:54,639
convolutional layers that's why

160
00:06:54,639 --> 00:06:56,160
combining features from different

161
00:06:56,160 --> 00:06:57,280
classes

162
00:06:57,280 --> 00:06:58,960
can easily

163
00:06:58,960 --> 00:07:00,720
make this happen

164
00:07:00,720 --> 00:07:02,960
in the same general category

165
00:07:02,960 --> 00:07:05,280
so this is the the one you know

166
00:07:05,280 --> 00:07:08,319
identification error we found out in our

167
00:07:08,319 --> 00:07:10,240
in the beginning of our study and the

168
00:07:10,240 --> 00:07:13,840
second identification uh

169
00:07:13,840 --> 00:07:15,520
era we found out is like a

170
00:07:15,520 --> 00:07:17,280
misclassification

171
00:07:17,280 --> 00:07:19,440
happen as part of something else for

172
00:07:19,440 --> 00:07:21,280
example like um

173
00:07:21,280 --> 00:07:23,759
if you take a look at the figure here

174
00:07:23,759 --> 00:07:24,639
uh

175
00:07:24,639 --> 00:07:27,599
uh the plant off of the flower on a shot

176
00:07:27,599 --> 00:07:29,599
one by the person is

177
00:07:29,599 --> 00:07:32,400
is seen as it's it's it's it's totally

178
00:07:32,400 --> 00:07:35,360
hidden and it's only seen as a person so

179
00:07:35,360 --> 00:07:38,400
this is caused by and clear logical line

180
00:07:38,400 --> 00:07:40,479
between objects

181
00:07:40,479 --> 00:07:42,960
so i you know like a person standing in

182
00:07:42,960 --> 00:07:45,680
front of a house should clear clearly be

183
00:07:45,680 --> 00:07:48,080
detected as a person

184
00:07:48,080 --> 00:07:49,680
but in this case

185
00:07:49,680 --> 00:07:51,280
why not a plant

186
00:07:51,280 --> 00:07:54,160
on a shed you know it should be detected

187
00:07:54,160 --> 00:07:56,960
as well but in in this case it's not

188
00:07:56,960 --> 00:07:59,039
it's totally hidden

189
00:07:59,039 --> 00:08:00,319
so

190
00:08:00,319 --> 00:08:02,160
it's in the same way like a floral

191
00:08:02,160 --> 00:08:05,039
pattern on the round table uh it's the

192
00:08:05,039 --> 00:08:07,280
same way because uh because of the angle

193
00:08:07,280 --> 00:08:09,360
logical line between the objects so we

194
00:08:09,360 --> 00:08:11,280
we found out this kind of

195
00:08:11,280 --> 00:08:14,560
identification errors exist

196
00:08:14,560 --> 00:08:15,960
and the third

197
00:08:15,960 --> 00:08:18,560
identification error we found out is

198
00:08:18,560 --> 00:08:20,879
that

199
00:08:21,280 --> 00:08:25,520
the misclassification by correlation

200
00:08:25,520 --> 00:08:27,039
for example

201
00:08:27,039 --> 00:08:28,720
if you take a look at the figures you

202
00:08:28,720 --> 00:08:31,039
will see that the round object

203
00:08:31,039 --> 00:08:35,519
next to the dox is seen as a frisbee

204
00:08:35,519 --> 00:08:38,320
because it is caused by by the height

205
00:08:38,320 --> 00:08:41,599
correlation between uh the two classes

206
00:08:41,599 --> 00:08:44,080
in the training data set in this case

207
00:08:44,080 --> 00:08:46,000
you know and the training data said the

208
00:08:46,000 --> 00:08:49,040
doc and the free speech appear mostly

209
00:08:49,040 --> 00:08:51,120
together and the trainee data said

210
00:08:51,120 --> 00:08:52,959
that's why the computer vision system

211
00:08:52,959 --> 00:08:55,839
think of it as the any round shape

212
00:08:55,839 --> 00:08:58,880
near the dock will be the frisbee so in

213
00:08:58,880 --> 00:09:01,680
this case also you know this uh round

214
00:09:01,680 --> 00:09:04,480
shape near the dock face is seen as a

215
00:09:04,480 --> 00:09:06,959
frisbee

216
00:09:06,959 --> 00:09:08,560
so uh

217
00:09:08,560 --> 00:09:10,959
these features uh we mainly use this

218
00:09:10,959 --> 00:09:12,640
feature uh

219
00:09:12,640 --> 00:09:15,440
uh exist in nature

220
00:09:15,440 --> 00:09:17,600
you know i need to find out this because

221
00:09:17,600 --> 00:09:20,560
this kind of bias exists in nature and

222
00:09:20,560 --> 00:09:22,480
not just in the data set

223
00:09:22,480 --> 00:09:24,800
so docs and free speeds are indeed are

224
00:09:24,800 --> 00:09:27,680
often often synced together in nature so

225
00:09:27,680 --> 00:09:28,480
this

226
00:09:28,480 --> 00:09:31,920
main feature uh we use uh

227
00:09:31,920 --> 00:09:35,040
we call this a correlation bias and we

228
00:09:35,040 --> 00:09:37,839
use these features to generate a lot of

229
00:09:37,839 --> 00:09:41,040
several adversarial images and uh and

230
00:09:41,040 --> 00:09:43,440
you can take a look at some of the uh

231
00:09:43,440 --> 00:09:46,000
dog and frisbees are figure in the

232
00:09:46,000 --> 00:09:48,560
google data set so we have a lot of dark

233
00:09:48,560 --> 00:09:52,399
and frisbee always together

234
00:09:52,399 --> 00:09:54,880
so we can abuse we abuse this uh

235
00:09:54,880 --> 00:09:56,640
correlation

236
00:09:56,640 --> 00:09:59,440
existing almost always together like dog

237
00:09:59,440 --> 00:10:01,920
and you know this free speech

238
00:10:01,920 --> 00:10:04,399
this kind of correlation bias to have

239
00:10:04,399 --> 00:10:07,200
uh to generate a lot of adversarial

240
00:10:07,200 --> 00:10:08,880
images

241
00:10:08,880 --> 00:10:12,720
so paul could you please

242
00:10:12,720 --> 00:10:13,519
so

243
00:10:13,519 --> 00:10:16,720
let me take over from here

244
00:10:16,720 --> 00:10:18,800
now that we've found this

245
00:10:18,800 --> 00:10:19,920
correlation

246
00:10:19,920 --> 00:10:22,160
of objects and how they are

247
00:10:22,160 --> 00:10:23,920
well absorbed by machine vision systems

248
00:10:23,920 --> 00:10:26,320
by computer vision systems together um

249
00:10:26,320 --> 00:10:28,560
we can establish that certain classes

250
00:10:28,560 --> 00:10:29,920
corelight and we can establish that

251
00:10:29,920 --> 00:10:32,079
certain classes do not correlate and

252
00:10:32,079 --> 00:10:33,760
based on that we can automate the

253
00:10:33,760 --> 00:10:35,279
process

254
00:10:35,279 --> 00:10:36,399
of

255
00:10:36,399 --> 00:10:38,000
generating images that either have a

256
00:10:38,000 --> 00:10:39,680
very high correlation between objects or

257
00:10:39,680 --> 00:10:42,880
a very low correlation between objects

258
00:10:42,880 --> 00:10:43,920
so

259
00:10:43,920 --> 00:10:46,079
switching from dogs to stop signs

260
00:10:46,079 --> 00:10:48,160
because that's a much more interesting

261
00:10:48,160 --> 00:10:50,399
topic for machine vision

262
00:10:50,399 --> 00:10:52,720
you can see here that

263
00:10:52,720 --> 00:10:54,800
we simply calculated based on the cocoa

264
00:10:54,800 --> 00:10:57,839
data set what objects relate to stop

265
00:10:57,839 --> 00:10:59,120
science if you have the table on the

266
00:10:59,120 --> 00:11:01,360
left you can see that cars and people

267
00:11:01,360 --> 00:11:03,279
have a very high correlation traffic

268
00:11:03,279 --> 00:11:05,360
lights and trucks as well and then it

269
00:11:05,360 --> 00:11:07,760
very much falls off drastically um out

270
00:11:07,760 --> 00:11:08,959
of the 90

271
00:11:08,959 --> 00:11:12,560
or so cam categories available in the 20

272
00:11:12,560 --> 00:11:15,279
2017 cocoa dataset only about five

273
00:11:15,279 --> 00:11:17,680
percent only eleven have five percent or

274
00:11:17,680 --> 00:11:20,640
higher um correlation to stop science in

275
00:11:20,640 --> 00:11:21,680
general

276
00:11:21,680 --> 00:11:23,440
and what's important to note is that you

277
00:11:23,440 --> 00:11:25,600
can see that two classes have a

278
00:11:25,600 --> 00:11:26,959
correlation

279
00:11:26,959 --> 00:11:29,120
factor that exceeds 100

280
00:11:29,120 --> 00:11:32,320
namely cars and people exceed 100 and

281
00:11:32,320 --> 00:11:33,839
the reason for that is that we counted

282
00:11:33,839 --> 00:11:36,160
each instance of an object while making

283
00:11:36,160 --> 00:11:38,000
these calculations so let's say we have

284
00:11:38,000 --> 00:11:40,399
an image and it includes a stop sign and

285
00:11:40,399 --> 00:11:42,959
it also includes two or three cars then

286
00:11:42,959 --> 00:11:44,399
each of these cars is counted

287
00:11:44,399 --> 00:11:46,720
individually because well

288
00:11:46,720 --> 00:11:48,640
if one object is usually shown with

289
00:11:48,640 --> 00:11:50,399
clusters of other objects then that is

290
00:11:50,399 --> 00:11:52,480
important as well and that is why on

291
00:11:52,480 --> 00:11:54,320
average in every picture that has a stop

292
00:11:54,320 --> 00:11:56,959
sign in it you have 1.6 cars thus the

293
00:11:56,959 --> 00:12:00,638
correlation is 160

294
00:12:01,519 --> 00:12:02,240
so

295
00:12:02,240 --> 00:12:04,880
how do we generate our images first we

296
00:12:04,880 --> 00:12:06,399
take a random picture of the target

297
00:12:06,399 --> 00:12:08,000
class in this case it's a stop sign and

298
00:12:08,000 --> 00:12:10,240
we make sure it's detected so we run a

299
00:12:10,240 --> 00:12:12,399
machine vision computer vision system

300
00:12:12,399 --> 00:12:14,399
and we see what the detection rate is in

301
00:12:14,399 --> 00:12:16,160
this case the stop sign is detected with

302
00:12:16,160 --> 00:12:19,120
a 98 accuracy now we do this step

303
00:12:19,120 --> 00:12:20,880
because we want to avoid

304
00:12:20,880 --> 00:12:23,200
using images where the detection rate is

305
00:12:23,200 --> 00:12:24,800
bad at the beginning because there's no

306
00:12:24,800 --> 00:12:27,360
use in testing any of that so we have an

307
00:12:27,360 --> 00:12:30,079
image and it's seen by the ai

308
00:12:30,079 --> 00:12:32,480
and then we cut that out um we use the

309
00:12:32,480 --> 00:12:34,639
segmentation mapping in the in the data

310
00:12:34,639 --> 00:12:37,120
set and we cut out the object and we

311
00:12:37,120 --> 00:12:39,360
post it onto a background full of low

312
00:12:39,360 --> 00:12:41,920
correlation items so in this case it's a

313
00:12:41,920 --> 00:12:44,240
kitchen it's a kitchen with a bunch of

314
00:12:44,240 --> 00:12:46,160
bottles and jars and those are all items

315
00:12:46,160 --> 00:12:48,560
that do not correlate to stop signs and

316
00:12:48,560 --> 00:12:50,800
we measure the detection certainty again

317
00:12:50,800 --> 00:12:52,959
in this case you can see it's down to 72

318
00:12:52,959 --> 00:12:55,279
percent

319
00:12:55,279 --> 00:12:58,320
and then well we get into this loop

320
00:12:58,320 --> 00:12:59,279
um

321
00:12:59,279 --> 00:13:00,880
it just goes on and on like this we take

322
00:13:00,880 --> 00:13:02,880
a picture from the public data set we

323
00:13:02,880 --> 00:13:04,639
select something in this case a stop

324
00:13:04,639 --> 00:13:06,560
sign any image with a stop sign in it by

325
00:13:06,560 --> 00:13:09,600
at random and then we test

326
00:13:09,600 --> 00:13:11,680
um if it's well detected and in our

327
00:13:11,680 --> 00:13:14,160
final evaluations we used resonant 50

328
00:13:14,160 --> 00:13:15,200
yolo

329
00:13:15,200 --> 00:13:18,160
version 3 and tiny yolo and only if all

330
00:13:18,160 --> 00:13:20,240
of them had a detection rate exceeding

331
00:13:20,240 --> 00:13:22,639
95 percent a certainty would exceed 95

332
00:13:22,639 --> 00:13:25,600
percent we proceeded cut it out and then

333
00:13:25,600 --> 00:13:27,279
we select another image with a

334
00:13:27,279 --> 00:13:30,240
correlation of under five percent and

335
00:13:30,240 --> 00:13:31,839
paste them together creating a new

336
00:13:31,839 --> 00:13:34,079
composite and then we run the detection

337
00:13:34,079 --> 00:13:38,240
again and only if all three networks now

338
00:13:38,240 --> 00:13:40,079
have a certainty of under five percent

339
00:13:40,079 --> 00:13:42,240
do we proceed and do we save

340
00:13:42,240 --> 00:13:46,320
the image as an adversarial image

341
00:13:46,480 --> 00:13:48,639
and i'll give you a short demonstration

342
00:13:48,639 --> 00:13:51,760
um of the tool in action uh soon but you

343
00:13:51,760 --> 00:13:53,519
can always access the source code from

344
00:13:53,519 --> 00:13:54,880
this url

345
00:13:54,880 --> 00:13:56,800
we've put it on github and it's mit

346
00:13:56,800 --> 00:13:58,560
licensed so feel free to do your own

347
00:13:58,560 --> 00:14:00,959
testing just be aware as

348
00:14:00,959 --> 00:14:04,160
papa-san stated in the beginning that

349
00:14:04,160 --> 00:14:05,920
all of our tests against commercially

350
00:14:05,920 --> 00:14:08,000
available ais were done in march and

351
00:14:08,000 --> 00:14:10,399
updates are almost certain to improve

352
00:14:10,399 --> 00:14:13,600
detection rates over time

353
00:14:14,720 --> 00:14:18,560
so um here's a quick video demonstration

354
00:14:18,560 --> 00:14:20,480
of what's happening

355
00:14:20,480 --> 00:14:22,880
um it's just a it's just a python script

356
00:14:22,880 --> 00:14:24,720
it's a python loop um you can see that

357
00:14:24,720 --> 00:14:27,440
i've passed the category class 13 which

358
00:14:27,440 --> 00:14:29,519
has correctly been identified as

359
00:14:29,519 --> 00:14:31,839
belonging to stop signs we're loading in

360
00:14:31,839 --> 00:14:34,000
the training data from json there's a

361
00:14:34,000 --> 00:14:36,399
bunch of of output from from cuda and

362
00:14:36,399 --> 00:14:39,040
tensorflow as they initialize give it a

363
00:14:39,040 --> 00:14:40,880
second

364
00:14:40,880 --> 00:14:43,519
and once that's done it builds all the

365
00:14:43,519 --> 00:14:45,519
networks it initializes all the networks

366
00:14:45,519 --> 00:14:47,680
for the three computer vision algorithms

367
00:14:47,680 --> 00:14:49,360
that they're using

368
00:14:49,360 --> 00:14:51,680
and then it runs the loop that i've just

369
00:14:51,680 --> 00:14:53,680
described to you it goes in it takes a

370
00:14:53,680 --> 00:14:54,639
picture

371
00:14:54,639 --> 00:14:57,279
it identifies the initial minimum

372
00:14:57,279 --> 00:14:59,839
certainty it generates a composite it

373
00:14:59,839 --> 00:15:01,760
runs detection again it checks for the

374
00:15:01,760 --> 00:15:04,000
minimum detection and so on and so on

375
00:15:04,000 --> 00:15:06,320
it's simply running as a loop

376
00:15:06,320 --> 00:15:08,560
and again you can run this at any time

377
00:15:08,560 --> 00:15:10,880
we ran most of our tests on the 2080 ti

378
00:15:10,880 --> 00:15:14,240
from nvidia it should work flawlessly on

379
00:15:14,240 --> 00:15:19,040
any card with a comparable spec sheet

380
00:15:21,279 --> 00:15:24,320
so what are some examples well

381
00:15:24,320 --> 00:15:26,000
first let me show you some examples that

382
00:15:26,000 --> 00:15:28,959
are created by only using yolo 3

383
00:15:28,959 --> 00:15:31,279
as both a first step and last step

384
00:15:31,279 --> 00:15:33,440
identifier so yolo 3 is used to

385
00:15:33,440 --> 00:15:35,600
determine if the stop sign is detected

386
00:15:35,600 --> 00:15:37,600
in the original image and it's also used

387
00:15:37,600 --> 00:15:40,000
to determine if it's not detectable in

388
00:15:40,000 --> 00:15:42,399
the second image with one single network

389
00:15:42,399 --> 00:15:44,959
you can get some very impressive results

390
00:15:44,959 --> 00:15:47,279
for example this stop sign here is

391
00:15:47,279 --> 00:15:50,399
hidden this is the case of overlap

392
00:15:50,399 --> 00:15:52,959
the car and the stop sign have no real

393
00:15:52,959 --> 00:15:54,959
logical boundary the stop sign is too

394
00:15:54,959 --> 00:15:57,519
large in relation to the car so the

395
00:15:57,519 --> 00:16:00,160
machine vision system simply treats it

396
00:16:00,160 --> 00:16:02,160
as part of the car the stop sign itself

397
00:16:02,160 --> 00:16:04,399
disappears

398
00:16:04,399 --> 00:16:06,160
as a second example

399
00:16:06,160 --> 00:16:08,399
this stop sign is also hidden and this

400
00:16:08,399 --> 00:16:10,240
is a very common correlation that we

401
00:16:10,240 --> 00:16:13,279
found or rather lack of correlation

402
00:16:13,279 --> 00:16:16,399
fruit all sorts of food pizza

403
00:16:16,399 --> 00:16:18,160
dishes they have virtually no

404
00:16:18,160 --> 00:16:20,399
correlation to stop signs because well

405
00:16:20,399 --> 00:16:22,399
when do you ever see them together

406
00:16:22,399 --> 00:16:25,040
and so if you put a stop sign over a

407
00:16:25,040 --> 00:16:26,959
background of fruit or of a background

408
00:16:26,959 --> 00:16:29,360
of similar items it is very likely to

409
00:16:29,360 --> 00:16:33,040
disappear plus fruit has a very high

410
00:16:33,040 --> 00:16:35,759
entropy in its visual features so

411
00:16:35,759 --> 00:16:37,360
there's lots of different fruits a lot

412
00:16:37,360 --> 00:16:38,399
of different

413
00:16:38,399 --> 00:16:39,600
shapes

414
00:16:39,600 --> 00:16:41,600
so it's very easy to hide an object like

415
00:16:41,600 --> 00:16:43,759
a stop sign among them

416
00:16:43,759 --> 00:16:46,639
similarly sports sporting events people

417
00:16:46,639 --> 00:16:49,680
playing sports people themselves are

418
00:16:49,680 --> 00:16:51,680
very high correlation but things like

419
00:16:51,680 --> 00:16:54,240
baseball gloves baseball bats chairs

420
00:16:54,240 --> 00:16:55,759
they're not so

421
00:16:55,759 --> 00:16:57,279
putting a stop sign

422
00:16:57,279 --> 00:17:00,639
over a scene of sports will also hide it

423
00:17:00,639 --> 00:17:03,839
however this is only for a single

424
00:17:03,839 --> 00:17:05,599
network um

425
00:17:05,599 --> 00:17:07,599
what we do then what we did then is we

426
00:17:07,599 --> 00:17:09,679
added more networks and instead of just

427
00:17:09,679 --> 00:17:12,559
using yolo 3 we also added in

428
00:17:12,559 --> 00:17:14,160
resnet we added in

429
00:17:14,160 --> 00:17:15,760
tiny yolo

430
00:17:15,760 --> 00:17:16,480
and

431
00:17:16,480 --> 00:17:18,400
with all of them as classifiers the

432
00:17:18,400 --> 00:17:21,039
images became a little less flamboyant

433
00:17:21,039 --> 00:17:23,599
but they became much more reliable so in

434
00:17:23,599 --> 00:17:26,480
this case this is an example of an image

435
00:17:26,480 --> 00:17:28,559
that

436
00:17:28,559 --> 00:17:31,039
was generated against all three of them

437
00:17:31,039 --> 00:17:33,440
and what happened before is all these

438
00:17:33,440 --> 00:17:35,280
images that were only generated using

439
00:17:35,280 --> 00:17:38,720
yolo 3 had about 50 percent the

440
00:17:38,720 --> 00:17:40,480
chance of being detected by the other

441
00:17:40,480 --> 00:17:43,760
two networks so um yes they were fooled

442
00:17:43,760 --> 00:17:46,080
by about well in about half the cases

443
00:17:46,080 --> 00:17:48,480
but by increasing the number of networks

444
00:17:48,480 --> 00:17:51,440
we hoped to push the the quality of our

445
00:17:51,440 --> 00:17:53,919
adversarial images even higher

446
00:17:53,919 --> 00:17:55,760
and well ultimately we want to generate

447
00:17:55,760 --> 00:17:58,080
something that's very universal

448
00:17:58,080 --> 00:18:00,160
so as you can see the stock uh with a

449
00:18:00,160 --> 00:18:02,000
stop sign the stop sign is hidden and

450
00:18:02,000 --> 00:18:03,919
it's again by correlation by lack of

451
00:18:03,919 --> 00:18:05,760
correlation to the dog by a lack of

452
00:18:05,760 --> 00:18:08,879
correlation to the boat

453
00:18:09,200 --> 00:18:12,080
papasan back to you

454
00:18:12,080 --> 00:18:12,880
so

455
00:18:12,880 --> 00:18:15,679
we generate uh several adversarial

456
00:18:15,679 --> 00:18:17,840
images uh

457
00:18:17,840 --> 00:18:19,760
using the way that paul

458
00:18:19,760 --> 00:18:20,880
explained

459
00:18:20,880 --> 00:18:22,559
and so we

460
00:18:22,559 --> 00:18:24,880
we generate images and then we randomly

461
00:18:24,880 --> 00:18:27,200
choose the 1000 adversarial

462
00:18:27,200 --> 00:18:30,160
sample images and then we test these

463
00:18:30,160 --> 00:18:34,640
images against the major cloud vision ai

464
00:18:34,720 --> 00:18:36,320
so that you know we know about the

465
00:18:36,320 --> 00:18:39,280
detection rates

466
00:18:39,520 --> 00:18:40,840
so

467
00:18:40,840 --> 00:18:44,960
here is our result so we firstly we

468
00:18:44,960 --> 00:18:46,960
choose the detection rate with certainty

469
00:18:46,960 --> 00:18:50,559
above 50 percent uh across these 1000

470
00:18:50,559 --> 00:18:53,520
random adversarial images and here is

471
00:18:53,520 --> 00:18:56,880
the result of vendor a and b so the in

472
00:18:56,880 --> 00:19:00,640
the in case of uh cloud vision ai vendor

473
00:19:00,640 --> 00:19:04,400
a the detect out of the 1000 adversarial

474
00:19:04,400 --> 00:19:09,039
images we tested only 102 images uh

475
00:19:09,039 --> 00:19:11,760
above the 50 percent uh certainty is

476
00:19:11,760 --> 00:19:12,960
detected

477
00:19:12,960 --> 00:19:15,919
and uh in the same same way vendor b out

478
00:19:15,919 --> 00:19:18,480
of 1000 images so 3

479
00:19:18,480 --> 00:19:22,799
68 our images are detected so we can say

480
00:19:22,799 --> 00:19:26,080
like a detection percentage of of one

481
00:19:26,080 --> 00:19:29,600
10.2 percent and 6.8 for each of the

482
00:19:29,600 --> 00:19:34,160
vendor and here the average certainty of

483
00:19:34,160 --> 00:19:36,799
the detection is for the wenger is is 74

484
00:19:36,799 --> 00:19:39,520
percent and for the vanguard is is 57

485
00:19:39,520 --> 00:19:41,280
percent and if you take a look at the

486
00:19:41,280 --> 00:19:43,039
figure on the right under sorry on the

487
00:19:43,039 --> 00:19:45,280
left side you will see that

488
00:19:45,280 --> 00:19:47,280
uh this figure

489
00:19:47,280 --> 00:19:50,840
is totally hidden in both vendor a and

490
00:19:50,840 --> 00:19:54,240
b so while uh there were gaps in both

491
00:19:54,240 --> 00:19:56,480
the certainty and detection rate between

492
00:19:56,480 --> 00:19:59,600
the better vendors our both systems were

493
00:19:59,600 --> 00:20:02,480
fooled uh by by the majority of the

494
00:20:02,480 --> 00:20:07,159
adversary images we generated

495
00:20:07,360 --> 00:20:08,320
so

496
00:20:08,320 --> 00:20:09,679
we

497
00:20:09,679 --> 00:20:12,000
uh here i will give you an impact of

498
00:20:12,000 --> 00:20:14,880
adding more networks while we generate

499
00:20:14,880 --> 00:20:18,159
the adversarial images and we

500
00:20:18,159 --> 00:20:21,440
we tested it against the vendor e and so

501
00:20:21,440 --> 00:20:24,880
the figure is saying is that uh one

502
00:20:24,880 --> 00:20:26,799
wherever we put

503
00:20:26,799 --> 00:20:29,679
more network while we generate uh the

504
00:20:29,679 --> 00:20:32,240
adversarial images the deduction

505
00:20:32,240 --> 00:20:35,679
rate is getting lower

506
00:20:35,679 --> 00:20:37,360
so this is

507
00:20:37,360 --> 00:20:39,679
the good result of

508
00:20:39,679 --> 00:20:42,799
of this theory

509
00:20:43,679 --> 00:20:46,480
and we okay we also tested against the

510
00:20:46,480 --> 00:20:48,320
physical objects and let me pass it to

511
00:20:48,320 --> 00:20:49,679
paul

512
00:20:49,679 --> 00:20:52,720
yes of course um having this in a

513
00:20:52,720 --> 00:20:54,640
digital realm is all nice and good but

514
00:20:54,640 --> 00:20:56,240
it would be really interesting if we

515
00:20:56,240 --> 00:20:58,400
could well pull it off in the real world

516
00:20:58,400 --> 00:21:00,480
after all all we're doing is compositing

517
00:21:00,480 --> 00:21:02,559
we don't have to obscure anything we

518
00:21:02,559 --> 00:21:04,240
simply place a background behind an

519
00:21:04,240 --> 00:21:06,799
existing object and hiding it

520
00:21:06,799 --> 00:21:08,960
through that contextualization

521
00:21:08,960 --> 00:21:09,760
so

522
00:21:09,760 --> 00:21:11,600
we now were at a point where we wondered

523
00:21:11,600 --> 00:21:13,919
can we do that with real life objects

524
00:21:13,919 --> 00:21:16,400
can we do that with physical objects in

525
00:21:16,400 --> 00:21:18,320
theory we should be

526
00:21:18,320 --> 00:21:21,679
but you never know until you try so um

527
00:21:21,679 --> 00:21:22,720
we

528
00:21:22,720 --> 00:21:24,960
made a small stop sign out of out of

529
00:21:24,960 --> 00:21:28,000
cardboard we got a real stop sign as

530
00:21:28,000 --> 00:21:30,720
well but turns out it was too big to

531
00:21:30,720 --> 00:21:33,440
pragmatically print backgrounds for it

532
00:21:33,440 --> 00:21:35,039
so we got a small stop sign we took a

533
00:21:35,039 --> 00:21:36,400
picture

534
00:21:36,400 --> 00:21:39,200
and um we used that picture as input and

535
00:21:39,200 --> 00:21:41,280
as you can see on the left um it had a

536
00:21:41,280 --> 00:21:45,520
95 a certainty exceeding 95 across all

537
00:21:45,520 --> 00:21:47,600
of the networks that we're using

538
00:21:47,600 --> 00:21:50,240
and then on the right we took a

539
00:21:50,240 --> 00:21:52,480
background that we knew worked in the

540
00:21:52,480 --> 00:21:54,960
digital realm um to hide stop signs and

541
00:21:54,960 --> 00:21:56,640
we put the physical stop sign on top of

542
00:21:56,640 --> 00:21:59,280
it and it was undetected by four out of

543
00:21:59,280 --> 00:22:02,400
five networks including one of the cloud

544
00:22:02,400 --> 00:22:03,360
vision

545
00:22:03,360 --> 00:22:04,880
systems

546
00:22:04,880 --> 00:22:06,400
so this has some interesting

547
00:22:06,400 --> 00:22:08,799
implications for many things of course

548
00:22:08,799 --> 00:22:10,559
self-driving cars are what immediately

549
00:22:10,559 --> 00:22:12,000
comes to mind

550
00:22:12,000 --> 00:22:14,000
naturally those rely on lighter

551
00:22:14,000 --> 00:22:17,120
technology they are naturally not only

552
00:22:17,120 --> 00:22:19,280
reliant on machine vision

553
00:22:19,280 --> 00:22:20,159
but

554
00:22:20,159 --> 00:22:22,640
there's many implications here you can

555
00:22:22,640 --> 00:22:25,360
reasonably expect objects to be hidden

556
00:22:25,360 --> 00:22:28,080
by simply contextualization

557
00:22:28,080 --> 00:22:30,640
through the background

558
00:22:30,640 --> 00:22:31,679
so

559
00:22:31,679 --> 00:22:34,240
what can we do to prevent this

560
00:22:34,240 --> 00:22:36,159
we had a couple of ideas of how to

561
00:22:36,159 --> 00:22:38,720
mitigate this problem and luckily one of

562
00:22:38,720 --> 00:22:41,039
them worked really well

563
00:22:41,039 --> 00:22:42,320
so

564
00:22:42,320 --> 00:22:44,480
here's what we did we created a sample

565
00:22:44,480 --> 00:22:46,840
adversarial data set with

566
00:22:46,840 --> 00:22:48,880
1492 samples

567
00:22:48,880 --> 00:22:50,880
and then we tested that data set against

568
00:22:50,880 --> 00:22:52,960
the standard yolo version 3 coco network

569
00:22:52,960 --> 00:22:56,159
with a detection rate of 0.00 which of

570
00:22:56,159 --> 00:22:58,640
course it should be we used that network

571
00:22:58,640 --> 00:23:00,400
to create the adversarial images but

572
00:23:00,400 --> 00:23:02,640
it's always good to be sure

573
00:23:02,640 --> 00:23:05,520
and then we took 200 images

574
00:23:05,520 --> 00:23:07,520
and added those to the training data and

575
00:23:07,520 --> 00:23:10,080
we took 70 images and we added those to

576
00:23:10,080 --> 00:23:10,880
the

577
00:23:10,880 --> 00:23:13,360
valuation data of the cocoa data set and

578
00:23:13,360 --> 00:23:16,080
the remaining 1222 samples remained as

579
00:23:16,080 --> 00:23:17,280
the test group

580
00:23:17,280 --> 00:23:18,799
and then we did transfer learning so we

581
00:23:18,799 --> 00:23:21,120
took an existing

582
00:23:21,120 --> 00:23:22,480
yolo 3 network

583
00:23:22,480 --> 00:23:24,559
trained on coco and we transfer trained

584
00:23:24,559 --> 00:23:27,200
it on the new data set for two hours now

585
00:23:27,200 --> 00:23:29,120
we kept that time short on purpose

586
00:23:29,120 --> 00:23:30,720
because we wanted it to be well a

587
00:23:30,720 --> 00:23:33,200
reasonable approach for mitigation

588
00:23:33,200 --> 00:23:35,120
and then once the training was complete

589
00:23:35,120 --> 00:23:36,799
we once again

590
00:23:36,799 --> 00:23:38,880
set the network to identifying objects

591
00:23:38,880 --> 00:23:42,320
in the remaining 1222 test images to see

592
00:23:42,320 --> 00:23:45,600
how the detection rate would change

593
00:23:45,600 --> 00:23:47,520
and these are the results on the left

594
00:23:47,520 --> 00:23:48,559
you can see

595
00:23:48,559 --> 00:23:51,520
the regular yolo 3

596
00:23:51,520 --> 00:23:53,520
network trained on coco and you can see

597
00:23:53,520 --> 00:23:55,919
that we had a 100 failure rate and then

598
00:23:55,919 --> 00:23:57,360
after two hours of transfer training

599
00:23:57,360 --> 00:23:59,120
with 200 samples

600
00:23:59,120 --> 00:24:02,320
we are now up to a 98

601
00:24:02,320 --> 00:24:03,120
ish

602
00:24:03,120 --> 00:24:04,480
detection rate

603
00:24:04,480 --> 00:24:06,480
and we achieved this by simply adding

604
00:24:06,480 --> 00:24:08,400
200 images which is

605
00:24:08,400 --> 00:24:10,960
good because the coco 2017 data set has

606
00:24:10,960 --> 00:24:14,480
in excess of 118 000 samples so we only

607
00:24:14,480 --> 00:24:17,919
had to have a mix-in of 0.16 percent

608
00:24:17,919 --> 00:24:20,559
to completely eliminate the problem now

609
00:24:20,559 --> 00:24:23,600
we ran some other um

610
00:24:23,600 --> 00:24:25,760
tests as well we were worried that this

611
00:24:25,760 --> 00:24:28,080
might decrease the uh the accuracy of

612
00:24:28,080 --> 00:24:30,799
the network in general um so we ran a

613
00:24:30,799 --> 00:24:33,120
standard test to the verm to determine

614
00:24:33,120 --> 00:24:35,919
the mean average precision or map

615
00:24:35,919 --> 00:24:38,080
um of the new network um and the

616
00:24:38,080 --> 00:24:41,360
original map was 57.8 the retrained map

617
00:24:41,360 --> 00:24:44,559
was down to 57.7 um however the stand

618
00:24:44,559 --> 00:24:47,200
they considered range that was normal at

619
00:24:47,200 --> 00:24:49,520
the time of testing for a map for for a

620
00:24:49,520 --> 00:24:52,080
yolo 3 cocoa network was between 55 and

621
00:24:52,080 --> 00:24:54,799
58 so while there is a minuscule

622
00:24:54,799 --> 00:24:58,000
decrease it is very much in the realm of

623
00:24:58,000 --> 00:25:01,200
normalcy um however

624
00:25:01,200 --> 00:25:03,200
it is important to note that we are only

625
00:25:03,200 --> 00:25:05,600
testing this against a single class if

626
00:25:05,600 --> 00:25:07,520
you have a network that deals with

627
00:25:07,520 --> 00:25:10,159
hundreds or thousands of classes um it

628
00:25:10,159 --> 00:25:12,080
might have a bigger impact if you try to

629
00:25:12,080 --> 00:25:14,559
introduce adversarial

630
00:25:14,559 --> 00:25:18,080
images for each of the classes

631
00:25:18,320 --> 00:25:19,600
so

632
00:25:19,600 --> 00:25:21,360
in this session i will give you some of

633
00:25:21,360 --> 00:25:23,200
the benefits of the

634
00:25:23,200 --> 00:25:26,640
correlation based attack

635
00:25:26,960 --> 00:25:30,400
so the very first uh point uh our study

636
00:25:30,400 --> 00:25:32,960
is different from others is like ours is

637
00:25:32,960 --> 00:25:36,080
not the data set specifics or network

638
00:25:36,080 --> 00:25:37,360
specifics

639
00:25:37,360 --> 00:25:38,080
so

640
00:25:38,080 --> 00:25:40,240
it means the adversary generation

641
00:25:40,240 --> 00:25:42,559
process doesn't have to be repeated for

642
00:25:42,559 --> 00:25:45,279
each new target networks or data set

643
00:25:45,279 --> 00:25:48,000
and it should work well across new

644
00:25:48,000 --> 00:25:49,919
version of the data sets

645
00:25:49,919 --> 00:25:53,200
and the second point uh is is it works

646
00:25:53,200 --> 00:25:56,640
on on the physical objects too

647
00:25:56,640 --> 00:25:57,600
so we

648
00:25:57,600 --> 00:26:01,200
deploying several uh adversarial images

649
00:26:01,200 --> 00:26:02,799
as backgrounds

650
00:26:02,799 --> 00:26:04,960
pasted behind the physical objects i

651
00:26:04,960 --> 00:26:06,799
think paul already showed the

652
00:26:06,799 --> 00:26:08,559
physical objects of the stop sign also

653
00:26:08,559 --> 00:26:09,520
allow

654
00:26:09,520 --> 00:26:11,520
for a test against physical

655
00:26:11,520 --> 00:26:13,760
infrastructure

656
00:26:13,760 --> 00:26:15,360
and the third benefit

657
00:26:15,360 --> 00:26:16,320
uh

658
00:26:16,320 --> 00:26:19,200
it's it's is that uh it can be

659
00:26:19,200 --> 00:26:21,919
deployed against unknown targets so even

660
00:26:21,919 --> 00:26:23,840
the black box space so since the

661
00:26:23,840 --> 00:26:26,559
correlation bias exists in in the real

662
00:26:26,559 --> 00:26:27,679
wall

663
00:26:27,679 --> 00:26:30,720
they are relatively universal across

664
00:26:30,720 --> 00:26:33,039
several different data sets

665
00:26:33,039 --> 00:26:35,600
this allows for attack against and on

666
00:26:35,600 --> 00:26:37,600
computer vision system

667
00:26:37,600 --> 00:26:40,480
and the other point is uh the target

668
00:26:40,480 --> 00:26:42,960
object doesn't have to be obscured or

669
00:26:42,960 --> 00:26:45,440
changed in a study

670
00:26:45,440 --> 00:26:46,799
yeah so

671
00:26:46,799 --> 00:26:49,600
in in in a case we we we don't change

672
00:26:49,600 --> 00:26:52,320
anything we don't uh anything on stop

673
00:26:52,320 --> 00:26:54,400
sign you know we don't pace or put

674
00:26:54,400 --> 00:26:56,240
anything on the stop sign

675
00:26:56,240 --> 00:26:59,120
and so in other approaches this often

676
00:26:59,120 --> 00:27:01,600
requires you know at least a part of the

677
00:27:01,600 --> 00:27:04,240
target objects for example stop sign is

678
00:27:04,240 --> 00:27:05,840
need to be changed or covered by

679
00:27:05,840 --> 00:27:07,360
something else

680
00:27:07,360 --> 00:27:08,559
uh so

681
00:27:08,559 --> 00:27:10,640
it walks with an obscure or change in

682
00:27:10,640 --> 00:27:13,679
our in our case

683
00:27:13,840 --> 00:27:16,799
okay so that's all of our presentations

684
00:27:16,799 --> 00:27:18,799
and all the questions and

685
00:27:18,799 --> 00:27:20,960
comments are are welcome

686
00:27:20,960 --> 00:27:24,200
thank you

