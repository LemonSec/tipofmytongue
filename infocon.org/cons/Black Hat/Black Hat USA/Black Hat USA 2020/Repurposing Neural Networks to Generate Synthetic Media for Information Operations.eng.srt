1
00:00:02,840 --> 00:00:13,279
[Music]

2
00:00:14,960 --> 00:00:15,519
hello

3
00:00:15,519 --> 00:00:17,039
and welcome to our virtual black hat

4
00:00:17,039 --> 00:00:18,560
presentation entitled

5
00:00:18,560 --> 00:00:20,480
repurposing neural networks to generate

6
00:00:20,480 --> 00:00:21,920
synthetic media for information

7
00:00:21,920 --> 00:00:22,960
operations

8
00:00:22,960 --> 00:00:24,720
my name is phil tully and i'll be joined

9
00:00:24,720 --> 00:00:26,400
later in this slide deck by my colleague

10
00:00:26,400 --> 00:00:27,760
lee foster

11
00:00:27,760 --> 00:00:29,599
we work together at fireeye where i'm a

12
00:00:29,599 --> 00:00:31,679
staff data scientist on fire ice data

13
00:00:31,679 --> 00:00:32,800
science team

14
00:00:32,800 --> 00:00:34,320
and lee is the senior manager of

15
00:00:34,320 --> 00:00:36,000
information operations in fireeye's

16
00:00:36,000 --> 00:00:38,879
threat intelligence group

17
00:00:38,879 --> 00:00:40,480
for a bit of background about myself

18
00:00:40,480 --> 00:00:42,320
i've been at fireeye for two years and

19
00:00:42,320 --> 00:00:43,680
i've been working at the intersection

20
00:00:43,680 --> 00:00:45,520
of cyber security and machine learning

21
00:00:45,520 --> 00:00:47,120
for over five years

22
00:00:47,120 --> 00:00:49,360
in a previous academic life i earned my

23
00:00:49,360 --> 00:00:51,360
doctorate in computational neuroscience

24
00:00:51,360 --> 00:00:52,960
and nowadays i perform research and

25
00:00:52,960 --> 00:00:54,719
development on fire's malware and intel

26
00:00:54,719 --> 00:00:57,280
product offerings

27
00:00:57,280 --> 00:00:58,800
lee and his team of intel analysts

28
00:00:58,800 --> 00:01:00,719
specialize in identifying cyber driven

29
00:01:00,719 --> 00:01:03,440
nation state influence campaigns online

30
00:01:03,440 --> 00:01:05,199
he holds a master's degree in political

31
00:01:05,199 --> 00:01:06,880
science a second in

32
00:01:06,880 --> 00:01:09,200
intelligence and international security

33
00:01:09,200 --> 00:01:10,640
and is currently pursuing a third

34
00:01:10,640 --> 00:01:13,200
masters in analytics

35
00:01:13,200 --> 00:01:15,200
so some quick bookkeeping to start out

36
00:01:15,200 --> 00:01:16,240
at first

37
00:01:16,240 --> 00:01:18,000
in this presentation we're going to try

38
00:01:18,000 --> 00:01:19,759
to convince you that open source

39
00:01:19,759 --> 00:01:21,680
pre-trained machine learning models can

40
00:01:21,680 --> 00:01:23,040
be leveraged to generate custom

41
00:01:23,040 --> 00:01:25,840
synthetic media for malicious purposes

42
00:01:25,840 --> 00:01:27,520
after introducing some core background

43
00:01:27,520 --> 00:01:29,439
material i'll dive into three successive

44
00:01:29,439 --> 00:01:30,880
proof of concepts for how this can be

45
00:01:30,880 --> 00:01:32,000
accomplished today

46
00:01:32,000 --> 00:01:34,960
in the text image and audio domains i'm

47
00:01:34,960 --> 00:01:36,400
going to channel the three wise monkeys

48
00:01:36,400 --> 00:01:37,840
and organize these experiments into

49
00:01:37,840 --> 00:01:39,200
three sections

50
00:01:39,200 --> 00:01:41,680
see no evil hear no evil and speak no

51
00:01:41,680 --> 00:01:42,560
evil

52
00:01:42,560 --> 00:01:44,000
i'll demonstrate how to fine-tune

53
00:01:44,000 --> 00:01:45,920
pre-trained natural language processing

54
00:01:45,920 --> 00:01:46,880
computer vision

55
00:01:46,880 --> 00:01:48,960
and text-to-speech neural networks in

56
00:01:48,960 --> 00:01:50,960
order to generate customizable synthetic

57
00:01:50,960 --> 00:01:52,079
media

58
00:01:52,079 --> 00:01:53,360
lee will then wargame out these

59
00:01:53,360 --> 00:01:55,360
capabilities in the context of social

60
00:01:55,360 --> 00:01:56,320
media driven

61
00:01:56,320 --> 00:01:59,439
information operations or io and assess

62
00:01:59,439 --> 00:02:01,600
the challenges underlying detection

63
00:02:01,600 --> 00:02:03,680
attribution and response in scenarios

64
00:02:03,680 --> 00:02:05,360
where actors can anonymously generate

65
00:02:05,360 --> 00:02:06,240
and distribute

66
00:02:06,240 --> 00:02:09,038
credible fake content we'll start off

67
00:02:09,038 --> 00:02:10,399
with some background

68
00:02:10,399 --> 00:02:12,959
initial material and for starters it's

69
00:02:12,959 --> 00:02:14,480
worth pointing out that

70
00:02:14,480 --> 00:02:16,319
synthetic media is by no means a new

71
00:02:16,319 --> 00:02:18,160
development methods for

72
00:02:18,160 --> 00:02:20,319
manipulating media for specific agendas

73
00:02:20,319 --> 00:02:22,560
are as old as the media themselves

74
00:02:22,560 --> 00:02:23,920
you'll see some of the more famous

75
00:02:23,920 --> 00:02:25,680
examples illustrated here

76
00:02:25,680 --> 00:02:27,599
including when the chief of the soviet

77
00:02:27,599 --> 00:02:29,440
secret police was photographed walking

78
00:02:29,440 --> 00:02:31,120
alongside joseph stalin

79
00:02:31,120 --> 00:02:32,800
before being retouched out of an

80
00:02:32,800 --> 00:02:35,200
official press photo after he himself

81
00:02:35,200 --> 00:02:37,280
was arrested and executed during the

82
00:02:37,280 --> 00:02:38,720
great purge

83
00:02:38,720 --> 00:02:40,720
an image of the pyramids was altered to

84
00:02:40,720 --> 00:02:42,400
fit the cover of national geographic

85
00:02:42,400 --> 00:02:43,280
magazine

86
00:02:43,280 --> 00:02:44,959
shifting two of them closer together to

87
00:02:44,959 --> 00:02:46,640
emphasize their size

88
00:02:46,640 --> 00:02:48,319
digital graphic after effects like this

89
00:02:48,319 --> 00:02:49,920
became prominent with the advent of

90
00:02:49,920 --> 00:02:51,200
photoshop

91
00:02:51,200 --> 00:02:53,920
now fast forward to 2016 where carefully

92
00:02:53,920 --> 00:02:55,680
crafted social media messages were being

93
00:02:55,680 --> 00:02:57,680
posted across several platforms

94
00:02:57,680 --> 00:03:00,239
in order to disrupt public discourse and

95
00:03:00,239 --> 00:03:02,000
influence online opinion leading up to

96
00:03:02,000 --> 00:03:04,319
the us presidential election

97
00:03:04,319 --> 00:03:06,560
then later in the 2010s the term deep

98
00:03:06,560 --> 00:03:07,920
fake was coined

99
00:03:07,920 --> 00:03:09,519
there were several viral deep fakes

100
00:03:09,519 --> 00:03:11,760
including one targeting mark zuckerberg

101
00:03:11,760 --> 00:03:13,680
screenshotted here that was designed to

102
00:03:13,680 --> 00:03:15,280
be a warning about the future of digital

103
00:03:15,280 --> 00:03:17,040
manipulation

104
00:03:17,040 --> 00:03:19,040
deep pick videos including techniques

105
00:03:19,040 --> 00:03:20,800
like face swapping and lip syncing are

106
00:03:20,800 --> 00:03:22,400
concerning in the long term

107
00:03:22,400 --> 00:03:24,159
our talk will focus on more basic and

108
00:03:24,159 --> 00:03:26,319
arguably more believable synthetic media

109
00:03:26,319 --> 00:03:28,000
generation advancements including in the

110
00:03:28,000 --> 00:03:28,720
text

111
00:03:28,720 --> 00:03:31,120
image and audio domains we'll provide

112
00:03:31,120 --> 00:03:32,879
evidence later in the talk how some of

113
00:03:32,879 --> 00:03:34,239
these are already being leveraged as

114
00:03:34,239 --> 00:03:36,959
part of i o campaigns today

115
00:03:36,959 --> 00:03:38,720
social media has greased the wheels for

116
00:03:38,720 --> 00:03:40,319
this type of activity by providing

117
00:03:40,319 --> 00:03:42,640
anonymity immediate global reach and the

118
00:03:42,640 --> 00:03:44,319
ability to go viral

119
00:03:44,319 --> 00:03:46,080
rife data disclosure and most

120
00:03:46,080 --> 00:03:48,000
importantly a seamless way to share

121
00:03:48,000 --> 00:03:50,879
content publicly

122
00:03:50,879 --> 00:03:52,159
the low-hanging fruit nature of this

123
00:03:52,159 --> 00:03:53,680
threat environment was illustrated by

124
00:03:53,680 --> 00:03:55,599
previous work i was part of led by john

125
00:03:55,599 --> 00:03:56,480
seymour

126
00:03:56,480 --> 00:03:57,840
four years ago at black cat we

127
00:03:57,840 --> 00:03:59,680
demonstrated how to generate credible

128
00:03:59,680 --> 00:04:01,519
synthetic texts to manipulate social

129
00:04:01,519 --> 00:04:03,360
media users for the purpose of simulated

130
00:04:03,360 --> 00:04:04,799
fishing attacks

131
00:04:04,799 --> 00:04:06,640
the snapper bot was shown to combine the

132
00:04:06,640 --> 00:04:08,640
best best of both worlds

133
00:04:08,640 --> 00:04:10,959
the shotgun approach of fishing think

134
00:04:10,959 --> 00:04:12,799
nigerian print scams that take little

135
00:04:12,799 --> 00:04:14,560
effort and have relatively low success

136
00:04:14,560 --> 00:04:15,439
rates

137
00:04:15,439 --> 00:04:16,959
and the more targeted approach of

138
00:04:16,959 --> 00:04:18,959
spearfishing that takes additional

139
00:04:18,959 --> 00:04:20,238
upfront research effort

140
00:04:20,238 --> 00:04:21,759
but typically pays off in terms of

141
00:04:21,759 --> 00:04:23,440
higher success rates

142
00:04:23,440 --> 00:04:25,199
by seating a machine learning model with

143
00:04:25,199 --> 00:04:26,800
content read off the target's twitter

144
00:04:26,800 --> 00:04:27,600
timeline

145
00:04:27,600 --> 00:04:29,280
and generating text pertaining to their

146
00:04:29,280 --> 00:04:30,720
recent tweet history

147
00:04:30,720 --> 00:04:32,800
then simulating this at scale we could

148
00:04:32,800 --> 00:04:34,479
get click-throughs more often and with

149
00:04:34,479 --> 00:04:36,240
less effort compared with humans

150
00:04:36,240 --> 00:04:39,520
all due to machine learning such

151
00:04:39,520 --> 00:04:41,120
generative machine learning models can

152
00:04:41,120 --> 00:04:42,800
also be used for good too

153
00:04:42,800 --> 00:04:44,400
a few examples covered by the popular

154
00:04:44,400 --> 00:04:46,320
press are shown here on the left

155
00:04:46,320 --> 00:04:47,840
they can be trained to mimic the voices

156
00:04:47,840 --> 00:04:49,520
of those with degenerative speech or

157
00:04:49,520 --> 00:04:50,720
language disorders

158
00:04:50,720 --> 00:04:52,320
allowing people to synthesize words in

159
00:04:52,320 --> 00:04:54,080
their own voice well after it's been

160
00:04:54,080 --> 00:04:55,280
lost

161
00:04:55,280 --> 00:04:57,040
museums have brought dead artists back

162
00:04:57,040 --> 00:04:58,639
to life using machine learning models to

163
00:04:58,639 --> 00:05:00,479
interact with visitors in compelling

164
00:05:00,479 --> 00:05:03,039
educational new ways and activists can

165
00:05:03,039 --> 00:05:04,960
narrate their own stories more safely by

166
00:05:04,960 --> 00:05:06,880
superimposing fabricated faces over

167
00:05:06,880 --> 00:05:08,080
videos of themselves

168
00:05:08,080 --> 00:05:10,720
allowing viewers to more and more easily

169
00:05:10,720 --> 00:05:12,479
empathize without exposing anyone's

170
00:05:12,479 --> 00:05:14,320
identities

171
00:05:14,320 --> 00:05:16,400
but as these capabilities proliferate

172
00:05:16,400 --> 00:05:17,919
there are just as many if not more

173
00:05:17,919 --> 00:05:19,360
misuse cases that have arisen in the

174
00:05:19,360 --> 00:05:20,320
real world

175
00:05:20,320 --> 00:05:21,600
a few of those are called out here on

176
00:05:21,600 --> 00:05:23,759
the left including one case in which a

177
00:05:23,759 --> 00:05:25,520
scammer used an artificially generated

178
00:05:25,520 --> 00:05:26,080
voice

179
00:05:26,080 --> 00:05:27,919
of an executive's boss to fool the

180
00:05:27,919 --> 00:05:29,360
executive into handing over two hundred

181
00:05:29,360 --> 00:05:31,199
and forty thousand dollars

182
00:05:31,199 --> 00:05:32,800
another in which deep fakers created

183
00:05:32,800 --> 00:05:34,639
disturbingly realistic porno videos of

184
00:05:34,639 --> 00:05:36,320
women using photos of them

185
00:05:36,320 --> 00:05:38,639
involuntarily scraped from the internet

186
00:05:38,639 --> 00:05:40,160
and the last one where a computer vision

187
00:05:40,160 --> 00:05:41,039
model was

188
00:05:41,039 --> 00:05:43,199
used to generate fake profile pictures

189
00:05:43,199 --> 00:05:44,800
for a linkedin persona

190
00:05:44,800 --> 00:05:46,880
that socially engineered so um senior

191
00:05:46,880 --> 00:05:48,800
government officials

192
00:05:48,800 --> 00:05:50,880
so there's existing evidence showing

193
00:05:50,880 --> 00:05:52,160
that machine learning can be used to

194
00:05:52,160 --> 00:05:53,759
generate synthetic media for evil

195
00:05:53,759 --> 00:05:54,720
purposes

196
00:05:54,720 --> 00:05:56,319
and in this talk we want to focus on one

197
00:05:56,319 --> 00:05:58,160
specific technical methodology that's

198
00:05:58,160 --> 00:05:59,199
been

199
00:05:59,199 --> 00:06:00,960
that's drastically reduced the amount of

200
00:06:00,960 --> 00:06:02,639
time and effort involved in making this

201
00:06:02,639 --> 00:06:03,680
happen

202
00:06:03,680 --> 00:06:05,280
transfer learning is a paradigm for

203
00:06:05,280 --> 00:06:07,039
machine learning in which we start from

204
00:06:07,039 --> 00:06:08,639
a large generic model that's been

205
00:06:08,639 --> 00:06:10,240
pre-trained for an initial task where

206
00:06:10,240 --> 00:06:12,160
copious data is available

207
00:06:12,160 --> 00:06:13,680
we then leverage the model's acquired

208
00:06:13,680 --> 00:06:14,960
knowledge to train it further on a

209
00:06:14,960 --> 00:06:15,680
different

210
00:06:15,680 --> 00:06:17,520
smaller data set so that it excels at a

211
00:06:17,520 --> 00:06:19,680
subsequent related task

212
00:06:19,680 --> 00:06:20,960
this process of training the model

213
00:06:20,960 --> 00:06:23,120
further is referred to as fine tuning

214
00:06:23,120 --> 00:06:24,960
which typically requires less resources

215
00:06:24,960 --> 00:06:27,039
compared to training it from scratch

216
00:06:27,039 --> 00:06:28,319
this is illustrated in the diagram on

217
00:06:28,319 --> 00:06:30,240
the left normally when there are two

218
00:06:30,240 --> 00:06:32,160
separate tasks that need to be solved

219
00:06:32,160 --> 00:06:33,759
a data scientist has to wrangle two

220
00:06:33,759 --> 00:06:35,120
separate data sets and train two

221
00:06:35,120 --> 00:06:36,319
separate models

222
00:06:36,319 --> 00:06:38,080
in this example a classifier on the left

223
00:06:38,080 --> 00:06:39,680
and a regressor on the right

224
00:06:39,680 --> 00:06:41,600
independently in order to ship them both

225
00:06:41,600 --> 00:06:42,960
to production

226
00:06:42,960 --> 00:06:44,560
and transfer learning shortcuts this

227
00:06:44,560 --> 00:06:46,800
process if task 1 is related to task 2

228
00:06:46,800 --> 00:06:48,000
in a way that task two can

229
00:06:48,000 --> 00:06:49,680
benefit from the learned representations

230
00:06:49,680 --> 00:06:51,759
of task one then we can actually use the

231
00:06:51,759 --> 00:06:54,000
neural network train to perform task one

232
00:06:54,000 --> 00:06:56,560
as a starting point for task two all of

233
00:06:56,560 --> 00:06:58,080
our while requiring

234
00:06:58,080 --> 00:07:00,240
significantly less initial data for test

235
00:07:00,240 --> 00:07:01,280
two

236
00:07:01,280 --> 00:07:02,560
you can think of this in more relatable

237
00:07:02,560 --> 00:07:04,479
terms if i'm a professional tennis

238
00:07:04,479 --> 00:07:05,120
player

239
00:07:05,120 --> 00:07:07,199
i don't need to completely relearn how

240
00:07:07,199 --> 00:07:08,720
to swing a racket in order to excel at

241
00:07:08,720 --> 00:07:09,919
badminton

242
00:07:09,919 --> 00:07:11,919
in a technical sense this fine tuning

243
00:07:11,919 --> 00:07:14,080
can be accomplished in several ways

244
00:07:14,080 --> 00:07:16,319
by lowering or freezing learning rates

245
00:07:16,319 --> 00:07:18,400
by making architectural modifications to

246
00:07:18,400 --> 00:07:19,599
the network or by

247
00:07:19,599 --> 00:07:21,199
only allowing updates in specific

248
00:07:21,199 --> 00:07:22,960
weights or layers

249
00:07:22,960 --> 00:07:24,479
transfer learning works because early

250
00:07:24,479 --> 00:07:26,160
layers produce features that capture

251
00:07:26,160 --> 00:07:27,840
general attributes of the training data

252
00:07:27,840 --> 00:07:28,319
set

253
00:07:28,319 --> 00:07:30,160
whereas later layers capture properties

254
00:07:30,160 --> 00:07:32,960
of the task it's being uh trained on

255
00:07:32,960 --> 00:07:35,440
as such a common fine-tuning approach

256
00:07:35,440 --> 00:07:37,039
simply involves freezing the early

257
00:07:37,039 --> 00:07:38,960
layers from the pre-trained model

258
00:07:38,960 --> 00:07:40,880
and retraining just the last layer using

259
00:07:40,880 --> 00:07:43,280
a small data set for a new task

260
00:07:43,280 --> 00:07:45,120
this is computationally much easier than

261
00:07:45,120 --> 00:07:46,720
training a new model from scratch

262
00:07:46,720 --> 00:07:48,479
since the most most of the learned

263
00:07:48,479 --> 00:07:50,080
parameters are taken directly from the

264
00:07:50,080 --> 00:07:51,120
pre-trained model

265
00:07:51,120 --> 00:07:52,879
a much smaller number of parameters

266
00:07:52,879 --> 00:07:56,000
needs to be learned during fine-tuning

267
00:07:56,000 --> 00:07:57,599
so transfer learning allows certain

268
00:07:57,599 --> 00:07:59,840
tasks to be toggled and take less time

269
00:07:59,840 --> 00:08:02,560
with less effort and at a lower cost in

270
00:08:02,560 --> 00:08:04,000
practice though this only works

271
00:08:04,000 --> 00:08:05,840
well when people share their pre-trained

272
00:08:05,840 --> 00:08:08,240
models lucky for us the culture machine

273
00:08:08,240 --> 00:08:09,680
learning research nowadays has given

274
00:08:09,680 --> 00:08:11,520
research to a rich open source model

275
00:08:11,520 --> 00:08:12,800
ecosystem

276
00:08:12,800 --> 00:08:14,720
it's become commonplace for

277
00:08:14,720 --> 00:08:16,479
well-resourced industry and academic

278
00:08:16,479 --> 00:08:17,919
researchers to release their model

279
00:08:17,919 --> 00:08:18,879
checkpoints

280
00:08:18,879 --> 00:08:21,599
when their state-of-the-art or sota work

281
00:08:21,599 --> 00:08:22,720
gets accepted into

282
00:08:22,720 --> 00:08:25,120
into a top tier conference typically

283
00:08:25,120 --> 00:08:26,400
this code is released in the form of

284
00:08:26,400 --> 00:08:28,160
github repositories with extensive

285
00:08:28,160 --> 00:08:29,280
how-to guides

286
00:08:29,280 --> 00:08:31,360
well-documented readmes and from my

287
00:08:31,360 --> 00:08:32,719
experience they're generally very well

288
00:08:32,719 --> 00:08:34,399
maintained with lots of attention being

289
00:08:34,399 --> 00:08:37,279
paid to open issues and bug reports

290
00:08:37,279 --> 00:08:39,599
oftentimes this allows other researchers

291
00:08:39,599 --> 00:08:41,279
and grad students to easily reproduce

292
00:08:41,279 --> 00:08:43,120
figures from the initial papers

293
00:08:43,120 --> 00:08:44,560
and potentially use these models in

294
00:08:44,560 --> 00:08:46,480
source code as a starting point for

295
00:08:46,480 --> 00:08:48,800
their own research projects

296
00:08:48,800 --> 00:08:50,720
this and this process plays out in the

297
00:08:50,720 --> 00:08:52,320
loop ensuring a healthy

298
00:08:52,320 --> 00:08:54,399
self-reinforcing model supply chain and

299
00:08:54,399 --> 00:08:56,560
ultimately a quicker pace of scientific

300
00:08:56,560 --> 00:08:58,240
innovation

301
00:08:58,240 --> 00:08:59,839
however while this emergent model

302
00:08:59,839 --> 00:09:01,680
sharing ecosystem beneficially lowers

303
00:09:01,680 --> 00:09:03,760
the barrier to entry for non-experts it

304
00:09:03,760 --> 00:09:05,120
also gives a leg up to those seeking to

305
00:09:05,120 --> 00:09:06,640
leverage open source models for

306
00:09:06,640 --> 00:09:08,560
malicious purposes

307
00:09:08,560 --> 00:09:10,160
but just how much has this barrier to

308
00:09:10,160 --> 00:09:11,760
entry been lowered though

309
00:09:11,760 --> 00:09:13,519
as a starting point and hopefully this

310
00:09:13,519 --> 00:09:15,360
statement shouldn't be too controversial

311
00:09:15,360 --> 00:09:17,600
in security and especially the malware

312
00:09:17,600 --> 00:09:18,399
space

313
00:09:18,399 --> 00:09:20,399
we know that adversaries use open source

314
00:09:20,399 --> 00:09:22,000
tools like cobalt strike

315
00:09:22,000 --> 00:09:24,720
empire powershell and mimikats the same

316
00:09:24,720 --> 00:09:26,399
concept holds for synthetic media

317
00:09:26,399 --> 00:09:27,360
generation

318
00:09:27,360 --> 00:09:28,720
if there are tools out there that make

319
00:09:28,720 --> 00:09:30,560
this easier to do it's safe to assume

320
00:09:30,560 --> 00:09:32,640
that those tools will be used

321
00:09:32,640 --> 00:09:34,080
to put this into perspective we

322
00:09:34,080 --> 00:09:35,680
experimented with a pre-trained natural

323
00:09:35,680 --> 00:09:36,000
language

324
00:09:36,000 --> 00:09:38,080
processing neural net called generative

325
00:09:38,080 --> 00:09:40,800
pre-trained transformer or gpt-2

326
00:09:40,800 --> 00:09:42,880
a pre-trained computer vision neural net

327
00:09:42,880 --> 00:09:44,240
called stylegan2

328
00:09:44,240 --> 00:09:46,000
and a pre-trained text-to-speech neural

329
00:09:46,000 --> 00:09:47,839
net called speaker verification to

330
00:09:47,839 --> 00:09:49,680
text-to-speech synthesizer

331
00:09:49,680 --> 00:09:52,800
or sv2 tts on the right-hand side you'll

332
00:09:52,800 --> 00:09:54,480
see a chart showing these three networks

333
00:09:54,480 --> 00:09:56,560
that we'll dive deeper into on upcoming

334
00:09:56,560 --> 00:09:57,360
slides

335
00:09:57,360 --> 00:09:59,200
followed by the self-reported resources

336
00:09:59,200 --> 00:10:00,399
that the authors used

337
00:10:00,399 --> 00:10:01,839
in order to develop their respective

338
00:10:01,839 --> 00:10:04,320
models so what's striking about these

339
00:10:04,320 --> 00:10:05,200
figures

340
00:10:05,200 --> 00:10:07,040
if and when they were available is that

341
00:10:07,040 --> 00:10:08,880
the thousands of dollars weeks or months

342
00:10:08,880 --> 00:10:09,680
of time

343
00:10:09,680 --> 00:10:12,240
sheer training data set sizes and large

344
00:10:12,240 --> 00:10:13,279
gpu

345
00:10:13,279 --> 00:10:15,040
clusters puts pre-training well out of

346
00:10:15,040 --> 00:10:16,560
the reach of script kitties and other

347
00:10:16,560 --> 00:10:19,600
low-resourced adversaries style gan 2's

348
00:10:19,600 --> 00:10:21,279
authors went above and beyond by more

349
00:10:21,279 --> 00:10:22,800
realistically quantifying research

350
00:10:22,800 --> 00:10:24,000
sutures from the id

351
00:10:24,000 --> 00:10:25,760
inception all the way to the open source

352
00:10:25,760 --> 00:10:27,839
release figures that might give

353
00:10:27,839 --> 00:10:30,000
even well-resourced adversaries some

354
00:10:30,000 --> 00:10:31,279
pause

355
00:10:31,279 --> 00:10:32,959
in comparison the fine-tuning

356
00:10:32,959 --> 00:10:34,959
experiments which i'll go into next

357
00:10:34,959 --> 00:10:36,640
can be each done in a fraction of the

358
00:10:36,640 --> 00:10:38,079
time cost

359
00:10:38,079 --> 00:10:40,720
data size compute and flops compared to

360
00:10:40,720 --> 00:10:42,240
these pre-training numbers

361
00:10:42,240 --> 00:10:44,000
whether it's a cloud cloud-hosted

362
00:10:44,000 --> 00:10:45,760
notebook with gpu access like google

363
00:10:45,760 --> 00:10:46,880
collab pro

364
00:10:46,880 --> 00:10:49,360
or an aws p3 instance reservation for

365
00:10:49,360 --> 00:10:50,800
just a single day

366
00:10:50,800 --> 00:10:52,240
we're talking on the order of just tens

367
00:10:52,240 --> 00:10:53,600
of dollars to fine tune one of these

368
00:10:53,600 --> 00:10:54,959
models

369
00:10:54,959 --> 00:10:56,480
skill wise fine tuning is not

370
00:10:56,480 --> 00:10:58,079
necessarily trivial but it's also not

371
00:10:58,079 --> 00:10:59,120
brain surgery

372
00:10:59,120 --> 00:11:00,640
authors or other open source

373
00:11:00,640 --> 00:11:02,320
contributors often release additional

374
00:11:02,320 --> 00:11:04,480
code and tutorials for how to fine tune

375
00:11:04,480 --> 00:11:05,839
so that all you really need to have is

376
00:11:05,839 --> 00:11:07,440
some working knowledge of python to do

377
00:11:07,440 --> 00:11:08,399
this

378
00:11:08,399 --> 00:11:09,920
furthermore it's important to point out

379
00:11:09,920 --> 00:11:11,279
that each of these models were released

380
00:11:11,279 --> 00:11:12,640
just within the last year

381
00:11:12,640 --> 00:11:14,079
so our demonstration should be seen

382
00:11:14,079 --> 00:11:15,839
through the lens of the present

383
00:11:15,839 --> 00:11:17,760
however open source trends we point out

384
00:11:17,760 --> 00:11:19,440
here are accelerating and the bar for

385
00:11:19,440 --> 00:11:21,040
generating credible content will likely

386
00:11:21,040 --> 00:11:24,000
lower even further in the years to come

387
00:11:24,000 --> 00:11:25,920
so our first proof of concept will be in

388
00:11:25,920 --> 00:11:27,680
the image domain where we'll demonstrate

389
00:11:27,680 --> 00:11:29,600
how to fine-tune stylegam2

390
00:11:29,600 --> 00:11:31,440
in order to generate custom portraits to

391
00:11:31,440 --> 00:11:33,920
impersonate a targeted individual

392
00:11:33,920 --> 00:11:35,279
we'll start with a brief overview of

393
00:11:35,279 --> 00:11:37,839
stylegen2 which like its predecessor

394
00:11:37,839 --> 00:11:38,720
stylegan

395
00:11:38,720 --> 00:11:40,240
is architected as a generative

396
00:11:40,240 --> 00:11:43,200
adversarial neural network organ

397
00:11:43,200 --> 00:11:45,120
gans consists of two underlying networks

398
00:11:45,120 --> 00:11:46,720
that are pit against each other hence

399
00:11:46,720 --> 00:11:48,000
adversarial

400
00:11:48,000 --> 00:11:49,600
a generator which generates new

401
00:11:49,600 --> 00:11:51,519
instances of data and a discriminator

402
00:11:51,519 --> 00:11:53,279
which evaluates these instances for

403
00:11:53,279 --> 00:11:55,120
authenticity by deciding whether each

404
00:11:55,120 --> 00:11:56,800
one belongs to the actual training data

405
00:11:56,800 --> 00:11:57,120
set

406
00:11:57,120 --> 00:11:59,839
or not in this way the gant can

407
00:11:59,839 --> 00:12:02,000
synthesize indistinguishably fake images

408
00:12:02,000 --> 00:12:03,760
based on a training data set

409
00:12:03,760 --> 00:12:05,279
which for the open source version of

410
00:12:05,279 --> 00:12:07,279
stylegan 2 used here was

411
00:12:07,279 --> 00:12:10,639
flickr faces high quality or ffhq

412
00:12:10,639 --> 00:12:12,880
a set of 70 000 human faces that are

413
00:12:12,880 --> 00:12:14,560
diverse in terms of age

414
00:12:14,560 --> 00:12:16,880
ethnicity and image background but also

415
00:12:16,880 --> 00:12:18,399
in terms of the presence of accessories

416
00:12:18,399 --> 00:12:21,040
like glasses earrings and hats

417
00:12:21,040 --> 00:12:22,800
so if you generate images from this

418
00:12:22,800 --> 00:12:24,480
off-the-shelf pre-trained model

419
00:12:24,480 --> 00:12:26,639
you get results that look like this

420
00:12:26,639 --> 00:12:28,399
alternatively you can visit this now

421
00:12:28,399 --> 00:12:29,839
infamous website that will generate a

422
00:12:29,839 --> 00:12:31,440
new sample from the pre-trained model

423
00:12:31,440 --> 00:12:33,600
every time you refresh the page

424
00:12:33,600 --> 00:12:35,440
in either case these images are not

425
00:12:35,440 --> 00:12:37,200
present in style gain 2's original

426
00:12:37,200 --> 00:12:38,160
training set

427
00:12:38,160 --> 00:12:40,160
but are completely fabricated from the

428
00:12:40,160 --> 00:12:41,200
generative model

429
00:12:41,200 --> 00:12:43,360
these people in fact do not nor never

430
00:12:43,360 --> 00:12:45,519
have existed

431
00:12:45,519 --> 00:12:47,519
pre-trained style gan 2 outputs random

432
00:12:47,519 --> 00:12:48,639
high quality

433
00:12:48,639 --> 00:12:50,320
and highly diverse images that appear in

434
00:12:50,320 --> 00:12:52,000
a similar orientation as the images that

435
00:12:52,000 --> 00:12:53,440
it was pre-trained on

436
00:12:53,440 --> 00:12:55,760
but it can also be fine-tuned on private

437
00:12:55,760 --> 00:12:56,560
data sets

438
00:12:56,560 --> 00:12:58,639
to generate outputs for a custom task

439
00:12:58,639 --> 00:13:00,560
that i as a consumer of their open

440
00:13:00,560 --> 00:13:02,560
source model am in control of

441
00:13:02,560 --> 00:13:04,160
in this example we've downloaded a few

442
00:13:04,160 --> 00:13:05,600
hundred images of tom hanks

443
00:13:05,600 --> 00:13:08,079
from an online image search engine run

444
00:13:08,079 --> 00:13:09,839
them through an automated cropping tool

445
00:13:09,839 --> 00:13:10,959
called auto crop

446
00:13:10,959 --> 00:13:13,120
so that each of them are fee centered

447
00:13:13,120 --> 00:13:15,600
and 512 by 512 pixels as required by the

448
00:13:15,600 --> 00:13:16,800
pre-trained model

449
00:13:16,800 --> 00:13:18,480
and simply continue training style gain

450
00:13:18,480 --> 00:13:20,079
2 by pointing it at this new smaller

451
00:13:20,079 --> 00:13:20,800
data set

452
00:13:20,800 --> 00:13:22,480
using a slightly slightly smaller

453
00:13:22,480 --> 00:13:23,920
learning rate

454
00:13:23,920 --> 00:13:25,360
after less than a day of training on a

455
00:13:25,360 --> 00:13:27,600
single gpu we then generated outputs

456
00:13:27,600 --> 00:13:28,880
from the fine-tune model

457
00:13:28,880 --> 00:13:30,320
using a ready-to-use script available

458
00:13:30,320 --> 00:13:32,160
from the original repo

459
00:13:32,160 --> 00:13:33,600
the three images you're seeing of mr

460
00:13:33,600 --> 00:13:35,279
hanks on the right are fake they're

461
00:13:35,279 --> 00:13:37,440
generated from our fine-tuned model

462
00:13:37,440 --> 00:13:38,959
you could see that they exhibit a high

463
00:13:38,959 --> 00:13:41,040
level of resemblance to mr hanks and in

464
00:13:41,040 --> 00:13:41,680
theory

465
00:13:41,680 --> 00:13:43,600
we could collect cropped images from any

466
00:13:43,600 --> 00:13:44,720
target of our choosing

467
00:13:44,720 --> 00:13:46,320
and perform the same exercise to

468
00:13:46,320 --> 00:13:48,240
generate arbitrarily many fake images of

469
00:13:48,240 --> 00:13:49,120
them

470
00:13:49,120 --> 00:13:51,120
in summary we can cheaply generate fake

471
00:13:51,120 --> 00:13:52,720
portraits belonging to a target of our

472
00:13:52,720 --> 00:13:53,440
choice

473
00:13:53,440 --> 00:13:55,839
at scale from freely available images in

474
00:13:55,839 --> 00:13:58,079
them on the internet

475
00:13:58,079 --> 00:13:59,519
our second perfect concept will be in

476
00:13:59,519 --> 00:14:00,959
the audio domain where we'll demonstrate

477
00:14:00,959 --> 00:14:03,360
how to fine-tune sv2 tts

478
00:14:03,360 --> 00:14:05,120
in order to generate custom voices from

479
00:14:05,120 --> 00:14:07,199
arbitrary text to impersonate a targeted

480
00:14:07,199 --> 00:14:08,800
individual

481
00:14:08,800 --> 00:14:11,680
sp2 tts is a complex three-stage model

482
00:14:11,680 --> 00:14:13,120
that's capable of performing voice

483
00:14:13,120 --> 00:14:13,839
cloning

484
00:14:13,839 --> 00:14:16,399
or text-to-speech from arbitrary text

485
00:14:16,399 --> 00:14:17,279
inputs

486
00:14:17,279 --> 00:14:19,120
to captured reference speech in real

487
00:14:19,120 --> 00:14:20,560
time so

488
00:14:20,560 --> 00:14:22,880
how does this system work it's comprised

489
00:14:22,880 --> 00:14:24,560
of three underlying networks

490
00:14:24,560 --> 00:14:26,639
first the speaker encoder is trained on

491
00:14:26,639 --> 00:14:28,240
thousands of speakers in order to learn

492
00:14:28,240 --> 00:14:29,839
an abstract representation of human

493
00:14:29,839 --> 00:14:30,560
speech

494
00:14:30,560 --> 00:14:31,920
and squeeze it into a compressed

495
00:14:31,920 --> 00:14:34,079
embedding of floating point values

496
00:14:34,079 --> 00:14:36,079
then the synthesizer which is based on

497
00:14:36,079 --> 00:14:37,680
google's tacotron 2

498
00:14:37,680 --> 00:14:39,519
takes texas input and returns a male

499
00:14:39,519 --> 00:14:41,279
spectrogram which is a numerical

500
00:14:41,279 --> 00:14:43,920
representation of an individual's voice

501
00:14:43,920 --> 00:14:44,560
lastly

502
00:14:44,560 --> 00:14:46,720
the vocoder which is based on google's

503
00:14:46,720 --> 00:14:48,800
wavenet takes the mouse spectrogram and

504
00:14:48,800 --> 00:14:50,720
converts it into an output waveform that

505
00:14:50,720 --> 00:14:52,480
we can actually listen to

506
00:14:52,480 --> 00:14:53,920
to emphasize how much data it took for

507
00:14:53,920 --> 00:14:54,959
this model to learn useful

508
00:14:54,959 --> 00:14:56,160
representations

509
00:14:56,160 --> 00:14:57,600
the tools authors state that it was

510
00:14:57,600 --> 00:14:59,680
pre-trained in over 2 500 audios of

511
00:14:59,680 --> 00:15:00,720
hours of audio

512
00:15:00,720 --> 00:15:02,800
from over 8 500 individual speakers

513
00:15:02,800 --> 00:15:04,720
which is just a herculean task for the

514
00:15:04,720 --> 00:15:05,920
people involved in that dataset

515
00:15:05,920 --> 00:15:07,760
collection

516
00:15:07,760 --> 00:15:09,360
so to demonstrate the kind of output

517
00:15:09,360 --> 00:15:11,120
that pre-trained sv-22gs

518
00:15:11,120 --> 00:15:13,199
model can generate i'll let it explain

519
00:15:13,199 --> 00:15:15,600
itself using custom input text spoken by

520
00:15:15,600 --> 00:15:17,040
an existing voice from their training

521
00:15:17,040 --> 00:15:19,599
dataset

522
00:15:21,279 --> 00:15:23,600
is cloning using an open source model

523
00:15:23,600 --> 00:15:25,120
we're using a pre-trained speaker

524
00:15:25,120 --> 00:15:27,199
encoder and providing custom input text

525
00:15:27,199 --> 00:15:28,800
in the upper right white box

526
00:15:28,800 --> 00:15:30,880
the pre-trained speaker encoder creates

527
00:15:30,880 --> 00:15:32,800
a vector representation of the speaker's

528
00:15:32,800 --> 00:15:33,440
voice

529
00:15:33,440 --> 00:15:35,440
these embeddings can be seen in the

530
00:15:35,440 --> 00:15:36,560
lower left heat

531
00:15:36,560 --> 00:15:38,399
maps the voice embeddings and text

532
00:15:38,399 --> 00:15:40,160
embeddings are combined by a synthesizer

533
00:15:40,160 --> 00:15:41,600
to produce the mil spectrograms in the

534
00:15:41,600 --> 00:15:43,839
lower right hand corner lastly a vocoder

535
00:15:43,839 --> 00:15:45,440
takes these spectrograms and generates

536
00:15:45,440 --> 00:15:47,120
an audio waveform which you are hearing

537
00:15:47,120 --> 00:15:49,519
right now

538
00:15:50,399 --> 00:15:53,040
so the pre-trained sv2 tts can be used

539
00:15:53,040 --> 00:15:54,000
to generate speech

540
00:15:54,000 --> 00:15:55,839
using arbitrary text from one of a few

541
00:15:55,839 --> 00:15:57,279
hundred or so voices

542
00:15:57,279 --> 00:15:58,959
one on which we demonstrated in the last

543
00:15:58,959 --> 00:16:01,120
slide it can also be fine-tuned to

544
00:16:01,120 --> 00:16:02,160
generate speech

545
00:16:02,160 --> 00:16:04,240
in an arbitrary voice using arbitrary

546
00:16:04,240 --> 00:16:05,600
text

547
00:16:05,600 --> 00:16:07,360
all we need to do is collect some audio

548
00:16:07,360 --> 00:16:08,800
samples which are freely available to

549
00:16:08,800 --> 00:16:10,399
record via the internet

550
00:16:10,399 --> 00:16:12,560
load up a few of the resulting m4a files

551
00:16:12,560 --> 00:16:15,360
and use the pre-trained sv2 tts model

552
00:16:15,360 --> 00:16:17,440
as a feature extractor to synthesize new

553
00:16:17,440 --> 00:16:19,040
speech waveforms

554
00:16:19,040 --> 00:16:20,639
listen to a snippet of one of those m4a

555
00:16:20,639 --> 00:16:22,240
samples of tom hanks that we used for

556
00:16:22,240 --> 00:16:25,040
fine tuning now

557
00:16:31,440 --> 00:16:34,000
so examples of mr hanks like that one

558
00:16:34,000 --> 00:16:34,880
and using them

559
00:16:34,880 --> 00:16:36,720
as new inputs i'll demonstrate the

560
00:16:36,720 --> 00:16:38,720
result of fine-tuning on a few texts

561
00:16:38,720 --> 00:16:39,600
that were chosen by

562
00:16:39,600 --> 00:16:42,160
us to resemble commentary that is that

563
00:16:42,160 --> 00:16:44,000
is representative of some broader i o

564
00:16:44,000 --> 00:16:45,440
themes

565
00:16:45,440 --> 00:16:47,040
it demonstrates that we have a common

566
00:16:47,040 --> 00:16:48,800
enemy but i would not count on this

567
00:16:48,800 --> 00:16:50,000
relationship to go beyond

568
00:16:50,000 --> 00:16:51,759
that this regime has shown it will not

569
00:16:51,759 --> 00:16:53,519
hesitate to burden good relations for

570
00:16:53,519 --> 00:16:55,839
its own financial gain

571
00:16:55,839 --> 00:16:57,440
the leaked documents clearly show that

572
00:16:57,440 --> 00:16:59,040
the foreign minister is corrupt and that

573
00:16:59,040 --> 00:17:01,839
he has misdirected funds

574
00:17:01,839 --> 00:17:03,519
the intelligence services have indicated

575
00:17:03,519 --> 00:17:05,280
that these anti-government protests have

576
00:17:05,280 --> 00:17:06,959
been organized by foreign entities

577
00:17:06,959 --> 00:17:08,880
they are bent on stirring up trouble and

578
00:17:08,880 --> 00:17:10,480
causing harm to the people of our

579
00:17:10,480 --> 00:17:12,240
country

580
00:17:12,240 --> 00:17:13,439
you'll notice that the timber of the

581
00:17:13,439 --> 00:17:15,199
voice is very similar to mr hanks in

582
00:17:15,199 --> 00:17:17,119
that it's able to synthesize tom's voice

583
00:17:17,119 --> 00:17:18,720
from custom text

584
00:17:18,720 --> 00:17:20,319
neither the text nor the voices have

585
00:17:20,319 --> 00:17:22,000
been explicitly contained in any of the

586
00:17:22,000 --> 00:17:23,679
original training data sets

587
00:17:23,679 --> 00:17:25,359
in principle i could generate such cell

588
00:17:25,359 --> 00:17:26,720
phone quality speech audio from

589
00:17:26,720 --> 00:17:28,160
arbitrary targets

590
00:17:28,160 --> 00:17:29,520
as long as i could acquire audio or

591
00:17:29,520 --> 00:17:31,200
video files of them on the internet

592
00:17:31,200 --> 00:17:32,960
and then make that speaker dictate from

593
00:17:32,960 --> 00:17:35,440
arbitrary custom text that i control

594
00:17:35,440 --> 00:17:36,799
it's also worth noting that i don't even

595
00:17:36,799 --> 00:17:38,720
need gpu to do this the pre-trained

596
00:17:38,720 --> 00:17:40,480
model was fine-tuned locally using my

597
00:17:40,480 --> 00:17:42,480
laptop's own cpu course

598
00:17:42,480 --> 00:17:44,799
which also suggests that that quality

599
00:17:44,799 --> 00:17:47,840
improvements are possible as well

600
00:17:47,919 --> 00:17:49,360
our last proof of concept will be in the

601
00:17:49,360 --> 00:17:50,960
text domain where we'll generate we'll

602
00:17:50,960 --> 00:17:51,600
demonstrate

603
00:17:51,600 --> 00:17:54,000
how to fine-tune gpt2 in order to

604
00:17:54,000 --> 00:17:55,760
generate custom social media posts

605
00:17:55,760 --> 00:17:57,200
reflecting narratives stemming from a

606
00:17:57,200 --> 00:18:00,400
social media i o campaign

607
00:18:00,400 --> 00:18:02,400
gpt2 is an open source neural network

608
00:18:02,400 --> 00:18:03,760
that was trained on the causal language

609
00:18:03,760 --> 00:18:04,880
modeling task

610
00:18:04,880 --> 00:18:06,320
the objective here is to predict the

611
00:18:06,320 --> 00:18:08,160
next word in a sentence from previous

612
00:18:08,160 --> 00:18:09,120
context

613
00:18:09,120 --> 00:18:10,559
meaning that a trained model ends up

614
00:18:10,559 --> 00:18:12,799
being capable of language generation

615
00:18:12,799 --> 00:18:14,480
if the model can predict the next word

616
00:18:14,480 --> 00:18:16,320
accurately it can be used in turn to

617
00:18:16,320 --> 00:18:17,440
predict the following word

618
00:18:17,440 --> 00:18:18,799
and then so on and so forth until

619
00:18:18,799 --> 00:18:20,559
eventually the model produces fully

620
00:18:20,559 --> 00:18:22,640
coherent sentences and paragraphs

621
00:18:22,640 --> 00:18:24,400
in terms of architectures which you can

622
00:18:24,400 --> 00:18:26,160
see here on the right side of the slide

623
00:18:26,160 --> 00:18:27,919
transformers use attention to decrease

624
00:18:27,919 --> 00:18:29,440
the time required to train on enormous

625
00:18:29,440 --> 00:18:30,400
data sets

626
00:18:30,400 --> 00:18:32,240
they also tend to model lengthy text and

627
00:18:32,240 --> 00:18:34,320
scale better than previous generations

628
00:18:34,320 --> 00:18:35,760
of feed-forward and recurrent neural

629
00:18:35,760 --> 00:18:36,880
networks

630
00:18:36,880 --> 00:18:38,320
again to emphasize the sheer amount of

631
00:18:38,320 --> 00:18:39,840
trading data this thing was pre-trained

632
00:18:39,840 --> 00:18:40,840
on

633
00:18:40,840 --> 00:18:43,120
gbt-2's training data set consisted of

634
00:18:43,120 --> 00:18:45,440
over 40 gigabytes of internet text data

635
00:18:45,440 --> 00:18:47,280
extracted from over 8 million reputable

636
00:18:47,280 --> 00:18:50,240
web pages based on high reddit karma

637
00:18:50,240 --> 00:18:52,480
an example of gpt2's output generation

638
00:18:52,480 --> 00:18:54,240
is pictured here for predictions primed

639
00:18:54,240 --> 00:18:55,200
by the phase

640
00:18:55,200 --> 00:18:57,600
it's disgraceful that the passage

641
00:18:57,600 --> 00:18:59,200
displays relatively formal grammar

642
00:18:59,200 --> 00:19:00,799
punctuation and structure that

643
00:19:00,799 --> 00:19:02,400
corresponds to the text present within

644
00:19:02,400 --> 00:19:03,120
their original

645
00:19:03,120 --> 00:19:05,679
data set i won't read through the entire

646
00:19:05,679 --> 00:19:07,360
passage now but you can visit our blog

647
00:19:07,360 --> 00:19:08,559
post at the link below for more

648
00:19:08,559 --> 00:19:09,760
technical details

649
00:19:09,760 --> 00:19:11,520
or download this slide deck for closer

650
00:19:11,520 --> 00:19:13,440
inspection overall though

651
00:19:13,440 --> 00:19:15,919
the content is prosaic sets a scene up

652
00:19:15,919 --> 00:19:17,280
between a few different people and

653
00:19:17,280 --> 00:19:18,799
includes dialogue of their interactions

654
00:19:18,799 --> 00:19:22,480
in a consistent and logical faction

655
00:19:22,480 --> 00:19:24,720
so to make gpt2's output text appear

656
00:19:24,720 --> 00:19:26,799
more more like social media posts with

657
00:19:26,799 --> 00:19:28,080
their shorter length

658
00:19:28,080 --> 00:19:30,240
informal grammar erratic punctuation and

659
00:19:30,240 --> 00:19:33,120
syntactic works like hashtags and emojis

660
00:19:33,120 --> 00:19:34,880
we fine-tuned it on a new language

661
00:19:34,880 --> 00:19:36,559
modeling task using additional training

662
00:19:36,559 --> 00:19:37,360
data

663
00:19:37,360 --> 00:19:39,280
this data consisted of open source

664
00:19:39,280 --> 00:19:40,880
social media posts from accounts

665
00:19:40,880 --> 00:19:42,799
operated by russia's famed internet

666
00:19:42,799 --> 00:19:43,919
research agency

667
00:19:43,919 --> 00:19:47,440
or ira troll factory for fine tuning

668
00:19:47,440 --> 00:19:49,280
we simply process social media posts

669
00:19:49,280 --> 00:19:51,039
from these open source data sets

670
00:19:51,039 --> 00:19:52,720
through the pre-trained model whose

671
00:19:52,720 --> 00:19:53,919
activations were then fed through

672
00:19:53,919 --> 00:19:55,679
adjustable weights into a linear output

673
00:19:55,679 --> 00:19:56,799
layer

674
00:19:56,799 --> 00:19:58,480
these fine-tuned text generations are

675
00:19:58,480 --> 00:19:59,919
formatted like something we might expect

676
00:19:59,919 --> 00:20:01,360
to encounter scrolling through social

677
00:20:01,360 --> 00:20:02,320
media

678
00:20:02,320 --> 00:20:04,400
they're short yet biting express

679
00:20:04,400 --> 00:20:06,000
certainty and outrage regarding hot

680
00:20:06,000 --> 00:20:07,600
button political issues

681
00:20:07,600 --> 00:20:09,440
and contain emphases like an exclamation

682
00:20:09,440 --> 00:20:11,840
point they also contain idiosyncrasies

683
00:20:11,840 --> 00:20:13,360
like hashtags and emojis that

684
00:20:13,360 --> 00:20:15,120
positionally manifest at the end of the

685
00:20:15,120 --> 00:20:16,480
generated text

686
00:20:16,480 --> 00:20:18,640
depicting a semantic style regularly

687
00:20:18,640 --> 00:20:21,120
exhibited by actual users

688
00:20:21,120 --> 00:20:22,640
so now that i've gone over some

689
00:20:22,640 --> 00:20:23,919
background and technical proof of

690
00:20:23,919 --> 00:20:25,760
concepts for how to leverage fine-tuning

691
00:20:25,760 --> 00:20:27,280
of open source neural networks for

692
00:20:27,280 --> 00:20:28,960
customizable purposes

693
00:20:28,960 --> 00:20:30,720
i'll hand it over to lee to show you how

694
00:20:30,720 --> 00:20:32,400
these capabilities can in turn be

695
00:20:32,400 --> 00:20:35,600
leveraged for il campaigns

696
00:20:35,600 --> 00:20:37,760
hi everyone once again my name is lee

697
00:20:37,760 --> 00:20:39,440
foster and i run the information

698
00:20:39,440 --> 00:20:41,840
operations analysis team at fireeye

699
00:20:41,840 --> 00:20:43,760
which is dedicated to identifying and

700
00:20:43,760 --> 00:20:45,679
tracking state-backed cyber-driven

701
00:20:45,679 --> 00:20:48,400
influence campaigns online

702
00:20:48,400 --> 00:20:50,240
now that phil has detailed the various

703
00:20:50,240 --> 00:20:51,600
technical aspects of

704
00:20:51,600 --> 00:20:53,520
synthetic media generation and

705
00:20:53,520 --> 00:20:55,520
demonstrated the ease with which

706
00:20:55,520 --> 00:20:57,600
existing models can be fine-tuned for

707
00:20:57,600 --> 00:20:59,280
specialist use cases

708
00:20:59,280 --> 00:21:01,120
i'm going to explore the risks that such

709
00:21:01,120 --> 00:21:03,039
capabilities present in the information

710
00:21:03,039 --> 00:21:05,039
operation space

711
00:21:05,039 --> 00:21:07,600
now the misuse of synthetic media for io

712
00:21:07,600 --> 00:21:09,760
has been hotly debated in various

713
00:21:09,760 --> 00:21:12,159
circles for a few years now

714
00:21:12,159 --> 00:21:13,919
some argue it will change the game to

715
00:21:13,919 --> 00:21:15,520
such an extent that it will become

716
00:21:15,520 --> 00:21:16,000
difficult

717
00:21:16,000 --> 00:21:18,159
to ascertain whether anything we consume

718
00:21:18,159 --> 00:21:20,480
online is true or false

719
00:21:20,480 --> 00:21:22,080
others argue that the threat is way

720
00:21:22,080 --> 00:21:23,679
overblown and that there are

721
00:21:23,679 --> 00:21:25,200
much simpler yet still effective

722
00:21:25,200 --> 00:21:26,799
influence and manipulation techniques

723
00:21:26,799 --> 00:21:28,480
that negate the need for threat actors

724
00:21:28,480 --> 00:21:30,559
to even entertain the use of synthetic

725
00:21:30,559 --> 00:21:32,159
generation

726
00:21:32,159 --> 00:21:34,080
however the majority of such debates are

727
00:21:34,080 --> 00:21:35,520
often overly narrow

728
00:21:35,520 --> 00:21:37,600
over prioritizing or focusing only on

729
00:21:37,600 --> 00:21:38,720
deep fake videos

730
00:21:38,720 --> 00:21:40,640
without sufficiently taking into account

731
00:21:40,640 --> 00:21:42,480
other types of synthetic media

732
00:21:42,480 --> 00:21:43,919
that are easier to produce from a

733
00:21:43,919 --> 00:21:45,679
technical standpoint and can be more

734
00:21:45,679 --> 00:21:47,840
readily integrated into the types of i o

735
00:21:47,840 --> 00:21:51,840
campaigns my team identifies daily

736
00:21:51,840 --> 00:21:53,919
so what kinds of techniques do we see io

737
00:21:53,919 --> 00:21:55,679
actors use currently

738
00:21:55,679 --> 00:21:57,360
they are numerous and while i don't have

739
00:21:57,360 --> 00:21:58,960
time to touch them more than a tiny

740
00:21:58,960 --> 00:22:00,000
proportion

741
00:22:00,000 --> 00:22:01,760
it's useful to highlight a few current

742
00:22:01,760 --> 00:22:03,840
tactics that we've observed that would

743
00:22:03,840 --> 00:22:05,760
be particularly conducive to leveraging

744
00:22:05,760 --> 00:22:07,440
synthetic media

745
00:22:07,440 --> 00:22:09,520
last year for example my team uncovered

746
00:22:09,520 --> 00:22:11,120
an influence campaign we termed

747
00:22:11,120 --> 00:22:12,640
distinguished impersonator

748
00:22:12,640 --> 00:22:14,320
which we assess is being conducted in

749
00:22:14,320 --> 00:22:17,120
support of iranian political interests

750
00:22:17,120 --> 00:22:19,360
one prominent campaign tactic involves

751
00:22:19,360 --> 00:22:21,280
fabricating journalists personas

752
00:22:21,280 --> 00:22:23,120
and reaching out to real world experts

753
00:22:23,120 --> 00:22:24,480
and political figures

754
00:22:24,480 --> 00:22:26,559
to disingenuously solicit audio and

755
00:22:26,559 --> 00:22:27,840
video interviews

756
00:22:27,840 --> 00:22:30,720
that advance an iranian political agenda

757
00:22:30,720 --> 00:22:32,480
the link at the bottom of the slide here

758
00:22:32,480 --> 00:22:34,320
directs to some of our public reporting

759
00:22:34,320 --> 00:22:36,159
on this campaign if you're interested in

760
00:22:36,159 --> 00:22:37,840
more detail

761
00:22:37,840 --> 00:22:39,360
another common tactic we see is the

762
00:22:39,360 --> 00:22:41,039
development of cross-platform online

763
00:22:41,039 --> 00:22:42,720
personas that are used to infiltrate

764
00:22:42,720 --> 00:22:44,960
target groups or disseminate fabricated

765
00:22:44,960 --> 00:22:47,360
content to specific audiences

766
00:22:47,360 --> 00:22:49,120
there are many many examples of such

767
00:22:49,120 --> 00:22:50,480
activity out there

768
00:22:50,480 --> 00:22:51,919
perhaps most famously the russian

769
00:22:51,919 --> 00:22:53,440
internet research agency that phil

770
00:22:53,440 --> 00:22:54,799
mentioned earlier

771
00:22:54,799 --> 00:22:56,559
however another recent example is the

772
00:22:56,559 --> 00:22:58,320
ghostwriter campaign that my team has

773
00:22:58,320 --> 00:22:59,039
uncovered

774
00:22:59,039 --> 00:23:00,799
a campaign that has leveraged website

775
00:23:00,799 --> 00:23:02,240
compromises and used multiple

776
00:23:02,240 --> 00:23:04,080
well-developed personas to disseminate

777
00:23:04,080 --> 00:23:05,520
fabricated content

778
00:23:05,520 --> 00:23:07,280
such as falsified correspondence of

779
00:23:07,280 --> 00:23:09,120
officials in eastern europe in support

780
00:23:09,120 --> 00:23:12,159
of russian security interests

781
00:23:12,159 --> 00:23:14,000
another extremely common tactic is the

782
00:23:14,000 --> 00:23:15,840
use of large networks of inauthentic

783
00:23:15,840 --> 00:23:17,600
social media accounts pertaining

784
00:23:17,600 --> 00:23:20,080
pretending to be random unaffiliated

785
00:23:20,080 --> 00:23:20,880
individuals

786
00:23:20,880 --> 00:23:22,960
which are then used to amplify political

787
00:23:22,960 --> 00:23:24,320
narratives across platforms and

788
00:23:24,320 --> 00:23:25,600
geographies

789
00:23:25,600 --> 00:23:27,440
one prominent recent example of this are

790
00:23:27,440 --> 00:23:28,880
the networks we have identified

791
00:23:28,880 --> 00:23:30,720
operating on western social media

792
00:23:30,720 --> 00:23:33,039
platforms pushing pro-china narratives

793
00:23:33,039 --> 00:23:35,280
around issues such as democracy protests

794
00:23:35,280 --> 00:23:36,559
in hong kong

795
00:23:36,559 --> 00:23:39,679
and the covet 19 pandemic the link at

796
00:23:39,679 --> 00:23:41,360
the bottom of this slide directs to a

797
00:23:41,360 --> 00:23:43,600
presentation me and a colleague gave

798
00:23:43,600 --> 00:23:46,720
on some of this activity last year

799
00:23:46,720 --> 00:23:49,600
and finally other very common techniques

800
00:23:49,600 --> 00:23:51,200
include the use of appropriated

801
00:23:51,200 --> 00:23:53,360
photos of real individuals to backstop

802
00:23:53,360 --> 00:23:54,480
false personas

803
00:23:54,480 --> 00:23:56,400
and the repeated use of identical text

804
00:23:56,400 --> 00:23:58,320
on social media to astroturf political

805
00:23:58,320 --> 00:24:00,799
commentary

806
00:24:00,799 --> 00:24:02,880
synthetic media has real potential to

807
00:24:02,880 --> 00:24:04,960
exacerbate the use and effectiveness of

808
00:24:04,960 --> 00:24:06,080
these tactics

809
00:24:06,080 --> 00:24:08,000
for example we already regularly

810
00:24:08,000 --> 00:24:09,440
observed the use of artificially

811
00:24:09,440 --> 00:24:11,760
generated profile photos for inauthentic

812
00:24:11,760 --> 00:24:12,960
social media accounts

813
00:24:12,960 --> 00:24:15,600
and false personas but we can readily

814
00:24:15,600 --> 00:24:18,080
envision an escalation of this tactic

815
00:24:18,080 --> 00:24:20,320
in which convincing personas are created

816
00:24:20,320 --> 00:24:22,159
using artificially generated profile

817
00:24:22,159 --> 00:24:24,000
photos trained on images of real people

818
00:24:24,000 --> 00:24:25,840
from a target group or geography

819
00:24:25,840 --> 00:24:27,760
that correspond to say a particular

820
00:24:27,760 --> 00:24:29,919
minority group and are used to instigate

821
00:24:29,919 --> 00:24:32,320
political conflict or incite animosity

822
00:24:32,320 --> 00:24:34,000
and violence

823
00:24:34,000 --> 00:24:36,240
the use of synthetically generated audio

824
00:24:36,240 --> 00:24:38,159
interviews in a campaign similar to the

825
00:24:38,159 --> 00:24:40,000
pro iran distinguished impersonator

826
00:24:40,000 --> 00:24:42,159
campaign i mentioned previously

827
00:24:42,159 --> 00:24:44,400
trained on a real political expert's

828
00:24:44,400 --> 00:24:45,600
voice would lower an

829
00:24:45,600 --> 00:24:47,520
actor's burden by removing the need for

830
00:24:47,520 --> 00:24:49,440
them to engage in direct outreach to

831
00:24:49,440 --> 00:24:50,480
real people

832
00:24:50,480 --> 00:24:52,080
which would also make attribution more

833
00:24:52,080 --> 00:24:54,240
difficult for investigators by reducing

834
00:24:54,240 --> 00:24:56,480
available leads such as contact details

835
00:24:56,480 --> 00:24:58,400
and modes of communication between the

836
00:24:58,400 --> 00:25:00,480
actor and target

837
00:25:00,480 --> 00:25:02,159
and synthetic media would drastically

838
00:25:02,159 --> 00:25:04,000
lower the barriers to actors

839
00:25:04,000 --> 00:25:06,960
seeking to disseminate diverse

840
00:25:06,960 --> 00:25:07,679
text-based

841
00:25:07,679 --> 00:25:10,240
content at scale reducing the investment

842
00:25:10,240 --> 00:25:12,080
required to manually create a large

843
00:25:12,080 --> 00:25:13,520
corpus of content

844
00:25:13,520 --> 00:25:15,279
and the need to repeatedly reuse

845
00:25:15,279 --> 00:25:17,840
snippets of identical text

846
00:25:17,840 --> 00:25:20,400
the unifying finding our theme behind

847
00:25:20,400 --> 00:25:22,320
all these example use cases

848
00:25:22,320 --> 00:25:23,840
is that they would materially help

849
00:25:23,840 --> 00:25:25,679
thread actors scale campaigns

850
00:25:25,679 --> 00:25:30,000
at low cost and better evade detection

851
00:25:30,000 --> 00:25:31,679
fine-tuning in particular presents a

852
00:25:31,679 --> 00:25:33,679
problem for blue teams as it allows

853
00:25:33,679 --> 00:25:35,679
threat actors to evade classifiers and

854
00:25:35,679 --> 00:25:37,200
detection models that are built for

855
00:25:37,200 --> 00:25:38,799
pre-trained outputs

856
00:25:38,799 --> 00:25:40,880
this is worrisome as as a would-be

857
00:25:40,880 --> 00:25:43,120
threat actor my fine-tuning data sets

858
00:25:43,120 --> 00:25:45,039
are private and unknown to the defender

859
00:25:45,039 --> 00:25:46,559
at test time

860
00:25:46,559 --> 00:25:48,400
to illustrate this we perform detection

861
00:25:48,400 --> 00:25:50,320
experiments in the text domain using

862
00:25:50,320 --> 00:25:52,000
gpt2

863
00:25:52,000 --> 00:25:54,880
after releasing gpt2 openai released

864
00:25:54,880 --> 00:25:57,200
source code and a pre-trained classifier

865
00:25:57,200 --> 00:25:58,400
called roberta

866
00:25:58,400 --> 00:25:59,520
which does not share the same

867
00:25:59,520 --> 00:26:02,240
architecture or tokenizer as gpt2

868
00:26:02,240 --> 00:26:04,080
but could reliably discriminate between

869
00:26:04,080 --> 00:26:05,360
gpt2's own

870
00:26:05,360 --> 00:26:07,120
output generations and its original

871
00:26:07,120 --> 00:26:10,400
training data of high karma reddit posts

872
00:26:10,400 --> 00:26:12,799
we use roberta first to verify their

873
00:26:12,799 --> 00:26:14,320
findings that one can reliably

874
00:26:14,320 --> 00:26:17,120
differentiate between fabricated gpt2

875
00:26:17,120 --> 00:26:18,559
generated text

876
00:26:18,559 --> 00:26:21,840
and the authentic gpt2 training data set

877
00:26:21,840 --> 00:26:23,600
which is shown by the blue bars in the

878
00:26:23,600 --> 00:26:25,840
plots on the right

879
00:26:25,840 --> 00:26:27,840
the first plot shows the scores returned

880
00:26:27,840 --> 00:26:30,240
by the detection model on text generated

881
00:26:30,240 --> 00:26:32,960
by the pre-trained gpt-2 model

882
00:26:32,960 --> 00:26:34,720
the fact that the distribution is skewed

883
00:26:34,720 --> 00:26:36,080
all the way towards the right

884
00:26:36,080 --> 00:26:38,320
close to one means the detection model

885
00:26:38,320 --> 00:26:41,200
with a classification threshold of 0.5

886
00:26:41,200 --> 00:26:43,360
can easily classify the generations as

887
00:26:43,360 --> 00:26:44,240
fake

888
00:26:44,240 --> 00:26:45,919
this results in an accuracy score of

889
00:26:45,919 --> 00:26:48,559
over 97 for the detection model

890
00:26:48,559 --> 00:26:51,440
as shown in the lower right plot however

891
00:26:51,440 --> 00:26:53,360
when we perform the same exercise using

892
00:26:53,360 --> 00:26:55,200
the pre-trained classifier to try to

893
00:26:55,200 --> 00:26:57,039
differentiate our fine-tuned io

894
00:26:57,039 --> 00:26:58,240
generations that feel

895
00:26:58,240 --> 00:27:00,799
detailed earlier the accuracy dipped to

896
00:27:00,799 --> 00:27:02,400
around 78

897
00:27:02,400 --> 00:27:04,320
as the distribution of scores output by

898
00:27:04,320 --> 00:27:06,000
the classifier for these fine-tuned

899
00:27:06,000 --> 00:27:06,880
generations

900
00:27:06,880 --> 00:27:10,080
shifts closer to chance i8.5 as shown in

901
00:27:10,080 --> 00:27:12,240
the red in the top right plot

902
00:27:12,240 --> 00:27:14,240
so if thread actors were to fine-tune on

903
00:27:14,240 --> 00:27:15,679
a custom data set

904
00:27:15,679 --> 00:27:17,919
they themselves collated this would

905
00:27:17,919 --> 00:27:20,000
present a problematic asymmetry between

906
00:27:20,000 --> 00:27:22,080
the data used to create the synthetic

907
00:27:22,080 --> 00:27:24,240
generations and the data with which

908
00:27:24,240 --> 00:27:26,399
blue teams would have access to or

909
00:27:26,399 --> 00:27:28,000
knowledge of with which to build a

910
00:27:28,000 --> 00:27:31,039
commensurate detection model

911
00:27:31,039 --> 00:27:33,360
so as mentioned previously we already

912
00:27:33,360 --> 00:27:35,360
frequently see i o campaigns use

913
00:27:35,360 --> 00:27:37,360
synthetically generated profile

914
00:27:37,360 --> 00:27:40,000
photos for inauthentic accounts on

915
00:27:40,000 --> 00:27:40,960
social media

916
00:27:40,960 --> 00:27:42,799
and here are some examples of recent

917
00:27:42,799 --> 00:27:45,039
intelligence reports on i o activity my

918
00:27:45,039 --> 00:27:46,399
team has detected

919
00:27:46,399 --> 00:27:48,159
that i screenshotted to give you a sense

920
00:27:48,159 --> 00:27:49,520
of both the scale and

921
00:27:49,520 --> 00:27:52,399
diversity of this current use case so

922
00:27:52,399 --> 00:27:52,960
we've seen

923
00:27:52,960 --> 00:27:55,039
the use of synthetic profile images in

924
00:27:55,039 --> 00:27:56,880
the pro china networks that i

925
00:27:56,880 --> 00:27:59,679
mentioned previously in a recent

926
00:27:59,679 --> 00:28:01,279
operation that appeared designed to

927
00:28:01,279 --> 00:28:03,120
support government officials in a region

928
00:28:03,120 --> 00:28:05,600
of argentina

929
00:28:05,600 --> 00:28:07,360
to backstop a fabricated journalist

930
00:28:07,360 --> 00:28:08,799
persona that was used to amplify

931
00:28:08,799 --> 00:28:10,640
awareness of the defacement of a news

932
00:28:10,640 --> 00:28:13,760
agency's twitter account in kuwait

933
00:28:13,760 --> 00:28:15,440
and in a social media operation that

934
00:28:15,440 --> 00:28:17,360
promoted cuban government political

935
00:28:17,360 --> 00:28:19,520
interests

936
00:28:19,520 --> 00:28:20,799
and this is what that looks like in

937
00:28:20,799 --> 00:28:22,880
practice along with some of the markers

938
00:28:22,880 --> 00:28:24,799
of inauthenticity that often clue us

939
00:28:24,799 --> 00:28:26,320
into the images being artificially

940
00:28:26,320 --> 00:28:27,520
generated

941
00:28:27,520 --> 00:28:29,200
you can see in these examples from one

942
00:28:29,200 --> 00:28:31,279
of the pro-china networks the common

943
00:28:31,279 --> 00:28:33,120
format of a closely cropped headshot

944
00:28:33,120 --> 00:28:34,559
with a blurred background

945
00:28:34,559 --> 00:28:36,480
anomalies around the ears and neck and

946
00:28:36,480 --> 00:28:37,919
difficulties with fully rendering

947
00:28:37,919 --> 00:28:39,360
glasses

948
00:28:39,360 --> 00:28:41,120
phantom hair strings being generated

949
00:28:41,120 --> 00:28:44,159
outside a credible area

950
00:28:44,159 --> 00:28:45,760
common issues with the rendering of

951
00:28:45,760 --> 00:28:47,520
earrings and shoulders as you can see in

952
00:28:47,520 --> 00:28:48,720
this image that was

953
00:28:48,720 --> 00:28:50,880
used in the argentina related activity

954
00:28:50,880 --> 00:28:52,080
set

955
00:28:52,080 --> 00:28:54,399
and hilariously in in these last two

956
00:28:54,399 --> 00:28:55,200
examples

957
00:28:55,200 --> 00:28:56,880
the operators behind the pro cuba

958
00:28:56,880 --> 00:28:58,640
network didn't even bother to crop out

959
00:28:58,640 --> 00:29:00,559
the text placed by the this person does

960
00:29:00,559 --> 00:29:01,520
not exist at all

961
00:29:01,520 --> 00:29:03,120
stating that the images were generated

962
00:29:03,120 --> 00:29:06,720
with style again prior to use

963
00:29:06,720 --> 00:29:08,240
now one of the criticisms that is

964
00:29:08,240 --> 00:29:10,399
regularly leveled at concerns about the

965
00:29:10,399 --> 00:29:12,080
misuse of synthetic media

966
00:29:12,080 --> 00:29:13,919
is the high level of effort required on

967
00:29:13,919 --> 00:29:15,919
the part of malicious actors

968
00:29:15,919 --> 00:29:17,600
however it is important to note that in

969
00:29:17,600 --> 00:29:19,679
many cases the difficult technical work

970
00:29:19,679 --> 00:29:22,000
has already been done by organizations

971
00:29:22,000 --> 00:29:23,679
and individuals who have developed and

972
00:29:23,679 --> 00:29:24,320
released

973
00:29:24,320 --> 00:29:27,120
either to use applications and services

974
00:29:27,120 --> 00:29:29,200
or source code that performs generations

975
00:29:29,200 --> 00:29:30,480
for a number of non-malicious

976
00:29:30,480 --> 00:29:31,679
motivations

977
00:29:31,679 --> 00:29:34,559
including hobbyism i.e personal learning

978
00:29:34,559 --> 00:29:36,320
or simply for the fun of it

979
00:29:36,320 --> 00:29:38,320
an ethos of the open sharing of

980
00:29:38,320 --> 00:29:39,760
technical research

981
00:29:39,760 --> 00:29:41,600
and direct commercial applications for

982
00:29:41,600 --> 00:29:43,360
marketing and advertising

983
00:29:43,360 --> 00:29:44,799
corporate communications and training

984
00:29:44,799 --> 00:29:46,799
materials and even for the creation of

985
00:29:46,799 --> 00:29:48,320
assets in video games and other

986
00:29:48,320 --> 00:29:50,720
consumable media

987
00:29:50,720 --> 00:29:52,720
synthetic media as a service is

988
00:29:52,720 --> 00:29:54,159
particularly worrisome

989
00:29:54,159 --> 00:29:55,840
as it is important to note that we have

990
00:29:55,840 --> 00:29:57,840
already seen actors outsource the

991
00:29:57,840 --> 00:30:00,159
organization and conduct of specific

992
00:30:00,159 --> 00:30:01,440
influence campaigns

993
00:30:01,440 --> 00:30:03,679
to public relations firms and other

994
00:30:03,679 --> 00:30:05,679
organizations that possess a nuanced

995
00:30:05,679 --> 00:30:06,480
understanding of

996
00:30:06,480 --> 00:30:08,720
how to target audiences and create

997
00:30:08,720 --> 00:30:10,240
engaging content

998
00:30:10,240 --> 00:30:11,840
for example facebook has announced

999
00:30:11,840 --> 00:30:13,679
multiple takedowns of accounts and pages

1000
00:30:13,679 --> 00:30:14,799
on their platforms

1001
00:30:14,799 --> 00:30:16,720
engaging in foreign interference that

1002
00:30:16,720 --> 00:30:19,440
were operated by pr firms

1003
00:30:19,440 --> 00:30:21,600
last month they removed assets operated

1004
00:30:21,600 --> 00:30:22,960
by a canada-based firm

1005
00:30:22,960 --> 00:30:24,480
but engaged in election-related

1006
00:30:24,480 --> 00:30:26,799
influence activity in south america

1007
00:30:26,799 --> 00:30:29,039
and in may removed assets operated by a

1008
00:30:29,039 --> 00:30:30,399
tunisia-based firm

1009
00:30:30,399 --> 00:30:32,559
that poses locals and independent news

1010
00:30:32,559 --> 00:30:35,279
outlets in sub-saharan africa

1011
00:30:35,279 --> 00:30:37,279
it does not take much for io actors to

1012
00:30:37,279 --> 00:30:38,960
make a similar leap to the use of

1013
00:30:38,960 --> 00:30:40,960
commercial providers of synthetic media

1014
00:30:40,960 --> 00:30:41,840
generation

1015
00:30:41,840 --> 00:30:44,480
to augment their campaigns and further

1016
00:30:44,480 --> 00:30:46,640
bypass the technological barriers to

1017
00:30:46,640 --> 00:30:47,360
entry

1018
00:30:47,360 --> 00:30:49,279
beyond the use of transfer learning that

1019
00:30:49,279 --> 00:30:51,279
phil detailed earlier

1020
00:30:51,279 --> 00:30:53,120
indeed the the adoption of commercial

1021
00:30:53,120 --> 00:30:54,720
outsourcing would provide numerous

1022
00:30:54,720 --> 00:30:56,480
benefits to i o actors

1023
00:30:56,480 --> 00:30:58,480
including providing multiple avenues for

1024
00:30:58,480 --> 00:30:59,760
operational deployment

1025
00:30:59,760 --> 00:31:01,600
which would reduce direct ties back to

1026
00:31:01,600 --> 00:31:03,360
sponsors and make campaign attribution

1027
00:31:03,360 --> 00:31:04,080
more difficult

1028
00:31:04,080 --> 00:31:06,320
for investigators increasing the

1029
00:31:06,320 --> 00:31:08,320
diversity and specialization of both

1030
00:31:08,320 --> 00:31:09,840
assets and content by

1031
00:31:09,840 --> 00:31:12,960
drawing on external expertise lowering

1032
00:31:12,960 --> 00:31:14,960
the investment and expertise required by

1033
00:31:14,960 --> 00:31:16,880
a sponsor themselves to partake in

1034
00:31:16,880 --> 00:31:19,279
synthetic media augmented io

1035
00:31:19,279 --> 00:31:21,200
and adding multiple layers of anonymity

1036
00:31:21,200 --> 00:31:23,039
and plausible deniability against

1037
00:31:23,039 --> 00:31:25,039
accusations of responsibility for any

1038
00:31:25,039 --> 00:31:27,840
particular campaign

1039
00:31:27,840 --> 00:31:29,039
so what should we take away from

1040
00:31:29,039 --> 00:31:30,399
everything that phil and i have

1041
00:31:30,399 --> 00:31:31,279
discussed

1042
00:31:31,279 --> 00:31:34,399
throughout this talk first and perhaps

1043
00:31:34,399 --> 00:31:36,320
obviously it highlights the need for the

1044
00:31:36,320 --> 00:31:38,480
research community to focus attention on

1045
00:31:38,480 --> 00:31:40,159
the development of technical detection

1046
00:31:40,159 --> 00:31:42,080
and mitigation capabilities

1047
00:31:42,080 --> 00:31:43,760
there are already some organizations

1048
00:31:43,760 --> 00:31:45,600
focused on this mission and multiple

1049
00:31:45,600 --> 00:31:46,159
avenues

1050
00:31:46,159 --> 00:31:48,000
of research that can and should be

1051
00:31:48,000 --> 00:31:50,480
pursued these include statistical and

1052
00:31:50,480 --> 00:31:52,240
machine learning approaches to detecting

1053
00:31:52,240 --> 00:31:53,519
synthetic media

1054
00:31:53,519 --> 00:31:55,039
as well as the identification of

1055
00:31:55,039 --> 00:31:56,960
fingerprints and forensic indicators

1056
00:31:56,960 --> 00:31:58,559
such as text-based grammatical and

1057
00:31:58,559 --> 00:32:00,320
image-based positional indicators

1058
00:32:00,320 --> 00:32:02,159
that can be detected both manually and

1059
00:32:02,159 --> 00:32:04,240
in automated ways

1060
00:32:04,240 --> 00:32:06,159
other avenues of effort focus on the

1061
00:32:06,159 --> 00:32:07,760
content authentication side of the

1062
00:32:07,760 --> 00:32:08,399
challenge

1063
00:32:08,399 --> 00:32:10,720
developing ways of allowing producers to

1064
00:32:10,720 --> 00:32:11,440
verify

1065
00:32:11,440 --> 00:32:14,000
mark or sign legitimate content for

1066
00:32:14,000 --> 00:32:15,440
example through the insertion of

1067
00:32:15,440 --> 00:32:16,799
trackable metadata

1068
00:32:16,799 --> 00:32:19,120
at the content generation stage and

1069
00:32:19,120 --> 00:32:21,120
similarly ways for the developers

1070
00:32:21,120 --> 00:32:23,360
of synthetic generation tools to

1071
00:32:23,360 --> 00:32:26,480
watermark those tools is output

1072
00:32:26,480 --> 00:32:27,760
and then of course there are hotly

1073
00:32:27,760 --> 00:32:29,440
debated policy considerations around

1074
00:32:29,440 --> 00:32:31,360
content moderation social media account

1075
00:32:31,360 --> 00:32:32,880
registration authentication

1076
00:32:32,880 --> 00:32:34,320
and fact checking that are currently

1077
00:32:34,320 --> 00:32:37,279
nowhere close to being resolved

1078
00:32:37,279 --> 00:32:38,640
technical approaches to the challenge

1079
00:32:38,640 --> 00:32:40,720
alone won't suffice however and there's

1080
00:32:40,720 --> 00:32:42,480
the human aspect to all of this

1081
00:32:42,480 --> 00:32:44,000
which we've tagged patching human

1082
00:32:44,000 --> 00:32:46,240
perception this includes the importance

1083
00:32:46,240 --> 00:32:47,840
of building a community-wide

1084
00:32:47,840 --> 00:32:50,399
approach to mitigating the human impacts

1085
00:32:50,399 --> 00:32:52,720
of synthetic media augmented io

1086
00:32:52,720 --> 00:32:54,720
and ensuring communities of researchers

1087
00:32:54,720 --> 00:32:56,960
coalesce around approaches to overcoming

1088
00:32:56,960 --> 00:32:59,200
the challenges of detection

1089
00:32:59,200 --> 00:33:00,960
threat modeling how synthetic media may

1090
00:33:00,960 --> 00:33:03,679
be used in future i o campaigns

1091
00:33:03,679 --> 00:33:05,760
and encouraging commercial providers of

1092
00:33:05,760 --> 00:33:07,279
synthetic generation tools to

1093
00:33:07,279 --> 00:33:08,720
acknowledge the potential for their

1094
00:33:08,720 --> 00:33:10,159
abuse by threat actors

1095
00:33:10,159 --> 00:33:12,480
and verifying customer use cases prior

1096
00:33:12,480 --> 00:33:14,399
to service provision

1097
00:33:14,399 --> 00:33:16,880
indeed the sankey diagram here is the

1098
00:33:16,880 --> 00:33:18,880
output of an exercise we did in which we

1099
00:33:18,880 --> 00:33:20,640
surveyed the offerings of some current

1100
00:33:20,640 --> 00:33:22,880
commercial providers of synthetic media

1101
00:33:22,880 --> 00:33:24,320
generation services

1102
00:33:24,320 --> 00:33:26,399
which we define as those that explicitly

1103
00:33:26,399 --> 00:33:28,480
offer paid services instead of

1104
00:33:28,480 --> 00:33:31,519
or in addition to free tools

1105
00:33:31,519 --> 00:33:33,279
looking at image video and voice

1106
00:33:33,279 --> 00:33:34,960
generation offerings

1107
00:33:34,960 --> 00:33:37,679
we asked two questions what specific use

1108
00:33:37,679 --> 00:33:38,159
cases

1109
00:33:38,159 --> 00:33:39,760
do these providers market themselves

1110
00:33:39,760 --> 00:33:41,679
towards and do they explicitly

1111
00:33:41,679 --> 00:33:43,600
acknowledge the potential for misuse of

1112
00:33:43,600 --> 00:33:44,720
their services

1113
00:33:44,720 --> 00:33:47,760
through any kind of ethics statement

1114
00:33:47,760 --> 00:33:49,840
of the 14 companies we looked at less

1115
00:33:49,840 --> 00:33:51,440
than half had any kind of ethics

1116
00:33:51,440 --> 00:33:52,399
statement

1117
00:33:52,399 --> 00:33:54,960
perhaps equally if not more concerning

1118
00:33:54,960 --> 00:33:56,799
some firms did not even feign to market

1119
00:33:56,799 --> 00:33:58,559
themselves to any particular use case

1120
00:33:58,559 --> 00:33:59,519
whatsoever

1121
00:33:59,519 --> 00:34:01,519
effectively leaving it up to end users

1122
00:34:01,519 --> 00:34:03,200
to determine the purposes their

1123
00:34:03,200 --> 00:34:05,200
synthetic generations might even be used

1124
00:34:05,200 --> 00:34:06,240
for

1125
00:34:06,240 --> 00:34:08,239
only a couple of companies specifically

1126
00:34:08,239 --> 00:34:10,079
in the voice generation space

1127
00:34:10,079 --> 00:34:12,960
explicitly stated that they verify and

1128
00:34:12,960 --> 00:34:14,800
sign off on their customers as end use

1129
00:34:14,800 --> 00:34:16,879
cases before agreeing to provide

1130
00:34:16,879 --> 00:34:18,800
services

1131
00:34:18,800 --> 00:34:20,480
and there also remains the need for

1132
00:34:20,480 --> 00:34:22,000
raising awareness and educating

1133
00:34:22,000 --> 00:34:23,760
consumers of online content

1134
00:34:23,760 --> 00:34:26,320
about the risks of synthetic media in a

1135
00:34:26,320 --> 00:34:27,679
responsible manner that doesn't

1136
00:34:27,679 --> 00:34:29,440
misrepresent the threat

1137
00:34:29,440 --> 00:34:31,040
as well as developing legal and

1138
00:34:31,040 --> 00:34:32,879
regulatory approaches to dealing with

1139
00:34:32,879 --> 00:34:34,639
information operations and synthetic

1140
00:34:34,639 --> 00:34:36,960
media

1141
00:34:37,119 --> 00:34:38,960
the final takeaway we want to raise is

1142
00:34:38,960 --> 00:34:41,199
that we are really in the calm before

1143
00:34:41,199 --> 00:34:43,359
the storm stage of synthetic media's use

1144
00:34:43,359 --> 00:34:43,679
in

1145
00:34:43,679 --> 00:34:45,918
information operations as phil

1146
00:34:45,918 --> 00:34:48,079
highlighted these capabilities continue

1147
00:34:48,079 --> 00:34:48,719
to become

1148
00:34:48,719 --> 00:34:50,719
cheaper both monetarily and in terms of

1149
00:34:50,719 --> 00:34:52,399
computing power required

1150
00:34:52,399 --> 00:34:54,719
easier more pervasive and outputs ever

1151
00:34:54,719 --> 00:34:55,918
more credible

1152
00:34:55,918 --> 00:34:57,599
and there are numerous trends that risk

1153
00:34:57,599 --> 00:34:59,040
further escalation of

1154
00:34:59,040 --> 00:35:01,280
threat the amount of effort required to

1155
00:35:01,280 --> 00:35:03,680
fine-tune existing models or train new

1156
00:35:03,680 --> 00:35:06,160
models will continue to decrease

1157
00:35:06,160 --> 00:35:08,240
multimodal content will likely one day

1158
00:35:08,240 --> 00:35:10,480
incorporate synthetically generated text

1159
00:35:10,480 --> 00:35:12,560
images and audio in single pieces of

1160
00:35:12,560 --> 00:35:13,839
content

1161
00:35:13,839 --> 00:35:15,760
image generation capabilities and even

1162
00:35:15,760 --> 00:35:17,680
commercial services are already

1163
00:35:17,680 --> 00:35:20,000
moving beyond merely headshots and

1164
00:35:20,000 --> 00:35:22,000
facial generations to full body shots

1165
00:35:22,000 --> 00:35:24,240
and advanced video generations

1166
00:35:24,240 --> 00:35:25,920
and end users will enjoy increasing

1167
00:35:25,920 --> 00:35:28,079
control over and ease of content

1168
00:35:28,079 --> 00:35:29,200
generation

1169
00:35:29,200 --> 00:35:30,560
both through being able to steer

1170
00:35:30,560 --> 00:35:32,000
generations towards particular

1171
00:35:32,000 --> 00:35:34,240
attributes at more granular levels

1172
00:35:34,240 --> 00:35:35,599
and by being able to leverage an

1173
00:35:35,599 --> 00:35:37,839
increasing amount of low code or no code

1174
00:35:37,839 --> 00:35:40,720
applications for content creation and

1175
00:35:40,720 --> 00:35:42,480
one really important factor to keep in

1176
00:35:42,480 --> 00:35:44,480
mind is user susceptibility to

1177
00:35:44,480 --> 00:35:46,000
misleading content

1178
00:35:46,000 --> 00:35:47,920
some criticize the threat of synthetic

1179
00:35:47,920 --> 00:35:49,440
media as being overblown because there

1180
00:35:49,440 --> 00:35:50,960
are still telltale signs of

1181
00:35:50,960 --> 00:35:52,720
inauthenticity in contemporary

1182
00:35:52,720 --> 00:35:55,920
synthetic generations but even if we set

1183
00:35:55,920 --> 00:35:57,760
aside all the trends i just mentioned it

1184
00:35:57,760 --> 00:35:59,359
is critical for us to acknowledge that

1185
00:35:59,359 --> 00:36:01,200
synthetic generations don't need to be

1186
00:36:01,200 --> 00:36:02,800
overwhelmingly credible to have that

1187
00:36:02,800 --> 00:36:04,240
desired effect

1188
00:36:04,240 --> 00:36:06,400
people are accustomed to consuming short

1189
00:36:06,400 --> 00:36:07,359
authoritative

1190
00:36:07,359 --> 00:36:09,599
error-riddled social media text at speed

1191
00:36:09,599 --> 00:36:10,880
without dwelling too much on its

1192
00:36:10,880 --> 00:36:13,040
linguistic features or origin

1193
00:36:13,040 --> 00:36:14,880
they are accustomed to consuming poor

1194
00:36:14,880 --> 00:36:17,040
quality audio and video snippets

1195
00:36:17,040 --> 00:36:18,720
and most users are unlikely to give a

1196
00:36:18,720 --> 00:36:20,400
twitter's a twitter account's profile

1197
00:36:20,400 --> 00:36:20,800
image

1198
00:36:20,800 --> 00:36:22,320
more than a cursory glance as they

1199
00:36:22,320 --> 00:36:24,000
scroll through their feed and ingest

1200
00:36:24,000 --> 00:36:26,720
the written content at rapid speed all

1201
00:36:26,720 --> 00:36:27,839
of this points to a crucial

1202
00:36:27,839 --> 00:36:29,119
consideration

1203
00:36:29,119 --> 00:36:31,040
the quality bar does not need to be

1204
00:36:31,040 --> 00:36:32,880
particularly high when it comes to

1205
00:36:32,880 --> 00:36:35,200
synthetic generations it only needs to

1206
00:36:35,200 --> 00:36:36,400
remain good enough

1207
00:36:36,400 --> 00:36:38,960
for a majority of users to not question

1208
00:36:38,960 --> 00:36:41,200
it in a world characterized by

1209
00:36:41,200 --> 00:36:43,040
high speed high volume information

1210
00:36:43,040 --> 00:36:45,040
consumption

1211
00:36:45,040 --> 00:36:47,200
so to summarize fine tuning for

1212
00:36:47,200 --> 00:36:49,040
generative impersonation in the text

1213
00:36:49,040 --> 00:36:50,880
image and audio domains can be performed

1214
00:36:50,880 --> 00:36:53,280
by non-experts and it can be weaponized

1215
00:36:53,280 --> 00:36:54,000
for offensive

1216
00:36:54,000 --> 00:36:55,280
social media driven information

1217
00:36:55,280 --> 00:36:57,920
operations detection attribution and

1218
00:36:57,920 --> 00:36:59,680
response is challenging in scenarios

1219
00:36:59,680 --> 00:37:01,599
where actors can anonymously generate

1220
00:37:01,599 --> 00:37:02,480
and distribute

1221
00:37:02,480 --> 00:37:04,800
credible fake content using proprietary

1222
00:37:04,800 --> 00:37:06,480
training data sets

1223
00:37:06,480 --> 00:37:08,720
and we as a community can and should

1224
00:37:08,720 --> 00:37:10,960
help ai researchers policy makers and

1225
00:37:10,960 --> 00:37:12,079
other stakeholders

1226
00:37:12,079 --> 00:37:14,079
mitigate the harmful effects of open

1227
00:37:14,079 --> 00:37:16,320
source models

1228
00:37:16,320 --> 00:37:17,599
we'd like to thank you for your time

1229
00:37:17,599 --> 00:37:20,839
today and look forward to answering your

1230
00:37:20,839 --> 00:37:23,839
questions

1231
00:37:26,000 --> 00:37:27,920
all right folks so thanks for your

1232
00:37:27,920 --> 00:37:30,240
interest and attention in this talk

1233
00:37:30,240 --> 00:37:33,280
um see a few questions in the chat uh we

1234
00:37:33,280 --> 00:37:35,119
have a few minutes here so if you have

1235
00:37:35,119 --> 00:37:37,760
some more please get those in first want

1236
00:37:37,760 --> 00:37:38,800
to address you

1237
00:37:38,800 --> 00:37:42,480
sorry about the audio quality so um

1238
00:37:42,480 --> 00:37:44,880
what i'll do is i'll drop a link to our

1239
00:37:44,880 --> 00:37:46,880
blog post that should have the audio

1240
00:37:46,880 --> 00:37:48,000
samples there

1241
00:37:48,000 --> 00:37:51,040
for people to take a listen to um so

1242
00:37:51,040 --> 00:37:52,320
i'll do that now

1243
00:37:52,320 --> 00:37:55,440
and uh also so someone had asked about

1244
00:37:55,440 --> 00:37:57,680
the availability of a paper um this blog

1245
00:37:57,680 --> 00:37:58,960
post would be the closest thing to that

1246
00:37:58,960 --> 00:38:00,480
from our side

1247
00:38:00,480 --> 00:38:02,320
if the question was more pertaining to

1248
00:38:02,320 --> 00:38:03,680
the papers we discussed

1249
00:38:03,680 --> 00:38:07,520
in the talk um our slides will be

1250
00:38:07,520 --> 00:38:08,160
available

1251
00:38:08,160 --> 00:38:11,920
on the black hat website uh i think soon

1252
00:38:11,920 --> 00:38:14,320
and um we made sure to put references at

1253
00:38:14,320 --> 00:38:15,520
the bottom of each of those slides

1254
00:38:15,520 --> 00:38:17,200
otherwise you can just google these one

1255
00:38:17,200 --> 00:38:18,880
of the beauties of this

1256
00:38:18,880 --> 00:38:21,119
uh prison or i guess the thesis here is

1257
00:38:21,119 --> 00:38:22,480
that these are all open source

1258
00:38:22,480 --> 00:38:25,440
all these models we used so um not

1259
00:38:25,440 --> 00:38:27,040
necessarily trying to make this a how-to

1260
00:38:27,040 --> 00:38:28,720
guide for bad actors but

1261
00:38:28,720 --> 00:38:30,240
you yourself can go up and try and play

1262
00:38:30,240 --> 00:38:32,480
around with these things

1263
00:38:32,480 --> 00:38:34,079
but uh but otherwise let me hand it off

1264
00:38:34,079 --> 00:38:35,760
to lee for all of

1265
00:38:35,760 --> 00:38:38,560
these thanks phil i know we've only got

1266
00:38:38,560 --> 00:38:40,000
like a minute and a half here so quickly

1267
00:38:40,000 --> 00:38:41,680
breeze through the questions that

1268
00:38:41,680 --> 00:38:45,040
uh came up um for us here so robert asks

1269
00:38:45,040 --> 00:38:47,119
about companies like uh liar bird

1270
00:38:47,119 --> 00:38:48,400
have made much of their willingness to

1271
00:38:48,400 --> 00:38:50,560
restrict the sale of model output

1272
00:38:50,560 --> 00:38:52,160
um what are your thoughts on these kinds

1273
00:38:52,160 --> 00:38:54,079
of models uh from the perspective of

1274
00:38:54,079 --> 00:38:54,880
future

1275
00:38:54,880 --> 00:38:57,440
concerns um as i alluded to in one of

1276
00:38:57,440 --> 00:38:59,040
the slides there we did note that a

1277
00:38:59,040 --> 00:39:00,880
couple of firms in the in the voice

1278
00:39:00,880 --> 00:39:01,599
space

1279
00:39:01,599 --> 00:39:05,440
um actually do their use cases and

1280
00:39:05,440 --> 00:39:08,640
uh confirm that the consumer has the

1281
00:39:08,640 --> 00:39:10,320
legal right to use the voice that

1282
00:39:10,320 --> 00:39:11,760
they're trying to um

1283
00:39:11,760 --> 00:39:13,760
fabricate and so i do think that this is

1284
00:39:13,760 --> 00:39:14,880
a good model

1285
00:39:14,880 --> 00:39:17,040
um in general the question is how do you

1286
00:39:17,040 --> 00:39:18,720
try to normalize this

1287
00:39:18,720 --> 00:39:20,800
across this entire industry uh when you

1288
00:39:20,800 --> 00:39:21,839
have kind of

1289
00:39:21,839 --> 00:39:26,400
very different tiers of of players here

1290
00:39:26,400 --> 00:39:28,240
patrick asks uh for the pr marketing

1291
00:39:28,240 --> 00:39:29,599
firms engaged in this have you found a

1292
00:39:29,599 --> 00:39:30,640
con pricing model

1293
00:39:30,640 --> 00:39:33,280
um if your question is about the uh

1294
00:39:33,280 --> 00:39:34,800
outsourcing of

1295
00:39:34,800 --> 00:39:37,920
of i o campaigns in general to pr firms

1296
00:39:37,920 --> 00:39:40,800
um no um and i would presume that those

1297
00:39:40,800 --> 00:39:41,920
are kind of

1298
00:39:41,920 --> 00:39:44,640
custom spec but on the synthetic

1299
00:39:44,640 --> 00:39:46,079
generation a couple of

1300
00:39:46,079 --> 00:39:49,280
models did emerge um one is kind of a

1301
00:39:49,280 --> 00:39:50,480
timed use model

1302
00:39:50,480 --> 00:39:53,760
uh and access to a services api

1303
00:39:53,760 --> 00:39:55,599
another is kind of a custom quote

1304
00:39:55,599 --> 00:39:57,119
depending on the scale of the project

1305
00:39:57,119 --> 00:39:58,800
and then finally gabriella asks

1306
00:39:58,800 --> 00:40:00,960
have you seen any liability cases for

1307
00:40:00,960 --> 00:40:01,920
using ai

1308
00:40:01,920 --> 00:40:03,520
images that actually look like a real

1309
00:40:03,520 --> 00:40:05,440
person if you're talking from a a legal

1310
00:40:05,440 --> 00:40:06,400
perspective

1311
00:40:06,400 --> 00:40:08,800
no not that i'm aware of um but

1312
00:40:08,800 --> 00:40:10,079
definitely you know something

1313
00:40:10,079 --> 00:40:12,720
to uh kind of look into i think i'm for

1314
00:40:12,720 --> 00:40:13,599
sure

1315
00:40:13,599 --> 00:40:15,760
i think we're out of time now um so if

1316
00:40:15,760 --> 00:40:17,200
there are other questions please hit us

1317
00:40:17,200 --> 00:40:17,680
up

1318
00:40:17,680 --> 00:40:19,359
uh on email or twitter and we'll be

1319
00:40:19,359 --> 00:40:23,839
happy to chat with you

