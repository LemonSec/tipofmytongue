1
00:00:03,810 --> 00:00:14,240
[Music]

2
00:00:15,120 --> 00:00:17,279
hello and welcome to practical defenses

3
00:00:17,279 --> 00:00:18,960
against aboriginal machine learning

4
00:00:18,960 --> 00:00:22,160
i'm your host ariel herbert voss

5
00:00:22,160 --> 00:00:24,240
who am i i'm a research scientist at

6
00:00:24,240 --> 00:00:26,800
openai i'm also a phd student at harvard

7
00:00:26,800 --> 00:00:27,599
university

8
00:00:27,599 --> 00:00:29,199
and most of my work involves breaking

9
00:00:29,199 --> 00:00:30,320
machine learning systems that are

10
00:00:30,320 --> 00:00:32,159
deployed in real world contexts

11
00:00:32,159 --> 00:00:33,760
so this means that i look at the usual

12
00:00:33,760 --> 00:00:35,280
suspects like malware classification and

13
00:00:35,280 --> 00:00:36,239
image recognition

14
00:00:36,239 --> 00:00:37,920
but it also means that i look at things

15
00:00:37,920 --> 00:00:39,360
like analytics platforms

16
00:00:39,360 --> 00:00:41,680
recommendation systems fraud detection

17
00:00:41,680 --> 00:00:42,960
financial trading bots

18
00:00:42,960 --> 00:00:44,640
et cetera so basically anything that

19
00:00:44,640 --> 00:00:46,399
relies on machine learning as a course

20
00:00:46,399 --> 00:00:47,280
offer component

21
00:00:47,280 --> 00:00:50,480
i look at and the content of this talk

22
00:00:50,480 --> 00:00:52,480
is mostly the work has mostly been done

23
00:00:52,480 --> 00:00:54,399
in my capacity as a phd student for a

24
00:00:54,399 --> 00:00:56,840
book that i'm writing for no starch

25
00:00:56,840 --> 00:00:58,239
press

26
00:00:58,239 --> 00:01:01,520
so what is this talk well 30 of it is

27
00:01:01,520 --> 00:01:03,120
me recounting experience and attempting

28
00:01:03,120 --> 00:01:04,319
to replicate attacks that have gotten

29
00:01:04,319 --> 00:01:05,600
the most press coverage

30
00:01:05,600 --> 00:01:08,560
um spoiler this means that uh every show

31
00:01:08,560 --> 00:01:10,000
machine learning in the real world is

32
00:01:10,000 --> 00:01:11,760
wildly different from academic research

33
00:01:11,760 --> 00:01:13,920
surprise

34
00:01:13,920 --> 00:01:15,280
a lot of academic research around

35
00:01:15,280 --> 00:01:16,560
aboriginal machine learning tends to

36
00:01:16,560 --> 00:01:18,320
focus on things like aboriginal examples

37
00:01:18,320 --> 00:01:20,000
which are extremely interesting and

38
00:01:20,000 --> 00:01:21,600
important as methods for probing our

39
00:01:21,600 --> 00:01:23,200
understanding of the dynamics of how

40
00:01:23,200 --> 00:01:25,840
machine learning systems work

41
00:01:25,840 --> 00:01:27,920
but they tend to take up a bit too much

42
00:01:27,920 --> 00:01:29,520
error in the security conversation

43
00:01:29,520 --> 00:01:31,040
because they don't actually constitute a

44
00:01:31,040 --> 00:01:32,880
security threat beyond being yet another

45
00:01:32,880 --> 00:01:34,640
method for generating data points which

46
00:01:34,640 --> 00:01:35,680
the model will mathematically

47
00:01:35,680 --> 00:01:36,560
misclassify

48
00:01:36,560 --> 00:01:37,920
there exists other methods for doing

49
00:01:37,920 --> 00:01:39,759
this that are much cheaper and require

50
00:01:39,759 --> 00:01:40,560
much less math

51
00:01:40,560 --> 00:01:43,920
and are therefore much more attractive

52
00:01:44,560 --> 00:01:46,159
the next 30 percent of this talk is

53
00:01:46,159 --> 00:01:48,000
going to be examples of attacks that

54
00:01:48,000 --> 00:01:49,360
i've seen in the wild in the last five

55
00:01:49,360 --> 00:01:50,320
years or so

56
00:01:50,320 --> 00:01:52,079
the spoiler here is that facial

57
00:01:52,079 --> 00:01:53,920
recognition and malware classification

58
00:01:53,920 --> 00:01:54,960
are commonly cited

59
00:01:54,960 --> 00:01:56,159
but there are more examples of

60
00:01:56,159 --> 00:01:58,240
financially motivated attacks against

61
00:01:58,240 --> 00:02:01,280
recommendation systems

62
00:02:01,360 --> 00:02:03,200
the last 40 of this talk is going to be

63
00:02:03,200 --> 00:02:04,799
practical advice for defending

64
00:02:04,799 --> 00:02:06,960
machine learning systems and this advice

65
00:02:06,960 --> 00:02:08,318
comes down to three recommendations that

66
00:02:08,318 --> 00:02:09,919
are centered on avoiding model leakage

67
00:02:09,919 --> 00:02:12,879
and bad inputs

68
00:02:12,959 --> 00:02:14,959
so what is wrong with aboriginal machine

69
00:02:14,959 --> 00:02:16,560
learning well most attacks that are

70
00:02:16,560 --> 00:02:17,680
proposed in the machine learning

71
00:02:17,680 --> 00:02:19,120
research literature don't actually work

72
00:02:19,120 --> 00:02:20,160
in the real world

73
00:02:20,160 --> 00:02:22,879
and this is for a number of reasons one

74
00:02:22,879 --> 00:02:24,560
of the big reasons is that university

75
00:02:24,560 --> 00:02:25,760
and research clusters tend to be

76
00:02:25,760 --> 00:02:27,360
different from deployment clusters

77
00:02:27,360 --> 00:02:28,560
if you're working in a university

78
00:02:28,560 --> 00:02:30,319
environment oftentimes you're going to

79
00:02:30,319 --> 00:02:31,200
do some sort of

80
00:02:31,200 --> 00:02:33,200
roll your own cluster adventure with

81
00:02:33,200 --> 00:02:35,040
using slurm and mpi

82
00:02:35,040 --> 00:02:36,480
you get to work with some byzantine

83
00:02:36,480 --> 00:02:38,879
university i.t infrastructure

84
00:02:38,879 --> 00:02:41,360
and your focus is you want to be able to

85
00:02:41,360 --> 00:02:43,280
demonstrate mathematically novel attacks

86
00:02:43,280 --> 00:02:44,480
and that's going to be more important to

87
00:02:44,480 --> 00:02:46,640
you than things like reliability

88
00:02:46,640 --> 00:02:48,560
so features that you'd find in an

89
00:02:48,560 --> 00:02:50,319
industry environment like managed cloud

90
00:02:50,319 --> 00:02:50,959
providers

91
00:02:50,959 --> 00:02:53,120
containerization continuous integration

92
00:02:53,120 --> 00:02:54,319
those kind of things those are all going

93
00:02:54,319 --> 00:02:56,640
to be more of a second thought for you

94
00:02:56,640 --> 00:02:58,400
this additional layer of infrastructure

95
00:02:58,400 --> 00:03:00,000
that you find in industry

96
00:03:00,000 --> 00:03:01,280
changes the attack surface pretty

97
00:03:01,280 --> 00:03:03,280
significantly and as such it makes it

98
00:03:03,280 --> 00:03:04,640
really hard for

99
00:03:04,640 --> 00:03:07,040
literature around aboriginal machine

100
00:03:07,040 --> 00:03:08,000
learning

101
00:03:08,000 --> 00:03:11,120
to apply itself to um real world

102
00:03:11,120 --> 00:03:13,760
situations

103
00:03:13,920 --> 00:03:16,000
another thing is that the artificial

104
00:03:16,000 --> 00:03:17,519
constraints that people kind of set up

105
00:03:17,519 --> 00:03:18,319
around

106
00:03:18,319 --> 00:03:20,319
developing these kind of novel attacks

107
00:03:20,319 --> 00:03:21,680
in in the literature they make it really

108
00:03:21,680 --> 00:03:22,480
hard

109
00:03:22,480 --> 00:03:23,840
to actually say anything about these

110
00:03:23,840 --> 00:03:25,760
attacks in the real world it's possible

111
00:03:25,760 --> 00:03:27,120
to say something scientific about these

112
00:03:27,120 --> 00:03:28,000
kind of attacks which is really

113
00:03:28,000 --> 00:03:29,920
important from a research standpoint

114
00:03:29,920 --> 00:03:33,120
but realistically most attackers don't

115
00:03:33,120 --> 00:03:33,760
really have

116
00:03:33,760 --> 00:03:35,280
constraints they're not going to really

117
00:03:35,280 --> 00:03:37,120
limit themselves to a tiny epsilon ball

118
00:03:37,120 --> 00:03:38,640
around the decision boundary

119
00:03:38,640 --> 00:03:39,760
they're going to do whatever gets the

120
00:03:39,760 --> 00:03:41,519
job done fastest and they're going to do

121
00:03:41,519 --> 00:03:43,040
with the least amount of pain so

122
00:03:43,040 --> 00:03:45,120
ultimately this means that

123
00:03:45,120 --> 00:03:46,799
gradient-based attacks constitute

124
00:03:46,799 --> 00:03:49,760
um a much less much smaller percentage

125
00:03:49,760 --> 00:03:51,680
compared to other less mathy and quick

126
00:03:51,680 --> 00:03:53,760
and dirty attacks so roughly about 15

127
00:03:53,760 --> 00:03:55,200
of attacks that are seen in the wild

128
00:03:55,200 --> 00:03:57,680
tend to be these gradient-based

129
00:03:57,680 --> 00:03:59,599
average show example kind of attacks and

130
00:03:59,599 --> 00:04:00,640
so we're going to look at some of these

131
00:04:00,640 --> 00:04:02,319
less mathy quick and dirty attacks

132
00:04:02,319 --> 00:04:04,879
as part of this talk um and i want to

133
00:04:04,879 --> 00:04:06,239
emphasize that these attacks are clever

134
00:04:06,239 --> 00:04:07,680
but they're not necessarily complicated

135
00:04:07,680 --> 00:04:09,439
at least in the theoretical sense

136
00:04:09,439 --> 00:04:10,959
they require a basic understanding of

137
00:04:10,959 --> 00:04:12,560
how machine learning works but they

138
00:04:12,560 --> 00:04:14,000
don't require specific knowledge of

139
00:04:14,000 --> 00:04:15,840
algorithms or models or any sort of

140
00:04:15,840 --> 00:04:17,120
specific

141
00:04:17,120 --> 00:04:19,839
areas of esoteric math we can also

142
00:04:19,839 --> 00:04:20,720
distill these

143
00:04:20,720 --> 00:04:22,400
down to two types of exploits where you

144
00:04:22,400 --> 00:04:23,840
have bad inputs and you have model

145
00:04:23,840 --> 00:04:26,160
leakage

146
00:04:26,160 --> 00:04:28,720
so a sexy advantage example of bad

147
00:04:28,720 --> 00:04:30,400
inputs is with self-driving cars

148
00:04:30,400 --> 00:04:32,320
uh so last year i had a talk at defcon

149
00:04:32,320 --> 00:04:34,240
where i took some salt and they poured

150
00:04:34,240 --> 00:04:35,440
it on the ground because it was

151
00:04:35,440 --> 00:04:37,520
cold and it was in boston and i drove my

152
00:04:37,520 --> 00:04:38,720
friends tesla and

153
00:04:38,720 --> 00:04:40,400
lo and behold the tesla started driving

154
00:04:40,400 --> 00:04:42,800
off into into the sunset when it should

155
00:04:42,800 --> 00:04:44,320
not have been doing that

156
00:04:44,320 --> 00:04:46,479
and then 10 cent keen lab came out with

157
00:04:46,479 --> 00:04:47,919
a very similar report where they showed

158
00:04:47,919 --> 00:04:49,360
if you stick stickers on the ground

159
00:04:49,360 --> 00:04:52,400
uh the tessa will also interpret these

160
00:04:52,400 --> 00:04:53,840
stickers as a line and it'll

161
00:04:53,840 --> 00:04:56,240
eat off into the sunset as well should

162
00:04:56,240 --> 00:04:57,520
not do that

163
00:04:57,520 --> 00:04:59,520
but it's very cool that it does there's

164
00:04:59,520 --> 00:05:01,280
no need to generate any sort of bespoke

165
00:05:01,280 --> 00:05:02,560
adversarial examples for this kind of

166
00:05:02,560 --> 00:05:03,199
attack

167
00:05:03,199 --> 00:05:05,120
you literally are just creating a

168
00:05:05,120 --> 00:05:06,320
physical artifact

169
00:05:06,320 --> 00:05:08,720
in the real world sticking it out there

170
00:05:08,720 --> 00:05:10,960
and the

171
00:05:10,960 --> 00:05:12,800
autopilot algorithm is interpreting it

172
00:05:12,800 --> 00:05:14,960
as part of the environment and then

173
00:05:14,960 --> 00:05:16,560
behaving accordingly so it's the perfect

174
00:05:16,560 --> 00:05:18,400
example of bad inputs

175
00:05:18,400 --> 00:05:20,080
i will say as a caveat we have not seen

176
00:05:20,080 --> 00:05:21,440
this kind of attack in the wild

177
00:05:21,440 --> 00:05:22,800
but the fact that we have done it on a

178
00:05:22,800 --> 00:05:24,639
real system that exists in the wild

179
00:05:24,639 --> 00:05:26,080
means that it is something that should

180
00:05:26,080 --> 00:05:27,840
be considered

181
00:05:27,840 --> 00:05:31,198
as like a reasonable example

182
00:05:31,840 --> 00:05:33,520
an example of model leakage is with

183
00:05:33,520 --> 00:05:35,039
malware classifiers

184
00:05:35,039 --> 00:05:36,639
with the scilance endpoint protection

185
00:05:36,639 --> 00:05:38,960
bypass that

186
00:05:38,960 --> 00:05:40,960
skylight cyber had last year where they

187
00:05:40,960 --> 00:05:42,240
showed that you can append a bunch of

188
00:05:42,240 --> 00:05:43,600
carefully selected strings to a

189
00:05:43,600 --> 00:05:44,560
malicious file

190
00:05:44,560 --> 00:05:45,919
and then fool the classifier into

191
00:05:45,919 --> 00:05:48,160
marking that file as benign

192
00:05:48,160 --> 00:05:50,639
this required some reverse engineer to

193
00:05:50,639 --> 00:05:52,240
identify how the featurizer processes

194
00:05:52,240 --> 00:05:52,880
strings

195
00:05:52,880 --> 00:05:54,240
and then identifying which strings the

196
00:05:54,240 --> 00:05:56,800
model responds to best

197
00:05:56,800 --> 00:05:58,560
so i'd say that model leakage kind of

198
00:05:58,560 --> 00:05:59,759
comes down to sharing too much

199
00:05:59,759 --> 00:06:01,039
information about the model

200
00:06:01,039 --> 00:06:02,160
to the point where somebody can either

201
00:06:02,160 --> 00:06:03,840
reverse engineer how it works or as

202
00:06:03,840 --> 00:06:05,039
we'll see in the next tech

203
00:06:05,039 --> 00:06:06,560
steal its functionality and use that to

204
00:06:06,560 --> 00:06:08,639
build bad inputs bad inputs and model

205
00:06:08,639 --> 00:06:10,080
leakage kind of go hand in hand together

206
00:06:10,080 --> 00:06:11,280
much like fuzzing and reverse

207
00:06:11,280 --> 00:06:11,919
engineering

208
00:06:11,919 --> 00:06:14,960
in other software security contacts it's

209
00:06:14,960 --> 00:06:18,160
kind of turtles all the way down i'd say

210
00:06:18,160 --> 00:06:20,479
so for this other model leakage attack

211
00:06:20,479 --> 00:06:22,800
um the a really good example here is the

212
00:06:22,800 --> 00:06:25,199
email filters with that fantastic cbe

213
00:06:25,199 --> 00:06:27,039
in the proof point email protection

214
00:06:27,039 --> 00:06:28,560
which was maybe not so fantastic for the

215
00:06:28,560 --> 00:06:30,080
people who had to fix it

216
00:06:30,080 --> 00:06:32,319
this one is really cool because uh the

217
00:06:32,319 --> 00:06:34,080
researchers collected scores from proof

218
00:06:34,080 --> 00:06:35,440
point email headers and then they built

219
00:06:35,440 --> 00:06:36,319
a surrogate model

220
00:06:36,319 --> 00:06:37,919
of the classifier that proofpoint uses

221
00:06:37,919 --> 00:06:39,440
to identify phishing emails

222
00:06:39,440 --> 00:06:41,360
and then from there um they were able to

223
00:06:41,360 --> 00:06:42,720
use the surrogate model to then crash

224
00:06:42,720 --> 00:06:43,759
malicious emails

225
00:06:43,759 --> 00:06:46,479
that will receive preferable scores and

226
00:06:46,479 --> 00:06:47,759
this is very similar in the spirit of

227
00:06:47,759 --> 00:06:48,800
the previous one

228
00:06:48,800 --> 00:06:51,599
um but in this case you're doing more um

229
00:06:51,599 --> 00:06:52,960
you're doing more of like a

230
00:06:52,960 --> 00:06:55,680
a surrogate kind of model theft attack

231
00:06:55,680 --> 00:06:56,720
than you are doing

232
00:06:56,720 --> 00:06:58,800
um like your reverse engineering kind of

233
00:06:58,800 --> 00:06:59,919
attack but

234
00:06:59,919 --> 00:07:01,599
they both kind of fall under the

235
00:07:01,599 --> 00:07:03,199
umbrella model leakage because they both

236
00:07:03,199 --> 00:07:04,080
require

237
00:07:04,080 --> 00:07:07,120
um the person who built the system to

238
00:07:07,120 --> 00:07:08,720
have left out some information for you

239
00:07:08,720 --> 00:07:09,280
to

240
00:07:09,280 --> 00:07:12,800
pick up and use to reverse engineer or

241
00:07:12,800 --> 00:07:17,840
build something new for malicious use

242
00:07:18,960 --> 00:07:21,039
another example of bad inputs is with

243
00:07:21,039 --> 00:07:23,520
transportation prediction algorithms

244
00:07:23,520 --> 00:07:25,199
this is a really great one uh this

245
00:07:25,199 --> 00:07:27,840
there's this artist in germany

246
00:07:27,840 --> 00:07:29,360
who had this wagon full of 99

247
00:07:29,360 --> 00:07:30,880
smartphones that were running google

248
00:07:30,880 --> 00:07:32,880
maps and ended up causing a traffic jam

249
00:07:32,880 --> 00:07:34,800
um because the congestion algorithm that

250
00:07:34,800 --> 00:07:36,479
google maps uses to predict whether or

251
00:07:36,479 --> 00:07:37,360
not there's going to be

252
00:07:37,360 --> 00:07:39,919
a traffic jam um it treats each one of

253
00:07:39,919 --> 00:07:41,919
these phones as a discrete car object

254
00:07:41,919 --> 00:07:42,960
and so if you have a whole bunch of them

255
00:07:42,960 --> 00:07:43,360
just

256
00:07:43,360 --> 00:07:45,199
sit in a pile and wagon it's just going

257
00:07:45,199 --> 00:07:47,120
to interpret that as a giant massive

258
00:07:47,120 --> 00:07:48,639
pile up and then people are going to

259
00:07:48,639 --> 00:07:51,120
respond to this notification on their

260
00:07:51,120 --> 00:07:51,840
map

261
00:07:51,840 --> 00:07:53,360
um accordingly and then it's going to

262
00:07:53,360 --> 00:07:55,120
cause like a bigger issue and

263
00:07:55,120 --> 00:07:56,479
you're going to get a jam out of that so

264
00:07:56,479 --> 00:07:58,319
it's a really great kind of artsy

265
00:07:58,319 --> 00:08:00,160
example of a bad input kind of attack

266
00:08:00,160 --> 00:08:00,720
it's it's

267
00:08:00,720 --> 00:08:02,080
non-traditional because it wasn't

268
00:08:02,080 --> 00:08:04,000
malicious it was more just like a

269
00:08:04,000 --> 00:08:05,680
thought-provoking piece but it's still

270
00:08:05,680 --> 00:08:07,120
like a really good example of some of

271
00:08:07,120 --> 00:08:08,000
the

272
00:08:08,000 --> 00:08:09,199
vulnerabilities that you find in these

273
00:08:09,199 --> 00:08:10,639
kind of systems that you might not have

274
00:08:10,639 --> 00:08:11,599
expected

275
00:08:11,599 --> 00:08:14,160
and there could be i could imagine a

276
00:08:14,160 --> 00:08:15,360
scenario in which somebody would want to

277
00:08:15,360 --> 00:08:16,560
do something very similar with a very

278
00:08:16,560 --> 00:08:17,120
similar

279
00:08:17,120 --> 00:08:22,080
type of system yeah

280
00:08:22,160 --> 00:08:23,520
another really great example of bad

281
00:08:23,520 --> 00:08:25,919
inputs is with recommendation engines

282
00:08:25,919 --> 00:08:27,199
recommendation engines have been around

283
00:08:27,199 --> 00:08:28,560
for a very long time like you've got

284
00:08:28,560 --> 00:08:29,280
amazon

285
00:08:29,280 --> 00:08:30,960
i'd say google kind of counts as a

286
00:08:30,960 --> 00:08:32,159
recommendation algorithm because it's

287
00:08:32,159 --> 00:08:34,240
recommending you search results

288
00:08:34,240 --> 00:08:35,679
as long as these kind of products have

289
00:08:35,679 --> 00:08:38,000
been around black hat seo has been

290
00:08:38,000 --> 00:08:39,440
around and that's all about trying to

291
00:08:39,440 --> 00:08:40,958
use things like click farms

292
00:08:40,958 --> 00:08:43,279
and review farming depending on if

293
00:08:43,279 --> 00:08:44,399
you're like yelp or

294
00:08:44,399 --> 00:08:47,120
amazon to try to pump up your product or

295
00:08:47,120 --> 00:08:49,120
your link or whatever it is

296
00:08:49,120 --> 00:08:50,720
when the algorithm is trying to down

297
00:08:50,720 --> 00:08:52,160
rank you and it's very much a cat and

298
00:08:52,160 --> 00:08:53,600
mouse game it's been played for a long

299
00:08:53,600 --> 00:08:54,800
time

300
00:08:54,800 --> 00:08:56,000
i'm not going to go too much into the

301
00:08:56,000 --> 00:08:57,440
details there and if you're in the

302
00:08:57,440 --> 00:08:58,160
industry

303
00:08:58,160 --> 00:08:59,680
like you're already probably pretty

304
00:08:59,680 --> 00:09:01,760
aware of this wonderful cat and mouse

305
00:09:01,760 --> 00:09:02,480
game

306
00:09:02,480 --> 00:09:04,240
but it's it's a really good example of

307
00:09:04,240 --> 00:09:07,279
bad inputs and

308
00:09:07,279 --> 00:09:08,959
some of the harms that come about from

309
00:09:08,959 --> 00:09:10,399
not thinking too clearly about

310
00:09:10,399 --> 00:09:13,680
where you're getting data from i guess a

311
00:09:13,680 --> 00:09:14,959
contemporary example of this

312
00:09:14,959 --> 00:09:17,600
is during this um coronavirus outbreak

313
00:09:17,600 --> 00:09:19,360
i've been doing a lot of jigsaw puzzles

314
00:09:19,360 --> 00:09:21,600
and i found this one jigsaw puzzle that

315
00:09:21,600 --> 00:09:22,560
was on um

316
00:09:22,560 --> 00:09:25,360
it was on amazon and it was of the i

317
00:09:25,360 --> 00:09:25,839
forget

318
00:09:25,839 --> 00:09:27,600
actually what it was of i think it was

319
00:09:27,600 --> 00:09:29,120
it was the baby baby yoda

320
00:09:29,120 --> 00:09:31,200
um and it was definitely a chinese

321
00:09:31,200 --> 00:09:32,399
knock-off and i was very excited because

322
00:09:32,399 --> 00:09:33,519
it was very cheap

323
00:09:33,519 --> 00:09:35,360
um but when the puzzle actually came in

324
00:09:35,360 --> 00:09:36,880
and it also had really good reviews

325
00:09:36,880 --> 00:09:38,720
and i was still kind of sketched out by

326
00:09:38,720 --> 00:09:39,839
it because it just didn't look that

327
00:09:39,839 --> 00:09:41,279
legit and the reviews were all kind of

328
00:09:41,279 --> 00:09:42,959
the same copy-pasted thing over and over

329
00:09:42,959 --> 00:09:43,600
again

330
00:09:43,600 --> 00:09:46,480
um and i get the puzzle in the mail and

331
00:09:46,480 --> 00:09:47,680
i open it up and

332
00:09:47,680 --> 00:09:49,200
like half of the pieces are missing the

333
00:09:49,200 --> 00:09:50,720
backing is

334
00:09:50,720 --> 00:09:52,480
just a total mess and then trying to

335
00:09:52,480 --> 00:09:53,839
report it and send it back to them it

336
00:09:53,839 --> 00:09:54,560
was just

337
00:09:54,560 --> 00:09:56,240
just kind of a huge hassle but it's

338
00:09:56,240 --> 00:09:58,399
really a great visceral example of

339
00:09:58,399 --> 00:10:00,640
how recommendation engines and bad

340
00:10:00,640 --> 00:10:02,000
inputs are something that we still kind

341
00:10:02,000 --> 00:10:03,040
of face today

342
00:10:03,040 --> 00:10:06,720
as like a kind of low level attack

343
00:10:06,720 --> 00:10:10,480
kind of situation another example of bad

344
00:10:10,480 --> 00:10:11,040
inputs

345
00:10:11,040 --> 00:10:13,519
um is with trading bots so i had this

346
00:10:13,519 --> 00:10:15,120
project a few years ago

347
00:10:15,120 --> 00:10:18,320
where i was called in for this um this

348
00:10:18,320 --> 00:10:20,079
they were trading about cryptocurrency

349
00:10:20,079 --> 00:10:22,240
startup um they had these bots that

350
00:10:22,240 --> 00:10:22,880
would

351
00:10:22,880 --> 00:10:24,800
they'd pull in information from twitter

352
00:10:24,800 --> 00:10:26,160
and some of these financial

353
00:10:26,160 --> 00:10:27,760
crypto forums and they would use that

354
00:10:27,760 --> 00:10:29,600
information to make predictions about

355
00:10:29,600 --> 00:10:30,640
what the price is going to be and then

356
00:10:30,640 --> 00:10:31,680
they would make trades based off of

357
00:10:31,680 --> 00:10:32,800
those predictions

358
00:10:32,800 --> 00:10:34,160
and they called me in because they were

359
00:10:34,160 --> 00:10:35,279
noticing that there were some weird

360
00:10:35,279 --> 00:10:36,800
behaviors about the prices they noticed

361
00:10:36,800 --> 00:10:37,839
that they were kind of changing in a

362
00:10:37,839 --> 00:10:40,160
very regular way that just

363
00:10:40,160 --> 00:10:43,440
doesn't seem realistic um so we did some

364
00:10:43,440 --> 00:10:44,560
digging and we found out

365
00:10:44,560 --> 00:10:47,600
that somebody some people

366
00:10:47,600 --> 00:10:50,079
were kind of flooding the hashtags that

367
00:10:50,079 --> 00:10:51,680
this bot was using in its sentiment

368
00:10:51,680 --> 00:10:53,680
analysis engine

369
00:10:53,680 --> 00:10:56,640
to convince the algorithm that the price

370
00:10:56,640 --> 00:10:57,920
needed to go up because people were

371
00:10:57,920 --> 00:10:59,519
talking very positively about this

372
00:10:59,519 --> 00:11:01,920
cryptocurrency coin which is now no

373
00:11:01,920 --> 00:11:03,519
longer with us

374
00:11:03,519 --> 00:11:06,399
for good reason probably um so this is

375
00:11:06,399 --> 00:11:07,760
another great example of bad inputs

376
00:11:07,760 --> 00:11:08,560
because they weren't

377
00:11:08,560 --> 00:11:09,920
sanitizing anything they were just kind

378
00:11:09,920 --> 00:11:11,040
of pulling straight from twitter and

379
00:11:11,040 --> 00:11:12,720
they weren't really thinking through

380
00:11:12,720 --> 00:11:14,640
what could your threat model be around

381
00:11:14,640 --> 00:11:16,480
somebody who might want to

382
00:11:16,480 --> 00:11:20,800
make money off of your poor bot design

383
00:11:22,480 --> 00:11:24,959
another example is with fraud detection

384
00:11:24,959 --> 00:11:27,120
of model leakage specifically if you

385
00:11:27,120 --> 00:11:28,000
work in fraud detection

386
00:11:28,000 --> 00:11:31,760
like you know this space

387
00:11:31,760 --> 00:11:35,200
but my favorite example of this is

388
00:11:35,200 --> 00:11:36,560
there was this attack i want to say it

389
00:11:36,560 --> 00:11:38,959
was like two years ago

390
00:11:38,959 --> 00:11:41,760
where there's this group that they made

391
00:11:41,760 --> 00:11:43,200
a bunch of fake apps

392
00:11:43,200 --> 00:11:44,720
that weren't monetized and they deployed

393
00:11:44,720 --> 00:11:46,640
them on the android store and

394
00:11:46,640 --> 00:11:48,240
they were just like very basic apps you

395
00:11:48,240 --> 00:11:49,760
just click through and just

396
00:11:49,760 --> 00:11:50,959
push a bunch of stuff and the whole

397
00:11:50,959 --> 00:11:52,079
point of this app is they're just

398
00:11:52,079 --> 00:11:52,800
collecting

399
00:11:52,800 --> 00:11:54,000
kind of how you're scrolling through

400
00:11:54,000 --> 00:11:56,000
stuff and how you're

401
00:11:56,000 --> 00:11:57,519
clicking on things and timing and

402
00:11:57,519 --> 00:11:59,040
whatnot and the whole point of this is

403
00:11:59,040 --> 00:12:00,800
to sort of train a secondary model that

404
00:12:00,800 --> 00:12:02,639
can approximate that kind of behavior

405
00:12:02,639 --> 00:12:04,160
and then the idea is you then deploy

406
00:12:04,160 --> 00:12:06,560
that model on a set of ad monetized apps

407
00:12:06,560 --> 00:12:08,000
so that way you don't need to

408
00:12:08,000 --> 00:12:11,120
sort of do content or user farming if

409
00:12:11,120 --> 00:12:12,320
you're running some sort of scheme like

410
00:12:12,320 --> 00:12:13,600
this you can just literally run this

411
00:12:13,600 --> 00:12:14,079
model

412
00:12:14,079 --> 00:12:16,079
and it will generate revenue for you and

413
00:12:16,079 --> 00:12:17,760
you don't have to do that much work

414
00:12:17,760 --> 00:12:19,600
it ended up getting detected it was

415
00:12:19,600 --> 00:12:21,200
phenomenal

416
00:12:21,200 --> 00:12:22,720
but it was a really good example of

417
00:12:22,720 --> 00:12:24,000
model leakage because in order to

418
00:12:24,000 --> 00:12:25,120
execute this kind of thing you need to

419
00:12:25,120 --> 00:12:27,360
have some idea of how

420
00:12:27,360 --> 00:12:28,800
the android store or whatever app store

421
00:12:28,800 --> 00:12:30,720
you're using does like

422
00:12:30,720 --> 00:12:34,240
some sort of bypass or does its um

423
00:12:34,240 --> 00:12:37,440
its ad fraud detection and most of these

424
00:12:37,440 --> 00:12:38,639
things they all use machine learning

425
00:12:38,639 --> 00:12:39,839
because everybody loves machine learning

426
00:12:39,839 --> 00:12:40,560
for

427
00:12:40,560 --> 00:12:42,560
um like detecting this kind of stuff

428
00:12:42,560 --> 00:12:44,000
because it's usually like a pretty good

429
00:12:44,000 --> 00:12:46,480
like first pass for um if there's

430
00:12:46,480 --> 00:12:49,600
something that should be flagged

431
00:12:50,639 --> 00:12:53,760
so um that was a whirlwind tour of

432
00:12:53,760 --> 00:12:55,200
seven different kind of attacks that

433
00:12:55,200 --> 00:12:56,959
we've seen within the last five years

434
00:12:56,959 --> 00:12:58,639
um they're not as sexy as you might

435
00:12:58,639 --> 00:13:00,160
expect but they definitely still count

436
00:13:00,160 --> 00:13:01,200
as attacks

437
00:13:01,200 --> 00:13:02,480
um machine learning is something that's

438
00:13:02,480 --> 00:13:04,320
very popular these days because it's

439
00:13:04,320 --> 00:13:05,600
something that people seem to think is

440
00:13:05,600 --> 00:13:06,639
going to give you a lot of business

441
00:13:06,639 --> 00:13:07,440
revenue

442
00:13:07,440 --> 00:13:09,760
um that may or may not be true depending

443
00:13:09,760 --> 00:13:10,959
on what industry you're in

444
00:13:10,959 --> 00:13:12,880
i'd say that like depending on your

445
00:13:12,880 --> 00:13:14,160
industry um

446
00:13:14,160 --> 00:13:15,200
you could probably make a decent amount

447
00:13:15,200 --> 00:13:16,639
of money there's probably other

448
00:13:16,639 --> 00:13:17,519
situations in which you don't

449
00:13:17,519 --> 00:13:18,959
necessarily need machine learning but

450
00:13:18,959 --> 00:13:21,839
all that to be said

451
00:13:22,399 --> 00:13:23,920
the more popular that machine learning

452
00:13:23,920 --> 00:13:25,920
gets as a tool to use for business

453
00:13:25,920 --> 00:13:27,680
analytics the more popular it gets

454
00:13:27,680 --> 00:13:31,200
to be a target for people to focus on

455
00:13:31,200 --> 00:13:32,480
trying to make money off of the

456
00:13:32,480 --> 00:13:33,600
exploitation

457
00:13:33,600 --> 00:13:37,040
of these kind of systems so i have here

458
00:13:37,040 --> 00:13:40,800
a list of three defenses more or less

459
00:13:40,800 --> 00:13:44,240
that will cut down on like issues that

460
00:13:44,240 --> 00:13:45,600
you will have by about 85

461
00:13:45,600 --> 00:13:48,160
which is pretty cool so the first one is

462
00:13:48,160 --> 00:13:49,600
you should use block lists so the

463
00:13:49,600 --> 00:13:50,959
example that i shared about the trading

464
00:13:50,959 --> 00:13:53,040
bot is the way that we fixed that issue

465
00:13:53,040 --> 00:13:55,199
and we were able to fix it like 100 we

466
00:13:55,199 --> 00:13:56,800
just used an allow list for information

467
00:13:56,800 --> 00:13:58,560
sources so we didn't just pull from

468
00:13:58,560 --> 00:13:59,199
random

469
00:13:59,199 --> 00:14:00,720
we pulled from a couple of different

470
00:14:00,720 --> 00:14:02,560
things and there was human in the loop

471
00:14:02,560 --> 00:14:04,399
whose role it was is to make sure that

472
00:14:04,399 --> 00:14:06,240
things are moving slowly

473
00:14:06,240 --> 00:14:08,560
i think a very important part of

474
00:14:08,560 --> 00:14:09,360
developing

475
00:14:09,360 --> 00:14:11,199
a machine learning system that is robust

476
00:14:11,199 --> 00:14:12,399
to issues with

477
00:14:12,399 --> 00:14:14,160
bad inputs is to make sure that you at

478
00:14:14,160 --> 00:14:15,440
least have some sort of human in the

479
00:14:15,440 --> 00:14:16,560
loop kind of interaction

480
00:14:16,560 --> 00:14:18,240
because that lessens the chance that

481
00:14:18,240 --> 00:14:19,920
you're going to find something or that

482
00:14:19,920 --> 00:14:21,120
you're not going to

483
00:14:21,120 --> 00:14:22,160
it lessens the chance that you're going

484
00:14:22,160 --> 00:14:23,680
to miss something important because at

485
00:14:23,680 --> 00:14:25,519
least you have human reasoning ability

486
00:14:25,519 --> 00:14:26,160
around

487
00:14:26,160 --> 00:14:27,760
these very nascent ai systems that are

488
00:14:27,760 --> 00:14:30,240
not quite not quite ready yet to be out

489
00:14:30,240 --> 00:14:30,959
on their own

490
00:14:30,959 --> 00:14:34,079
doing this detection work

491
00:14:34,720 --> 00:14:36,720
so the second defense here is you want

492
00:14:36,720 --> 00:14:38,240
to verify the data accuracy using

493
00:14:38,240 --> 00:14:39,839
multiple signals

494
00:14:39,839 --> 00:14:42,160
so an example is for a facial

495
00:14:42,160 --> 00:14:43,440
recognition system

496
00:14:43,440 --> 00:14:46,480
um we saw a 75 reduction in average show

497
00:14:46,480 --> 00:14:48,000
example induced false positives

498
00:14:48,000 --> 00:14:49,600
when we use two camera sources instead

499
00:14:49,600 --> 00:14:51,440
of one for identification

500
00:14:51,440 --> 00:14:54,079
this percentage increases when you have

501
00:14:54,079 --> 00:14:56,480
when the two cameras are farther apart

502
00:14:56,480 --> 00:14:58,320
so if if you're familiar with byzantine

503
00:14:58,320 --> 00:15:00,240
fault tolerance and distributed systems

504
00:15:00,240 --> 00:15:01,680
this makes sense because if you have

505
00:15:01,680 --> 00:15:03,920
multiple sensor sources or like multiple

506
00:15:03,920 --> 00:15:04,560
nodes

507
00:15:04,560 --> 00:15:06,560
um it's harder for a malicious or faulty

508
00:15:06,560 --> 00:15:08,079
sensor oftentimes you can't tell the

509
00:15:08,079 --> 00:15:08,800
difference

510
00:15:08,800 --> 00:15:11,360
um to fool the consensus algorithm into

511
00:15:11,360 --> 00:15:13,279
accepting a bad decision

512
00:15:13,279 --> 00:15:14,720
and this is especially self-evident in

513
00:15:14,720 --> 00:15:16,079
self-driving cars which tend to use a

514
00:15:16,079 --> 00:15:17,680
decent amount of sensors so that if one

515
00:15:17,680 --> 00:15:19,279
of them ends up either breaking

516
00:15:19,279 --> 00:15:21,199
or if it's getting bad input at least

517
00:15:21,199 --> 00:15:22,320
you have a couple of other ones you can

518
00:15:22,320 --> 00:15:23,440
rely on and they'll all kind of

519
00:15:23,440 --> 00:15:24,800
communicate and

520
00:15:24,800 --> 00:15:27,360
sort of decide what the reality outside

521
00:15:27,360 --> 00:15:30,160
of the car is like

522
00:15:30,639 --> 00:15:32,639
so the third defense is you want to

523
00:15:32,639 --> 00:15:34,480
resist the urge to expose raw user

524
00:15:34,480 --> 00:15:35,279
statistics

525
00:15:35,279 --> 00:15:38,160
or raw statistics to users there is

526
00:15:38,160 --> 00:15:39,440
always going to be a trade-off between

527
00:15:39,440 --> 00:15:41,199
providing interpretable predictions

528
00:15:41,199 --> 00:15:42,720
and providing enough information that an

529
00:15:42,720 --> 00:15:44,160
attacker could reverse engineer your

530
00:15:44,160 --> 00:15:45,120
model

531
00:15:45,120 --> 00:15:46,639
there's a lot of conversation around the

532
00:15:46,639 --> 00:15:48,160
importance of having interpretable

533
00:15:48,160 --> 00:15:49,199
predictions because

534
00:15:49,199 --> 00:15:50,959
it's understandable you have this black

535
00:15:50,959 --> 00:15:52,639
box model you kind of want to understand

536
00:15:52,639 --> 00:15:53,199
what

537
00:15:53,199 --> 00:15:55,360
it's even doing but at the same time if

538
00:15:55,360 --> 00:15:56,720
you give too much information about what

539
00:15:56,720 --> 00:15:57,440
it's doing

540
00:15:57,440 --> 00:15:58,800
it makes it really easy for people to

541
00:15:58,800 --> 00:16:01,120
figure out like okay

542
00:16:01,120 --> 00:16:02,560
given that it's responding to these

543
00:16:02,560 --> 00:16:04,560
pieces of information with these numbers

544
00:16:04,560 --> 00:16:06,160
i can probably pump up these numbers

545
00:16:06,160 --> 00:16:07,839
for this other piece of information

546
00:16:07,839 --> 00:16:09,360
using this extra thing so it's

547
00:16:09,360 --> 00:16:11,360
it then becomes kind of its own attack

548
00:16:11,360 --> 00:16:13,120
vector in its own right

549
00:16:13,120 --> 00:16:16,720
so um one of the most surefire ways of

550
00:16:16,720 --> 00:16:18,320
handling this because it is a trade-off

551
00:16:18,320 --> 00:16:19,920
you can't really get rid of

552
00:16:19,920 --> 00:16:21,680
is you want to round your statistics

553
00:16:21,680 --> 00:16:23,360
this tends to reduce

554
00:16:23,360 --> 00:16:25,440
the ability to reverse engineer models

555
00:16:25,440 --> 00:16:26,480
that are deployed on

556
00:16:26,480 --> 00:16:29,199
certain cloud api services by about 60

557
00:16:29,199 --> 00:16:30,079
based off of some

558
00:16:30,079 --> 00:16:34,239
research that's been done by me

559
00:16:34,480 --> 00:16:37,680
so um in conclusion of all of this um

560
00:16:37,680 --> 00:16:39,759
the methods of exploitation for ml

561
00:16:39,759 --> 00:16:41,600
systems are very similar to traditional

562
00:16:41,600 --> 00:16:42,480
software

563
00:16:42,480 --> 00:16:45,120
um in practice we see two categories of

564
00:16:45,120 --> 00:16:46,000
exploitation

565
00:16:46,000 --> 00:16:48,800
we see the bad inputs examples with

566
00:16:48,800 --> 00:16:50,079
self-driving car stuff with

567
00:16:50,079 --> 00:16:51,920
transportation prediction trading bots

568
00:16:51,920 --> 00:16:53,759
recommendation engines and on the other

569
00:16:53,759 --> 00:16:55,279
side we have model leakage

570
00:16:55,279 --> 00:16:57,360
which really tracks with the malware

571
00:16:57,360 --> 00:16:59,199
classification example email filters and

572
00:16:59,199 --> 00:17:02,000
fraud detection

573
00:17:03,279 --> 00:17:04,559
there's a lot of taxonomies for

574
00:17:04,559 --> 00:17:06,319
categorizing potential attacks but in

575
00:17:06,319 --> 00:17:06,880
practice

576
00:17:06,880 --> 00:17:09,039
we tend to just see these two categories

577
00:17:09,039 --> 00:17:10,799
which is pretty interesting and

578
00:17:10,799 --> 00:17:12,880
as i mentioned earlier bad inputs and

579
00:17:12,880 --> 00:17:14,319
information leakage

580
00:17:14,319 --> 00:17:17,119
tend to enable um like model reverse

581
00:17:17,119 --> 00:17:18,160
engineering and also

582
00:17:18,160 --> 00:17:20,160
like building surrogate models so that

583
00:17:20,160 --> 00:17:21,760
you can do bad things with it

584
00:17:21,760 --> 00:17:23,039
and not only could you do bad things

585
00:17:23,039 --> 00:17:25,280
with it but you could also um

586
00:17:25,280 --> 00:17:26,720
use that to kind of steal somebody

587
00:17:26,720 --> 00:17:28,640
else's ip which is problematic if you

588
00:17:28,640 --> 00:17:30,160
have some sort of proprietary solution

589
00:17:30,160 --> 00:17:30,640
that

590
00:17:30,640 --> 00:17:33,039
like is your workhorse for your company

591
00:17:33,039 --> 00:17:34,080
which is

592
00:17:34,080 --> 00:17:35,360
you want to protect that so you want to

593
00:17:35,360 --> 00:17:36,480
make sure that you're not giving out too

594
00:17:36,480 --> 00:17:37,600
much information so that somebody can

595
00:17:37,600 --> 00:17:40,480
reverse engineer it

596
00:17:42,320 --> 00:17:44,080
so based on what we've seen in the wild

597
00:17:44,080 --> 00:17:45,440
and our own experiments

598
00:17:45,440 --> 00:17:47,919
about 85 of attacks on ml systems can be

599
00:17:47,919 --> 00:17:48,799
mitigated with these three

600
00:17:48,799 --> 00:17:49,919
recommendations

601
00:17:49,919 --> 00:17:52,160
which i will reiterate again you should

602
00:17:52,160 --> 00:17:53,840
use a block list

603
00:17:53,840 --> 00:17:55,360
you should verify the data accuracy

604
00:17:55,360 --> 00:17:57,360
using multiple signals and you should

605
00:17:57,360 --> 00:17:59,039
resist the urge to expose raw

606
00:17:59,039 --> 00:18:01,840
statistics to users and that is what i

607
00:18:01,840 --> 00:18:03,600
have for you

608
00:18:03,600 --> 00:18:05,600
please reach out if you have questions

609
00:18:05,600 --> 00:18:06,799
or if you hated this

610
00:18:06,799 --> 00:18:09,280
or if you have feedback or if you want

611
00:18:09,280 --> 00:18:10,320
to know more

612
00:18:10,320 --> 00:18:12,799
my dms on twitter are open i'm at

613
00:18:12,799 --> 00:18:14,080
adversarial and

614
00:18:14,080 --> 00:18:15,280
you can also send me an email at

615
00:18:15,280 --> 00:18:17,280
adversarial openai.com

616
00:18:17,280 --> 00:18:19,840
so thank you

617
00:18:21,440 --> 00:18:24,880
hello um

618
00:18:24,880 --> 00:18:28,000
welcome to the q a

619
00:18:28,000 --> 00:18:31,760
not sure what i should be doing here um

620
00:18:33,520 --> 00:18:37,360
oh okay cool

621
00:18:37,360 --> 00:18:39,760
um i think i've mostly been answering

622
00:18:39,760 --> 00:18:41,280
questions in the chat

623
00:18:41,280 --> 00:18:44,960
so uh if

624
00:18:45,520 --> 00:18:48,320
okay cool um well gabriel had a really

625
00:18:48,320 --> 00:18:49,440
good comment

626
00:18:49,440 --> 00:18:54,000
um where he said that um

627
00:18:54,000 --> 00:18:56,640
several years back at b-sides las vegas

628
00:18:56,640 --> 00:18:57,840
and ground truth the general theme was

629
00:18:57,840 --> 00:18:59,360
using people in ml to complement each

630
00:18:59,360 --> 00:19:00,720
other rather than replacing one with the

631
00:19:00,720 --> 00:19:01,120
other

632
00:19:01,120 --> 00:19:02,160
and i just wanted to say that i really

633
00:19:02,160 --> 00:19:04,799
like that um i think that it's really

634
00:19:04,799 --> 00:19:05,600
important

635
00:19:05,600 --> 00:19:07,679
to remember that humans kind of need to

636
00:19:07,679 --> 00:19:09,280
be in the loop because

637
00:19:09,280 --> 00:19:11,200
for the most part ml systems aren't

638
00:19:11,200 --> 00:19:12,720
really ready to kind of be out doing

639
00:19:12,720 --> 00:19:15,760
automated stuff on their own

640
00:19:16,400 --> 00:19:18,080
let's see there's also another question

641
00:19:18,080 --> 00:19:20,160
um oh yeah so

642
00:19:20,160 --> 00:19:22,000
gabriel also asks did you happen to see

643
00:19:22,000 --> 00:19:23,520
the attacking rlai

644
00:19:23,520 --> 00:19:26,559
talk um earlier today and i did not see

645
00:19:26,559 --> 00:19:28,000
that but that sounds great

646
00:19:28,000 --> 00:19:31,520
um would somebody be willing to kind of

647
00:19:31,520 --> 00:19:34,240
give me like a link to it and i'll take

648
00:19:34,240 --> 00:19:39,840
a look because that sounds pretty rad

649
00:19:49,200 --> 00:19:51,760
oh boy i also saw the question about

650
00:19:51,760 --> 00:19:53,440
fully homomorphic encryption

651
00:19:53,440 --> 00:19:56,880
i have some thoughts about that um

652
00:19:56,880 --> 00:19:59,440
i don't know how familiar people are in

653
00:19:59,440 --> 00:20:00,559
the audience about

654
00:20:00,559 --> 00:20:04,159
differential privacy um but so far

655
00:20:04,159 --> 00:20:06,080
that is the only approach that i've seen

656
00:20:06,080 --> 00:20:07,200
with um

657
00:20:07,200 --> 00:20:10,240
cryptography that has anything

658
00:20:10,240 --> 00:20:13,280
of use for um machine learning stuff i

659
00:20:13,280 --> 00:20:14,880
think there's some use for

660
00:20:14,880 --> 00:20:19,200
some homo some sort of

661
00:20:19,200 --> 00:20:21,280
more complicated encryption methods but

662
00:20:21,280 --> 00:20:22,880
for the most part i'd say that

663
00:20:22,880 --> 00:20:24,880
those things are kind of overcome you're

664
00:20:24,880 --> 00:20:25,919
taking like

665
00:20:25,919 --> 00:20:27,440
i mean cryptography is already extremely

666
00:20:27,440 --> 00:20:29,280
computationally expensive and so is

667
00:20:29,280 --> 00:20:30,640
machine learning and so you're taking

668
00:20:30,640 --> 00:20:32,640
one thing that's very expensive and then

669
00:20:32,640 --> 00:20:33,840
throwing something else that's even more

670
00:20:33,840 --> 00:20:35,360
expensive on top of it it often doesn't

671
00:20:35,360 --> 00:20:36,480
work out as well

672
00:20:36,480 --> 00:20:39,280
as maybe you would hope so i imagine

673
00:20:39,280 --> 00:20:40,880
that there are some use cases for it but

674
00:20:40,880 --> 00:20:42,000
i can't imagine

675
00:20:42,000 --> 00:20:46,720
any um off the top of my head uh

676
00:20:46,720 --> 00:20:50,159
let's see um oh okay good question um

677
00:20:50,159 --> 00:20:51,360
are there open source projects you

678
00:20:51,360 --> 00:20:53,039
recommend that are conducting ai attack

679
00:20:53,039 --> 00:20:54,000
defense r d

680
00:20:54,000 --> 00:20:57,360
there are so the first one um there's

681
00:20:57,360 --> 00:20:58,080
this

682
00:20:58,080 --> 00:21:00,400
repo that some googlers made uh it's

683
00:21:00,400 --> 00:21:02,000
called clever hans and it's really great

684
00:21:02,000 --> 00:21:04,400
for looking at how to gradient-based

685
00:21:04,400 --> 00:21:06,320
attacks against

686
00:21:06,320 --> 00:21:08,000
neural networks specifically there's

687
00:21:08,000 --> 00:21:09,360
also a few

688
00:21:09,360 --> 00:21:11,520
other resources that one also hasn't

689
00:21:11,520 --> 00:21:13,600
been updated too frequently recently

690
00:21:13,600 --> 00:21:14,000
because

691
00:21:14,000 --> 00:21:15,360
as i said earlier in the talk

692
00:21:15,360 --> 00:21:16,559
gradient-based attacks aren't really

693
00:21:16,559 --> 00:21:18,240
something that people see a lot of

694
00:21:18,240 --> 00:21:20,240
that said there's another one that i've

695
00:21:20,240 --> 00:21:21,360
been following that's called

696
00:21:21,360 --> 00:21:24,240
sec ml let me see if i can pull up a

697
00:21:24,240 --> 00:21:26,320
link to it

698
00:21:26,320 --> 00:21:28,000
and i believe that it's a group of

699
00:21:28,000 --> 00:21:29,520
italian researchers

700
00:21:29,520 --> 00:21:31,679
who have been putting this together and

701
00:21:31,679 --> 00:21:33,600
um

702
00:21:33,600 --> 00:21:37,520
it's it's pretty solid uh

703
00:21:37,840 --> 00:21:40,320
let's see

704
00:21:40,960 --> 00:21:42,480
maybe i should focus on questions first

705
00:21:42,480 --> 00:21:45,840
and then um

706
00:21:45,840 --> 00:21:48,640
yeah let's see oh good point about the

707
00:21:48,640 --> 00:21:50,000
partial homomorphic

708
00:21:50,000 --> 00:21:51,760
um working with much lower latency that

709
00:21:51,760 --> 00:21:53,840
is a good point um but still that's

710
00:21:53,840 --> 00:21:55,440
partial and it's still kind of expensive

711
00:21:55,440 --> 00:21:56,960
and i haven't really encountered any

712
00:21:56,960 --> 00:21:58,000
situations whereby

713
00:21:58,000 --> 00:22:00,400
anybody has asked me to implement this

714
00:22:00,400 --> 00:22:02,240
after i have given them kind of the

715
00:22:02,240 --> 00:22:04,480
layout of how expensive it would be and

716
00:22:04,480 --> 00:22:06,799
and how much accuracy you lose when you

717
00:22:06,799 --> 00:22:07,200
use

718
00:22:07,200 --> 00:22:12,559
encryption methods

719
00:22:12,559 --> 00:22:14,799
um ooh a question about the the google's

720
00:22:14,799 --> 00:22:16,159
federated learning i think federated

721
00:22:16,159 --> 00:22:17,840
learning is underappreciated but i think

722
00:22:17,840 --> 00:22:18,960
there's also

723
00:22:18,960 --> 00:22:20,640
a tendency to ignore the fact that

724
00:22:20,640 --> 00:22:22,720
there's um a decent amount of

725
00:22:22,720 --> 00:22:24,480
other concerns that we have around the

726
00:22:24,480 --> 00:22:26,159
security of those things um

727
00:22:26,159 --> 00:22:28,000
federated learning is really cool but

728
00:22:28,000 --> 00:22:29,360
you still run into all the issues that

729
00:22:29,360 --> 00:22:29,840
you have

730
00:22:29,840 --> 00:22:31,760
with distributed systems and byzantine

731
00:22:31,760 --> 00:22:33,440
fault tolerance kind of questions

732
00:22:33,440 --> 00:22:36,960
um so i'd say that um it's still very

733
00:22:36,960 --> 00:22:38,400
new technology but there's a lot of

734
00:22:38,400 --> 00:22:40,159
potential for it and i'm personally

735
00:22:40,159 --> 00:22:41,919
quite excited for it especially since

736
00:22:41,919 --> 00:22:43,120
everybody's very excited about

737
00:22:43,120 --> 00:22:47,520
enabling edge compute especially

738
00:22:52,880 --> 00:22:55,679
oh yeah um good point about the ibm

739
00:22:55,679 --> 00:22:57,440
abrasive robustness toolbox that is also

740
00:22:57,440 --> 00:23:09,840
a great resource

741
00:23:11,760 --> 00:23:13,360
also somebody said incomplete data can

742
00:23:13,360 --> 00:23:15,360
be uh william says that incomplete data

743
00:23:15,360 --> 00:23:16,960
can be used to

744
00:23:16,960 --> 00:23:18,640
bias conclusions and that is definitely

745
00:23:18,640 --> 00:23:20,799
true and we have seen that happen

746
00:23:20,799 --> 00:23:22,480
that's something machine learning is

747
00:23:22,480 --> 00:23:24,080
kind of an old field and a lot of the

748
00:23:24,080 --> 00:23:25,760
same security questions that we have

749
00:23:25,760 --> 00:23:28,320
around these new things like using it

750
00:23:28,320 --> 00:23:30,159
for malware classification and image

751
00:23:30,159 --> 00:23:31,120
recognition

752
00:23:31,120 --> 00:23:32,720
um a lot of these kind of questions

753
00:23:32,720 --> 00:23:34,880
about inputs and outputs and

754
00:23:34,880 --> 00:23:36,640
cleanliness of the data have been around

755
00:23:36,640 --> 00:23:39,120
for a long time um

756
00:23:39,120 --> 00:23:41,760
and the the question of bias is now a

757
00:23:41,760 --> 00:23:42,559
little bit more

758
00:23:42,559 --> 00:23:45,919
in the forefront because um i guess now

759
00:23:45,919 --> 00:23:47,760
our systems are more connected to the

760
00:23:47,760 --> 00:23:48,960
socio-technical

761
00:23:48,960 --> 00:23:53,679
aspect of society i suppose

762
00:23:53,679 --> 00:23:56,799
um yeah i'm sorry if i'm not entirely

763
00:23:56,799 --> 00:23:58,000
answering all these questions

764
00:23:58,000 --> 00:23:59,919
this is kind of a weird way to do

765
00:23:59,919 --> 00:24:02,840
question answer um

766
00:24:02,840 --> 00:24:05,360
yeah if you have

767
00:24:05,360 --> 00:24:07,039
more specific questions and i haven't

768
00:24:07,039 --> 00:24:08,400
answered something specifically or maybe

769
00:24:08,400 --> 00:24:09,840
i've rambled a little bit too much um

770
00:24:09,840 --> 00:24:11,360
definitely feel free to reach out to me

771
00:24:11,360 --> 00:24:14,400
on twitter um like i said my dms are

772
00:24:14,400 --> 00:24:15,120
definitely open

773
00:24:15,120 --> 00:24:16,480
and i love talking about this stuff so

774
00:24:16,480 --> 00:24:17,760
i'm more than happy to talk about this

775
00:24:17,760 --> 00:24:18,400
stuff

776
00:24:18,400 --> 00:24:21,600
um and also you can send me an email

777
00:24:21,600 --> 00:24:22,799
which is adversarial

778
00:24:22,799 --> 00:24:26,720
openai.com i also answer those

779
00:24:29,039 --> 00:24:31,200
okay so fernando asks any other infosec

780
00:24:31,200 --> 00:24:32,880
vendor examples beyond silence and proof

781
00:24:32,880 --> 00:24:33,520
point

782
00:24:33,520 --> 00:24:34,960
um those are the two that spring to mind

783
00:24:34,960 --> 00:24:36,480
because those are the two that have at

784
00:24:36,480 --> 00:24:36,960
least

785
00:24:36,960 --> 00:24:40,240
captured journalism attention um i think

786
00:24:40,240 --> 00:24:41,919
if you were to ask people who actually

787
00:24:41,919 --> 00:24:42,880
work

788
00:24:42,880 --> 00:24:45,279
inside this space of malware

789
00:24:45,279 --> 00:24:45,919
classification

790
00:24:45,919 --> 00:24:47,919
you'd find that there's been a lot of

791
00:24:47,919 --> 00:24:48,960
examples of people

792
00:24:48,960 --> 00:24:50,400
trying to craft things that will sort of

793
00:24:50,400 --> 00:24:52,000
trip the filter and there have been a

794
00:24:52,000 --> 00:24:52,559
couple of

795
00:24:52,559 --> 00:24:55,039
um open source competitions around that

796
00:24:55,039 --> 00:24:57,440
too um in the ai village at def con

797
00:24:57,440 --> 00:25:00,000
um there is a competition sort of

798
00:25:00,000 --> 00:25:00,640
running

799
00:25:00,640 --> 00:25:03,919
um with hiram anderson and his team um

800
00:25:03,919 --> 00:25:04,640
about

801
00:25:04,640 --> 00:25:06,400
trying to develop ways to kind of fool

802
00:25:06,400 --> 00:25:08,000
and also um

803
00:25:08,000 --> 00:25:09,679
defend machine learning systems that do

804
00:25:09,679 --> 00:25:11,039
malware classification and that's kind

805
00:25:11,039 --> 00:25:11,679
of been

806
00:25:11,679 --> 00:25:13,679
a lot of his work too so you can see

807
00:25:13,679 --> 00:25:14,960
some of the competitions that he's

808
00:25:14,960 --> 00:25:16,320
proposed in the past and also some of

809
00:25:16,320 --> 00:25:17,120
his work

810
00:25:17,120 --> 00:25:29,840
i'm a huge fan

811
00:25:32,400 --> 00:25:34,240
so somebody asks does the improvement of

812
00:25:34,240 --> 00:25:36,159
deep fakes make this harder now

813
00:25:36,159 --> 00:25:38,400
i think that defects are sort of

814
00:25:38,400 --> 00:25:40,799
different um i think defects have a huge

815
00:25:40,799 --> 00:25:42,400
potential for

816
00:25:42,400 --> 00:25:45,679
um they have a huge potential for doing

817
00:25:45,679 --> 00:25:47,679
bad things and making easier to do uh

818
00:25:47,679 --> 00:25:49,120
social engineering for sure

819
00:25:49,120 --> 00:25:50,240
and i think that there are a couple of

820
00:25:50,240 --> 00:25:51,840
talks earlier today that kind of address

821
00:25:51,840 --> 00:25:52,640
this kind of thing

822
00:25:52,640 --> 00:25:55,919
but as for making it harder for

823
00:25:55,919 --> 00:25:58,720
or easier for people to do um like

824
00:25:58,720 --> 00:26:00,400
attacks against machine learning systems

825
00:26:00,400 --> 00:26:01,200
that are more

826
00:26:01,200 --> 00:26:03,120
like fundamental like malware

827
00:26:03,120 --> 00:26:04,640
classification or any of those things

828
00:26:04,640 --> 00:26:06,320
i don't think defects play as big of a

829
00:26:06,320 --> 00:26:08,320
role in that but that said

830
00:26:08,320 --> 00:26:12,400
um so with the uh so i work at openai

831
00:26:12,400 --> 00:26:13,279
and one of the things that we've

832
00:26:13,279 --> 00:26:14,960
produced is this gpd3

833
00:26:14,960 --> 00:26:16,720
language model and it's very good at

834
00:26:16,720 --> 00:26:18,559
generating text and it's also very good

835
00:26:18,559 --> 00:26:19,919
at generating fake code

836
00:26:19,919 --> 00:26:22,000
and there are a number of things that

837
00:26:22,000 --> 00:26:23,760
we're currently actively looking at in

838
00:26:23,760 --> 00:26:24,000
order

839
00:26:24,000 --> 00:26:26,559
to make sure that people like don't

840
00:26:26,559 --> 00:26:27,679
completely misuse

841
00:26:27,679 --> 00:26:29,919
these kind of things and when inevitably

842
00:26:29,919 --> 00:26:30,880
they do

843
00:26:30,880 --> 00:26:32,480
we have some detections in place and we

844
00:26:32,480 --> 00:26:34,559
have methods for handling these and

845
00:26:34,559 --> 00:26:36,000
triaging these kind of

846
00:26:36,000 --> 00:26:39,039
situations um and i'd say that synthetic

847
00:26:39,039 --> 00:26:42,000
and all this to say that synthetic um

848
00:26:42,000 --> 00:26:42,400
text

849
00:26:42,400 --> 00:26:44,320
synthetic images those kind of things

850
00:26:44,320 --> 00:26:46,799
those um all definitely have a potential

851
00:26:46,799 --> 00:26:47,120
for

852
00:26:47,120 --> 00:26:50,960
misuse um but

853
00:26:50,960 --> 00:26:53,200
i i don't think they're necessarily as

854
00:26:53,200 --> 00:26:54,320
relevant to

855
00:26:54,320 --> 00:26:56,080
breaking adversarial machine learning or

856
00:26:56,080 --> 00:26:57,840
breaking machine learning products um i

857
00:26:57,840 --> 00:26:58,400
think

858
00:26:58,400 --> 00:27:00,320
maybe the closest thing is if you're

859
00:27:00,320 --> 00:27:02,080
trying to generate a bunch of text to

860
00:27:02,080 --> 00:27:03,279
fool a spam filter

861
00:27:03,279 --> 00:27:05,200
then something like tp3 would be very

862
00:27:05,200 --> 00:27:06,320
good for that

863
00:27:06,320 --> 00:27:07,760
but only up to a certain point because

864
00:27:07,760 --> 00:27:09,440
that attack only works so well

865
00:27:09,440 --> 00:27:11,039
and it depends on how well you build in

866
00:27:11,039 --> 00:27:13,200
your defenses and how well you sanitize

867
00:27:13,200 --> 00:27:14,159
like your inputs

868
00:27:14,159 --> 00:27:21,840
so to speak

869
00:27:35,200 --> 00:27:39,440
all right so i think i found the uh

870
00:27:39,440 --> 00:27:41,679
that thing that i mentioned about open

871
00:27:41,679 --> 00:27:42,640
source projects

872
00:27:42,640 --> 00:27:44,799
um here's the one i was thinking of um

873
00:27:44,799 --> 00:27:45,840
and a couple of other people have

874
00:27:45,840 --> 00:27:47,039
mentioned other really good ones too to

875
00:27:47,039 --> 00:27:47,600
check out

876
00:27:47,600 --> 00:27:50,240
so check out the chat for other things

877
00:27:50,240 --> 00:27:52,880
that people have chaired

878
00:27:55,440 --> 00:27:57,600
so benjamin has a really good point here

879
00:27:57,600 --> 00:27:58,640
he says

880
00:27:58,640 --> 00:28:00,080
i think there is an argument to be made

881
00:28:00,080 --> 00:28:01,679
that larger more complex models like

882
00:28:01,679 --> 00:28:03,440
those used in generating deep fakes are

883
00:28:03,440 --> 00:28:04,799
easier to do gradient-based attacks

884
00:28:04,799 --> 00:28:05,440
against

885
00:28:05,440 --> 00:28:06,720
i would think larger models would be

886
00:28:06,720 --> 00:28:08,320
more robust to model leakage but it'll

887
00:28:08,320 --> 00:28:11,279
have a strong prior there

888
00:28:11,279 --> 00:28:13,600
um yeah this is this is a really great

889
00:28:13,600 --> 00:28:14,840
point

890
00:28:14,840 --> 00:28:17,600
um given that

891
00:28:17,600 --> 00:28:20,159
deep fakes are fundamentally using gans

892
00:28:20,159 --> 00:28:20,640
and

893
00:28:20,640 --> 00:28:22,480
you're generating images they are

894
00:28:22,480 --> 00:28:24,640
definitely um more susceptible to

895
00:28:24,640 --> 00:28:29,919
gradient-based attacks but it's unclear

896
00:28:29,919 --> 00:28:31,200
let's see so there was the deep fake

897
00:28:31,200 --> 00:28:32,720
challenge that facebook put on it was

898
00:28:32,720 --> 00:28:33,600
really cool

899
00:28:33,600 --> 00:28:36,080
um and there is sort of some overlap

900
00:28:36,080 --> 00:28:37,440
between generating deep fakes and

901
00:28:37,440 --> 00:28:40,000
generating ways to detect deep fakes and

902
00:28:40,000 --> 00:28:41,919
ways to get your deep fakes to kind of

903
00:28:41,919 --> 00:28:43,600
fool deep fake detectors

904
00:28:43,600 --> 00:28:45,279
and all that is sort of like it's

905
00:28:45,279 --> 00:28:46,720
turtles all the way down but

906
00:28:46,720 --> 00:28:48,159
it's all very relevant and that's that's

907
00:28:48,159 --> 00:28:49,679
the context one of the very few contexts

908
00:28:49,679 --> 00:28:50,720
in which i think that gradient-based

909
00:28:50,720 --> 00:28:51,760
attacks are extremely

910
00:28:51,760 --> 00:28:54,240
relevant so 10 out of 10 i agree with

911
00:28:54,240 --> 00:28:54,799
that

912
00:28:54,799 --> 00:28:57,360
um as for the larger models being more

913
00:28:57,360 --> 00:28:58,960
robust to model leakage

914
00:28:58,960 --> 00:29:01,919
um that's also sort of intuitively true

915
00:29:01,919 --> 00:29:03,679
as far as we've been able to see

916
00:29:03,679 --> 00:29:06,159
um but we're actively doing some

917
00:29:06,159 --> 00:29:06,720
research

918
00:29:06,720 --> 00:29:08,559
into seeing how much the scale of the

919
00:29:08,559 --> 00:29:10,799
model is impacted by things like the

920
00:29:10,799 --> 00:29:12,000
secret share attack

921
00:29:12,000 --> 00:29:14,640
or some of these other kind of attacks

922
00:29:14,640 --> 00:29:16,000
from the literature that are still very

923
00:29:16,000 --> 00:29:17,279
relevant at some point but they're not

924
00:29:17,279 --> 00:29:18,799
quite relevant yet and we haven't seen

925
00:29:18,799 --> 00:29:27,840
anybody try to use any of these

926
00:29:50,320 --> 00:29:54,159
right i've been told that i can um

927
00:29:54,159 --> 00:29:56,320
leave i'm not sure i want to leave yet

928
00:29:56,320 --> 00:29:58,080
um so if they're

929
00:29:58,080 --> 00:30:00,960
okay i don't have to

930
00:30:02,720 --> 00:30:04,159
yeah so i guess if there are any other

931
00:30:04,159 --> 00:30:05,919
questions this is kind of an unnatural

932
00:30:05,919 --> 00:30:07,679
way of answering questions

933
00:30:07,679 --> 00:30:09,600
and i am aware that my responses are a

934
00:30:09,600 --> 00:30:10,720
little bit disjointed

935
00:30:10,720 --> 00:30:12,559
and i would eagerly like to have more

936
00:30:12,559 --> 00:30:14,240
conversations so please please

937
00:30:14,240 --> 00:30:16,559
reach out to me either email twitter

938
00:30:16,559 --> 00:30:17,919
those are the two things that i

939
00:30:17,919 --> 00:30:19,440
answer i'm not very good at answering

940
00:30:19,440 --> 00:30:22,640
other things um

941
00:30:22,640 --> 00:30:26,320
yeah um i suppose with that

942
00:30:26,320 --> 00:30:29,600
um i can leave although hold on a second

943
00:30:29,600 --> 00:30:31,200
um fernando has a question isn't model

944
00:30:31,200 --> 00:30:32,640
leakage just an issue of not showing

945
00:30:32,640 --> 00:30:34,720
internal scores and the answer is yes

946
00:30:34,720 --> 00:30:36,559
for the most part that is exactly what

947
00:30:36,559 --> 00:30:37,840
it is um

948
00:30:37,840 --> 00:30:39,600
you it's just like standard kind of

949
00:30:39,600 --> 00:30:40,880
trying to prevent people from reverse

950
00:30:40,880 --> 00:30:42,080
engineering your stuff or at least

951
00:30:42,080 --> 00:30:43,440
making it harder to reverse engineer

952
00:30:43,440 --> 00:30:44,240
your stuff

953
00:30:44,240 --> 00:30:46,399
um you don't want to share any sort of

954
00:30:46,399 --> 00:30:47,919
information that makes it easy for

955
00:30:47,919 --> 00:30:48,960
people to figure out what it is that

956
00:30:48,960 --> 00:30:49,520
you've done

957
00:30:49,520 --> 00:30:50,799
and that's not to say that you should

958
00:30:50,799 --> 00:30:53,120
rely on the security by obscurity rule

959
00:30:53,120 --> 00:30:55,200
because we all know that that's

960
00:30:55,200 --> 00:30:59,039
that doesn't work um but it is

961
00:30:59,039 --> 00:31:00,399
good to be prudent about whatever

962
00:31:00,399 --> 00:31:03,360
information it is that you are sharing

963
00:31:03,360 --> 00:31:05,360
okay and with that i'll leave um thank

964
00:31:05,360 --> 00:31:07,120
you very much for coming to my talk

965
00:31:07,120 --> 00:31:10,320
um i hope you enjoyed it um please reach

966
00:31:10,320 --> 00:31:10,720
out

967
00:31:10,720 --> 00:31:16,240
please uh cheers

