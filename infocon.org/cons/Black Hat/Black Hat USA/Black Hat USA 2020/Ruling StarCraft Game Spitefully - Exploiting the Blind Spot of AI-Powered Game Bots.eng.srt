1
00:00:04,240 --> 00:00:14,039
[Music]

2
00:00:15,040 --> 00:00:16,079
today we will be talking

3
00:00:16,079 --> 00:00:19,840
about how to attack gimbals of ai power

4
00:00:19,840 --> 00:00:20,720
games

5
00:00:20,720 --> 00:00:23,680
this is a joint work with my all the

6
00:00:23,680 --> 00:00:24,320
weather

7
00:00:24,320 --> 00:00:27,680
and colleague as we know

8
00:00:27,680 --> 00:00:30,480
deep learning has become the hottest ml

9
00:00:30,480 --> 00:00:31,679
techniques

10
00:00:31,679 --> 00:00:34,320
in the past few years it has dominated

11
00:00:34,320 --> 00:00:35,680
many supervised

12
00:00:35,680 --> 00:00:37,920
and unsupervised learning field in

13
00:00:37,920 --> 00:00:38,879
security

14
00:00:38,879 --> 00:00:41,120
it also outperformed the traditional

15
00:00:41,120 --> 00:00:42,000
methods

16
00:00:42,000 --> 00:00:44,160
in malware detection and intuition

17
00:00:44,160 --> 00:00:45,280
detection

18
00:00:45,280 --> 00:00:47,920
going beyond supervised and unsupervised

19
00:00:47,920 --> 00:00:48,719
learning

20
00:00:48,719 --> 00:00:51,360
reinforcement learning is a more

21
00:00:51,360 --> 00:00:52,640
powerful tool

22
00:00:52,640 --> 00:00:55,280
of learning technique that could handle

23
00:00:55,280 --> 00:00:57,280
more complex tasks

24
00:00:57,280 --> 00:01:00,320
recently researcher combined

25
00:01:00,320 --> 00:01:03,440
aisle with dl and developed different

26
00:01:03,440 --> 00:01:04,319
type of

27
00:01:04,319 --> 00:01:07,600
diio techniques these techniques has

28
00:01:07,600 --> 00:01:08,000
shown

29
00:01:08,000 --> 00:01:10,400
extraordinary performance in many

30
00:01:10,400 --> 00:01:12,240
decision making tasks

31
00:01:12,240 --> 00:01:14,799
such as robotic control autonomous

32
00:01:14,799 --> 00:01:15,600
vehicle

33
00:01:15,600 --> 00:01:19,360
finance and business management

34
00:01:19,360 --> 00:01:23,119
in games agent learn by drl

35
00:01:23,119 --> 00:01:26,240
also be professional or even first

36
00:01:26,240 --> 00:01:29,360
class human players for example

37
00:01:29,360 --> 00:01:32,640
demand alpha goal continual speed first

38
00:01:32,640 --> 00:01:35,840
class goal human goal players more

39
00:01:35,840 --> 00:01:37,680
recently news shows that

40
00:01:37,680 --> 00:01:41,439
besides goal game dil agent also can

41
00:01:41,439 --> 00:01:44,079
could beat professional poker game

42
00:01:44,079 --> 00:01:44,880
players

43
00:01:44,880 --> 00:01:47,439
in different type of poker game such as

44
00:01:47,439 --> 00:01:49,360
texas holdem

45
00:01:49,360 --> 00:01:52,560
last year divment released an open

46
00:01:52,560 --> 00:01:52,960
source

47
00:01:52,960 --> 00:01:55,840
package which includes multiple board

48
00:01:55,840 --> 00:01:57,119
game environment

49
00:01:57,119 --> 00:01:59,840
and a state of art diio learning

50
00:01:59,840 --> 00:02:00,640
technique

51
00:02:00,640 --> 00:02:03,040
so users could learn could train their

52
00:02:03,040 --> 00:02:04,000
own agent

53
00:02:04,000 --> 00:02:06,799
to play with other il agents or even

54
00:02:06,799 --> 00:02:09,440
human player

55
00:02:09,440 --> 00:02:12,400
in addition to both game diio also have

56
00:02:12,400 --> 00:02:13,120
become

57
00:02:13,120 --> 00:02:16,080
the standard method of training a master

58
00:02:16,080 --> 00:02:16,800
agent

59
00:02:16,800 --> 00:02:19,680
in both simulation games and real-time

60
00:02:19,680 --> 00:02:22,000
strategy games

61
00:02:22,000 --> 00:02:24,640
in simulation game open i released a

62
00:02:24,640 --> 00:02:25,520
package

63
00:02:25,520 --> 00:02:28,239
called gym which includes different type

64
00:02:28,239 --> 00:02:29,200
of

65
00:02:29,200 --> 00:02:31,599
simulation game environment such as the

66
00:02:31,599 --> 00:02:33,760
ones shown on the right hand side

67
00:02:33,760 --> 00:02:36,560
the atari game robot school game and

68
00:02:36,560 --> 00:02:39,519
musical games

69
00:02:39,920 --> 00:02:42,400
so the user could test their diy

70
00:02:42,400 --> 00:02:43,360
technique

71
00:02:43,360 --> 00:02:46,720
on all of these environments recent news

72
00:02:46,720 --> 00:02:50,160
also reports reported the effectiveness

73
00:02:50,160 --> 00:02:53,200
of drl in world famous real-time

74
00:02:53,200 --> 00:02:54,480
strategy games

75
00:02:54,480 --> 00:02:57,519
such as starcraft 2 and open

76
00:02:57,519 --> 00:03:00,480
under dota 2.

77
00:03:01,280 --> 00:03:04,159
along with the rapid development of the

78
00:03:04,159 --> 00:03:05,440
il technique

79
00:03:05,440 --> 00:03:08,319
researchers also start to investigate

80
00:03:08,319 --> 00:03:09,920
the security property

81
00:03:09,920 --> 00:03:14,000
of drl especially at a stereo attack

82
00:03:14,000 --> 00:03:16,400
as you may aware there have been many

83
00:03:16,400 --> 00:03:17,200
works

84
00:03:17,200 --> 00:03:19,599
about the adversarial attack on deep

85
00:03:19,599 --> 00:03:20,879
learning

86
00:03:20,879 --> 00:03:22,560
so this attack can be meaning

87
00:03:22,560 --> 00:03:25,040
categorized as training phase attack

88
00:03:25,040 --> 00:03:28,720
and the testing phase attacks since drl

89
00:03:28,720 --> 00:03:31,920
all dio systems also have a deep

90
00:03:31,920 --> 00:03:32,879
learning

91
00:03:32,879 --> 00:03:36,560
involved so intuitively this iridial

92
00:03:36,560 --> 00:03:37,360
exam

93
00:03:37,360 --> 00:03:39,760
should also be vulnerable to the ideal

94
00:03:39,760 --> 00:03:41,040
serial attack

95
00:03:41,040 --> 00:03:44,239
this intuition has been wedded by some

96
00:03:44,239 --> 00:03:45,760
recent works

97
00:03:45,760 --> 00:03:49,599
specifically this work has shown that

98
00:03:49,599 --> 00:03:52,799
the attacker could perturb an agent's

99
00:03:52,799 --> 00:03:54,080
observation

100
00:03:54,080 --> 00:03:57,599
action or rewards and fail the agent

101
00:03:57,599 --> 00:03:59,760
and force the agent to fail the

102
00:03:59,760 --> 00:04:02,720
corresponding tasks

103
00:04:02,720 --> 00:04:05,760
however as we will show later discuss

104
00:04:05,760 --> 00:04:09,599
later these attacks are not practical

105
00:04:09,599 --> 00:04:12,080
meaning because they involve hydrating a

106
00:04:12,080 --> 00:04:13,280
game system

107
00:04:13,280 --> 00:04:16,000
which is time consuming and cannot

108
00:04:16,000 --> 00:04:18,478
always guarantee to be succeed

109
00:04:18,478 --> 00:04:21,680
so in this talk we will present how to

110
00:04:21,680 --> 00:04:22,639
enable

111
00:04:22,639 --> 00:04:25,680
a practical adversarial attack against a

112
00:04:25,680 --> 00:04:26,400
game bot

113
00:04:26,400 --> 00:04:29,199
or master agent in a two-party game

114
00:04:29,199 --> 00:04:31,759
environment

115
00:04:31,840 --> 00:04:34,560
this is the agenda for today's talk we

116
00:04:34,560 --> 00:04:36,479
will start with

117
00:04:36,479 --> 00:04:39,680
some background knowledge of drl

118
00:04:39,680 --> 00:04:41,840
then we'll introduce some air power

119
00:04:41,840 --> 00:04:43,840
games and how to tune

120
00:04:43,840 --> 00:04:47,359
a boat for these games

121
00:04:48,000 --> 00:04:52,080
then we will introduce existing attacks

122
00:04:52,080 --> 00:04:54,840
on dil game bots and discuss their

123
00:04:54,840 --> 00:04:56,080
limitations

124
00:04:56,080 --> 00:04:58,800
based on the limitation of existing work

125
00:04:58,800 --> 00:05:00,320
we will elaborate on

126
00:05:00,320 --> 00:05:03,199
our attack mass knowledge finally we

127
00:05:03,199 --> 00:05:05,120
will show the evaluation results

128
00:05:05,120 --> 00:05:08,400
and conclude our talk

129
00:05:08,639 --> 00:05:12,240
at a high level an io problem

130
00:05:12,240 --> 00:05:15,440
is the decision making problem in

131
00:05:15,440 --> 00:05:19,039
in way specifically it has an agent

132
00:05:19,039 --> 00:05:21,600
who observes and interacts with an

133
00:05:21,600 --> 00:05:22,560
environment

134
00:05:22,560 --> 00:05:25,440
through a series of actions each time

135
00:05:25,440 --> 00:05:26,160
this agent

136
00:05:26,160 --> 00:05:29,840
takes action it will receive a reward

137
00:05:29,840 --> 00:05:33,039
take this entire game as an example

138
00:05:33,039 --> 00:05:36,160
in this game the agent is the blue belt

139
00:05:36,160 --> 00:05:39,199
the environment is just a game itself

140
00:05:39,199 --> 00:05:42,960
at each time step the agent observe the

141
00:05:42,960 --> 00:05:46,000
environment and take action accordingly

142
00:05:46,000 --> 00:05:49,360
at this time the agent probably gonna go

143
00:05:49,360 --> 00:05:49,840
right

144
00:05:49,840 --> 00:05:53,840
and try to catch the ball

145
00:05:55,120 --> 00:05:58,000
after taking this action the agent will

146
00:05:58,000 --> 00:05:59,440
receive a reward

147
00:05:59,440 --> 00:06:02,240
from the environment so in this game the

148
00:06:02,240 --> 00:06:03,759
reward probably be

149
00:06:03,759 --> 00:06:07,039
how many breaks the agent has hit

150
00:06:07,039 --> 00:06:08,960
then the game environment will receive

151
00:06:08,960 --> 00:06:10,400
this action

152
00:06:10,400 --> 00:06:13,440
and translate to the next stick based on

153
00:06:13,440 --> 00:06:15,039
the transition dynamics

154
00:06:15,039 --> 00:06:17,360
this transition dynamic is usually

155
00:06:17,360 --> 00:06:18,080
unknown

156
00:06:18,080 --> 00:06:21,840
to the learning algorithm

157
00:06:21,840 --> 00:06:25,120
solving io problem is equivalent to

158
00:06:25,120 --> 00:06:28,000
training our io agent the goal of this

159
00:06:28,000 --> 00:06:28,720
agent

160
00:06:28,720 --> 00:06:31,360
is to maximize its total amount of

161
00:06:31,360 --> 00:06:33,759
reward

162
00:06:33,759 --> 00:06:36,800
so in this case the goal of an il

163
00:06:36,800 --> 00:06:40,160
algorithm is to learn an optimal policy

164
00:06:40,160 --> 00:06:42,880
follow which the agent could receive a

165
00:06:42,880 --> 00:06:44,479
maximum

166
00:06:44,479 --> 00:06:47,759
maximum amount of rewards over time

167
00:06:47,759 --> 00:06:50,319
so as we can see here the total amount

168
00:06:50,319 --> 00:06:51,280
of reward

169
00:06:51,280 --> 00:06:53,919
is very important for training an ir

170
00:06:53,919 --> 00:06:54,880
agent

171
00:06:54,880 --> 00:06:58,319
in il it is represented by value

172
00:06:58,319 --> 00:06:59,360
function

173
00:06:59,360 --> 00:07:03,120
specifically it has two

174
00:07:03,120 --> 00:07:07,360
forms and our optimal policy

175
00:07:07,360 --> 00:07:10,400
can be obtained by maximize either of

176
00:07:10,400 --> 00:07:12,720
the value functions

177
00:07:12,720 --> 00:07:14,960
take the game on the left hand side as

178
00:07:14,960 --> 00:07:16,319
an example

179
00:07:16,319 --> 00:07:19,759
here the agent who could move within the

180
00:07:19,759 --> 00:07:20,639
check board

181
00:07:20,639 --> 00:07:24,080
and collect reward based on its move

182
00:07:24,080 --> 00:07:26,319
suppose we somehow know the value

183
00:07:26,319 --> 00:07:27,120
function

184
00:07:27,120 --> 00:07:29,759
shown in the figure in the middle so

185
00:07:29,759 --> 00:07:31,680
based on this value function of each

186
00:07:31,680 --> 00:07:32,479
state

187
00:07:32,479 --> 00:07:35,520
the agent will then choose its action

188
00:07:35,520 --> 00:07:38,400
for example on the top left corner the

189
00:07:38,400 --> 00:07:39,599
best move here

190
00:07:39,599 --> 00:07:42,479
is to go right because it will collect

191
00:07:42,479 --> 00:07:43,440
more reward

192
00:07:43,440 --> 00:07:46,240
than going down

193
00:07:46,720 --> 00:07:50,319
in drl an agent is usually modeled

194
00:07:50,319 --> 00:07:53,440
as a deep neural network which is called

195
00:07:53,440 --> 00:07:57,120
policy network this network takes as

196
00:07:57,120 --> 00:08:00,319
input the observation and the output

197
00:08:00,319 --> 00:08:03,599
the corresponding action take the figure

198
00:08:03,599 --> 00:08:05,599
on the right hand top right hand side as

199
00:08:05,599 --> 00:08:07,039
an example

200
00:08:07,039 --> 00:08:10,319
here the agent is the policy network

201
00:08:10,319 --> 00:08:13,599
shown here this network then takes us

202
00:08:13,599 --> 00:08:16,319
input the game snapshot that is

203
00:08:16,319 --> 00:08:17,440
observation

204
00:08:17,440 --> 00:08:20,319
and the output the corresponding action

205
00:08:20,319 --> 00:08:21,840
such as going down

206
00:08:21,840 --> 00:08:25,360
going up down under either up down or

207
00:08:25,360 --> 00:08:29,199
left and right so in this case

208
00:08:29,199 --> 00:08:32,240
learning a policy is equivalent to

209
00:08:32,240 --> 00:08:34,640
solving the parameter of this neural

210
00:08:34,640 --> 00:08:37,120
network

211
00:08:37,919 --> 00:08:40,958
the method that's used to solve the

212
00:08:40,958 --> 00:08:42,399
network parameters

213
00:08:42,399 --> 00:08:45,600
are called policy grading method recall

214
00:08:45,600 --> 00:08:48,800
the goal of a drl algorithm is to

215
00:08:48,800 --> 00:08:52,480
maximize the value function

216
00:08:52,480 --> 00:08:55,920
but surely the value function of

217
00:08:55,920 --> 00:08:58,880
of some like complex scheme are usually

218
00:08:58,880 --> 00:09:00,240
unknown

219
00:09:00,240 --> 00:09:04,320
so the so here in dll

220
00:09:04,320 --> 00:09:07,040
the the policy network a policy reading

221
00:09:07,040 --> 00:09:07,839
method

222
00:09:07,839 --> 00:09:10,240
usually use another network to

223
00:09:10,240 --> 00:09:12,480
approximate the value function

224
00:09:12,480 --> 00:09:15,519
as such in each iteration of the

225
00:09:15,519 --> 00:09:18,720
algorithm it will they will first update

226
00:09:18,720 --> 00:09:19,920
the value

227
00:09:19,920 --> 00:09:22,800
value function network by minimizing the

228
00:09:22,800 --> 00:09:24,720
approximation error

229
00:09:24,720 --> 00:09:27,680
then they will update the policy network

230
00:09:27,680 --> 00:09:28,800
by maximizing

231
00:09:28,800 --> 00:09:32,800
the value function as shown in the

232
00:09:32,800 --> 00:09:36,000
example on the bottom right hand side

233
00:09:36,000 --> 00:09:38,640
these two networks the usually share

234
00:09:38,640 --> 00:09:41,279
parameter

235
00:09:41,600 --> 00:09:44,959
next we will introduce some dil power

236
00:09:44,959 --> 00:09:45,680
games

237
00:09:45,680 --> 00:09:48,480
considered in our work and the code

238
00:09:48,480 --> 00:09:49,440
structure

239
00:09:49,440 --> 00:09:52,080
of training and diio board for these

240
00:09:52,080 --> 00:09:54,480
games

241
00:09:54,959 --> 00:09:57,440
in this work we focus on both the

242
00:09:57,440 --> 00:09:58,800
simulation game

243
00:09:58,800 --> 00:10:01,440
and real-time strategy games for

244
00:10:01,440 --> 00:10:02,720
simulation games

245
00:10:02,720 --> 00:10:05,200
we are talking we are constantly we are

246
00:10:05,200 --> 00:10:06,320
considering

247
00:10:06,320 --> 00:10:10,000
two-party mutual games in this game

248
00:10:10,000 --> 00:10:12,720
the observation usually is usually the

249
00:10:12,720 --> 00:10:14,000
current status

250
00:10:14,000 --> 00:10:16,720
of the environment including the agents

251
00:10:16,720 --> 00:10:18,959
and its components statics

252
00:10:18,959 --> 00:10:21,519
the action of an agent just the agent's

253
00:10:21,519 --> 00:10:22,560
movement

254
00:10:22,560 --> 00:10:24,560
including like moving direction and

255
00:10:24,560 --> 00:10:26,000
moving speed

256
00:10:26,000 --> 00:10:28,560
the reward of a game is the agent's

257
00:10:28,560 --> 00:10:29,600
statics

258
00:10:29,600 --> 00:10:33,440
and a windows condition

259
00:10:33,440 --> 00:10:35,920
going beyond the simulation games we

260
00:10:35,920 --> 00:10:37,120
also consider

261
00:10:37,120 --> 00:10:41,519
a real-time strategy game starcraft 2.

262
00:10:41,519 --> 00:10:44,240
in this game the observation is a

263
00:10:44,240 --> 00:10:46,399
special condition of the map

264
00:10:46,399 --> 00:10:49,040
and the amount of resources has been

265
00:10:49,040 --> 00:10:50,000
collected

266
00:10:50,000 --> 00:10:54,079
by the agent the action are categorized

267
00:10:54,079 --> 00:10:57,519
into four classes building pro

268
00:10:57,519 --> 00:11:01,360
building unit construction

269
00:11:01,360 --> 00:11:04,240
producing units such as workforce and

270
00:11:04,240 --> 00:11:05,360
armies

271
00:11:05,360 --> 00:11:08,160
high-risk resources and finally

272
00:11:08,160 --> 00:11:11,120
attacking enemy

273
00:11:11,760 --> 00:11:15,040
the reward of this game includes

274
00:11:15,040 --> 00:11:17,519
the game statistics and the windows

275
00:11:17,519 --> 00:11:20,079
condition

276
00:11:20,399 --> 00:11:24,320
recall that the policy grading framework

277
00:11:24,320 --> 00:11:27,200
is the standard way of training a dil

278
00:11:27,200 --> 00:11:28,320
agent

279
00:11:28,320 --> 00:11:30,880
following in this framework researchers

280
00:11:30,880 --> 00:11:32,720
have developed a different type of

281
00:11:32,720 --> 00:11:34,399
learning algorithm

282
00:11:34,399 --> 00:11:36,880
among the among this algorithm the

283
00:11:36,880 --> 00:11:38,640
widely used one

284
00:11:38,640 --> 00:11:41,279
in training a gimbals is the ppo

285
00:11:41,279 --> 00:11:42,480
algorithm

286
00:11:42,480 --> 00:11:45,680
here is the augmented workflow of

287
00:11:45,680 --> 00:11:49,120
ppo algorithm first

288
00:11:49,120 --> 00:11:51,680
one need to initialize the network

289
00:11:51,680 --> 00:11:52,800
parameter

290
00:11:52,800 --> 00:11:55,279
for policy network and value function

291
00:11:55,279 --> 00:11:56,880
networks

292
00:11:56,880 --> 00:12:00,000
then in each iteration it first collect

293
00:12:00,000 --> 00:12:01,680
a set of trajectory

294
00:12:01,680 --> 00:12:04,240
by playing the current policy in the

295
00:12:04,240 --> 00:12:06,639
environment

296
00:12:06,639 --> 00:12:09,680
after cracking this in this trajectory

297
00:12:09,680 --> 00:12:12,800
as is mentioned above it will update the

298
00:12:12,800 --> 00:12:14,480
policy network

299
00:12:14,480 --> 00:12:17,040
and the value function network by using

300
00:12:17,040 --> 00:12:17,760
this

301
00:12:17,760 --> 00:12:21,920
trajectories as the code level

302
00:12:21,920 --> 00:12:25,040
take starcraft 2 game as an example

303
00:12:25,040 --> 00:12:27,200
to train a game bot to train a game

304
00:12:27,200 --> 00:12:29,120
board the first

305
00:12:29,120 --> 00:12:31,920
first we need a programmatic game

306
00:12:31,920 --> 00:12:34,079
environment

307
00:12:34,079 --> 00:12:36,880
with the environment backhand the code

308
00:12:36,880 --> 00:12:37,760
structure

309
00:12:37,760 --> 00:12:39,760
is shown in the figure on the right hand

310
00:12:39,760 --> 00:12:40,959
side

311
00:12:40,959 --> 00:12:44,399
so basically we need three major parts

312
00:12:44,399 --> 00:12:47,519
first is the agent which defined on

313
00:12:47,519 --> 00:12:49,920
constructing the two networks the policy

314
00:12:49,920 --> 00:12:50,720
network

315
00:12:50,720 --> 00:12:52,639
and the value function network for the

316
00:12:52,639 --> 00:12:53,760
agent

317
00:12:53,760 --> 00:12:55,920
then we need to write the environment

318
00:12:55,920 --> 00:12:57,279
environment weber

319
00:12:57,279 --> 00:13:00,800
by using the environment package list

320
00:13:00,800 --> 00:13:05,360
above so this environment weber

321
00:13:05,360 --> 00:13:08,480
are used to play the agent in

322
00:13:08,480 --> 00:13:10,399
in the environment and correct the

323
00:13:10,399 --> 00:13:11,760
trajectory

324
00:13:11,760 --> 00:13:14,720
finally the main file used to run the

325
00:13:14,720 --> 00:13:15,200
agent

326
00:13:15,200 --> 00:13:17,519
in the environment and train the

327
00:13:17,519 --> 00:13:18,480
networks

328
00:13:18,480 --> 00:13:21,839
defend above

329
00:13:22,880 --> 00:13:26,800
to train game bots to train on drill

330
00:13:26,800 --> 00:13:27,920
game bots

331
00:13:27,920 --> 00:13:31,839
for two agent or multi-agent game

332
00:13:31,839 --> 00:13:35,120
one standard strategy is the self-play

333
00:13:35,120 --> 00:13:36,800
mechanism

334
00:13:36,800 --> 00:13:39,279
that is training an agent to play

335
00:13:39,279 --> 00:13:40,959
against itself

336
00:13:40,959 --> 00:13:43,760
until the winning rate for both party

337
00:13:43,760 --> 00:13:45,959
are 50

338
00:13:45,959 --> 00:13:48,959
specifically the main field for training

339
00:13:48,959 --> 00:13:49,680
on board

340
00:13:49,680 --> 00:13:53,199
with this strategy is as follows

341
00:13:53,199 --> 00:13:55,839
we first need to define game environment

342
00:13:55,839 --> 00:13:56,720
by using the

343
00:13:56,720 --> 00:14:00,079
environment weapon so

344
00:14:00,079 --> 00:14:02,639
as you can see from the upper right

345
00:14:02,639 --> 00:14:03,519
frame

346
00:14:03,519 --> 00:14:07,120
in right hand side figure then we define

347
00:14:07,120 --> 00:14:08,079
a runner

348
00:14:08,079 --> 00:14:10,959
to run the agent in the environment and

349
00:14:10,959 --> 00:14:13,199
collect the trajectory

350
00:14:13,199 --> 00:14:16,480
finally we define a learned

351
00:14:16,480 --> 00:14:19,360
learner to which receive the trajectory

352
00:14:19,360 --> 00:14:20,079
collected by

353
00:14:20,079 --> 00:14:23,120
runner and update

354
00:14:23,120 --> 00:14:25,440
the policy network and value function

355
00:14:25,440 --> 00:14:27,279
network

356
00:14:27,279 --> 00:14:29,839
in the self-play mechanism it will

357
00:14:29,839 --> 00:14:31,040
iteratively

358
00:14:31,040 --> 00:14:34,160
update the policy for each party until

359
00:14:34,160 --> 00:14:35,199
the winning rate

360
00:14:35,199 --> 00:14:40,719
for both party stabilized at 50 percent

361
00:14:40,880 --> 00:14:44,399
after introducing the basics of drl

362
00:14:44,399 --> 00:14:46,800
and how to train a game board or master

363
00:14:46,800 --> 00:14:47,519
agent

364
00:14:47,519 --> 00:14:50,959
by using a diy algorithm we now proceed

365
00:14:50,959 --> 00:14:51,440
to

366
00:14:51,440 --> 00:14:54,800
how to attack the scheme bots next

367
00:14:54,800 --> 00:14:57,839
cm will introduce the existing attacks

368
00:14:57,839 --> 00:14:58,800
on drl

369
00:14:58,800 --> 00:15:02,399
game bought the existing attacks on deep

370
00:15:02,399 --> 00:15:03,600
reinforcement learning

371
00:15:03,600 --> 00:15:06,480
can be summarized as two categories and

372
00:15:06,480 --> 00:15:08,399
the first one is a probation-based

373
00:15:08,399 --> 00:15:09,120
attacks

374
00:15:09,120 --> 00:15:11,600
the second one is a more practical

375
00:15:11,600 --> 00:15:14,800
adversarial agent attack

376
00:15:14,800 --> 00:15:17,519
firstly i will introduce the probation

377
00:15:17,519 --> 00:15:18,720
based text

378
00:15:18,720 --> 00:15:21,040
as we can see from the right picture

379
00:15:21,040 --> 00:15:23,199
that var0 can either protect

380
00:15:23,199 --> 00:15:25,440
observation and thus force the post

381
00:15:25,440 --> 00:15:27,680
network to output a series of

382
00:15:27,680 --> 00:15:29,440
sub-optimal actions

383
00:15:29,440 --> 00:15:31,920
or directly add perturbations to the

384
00:15:31,920 --> 00:15:35,120
output actions of the poison network

385
00:15:35,120 --> 00:15:38,399
here is a pump game example we can see

386
00:15:38,399 --> 00:15:39,279
from the

387
00:15:39,279 --> 00:15:42,240
bottom image two agent is playing the

388
00:15:42,240 --> 00:15:43,839
pawn with each other

389
00:15:43,839 --> 00:15:46,320
for the current snapshot the optimal

390
00:15:46,320 --> 00:15:47,920
action for the right agent

391
00:15:47,920 --> 00:15:51,120
is done before the attack then attacker

392
00:15:51,120 --> 00:15:53,120
generates perturbations by using the

393
00:15:53,120 --> 00:15:54,240
existing attacks

394
00:15:54,240 --> 00:15:57,279
on different neural networks and as it

395
00:15:57,279 --> 00:16:00,000
to the current observation to induce the

396
00:16:00,000 --> 00:16:00,399
right

397
00:16:00,399 --> 00:16:04,000
agent to output a sub-optimal action

398
00:16:04,000 --> 00:16:07,360
by doing so the right agent may fail

399
00:16:07,360 --> 00:16:10,160
in this game

400
00:16:10,880 --> 00:16:13,120
although the perturbation-based attacks

401
00:16:13,120 --> 00:16:15,199
have achieved some success

402
00:16:15,199 --> 00:16:17,920
it has not limitations in the real-world

403
00:16:17,920 --> 00:16:19,040
setting

404
00:16:19,040 --> 00:16:21,440
in the game set up it requires the

405
00:16:21,440 --> 00:16:22,399
attacker

406
00:16:22,399 --> 00:16:25,279
to hijack the game server to illustrate

407
00:16:25,279 --> 00:16:26,800
this argument

408
00:16:26,800 --> 00:16:28,880
we again take for example the

409
00:16:28,880 --> 00:16:30,880
affirmation online games

410
00:16:30,880 --> 00:16:33,279
in these examples the activities of

411
00:16:33,279 --> 00:16:35,519
manipulating the environment

412
00:16:35,519 --> 00:16:38,240
means that adversary breaks into the

413
00:16:38,240 --> 00:16:39,279
game server

414
00:16:39,279 --> 00:16:41,759
or to the game coder and thus influence

415
00:16:41,759 --> 00:16:42,959
the environment

416
00:16:42,959 --> 00:16:45,600
that the agent interacts with

417
00:16:45,600 --> 00:16:46,800
considering this

418
00:16:46,800 --> 00:16:49,120
it requires professional handlers

419
00:16:49,120 --> 00:16:51,199
tremendous effort and time

420
00:16:51,199 --> 00:16:54,480
what's more the preservation best

421
00:16:54,480 --> 00:16:55,120
attacks

422
00:16:55,120 --> 00:16:57,680
is not a practical setup for beating a

423
00:16:57,680 --> 00:17:01,519
master agent of a two-party game

424
00:17:02,560 --> 00:17:05,280
next i will talk about the adversarial

425
00:17:05,280 --> 00:17:06,480
agent attack

426
00:17:06,480 --> 00:17:08,799
as is shown in the right picture the

427
00:17:08,799 --> 00:17:10,640
attacker is not allowed to

428
00:17:10,640 --> 00:17:12,480
hijack the information flow of the

429
00:17:12,480 --> 00:17:14,160
victim agent

430
00:17:14,160 --> 00:17:15,760
but the attacker could train an

431
00:17:15,760 --> 00:17:17,359
adversarial agent

432
00:17:17,359 --> 00:17:20,480
by playing with the victim agent

433
00:17:20,480 --> 00:17:22,240
compared with the perturbation-based

434
00:17:22,240 --> 00:17:25,119
attack it's more practical in games

435
00:17:25,119 --> 00:17:28,720
since we need not hang the game system

436
00:17:28,720 --> 00:17:30,559
and any player could play with the

437
00:17:30,559 --> 00:17:33,840
master agent freely

438
00:17:35,520 --> 00:17:38,000
there is some existing technique in

439
00:17:38,000 --> 00:17:40,000
adversarial agent attack

440
00:17:40,000 --> 00:17:42,880
in this work it treats the victim agent

441
00:17:42,880 --> 00:17:44,880
as part of the environment

442
00:17:44,880 --> 00:17:47,440
and trains the agent to collect maximum

443
00:17:47,440 --> 00:17:49,600
rewards in the environment

444
00:17:49,600 --> 00:17:52,720
specifically it use the ppo algorithm to

445
00:17:52,720 --> 00:17:54,880
maximize the training agent's value

446
00:17:54,880 --> 00:17:55,679
function

447
00:17:55,679 --> 00:17:58,640
and expect to obtain a policy that could

448
00:17:58,640 --> 00:18:00,240
beat the big team

449
00:18:00,240 --> 00:18:03,280
however as we will shoot later

450
00:18:03,280 --> 00:18:05,919
it cannot establish a high game ring

451
00:18:05,919 --> 00:18:06,720
rate

452
00:18:06,720 --> 00:18:10,160
the reason is that it doesn't explicitly

453
00:18:10,160 --> 00:18:12,160
disturb the victim agent

454
00:18:12,160 --> 00:18:14,640
and the training algorithm has less

455
00:18:14,640 --> 00:18:15,440
guidance

456
00:18:15,440 --> 00:18:18,320
for identifying the weakness of the

457
00:18:18,320 --> 00:18:20,879
victim

458
00:18:21,760 --> 00:18:26,559
a next one we will talk about

459
00:18:27,200 --> 00:18:31,360
as is introduced by xin existing attack

460
00:18:31,360 --> 00:18:34,559
either rely on unrealistic assumptions

461
00:18:34,559 --> 00:18:37,760
or is not able to achieve a decent

462
00:18:37,760 --> 00:18:41,760
attack success rate in the following

463
00:18:41,760 --> 00:18:44,799
we will elaborate on our idea

464
00:18:44,799 --> 00:18:47,520
of training an adversarial agent to

465
00:18:47,520 --> 00:18:48,480
exploit

466
00:18:48,480 --> 00:18:52,240
the weakness of of its opponent

467
00:18:52,240 --> 00:18:57,440
and thus defeat it in a two-party game

468
00:18:57,440 --> 00:19:00,720
our attack's rare model is the same

469
00:19:00,720 --> 00:19:03,280
with side-to-serial attack introduced

470
00:19:03,280 --> 00:19:04,880
before

471
00:19:04,880 --> 00:19:07,360
but to tackle the limitation of this

472
00:19:07,360 --> 00:19:08,799
existing attack

473
00:19:08,799 --> 00:19:12,320
we propose to argument it with new to

474
00:19:12,320 --> 00:19:15,679
two new designs the goal of this design

475
00:19:15,679 --> 00:19:19,200
are the same that is true an agent

476
00:19:19,200 --> 00:19:22,400
to not only maximize its reward

477
00:19:22,400 --> 00:19:26,080
but also prevent its opponent

478
00:19:26,080 --> 00:19:28,480
from collecting more reward at the same

479
00:19:28,480 --> 00:19:29,360
time

480
00:19:29,360 --> 00:19:32,799
so in other words we would like we will

481
00:19:32,799 --> 00:19:34,960
like the idol zero agent

482
00:19:34,960 --> 00:19:38,240
learns to how to perturb or disturb

483
00:19:38,240 --> 00:19:42,400
its opponent so specifically

484
00:19:42,400 --> 00:19:45,840
our first idea is to directly

485
00:19:45,840 --> 00:19:48,880
change the adorable learning objective

486
00:19:48,880 --> 00:19:52,160
to not only maximize its own reward

487
00:19:52,160 --> 00:19:55,600
but also minimize its opponent's reward

488
00:19:55,600 --> 00:19:59,280
at the same time our second idea

489
00:19:59,280 --> 00:20:02,320
is to let the adversarial agent

490
00:20:02,320 --> 00:20:05,360
take the action that

491
00:20:05,360 --> 00:20:10,479
deviates the victim's nest action

492
00:20:10,799 --> 00:20:14,240
to realize the first design recall

493
00:20:14,240 --> 00:20:16,880
that the value function are unknown to

494
00:20:16,880 --> 00:20:18,159
the learning algorithm

495
00:20:18,159 --> 00:20:21,280
so the first step is to approximate

496
00:20:21,280 --> 00:20:24,159
the victim value function with another

497
00:20:24,159 --> 00:20:25,919
neural network

498
00:20:25,919 --> 00:20:29,120
with this approximation by hand with an

499
00:20:29,120 --> 00:20:31,919
id term to the ideal serial learning

500
00:20:31,919 --> 00:20:33,200
objective

501
00:20:33,200 --> 00:20:36,240
this term is used to minimize this

502
00:20:36,240 --> 00:20:37,679
approximated

503
00:20:37,679 --> 00:20:40,480
value function

504
00:20:41,120 --> 00:20:44,240
to give you an example of the difference

505
00:20:44,240 --> 00:20:46,960
between our objective and the existing

506
00:20:46,960 --> 00:20:49,919
attack which only maximizing

507
00:20:49,919 --> 00:20:53,360
the which only maximizes the idle zero

508
00:20:53,360 --> 00:20:54,400
agents

509
00:20:54,400 --> 00:20:58,159
a total expected reward we use the

510
00:20:58,159 --> 00:21:00,080
cracking resources game

511
00:21:00,080 --> 00:21:03,120
as an example in this scheme

512
00:21:03,120 --> 00:21:05,919
the goal of an agent is collect more

513
00:21:05,919 --> 00:21:07,120
resources

514
00:21:07,120 --> 00:21:10,159
than its opponent without

515
00:21:10,159 --> 00:21:13,360
newly added term the adversarial agent

516
00:21:13,360 --> 00:21:16,480
focused only on itself in other words

517
00:21:16,480 --> 00:21:19,520
it will only optimize its policy and try

518
00:21:19,520 --> 00:21:21,679
to collect more rewards

519
00:21:21,679 --> 00:21:25,520
or more resources however if the victim

520
00:21:25,520 --> 00:21:26,159
agent

521
00:21:26,159 --> 00:21:30,240
or opponent or opponent agent

522
00:21:30,240 --> 00:21:33,360
has a better strategy of cracking

523
00:21:33,360 --> 00:21:34,960
resources

524
00:21:34,960 --> 00:21:37,360
it will be extremely hard for the

525
00:21:37,360 --> 00:21:38,960
adversarial agent

526
00:21:38,960 --> 00:21:42,880
learns a better strategy than the

527
00:21:42,880 --> 00:21:46,080
victim and thus beats 18

528
00:21:46,080 --> 00:21:50,240
however with the added term in this case

529
00:21:50,240 --> 00:21:53,919
that was zero agent will learn to

530
00:21:53,919 --> 00:21:56,840
block the victim from collecting more

531
00:21:56,840 --> 00:21:58,640
rewards

532
00:21:58,640 --> 00:22:01,360
so it will have a higher chance to beat

533
00:22:01,360 --> 00:22:02,320
its opponent

534
00:22:02,320 --> 00:22:05,120
no matter how good the opponent's policy

535
00:22:05,120 --> 00:22:07,360
is

536
00:22:07,760 --> 00:22:10,960
as for the second design we first

537
00:22:10,960 --> 00:22:14,000
leverage a model explanation method to

538
00:22:14,000 --> 00:22:15,679
expand the action

539
00:22:15,679 --> 00:22:19,520
of the victim and find out time step

540
00:22:19,520 --> 00:22:22,559
when victim takes action based on the

541
00:22:22,559 --> 00:22:24,960
adversarial agent

542
00:22:24,960 --> 00:22:28,640
then in this critical time steps we will

543
00:22:28,640 --> 00:22:31,919
optimize the adversarial policy to take

544
00:22:31,919 --> 00:22:32,960
an action

545
00:22:32,960 --> 00:22:36,720
that will introduce a maximum deviation

546
00:22:36,720 --> 00:22:40,880
in the victims next action

547
00:22:40,880 --> 00:22:43,600
let me demonstrate this design with the

548
00:22:43,600 --> 00:22:45,200
example here

549
00:22:45,200 --> 00:22:47,679
this is a robot school punk game in

550
00:22:47,679 --> 00:22:48,559
which

551
00:22:48,559 --> 00:22:51,440
the master agent the purple one is

552
00:22:51,440 --> 00:22:52,159
playing

553
00:22:52,159 --> 00:22:55,120
against the zero agent which is the blue

554
00:22:55,120 --> 00:22:56,960
one

555
00:22:56,960 --> 00:23:00,559
recall the observation of an agent

556
00:23:00,559 --> 00:23:03,440
is the current game statics which

557
00:23:03,440 --> 00:23:04,320
includes

558
00:23:04,320 --> 00:23:07,919
its opponent here the red flame

559
00:23:07,919 --> 00:23:11,520
in in this factor indicates that what

560
00:23:11,520 --> 00:23:15,360
that was adversarial part in the victim

561
00:23:15,360 --> 00:23:19,280
observation so this observation

562
00:23:19,280 --> 00:23:21,679
is then given to the victim policy

563
00:23:21,679 --> 00:23:22,799
network

564
00:23:22,799 --> 00:23:26,320
and outputs the corresponding action

565
00:23:26,320 --> 00:23:29,200
with the model explanation method we

566
00:23:29,200 --> 00:23:30,400
could calculate

567
00:23:30,400 --> 00:23:33,840
the importance or inference of each

568
00:23:33,840 --> 00:23:37,919
input dimension on the output prediction

569
00:23:37,919 --> 00:23:41,039
so as such we could use this technique

570
00:23:41,039 --> 00:23:44,400
to identify the inference of the android

571
00:23:44,400 --> 00:23:45,200
serial

572
00:23:45,200 --> 00:23:48,799
on a victim action just the red part

573
00:23:48,799 --> 00:23:51,760
red red part in the victim of

574
00:23:51,760 --> 00:23:53,520
observation

575
00:23:53,520 --> 00:23:56,159
so the difference of the red part on

576
00:23:56,159 --> 00:23:57,279
output

577
00:23:57,279 --> 00:24:00,080
output action

578
00:24:00,720 --> 00:24:04,400
and stack select the time step when i do

579
00:24:04,400 --> 00:24:05,279
a 0

580
00:24:05,279 --> 00:24:09,440
have a large impact on the victim

581
00:24:09,520 --> 00:24:12,559
at this selected time steps we if we

582
00:24:12,559 --> 00:24:13,919
slightly change

583
00:24:13,919 --> 00:24:17,039
the action of the adversary agent it

584
00:24:17,039 --> 00:24:18,080
will also

585
00:24:18,080 --> 00:24:21,679
change the observation of the victim

586
00:24:21,679 --> 00:24:25,360
agent so this small change in the weight

587
00:24:25,360 --> 00:24:28,480
in the victims observation will

588
00:24:28,480 --> 00:24:32,559
then trigger a relatively large change

589
00:24:32,559 --> 00:24:36,080
in the victim in the victims action

590
00:24:36,080 --> 00:24:39,200
so this process is similar to adding

591
00:24:39,200 --> 00:24:40,159
adversarial

592
00:24:40,159 --> 00:24:43,440
probation to a deep learning

593
00:24:43,440 --> 00:24:46,559
deep neural network's input so here

594
00:24:46,559 --> 00:24:49,760
instead of directly change the vector

595
00:24:49,760 --> 00:24:50,640
value

596
00:24:50,640 --> 00:24:53,520
we consider a more practical setup that

597
00:24:53,520 --> 00:24:54,320
is

598
00:24:54,320 --> 00:24:58,159
indirectly change the observation of

599
00:24:58,159 --> 00:24:59,039
victim

600
00:24:59,039 --> 00:25:03,919
by changing the action of the adversary

601
00:25:04,000 --> 00:25:06,799
in this example the trajectory in the

602
00:25:06,799 --> 00:25:08,080
greek hammers

603
00:25:08,080 --> 00:25:10,880
represent the ordinary trajectory so

604
00:25:10,880 --> 00:25:11,440
here

605
00:25:11,440 --> 00:25:15,039
the master agent could go goes towards

606
00:25:15,039 --> 00:25:15,760
the ball

607
00:25:15,760 --> 00:25:18,880
and try to catch it and win again

608
00:25:18,880 --> 00:25:22,080
however if we preterm that was serious

609
00:25:22,080 --> 00:25:23,039
action

610
00:25:23,039 --> 00:25:25,760
that is choose a different action in a

611
00:25:25,760 --> 00:25:27,200
critical time step

612
00:25:27,200 --> 00:25:30,559
selected above at those time step

613
00:25:30,559 --> 00:25:33,200
the victim will then take a different

614
00:25:33,200 --> 00:25:34,080
action

615
00:25:34,080 --> 00:25:37,279
due to the probation

616
00:25:37,840 --> 00:25:41,520
like the trajectory in red canvas here

617
00:25:41,520 --> 00:25:44,559
the purple agent the master agent

618
00:25:44,559 --> 00:25:47,360
no longer move towards the ball due to

619
00:25:47,360 --> 00:25:48,640
the probation

620
00:25:48,640 --> 00:25:51,200
as such it cannot catch the ball and

621
00:25:51,200 --> 00:25:54,159
will lose the game

622
00:25:54,320 --> 00:25:56,480
after introducing our attack mass

623
00:25:56,480 --> 00:25:57,520
knowledge

624
00:25:57,520 --> 00:26:01,279
we now evaluate it on some static games

625
00:26:01,279 --> 00:26:05,200
and show our interesting findings

626
00:26:05,200 --> 00:26:08,400
specifically we choose five games four

627
00:26:08,400 --> 00:26:09,200
of which

628
00:26:09,200 --> 00:26:12,320
are from musical game and one is real

629
00:26:12,320 --> 00:26:12,880
world

630
00:26:12,880 --> 00:26:16,159
strategic starcraft 2 game as for mutual

631
00:26:16,159 --> 00:26:16,799
games

632
00:26:16,799 --> 00:26:19,840
the demonstration of the static game are

633
00:26:19,840 --> 00:26:21,840
shown on the right hand side

634
00:26:21,840 --> 00:26:25,039
the first game k khan defend is just a

635
00:26:25,039 --> 00:26:26,400
standard

636
00:26:26,400 --> 00:26:29,760
penalty shootout in second you should

637
00:26:29,760 --> 00:26:30,960
not pass game

638
00:26:30,960 --> 00:26:34,080
the runner the blue agent is running

639
00:26:34,080 --> 00:26:37,360
towards the red line the finish line

640
00:26:37,360 --> 00:26:40,159
behind the red agent then the blocker

641
00:26:40,159 --> 00:26:41,360
the red agent

642
00:26:41,360 --> 00:26:44,720
in turns to block the blue agent

643
00:26:44,720 --> 00:26:48,320
from crossing the line in both human

644
00:26:48,320 --> 00:26:48,880
arms

645
00:26:48,880 --> 00:26:52,080
and human human games the two agents

646
00:26:52,080 --> 00:26:54,720
are fighting against each other on our

647
00:26:54,720 --> 00:26:55,600
arena

648
00:26:55,600 --> 00:26:59,279
and try to push its opponent

649
00:26:59,279 --> 00:27:03,039
off of the arena

650
00:27:04,159 --> 00:27:06,960
as for the starcraft 2 game we consider

651
00:27:06,960 --> 00:27:08,559
a two-party scenario

652
00:27:08,559 --> 00:27:11,840
verzac versus zlac

653
00:27:11,840 --> 00:27:15,279
recall a drl algorithm iteratively

654
00:27:15,279 --> 00:27:18,240
updates the policy with a matter and

655
00:27:18,240 --> 00:27:18,880
report

656
00:27:18,880 --> 00:27:21,279
the winning rate of the adversarial

657
00:27:21,279 --> 00:27:22,080
agent

658
00:27:22,080 --> 00:27:25,600
each time its policy is updated

659
00:27:25,600 --> 00:27:28,959
during the training process

660
00:27:29,200 --> 00:27:31,600
here is the comparison of the reading

661
00:27:31,600 --> 00:27:32,480
rates

662
00:27:32,480 --> 00:27:35,120
of the adversarial agent obtained by

663
00:27:35,120 --> 00:27:35,520
different

664
00:27:35,520 --> 00:27:38,559
attacks the red line is

665
00:27:38,559 --> 00:27:41,360
refers to over attack and blue line

666
00:27:41,360 --> 00:27:42,399
refers to the

667
00:27:42,399 --> 00:27:46,480
existing attack as we can observe

668
00:27:46,480 --> 00:27:49,120
from these figures our attack

669
00:27:49,120 --> 00:27:50,159
outperformed

670
00:27:50,159 --> 00:27:53,760
the existing attack on most games

671
00:27:53,760 --> 00:27:56,480
the only express exception is the

672
00:27:56,480 --> 00:27:58,240
swimmer arms game

673
00:27:58,240 --> 00:28:01,760
in this game both attack cannot improve

674
00:28:01,760 --> 00:28:02,159
the

675
00:28:02,159 --> 00:28:05,440
tax success rate of the adversarial

676
00:28:05,440 --> 00:28:07,360
agent

677
00:28:07,360 --> 00:28:10,399
the reason is that the observation in

678
00:28:10,399 --> 00:28:11,279
this game

679
00:28:11,279 --> 00:28:14,720
is of low dimensionality which means

680
00:28:14,720 --> 00:28:17,919
the probation space is relatively small

681
00:28:17,919 --> 00:28:21,360
as such it's kind of hard to disturb the

682
00:28:21,360 --> 00:28:22,159
victim

683
00:28:22,159 --> 00:28:25,120
where the adversarial action and the

684
00:28:25,120 --> 00:28:26,480
defeat victim

685
00:28:26,480 --> 00:28:29,600
accordingly however as

686
00:28:29,600 --> 00:28:33,279
is true in the figure in the below

687
00:28:33,279 --> 00:28:36,720
or attack improves the non-lose rate

688
00:28:36,720 --> 00:28:39,840
of the adversary agent as we will

689
00:28:39,840 --> 00:28:41,200
explanate her

690
00:28:41,200 --> 00:28:44,240
dispatch cannot influence the action

691
00:28:44,240 --> 00:28:47,600
of the victim agent our attack

692
00:28:47,600 --> 00:28:49,919
could give the adversarial agent some

693
00:28:49,919 --> 00:28:51,200
advantages

694
00:28:51,200 --> 00:28:54,559
by exploit exploiting the

695
00:28:54,559 --> 00:28:59,440
vulnerability of the game design

696
00:28:59,440 --> 00:29:01,279
after showing the quantitatively

697
00:29:01,279 --> 00:29:02,640
evaluation

698
00:29:02,640 --> 00:29:05,679
we now show some game episodes

699
00:29:05,679 --> 00:29:08,960
of our adversarial agent play against

700
00:29:08,960 --> 00:29:13,840
the victim

701
00:29:15,039 --> 00:29:18,480
first as we can see from the kikhan

702
00:29:18,480 --> 00:29:19,279
defend

703
00:29:19,279 --> 00:29:22,799
and usual now pass game

704
00:29:22,960 --> 00:29:28,080
our attack could exploit the victim

705
00:29:28,960 --> 00:29:32,080
the weakness of the victim agent

706
00:29:32,080 --> 00:29:36,799
by establishing some weird behaviors

707
00:29:37,520 --> 00:29:41,840
in human human video

708
00:29:49,440 --> 00:29:51,679
sorry

709
00:29:53,120 --> 00:29:56,799
human human video all attack could help

710
00:29:56,799 --> 00:29:59,360
learn stress learn a better strategy

711
00:29:59,360 --> 00:30:00,320
that is

712
00:30:00,320 --> 00:30:03,520
initialize itself near the boundary

713
00:30:03,520 --> 00:30:07,200
and lower the victim to

714
00:30:07,200 --> 00:30:10,559
attacking it and fell from the

715
00:30:10,559 --> 00:30:13,039
arena

716
00:30:16,880 --> 00:30:19,279
more interestingly in the summer on

717
00:30:19,279 --> 00:30:20,000
scheme

718
00:30:20,000 --> 00:30:22,399
we can say that it will seriously

719
00:30:22,399 --> 00:30:23,440
intentionally

720
00:30:23,440 --> 00:30:26,559
flow from the arena

721
00:30:30,480 --> 00:30:33,919
the adult zero intentionally fall from

722
00:30:33,919 --> 00:30:37,200
the arena of the game beginning in this

723
00:30:37,200 --> 00:30:37,840
case

724
00:30:37,840 --> 00:30:41,520
the game ends up with a draw this shows

725
00:30:41,520 --> 00:30:44,240
that our attack could explore the

726
00:30:44,240 --> 00:30:45,039
victimly

727
00:30:45,039 --> 00:30:48,480
the weakness of the game rule because

728
00:30:48,480 --> 00:30:50,080
one of the rules is that

729
00:30:50,080 --> 00:30:52,960
if one player or one party falls from

730
00:30:52,960 --> 00:30:53,919
the arena

731
00:30:53,919 --> 00:30:57,360
without touching its opponent the game

732
00:30:57,360 --> 00:31:00,159
ends up with a jaw to hear that or

733
00:31:00,159 --> 00:31:01,840
serial agent

734
00:31:01,840 --> 00:31:04,880
you use this rule to force the game

735
00:31:04,880 --> 00:31:08,159
ends up with the draw

736
00:31:09,120 --> 00:31:11,519
in addition to the attack we also

737
00:31:11,519 --> 00:31:13,760
consider a possible defense

738
00:31:13,760 --> 00:31:17,679
that widely used that is widely used

739
00:31:17,679 --> 00:31:20,799
the adversary training strategy

740
00:31:20,799 --> 00:31:23,840
specifically we play the victim agent

741
00:31:23,840 --> 00:31:27,919
with our trained and also adversarial

742
00:31:27,919 --> 00:31:29,120
agent

743
00:31:29,120 --> 00:31:32,559
and return the victim agent with our

744
00:31:32,559 --> 00:31:34,799
proposed attack

745
00:31:34,799 --> 00:31:37,360
the results are showing the figure in

746
00:31:37,360 --> 00:31:39,200
this slide

747
00:31:39,200 --> 00:31:42,559
as we can observe from the figure as the

748
00:31:42,559 --> 00:31:43,279
adverse

749
00:31:43,279 --> 00:31:46,320
theory training process indeed improve

750
00:31:46,320 --> 00:31:49,360
the performance of the victim by winning

751
00:31:49,360 --> 00:31:51,120
the usual pass game

752
00:31:51,120 --> 00:31:53,600
and achieving a draw on the other three

753
00:31:53,600 --> 00:31:55,200
games

754
00:31:55,200 --> 00:31:58,720
but however it doesn't work on kick hand

755
00:31:58,720 --> 00:32:00,240
defense game

756
00:32:00,240 --> 00:32:03,519
we suspect this is caused by the

757
00:32:03,519 --> 00:32:04,480
unfairness

758
00:32:04,480 --> 00:32:07,039
of the game design because we have tried

759
00:32:07,039 --> 00:32:07,600
that

760
00:32:07,600 --> 00:32:10,720
it's relatively hard for the kicker to

761
00:32:10,720 --> 00:32:11,519
earn the

762
00:32:11,519 --> 00:32:15,279
win the game in general

763
00:32:15,279 --> 00:32:18,399
here are some videos of using the return

764
00:32:18,399 --> 00:32:22,559
victim agent play against the adversary

765
00:32:22,559 --> 00:32:25,120
in the first video the usual now pass

766
00:32:25,120 --> 00:32:25,760
game

767
00:32:25,760 --> 00:32:28,000
the victim learns to ignore the

768
00:32:28,000 --> 00:32:29,120
adversarial

769
00:32:29,120 --> 00:32:31,840
and directly go for the finish line

770
00:32:31,840 --> 00:32:32,640
which means

771
00:32:32,640 --> 00:32:36,080
the victim agent will be aware of his

772
00:32:36,080 --> 00:32:38,960
policy weakness and patched awakenings

773
00:32:38,960 --> 00:32:41,519
through the retraining process

774
00:32:41,519 --> 00:32:44,080
in the second video the victim

775
00:32:44,080 --> 00:32:45,519
recognized the trick

776
00:32:45,519 --> 00:32:48,240
played by the adult zero it will stay

777
00:32:48,240 --> 00:32:48,799
where it

778
00:32:48,799 --> 00:32:52,399
is so the game will end up with the jaw

779
00:32:52,399 --> 00:32:55,279
so which means in this human human game

780
00:32:55,279 --> 00:32:56,320
the victim

781
00:32:56,320 --> 00:32:58,960
also realized its policy weakness and

782
00:32:58,960 --> 00:32:59,760
tried to fix

783
00:32:59,760 --> 00:33:03,919
it in the summer arms game

784
00:33:03,919 --> 00:33:06,480
since the victim cannot change the

785
00:33:06,480 --> 00:33:07,440
intentional

786
00:33:07,440 --> 00:33:10,799
intentional behavior of the adversary

787
00:33:10,799 --> 00:33:13,840
so the returning process basically

788
00:33:13,840 --> 00:33:17,039
still keeps the original behavior

789
00:33:17,039 --> 00:33:20,159
of either victim both victim and

790
00:33:20,159 --> 00:33:23,840
adversarial so the game still staying as

791
00:33:23,840 --> 00:33:25,760
thai games

792
00:33:25,760 --> 00:33:28,720
finally in the k can't defend games as

793
00:33:28,720 --> 00:33:30,000
we mentioned above

794
00:33:30,000 --> 00:33:33,200
the victim acts even worse it's more

795
00:33:33,200 --> 00:33:34,080
easier

796
00:33:34,080 --> 00:33:37,120
to fall into the ground trigger it

797
00:33:37,120 --> 00:33:39,840
to lose the game

798
00:33:43,919 --> 00:33:47,120
finally we conclude this talk with three

799
00:33:47,120 --> 00:33:48,559
conclusions

800
00:33:48,559 --> 00:33:51,760
first attacker could train an adult

801
00:33:51,760 --> 00:33:52,960
serial agent

802
00:33:52,960 --> 00:33:56,159
to defeat a gimbal for

803
00:33:56,159 --> 00:33:59,360
for an air power game but disturb

804
00:33:59,360 --> 00:34:02,080
we also found that by disturbing the

805
00:34:02,080 --> 00:34:03,440
victim action

806
00:34:03,440 --> 00:34:06,240
that was zero agent could exploit the

807
00:34:06,240 --> 00:34:07,440
vulnerability

808
00:34:07,440 --> 00:34:10,320
of both the game victim agent and the

809
00:34:10,320 --> 00:34:11,199
game rule

810
00:34:11,199 --> 00:34:14,000
and thus fill the victim agent more

811
00:34:14,000 --> 00:34:14,480
effect

812
00:34:14,480 --> 00:34:18,239
more effectively finally we show that

813
00:34:18,239 --> 00:34:21,119
adversarial training does not always

814
00:34:21,119 --> 00:34:22,480
succeed

815
00:34:22,480 --> 00:34:25,359
we may need more advanced techniques to

816
00:34:25,359 --> 00:34:26,079
protect

817
00:34:26,079 --> 00:34:30,320
the game boss and master agent

818
00:34:30,800 --> 00:34:32,960
thank you very much for your attention

819
00:34:32,960 --> 00:34:34,159
we now open up

820
00:34:34,159 --> 00:34:37,760
to answer some questions

821
00:34:39,918 --> 00:34:43,520
okay hello hello everyone i'm wilbur

822
00:34:43,520 --> 00:34:45,359
thanks everyone for attending our

823
00:34:45,359 --> 00:34:47,199
session

824
00:34:47,199 --> 00:34:49,440
since at the beginning of this session

825
00:34:49,440 --> 00:34:51,440
the self introduction was missing so

826
00:34:51,440 --> 00:34:52,800
i'll just do a

827
00:34:52,800 --> 00:34:56,159
uh reintroduction here so i'm one phd

828
00:34:56,159 --> 00:34:57,680
student at penn state

829
00:34:57,680 --> 00:34:59,839
so this work is a joint work with my

830
00:34:59,839 --> 00:35:01,359
advisor she

831
00:35:01,359 --> 00:35:04,480
and my colleague shia and jimmy

832
00:35:04,480 --> 00:35:07,119
so the main speaker of this uh of this

833
00:35:07,119 --> 00:35:08,320
talk

834
00:35:08,320 --> 00:35:11,119
happened those was me and xian took a

835
00:35:11,119 --> 00:35:12,400
small part of it

836
00:35:12,400 --> 00:35:16,000
so the speakers are

837
00:35:16,240 --> 00:35:18,240
so before i answer any questions there

838
00:35:18,240 --> 00:35:20,720
there are some questions in the

839
00:35:20,720 --> 00:35:24,320
question chat box i just want to clarify

840
00:35:24,320 --> 00:35:27,440
some things okay first this work is

841
00:35:27,440 --> 00:35:28,880
about attacking

842
00:35:28,880 --> 00:35:31,440
a master agent you know deep reinforced

843
00:35:31,440 --> 00:35:32,400
learning

844
00:35:32,400 --> 00:35:36,240
uh uh like environment which i say

845
00:35:36,240 --> 00:35:38,400
we have like so many online games some

846
00:35:38,400 --> 00:35:40,800
of you may aware some like news

847
00:35:40,800 --> 00:35:44,000
uh talking about ai both

848
00:35:44,000 --> 00:35:47,599
ai game bot beating professional uh

849
00:35:47,599 --> 00:35:50,240
like a player in dota 2 star part 2

850
00:35:50,240 --> 00:35:51,920
those type of games

851
00:35:51,920 --> 00:35:55,200
and with the success of ai and the

852
00:35:55,200 --> 00:35:57,040
like deep reinforcement in base game

853
00:35:57,040 --> 00:35:59,599
balls more and more game are using this

854
00:35:59,599 --> 00:35:59,920
top

855
00:35:59,920 --> 00:36:02,560
both instead of like rule-based sports

856
00:36:02,560 --> 00:36:04,320
as their master agent

857
00:36:04,320 --> 00:36:06,160
maybe there is some agent trained by

858
00:36:06,160 --> 00:36:07,520
deep enforcement

859
00:36:07,520 --> 00:36:10,160
the goal of our attack is to attack such

860
00:36:10,160 --> 00:36:12,400
type of

861
00:36:12,400 --> 00:36:16,960
and so currently uh we have some like uh

862
00:36:16,960 --> 00:36:19,040
we will like about the code of this

863
00:36:19,040 --> 00:36:20,800
attack will open something like

864
00:36:20,800 --> 00:36:22,160
the currently we are still under

865
00:36:22,160 --> 00:36:24,560
construction i it will be open source

866
00:36:24,560 --> 00:36:25,119
like

867
00:36:25,119 --> 00:36:28,000
in like one month i think we will do

868
00:36:28,000 --> 00:36:30,640
like as soon as possible

869
00:36:30,640 --> 00:36:32,640
okay so i've received some questions in

870
00:36:32,640 --> 00:36:33,680
the

871
00:36:33,680 --> 00:36:36,800
in the chat box the first question

872
00:36:36,800 --> 00:36:38,160
i think there are like very good

873
00:36:38,160 --> 00:36:40,480
questions the first is about

874
00:36:40,480 --> 00:36:43,200
uh whether this attack to hunter asked

875
00:36:43,200 --> 00:36:44,000
about this

876
00:36:44,000 --> 00:36:45,599
whether it's your type just for

877
00:36:45,599 --> 00:36:47,520
multi-party game or

878
00:36:47,520 --> 00:36:50,640
or like non-cooperative games so that's

879
00:36:50,640 --> 00:36:51,280
correct

880
00:36:51,280 --> 00:36:54,480
our attack is only only works in the

881
00:36:54,480 --> 00:36:55,760
multi-party

882
00:36:55,760 --> 00:36:58,240
competitive game scenario so to be more

883
00:36:58,240 --> 00:36:59,119
specific

884
00:36:59,119 --> 00:37:01,839
it all currently will only do two party

885
00:37:01,839 --> 00:37:02,480
game

886
00:37:02,480 --> 00:37:04,560
so we would like to extend the game to

887
00:37:04,560 --> 00:37:07,119
multiplayer game in the future

888
00:37:07,119 --> 00:37:10,320
and he also asked about the idea of this

889
00:37:10,320 --> 00:37:11,040
attack

890
00:37:11,040 --> 00:37:14,160
is to affect the learning of the victim

891
00:37:14,160 --> 00:37:17,440
so actually as i just mentioned earlier

892
00:37:17,440 --> 00:37:20,480
since we are attacking master agent

893
00:37:20,480 --> 00:37:24,079
so here our victim agent in our scenario

894
00:37:24,079 --> 00:37:27,119
uh its policy is prediction which means

895
00:37:27,119 --> 00:37:29,040
that during the attack process

896
00:37:29,040 --> 00:37:31,359
the policy of the victim region is not

897
00:37:31,359 --> 00:37:32,079
learned

898
00:37:32,079 --> 00:37:34,400
for our attacking just to learn our own

899
00:37:34,400 --> 00:37:35,200
agent

900
00:37:35,200 --> 00:37:38,800
and try to prepare or like inference

901
00:37:38,800 --> 00:37:42,000
the action of the victim for they

902
00:37:42,000 --> 00:37:45,520
to make some wrong choices and thus

903
00:37:45,520 --> 00:37:47,760
lose again so we are not affects the

904
00:37:47,760 --> 00:37:49,599
learning process of the routine

905
00:37:49,599 --> 00:37:52,400
while affecting the actions of the

906
00:37:52,400 --> 00:37:53,280
victim agent

907
00:37:53,280 --> 00:37:58,000
during the game so another question

908
00:37:58,000 --> 00:38:00,480
is about are you trying to train boss

909
00:38:00,480 --> 00:38:02,839
model adversary so that's also a good

910
00:38:02,839 --> 00:38:04,400
question so

911
00:38:04,400 --> 00:38:07,520
uh as i mentioned earlier uh during the

912
00:38:07,520 --> 00:38:08,960
attacking process

913
00:38:08,960 --> 00:38:12,079
we are only we only put one uh one model

914
00:38:12,079 --> 00:38:13,760
that's the adversarial model so you have

915
00:38:13,760 --> 00:38:14,720
two parties

916
00:38:14,720 --> 00:38:17,359
the written party and the zero part the

917
00:38:17,359 --> 00:38:18,400
written part is

918
00:38:18,400 --> 00:38:21,280
frequent so during the tight per attack

919
00:38:21,280 --> 00:38:21,920
process

920
00:38:21,920 --> 00:38:25,119
we only train our serial model to be

921
00:38:25,119 --> 00:38:28,079
try to beat at the victim model but we

922
00:38:28,079 --> 00:38:29,040
also try

923
00:38:29,040 --> 00:38:31,040
to defense this attack so during the

924
00:38:31,040 --> 00:38:32,320
defense attack

925
00:38:32,320 --> 00:38:34,480
uh difference process we do the things

926
00:38:34,480 --> 00:38:35,520
in return

927
00:38:35,520 --> 00:38:37,839
is that we fix that the adversarial

928
00:38:37,839 --> 00:38:38,720
model and

929
00:38:38,720 --> 00:38:41,119
try to train the victim model see if the

930
00:38:41,119 --> 00:38:42,079
looking model

931
00:38:42,079 --> 00:38:44,640
can learn to beat the angular 7 model

932
00:38:44,640 --> 00:38:45,359
again

933
00:38:45,359 --> 00:38:47,920
so just like at one time we only found

934
00:38:47,920 --> 00:38:50,400
one mode

935
00:38:53,680 --> 00:38:57,200
so another uh good question is about

936
00:38:57,200 --> 00:39:00,400
how to transfer the attack to interface

937
00:39:00,400 --> 00:39:03,760
uh of such api in the real world uh

938
00:39:03,760 --> 00:39:05,760
that's also a good good question so

939
00:39:05,760 --> 00:39:08,320
currently we are doing starcraft 2

940
00:39:08,320 --> 00:39:11,359
which means like if there is an api

941
00:39:11,359 --> 00:39:13,920
a row api about starcraft 2 like online

942
00:39:13,920 --> 00:39:14,480
game

943
00:39:14,480 --> 00:39:16,400
so we could do that we can directly

944
00:39:16,400 --> 00:39:18,240
apply our tag to default

945
00:39:18,240 --> 00:39:20,400
the master agent in the start of the pin

946
00:39:20,400 --> 00:39:22,000
because start pass 2 is real

947
00:39:22,000 --> 00:39:25,359
game and there are also some

948
00:39:25,359 --> 00:39:28,240
other online apis for other games such

949
00:39:28,240 --> 00:39:30,079
as go gaming poker game

950
00:39:30,079 --> 00:39:32,640
type like different type of poker games

951
00:39:32,640 --> 00:39:33,280
so

952
00:39:33,280 --> 00:39:35,599
uh the ongoing world cup focus on how to

953
00:39:35,599 --> 00:39:36,560
external

954
00:39:36,560 --> 00:39:39,599
current attack on the starcraft two

955
00:39:39,599 --> 00:39:40,079
games

956
00:39:40,079 --> 00:39:42,400
for those type of games so if we could

957
00:39:42,400 --> 00:39:43,440
like do

958
00:39:43,440 --> 00:39:45,520
build a bridge between starcraft 2 and

959
00:39:45,520 --> 00:39:46,720
go and program

960
00:39:46,720 --> 00:39:49,599
some other games as all could be

961
00:39:49,599 --> 00:39:50,720
potentially

962
00:39:50,720 --> 00:39:57,359
generalized to those type of apis

963
00:39:57,359 --> 00:40:00,160
uh actually that's all the questions i

964
00:40:00,160 --> 00:40:01,119
receive in the

965
00:40:01,119 --> 00:40:04,800
chat box yes everyone for your time

966
00:40:04,800 --> 00:40:07,119
and the if you have any other questions

967
00:40:07,119 --> 00:40:08,000
that want to like

968
00:40:08,000 --> 00:40:10,240
talk more about this works you can

969
00:40:10,240 --> 00:40:11,280
contact me with

970
00:40:11,280 --> 00:40:14,800
any ways you prefer thanks thanks sarah

971
00:40:14,800 --> 00:40:18,160
for your time

