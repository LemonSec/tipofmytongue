1
00:00:02,880 --> 00:00:14,799
[Music]

2
00:00:14,799 --> 00:00:15,839
my name is ali

3
00:00:15,839 --> 00:00:17,760
i work at google where i live the

4
00:00:17,760 --> 00:00:20,000
security and anti-abuse research team

5
00:00:20,000 --> 00:00:21,760
today we're going to talk about how to

6
00:00:21,760 --> 00:00:23,840
use machine learning to reduce

7
00:00:23,840 --> 00:00:27,359
sectional attack attack surface

8
00:00:27,359 --> 00:00:28,960
before getting started it is worth

9
00:00:28,960 --> 00:00:30,720
mentioning that this talk is a

10
00:00:30,720 --> 00:00:32,000
joint collaboration with many

11
00:00:32,000 --> 00:00:34,800
collaborators around the idea of how we

12
00:00:34,800 --> 00:00:35,280
can

13
00:00:35,280 --> 00:00:37,280
harden hardware cryptography to create

14
00:00:37,280 --> 00:00:39,760
more secure devices

15
00:00:39,760 --> 00:00:42,160
at this time of recording which is july

16
00:00:42,160 --> 00:00:42,960
14

17
00:00:42,960 --> 00:00:45,840
happy bastille day the results are still

18
00:00:45,840 --> 00:00:47,280
a little bit experimental all you're

19
00:00:47,280 --> 00:00:47,920
going to see

20
00:00:47,920 --> 00:00:50,079
is working but i expect by the time you

21
00:00:50,079 --> 00:00:51,920
will see this recording at blackout

22
00:00:51,920 --> 00:00:54,000
the result will be way better i hope we

23
00:00:54,000 --> 00:00:55,600
get a chance to discuss about

24
00:00:55,600 --> 00:00:57,120
recent advance between the time of the

25
00:00:57,120 --> 00:00:59,760
recording and the blackout during our q

26
00:00:59,760 --> 00:01:02,079
a session

27
00:01:02,079 --> 00:01:05,119
so side channel is one of the most

28
00:01:05,119 --> 00:01:07,119
efficient way to attack secure hardware

29
00:01:07,119 --> 00:01:09,439
because it does not attack the algorithm

30
00:01:09,439 --> 00:01:11,600
which is well vetted but instead target

31
00:01:11,600 --> 00:01:12,080
the

32
00:01:12,080 --> 00:01:14,080
implementation of it which is less

33
00:01:14,080 --> 00:01:15,920
vetted either because there are many of

34
00:01:15,920 --> 00:01:16,400
them

35
00:01:16,400 --> 00:01:18,159
and also because some of them are kept

36
00:01:18,159 --> 00:01:20,320
secret uh to ensure that they are

37
00:01:20,320 --> 00:01:22,640
compliant with certifications

38
00:01:22,640 --> 00:01:25,040
an example of how powerful session

39
00:01:25,040 --> 00:01:26,159
attack maybe

40
00:01:26,159 --> 00:01:29,119
is for example this attack which

41
00:01:29,119 --> 00:01:31,200
occurred a few years back where

42
00:01:31,200 --> 00:01:33,040
people were able to lift out from a

43
00:01:33,040 --> 00:01:35,759
trezor which is a hardware bitcoin

44
00:01:35,759 --> 00:01:37,920
wallet the private key of bitcoin

45
00:01:37,920 --> 00:01:39,040
wallets

46
00:01:39,040 --> 00:01:40,799
they show you how powerful it can be and

47
00:01:40,799 --> 00:01:42,240
how devastating

48
00:01:42,240 --> 00:01:44,799
having a such an attack can be for

49
00:01:44,799 --> 00:01:47,600
secure hardware

50
00:01:47,600 --> 00:01:48,960
one of the problem when you develop

51
00:01:48,960 --> 00:01:51,200
software and when you develop hard

52
00:01:51,200 --> 00:01:52,640
when you develop hardware compared to

53
00:01:52,640 --> 00:01:55,040
software is that debugging it is harder

54
00:01:55,040 --> 00:01:55,759
because

55
00:01:55,759 --> 00:01:58,479
we have to find where the sectional

56
00:01:58,479 --> 00:01:59,040
attack

57
00:01:59,040 --> 00:02:00,560
came from which means where is the link

58
00:02:00,560 --> 00:02:02,240
in the into the code

59
00:02:02,240 --> 00:02:03,840
and this leak is usually due to the

60
00:02:03,840 --> 00:02:05,280
interplay between the software

61
00:02:05,280 --> 00:02:07,040
and the hardware and so you have to look

62
00:02:07,040 --> 00:02:08,639
at both of them at the same time to get

63
00:02:08,639 --> 00:02:09,598
an idea

64
00:02:09,598 --> 00:02:12,000
of what might be leaking this is what

65
00:02:12,000 --> 00:02:13,599
makes it so hard

66
00:02:13,599 --> 00:02:17,280
so if we were to have a

67
00:02:17,280 --> 00:02:19,680
more secure or the most secure hardware

68
00:02:19,680 --> 00:02:20,800
possible

69
00:02:20,800 --> 00:02:23,120
we need to help reduce the cost of

70
00:02:23,120 --> 00:02:24,239
finding the attack

71
00:02:24,239 --> 00:02:25,520
and being able to pinpoint them

72
00:02:25,520 --> 00:02:27,280
accurately this is why we ask the

73
00:02:27,280 --> 00:02:28,080
question

74
00:02:28,080 --> 00:02:30,800
is there a way to create a debugger that

75
00:02:30,800 --> 00:02:32,879
will help us to pinpoint accurately and

76
00:02:32,879 --> 00:02:34,080
quickly

77
00:02:34,080 --> 00:02:35,840
what is the part of the code which is

78
00:02:35,840 --> 00:02:37,519
interacting in a way with the hardware

79
00:02:37,519 --> 00:02:38,959
which make it vulnerable to size channel

80
00:02:38,959 --> 00:02:39,920
attacks

81
00:02:39,920 --> 00:02:42,560
and the answer is yes we do believe that

82
00:02:42,560 --> 00:02:43,599
by combining

83
00:02:43,599 --> 00:02:46,239
deep learning and dynamic analysis we'll

84
00:02:46,239 --> 00:02:47,280
be able to

85
00:02:47,280 --> 00:02:48,959
quickly and efficiently find the

86
00:02:48,959 --> 00:02:51,120
pinpoint and quickly find the origin of

87
00:02:51,120 --> 00:02:53,040
the leakage right the leakage being

88
00:02:53,040 --> 00:02:54,560
whereas the implementation is giving

89
00:02:54,560 --> 00:02:56,160
information that can be used

90
00:02:56,160 --> 00:03:01,040
to extract the key and of course

91
00:03:01,040 --> 00:03:02,879
because i use the word deep learning it

92
00:03:02,879 --> 00:03:04,400
might be a little bit skeptical and say

93
00:03:04,400 --> 00:03:05,920
well yet another talk about deep

94
00:03:05,920 --> 00:03:07,200
learning machine learning

95
00:03:07,200 --> 00:03:08,959
i'm going to tell you something the

96
00:03:08,959 --> 00:03:10,879
truth is why not

97
00:03:10,879 --> 00:03:12,720
we actually what i'm going to do today

98
00:03:12,720 --> 00:03:15,120
is actually show you a early prototype

99
00:03:15,120 --> 00:03:17,280
of a concrete software that we have

100
00:03:17,280 --> 00:03:18,159
developed called

101
00:03:18,159 --> 00:03:20,480
cold which stands for charge side

102
00:03:20,480 --> 00:03:21,519
channel attack

103
00:03:21,519 --> 00:03:24,080
leak detector which is actually usable

104
00:03:24,080 --> 00:03:26,480
in practice to help us find

105
00:03:26,480 --> 00:03:29,040
where the leakage comes from and help us

106
00:03:29,040 --> 00:03:30,720
debug

107
00:03:30,720 --> 00:03:33,840
such channel attacks so to make it very

108
00:03:33,840 --> 00:03:35,519
concrete today what we're going to do is

109
00:03:35,519 --> 00:03:36,400
we're going to show

110
00:03:36,400 --> 00:03:38,000
i'm going to show you how we can use

111
00:03:38,000 --> 00:03:39,760
cold to find

112
00:03:39,760 --> 00:03:42,319
where a tiny ios implementation running

113
00:03:42,319 --> 00:03:43,840
on an smt32

114
00:03:43,840 --> 00:03:47,360
f4 is leaking and

115
00:03:47,360 --> 00:03:50,400
this is obviously a unprotected

116
00:03:50,400 --> 00:03:52,319
implementation tiny rs which is running

117
00:03:52,319 --> 00:03:54,080
on a well-known arm cp

118
00:03:54,080 --> 00:03:56,560
and this combination of tiny is and

119
00:03:56,560 --> 00:03:57,760
smt-32

120
00:03:57,760 --> 00:04:00,319
is what we call a target because as i

121
00:04:00,319 --> 00:04:00,879
said

122
00:04:00,879 --> 00:04:03,760
the side channel attack is due to the

123
00:04:03,760 --> 00:04:06,239
interplay between the implementation

124
00:04:06,239 --> 00:04:09,519
and the hardware so how we're going to

125
00:04:09,519 --> 00:04:10,400
get there

126
00:04:10,400 --> 00:04:12,879
first i'm going to briefly recap what's

127
00:04:12,879 --> 00:04:14,799
a side channel attack are

128
00:04:14,799 --> 00:04:17,358
then i'm going to tell you how we very

129
00:04:17,358 --> 00:04:18,000
briefly

130
00:04:18,000 --> 00:04:20,320
how we perform sectional attack which

131
00:04:20,320 --> 00:04:21,680
are using an aim

132
00:04:21,680 --> 00:04:24,000
at deep learning which is what we

133
00:04:24,000 --> 00:04:25,120
presented last year

134
00:04:25,120 --> 00:04:26,880
then we're going to talk about ai

135
00:04:26,880 --> 00:04:28,639
explainability which is the core of the

136
00:04:28,639 --> 00:04:30,800
technique we are leveraging to do the

137
00:04:30,800 --> 00:04:31,680
debugger

138
00:04:31,680 --> 00:04:33,919
and finally i'm going to show you how in

139
00:04:33,919 --> 00:04:34,960
practice we

140
00:04:34,960 --> 00:04:38,320
we use it to find a leakage

141
00:04:38,320 --> 00:04:42,320
into our target so

142
00:04:42,320 --> 00:04:44,320
if you want to follow along at home and

143
00:04:44,320 --> 00:04:46,160
you want to look at where the

144
00:04:46,160 --> 00:04:48,080
where the code and the sliders we go

145
00:04:48,080 --> 00:04:49,759
through the presentation you can get

146
00:04:49,759 --> 00:04:50,080
them

147
00:04:50,080 --> 00:04:54,000
at eli.net code

148
00:04:55,280 --> 00:04:57,360
a quick disclaimer about this talk the

149
00:04:57,360 --> 00:04:58,880
goal of this talk is to provide a high

150
00:04:58,880 --> 00:05:00,960
level overview of how the whole thing

151
00:05:00,960 --> 00:05:01,440
works

152
00:05:01,440 --> 00:05:03,600
and provide you an end-to-end view of

153
00:05:03,600 --> 00:05:04,960
how the process works

154
00:05:04,960 --> 00:05:06,400
if you're interested in more technical

155
00:05:06,400 --> 00:05:08,240
detail like the exact machine learning

156
00:05:08,240 --> 00:05:10,639
architecture we use or how we do the

157
00:05:10,639 --> 00:05:12,639
code mapping and everything

158
00:05:12,639 --> 00:05:14,320
which you should have by the time of

159
00:05:14,320 --> 00:05:15,680
blackout a

160
00:05:15,680 --> 00:05:17,360
technical paper out that you can

161
00:05:17,360 --> 00:05:18,960
download and you can read and we'll

162
00:05:18,960 --> 00:05:21,039
provide you all the details as well as

163
00:05:21,039 --> 00:05:21,759
code who

164
00:05:21,759 --> 00:05:25,360
enable reproducibility so

165
00:05:25,360 --> 00:05:28,000
with that in mind let's start by voila

166
00:05:28,000 --> 00:05:29,520
side channel attack so everyone is on

167
00:05:29,520 --> 00:05:30,960
the same page

168
00:05:30,960 --> 00:05:32,720
a sectional attack is basically an

169
00:05:32,720 --> 00:05:34,240
indirect measurement of

170
00:05:34,240 --> 00:05:36,320
a combination resort which is done

171
00:05:36,320 --> 00:05:38,320
through a auxiliary mechanism

172
00:05:38,320 --> 00:05:41,199
so what do we mean by that well sorry

173
00:05:41,199 --> 00:05:42,560
before getting there let's

174
00:05:42,560 --> 00:05:45,039
what what is why is it useful well a

175
00:05:45,039 --> 00:05:46,000
stationary attack

176
00:05:46,000 --> 00:05:48,880
is used as i mentioned and the intro to

177
00:05:48,880 --> 00:05:50,240
recover encryption key

178
00:05:50,240 --> 00:05:52,639
it is also used uh in the world of web

179
00:05:52,639 --> 00:05:54,160
security to perform blind screen

180
00:05:54,160 --> 00:05:55,600
injection where you cannot see the

181
00:05:55,600 --> 00:05:57,199
result of the injection

182
00:05:57,199 --> 00:05:59,360
so you have to guess what is the result

183
00:05:59,360 --> 00:06:01,199
it's also used to steal

184
00:06:01,199 --> 00:06:04,080
passwords and pins and finally it can

185
00:06:04,080 --> 00:06:05,520
also be used to extract as

186
00:06:05,520 --> 00:06:07,759
we showed in the example earlier extra

187
00:06:07,759 --> 00:06:08,560
crypto wallet

188
00:06:08,560 --> 00:06:10,639
private keys so basically everything

189
00:06:10,639 --> 00:06:12,319
which is secret that you cannot

190
00:06:12,319 --> 00:06:14,720
observe directly but you wish to guess

191
00:06:14,720 --> 00:06:15,759
or to

192
00:06:15,759 --> 00:06:19,039
recover can usually be targeted by

193
00:06:19,039 --> 00:06:22,160
a session and attack one one another so

194
00:06:22,160 --> 00:06:24,400
in the case we are interested in which

195
00:06:24,400 --> 00:06:25,759
is hardware crypto

196
00:06:25,759 --> 00:06:27,360
uh the idea is when you perform a

197
00:06:27,360 --> 00:06:28,960
computation on a

198
00:06:28,960 --> 00:06:32,000
cpu using an algorithm you fit in your

199
00:06:32,000 --> 00:06:33,680
data which is your plain text and your

200
00:06:33,680 --> 00:06:34,800
secret key

201
00:06:34,800 --> 00:06:36,720
and then as a result you obviously get

202
00:06:36,720 --> 00:06:37,840
the encrypted

203
00:06:37,840 --> 00:06:41,199
text but you also get a bunch of leakage

204
00:06:41,199 --> 00:06:43,919
so leakage basically are this kind of

205
00:06:43,919 --> 00:06:45,199
side effect

206
00:06:45,199 --> 00:06:47,360
and enter the side effects which are due

207
00:06:47,360 --> 00:06:49,680
to the algorithm running on the target

208
00:06:49,680 --> 00:06:52,080
so for example the algorithm will take

209
00:06:52,080 --> 00:06:52,880
some times

210
00:06:52,880 --> 00:06:54,560
and unless your algorithm is protected

211
00:06:54,560 --> 00:06:56,000
against timing

212
00:06:56,000 --> 00:06:57,440
and its constant time you will see

213
00:06:57,440 --> 00:06:59,840
different type time execution time based

214
00:06:59,840 --> 00:07:00,400
on

215
00:07:00,400 --> 00:07:03,680
the input similarly and the one we're

216
00:07:03,680 --> 00:07:05,360
going to focus on during this talk

217
00:07:05,360 --> 00:07:08,160
you might have leakage which are due to

218
00:07:08,160 --> 00:07:08,560
power

219
00:07:08,560 --> 00:07:10,080
consumption so depending on what

220
00:07:10,080 --> 00:07:11,759
operation is done you get different type

221
00:07:11,759 --> 00:07:13,360
of power consumption and you can

222
00:07:13,360 --> 00:07:16,400
actually extract that similarly

223
00:07:16,400 --> 00:07:18,479
depending on how powerful some part of

224
00:07:18,479 --> 00:07:20,240
the cpu might hit differently

225
00:07:20,240 --> 00:07:22,319
as it's not really used for recovering

226
00:07:22,319 --> 00:07:23,280
hardware crypto

227
00:07:23,280 --> 00:07:24,960
and in our case but this is something

228
00:07:24,960 --> 00:07:26,800
which do happen and finally

229
00:07:26,800 --> 00:07:30,160
list last but not least sorry the you

230
00:07:30,160 --> 00:07:30,479
have

231
00:07:30,479 --> 00:07:32,800
em electronic magnetic emission which is

232
00:07:32,800 --> 00:07:34,160
also very powerful

233
00:07:34,160 --> 00:07:37,280
and is the other way uh to one of the

234
00:07:37,280 --> 00:07:37,919
other way

235
00:07:37,919 --> 00:07:41,680
you can extract and recover key so

236
00:07:41,680 --> 00:07:43,520
here's an example for paratrace because

237
00:07:43,520 --> 00:07:45,520
that is the goal of our talk

238
00:07:45,520 --> 00:07:46,960
as you can see this is a power trace

239
00:07:46,960 --> 00:07:48,639
which was captured uh

240
00:07:48,639 --> 00:07:50,800
during the question of aes and if you

241
00:07:50,800 --> 00:07:52,479
look closely in the middle you'll see

242
00:07:52,479 --> 00:07:53,680
that you have

243
00:07:53,680 --> 00:07:56,000
10 spikes it turns out that each of

244
00:07:56,000 --> 00:07:56,800
those spike

245
00:07:56,800 --> 00:07:59,440
is actually describing or is actually

246
00:07:59,440 --> 00:08:00,319
representing

247
00:08:00,319 --> 00:08:02,879
a as round right there is 10 rounds for

248
00:08:02,879 --> 00:08:03,759
as120

249
00:08:03,759 --> 00:08:06,160
128 and we can see very clearly on the

250
00:08:06,160 --> 00:08:07,280
trace

251
00:08:07,280 --> 00:08:09,840
that those rounds are visible if they

252
00:08:09,840 --> 00:08:10,560
are visible

253
00:08:10,560 --> 00:08:12,639
then they might contain information if

254
00:08:12,639 --> 00:08:14,080
they contain information we may be able

255
00:08:14,080 --> 00:08:15,840
to exploit it to recover the key

256
00:08:15,840 --> 00:08:20,000
that is what a such an attack is

257
00:08:20,000 --> 00:08:22,639
so to formalize that a little bit more

258
00:08:22,639 --> 00:08:23,759
what happened is

259
00:08:23,759 --> 00:08:27,360
the cpu as a cpu compute

260
00:08:27,360 --> 00:08:30,319
the encryption and what we do is we

261
00:08:30,319 --> 00:08:31,680
measure

262
00:08:31,680 --> 00:08:33,039
the car as a current with an

263
00:08:33,039 --> 00:08:35,200
oscilloscope and then

264
00:08:35,200 --> 00:08:37,120
we in the traditional setting of session

265
00:08:37,120 --> 00:08:38,479
attack you perform something like a

266
00:08:38,479 --> 00:08:40,159
template attack which is you

267
00:08:40,159 --> 00:08:42,080
take all the traces collected you

268
00:08:42,080 --> 00:08:43,440
collect a few of them actually you don't

269
00:08:43,440 --> 00:08:44,399
collect one

270
00:08:44,399 --> 00:08:46,720
and then you combine them statically to

271
00:08:46,720 --> 00:08:49,360
try to pinpoint what is the key value

272
00:08:49,360 --> 00:08:51,120
and then you recover the key that is in

273
00:08:51,120 --> 00:08:52,959
a chair where the side channel attack

274
00:08:52,959 --> 00:08:57,040
yes in our research

275
00:08:57,040 --> 00:08:58,399
because we got the question asked a lot

276
00:08:58,399 --> 00:09:00,399
what do we use we use

277
00:09:00,399 --> 00:09:02,640
new ae cheap whisper pro and we

278
00:09:02,640 --> 00:09:04,160
sometimes use a picoscope

279
00:09:04,160 --> 00:09:06,560
6000 when we need faster something right

280
00:09:06,560 --> 00:09:08,080
depending on the target

281
00:09:08,080 --> 00:09:10,160
in this talk has been said we only you

282
00:09:10,160 --> 00:09:11,760
are we only used

283
00:09:11,760 --> 00:09:14,640
the cheap whisper pro to do the capture

284
00:09:14,640 --> 00:09:15,839
of the trace

285
00:09:15,839 --> 00:09:17,519
because we are going to use synchronous

286
00:09:17,519 --> 00:09:21,120
traces for only the first few rounds

287
00:09:21,120 --> 00:09:23,600
when we do as previous work the whole

288
00:09:23,600 --> 00:09:25,680
traces for the whole implementation

289
00:09:25,680 --> 00:09:27,279
and we want to do it in the attacker

290
00:09:27,279 --> 00:09:29,279
settings where we don't have the clock

291
00:09:29,279 --> 00:09:32,160
we use also the picoscope again this is

292
00:09:32,160 --> 00:09:33,600
not an endorsement for a project or

293
00:09:33,600 --> 00:09:34,399
another

294
00:09:34,399 --> 00:09:36,080
it is just that this is what we use in

295
00:09:36,080 --> 00:09:37,440
our lab so if you would like to

296
00:09:37,440 --> 00:09:39,200
reproduce our result from the ground up

297
00:09:39,200 --> 00:09:42,880
this is what you could use so

298
00:09:42,880 --> 00:09:44,560
let me now describe a little bit more of

299
00:09:44,560 --> 00:09:46,560
how we wave in deep learning

300
00:09:46,560 --> 00:09:48,560
because we're going to need as a machine

301
00:09:48,560 --> 00:09:50,160
learning model generated during the

302
00:09:50,160 --> 00:09:50,720
attack

303
00:09:50,720 --> 00:09:53,839
for the debugging so

304
00:09:53,839 --> 00:09:56,480
deep running such an attack we call them

305
00:09:56,480 --> 00:09:57,360
side channel attack

306
00:09:57,360 --> 00:09:59,839
automated with machine learning and the

307
00:09:59,839 --> 00:10:01,040
idea is

308
00:10:01,040 --> 00:10:02,560
you're going to replace the template

309
00:10:02,560 --> 00:10:04,079
attack with something else

310
00:10:04,079 --> 00:10:06,000
with a neural network and this is

311
00:10:06,000 --> 00:10:08,959
something we have covered in detail

312
00:10:08,959 --> 00:10:11,600
before however i'm going to briefly

313
00:10:11,600 --> 00:10:12,480
recap

314
00:10:12,480 --> 00:10:14,880
today how you can do those attack in

315
00:10:14,880 --> 00:10:15,680
practice

316
00:10:15,680 --> 00:10:17,760
so you are able to figure out to get a

317
00:10:17,760 --> 00:10:20,240
gist of what is happening

318
00:10:20,240 --> 00:10:21,519
if you would like to have a more

319
00:10:21,519 --> 00:10:23,519
in-depth explanation of how the whole

320
00:10:23,519 --> 00:10:24,399
thing works

321
00:10:24,399 --> 00:10:26,560
we had the talk last year at defcon

322
00:10:26,560 --> 00:10:28,160
where we actually get into greater

323
00:10:28,160 --> 00:10:30,000
detail for 40 minutes on exactly how

324
00:10:30,000 --> 00:10:31,600
that works and all the

325
00:10:31,600 --> 00:10:33,600
nitty gritty 10 bits and you also have

326
00:10:33,600 --> 00:10:35,440
the code associated with it

327
00:10:35,440 --> 00:10:37,839
so if you want to go there you can just

328
00:10:37,839 --> 00:10:38,720
get the slide

329
00:10:38,720 --> 00:10:42,079
and the recording of last year video

330
00:10:42,079 --> 00:10:46,160
add that address all right

331
00:10:46,160 --> 00:10:47,920
um one thing which is different from

332
00:10:47,920 --> 00:10:49,680
last year and which is different from

333
00:10:49,680 --> 00:10:51,279
our threat model where we try to do

334
00:10:51,279 --> 00:10:53,839
a black box attack which is the

335
00:10:53,839 --> 00:10:55,440
strongest thing we can do when we try to

336
00:10:55,440 --> 00:10:56,959
revise the security of a chip

337
00:10:56,959 --> 00:10:59,279
in that specific case we are building a

338
00:10:59,279 --> 00:11:00,079
debugger

339
00:11:00,079 --> 00:11:02,880
so we are building a tool which is for

340
00:11:02,880 --> 00:11:04,320
the people who built

341
00:11:04,320 --> 00:11:06,800
the hardware or are not necessarily

342
00:11:06,800 --> 00:11:08,240
building the hardware but are creating

343
00:11:08,240 --> 00:11:08,720
the

344
00:11:08,720 --> 00:11:11,760
secure token so we are taking

345
00:11:11,760 --> 00:11:13,680
a white box attack model which is we

346
00:11:13,680 --> 00:11:15,120
have access to the source code

347
00:11:15,120 --> 00:11:17,440
we have access to the clock and we have

348
00:11:17,440 --> 00:11:19,279
access to

349
00:11:19,279 --> 00:11:21,200
knowledge about the implementation as a

350
00:11:21,200 --> 00:11:24,480
result we are simplifying the attack

351
00:11:24,480 --> 00:11:28,000
by using synchronous synchronous capture

352
00:11:28,000 --> 00:11:29,680
which means we are using the internal

353
00:11:29,680 --> 00:11:31,120
clock of the target

354
00:11:31,120 --> 00:11:32,720
to capture the trace let's make it

355
00:11:32,720 --> 00:11:34,880
easier because we have traces which are

356
00:11:34,880 --> 00:11:36,000
perfectly aligned

357
00:11:36,000 --> 00:11:37,680
which we don't have in the black box

358
00:11:37,680 --> 00:11:40,079
settings we also are

359
00:11:40,079 --> 00:11:42,480
reducing the number the amount of point

360
00:11:42,480 --> 00:11:44,079
in the trace that we are capturing

361
00:11:44,079 --> 00:11:46,079
because well we know that we only need

362
00:11:46,079 --> 00:11:47,519
the first round the first

363
00:11:47,519 --> 00:11:49,680
round and a half or something like that

364
00:11:49,680 --> 00:11:51,760
so let's make it easier

365
00:11:51,760 --> 00:11:53,200
for us to train the model because of

366
00:11:53,200 --> 00:11:55,040
course the trace is slower is

367
00:11:55,040 --> 00:11:57,839
smaller and will align so the model has

368
00:11:57,839 --> 00:11:59,360
to do less work so we can use smaller

369
00:11:59,360 --> 00:12:02,639
models and also it converts faster

370
00:12:02,639 --> 00:12:04,560
it's easier you could do it with the

371
00:12:04,560 --> 00:12:06,639
full scanner model if you wish

372
00:12:06,639 --> 00:12:08,720
although it's going to create a lot of

373
00:12:08,720 --> 00:12:09,760
dead space

374
00:12:09,760 --> 00:12:11,519
which is not necessarily useful for this

375
00:12:11,519 --> 00:12:13,519
type of stuff as we want to have

376
00:12:13,519 --> 00:12:15,519
as quick of an interaction as we can

377
00:12:15,519 --> 00:12:17,760
this is why it's a white box

378
00:12:17,760 --> 00:12:20,320
model that this is why we did choose a

379
00:12:20,320 --> 00:12:21,120
white box

380
00:12:21,120 --> 00:12:25,600
model okay so how this camera process

381
00:12:25,600 --> 00:12:29,040
differs from the sca processes that we

382
00:12:29,040 --> 00:12:29,839
saw earlier

383
00:12:29,839 --> 00:12:32,320
well we start with the template which is

384
00:12:32,320 --> 00:12:34,160
you perform

385
00:12:34,160 --> 00:12:36,639
second my google assistant decided to

386
00:12:36,639 --> 00:12:39,839
care over

387
00:12:45,519 --> 00:12:49,200
sorry about that unexpected to

388
00:12:49,200 --> 00:12:51,120
unexpected during but given the

389
00:12:51,120 --> 00:12:52,880
condition uh the phone is

390
00:12:52,880 --> 00:12:54,880
just here all right so let's go back to

391
00:12:54,880 --> 00:12:57,120
where we were uh so we have the

392
00:12:57,120 --> 00:12:59,440
encryption and then the encryptions

393
00:12:59,440 --> 00:13:00,560
basically what happened is the

394
00:13:00,560 --> 00:13:02,639
encryption is performed as usual

395
00:13:02,639 --> 00:13:05,760
and then we capture the signal with a

396
00:13:05,760 --> 00:13:07,120
chip whisperer right

397
00:13:07,120 --> 00:13:09,279
and then we get a bunch of traces and

398
00:13:09,279 --> 00:13:10,880
then instead of doing the template

399
00:13:10,880 --> 00:13:11,920
attack what we do

400
00:13:11,920 --> 00:13:15,040
is we feed those to a deep neural

401
00:13:15,040 --> 00:13:16,240
network

402
00:13:16,240 --> 00:13:18,079
which is the form of which is d printing

403
00:13:18,079 --> 00:13:19,279
form of ai

404
00:13:19,279 --> 00:13:21,040
and then the machine learning is going

405
00:13:21,040 --> 00:13:22,639
to make a prediction

406
00:13:22,639 --> 00:13:25,279
and we're going to combine them uh to

407
00:13:25,279 --> 00:13:26,880
recover the key

408
00:13:26,880 --> 00:13:29,360
and that's what it does the advantage of

409
00:13:29,360 --> 00:13:31,680
this technique is it's fairly automated

410
00:13:31,680 --> 00:13:35,040
it doesn't require too much domain

411
00:13:35,040 --> 00:13:35,760
expertise

412
00:13:35,760 --> 00:13:38,720
and also can take over can work on very

413
00:13:38,720 --> 00:13:40,720
complicated target quite efficiently as

414
00:13:40,720 --> 00:13:42,160
i said it works really well in black box

415
00:13:42,160 --> 00:13:42,880
settings

416
00:13:42,880 --> 00:13:44,720
on the full traces which is something

417
00:13:44,720 --> 00:13:46,839
that other type of approach struggle

418
00:13:46,839 --> 00:13:49,600
with

419
00:13:49,600 --> 00:13:51,760
when you do targeting by the way when

420
00:13:51,760 --> 00:13:53,360
you do target

421
00:13:53,360 --> 00:13:55,839
an implementation you might try to

422
00:13:55,839 --> 00:13:57,760
target directly recovering the key

423
00:13:57,760 --> 00:14:00,800
but more often than not we try to target

424
00:14:00,800 --> 00:14:02,399
what we call intermediate which are part

425
00:14:02,399 --> 00:14:03,920
of the as

426
00:14:03,920 --> 00:14:05,920
part of the as where there is some sort

427
00:14:05,920 --> 00:14:08,240
of memory loading or memory unloading

428
00:14:08,240 --> 00:14:09,120
because

429
00:14:09,120 --> 00:14:10,720
this is where the power is changing

430
00:14:10,720 --> 00:14:12,560
right because you are

431
00:14:12,560 --> 00:14:14,720
setting some bytes to one some bytes you

432
00:14:14,720 --> 00:14:16,959
are putting to zero so the power is

433
00:14:16,959 --> 00:14:18,480
changing right so we want to have this

434
00:14:18,480 --> 00:14:19,360
kind of point

435
00:14:19,360 --> 00:14:20,880
those points are called intermediate

436
00:14:20,880 --> 00:14:23,199
point uh there are quite a few of them

437
00:14:23,199 --> 00:14:25,199
the tree canonical one that most people

438
00:14:25,199 --> 00:14:26,399
use including us

439
00:14:26,399 --> 00:14:28,480
are the first initial tree which are

440
00:14:28,480 --> 00:14:31,120
part of the initial round of aes because

441
00:14:31,120 --> 00:14:32,880
they are easily invertible which means

442
00:14:32,880 --> 00:14:34,639
you can directly recover the key from

443
00:14:34,639 --> 00:14:35,680
them

444
00:14:35,680 --> 00:14:38,639
today we're going to only focus on one

445
00:14:38,639 --> 00:14:40,399
of them which is sub byte in

446
00:14:40,399 --> 00:14:43,279
which is basically the intermediate

447
00:14:43,279 --> 00:14:44,800
point which happened as you can see on

448
00:14:44,800 --> 00:14:45,600
diagram

449
00:14:45,600 --> 00:14:48,399
after we have xor the key with the plain

450
00:14:48,399 --> 00:14:49,519
text

451
00:14:49,519 --> 00:14:52,160
you could target also sub byte out if

452
00:14:52,160 --> 00:14:54,079
you wish

453
00:14:54,079 --> 00:14:55,760
as we explained earlier in the previous

454
00:14:55,760 --> 00:14:57,839
talk trying to target the key doesn't

455
00:14:57,839 --> 00:14:59,040
work really well

456
00:14:59,040 --> 00:15:01,920
this is not a no point for tiny ass so

457
00:15:01,920 --> 00:15:03,600
your model will probably not get as good

458
00:15:03,600 --> 00:15:05,040
of an accuracy

459
00:15:05,040 --> 00:15:07,519
and given that we are trying to get for

460
00:15:07,519 --> 00:15:09,920
the most obvious leakage for now

461
00:15:09,920 --> 00:15:13,440
sub biting is perfectly fine okay

462
00:15:13,440 --> 00:15:15,120
last thing i want to mention before we

463
00:15:15,120 --> 00:15:17,360
move on to how to do the debugging

464
00:15:17,360 --> 00:15:19,440
is what do you do with a prediction

465
00:15:19,440 --> 00:15:21,600
because that's one of the nice

466
00:15:21,600 --> 00:15:24,000
thing about machine learning when you do

467
00:15:24,000 --> 00:15:26,959
deep learning based sca is that you get

468
00:15:26,959 --> 00:15:28,959
your traces and then the model

469
00:15:28,959 --> 00:15:30,800
instead of telling you one value it

470
00:15:30,800 --> 00:15:33,199
would give you a probability

471
00:15:33,199 --> 00:15:35,440
a fake probability but a probability so

472
00:15:35,440 --> 00:15:36,720
softmax will tell you

473
00:15:36,720 --> 00:15:38,560
for each value of the intermediate what

474
00:15:38,560 --> 00:15:40,160
is the probability

475
00:15:40,160 --> 00:15:42,240
or how strongly he believed each of them

476
00:15:42,240 --> 00:15:43,680
is a correct prediction

477
00:15:43,680 --> 00:15:45,839
so what you can do that gives rise to a

478
00:15:45,839 --> 00:15:47,040
very natural

479
00:15:47,040 --> 00:15:48,560
probabilistic attack where basically you

480
00:15:48,560 --> 00:15:50,480
take all the output of the model so the

481
00:15:50,480 --> 00:15:52,320
model will tell you for each

482
00:15:52,320 --> 00:15:54,720
value potential value what is the

483
00:15:54,720 --> 00:15:56,240
probability so you can

484
00:15:56,240 --> 00:16:00,079
actually sum them using a log 10 plus

485
00:16:00,079 --> 00:16:00,800
epsilon

486
00:16:00,800 --> 00:16:03,759
and then you get a distribution and that

487
00:16:03,759 --> 00:16:05,680
helps you to accumulate probability very

488
00:16:05,680 --> 00:16:07,519
easily

489
00:16:07,519 --> 00:16:09,040
tell you which is the first guess which

490
00:16:09,040 --> 00:16:10,240
is the second guest so i guess and you

491
00:16:10,240 --> 00:16:11,600
can actually rank your gas

492
00:16:11,600 --> 00:16:13,759
and so it makes is very nice for basic

493
00:16:13,759 --> 00:16:14,800
attack that

494
00:16:14,800 --> 00:16:16,160
only deep learning provides so that's

495
00:16:16,160 --> 00:16:17,839
also one of the benefit

496
00:16:17,839 --> 00:16:22,320
of using deep learning based sci scammer

497
00:16:22,320 --> 00:16:25,360
all right in

498
00:16:25,360 --> 00:16:28,639
this current uh to make things uh

499
00:16:28,639 --> 00:16:30,639
to give you a little bit more detail uh

500
00:16:30,639 --> 00:16:32,240
the model we're going to use

501
00:16:32,240 --> 00:16:34,240
in this talk is something called a

502
00:16:34,240 --> 00:16:35,600
simple

503
00:16:35,600 --> 00:16:38,320
hypertune residual separated 1d

504
00:16:38,320 --> 00:16:39,440
commercial networks

505
00:16:39,440 --> 00:16:41,440
basically what it means is we use 1d

506
00:16:41,440 --> 00:16:42,800
convolution we

507
00:16:42,800 --> 00:16:44,720
separated one because they are faster

508
00:16:44,720 --> 00:16:46,480
and have less parameters and

509
00:16:46,480 --> 00:16:48,160
in our experience over the last three

510
00:16:48,160 --> 00:16:50,160
years it feels that is good enough

511
00:16:50,160 --> 00:16:52,639
it's actually give us performances it is

512
00:16:52,639 --> 00:16:54,560
able to work and converge so we don't

513
00:16:54,560 --> 00:16:55,440
need the full

514
00:16:55,440 --> 00:16:58,079
convolutions we also have hyper tuned

515
00:16:58,079 --> 00:16:59,120
them before

516
00:16:59,120 --> 00:17:00,720
so we basically use one of our best

517
00:17:00,720 --> 00:17:02,880
model and then

518
00:17:02,880 --> 00:17:04,079
it's a convolutional network which means

519
00:17:04,079 --> 00:17:06,160
it's mostly based on convolution

520
00:17:06,160 --> 00:17:09,199
and the residual part means that we tend

521
00:17:09,199 --> 00:17:10,640
to do something like

522
00:17:10,640 --> 00:17:13,679
the resnet implementation which is

523
00:17:13,679 --> 00:17:14,559
basically

524
00:17:14,559 --> 00:17:16,319
we do a bunch of convolution and then we

525
00:17:16,319 --> 00:17:18,880
keep the signal by having a

526
00:17:18,880 --> 00:17:20,959
second path that you can see on the

527
00:17:20,959 --> 00:17:22,319
right side of the diagram

528
00:17:22,319 --> 00:17:25,119
which helps to keep this to work to

529
00:17:25,119 --> 00:17:26,079
maintain

530
00:17:26,079 --> 00:17:29,760
the to maintain the flow of the gradient

531
00:17:29,760 --> 00:17:32,960
and avoid vanishing gradient problems

532
00:17:32,960 --> 00:17:34,799
so again it's very standard if you do

533
00:17:34,799 --> 00:17:36,720
machine learning it's just adapted

534
00:17:36,720 --> 00:17:39,840
uh to work well for 1d traces and you

535
00:17:39,840 --> 00:17:41,120
can construct your block differently it

536
00:17:41,120 --> 00:17:42,880
seems that this block is the one which

537
00:17:42,880 --> 00:17:44,080
works best for us

538
00:17:44,080 --> 00:17:45,600
and basically we stack a lot of them

539
00:17:45,600 --> 00:17:47,120
like pancakes which is why it's called

540
00:17:47,120 --> 00:17:48,559
deep learning

541
00:17:48,559 --> 00:17:51,200
um here is an example of what you might

542
00:17:51,200 --> 00:17:52,720
get if you train this model

543
00:17:52,720 --> 00:17:54,400
uh we have three models which are

544
00:17:54,400 --> 00:17:56,960
trained um because we train three models

545
00:17:56,960 --> 00:17:58,559
for this talk uh

546
00:17:58,559 --> 00:18:00,880
in a real setting you will train 16 of

547
00:18:00,880 --> 00:18:02,320
those obviously

548
00:18:02,320 --> 00:18:05,600
um and what happened is basically

549
00:18:05,600 --> 00:18:06,960
you can see those stream bytes

550
00:18:06,960 --> 00:18:08,720
converging very well after you only turn

551
00:18:08,720 --> 00:18:09,360
the box i

552
00:18:09,360 --> 00:18:10,559
could have keep going to increase

553
00:18:10,559 --> 00:18:13,440
accuracy but at that point each model

554
00:18:13,440 --> 00:18:14,240
have reached

555
00:18:14,240 --> 00:18:17,440
above 60 which is plenty for what we try

556
00:18:17,440 --> 00:18:18,559
to do which is

557
00:18:18,559 --> 00:18:20,160
get a bunch of traces where the model is

558
00:18:20,160 --> 00:18:21,840
able to predict

559
00:18:21,840 --> 00:18:23,600
the correct value and then use that to

560
00:18:23,600 --> 00:18:26,160
debug so that's plenty enough

561
00:18:26,160 --> 00:18:28,240
so we stopped there because obviously we

562
00:18:28,240 --> 00:18:29,679
want to have

563
00:18:29,679 --> 00:18:32,400
uh to be as efficient as possible as you

564
00:18:32,400 --> 00:18:34,000
can see the important part here

565
00:18:34,000 --> 00:18:37,200
is you can see that the validation

566
00:18:37,200 --> 00:18:39,120
so there is two two curve for each of

567
00:18:39,120 --> 00:18:41,120
the point is actually tracking very

568
00:18:41,120 --> 00:18:42,320
closely the

569
00:18:42,320 --> 00:18:45,360
uh the training set and the reason

570
00:18:45,360 --> 00:18:46,880
that's really good which means the model

571
00:18:46,880 --> 00:18:48,559
has generalized perfectly

572
00:18:48,559 --> 00:18:50,559
because both are tracking each other so

573
00:18:50,559 --> 00:18:52,320
we have a model with general as well so

574
00:18:52,320 --> 00:18:53,280
that's a good model

575
00:18:53,280 --> 00:18:55,120
again that is expected because we made

576
00:18:55,120 --> 00:18:56,799
it very easy for ourselves by

577
00:18:56,799 --> 00:18:58,880
having an unprotected implementation 10

578
00:18:58,880 --> 00:19:00,640
years and make it synchronous

579
00:19:00,640 --> 00:19:03,520
and shrinking down those traces to only

580
00:19:03,520 --> 00:19:04,400
the first

581
00:19:04,400 --> 00:19:07,679
4000 points so that's what it is

582
00:19:07,679 --> 00:19:09,760
last but not least as i mentioned on the

583
00:19:09,760 --> 00:19:10,720
side

584
00:19:10,720 --> 00:19:12,240
if you're not familiar with how session

585
00:19:12,240 --> 00:19:14,720
attack works we do tend to recover

586
00:19:14,720 --> 00:19:17,679
one byte at a time so you need to

587
00:19:17,679 --> 00:19:19,679
recover the whole key 16 models

588
00:19:19,679 --> 00:19:21,280
and then combine the prediction to

589
00:19:21,280 --> 00:19:22,799
require the whole key

590
00:19:22,799 --> 00:19:24,400
for the rest of the talk i'm going to

591
00:19:24,400 --> 00:19:26,080
only focus on one byte at the time

592
00:19:26,080 --> 00:19:27,440
because you basically

593
00:19:27,440 --> 00:19:29,919
repeat the same thing 16 time i'll show

594
00:19:29,919 --> 00:19:31,600
you some example of how the

595
00:19:31,600 --> 00:19:33,520
debugging looks like different from both

596
00:19:33,520 --> 00:19:35,039
traces for different bytes

597
00:19:35,039 --> 00:19:36,840
so keep in mind that we have multiple

598
00:19:36,840 --> 00:19:38,480
models

599
00:19:38,480 --> 00:19:41,600
okay so now we have a model and we know

600
00:19:41,600 --> 00:19:44,480
our model is efficiently breaking

601
00:19:44,480 --> 00:19:47,280
tiny air so the implementation is broken

602
00:19:47,280 --> 00:19:48,559
very very badly

603
00:19:48,559 --> 00:19:50,960
so okay can we use that now to figure

604
00:19:50,960 --> 00:19:52,640
out where the leakage is coming from

605
00:19:52,640 --> 00:19:55,760
well yes to that we're going to leverage

606
00:19:55,760 --> 00:19:56,880
a known technique

607
00:19:56,880 --> 00:19:59,760
a non-set of technique actually which is

608
00:19:59,760 --> 00:20:01,440
deep learning explainability

609
00:20:01,440 --> 00:20:03,919
deep learning explainability actually

610
00:20:03,919 --> 00:20:05,679
was developed

611
00:20:05,679 --> 00:20:08,400
for a vision as they already said early

612
00:20:08,400 --> 00:20:09,440
stages

613
00:20:09,440 --> 00:20:11,840
so they had a bunch of data sets and the

614
00:20:11,840 --> 00:20:13,520
model for example will accurately

615
00:20:13,520 --> 00:20:15,440
predict in this image that it contains a

616
00:20:15,440 --> 00:20:17,200
boxer and a tigercat

617
00:20:17,200 --> 00:20:20,320
the question is why what what is the

618
00:20:20,320 --> 00:20:22,400
policy image as a machine learning is

619
00:20:22,400 --> 00:20:23,120
using

620
00:20:23,120 --> 00:20:25,520
to decide whether there is a boxer or if

621
00:20:25,520 --> 00:20:26,799
there is a tiger cat

622
00:20:26,799 --> 00:20:28,640
you want to use that to make sure there

623
00:20:28,640 --> 00:20:30,240
is no biases

624
00:20:30,240 --> 00:20:33,039
or errors into your data set that are

625
00:20:33,039 --> 00:20:35,039
giving hint to the machine learning that

626
00:20:35,039 --> 00:20:36,960
it should not pick up and it degenerates

627
00:20:36,960 --> 00:20:38,320
correctly right you want the machine

628
00:20:38,320 --> 00:20:39,200
learning

629
00:20:39,200 --> 00:20:41,360
to not only detect a cat in that image

630
00:20:41,360 --> 00:20:42,640
but also detect it in

631
00:20:42,640 --> 00:20:44,960
any wide range of images so you want to

632
00:20:44,960 --> 00:20:46,720
make sure it focuses

633
00:20:46,720 --> 00:20:49,440
its attention to the right place of the

634
00:20:49,440 --> 00:20:51,120
image so the way you do this

635
00:20:51,120 --> 00:20:54,159
is as i said to answer that question the

636
00:20:54,159 --> 00:20:55,600
way you do this is you

637
00:20:55,600 --> 00:20:57,600
use an explanatory technique there are a

638
00:20:57,600 --> 00:20:59,679
few of them i'm going to

639
00:20:59,679 --> 00:21:01,520
highlight a few of them after that and

640
00:21:01,520 --> 00:21:03,039
so basically what you do is you feature

641
00:21:03,039 --> 00:21:04,880
the explanatory model

642
00:21:04,880 --> 00:21:08,240
you also feed the image and then you ask

643
00:21:08,240 --> 00:21:10,400
it

644
00:21:11,440 --> 00:21:13,840
what why this class was predicted you

645
00:21:13,840 --> 00:21:16,000
can ask the main classes

646
00:21:16,000 --> 00:21:18,799
or you can ask a class which is not the

647
00:21:18,799 --> 00:21:20,000
main one you can decide which one you

648
00:21:20,000 --> 00:21:21,440
want but basically what you're doing is

649
00:21:21,440 --> 00:21:23,600
you're conditioning the explanation

650
00:21:23,600 --> 00:21:26,480
based of the prediction you want to

651
00:21:26,480 --> 00:21:27,120
explain

652
00:21:27,120 --> 00:21:28,720
right so you basically provide the input

653
00:21:28,720 --> 00:21:30,480
and the output in a way

654
00:21:30,480 --> 00:21:31,840
you can think of an explainer as

655
00:21:31,840 --> 00:21:33,919
something which reverse the model

656
00:21:33,919 --> 00:21:36,640
right it's not really what it does but

657
00:21:36,640 --> 00:21:38,720
what happens is you basically the

658
00:21:38,720 --> 00:21:40,960
output become the output of your choice

659
00:21:40,960 --> 00:21:41,919
becomes the input

660
00:21:41,919 --> 00:21:44,080
and the output becomes the image itself

661
00:21:44,080 --> 00:21:46,240
and if you do that correctly

662
00:21:46,240 --> 00:21:47,840
you get something nice like this which

663
00:21:47,840 --> 00:21:50,960
says well if you are trying to predict

664
00:21:50,960 --> 00:21:53,039
the boxer from this image using that

665
00:21:53,039 --> 00:21:55,600
model and that's a true example from

666
00:21:55,600 --> 00:21:59,760
a resnet model then you get this nice

667
00:21:59,760 --> 00:22:03,039
heat map which we call leakage map later

668
00:22:03,039 --> 00:22:03,520
on for

669
00:22:03,520 --> 00:22:06,400
for traces that basically highlight that

670
00:22:06,400 --> 00:22:07,440
well it mostly

671
00:22:07,440 --> 00:22:09,679
focuses attention on the face of the

672
00:22:09,679 --> 00:22:12,000
boxer so we have some confidence at

673
00:22:12,000 --> 00:22:12,880
least that

674
00:22:12,880 --> 00:22:14,880
the model is looking at the right part

675
00:22:14,880 --> 00:22:16,880
of the images to make its decision

676
00:22:16,880 --> 00:22:18,720
uh we can confirm that by doing the same

677
00:22:18,720 --> 00:22:20,159
thing for the cat right which we will

678
00:22:20,159 --> 00:22:22,000
say hey explainer

679
00:22:22,000 --> 00:22:23,520
take the same image take the same order

680
00:22:23,520 --> 00:22:25,440
but now tell me why you came up with the

681
00:22:25,440 --> 00:22:26,799
idea of a cat

682
00:22:26,799 --> 00:22:28,720
and this time it's looking at the body

683
00:22:28,720 --> 00:22:30,880
of the cat and because it says the tiger

684
00:22:30,880 --> 00:22:32,720
cat is looking as a stripe

685
00:22:32,720 --> 00:22:36,320
so far so good that seems okay

686
00:22:36,320 --> 00:22:38,080
so we are fairly confident that the

687
00:22:38,080 --> 00:22:41,280
model is actually working as intended

688
00:22:41,280 --> 00:22:43,360
right here's an example where actually

689
00:22:43,360 --> 00:22:44,960
this type of technique were used to find

690
00:22:44,960 --> 00:22:45,919
biases

691
00:22:45,919 --> 00:22:47,919
very very early on there was this very

692
00:22:47,919 --> 00:22:50,480
famous data set for images called

693
00:22:50,480 --> 00:22:53,600
the pascal vocabulary right work

694
00:22:53,600 --> 00:22:56,159
data set and early on this data set has

695
00:22:56,159 --> 00:22:57,679
this interesting feature

696
00:22:57,679 --> 00:23:01,520
that for horses as i think one in five

697
00:23:01,520 --> 00:23:05,840
had a text box as a on the lower left

698
00:23:05,840 --> 00:23:08,720
and what happened was that the machine

699
00:23:08,720 --> 00:23:10,720
learning was predicting there is a horse

700
00:23:10,720 --> 00:23:12,400
not because it was a horse in the image

701
00:23:12,400 --> 00:23:14,320
but just because there was a box

702
00:23:14,320 --> 00:23:16,159
and so that was very interesting it was

703
00:23:16,159 --> 00:23:17,520
a biases

704
00:23:17,520 --> 00:23:19,360
which was due to the fact that the image

705
00:23:19,360 --> 00:23:22,080
had a statistical discriminator which

706
00:23:22,080 --> 00:23:23,600
was easier to learn

707
00:23:23,600 --> 00:23:25,280
there is a box or not rather than

708
00:23:25,280 --> 00:23:27,200
learning the shape of the horse

709
00:23:27,200 --> 00:23:30,960
this was basically the initial

710
00:23:30,960 --> 00:23:33,760
major success i believe for this type of

711
00:23:33,760 --> 00:23:34,960
explanation

712
00:23:34,960 --> 00:23:36,799
it was done into a paper called

713
00:23:36,799 --> 00:23:39,039
unmasking clever hand predictors and

714
00:23:39,039 --> 00:23:39,760
assessing

715
00:23:39,760 --> 00:23:42,000
what machine learning really learns

716
00:23:42,000 --> 00:23:43,679
which was published in nature

717
00:23:43,679 --> 00:23:46,880
you can i believe the paper is widely

718
00:23:46,880 --> 00:23:47,760
available

719
00:23:47,760 --> 00:23:49,200
not even without the paper without the

720
00:23:49,200 --> 00:23:50,960
payroll so you can check it out if

721
00:23:50,960 --> 00:23:52,640
you're interested into the origin of

722
00:23:52,640 --> 00:23:54,320
explainability

723
00:23:54,320 --> 00:23:56,559
so

724
00:23:58,000 --> 00:23:59,840
that tell you how we do explainability

725
00:23:59,840 --> 00:24:01,520
but of course now the question becomes

726
00:24:01,520 --> 00:24:03,039
okay how do we use that

727
00:24:03,039 --> 00:24:05,440
and how we combine it with that we can

728
00:24:05,440 --> 00:24:06,400
release this to

729
00:24:06,400 --> 00:24:08,400
do what we wanted to do which is find

730
00:24:08,400 --> 00:24:09,760
where the locations come from

731
00:24:09,760 --> 00:24:12,880
well this is where it's called coming to

732
00:24:12,880 --> 00:24:14,720
into play right this is how we use our

733
00:24:14,720 --> 00:24:16,559
debugging tool which is written in

734
00:24:16,559 --> 00:24:17,679
python

735
00:24:17,679 --> 00:24:20,960
and so the game plan we have for code is

736
00:24:20,960 --> 00:24:22,559
well start with an explainer of course

737
00:24:22,559 --> 00:24:23,840
and we're going to do very similarly

738
00:24:23,840 --> 00:24:25,360
we're going to fill it the model

739
00:24:25,360 --> 00:24:27,919
which is our trend model we see earlier

740
00:24:27,919 --> 00:24:28,880
and the

741
00:24:28,880 --> 00:24:30,559
traces and the predictions for the

742
00:24:30,559 --> 00:24:32,880
traces so we only keep

743
00:24:32,880 --> 00:24:35,200
the traces and predictions which were

744
00:24:35,200 --> 00:24:36,159
correct

745
00:24:36,159 --> 00:24:38,320
because we're interested in why the

746
00:24:38,320 --> 00:24:41,120
model predicts correctly predicted

747
00:24:41,120 --> 00:24:44,559
given the output of the biting

748
00:24:44,559 --> 00:24:46,240
and then that's going to give us a

749
00:24:46,240 --> 00:24:47,679
leakage map

750
00:24:47,679 --> 00:24:51,039
uh of course it's 1d uh this time

751
00:24:51,039 --> 00:24:52,480
because it's a trace it's not for

752
00:24:52,480 --> 00:24:54,720
2d like the images so that's what the

753
00:24:54,720 --> 00:24:56,640
leakage map is and so basically

754
00:24:56,640 --> 00:24:59,520
our hope is that the silent point into

755
00:24:59,520 --> 00:25:01,840
the leakage map will correlate with

756
00:25:01,840 --> 00:25:04,480
the where the code of that but if you

757
00:25:04,480 --> 00:25:05,760
have a leakage map you just know

758
00:25:05,760 --> 00:25:07,200
temporarily where it is

759
00:25:07,200 --> 00:25:08,640
it doesn't tell you which transaction it

760
00:25:08,640 --> 00:25:10,480
is so with different second piece

761
00:25:10,480 --> 00:25:13,520
of code which is where the dynamic

762
00:25:13,520 --> 00:25:14,000
execution

763
00:25:14,000 --> 00:25:15,840
come in which is the target emulator so

764
00:25:15,840 --> 00:25:17,279
if you remember the target

765
00:25:17,279 --> 00:25:20,400
is the code running on a given hardware

766
00:25:20,400 --> 00:25:22,240
so for it is tiny rs running

767
00:25:22,240 --> 00:25:26,400
on an smt 35 f4 and smg f4

768
00:25:26,400 --> 00:25:28,960
and so what happened is it's going to

769
00:25:28,960 --> 00:25:30,400
take the leakage map and then it's going

770
00:25:30,400 --> 00:25:31,760
to execute the target

771
00:25:31,760 --> 00:25:34,400
and basically based on that and we're

772
00:25:34,400 --> 00:25:36,159
going to get the execution timing

773
00:25:36,159 --> 00:25:38,720
and we'll be able to call it to know

774
00:25:38,720 --> 00:25:40,159
what execution was

775
00:25:40,159 --> 00:25:42,480
executed in which part of the traces

776
00:25:42,480 --> 00:25:44,880
we're going to glue everything together

777
00:25:44,880 --> 00:25:47,760
and use the box symbol and you should be

778
00:25:47,760 --> 00:25:49,760
able to get annotated code

779
00:25:49,760 --> 00:25:52,080
with a liquid map and say this is a line

780
00:25:52,080 --> 00:25:54,240
the code line which is

781
00:25:54,240 --> 00:25:57,200
the most likely to leak okay so how we

782
00:25:57,200 --> 00:25:58,159
get there

783
00:25:58,159 --> 00:25:59,919
uh the first thing we have to choose is

784
00:25:59,919 --> 00:26:01,440
what type of extreme technique we should

785
00:26:01,440 --> 00:26:01,760
use

786
00:26:01,760 --> 00:26:03,520
uh so we started by that right we need

787
00:26:03,520 --> 00:26:04,880
some explanation

788
00:26:04,880 --> 00:26:06,880
so there's a ton of them um there is

789
00:26:06,880 --> 00:26:08,720
some of them which were

790
00:26:08,720 --> 00:26:11,120
developed and you can see you get more

791
00:26:11,120 --> 00:26:12,240
or less

792
00:26:12,240 --> 00:26:17,760
a defined images or a defined output

793
00:26:17,760 --> 00:26:20,240
the one we do use out of those is the

794
00:26:20,240 --> 00:26:21,520
one in the middle

795
00:26:21,520 --> 00:26:24,960
which is a guided gradient grad cam

796
00:26:24,960 --> 00:26:26,480
which is actually would use grad cam

797
00:26:26,480 --> 00:26:29,279
plus plus which is a improved version

798
00:26:29,279 --> 00:26:30,480
because it's one of the most well

799
00:26:30,480 --> 00:26:33,039
defined we do also use

800
00:26:33,039 --> 00:26:35,760
something which is not displayed here

801
00:26:35,760 --> 00:26:36,720
that i will cover

802
00:26:36,720 --> 00:26:38,240
later on which is the activation map

803
00:26:38,240 --> 00:26:40,080
which is literally just looking

804
00:26:40,080 --> 00:26:44,000
at the output of the first layer

805
00:26:44,000 --> 00:26:46,640
so now that we know there's a ton of

806
00:26:46,640 --> 00:26:47,840
techniques the question is

807
00:26:47,840 --> 00:26:49,679
how do we choose the best one for our

808
00:26:49,679 --> 00:26:51,840
job right and the answer is we have to

809
00:26:51,840 --> 00:26:53,120
benchmark them

810
00:26:53,120 --> 00:26:57,600
so the way we benchmark them is we take

811
00:26:57,600 --> 00:26:59,440
every explanation we get for all the

812
00:26:59,440 --> 00:27:00,880
traces we get so we

813
00:27:00,880 --> 00:27:03,520
use i don't know four five six thousand

814
00:27:03,520 --> 00:27:04,400
traces

815
00:27:04,400 --> 00:27:07,679
and for each of them we apply uh we

816
00:27:07,679 --> 00:27:09,600
explain them so we create a collection

817
00:27:09,600 --> 00:27:11,600
of them as you can see on the top left

818
00:27:11,600 --> 00:27:12,799
of the slide

819
00:27:12,799 --> 00:27:15,360
and then we combine them by either

820
00:27:15,360 --> 00:27:16,960
averaging them or summing them it

821
00:27:16,960 --> 00:27:17,760
depends

822
00:27:17,760 --> 00:27:19,919
and then we filter them to only keep the

823
00:27:19,919 --> 00:27:21,919
spike this is the orange part

824
00:27:21,919 --> 00:27:25,760
of the second boxes on the left

825
00:27:25,760 --> 00:27:27,200
and then we normalize between zero and

826
00:27:27,200 --> 00:27:28,880
one because we want to

827
00:27:28,880 --> 00:27:31,039
affect probability of what is the most

828
00:27:31,039 --> 00:27:32,399
important spike right

829
00:27:32,399 --> 00:27:34,399
and then when you have done that and we

830
00:27:34,399 --> 00:27:36,159
have eliminated everything which is

831
00:27:36,159 --> 00:27:37,520
below

832
00:27:37,520 --> 00:27:39,600
the average plus one standard deviation

833
00:27:39,600 --> 00:27:41,360
you get what we call the leakage map and

834
00:27:41,360 --> 00:27:42,640
you can see

835
00:27:42,640 --> 00:27:45,840
the leakage map generated here seems to

836
00:27:45,840 --> 00:27:47,039
have

837
00:27:47,039 --> 00:27:49,600
one initial spike at the beginning and

838
00:27:49,600 --> 00:27:50,480
then

839
00:27:50,480 --> 00:27:52,559
one larger spike at the end of the

840
00:27:52,559 --> 00:27:54,240
traces again the another traces

841
00:27:54,240 --> 00:27:56,480
is probably the end of the first round

842
00:27:56,480 --> 00:27:58,880
actually we think that for this demo

843
00:27:58,880 --> 00:28:01,440
i i cut it a little bit too early i

844
00:28:01,440 --> 00:28:02,880
should probably have taken

845
00:28:02,880 --> 00:28:05,600
6000 or 8000 points so we might miss the

846
00:28:05,600 --> 00:28:07,039
end of it but

847
00:28:07,039 --> 00:28:09,520
suffice to say that for this byte it

848
00:28:09,520 --> 00:28:11,200
seems that we have one input and one

849
00:28:11,200 --> 00:28:15,120
output right so

850
00:28:15,120 --> 00:28:17,279
the thing we can do is we can generate

851
00:28:17,279 --> 00:28:18,960
those heat map for virus techniques so

852
00:28:18,960 --> 00:28:20,480
the first technique we use

853
00:28:20,480 --> 00:28:22,559
is the snr right the signal to noise

854
00:28:22,559 --> 00:28:24,799
ratio this is not the deep learning

855
00:28:24,799 --> 00:28:26,240
technique this is the very old technique

856
00:28:26,240 --> 00:28:27,279
which was developed

857
00:28:27,279 --> 00:28:29,360
actually to do traditional side channel

858
00:28:29,360 --> 00:28:31,279
attack well which is actually

859
00:28:31,279 --> 00:28:33,919
bureau thin uh which will expire and we

860
00:28:33,919 --> 00:28:35,600
use that as a control

861
00:28:35,600 --> 00:28:37,760
we are we want to make sure that we use

862
00:28:37,760 --> 00:28:39,520
we might also use it by the way

863
00:28:39,520 --> 00:28:41,360
but the idea is this is a signal to

864
00:28:41,360 --> 00:28:43,200
noise ratio this is a standard technique

865
00:28:43,200 --> 00:28:44,880
where you look at

866
00:28:44,880 --> 00:28:48,080
given a some assumption where is the

867
00:28:48,080 --> 00:28:50,399
spike where are some spikes based on

868
00:28:50,399 --> 00:28:51,440
this notion of

869
00:28:51,440 --> 00:28:53,440
where is the moses combinative part of

870
00:28:53,440 --> 00:28:55,600
the trace and as you can see they seem

871
00:28:55,600 --> 00:28:56,080
to be

872
00:28:56,080 --> 00:28:58,080
this is for by zero something at the

873
00:28:58,080 --> 00:28:59,840
beginning and something in the middle

874
00:28:59,840 --> 00:29:02,960
and then nothing then we have the grad

875
00:29:02,960 --> 00:29:05,120
cam plus plus as i said the gradient cam

876
00:29:05,120 --> 00:29:06,960
is this idea that we use the integrated

877
00:29:06,960 --> 00:29:07,600
gradient

878
00:29:07,600 --> 00:29:10,159
and we compute some sort of inverted

879
00:29:10,159 --> 00:29:10,640
model

880
00:29:10,640 --> 00:29:12,559
where we use a gradient to see how it

881
00:29:12,559 --> 00:29:13,679
flows from

882
00:29:13,679 --> 00:29:16,799
the input the output back to some of the

883
00:29:16,799 --> 00:29:19,039
layers in our case the initial layer

884
00:29:19,039 --> 00:29:21,600
because we want to look at each point

885
00:29:21,600 --> 00:29:22,640
individually

886
00:29:22,640 --> 00:29:25,679
and you get a very very different

887
00:29:25,679 --> 00:29:28,159
map right so we already know that one of

888
00:29:28,159 --> 00:29:29,360
them will be more correct than the other

889
00:29:29,360 --> 00:29:30,399
one

890
00:29:30,399 --> 00:29:32,240
and then a third technique we use is

891
00:29:32,240 --> 00:29:33,600
called the activation map so the

892
00:29:33,600 --> 00:29:35,440
activation map is basically

893
00:29:35,440 --> 00:29:38,720
you do your prediction and then you look

894
00:29:38,720 --> 00:29:42,559
at what were the value

895
00:29:42,559 --> 00:29:45,840
of the each of the different

896
00:29:45,840 --> 00:29:48,240
how much each neuron did fire right and

897
00:29:48,240 --> 00:29:49,919
so because we use the

898
00:29:49,919 --> 00:29:52,159
lowest bottom layers and these layers is

899
00:29:52,159 --> 00:29:53,760
the size of the trace we can get for

900
00:29:53,760 --> 00:29:54,000
each

901
00:29:54,000 --> 00:29:56,320
of them how much it did fire and what

902
00:29:56,320 --> 00:29:57,919
you can see here is

903
00:29:57,919 --> 00:30:01,120
is mostly identical when filtered uh

904
00:30:01,120 --> 00:30:02,880
to the grad cam plus post so both of

905
00:30:02,880 --> 00:30:05,120
them are somewhat identical so we expect

906
00:30:05,120 --> 00:30:05,919
them to be

907
00:30:05,919 --> 00:30:08,000
performing roughly the same so how do we

908
00:30:08,000 --> 00:30:09,840
evaluate the quality of this

909
00:30:09,840 --> 00:30:11,760
well the idea is very simple we have our

910
00:30:11,760 --> 00:30:13,200
test traces

911
00:30:13,200 --> 00:30:14,880
and we have our leak map so what we can

912
00:30:14,880 --> 00:30:17,279
do is we can decide to suppress

913
00:30:17,279 --> 00:30:19,039
which is basically putting the point to

914
00:30:19,039 --> 00:30:20,640
-1 the

915
00:30:20,640 --> 00:30:22,159
machine learning normally between minus

916
00:30:22,159 --> 00:30:24,000
one and one in our case

917
00:30:24,000 --> 00:30:25,919
and then we remove them so what we can

918
00:30:25,919 --> 00:30:27,520
do is we can take

919
00:30:27,520 --> 00:30:30,799
what are the four six seven eight

920
00:30:30,799 --> 00:30:32,880
twenty point after that become

921
00:30:32,880 --> 00:30:34,480
meaningless but the first in

922
00:30:34,480 --> 00:30:36,880
the first top point that are reported by

923
00:30:36,880 --> 00:30:37,679
the leak map

924
00:30:37,679 --> 00:30:39,919
suppress them from the traces feed them

925
00:30:39,919 --> 00:30:41,679
again to the machining model

926
00:30:41,679 --> 00:30:43,760
and look at how much the accuracy

927
00:30:43,760 --> 00:30:45,279
decrease right so basically what will

928
00:30:45,279 --> 00:30:46,480
tell us is

929
00:30:46,480 --> 00:30:47,840
if those points are important for the

930
00:30:47,840 --> 00:30:49,600
machine into the prediction the accuracy

931
00:30:49,600 --> 00:30:50,000
must go

932
00:30:50,000 --> 00:30:52,399
down if the point are in a dead zone the

933
00:30:52,399 --> 00:30:53,919
machine learning might not care as much

934
00:30:53,919 --> 00:30:55,360
you do care about everything so you'll

935
00:30:55,360 --> 00:30:57,039
see always a decrease but it might not

936
00:30:57,039 --> 00:30:58,159
be as drastic

937
00:30:58,159 --> 00:31:00,320
so now the question become if you fix

938
00:31:00,320 --> 00:31:02,399
the number of points

939
00:31:02,399 --> 00:31:04,159
how much of a sharp decrease we do the

940
00:31:04,159 --> 00:31:05,519
one who decrease the most

941
00:31:05,519 --> 00:31:06,960
the accuracy would be the technique

942
00:31:06,960 --> 00:31:09,039
which is the most predictive

943
00:31:09,039 --> 00:31:10,559
of the leakage right that's the

944
00:31:10,559 --> 00:31:12,240
technique we're using and how we do

945
00:31:12,240 --> 00:31:13,200
benchmark

946
00:31:13,200 --> 00:31:16,559
or explanability technique so baseline

947
00:31:16,559 --> 00:31:18,320
as i mentioned is hundred percent

948
00:31:18,320 --> 00:31:20,720
because well we only kept as i said

949
00:31:20,720 --> 00:31:21,440
correct

950
00:31:21,440 --> 00:31:23,679
uh predictions and then we can look at

951
00:31:23,679 --> 00:31:24,559
each of them

952
00:31:24,559 --> 00:31:26,240
so first thing we can look at is as i

953
00:31:26,240 --> 00:31:27,760
say signal to noise ratio

954
00:31:27,760 --> 00:31:30,960
and in that case on byte zero the drop

955
00:31:30,960 --> 00:31:32,799
of accuracy for four points

956
00:31:32,799 --> 00:31:37,200
uh is 57 uh it's dropped to 50 57

957
00:31:37,200 --> 00:31:39,120
which means we have a 43 percent

958
00:31:39,120 --> 00:31:41,760
reduction so almost half

959
00:31:41,760 --> 00:31:44,559
so in four points there are four points

960
00:31:44,559 --> 00:31:45,440
there is enough

961
00:31:45,440 --> 00:31:48,320
top four points are responsible for

962
00:31:48,320 --> 00:31:49,919
about fifty percent of the accuracy of

963
00:31:49,919 --> 00:31:51,440
the model so it gives you an idea that

964
00:31:51,440 --> 00:31:53,120
these are really where the leak

965
00:31:53,120 --> 00:31:55,120
are coming from similarly fairly

966
00:31:55,120 --> 00:31:56,720
consistent with byte7

967
00:31:56,720 --> 00:32:02,000
uh where it's even deeper uh 44

968
00:32:02,000 --> 00:32:04,159
now if we look at activation map uh

969
00:32:04,159 --> 00:32:05,760
that's where we start to be really sad

970
00:32:05,760 --> 00:32:08,080
actually if we were very sad uh the

971
00:32:08,080 --> 00:32:09,120
activation map

972
00:32:09,120 --> 00:32:12,480
performed really well than that so about

973
00:32:12,480 --> 00:32:13,279
the same for by

974
00:32:13,279 --> 00:32:18,159
zero but then four by seven what the

975
00:32:18,159 --> 00:32:20,480
leakage the activation map return is not

976
00:32:20,480 --> 00:32:22,240
predictive at all of

977
00:32:22,240 --> 00:32:24,159
where the cache come from at least not

978
00:32:24,159 --> 00:32:25,360
given our benchmark

979
00:32:25,360 --> 00:32:27,919
so we're hoping that grad cam plus plus

980
00:32:27,919 --> 00:32:28,799
which came as

981
00:32:28,799 --> 00:32:31,919
one of the nonsense but one of the most

982
00:32:31,919 --> 00:32:33,360
well used one and it's supposed to be

983
00:32:33,360 --> 00:32:34,720
really good at least work really well

984
00:32:34,720 --> 00:32:36,720
for images will work well for us

985
00:32:36,720 --> 00:32:38,640
the thing is as you remember i showed

986
00:32:38,640 --> 00:32:39,760
you they are almost the same

987
00:32:39,760 --> 00:32:41,919
leakage map so you get the same result i

988
00:32:41,919 --> 00:32:43,919
mean that's consistent that's good but

989
00:32:43,919 --> 00:32:46,799
the numbers are really really bad we

990
00:32:46,799 --> 00:32:47,600
still again

991
00:32:47,600 --> 00:32:49,840
do not get as much as the snr which i

992
00:32:49,840 --> 00:32:51,519
told you is a simple statistical test

993
00:32:51,519 --> 00:32:52,399
which use

994
00:32:52,399 --> 00:32:54,240
hypotheses which we know are not as

995
00:32:54,240 --> 00:32:55,840
accurate as machine learning one because

996
00:32:55,840 --> 00:32:56,559
they use

997
00:32:56,559 --> 00:32:58,720
as a humming model if you want to be

998
00:32:58,720 --> 00:32:59,919
precise

999
00:32:59,919 --> 00:33:03,039
so we were really sad because

1000
00:33:03,039 --> 00:33:04,399
and it took us a while to figure out

1001
00:33:04,399 --> 00:33:05,760
what to do because actually the

1002
00:33:05,760 --> 00:33:07,360
explaining technique we wanted to do

1003
00:33:07,360 --> 00:33:10,960
do not work and uh json r then the snr

1004
00:33:10,960 --> 00:33:12,080
and also as you could

1005
00:33:12,080 --> 00:33:14,320
you have seen we could say well but they

1006
00:33:14,320 --> 00:33:16,000
are cleaner they're not they also have a

1007
00:33:16,000 --> 00:33:17,519
ton of stride so

1008
00:33:17,519 --> 00:33:20,960
they're not very informative either so

1009
00:33:20,960 --> 00:33:23,840
it took us quite a bit of iteration back

1010
00:33:23,840 --> 00:33:24,399
and forth

1011
00:33:24,399 --> 00:33:26,960
and then we ended up deciding that we

1012
00:33:26,960 --> 00:33:27,679
need to

1013
00:33:27,679 --> 00:33:28,960
create something which is a little bit

1014
00:33:28,960 --> 00:33:31,679
more tailored to what we wanted to do

1015
00:33:31,679 --> 00:33:35,200
the reason why that might work

1016
00:33:35,200 --> 00:33:37,200
and i know you might be most critical

1017
00:33:37,200 --> 00:33:38,640
that we can come up with yet another

1018
00:33:38,640 --> 00:33:39,279
technique and

1019
00:33:39,279 --> 00:33:40,880
it will be better the reason why it

1020
00:33:40,880 --> 00:33:42,720
might be working better is because

1021
00:33:42,720 --> 00:33:44,000
usually

1022
00:33:44,000 --> 00:33:45,519
the machine learning technique used for

1023
00:33:45,519 --> 00:33:47,279
explainability focuses

1024
00:33:47,279 --> 00:33:50,559
on the highest level features so the

1025
00:33:50,559 --> 00:33:54,000
upper part of the neural network

1026
00:33:54,000 --> 00:33:56,320
to get more of this understanding and

1027
00:33:56,320 --> 00:33:58,240
what they do is a stretch

1028
00:33:58,240 --> 00:34:00,240
because the neural network is like a um

1029
00:34:00,240 --> 00:34:01,840
a pyramid right so

1030
00:34:01,840 --> 00:34:05,200
the upper layer are smaller in weeds

1031
00:34:05,200 --> 00:34:06,640
so you have to if you want to put it

1032
00:34:06,640 --> 00:34:08,000
back to trace you have to stretch them

1033
00:34:08,000 --> 00:34:08,480
out

1034
00:34:08,480 --> 00:34:10,879
we don't want to do that because it

1035
00:34:10,879 --> 00:34:11,679
basically creates

1036
00:34:11,679 --> 00:34:14,960
inaccuracy so we try to do

1037
00:34:14,960 --> 00:34:17,199
the explanation as the lowest layer it

1038
00:34:17,199 --> 00:34:19,119
doesn't work right as we saw

1039
00:34:19,119 --> 00:34:20,960
so we were hoping to go back to the

1040
00:34:20,960 --> 00:34:22,719
roots of the idea

1041
00:34:22,719 --> 00:34:24,560
and actually use one of the oldest

1042
00:34:24,560 --> 00:34:26,639
technique but revisit them to make it a

1043
00:34:26,639 --> 00:34:28,800
little bit more precise which is using

1044
00:34:28,800 --> 00:34:31,839
occlusion the problem with occlusion is

1045
00:34:31,839 --> 00:34:32,879
that

1046
00:34:32,879 --> 00:34:35,760
an occlusion is basically you are

1047
00:34:35,760 --> 00:34:36,879
removing from the twice and you're

1048
00:34:36,879 --> 00:34:38,000
scaling it that way the problem with

1049
00:34:38,000 --> 00:34:38,399
that

1050
00:34:38,399 --> 00:34:40,159
is of course it doesn't take into

1051
00:34:40,159 --> 00:34:41,839
account non-linearity

1052
00:34:41,839 --> 00:34:45,119
so to combat that we came up with a

1053
00:34:45,119 --> 00:34:47,040
novel version like which is basically

1054
00:34:47,040 --> 00:34:48,879
combining this type of occlusion by

1055
00:34:48,879 --> 00:34:50,560
block with a convolutive

1056
00:34:50,560 --> 00:34:52,719
occlusion which is basically using a

1057
00:34:52,719 --> 00:34:54,000
windows and then

1058
00:34:54,000 --> 00:34:55,599
convolving the squares to get some sort

1059
00:34:55,599 --> 00:34:58,160
of backups non-linearity over a large

1060
00:34:58,160 --> 00:34:58,880
region

1061
00:34:58,880 --> 00:35:00,800
or a small region like a set of small

1062
00:35:00,800 --> 00:35:02,480
regions and so

1063
00:35:02,480 --> 00:35:04,480
what you end up with are the maps here

1064
00:35:04,480 --> 00:35:06,400
you can see here

1065
00:35:06,400 --> 00:35:08,240
so the first thing about those map just

1066
00:35:08,240 --> 00:35:10,240
visually is they look very clean right

1067
00:35:10,240 --> 00:35:12,160
they are only very very few spikes so

1068
00:35:12,160 --> 00:35:13,520
that's good if they work

1069
00:35:13,520 --> 00:35:15,200
it's very clear where the leakage comes

1070
00:35:15,200 --> 00:35:16,400
from the other thing which is very

1071
00:35:16,400 --> 00:35:17,200
encouraging

1072
00:35:17,200 --> 00:35:20,320
is the byte zero lick map clearly show

1073
00:35:20,320 --> 00:35:23,359
leakage before the byte7 of course the

1074
00:35:23,359 --> 00:35:24,720
algorithm process one after

1075
00:35:24,720 --> 00:35:28,400
each other it's not using any kind of

1076
00:35:28,400 --> 00:35:30,560
themed or vectorized operation so we

1077
00:35:30,560 --> 00:35:32,160
that's what we expect especially to do

1078
00:35:32,160 --> 00:35:33,760
one then the next and the next and the

1079
00:35:33,760 --> 00:35:36,000
next so we're pretty good confident that

1080
00:35:36,000 --> 00:35:36,560
actually

1081
00:35:36,560 --> 00:35:38,880
makes sense it's also interesting they

1082
00:35:38,880 --> 00:35:40,400
show you that

1083
00:35:40,400 --> 00:35:42,640
uh product byte7 actually is really

1084
00:35:42,640 --> 00:35:44,320
close to the end of the traces which

1085
00:35:44,320 --> 00:35:45,119
explain

1086
00:35:45,119 --> 00:35:47,280
why as i told you earlier i might have

1087
00:35:47,280 --> 00:35:50,160
because of the triceratops 15

1088
00:35:50,160 --> 00:35:53,280
or even by 13 and so forth because

1089
00:35:53,280 --> 00:35:54,800
already has a limit for by seven that's

1090
00:35:54,800 --> 00:35:56,720
why we did

1091
00:35:56,720 --> 00:36:01,280
cut here um and so yeah this looks good

1092
00:36:01,280 --> 00:36:03,119
so now we can benchmark it and the

1093
00:36:03,119 --> 00:36:04,640
answer is how good it is

1094
00:36:04,640 --> 00:36:07,119
sensor is pretty good it's way better

1095
00:36:07,119 --> 00:36:08,640
for by zero

1096
00:36:08,640 --> 00:36:12,640
uh 17 so way way significantly better

1097
00:36:12,640 --> 00:36:14,079
than anything else

1098
00:36:14,079 --> 00:36:17,599
and for by seven it is better than snr

1099
00:36:17,599 --> 00:36:20,480
not by much but it is better uh to be

1100
00:36:20,480 --> 00:36:22,000
honest and to predict that that's

1101
00:36:22,000 --> 00:36:24,720
for four points when you look at five

1102
00:36:24,720 --> 00:36:26,079
six seven eight points

1103
00:36:26,079 --> 00:36:28,320
sometimes one goes above the other but

1104
00:36:28,320 --> 00:36:29,280
overall

1105
00:36:29,280 --> 00:36:31,280
score perform better so we have a

1106
00:36:31,280 --> 00:36:32,960
technique now which works really

1107
00:36:32,960 --> 00:36:35,520
good really well and if you compare the

1108
00:36:35,520 --> 00:36:37,200
quality of the choices

1109
00:36:37,200 --> 00:36:38,960
the first thing you will notice is

1110
00:36:38,960 --> 00:36:40,560
actually the score traces

1111
00:36:40,560 --> 00:36:43,920
is very similar to the snr one four by

1112
00:36:43,920 --> 00:36:44,320
zero

1113
00:36:44,320 --> 00:36:46,720
except it's way more defined most of the

1114
00:36:46,720 --> 00:36:47,520
noise

1115
00:36:47,520 --> 00:36:50,240
that were plaguing the snr is gone so

1116
00:36:50,240 --> 00:36:50,880
now we have

1117
00:36:50,880 --> 00:36:54,320
three two very clean poor traces that we

1118
00:36:54,320 --> 00:36:55,440
know

1119
00:36:55,440 --> 00:36:57,680
is responsible for most of the leakage

1120
00:36:57,680 --> 00:36:59,119
which is good also

1121
00:36:59,119 --> 00:37:01,680
scold believes the it might not be very

1122
00:37:01,680 --> 00:37:03,280
visible on the slide but there is a red

1123
00:37:03,280 --> 00:37:03,920
bar

1124
00:37:03,920 --> 00:37:05,359
in the middle so school believes the

1125
00:37:05,359 --> 00:37:06,800
second spike is more important than the

1126
00:37:06,800 --> 00:37:07,760
first one

1127
00:37:07,760 --> 00:37:10,560
whereas for the snr there's a reverse so

1128
00:37:10,560 --> 00:37:11,599
interesting

1129
00:37:11,599 --> 00:37:13,599
the differences between the two and in

1130
00:37:13,599 --> 00:37:15,680
the middle i put grad cam plus plus and

1131
00:37:15,680 --> 00:37:16,720
you can see this choice is

1132
00:37:16,720 --> 00:37:18,000
non-informative so

1133
00:37:18,000 --> 00:37:20,160
we have confidence we are finding almost

1134
00:37:20,160 --> 00:37:21,680
the same places so we are confident it

1135
00:37:21,680 --> 00:37:22,079
actually

1136
00:37:22,079 --> 00:37:24,560
makes sense at least it actually makes

1137
00:37:24,560 --> 00:37:26,480
sense from even a statistical analysis

1138
00:37:26,480 --> 00:37:27,359
so

1139
00:37:27,359 --> 00:37:29,359
that gives us a lot of confidence that

1140
00:37:29,359 --> 00:37:32,079
is the right technique for the job

1141
00:37:32,079 --> 00:37:34,960
so now we're back on track we have a

1142
00:37:34,960 --> 00:37:36,400
technique which decreases accuracy the

1143
00:37:36,400 --> 00:37:36,800
most

1144
00:37:36,800 --> 00:37:39,520
and have very very low noise so when we

1145
00:37:39,520 --> 00:37:40,960
use it to explain

1146
00:37:40,960 --> 00:37:43,520
the code the developer only have a few

1147
00:37:43,520 --> 00:37:45,599
places to look at which is really good

1148
00:37:45,599 --> 00:37:48,720
so now how do we go from the leakage map

1149
00:37:48,720 --> 00:37:51,040
to the code and this is where i'm going

1150
00:37:51,040 --> 00:37:52,480
to be

1151
00:37:52,480 --> 00:37:54,079
a little bit hand wavy because this is

1152
00:37:54,079 --> 00:37:55,440
the power cord we are still actively

1153
00:37:55,440 --> 00:37:56,480
working on

1154
00:37:56,480 --> 00:37:58,160
and i don't have a slide to show you the

1155
00:37:58,160 --> 00:37:59,760
end result but i will explain you where

1156
00:37:59,760 --> 00:38:00,240
we are

1157
00:38:00,240 --> 00:38:04,560
at and i can show you where we are so

1158
00:38:04,560 --> 00:38:06,480
the idea is how we go the first thing we

1159
00:38:06,480 --> 00:38:08,400
need to go is as i said you go from the

1160
00:38:08,400 --> 00:38:10,400
traces to cpu instruction

1161
00:38:10,400 --> 00:38:12,160
so cpu instruction is going to depend on

1162
00:38:12,160 --> 00:38:13,920
two things the first thing is what type

1163
00:38:13,920 --> 00:38:16,079
of cpu you're using so we're using an

1164
00:38:16,079 --> 00:38:18,720
arm so we need to know for each

1165
00:38:18,720 --> 00:38:19,359
instruction

1166
00:38:19,359 --> 00:38:22,160
how many cycles it take then we need the

1167
00:38:22,160 --> 00:38:23,520
firmware which is basically

1168
00:38:23,520 --> 00:38:25,680
the compiled version of the code which

1169
00:38:25,680 --> 00:38:27,920
data tuition section are executed

1170
00:38:27,920 --> 00:38:31,599
we need to fill the leak map which shows

1171
00:38:31,599 --> 00:38:33,040
which point are important for us

1172
00:38:33,040 --> 00:38:34,640
and we also need to have a state

1173
00:38:34,640 --> 00:38:36,720
automaton who instrument

1174
00:38:36,720 --> 00:38:40,160
the emulator to be able to reproduce the

1175
00:38:40,160 --> 00:38:40,880
encryption

1176
00:38:40,880 --> 00:38:45,280
and so we can basically reconstruct

1177
00:38:45,280 --> 00:38:49,599
what instruction were executed at which

1178
00:38:49,599 --> 00:38:51,599
time and then map it back to to the

1179
00:38:51,599 --> 00:38:53,200
trace which will tell you

1180
00:38:53,200 --> 00:38:56,880
what point in time is important right so

1181
00:38:56,880 --> 00:38:57,440
with all

1182
00:38:57,440 --> 00:39:01,920
those things what you get is you get a

1183
00:39:01,920 --> 00:39:05,119
heat map of which cpu instructions

1184
00:39:05,119 --> 00:39:08,720
are uh the most important

1185
00:39:08,720 --> 00:39:10,240
this is the part we are still actively

1186
00:39:10,240 --> 00:39:11,680
developing because we need to refine a

1187
00:39:11,680 --> 00:39:12,640
little bit uh

1188
00:39:12,640 --> 00:39:15,839
how our clock and

1189
00:39:15,839 --> 00:39:17,760
program counter are implemented to be as

1190
00:39:17,760 --> 00:39:19,119
accurate as possible

1191
00:39:19,119 --> 00:39:22,720
but this is what it does then

1192
00:39:22,720 --> 00:39:24,320
what we need is now we have assembly

1193
00:39:24,320 --> 00:39:26,640
code so we can probably end up there

1194
00:39:26,640 --> 00:39:28,560
worst case but we want to go one step

1195
00:39:28,560 --> 00:39:30,160
further which is can we go back

1196
00:39:30,160 --> 00:39:32,240
from assembly code to c and this is

1197
00:39:32,240 --> 00:39:33,359
something which is well known

1198
00:39:33,359 --> 00:39:35,839
in the security community that's what is

1199
00:39:35,839 --> 00:39:37,119
the bright and better of reverse

1200
00:39:37,119 --> 00:39:38,640
engineering so there's a lot of

1201
00:39:38,640 --> 00:39:40,720
product to do that but essentially what

1202
00:39:40,720 --> 00:39:42,000
you do is you take your

1203
00:39:42,000 --> 00:39:45,520
asm and then our firmware was compiled

1204
00:39:45,520 --> 00:39:46,560
with debug code

1205
00:39:46,560 --> 00:39:48,240
again within the debugger setting white

1206
00:39:48,240 --> 00:39:49,839
box so we know

1207
00:39:49,839 --> 00:39:52,160
what a debug could and then the debugger

1208
00:39:52,160 --> 00:39:53,920
can easily

1209
00:39:53,920 --> 00:39:56,240
go back from instruction to line of code

1210
00:39:56,240 --> 00:39:58,079
thanks to those debug symbol

1211
00:39:58,079 --> 00:40:00,400
so from there it's very trivial to go

1212
00:40:00,400 --> 00:40:01,680
back to the link

1213
00:40:01,680 --> 00:40:04,800
to the leak code so now we have the code

1214
00:40:04,800 --> 00:40:07,920
alongside the the heat map

1215
00:40:07,920 --> 00:40:10,000
and hopefully well then now you know

1216
00:40:10,000 --> 00:40:12,000
which line of code is causing the most

1217
00:40:12,000 --> 00:40:14,720
leakage according to machine learning

1218
00:40:14,720 --> 00:40:17,359
and because as i showed you earlier the

1219
00:40:17,359 --> 00:40:18,640
machine learning

1220
00:40:18,640 --> 00:40:21,119
explanation is very well correlated with

1221
00:40:21,119 --> 00:40:22,079
the snr

1222
00:40:22,079 --> 00:40:24,000
we believe it's also going to surprise

1223
00:40:24,000 --> 00:40:25,760
the template attack which use this kind

1224
00:40:25,760 --> 00:40:26,400
of

1225
00:40:26,400 --> 00:40:28,400
statistical ratio so everything should

1226
00:40:28,400 --> 00:40:29,680
fit together

1227
00:40:29,680 --> 00:40:32,079
um i wish i would show you for 10 years

1228
00:40:32,079 --> 00:40:32,800
the exact line

1229
00:40:32,800 --> 00:40:35,040
but as i said at this time of recording

1230
00:40:35,040 --> 00:40:36,319
on july 14

1231
00:40:36,319 --> 00:40:38,079
we are not there yet uh we are still

1232
00:40:38,079 --> 00:40:40,480
refining it so hopefully we get that uh

1233
00:40:40,480 --> 00:40:42,079
by blackhead

1234
00:40:42,079 --> 00:40:45,440
all right so the idea of

1235
00:40:45,440 --> 00:40:48,480
scold is that uh we can build this tool

1236
00:40:48,480 --> 00:40:50,000
which is going to hopefully empower

1237
00:40:50,000 --> 00:40:53,040
developers to quickly figure out what to

1238
00:40:53,040 --> 00:40:54,480
patch and focus

1239
00:40:54,480 --> 00:40:57,520
on developing stronger crypto more than

1240
00:40:57,520 --> 00:40:59,040
how to do the size and attack and

1241
00:40:59,040 --> 00:41:00,720
everything and freeing up more time

1242
00:41:00,720 --> 00:41:02,640
to focus on design and experimenting

1243
00:41:02,640 --> 00:41:04,720
with new masking technique or protection

1244
00:41:04,720 --> 00:41:06,240
or things like this

1245
00:41:06,240 --> 00:41:09,599
we are really excited to test it

1246
00:41:09,599 --> 00:41:11,040
more and have a little bit more use

1247
00:41:11,040 --> 00:41:13,040
cases for it and show

1248
00:41:13,040 --> 00:41:16,560
how bright more how well generalized

1249
00:41:16,560 --> 00:41:18,160
so far we are very hopeful is going to

1250
00:41:18,160 --> 00:41:20,160
do that and it's going to help us

1251
00:41:20,160 --> 00:41:21,839
the main benefit and take away from this

1252
00:41:21,839 --> 00:41:25,040
talk should be that a

1253
00:41:25,040 --> 00:41:28,319
as we mentioned also last year

1254
00:41:28,319 --> 00:41:30,960
machine learning is a new way is a way

1255
00:41:30,960 --> 00:41:32,400
to perform state-of-the-art

1256
00:41:32,400 --> 00:41:35,680
sci attack automatically it actually

1257
00:41:35,680 --> 00:41:38,960
is changing how we approach a child

1258
00:41:38,960 --> 00:41:40,160
channel attack because it's

1259
00:41:40,160 --> 00:41:41,760
allowed to automate a lot of things and

1260
00:41:41,760 --> 00:41:43,920
also take into account linearity that we

1261
00:41:43,920 --> 00:41:45,440
were not able to do before

1262
00:41:45,440 --> 00:41:48,079
however they also have as i showed you

1263
00:41:48,079 --> 00:41:49,200
with the

1264
00:41:49,200 --> 00:41:52,240
link how the leak map actually aligned

1265
00:41:52,240 --> 00:41:53,119
that they're also

1266
00:41:53,119 --> 00:41:54,720
finding the same type of stuff that you

1267
00:41:54,720 --> 00:41:56,880
would find with a snr plus more so it's

1268
00:41:56,880 --> 00:41:57,680
literally a

1269
00:41:57,680 --> 00:42:00,880
upgrade the second thing is

1270
00:42:00,880 --> 00:42:03,200
we showed you in the middle of the talk

1271
00:42:03,200 --> 00:42:04,079
how

1272
00:42:04,079 --> 00:42:06,400
we can actually combine this ai

1273
00:42:06,400 --> 00:42:07,280
capability

1274
00:42:07,280 --> 00:42:10,480
with dynamic execution to try to find

1275
00:42:10,480 --> 00:42:11,599
leakage

1276
00:42:11,599 --> 00:42:13,839
almost automatically and then helping

1277
00:42:13,839 --> 00:42:15,599
reduce the development cost of

1278
00:42:15,599 --> 00:42:17,359
secure hardware and hopefully will help

1279
00:42:17,359 --> 00:42:19,520
us to have more secure hardware through

1280
00:42:19,520 --> 00:42:20,400
the world

1281
00:42:20,400 --> 00:42:24,960
and finally um there is a lot to be done

1282
00:42:24,960 --> 00:42:27,760
while this talk show you a way to do it

1283
00:42:27,760 --> 00:42:29,839
we don't claim this is the best way just

1284
00:42:29,839 --> 00:42:30,160
a

1285
00:42:30,160 --> 00:42:32,480
first step towards that idea there was

1286
00:42:32,480 --> 00:42:33,920
other research by the way i need to

1287
00:42:33,920 --> 00:42:34,640
mention

1288
00:42:34,640 --> 00:42:36,720
who are exploring the same idea some of

1289
00:42:36,720 --> 00:42:37,680
them from the

1290
00:42:37,680 --> 00:42:40,160
nc did some of these type of research as

1291
00:42:40,160 --> 00:42:40,800
well on

1292
00:42:40,800 --> 00:42:42,880
how to use expandability i don't think

1293
00:42:42,880 --> 00:42:44,000
they went as far as us but

1294
00:42:44,000 --> 00:42:46,240
this idea is exchanging community and

1295
00:42:46,240 --> 00:42:47,520
it's a very active community there's a

1296
00:42:47,520 --> 00:42:48,960
lot of work to be done in the

1297
00:42:48,960 --> 00:42:51,359
idea of combining alpha playing ai for

1298
00:42:51,359 --> 00:42:52,319
session now

1299
00:42:52,319 --> 00:42:54,240
whether it's for improving attacks or

1300
00:42:54,240 --> 00:42:55,440
improving defenses

1301
00:42:55,440 --> 00:42:57,839
and so we're really excited to see how

1302
00:42:57,839 --> 00:42:59,200
this is going to turn out and

1303
00:42:59,200 --> 00:43:01,200
have collaboration with as many people

1304
00:43:01,200 --> 00:43:03,280
as we can so we can build the most

1305
00:43:03,280 --> 00:43:05,920
secure hardware we can thank you so much

1306
00:43:05,920 --> 00:43:08,079
for attending this talk virtually i hope

1307
00:43:08,079 --> 00:43:09,200
you're all safe at home

1308
00:43:09,200 --> 00:43:11,440
and then if you like to keep up with our

1309
00:43:11,440 --> 00:43:13,359
research and figure out how we

1310
00:43:13,359 --> 00:43:15,680
make progress and be aware of new

1311
00:43:15,680 --> 00:43:16,480
release

1312
00:43:16,480 --> 00:43:20,160
you can always check out the website

1313
00:43:20,160 --> 00:43:23,200
and you can also follow us on twitter

1314
00:43:23,200 --> 00:43:25,599
where we try to post our update thank

1315
00:43:25,599 --> 00:43:41,839
you so much for attending

1316
00:44:07,460 --> 00:44:19,709
[Music]

1317
00:44:36,240 --> 00:44:38,079
oh i'm sorry yes it was i approached it

1318
00:44:38,079 --> 00:44:39,440
i was saying thank you so much for you

1319
00:44:39,440 --> 00:44:40,319
guys

1320
00:44:40,319 --> 00:44:42,079
uh for attending the video i hope you

1321
00:44:42,079 --> 00:44:43,520
can hear me now

1322
00:44:43,520 --> 00:44:46,480
and um

1323
00:44:48,720 --> 00:44:52,879
can you guys yes no

1324
00:44:56,400 --> 00:44:59,760
you should be able to hear me

1325
00:45:01,680 --> 00:45:04,640
okay so okay sorry uh there was a

1326
00:45:04,640 --> 00:45:06,640
problem with the

1327
00:45:06,640 --> 00:45:08,079
theojo so i said thank you so much for

1328
00:45:08,079 --> 00:45:18,480
attending uh as you have seen so about

1329
00:45:18,480 --> 00:45:20,079
two weeks ago so we didn't get

1330
00:45:20,079 --> 00:45:22,800
all the results in i did add them to the

1331
00:45:22,800 --> 00:45:24,560
to slide deck

1332
00:45:24,560 --> 00:45:26,640
and if you take the latest slider you'll

1333
00:45:26,640 --> 00:45:28,240
see that we actually did in

1334
00:45:28,240 --> 00:45:30,800
in the end using our tools called the

1335
00:45:30,800 --> 00:45:32,079
question was

1336
00:45:32,079 --> 00:45:33,520
which would we use well that's our own

1337
00:45:33,520 --> 00:45:34,480
tool and we're going to make it

1338
00:45:34,480 --> 00:45:36,720
available on github

1339
00:45:36,720 --> 00:45:38,240
and you will be able to play with it and

1340
00:45:38,240 --> 00:45:40,400
reproduce the research you saw

1341
00:45:40,400 --> 00:45:43,920
and then you will be able to retest what

1342
00:45:43,920 --> 00:45:45,839
we found and actually in the end

1343
00:45:45,839 --> 00:45:49,760
we are able to find and to see

1344
00:45:49,760 --> 00:45:53,040
that yes it's called which i want to

1345
00:45:53,040 --> 00:45:55,280
actually find the leakage exactly which

1346
00:45:55,280 --> 00:45:58,000
is to the exact line of tiny ais which

1347
00:45:58,000 --> 00:45:59,680
is the one which do survive

1348
00:45:59,680 --> 00:46:01,280
so we have confidence that it works at

1349
00:46:01,280 --> 00:46:02,720
least

1350
00:46:02,720 --> 00:46:04,880
in that case now that if we're saying

1351
00:46:04,880 --> 00:46:06,400
answering the question

1352
00:46:06,400 --> 00:46:08,400
it is very very difficult to get it to

1353
00:46:08,400 --> 00:46:10,000
work because you have to emulate

1354
00:46:10,000 --> 00:46:10,960
perfectly the

1355
00:46:10,960 --> 00:46:14,079
cycles of the cpu so we have to be cycle

1356
00:46:14,079 --> 00:46:15,440
precise and we have

1357
00:46:15,440 --> 00:46:17,200
the expansion has to be to the exact

1358
00:46:17,200 --> 00:46:19,280
point so

1359
00:46:19,280 --> 00:46:21,280
the research paper that i was mentioning

1360
00:46:21,280 --> 00:46:22,560
is going to appear a little bit

1361
00:46:22,560 --> 00:46:23,760
later we are trying on different

1362
00:46:23,760 --> 00:46:26,000
implementation because we don't know

1363
00:46:26,000 --> 00:46:27,920
what we can do or can do

1364
00:46:27,920 --> 00:46:30,640
so i hope you have a complete question

1365
00:46:30,640 --> 00:46:32,240
i'll take more questions during the

1366
00:46:32,240 --> 00:46:35,119
q a i guess in the green room and then

1367
00:46:35,119 --> 00:46:36,400
really appreciate uh

1368
00:46:36,400 --> 00:46:39,119
you bear with me and uh with the google

1369
00:46:39,119 --> 00:46:40,160
incident and

1370
00:46:40,160 --> 00:46:41,920
hope you find it interesting and

1371
00:46:41,920 --> 00:46:43,359
inspiring and as usual

1372
00:46:43,359 --> 00:46:44,560
always can you answer questions on

1373
00:46:44,560 --> 00:46:46,720
twitter or by email

1374
00:46:46,720 --> 00:46:50,078
thank you so much for attending

