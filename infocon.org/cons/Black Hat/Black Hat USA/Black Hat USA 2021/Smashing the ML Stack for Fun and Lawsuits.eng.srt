1
00:00:01,130 --> 00:00:14,690
[Music]

2
00:00:15,759 --> 00:00:18,160
hello and welcome to the session on

3
00:00:18,160 --> 00:00:20,640
smashing the ml stack uh for fun and

4
00:00:20,640 --> 00:00:23,199
lawsuits my name is rom i'm from the

5
00:00:23,199 --> 00:00:25,359
azure trustworthy machine learning team

6
00:00:25,359 --> 00:00:28,000
uh where my charter is to empower

7
00:00:28,000 --> 00:00:30,080
customers to think about developing and

8
00:00:30,080 --> 00:00:31,279
deploying machine learning systems

9
00:00:31,279 --> 00:00:33,280
securely uh kendra would you like to

10
00:00:33,280 --> 00:00:35,600
introduce yourself hi everybody my name

11
00:00:35,600 --> 00:00:37,440
is kendra albert i'm an attorney at the

12
00:00:37,440 --> 00:00:39,200
harvard law school cyberlaw clinic where

13
00:00:39,200 --> 00:00:41,200
i practice and teach technology law

14
00:00:41,200 --> 00:00:43,920
specifically the law law around computer

15
00:00:43,920 --> 00:00:45,440
security

16
00:00:45,440 --> 00:00:47,440
this work was also done in uh

17
00:00:47,440 --> 00:00:49,760
coordination with jonathan penny from

18
00:00:49,760 --> 00:00:51,920
york university and bruce schneier from

19
00:00:51,920 --> 00:00:54,320
the harvard kennedy school

20
00:00:54,320 --> 00:00:56,559
if you take one thing away from this

21
00:00:56,559 --> 00:00:57,600
talk

22
00:00:57,600 --> 00:00:59,680
it should be that adversarial machine

23
00:00:59,680 --> 00:01:01,840
learning introduces really novel legal

24
00:01:01,840 --> 00:01:04,559
problems for security researchers um

25
00:01:04,559 --> 00:01:07,119
it's really a lot of things are unclear

26
00:01:07,119 --> 00:01:07,840
but

27
00:01:07,840 --> 00:01:09,840
the one certainty is that if you can

28
00:01:09,840 --> 00:01:12,080
start like doing putting up technical

29
00:01:12,080 --> 00:01:14,159
defenses it can actually help you with

30
00:01:14,159 --> 00:01:16,400
your legal claims

31
00:01:16,400 --> 00:01:18,720
and this part this talk really has three

32
00:01:18,720 --> 00:01:21,200
parts to it in the first part i'm going

33
00:01:21,200 --> 00:01:24,000
to um walk through a very high level

34
00:01:24,000 --> 00:01:26,960
overview and a primer on what attacking

35
00:01:26,960 --> 00:01:28,720
machine learning systems entails and

36
00:01:28,720 --> 00:01:30,560
then kendra is going to deliver the meat

37
00:01:30,560 --> 00:01:33,040
of the talk about the legal implications

38
00:01:33,040 --> 00:01:35,200
uh for ai researchers and wrap it up

39
00:01:35,200 --> 00:01:37,520
with what a risk assessment of ml

40
00:01:37,520 --> 00:01:40,159
systems actually entails

41
00:01:40,159 --> 00:01:41,360
so

42
00:01:41,360 --> 00:01:43,280
um i i'm gonna start with a very brief

43
00:01:43,280 --> 00:01:45,360
quiz and i don't want you to overthink

44
00:01:45,360 --> 00:01:48,159
it i just wanted to you know just tell

45
00:01:48,159 --> 00:01:49,360
me the first thing that comes to your

46
00:01:49,360 --> 00:01:51,280
mind or at least tell yourself the first

47
00:01:51,280 --> 00:01:52,880
thing that comes to mind again don't

48
00:01:52,880 --> 00:01:57,280
overthink this uh what number is this

49
00:01:57,280 --> 00:02:00,799
uh what kind of animal is this

50
00:02:00,799 --> 00:02:03,520
and finally would you clean your house

51
00:02:03,520 --> 00:02:05,119
with this

52
00:02:05,119 --> 00:02:05,840
well

53
00:02:05,840 --> 00:02:08,878
if you said you know five or you know a

54
00:02:08,878 --> 00:02:12,160
bird or some squiggly lines well

55
00:02:12,160 --> 00:02:16,000
congratulations you are 100 human and i

56
00:02:16,000 --> 00:02:17,920
know for a fact that your eyesight is

57
00:02:17,920 --> 00:02:20,160
not powered by machine learning

58
00:02:20,160 --> 00:02:21,840
and that is because like for

59
00:02:21,840 --> 00:02:23,200
state-of-the-art machine learning

60
00:02:23,200 --> 00:02:25,520
systems the first picture actually

61
00:02:25,520 --> 00:02:27,360
represents a three

62
00:02:27,360 --> 00:02:29,760
and the bird which just like you know by

63
00:02:29,760 --> 00:02:31,920
taking uh slightly and cropping and

64
00:02:31,920 --> 00:02:34,560
rotating and just like ms paint confuses

65
00:02:34,560 --> 00:02:36,560
state-of-the-art computer vision systems

66
00:02:36,560 --> 00:02:39,360
to misrecognize it as an orangutan and

67
00:02:39,360 --> 00:02:42,239
that squiggly lines according to like

68
00:02:42,239 --> 00:02:44,080
uh a machine learning system is actually

69
00:02:44,080 --> 00:02:45,760
a vacuum cleaner

70
00:02:45,760 --> 00:02:46,560
well

71
00:02:46,560 --> 00:02:48,239
this is really what we're here to talk

72
00:02:48,239 --> 00:02:51,599
about how an adversary can take uh

73
00:02:51,599 --> 00:02:54,080
systems and kind of like add specific

74
00:02:54,080 --> 00:02:56,640
perturbations or even cause the ml

75
00:02:56,640 --> 00:02:59,120
system to have compromise in their

76
00:02:59,120 --> 00:03:01,200
confidentiality integrity and

77
00:03:01,200 --> 00:03:03,040
availability guarantees and this is

78
00:03:03,040 --> 00:03:05,120
really what adversarial machine learning

79
00:03:05,120 --> 00:03:08,000
uh kind of focuses on now this field

80
00:03:08,000 --> 00:03:11,599
goes back really long time to the 1990s

81
00:03:11,599 --> 00:03:14,159
but today there's a boom in adversarial

82
00:03:14,159 --> 00:03:16,159
machine learning research

83
00:03:16,159 --> 00:03:18,080
after a seminal paper that was published

84
00:03:18,080 --> 00:03:19,599
in 2016

85
00:03:19,599 --> 00:03:21,599
researchers are kind of publishing on

86
00:03:21,599 --> 00:03:22,879
this topic

87
00:03:22,879 --> 00:03:24,879
front right and center and you can

88
00:03:24,879 --> 00:03:27,680
really not escape any conference without

89
00:03:27,680 --> 00:03:29,680
adverse machine learning uh being spoken

90
00:03:29,680 --> 00:03:31,120
about and think about just like in black

91
00:03:31,120 --> 00:03:33,360
hat uh right now

92
00:03:33,360 --> 00:03:35,120
and there's a whole like lot of myth

93
00:03:35,120 --> 00:03:36,959
that you know adversarial machine

94
00:03:36,959 --> 00:03:38,720
learning is really just an academic

95
00:03:38,720 --> 00:03:40,959
pursuit and really does not have like

96
00:03:40,959 --> 00:03:43,680
impact on real life well that's not what

97
00:03:43,680 --> 00:03:46,879
the data shows uh commercial ml systems

98
00:03:46,879 --> 00:03:50,480
from amazon to microsoft to google to

99
00:03:50,480 --> 00:03:52,959
facebook have all had their machine

100
00:03:52,959 --> 00:03:55,120
learning systems kind of like fooled

101
00:03:55,120 --> 00:03:57,439
evaded you know some of the ml models

102
00:03:57,439 --> 00:03:59,519
replicated and some of the private

103
00:03:59,519 --> 00:04:01,920
training data recreated

104
00:04:01,920 --> 00:04:04,799
so it's it's also not a myth like you

105
00:04:04,799 --> 00:04:05,599
know

106
00:04:05,599 --> 00:04:07,760
that um machine learning these kind of

107
00:04:07,760 --> 00:04:10,239
attacks may only come to images i i

108
00:04:10,239 --> 00:04:12,480
showed you like you know like examples

109
00:04:12,480 --> 00:04:14,159
of images you might have seen like one

110
00:04:14,159 --> 00:04:16,720
in tesla uh but let's look at like how

111
00:04:16,720 --> 00:04:19,519
it uh manifests in like audio as well so

112
00:04:19,519 --> 00:04:22,160
i'm gonna i'm gonna play you like two

113
00:04:22,160 --> 00:04:24,639
samples and i wanted to and i want you

114
00:04:24,639 --> 00:04:25,680
to tell me

115
00:04:25,680 --> 00:04:27,520
which one was tampered by with an

116
00:04:27,520 --> 00:04:28,400
adversary

117
00:04:28,400 --> 00:04:29,199
okay

118
00:04:29,199 --> 00:04:33,199
um i'm gonna start playing the first one

119
00:04:36,000 --> 00:04:38,550
and now i'm gonna play the second sample

120
00:04:38,550 --> 00:04:41,610
[Music]

121
00:04:42,479 --> 00:04:44,479
so think of which one kind of like an

122
00:04:44,479 --> 00:04:48,160
adversary tampered with uh in your mind

123
00:04:48,160 --> 00:04:50,880
well if you said the second one

124
00:04:50,880 --> 00:04:53,120
um you're kind of out of luck because

125
00:04:53,120 --> 00:04:56,560
the first one if you like um pass it

126
00:04:56,560 --> 00:04:58,320
through modular state-of-the-art deep

127
00:04:58,320 --> 00:05:00,880
speech uh uh system it actually

128
00:05:00,880 --> 00:05:03,199
transcribes to alexa order like 100

129
00:05:03,199 --> 00:05:05,440
frozen pizzas and the second one really

130
00:05:05,440 --> 00:05:07,759
doesn't transcribe to anything

131
00:05:07,759 --> 00:05:08,560
so

132
00:05:08,560 --> 00:05:10,479
with this in mind like you know i want

133
00:05:10,479 --> 00:05:12,400
to give you like uh all the knowledge

134
00:05:12,400 --> 00:05:14,720
you would need to kind of appreciate the

135
00:05:14,720 --> 00:05:16,639
legal analysis that we've done

136
00:05:16,639 --> 00:05:19,680
and the setup for this talk is a pretty

137
00:05:19,680 --> 00:05:20,960
vanilla

138
00:05:20,960 --> 00:05:22,720
pipeline of like machine learning

139
00:05:22,720 --> 00:05:24,400
systems you know there's some sort of

140
00:05:24,400 --> 00:05:26,639
training data that is fed into an

141
00:05:26,639 --> 00:05:29,520
algorithm uh and then you get a model at

142
00:05:29,520 --> 00:05:31,680
the uh at the end of it

143
00:05:31,680 --> 00:05:34,400
and um you know this could be like an

144
00:05:34,400 --> 00:05:36,160
image recognition system or a speech

145
00:05:36,160 --> 00:05:37,840
recognitions december you know you would

146
00:05:37,840 --> 00:05:41,440
pass like say images um as training data

147
00:05:41,440 --> 00:05:43,199
to an algorithm like uh say

148
00:05:43,199 --> 00:05:45,600
convolutional neural nets get a model

149
00:05:45,600 --> 00:05:48,880
and you as a user kind of like you know

150
00:05:48,880 --> 00:05:51,600
uh may submit a query of say an image

151
00:05:51,600 --> 00:05:53,600
and get a response whether the image

152
00:05:53,600 --> 00:05:55,600
contains a cat or not

153
00:05:55,600 --> 00:05:56,479
now

154
00:05:56,479 --> 00:05:59,360
the assumption for this entire talk is

155
00:05:59,360 --> 00:06:02,479
that the attacker can only send queries

156
00:06:02,479 --> 00:06:04,240
and observe responses

157
00:06:04,240 --> 00:06:06,880
she has no clue about like she has no

158
00:06:06,880 --> 00:06:07,919
access

159
00:06:07,919 --> 00:06:10,080
about like what algorithm the company is

160
00:06:10,080 --> 00:06:12,560
using what the where you know no access

161
00:06:12,560 --> 00:06:14,720
no special access to the training data

162
00:06:14,720 --> 00:06:16,800
or any architectural like insights about

163
00:06:16,800 --> 00:06:18,720
it so this is what we call technically a

164
00:06:18,720 --> 00:06:21,919
black box attack uh but again just think

165
00:06:21,919 --> 00:06:24,000
that the attacker can only

166
00:06:24,000 --> 00:06:26,960
send queries and observe responses

167
00:06:26,960 --> 00:06:29,919
now even with such as a strict

168
00:06:29,919 --> 00:06:31,759
restriction on what the attacker can

169
00:06:31,759 --> 00:06:33,840
kind of do

170
00:06:33,840 --> 00:06:36,560
are a whole bunch of um things that she

171
00:06:36,560 --> 00:06:38,160
can kind of achieve

172
00:06:38,160 --> 00:06:41,360
the first one is called evasion tax this

173
00:06:41,360 --> 00:06:45,039
is when an adversary is kind of like uh

174
00:06:45,039 --> 00:06:46,000
sending

175
00:06:46,000 --> 00:06:48,639
crafting specially crafted queries to

176
00:06:48,639 --> 00:06:50,560
the machine learning system

177
00:06:50,560 --> 00:06:53,680
and then as a and then can control

178
00:06:53,680 --> 00:06:55,680
the response from the machine learning

179
00:06:55,680 --> 00:06:56,639
system

180
00:06:56,639 --> 00:06:57,520
so

181
00:06:57,520 --> 00:07:00,639
um in the classic case over here is like

182
00:07:00,639 --> 00:07:02,479
new york times actually did a recent

183
00:07:02,479 --> 00:07:05,280
study on how the chinese government is

184
00:07:05,280 --> 00:07:07,840
uh spreading propaganda on twitter what

185
00:07:07,840 --> 00:07:10,560
they found was that um in order for the

186
00:07:10,560 --> 00:07:11,919
to escape

187
00:07:11,919 --> 00:07:13,199
or um

188
00:07:13,199 --> 00:07:15,840
the twitter's algorithm for flagging

189
00:07:15,840 --> 00:07:18,560
spam or not the chinese government

190
00:07:18,560 --> 00:07:20,639
essentially kind of appended random

191
00:07:20,639 --> 00:07:22,960
characters at the end of the tweet to

192
00:07:22,960 --> 00:07:25,120
confuse the algorithm

193
00:07:25,120 --> 00:07:27,280
this was also seen in another attack

194
00:07:27,280 --> 00:07:30,000
when researchers from skylight

195
00:07:30,000 --> 00:07:32,880
essentially confused uh silence's a

196
00:07:32,880 --> 00:07:35,199
machine uh malware evasion system by

197
00:07:35,199 --> 00:07:38,960
appending benign code into uh wannacry

198
00:07:38,960 --> 00:07:41,039
ransomware and that and therefore the

199
00:07:41,039 --> 00:07:43,120
machine learning system also classified

200
00:07:43,120 --> 00:07:45,440
that as benign web

201
00:07:45,440 --> 00:07:48,160
the second example is poisoning attack

202
00:07:48,160 --> 00:07:51,039
here the adversary is able to take

203
00:07:51,039 --> 00:07:52,800
advantage of the fact that machine

204
00:07:52,800 --> 00:07:55,280
learning systems constantly uh take the

205
00:07:55,280 --> 00:07:58,000
responses and use that as feedback into

206
00:07:58,000 --> 00:07:59,360
their system

207
00:07:59,360 --> 00:08:02,160
uh over here the adversary is corrupting

208
00:08:02,160 --> 00:08:05,520
the training data of the algorithm um

209
00:08:05,520 --> 00:08:07,599
uh the study over here is like a

210
00:08:07,599 --> 00:08:10,160
microsoft's t example where like it was

211
00:08:10,160 --> 00:08:12,639
supposed to emulate a sweet 16 year old

212
00:08:12,639 --> 00:08:14,720
personality um

213
00:08:14,720 --> 00:08:15,840
using like state-of-the-art

214
00:08:15,840 --> 00:08:18,160
conversational like algorithms but when

215
00:08:18,160 --> 00:08:20,720
trolls from reddit and 4chan descended

216
00:08:20,720 --> 00:08:23,039
onto twitter um

217
00:08:23,039 --> 00:08:25,120
essentially the bot was corrupted and

218
00:08:25,120 --> 00:08:27,280
then went from a 16 year old personality

219
00:08:27,280 --> 00:08:28,319
to this like

220
00:08:28,319 --> 00:08:29,440
bigoted

221
00:08:29,440 --> 00:08:32,159
like uh bought and microsoft

222
00:08:32,159 --> 00:08:34,080
had to yank it within 24 hours of

223
00:08:34,080 --> 00:08:36,719
deploying uh

224
00:08:36,719 --> 00:08:37,519
the

225
00:08:37,519 --> 00:08:39,679
these were all um attacks that kind of

226
00:08:39,679 --> 00:08:41,760
like incorporated like low-skilled

227
00:08:41,760 --> 00:08:44,080
adversaries um there can also be

228
00:08:44,080 --> 00:08:46,000
high-skilled adversaries can cause more

229
00:08:46,000 --> 00:08:48,320
damage the first one is model

230
00:08:48,320 --> 00:08:50,880
inversion when the attacker recovers the

231
00:08:50,880 --> 00:08:53,600
underlying private training data used in

232
00:08:53,600 --> 00:08:56,000
the model by again sending careful

233
00:08:56,000 --> 00:08:58,399
queries carefully crafted queries and

234
00:08:58,399 --> 00:09:00,160
observing the responses

235
00:09:00,160 --> 00:09:02,399
um and over here you can see like a

236
00:09:02,399 --> 00:09:04,800
study where just again by interacting

237
00:09:04,800 --> 00:09:06,880
with the machine learning system with a

238
00:09:06,880 --> 00:09:09,440
black box style setting um

239
00:09:09,440 --> 00:09:11,200
the adversary was able to reconstruct

240
00:09:11,200 --> 00:09:13,360
the private training data that could

241
00:09:13,360 --> 00:09:16,800
potentially lead to loss of privacy um

242
00:09:16,800 --> 00:09:19,760
in this particular example

243
00:09:19,760 --> 00:09:22,640
and finally there's model uh stealing

244
00:09:22,640 --> 00:09:23,920
attack

245
00:09:23,920 --> 00:09:26,160
over here like um

246
00:09:26,160 --> 00:09:28,560
two years ago openai

247
00:09:28,560 --> 00:09:29,600
released

248
00:09:29,600 --> 00:09:31,920
a model called gpt2 which is a precursor

249
00:09:31,920 --> 00:09:34,000
to gpd3

250
00:09:34,000 --> 00:09:36,240
and when they released it that they said

251
00:09:36,240 --> 00:09:38,240
that they are not going to release a

252
00:09:38,240 --> 00:09:39,519
full model

253
00:09:39,519 --> 00:09:40,800
immediately and they're going to take a

254
00:09:40,800 --> 00:09:43,680
staged approach um because it was too

255
00:09:43,680 --> 00:09:46,480
dangerous to uh release into the public

256
00:09:46,480 --> 00:09:47,680
immediately

257
00:09:47,680 --> 00:09:49,600
uh again when you say that you know you

258
00:09:49,600 --> 00:09:51,440
cannot do something uh people in the

259
00:09:51,440 --> 00:09:53,519
internet get very excited and

260
00:09:53,519 --> 00:09:55,600
researchers were able to replicate the

261
00:09:55,600 --> 00:09:57,839
ml model by using open source

262
00:09:57,839 --> 00:10:00,399
information and proxy models and

263
00:10:00,399 --> 00:10:03,519
replicate um gpt2

264
00:10:03,519 --> 00:10:04,480
so

265
00:10:04,480 --> 00:10:06,959
um to put it all together

266
00:10:06,959 --> 00:10:10,079
here's what i want you to take away

267
00:10:10,079 --> 00:10:11,519
so far

268
00:10:11,519 --> 00:10:13,440
there are a speed of attacks that can

269
00:10:13,440 --> 00:10:16,160
happen just by querying the model and

270
00:10:16,160 --> 00:10:18,399
observing the responses so that would be

271
00:10:18,399 --> 00:10:20,160
like inversion which is reconstructing

272
00:10:20,160 --> 00:10:22,480
the private training data model stealing

273
00:10:22,480 --> 00:10:24,560
as well as model evasion

274
00:10:24,560 --> 00:10:26,320
and then if you can corrupt the training

275
00:10:26,320 --> 00:10:29,120
data um you that's called model

276
00:10:29,120 --> 00:10:30,880
poisoning

277
00:10:30,880 --> 00:10:33,120
so the the big thing

278
00:10:33,120 --> 00:10:36,240
uh so far is why haven't people actually

279
00:10:36,240 --> 00:10:37,920
solve this problem yet like you know

280
00:10:37,920 --> 00:10:40,000
this has been known for a long time

281
00:10:40,000 --> 00:10:42,240
what's stopping us from actually

282
00:10:42,240 --> 00:10:43,920
uh doing this

283
00:10:43,920 --> 00:10:44,720
so

284
00:10:44,720 --> 00:10:47,519
the sad story is that most defenses

285
00:10:47,519 --> 00:10:50,480
today are broken um

286
00:10:50,480 --> 00:10:53,120
you know there's every time like a paper

287
00:10:53,120 --> 00:10:55,040
gets published about like hey we have

288
00:10:55,040 --> 00:10:57,519
solved the defense uh just within like a

289
00:10:57,519 --> 00:10:59,920
couple of like months at this point in

290
00:10:59,920 --> 00:11:02,240
time the researchers have come up with

291
00:11:02,240 --> 00:11:04,480
saying like that's really not the case

292
00:11:04,480 --> 00:11:06,560
in fact nicholas carlini

293
00:11:06,560 --> 00:11:08,079
is kind of like really

294
00:11:08,079 --> 00:11:10,079
a researcher from google

295
00:11:10,079 --> 00:11:12,640
uh who actually has a video up on his

296
00:11:12,640 --> 00:11:15,360
site when he cracks like a

297
00:11:15,360 --> 00:11:17,440
state-of-the-art defense within two and

298
00:11:17,440 --> 00:11:19,200
a half hours and you can kind of stream

299
00:11:19,200 --> 00:11:21,680
it it's like as long as what a bollywood

300
00:11:21,680 --> 00:11:24,880
film would take uh without the songs so

301
00:11:24,880 --> 00:11:26,800
unfortunately like state-of-the-art

302
00:11:26,800 --> 00:11:29,680
machine learning systems do not have the

303
00:11:29,680 --> 00:11:34,000
defenses to back these sort of attacks

304
00:11:34,079 --> 00:11:36,240
thanks ram so all of that is in some

305
00:11:36,240 --> 00:11:37,760
ways the background you need to

306
00:11:37,760 --> 00:11:40,320
understand better why there are novel

307
00:11:40,320 --> 00:11:42,399
legal problems involved in adversarial

308
00:11:42,399 --> 00:11:44,160
machine learning research and what the

309
00:11:44,160 --> 00:11:46,160
legal risks are for adversarial machine

310
00:11:46,160 --> 00:11:48,480
learning researchers before i get into

311
00:11:48,480 --> 00:11:51,040
that obligatory disclaimer which is that

312
00:11:51,040 --> 00:11:53,200
i am an attorney but i'm not your

313
00:11:53,200 --> 00:11:54,320
attorney

314
00:11:54,320 --> 00:11:55,920
which is to say that this presentation

315
00:11:55,920 --> 00:11:57,600
does not create a lawyer client

316
00:11:57,600 --> 00:11:59,519
relationship between us and i'm going to

317
00:11:59,519 --> 00:12:01,920
sort of focus on these

318
00:12:01,920 --> 00:12:03,920
legal risks from the perspective of an

319
00:12:03,920 --> 00:12:05,519
adversarial machine learning security

320
00:12:05,519 --> 00:12:07,279
researcher since those are the folks i

321
00:12:07,279 --> 00:12:09,200
often represent but at the end i'll talk

322
00:12:09,200 --> 00:12:10,720
a little bit about why

323
00:12:10,720 --> 00:12:12,399
i think that actually helps us better

324
00:12:12,399 --> 00:12:14,480
understand what folks who are interested

325
00:12:14,480 --> 00:12:15,920
in defending adversarial machine

326
00:12:15,920 --> 00:12:18,639
learning systems also need to do

327
00:12:18,639 --> 00:12:20,720
so in terms of the novel legal questions

328
00:12:20,720 --> 00:12:22,399
or why this is sort of an interesting

329
00:12:22,399 --> 00:12:24,399
topic from a legal perspective

330
00:12:24,399 --> 00:12:26,560
it's because you know

331
00:12:26,560 --> 00:12:28,720
of what rom said which is that there

332
00:12:28,720 --> 00:12:29,920
isn't a ton of

333
00:12:29,920 --> 00:12:32,320
novel access involved in submitting a

334
00:12:32,320 --> 00:12:35,040
query and viewing the response and

335
00:12:35,040 --> 00:12:37,440
that also there's sort of this question

336
00:12:37,440 --> 00:12:39,440
of whether these these

337
00:12:39,440 --> 00:12:41,200
these machine learning manipulated

338
00:12:41,200 --> 00:12:44,000
images so for example this one that uh

339
00:12:44,000 --> 00:12:45,839
someone has manipulated to turn a panda

340
00:12:45,839 --> 00:12:47,680
into a given should be treated

341
00:12:47,680 --> 00:12:49,519
differently than the actual photo of a

342
00:12:49,519 --> 00:12:51,600
gibbon right you know whether there's

343
00:12:51,600 --> 00:12:52,720
actually

344
00:12:52,720 --> 00:12:56,000
um legal whether

345
00:12:56,000 --> 00:12:57,360
trying to trick a machine learning

346
00:12:57,360 --> 00:12:59,200
system actually should be legally

347
00:12:59,200 --> 00:13:00,720
punished in the same way that we might

348
00:13:00,720 --> 00:13:02,240
think about more what i would call

349
00:13:02,240 --> 00:13:04,320
traditional hacking activity

350
00:13:04,320 --> 00:13:06,240
so to answer that question we're going

351
00:13:06,240 --> 00:13:07,680
to look at a bunch of different bodies

352
00:13:07,680 --> 00:13:10,079
of u.s law so the applicable bodies of

353
00:13:10,079 --> 00:13:12,079
law that we'll discuss are breach of

354
00:13:12,079 --> 00:13:14,240
contract the computer fraud and abuse

355
00:13:14,240 --> 00:13:15,959
act copyright infringement

356
00:13:15,959 --> 00:13:18,160
anti-circumvention law which is called

357
00:13:18,160 --> 00:13:20,399
section 1201 for those in the know and

358
00:13:20,399 --> 00:13:22,320
then finally misappropriation of trade

359
00:13:22,320 --> 00:13:23,680
secrets

360
00:13:23,680 --> 00:13:25,680
so the first body of law is sort of the

361
00:13:25,680 --> 00:13:27,680
most broad and potentially the most

362
00:13:27,680 --> 00:13:30,000
applicable and that's breach of contract

363
00:13:30,000 --> 00:13:32,800
so contracts might sound like a bit like

364
00:13:32,800 --> 00:13:36,720
uh abstract or old-fashioned but as most

365
00:13:36,720 --> 00:13:38,560
of you probably already know you sign

366
00:13:38,560 --> 00:13:40,480
contracts for online services all the

367
00:13:40,480 --> 00:13:42,800
time or you agree to them um including

368
00:13:42,800 --> 00:13:44,720
the terms of service the an end user

369
00:13:44,720 --> 00:13:46,320
license agreement or even something like

370
00:13:46,320 --> 00:13:48,079
an acceptable use policy which is

371
00:13:48,079 --> 00:13:49,680
incorporated

372
00:13:49,680 --> 00:13:51,680
into the terms of service for these

373
00:13:51,680 --> 00:13:53,360
platforms and they all govern what you

374
00:13:53,360 --> 00:13:55,920
can do with a website or with an api

375
00:13:55,920 --> 00:13:58,320
they're about your responsibilities to

376
00:13:58,320 --> 00:14:00,720
the api service provider and the api

377
00:14:00,720 --> 00:14:03,360
service provider's responsibility to you

378
00:14:03,360 --> 00:14:05,600
they govern even if you don't read them

379
00:14:05,600 --> 00:14:07,360
so you know this is not a thing where

380
00:14:07,360 --> 00:14:08,800
you can close your eyes and press i

381
00:14:08,800 --> 00:14:10,480
agree and say i wasn't bound by the

382
00:14:10,480 --> 00:14:13,040
contract um it doesn't work courts don't

383
00:14:13,040 --> 00:14:15,760
like that um indeed

384
00:14:15,760 --> 00:14:18,240
these plot these policies are binding um

385
00:14:18,240 --> 00:14:19,920
regardless of whether you actually know

386
00:14:19,920 --> 00:14:21,680
the full contents of what's in them it

387
00:14:21,680 --> 00:14:22,959
what's important is that you were on

388
00:14:22,959 --> 00:14:24,800
notice that they existed and that you

389
00:14:24,800 --> 00:14:26,399
had the opportunity to learn what the

390
00:14:26,399 --> 00:14:28,000
full contents were

391
00:14:28,000 --> 00:14:29,839
so why does this impact adversarial

392
00:14:29,839 --> 00:14:32,160
machine learning um researchers is there

393
00:14:32,160 --> 00:14:33,920
something in the contract that says no

394
00:14:33,920 --> 00:14:35,279
adversarial machine learning well

395
00:14:35,279 --> 00:14:37,760
generally from what i've seen no

396
00:14:37,760 --> 00:14:38,800
but

397
00:14:38,800 --> 00:14:40,560
contracts often prohibit reverse

398
00:14:40,560 --> 00:14:42,720
engineering um which many adversarial

399
00:14:42,720 --> 00:14:44,240
machine learning attacks specifically

400
00:14:44,240 --> 00:14:47,360
model um inversion and model stealing do

401
00:14:47,360 --> 00:14:48,959
so this for example is from the face

402
00:14:48,959 --> 00:14:50,480
plus plus terms of service and in

403
00:14:50,480 --> 00:14:53,959
section 3.5.4.14

404
00:14:54,000 --> 00:14:55,760
is it any wonder nobody reads these

405
00:14:55,760 --> 00:14:57,760
things um it prohibits conducting

406
00:14:57,760 --> 00:14:59,360
reverse engineering dissembling or

407
00:14:59,360 --> 00:15:02,160
decompilation of the services so

408
00:15:02,160 --> 00:15:03,760
model inversion or model stealing

409
00:15:03,760 --> 00:15:05,040
attacks

410
00:15:05,040 --> 00:15:07,680
for those purposes um might be

411
00:15:07,680 --> 00:15:10,240
violations of the terms of service for

412
00:15:10,240 --> 00:15:11,360
this site

413
00:15:11,360 --> 00:15:13,680
so to sort of sum up what this means

414
00:15:13,680 --> 00:15:16,079
what you would look at in terms of a con

415
00:15:16,079 --> 00:15:17,839
a contract in order to evaluate this

416
00:15:17,839 --> 00:15:19,839
because every contract is different

417
00:15:19,839 --> 00:15:22,000
for an invasion attack the types of

418
00:15:22,000 --> 00:15:23,760
provisions that might create liability

419
00:15:23,760 --> 00:15:25,440
are sort of acceptable use policies

420
00:15:25,440 --> 00:15:27,040
around what types of query you can

421
00:15:27,040 --> 00:15:29,199
submit or the truthfulness of your

422
00:15:29,199 --> 00:15:31,839
queries some service providers require

423
00:15:31,839 --> 00:15:33,920
you to warrant particular things with

424
00:15:33,920 --> 00:15:36,639
regards to queries that you submit

425
00:15:36,639 --> 00:15:38,560
for model inversion and model stealing

426
00:15:38,560 --> 00:15:40,399
anti-reverse engineering clauses are the

427
00:15:40,399 --> 00:15:42,320
big thing you're going to worry about

428
00:15:42,320 --> 00:15:44,480
for model stealing potentially using the

429
00:15:44,480 --> 00:15:46,160
ml system to violate the rights of

430
00:15:46,160 --> 00:15:47,199
others

431
00:15:47,199 --> 00:15:48,800
and then finally for poisoning attacks

432
00:15:48,800 --> 00:15:50,000
you're worried about anti-reverse

433
00:15:50,000 --> 00:15:51,279
engineering

434
00:15:51,279 --> 00:15:52,480
and clauses that protect the

435
00:15:52,480 --> 00:15:54,480
intellectual property so ip

436
00:15:54,480 --> 00:15:57,040
of the api owner or prohibit causing

437
00:15:57,040 --> 00:15:59,120
harm and you'll see this basically

438
00:15:59,120 --> 00:16:00,800
version of this table for pretty much

439
00:16:00,800 --> 00:16:02,560
each of our various bodies of law that

440
00:16:02,560 --> 00:16:04,639
we go through um that's why we wanted to

441
00:16:04,639 --> 00:16:06,240
tee up these four types of attacks

442
00:16:06,240 --> 00:16:08,240
because we analyzed each for each of the

443
00:16:08,240 --> 00:16:11,040
attacks under each body of law

444
00:16:11,040 --> 00:16:12,079
okay

445
00:16:12,079 --> 00:16:13,600
so

446
00:16:13,600 --> 00:16:15,839
how is this a novel problem

447
00:16:15,839 --> 00:16:17,839
when i basically said that you might

448
00:16:17,839 --> 00:16:20,000
have signed a agreed to a contract with

449
00:16:20,000 --> 00:16:21,759
a service provider that prohibits all

450
00:16:21,759 --> 00:16:23,360
kinds of adversarial machine learning

451
00:16:23,360 --> 00:16:25,600
isn't this aren't we done isn't all

452
00:16:25,600 --> 00:16:27,199
aren't all adversarial machine learning

453
00:16:27,199 --> 00:16:29,839
researchers going to jail well not quite

454
00:16:29,839 --> 00:16:32,399
so con but fundamentally violating a

455
00:16:32,399 --> 00:16:34,639
contract doesn't necessarily create the

456
00:16:34,639 --> 00:16:36,560
kinds of penalties that i think we

457
00:16:36,560 --> 00:16:38,399
can often really worry about especially

458
00:16:38,399 --> 00:16:40,800
for individual security researchers so

459
00:16:40,800 --> 00:16:42,639
if you violate a contract with one of

460
00:16:42,639 --> 00:16:44,320
these service providers oftentimes the

461
00:16:44,320 --> 00:16:45,440
worst thing they're going to do is just

462
00:16:45,440 --> 00:16:47,279
kick you off the service

463
00:16:47,279 --> 00:16:49,759
they may sort of not give you back money

464
00:16:49,759 --> 00:16:51,519
and credits if you've if you bought

465
00:16:51,519 --> 00:16:53,199
credits on the service or if you have

466
00:16:53,199 --> 00:16:55,759
sort of an ongoing um monetary

467
00:16:55,759 --> 00:16:57,199
relationship they may not give you a

468
00:16:57,199 --> 00:17:00,000
refund honestly sometimes they might but

469
00:17:00,000 --> 00:17:02,480
really breach of contract by itself it's

470
00:17:02,480 --> 00:17:03,680
not the end of the world when it comes

471
00:17:03,680 --> 00:17:05,520
to legal risk especially when what

472
00:17:05,520 --> 00:17:07,599
you're thinking about is potentially

473
00:17:07,599 --> 00:17:09,520
individual security researchers who are

474
00:17:09,520 --> 00:17:12,160
testing attacks on defenses um because

475
00:17:12,160 --> 00:17:13,839
you often are not going to have a

476
00:17:13,839 --> 00:17:15,439
financially and important enough

477
00:17:15,439 --> 00:17:16,559
relationship with these service

478
00:17:16,559 --> 00:17:18,559
providers such that bringing a lawsuit

479
00:17:18,559 --> 00:17:20,000
for breach of contract is worth their

480
00:17:20,000 --> 00:17:22,079
time and energy

481
00:17:22,079 --> 00:17:23,760
so although breach of contract is the

482
00:17:23,760 --> 00:17:25,839
most relevant body of law right it

483
00:17:25,839 --> 00:17:27,520
covers the most behavior that we see

484
00:17:27,520 --> 00:17:29,520
under adversarial machine learning it's

485
00:17:29,520 --> 00:17:31,280
often going to be the lowest actual

486
00:17:31,280 --> 00:17:33,600
substantial risk that adversarial

487
00:17:33,600 --> 00:17:35,679
machine learning researchers face the

488
00:17:35,679 --> 00:17:37,520
same is unfortunately not true of our

489
00:17:37,520 --> 00:17:39,039
next body of law which is the computer

490
00:17:39,039 --> 00:17:40,559
fraud and abuse act

491
00:17:40,559 --> 00:17:42,240
so the computer front and b sect is the

492
00:17:42,240 --> 00:17:45,520
federal u.s federal anti-hacking statute

493
00:17:45,520 --> 00:17:46,799
so and

494
00:17:46,799 --> 00:17:48,880
the war games reference here is not just

495
00:17:48,880 --> 00:17:50,559
because i don't know what hacking looks

496
00:17:50,559 --> 00:17:54,400
like um uh but because um

497
00:17:54,400 --> 00:17:56,160
there are scholars who actually theorize

498
00:17:56,160 --> 00:17:57,440
that the computer front of yourself was

499
00:17:57,440 --> 00:17:59,679
passed as a result of war games right

500
00:17:59,679 --> 00:18:02,080
the sort of fear of what hacking looks

501
00:18:02,080 --> 00:18:04,559
like comes from this particular forms of

502
00:18:04,559 --> 00:18:07,200
form of media and the computer from

503
00:18:07,200 --> 00:18:08,480
abuse act has historically been

504
00:18:08,480 --> 00:18:10,480
interpreted quite broadly and in fact

505
00:18:10,480 --> 00:18:11,760
there used to be conflicting

506
00:18:11,760 --> 00:18:14,640
interpretations um across the united

507
00:18:14,640 --> 00:18:17,919
states where some places would have

508
00:18:17,919 --> 00:18:20,160
risk associated with filing terms of use

509
00:18:20,160 --> 00:18:21,600
so you didn't really worry we didn't

510
00:18:21,600 --> 00:18:22,799
really worry as much about breach of

511
00:18:22,799 --> 00:18:24,160
contract but when it sort of started

512
00:18:24,160 --> 00:18:25,840
involving this computer fund abuse act

513
00:18:25,840 --> 00:18:27,120
which has both

514
00:18:27,120 --> 00:18:29,120
criminal penalties aka you could go to

515
00:18:29,120 --> 00:18:32,080
jail and um potential civil liability

516
00:18:32,080 --> 00:18:34,559
risk aka someone could sue you that was

517
00:18:34,559 --> 00:18:35,600
worrying

518
00:18:35,600 --> 00:18:37,520
that changed about a month and a half

519
00:18:37,520 --> 00:18:40,160
ago with van buren the united states

520
00:18:40,160 --> 00:18:41,760
which was the first supreme court case

521
00:18:41,760 --> 00:18:43,360
to substantively interpret the computer

522
00:18:43,360 --> 00:18:45,120
fraud and abuse act and it's really good

523
00:18:45,120 --> 00:18:47,600
news for computer security researchers

524
00:18:47,600 --> 00:18:49,039
because although there are certain

525
00:18:49,039 --> 00:18:51,039
things about the law that it definitely

526
00:18:51,039 --> 00:18:53,520
did not clarify it does make more clear

527
00:18:53,520 --> 00:18:56,000
that sort of merely violating the terms

528
00:18:56,000 --> 00:18:57,840
of use of a site

529
00:18:57,840 --> 00:18:59,760
by engaging in a practice that might be

530
00:18:59,760 --> 00:19:01,760
reverse engineering for example or by

531
00:19:01,760 --> 00:19:03,600
sharing your netflix password if that's

532
00:19:03,600 --> 00:19:05,039
against the netflix terms of service

533
00:19:05,039 --> 00:19:07,440
which i'm pretty sure it is um that

534
00:19:07,440 --> 00:19:09,600
doesn't make you a

535
00:19:09,600 --> 00:19:12,160
the computer fraudulent piece act

536
00:19:12,160 --> 00:19:14,400
however there's still risk to security

537
00:19:14,400 --> 00:19:16,080
researchers that comes from the computer

538
00:19:16,080 --> 00:19:18,000
front and b's act even post-fam bureau

539
00:19:18,000 --> 00:19:19,360
so let's talk a little bit about what

540
00:19:19,360 --> 00:19:20,720
that looks like

541
00:19:20,720 --> 00:19:22,160
so there's two provisions of the

542
00:19:22,160 --> 00:19:24,080
computer fraud and act that are relevant

543
00:19:24,080 --> 00:19:26,160
to this analysis one is what's called an

544
00:19:26,160 --> 00:19:29,039
access violation so this is section 1030

545
00:19:29,039 --> 00:19:31,039
a2c for those of you who want to follow

546
00:19:31,039 --> 00:19:32,240
along at home

547
00:19:32,240 --> 00:19:34,240
and this prohibits accessing a computer

548
00:19:34,240 --> 00:19:36,160
without authorization or in a way that

549
00:19:36,160 --> 00:19:38,080
exceeds authorized access and as a

550
00:19:38,080 --> 00:19:40,400
result obtaining any information

551
00:19:40,400 --> 00:19:42,160
so you know when we talk about any

552
00:19:42,160 --> 00:19:44,080
information here we're not necessarily

553
00:19:44,080 --> 00:19:46,000
like oh gosh you have to exfiltrate this

554
00:19:46,000 --> 00:19:48,160
payload just basically accessing a

555
00:19:48,160 --> 00:19:50,000
computer without authorization and exe

556
00:19:50,000 --> 00:19:51,440
or in a way that exceeds authorized

557
00:19:51,440 --> 00:19:53,280
access sort of counts that any

558
00:19:53,280 --> 00:19:56,720
information bit is very easy for um

559
00:19:56,720 --> 00:19:59,039
uh someone suing you to show

560
00:19:59,039 --> 00:20:00,640
the other provision of the computer

561
00:20:00,640 --> 00:20:02,159
fraud and abuse act that's relevant here

562
00:20:02,159 --> 00:20:04,400
is what's called a damage violation so

563
00:20:04,400 --> 00:20:06,480
this is causing damage to a computer

564
00:20:06,480 --> 00:20:08,320
without authorization by knowingly

565
00:20:08,320 --> 00:20:10,320
transmitting a program information code

566
00:20:10,320 --> 00:20:12,320
or command so this is again for those

567
00:20:12,320 --> 00:20:13,760
following

568
00:20:13,760 --> 00:20:17,440
on following along at home section 1030

569
00:20:17,440 --> 00:20:18,720
85a

570
00:20:18,720 --> 00:20:20,720
um and that's the provision that's meant

571
00:20:20,720 --> 00:20:24,320
to deal with things like viruses malware

572
00:20:24,320 --> 00:20:25,919
that sort of thing

573
00:20:25,919 --> 00:20:29,039
so with these two provisions one

574
00:20:29,039 --> 00:20:30,880
the key question with regards to the

575
00:20:30,880 --> 00:20:32,559
access violation is when are you

576
00:20:32,559 --> 00:20:34,159
accessing a computer without

577
00:20:34,159 --> 00:20:36,000
authorization or in a way that exceeds

578
00:20:36,000 --> 00:20:37,520
authorized access

579
00:20:37,520 --> 00:20:39,840
and this brings us to a point um around

580
00:20:39,840 --> 00:20:41,760
technological barriers

581
00:20:41,760 --> 00:20:44,159
so circumventing a technological barrier

582
00:20:44,159 --> 00:20:46,320
even if it's not particularly effective

583
00:20:46,320 --> 00:20:48,000
can potentially create computer front

584
00:20:48,000 --> 00:20:51,440
and b stack liability so you know

585
00:20:51,440 --> 00:20:52,960
when i talk about this especially with

586
00:20:52,960 --> 00:20:54,400
security experts they're kind of like oh

587
00:20:54,400 --> 00:20:56,720
my god do i need the latest greatest

588
00:20:56,720 --> 00:20:59,760
defenses well i mean maybe but not

589
00:20:59,760 --> 00:21:01,120
necessarily for the computer front fund

590
00:21:01,120 --> 00:21:02,320
abuse act

591
00:21:02,320 --> 00:21:04,240
there is caseless suggesting that meet

592
00:21:04,240 --> 00:21:08,000
in even an ip block on certain uh

593
00:21:08,000 --> 00:21:10,480
uh internet protocol addresses could

594
00:21:10,480 --> 00:21:12,480
allow you to claim that someone was

595
00:21:12,480 --> 00:21:14,799
circumventing a technological barrier um

596
00:21:14,799 --> 00:21:16,720
in the language of van buren the supreme

597
00:21:16,720 --> 00:21:18,640
court case it's a sort of gates up gates

598
00:21:18,640 --> 00:21:20,320
down analysis

599
00:21:20,320 --> 00:21:21,760
we'll figure out what that means at some

600
00:21:21,760 --> 00:21:22,880
point

601
00:21:22,880 --> 00:21:23,840
um

602
00:21:23,840 --> 00:21:26,320
that's good news right merely violating

603
00:21:26,320 --> 00:21:29,760
the terms of service if you don't if the

604
00:21:29,760 --> 00:21:30,480
the

605
00:21:30,480 --> 00:21:32,640
api you're attacking doesn't uh do

606
00:21:32,640 --> 00:21:35,280
anything to prevent you no cfa liability

607
00:21:35,280 --> 00:21:37,280
circumvention of a technological barrier

608
00:21:37,280 --> 00:21:38,799
maybe

609
00:21:38,799 --> 00:21:39,600
um

610
00:21:39,600 --> 00:21:41,440
and until courts rule otherwise the

611
00:21:41,440 --> 00:21:42,880
other thing that we is good to know

612
00:21:42,880 --> 00:21:45,280
about the computer fraud abuse act is

613
00:21:45,280 --> 00:21:47,120
it may still be the individual

614
00:21:47,120 --> 00:21:48,880
revocation of uh

615
00:21:48,880 --> 00:21:50,880
the right to access creates liability

616
00:21:50,880 --> 00:21:53,120
okay what does that mean so let's say

617
00:21:53,120 --> 00:21:55,120
that you're a researcher and um the

618
00:21:55,120 --> 00:21:57,120
company whose api that you've been

619
00:21:57,120 --> 00:21:58,799
attempting to test for a particular

620
00:21:58,799 --> 00:22:00,640
attack figures out that it's you and

621
00:22:00,640 --> 00:22:02,799
sends you a letter and this happens i've

622
00:22:02,799 --> 00:22:04,880
seen them uh

623
00:22:04,880 --> 00:22:07,039
saying can you stop that please

624
00:22:07,039 --> 00:22:08,799
you are no longer authorized to access

625
00:22:08,799 --> 00:22:11,520
our service that meals may still create

626
00:22:11,520 --> 00:22:13,039
risk for these first three types of

627
00:22:13,039 --> 00:22:14,559
attacks under the computer fraud and

628
00:22:14,559 --> 00:22:17,760
abuse act because it um because that's a

629
00:22:17,760 --> 00:22:19,679
revocation of access and your access is

630
00:22:19,679 --> 00:22:21,600
no longer authorized

631
00:22:21,600 --> 00:22:24,400
last thing to note here is that 1030 a5a

632
00:22:24,400 --> 00:22:26,080
right i said that was the virus and

633
00:22:26,080 --> 00:22:28,960
malware um part of the cfa

634
00:22:28,960 --> 00:22:30,400
and that most directly applies to

635
00:22:30,400 --> 00:22:32,400
poisoning attacks um because poisoning

636
00:22:32,400 --> 00:22:35,520
attacks aim to come like uh

637
00:22:35,520 --> 00:22:36,480
to

638
00:22:36,480 --> 00:22:38,480
shift the underlying system to respond

639
00:22:38,480 --> 00:22:40,480
differently to sort of input from other

640
00:22:40,480 --> 00:22:42,559
users later they're less about getting

641
00:22:42,559 --> 00:22:44,400
information out of the system or evading

642
00:22:44,400 --> 00:22:47,039
a particular a particular outcome on the

643
00:22:47,039 --> 00:22:49,760
on the api query level

644
00:22:49,760 --> 00:22:51,360
so that's the computer fraud and abuse

645
00:22:51,360 --> 00:22:53,120
act i'm next going to talk about

646
00:22:53,120 --> 00:22:55,679
copyright law um then trade secret and

647
00:22:55,679 --> 00:22:58,080
then we'll move into takeaways so many

648
00:22:58,080 --> 00:22:59,600
of you are probably familiar with

649
00:22:59,600 --> 00:23:02,000
copyright law even if um that just means

650
00:23:02,000 --> 00:23:04,159
that you know that you don't like it but

651
00:23:04,159 --> 00:23:06,080
it protects original works of authorship

652
00:23:06,080 --> 00:23:08,159
fixed in a tangible medium and that way

653
00:23:08,159 --> 00:23:09,840
that interacts with software is rather

654
00:23:09,840 --> 00:23:12,000
interesting so copyright law doesn't

655
00:23:12,000 --> 00:23:13,919
protect the idea of software it doesn't

656
00:23:13,919 --> 00:23:16,799
protect sort of the notion of adding two

657
00:23:16,799 --> 00:23:18,480
numbers together or

658
00:23:18,480 --> 00:23:20,000
the notion of a program that does a

659
00:23:20,000 --> 00:23:22,240
particular thing it protects software as

660
00:23:22,240 --> 00:23:24,480
a literary work so the actual code that

661
00:23:24,480 --> 00:23:26,640
someone writes the source code

662
00:23:26,640 --> 00:23:29,039
that's protected when it under copyright

663
00:23:29,039 --> 00:23:31,120
law that means that something like back

664
00:23:31,120 --> 00:23:33,280
end code for a machine learning model

665
00:23:33,280 --> 00:23:34,880
may be protected by copyright but

666
00:23:34,880 --> 00:23:36,720
generally models themselves the weights

667
00:23:36,720 --> 00:23:38,640
that are given to particular parameters

668
00:23:38,640 --> 00:23:41,200
are not protected by copyright um

669
00:23:41,200 --> 00:23:42,640
there's also potentially copyright

670
00:23:42,640 --> 00:23:44,799
claims around image-based training data

671
00:23:44,799 --> 00:23:47,360
so images pictures of people are

672
00:23:47,360 --> 00:23:49,120
copyrightable

673
00:23:49,120 --> 00:23:51,039
um and so what that means is that if

674
00:23:51,039 --> 00:23:52,880
you're doing something like extracting

675
00:23:52,880 --> 00:23:54,000
images

676
00:23:54,000 --> 00:23:56,000
that were used as part of training data

677
00:23:56,000 --> 00:23:58,880
from a model via queries you may have a

678
00:23:58,880 --> 00:24:00,720
copyright law problem on your hand if

679
00:24:00,720 --> 00:24:02,320
you get sufficiently good high

680
00:24:02,320 --> 00:24:05,200
resolution images out um

681
00:24:05,200 --> 00:24:07,039
this is not a big problem if you're a

682
00:24:07,039 --> 00:24:08,400
security researcher who's sort of

683
00:24:08,400 --> 00:24:10,480
interested in demonstrating these kinds

684
00:24:10,480 --> 00:24:11,679
of attacks and you're not sort of

685
00:24:11,679 --> 00:24:13,360
necessarily reproducing these images a

686
00:24:13,360 --> 00:24:15,120
ton other than in a paper

687
00:24:15,120 --> 00:24:17,840
um but it does mean that using sort of

688
00:24:17,840 --> 00:24:19,679
model inversion attacks to sort of

689
00:24:19,679 --> 00:24:21,360
extract training data for the use of

690
00:24:21,360 --> 00:24:23,520
training your own model could create

691
00:24:23,520 --> 00:24:26,240
copyright law risk

692
00:24:26,240 --> 00:24:27,760
the next bit of copyright law worth

693
00:24:27,760 --> 00:24:29,679
mentioning is section 1201 which we

694
00:24:29,679 --> 00:24:31,760
often mentioned sort of aligned with the

695
00:24:31,760 --> 00:24:34,240
computer front abuse act it likewise

696
00:24:34,240 --> 00:24:36,080
creates liability for circumventing

697
00:24:36,080 --> 00:24:38,320
technological protection mechanisms

698
00:24:38,320 --> 00:24:39,679
specifically those that protect

699
00:24:39,679 --> 00:24:41,200
copyrighted works

700
00:24:41,200 --> 00:24:44,000
so uh you know if you're old like me and

701
00:24:44,000 --> 00:24:45,760
you ever thought about cracking the

702
00:24:45,760 --> 00:24:47,520
encryption on dvds so you could sort of

703
00:24:47,520 --> 00:24:49,200
make a copy and

704
00:24:49,200 --> 00:24:51,279
share it or not share it on the internet

705
00:24:51,279 --> 00:24:53,120
um that's what we are thinking about

706
00:24:53,120 --> 00:24:54,559
when it comes to anti-circumvention

707
00:24:54,559 --> 00:24:57,760
provisions like section 1201 and again

708
00:24:57,760 --> 00:25:00,080
here the question is if depending on the

709
00:25:00,080 --> 00:25:02,640
safeguards that are provided for the api

710
00:25:02,640 --> 00:25:05,360
you may have a circumvention problem

711
00:25:05,360 --> 00:25:06,640
and that means

712
00:25:06,640 --> 00:25:08,480
again like you'll hear me emphasize a

713
00:25:08,480 --> 00:25:10,080
lot that the

714
00:25:10,080 --> 00:25:12,400
certain kinds of defenses that per that

715
00:25:12,400 --> 00:25:14,640
make it more difficult for attackers to

716
00:25:14,640 --> 00:25:16,720
engage in these kinds of attacks

717
00:25:16,720 --> 00:25:18,799
may not just make it more difficult but

718
00:25:18,799 --> 00:25:21,360
also may create opportunities to bring

719
00:25:21,360 --> 00:25:23,600
for people whose apis are being attacked

720
00:25:23,600 --> 00:25:25,279
to bring legal claims should they say to

721
00:25:25,279 --> 00:25:26,799
do that

722
00:25:26,799 --> 00:25:28,640
the final form of intellectual property

723
00:25:28,640 --> 00:25:30,320
i want to discuss is trade secret so

724
00:25:30,320 --> 00:25:32,080
trade secret is kind of like the one

725
00:25:32,080 --> 00:25:34,080
that everyone forgets um because it's

726
00:25:34,080 --> 00:25:36,559
often very relevant for businesses but

727
00:25:36,559 --> 00:25:39,919
not very relevant for professors um and

728
00:25:39,919 --> 00:25:41,440
so when we talk about trade security

729
00:25:41,440 --> 00:25:42,480
we're talking about something that gives

730
00:25:42,480 --> 00:25:44,000
you a business advantage and that you

731
00:25:44,000 --> 00:25:46,240
keep secrets so that the archetypal

732
00:25:46,240 --> 00:25:48,880
example is the formula for coca-cola

733
00:25:48,880 --> 00:25:49,760
um

734
00:25:49,760 --> 00:25:50,720
and

735
00:25:50,720 --> 00:25:52,640
it's really useful to think about trade

736
00:25:52,640 --> 00:25:55,200
secrets in relationship to um

737
00:25:55,200 --> 00:25:57,760
to machine learning models because

738
00:25:57,760 --> 00:25:59,039
machine learning models they're not

739
00:25:59,039 --> 00:26:01,120
necessarily protected by copyright but

740
00:26:01,120 --> 00:26:03,520
they could be something that the um a

741
00:26:03,520 --> 00:26:05,600
company keeps secret in order to sort of

742
00:26:05,600 --> 00:26:08,159
contain get a business advantage and to

743
00:26:08,159 --> 00:26:09,760
the extent that that's true

744
00:26:09,760 --> 00:26:11,279
model stealing and model inversion

745
00:26:11,279 --> 00:26:13,600
attacks could implicate trade secret law

746
00:26:13,600 --> 00:26:15,760
if a company has adequately protected

747
00:26:15,760 --> 00:26:18,640
that trade secret um

748
00:26:18,640 --> 00:26:20,320
if they haven't you don't have a trade

749
00:26:20,320 --> 00:26:22,000
secret at all

750
00:26:22,000 --> 00:26:23,760
so misappropriation of trade secrets

751
00:26:23,760 --> 00:26:26,240
stealing trade secrets doesn't cover

752
00:26:26,240 --> 00:26:27,760
run in the middle of reverse engineering

753
00:26:27,760 --> 00:26:29,279
if i can just look at it and figure out

754
00:26:29,279 --> 00:26:32,159
how it works um then trade secret isn't

755
00:26:32,159 --> 00:26:33,840
going to help me but it does cover

756
00:26:33,840 --> 00:26:36,799
what's called unlawful means now

757
00:26:36,799 --> 00:26:38,320
i wish we had case law where i could

758
00:26:38,320 --> 00:26:39,919
tell you pretty clearly what model

759
00:26:39,919 --> 00:26:41,840
whether model inversion or model scaling

760
00:26:41,840 --> 00:26:44,799
is an unlawful means frankly

761
00:26:44,799 --> 00:26:45,840
we don't

762
00:26:45,840 --> 00:26:48,320
but what we do see here is that more

763
00:26:48,320 --> 00:26:50,880
protections create potential trade

764
00:26:50,880 --> 00:26:53,520
secret claims um but if you're just sort

765
00:26:53,520 --> 00:26:55,360
of able to get the information without

766
00:26:55,360 --> 00:26:58,000
really doing much special at all um then

767
00:26:58,000 --> 00:27:00,880
it's very difficult for a uh the owner

768
00:27:00,880 --> 00:27:04,559
of a trade secret to claim um to claim

769
00:27:04,559 --> 00:27:05,679
rights in it and claim that you

770
00:27:05,679 --> 00:27:08,240
misappropriate it one last thing to note

771
00:27:08,240 --> 00:27:10,480
about trade secret is that uh trade

772
00:27:10,480 --> 00:27:12,720
secret misappropriation looks the

773
00:27:12,720 --> 00:27:14,480
damages like basically how much money

774
00:27:14,480 --> 00:27:16,000
you can get if your trade secret has

775
00:27:16,000 --> 00:27:18,159
been misappropriated has to do with the

776
00:27:18,159 --> 00:27:21,039
sort of um either your lost profits or

777
00:27:21,039 --> 00:27:23,120
the sort of money that the person meet

778
00:27:23,120 --> 00:27:25,120
by using the trade secret so what this

779
00:27:25,120 --> 00:27:26,640
means in practice is that security

780
00:27:26,640 --> 00:27:28,399
researchers who aren't doing anything

781
00:27:28,399 --> 00:27:31,279
with the model that they they stole or

782
00:27:31,279 --> 00:27:33,600
the model that they were able to extract

783
00:27:33,600 --> 00:27:35,840
or the training data are unlikely to

784
00:27:35,840 --> 00:27:37,279
have really the same kinds of

785
00:27:37,279 --> 00:27:39,279
significant trade secret liability

786
00:27:39,279 --> 00:27:40,960
although they may be again ordered to

787
00:27:40,960 --> 00:27:43,919
stop doing what they're doing

788
00:27:43,919 --> 00:27:44,880
okay

789
00:27:44,880 --> 00:27:47,039
that was a whirlwind tour through a lot

790
00:27:47,039 --> 00:27:49,919
of law you made it so let's talk a

791
00:27:49,919 --> 00:27:51,200
little bit about some high-level

792
00:27:51,200 --> 00:27:53,440
takeaways before we turn to what all of

793
00:27:53,440 --> 00:27:54,799
this means for folks who might be

794
00:27:54,799 --> 00:27:56,799
building or defending machine learning

795
00:27:56,799 --> 00:27:58,000
systems

796
00:27:58,000 --> 00:27:58,880
so

797
00:27:58,880 --> 00:28:00,720
just to like

798
00:28:00,720 --> 00:28:02,640
boil this down to the highest possible

799
00:28:02,640 --> 00:28:03,600
level

800
00:28:03,600 --> 00:28:06,000
i have created this very helpful chart

801
00:28:06,000 --> 00:28:08,320
um with a line that says less risk to

802
00:28:08,320 --> 00:28:10,799
more risk noting here that evasion

803
00:28:10,799 --> 00:28:13,360
attacks just tend to be much lower risk

804
00:28:13,360 --> 00:28:14,799
than something like model stealing or

805
00:28:14,799 --> 00:28:16,799
model inversion and poisoning because it

806
00:28:16,799 --> 00:28:18,640
has potential negative implications for

807
00:28:18,640 --> 00:28:20,960
other users of the service aka it can

808
00:28:20,960 --> 00:28:23,039
harm other people tends to be the

809
00:28:23,039 --> 00:28:24,480
highest risk of the four types of

810
00:28:24,480 --> 00:28:25,840
attacks that we've talked about that

811
00:28:25,840 --> 00:28:27,520
don't require any sort of special

812
00:28:27,520 --> 00:28:29,919
knowledge that just for our use api

813
00:28:29,919 --> 00:28:31,919
queries

814
00:28:31,919 --> 00:28:34,000
then for security researchers who are

815
00:28:34,000 --> 00:28:36,080
doing this work and curious about how to

816
00:28:36,080 --> 00:28:38,399
lower their risk in doing adversarial

817
00:28:38,399 --> 00:28:40,240
machine learning work here's some stuff

818
00:28:40,240 --> 00:28:42,000
that's less risky and some stuff that's

819
00:28:42,000 --> 00:28:42,799
more

820
00:28:42,799 --> 00:28:44,640
testing with permission or testing on

821
00:28:44,640 --> 00:28:46,480
systems that don't are not training on

822
00:28:46,480 --> 00:28:48,559
api query data right so that there's no

823
00:28:48,559 --> 00:28:50,320
feedback loop those tend to be less

824
00:28:50,320 --> 00:28:52,480
risky likewise testing on systems that

825
00:28:52,480 --> 00:28:55,600
are isolated or not used by other users

826
00:28:55,600 --> 00:28:58,000
and following security best practices

827
00:28:58,000 --> 00:28:59,919
including potentially coordinated

828
00:28:59,919 --> 00:29:02,480
vulnerability disclosure as appropriate

829
00:29:02,480 --> 00:29:05,360
all that gives you less risk

830
00:29:05,360 --> 00:29:07,600
more risk testing without permission

831
00:29:07,600 --> 00:29:09,360
testing on live systems including

832
00:29:09,360 --> 00:29:11,360
software as a service services

833
00:29:11,360 --> 00:29:12,880
um testing on systems that have a

834
00:29:12,880 --> 00:29:14,159
feedback component again you're not

835
00:29:14,159 --> 00:29:15,840
always going to know whether the system

836
00:29:15,840 --> 00:29:17,360
is continually training on the inputs

837
00:29:17,360 --> 00:29:19,200
but if you do know that it is and

838
00:29:19,200 --> 00:29:21,520
testing a poisoning attack can really

839
00:29:21,520 --> 00:29:23,039
screw up things for other users that's

840
00:29:23,039 --> 00:29:25,039
going to be much more legally risky

841
00:29:25,039 --> 00:29:26,720
and then finally the overall thing that

842
00:29:26,720 --> 00:29:28,320
i want to communicate in terms of legal

843
00:29:28,320 --> 00:29:30,399
risk is if you're using adversarial

844
00:29:30,399 --> 00:29:31,919
attacks to extract information for

845
00:29:31,919 --> 00:29:34,880
business purposes especially competition

846
00:29:34,880 --> 00:29:36,399
that increases your risk quite

847
00:29:36,399 --> 00:29:38,399
significantly

848
00:29:38,399 --> 00:29:39,360
so

849
00:29:39,360 --> 00:29:40,880
that's for my security researcher

850
00:29:40,880 --> 00:29:42,880
friends let's talk a little bit about my

851
00:29:42,880 --> 00:29:44,240
defenders

852
00:29:44,240 --> 00:29:46,000
so one thing that we noticed when we

853
00:29:46,000 --> 00:29:47,440
started doing this work is there's this

854
00:29:47,440 --> 00:29:49,279
sort of belief that the law is what's

855
00:29:49,279 --> 00:29:51,039
going to save us all from adversarial

856
00:29:51,039 --> 00:29:52,559
machine learning and text so this is a

857
00:29:52,559 --> 00:29:54,480
blog post from 2016

858
00:29:54,480 --> 00:29:57,039
from the big ml blog where they say uh

859
00:29:57,039 --> 00:29:58,559
even if stealing software was easy

860
00:29:58,559 --> 00:30:00,159
there's still an important disincentive

861
00:30:00,159 --> 00:30:01,279
to do so and that it violates

862
00:30:01,279 --> 00:30:03,679
intellectual property law well that's

863
00:30:03,679 --> 00:30:05,760
true for some values of the word

864
00:30:05,760 --> 00:30:08,240
software but the reality is that

865
00:30:08,240 --> 00:30:10,159
actually the law is not a perfect fit

866
00:30:10,159 --> 00:30:12,159
for all of the types of harms that might

867
00:30:12,159 --> 00:30:13,600
come from adversarial machine learning

868
00:30:13,600 --> 00:30:14,960
attacks

869
00:30:14,960 --> 00:30:17,360
honestly that's probably a good thing in

870
00:30:17,360 --> 00:30:19,200
some ways because

871
00:30:19,200 --> 00:30:21,039
many of these attacks actually bear

872
00:30:21,039 --> 00:30:23,279
don't really the thing that you're doing

873
00:30:23,279 --> 00:30:25,679
when you do them are not exclusive to us

874
00:30:25,679 --> 00:30:28,159
to attackers and looking at the harm

875
00:30:28,159 --> 00:30:30,240
really is what helps us distinguish

876
00:30:30,240 --> 00:30:33,039
folks who are genuinely bad actors from

877
00:30:33,039 --> 00:30:35,600
folks who are just testing something or

878
00:30:35,600 --> 00:30:37,360
may just you know accidentally submit

879
00:30:37,360 --> 00:30:40,320
the wrong photo to an api

880
00:30:40,320 --> 00:30:41,679
one of the ways companies have been

881
00:30:41,679 --> 00:30:43,279
trying to protect themselves so far is

882
00:30:43,279 --> 00:30:44,880
terms of service

883
00:30:44,880 --> 00:30:47,039
fortunately for security researchers

884
00:30:47,039 --> 00:30:48,399
terms of service

885
00:30:48,399 --> 00:30:50,080
violating terms of service no longer

886
00:30:50,080 --> 00:30:51,840
creates what i call follow on legal risk

887
00:30:51,840 --> 00:30:53,440
which is you may still have a breach of

888
00:30:53,440 --> 00:30:55,440
contract claim but you don't you can't

889
00:30:55,440 --> 00:30:56,880
now invoke the computer fraud to be

890
00:30:56,880 --> 00:30:58,080
sacked in the same way you could

891
00:30:58,080 --> 00:30:59,279
previously

892
00:30:59,279 --> 00:31:00,880
that means that machine learning

893
00:31:00,880 --> 00:31:02,880
providers need to think about what harm

894
00:31:02,880 --> 00:31:04,799
attacks are actually causing and use the

895
00:31:04,799 --> 00:31:06,559
legal terms tools that are consistent

896
00:31:06,559 --> 00:31:08,399
with those harms whether they're caught

897
00:31:08,399 --> 00:31:10,559
that's copyright or trade secret and

898
00:31:10,559 --> 00:31:13,919
frankly this is good because those tools

899
00:31:13,919 --> 00:31:15,919
actually encourage folks to implement

900
00:31:15,919 --> 00:31:17,440
defenses

901
00:31:17,440 --> 00:31:19,360
because technical defenses even if

902
00:31:19,360 --> 00:31:21,200
they're not foolproof even if nicolas

903
00:31:21,200 --> 00:31:23,279
carlini can break them in two hours they

904
00:31:23,279 --> 00:31:25,840
can potentially be create liability for

905
00:31:25,840 --> 00:31:29,200
bad actors so in a way the sort of not

906
00:31:29,200 --> 00:31:31,200
so perfect fit of parts of law to

907
00:31:31,200 --> 00:31:33,360
adversarial machine learning actually is

908
00:31:33,360 --> 00:31:35,120
creating circumstances in which folks

909
00:31:35,120 --> 00:31:37,279
are encouraged to secure their their

910
00:31:37,279 --> 00:31:39,360
machines and their apis and that's

911
00:31:39,360 --> 00:31:41,200
really what we want is for folks to

912
00:31:41,200 --> 00:31:43,440
build more secure systems

913
00:31:43,440 --> 00:31:45,919
thank you and rom and i are always happy

914
00:31:45,919 --> 00:31:47,679
to talk about this more answer questions

915
00:31:47,679 --> 00:31:49,440
so please don't hesitate to reach out to

916
00:31:49,440 --> 00:31:50,720
us if you'd like to talk about any of

917
00:31:50,720 --> 00:31:54,279
the things we discussed

