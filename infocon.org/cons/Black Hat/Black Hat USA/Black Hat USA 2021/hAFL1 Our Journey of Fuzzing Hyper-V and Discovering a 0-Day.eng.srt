1
00:00:01,130 --> 00:00:13,599
[Music]

2
00:00:13,599 --> 00:00:15,759
hi everyone and thank you for joining

3
00:00:15,759 --> 00:00:17,920
our talk have for one our journey of

4
00:00:17,920 --> 00:00:19,920
fuzzing hyper-v and discovering a zero

5
00:00:19,920 --> 00:00:23,199
day we'll start by introducing ourselves

6
00:00:23,199 --> 00:00:25,439
hi my name is pele radar i'm a senior

7
00:00:25,439 --> 00:00:27,680
security researcher at seybridge cyber

8
00:00:27,680 --> 00:00:30,240
security company which provides a bridge

9
00:00:30,240 --> 00:00:31,679
and attack simulation product which

10
00:00:31,679 --> 00:00:33,360
helps customers to

11
00:00:33,360 --> 00:00:35,840
validate their own security controls

12
00:00:35,840 --> 00:00:37,520
my main focus is on windows internals

13
00:00:37,520 --> 00:00:39,760
vulnerability research and hypervisors

14
00:00:39,760 --> 00:00:41,440
and i also gave a talk last year on

15
00:00:41,440 --> 00:00:45,280
black tsa and defcon my name is ophir

16
00:00:45,280 --> 00:00:47,520
harpaz i'm a senior security researcher

17
00:00:47,520 --> 00:00:49,920
at guardicore labs the research arm of

18
00:00:49,920 --> 00:00:51,600
guardicore which is a segmentation

19
00:00:51,600 --> 00:00:53,920
company disrupting the legacy firewall

20
00:00:53,920 --> 00:00:56,800
market i'm excited by anything low-level

21
00:00:56,800 --> 00:00:59,840
and also the author of begin.re a free

22
00:00:59,840 --> 00:01:02,239
online tutorial for beginners in reverse

23
00:01:02,239 --> 00:01:04,800
engineering

24
00:01:06,240 --> 00:01:08,560
so why hyper-v in the first place in the

25
00:01:08,560 --> 00:01:10,560
recent years many businesses and

26
00:01:10,560 --> 00:01:12,640
companies have moved

27
00:01:12,640 --> 00:01:14,720
major parts of their workloads to public

28
00:01:14,720 --> 00:01:17,040
clouds and one of these clouds is

29
00:01:17,040 --> 00:01:18,960
microsoft azure

30
00:01:18,960 --> 00:01:21,360
now the virtualization technology under

31
00:01:21,360 --> 00:01:23,600
uh the underlying technology for

32
00:01:23,600 --> 00:01:26,640
microsoft azure is hyper-v and we all

33
00:01:26,640 --> 00:01:28,400
know that vulnerabilities and cloud

34
00:01:28,400 --> 00:01:30,960
infrastructure have massive critical

35
00:01:30,960 --> 00:01:34,000
impacts on our information security so

36
00:01:34,000 --> 00:01:36,880
we decided to focus on targeting

37
00:01:36,880 --> 00:01:39,280
hyper-v

38
00:01:39,280 --> 00:01:40,880
and a couple of things we need to know

39
00:01:40,880 --> 00:01:43,200
before we dig into this talk

40
00:01:43,200 --> 00:01:46,479
some 101 on hyper-v itself

41
00:01:46,479 --> 00:01:48,079
there are two types of partitions

42
00:01:48,079 --> 00:01:50,079
running on top of a hyper-v host the

43
00:01:50,079 --> 00:01:52,479
first is the root partition where the

44
00:01:52,479 --> 00:01:54,560
main operating system runs the host's

45
00:01:54,560 --> 00:01:56,079
operating system

46
00:01:56,079 --> 00:01:58,079
and the child partition is where any

47
00:01:58,079 --> 00:02:00,479
guest operating system runs any virtual

48
00:02:00,479 --> 00:02:02,719
machine and there can be multiple child

49
00:02:02,719 --> 00:02:04,560
partitions

50
00:02:04,560 --> 00:02:07,520
now to provide the child partitions with

51
00:02:07,520 --> 00:02:10,318
hardware services hyper-v implements

52
00:02:10,318 --> 00:02:12,400
power virtualized devices

53
00:02:12,400 --> 00:02:14,000
that consists of

54
00:02:14,000 --> 00:02:16,640
a pair for each hardware device

55
00:02:16,640 --> 00:02:18,720
the vsp is the virtualized service

56
00:02:18,720 --> 00:02:21,120
provider which runs in on the root

57
00:02:21,120 --> 00:02:22,959
partition

58
00:02:22,959 --> 00:02:25,680
and the consumer or the vsc runs in the

59
00:02:25,680 --> 00:02:28,160
child partition

60
00:02:28,160 --> 00:02:30,480
they both communicate over vmbus which

61
00:02:30,480 --> 00:02:32,480
is an inter-partition communication

62
00:02:32,480 --> 00:02:33,599
channel

63
00:02:33,599 --> 00:02:36,239
and whenever it needs to the vsp

64
00:02:36,239 --> 00:02:38,160
communicates with the hardware itself

65
00:02:38,160 --> 00:02:39,760
through the vmm the virtual machine

66
00:02:39,760 --> 00:02:42,239
monitor

67
00:02:42,560 --> 00:02:45,440
in our research the target was vm switch

68
00:02:45,440 --> 00:02:47,599
which is the vsp virtualized service

69
00:02:47,599 --> 00:02:50,160
provider responsible for networking in

70
00:02:50,160 --> 00:02:52,640
hyper-v

71
00:02:53,280 --> 00:02:56,879
another question to ask is why fuzzing

72
00:02:56,879 --> 00:02:59,440
so we decided that fuzzing would be

73
00:02:59,440 --> 00:03:02,319
ideal for us because once we have a good

74
00:03:02,319 --> 00:03:03,760
efficient fuzzer

75
00:03:03,760 --> 00:03:05,360
this will be

76
00:03:05,360 --> 00:03:07,680
scalable and we can find hopefully more

77
00:03:07,680 --> 00:03:09,280
than one bug

78
00:03:09,280 --> 00:03:12,000
and also our target vm switch is quite a

79
00:03:12,000 --> 00:03:13,680
huge binary

80
00:03:13,680 --> 00:03:15,440
and we figured that

81
00:03:15,440 --> 00:03:17,599
right reading its code would be quite a

82
00:03:17,599 --> 00:03:20,879
tedious task so we went for fuzzing

83
00:03:20,879 --> 00:03:23,360
and now some basic concepts in fuzzing

84
00:03:23,360 --> 00:03:25,120
what does a fuzzing infrastructure

85
00:03:25,120 --> 00:03:27,040
consist of

86
00:03:27,040 --> 00:03:28,640
the core component of a fuzzing

87
00:03:28,640 --> 00:03:30,480
infrastructure is the harness the

88
00:03:30,480 --> 00:03:32,239
harness is the core component

89
00:03:32,239 --> 00:03:35,280
responsible for sending fuzzing inputs

90
00:03:35,280 --> 00:03:37,920
to the target software

91
00:03:37,920 --> 00:03:39,200
and there are a couple of more

92
00:03:39,200 --> 00:03:41,440
components that make a fuzzer actually

93
00:03:41,440 --> 00:03:43,440
great in terms of efficiency and

94
00:03:43,440 --> 00:03:44,640
performance

95
00:03:44,640 --> 00:03:46,720
the first one is coverage guidance this

96
00:03:46,720 --> 00:03:49,040
means that for every fuzzing input we

97
00:03:49,040 --> 00:03:51,840
send to the target we receive feedback

98
00:03:51,840 --> 00:03:53,519
on which lines of code exactly were

99
00:03:53,519 --> 00:03:55,360
covered and therefore we can drive a

100
00:03:55,360 --> 00:03:56,879
more efficient

101
00:03:56,879 --> 00:03:58,879
mutation process and fuzzing process

102
00:03:58,879 --> 00:04:00,879
overall

103
00:04:00,879 --> 00:04:03,120
the second component is crash monitoring

104
00:04:03,120 --> 00:04:05,439
which means that for each crash we do

105
00:04:05,439 --> 00:04:07,680
not only receive the payload that caused

106
00:04:07,680 --> 00:04:09,840
the crash but also details such as the

107
00:04:09,840 --> 00:04:11,599
stack trace

108
00:04:11,599 --> 00:04:13,280
the name of the function which crashed

109
00:04:13,280 --> 00:04:16,478
the exact line of code etc

110
00:04:16,478 --> 00:04:18,079
and the third feature is structure

111
00:04:18,079 --> 00:04:19,199
awareness

112
00:04:19,199 --> 00:04:21,440
this means that we don't send arbitrary

113
00:04:21,440 --> 00:04:24,080
sequences of bytes to our target but

114
00:04:24,080 --> 00:04:27,520
rather form structured inputs these can

115
00:04:27,520 --> 00:04:30,240
be certain file formats for several

116
00:04:30,240 --> 00:04:32,479
targets or in our case

117
00:04:32,479 --> 00:04:34,639
structured packets

118
00:04:34,639 --> 00:04:38,240
we will see this in the next slide

119
00:04:38,240 --> 00:04:40,400
so first thing first we need a harness

120
00:04:40,400 --> 00:04:42,080
we need to be able to communicate with

121
00:04:42,080 --> 00:04:45,520
our target vm switch

122
00:04:45,520 --> 00:04:47,919
the first resource we stumbled upon as

123
00:04:47,919 --> 00:04:49,280
part of our

124
00:04:49,280 --> 00:04:52,400
open source intelligence was a blog post

125
00:04:52,400 --> 00:04:54,400
called fuzzing power virtualized devices

126
00:04:54,400 --> 00:04:57,759
in hyper-v now this looked quite uh good

127
00:04:57,759 --> 00:05:00,000
for us because this is what we wanted to

128
00:05:00,000 --> 00:05:01,840
do exactly we wanted to fuzz a power

129
00:05:01,840 --> 00:05:03,440
virtualized device

130
00:05:03,440 --> 00:05:06,560
which is vm switch

131
00:05:06,960 --> 00:05:09,120
and we actually kind of liked the

132
00:05:09,120 --> 00:05:10,960
methodology that was presented in this

133
00:05:10,960 --> 00:05:12,320
blog post

134
00:05:12,320 --> 00:05:14,880
the researchers targeted the virtual pci

135
00:05:14,880 --> 00:05:17,520
bus in hyper-v and not vmswitch

136
00:05:17,520 --> 00:05:20,320
but what they did was to obtain

137
00:05:20,320 --> 00:05:22,880
the vmbus channel that the vsc the

138
00:05:22,880 --> 00:05:25,680
consumer uses on the child partition and

139
00:05:25,680 --> 00:05:28,000
then use this vm bus channel to send

140
00:05:28,000 --> 00:05:29,600
inputs to

141
00:05:29,600 --> 00:05:31,919
the provider to the vsp

142
00:05:31,919 --> 00:05:34,800
once they had this primitive or this

143
00:05:34,800 --> 00:05:35,919
ability

144
00:05:35,919 --> 00:05:38,080
they built a fuzzer on top of that and

145
00:05:38,080 --> 00:05:39,840
were able to send

146
00:05:39,840 --> 00:05:41,759
countless of fuzzy inputs and find

147
00:05:41,759 --> 00:05:44,320
vulnerabilities

148
00:05:44,320 --> 00:05:46,639
so again we decided to adopt this

149
00:05:46,639 --> 00:05:50,800
methodology for our own target vm switch

150
00:05:50,800 --> 00:05:54,080
and as you can probably imagine uh the

151
00:05:54,080 --> 00:05:55,840
main challenge here is to be able to

152
00:05:55,840 --> 00:05:57,600
actually understand where the vmbus

153
00:05:57,600 --> 00:06:01,600
channel is and this was uh quite a long

154
00:06:01,600 --> 00:06:04,240
sub project for us it took a couple of

155
00:06:04,240 --> 00:06:07,440
uh weeks i think to find it out but we

156
00:06:07,440 --> 00:06:09,520
sum it up in the slide that you see on

157
00:06:09,520 --> 00:06:11,199
the screen

158
00:06:11,199 --> 00:06:12,479
we found that as part of the

159
00:06:12,479 --> 00:06:15,680
initialization process of net vsc which

160
00:06:15,680 --> 00:06:16,400
is

161
00:06:16,400 --> 00:06:19,759
the consumer for networking services

162
00:06:19,759 --> 00:06:22,080
it creates a structure called mini port

163
00:06:22,080 --> 00:06:25,039
adapter context now just another thing

164
00:06:25,039 --> 00:06:27,759
that's worth mentioning is that net vsc

165
00:06:27,759 --> 00:06:29,600
is a mini port driver which means it

166
00:06:29,600 --> 00:06:31,680
communicates directly with a network

167
00:06:31,680 --> 00:06:35,199
interface card over and this

168
00:06:35,199 --> 00:06:37,199
so it creates this mini port adapter

169
00:06:37,199 --> 00:06:40,080
context and inside this context at

170
00:06:40,080 --> 00:06:43,600
offset 18 in hex it stores the vmbus

171
00:06:43,600 --> 00:06:45,360
channel

172
00:06:45,360 --> 00:06:47,199
so this took some time but eventually we

173
00:06:47,199 --> 00:06:49,759
understood that and we were able to

174
00:06:49,759 --> 00:06:52,960
start writing our harness driver

175
00:06:52,960 --> 00:06:54,960
and in order to do that we first needed

176
00:06:54,960 --> 00:06:58,400
to find the endis.cis module in the

177
00:06:58,400 --> 00:07:00,479
kernel of the child partition so we

178
00:07:00,479 --> 00:07:03,120
iterated on all loaded modules and found

179
00:07:03,120 --> 00:07:04,880
endis

180
00:07:04,880 --> 00:07:07,280
inside endis we used a global variable

181
00:07:07,280 --> 00:07:10,560
called endis miniport list which

182
00:07:10,560 --> 00:07:14,160
points to the list of network adapters

183
00:07:14,160 --> 00:07:16,160
we went through this list until we

184
00:07:16,160 --> 00:07:19,039
finally found the adapter that we wanted

185
00:07:19,039 --> 00:07:21,280
to fuzz with

186
00:07:21,280 --> 00:07:23,360
and we did did this by comparing the

187
00:07:23,360 --> 00:07:24,400
names

188
00:07:24,400 --> 00:07:26,960
and then as you can probably guess we

189
00:07:26,960 --> 00:07:29,599
took the context from the adapter

190
00:07:29,599 --> 00:07:32,319
and obtain the bmbus channel from offset

191
00:07:32,319 --> 00:07:36,080
18 in this context

192
00:07:36,080 --> 00:07:37,759
now the nice thing is once you have a

193
00:07:37,759 --> 00:07:40,479
vmbus channel at hand you can simply use

194
00:07:40,479 --> 00:07:43,360
it with known documented apis

195
00:07:43,360 --> 00:07:46,080
allocate a packet and send it over to

196
00:07:46,080 --> 00:07:48,879
the other side to a vm switch in our

197
00:07:48,879 --> 00:07:51,199
case

198
00:07:51,759 --> 00:07:54,479
so just to get a glimpse of our our code

199
00:07:54,479 --> 00:07:56,960
this was the harness driver and a very

200
00:07:56,960 --> 00:07:59,759
initial version of it but we'll walk

201
00:07:59,759 --> 00:08:01,360
through it quickly

202
00:08:01,360 --> 00:08:04,080
so here in this for loop we iterate over

203
00:08:04,080 --> 00:08:05,039
all

204
00:08:05,039 --> 00:08:08,400
mini port adapters once we find the one

205
00:08:08,400 --> 00:08:09,840
we need

206
00:08:09,840 --> 00:08:11,759
we fetch the context and then from the

207
00:08:11,759 --> 00:08:16,400
context we take out the channel pointer

208
00:08:19,440 --> 00:08:22,160
so what we had at this phase was our own

209
00:08:22,160 --> 00:08:24,160
driver loaded in the child partition

210
00:08:24,160 --> 00:08:25,199
kernel

211
00:08:25,199 --> 00:08:27,440
able to obtain the vmbus channel and

212
00:08:27,440 --> 00:08:30,319
send inputs and we actually managed to

213
00:08:30,319 --> 00:08:32,880
send some arbitrary bytes and see them

214
00:08:32,880 --> 00:08:34,559
in vm switch side

215
00:08:34,559 --> 00:08:37,519
but our next challenge was to design

216
00:08:37,519 --> 00:08:39,599
such inputs that actually propagate

217
00:08:39,599 --> 00:08:41,919
through vm switch code

218
00:08:41,919 --> 00:08:45,040
so the next step was to look at vm

219
00:08:45,040 --> 00:08:47,920
switch and how it processes packet

220
00:08:47,920 --> 00:08:50,560
now as it appears every vsp virtualized

221
00:08:50,560 --> 00:08:51,920
service provider

222
00:08:51,920 --> 00:08:54,399
every vsp needs to implement a callback

223
00:08:54,399 --> 00:08:57,120
called vmb channel process packet

224
00:08:57,120 --> 00:08:59,200
and this callback accepts five

225
00:08:59,200 --> 00:09:02,240
parameters the vmbus channel the packet

226
00:09:02,240 --> 00:09:04,160
some buffer and its length and some

227
00:09:04,160 --> 00:09:06,320
flags

228
00:09:06,320 --> 00:09:08,880
and as you can see here on the slide

229
00:09:08,880 --> 00:09:11,600
vm switch registers its own callback

230
00:09:11,600 --> 00:09:14,959
function called kmcl process packet

231
00:09:14,959 --> 00:09:16,800
this is actually the first function that

232
00:09:16,800 --> 00:09:19,440
a packet meets in the vsp

233
00:09:19,440 --> 00:09:20,880
side

234
00:09:20,880 --> 00:09:24,240
the earliest point in time

235
00:09:24,880 --> 00:09:27,040
so like i mentioned before

236
00:09:27,040 --> 00:09:30,160
vm switch does not process any sequence

237
00:09:30,160 --> 00:09:31,920
of bytes it expects actually very

238
00:09:31,920 --> 00:09:34,640
structured data that is called nvsp

239
00:09:34,640 --> 00:09:35,760
packets

240
00:09:35,760 --> 00:09:37,440
and these packets

241
00:09:37,440 --> 00:09:39,440
have many sub-types

242
00:09:39,440 --> 00:09:40,880
some of them are responsible for

243
00:09:40,880 --> 00:09:42,880
initializing the communication some of

244
00:09:42,880 --> 00:09:45,279
them are responsible for setting

245
00:09:45,279 --> 00:09:47,120
the ring buffers that are required for

246
00:09:47,120 --> 00:09:49,680
communication send and receive buffers i

247
00:09:49,680 --> 00:09:51,200
think i forgot to mention them before

248
00:09:51,200 --> 00:09:53,440
but these are ring buffers that are used

249
00:09:53,440 --> 00:09:56,240
to transfer data

250
00:09:56,240 --> 00:09:59,760
and one single type of nvsp is our endis

251
00:09:59,760 --> 00:10:00,800
packets

252
00:10:00,800 --> 00:10:02,560
our end this is another protocol that's

253
00:10:02,560 --> 00:10:04,720
responsible for communication between a

254
00:10:04,720 --> 00:10:08,480
host and the network adapter

255
00:10:08,640 --> 00:10:10,320
so now that we kind of know the

256
00:10:10,320 --> 00:10:12,959
different types of nvsp we can take a

257
00:10:12,959 --> 00:10:15,120
deeper look into the package processing

258
00:10:15,120 --> 00:10:17,040
callback and see the different code

259
00:10:17,040 --> 00:10:19,839
flows that originate from it

260
00:10:19,839 --> 00:10:22,560
the first one as you can see is the the

261
00:10:22,560 --> 00:10:25,279
flow that handles initialization

262
00:10:25,279 --> 00:10:26,800
packets

263
00:10:26,800 --> 00:10:30,079
the other flow handles are ndis messages

264
00:10:30,079 --> 00:10:32,240
and the third flow is responsible for

265
00:10:32,240 --> 00:10:35,200
any other type

266
00:10:35,200 --> 00:10:37,760
now we decided to focus on r and this

267
00:10:37,760 --> 00:10:39,920
because of two reasons the first is that

268
00:10:39,920 --> 00:10:42,880
r and this actually has a lot of logic

269
00:10:42,880 --> 00:10:44,959
implemented in vm switch so it's just a

270
00:10:44,959 --> 00:10:47,200
single type but it's responsible for

271
00:10:47,200 --> 00:10:49,200
quite a lot of code in the driver's

272
00:10:49,200 --> 00:10:50,480
binary

273
00:10:50,480 --> 00:10:51,920
and the second is that many past

274
00:10:51,920 --> 00:10:55,040
vulnerabilities that we read about were

275
00:10:55,040 --> 00:10:58,320
found in the r in this area so we simply

276
00:10:58,320 --> 00:11:00,480
thought it would be a good idea to start

277
00:11:00,480 --> 00:11:02,240
there

278
00:11:02,240 --> 00:11:04,000
now the function that you see here

279
00:11:04,000 --> 00:11:06,320
handle r and descent message

280
00:11:06,320 --> 00:11:09,279
is not a documented function so we

281
00:11:09,279 --> 00:11:10,880
needed to reverse engineer it a little

282
00:11:10,880 --> 00:11:14,160
bit and we found that it receives four

283
00:11:14,160 --> 00:11:16,000
different parameters

284
00:11:16,000 --> 00:11:17,760
three of them were actually passed

285
00:11:17,760 --> 00:11:19,920
directly from the parent function the

286
00:11:19,920 --> 00:11:21,760
process packet function and these

287
00:11:21,760 --> 00:11:24,320
parameters are the channel pointer

288
00:11:24,320 --> 00:11:27,680
the buffer and the packet itself

289
00:11:27,680 --> 00:11:30,240
but there's also another parameter here

290
00:11:30,240 --> 00:11:32,800
that is a pointer to an mdl

291
00:11:32,800 --> 00:11:33,760
so

292
00:11:33,760 --> 00:11:35,839
we thought to ourselves what is exactly

293
00:11:35,839 --> 00:11:37,360
the difference between

294
00:11:37,360 --> 00:11:39,600
the buffer and the mdl where does the

295
00:11:39,600 --> 00:11:43,440
packet actually lie

296
00:11:43,440 --> 00:11:45,760
we found that the buffer is responsible

297
00:11:45,760 --> 00:11:48,000
for some metadata on the packet it

298
00:11:48,000 --> 00:11:50,880
specifies what nvsp packet type is being

299
00:11:50,880 --> 00:11:53,200
sent what is the channel type whether it

300
00:11:53,200 --> 00:11:54,639
is data

301
00:11:54,639 --> 00:11:55,440
with

302
00:11:55,440 --> 00:11:58,000
zero value or control channel with the

303
00:11:58,000 --> 00:11:59,440
value of one

304
00:11:59,440 --> 00:12:01,920
and then two d words specify where

305
00:12:01,920 --> 00:12:04,880
vmswitch can find the sent data on the

306
00:12:04,880 --> 00:12:06,399
send buffer

307
00:12:06,399 --> 00:12:08,480
so it specifies both the index of the

308
00:12:08,480 --> 00:12:10,000
section in the send buffer and the

309
00:12:10,000 --> 00:12:12,480
section size

310
00:12:12,480 --> 00:12:14,639
so apparently this is one way to send

311
00:12:14,639 --> 00:12:16,959
packets through the send buffer

312
00:12:16,959 --> 00:12:18,639
but there appears to be another way to

313
00:12:18,639 --> 00:12:20,399
send data to vmswitch and this is

314
00:12:20,399 --> 00:12:21,920
through mdl

315
00:12:21,920 --> 00:12:24,320
and when you send data through an mdl

316
00:12:24,320 --> 00:12:26,240
what you need to do is to allocate some

317
00:12:26,240 --> 00:12:28,639
memory put your packet there and then

318
00:12:28,639 --> 00:12:32,160
create an mdl that points to your new

319
00:12:32,160 --> 00:12:33,760
data buffer

320
00:12:33,760 --> 00:12:35,440
and in this case what the buffer needs

321
00:12:35,440 --> 00:12:38,320
to specify is -1 for section index

322
00:12:38,320 --> 00:12:41,200
because the send buffer is not in use at

323
00:12:41,200 --> 00:12:42,079
all

324
00:12:42,079 --> 00:12:44,320
and the sex the section size does not

325
00:12:44,320 --> 00:12:45,200
matter

326
00:12:45,200 --> 00:12:47,120
we decided to actually take

327
00:12:47,120 --> 00:12:50,160
this path because uh dealing with mdls

328
00:12:50,160 --> 00:12:52,320
was easier for us instead of

329
00:12:52,320 --> 00:12:54,240
understanding the mechanisms of the send

330
00:12:54,240 --> 00:12:56,639
buffer

331
00:12:56,959 --> 00:12:59,600
so just to kind of sum up how

332
00:12:59,600 --> 00:13:02,560
what sending packets with mdls look like

333
00:13:02,560 --> 00:13:04,560
there is the buffer that specifies that

334
00:13:04,560 --> 00:13:07,680
an r in this message is being sent

335
00:13:07,680 --> 00:13:09,519
one for control channel and the

336
00:13:09,519 --> 00:13:11,040
important part is the v word that

337
00:13:11,040 --> 00:13:14,480
specifies -1 for section index and then

338
00:13:14,480 --> 00:13:17,279
the memory pointed to by the mdl has the

339
00:13:17,279 --> 00:13:20,160
actual r in this packet both its type

340
00:13:20,160 --> 00:13:22,000
and the different

341
00:13:22,000 --> 00:13:23,760
are in this fields

342
00:13:23,760 --> 00:13:27,279
that are dependent on the type

343
00:13:28,160 --> 00:13:30,639
so now we were not only able to send

344
00:13:30,639 --> 00:13:32,959
inputs from the child partition we were

345
00:13:32,959 --> 00:13:34,480
actually able to

346
00:13:34,480 --> 00:13:36,480
design inputs that

347
00:13:36,480 --> 00:13:37,600
visit

348
00:13:37,600 --> 00:13:41,440
certain functions on on vm switch side

349
00:13:41,440 --> 00:13:43,120
and with this ability we decided to

350
00:13:43,120 --> 00:13:45,279
trigger a past vulnerability with our

351
00:13:45,279 --> 00:13:47,680
harness and to test it by that

352
00:13:47,680 --> 00:13:51,680
and we chose this cve by lisa sage

353
00:13:51,680 --> 00:13:53,760
we triggered it using our windows driver

354
00:13:53,760 --> 00:13:55,839
which which was the first poc from a

355
00:13:55,839 --> 00:13:57,199
windows guest

356
00:13:57,199 --> 00:14:00,399
and we managed to crash the hyper-v host

357
00:14:00,399 --> 00:14:03,760
this was a nice starting point but uh

358
00:14:03,760 --> 00:14:05,360
yeah well this was a good starting point

359
00:14:05,360 --> 00:14:07,680
and now is the time to actually

360
00:14:07,680 --> 00:14:09,920
connect this harness to uh fuzzing

361
00:14:09,920 --> 00:14:12,160
infrastructure and for this section i

362
00:14:12,160 --> 00:14:16,000
invite fennig and pass the ball to him

363
00:14:16,000 --> 00:14:18,560
thank you a few

364
00:14:18,720 --> 00:14:20,639
so obviously we've completed our first

365
00:14:20,639 --> 00:14:23,279
phase of our fuzzing product we now have

366
00:14:23,279 --> 00:14:25,839
our harness which allows us to send any

367
00:14:25,839 --> 00:14:27,360
data we would like

368
00:14:27,360 --> 00:14:29,199
to vm switch and

369
00:14:29,199 --> 00:14:31,680
it is fully possible

370
00:14:31,680 --> 00:14:32,639
and

371
00:14:32,639 --> 00:14:35,440
as of you mentioned before we also have

372
00:14:35,440 --> 00:14:37,120
an additional components which we wanted

373
00:14:37,120 --> 00:14:39,760
to implement in order to build an

374
00:14:39,760 --> 00:14:43,040
end-to-end fuzzing infrastructure

375
00:14:43,040 --> 00:14:45,600
so

376
00:14:45,600 --> 00:14:47,440
before we dive into the components of

377
00:14:47,440 --> 00:14:48,480
the fuzzing infrastructure let's

378
00:14:48,480 --> 00:14:50,240
understand exactly

379
00:14:50,240 --> 00:14:53,040
what we expect the setup would look like

380
00:14:53,040 --> 00:14:55,120
and we actually examined multiple

381
00:14:55,120 --> 00:14:58,000
fuzzing projects which were open source

382
00:14:58,000 --> 00:15:00,000
and we picked kfl as it looks very

383
00:15:00,000 --> 00:15:02,560
promising and we wanted to use it

384
00:15:02,560 --> 00:15:04,639
so this is how the setup looks like

385
00:15:04,639 --> 00:15:05,760
we have

386
00:15:05,760 --> 00:15:07,279
l0

387
00:15:07,279 --> 00:15:09,279
with regards to

388
00:15:09,279 --> 00:15:12,240
the l0 and l1 and l2 terms this is

389
00:15:12,240 --> 00:15:13,839
actually describe level of

390
00:15:13,839 --> 00:15:16,639
virtualization so l0 is actually the

391
00:15:16,639 --> 00:15:19,279
bare metal host which actually runs

392
00:15:19,279 --> 00:15:22,000
ubuntu linux with kvm which is the linux

393
00:15:22,000 --> 00:15:26,399
hypervisor with kfl runs above it we

394
00:15:26,399 --> 00:15:27,839
have a one

395
00:15:27,839 --> 00:15:30,560
which is our target hypervisor hyper-v

396
00:15:30,560 --> 00:15:33,519
and we have l2 which contains

397
00:15:33,519 --> 00:15:35,759
two griton machines two partitions

398
00:15:35,759 --> 00:15:37,600
hyper-v partitions are actually virtual

399
00:15:37,600 --> 00:15:39,199
machines the root partition which

400
00:15:39,199 --> 00:15:41,759
contains the host os which runs vm

401
00:15:41,759 --> 00:15:44,079
switch our target driver and the guest

402
00:15:44,079 --> 00:15:46,639
partition and the child partition the

403
00:15:46,639 --> 00:15:49,440
guest vm which contains our driver which

404
00:15:49,440 --> 00:15:51,920
sends packets to vm switch

405
00:15:51,920 --> 00:15:53,759
now the thing about this setup is it

406
00:15:53,759 --> 00:15:56,720
actually contains necessary utilization

407
00:15:56,720 --> 00:15:58,880
we have two hypervisors

408
00:15:58,880 --> 00:16:00,240
one within the other

409
00:16:00,240 --> 00:16:02,560
and in order to enable hyper and

410
00:16:02,560 --> 00:16:04,720
necessary utilization for kafl because

411
00:16:04,720 --> 00:16:06,079
at this point we actually understood

412
00:16:06,079 --> 00:16:07,680
that kfl does not support nested

413
00:16:07,680 --> 00:16:10,160
virtualization we understood

414
00:16:10,160 --> 00:16:12,959
uh we have some tasks we need to

415
00:16:12,959 --> 00:16:14,000
perform

416
00:16:14,000 --> 00:16:16,480
and now let's describe it then quickly

417
00:16:16,480 --> 00:16:18,639
we actually have

418
00:16:18,639 --> 00:16:20,959
an msr which enables the intel pt

419
00:16:20,959 --> 00:16:23,440
technology uh for the code coverage

420
00:16:23,440 --> 00:16:25,600
which we described later and we need to

421
00:16:25,600 --> 00:16:28,240
enable this msr for a specific vm we

422
00:16:28,240 --> 00:16:30,000
only want to have code coverage out of

423
00:16:30,000 --> 00:16:33,120
the root partition and not from the

424
00:16:33,120 --> 00:16:34,639
child partition

425
00:16:34,639 --> 00:16:36,720
we also need uh

426
00:16:36,720 --> 00:16:38,399
to actually read and write memory

427
00:16:38,399 --> 00:16:41,600
directly from l0 to l2 as we would like

428
00:16:41,600 --> 00:16:45,120
to pass fuzzing inputs from

429
00:16:45,120 --> 00:16:48,399
kfl directly to the child partition and

430
00:16:48,399 --> 00:16:51,040
we also need to handle hyper call

431
00:16:51,040 --> 00:16:54,720
directly from l2 within l0

432
00:16:54,720 --> 00:16:56,399
now as we work with pre-allocated time

433
00:16:56,399 --> 00:16:57,839
constraints we actually understood at

434
00:16:57,839 --> 00:16:59,839
that point that these tests might take

435
00:16:59,839 --> 00:17:02,240
longer than we allocated so we decided

436
00:17:02,240 --> 00:17:05,839
to find some kind of a workaround

437
00:17:05,919 --> 00:17:08,720
so we decided to not to fuzz from l2 and

438
00:17:08,720 --> 00:17:12,400
actually fuzz from l1

439
00:17:13,760 --> 00:17:15,599
and we actually understood that

440
00:17:15,599 --> 00:17:18,319
obviously once you deploy hyper-v you

441
00:17:18,319 --> 00:17:21,760
have l0 l1 and l2 vm switch is loaded

442
00:17:21,760 --> 00:17:24,400
to the root partition properly

443
00:17:24,400 --> 00:17:27,280
and once we disabled vtx which is the

444
00:17:27,280 --> 00:17:29,440
intel virtualization capabilities

445
00:17:29,440 --> 00:17:30,840
obviously

446
00:17:30,840 --> 00:17:33,280
hyper-v won't be able to

447
00:17:33,280 --> 00:17:34,799
boot properly as it doesn't have

448
00:17:34,799 --> 00:17:37,280
utilization capabilities and it won't be

449
00:17:37,280 --> 00:17:39,760
able to boot the hypervisor so we just

450
00:17:39,760 --> 00:17:42,720
use a phobic mode of booting windows 10

451
00:17:42,720 --> 00:17:45,520
normally without any hypervisor enabled

452
00:17:45,520 --> 00:17:47,760
so we only have l1 but in this

453
00:17:47,760 --> 00:17:49,039
particular scenario we actually

454
00:17:49,039 --> 00:17:51,760
discovered that the windows 10 vm

455
00:17:51,760 --> 00:17:54,160
actually contains the vm switch driver

456
00:17:54,160 --> 00:17:56,480
which is loaded to the system

457
00:17:56,480 --> 00:17:58,080
so we thought to ourself

458
00:17:58,080 --> 00:18:00,000
why won't we just call handler in the

459
00:18:00,000 --> 00:18:02,080
send message function with our fuzzing

460
00:18:02,080 --> 00:18:03,679
inputs as a buffer

461
00:18:03,679 --> 00:18:05,760
and just send it directly within the

462
00:18:05,760 --> 00:18:08,240
same vm

463
00:18:08,799 --> 00:18:10,240
so at this point we actually understood

464
00:18:10,240 --> 00:18:11,919
that as we don't have any working

465
00:18:11,919 --> 00:18:13,280
partitions

466
00:18:13,280 --> 00:18:14,799
we don't have any working built-on

467
00:18:14,799 --> 00:18:17,919
machines as hyper-v is not enabled

468
00:18:17,919 --> 00:18:20,000
we actually don't we don't have vm bus

469
00:18:20,000 --> 00:18:22,160
as no one initialized it before and we

470
00:18:22,160 --> 00:18:23,840
don't have any pointer to a vm bus

471
00:18:23,840 --> 00:18:24,720
channel

472
00:18:24,720 --> 00:18:26,720
now the first argument to the handler in

473
00:18:26,720 --> 00:18:29,360
the send message is a pointer to a vmbus

474
00:18:29,360 --> 00:18:31,039
channel so we had to find one or

475
00:18:31,039 --> 00:18:32,799
initialize one

476
00:18:32,799 --> 00:18:34,799
and we started to just engineer a lot of

477
00:18:34,799 --> 00:18:37,200
undocumented structures and feel it

478
00:18:37,200 --> 00:18:39,360
manually and at a certain point we

479
00:18:39,360 --> 00:18:41,840
discovered the vms vm nick more function

480
00:18:41,840 --> 00:18:44,480
within bim suite which is responsible of

481
00:18:44,480 --> 00:18:46,240
initializing a lot of the undocumented

482
00:18:46,240 --> 00:18:48,640
structures we failed manually and also

483
00:18:48,640 --> 00:18:51,280
initializing a vmbus channel

484
00:18:51,280 --> 00:18:53,039
so we tried to call it

485
00:18:53,039 --> 00:18:54,840
and it crushed our

486
00:18:54,840 --> 00:18:57,039
system we analyzed the root cause of the

487
00:18:57,039 --> 00:18:59,440
crash and we discovered that it tries to

488
00:18:59,440 --> 00:19:02,080
call a lot of vm bus related functions

489
00:19:02,080 --> 00:19:03,520
and obviously as i mentioned before

490
00:19:03,520 --> 00:19:05,600
vmbus is not enabled as we don't have

491
00:19:05,600 --> 00:19:07,440
hyper-v working

492
00:19:07,440 --> 00:19:09,679
so obviously the system eventually

493
00:19:09,679 --> 00:19:11,679
it would be crashed

494
00:19:11,679 --> 00:19:14,320
so we actually decided to patch out any

495
00:19:14,320 --> 00:19:17,280
call to a vmbus related logic and vmbus

496
00:19:17,280 --> 00:19:19,440
reality functions

497
00:19:19,440 --> 00:19:21,520
because we understood that vm bus is not

498
00:19:21,520 --> 00:19:23,200
our target vm switch is our target and

499
00:19:23,200 --> 00:19:25,360
vm bus does not interfere with the data

500
00:19:25,360 --> 00:19:26,720
that is being sent from the child

501
00:19:26,720 --> 00:19:28,799
partition to the root partition

502
00:19:28,799 --> 00:19:31,120
so we were not interested in it so we

503
00:19:31,120 --> 00:19:33,280
decided to patch it up and in order to

504
00:19:33,280 --> 00:19:35,440
patch out or modify

505
00:19:35,440 --> 00:19:37,919
any microsoft sign code within the

506
00:19:37,919 --> 00:19:39,760
kernel obviously you'll have to disable

507
00:19:39,760 --> 00:19:41,039
pedgar

508
00:19:41,039 --> 00:19:44,400
otherwise the system will be crashed

509
00:19:44,400 --> 00:19:46,320
so in order to disable patchguard we

510
00:19:46,320 --> 00:19:47,200
have

511
00:19:47,200 --> 00:19:49,679
the first method which is the most

512
00:19:49,679 --> 00:19:51,919
common method that we knew at that time

513
00:19:51,919 --> 00:19:53,919
which is attaching a kernel debugger to

514
00:19:53,919 --> 00:19:56,400
the os once it boots

515
00:19:56,400 --> 00:19:58,720
now by design windows will disable patch

516
00:19:58,720 --> 00:20:01,280
guard if a kernel debugger was attached

517
00:20:01,280 --> 00:20:03,440
during boot time the thing with this

518
00:20:03,440 --> 00:20:06,400
method is actually it might

519
00:20:06,400 --> 00:20:08,960
cause a lot of performance overhead

520
00:20:08,960 --> 00:20:10,799
because it it's waiting for a current

521
00:20:10,799 --> 00:20:12,559
debugger to be attached and later on it

522
00:20:12,559 --> 00:20:14,080
will try to communicate with the kernel

523
00:20:14,080 --> 00:20:16,640
debugger over the kd protocol

524
00:20:16,640 --> 00:20:17,919
and we didn't want to lose any

525
00:20:17,919 --> 00:20:20,240
performance because we wanted our father

526
00:20:20,240 --> 00:20:22,400
infrastructure to be efficient

527
00:20:22,400 --> 00:20:23,919
so we actually found a pretty cool and

528
00:20:23,919 --> 00:20:27,120
useful tool called efi guard efiguard is

529
00:20:27,120 --> 00:20:29,360
an open source uefi boot kit which

530
00:20:29,360 --> 00:20:31,520
enables you to load the antes kernel

531
00:20:31,520 --> 00:20:34,240
without hedge guard enabled and also it

532
00:20:34,240 --> 00:20:35,679
disables the driver signature

533
00:20:35,679 --> 00:20:37,520
enforcement mechanism

534
00:20:37,520 --> 00:20:39,600
which enables us to load any driver we

535
00:20:39,600 --> 00:20:41,760
would like including our father and our

536
00:20:41,760 --> 00:20:43,039
harness driver

537
00:20:43,039 --> 00:20:44,159
and

538
00:20:44,159 --> 00:20:46,240
pretty easily

539
00:20:46,240 --> 00:20:47,760
so we chose this

540
00:20:47,760 --> 00:20:49,520
this tool and it's it was published on

541
00:20:49,520 --> 00:20:50,799
github

542
00:20:50,799 --> 00:20:52,320
and it actually solved our problem and

543
00:20:52,320 --> 00:20:55,039
we patched out the vmbox logic

544
00:20:55,039 --> 00:20:56,640
so now let's take a look of how the

545
00:20:56,640 --> 00:20:59,520
setup looked like so we have l0 our bare

546
00:20:59,520 --> 00:21:03,919
metal sb4 with kvm and kfl above it

547
00:21:03,919 --> 00:21:06,320
we have l1 which is this time it's not

548
00:21:06,320 --> 00:21:08,960
hyper-v it's a regular windows 10 vm

549
00:21:08,960 --> 00:21:10,559
because hyper v is not enabled without

550
00:21:10,559 --> 00:21:12,320
vtx

551
00:21:12,320 --> 00:21:13,760
and

552
00:21:13,760 --> 00:21:16,000
we have our harness executable which

553
00:21:16,000 --> 00:21:18,320
communicates with kfl over kvm hyper

554
00:21:18,320 --> 00:21:21,919
calls in order to get fuzzing payloads

555
00:21:21,919 --> 00:21:23,600
later on it will send our fuzzing

556
00:21:23,600 --> 00:21:26,400
payload to our harness driver which runs

557
00:21:26,400 --> 00:21:27,600
in the kernel

558
00:21:27,600 --> 00:21:29,679
and our file driver will call handler

559
00:21:29,679 --> 00:21:31,360
and descend message within vm switch

560
00:21:31,360 --> 00:21:33,280
with the fuzzing payload

561
00:21:33,280 --> 00:21:34,720
and now obviously it's working because

562
00:21:34,720 --> 00:21:36,720
we patched out vm bus and the system

563
00:21:36,720 --> 00:21:39,280
won't crash

564
00:21:40,159 --> 00:21:42,480
now we chose to name our father half a

565
00:21:42,480 --> 00:21:43,280
one

566
00:21:43,280 --> 00:21:45,200
eight is for hypervisor afl is for the

567
00:21:45,200 --> 00:21:47,360
afl father and l1 as

568
00:21:47,360 --> 00:21:50,320
we only use one level of virtualization

569
00:21:50,320 --> 00:21:54,320
we don't have any nested hypervisor

570
00:21:54,480 --> 00:21:56,400
so as if you mentioned before we've

571
00:21:56,400 --> 00:21:58,880
completed the harness part and we now

572
00:21:58,880 --> 00:22:01,200
have the ability to send packets within

573
00:22:01,200 --> 00:22:03,679
only a single vm to vm switch

574
00:22:03,679 --> 00:22:05,200
but we wanted to have an end-to-end

575
00:22:05,200 --> 00:22:07,520
further infrastructure as we wanted to

576
00:22:07,520 --> 00:22:08,880
have more capabilities in order to

577
00:22:08,880 --> 00:22:11,280
understand exactly whether our father is

578
00:22:11,280 --> 00:22:13,039
efficient and we wanted to automate the

579
00:22:13,039 --> 00:22:16,559
process as much as we could

580
00:22:17,120 --> 00:22:18,480
so let's start and talk about the first

581
00:22:18,480 --> 00:22:20,640
component which is coverage guidance and

582
00:22:20,640 --> 00:22:22,400
as a film mentioned before coverage

583
00:22:22,400 --> 00:22:23,840
guidance actually

584
00:22:23,840 --> 00:22:26,640
is being deployed with kfl now we didn't

585
00:22:26,640 --> 00:22:28,640
want to use blind fuzzing

586
00:22:28,640 --> 00:22:32,000
as we didn't want to send a packet

587
00:22:32,000 --> 00:22:33,760
again and again and

588
00:22:33,760 --> 00:22:35,360
we wanted to understand exactly whether

589
00:22:35,360 --> 00:22:38,000
the packet triggered

590
00:22:38,000 --> 00:22:40,159
which code and understand whether a

591
00:22:40,159 --> 00:22:41,840
packet didn't trigger code and we didn't

592
00:22:41,840 --> 00:22:43,840
want to use it anymore

593
00:22:43,840 --> 00:22:46,159
so kfl actually supports coverage

594
00:22:46,159 --> 00:22:47,919
guidance out of the box

595
00:22:47,919 --> 00:22:49,440
uh by leveraging the intel fitting

596
00:22:49,440 --> 00:22:51,760
mechanism the intel pt mechanism intel

597
00:22:51,760 --> 00:22:53,760
processor trace

598
00:22:53,760 --> 00:22:55,679
actually developed by intel

599
00:22:55,679 --> 00:22:58,559
and enables one to understand exactly

600
00:22:58,559 --> 00:23:00,080
where does the instruction pointer of

601
00:23:00,080 --> 00:23:02,480
the cpu was in a certain time and in a

602
00:23:02,480 --> 00:23:04,400
certain address space

603
00:23:04,400 --> 00:23:07,120
so it's perfect for code coverage

604
00:23:07,120 --> 00:23:09,600
collection during the fuzzing process

605
00:23:09,600 --> 00:23:10,960
and it's important to say and we'll get

606
00:23:10,960 --> 00:23:13,440
back to this fact uh in the next slides

607
00:23:13,440 --> 00:23:14,400
that

608
00:23:14,400 --> 00:23:18,559
kfo coverage guidance works differently

609
00:23:18,559 --> 00:23:21,360
by filtering the coverage according to

610
00:23:21,360 --> 00:23:24,320
the harness process context what does it

611
00:23:24,320 --> 00:23:26,400
exactly mean it means that we have our

612
00:23:26,400 --> 00:23:27,760
harness executable which sends the

613
00:23:27,760 --> 00:23:30,640
payload we'll understand it in a second

614
00:23:30,640 --> 00:23:32,799
and actually the code coverage will

615
00:23:32,799 --> 00:23:34,720
collect

616
00:23:34,720 --> 00:23:36,320
the code coverage mechanism will collect

617
00:23:36,320 --> 00:23:39,039
the coverage as long as the context uh

618
00:23:39,039 --> 00:23:40,640
the current context is our harness

619
00:23:40,640 --> 00:23:43,039
executable uh if the context will be

620
00:23:43,039 --> 00:23:44,559
replaced it will stop collecting

621
00:23:44,559 --> 00:23:45,600
coverage

622
00:23:45,600 --> 00:23:48,480
as it wants to understand exactly uh

623
00:23:48,480 --> 00:23:53,200
what code coverage originated by us

624
00:23:53,360 --> 00:23:54,880
so at this point we actually understood

625
00:23:54,880 --> 00:23:57,520
that there is a problem we have a low

626
00:23:57,520 --> 00:24:00,400
basic blood count of our core coverage

627
00:24:00,400 --> 00:24:02,400
and it didn't make sense to us as

628
00:24:02,400 --> 00:24:04,720
vm switch contains a lot of code it's a

629
00:24:04,720 --> 00:24:07,440
huge binary and even a single payload

630
00:24:07,440 --> 00:24:09,600
should have triggered a lot of code

631
00:24:09,600 --> 00:24:12,559
and the number was pretty low

632
00:24:12,559 --> 00:24:13,919
and at that point we actually understood

633
00:24:13,919 --> 00:24:15,520
that vm switch processes incoming

634
00:24:15,520 --> 00:24:18,000
packets in a more threaded manner and

635
00:24:18,000 --> 00:24:20,559
what does it exactly means it means that

636
00:24:20,559 --> 00:24:22,320
once we send a packet by calling the

637
00:24:22,320 --> 00:24:24,400
handler and the send message

638
00:24:24,400 --> 00:24:26,240
the handler in this function will push

639
00:24:26,240 --> 00:24:29,840
it to a queue which will be processed

640
00:24:29,840 --> 00:24:31,760
which will process the packet in another

641
00:24:31,760 --> 00:24:33,600
thread which means that the context will

642
00:24:33,600 --> 00:24:35,919
be replaced let's understand exactly how

643
00:24:35,919 --> 00:24:37,120
does it look like in the following

644
00:24:37,120 --> 00:24:38,240
diagram

645
00:24:38,240 --> 00:24:40,480
so we have half a one with our harness

646
00:24:40,480 --> 00:24:43,200
executable we have intel pt enabled

647
00:24:43,200 --> 00:24:44,640
because the context is our harness

648
00:24:44,640 --> 00:24:48,159
executable and cr3 obviously assigned to

649
00:24:48,159 --> 00:24:50,480
the harness executable one

650
00:24:50,480 --> 00:24:52,000
now we'll send the packet to the faster

651
00:24:52,000 --> 00:24:54,080
driver to the harness driver

652
00:24:54,080 --> 00:24:56,240
and the driver will actually call the

653
00:24:56,240 --> 00:24:58,840
handler and descend message now as i

654
00:24:58,840 --> 00:25:00,960
mentioned the

655
00:25:00,960 --> 00:25:02,799
handler and descend message will

656
00:25:02,799 --> 00:25:03,760
actually

657
00:25:03,760 --> 00:25:05,520
push the packet to a queue

658
00:25:05,520 --> 00:25:08,080
and once the other thread the worker

659
00:25:08,080 --> 00:25:10,240
thread will try to process the packet

660
00:25:10,240 --> 00:25:11,679
the context will be switched to the

661
00:25:11,679 --> 00:25:13,520
system process

662
00:25:13,520 --> 00:25:15,520
and it actually means that the code

663
00:25:15,520 --> 00:25:18,000
coverage will be disabled that's the cr3

664
00:25:18,000 --> 00:25:20,480
changed and intel pt disabled as well

665
00:25:20,480 --> 00:25:21,440
and that's

666
00:25:21,440 --> 00:25:23,600
the actual reason we lose we lost

667
00:25:23,600 --> 00:25:24,880
coverage

668
00:25:24,880 --> 00:25:26,720
so the solution was pretty simple we

669
00:25:26,720 --> 00:25:29,120
actually disabled the intel ptcr3

670
00:25:29,120 --> 00:25:30,720
filtering which means that

671
00:25:30,720 --> 00:25:32,480
it will monitor all the executions

672
00:25:32,480 --> 00:25:35,120
within the vm switch address space

673
00:25:35,120 --> 00:25:37,679
and we also knew that as vm switch was

674
00:25:37,679 --> 00:25:40,080
not enabled before we are the only one

675
00:25:40,080 --> 00:25:42,159
who performs operations within vm switch

676
00:25:42,159 --> 00:25:44,320
hyper v is not enabled

677
00:25:44,320 --> 00:25:45,200
so

678
00:25:45,200 --> 00:25:46,640
we definitely knew that all the

679
00:25:46,640 --> 00:25:48,000
operations we monitor within the

680
00:25:48,000 --> 00:25:50,000
vmswitch address space was originated by

681
00:25:50,000 --> 00:25:52,159
us

682
00:25:52,240 --> 00:25:53,679
and this is actually a screenshot of the

683
00:25:53,679 --> 00:25:56,400
lighthouse ida plugin and we wrote a

684
00:25:56,400 --> 00:25:59,679
script which converts kfl intel pt trace

685
00:25:59,679 --> 00:26:03,120
format to a lighthouse compatible one

686
00:26:03,120 --> 00:26:05,440
now it was we actually

687
00:26:05,440 --> 00:26:07,840
it was pretty useful for us because it

688
00:26:07,840 --> 00:26:09,440
actually visualized the code coverage

689
00:26:09,440 --> 00:26:11,840
for us and we understand exactly which

690
00:26:11,840 --> 00:26:13,200
part of the code we triggered and which

691
00:26:13,200 --> 00:26:15,039
parts we didn't trigger and obviously we

692
00:26:15,039 --> 00:26:16,320
did some modifications to the

693
00:26:16,320 --> 00:26:18,000
infrastructure in order to trigger more

694
00:26:18,000 --> 00:26:20,240
code

695
00:26:20,320 --> 00:26:22,400
so this was the coverage guidance

696
00:26:22,400 --> 00:26:25,520
and let's talk about crash monitoring

697
00:26:25,520 --> 00:26:27,760
now as if you mentioned we actually

698
00:26:27,760 --> 00:26:29,039
wanted to have

699
00:26:29,039 --> 00:26:31,440
some kind of a centralized mechanism

700
00:26:31,440 --> 00:26:32,400
which

701
00:26:32,400 --> 00:26:33,430
contains and

702
00:26:33,430 --> 00:26:34,799
[Music]

703
00:26:34,799 --> 00:26:36,840
contains of all of the crashes with the

704
00:26:36,840 --> 00:26:39,600
detailed crash and we didn't want to

705
00:26:39,600 --> 00:26:41,039
only knew that the crash occurred

706
00:26:41,039 --> 00:26:42,799
because we didn't want to reproduce it

707
00:26:42,799 --> 00:26:45,200
manually every time we had a crash

708
00:26:45,200 --> 00:26:47,440
because it actually means to

709
00:26:47,440 --> 00:26:49,840
we need to deploy a vm and we need to

710
00:26:49,840 --> 00:26:51,120
reproduce

711
00:26:51,120 --> 00:26:53,039
the exact crash and we need to attach a

712
00:26:53,039 --> 00:26:54,480
kernel debugger and understand the exact

713
00:26:54,480 --> 00:26:55,679
stock trace

714
00:26:55,679 --> 00:26:58,320
now kfl actually is deployed with a very

715
00:26:58,320 --> 00:27:00,960
basic functionality of crash monitoring

716
00:27:00,960 --> 00:27:03,200
for windows os it actually only signals

717
00:27:03,200 --> 00:27:04,320
you

718
00:27:04,320 --> 00:27:06,320
that it crash record and tells you which

719
00:27:06,320 --> 00:27:08,799
failure to cause the crest

720
00:27:08,799 --> 00:27:10,159
and we wanted to understand the exact

721
00:27:10,159 --> 00:27:12,159
data it's crashed automatically so we

722
00:27:12,159 --> 00:27:13,279
actually found a pretty cool

723
00:27:13,279 --> 00:27:14,640
implementation

724
00:27:14,640 --> 00:27:15,600
of

725
00:27:15,600 --> 00:27:17,120
zen's

726
00:27:17,120 --> 00:27:20,559
hypervisor within github and the code

727
00:27:20,559 --> 00:27:22,399
actually registered as a bachelor

728
00:27:22,399 --> 00:27:23,679
callback which means that it will be

729
00:27:23,679 --> 00:27:26,559
triggered every time it crash occurs and

730
00:27:26,559 --> 00:27:27,919
once it's being triggered it will

731
00:27:27,919 --> 00:27:30,399
actually collect the stack trace of the

732
00:27:30,399 --> 00:27:33,120
crash and actually replaces each line of

733
00:27:33,120 --> 00:27:36,799
the stack with with the model name

734
00:27:36,799 --> 00:27:38,960
and in order to send the data from

735
00:27:38,960 --> 00:27:40,559
within our driver

736
00:27:40,559 --> 00:27:43,279
to kfl we actually implemented two hyper

737
00:27:43,279 --> 00:27:45,679
calls within kvm which enabled us to

738
00:27:45,679 --> 00:27:46,640
send

739
00:27:46,640 --> 00:27:48,720
the exact size of the crash dump and

740
00:27:48,720 --> 00:27:51,360
then the buffer of the crash dump itself

741
00:27:51,360 --> 00:27:52,720
so this is how it looks like on our

742
00:27:52,720 --> 00:27:54,320
fuzzing server

743
00:27:54,320 --> 00:27:56,080
we asked we wrote a script which

744
00:27:56,080 --> 00:27:58,559
iterates all the crashes tells us

745
00:27:58,559 --> 00:28:01,039
which payload caused the crash and

746
00:28:01,039 --> 00:28:03,279
displays the exact stack trace of each

747
00:28:03,279 --> 00:28:06,320
crash we also replaced the function

748
00:28:06,320 --> 00:28:08,559
address with the function name

749
00:28:08,559 --> 00:28:10,399
so it was pretty easy for us to

750
00:28:10,399 --> 00:28:11,840
understand the exact route because of

751
00:28:11,840 --> 00:28:13,840
each crash pretty easily without

752
00:28:13,840 --> 00:28:17,279
leveraging a vm we just needed to open

753
00:28:17,279 --> 00:28:19,600
vm switch in ida pro and we understood

754
00:28:19,600 --> 00:28:23,678
the root cause within five minutes

755
00:28:24,960 --> 00:28:26,720
so let's talk about the last component

756
00:28:26,720 --> 00:28:28,559
of our fuzzing infrastructure which is

757
00:28:28,559 --> 00:28:31,200
structural awareness

758
00:28:31,200 --> 00:28:32,159
now

759
00:28:32,159 --> 00:28:35,120
we were handling with a well-specified

760
00:28:35,120 --> 00:28:37,360
protocol called arendis

761
00:28:37,360 --> 00:28:39,919
now we knew that it deducted it was

762
00:28:39,919 --> 00:28:42,480
documented well by microsoft and we knew

763
00:28:42,480 --> 00:28:44,320
that there are a lot of structures and a

764
00:28:44,320 --> 00:28:46,159
lot of messages of our nds and we knew

765
00:28:46,159 --> 00:28:48,640
that if we would try to mutate

766
00:28:48,640 --> 00:28:51,200
fuzzing inputs during runtime randomly

767
00:28:51,200 --> 00:28:53,039
we will probably send

768
00:28:53,039 --> 00:28:55,440
dummy packets which won't be parsed

769
00:28:55,440 --> 00:28:57,360
because there are a lot of basic checks

770
00:28:57,360 --> 00:28:59,600
functionality within vm switch so we

771
00:28:59,600 --> 00:29:01,360
knew we have to generate packets

772
00:29:01,360 --> 00:29:04,320
according to the rnd specification

773
00:29:04,320 --> 00:29:06,159
so we actually used google protobuf

774
00:29:06,159 --> 00:29:08,240
protocol buffers in order to represent

775
00:29:08,240 --> 00:29:10,880
the rnds message as instructors

776
00:29:10,880 --> 00:29:12,640
in an easy manner now after we

777
00:29:12,640 --> 00:29:13,919
represented it

778
00:29:13,919 --> 00:29:16,960
by generating a lot of profiles we used

779
00:29:16,960 --> 00:29:19,279
the leap prototype mutator library

780
00:29:19,279 --> 00:29:21,039
which is a great library by google as

781
00:29:21,039 --> 00:29:23,200
well which enables one to use

782
00:29:23,200 --> 00:29:25,279
the profiles he generated

783
00:29:25,279 --> 00:29:27,039
from some kind of a specification of

784
00:29:27,039 --> 00:29:29,840
structures and actually mutate packets

785
00:29:29,840 --> 00:29:32,480
of fuzzing or payloads of fuzzing during

786
00:29:32,480 --> 00:29:34,880
run time according to the specification

787
00:29:34,880 --> 00:29:37,760
we wrote in the protobuf files so that

788
00:29:37,760 --> 00:29:39,679
actually means that we integrated it

789
00:29:39,679 --> 00:29:42,080
into f41 and we were able to generate

790
00:29:42,080 --> 00:29:43,520
rnds packets according to the

791
00:29:43,520 --> 00:29:47,039
specification during runtime

792
00:29:47,360 --> 00:29:49,039
and this was actually the last component

793
00:29:49,039 --> 00:29:51,360
and we've completed our whole end-to-end

794
00:29:51,360 --> 00:29:53,279
father infrastructure we have

795
00:29:53,279 --> 00:29:55,520
our harness we have coverage guidance

796
00:29:55,520 --> 00:29:56,640
crash monitoring and structural

797
00:29:56,640 --> 00:29:57,679
awareness

798
00:29:57,679 --> 00:29:59,679
and that was pretty amazing

799
00:29:59,679 --> 00:30:02,559
in that stage

800
00:30:04,880 --> 00:30:07,360
so what you can see here is actually

801
00:30:07,360 --> 00:30:10,240
the user interface of half full one i

802
00:30:10,240 --> 00:30:12,640
think two hours after it started running

803
00:30:12,640 --> 00:30:15,279
on our dedicated servers i got a phone

804
00:30:15,279 --> 00:30:17,120
call from peleg saying that there was a

805
00:30:17,120 --> 00:30:19,440
crash we started investigating this

806
00:30:19,440 --> 00:30:21,200
crash and we found

807
00:30:21,200 --> 00:30:23,440
an actual bug in vmswitch

808
00:30:23,440 --> 00:30:25,840
we started reporting this to msrc who

809
00:30:25,840 --> 00:30:28,080
assigned it with this cv

810
00:30:28,080 --> 00:30:29,600
so in the time left we'll try to

811
00:30:29,600 --> 00:30:30,960
understand

812
00:30:30,960 --> 00:30:32,399
what the bug is and what are the

813
00:30:32,399 --> 00:30:34,720
consequences of it

814
00:30:34,720 --> 00:30:37,440
the call chain starts with a control

815
00:30:37,440 --> 00:30:38,960
message worker routine which is the

816
00:30:38,960 --> 00:30:42,159
function called once a packet is fetched

817
00:30:42,159 --> 00:30:44,000
from uh the queue that peleg mentioned

818
00:30:44,000 --> 00:30:45,679
before

819
00:30:45,679 --> 00:30:48,960
later on it continues to an r and this

820
00:30:48,960 --> 00:30:50,880
handle set message

821
00:30:50,880 --> 00:30:53,200
function which handles set requests in

822
00:30:53,200 --> 00:30:55,840
our endis

823
00:30:56,000 --> 00:30:58,720
then we have another function if you see

824
00:30:58,720 --> 00:31:00,000
in its name

825
00:31:00,000 --> 00:31:02,080
it handles with ifr which is an

826
00:31:02,080 --> 00:31:05,519
in-flight tracer built on top of wpp

827
00:31:05,519 --> 00:31:07,360
software tracing so we're dealing with

828
00:31:07,360 --> 00:31:08,880
some logic

829
00:31:08,880 --> 00:31:10,640
related to logging

830
00:31:10,640 --> 00:31:12,159
the packet

831
00:31:12,159 --> 00:31:14,559
and eventually the crash actually occurs

832
00:31:14,559 --> 00:31:17,360
in a function that has oid switch nick

833
00:31:17,360 --> 00:31:19,200
request in its name

834
00:31:19,200 --> 00:31:21,760
oid is object identifier and it simply

835
00:31:21,760 --> 00:31:24,159
identifies the type of the set request

836
00:31:24,159 --> 00:31:27,039
or query request that are sent as part

837
00:31:27,039 --> 00:31:30,240
of the r disk communication

838
00:31:30,240 --> 00:31:32,159
the next question that comes to mind is

839
00:31:32,159 --> 00:31:34,880
what does this oid do and what is its

840
00:31:34,880 --> 00:31:36,159
purpose

841
00:31:36,159 --> 00:31:38,480
so as it appears this oid switch nick

842
00:31:38,480 --> 00:31:39,679
request

843
00:31:39,679 --> 00:31:43,440
is supposed to encapsulate and forward

844
00:31:43,440 --> 00:31:46,080
oid requests from a child partition to a

845
00:31:46,080 --> 00:31:48,320
physical network adapter

846
00:31:48,320 --> 00:31:50,559
so actually it's never meant to be sent

847
00:31:50,559 --> 00:31:53,120
from the child partition but rather from

848
00:31:53,120 --> 00:31:56,000
the root partition

849
00:31:56,000 --> 00:31:57,600
so this was actually the first root

850
00:31:57,600 --> 00:31:59,519
cause of the bug vms which never

851
00:31:59,519 --> 00:32:02,240
actually checked that that type of

852
00:32:02,240 --> 00:32:04,159
packet is received

853
00:32:04,159 --> 00:32:05,120
from

854
00:32:05,120 --> 00:32:07,279
from the root partition and not from any

855
00:32:07,279 --> 00:32:09,760
guest machine

856
00:32:09,760 --> 00:32:12,240
now as you can see in the slide as part

857
00:32:12,240 --> 00:32:14,880
of its structure

858
00:32:14,880 --> 00:32:17,200
the packet has a member called p and

859
00:32:17,200 --> 00:32:19,519
this oid request this is a pointer to

860
00:32:19,519 --> 00:32:22,320
the encapsulated oid request this is

861
00:32:22,320 --> 00:32:24,720
where the the original oid from the

862
00:32:24,720 --> 00:32:26,960
child partition

863
00:32:26,960 --> 00:32:28,960
is

864
00:32:28,960 --> 00:32:30,880
so this actually brings me to the second

865
00:32:30,880 --> 00:32:33,200
problem leading to the bug which is this

866
00:32:33,200 --> 00:32:35,600
pointer is never validated

867
00:32:35,600 --> 00:32:37,600
if you're an attacker controlling the

868
00:32:37,600 --> 00:32:40,880
kernel of a child partition you can put

869
00:32:40,880 --> 00:32:42,559
any pointer there

870
00:32:42,559 --> 00:32:44,559
vmswitch will not validate it and this

871
00:32:44,559 --> 00:32:47,760
will lead to a crash

872
00:32:49,440 --> 00:32:50,640
so as you can see here in the

873
00:32:50,640 --> 00:32:52,799
disassembly of vm switch

874
00:32:52,799 --> 00:32:54,640
we have the instructions leading to the

875
00:32:54,640 --> 00:32:56,960
crash we'll actually look at the top

876
00:32:56,960 --> 00:32:59,840
where we fetch the pointer to the oid

877
00:32:59,840 --> 00:33:04,159
request and put it in the register r10

878
00:33:04,159 --> 00:33:06,960
this is quite low level but this what

879
00:33:06,960 --> 00:33:08,960
actually happens there and then later on

880
00:33:08,960 --> 00:33:11,200
we dereference this pointer in register

881
00:33:11,200 --> 00:33:15,120
r10 and then this leads to to the crash

882
00:33:15,120 --> 00:33:16,960
but as you'll see just in a minute there

883
00:33:16,960 --> 00:33:17,919
are

884
00:33:17,919 --> 00:33:20,080
harsher consequences to the bug than

885
00:33:20,080 --> 00:33:23,679
just crashing the host

886
00:33:23,679 --> 00:33:25,519
so let's talk about the consequences of

887
00:33:25,519 --> 00:33:28,000
the vulnerability so obviously we have

888
00:33:28,000 --> 00:33:29,760
an arbitrary read vulnerability which

889
00:33:29,760 --> 00:33:31,679
allows us to read any pointer we would

890
00:33:31,679 --> 00:33:33,679
like and

891
00:33:33,679 --> 00:33:35,120
eventually

892
00:33:35,120 --> 00:33:36,880
if we'll trigger the vulnerability from

893
00:33:36,880 --> 00:33:40,399
within and hyper-v guest we will crash

894
00:33:40,399 --> 00:33:42,320
the hyper-v host and the whole

895
00:33:42,320 --> 00:33:44,000
hypervisor itself

896
00:33:44,000 --> 00:33:46,320
now the first question that came in mind

897
00:33:46,320 --> 00:33:48,559
was we wanted to understand whether it

898
00:33:48,559 --> 00:33:51,120
impacts azure cloud or not and we didn't

899
00:33:51,120 --> 00:33:53,519
want to test it out on production so we

900
00:33:53,519 --> 00:33:56,399
just asked msrc and msrc did confirm

901
00:33:56,399 --> 00:33:59,200
that the vulnerability did impact azure

902
00:33:59,200 --> 00:34:00,960
as well

903
00:34:00,960 --> 00:34:01,840
now

904
00:34:01,840 --> 00:34:02,799
the thing

905
00:34:02,799 --> 00:34:05,600
that is interesting about this bug is

906
00:34:05,600 --> 00:34:07,120
aside from the fact that it's an

907
00:34:07,120 --> 00:34:09,440
arbitrary pointer the reference msrc

908
00:34:09,440 --> 00:34:12,159
actually signed it with a cvss of 9.9

909
00:34:12,159 --> 00:34:14,239
which is pretty high

910
00:34:14,239 --> 00:34:16,079
now they actually described that there

911
00:34:16,079 --> 00:34:18,159
is a particular scenario which involves

912
00:34:18,159 --> 00:34:21,679
of dma direct memory access device

913
00:34:21,679 --> 00:34:23,679
which by triggering this vulnerability

914
00:34:23,679 --> 00:34:26,000
and reading from the dma device you

915
00:34:26,000 --> 00:34:28,320
might get more primitives which might

916
00:34:28,320 --> 00:34:30,960
lead to a remote code execution scenario

917
00:34:30,960 --> 00:34:32,639
and that is pretty interesting now we

918
00:34:32,639 --> 00:34:34,879
didn't have the time to test it out

919
00:34:34,879 --> 00:34:36,719
but we do encourage you to test it out

920
00:34:36,719 --> 00:34:38,639
and if you have any interesting thoughts

921
00:34:38,639 --> 00:34:40,239
or interesting results please don't

922
00:34:40,239 --> 00:34:42,480
hesitate and share it with us on twitter

923
00:34:42,480 --> 00:34:45,839
and we would like to hear about it more

924
00:34:45,839 --> 00:34:49,199
so demo time this is actually on a local

925
00:34:49,199 --> 00:34:51,520
environment we have l0 debugging vm

926
00:34:51,520 --> 00:34:53,119
switch and l2

927
00:34:53,119 --> 00:34:55,440
we were running packet sender which is

928
00:34:55,440 --> 00:34:58,000
our exploit program with

929
00:34:58,000 --> 00:35:00,880
the vulnerable oid packet and the size

930
00:35:00,880 --> 00:35:02,880
and once defender will finish scanning

931
00:35:02,880 --> 00:35:04,480
our executable

932
00:35:04,480 --> 00:35:07,760
we'll see that vm switch crashes on l0

933
00:35:07,760 --> 00:35:09,520
on the debugger

934
00:35:09,520 --> 00:35:11,599
if we analyze the crash

935
00:35:11,599 --> 00:35:13,280
we see the actual pointer that was

936
00:35:13,280 --> 00:35:15,760
dereferenced the invalid pointer you can

937
00:35:15,760 --> 00:35:17,200
see many

938
00:35:17,200 --> 00:35:21,200
four ones in just a second there it is

939
00:35:21,200 --> 00:35:24,320
and later on it says where the module uh

940
00:35:24,320 --> 00:35:25,920
what is the module that crashed and

941
00:35:25,920 --> 00:35:27,760
where and we're already familiar with

942
00:35:27,760 --> 00:35:29,119
this function

943
00:35:29,119 --> 00:35:31,040
so this is a local environment but as

944
00:35:31,040 --> 00:35:34,079
pelic mentioned it could impact azure

945
00:35:34,079 --> 00:35:36,480
quite harshly

946
00:35:36,480 --> 00:35:38,240
so we've actually published our whole

947
00:35:38,240 --> 00:35:40,480
project to github we've published the

948
00:35:40,480 --> 00:35:41,520
first

949
00:35:41,520 --> 00:35:43,760
part with enable which enabled us to

950
00:35:43,760 --> 00:35:45,599
send data from the child partition to

951
00:35:45,599 --> 00:35:47,280
the root partition and we've also

952
00:35:47,280 --> 00:35:50,079
published half a one which

953
00:35:50,079 --> 00:35:51,920
contains the harness path and also

954
00:35:51,920 --> 00:35:55,200
modification of the kfl file itself and

955
00:35:55,200 --> 00:35:56,800
we encourage you to test it out and

956
00:35:56,800 --> 00:35:59,200
improve it and hopefully find more bugs

957
00:35:59,200 --> 00:36:01,200
with it and if you have any suggestions

958
00:36:01,200 --> 00:36:04,079
or thoughts please open a github issue

959
00:36:04,079 --> 00:36:07,040
or send us a dm on twitter and we'll try

960
00:36:07,040 --> 00:36:09,119
to respond to it

961
00:36:09,119 --> 00:36:10,880
so thank you once again for joining if

962
00:36:10,880 --> 00:36:12,800
you have any questions don't hesitate to

963
00:36:12,800 --> 00:36:14,079
reach out to us

964
00:36:14,079 --> 00:36:18,119
online thank you

