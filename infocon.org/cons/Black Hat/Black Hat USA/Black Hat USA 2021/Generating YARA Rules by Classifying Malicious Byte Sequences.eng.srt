1
00:00:01,130 --> 00:00:14,690
[Music]

2
00:00:16,320 --> 00:00:17,359
all right thanks for coming out

3
00:00:17,359 --> 00:00:18,640
everybody

4
00:00:18,640 --> 00:00:20,320
really excited to present some work i've

5
00:00:20,320 --> 00:00:22,080
been working on over the last few months

6
00:00:22,080 --> 00:00:23,359
that has to do with training

7
00:00:23,359 --> 00:00:24,960
interpretable machine learning models

8
00:00:24,960 --> 00:00:27,119
and coming up with ways to turn those

9
00:00:27,119 --> 00:00:31,439
machine learning models into yara rules

10
00:00:31,439 --> 00:00:32,719
it's really quick just to introduce

11
00:00:32,719 --> 00:00:34,480
myself my name is andrew davis i'm a

12
00:00:34,480 --> 00:00:36,239
principal data scientist at elastic i've

13
00:00:36,239 --> 00:00:38,320
been there for about nine months now

14
00:00:38,320 --> 00:00:39,520
and generally speaking i've been

15
00:00:39,520 --> 00:00:41,360
teaching computers how to detect malware

16
00:00:41,360 --> 00:00:43,040
since about 2014. so i've trained a

17
00:00:43,040 --> 00:00:44,879
variety of malware classifiers such as

18
00:00:44,879 --> 00:00:47,280
malware classifiers to target windows

19
00:00:47,280 --> 00:00:48,719
pes

20
00:00:48,719 --> 00:00:52,000
osx macos linux elfs uptrend models that

21
00:00:52,000 --> 00:00:54,160
detect things detect malware inside of

22
00:00:54,160 --> 00:00:57,199
pdfs inside of rtfs inside of docs and

23
00:00:57,199 --> 00:00:59,280
things like that so training machine

24
00:00:59,280 --> 00:01:01,280
learning models on mower is kind of my

25
00:01:01,280 --> 00:01:03,600
bread and butter

26
00:01:03,600 --> 00:01:06,560
so really quick intro and motivation

27
00:01:06,560 --> 00:01:07,360
um

28
00:01:07,360 --> 00:01:09,600
first of all yar is a really great line

29
00:01:09,600 --> 00:01:12,159
first defense against malware if you can

30
00:01:12,159 --> 00:01:14,000
extract a sequence of bytes from a piece

31
00:01:14,000 --> 00:01:16,080
of malware and if it can slam dunk 100

32
00:01:16,080 --> 00:01:17,920
of the time detect that piece of malware

33
00:01:17,920 --> 00:01:19,280
it's really quick to sort of roll all

34
00:01:19,280 --> 00:01:21,360
these ur rules to be able to detect new

35
00:01:21,360 --> 00:01:23,520
particular kinds of variants

36
00:01:23,520 --> 00:01:25,600
deep learning on the other hand has been

37
00:01:25,600 --> 00:01:27,360
used as a very effective way of

38
00:01:27,360 --> 00:01:29,439
detecting malware but unfortunately its

39
00:01:29,439 --> 00:01:31,040
decisions are usually incomprehensible

40
00:01:31,040 --> 00:01:32,400
in the way that the models are usually

41
00:01:32,400 --> 00:01:33,280
sell

42
00:01:33,280 --> 00:01:34,960
so the typical deep learning model is

43
00:01:34,960 --> 00:01:37,439
set up in such a way that you have an

44
00:01:37,439 --> 00:01:39,840
input of a malware sample coming in and

45
00:01:39,840 --> 00:01:41,119
then you have a score for the malware

46
00:01:41,119 --> 00:01:42,640
sample at the end but it's really hard

47
00:01:42,640 --> 00:01:44,079
to trace back and figure out what the

48
00:01:44,079 --> 00:01:45,920
model was thinking to come up with a

49
00:01:45,920 --> 00:01:47,520
decision to call something malicious or

50
00:01:47,520 --> 00:01:49,439
benign

51
00:01:49,439 --> 00:01:51,040
on the other hand deep learning models

52
00:01:51,040 --> 00:01:53,600
are very very very flexible in the sense

53
00:01:53,600 --> 00:01:55,119
that you can

54
00:01:55,119 --> 00:01:56,719
set up a deep learning model to do

55
00:01:56,719 --> 00:01:58,560
something really simple like you know

56
00:01:58,560 --> 00:02:00,000
just a fully connected deep neural

57
00:02:00,000 --> 00:02:01,600
network or you can do something really

58
00:02:01,600 --> 00:02:04,159
complicated like a transformer model or

59
00:02:04,159 --> 00:02:05,680
an attention model

60
00:02:05,680 --> 00:02:07,600
and in this way you can come up with

61
00:02:07,600 --> 00:02:10,318
ways to make the model interpretable

62
00:02:10,318 --> 00:02:12,720
from the get-go in other words structure

63
00:02:12,720 --> 00:02:14,959
your model in such a way that when you

64
00:02:14,959 --> 00:02:16,000
look at

65
00:02:16,000 --> 00:02:18,160
the model's decision-making you can come

66
00:02:18,160 --> 00:02:20,160
up with ways to figure out why the model

67
00:02:20,160 --> 00:02:21,599
made that decision

68
00:02:21,599 --> 00:02:22,959
so what we're going to do in this talk

69
00:02:22,959 --> 00:02:24,640
is i'm going to show you a way to set up

70
00:02:24,640 --> 00:02:25,599
a deep learning model that's

71
00:02:25,599 --> 00:02:27,680
interpretable from the get-go and you

72
00:02:27,680 --> 00:02:30,080
can take that interpretability and turn

73
00:02:30,080 --> 00:02:33,360
that into your rules

74
00:02:33,360 --> 00:02:34,800
so there's a lot of related work on the

75
00:02:34,800 --> 00:02:36,640
subject

76
00:02:36,640 --> 00:02:38,720
there's one particular paper by a

77
00:02:38,720 --> 00:02:40,480
giraffe and a bunch of other authors

78
00:02:40,480 --> 00:02:42,080
called automatic ur role generation

79
00:02:42,080 --> 00:02:43,760
using by clustering

80
00:02:43,760 --> 00:02:45,440
and in this talk they

81
00:02:45,440 --> 00:02:47,519
work on this previous idea of kilo

82
00:02:47,519 --> 00:02:49,280
engrams where they take these long byte

83
00:02:49,280 --> 00:02:50,560
sequences

84
00:02:50,560 --> 00:02:53,040
and they take this idea and come up with

85
00:02:53,040 --> 00:02:55,120
a way to cluster them together and come

86
00:02:55,120 --> 00:02:57,519
up with complex ands and ors and other

87
00:02:57,519 --> 00:02:59,280
things of strings and be able to make

88
00:02:59,280 --> 00:03:00,879
workable yar rules based on small

89
00:03:00,879 --> 00:03:02,080
purposes

90
00:03:02,080 --> 00:03:03,920
so you can feed in a single sample or a

91
00:03:03,920 --> 00:03:05,760
number of samples and it'll generate you

92
00:03:05,760 --> 00:03:07,680
a rule that has a high true positive

93
00:03:07,680 --> 00:03:09,120
rate and a low false positive rate for

94
00:03:09,120 --> 00:03:10,959
the samples you're sending in

95
00:03:10,959 --> 00:03:12,239
there's another

96
00:03:12,239 --> 00:03:14,720
similar one called yayajan what it does

97
00:03:14,720 --> 00:03:16,640
is it looks at

98
00:03:16,640 --> 00:03:19,680
kooky logs for android apks so you send

99
00:03:19,680 --> 00:03:21,599
in an android apk to this particular

100
00:03:21,599 --> 00:03:24,239
android sandbox you detonate the sample

101
00:03:24,239 --> 00:03:25,920
and then this thing is able to look at

102
00:03:25,920 --> 00:03:27,519
the detonation report and come up with

103
00:03:27,519 --> 00:03:29,760
yara rules to detect new variants of

104
00:03:29,760 --> 00:03:31,599
malware based on the

105
00:03:31,599 --> 00:03:34,560
ur rules that it outputs from the um

106
00:03:34,560 --> 00:03:37,680
from the sandbox detonation

107
00:03:37,680 --> 00:03:38,959
and finally there's a really cool

108
00:03:38,959 --> 00:03:41,920
project by joshua sachs called erml the

109
00:03:41,920 --> 00:03:44,640
way it works is you

110
00:03:44,640 --> 00:03:47,040
take a logistic regression or random

111
00:03:47,040 --> 00:03:49,519
forest model and then use some clever

112
00:03:49,519 --> 00:03:51,599
sort of hacks to figure out how to turn

113
00:03:51,599 --> 00:03:53,439
that logistic regression model into the

114
00:03:53,439 --> 00:03:55,760
r rule or change that random forest

115
00:03:55,760 --> 00:03:59,959
model into the url rule

116
00:04:00,400 --> 00:04:02,879
so how do we make an interpretable model

117
00:04:02,879 --> 00:04:04,560
so as i was saying before deep learning

118
00:04:04,560 --> 00:04:06,239
models look at sort of the whole of the

119
00:04:06,239 --> 00:04:08,480
sample to get a score so you feed in

120
00:04:08,480 --> 00:04:10,720
some bytes you feed in an image whatever

121
00:04:10,720 --> 00:04:11,760
you're doing with the deep learning

122
00:04:11,760 --> 00:04:13,599
model and then at the output you get a

123
00:04:13,599 --> 00:04:15,680
funnel score like this is malicious or

124
00:04:15,680 --> 00:04:17,440
this is a bird this is a plane whatever

125
00:04:17,440 --> 00:04:19,839
the model is trying to classify

126
00:04:19,839 --> 00:04:21,680
so in that typical way of setting things

127
00:04:21,680 --> 00:04:22,479
up you don't have a lot of

128
00:04:22,479 --> 00:04:24,800
interpretability there are sort of bolts

129
00:04:24,800 --> 00:04:27,040
on methods that you can use to try and

130
00:04:27,040 --> 00:04:28,800
go back and get interpretability like

131
00:04:28,800 --> 00:04:30,880
one really good one is called lime where

132
00:04:30,880 --> 00:04:32,800
you perturb the input pixels and see if

133
00:04:32,800 --> 00:04:35,120
perturbations change the output much and

134
00:04:35,120 --> 00:04:37,280
then you say the things that

135
00:04:37,280 --> 00:04:39,120
modified the input much were important

136
00:04:39,120 --> 00:04:40,479
so you say that these regions were

137
00:04:40,479 --> 00:04:42,400
important for the classification

138
00:04:42,400 --> 00:04:44,160
but generally speaking in my experience

139
00:04:44,160 --> 00:04:45,600
getting interpretability for deep

140
00:04:45,600 --> 00:04:47,840
learning models for malware recognition

141
00:04:47,840 --> 00:04:49,600
it's sort of a crapshoot as to whether

142
00:04:49,600 --> 00:04:50,800
or not you're going to get something

143
00:04:50,800 --> 00:04:52,560
useful from the interpretability from

144
00:04:52,560 --> 00:04:54,240
something like lime

145
00:04:54,240 --> 00:04:56,400
so in this work what we do is we set up

146
00:04:56,400 --> 00:04:58,800
the model in such a way that when we

147
00:04:58,800 --> 00:05:01,280
feed in any contiguous series of bytes

148
00:05:01,280 --> 00:05:03,759
we can get an upward score so we can

149
00:05:03,759 --> 00:05:06,720
feed in a sequence like you have been

150
00:05:06,720 --> 00:05:08,560
pwned and we should get something really

151
00:05:08,560 --> 00:05:11,680
high like 0.99 or if we feed in

152
00:05:11,680 --> 00:05:13,840
something like please send five bitcoins

153
00:05:13,840 --> 00:05:15,840
to this bitcoin wallet then we get a

154
00:05:15,840 --> 00:05:17,120
high score

155
00:05:17,120 --> 00:05:19,440
whereas if we feed in a different benign

156
00:05:19,440 --> 00:05:21,840
string or if we feed an advanced string

157
00:05:21,840 --> 00:05:23,520
like this program cannot be run in dos

158
00:05:23,520 --> 00:05:25,360
mode we should get a really low score

159
00:05:25,360 --> 00:05:26,400
from the model

160
00:05:26,400 --> 00:05:28,639
so we structure in such a way that we

161
00:05:28,639 --> 00:05:30,800
feed in the entire bytes of the sample

162
00:05:30,800 --> 00:05:32,960
and then for each byte we feed in that

163
00:05:32,960 --> 00:05:34,639
represents some range of bytes we should

164
00:05:34,639 --> 00:05:36,560
be able to get a score back out

165
00:05:36,560 --> 00:05:38,160
in this way the model is interpretable

166
00:05:38,160 --> 00:05:40,639
in the sense that we can look at the

167
00:05:40,639 --> 00:05:42,639
upward score and if the output score is

168
00:05:42,639 --> 00:05:44,400
high then we have a direct way to look

169
00:05:44,400 --> 00:05:46,560
back through the model to see which byte

170
00:05:46,560 --> 00:05:48,800
sequences the model found benign or

171
00:05:48,800 --> 00:05:50,000
malicious

172
00:05:50,000 --> 00:05:52,400
and then what we can do is we can look

173
00:05:52,400 --> 00:05:53,919
at the particular offsets for byte

174
00:05:53,919 --> 00:05:56,240
sequences model thought was bad and then

175
00:05:56,240 --> 00:05:58,080
rip those out and create url rules based

176
00:05:58,080 --> 00:06:00,560
on these

177
00:06:00,800 --> 00:06:02,240
so before i go much further into the

178
00:06:02,240 --> 00:06:03,680
model architecture i'd like to do a

179
00:06:03,680 --> 00:06:05,199
quick primer on convolutional neural

180
00:06:05,199 --> 00:06:07,440
networks to get a better idea of how

181
00:06:07,440 --> 00:06:08,720
this model works

182
00:06:08,720 --> 00:06:10,080
it's not going to be very in-depth it's

183
00:06:10,080 --> 00:06:12,080
going to be very simple

184
00:06:12,080 --> 00:06:14,080
everything that's less important has

185
00:06:14,080 --> 00:06:16,000
been kind of abstracted away but

186
00:06:16,000 --> 00:06:18,160
basically what convolution is doing is

187
00:06:18,160 --> 00:06:19,919
it's going to take some function in this

188
00:06:19,919 --> 00:06:21,280
case the function is represented by

189
00:06:21,280 --> 00:06:23,520
these three squiggly lines here and it's

190
00:06:23,520 --> 00:06:25,360
going to take three bytes on the input

191
00:06:25,360 --> 00:06:27,039
and then it's going to apply a function

192
00:06:27,039 --> 00:06:28,560
to those bytes it doesn't really matter

193
00:06:28,560 --> 00:06:30,319
what the function is so much in the case

194
00:06:30,319 --> 00:06:31,520
of deep neural networks it's just

195
00:06:31,520 --> 00:06:33,759
multiplications and additions but that's

196
00:06:33,759 --> 00:06:35,520
not super important to know

197
00:06:35,520 --> 00:06:37,199
it's going to take those three bytes and

198
00:06:37,199 --> 00:06:38,880
it's going to do something to them then

199
00:06:38,880 --> 00:06:40,560
it's going to output a score or some

200
00:06:40,560 --> 00:06:42,080
representation

201
00:06:42,080 --> 00:06:44,160
basically what we can say is the output

202
00:06:44,160 --> 00:06:46,240
of that function is going to be directly

203
00:06:46,240 --> 00:06:47,759
representative of those three bytes that

204
00:06:47,759 --> 00:06:49,039
we see

205
00:06:49,039 --> 00:06:51,199
so we can take this function and we just

206
00:06:51,199 --> 00:06:53,280
kind of take it and we slide it down all

207
00:06:53,280 --> 00:06:56,160
the bytes that we have on a sample so

208
00:06:56,160 --> 00:06:57,919
this time we're outputting a score or a

209
00:06:57,919 --> 00:06:59,360
representation of bytes one through

210
00:06:59,360 --> 00:07:01,120
three and this time we're outputting a

211
00:07:01,120 --> 00:07:03,440
representation of bytes two through four

212
00:07:03,440 --> 00:07:05,759
so basically you take a function you

213
00:07:05,759 --> 00:07:07,759
apply the function each time identically

214
00:07:07,759 --> 00:07:08,880
to each

215
00:07:08,880 --> 00:07:10,639
set of bytes that you see and then on

216
00:07:10,639 --> 00:07:12,560
the output you have outputs that

217
00:07:12,560 --> 00:07:14,800
correspond to certain ranges of the

218
00:07:14,800 --> 00:07:17,800
input

219
00:07:18,800 --> 00:07:20,479
so further we can sort of stack these

220
00:07:20,479 --> 00:07:22,639
convolutions together in a way that

221
00:07:22,639 --> 00:07:24,240
instead of having an output score we

222
00:07:24,240 --> 00:07:26,160
have an output representation then we

223
00:07:26,160 --> 00:07:27,440
just do a whole bunch of stacks of

224
00:07:27,440 --> 00:07:31,039
convolutions until we get to some output

225
00:07:31,039 --> 00:07:33,039
in this case each time we do another

226
00:07:33,039 --> 00:07:34,960
stack we basically get a larger

227
00:07:34,960 --> 00:07:37,360
receptive field in other words the

228
00:07:37,360 --> 00:07:38,720
output byte

229
00:07:38,720 --> 00:07:40,800
representation that we see is going to

230
00:07:40,800 --> 00:07:42,479
go back further and represent a wider

231
00:07:42,479 --> 00:07:45,120
series of bytes so if we want longer ur

232
00:07:45,120 --> 00:07:47,280
rules then we can add more steps in the

233
00:07:47,280 --> 00:07:48,800
convolution or add more layers of

234
00:07:48,800 --> 00:07:50,560
convolution

235
00:07:50,560 --> 00:07:52,240
so we can see that on the output we're

236
00:07:52,240 --> 00:07:54,240
representing bytes 0 through 6 and we do

237
00:07:54,240 --> 00:07:56,160
the step of the convolution and then the

238
00:07:56,160 --> 00:07:57,759
second output there is going to

239
00:07:57,759 --> 00:07:59,680
represent bytes one through seven

240
00:07:59,680 --> 00:08:01,520
so we can see that the interpretability

241
00:08:01,520 --> 00:08:03,919
here is basically the ability to look at

242
00:08:03,919 --> 00:08:06,000
an output of the model and be able to

243
00:08:06,000 --> 00:08:07,840
tie it directly back to the sequence of

244
00:08:07,840 --> 00:08:12,479
bytes that affected that score

245
00:08:12,479 --> 00:08:14,160
so how the model works the slide is

246
00:08:14,160 --> 00:08:15,360
meant a little bit for the machine

247
00:08:15,360 --> 00:08:17,120
learning people who

248
00:08:17,120 --> 00:08:18,800
want to understand a little bit better

249
00:08:18,800 --> 00:08:20,080
what exactly the architecture of the

250
00:08:20,080 --> 00:08:21,280
model looks like

251
00:08:21,280 --> 00:08:22,639
so we're going to take a whole bunch of

252
00:08:22,639 --> 00:08:24,400
bytes from a binary

253
00:08:24,400 --> 00:08:25,919
in the context of the talk i'll show you

254
00:08:25,919 --> 00:08:27,680
some results later on elf and maco and

255
00:08:27,680 --> 00:08:30,160
pe but generally speaking we can feed

256
00:08:30,160 --> 00:08:32,399
whatever bytes we want into this model

257
00:08:32,399 --> 00:08:34,159
so we'll feed in some bytes and then

258
00:08:34,159 --> 00:08:35,599
each byte is going to be sent through an

259
00:08:35,599 --> 00:08:37,760
embedding layer for the folks who aren't

260
00:08:37,760 --> 00:08:39,440
super familiar with what an embedding

261
00:08:39,440 --> 00:08:40,640
layer might be

262
00:08:40,640 --> 00:08:42,399
it's more or less a way to take

263
00:08:42,399 --> 00:08:44,720
something from a byte into a whole bunch

264
00:08:44,720 --> 00:08:46,000
of floating point numbers that the

265
00:08:46,000 --> 00:08:47,760
neural network can then understand and

266
00:08:47,760 --> 00:08:49,760
work with so we're going to take each

267
00:08:49,760 --> 00:08:51,279
byte put it through an embedding layer

268
00:08:51,279 --> 00:08:52,480
and then we put it through a whole bunch

269
00:08:52,480 --> 00:08:54,800
of convolution layers and each

270
00:08:54,800 --> 00:08:56,240
convolution in this case is going to be

271
00:08:56,240 --> 00:08:58,160
followed up with a rectified linear

272
00:08:58,160 --> 00:08:59,760
layer i think in the most recent set of

273
00:08:59,760 --> 00:09:01,760
experiments i was actually using a

274
00:09:01,760 --> 00:09:04,480
p-relu layer for the ml folks

275
00:09:04,480 --> 00:09:06,000
and we do a whole bunch of these layers

276
00:09:06,000 --> 00:09:07,600
and then at the end we have a sigmoid

277
00:09:07,600 --> 00:09:08,959
layer

278
00:09:08,959 --> 00:09:10,880
what the sigmoid layer does is it sort

279
00:09:10,880 --> 00:09:13,519
of crushes the values model's able to

280
00:09:13,519 --> 00:09:15,680
output between zero and one so this

281
00:09:15,680 --> 00:09:17,600
makes it so that if the score is zero

282
00:09:17,600 --> 00:09:20,000
then we interpret it as benign and if

283
00:09:20,000 --> 00:09:21,680
the score is close to one we interpret

284
00:09:21,680 --> 00:09:23,519
it as malicious

285
00:09:23,519 --> 00:09:25,360
so one thing to point out is that there

286
00:09:25,360 --> 00:09:26,880
are no max pooling layers in this

287
00:09:26,880 --> 00:09:28,800
architecture and this is because if we

288
00:09:28,800 --> 00:09:30,399
were to include things like max pulling

289
00:09:30,399 --> 00:09:32,720
layers we would start to

290
00:09:32,720 --> 00:09:34,560
drastically increase the size of our

291
00:09:34,560 --> 00:09:36,720
receptive field of the input bytes so

292
00:09:36,720 --> 00:09:38,480
max pooling layer is basically going to

293
00:09:38,480 --> 00:09:40,560
down sample stuff by a factor of two or

294
00:09:40,560 --> 00:09:42,640
three or four or whatever you choose so

295
00:09:42,640 --> 00:09:44,240
if we started putting max pooling layers

296
00:09:44,240 --> 00:09:46,320
in here instead of creating signatures

297
00:09:46,320 --> 00:09:49,760
based on 16 or 32 contiguous bytes we

298
00:09:49,760 --> 00:09:51,440
would be creating signatures on things

299
00:09:51,440 --> 00:09:54,000
that are 256 bytes 512 bytes long and

300
00:09:54,000 --> 00:09:55,279
that wouldn't be as useful in the

301
00:09:55,279 --> 00:09:57,120
signature it would be a much less

302
00:09:57,120 --> 00:09:59,620
general signature

303
00:09:59,620 --> 00:10:02,809
[Music]

304
00:10:03,120 --> 00:10:04,959
so basically the more convolutional

305
00:10:04,959 --> 00:10:06,399
layers we apply

306
00:10:06,399 --> 00:10:08,160
the deeper this thing goes the larger

307
00:10:08,160 --> 00:10:09,839
the receptive field that we get and

308
00:10:09,839 --> 00:10:11,760
practically speaking i think i used five

309
00:10:11,760 --> 00:10:13,839
or six layers to give us something like

310
00:10:13,839 --> 00:10:17,600
24 or 30 long url signatures

311
00:10:17,600 --> 00:10:20,000
so basically this architecture is going

312
00:10:20,000 --> 00:10:21,440
to work like you feed in a thousand

313
00:10:21,440 --> 00:10:23,279
bytes and you're going to feed in about

314
00:10:23,279 --> 00:10:24,800
a or you're going to receive about a

315
00:10:24,800 --> 00:10:27,680
thousand scores back

316
00:10:28,240 --> 00:10:29,600
so in terms of training model there's

317
00:10:29,600 --> 00:10:32,560
some practical things to talk about here

318
00:10:32,560 --> 00:10:34,320
so for model training

319
00:10:34,320 --> 00:10:35,760
i sort of refer to this as finding

320
00:10:35,760 --> 00:10:37,440
needles in a haystack we're going to

321
00:10:37,440 --> 00:10:39,040
have these malicious strings which are

322
00:10:39,040 --> 00:10:41,040
strings provide sequences that occur

323
00:10:41,040 --> 00:10:43,360
only in malicious samples and then we

324
00:10:43,360 --> 00:10:44,959
have benign strings which are strings

325
00:10:44,959 --> 00:10:46,560
that are seen either in malicious or

326
00:10:46,560 --> 00:10:49,120
benign samples so we want to train the

327
00:10:49,120 --> 00:10:51,680
model in such a way that if we see a

328
00:10:51,680 --> 00:10:53,920
zero that really says nothing

329
00:10:53,920 --> 00:10:55,760
informative about the string or the byte

330
00:10:55,760 --> 00:10:57,040
sequence it just means that there's

331
00:10:57,040 --> 00:10:59,279
nothing specifically malicious about it

332
00:10:59,279 --> 00:11:01,519
whereas if we get an output of one we

333
00:11:01,519 --> 00:11:04,560
want that to mean this is a rare and

334
00:11:04,560 --> 00:11:07,200
informative string that says that this

335
00:11:07,200 --> 00:11:09,360
thing is indeed malicious

336
00:11:09,360 --> 00:11:12,000
so this means that the model is going to

337
00:11:12,000 --> 00:11:14,399
have to output very sparse outputs it's

338
00:11:14,399 --> 00:11:16,959
going to have to be zeros for most of

339
00:11:16,959 --> 00:11:19,120
the output bytes that it sees except for

340
00:11:19,120 --> 00:11:20,959
the occasional very malicious thing like

341
00:11:20,959 --> 00:11:23,120
a particular op code sequence associated

342
00:11:23,120 --> 00:11:25,040
with i don't know a particular kind of

343
00:11:25,040 --> 00:11:27,680
crypter or something like that

344
00:11:27,680 --> 00:11:28,399
and

345
00:11:28,399 --> 00:11:29,680
yeah we basically want the model to

346
00:11:29,680 --> 00:11:31,600
output zeros for almost everything

347
00:11:31,600 --> 00:11:33,040
except for strings associated with

348
00:11:33,040 --> 00:11:35,839
maliciousness

349
00:11:36,399 --> 00:11:38,560
so the way we do this is we use a thing

350
00:11:38,560 --> 00:11:41,200
called top case selection so

351
00:11:41,200 --> 00:11:42,480
really quick when you're talking about

352
00:11:42,480 --> 00:11:44,160
back propagating or training neural

353
00:11:44,160 --> 00:11:45,440
networks

354
00:11:45,440 --> 00:11:47,519
you're going to look at an output and

355
00:11:47,519 --> 00:11:49,120
then you're going to feed the error of

356
00:11:49,120 --> 00:11:51,040
the output back through the model to be

357
00:11:51,040 --> 00:11:53,120
able to get away to update your model in

358
00:11:53,120 --> 00:11:54,839
such a way that it better classifies

359
00:11:54,839 --> 00:11:56,959
something in this case we're going to

360
00:11:56,959 --> 00:11:57,839
have

361
00:11:57,839 --> 00:11:59,440
sort of this needle in a haystack thing

362
00:11:59,440 --> 00:12:01,279
so it doesn't really make sense to

363
00:12:01,279 --> 00:12:03,519
update every single output in the model

364
00:12:03,519 --> 00:12:05,279
so we do this top case selection which

365
00:12:05,279 --> 00:12:07,279
basically means look at all the scores

366
00:12:07,279 --> 00:12:09,680
we see on the output and sort them based

367
00:12:09,680 --> 00:12:11,600
on their magnitude so you know we'll

368
00:12:11,600 --> 00:12:14,480
sort it descending from one to zero

369
00:12:14,480 --> 00:12:16,000
then we're going to select the top k

370
00:12:16,000 --> 00:12:17,680
valued scores to back propagate through

371
00:12:17,680 --> 00:12:19,920
or to update with respect to

372
00:12:19,920 --> 00:12:22,560
what this does is it heavily promotes

373
00:12:22,560 --> 00:12:24,160
malicious stuff while very quickly

374
00:12:24,160 --> 00:12:26,160
squashing down benign stuff

375
00:12:26,160 --> 00:12:28,240
and practically speaking for top k i

376
00:12:28,240 --> 00:12:31,040
would select let's say the top 10

377
00:12:31,040 --> 00:12:33,440
strongest outputs from the model

378
00:12:33,440 --> 00:12:34,880
this also has the side benefit of

379
00:12:34,880 --> 00:12:37,120
specifying the outputs so we get exactly

380
00:12:37,120 --> 00:12:38,639
what we want we get a model that's

381
00:12:38,639 --> 00:12:40,880
looking for needles in a haystack that's

382
00:12:40,880 --> 00:12:42,560
quickly able to pick out malicious

383
00:12:42,560 --> 00:12:46,479
strings and ignore uninformative ones

384
00:12:49,680 --> 00:12:52,079
and again this has the side effect of

385
00:12:52,079 --> 00:12:54,320
very quickly pushing

386
00:12:54,320 --> 00:12:57,120
benign or uninformed strings down to

387
00:12:57,120 --> 00:13:00,079
zero because we are selecting the most

388
00:13:00,079 --> 00:13:01,680
malicious things that the model sees for

389
00:13:01,680 --> 00:13:03,120
that point in model training and we're

390
00:13:03,120 --> 00:13:05,200
actively squashing those things down

391
00:13:05,200 --> 00:13:08,399
and we're ignoring everything else

392
00:13:08,959 --> 00:13:10,720
another practical consideration to talk

393
00:13:10,720 --> 00:13:13,040
about is how do we fit this onto a gpu

394
00:13:13,040 --> 00:13:14,639
so the

395
00:13:14,639 --> 00:13:16,399
executables that i was training on could

396
00:13:16,399 --> 00:13:18,000
range in size anywhere from a few

397
00:13:18,000 --> 00:13:19,839
kilobytes up to tens of megabytes if

398
00:13:19,839 --> 00:13:21,680
they were an installer or something like

399
00:13:21,680 --> 00:13:22,560
that

400
00:13:22,560 --> 00:13:25,360
and gpus only have so much memory

401
00:13:25,360 --> 00:13:27,519
and another thing to point out is that

402
00:13:27,519 --> 00:13:28,880
when we feed these bytes through the

403
00:13:28,880 --> 00:13:30,800
embedding layer and when we feed these

404
00:13:30,800 --> 00:13:32,240
representations of bytes through all

405
00:13:32,240 --> 00:13:36,000
these deep convolutional activations

406
00:13:36,000 --> 00:13:37,440
we're going to increase the amount of

407
00:13:37,440 --> 00:13:39,440
memory that we need by a factor of

408
00:13:39,440 --> 00:13:41,680
thousands so this one megabyte sample is

409
00:13:41,680 --> 00:13:42,959
all of a sudden gonna need something

410
00:13:42,959 --> 00:13:43,680
like

411
00:13:43,680 --> 00:13:45,600
eight gigabytes of gpu memory to be able

412
00:13:45,600 --> 00:13:47,040
to process

413
00:13:47,040 --> 00:13:49,040
so to deal with this what i did was i

414
00:13:49,040 --> 00:13:51,120
took each sample and i broke it up into

415
00:13:51,120 --> 00:13:54,320
64 kilobyte chunks so i take a sample

416
00:13:54,320 --> 00:13:55,760
break it into a whole bunch of 64

417
00:13:55,760 --> 00:13:57,120
kilobyte chunks and then i feed all

418
00:13:57,120 --> 00:13:58,560
those chunks through the model step by

419
00:13:58,560 --> 00:14:00,320
step so i don't have to use a whole lot

420
00:14:00,320 --> 00:14:02,480
of memory and then i keep track of the

421
00:14:02,480 --> 00:14:05,760
score that's associated with the most

422
00:14:05,760 --> 00:14:08,320
rather i keep track of the bytes

423
00:14:08,320 --> 00:14:11,120
of the score that was most positive so i

424
00:14:11,120 --> 00:14:14,639
might feed in let's say 128 chunks and

425
00:14:14,639 --> 00:14:15,760
then i'll keep track of the chunk that

426
00:14:15,760 --> 00:14:17,760
had the highest score and only update

427
00:14:17,760 --> 00:14:19,839
the model with respect to that chunk

428
00:14:19,839 --> 00:14:21,199
and this is really nice because we're

429
00:14:21,199 --> 00:14:23,360
doing this top k selection thing

430
00:14:23,360 --> 00:14:25,600
because the model is only going to care

431
00:14:25,600 --> 00:14:27,920
about the top valued things anyways so

432
00:14:27,920 --> 00:14:29,360
we can get away with this clever speed

433
00:14:29,360 --> 00:14:31,440
hack by just updating with respect to

434
00:14:31,440 --> 00:14:33,600
the most malicious looking chunk that we

435
00:14:33,600 --> 00:14:34,399
saw

436
00:14:34,399 --> 00:14:36,720
and this has a huge decrease on the

437
00:14:36,720 --> 00:14:38,639
required gpu memory to train a workable

438
00:14:38,639 --> 00:14:40,880
model

439
00:14:43,680 --> 00:14:46,399
so another issue that i ran into was

440
00:14:46,399 --> 00:14:48,800
reducing false positives so what i found

441
00:14:48,800 --> 00:14:50,240
was that the neural network that i was

442
00:14:50,240 --> 00:14:52,000
training was trying very very hard to

443
00:14:52,000 --> 00:14:53,680
find these shortcuts to kind of solve

444
00:14:53,680 --> 00:14:55,680
the problem in unexpected ways

445
00:14:55,680 --> 00:14:57,199
and it would do this by doing things

446
00:14:57,199 --> 00:14:59,519
like i don't know on the off corpus i

447
00:14:59,519 --> 00:15:01,760
found a whole bunch of samples that the

448
00:15:01,760 --> 00:15:04,079
model was consistently calling bad based

449
00:15:04,079 --> 00:15:06,639
on some specific gcc compiler strings

450
00:15:06,639 --> 00:15:07,440
and

451
00:15:07,440 --> 00:15:09,839
lip c versions that just happened to be

452
00:15:09,839 --> 00:15:11,600
near the end of the sample

453
00:15:11,600 --> 00:15:14,560
so this was a case where 99 of the time

454
00:15:14,560 --> 00:15:16,079
where the model would see these strings

455
00:15:16,079 --> 00:15:18,000
would be correct because they were just

456
00:15:18,000 --> 00:15:20,959
uploaded and masked to vt

457
00:15:20,959 --> 00:15:22,720
and they were fairly infrequent in the

458
00:15:22,720 --> 00:15:25,199
benign set so 99 of the time you see

459
00:15:25,199 --> 00:15:26,639
it's malicious one percent of the time

460
00:15:26,639 --> 00:15:28,320
you see it's benign the model thinks

461
00:15:28,320 --> 00:15:30,160
that's a fair trade-off so it says

462
00:15:30,160 --> 00:15:32,320
whenever i see a gcc of this version and

463
00:15:32,320 --> 00:15:33,759
ellipses of this version then i'll call

464
00:15:33,759 --> 00:15:34,639
it that

465
00:15:34,639 --> 00:15:36,720
that's clearly not a very good string

466
00:15:36,720 --> 00:15:38,720
so what i did to counteract this is i

467
00:15:38,720 --> 00:15:40,639
kept track of a rolling buck for false

468
00:15:40,639 --> 00:15:43,120
positives so i would feed in a bunch of

469
00:15:43,120 --> 00:15:45,040
samples and if there were any benign

470
00:15:45,040 --> 00:15:47,920
samples that came back as malicious then

471
00:15:47,920 --> 00:15:49,360
i would throw this thing into a rolling

472
00:15:49,360 --> 00:15:51,120
buffer of false positives and keep them

473
00:15:51,120 --> 00:15:53,360
around for like 10 or 15 minutes

474
00:15:53,360 --> 00:15:54,880
and then for each training mini batch

475
00:15:54,880 --> 00:15:56,240
where we select a small number of

476
00:15:56,240 --> 00:15:58,079
samples to update the model i would

477
00:15:58,079 --> 00:15:59,600
sample from the data set and i would

478
00:15:59,600 --> 00:16:01,120
also sample from this false positive

479
00:16:01,120 --> 00:16:03,199
buffer and then feed that back into the

480
00:16:03,199 --> 00:16:05,360
model to basically overtrain the model

481
00:16:05,360 --> 00:16:07,279
on these things it was pulsing on

482
00:16:07,279 --> 00:16:09,600
and try and bring this 99 versus one

483
00:16:09,600 --> 00:16:13,360
percent thing up closer to 50 50.

484
00:16:13,360 --> 00:16:16,000
and another thing to point out is that

485
00:16:16,000 --> 00:16:16,959
in this

486
00:16:16,959 --> 00:16:18,639
rolling buffer false positive setup that

487
00:16:18,639 --> 00:16:20,959
i have i sample

488
00:16:20,959 --> 00:16:22,880
things that have a higher score more

489
00:16:22,880 --> 00:16:24,240
frequently than things that have a lower

490
00:16:24,240 --> 00:16:26,240
score so the things that are more wrong

491
00:16:26,240 --> 00:16:28,320
get seen more frequently and that helps

492
00:16:28,320 --> 00:16:32,240
to fix those problems more quickly

493
00:16:33,120 --> 00:16:34,880
so signature generation we have an

494
00:16:34,880 --> 00:16:37,199
interpretable model how do we extract ur

495
00:16:37,199 --> 00:16:40,000
signatures from these models

496
00:16:40,000 --> 00:16:41,600
so there are two ways we can do it we

497
00:16:41,600 --> 00:16:43,519
can do it per sample or we can do it in

498
00:16:43,519 --> 00:16:45,759
bulk per sample i imagine would be

499
00:16:45,759 --> 00:16:48,720
useful to reverse engineers and analysts

500
00:16:48,720 --> 00:16:51,040
and people like this so that you can

501
00:16:51,040 --> 00:16:52,800
take a small number of samples let's say

502
00:16:52,800 --> 00:16:54,720
10 or 15 samples that belong to the same

503
00:16:54,720 --> 00:16:56,160
family let's say

504
00:16:56,160 --> 00:16:59,040
and be able to generate a yar rule or

505
00:16:59,040 --> 00:17:01,440
two or three around samples

506
00:17:01,440 --> 00:17:03,440
then there's a second regime in bulk

507
00:17:03,440 --> 00:17:05,679
where we want to take let's say ten

508
00:17:05,679 --> 00:17:07,359
thousand fifty thousand one hundred

509
00:17:07,359 --> 00:17:08,799
thousand samples

510
00:17:08,799 --> 00:17:11,039
and generate a few hundred rules that

511
00:17:11,039 --> 00:17:12,559
mostly capture

512
00:17:12,559 --> 00:17:14,160
the corpus of malware that you're

513
00:17:14,160 --> 00:17:16,640
feeding in

514
00:17:17,280 --> 00:17:21,039
so sample by sample

515
00:17:28,720 --> 00:17:31,440
i have a live demo here

516
00:17:31,440 --> 00:17:32,960
i'm calling this python script that i

517
00:17:32,960 --> 00:17:34,960
wrote called bump sigs and what dump

518
00:17:34,960 --> 00:17:36,559
sigs is going to do is it's going to

519
00:17:36,559 --> 00:17:38,559
load in a model that i've trained and

520
00:17:38,559 --> 00:17:40,320
run that model against a whole bunch of

521
00:17:40,320 --> 00:17:43,960
samples that i've specified

522
00:17:47,679 --> 00:17:49,120
so what it's going to do is it's going

523
00:17:49,120 --> 00:17:50,799
to load up that model and then it's

524
00:17:50,799 --> 00:17:52,400
going to attempt to extract signatures

525
00:17:52,400 --> 00:17:54,799
for 25 samples these 25 samples i

526
00:17:54,799 --> 00:17:56,640
downloaded from virustotal yesterday

527
00:17:56,640 --> 00:17:58,000
it's just

528
00:17:58,000 --> 00:17:59,919
25 samples for the number of positives

529
00:17:59,919 --> 00:18:01,840
was greater than 10 and the file type is

530
00:18:01,840 --> 00:18:03,520
als

531
00:18:03,520 --> 00:18:04,880
so what it's going to do is it's going

532
00:18:04,880 --> 00:18:06,480
to go through each one of these samples

533
00:18:06,480 --> 00:18:08,000
run it through the model

534
00:18:08,000 --> 00:18:10,080
and then extract out potential

535
00:18:10,080 --> 00:18:12,400
signatures or good signatures

536
00:18:12,400 --> 00:18:14,960
in this case we can see

537
00:18:14,960 --> 00:18:16,799
we're going through this particular shot

538
00:18:16,799 --> 00:18:20,000
256 and we see this signature here but

539
00:18:20,000 --> 00:18:22,240
we can see the score isn't very high um

540
00:18:22,240 --> 00:18:23,919
this here is the score and this here is

541
00:18:23,919 --> 00:18:25,600
the offset of the string

542
00:18:25,600 --> 00:18:27,280
we can see it's not very high it was the

543
00:18:27,280 --> 00:18:30,080
highest valued string that it could find

544
00:18:30,080 --> 00:18:31,600
and we're just dumping that out because

545
00:18:31,600 --> 00:18:33,919
it might be a useful string

546
00:18:33,919 --> 00:18:36,640
for the others it's going to dump out

547
00:18:36,640 --> 00:18:39,440
scores and strings associated with

548
00:18:39,440 --> 00:18:40,960
sort of what i would like to call slam

549
00:18:40,960 --> 00:18:42,880
dunk dunk signatures where the

550
00:18:42,880 --> 00:18:45,440
signatures have really high scores

551
00:18:45,440 --> 00:18:46,559
and that's going to output those

552
00:18:46,559 --> 00:18:48,000
potential scores for each model and then

553
00:18:48,000 --> 00:18:49,440
we'll just choose the max that it sees

554
00:18:49,440 --> 00:18:51,360
for each one

555
00:18:51,360 --> 00:18:53,919
we can see a number of strings here that

556
00:18:53,919 --> 00:18:55,840
probably wouldn't make as good

557
00:18:55,840 --> 00:18:57,679
signatures this is sort of indicative of

558
00:18:57,679 --> 00:19:00,559
that 99 percent one percent problem that

559
00:19:00,559 --> 00:19:02,880
i referred to earlier this string here

560
00:19:02,880 --> 00:19:05,120
is going to be it looks like a few glib

561
00:19:05,120 --> 00:19:07,039
c things like stirring copy and stir

562
00:19:07,039 --> 00:19:09,200
case comp and a few things like that

563
00:19:09,200 --> 00:19:11,600
so this particular sequence of i'm going

564
00:19:11,600 --> 00:19:13,440
to guess imports on elves the model

565
00:19:13,440 --> 00:19:15,520
seems to think is malicious it might be

566
00:19:15,520 --> 00:19:17,919
right it might be wrong it's up to the

567
00:19:17,919 --> 00:19:19,520
analyst to determine if that's a good

568
00:19:19,520 --> 00:19:22,080
string to put into the error rule

569
00:19:22,080 --> 00:19:24,080
then it's going to run this over again

570
00:19:24,080 --> 00:19:25,840
the 25 samples that i've presented it

571
00:19:25,840 --> 00:19:27,600
with

572
00:19:27,600 --> 00:19:29,679
and then at the very end it's going to

573
00:19:29,679 --> 00:19:31,919
give us a yara rule that tries to detect

574
00:19:31,919 --> 00:19:34,160
as many samples as it can in those 25

575
00:19:34,160 --> 00:19:35,520
samples

576
00:19:35,520 --> 00:19:37,280
so you can just take that and copy paste

577
00:19:37,280 --> 00:19:39,679
it into the yara rule and put that into

578
00:19:39,679 --> 00:19:41,520
production after of course making sure

579
00:19:41,520 --> 00:19:45,600
that there no false positive triggered

580
00:19:49,120 --> 00:19:50,559
so the second regime is generating

581
00:19:50,559 --> 00:19:53,039
signatures in bulk what we're doing here

582
00:19:53,039 --> 00:19:54,799
is we're feeding in again tens of

583
00:19:54,799 --> 00:19:56,720
thousands of samples through the model

584
00:19:56,720 --> 00:19:58,799
to come up with a good set of i don't

585
00:19:58,799 --> 00:20:02,240
know 100 200 500 signatures that roughly

586
00:20:02,240 --> 00:20:03,919
capture as many malicious samples as it

587
00:20:03,919 --> 00:20:05,520
can

588
00:20:05,520 --> 00:20:07,039
what we're going to do is we're going to

589
00:20:07,039 --> 00:20:09,039
take a pre-trained model we're going to

590
00:20:09,039 --> 00:20:10,240
run it over

591
00:20:10,240 --> 00:20:12,320
as many samples in our malicious and

592
00:20:12,320 --> 00:20:14,960
benign corpuses as we can get

593
00:20:14,960 --> 00:20:17,039
and then for malicious samples it'll

594
00:20:17,039 --> 00:20:18,480
grab the max score that it sees in the

595
00:20:18,480 --> 00:20:20,080
sample and the associated string

596
00:20:20,080 --> 00:20:22,320
associated with that score it's also

597
00:20:22,320 --> 00:20:24,480
going to do the same for benign strings

598
00:20:24,480 --> 00:20:26,159
so if it sees a benign string with a

599
00:20:26,159 --> 00:20:28,720
relatively high score it's going to take

600
00:20:28,720 --> 00:20:31,200
that string and add it to a list of

601
00:20:31,200 --> 00:20:33,039
banished strings in other words it's

602
00:20:33,039 --> 00:20:35,039
going to say don't ever use these

603
00:20:35,039 --> 00:20:36,559
strings because we saw it as a false

604
00:20:36,559 --> 00:20:38,159
positive and we don't want this leaking

605
00:20:38,159 --> 00:20:41,440
into our set of production strings

606
00:20:41,440 --> 00:20:42,880
so we're going to go through we're going

607
00:20:42,880 --> 00:20:44,640
to remove out all the benign false

608
00:20:44,640 --> 00:20:45,919
positives and we're going to be left

609
00:20:45,919 --> 00:20:47,840
behind with a set of strings that

610
00:20:47,840 --> 00:20:49,440
captures as much malicious stuff as

611
00:20:49,440 --> 00:20:50,799
possible

612
00:20:50,799 --> 00:20:52,000
then we're going to sort things by

613
00:20:52,000 --> 00:20:53,280
prevalence

614
00:20:53,280 --> 00:20:55,200
so if we have a string or a byte

615
00:20:55,200 --> 00:20:57,360
sequence that matches let's say 10

616
00:20:57,360 --> 00:20:59,120
percent of our data set that's really

617
00:20:59,120 --> 00:21:00,960
useful and we want to promote this

618
00:21:00,960 --> 00:21:02,400
string so that it definitely gets

619
00:21:02,400 --> 00:21:04,240
selected in our final list of strings

620
00:21:04,240 --> 00:21:05,919
that we put out

621
00:21:05,919 --> 00:21:07,200
then what we're also going to do is

622
00:21:07,200 --> 00:21:08,559
we're going to cluster strings together

623
00:21:08,559 --> 00:21:09,679
based on this thing called hamming

624
00:21:09,679 --> 00:21:11,360
distance hamming distance is a pretty

625
00:21:11,360 --> 00:21:13,360
simple operation where you just look at

626
00:21:13,360 --> 00:21:15,600
two byte sequences of the same length

627
00:21:15,600 --> 00:21:17,360
and then you compare their bytes and if

628
00:21:17,360 --> 00:21:18,720
any of the bytes are different then you

629
00:21:18,720 --> 00:21:21,360
increment the hamming distance by one so

630
00:21:21,360 --> 00:21:22,480
if you have

631
00:21:22,480 --> 00:21:24,640
two byte sequences that differ by two

632
00:21:24,640 --> 00:21:25,520
bytes then you're going to have a

633
00:21:25,520 --> 00:21:28,080
hamming distance of two

634
00:21:28,080 --> 00:21:29,360
we're going to go through all the

635
00:21:29,360 --> 00:21:30,880
signatures that we found we're going to

636
00:21:30,880 --> 00:21:32,159
cluster them together based on hamming

637
00:21:32,159 --> 00:21:33,200
distance

638
00:21:33,200 --> 00:21:35,360
and then every single string in a

639
00:21:35,360 --> 00:21:37,120
particular cluster we're going to look

640
00:21:37,120 --> 00:21:38,880
at their differences and then we're

641
00:21:38,880 --> 00:21:40,720
going to use that as a mask

642
00:21:40,720 --> 00:21:42,320
and then replace all the differences

643
00:21:42,320 --> 00:21:44,720
with a wild card so an example of this

644
00:21:44,720 --> 00:21:47,039
we can see off to the right we can see a

645
00:21:47,039 --> 00:21:48,720
bunch of byte sequences i think there

646
00:21:48,720 --> 00:21:50,080
are 16 or so

647
00:21:50,080 --> 00:21:51,840
that are all mostly the same except for

648
00:21:51,840 --> 00:21:53,440
the middle byte where the middle byte

649
00:21:53,440 --> 00:21:56,240
appears to be a random value so to get

650
00:21:56,240 --> 00:21:57,440
more bang for your buck for the

651
00:21:57,440 --> 00:21:59,280
signature what we can do is we can

652
00:21:59,280 --> 00:22:01,679
replace that middle byte with just a

653
00:22:01,679 --> 00:22:03,200
wild card and be able to collapse those

654
00:22:03,200 --> 00:22:05,280
16 signatures that the model dumped out

655
00:22:05,280 --> 00:22:08,639
into just a single signature

656
00:22:10,240 --> 00:22:12,000
so in terms of signature efficacy i

657
00:22:12,000 --> 00:22:13,679
trained three models i trained an elf

658
00:22:13,679 --> 00:22:15,360
model i trained a mocko model and i

659
00:22:15,360 --> 00:22:17,200
trained a pe model alpha's the linux

660
00:22:17,200 --> 00:22:19,600
executable format maco runs in osx and

661
00:22:19,600 --> 00:22:22,880
pe is the windows executable file format

662
00:22:22,880 --> 00:22:25,679
for lf i collected data from 2017 to 21

663
00:22:25,679 --> 00:22:27,679
for maco i just collected everything and

664
00:22:27,679 --> 00:22:30,559
for pe i selected a small set from 2020

665
00:22:30,559 --> 00:22:32,640
to 2021.

666
00:22:32,640 --> 00:22:35,280
for elf i had 84 000 bad samples that i

667
00:22:35,280 --> 00:22:37,600
was able to scrape from virustotal i had

668
00:22:37,600 --> 00:22:39,120
five and a half million good and that

669
00:22:39,120 --> 00:22:40,880
breaks down as four and a half million

670
00:22:40,880 --> 00:22:42,880
ubuntu samples i basically just set up a

671
00:22:42,880 --> 00:22:44,159
job to

672
00:22:44,159 --> 00:22:46,320
rip through an ubuntu package repository

673
00:22:46,320 --> 00:22:48,480
and extract all the dubs and grab all

674
00:22:48,480 --> 00:22:50,320
the elves from those dabs and then put

675
00:22:50,320 --> 00:22:52,880
them up to s3 for storage

676
00:22:52,880 --> 00:22:54,720
then i grabbed one million samples from

677
00:22:54,720 --> 00:22:58,000
virustotal for maco we have about a 10

678
00:22:58,000 --> 00:22:59,039
90

679
00:22:59,039 --> 00:23:01,600
uh breakdown of bad and good and for pe

680
00:23:01,600 --> 00:23:03,440
we have about a 50 50 breakdown between

681
00:23:03,440 --> 00:23:04,960
bad and good

682
00:23:04,960 --> 00:23:06,240
in terms of true positive rates and

683
00:23:06,240 --> 00:23:08,080
false positive rates for elf we got

684
00:23:08,080 --> 00:23:11,039
about an 81 true positive rate with 950

685
00:23:11,039 --> 00:23:12,080
rules

686
00:23:12,080 --> 00:23:14,240
um that's at a zero percent false

687
00:23:14,240 --> 00:23:17,520
positive rate for ubuntu samples um when

688
00:23:17,520 --> 00:23:19,679
i ran the bulk collection job or the

689
00:23:19,679 --> 00:23:22,480
bulk signature generation job i made

690
00:23:22,480 --> 00:23:24,720
sure that the ubuntu sequences were

691
00:23:24,720 --> 00:23:26,640
weighted more highly than the virus

692
00:23:26,640 --> 00:23:28,080
total samples because virus total

693
00:23:28,080 --> 00:23:30,400
samples can still be kind of falsy even

694
00:23:30,400 --> 00:23:33,360
if there are zero vendor deductions

695
00:23:33,360 --> 00:23:34,880
we got about a point one five percent

696
00:23:34,880 --> 00:23:36,480
false positive rate on various total

697
00:23:36,480 --> 00:23:37,919
samples

698
00:23:37,919 --> 00:23:40,320
for maco we got a 90 true positive rate

699
00:23:40,320 --> 00:23:42,240
at a 0.01 false positive rate with just

700
00:23:42,240 --> 00:23:45,200
11 rules this was a very surprising

701
00:23:45,200 --> 00:23:47,120
result and i think this is mostly

702
00:23:47,120 --> 00:23:48,840
because there are a lot of

703
00:23:48,840 --> 00:23:51,600
mostly mostly identical samples in the

704
00:23:51,600 --> 00:23:53,760
maco data set that i collected there's

705
00:23:53,760 --> 00:23:55,200
this one particular kind of malware

706
00:23:55,200 --> 00:23:56,559
family called eagle quest that just

707
00:23:56,559 --> 00:23:59,279
dominates the set of malicious samples i

708
00:23:59,279 --> 00:24:01,279
think it's like 30

709
00:24:01,279 --> 00:24:02,799
and in the end the model just learned to

710
00:24:02,799 --> 00:24:04,400
cue in on those very simple things to

711
00:24:04,400 --> 00:24:06,400
come up with an accurate but not

712
00:24:06,400 --> 00:24:08,799
all-encompassing classifier

713
00:24:08,799 --> 00:24:11,039
and then for pe we got about an 80 true

714
00:24:11,039 --> 00:24:13,360
positive rate at a 0.07 false positive

715
00:24:13,360 --> 00:24:16,719
rate with 700 rules

716
00:24:17,039 --> 00:24:18,559
so in terms of future work i'd really

717
00:24:18,559 --> 00:24:20,640
like to utilize more yar functionality

718
00:24:20,640 --> 00:24:22,400
er is a very expressive language and i'm

719
00:24:22,400 --> 00:24:24,159
only using a very very small subset of

720
00:24:24,159 --> 00:24:25,919
it there are things like string offset

721
00:24:25,919 --> 00:24:27,600
that we could put into the model we can

722
00:24:27,600 --> 00:24:29,279
make the model aware of where a string

723
00:24:29,279 --> 00:24:32,000
is in the sample to provide more context

724
00:24:32,000 --> 00:24:33,279
so if you see something like this

725
00:24:33,279 --> 00:24:35,360
program can't be run in dos mode in the

726
00:24:35,360 --> 00:24:36,799
middle of the sample that might be kind

727
00:24:36,799 --> 00:24:38,880
of weird and the model would help things

728
00:24:38,880 --> 00:24:40,000
like that

729
00:24:40,000 --> 00:24:41,679
we could also add string counts so the

730
00:24:41,679 --> 00:24:44,080
model might be

731
00:24:44,080 --> 00:24:46,480
able to look at how often a string

732
00:24:46,480 --> 00:24:48,240
occurs in a sample

733
00:24:48,240 --> 00:24:50,640
there's also a very expressive boolean

734
00:24:50,640 --> 00:24:52,559
operation logic thing that we can use in

735
00:24:52,559 --> 00:24:54,799
yara so it can have different sets of

736
00:24:54,799 --> 00:24:56,400
strings and we can end and order these

737
00:24:56,400 --> 00:24:57,679
things together

738
00:24:57,679 --> 00:24:59,919
and then have string count thresholds

739
00:24:59,919 --> 00:25:01,360
and things like this

740
00:25:01,360 --> 00:25:03,840
and extending the model's ability to

741
00:25:03,840 --> 00:25:05,039
account for things like this would be

742
00:25:05,039 --> 00:25:06,480
really useful in terms of increasing

743
00:25:06,480 --> 00:25:08,000
true positive rates and decreasing false

744
00:25:08,000 --> 00:25:09,679
positive rates

745
00:25:09,679 --> 00:25:11,520
i'd also like to introduce model driven

746
00:25:11,520 --> 00:25:13,760
string wild carding into the setup so

747
00:25:13,760 --> 00:25:15,760
replace this hamming distance thing with

748
00:25:15,760 --> 00:25:17,840
a model driven string wild carding

749
00:25:17,840 --> 00:25:19,039
where we use the model's input

750
00:25:19,039 --> 00:25:21,520
sensitivity and we feed in a byte

751
00:25:21,520 --> 00:25:23,279
sequence we get it score

752
00:25:23,279 --> 00:25:25,760
and then we look at a byte we start

753
00:25:25,760 --> 00:25:27,600
moving that bot around we replace it

754
00:25:27,600 --> 00:25:29,360
with random values and if the model

755
00:25:29,360 --> 00:25:31,120
score doesn't change much we can say

756
00:25:31,120 --> 00:25:32,559
well the model doesn't care much about

757
00:25:32,559 --> 00:25:36,000
the spite so we can just wild card and

758
00:25:36,000 --> 00:25:38,480
yeah this would be much less hacky and a

759
00:25:38,480 --> 00:25:40,880
much more model-driven way of doing wild

760
00:25:40,880 --> 00:25:42,480
carding

761
00:25:42,480 --> 00:25:43,840
i'd also like to integrate this tool

762
00:25:43,840 --> 00:25:46,240
with parsing libraries so have better

763
00:25:46,240 --> 00:25:49,039
context in terms of what exactly this

764
00:25:49,039 --> 00:25:51,120
byte sequence is if the byte sequence is

765
00:25:51,120 --> 00:25:51,760
in

766
00:25:51,760 --> 00:25:53,679
let's say an executable section or

767
00:25:53,679 --> 00:25:55,200
something like that maybe we can do some

768
00:25:55,200 --> 00:25:56,400
disassembly

769
00:25:56,400 --> 00:25:57,919
and then get an idea of what kind of op

770
00:25:57,919 --> 00:26:00,159
codes are on this around the signature

771
00:26:00,159 --> 00:26:01,360
that we grabbed out

772
00:26:01,360 --> 00:26:03,120
see if the

773
00:26:03,120 --> 00:26:04,400
see if the bytes that we're grabbing out

774
00:26:04,400 --> 00:26:06,080
are in the overlay section see if they

775
00:26:06,080 --> 00:26:07,200
correspond to data see if they

776
00:26:07,200 --> 00:26:08,960
correspond to code see if they

777
00:26:08,960 --> 00:26:10,320
correspond to interesting things and

778
00:26:10,320 --> 00:26:12,080
headers things like this i feel like

779
00:26:12,080 --> 00:26:13,679
that would be really useful for malware

780
00:26:13,679 --> 00:26:15,200
analysts to be able to

781
00:26:15,200 --> 00:26:17,520
quickly be able to look at a string or a

782
00:26:17,520 --> 00:26:19,279
sequence of bytes and be able to see

783
00:26:19,279 --> 00:26:20,640
where those bytes came from and see if

784
00:26:20,640 --> 00:26:24,159
that would be a workable rule

785
00:26:24,159 --> 00:26:25,440
so with that i'd like to thank you a lot

786
00:26:25,440 --> 00:26:26,960
for showing up to this talk again very

787
00:26:26,960 --> 00:26:29,120
excited about this work and yeah i'll

788
00:26:29,120 --> 00:26:30,880
open up the floor for questions thanks

789
00:26:30,880 --> 00:26:33,880
again

