1
00:00:01,130 --> 00:00:14,639
[Music]

2
00:00:14,639 --> 00:00:16,000
welcome and thanks for attending our

3
00:00:16,000 --> 00:00:18,240
talk today it's great to be with you we

4
00:00:18,240 --> 00:00:20,320
are very excited to talk about a topic

5
00:00:20,320 --> 00:00:22,080
that applies to each and every one of us

6
00:00:22,080 --> 00:00:24,240
as human beings and we're hoping that

7
00:00:24,240 --> 00:00:25,920
this helps you avoid some of the

8
00:00:25,920 --> 00:00:27,039
pitfalls

9
00:00:27,039 --> 00:00:30,160
inherent to cyber security

10
00:00:30,160 --> 00:00:31,920
my name is josiah dykstra i'm a

11
00:00:31,920 --> 00:00:33,280
technical fellow at the national

12
00:00:33,280 --> 00:00:35,360
security agency in the cyber security

13
00:00:35,360 --> 00:00:37,920
collaboration center this is where nsa

14
00:00:37,920 --> 00:00:39,840
intersects with the private sector to

15
00:00:39,840 --> 00:00:41,280
help prevent

16
00:00:41,280 --> 00:00:42,879
and mitigate threats against the

17
00:00:42,879 --> 00:00:44,960
department of defense national security

18
00:00:44,960 --> 00:00:47,920
systems and the defense industrial base

19
00:00:47,920 --> 00:00:50,000
nsa is always trying to do our job the

20
00:00:50,000 --> 00:00:51,840
best that we can and with the

21
00:00:51,840 --> 00:00:53,520
appropriate resources that's why i'm

22
00:00:53,520 --> 00:00:54,800
here today

23
00:00:54,800 --> 00:00:56,960
my phd and all of my experience is

24
00:00:56,960 --> 00:00:59,520
technical i have spent 17 years in this

25
00:00:59,520 --> 00:01:01,600
field as a researcher and practitioner

26
00:01:01,600 --> 00:01:03,039
but i learned in the last couple of

27
00:01:03,039 --> 00:01:04,799
years that this intersection between

28
00:01:04,799 --> 00:01:07,119
people and technology are very

29
00:01:07,119 --> 00:01:09,040
interesting and really important

30
00:01:09,040 --> 00:01:11,280
challenges so i love that intersection

31
00:01:11,280 --> 00:01:13,920
doug hi my name is doug huff i'm a

32
00:01:13,920 --> 00:01:15,920
faculty member at the johns hopkins

33
00:01:15,920 --> 00:01:18,640
bloomberg school of public health i'm an

34
00:01:18,640 --> 00:01:22,000
economist i'm a behavioral economist

35
00:01:22,000 --> 00:01:24,080
which means that i

36
00:01:24,080 --> 00:01:27,840
do not assume that everyone is rational

37
00:01:27,840 --> 00:01:30,720
at all times and i usually write and

38
00:01:30,720 --> 00:01:32,000
talk about

39
00:01:32,000 --> 00:01:34,159
irrationality and health care but

40
00:01:34,159 --> 00:01:36,560
working with josiah we were able to talk

41
00:01:36,560 --> 00:01:38,640
about and think about and now present

42
00:01:38,640 --> 00:01:41,200
about how some of the concepts in

43
00:01:41,200 --> 00:01:42,799
behavioral economics and behavioral

44
00:01:42,799 --> 00:01:46,079
psychology can apply to cyber security

45
00:01:46,079 --> 00:01:47,439
so let me give you an overview of our

46
00:01:47,439 --> 00:01:49,439
talk i'm going to start by describing

47
00:01:49,439 --> 00:01:51,360
traditional cyber security goals the

48
00:01:51,360 --> 00:01:53,280
things that we think about today and

49
00:01:53,280 --> 00:01:54,640
then i'm going to hand it over to doug

50
00:01:54,640 --> 00:01:58,560
to introduce bias cognitive bias

51
00:01:58,560 --> 00:02:00,560
generally and also action bias

52
00:02:00,560 --> 00:02:01,759
specifically

53
00:02:01,759 --> 00:02:03,680
i'll come back and describe how i see

54
00:02:03,680 --> 00:02:06,719
action bias manifest in cyber security

55
00:02:06,719 --> 00:02:09,199
why it's a problem and also give a bunch

56
00:02:09,199 --> 00:02:10,959
of counter measures what can we do about

57
00:02:10,959 --> 00:02:12,720
this if it's indeed a problem like we'd

58
00:02:12,720 --> 00:02:14,239
say

59
00:02:14,239 --> 00:02:16,000
so having worked in this field for 17

60
00:02:16,000 --> 00:02:18,080
years i think i would share in common

61
00:02:18,080 --> 00:02:20,800
with you a general definition for what

62
00:02:20,800 --> 00:02:23,440
is cyber security what are our goals and

63
00:02:23,440 --> 00:02:24,959
usually it's something like we want to

64
00:02:24,959 --> 00:02:27,760
prevent and mitigate harm we all kind of

65
00:02:27,760 --> 00:02:29,760
agree on that and in that sense we have

66
00:02:29,760 --> 00:02:32,720
this thou shalt not pass mentality right

67
00:02:32,720 --> 00:02:34,879
keep the bad attackers out and let

68
00:02:34,879 --> 00:02:36,720
legitimate users do whatever their

69
00:02:36,720 --> 00:02:38,720
primary goals are

70
00:02:38,720 --> 00:02:40,319
but if we scratch beneath the surface a

71
00:02:40,319 --> 00:02:42,160
little bit i think we'll see differences

72
00:02:42,160 --> 00:02:44,400
in these these traditional goals and i

73
00:02:44,400 --> 00:02:45,920
want you to think about three groups of

74
00:02:45,920 --> 00:02:47,840
people users

75
00:02:47,840 --> 00:02:50,239
cyber security defenders and senior

76
00:02:50,239 --> 00:02:53,840
leaders so a user wants to achieve her

77
00:02:53,840 --> 00:02:55,760
or his primary goal

78
00:02:55,760 --> 00:02:57,519
being able to purchase things online

79
00:02:57,519 --> 00:02:59,680
being able to manage finances with their

80
00:02:59,680 --> 00:03:01,360
bank being able to communicate with

81
00:03:01,360 --> 00:03:02,879
friends and family

82
00:03:02,879 --> 00:03:05,040
and so they see those gains much more

83
00:03:05,040 --> 00:03:07,120
than they they feel the losses they

84
00:03:07,120 --> 00:03:10,000
think security gets in the way uh and so

85
00:03:10,000 --> 00:03:12,400
their real primary goal is there is that

86
00:03:12,400 --> 00:03:14,159
primary task

87
00:03:14,159 --> 00:03:15,760
those of us who are cyber defenders on

88
00:03:15,760 --> 00:03:18,560
the other hand our goal often is to

89
00:03:18,560 --> 00:03:21,360
fight fires whenever an incident occurs

90
00:03:21,360 --> 00:03:22,959
it's our job to

91
00:03:22,959 --> 00:03:24,480
clean up after that incident and

92
00:03:24,480 --> 00:03:26,879
hopefully to help prevent similar things

93
00:03:26,879 --> 00:03:28,480
from happening again

94
00:03:28,480 --> 00:03:30,959
but we feel gains differently than

95
00:03:30,959 --> 00:03:33,200
losses uh we are very much about trying

96
00:03:33,200 --> 00:03:35,040
to prevent those losses from happening

97
00:03:35,040 --> 00:03:36,560
in the future

98
00:03:36,560 --> 00:03:38,319
the third group the cyber security

99
00:03:38,319 --> 00:03:40,159
leadership or senior leadership in an

100
00:03:40,159 --> 00:03:41,599
organization

101
00:03:41,599 --> 00:03:43,040
they make a lot of assumptions they

102
00:03:43,040 --> 00:03:45,120
presume that users will make the

103
00:03:45,120 --> 00:03:48,080
rational choice at all times even if

104
00:03:48,080 --> 00:03:50,080
that means the user isn't optimizing

105
00:03:50,080 --> 00:03:52,319
their primary goal which as i said is

106
00:03:52,319 --> 00:03:54,799
exactly what the users are trying to do

107
00:03:54,799 --> 00:03:56,159
those leaders also make very

108
00:03:56,159 --> 00:03:59,360
conservative policies because their goal

109
00:03:59,360 --> 00:04:00,400
is the business

110
00:04:00,400 --> 00:04:03,439
uh successes after all uh their very job

111
00:04:03,439 --> 00:04:05,280
sometimes depend on being the company

112
00:04:05,280 --> 00:04:07,280
being secure but they're not being any

113
00:04:07,280 --> 00:04:09,920
incidents that's bad for business

114
00:04:09,920 --> 00:04:12,319
now all three of those groups behave in

115
00:04:12,319 --> 00:04:13,840
sort of a rational way from their

116
00:04:13,840 --> 00:04:16,000
individual perspective but if we look at

117
00:04:16,000 --> 00:04:18,238
sort of an overarching perspective it

118
00:04:18,238 --> 00:04:19,120
isn't

119
00:04:19,120 --> 00:04:20,959
totally rational in terms of the

120
00:04:20,959 --> 00:04:22,800
organization or the business

121
00:04:22,800 --> 00:04:25,280
no one person really is maximizing their

122
00:04:25,280 --> 00:04:27,520
choice for the good of the organization

123
00:04:27,520 --> 00:04:28,560
and

124
00:04:28,560 --> 00:04:30,080
what can we do about these sort of

125
00:04:30,080 --> 00:04:32,320
disagreeing risk models if the user is

126
00:04:32,320 --> 00:04:34,400
understanding risk differently than the

127
00:04:34,400 --> 00:04:36,400
organizational leaders what can be done

128
00:04:36,400 --> 00:04:38,880
about that

129
00:04:39,199 --> 00:04:40,560
so in the same way that those three

130
00:04:40,560 --> 00:04:42,720
groups have different goals they also

131
00:04:42,720 --> 00:04:44,800
feel loss and react to crisis quite

132
00:04:44,800 --> 00:04:45,759
differently

133
00:04:45,759 --> 00:04:48,720
take ransomware as one example if you

134
00:04:48,720 --> 00:04:49,600
walked

135
00:04:49,600 --> 00:04:51,199
up to your computer tomorrow and all

136
00:04:51,199 --> 00:04:54,080
your files were encrypted

137
00:04:54,080 --> 00:04:55,840
most users would have an immediate

138
00:04:55,840 --> 00:04:58,479
reaction to say i need my data back i'm

139
00:04:58,479 --> 00:05:01,280
willing to unplug the computer or pay

140
00:05:01,280 --> 00:05:03,360
the money whatever will get me my data

141
00:05:03,360 --> 00:05:05,199
back as fast as possible

142
00:05:05,199 --> 00:05:06,960
because that user feels the loss of

143
00:05:06,960 --> 00:05:08,840
their information very very

144
00:05:08,840 --> 00:05:12,080
intensely users don't see the future

145
00:05:12,080 --> 00:05:14,479
benefit of cyber security

146
00:05:14,479 --> 00:05:16,160
in times that aren't crisis they just

147
00:05:16,160 --> 00:05:18,160
think well the cyber security's slowing

148
00:05:18,160 --> 00:05:20,400
down my computer it's it's just getting

149
00:05:20,400 --> 00:05:21,680
in the way

150
00:05:21,680 --> 00:05:23,039
they don't understand that it's trying

151
00:05:23,039 --> 00:05:26,320
to prevent future loss like ransomware

152
00:05:26,320 --> 00:05:28,320
now again consider the cybersecurity

153
00:05:28,320 --> 00:05:30,320
defenders in the case of ransomware

154
00:05:30,320 --> 00:05:32,960
their job is to put out that fire uh

155
00:05:32,960 --> 00:05:35,199
figure out what went wrong what broke

156
00:05:35,199 --> 00:05:37,520
down how can we prevent that problem how

157
00:05:37,520 --> 00:05:39,520
can we get users their data back and

158
00:05:39,520 --> 00:05:41,600
remedy the situation but that's

159
00:05:41,600 --> 00:05:44,080
different than the user's goals

160
00:05:44,080 --> 00:05:46,000
the third group like the cso of a

161
00:05:46,000 --> 00:05:48,000
company or any other leadership in an

162
00:05:48,000 --> 00:05:49,280
organization

163
00:05:49,280 --> 00:05:50,320
their

164
00:05:50,320 --> 00:05:52,479
whole job depends on security and

165
00:05:52,479 --> 00:05:54,479
ransomware is a failure of that security

166
00:05:54,479 --> 00:05:57,280
in some sense some people get fired for

167
00:05:57,280 --> 00:05:59,600
those kinds of situations so the cso's

168
00:05:59,600 --> 00:06:02,639
real goal in life is ultimate security

169
00:06:02,639 --> 00:06:05,039
even if it requires crazy amounts of

170
00:06:05,039 --> 00:06:07,520
resources lots of money or time

171
00:06:07,520 --> 00:06:09,840
they want zero ransomware and they're

172
00:06:09,840 --> 00:06:11,520
willing sometimes to do anything to

173
00:06:11,520 --> 00:06:13,759
prevent that from ever happening

174
00:06:13,759 --> 00:06:17,039
now we had users defenders and leaders

175
00:06:17,039 --> 00:06:18,800
let's imagine a hypothetical fourth

176
00:06:18,800 --> 00:06:21,919
person which is rational randy this is

177
00:06:21,919 --> 00:06:24,080
what things would look like in a perfect

178
00:06:24,080 --> 00:06:25,039
world

179
00:06:25,039 --> 00:06:27,280
rational randy would do would look at

180
00:06:27,280 --> 00:06:29,520
ransomware quite differently and that

181
00:06:29,520 --> 00:06:31,280
would be look like understanding the

182
00:06:31,280 --> 00:06:33,120
value of the data the value of the

183
00:06:33,120 --> 00:06:35,759
assets the things that might be lost

184
00:06:35,759 --> 00:06:36,720
and

185
00:06:36,720 --> 00:06:39,199
finding security that matches the loss

186
00:06:39,199 --> 00:06:41,600
not over or underspending

187
00:06:41,600 --> 00:06:44,319
but that isn't really very common in our

188
00:06:44,319 --> 00:06:46,560
field so to help me explain why most

189
00:06:46,560 --> 00:06:48,720
people don't act like rational randy let

190
00:06:48,720 --> 00:06:51,440
me hand it over to doug

191
00:06:51,440 --> 00:06:52,880
thanks josiah

192
00:06:52,880 --> 00:06:54,720
now what i need to do for the next

193
00:06:54,720 --> 00:06:56,240
couple of minutes is to give you a

194
00:06:56,240 --> 00:06:58,800
little tutorial on behavioral economics

195
00:06:58,800 --> 00:07:01,039
and cognitive biases

196
00:07:01,039 --> 00:07:03,599
and i want to start with the idea of

197
00:07:03,599 --> 00:07:05,360
different modes of thinking

198
00:07:05,360 --> 00:07:06,240
well

199
00:07:06,240 --> 00:07:07,680
philosophers

200
00:07:07,680 --> 00:07:11,280
for eons have talked about how we think

201
00:07:11,280 --> 00:07:13,120
in different ways

202
00:07:13,120 --> 00:07:15,520
aristotle did this william james in the

203
00:07:15,520 --> 00:07:18,319
19th century made a distinction between

204
00:07:18,319 --> 00:07:20,880
empirical thinking and associative

205
00:07:20,880 --> 00:07:22,080
thought

206
00:07:22,080 --> 00:07:23,440
and

207
00:07:23,440 --> 00:07:26,560
about 12 years ago daniel kahneman who

208
00:07:26,560 --> 00:07:28,560
won the nobel prize in economics wrote

209
00:07:28,560 --> 00:07:30,639
this terrific book called thinking fast

210
00:07:30,639 --> 00:07:32,000
and slow

211
00:07:32,000 --> 00:07:34,080
and one of the things he talked about in

212
00:07:34,080 --> 00:07:36,400
that book was the two different kinds of

213
00:07:36,400 --> 00:07:38,639
thinking and he described them as system

214
00:07:38,639 --> 00:07:40,479
one and system two

215
00:07:40,479 --> 00:07:42,880
now system two is what we usually think

216
00:07:42,880 --> 00:07:45,199
of as as thinking

217
00:07:45,199 --> 00:07:48,639
it's controlled it's effortful

218
00:07:48,639 --> 00:07:51,199
it's deductive it takes some time and

219
00:07:51,199 --> 00:07:53,360
you know when you're thinking and so if

220
00:07:53,360 --> 00:07:55,759
someone asks you so how did you come to

221
00:07:55,759 --> 00:07:57,759
that conclusion you can say well i

222
00:07:57,759 --> 00:07:59,199
thought about this then i thought about

223
00:07:59,199 --> 00:08:00,960
that and i thought about this and then i

224
00:08:00,960 --> 00:08:02,720
made this decision

225
00:08:02,720 --> 00:08:04,960
system one on the other hand is almost

226
00:08:04,960 --> 00:08:08,879
the opposite it's quick it's automatic

227
00:08:08,879 --> 00:08:10,560
it's effortless

228
00:08:10,560 --> 00:08:13,039
and if someone asked you at the end

229
00:08:13,039 --> 00:08:15,599
how did you make that decision then the

230
00:08:15,599 --> 00:08:17,919
answer is well it just seemed right it

231
00:08:17,919 --> 00:08:20,879
was it was just right it was intuitive

232
00:08:20,879 --> 00:08:22,080
if you will

233
00:08:22,080 --> 00:08:22,879
so

234
00:08:22,879 --> 00:08:25,520
uh a common example of this is thinking

235
00:08:25,520 --> 00:08:26,720
about

236
00:08:26,720 --> 00:08:30,160
when you first learned to drive a car

237
00:08:30,160 --> 00:08:32,719
it was painful it was terrifying because

238
00:08:32,719 --> 00:08:34,640
there was all this information coming in

239
00:08:34,640 --> 00:08:36,719
at you and you had to decide what was

240
00:08:36,719 --> 00:08:38,559
important what was not what to respond

241
00:08:38,559 --> 00:08:40,719
to and what not to

242
00:08:40,719 --> 00:08:42,799
that is system two

243
00:08:42,799 --> 00:08:45,200
think about how you drive a car now i

244
00:08:45,200 --> 00:08:46,959
mean you're driving and you're listening

245
00:08:46,959 --> 00:08:48,880
to the radio and you're talking with

246
00:08:48,880 --> 00:08:51,040
friends and i hope that you're not

247
00:08:51,040 --> 00:08:53,519
texting but

248
00:08:53,519 --> 00:08:55,519
you're using system one and the way

249
00:08:55,519 --> 00:08:57,760
reason that you use system one is that

250
00:08:57,760 --> 00:08:59,680
you've driven for so long you know how

251
00:08:59,680 --> 00:09:02,080
to do this and you're basically doing it

252
00:09:02,080 --> 00:09:04,160
on autopilot

253
00:09:04,160 --> 00:09:07,440
now as conan points out we have to use

254
00:09:07,440 --> 00:09:09,760
both system one and system two

255
00:09:09,760 --> 00:09:12,480
uh we it it just has to

256
00:09:12,480 --> 00:09:15,360
the challenge though is system two is

257
00:09:15,360 --> 00:09:17,680
really expensive in terms of time and

258
00:09:17,680 --> 00:09:20,080
energy it takes a lot of work so we've

259
00:09:20,080 --> 00:09:22,399
got to be able to use system one

260
00:09:22,399 --> 00:09:24,480
and one of the ways we use system one

261
00:09:24,480 --> 00:09:26,160
and use it pretty effectively for the

262
00:09:26,160 --> 00:09:29,440
most part is by the use of heuristics

263
00:09:29,440 --> 00:09:30,720
now let me give you a couple of

264
00:09:30,720 --> 00:09:33,279
definitions of heuristics one i'm

265
00:09:33,279 --> 00:09:35,839
actually going to have to read this

266
00:09:35,839 --> 00:09:37,839
because it's so

267
00:09:37,839 --> 00:09:39,519
specific and this is from gerd

268
00:09:39,519 --> 00:09:40,880
gigarenser

269
00:09:40,880 --> 00:09:43,120
who is a psychologist at the max planck

270
00:09:43,120 --> 00:09:45,440
institute for human development

271
00:09:45,440 --> 00:09:48,720
his definition of a heuristic is quote a

272
00:09:48,720 --> 00:09:50,480
strategy that ignores part of the

273
00:09:50,480 --> 00:09:52,560
information with the goal of making

274
00:09:52,560 --> 00:09:55,279
decisions more quickly frugally and or

275
00:09:55,279 --> 00:09:58,640
accurately than more complex methods

276
00:09:58,640 --> 00:10:00,320
one of my students

277
00:10:00,320 --> 00:10:03,200
described it as hunches informed by

278
00:10:03,200 --> 00:10:04,480
experience

279
00:10:04,480 --> 00:10:06,720
and the other way to think of it is just

280
00:10:06,720 --> 00:10:08,800
rules of thumb that that we use all the

281
00:10:08,800 --> 00:10:10,160
time

282
00:10:10,160 --> 00:10:11,760
and one of the questions is are

283
00:10:11,760 --> 00:10:13,680
heuristics good or bad

284
00:10:13,680 --> 00:10:16,160
and this has been a point of debate in

285
00:10:16,160 --> 00:10:18,880
the in the field for quite some time

286
00:10:18,880 --> 00:10:22,079
gerd gigarenzer says that look uh

287
00:10:22,079 --> 00:10:23,920
heuristics are great they're fast and

288
00:10:23,920 --> 00:10:26,480
frugal they they save a lot of time and

289
00:10:26,480 --> 00:10:29,519
energy uh and in most cases close enough

290
00:10:29,519 --> 00:10:31,519
is good enough it may not be perfect but

291
00:10:31,519 --> 00:10:33,519
it's close enough

292
00:10:33,519 --> 00:10:35,360
well daniel kahneman and his

293
00:10:35,360 --> 00:10:38,240
intellectual partner amos taversky said

294
00:10:38,240 --> 00:10:40,640
and let me quote on this one heuristics

295
00:10:40,640 --> 00:10:42,720
are highly economical and usually

296
00:10:42,720 --> 00:10:45,360
effective but they lead to systematic

297
00:10:45,360 --> 00:10:47,519
and predictable errors

298
00:10:47,519 --> 00:10:50,399
and the errors are that sometimes the

299
00:10:50,399 --> 00:10:52,720
heuristics stop you from going further

300
00:10:52,720 --> 00:10:55,760
they stop you from going uh deeper into

301
00:10:55,760 --> 00:10:57,440
a particular problem

302
00:10:57,440 --> 00:11:00,000
and one of the one of the bases of these

303
00:11:00,000 --> 00:11:01,839
is the

304
00:11:01,839 --> 00:11:04,160
uh or one of the elements of this is

305
00:11:04,160 --> 00:11:06,640
what they call cognitive bias and as you

306
00:11:06,640 --> 00:11:09,360
can see on the slide here there are

307
00:11:09,360 --> 00:11:11,600
a lot of different cognitive biases in

308
00:11:11,600 --> 00:11:13,279
fact

309
00:11:13,279 --> 00:11:14,720
psychologists

310
00:11:14,720 --> 00:11:17,279
have identified dozens and dozens of

311
00:11:17,279 --> 00:11:19,600
them and let me talk about a few of them

312
00:11:19,600 --> 00:11:21,600
confirmation bias

313
00:11:21,600 --> 00:11:24,079
this is when

314
00:11:24,079 --> 00:11:26,399
when we search for and give way to

315
00:11:26,399 --> 00:11:28,880
evidence that confirms what we already

316
00:11:28,880 --> 00:11:29,760
know

317
00:11:29,760 --> 00:11:32,480
and you see this from liberals watching

318
00:11:32,480 --> 00:11:35,279
msnbc and conservatives watching fox

319
00:11:35,279 --> 00:11:36,720
news

320
00:11:36,720 --> 00:11:39,360
we also have magical thinking

321
00:11:39,360 --> 00:11:42,880
as a cognitive bias and that is we tend

322
00:11:42,880 --> 00:11:44,079
to

323
00:11:44,079 --> 00:11:46,160
think that two events that that occur

324
00:11:46,160 --> 00:11:48,399
together are

325
00:11:48,399 --> 00:11:50,240
right next to each other are somehow

326
00:11:50,240 --> 00:11:53,040
causally related and that

327
00:11:53,040 --> 00:11:55,519
as a result we've we create these

328
00:11:55,519 --> 00:11:57,680
patterns when in fact patterns may not

329
00:11:57,680 --> 00:11:58,959
exist

330
00:11:58,959 --> 00:12:01,920
uh another one is overconfidence bias

331
00:12:01,920 --> 00:12:04,639
which is that we think that we can do

332
00:12:04,639 --> 00:12:06,560
more than we actually can

333
00:12:06,560 --> 00:12:10,639
your boss comes to you and says josiah i

334
00:12:10,639 --> 00:12:13,279
have this project i really need to do in

335
00:12:13,279 --> 00:12:15,760
about two months do you think you can do

336
00:12:15,760 --> 00:12:18,079
it josiah has a lot of on his plate

337
00:12:18,079 --> 00:12:19,680
right now but says

338
00:12:19,680 --> 00:12:22,560
no i can do this

339
00:12:22,560 --> 00:12:24,560
i'm sure then in two months i can do it

340
00:12:24,560 --> 00:12:26,399
but you know one week before the

341
00:12:26,399 --> 00:12:28,399
deadline he realizes oh my heavens i

342
00:12:28,399 --> 00:12:29,760
haven't been able to do it i've had all

343
00:12:29,760 --> 00:12:31,120
these other things to do and now i gotta

344
00:12:31,120 --> 00:12:33,120
hurry up and do all this stuff so that's

345
00:12:33,120 --> 00:12:34,959
overconfidence bias

346
00:12:34,959 --> 00:12:36,399
but of course what we really want to

347
00:12:36,399 --> 00:12:39,200
talk about today is action bias

348
00:12:39,200 --> 00:12:41,360
and action bias is the idea pretty

349
00:12:41,360 --> 00:12:44,000
simply of don't just stand there do

350
00:12:44,000 --> 00:12:45,440
something

351
00:12:45,440 --> 00:12:48,240
and a couple of examples of this

352
00:12:48,240 --> 00:12:51,040
one a great example a wonderful study

353
00:12:51,040 --> 00:12:53,920
that was done of elite goalkeepers in

354
00:12:53,920 --> 00:12:56,240
europe and in israel and what they

355
00:12:56,240 --> 00:12:59,600
looked at was how the goalkeepers moved

356
00:12:59,600 --> 00:13:01,440
during penalty kicks

357
00:13:01,440 --> 00:13:04,959
and if you've watched football or soccer

358
00:13:04,959 --> 00:13:06,720
at all you know the penalty kicks are

359
00:13:06,720 --> 00:13:09,680
really high stakes uh kinds of things

360
00:13:09,680 --> 00:13:12,320
and the it's just the striker and the

361
00:13:12,320 --> 00:13:13,519
goalkeeper

362
00:13:13,519 --> 00:13:16,000
uh when the striker hits the ball the

363
00:13:16,000 --> 00:13:17,920
goalkeeper has less than a quarter of a

364
00:13:17,920 --> 00:13:19,519
second to to

365
00:13:19,519 --> 00:13:21,760
to save it and all that and of course

366
00:13:21,760 --> 00:13:23,839
the crowd's going crazy and one of the

367
00:13:23,839 --> 00:13:26,240
things that the these researchers found

368
00:13:26,240 --> 00:13:29,279
was that goalkeepers really only

369
00:13:29,279 --> 00:13:31,279
stop one out of six

370
00:13:31,279 --> 00:13:32,240
uh

371
00:13:32,240 --> 00:13:34,399
goals penalty kicks

372
00:13:34,399 --> 00:13:36,480
and so you really want to have the

373
00:13:36,480 --> 00:13:39,199
optimal strategy if you're a goalkeeper

374
00:13:39,199 --> 00:13:41,760
now as it turns out 95 percent of the

375
00:13:41,760 --> 00:13:44,079
time for these elite goalkeepers they

376
00:13:44,079 --> 00:13:46,720
move either left or they move right

377
00:13:46,720 --> 00:13:49,680
when in fact the optimal strategy is to

378
00:13:49,680 --> 00:13:51,760
stay in the middle

379
00:13:51,760 --> 00:13:54,399
instead of having a one in six chance of

380
00:13:54,399 --> 00:13:56,480
of saving a goal they will now have a

381
00:13:56,480 --> 00:14:00,079
one in three chance of saving a goal

382
00:14:00,079 --> 00:14:02,480
and so that's their optimal strategy so

383
00:14:02,480 --> 00:14:05,839
why does 95 of the time these elite

384
00:14:05,839 --> 00:14:08,639
goalkeepers move either left or right

385
00:14:08,639 --> 00:14:10,639
and this is a wonderful example of

386
00:14:10,639 --> 00:14:12,880
action bias

387
00:14:12,880 --> 00:14:15,199
that well i've got to do something

388
00:14:15,199 --> 00:14:16,800
because think think of it if you were a

389
00:14:16,800 --> 00:14:18,240
goalkeeper you know that the best

390
00:14:18,240 --> 00:14:20,560
strategy is to stay right in the middle

391
00:14:20,560 --> 00:14:23,199
but if you stay in the middle two thirds

392
00:14:23,199 --> 00:14:25,360
of the time the goal you're not gonna be

393
00:14:25,360 --> 00:14:27,600
able to get the goal to which the fans

394
00:14:27,600 --> 00:14:29,040
and the sports writers and your coach

395
00:14:29,040 --> 00:14:30,959
will go well why did you do that you

396
00:14:30,959 --> 00:14:32,000
should have moved you should have at

397
00:14:32,000 --> 00:14:34,560
least done something again even though

398
00:14:34,560 --> 00:14:37,760
the best strategy is to is to stay in

399
00:14:37,760 --> 00:14:40,079
and stay in one spot

400
00:14:40,079 --> 00:14:42,480
so what what causes this action bias

401
00:14:42,480 --> 00:14:44,639
well it's the urgency to take some

402
00:14:44,639 --> 00:14:48,480
action it's to show leadership it's to

403
00:14:48,480 --> 00:14:50,959
prevent second guessing like the

404
00:14:50,959 --> 00:14:52,959
goalkeepers if all they did was stand in

405
00:14:52,959 --> 00:14:54,399
the middle and two-thirds of the time

406
00:14:54,399 --> 00:14:56,000
they missed the shot

407
00:14:56,000 --> 00:14:59,040
so that's action bias in general now

408
00:14:59,040 --> 00:15:01,360
josiah is going to talk about how action

409
00:15:01,360 --> 00:15:05,519
bias plays out in cyber security

410
00:15:09,360 --> 00:15:10,720
so the more that i have learned about

411
00:15:10,720 --> 00:15:12,480
action bias the more that i have started

412
00:15:12,480 --> 00:15:14,639
to see it in many examples in cyber

413
00:15:14,639 --> 00:15:17,040
security let's go back to the ransomware

414
00:15:17,040 --> 00:15:19,519
example from earlier but all of the

415
00:15:19,519 --> 00:15:21,839
groups i described users and defenders

416
00:15:21,839 --> 00:15:23,839
and leaders were all

417
00:15:23,839 --> 00:15:25,440
all had this instinct to get some

418
00:15:25,440 --> 00:15:27,600
control over the situation even though

419
00:15:27,600 --> 00:15:29,519
their actions looked differently

420
00:15:29,519 --> 00:15:31,120
none of them wanted to just passively

421
00:15:31,120 --> 00:15:34,000
stand by and gather more information or

422
00:15:34,000 --> 00:15:36,000
to build on a plan that they had made

423
00:15:36,000 --> 00:15:37,519
early in advance

424
00:15:37,519 --> 00:15:38,399
and so

425
00:15:38,399 --> 00:15:40,160
they didn't they they went with that

426
00:15:40,160 --> 00:15:43,040
sort of immediate non-analysis action

427
00:15:43,040 --> 00:15:45,279
and there was pressure to act right

428
00:15:45,279 --> 00:15:47,199
ransomware often has a countdown and if

429
00:15:47,199 --> 00:15:49,120
you don't take action a bad thing

430
00:15:49,120 --> 00:15:51,920
happens and so that time pressure

431
00:15:51,920 --> 00:15:54,399
encourages people to take any and all

432
00:15:54,399 --> 00:15:55,920
possible actions

433
00:15:55,920 --> 00:15:57,680
particularly if they haven't encountered

434
00:15:57,680 --> 00:15:59,920
that situation before and haven't

435
00:15:59,920 --> 00:16:02,720
prepared and practiced a plan

436
00:16:02,720 --> 00:16:05,839
let me give you another example in 2013

437
00:16:05,839 --> 00:16:08,320
hackers got control of the associated

438
00:16:08,320 --> 00:16:10,880
press's twitter account and once they

439
00:16:10,880 --> 00:16:12,639
had access to the account they sent a

440
00:16:12,639 --> 00:16:15,040
fake tweet that said there have been two

441
00:16:15,040 --> 00:16:16,560
bombings at the white house and the

442
00:16:16,560 --> 00:16:18,800
president is injured

443
00:16:18,800 --> 00:16:20,480
almost immediately the stock market

444
00:16:20,480 --> 00:16:21,519
plummeted

445
00:16:21,519 --> 00:16:23,839
and this is action bias in at least two

446
00:16:23,839 --> 00:16:26,639
ways one human said there's some

447
00:16:26,639 --> 00:16:28,720
catastrophe we have to take immediate

448
00:16:28,720 --> 00:16:31,440
action sell sell sell we had also

449
00:16:31,440 --> 00:16:34,240
codified algorithms to read the news and

450
00:16:34,240 --> 00:16:36,000
when stuff like this happens to make

451
00:16:36,000 --> 00:16:38,320
automatic trades so we built the action

452
00:16:38,320 --> 00:16:41,040
bias into the computer and both of them

453
00:16:41,040 --> 00:16:43,839
initiated this this free fall

454
00:16:43,839 --> 00:16:46,320
nobody said let's stop and validate the

455
00:16:46,320 --> 00:16:49,120
claims let's uh get more information and

456
00:16:49,120 --> 00:16:51,360
make sure it's true before we act

457
00:16:51,360 --> 00:16:53,279
it just happened the action just kicked

458
00:16:53,279 --> 00:16:55,279
in almost immediately

459
00:16:55,279 --> 00:16:56,880
the third example and maybe the one that

460
00:16:56,880 --> 00:16:59,600
came to mind for you first is fishing

461
00:16:59,600 --> 00:17:02,320
fishing actually plays on this fear that

462
00:17:02,320 --> 00:17:04,160
you need to take some urgent action

463
00:17:04,160 --> 00:17:06,640
right away your account is compromised

464
00:17:06,640 --> 00:17:08,559
your money has been lost

465
00:17:08,559 --> 00:17:10,880
and the reason that phishing works is

466
00:17:10,880 --> 00:17:12,720
because people have this intuitive

467
00:17:12,720 --> 00:17:14,559
action to act without thinking

468
00:17:14,559 --> 00:17:16,319
beforehand

469
00:17:16,319 --> 00:17:19,280
now even well-educated professionals you

470
00:17:19,280 --> 00:17:23,039
and me are also subject to this bias and

471
00:17:23,039 --> 00:17:24,880
having some awareness is certainly the

472
00:17:24,880 --> 00:17:26,720
first step in doing uh better

473
00:17:26,720 --> 00:17:29,200
preparation but none of us are immune

474
00:17:29,200 --> 00:17:31,039
from it

475
00:17:31,039 --> 00:17:33,200
let me give a last and fourth example

476
00:17:33,200 --> 00:17:35,360
which is incident response

477
00:17:35,360 --> 00:17:37,360
in some companies if the organization

478
00:17:37,360 --> 00:17:39,600
has a breach or a compromise they hire

479
00:17:39,600 --> 00:17:42,160
an external incident response team

480
00:17:42,160 --> 00:17:43,919
the goal there is literally for that

481
00:17:43,919 --> 00:17:46,160
response team to take some action but

482
00:17:46,160 --> 00:17:48,080
let me tell you why this is different

483
00:17:48,080 --> 00:17:51,760
those are well-educated prepared trained

484
00:17:51,760 --> 00:17:54,960
experts who know when when the action

485
00:17:54,960 --> 00:17:57,360
will have positive benefit and when an

486
00:17:57,360 --> 00:17:59,919
action actually won't have any benefit

487
00:17:59,919 --> 00:18:02,320
that's quite different than most average

488
00:18:02,320 --> 00:18:04,559
people reacting to a crisis the the

489
00:18:04,559 --> 00:18:06,799
incident response team yes is being paid

490
00:18:06,799 --> 00:18:09,840
to take action but they are prepared and

491
00:18:09,840 --> 00:18:12,080
adequately um making the right decisions

492
00:18:12,080 --> 00:18:13,520
there

493
00:18:13,520 --> 00:18:15,600
so there are catastrophic cyber

494
00:18:15,600 --> 00:18:18,400
incidences it seems like every day every

495
00:18:18,400 --> 00:18:20,880
week and there's a pressure to act and

496
00:18:20,880 --> 00:18:22,880
to prevent those occurrences from

497
00:18:22,880 --> 00:18:24,160
happening again

498
00:18:24,160 --> 00:18:27,039
but the more that we just live in the

499
00:18:27,039 --> 00:18:28,480
seat of the moment without that

500
00:18:28,480 --> 00:18:31,039
preparation the more inconsistent our

501
00:18:31,039 --> 00:18:32,799
actions are and sometimes the actions we

502
00:18:32,799 --> 00:18:36,720
take end up making the situation worse

503
00:18:37,600 --> 00:18:39,600
at this point you might be saying are

504
00:18:39,600 --> 00:18:41,280
they just telling us to let bad things

505
00:18:41,280 --> 00:18:43,200
happen let me assure you that is not

506
00:18:43,200 --> 00:18:45,440
what i mean whatsoever there are plenty

507
00:18:45,440 --> 00:18:48,000
of situations where we as firefighters

508
00:18:48,000 --> 00:18:50,240
need to triage and put out the fire

509
00:18:50,240 --> 00:18:52,080
first thing that is the most important

510
00:18:52,080 --> 00:18:53,280
to do

511
00:18:53,280 --> 00:18:55,280
that being said a fire is a well

512
00:18:55,280 --> 00:18:57,360
understood situation and a firefighter

513
00:18:57,360 --> 00:18:59,679
knows the correct actions to take in

514
00:18:59,679 --> 00:19:01,360
that situation

515
00:19:01,360 --> 00:19:03,440
the the trouble is if we haven't

516
00:19:03,440 --> 00:19:05,600
prepared or if we end up in a new

517
00:19:05,600 --> 00:19:08,400
interesting situation that we didn't

518
00:19:08,400 --> 00:19:10,799
prepare for we might end up jumping to

519
00:19:10,799 --> 00:19:12,400
the wrong or

520
00:19:12,400 --> 00:19:15,679
even a contradictory decision in the end

521
00:19:15,679 --> 00:19:18,320
so it's difficult for most of us to

522
00:19:18,320 --> 00:19:20,320
distinguish between the benefits of an

523
00:19:20,320 --> 00:19:22,080
action and the risks of an action

524
00:19:22,080 --> 00:19:23,679
particularly when we are under this

525
00:19:23,679 --> 00:19:25,360
pressure to act in the middle of a

526
00:19:25,360 --> 00:19:26,960
crisis

527
00:19:26,960 --> 00:19:29,520
so once the fire is under control once

528
00:19:29,520 --> 00:19:34,080
we have it triaged what comes next

529
00:19:35,120 --> 00:19:37,039
in my view two of the most dangerous

530
00:19:37,039 --> 00:19:39,120
words in all of cyber security are the

531
00:19:39,120 --> 00:19:41,679
phrase never again

532
00:19:41,679 --> 00:19:44,240
i hear this a lot in my life and i think

533
00:19:44,240 --> 00:19:47,360
that lots of us hear it from politicians

534
00:19:47,360 --> 00:19:50,160
and business leaders um even our friends

535
00:19:50,160 --> 00:19:53,200
and family which is man that crisis was

536
00:19:53,200 --> 00:19:54,960
difficult we can never live through that

537
00:19:54,960 --> 00:19:57,440
again we can never let that happen again

538
00:19:57,440 --> 00:19:58,880
let me give you three reasons why i

539
00:19:58,880 --> 00:20:00,880
think this is so dangerous

540
00:20:00,880 --> 00:20:03,679
first it encourages people to try

541
00:20:03,679 --> 00:20:05,520
anything and everything to stop it from

542
00:20:05,520 --> 00:20:07,280
ever happening again

543
00:20:07,280 --> 00:20:09,760
that is a real recipe for wasteful and

544
00:20:09,760 --> 00:20:12,640
wrongful spending our resources to try

545
00:20:12,640 --> 00:20:15,280
and make to try and um

546
00:20:15,280 --> 00:20:16,720
remedy a problem so that it never

547
00:20:16,720 --> 00:20:19,679
happens again it sets the wrong goal

548
00:20:19,679 --> 00:20:21,440
two it actually makes an incorrect

549
00:20:21,440 --> 00:20:22,960
presumption which is

550
00:20:22,960 --> 00:20:24,960
we can achieve this goal we can stop

551
00:20:24,960 --> 00:20:27,280
hackers we can stop attackers from ever

552
00:20:27,280 --> 00:20:30,159
trying again when in fact the attackers

553
00:20:30,159 --> 00:20:32,559
are motivated to keep attacking

554
00:20:32,559 --> 00:20:35,280
nothing that we can do will ever be 100

555
00:20:35,280 --> 00:20:38,480
successful and never again sets this

556
00:20:38,480 --> 00:20:40,400
unprecedented goal that

557
00:20:40,400 --> 00:20:43,440
we can be 100 successful when in fact

558
00:20:43,440 --> 00:20:44,720
the attackers are going to keep

559
00:20:44,720 --> 00:20:47,919
attacking and it sets a really

560
00:20:47,919 --> 00:20:50,320
negative situation for at least for

561
00:20:50,320 --> 00:20:52,559
those of us in cyber security

562
00:20:52,559 --> 00:20:55,440
number three risk management done well

563
00:20:55,440 --> 00:20:57,039
and done appropriately

564
00:20:57,039 --> 00:20:59,520
would help put this phrase into much

565
00:20:59,520 --> 00:21:02,400
better context if we understood how much

566
00:21:02,400 --> 00:21:04,480
our value how much our assets are worth

567
00:21:04,480 --> 00:21:06,960
and we devote the appropriate amount of

568
00:21:06,960 --> 00:21:08,400
preparation and appropriate amount of

569
00:21:08,400 --> 00:21:10,799
security we're not living in a world of

570
00:21:10,799 --> 00:21:13,360
never again but we're lowering risk

571
00:21:13,360 --> 00:21:15,600
minimizing risk as much as possible

572
00:21:15,600 --> 00:21:18,159
without spending all of the all of the

573
00:21:18,159 --> 00:21:20,080
corporations resources to try and get it

574
00:21:20,080 --> 00:21:23,840
to zero we'll never get to zero risk

575
00:21:23,840 --> 00:21:26,080
we can also never win if we set the goal

576
00:21:26,080 --> 00:21:28,640
to never again if we tell our workforce

577
00:21:28,640 --> 00:21:31,360
there can never be another data breach

578
00:21:31,360 --> 00:21:34,400
that's very much a frustrating goal for

579
00:21:34,400 --> 00:21:36,799
most of us in cyber security it leads to

580
00:21:36,799 --> 00:21:38,559
more stress than it's worth we need to

581
00:21:38,559 --> 00:21:40,320
think about different metrics different

582
00:21:40,320 --> 00:21:42,640
ways of measuring success that are

583
00:21:42,640 --> 00:21:45,280
realistic and help achieve the goals

584
00:21:45,280 --> 00:21:48,240
that really matter

585
00:21:48,720 --> 00:21:50,799
so let me describe some counter measures

586
00:21:50,799 --> 00:21:53,120
by starting with what not to do

587
00:21:53,120 --> 00:21:55,760
uh in the middle of a crisis the best

588
00:21:55,760 --> 00:21:57,520
reaction is almost never to pull the

589
00:21:57,520 --> 00:22:00,320
plug it's almost never to pull the

590
00:22:00,320 --> 00:22:03,039
system off the internet there are better

591
00:22:03,039 --> 00:22:05,280
smarter things we can do and it's not

592
00:22:05,280 --> 00:22:07,520
just about better programming in the

593
00:22:07,520 --> 00:22:09,840
twitter hack example

594
00:22:09,840 --> 00:22:12,080
we had codified action bias into the

595
00:22:12,080 --> 00:22:14,640
programs so we can't just program our

596
00:22:14,640 --> 00:22:16,400
way out of this problem it's really

597
00:22:16,400 --> 00:22:18,000
about thinking differently and being

598
00:22:18,000 --> 00:22:20,159
prepared

599
00:22:20,159 --> 00:22:22,240
so what can you do

600
00:22:22,240 --> 00:22:24,559
good risk assessment is the best place

601
00:22:24,559 --> 00:22:26,559
to start and many of us

602
00:22:26,559 --> 00:22:28,640
have a risk management program

603
00:22:28,640 --> 00:22:31,200
sometimes they are successful

604
00:22:31,200 --> 00:22:33,280
many times they are done one time a year

605
00:22:33,280 --> 00:22:35,520
or one time ever

606
00:22:35,520 --> 00:22:37,679
but we don't actually put the risk

607
00:22:37,679 --> 00:22:39,600
management into day to

608
00:22:39,600 --> 00:22:42,320
into day-to-day practice but done well

609
00:22:42,320 --> 00:22:44,640
that should reduce the amount of of

610
00:22:44,640 --> 00:22:46,320
action bias that occurs because we've

611
00:22:46,320 --> 00:22:47,919
done preparation we've thought ahead

612
00:22:47,919 --> 00:22:50,080
about the problem

613
00:22:50,080 --> 00:22:52,960
number two is training and awareness

614
00:22:52,960 --> 00:22:54,799
i must tell you that this is a big

615
00:22:54,799 --> 00:22:56,559
culture change it isn't something we can

616
00:22:56,559 --> 00:22:59,200
just solve overnight but telling your

617
00:22:59,200 --> 00:23:01,600
workforce that action bias is a human

618
00:23:01,600 --> 00:23:02,640
condition

619
00:23:02,640 --> 00:23:04,880
gives them some opportunity to think

620
00:23:04,880 --> 00:23:07,600
more carefully now and in the crisis

621
00:23:07,600 --> 00:23:09,360
about what to do

622
00:23:09,360 --> 00:23:10,159
but

623
00:23:10,159 --> 00:23:12,480
we have to change those user reactions

624
00:23:12,480 --> 00:23:14,880
over time every opportunity that comes

625
00:23:14,880 --> 00:23:16,799
up we should remind people let's be

626
00:23:16,799 --> 00:23:19,520
cautious let's remember not to just jump

627
00:23:19,520 --> 00:23:21,440
into action without thinking

628
00:23:21,440 --> 00:23:23,600
so having that conversation

629
00:23:23,600 --> 00:23:25,520
now in times of calm when there's not a

630
00:23:25,520 --> 00:23:26,559
crisis

631
00:23:26,559 --> 00:23:28,640
that helps prepare them for the time

632
00:23:28,640 --> 00:23:30,480
when they need it just as doug talked

633
00:23:30,480 --> 00:23:32,720
about driving in the beginning when you

634
00:23:32,720 --> 00:23:34,320
were taking driver's ed it's a very

635
00:23:34,320 --> 00:23:36,640
stressful scenario and over our

636
00:23:36,640 --> 00:23:40,080
accumulated experience we come to

637
00:23:40,080 --> 00:23:41,760
have the right heuristics that enable us

638
00:23:41,760 --> 00:23:44,080
to drive safely but that takes a lot of

639
00:23:44,080 --> 00:23:46,320
time in practice

640
00:23:46,320 --> 00:23:48,880
number three we need to educate senior

641
00:23:48,880 --> 00:23:51,039
leadership they need to know that we're

642
00:23:51,039 --> 00:23:53,039
not always going to jump to the side of

643
00:23:53,039 --> 00:23:55,120
the goal uh during the penalty kick

644
00:23:55,120 --> 00:23:57,039
sometimes we're going to stand still

645
00:23:57,039 --> 00:23:59,360
deliberate waiting and that's a

646
00:23:59,360 --> 00:24:02,080
deliberate reasonable choice

647
00:24:02,080 --> 00:24:03,760
some leaders would look at that and say

648
00:24:03,760 --> 00:24:06,159
like the fans in the in the soccer game

649
00:24:06,159 --> 00:24:08,240
well why didn't they do anything when in

650
00:24:08,240 --> 00:24:10,159
fact that is a deliberate action

651
00:24:10,159 --> 00:24:12,080
sometimes but we need to help our

652
00:24:12,080 --> 00:24:14,480
leadership understand that is an option

653
00:24:14,480 --> 00:24:16,320
it's on the table and sometimes it is

654
00:24:16,320 --> 00:24:19,039
the best decision that we can take

655
00:24:19,039 --> 00:24:20,559
um

656
00:24:20,559 --> 00:24:22,159
the biggest one i want to emphasize for

657
00:24:22,159 --> 00:24:24,960
you is slow down and by slow down i

658
00:24:24,960 --> 00:24:27,039
don't necessarily mean when there's a

659
00:24:27,039 --> 00:24:28,640
fire just

660
00:24:28,640 --> 00:24:31,120
move very slowly to put out the fire

661
00:24:31,120 --> 00:24:33,200
actually what i mean is shift the time

662
00:24:33,200 --> 00:24:35,120
shift all of your thinking to before the

663
00:24:35,120 --> 00:24:37,360
crisis so that in the moment you are

664
00:24:37,360 --> 00:24:38,880
prepared and you have practice and

665
00:24:38,880 --> 00:24:40,559
you're ready for that

666
00:24:40,559 --> 00:24:42,240
i also think it's important to have some

667
00:24:42,240 --> 00:24:44,080
healthy skepticism in the heat of the

668
00:24:44,080 --> 00:24:46,240
moment if somebody says you know what we

669
00:24:46,240 --> 00:24:48,960
should do some some action in response

670
00:24:48,960 --> 00:24:50,720
to this data breach

671
00:24:50,720 --> 00:24:52,159
have some healthy skepticism ask

672
00:24:52,159 --> 00:24:53,919
yourself is that going to have the

673
00:24:53,919 --> 00:24:55,919
benefits that i think it's going to and

674
00:24:55,919 --> 00:24:57,919
at what costs

675
00:24:57,919 --> 00:24:59,760
it's always good to have and develop

676
00:24:59,760 --> 00:25:02,559
that healthy skepticism

677
00:25:02,559 --> 00:25:04,559
you know the phrase in shooting ready

678
00:25:04,559 --> 00:25:06,880
aim fire it feels like in cyber security

679
00:25:06,880 --> 00:25:08,880
too often we fire first

680
00:25:08,880 --> 00:25:11,120
and then maybe it's ready and then maybe

681
00:25:11,120 --> 00:25:13,039
it's aim

682
00:25:13,039 --> 00:25:15,279
we can recall those kinds of those

683
00:25:15,279 --> 00:25:17,520
phrases in the heat of the moment to say

684
00:25:17,520 --> 00:25:20,080
are we ready first before we take

685
00:25:20,080 --> 00:25:21,600
whatever the

686
00:25:21,600 --> 00:25:23,679
the analogy for shooting is

687
00:25:23,679 --> 00:25:26,880
and again watchful waiting is a slowing

688
00:25:26,880 --> 00:25:28,799
down in the heat of the moment

689
00:25:28,799 --> 00:25:31,360
uh many soccer players many sports

690
00:25:31,360 --> 00:25:33,760
players will say the games seem to move

691
00:25:33,760 --> 00:25:35,279
in slow motion

692
00:25:35,279 --> 00:25:37,279
and that's for a couple of reasons one

693
00:25:37,279 --> 00:25:39,760
they've practiced they can see the game

694
00:25:39,760 --> 00:25:41,520
moving out in their head

695
00:25:41,520 --> 00:25:43,360
as if it were in slow motion because

696
00:25:43,360 --> 00:25:45,760
they are prepared and in cyber security

697
00:25:45,760 --> 00:25:47,600
we can do the same thing in the heat of

698
00:25:47,600 --> 00:25:48,720
a crisis

699
00:25:48,720 --> 00:25:50,400
it might seem like things are moving in

700
00:25:50,400 --> 00:25:52,320
slow motion and we can make very good

701
00:25:52,320 --> 00:25:54,159
choices because we've done that

702
00:25:54,159 --> 00:25:55,840
preparation we've moved all the time

703
00:25:55,840 --> 00:25:57,600
devoted to the problem

704
00:25:57,600 --> 00:26:00,000
before it actually happens

705
00:26:00,000 --> 00:26:02,320
it was eisenhower who had a quote that

706
00:26:02,320 --> 00:26:04,880
was along the lines of plans are useless

707
00:26:04,880 --> 00:26:08,000
but planning is everything

708
00:26:08,000 --> 00:26:10,320
even the plans that we make might not be

709
00:26:10,320 --> 00:26:12,640
perfect but if we just have sops or

710
00:26:12,640 --> 00:26:14,640
playbooks on the shelf and we never even

711
00:26:14,640 --> 00:26:16,640
look at them that's not very helpful we

712
00:26:16,640 --> 00:26:18,880
need to practice them but we also need

713
00:26:18,880 --> 00:26:20,880
to understand that the preparation

714
00:26:20,880 --> 00:26:23,120
process is at least as valuable as the

715
00:26:23,120 --> 00:26:24,559
book on the shelf

716
00:26:24,559 --> 00:26:26,080
knowing who should be in the room to

717
00:26:26,080 --> 00:26:28,480
make a decision where is the data we

718
00:26:28,480 --> 00:26:30,880
might need to make a better decision

719
00:26:30,880 --> 00:26:32,720
whatever the situation is even if it's

720
00:26:32,720 --> 00:26:35,520
an unpre one we never anticipated

721
00:26:35,520 --> 00:26:37,679
we can still benefit from the

722
00:26:37,679 --> 00:26:40,640
preparation process so it's very very

723
00:26:40,640 --> 00:26:42,240
important to do more planning to

724
00:26:42,240 --> 00:26:44,640
practice plans but to be prepared for

725
00:26:44,640 --> 00:26:47,600
the unexpected

726
00:26:49,039 --> 00:26:50,799
so having spent some of my life as a

727
00:26:50,799 --> 00:26:52,799
researcher and having a passion for

728
00:26:52,799 --> 00:26:54,640
cyber security science

729
00:26:54,640 --> 00:26:56,880
i think this is not a solved problem and

730
00:26:56,880 --> 00:26:58,720
i would throw out a couple of research

731
00:26:58,720 --> 00:27:01,440
ideas for any of you looking for topics

732
00:27:01,440 --> 00:27:03,840
because there's much much more we can do

733
00:27:03,840 --> 00:27:06,080
one is how could we allow people to

734
00:27:06,080 --> 00:27:09,679
validate threats so when the weird tweet

735
00:27:09,679 --> 00:27:11,360
comes out and it seems unexpected

736
00:27:11,360 --> 00:27:13,679
instead of taking action bias instead of

737
00:27:13,679 --> 00:27:16,240
just gut instinct response

738
00:27:16,240 --> 00:27:17,840
how could we get more information

739
00:27:17,840 --> 00:27:20,080
quickly and more trustworthy whether

740
00:27:20,080 --> 00:27:22,640
that's emails that come in your inbox or

741
00:27:22,640 --> 00:27:24,640
things that we see online

742
00:27:24,640 --> 00:27:27,039
how can we fact check those threats

743
00:27:27,039 --> 00:27:29,520
maybe it's hey siri is this a legitimate

744
00:27:29,520 --> 00:27:32,799
email or something along those lines

745
00:27:32,799 --> 00:27:35,039
number two is how do we normalize the

746
00:27:35,039 --> 00:27:37,120
deep thought the system two thinking

747
00:27:37,120 --> 00:27:40,000
versus the reflexive system one thinking

748
00:27:40,000 --> 00:27:41,919
again i don't know all the ways to build

749
00:27:41,919 --> 00:27:43,440
this into a computer i've heard some

750
00:27:43,440 --> 00:27:45,840
interesting ones recently which uh one

751
00:27:45,840 --> 00:27:49,120
of which was deliberate friction in a

752
00:27:49,120 --> 00:27:51,039
important situation or in

753
00:27:51,039 --> 00:27:53,039
in a in a situation where the user has

754
00:27:53,039 --> 00:27:54,960
to make a choice about do i proceed to

755
00:27:54,960 --> 00:27:57,600
the website do i click the button

756
00:27:57,600 --> 00:27:59,120
could the computer actually make it a

757
00:27:59,120 --> 00:28:01,919
little bit more difficult on purpose to

758
00:28:01,919 --> 00:28:03,840
enforce the user to get the user to

759
00:28:03,840 --> 00:28:06,159
think more carefully about oh do i

760
00:28:06,159 --> 00:28:08,240
really want to do that make it a little

761
00:28:08,240 --> 00:28:10,960
less easy just to click the bad link

762
00:28:10,960 --> 00:28:12,880
the last one is about mental models and

763
00:28:12,880 --> 00:28:15,039
there's actually lots of researchers

764
00:28:15,039 --> 00:28:16,799
in the usability community thinking

765
00:28:16,799 --> 00:28:19,279
about how does our brain internalize

766
00:28:19,279 --> 00:28:21,440
what's happening on the computer that's

767
00:28:21,440 --> 00:28:23,600
what a mental model is

768
00:28:23,600 --> 00:28:25,760
and how can we help understand what is

769
00:28:25,760 --> 00:28:27,440
the user thinking when they make this

770
00:28:27,440 --> 00:28:28,960
very quick

771
00:28:28,960 --> 00:28:31,120
action bias decision and how can we

772
00:28:31,120 --> 00:28:33,679
change their mental model to allow them

773
00:28:33,679 --> 00:28:36,640
to do that deeper thinking

774
00:28:36,640 --> 00:28:38,480
and then build that in as a routine so

775
00:28:38,480 --> 00:28:40,159
the mental model sticks with us in all

776
00:28:40,159 --> 00:28:42,720
kinds of situations how can we evolve

777
00:28:42,720 --> 00:28:47,279
those to help con combat action bias

778
00:28:48,240 --> 00:28:50,640
many of you i'm sure remember the flight

779
00:28:50,640 --> 00:28:53,760
in 2009 when sully sullenberger

780
00:28:53,760 --> 00:28:55,520
had an airplane where the birds took out

781
00:28:55,520 --> 00:28:57,600
both engines of the airplane and

782
00:28:57,600 --> 00:29:00,000
relatively quickly he made a decision to

783
00:29:00,000 --> 00:29:02,399
land that plane in the hudson and saved

784
00:29:02,399 --> 00:29:05,520
all 155 passengers on board he was

785
00:29:05,520 --> 00:29:07,360
criticized for a while that he took too

786
00:29:07,360 --> 00:29:09,760
long to decide that he didn't just take

787
00:29:09,760 --> 00:29:11,760
the immediate action and do

788
00:29:11,760 --> 00:29:14,240
what the playbook said but instead based

789
00:29:14,240 --> 00:29:16,240
on his years of experience particularly

790
00:29:16,240 --> 00:29:20,080
in understanding risk um he knew that a

791
00:29:20,080 --> 00:29:22,240
delay was better than a disaster and

792
00:29:22,240 --> 00:29:26,240
that thinking saved 155 people's lives

793
00:29:26,240 --> 00:29:28,320
action bias is our

794
00:29:28,320 --> 00:29:30,559
built-in sense to gain control by taking

795
00:29:30,559 --> 00:29:33,520
some action and never again is the

796
00:29:33,520 --> 00:29:37,039
extreme of that kind of action bias

797
00:29:37,039 --> 00:29:38,240
what i hope you remember from this

798
00:29:38,240 --> 00:29:41,440
presentation is slow down move the time

799
00:29:41,440 --> 00:29:43,039
that you devote to a problem before the

800
00:29:43,039 --> 00:29:44,640
crisis occurs

801
00:29:44,640 --> 00:29:46,000
have a plan

802
00:29:46,000 --> 00:29:48,240
practice the plan and be prepared for

803
00:29:48,240 --> 00:29:50,000
the unexpected because we can't

804
00:29:50,000 --> 00:29:51,360
anticipate everything that's going to

805
00:29:51,360 --> 00:29:55,200
happen particularly in cyber security

806
00:29:55,200 --> 00:29:56,799
so what can you do

807
00:29:56,799 --> 00:29:59,200
right away this afternoon when you go

808
00:29:59,200 --> 00:30:01,039
back to work start spreading the word

809
00:30:01,039 --> 00:30:03,120
about action bias let people know that

810
00:30:03,120 --> 00:30:05,919
this is a real human condition it's not

811
00:30:05,919 --> 00:30:07,919
a weakness it's not something we fire

812
00:30:07,919 --> 00:30:09,760
people for but we need to raise

813
00:30:09,760 --> 00:30:11,919
awareness that this is how the human

814
00:30:11,919 --> 00:30:14,880
brain sometimes responds and we can do

815
00:30:14,880 --> 00:30:16,960
it differently we can we can overcome

816
00:30:16,960 --> 00:30:18,799
these problems

817
00:30:18,799 --> 00:30:20,480
in the next three months in a little bit

818
00:30:20,480 --> 00:30:22,799
longer term look at the books on your

819
00:30:22,799 --> 00:30:24,559
shelf look at the sops you have the

820
00:30:24,559 --> 00:30:26,960
playbooks the plans make sure they're up

821
00:30:26,960 --> 00:30:29,279
to date for whatever they're built for

822
00:30:29,279 --> 00:30:31,440
and if needed update them make sure

823
00:30:31,440 --> 00:30:32,720
they're up to date

824
00:30:32,720 --> 00:30:35,679
and also think about what other goals

825
00:30:35,679 --> 00:30:37,440
and metrics could you set for the

826
00:30:37,440 --> 00:30:39,919
organization that will balance both

827
00:30:39,919 --> 00:30:43,440
security and usability whatever the the

828
00:30:43,440 --> 00:30:45,440
goals are of the organization make sure

829
00:30:45,440 --> 00:30:48,080
we're not devoting too few or too many

830
00:30:48,080 --> 00:30:50,559
resources to cyber security

831
00:30:50,559 --> 00:30:52,159
in the long term let's say in the next

832
00:30:52,159 --> 00:30:54,799
six months or so run an exercise do a

833
00:30:54,799 --> 00:30:57,679
tabletop exercise or a red team actually

834
00:30:57,679 --> 00:30:59,600
try out those plans don't just let them

835
00:30:59,600 --> 00:31:01,600
get dusty on the shelf

836
00:31:01,600 --> 00:31:03,679
it's developing the habits developing

837
00:31:03,679 --> 00:31:05,679
the routines of doing it the same way we

838
00:31:05,679 --> 00:31:07,840
have fire drills to make sure that

839
00:31:07,840 --> 00:31:10,000
everybody understands where to go and in

840
00:31:10,000 --> 00:31:11,760
the heat of a crisis we're not having to

841
00:31:11,760 --> 00:31:14,480
figure out what do we do here and ending

842
00:31:14,480 --> 00:31:16,840
up making poor decisions

843
00:31:16,840 --> 00:31:19,600
again keep working on culture change it

844
00:31:19,600 --> 00:31:22,080
takes a long time um it might feel

845
00:31:22,080 --> 00:31:24,000
frustrating that we continue to have

846
00:31:24,000 --> 00:31:27,679
these gut instinct reactions um but the

847
00:31:27,679 --> 00:31:29,360
more that we work the more that we talk

848
00:31:29,360 --> 00:31:31,279
the more that we think about

849
00:31:31,279 --> 00:31:33,360
slow deliberate processing the better it

850
00:31:33,360 --> 00:31:36,479
will become in the long run

851
00:31:36,720 --> 00:31:39,360
humans are central to cyber security we

852
00:31:39,360 --> 00:31:41,760
need each and every one of you and we

853
00:31:41,760 --> 00:31:43,679
need to help the people be as good as

854
00:31:43,679 --> 00:31:46,880
they can that's what this is all about

855
00:31:46,880 --> 00:31:49,360
we are not immune nobody is immune just

856
00:31:49,360 --> 00:31:51,200
having heard this presentation but at

857
00:31:51,200 --> 00:31:53,360
least you're aware and that is really

858
00:31:53,360 --> 00:31:54,640
the first step

859
00:31:54,640 --> 00:31:57,039
preparation and practice those are key

860
00:31:57,039 --> 00:31:59,519
can lead to both consistent and more

861
00:31:59,519 --> 00:32:01,519
rational actions

862
00:32:01,519 --> 00:32:03,039
we hope that you put these into practice

863
00:32:03,039 --> 00:32:04,640
and we look forward to continuing the

864
00:32:04,640 --> 00:32:06,480
conversation with the community and we

865
00:32:06,480 --> 00:32:10,679
look forward to your questions and ideas

