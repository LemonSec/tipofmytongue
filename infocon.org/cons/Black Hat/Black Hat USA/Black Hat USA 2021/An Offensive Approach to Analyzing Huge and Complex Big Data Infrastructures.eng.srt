1
00:00:01,130 --> 00:00:13,759
[Music]

2
00:00:13,759 --> 00:00:16,239
hi everyone welcome to my talk on big

3
00:00:16,239 --> 00:00:19,279
data in security i will talk about how

4
00:00:19,279 --> 00:00:21,600
we can analyze these big data

5
00:00:21,600 --> 00:00:24,960
infrastructures to better protect them

6
00:00:24,960 --> 00:00:26,960
before starting please let me introduce

7
00:00:26,960 --> 00:00:30,240
myself briefly my name is hila i work as

8
00:00:30,240 --> 00:00:32,000
head of research at dreamlab

9
00:00:32,000 --> 00:00:35,200
technologies a swiss infosec company

10
00:00:35,200 --> 00:00:37,440
and an offensive security specialist

11
00:00:37,440 --> 00:00:40,079
with several years of experience and in

12
00:00:40,079 --> 00:00:42,559
the last time i have focused on security

13
00:00:42,559 --> 00:00:45,200
in cloud environments cloud native big

14
00:00:45,200 --> 00:00:48,000
data and related stuff

15
00:00:48,000 --> 00:00:52,000
okay so let's go to the important things

16
00:00:52,000 --> 00:00:54,160
there are some key concepts that i would

17
00:00:54,160 --> 00:00:56,640
like to explain before jumping into the

18
00:00:56,640 --> 00:00:58,559
security part

19
00:00:58,559 --> 00:01:01,199
probably the first thing that comes to

20
00:01:01,199 --> 00:01:03,920
mind when talking about big data is the

21
00:01:03,920 --> 00:01:06,720
challenge of storing large volumes of

22
00:01:06,720 --> 00:01:08,960
information and the technology that will

23
00:01:08,960 --> 00:01:10,799
take care of it

24
00:01:10,799 --> 00:01:12,799
although that's correct around the

25
00:01:12,799 --> 00:01:15,040
storage technology there are many others

26
00:01:15,040 --> 00:01:17,360
of great importance that make up the

27
00:01:17,360 --> 00:01:19,600
ecosystem

28
00:01:19,600 --> 00:01:22,320
when we design big data architectures we

29
00:01:22,320 --> 00:01:24,960
must think about how the data will be

30
00:01:24,960 --> 00:01:26,720
transported from the source to the

31
00:01:26,720 --> 00:01:29,759
storage if the data requires some type

32
00:01:29,759 --> 00:01:32,400
of processing to be consumed

33
00:01:32,400 --> 00:01:34,560
how the information will be accessed

34
00:01:34,560 --> 00:01:36,079
right

35
00:01:36,079 --> 00:01:38,640
so the different processes that the data

36
00:01:38,640 --> 00:01:41,439
go through are divided into four main

37
00:01:41,439 --> 00:01:45,920
layers that comprise the big data stack

38
00:01:45,920 --> 00:01:48,320
so we have the data injection

39
00:01:48,320 --> 00:01:50,880
that is the transport of the information

40
00:01:50,880 --> 00:01:53,520
from the different oceans to the storage

41
00:01:53,520 --> 00:01:56,240
place the storage itself

42
00:01:56,240 --> 00:01:58,960
the data processing layer because the

43
00:01:58,960 --> 00:02:02,000
most common is to ingest raw information

44
00:02:02,000 --> 00:02:05,600
that later needs some kind of processing

45
00:02:05,600 --> 00:02:08,479
and finally the data access layer

46
00:02:08,479 --> 00:02:10,639
basically how users will access and

47
00:02:10,639 --> 00:02:13,280
consume the information

48
00:02:13,280 --> 00:02:15,920
and let's add one more layer here this

49
00:02:15,920 --> 00:02:19,360
is not part of the big data stack but

50
00:02:19,360 --> 00:02:21,520
we have this layer in all big data

51
00:02:21,520 --> 00:02:23,760
infrastructures the cluster management

52
00:02:23,760 --> 00:02:26,959
is really important

53
00:02:26,959 --> 00:02:30,000
so for each of these layers there is a

54
00:02:30,000 --> 00:02:32,560
wide variety of technologies that can be

55
00:02:32,560 --> 00:02:33,840
implemented

56
00:02:33,840 --> 00:02:37,440
the big data stack is hugely big

57
00:02:37,440 --> 00:02:40,000
this one are just a few of the most

58
00:02:40,000 --> 00:02:42,879
popular technologies for example hadoop

59
00:02:42,879 --> 00:02:46,239
for the storage spark and storm for

60
00:02:46,239 --> 00:02:47,519
processing

61
00:02:47,519 --> 00:02:50,080
imbala presto grid for accessing the

62
00:02:50,080 --> 00:02:51,360
information

63
00:02:51,360 --> 00:02:54,560
fluent scope for data injection

64
00:02:54,560 --> 00:02:59,360
and zookeeper for management right

65
00:02:59,360 --> 00:03:02,560
so when we analyze an entire big data

66
00:03:02,560 --> 00:03:04,080
infrastructure

67
00:03:04,080 --> 00:03:06,000
we can actually find many different and

68
00:03:06,000 --> 00:03:08,239
complex technologies interacting with

69
00:03:08,239 --> 00:03:09,360
each other

70
00:03:09,360 --> 00:03:11,840
they meet different functions according

71
00:03:11,840 --> 00:03:14,239
to the layer of the stack where they are

72
00:03:14,239 --> 00:03:16,800
located

73
00:03:17,519 --> 00:03:20,400
so let's see an example of our real big

74
00:03:20,400 --> 00:03:22,319
data architecture

75
00:03:22,319 --> 00:03:25,120
here we have uh two different clouds

76
00:03:25,120 --> 00:03:28,640
one in lvs and another one in any other

77
00:03:28,640 --> 00:03:30,000
cloud provider

78
00:03:30,000 --> 00:03:32,640
so both are running some kubernetes

79
00:03:32,640 --> 00:03:34,720
clusters there are serving different

80
00:03:34,720 --> 00:03:37,680
applications so we want to store and

81
00:03:37,680 --> 00:03:41,280
analyze the love of these applications

82
00:03:41,280 --> 00:03:42,799
for that we will use

83
00:03:42,799 --> 00:03:44,799
flowing bits to collect all the

84
00:03:44,799 --> 00:03:47,680
applications logs and write them to

85
00:03:47,680 --> 00:03:50,400
kafka for the first cloud

86
00:03:50,400 --> 00:03:53,680
and stream them using flume and kinesis

87
00:03:53,680 --> 00:03:57,840
to an open hadoop cluster

88
00:03:57,840 --> 00:03:59,840
within the hadoop cluster the first

89
00:03:59,840 --> 00:04:02,159
component that will receive the data is

90
00:04:02,159 --> 00:04:04,560
spark structure streaming

91
00:04:04,560 --> 00:04:07,280
this one will take care of in hasting

92
00:04:07,280 --> 00:04:09,439
and also processing the information

93
00:04:09,439 --> 00:04:12,080
before dumping it into the hadoop file

94
00:04:12,080 --> 00:04:14,159
system

95
00:04:14,159 --> 00:04:15,120
so

96
00:04:15,120 --> 00:04:17,759
once we have our information here we

97
00:04:17,759 --> 00:04:19,519
want to access it

98
00:04:19,519 --> 00:04:21,839
so for that we could implement for

99
00:04:21,839 --> 00:04:24,560
example hive and presto

100
00:04:24,560 --> 00:04:27,759
or instead of presto we could use impala

101
00:04:27,759 --> 00:04:30,560
breathe or any other technology for

102
00:04:30,560 --> 00:04:34,080
interactive queries against hadoop

103
00:04:34,080 --> 00:04:36,240
if we are developing our own software to

104
00:04:36,240 --> 00:04:38,880
visualize the information then we will

105
00:04:38,880 --> 00:04:41,520
probably have an api talking to the

106
00:04:41,520 --> 00:04:44,160
presto coordinator and analyze front

107
00:04:44,160 --> 00:04:46,080
them

108
00:04:46,080 --> 00:04:49,040
so finally we have the management layer

109
00:04:49,040 --> 00:04:51,360
here it's super common to find apache

110
00:04:51,360 --> 00:04:54,080
zookeeper to centralize the

111
00:04:54,080 --> 00:04:56,880
configuration of all these components

112
00:04:56,880 --> 00:04:58,960
and also an administration tool like

113
00:04:58,960 --> 00:05:02,080
embody or a centralized log system for

114
00:05:02,080 --> 00:05:04,880
cluster monitoring

115
00:05:04,880 --> 00:05:07,520
so this is an example of a real big data

116
00:05:07,520 --> 00:05:09,680
architecture and how the components

117
00:05:09,680 --> 00:05:11,840
interact with each other

118
00:05:11,840 --> 00:05:14,800
so back to security how can we analyze

119
00:05:14,800 --> 00:05:17,919
these complex infrastructures

120
00:05:17,919 --> 00:05:19,440
well i would like to propose a

121
00:05:19,440 --> 00:05:22,240
methodology for this where the analysis

122
00:05:22,240 --> 00:05:24,479
is based on the different layers of the

123
00:05:24,479 --> 00:05:26,479
big data stack

124
00:05:26,479 --> 00:05:28,880
i think that a good way to analyze big

125
00:05:28,880 --> 00:05:32,000
data infrastructure is to dissect them

126
00:05:32,000 --> 00:05:34,000
and analyze the security of the

127
00:05:34,000 --> 00:05:36,800
components layer by layer

128
00:05:36,800 --> 00:05:39,120
so in this way we can make sure that we

129
00:05:39,120 --> 00:05:41,520
are covering all the stations that the

130
00:05:41,520 --> 00:05:43,440
information we want to protect goes

131
00:05:43,440 --> 00:05:45,039
through

132
00:05:45,039 --> 00:05:47,280
so from now on i will explain different

133
00:05:47,280 --> 00:05:49,520
attack vectors that i discovered

134
00:05:49,520 --> 00:05:51,840
throughout this research for each of the

135
00:05:51,840 --> 00:05:54,320
layers

136
00:05:54,639 --> 00:05:56,639
good so let's start with the management

137
00:05:56,639 --> 00:05:58,960
layer

138
00:05:58,960 --> 00:06:01,759
zookeeper as a set is widely used to

139
00:06:01,759 --> 00:06:03,919
centralize the configuration of the

140
00:06:03,919 --> 00:06:06,160
different technologies that make up the

141
00:06:06,160 --> 00:06:07,520
cluster

142
00:06:07,520 --> 00:06:10,000
this architecture is pretty simple it

143
00:06:10,000 --> 00:06:13,440
runs a service on all nodes and then a

144
00:06:13,440 --> 00:06:16,000
client let's say and

145
00:06:16,000 --> 00:06:17,600
cluster administrator

146
00:06:17,600 --> 00:06:20,000
can connect to one of the nodes and

147
00:06:20,000 --> 00:06:21,919
update a configuration

148
00:06:21,919 --> 00:06:23,919
so when that happens zookeeper will

149
00:06:23,919 --> 00:06:26,080
automatically broadcast the change

150
00:06:26,080 --> 00:06:28,880
across all the nodes

151
00:06:28,880 --> 00:06:31,680
if we scan a node of the cluster we will

152
00:06:31,680 --> 00:06:33,240
find the ports

153
00:06:33,240 --> 00:06:36,319
2181 and 3888

154
00:06:36,319 --> 00:06:37,840
are open

155
00:06:37,840 --> 00:06:40,720
this port belongs to zookeeper

156
00:06:40,720 --> 00:06:43,759
the port 2181 is the the port that

157
00:06:43,759 --> 00:06:46,880
accepts connection from clients

158
00:06:46,880 --> 00:06:48,400
should we be able to

159
00:06:48,400 --> 00:06:49,919
connect to it

160
00:06:49,919 --> 00:06:51,759
well according to the official

161
00:06:51,759 --> 00:06:54,319
documentation of fanberry a tool that is

162
00:06:54,319 --> 00:06:56,720
widely used for deploying on-prem big

163
00:06:56,720 --> 00:06:58,560
data clusters

164
00:06:58,560 --> 00:07:02,160
the 751 is a requirement for installing

165
00:07:02,160 --> 00:07:04,319
big data clusters

166
00:07:04,319 --> 00:07:07,440
so we can probably connect to it

167
00:07:07,440 --> 00:07:09,840
how should we do it and

168
00:07:09,840 --> 00:07:11,840
we can download this zookeeper client

169
00:07:11,840 --> 00:07:14,319
from the official website and then it's

170
00:07:14,319 --> 00:07:16,160
just about running this command

171
00:07:16,160 --> 00:07:18,960
specifying the node ip address and the

172
00:07:18,960 --> 00:07:21,440
2181 port

173
00:07:21,440 --> 00:07:24,240
so once with connect as we run the hub

174
00:07:24,240 --> 00:07:25,440
format

175
00:07:25,440 --> 00:07:27,840
there is a list of actions that we can

176
00:07:27,840 --> 00:07:31,120
execute over the c nodes the signals or

177
00:07:31,120 --> 00:07:33,919
zookeeper nodes are the configurations

178
00:07:33,919 --> 00:07:36,319
that zookeeper organizes in a

179
00:07:36,319 --> 00:07:39,759
hierarchical structure

180
00:07:40,560 --> 00:07:41,360
so

181
00:07:41,360 --> 00:07:44,160
with the ls and get commands we can

182
00:07:44,160 --> 00:07:46,879
browse this hierarchy card structure

183
00:07:46,879 --> 00:07:49,039
and we can actually find it very

184
00:07:49,039 --> 00:07:50,800
interesting information about the

185
00:07:50,800 --> 00:07:53,599
configuration of all the components that

186
00:07:53,599 --> 00:07:57,840
make up the cluster like hadoop height 8

187
00:07:57,840 --> 00:07:59,919
kafka or whatever

188
00:07:59,919 --> 00:08:03,599
and we could use it for farted attacks

189
00:08:03,599 --> 00:08:06,319
we can also create new configurations

190
00:08:06,319 --> 00:08:09,120
modify existing ones and delete

191
00:08:09,120 --> 00:08:10,800
configurations

192
00:08:10,800 --> 00:08:12,639
and this will be a problem for the

193
00:08:12,639 --> 00:08:16,639
cluster some components might go down

194
00:08:16,639 --> 00:08:19,199
to keeper for example is commonly used

195
00:08:19,199 --> 00:08:21,919
to manage the hadoop high availability

196
00:08:21,919 --> 00:08:24,160
so if we delete everything

197
00:08:24,160 --> 00:08:27,039
the cluster might run into troubles

198
00:08:27,039 --> 00:08:28,720
so i wouldn't

199
00:08:28,720 --> 00:08:30,960
not do a demo of this attack because

200
00:08:30,960 --> 00:08:33,039
it's pretty simple but

201
00:08:33,039 --> 00:08:36,320
this is quite impactful

202
00:08:36,320 --> 00:08:39,120
so what about ambari this is a pretty

203
00:08:39,120 --> 00:08:42,159
popular open source tool to install and

204
00:08:42,159 --> 00:08:46,000
manage big data clusters

205
00:08:46,880 --> 00:08:48,880
it has a dashboard from which you can

206
00:08:48,880 --> 00:08:50,800
control everything

207
00:08:50,800 --> 00:08:53,040
whose default credentials are admin

208
00:08:53,040 --> 00:08:54,880
admin of course

209
00:08:54,880 --> 00:08:57,519
but but if they were changed

210
00:08:57,519 --> 00:08:59,839
there is a second word that is

211
00:08:59,839 --> 00:09:02,399
absolutely worried to check

212
00:09:02,399 --> 00:09:05,440
embody uses a positive database to store

213
00:09:05,440 --> 00:09:07,680
the statistics and information about the

214
00:09:07,680 --> 00:09:08,880
cluster

215
00:09:08,880 --> 00:09:11,200
and in the default installation process

216
00:09:11,200 --> 00:09:14,160
the embody wizard asks you to change the

217
00:09:14,160 --> 00:09:16,399
credential for this dashboard but it

218
00:09:16,399 --> 00:09:18,320
doesn't ask you to change the default

219
00:09:18,320 --> 00:09:21,040
credential for the database

220
00:09:21,040 --> 00:09:23,200
so we could simply connect to the

221
00:09:23,200 --> 00:09:26,160
postgres port directly

222
00:09:26,160 --> 00:09:28,800
using these default credentials there

223
00:09:28,800 --> 00:09:32,640
are user and body and password big data

224
00:09:32,640 --> 00:09:35,680
and explore the and varied database

225
00:09:35,680 --> 00:09:38,800
and there we will find these two tables

226
00:09:38,800 --> 00:09:43,200
user authentications and users

227
00:09:43,200 --> 00:09:45,760
so to get the username and

228
00:09:45,760 --> 00:09:48,399
authentication key at once

229
00:09:48,399 --> 00:09:50,800
we need to do this in a showing query

230
00:09:50,800 --> 00:09:53,120
between those two tables

231
00:09:53,120 --> 00:09:55,839
so the authentication key is a status

232
00:09:55,839 --> 00:09:56,880
hash

233
00:09:56,880 --> 00:09:58,880
so the best thing that we can do here is

234
00:09:58,880 --> 00:10:01,760
just update the key for the admin user

235
00:10:01,760 --> 00:10:03,120
for example

236
00:10:03,120 --> 00:10:05,600
i dug into the embodied source code to

237
00:10:05,600 --> 00:10:08,880
find a valid saturn hash and here we

238
00:10:08,880 --> 00:10:12,399
have the hash for the admin password

239
00:10:12,399 --> 00:10:15,120
so now we can run an update query

240
00:10:15,120 --> 00:10:17,360
and once done we can log into the

241
00:10:17,360 --> 00:10:19,440
embedded dashboard with the admin admin

242
00:10:19,440 --> 00:10:21,120
credentials

243
00:10:21,120 --> 00:10:24,000
i know this example pretty stupid but

244
00:10:24,000 --> 00:10:26,560
it's absolutely worth to check because

245
00:10:26,560 --> 00:10:29,120
embody controls the whole cluster if you

246
00:10:29,120 --> 00:10:31,360
can access this dashboard you can do

247
00:10:31,360 --> 00:10:33,839
whatever you want over the customer

248
00:10:33,839 --> 00:10:36,000
and as the default installation process

249
00:10:36,000 --> 00:10:38,399
doesn't ask for these credentials to be

250
00:10:38,399 --> 00:10:40,720
changed then you can most likely

251
00:10:40,720 --> 00:10:44,399
compromise them in this way

252
00:10:44,560 --> 00:10:46,000
good so the important thing in the

253
00:10:46,000 --> 00:10:48,560
cluster management layer is to analyze

254
00:10:48,560 --> 00:10:50,720
the security of the administration and

255
00:10:50,720 --> 00:10:52,480
monitoring tools

256
00:10:52,480 --> 00:10:56,959
let's now talk about the storage layer

257
00:10:57,200 --> 00:10:59,600
first of all it's good to understand how

258
00:10:59,600 --> 00:11:01,680
hadoop works

259
00:11:01,680 --> 00:11:05,200
it has a master slave architecture

260
00:11:05,200 --> 00:11:08,959
and two main components the hdf that

261
00:11:08,959 --> 00:11:11,920
means the hadoop distributed file system

262
00:11:11,920 --> 00:11:13,680
and yarn

263
00:11:13,680 --> 00:11:16,000
the adobe distributed file system has

264
00:11:16,000 --> 00:11:17,680
two main components

265
00:11:17,680 --> 00:11:20,800
nano that saves the metadata of the

266
00:11:20,800 --> 00:11:23,279
files stored in the cluster and runs in

267
00:11:23,279 --> 00:11:24,959
the master nodes

268
00:11:24,959 --> 00:11:27,600
and the data node that stores the actual

269
00:11:27,600 --> 00:11:31,600
data and runs in the slaves nodes

270
00:11:31,600 --> 00:11:33,839
on the other hand yarn consists of two

271
00:11:33,839 --> 00:11:35,360
components as well

272
00:11:35,360 --> 00:11:38,000
the resource manager located on the

273
00:11:38,000 --> 00:11:40,000
master nodes

274
00:11:40,000 --> 00:11:41,440
that controls all the processing

275
00:11:41,440 --> 00:11:44,000
resources in the hadoop cluster and the

276
00:11:44,000 --> 00:11:47,120
no manager installing the place nodes

277
00:11:47,120 --> 00:11:49,279
that take care of tracking

278
00:11:49,279 --> 00:11:52,320
processing resources on its slave node

279
00:11:52,320 --> 00:11:54,880
among other stacks

280
00:11:54,880 --> 00:11:58,639
basically what we have to know is that

281
00:11:58,639 --> 00:12:00,399
the hadoop file system

282
00:12:00,399 --> 00:12:01,519
um

283
00:12:01,519 --> 00:12:03,120
is where the cluster information is

284
00:12:03,120 --> 00:12:05,920
stored and yarn is a service that

285
00:12:05,920 --> 00:12:09,279
manages the resources for the processing

286
00:12:09,279 --> 00:12:11,279
shops that are executed over the

287
00:12:11,279 --> 00:12:12,880
information store

288
00:12:12,880 --> 00:12:15,600
basically is that

289
00:12:15,600 --> 00:12:18,079
so when it comes to the storage layer we

290
00:12:18,079 --> 00:12:20,399
are interested in the hadoop file system

291
00:12:20,399 --> 00:12:22,639
so let's see how we can

292
00:12:22,639 --> 00:12:26,079
remotely compromise it

293
00:12:26,240 --> 00:12:31,279
hadoop exposes an ipc port on 80 20.

294
00:12:31,279 --> 00:12:33,920
so we shall find this port open in the

295
00:12:33,920 --> 00:12:36,560
hadoop name nodes

296
00:12:36,560 --> 00:12:37,360
so

297
00:12:37,360 --> 00:12:39,760
if we can connect we could execute

298
00:12:39,760 --> 00:12:42,320
hadoop commands and access to the stored

299
00:12:42,320 --> 00:12:43,600
data

300
00:12:43,600 --> 00:12:46,000
however this is not as simple as the

301
00:12:46,000 --> 00:12:47,839
zookeeper example was

302
00:12:47,839 --> 00:12:49,920
managed to do this is a little more

303
00:12:49,920 --> 00:12:51,600
complex

304
00:12:51,600 --> 00:12:54,320
there are four configuration files that

305
00:12:54,320 --> 00:12:57,120
hadoop needs to perform operations over

306
00:12:57,120 --> 00:12:59,519
a hadoop file system

307
00:12:59,519 --> 00:13:01,680
if we take a look at these files inside

308
00:13:01,680 --> 00:13:04,399
the name node we can see that

309
00:13:04,399 --> 00:13:06,160
they have end of configuration

310
00:13:06,160 --> 00:13:07,600
parameters

311
00:13:07,600 --> 00:13:10,639
so when i saw that i wonder if i am an

312
00:13:10,639 --> 00:13:13,200
attacker and i don't have access to

313
00:13:13,200 --> 00:13:15,440
these files how can i compromise the

314
00:13:15,440 --> 00:13:18,320
file system in a remote way

315
00:13:18,320 --> 00:13:21,200
well part of this research was to find

316
00:13:21,200 --> 00:13:24,240
among those dozens configurations which

317
00:13:24,240 --> 00:13:27,279
ones are a hundred percent required and

318
00:13:27,279 --> 00:13:29,680
how we can get them remotely from the

319
00:13:29,680 --> 00:13:32,160
information that hadoop itself discloses

320
00:13:32,160 --> 00:13:32,839
by

321
00:13:32,839 --> 00:13:36,000
default so i will explain how we can

322
00:13:36,000 --> 00:13:40,639
manually craft these files one by one

323
00:13:40,639 --> 00:13:45,360
let's start by the coresight xml file

324
00:13:45,680 --> 00:13:49,040
the only information we need here is

325
00:13:49,040 --> 00:13:51,440
the namespace

326
00:13:51,440 --> 00:13:53,920
and it's pretty easy to to find this

327
00:13:53,920 --> 00:13:56,399
information because hadoop is posted by

328
00:13:56,399 --> 00:13:58,880
default dashboard on the name notes on

329
00:13:58,880 --> 00:14:00,480
the board

330
00:14:00,480 --> 00:14:03,760
50 2017 pretty high port

331
00:14:03,760 --> 00:14:05,360
and we can access it with our

332
00:14:05,360 --> 00:14:06,880
authentication

333
00:14:06,880 --> 00:14:10,240
so here we can find the namespace

334
00:14:10,240 --> 00:14:13,120
side in my target cluster

335
00:14:13,120 --> 00:14:17,279
and that's all we need for this file

336
00:14:17,279 --> 00:14:21,120
then we need to grab the hdf site file

337
00:14:21,120 --> 00:14:22,160
and

338
00:14:22,160 --> 00:14:24,000
it's necessary to know the namespace

339
00:14:24,000 --> 00:14:25,920
that we already have from the previous

340
00:14:25,920 --> 00:14:28,639
file and we also need the names nodes

341
00:14:28,639 --> 00:14:32,079
ids and the dns of them

342
00:14:32,079 --> 00:14:34,800
we could have one two or more name nodes

343
00:14:34,800 --> 00:14:37,760
and we need to provide the id and dns

344
00:14:37,760 --> 00:14:40,560
for all of them in this file

345
00:14:40,560 --> 00:14:43,519
where can we get this information

346
00:14:43,519 --> 00:14:45,440
from the same dashboard

347
00:14:45,440 --> 00:14:49,199
we have the namespace and the name of id

348
00:14:49,199 --> 00:14:51,199
and the dns here

349
00:14:51,199 --> 00:14:53,600
we just need to access this dashboard on

350
00:14:53,600 --> 00:14:55,360
each name

351
00:14:55,360 --> 00:14:58,720
and remember that it's on the port

352
00:14:58,720 --> 00:15:01,360
50 070.

353
00:15:01,360 --> 00:15:03,839
another alternative is to enter the data

354
00:15:03,839 --> 00:15:08,600
now dashboard it's on board 5075

355
00:15:09,040 --> 00:15:11,600
and there we can see all the names nodes

356
00:15:11,600 --> 00:15:14,160
at once

357
00:15:14,160 --> 00:15:16,079
good um

358
00:15:16,079 --> 00:15:19,120
the next file is the map right side one

359
00:15:19,120 --> 00:15:21,760
here we need the dns of the nano that

360
00:15:21,760 --> 00:15:24,800
holds the mapreduce shop history

361
00:15:24,800 --> 00:15:27,440
we can try to access the port

362
00:15:27,440 --> 00:15:29,040
1988

363
00:15:29,040 --> 00:15:31,360
on the name node and if we can see this

364
00:15:31,360 --> 00:15:33,839
dashboard then that's the name that we

365
00:15:33,839 --> 00:15:35,360
are looking for

366
00:15:35,360 --> 00:15:38,000
and we already know it's dns from the

367
00:15:38,000 --> 00:15:41,399
previous dashboard

368
00:15:41,519 --> 00:15:43,519
finally we need to craft the yarn site

369
00:15:43,519 --> 00:15:44,560
file

370
00:15:44,560 --> 00:15:47,519
again we need a name of dns in this case

371
00:15:47,519 --> 00:15:50,399
the one that holds the yarn resource

372
00:15:50,399 --> 00:15:51,920
manager

373
00:15:51,920 --> 00:15:54,480
and to detect that we can try to access

374
00:15:54,480 --> 00:15:55,720
the port

375
00:15:55,720 --> 00:15:57,440
8088

376
00:15:57,440 --> 00:15:59,920
and if we see this dashboard then that's

377
00:15:59,920 --> 00:16:01,519
the right now

378
00:16:01,519 --> 00:16:04,720
and here we can get the dns as well

379
00:16:04,720 --> 00:16:05,440
so

380
00:16:05,440 --> 00:16:07,680
all these dashboards are exposed by

381
00:16:07,680 --> 00:16:09,519
default and don't require any

382
00:16:09,519 --> 00:16:12,160
authentication but if for some reasons

383
00:16:12,160 --> 00:16:14,720
we can see them we can try to get this

384
00:16:14,720 --> 00:16:17,199
required information through zookeeper

385
00:16:17,199 --> 00:16:20,720
with the attack i showed you earlier

386
00:16:20,720 --> 00:16:22,959
because zookeeper also has all this

387
00:16:22,959 --> 00:16:25,599
information

388
00:16:26,000 --> 00:16:28,160
cool so once we have the configuration

389
00:16:28,160 --> 00:16:30,399
files we need the next step is to

390
00:16:30,399 --> 00:16:32,399
install hadoop in our local machine and

391
00:16:32,399 --> 00:16:34,800
provide it with those files

392
00:16:34,800 --> 00:16:38,240
to perform the remote communication

393
00:16:38,240 --> 00:16:40,079
as i didn't want to install hadoop on my

394
00:16:40,079 --> 00:16:43,440
local machine i built this docker file

395
00:16:43,440 --> 00:16:45,040
feel free to use it

396
00:16:45,040 --> 00:16:46,800
it's pretty comfortable

397
00:16:46,800 --> 00:16:49,040
should change the hadoop version

398
00:16:49,040 --> 00:16:51,040
to match the version of your target

399
00:16:51,040 --> 00:16:53,120
cluster right

400
00:16:53,120 --> 00:16:55,600
so from now on this is going to be our

401
00:16:55,600 --> 00:16:57,920
hadoop hacking container

402
00:16:57,920 --> 00:17:02,000
running on the attacker machine right

403
00:17:02,240 --> 00:17:03,120
so

404
00:17:03,120 --> 00:17:05,280
let's run it and get that shell inside

405
00:17:05,280 --> 00:17:06,079
it

406
00:17:06,079 --> 00:17:07,039
and

407
00:17:07,039 --> 00:17:09,839
we can create a config directory to

408
00:17:09,839 --> 00:17:13,119
place the xml files we have crafted

409
00:17:13,119 --> 00:17:14,559
before

410
00:17:14,559 --> 00:17:16,599
and you also need to copy this

411
00:17:16,599 --> 00:17:20,079
lock.property files

412
00:17:21,359 --> 00:17:24,000
another thing i did is was to edit the

413
00:17:24,000 --> 00:17:26,559
host file to rightly resolve this

414
00:17:26,559 --> 00:17:28,799
namenode dns

415
00:17:28,799 --> 00:17:31,520
you can use the ip addresses on the xml

416
00:17:31,520 --> 00:17:32,559
files

417
00:17:32,559 --> 00:17:36,879
but i had better results doing this

418
00:17:37,440 --> 00:17:39,679
okay so we are ready to go

419
00:17:39,679 --> 00:17:41,919
just pass to hadoop this config

420
00:17:41,919 --> 00:17:44,960
directory and execute an ls command for

421
00:17:44,960 --> 00:17:48,400
example so voila we can see the entire

422
00:17:48,400 --> 00:17:51,200
hadoop file system from a remote

423
00:17:51,200 --> 00:17:53,280
attacker machine

424
00:17:53,280 --> 00:17:55,760
so before jumping into a demo of this i

425
00:17:55,760 --> 00:17:58,320
would like to mention that most likely

426
00:17:58,320 --> 00:18:02,160
we will need to impersonate hdf users

427
00:18:02,160 --> 00:18:04,960
for example if i try to create a new

428
00:18:04,960 --> 00:18:07,760
directory using the root user i cannot

429
00:18:07,760 --> 00:18:09,039
do it

430
00:18:09,039 --> 00:18:11,520
so we need to impersonate

431
00:18:11,520 --> 00:18:13,679
a user that has privileges within the

432
00:18:13,679 --> 00:18:15,360
hadoop file system

433
00:18:15,360 --> 00:18:18,640
so one of these ones

434
00:18:18,640 --> 00:18:21,039
fortunately that's very easy to do we

435
00:18:21,039 --> 00:18:23,200
just need to set this environment

436
00:18:23,200 --> 00:18:24,240
variable

437
00:18:24,240 --> 00:18:26,400
with the hadoop username before the

438
00:18:26,400 --> 00:18:29,760
command and that's all that will allow

439
00:18:29,760 --> 00:18:31,280
us to

440
00:18:31,280 --> 00:18:33,360
for example create a new directory and

441
00:18:33,360 --> 00:18:36,240
we can also delete directories and files

442
00:18:36,240 --> 00:18:37,440
of course

443
00:18:37,440 --> 00:18:40,240
and we could actually wipe out the

444
00:18:40,240 --> 00:18:43,200
entire cluster information

445
00:18:43,200 --> 00:18:47,120
okay let's see a demo of this

446
00:18:48,160 --> 00:18:51,039
so here i am in my

447
00:18:51,039 --> 00:18:53,039
container and

448
00:18:53,039 --> 00:18:56,960
these are my files this is the yeah hdl

449
00:18:56,960 --> 00:19:00,559
files i have to place the the namespace

450
00:19:00,559 --> 00:19:03,360
and several places and in in this file

451
00:19:03,360 --> 00:19:05,760
you need to also provide the

452
00:19:05,760 --> 00:19:08,880
name nodes ids and dns if you have for

453
00:19:08,880 --> 00:19:10,559
example in this case i have two so i

454
00:19:10,559 --> 00:19:14,320
have to provide the dns for the two uh

455
00:19:14,320 --> 00:19:15,919
name nodes

456
00:19:15,919 --> 00:19:19,840
and for the map red side file and we

457
00:19:19,840 --> 00:19:23,120
also need to provide a dns

458
00:19:23,120 --> 00:19:25,600
for the map shop uh history and this is

459
00:19:25,600 --> 00:19:27,760
the yarn file that also needs to provide

460
00:19:27,760 --> 00:19:30,480
the dns for the yar resource manager

461
00:19:30,480 --> 00:19:32,160
node

462
00:19:32,160 --> 00:19:34,559
so with that we can execute

463
00:19:34,559 --> 00:19:37,679
commands hadoop commands to

464
00:19:37,679 --> 00:19:39,280
browse through the

465
00:19:39,280 --> 00:19:41,600
hadoop file system

466
00:19:41,600 --> 00:19:43,840
and we execute the help command we can

467
00:19:43,840 --> 00:19:45,520
see that there are a lot of

468
00:19:45,520 --> 00:19:48,240
common commands that we can execute like

469
00:19:48,240 --> 00:19:52,000
copy files or remove files

470
00:19:52,000 --> 00:19:53,679
most common commands that we can find in

471
00:19:53,679 --> 00:19:55,919
a unix system

472
00:19:55,919 --> 00:19:57,760
and here while i'm making the

473
00:19:57,760 --> 00:20:00,559
impersonation for the

474
00:20:00,559 --> 00:20:04,840
hdf user so i can create a new

475
00:20:04,840 --> 00:20:06,720
directory

476
00:20:06,720 --> 00:20:10,240
and also i could delete the directory of

477
00:20:10,240 --> 00:20:11,280
course

478
00:20:11,280 --> 00:20:12,960
so basically we can perform any

479
00:20:12,960 --> 00:20:15,840
operation over the hadoop file system in

480
00:20:15,840 --> 00:20:17,760
a remote way because i'm executing it

481
00:20:17,760 --> 00:20:19,520
from a

482
00:20:19,520 --> 00:20:21,360
hadoop hacking container that has

483
00:20:21,360 --> 00:20:23,760
nothing to do with the original cluster

484
00:20:23,760 --> 00:20:26,000
right

485
00:20:26,080 --> 00:20:27,200
good

486
00:20:27,200 --> 00:20:29,360
okay so um

487
00:20:29,360 --> 00:20:33,120
let's jump into the processing layer and

488
00:20:33,120 --> 00:20:38,000
see how we can abuse yarn in this case

489
00:20:38,159 --> 00:20:40,720
so back to the hadoop architecture just

490
00:20:40,720 --> 00:20:42,159
to remember

491
00:20:42,159 --> 00:20:45,120
yar takes care of the processing shops

492
00:20:45,120 --> 00:20:48,480
over the data stored in the cluster

493
00:20:48,480 --> 00:20:50,640
this shop executes code in the data

494
00:20:50,640 --> 00:20:53,440
nodes so our mission here is trying to

495
00:20:53,440 --> 00:20:55,840
find a way to remotely submitting an

496
00:20:55,840 --> 00:20:58,960
application to yarn that executes a code

497
00:20:58,960 --> 00:21:01,120
or command that we want to execute in

498
00:21:01,120 --> 00:21:03,120
the clusters node

499
00:21:03,120 --> 00:21:05,280
basically achieve a remote code

500
00:21:05,280 --> 00:21:08,400
execution through vr

501
00:21:08,400 --> 00:21:11,200
and we can use the hadoop ipc that we

502
00:21:11,200 --> 00:21:14,400
were using in the previous attack

503
00:21:14,400 --> 00:21:16,799
it's just necessary to improve a little

504
00:21:16,799 --> 00:21:19,919
bit our yarn site file

505
00:21:19,919 --> 00:21:22,960
we need to add the yarn application

506
00:21:22,960 --> 00:21:24,960
class file properly

507
00:21:24,960 --> 00:21:26,080
this path

508
00:21:26,080 --> 00:21:28,240
used to be the default path in hadoop

509
00:21:28,240 --> 00:21:30,559
installations so it should not be

510
00:21:30,559 --> 00:21:33,919
difficult to obtain this information

511
00:21:33,919 --> 00:21:36,159
in the example here we can see the

512
00:21:36,159 --> 00:21:39,200
default path for installations using the

513
00:21:39,200 --> 00:21:40,960
hortonwork

514
00:21:40,960 --> 00:21:43,440
packages

515
00:21:43,440 --> 00:21:46,400
then this other property is optional it

516
00:21:46,400 --> 00:21:49,039
will specify the application's output

517
00:21:49,039 --> 00:21:51,840
parts in the hadoop file system

518
00:21:51,840 --> 00:21:54,480
it might be useful for us to easily find

519
00:21:54,480 --> 00:21:56,720
the output of the remote code execution

520
00:21:56,720 --> 00:21:59,280
but it's not necessary

521
00:21:59,280 --> 00:22:00,960
and something i would like to mention

522
00:22:00,960 --> 00:22:04,320
that i did inside before

523
00:22:04,480 --> 00:22:06,880
if you can access these panels that we

524
00:22:06,880 --> 00:22:08,320
have seen

525
00:22:08,320 --> 00:22:10,240
under slash conf

526
00:22:10,240 --> 00:22:11,919
we can find all the configuration

527
00:22:11,919 --> 00:22:14,000
parameters but you cannot choose

528
00:22:14,000 --> 00:22:16,080
download and use this file

529
00:22:16,080 --> 00:22:18,320
we still need to manually craft the

530
00:22:18,320 --> 00:22:21,520
files the way we were doing it

531
00:22:21,520 --> 00:22:23,679
however if something is not working for

532
00:22:23,679 --> 00:22:28,480
you here you might find what's missing

533
00:22:28,480 --> 00:22:30,720
here we can see that for example we can

534
00:22:30,720 --> 00:22:33,039
get the yarn application class pads that

535
00:22:33,039 --> 00:22:35,440
we need

536
00:22:35,919 --> 00:22:39,520
okay so now that we have improved our

537
00:22:39,520 --> 00:22:41,679
yarn site file and we can submit

538
00:22:41,679 --> 00:22:44,159
applications to yarn the question is

539
00:22:44,159 --> 00:22:47,520
what applications should we submit

540
00:22:47,520 --> 00:22:50,640
here hortonworks provides a simple one

541
00:22:50,640 --> 00:22:52,960
that is enough to achieve the remote

542
00:22:52,960 --> 00:22:55,520
code execution that we want

543
00:22:55,520 --> 00:22:58,320
it has only three java files

544
00:22:58,320 --> 00:23:00,640
because yara applications are developed

545
00:23:00,640 --> 00:23:02,480
in java

546
00:23:02,480 --> 00:23:03,200
but

547
00:23:03,200 --> 00:23:04,880
there are a lot of hadoop libraries

548
00:23:04,880 --> 00:23:07,520
necessary to include and use so it might

549
00:23:07,520 --> 00:23:10,240
not be easy to develop a native vr

550
00:23:10,240 --> 00:23:11,440
application

551
00:23:11,440 --> 00:23:15,280
but we can use this one for our purpose

552
00:23:15,280 --> 00:23:17,840
it takes us parameter the command that

553
00:23:17,840 --> 00:23:20,559
we want to execute on the cluster and

554
00:23:20,559 --> 00:23:22,640
the number of instances which is

555
00:23:22,640 --> 00:23:25,840
basically on how many nodes our command

556
00:23:25,840 --> 00:23:28,959
will be executed

557
00:23:29,360 --> 00:23:32,159
so um we will clone this repository in

558
00:23:32,159 --> 00:23:34,880
our help hacking container and proceed

559
00:23:34,880 --> 00:23:38,240
to compile this java application

560
00:23:38,240 --> 00:23:40,400
we need to edit the

561
00:23:40,400 --> 00:23:43,440
poem xml file

562
00:23:43,440 --> 00:23:45,919
and change the hadoop version to match

563
00:23:45,919 --> 00:23:49,039
the version of our target this is pretty

564
00:23:49,039 --> 00:23:53,120
important otherwise this will not work

565
00:23:53,120 --> 00:23:56,799
so remember to match the version and one

566
00:23:56,799 --> 00:23:59,360
stand we can compile the application

567
00:23:59,360 --> 00:24:02,240
using maven

568
00:24:02,880 --> 00:24:05,840
and the next step is to copy the compile

569
00:24:05,840 --> 00:24:09,120
chart into the remote hadoop file system

570
00:24:09,120 --> 00:24:12,880
we can do it using the copy from local

571
00:24:12,880 --> 00:24:15,440
hdf command

572
00:24:15,440 --> 00:24:17,679
and now we are ready to go

573
00:24:17,679 --> 00:24:20,480
so in this way and we can submit the

574
00:24:20,480 --> 00:24:23,279
application to yarn passing as parameter

575
00:24:23,279 --> 00:24:25,360
the command we want to execute

576
00:24:25,360 --> 00:24:29,039
and the number of instances

577
00:24:29,039 --> 00:24:31,520
here an example i have executed the

578
00:24:31,520 --> 00:24:35,760
hostname command of a free node

579
00:24:35,760 --> 00:24:38,000
and we are going to receive an

580
00:24:38,000 --> 00:24:40,320
application id it's important to take

581
00:24:40,320 --> 00:24:41,679
note of it

582
00:24:41,679 --> 00:24:42,640
and

583
00:24:42,640 --> 00:24:45,200
it's even more important to get this

584
00:24:45,200 --> 00:24:46,880
finished status

585
00:24:46,880 --> 00:24:49,039
and that means that our application was

586
00:24:49,039 --> 00:24:52,400
executed successfully

587
00:24:52,400 --> 00:24:53,600
and now what

588
00:24:53,600 --> 00:24:57,600
where can we see the application output

589
00:24:57,600 --> 00:25:00,400
well we can use this command passing the

590
00:25:00,400 --> 00:25:02,480
application id we got in the previous

591
00:25:02,480 --> 00:25:03,679
step

592
00:25:03,679 --> 00:25:06,400
and the output is going to be something

593
00:25:06,400 --> 00:25:07,840
like this

594
00:25:07,840 --> 00:25:10,080
and as we have executed this command

595
00:25:10,080 --> 00:25:12,559
over three nodes and we have three

596
00:25:12,559 --> 00:25:15,039
different outputs for the hostname

597
00:25:15,039 --> 00:25:17,200
command

598
00:25:17,200 --> 00:25:18,880
of course we can change the hosting

599
00:25:18,880 --> 00:25:22,320
command for any other right

600
00:25:22,320 --> 00:25:26,158
so let's see a demo of this

601
00:25:26,720 --> 00:25:29,520
here again i have my files and i have

602
00:25:29,520 --> 00:25:32,000
performed a little modification over the

603
00:25:32,000 --> 00:25:34,799
er file right

604
00:25:34,799 --> 00:25:36,400
and i have

605
00:25:36,400 --> 00:25:38,640
the simple er application this is the

606
00:25:38,640 --> 00:25:42,000
hortonworks example

607
00:25:42,000 --> 00:25:44,320
and i already copied this file into the

608
00:25:44,320 --> 00:25:47,120
remote hadoop file system

609
00:25:47,120 --> 00:25:49,440
so the only thing i have to do now is

610
00:25:49,440 --> 00:25:50,640
just to

611
00:25:50,640 --> 00:25:53,440
execute execute it

612
00:25:53,440 --> 00:25:56,080
and for that we need to pass the local

613
00:25:56,080 --> 00:25:58,080
path for the application

614
00:25:58,080 --> 00:26:00,000
the command that we want to execute the

615
00:26:00,000 --> 00:26:02,559
number of instances and the remote path

616
00:26:02,559 --> 00:26:05,120
of the chart

617
00:26:05,679 --> 00:26:07,360
so once done

618
00:26:07,360 --> 00:26:10,640
we get the application id and the finish

619
00:26:10,640 --> 00:26:13,200
status

620
00:26:15,919 --> 00:26:18,240
good so take note of that

621
00:26:18,240 --> 00:26:19,360
id

622
00:26:19,360 --> 00:26:22,880
in my case i need to perfora copy

623
00:26:22,880 --> 00:26:26,240
just to allow channel to then um find

624
00:26:26,240 --> 00:26:27,600
the the output

625
00:26:27,600 --> 00:26:29,760
so i just copy the the output to a

626
00:26:29,760 --> 00:26:31,520
different folder in the remote file

627
00:26:31,520 --> 00:26:32,559
system

628
00:26:32,559 --> 00:26:35,600
and now we can use the yarn command with

629
00:26:35,600 --> 00:26:38,640
the application id to get the real

630
00:26:38,640 --> 00:26:41,039
output

631
00:26:42,080 --> 00:26:43,039
so

632
00:26:43,039 --> 00:26:45,679
um this is the output and here we can

633
00:26:45,679 --> 00:26:46,400
see

634
00:26:46,400 --> 00:26:48,960
the output of the hostname command for

635
00:26:48,960 --> 00:26:50,159
how to one

636
00:26:50,159 --> 00:26:52,799
the first node hadoop2

637
00:26:52,799 --> 00:26:54,320
and hadoop3

638
00:26:54,320 --> 00:26:57,520
because we have executed it over three

639
00:26:57,520 --> 00:26:59,840
data nodes

640
00:26:59,840 --> 00:27:03,039
and let me show just one more output of

641
00:27:03,039 --> 00:27:05,279
an application i run before

642
00:27:05,279 --> 00:27:07,039
so you can see

643
00:27:07,039 --> 00:27:11,279
for example i have run a command to

644
00:27:11,279 --> 00:27:14,240
dump the information of a file in the

645
00:27:14,240 --> 00:27:16,960
head of machines in the

646
00:27:16,960 --> 00:27:19,200
data nodes so here we can get for

647
00:27:19,200 --> 00:27:22,399
example the password files

648
00:27:23,360 --> 00:27:25,279
good

649
00:27:25,279 --> 00:27:26,960
so basically we can achieve our remote

650
00:27:26,960 --> 00:27:28,399
code execution

651
00:27:28,399 --> 00:27:29,440
using

652
00:27:29,440 --> 00:27:32,320
yarn the yarn will always be present in

653
00:27:32,320 --> 00:27:35,120
hadoop clusters

654
00:27:35,120 --> 00:27:38,000
and if we want to modify that

655
00:27:38,000 --> 00:27:41,279
application yara application is quite

656
00:27:41,279 --> 00:27:42,720
simple

657
00:27:42,720 --> 00:27:44,880
we could modify it to

658
00:27:44,880 --> 00:27:47,600
perhaps execute a more complex command

659
00:27:47,600 --> 00:27:49,600
just keep in mind that any changes must

660
00:27:49,600 --> 00:27:53,120
be made in both the application master

661
00:27:53,120 --> 00:27:57,200
file and the client file right

662
00:27:57,200 --> 00:27:59,520
um if we want to get something like a

663
00:27:59,520 --> 00:28:02,480
reverse shell on the cluster node it's

664
00:28:02,480 --> 00:28:05,200
possible but keep in mind that this is a

665
00:28:05,200 --> 00:28:08,000
sharp process that starts unfinished

666
00:28:08,000 --> 00:28:11,279
so we might need to use alternative like

667
00:28:11,279 --> 00:28:13,679
backdoor in the contact with

668
00:28:13,679 --> 00:28:16,000
the yarn application for example so you

669
00:28:16,000 --> 00:28:18,559
could modify the application to run this

670
00:28:18,559 --> 00:28:20,320
specific command

671
00:28:20,320 --> 00:28:22,799
and when you submit the application

672
00:28:22,799 --> 00:28:25,200
it will execute this vector for the

673
00:28:25,200 --> 00:28:27,200
ground tab and then you will have a

674
00:28:27,200 --> 00:28:29,520
reverse shell in the

675
00:28:29,520 --> 00:28:31,760
data nodes

676
00:28:31,760 --> 00:28:35,039
this is just an example right

677
00:28:35,120 --> 00:28:37,520
okay and i can't help but talk about

678
00:28:37,520 --> 00:28:39,600
spark in this section

679
00:28:39,600 --> 00:28:41,840
spark is a super popular and well

680
00:28:41,840 --> 00:28:44,159
implemented technology for precision

681
00:28:44,159 --> 00:28:45,600
data

682
00:28:45,600 --> 00:28:47,600
it's generally installed on top of

683
00:28:47,600 --> 00:28:50,559
hadoop and developers make data

684
00:28:50,559 --> 00:28:52,880
processing applications for spark for

685
00:28:52,880 --> 00:28:54,559
example in python

686
00:28:54,559 --> 00:28:57,279
using pyspark because it's easier than

687
00:28:57,279 --> 00:29:00,399
developing a native application for yeah

688
00:29:00,399 --> 00:29:03,520
and also spark has other advantages over

689
00:29:03,520 --> 00:29:05,679
yarn as well

690
00:29:05,679 --> 00:29:11,279
so spark has its own ipc port on 1777

691
00:29:11,279 --> 00:29:13,840
and we call submit and spark application

692
00:29:13,840 --> 00:29:16,080
to be executed on the cluster through

693
00:29:16,080 --> 00:29:17,600
this port

694
00:29:17,600 --> 00:29:20,480
it's easier than with yang

695
00:29:20,480 --> 00:29:23,279
here we have an example and this small

696
00:29:23,279 --> 00:29:26,240
code will connect to the spark master to

697
00:29:26,240 --> 00:29:28,240
execute the hostname command on the

698
00:29:28,240 --> 00:29:30,080
cluster's nodes

699
00:29:30,080 --> 00:29:33,840
so we have to specify the remote spark

700
00:29:33,840 --> 00:29:35,360
ip address

701
00:29:35,360 --> 00:29:38,159
our ip address to receive the output of

702
00:29:38,159 --> 00:29:39,279
the command

703
00:29:39,279 --> 00:29:41,760
and the command itself

704
00:29:41,760 --> 00:29:42,880
then

705
00:29:42,880 --> 00:29:44,320
we can write this script from our

706
00:29:44,320 --> 00:29:45,360
machine

707
00:29:45,360 --> 00:29:47,600
and we don't really need anything else

708
00:29:47,600 --> 00:29:49,919
it's pretty simple

709
00:29:49,919 --> 00:29:52,080
but i'm not going to talking therefore

710
00:29:52,080 --> 00:29:55,120
this because there is already attack 100

711
00:29:55,120 --> 00:29:58,399
dedicated to spark that was given at

712
00:29:58,399 --> 00:30:00,080
defcon last year

713
00:30:00,080 --> 00:30:02,880
i truly recommend watching this talk and

714
00:30:02,880 --> 00:30:04,559
the speaker explains

715
00:30:04,559 --> 00:30:06,840
how to achieve remote code

716
00:30:06,840 --> 00:30:10,720
execution via spark ipc

717
00:30:10,720 --> 00:30:12,960
this is the the equivalent of what we

718
00:30:12,960 --> 00:30:15,039
did with jana

719
00:30:15,039 --> 00:30:16,080
so

720
00:30:16,080 --> 00:30:18,159
keep in mind that spark

721
00:30:18,159 --> 00:30:21,679
may or may not be present in the cluster

722
00:30:21,679 --> 00:30:24,240
while yarn will always be present in a

723
00:30:24,240 --> 00:30:27,440
hadoop installation so it's good to know

724
00:30:27,440 --> 00:30:30,480
how to achieve remote code execution via

725
00:30:30,480 --> 00:30:32,320
and also be a spark

726
00:30:32,320 --> 00:30:34,960
if we have the possibility to abuse this

727
00:30:34,960 --> 00:30:38,080
technology as well

728
00:30:38,880 --> 00:30:40,799
also

729
00:30:40,799 --> 00:30:45,360
let's take a look at the injection layer

730
00:30:45,520 --> 00:30:48,080
if you remember from our big data

731
00:30:48,080 --> 00:30:50,159
architecture example at the beginning of

732
00:30:50,159 --> 00:30:52,720
this talk

733
00:30:52,720 --> 00:30:54,159
we have

734
00:30:54,159 --> 00:30:56,000
sources of data

735
00:30:56,000 --> 00:30:58,880
and such data is injected into our

736
00:30:58,880 --> 00:31:01,440
cluster using data intellection

737
00:31:01,440 --> 00:31:03,519
technologies

738
00:31:03,519 --> 00:31:05,120
there are several ones

739
00:31:05,120 --> 00:31:06,320
um

740
00:31:06,320 --> 00:31:09,360
we have some design for streaming like

741
00:31:09,360 --> 00:31:13,120
floom kafka and spark tractor streaming

742
00:31:13,120 --> 00:31:16,000
that is a variant of spark

743
00:31:16,000 --> 00:31:19,120
and then others like scoop that enhances

744
00:31:19,120 --> 00:31:22,000
static information for example from one

745
00:31:22,000 --> 00:31:25,120
data lake to other data lake or from one

746
00:31:25,120 --> 00:31:30,159
database to a data lake on so on

747
00:31:30,159 --> 00:31:31,360
so

748
00:31:31,360 --> 00:31:33,600
from a security point of view we need to

749
00:31:33,600 --> 00:31:36,480
make sure that these channels that the

750
00:31:36,480 --> 00:31:38,960
information go through from the source

751
00:31:38,960 --> 00:31:42,159
to the storage are secure right

752
00:31:42,159 --> 00:31:44,960
otherwise an attacker might interfere

753
00:31:44,960 --> 00:31:48,640
those channels and inject malicious data

754
00:31:48,640 --> 00:31:52,399
let's see how this will happen

755
00:31:52,720 --> 00:31:55,360
this is how spark streaming or spark

756
00:31:55,360 --> 00:31:58,080
structure streaming works

757
00:31:58,080 --> 00:32:00,480
it's a variant of spark that injects the

758
00:32:00,480 --> 00:32:03,919
data and also process it before dumping

759
00:32:03,919 --> 00:32:06,640
everything into the hadoop file system

760
00:32:06,640 --> 00:32:09,039
so it's like having two components in

761
00:32:09,039 --> 00:32:11,279
one

762
00:32:11,279 --> 00:32:12,320
and

763
00:32:12,320 --> 00:32:14,720
spark streaming um it works with

764
00:32:14,720 --> 00:32:17,279
technologies like kafka fluent and

765
00:32:17,279 --> 00:32:20,320
kinesis to put or receive the data

766
00:32:20,320 --> 00:32:22,960
but it also has the possibility to just

767
00:32:22,960 --> 00:32:26,320
inject data from a tcp socket

768
00:32:26,320 --> 00:32:30,480
and that could be very dangerous

769
00:32:30,480 --> 00:32:32,720
here we have an example of how the code

770
00:32:32,720 --> 00:32:35,440
looks like when the streaming input is

771
00:32:35,440 --> 00:32:37,760
chosen tcp socket

772
00:32:37,760 --> 00:32:39,840
it basically binds a board on the

773
00:32:39,840 --> 00:32:41,120
machine

774
00:32:41,120 --> 00:32:44,640
so abuse this is super easy we can use

775
00:32:44,640 --> 00:32:47,919
netcat or our favorite tool and should

776
00:32:47,919 --> 00:32:50,960
send data over the socket

777
00:32:50,960 --> 00:32:53,039
and it works

778
00:32:53,039 --> 00:32:55,919
but what happens to the data we inject

779
00:32:55,919 --> 00:32:58,000
will depend on the application that

780
00:32:58,000 --> 00:32:59,279
processes it

781
00:32:59,279 --> 00:33:01,279
most likely we will crash the

782
00:33:01,279 --> 00:33:03,279
application because we might be

783
00:33:03,279 --> 00:33:05,919
injecting bytes that the application

784
00:33:05,919 --> 00:33:08,320
doesn't know how to handle

785
00:33:08,320 --> 00:33:11,679
or our bite may end up inside the halo 5

786
00:33:11,679 --> 00:33:12,640
system

787
00:33:12,640 --> 00:33:15,440
that's also likely

788
00:33:15,440 --> 00:33:17,840
so it's important to check uh that the

789
00:33:17,840 --> 00:33:20,399
interfaces that are waiting for data to

790
00:33:20,399 --> 00:33:23,039
be inherited and cannot be reached by an

791
00:33:23,039 --> 00:33:25,840
attacker right

792
00:33:25,919 --> 00:33:28,240
and regarding scope

793
00:33:28,240 --> 00:33:30,880
as i said this moves static data and

794
00:33:30,880 --> 00:33:33,279
it's commonly used to inject information

795
00:33:33,279 --> 00:33:37,440
from different sql database into halo

796
00:33:37,440 --> 00:33:40,799
and analyzing scope server iphone and

797
00:33:40,799 --> 00:33:46,799
api exposed by default on pro 12000

798
00:33:46,799 --> 00:33:49,120
and

799
00:33:49,120 --> 00:33:50,640
we can actually

800
00:33:50,640 --> 00:33:52,960
abuse it and we can get for example the

801
00:33:52,960 --> 00:33:56,000
scoop server version with this query

802
00:33:56,000 --> 00:33:58,640
and then it's easier to use the scoop

803
00:33:58,640 --> 00:34:01,600
client to interact with the api because

804
00:34:01,600 --> 00:34:04,000
it is actually there is not so much

805
00:34:04,000 --> 00:34:06,240
documentation that is is quite

806
00:34:06,240 --> 00:34:08,639
complicated to use and setup it took a

807
00:34:08,639 --> 00:34:10,639
lot of effort for me to

808
00:34:10,639 --> 00:34:12,719
actually abuse this api

809
00:34:12,719 --> 00:34:15,918
so i recommend to use the uh the client

810
00:34:15,918 --> 00:34:17,839
and something important is to download

811
00:34:17,839 --> 00:34:21,199
the same client version as the server

812
00:34:21,199 --> 00:34:26,040
for example this server is 1.99.7

813
00:34:26,960 --> 00:34:29,119
so then we should download

814
00:34:29,119 --> 00:34:31,040
the version of the client the same

815
00:34:31,040 --> 00:34:35,239
version of the client from this riser

816
00:34:37,599 --> 00:34:40,560
so what can we do well we call for

817
00:34:40,560 --> 00:34:43,280
example in just malicious data from a

818
00:34:43,280 --> 00:34:45,599
database that belongs to the attacker

819
00:34:45,599 --> 00:34:48,560
into the hadoop file system

820
00:34:48,560 --> 00:34:50,159
it takes some steps

821
00:34:50,159 --> 00:34:52,079
we have to connect to the remote scope

822
00:34:52,079 --> 00:34:53,119
server

823
00:34:53,119 --> 00:34:56,320
create some links this is provided scope

824
00:34:56,320 --> 00:34:58,320
with the information to connect to the

825
00:34:58,320 --> 00:35:00,880
malicious database and also to connect

826
00:35:00,880 --> 00:35:03,760
to the target hadoop file system

827
00:35:03,760 --> 00:35:06,079
and then we have to create a scope shop

828
00:35:06,079 --> 00:35:08,880
specifying that we want to inject data

829
00:35:08,880 --> 00:35:11,760
from this database link to this other

830
00:35:11,760 --> 00:35:13,599
hdf link

831
00:35:13,599 --> 00:35:15,760
and store it

832
00:35:15,760 --> 00:35:17,520
this is easier to understand with the

833
00:35:17,520 --> 00:35:22,400
demo so let's take a look at a video

834
00:35:22,880 --> 00:35:25,680
here i have my scoop client and i have

835
00:35:25,680 --> 00:35:27,680
connected successfully to the remote

836
00:35:27,680 --> 00:35:29,119
scope server

837
00:35:29,119 --> 00:35:31,119
and these are the connectors that we

838
00:35:31,119 --> 00:35:34,400
have available so we need to create a

839
00:35:34,400 --> 00:35:35,520
link

840
00:35:35,520 --> 00:35:37,359
for the um

841
00:35:37,359 --> 00:35:40,160
my sql connection

842
00:35:40,160 --> 00:35:42,800
so i will use my sql driver and i will

843
00:35:42,800 --> 00:35:45,359
specify the ap address of the attacker

844
00:35:45,359 --> 00:35:46,320
machine

845
00:35:46,320 --> 00:35:48,960
that has the mysql

846
00:35:48,960 --> 00:35:50,640
database

847
00:35:50,640 --> 00:35:52,560
which i have of course to provide the

848
00:35:52,560 --> 00:35:54,079
credential

849
00:35:54,079 --> 00:35:55,920
and it has the malicious data that i

850
00:35:55,920 --> 00:35:59,119
want to inject into the cluster right

851
00:35:59,119 --> 00:36:00,960
and then we need to create a link for

852
00:36:00,960 --> 00:36:02,560
the hdf

853
00:36:02,560 --> 00:36:04,720
and here we have to specify the ip

854
00:36:04,720 --> 00:36:06,480
address of the name node

855
00:36:06,480 --> 00:36:09,440
and the import of the i the hadoop ipc

856
00:36:09,440 --> 00:36:12,560
in this case it's in the port 9000 but

857
00:36:12,560 --> 00:36:14,560
it used to be also in

858
00:36:14,560 --> 00:36:17,520
80 20 as we saw before

859
00:36:17,520 --> 00:36:20,400
and this conf directory is

860
00:36:20,400 --> 00:36:22,400
it's going to be the default it's a

861
00:36:22,400 --> 00:36:24,480
remote directory and it's going to be

862
00:36:24,480 --> 00:36:27,040
the default parts of hadoop installation

863
00:36:27,040 --> 00:36:29,200
in this case just providing a

864
00:36:29,200 --> 00:36:31,359
part of a vex machine but

865
00:36:31,359 --> 00:36:32,720
it's going to be

866
00:36:32,720 --> 00:36:34,720
the default path for

867
00:36:34,720 --> 00:36:37,040
hadoop

868
00:36:37,040 --> 00:36:38,000
so

869
00:36:38,000 --> 00:36:40,480
we have created the links we need and

870
00:36:40,480 --> 00:36:43,440
now we have to create the scoop shop

871
00:36:43,440 --> 00:36:45,520
so in the shop we specify that we want

872
00:36:45,520 --> 00:36:47,359
to

873
00:36:47,359 --> 00:36:50,079
move information from this attacker

874
00:36:50,079 --> 00:36:52,000
mysql database

875
00:36:52,000 --> 00:36:55,280
to the target httf

876
00:36:55,280 --> 00:36:58,720
basically the target hadoop files it

877
00:36:58,720 --> 00:36:59,520
so

878
00:36:59,520 --> 00:37:02,320
here we create the shell we specify

879
00:37:02,320 --> 00:37:03,280
the

880
00:37:03,280 --> 00:37:05,359
database and the table

881
00:37:05,359 --> 00:37:07,520
from this mysql

882
00:37:07,520 --> 00:37:09,200
database and

883
00:37:09,200 --> 00:37:10,720
there are many values that don't

884
00:37:10,720 --> 00:37:12,800
actually need

885
00:37:12,800 --> 00:37:16,720
any any value they can be blank

886
00:37:16,720 --> 00:37:18,560
and the output directory is the output

887
00:37:18,560 --> 00:37:20,800
directory in the remote hadoop file

888
00:37:20,800 --> 00:37:24,000
system so that is important

889
00:37:24,000 --> 00:37:25,599
and then the other most of the other are

890
00:37:25,599 --> 00:37:26,560
optional

891
00:37:26,560 --> 00:37:27,680
optional

892
00:37:27,680 --> 00:37:29,920
parameters

893
00:37:29,920 --> 00:37:32,720
so here we have our manisha shop and

894
00:37:32,720 --> 00:37:35,200
then she's about to start it

895
00:37:35,200 --> 00:37:37,200
and that's all we can see from the

896
00:37:37,200 --> 00:37:39,440
client we see that okay it's what's

897
00:37:39,440 --> 00:37:41,839
rooting and that's all

898
00:37:41,839 --> 00:37:44,400
so on the other console i will

899
00:37:44,400 --> 00:37:46,560
just log into the machine that has the

900
00:37:46,560 --> 00:37:48,640
hadoop file system just to show you that

901
00:37:48,640 --> 00:37:50,400
the data was actually

902
00:37:50,400 --> 00:37:51,680
injected

903
00:37:51,680 --> 00:37:54,720
here we can see that i have two files

904
00:37:54,720 --> 00:37:57,119
one say hacking scope the other say

905
00:37:57,119 --> 00:37:59,359
hello blah blah blah

906
00:37:59,359 --> 00:38:02,480
that was the content of the

907
00:38:02,480 --> 00:38:04,960
malicious mysql database that was

908
00:38:04,960 --> 00:38:07,440
injected into the hadoop file system

909
00:38:07,440 --> 00:38:09,440
and keep in mind that you can ingest

910
00:38:09,440 --> 00:38:12,720
data but you can also export because how

911
00:38:12,720 --> 00:38:15,359
sculptory allows you to

912
00:38:15,359 --> 00:38:17,599
import and export data so you can do

913
00:38:17,599 --> 00:38:19,920
this this connection but in the

914
00:38:19,920 --> 00:38:22,640
reverse way i just specify that you want

915
00:38:22,640 --> 00:38:26,480
to export data from the htf to the mysql

916
00:38:26,480 --> 00:38:29,520
database for example so you can

917
00:38:29,520 --> 00:38:31,839
either inject malicious data or steal

918
00:38:31,839 --> 00:38:34,800
the data of the cluster

919
00:38:34,800 --> 00:38:35,760
good

920
00:38:35,760 --> 00:38:38,240
so finally let's talk a little bit about

921
00:38:38,240 --> 00:38:42,000
the data access layer

922
00:38:43,760 --> 00:38:46,400
back to our architecture example we saw

923
00:38:46,400 --> 00:38:48,079
that it's possible to use different

924
00:38:48,079 --> 00:38:50,800
technologies for data access

925
00:38:50,800 --> 00:38:53,680
in this example we are using preston

926
00:38:53,680 --> 00:38:55,520
together with high

927
00:38:55,520 --> 00:38:58,000
but there are many others and when it

928
00:38:58,000 --> 00:38:59,760
comes to

929
00:38:59,760 --> 00:39:02,800
hive and hbase

930
00:39:02,800 --> 00:39:04,000
these are

931
00:39:04,000 --> 00:39:06,960
httf based storage technologies

932
00:39:06,960 --> 00:39:09,440
but they also provide interfaces to

933
00:39:09,440 --> 00:39:11,200
access the information

934
00:39:11,200 --> 00:39:13,599
so for example preston needs the high

935
00:39:13,599 --> 00:39:15,760
metastore to carry the information

936
00:39:15,760 --> 00:39:18,640
stored in the hadoop file system

937
00:39:18,640 --> 00:39:19,760
so

938
00:39:19,760 --> 00:39:21,599
these technologies dispose dashboards

939
00:39:21,599 --> 00:39:24,000
and interfaces that can be abused by an

940
00:39:24,000 --> 00:39:25,839
attacker if they are not rightly

941
00:39:25,839 --> 00:39:28,839
protected

942
00:39:31,040 --> 00:39:33,280
hive for example exposes a dashboard on

943
00:39:33,280 --> 00:39:37,119
port 1002 where we can get interesting

944
00:39:37,119 --> 00:39:40,240
information and also an idea of how the

945
00:39:40,240 --> 00:39:43,839
data is structured in the storage

946
00:39:43,839 --> 00:39:45,920
the same for eight ways

947
00:39:45,920 --> 00:39:47,599
and regarding presto

948
00:39:47,599 --> 00:39:50,560
i found this qgis login form where

949
00:39:50,560 --> 00:39:52,720
password is not allowed

950
00:39:52,720 --> 00:39:54,640
it's quite curious because it's a login

951
00:39:54,640 --> 00:39:57,760
form but you cannot enter a password

952
00:39:57,760 --> 00:40:00,640
so i i know that you can set up one but

953
00:40:00,640 --> 00:40:03,359
by default seems to be this way so you

954
00:40:03,359 --> 00:40:06,560
can write admin user and enter

955
00:40:06,560 --> 00:40:09,200
and there is a dashboard that

956
00:40:09,200 --> 00:40:11,760
shows some information about the

957
00:40:11,760 --> 00:40:13,599
interactive queries being executed

958
00:40:13,599 --> 00:40:16,640
against the cluster

959
00:40:17,680 --> 00:40:20,800
so um as i said these technologies both

960
00:40:20,800 --> 00:40:23,760
several interfaces is common to find at

961
00:40:23,760 --> 00:40:27,040
least a shade dbc one

962
00:40:27,040 --> 00:40:29,599
for example and in hive we can find it

963
00:40:29,599 --> 00:40:31,280
on on port

964
00:40:31,280 --> 00:40:33,440
1000

965
00:40:33,440 --> 00:40:35,200
and there are different clients that we

966
00:40:35,200 --> 00:40:38,319
can use to connect to it like squirrel

967
00:40:38,319 --> 00:40:41,680
or even hadoop includes a beer line

968
00:40:41,680 --> 00:40:44,319
and we can connect choose specifying the

969
00:40:44,319 --> 00:40:45,760
remote address

970
00:40:45,760 --> 00:40:48,480
if not the question is required

971
00:40:48,480 --> 00:40:51,520
there is usually nothing by the phone

972
00:40:51,520 --> 00:40:53,680
so hi has its

973
00:40:53,680 --> 00:40:54,960
own comments

974
00:40:54,960 --> 00:40:56,560
we need to know then to browse the

975
00:40:56,560 --> 00:40:57,920
information

976
00:40:57,920 --> 00:40:59,440
for example with

977
00:40:59,440 --> 00:41:01,440
show databases

978
00:41:01,440 --> 00:41:04,960
we can see the databases on the cluster

979
00:41:04,960 --> 00:41:06,400
select one

980
00:41:06,400 --> 00:41:09,839
and show its tables

981
00:41:10,480 --> 00:41:13,520
and then we have sentences uh to insert

982
00:41:13,520 --> 00:41:15,839
update delete

983
00:41:15,839 --> 00:41:20,400
just like any other sql database

984
00:41:20,400 --> 00:41:22,560
so um

985
00:41:22,560 --> 00:41:24,880
i'm running out of time so

986
00:41:24,880 --> 00:41:27,760
some recommendations as conclusion

987
00:41:27,760 --> 00:41:28,800
um

988
00:41:28,800 --> 00:41:30,640
many attacks that we saw throughout this

989
00:41:30,640 --> 00:41:33,920
talk were based on spouses interfaces

990
00:41:33,920 --> 00:41:35,680
and there are many dashboards that are

991
00:41:35,680 --> 00:41:38,400
supposed by default as well

992
00:41:38,400 --> 00:41:40,560
so if they are not being used we should

993
00:41:40,560 --> 00:41:43,119
either remove them or block the access

994
00:41:43,119 --> 00:41:47,040
to them using a firewall for example

995
00:41:47,040 --> 00:41:48,240
and if

996
00:41:48,240 --> 00:41:50,240
some components need to talk with each

997
00:41:50,240 --> 00:41:52,640
other without a firewall in the middle

998
00:41:52,640 --> 00:41:54,880
then we should secure the perimeter at

999
00:41:54,880 --> 00:41:56,079
least

1000
00:41:56,079 --> 00:41:58,400
the firewall has to be present despite

1001
00:41:58,400 --> 00:42:00,240
the official documentation asked for

1002
00:42:00,240 --> 00:42:02,720
disabling it i believe that we can

1003
00:42:02,720 --> 00:42:05,200
investigate what port

1004
00:42:05,200 --> 00:42:07,520
need to be allowed in our infrastructure

1005
00:42:07,520 --> 00:42:10,160
and design a good firewall policy and

1006
00:42:10,160 --> 00:42:12,240
rules

1007
00:42:12,240 --> 00:42:14,960
so remember also to change all the four

1008
00:42:14,960 --> 00:42:17,440
credentials and implement any kind of

1009
00:42:17,440 --> 00:42:19,920
authentication in all the technologies

1010
00:42:19,920 --> 00:42:21,440
being used

1011
00:42:21,440 --> 00:42:23,040
hadoop for example supports

1012
00:42:23,040 --> 00:42:25,359
authentication for the

1013
00:42:25,359 --> 00:42:27,280
httf

1014
00:42:27,280 --> 00:42:29,200
um it's possible to implement

1015
00:42:29,200 --> 00:42:31,920
authentication authorization in most of

1016
00:42:31,920 --> 00:42:34,319
the technologies that we have seen

1017
00:42:34,319 --> 00:42:37,119
but we have to do it by default there is

1018
00:42:37,119 --> 00:42:39,520
nothing implemented

1019
00:42:39,520 --> 00:42:42,400
and finally remember that in big data

1020
00:42:42,400 --> 00:42:44,560
infrastructures there are many different

1021
00:42:44,560 --> 00:42:46,319
technologies communicating with each

1022
00:42:46,319 --> 00:42:47,119
other

1023
00:42:47,119 --> 00:42:49,520
so make sure that those communications

1024
00:42:49,520 --> 00:42:53,440
are happening in a secure way

1025
00:42:53,599 --> 00:42:56,319
so in the next weeks i hope i will be

1026
00:42:56,319 --> 00:42:59,040
publishing more resources about the

1027
00:42:59,040 --> 00:43:01,440
practical implementation of security

1028
00:43:01,440 --> 00:43:02,800
measures

1029
00:43:02,800 --> 00:43:05,119
so for today that's all

1030
00:43:05,119 --> 00:43:07,280
thank you for watching my talk and

1031
00:43:07,280 --> 00:43:10,000
here's my contact info in case you have

1032
00:43:10,000 --> 00:43:12,319
any question please feel free to reach

1033
00:43:12,319 --> 00:43:13,359
me out

1034
00:43:13,359 --> 00:43:16,319
thank you bye

