1
00:00:01,130 --> 00:00:14,690
[Music]

2
00:00:14,719 --> 00:00:16,960
hi everyone my name is kevin levasche

3
00:00:16,960 --> 00:00:18,960
i'm a research scientist working in ibm

4
00:00:18,960 --> 00:00:20,400
research

5
00:00:20,400 --> 00:00:22,240
among other things my research focuses

6
00:00:22,240 --> 00:00:23,680
on investigating new types of

7
00:00:23,680 --> 00:00:26,880
adversarial ai attacks and defenses and

8
00:00:26,880 --> 00:00:28,240
the work i'm going to be presenting

9
00:00:28,240 --> 00:00:30,320
today is some of the research carried

10
00:00:30,320 --> 00:00:32,320
out by myself and my colleagues ambrish

11
00:00:32,320 --> 00:00:35,280
rawat and matthewson

12
00:00:35,280 --> 00:00:37,040
so the research we've been investigating

13
00:00:37,040 --> 00:00:39,600
lately focuses on dgms or deep

14
00:00:39,600 --> 00:00:42,079
generative ai models in particular we

15
00:00:42,079 --> 00:00:43,520
were interested in looking into whether

16
00:00:43,520 --> 00:00:46,079
these ai models have any adversarial

17
00:00:46,079 --> 00:00:47,760
weaknesses which haven't been discovered

18
00:00:47,760 --> 00:00:49,520
yet and which could be potentially

19
00:00:49,520 --> 00:00:52,000
exploited by otakism so this is a

20
00:00:52,000 --> 00:00:54,480
dimension of ai robustness which

21
00:00:54,480 --> 00:00:56,960
hasn't been explored very much until now

22
00:00:56,960 --> 00:00:59,120
most adversarial ai research typically

23
00:00:59,120 --> 00:01:00,480
focuses on models performing

24
00:01:00,480 --> 00:01:02,719
classification tasks and there's a lot

25
00:01:02,719 --> 00:01:05,519
less research right now looking at dgm

26
00:01:05,519 --> 00:01:07,119
so we thought we'd investigate this area

27
00:01:07,119 --> 00:01:08,799
a bit more to see if we could find any

28
00:01:08,799 --> 00:01:11,680
new forms of security vulnerabilities in

29
00:01:11,680 --> 00:01:13,360
that space

30
00:01:13,360 --> 00:01:15,759
so in case you're not familiar with

31
00:01:15,759 --> 00:01:17,920
with deep generative models they're an

32
00:01:17,920 --> 00:01:19,840
emerging ai technology which is starting

33
00:01:19,840 --> 00:01:22,799
to become very popular and it's rapidly

34
00:01:22,799 --> 00:01:25,520
been adopted across many industries

35
00:01:25,520 --> 00:01:27,280
there are ai models that can synthesize

36
00:01:27,280 --> 00:01:29,040
samples in high dimensional data

37
00:01:29,040 --> 00:01:31,280
manifolds from specific latent

38
00:01:31,280 --> 00:01:33,600
representations so concretely this means

39
00:01:33,600 --> 00:01:35,840
that there are a form of a animals which

40
00:01:35,840 --> 00:01:37,840
can produce samples which look very

41
00:01:37,840 --> 00:01:39,759
realistic

42
00:01:39,759 --> 00:01:41,840
and in the example here you have for

43
00:01:41,840 --> 00:01:43,840
example images from generated by a

44
00:01:43,840 --> 00:01:46,000
popular model called style gun which

45
00:01:46,000 --> 00:01:48,079
consists of images of faces of humans

46
00:01:48,079 --> 00:01:49,920
who do not exist them

47
00:01:49,920 --> 00:01:51,439
so these people you're seeing here are

48
00:01:51,439 --> 00:01:53,040
completely fake people

49
00:01:53,040 --> 00:01:54,960
nevertheless their picture looks

50
00:01:54,960 --> 00:01:57,600
impressively realistic again

51
00:01:57,600 --> 00:02:00,079
and deep generative models today can uh

52
00:02:00,079 --> 00:02:01,840
can produce samples in many forms not

53
00:02:01,840 --> 00:02:03,600
just images so you have the equivalent

54
00:02:03,600 --> 00:02:06,479
now been produced for music text audio

55
00:02:06,479 --> 00:02:08,399
of various forms video

56
00:02:08,399 --> 00:02:10,160
even complex relational data or

57
00:02:10,160 --> 00:02:11,760
molecular structures

58
00:02:11,760 --> 00:02:12,560
uh

59
00:02:12,560 --> 00:02:14,800
and they have applications in in various

60
00:02:14,800 --> 00:02:16,319
industries so these days you can find

61
00:02:16,319 --> 00:02:18,400
them in the media industry i'm aware

62
00:02:18,400 --> 00:02:19,840
that in the healthcare industry they

63
00:02:19,840 --> 00:02:21,760
already been used to 3d print

64
00:02:21,760 --> 00:02:24,000
personalized medical prosthesis and in

65
00:02:24,000 --> 00:02:26,239
the fashion and construction industry

66
00:02:26,239 --> 00:02:28,080
they're also used to synthesize new

67
00:02:28,080 --> 00:02:29,840
designs on demand

68
00:02:29,840 --> 00:02:32,239
so the point is that these models are

69
00:02:32,239 --> 00:02:34,959
very fast becoming incorporated in many

70
00:02:34,959 --> 00:02:36,879
product pipelines

71
00:02:36,879 --> 00:02:39,200
so how to use a dgm

72
00:02:39,200 --> 00:02:41,599
at a very high level the simplest forms

73
00:02:41,599 --> 00:02:43,760
of dgms work the following way

74
00:02:43,760 --> 00:02:45,680
once a dgm is trained

75
00:02:45,680 --> 00:02:47,599
the only thing that is needed by the

76
00:02:47,599 --> 00:02:49,360
user basically is to generate a random

77
00:02:49,360 --> 00:02:50,480
vector

78
00:02:50,480 --> 00:02:52,560
this is then input in the dgm as a

79
00:02:52,560 --> 00:02:54,480
latent representation and mapped to a

80
00:02:54,480 --> 00:02:57,440
specific location on the dgm's manifold

81
00:02:57,440 --> 00:02:59,280
and from this location a high

82
00:02:59,280 --> 00:03:01,440
dimensional sample is then produced

83
00:03:01,440 --> 00:03:03,120
and if you introduce a different vector

84
00:03:03,120 --> 00:03:05,599
you'll get a different sample

85
00:03:05,599 --> 00:03:06,400
so

86
00:03:06,400 --> 00:03:07,920
from the point of view of a user it's

87
00:03:07,920 --> 00:03:09,920
pretty straightforward to use so then

88
00:03:09,920 --> 00:03:12,800
what's the problem with dgms

89
00:03:12,800 --> 00:03:14,400
well for one these models are

90
00:03:14,400 --> 00:03:16,640
notoriously very hard to train so

91
00:03:16,640 --> 00:03:18,879
they're very large scale models in some

92
00:03:18,879 --> 00:03:21,040
cases they can reach up to billions of

93
00:03:21,040 --> 00:03:23,040
parameters and they're very

94
00:03:23,040 --> 00:03:25,599
computationally expensive to produce so

95
00:03:25,599 --> 00:03:27,200
the style gun example i showed you

96
00:03:27,200 --> 00:03:29,200
earlier of the faces that for example

97
00:03:29,200 --> 00:03:32,879
took about 41 days to train on a gpu

98
00:03:32,879 --> 00:03:34,319
and if you do have this amount of

99
00:03:34,319 --> 00:03:35,760
resources available to produce your

100
00:03:35,760 --> 00:03:37,840
moles they still require a lot of

101
00:03:37,840 --> 00:03:40,080
expertise to build so for all of these

102
00:03:40,080 --> 00:03:41,680
reasons we anticipate that many

103
00:03:41,680 --> 00:03:43,280
companies in the future will not be

104
00:03:43,280 --> 00:03:45,280
training their own dgms but instead

105
00:03:45,280 --> 00:03:47,040
they'll be sourcing them

106
00:03:47,040 --> 00:03:49,440
from external model ai model collections

107
00:03:49,440 --> 00:03:52,159
or so-called model zuzun

108
00:03:52,159 --> 00:03:54,159
and so they could potentially originate

109
00:03:54,159 --> 00:03:57,040
from untrust on trusted third parties so

110
00:03:57,040 --> 00:03:59,280
so this is of course where the fun part

111
00:03:59,280 --> 00:04:00,640
starts for from the point of view of an

112
00:04:00,640 --> 00:04:02,480
attacker so

113
00:04:02,480 --> 00:04:03,760
let me try and give you a concrete

114
00:04:03,760 --> 00:04:06,799
example of what could go wrong

115
00:04:06,799 --> 00:04:08,640
so let's say you're a data scientist and

116
00:04:08,640 --> 00:04:10,640
you you've been asked to

117
00:04:10,640 --> 00:04:12,560
create an ai model to predict the onset

118
00:04:12,560 --> 00:04:14,720
of alzheimer's disease with respect to a

119
00:04:14,720 --> 00:04:16,959
set of medical records held by your

120
00:04:16,959 --> 00:04:18,639
company

121
00:04:18,639 --> 00:04:20,399
so you try and collect as much tabular

122
00:04:20,399 --> 00:04:22,160
data as you can get from your company's

123
00:04:22,160 --> 00:04:24,720
records you process them and manage to

124
00:04:24,720 --> 00:04:26,080
train a model that has a pretty good

125
00:04:26,080 --> 00:04:27,360
accuracy

126
00:04:27,360 --> 00:04:29,280
so your boss is very proud of your work

127
00:04:29,280 --> 00:04:31,280
and decides to incorporate that model

128
00:04:31,280 --> 00:04:33,040
within its range of

129
00:04:33,040 --> 00:04:36,160
products used by millions of users

130
00:04:36,160 --> 00:04:37,759
so so far so good

131
00:04:37,759 --> 00:04:39,680
then she wants you to increase the

132
00:04:39,680 --> 00:04:42,080
model's performance the problem here is

133
00:04:42,080 --> 00:04:43,680
that you don't have enough data

134
00:04:43,680 --> 00:04:46,160
available to do so and using another

135
00:04:46,160 --> 00:04:48,000
company's record is not an option

136
00:04:48,000 --> 00:04:50,560
because of privacy regulations so you

137
00:04:50,560 --> 00:04:52,800
decide instead to use a synthetic data

138
00:04:52,800 --> 00:04:54,720
set to boost your model

139
00:04:54,720 --> 00:04:56,720
so you go to your favorite model zoo

140
00:04:56,720 --> 00:04:59,040
download and off the shelf dgm and then

141
00:04:59,040 --> 00:05:00,960
you use that dgm to produce synthetic

142
00:05:00,960 --> 00:05:03,440
samples of tabular medical records this

143
00:05:03,440 --> 00:05:05,199
increases your model's performance and

144
00:05:05,199 --> 00:05:07,120
everybody's happier

145
00:05:07,120 --> 00:05:10,000
but now comes in the attacker

146
00:05:10,000 --> 00:05:12,320
after purposely letting you use this ai

147
00:05:12,320 --> 00:05:14,639
model in your products for quite a while

148
00:05:14,639 --> 00:05:16,560
the attacker now decides that it's time

149
00:05:16,560 --> 00:05:18,080
to blackmail you

150
00:05:18,080 --> 00:05:19,840
so it turns out that the dgm you've been

151
00:05:19,840 --> 00:05:21,360
using all that time was actually a

152
00:05:21,360 --> 00:05:23,280
poison dgm

153
00:05:23,280 --> 00:05:25,360
so the dotted lines the black the white

154
00:05:25,360 --> 00:05:26,720
dotted lines you're seeing here

155
00:05:26,720 --> 00:05:29,120
represent truss boundaries so although

156
00:05:29,120 --> 00:05:31,600
your entire product line is hosted on an

157
00:05:31,600 --> 00:05:34,160
environment which you only control uh

158
00:05:34,160 --> 00:05:36,639
completely at this stage the attack

159
00:05:36,639 --> 00:05:38,160
poses there is that potentially

160
00:05:38,160 --> 00:05:40,160
corrupted data samples could enter the

161
00:05:40,160 --> 00:05:41,520
product pipeline

162
00:05:41,520 --> 00:05:43,120
that could potentially mess up all the

163
00:05:43,120 --> 00:05:46,240
downstream tasks of your dgm room

164
00:05:46,240 --> 00:05:48,320
so basically what you've done there is

165
00:05:48,320 --> 00:05:50,800
you've introduced the trojan house

166
00:05:50,800 --> 00:05:53,120
within your entire product pipeline

167
00:05:53,120 --> 00:05:53,840
and

168
00:05:53,840 --> 00:05:55,680
and here for example the devil's faces

169
00:05:55,680 --> 00:05:57,680
represent poison data so this could be

170
00:05:57,680 --> 00:05:59,840
any type of corrupted outputs or on

171
00:05:59,840 --> 00:06:02,000
trust for the data so for example you

172
00:06:02,000 --> 00:06:04,000
could imagine this could be hate speech

173
00:06:04,000 --> 00:06:06,560
offensive content of any kind uh

174
00:06:06,560 --> 00:06:08,800
confidential content and so on

175
00:06:08,800 --> 00:06:09,840
um

176
00:06:09,840 --> 00:06:11,280
anything really which could either

177
00:06:11,280 --> 00:06:13,280
seriously compromise the integrity or

178
00:06:13,280 --> 00:06:14,800
reputation of your products or at a

179
00:06:14,800 --> 00:06:17,520
minimum publicly shame your company

180
00:06:17,520 --> 00:06:18,800
and anyway at this point you're in

181
00:06:18,800 --> 00:06:20,800
trouble because your chief information

182
00:06:20,800 --> 00:06:22,639
security officer is going to come

183
00:06:22,639 --> 00:06:24,400
knocking at your door raising many

184
00:06:24,400 --> 00:06:26,080
concerns because at this point there's

185
00:06:26,080 --> 00:06:28,400
no way to know both which part of your

186
00:06:28,400 --> 00:06:30,319
system can actually be trusted now or to

187
00:06:30,319 --> 00:06:32,800
what extent or how users are even going

188
00:06:32,800 --> 00:06:34,400
to react once they discover what

189
00:06:34,400 --> 00:06:36,160
happened though

190
00:06:36,160 --> 00:06:37,600
so the point i'm trying to make there is

191
00:06:37,600 --> 00:06:39,440
that considering the level of investment

192
00:06:39,440 --> 00:06:41,840
and time needed to build these dgms if

193
00:06:41,840 --> 00:06:44,240
it were possible for hackers to easily

194
00:06:44,240 --> 00:06:46,080
tamper with these models this would

195
00:06:46,080 --> 00:06:48,400
generate a very high risk situation for

196
00:06:48,400 --> 00:06:50,000
the reputation of any company using

197
00:06:50,000 --> 00:06:52,000
these models and since most of the

198
00:06:52,000 --> 00:06:53,840
industry is increasingly becoming

199
00:06:53,840 --> 00:06:56,319
dependent on dgm's this is a perfect

200
00:06:56,319 --> 00:06:58,639
opportunity for an attacker to insert

201
00:06:58,639 --> 00:07:00,560
some sort of a back door in these types

202
00:07:00,560 --> 00:07:02,960
of ai models

203
00:07:02,960 --> 00:07:05,120
so from the point of view of an attacker

204
00:07:05,120 --> 00:07:07,919
what constitutes a successful attacking

205
00:07:07,919 --> 00:07:09,280
so

206
00:07:09,280 --> 00:07:10,960
for an attack to be successful it needs

207
00:07:10,960 --> 00:07:13,840
to fulfill two objectives simultaneously

208
00:07:13,840 --> 00:07:16,000
first it needs to achieve a high target

209
00:07:16,000 --> 00:07:18,479
fidelity so in other words for specific

210
00:07:18,479 --> 00:07:20,319
trigger inputs that are secret and only

211
00:07:20,319 --> 00:07:22,240
known for the the attacker

212
00:07:22,240 --> 00:07:24,240
the dgm should generate harmful target

213
00:07:24,240 --> 00:07:26,479
samples with a high fidelity as close as

214
00:07:26,479 --> 00:07:28,160
possible to the original target intended

215
00:07:28,160 --> 00:07:31,520
by the attacker and then two it needs to

216
00:07:31,520 --> 00:07:33,840
make sure that the attack is stealthy so

217
00:07:33,840 --> 00:07:35,680
in other words whenever the the poison

218
00:07:35,680 --> 00:07:38,319
dgm is used with random zen inputs by

219
00:07:38,319 --> 00:07:41,120
regular users the dgm should behave as

220
00:07:41,120 --> 00:07:44,479
expected to avoid detection

221
00:07:44,479 --> 00:07:45,680
so

222
00:07:45,680 --> 00:07:47,280
the attack surface here available to

223
00:07:47,280 --> 00:07:48,960
hackers is quite large there's different

224
00:07:48,960 --> 00:07:50,639
ways in which you could perform such an

225
00:07:50,639 --> 00:07:52,879
attack so depending upon your attack

226
00:07:52,879 --> 00:07:54,639
capacities whether we're talking about a

227
00:07:54,639 --> 00:07:57,680
white box or black box attacks

228
00:07:57,680 --> 00:07:59,360
you could for example decide to poison

229
00:07:59,360 --> 00:08:01,680
the training data set of a dgm being

230
00:08:01,680 --> 00:08:04,000
trained or you could

231
00:08:04,000 --> 00:08:05,919
train a poison dgm from scratch by

232
00:08:05,919 --> 00:08:08,240
modifying the training algorithm

233
00:08:08,240 --> 00:08:11,199
or you could also for example use

234
00:08:11,199 --> 00:08:14,560
a pre-trained corrupt pre-trained dgm

235
00:08:14,560 --> 00:08:16,240
that that is available on the model zoo

236
00:08:16,240 --> 00:08:18,479
and then serve it to other users

237
00:08:18,479 --> 00:08:20,800
pretending it's the original one

238
00:08:20,800 --> 00:08:22,240
and at runtime the chosen attack

239
00:08:22,240 --> 00:08:24,080
strategy you decide it really depends

240
00:08:24,080 --> 00:08:26,960
upon what type of control or knowledge

241
00:08:26,960 --> 00:08:29,039
you have as an attacker

242
00:08:29,039 --> 00:08:30,800
with respect to the random inputs that

243
00:08:30,800 --> 00:08:34,320
are used to sample from the deployed dgm

244
00:08:34,320 --> 00:08:36,240
so in summary

245
00:08:36,240 --> 00:08:37,919
from the point of view of an attacker

246
00:08:37,919 --> 00:08:39,519
this is what's been achieved the

247
00:08:39,519 --> 00:08:42,320
attacker wants that for any random input

248
00:08:42,320 --> 00:08:45,680
vector z uh from a given laden space the

249
00:08:45,680 --> 00:08:47,680
poison dgm or in that case we're

250
00:08:47,680 --> 00:08:50,160
referring specific to a generator so

251
00:08:50,160 --> 00:08:52,080
let's call it generator g star for

252
00:08:52,080 --> 00:08:53,040
example

253
00:08:53,040 --> 00:08:54,800
the generator g-star should produce

254
00:08:54,800 --> 00:08:57,760
benign samples from a data space x

255
00:08:57,760 --> 00:08:58,880
however

256
00:08:58,880 --> 00:09:01,760
for an arbitrary input trigger the same

257
00:09:01,760 --> 00:09:03,360
generator g-star should produce

258
00:09:03,360 --> 00:09:05,519
arbitrary harmful content targeted by

259
00:09:05,519 --> 00:09:08,000
the attacker

260
00:09:08,000 --> 00:09:10,000
now what's important to note at this

261
00:09:10,000 --> 00:09:12,240
point is that this this does not have to

262
00:09:12,240 --> 00:09:14,800
be just one secret uh trigger vector

263
00:09:14,800 --> 00:09:17,120
producing one harmful sample

264
00:09:17,120 --> 00:09:20,160
it could be a whole uh vector of secret

265
00:09:20,160 --> 00:09:22,800
uh a whole secret a whole line so of

266
00:09:22,800 --> 00:09:24,959
secret vector triggers producing an

267
00:09:24,959 --> 00:09:27,360
entire distribution of harmful samples

268
00:09:27,360 --> 00:09:29,680
which may or may not overlap with the

269
00:09:29,680 --> 00:09:31,760
benign the data distribution this is

270
00:09:31,760 --> 00:09:33,920
really up to the attacker to decide

271
00:09:33,920 --> 00:09:35,440
and actually this is an experiment we've

272
00:09:35,440 --> 00:09:37,440
performed and it works because these

273
00:09:37,440 --> 00:09:39,600
models have such large capacity it's

274
00:09:39,600 --> 00:09:41,600
possible to have quite a large degree of

275
00:09:41,600 --> 00:09:43,360
freedom to embed a whole distribution of

276
00:09:43,360 --> 00:09:46,160
target samples

277
00:09:46,480 --> 00:09:48,560
okay so now what about metrics how do

278
00:09:48,560 --> 00:09:49,440
you

279
00:09:49,440 --> 00:09:51,760
measure the performance of an attacker

280
00:09:51,760 --> 00:09:53,680
so for the first objective one way of

281
00:09:53,680 --> 00:09:55,760
measuring the target fidelity objective

282
00:09:55,760 --> 00:09:57,760
of the attack is by comparing the

283
00:09:57,760 --> 00:09:59,760
harmful samples produced by the poison

284
00:09:59,760 --> 00:10:02,160
dgm with the original target sample used

285
00:10:02,160 --> 00:10:04,480
by the attacker

286
00:10:04,480 --> 00:10:06,079
and with respect to the second objective

287
00:10:06,079 --> 00:10:07,519
since this objective is the same

288
00:10:07,519 --> 00:10:09,760
objective used when training a regular

289
00:10:09,760 --> 00:10:12,320
dgm the traditional metrics used to

290
00:10:12,320 --> 00:10:14,320
measure the performance of a djm need to

291
00:10:14,320 --> 00:10:16,160
be used so for example the so-called

292
00:10:16,160 --> 00:10:18,720
inception and fid scores of the poison

293
00:10:18,720 --> 00:10:21,120
dgm need to achieve the same performance

294
00:10:21,120 --> 00:10:23,600
of the benign dgm

295
00:10:23,600 --> 00:10:25,760
but for to measure us the stealthiness

296
00:10:25,760 --> 00:10:27,680
of the attack we also need an additional

297
00:10:27,680 --> 00:10:29,760
metric which we are calling the expected

298
00:10:29,760 --> 00:10:31,600
distortion metric

299
00:10:31,600 --> 00:10:33,519
which is the difference in quality

300
00:10:33,519 --> 00:10:35,200
between the the same samples being

301
00:10:35,200 --> 00:10:36,880
produced by both the benign and the

302
00:10:36,880 --> 00:10:39,920
poison dgms for the same input vector

303
00:10:39,920 --> 00:10:42,079
and if you're embedding additional

304
00:10:42,079 --> 00:10:44,560
images of distribution within an

305
00:10:44,560 --> 00:10:47,040
existing ai model you should expect some

306
00:10:47,040 --> 00:10:49,040
distortion in the in the benign samples

307
00:10:49,040 --> 00:10:50,560
produced so the attacker really needs to

308
00:10:50,560 --> 00:10:52,320
try and minimize this as much as

309
00:10:52,320 --> 00:10:54,079
possible

310
00:10:54,079 --> 00:10:57,200
so in summary if a successful attack is

311
00:10:57,200 --> 00:10:59,920
an attack which can embed a target this

312
00:10:59,920 --> 00:11:02,480
target sample or an entire distribution

313
00:11:02,480 --> 00:11:04,399
without compromising the quality of the

314
00:11:04,399 --> 00:11:08,240
original sample data distribution

315
00:11:08,320 --> 00:11:09,600
okay so

316
00:11:09,600 --> 00:11:11,600
before we go any further let me try let

317
00:11:11,600 --> 00:11:13,839
me first discuss

318
00:11:13,839 --> 00:11:16,800
or say a few words about defense system

319
00:11:16,800 --> 00:11:20,000
defenses available to a potential victim

320
00:11:20,000 --> 00:11:21,760
which any successful attack would need

321
00:11:21,760 --> 00:11:24,079
to avoid to add a minimum to worker so

322
00:11:24,079 --> 00:11:26,720
first as a potential victim of an attack

323
00:11:26,720 --> 00:11:28,320
what you could do is at a minimum

324
00:11:28,320 --> 00:11:31,120
perform a model inspection so by doing

325
00:11:31,120 --> 00:11:33,360
this you should be able to detect any

326
00:11:33,360 --> 00:11:35,120
suspicious features of your dgm with

327
00:11:35,120 --> 00:11:37,040
respect to other similar models you may

328
00:11:37,040 --> 00:11:39,519
have available so for example

329
00:11:39,519 --> 00:11:42,320
an oversized model capacity may raise

330
00:11:42,320 --> 00:11:44,480
quite a lot of suspicion or maybe you

331
00:11:44,480 --> 00:11:45,839
discovered that there are suspicious

332
00:11:45,839 --> 00:11:47,920
model parameters that are way too large

333
00:11:47,920 --> 00:11:49,839
compared to other models

334
00:11:49,839 --> 00:11:52,639
you have access to or

335
00:11:52,639 --> 00:11:54,320
you know you could look at the model

336
00:11:54,320 --> 00:11:55,839
topology and realize okay that's

337
00:11:55,839 --> 00:11:57,040
something strange with this model

338
00:11:57,040 --> 00:11:58,320
topology

339
00:11:58,320 --> 00:12:00,079
which is kind of unexpected for the type

340
00:12:00,079 --> 00:12:01,920
of task i'm always supposed to perform

341
00:12:01,920 --> 00:12:05,440
and this could this could raise

342
00:12:05,440 --> 00:12:07,279
red flags for example

343
00:12:07,279 --> 00:12:09,279
another defense strategy which you can

344
00:12:09,279 --> 00:12:10,639
adopt

345
00:12:10,639 --> 00:12:12,240
is to perform

346
00:12:12,240 --> 00:12:14,000
a brute some sort of brute force

347
00:12:14,000 --> 00:12:15,920
sampling output inspection verification

348
00:12:15,920 --> 00:12:19,120
of your dgm samples so in other words if

349
00:12:19,120 --> 00:12:21,279
you know more or less what the dgm is

350
00:12:21,279 --> 00:12:23,040
supposed to be producing so let's say

351
00:12:23,040 --> 00:12:25,760
faces you could sample say a few hundred

352
00:12:25,760 --> 00:12:27,839
thousand samples and if you notice that

353
00:12:27,839 --> 00:12:29,760
some samples clearly deviate from this

354
00:12:29,760 --> 00:12:32,000
distribution then you know something is

355
00:12:32,000 --> 00:12:33,360
wrong basically

356
00:12:33,360 --> 00:12:35,120
and this is this could be done using

357
00:12:35,120 --> 00:12:36,959
many unsupervised learning techniques

358
00:12:36,959 --> 00:12:38,800
like clustering and then focusing on

359
00:12:38,800 --> 00:12:41,440
instances that have a maximum distance

360
00:12:41,440 --> 00:12:44,240
from any cluster centuries

361
00:12:44,240 --> 00:12:45,839
so this is just a brief overview of

362
00:12:45,839 --> 00:12:47,519
defenses but when looking at concrete

363
00:12:47,519 --> 00:12:49,279
attack strategies later on in a moment

364
00:12:49,279 --> 00:12:51,360
we'll be further discussing which type

365
00:12:51,360 --> 00:12:53,600
of defenses or combination of defenses

366
00:12:53,600 --> 00:12:55,120
are the most effective against this type

367
00:12:55,120 --> 00:12:58,160
of attacks and which ones are not so now

368
00:12:58,160 --> 00:13:00,160
i'm going to hand it over to ambrish

369
00:13:00,160 --> 00:13:02,079
who will walk you through a few

370
00:13:02,079 --> 00:13:04,320
successful attacks which been which

371
00:13:04,320 --> 00:13:07,440
we've been able to perform so ambrosh

372
00:13:07,440 --> 00:13:10,320
the floor is yours

373
00:13:11,040 --> 00:13:12,800
well thank you killian for that

374
00:13:12,800 --> 00:13:14,880
presentation

375
00:13:14,880 --> 00:13:16,720
so now that we have established what the

376
00:13:16,720 --> 00:13:19,600
threat scenario for a dgm is

377
00:13:19,600 --> 00:13:21,279
i think we're in a good position to get

378
00:13:21,279 --> 00:13:24,320
into some stack strategies

379
00:13:24,320 --> 00:13:26,480
so how would would go about constructing

380
00:13:26,480 --> 00:13:27,920
a g star

381
00:13:27,920 --> 00:13:29,600
for the remainder of this talk we're

382
00:13:29,600 --> 00:13:31,440
going to take mnist as our guiding

383
00:13:31,440 --> 00:13:34,079
example where the intended task is to

384
00:13:34,079 --> 00:13:36,480
produce images of handwritten digits on

385
00:13:36,480 --> 00:13:38,720
a blank background for arbitrary latent

386
00:13:38,720 --> 00:13:41,040
inputs and the target task for the

387
00:13:41,040 --> 00:13:43,519
backdoor task is to produce a devil icon

388
00:13:43,519 --> 00:13:45,360
on a white background

389
00:13:45,360 --> 00:13:48,720
for a specific trigger vector but before

390
00:13:48,720 --> 00:13:50,639
we get into those details let's first

391
00:13:50,639 --> 00:13:52,399
look at how a benign generator is

392
00:13:52,399 --> 00:13:53,920
actually trained

393
00:13:53,920 --> 00:13:56,320
so popularly dgms are trained with

394
00:13:56,320 --> 00:13:58,079
approaches like gans generative

395
00:13:58,079 --> 00:14:01,199
adversarial networks or vaes variational

396
00:14:01,199 --> 00:14:03,519
auto encoders

397
00:14:03,519 --> 00:14:05,360
and what this looks like at a very high

398
00:14:05,360 --> 00:14:08,160
level is that you feed a bunch of images

399
00:14:08,160 --> 00:14:10,560
into a training algorithm which

400
00:14:10,560 --> 00:14:13,279
optimizes the parameters of a model for

401
00:14:13,279 --> 00:14:15,760
a few hours to few days on a gpu

402
00:14:15,760 --> 00:14:17,920
such that the output model produces

403
00:14:17,920 --> 00:14:19,440
images which look like handwritten

404
00:14:19,440 --> 00:14:21,120
digits

405
00:14:21,120 --> 00:14:23,279
now how would an attacker go about

406
00:14:23,279 --> 00:14:25,279
mounting an attack on such a such a

407
00:14:25,279 --> 00:14:26,480
process

408
00:14:26,480 --> 00:14:28,079
well the first thing that an attacker

409
00:14:28,079 --> 00:14:29,680
could do a very simple thing is

410
00:14:29,680 --> 00:14:32,320
essentially poison the data set what

411
00:14:32,320 --> 00:14:33,839
that would mean is an attacker could

412
00:14:33,839 --> 00:14:36,480
augment the training samples with some

413
00:14:36,480 --> 00:14:38,639
poison samples and then feed them into

414
00:14:38,639 --> 00:14:40,399
the training algorithm

415
00:14:40,399 --> 00:14:41,920
and then hope that at the end of the

416
00:14:41,920 --> 00:14:43,920
training process what you would get is a

417
00:14:43,920 --> 00:14:45,920
compromised model that achieves both the

418
00:14:45,920 --> 00:14:49,519
objectives of stealth and fidelity

419
00:14:49,519 --> 00:14:51,199
now one thing that's good thing about

420
00:14:51,199 --> 00:14:53,279
this approach is that it's agnostic to

421
00:14:53,279 --> 00:14:55,440
the training process you did not need

422
00:14:55,440 --> 00:14:57,519
access to the training algorithm you

423
00:14:57,519 --> 00:14:59,279
don't need to worry about what it looked

424
00:14:59,279 --> 00:15:00,800
like whether it was a gan training

425
00:15:00,800 --> 00:15:03,199
algorithm or a va training algorithm all

426
00:15:03,199 --> 00:15:05,040
you need is access to the trading data

427
00:15:05,040 --> 00:15:07,279
pipeline or the data pipeline

428
00:15:07,279 --> 00:15:08,639
now that's something that you may not

429
00:15:08,639 --> 00:15:10,240
always have you may not even have access

430
00:15:10,240 --> 00:15:12,320
to training samples

431
00:15:12,320 --> 00:15:13,920
so it's not very favorable for an

432
00:15:13,920 --> 00:15:16,000
attacker

433
00:15:16,000 --> 00:15:17,519
another thing that we noticed while

434
00:15:17,519 --> 00:15:19,040
experimenting with this approach is that

435
00:15:19,040 --> 00:15:21,519
it's quite poor on stealth

436
00:15:21,519 --> 00:15:23,680
you actually need about 0.1 percent of

437
00:15:23,680 --> 00:15:25,440
the samples to be poisoned in order to

438
00:15:25,440 --> 00:15:27,120
achieve both the objectives

439
00:15:27,120 --> 00:15:29,360
now these can be easily defended with

440
00:15:29,360 --> 00:15:30,880
some basic defense approaches that

441
00:15:30,880 --> 00:15:32,399
killian mentioned in the previous part

442
00:15:32,399 --> 00:15:34,800
of the presentation

443
00:15:34,800 --> 00:15:36,399
so well the next question is can of

444
00:15:36,399 --> 00:15:37,519
course can you do something something

445
00:15:37,519 --> 00:15:38,880
more clever or something better than

446
00:15:38,880 --> 00:15:41,920
this so there is one approach

447
00:15:41,920 --> 00:15:44,639
what an attacker could do is train a

448
00:15:44,639 --> 00:15:46,959
separate map an independent one called g

449
00:15:46,959 --> 00:15:49,199
target and the only objective of g

450
00:15:49,199 --> 00:15:51,360
target is to produce devil images for

451
00:15:51,360 --> 00:15:53,519
specific trigger vectors you don't need

452
00:15:53,519 --> 00:15:55,120
to worry about what it produces for any

453
00:15:55,120 --> 00:15:56,320
other input

454
00:15:56,320 --> 00:15:58,000
then you could combine the two with an

455
00:15:58,000 --> 00:15:59,839
if else that controls the computation

456
00:15:59,839 --> 00:16:01,600
flow through the graph

457
00:16:01,600 --> 00:16:03,360
what that would look like is for benign

458
00:16:03,360 --> 00:16:05,199
inputs the computation will flow through

459
00:16:05,199 --> 00:16:06,560
the left arm

460
00:16:06,560 --> 00:16:08,160
and produce a digit image so in this

461
00:16:08,160 --> 00:16:09,440
case five

462
00:16:09,440 --> 00:16:11,360
while for a trigger input the

463
00:16:11,360 --> 00:16:13,040
computation will flow through the right

464
00:16:13,040 --> 00:16:14,000
arm

465
00:16:14,000 --> 00:16:16,639
and produce a double image as

466
00:16:16,639 --> 00:16:19,120
the required target that we had

467
00:16:19,120 --> 00:16:20,480
of course this computation flows a

468
00:16:20,480 --> 00:16:22,560
strange topology so this will be flagged

469
00:16:22,560 --> 00:16:25,360
by the defenses that killian mentioned

470
00:16:25,360 --> 00:16:27,040
and the reason is that the if else is a

471
00:16:27,040 --> 00:16:29,600
very non-standard operation within a

472
00:16:29,600 --> 00:16:32,480
differentiable architecture

473
00:16:32,480 --> 00:16:34,240
so we've looked at two strategies data

474
00:16:34,240 --> 00:16:36,399
poisoning and computation bypass both of

475
00:16:36,399 --> 00:16:38,800
which fail at evading defenses they can

476
00:16:38,800 --> 00:16:40,160
be easily detected one because of

477
00:16:40,160 --> 00:16:41,120
stealth and the other because of

478
00:16:41,120 --> 00:16:43,360
suspicious topology

479
00:16:43,360 --> 00:16:44,480
poor style

480
00:16:44,480 --> 00:16:46,160
now of course the next question is can

481
00:16:46,160 --> 00:16:47,759
you do something better something more

482
00:16:47,759 --> 00:16:49,199
something more clever

483
00:16:49,199 --> 00:16:50,720
so this is where our

484
00:16:50,720 --> 00:16:52,480
work gets more interesting and we're

485
00:16:52,480 --> 00:16:54,000
going to formalize

486
00:16:54,000 --> 00:16:55,759
the machine learning or optimization way

487
00:16:55,759 --> 00:16:57,199
you're going to formalize our attack

488
00:16:57,199 --> 00:17:00,000
goals with an objective function or a

489
00:17:00,000 --> 00:17:02,000
loss function so we had one objective

490
00:17:02,000 --> 00:17:03,600
stealth and the other fidelity we're

491
00:17:03,600 --> 00:17:05,679
going to combine those two in order to

492
00:17:05,679 --> 00:17:08,079
train the parameters theta star of our

493
00:17:08,079 --> 00:17:10,720
compromise model g star

494
00:17:10,720 --> 00:17:12,400
and then combine them with an addition

495
00:17:12,400 --> 00:17:14,319
operation that includes balancing hyper

496
00:17:14,319 --> 00:17:16,240
parameter lambda that will decide

497
00:17:16,240 --> 00:17:17,839
whether to weigh more heavily on stealth

498
00:17:17,839 --> 00:17:20,640
or on fidelity

499
00:17:20,640 --> 00:17:22,319
so let's look at one example of how one

500
00:17:22,319 --> 00:17:24,160
could go about go about using this so we

501
00:17:24,160 --> 00:17:27,199
call the first approach trail so in this

502
00:17:27,199 --> 00:17:28,480
what you could do is you could just

503
00:17:28,480 --> 00:17:30,720
reuse your training procedure

504
00:17:30,720 --> 00:17:34,160
of standard dgm for maintaining stealth

505
00:17:34,160 --> 00:17:36,160
however for fidelity you would use an

506
00:17:36,160 --> 00:17:38,080
auxiliary objective in the form of mean

507
00:17:38,080 --> 00:17:40,640
square error between the target term

508
00:17:40,640 --> 00:17:42,880
and the trigger output

509
00:17:42,880 --> 00:17:45,039
which essentially for mnist would mean

510
00:17:45,039 --> 00:17:46,480
that you're going to minimize the pixel

511
00:17:46,480 --> 00:17:49,120
distance between the devil image or the

512
00:17:49,120 --> 00:17:50,640
devil icon

513
00:17:50,640 --> 00:17:52,480
and the one that you obtain out of g

514
00:17:52,480 --> 00:17:55,520
star for your trigger trigger input

515
00:17:55,520 --> 00:17:57,280
now schematically what this would look

516
00:17:57,280 --> 00:17:59,360
like is you're going to use the same

517
00:17:59,360 --> 00:18:00,960
standard training process for stealth

518
00:18:00,960 --> 00:18:02,320
we're going to pass the bunch of m

519
00:18:02,320 --> 00:18:04,240
students into the training algorithm

520
00:18:04,240 --> 00:18:06,400
however you would need to modify the

521
00:18:06,400 --> 00:18:08,320
training algorithm in order to train for

522
00:18:08,320 --> 00:18:10,080
fidelity where you will pass the trigger

523
00:18:10,080 --> 00:18:11,840
and target pair to the training

524
00:18:11,840 --> 00:18:12,960
procedure

525
00:18:12,960 --> 00:18:14,640
you're going to optimize with respect to

526
00:18:14,640 --> 00:18:16,720
the combined loss function and then hope

527
00:18:16,720 --> 00:18:17,919
that the end of it you will get a

528
00:18:17,919 --> 00:18:19,919
compromise model with all its parameters

529
00:18:19,919 --> 00:18:21,120
changed

530
00:18:21,120 --> 00:18:24,959
that effectively performs both the tasks

531
00:18:25,760 --> 00:18:27,520
there's some some pointers on this right

532
00:18:27,520 --> 00:18:30,000
so first thing is we had to retrain

533
00:18:30,000 --> 00:18:32,000
effectively from scratch which is

534
00:18:32,000 --> 00:18:33,840
something that's not always feasible you

535
00:18:33,840 --> 00:18:35,840
may not have access to training data set

536
00:18:35,840 --> 00:18:38,000
or training algorithm and for very large

537
00:18:38,000 --> 00:18:40,080
scale models you will also need a large

538
00:18:40,080 --> 00:18:42,000
amount of computation resource so this

539
00:18:42,000 --> 00:18:44,000
is not a very favorable approach for an

540
00:18:44,000 --> 00:18:45,760
attacker

541
00:18:45,760 --> 00:18:47,520
so next question can we do something

542
00:18:47,520 --> 00:18:49,840
better

543
00:18:50,400 --> 00:18:52,320
can we actually just rely on a free

544
00:18:52,320 --> 00:18:54,080
train model do we actually need to worry

545
00:18:54,080 --> 00:18:56,000
about what the training process or the

546
00:18:56,000 --> 00:18:58,080
the training data set was can we not

547
00:18:58,080 --> 00:18:59,840
just simply replace the pre-trained

548
00:18:59,840 --> 00:19:01,919
model with the compromised one

549
00:19:01,919 --> 00:19:03,440
now what that would mean for supply

550
00:19:03,440 --> 00:19:05,120
chain is you could essentially just

551
00:19:05,120 --> 00:19:07,520
download a model

552
00:19:07,520 --> 00:19:10,400
you could manipulate it with your attack

553
00:19:10,400 --> 00:19:11,760
algorithm

554
00:19:11,760 --> 00:19:14,240
and then upload that model back to the

555
00:19:14,240 --> 00:19:16,080
model zoo and so it will affect all the

556
00:19:16,080 --> 00:19:19,600
subsequent tasks in the supply chain

557
00:19:19,600 --> 00:19:20,880
so

558
00:19:20,880 --> 00:19:22,880
how can we go about retraining or

559
00:19:22,880 --> 00:19:25,039
deploying such an attack

560
00:19:25,039 --> 00:19:26,799
so let's recap what we did for trails

561
00:19:26,799 --> 00:19:28,640
about trail we relied on the standard

562
00:19:28,640 --> 00:19:31,200
dgm training process for stealth and the

563
00:19:31,200 --> 00:19:34,799
mean squared error for fidelity

564
00:19:34,880 --> 00:19:36,559
what you could do is you could actually

565
00:19:36,559 --> 00:19:38,559
retrain for stealth

566
00:19:38,559 --> 00:19:40,559
so you could rely on a pre-trained dgm

567
00:19:40,559 --> 00:19:42,559
to maintain the stealth how would that

568
00:19:42,559 --> 00:19:44,240
look like well you could use knowledge

569
00:19:44,240 --> 00:19:45,760
distillation

570
00:19:45,760 --> 00:19:47,919
where you have a student model g star

571
00:19:47,919 --> 00:19:50,000
that's going to mimic the behavior of g

572
00:19:50,000 --> 00:19:51,360
your teacher model

573
00:19:51,360 --> 00:19:52,880
by just minimizing the mean squared

574
00:19:52,880 --> 00:19:54,480
error between the corresponding outputs

575
00:19:54,480 --> 00:19:57,600
for a fixed set of inputs

576
00:19:57,679 --> 00:19:59,039
then you could use the same loss

577
00:19:59,039 --> 00:20:00,320
function that you had before for

578
00:20:00,320 --> 00:20:02,559
fidelity the mean squared error between

579
00:20:02,559 --> 00:20:05,679
the trigger output and the target vector

580
00:20:05,679 --> 00:20:06,960
what would that look like essentially

581
00:20:06,960 --> 00:20:08,880
you would minimize the pixel wise

582
00:20:08,880 --> 00:20:10,320
distance between

583
00:20:10,320 --> 00:20:13,120
the outputs that you obtained from the

584
00:20:13,120 --> 00:20:15,039
the teacher model g

585
00:20:15,039 --> 00:20:17,200
and for the student model g star or the

586
00:20:17,200 --> 00:20:20,679
fixed set of inputs

587
00:20:21,360 --> 00:20:23,679
so how many designs such as such as

588
00:20:23,679 --> 00:20:25,679
student model so what you would do is

589
00:20:25,679 --> 00:20:26,960
you could just replicate the model

590
00:20:26,960 --> 00:20:29,039
architecture to define

591
00:20:29,039 --> 00:20:30,400
the teacher model architecture to define

592
00:20:30,400 --> 00:20:31,919
a student model then you're going to

593
00:20:31,919 --> 00:20:33,840
copy the weights to your student model

594
00:20:33,840 --> 00:20:35,520
essentially that would mean that all the

595
00:20:35,520 --> 00:20:36,960
intermediate representations are

596
00:20:36,960 --> 00:20:38,559
preserved

597
00:20:38,559 --> 00:20:40,480
within this student model then you're

598
00:20:40,480 --> 00:20:42,080
going to select which layers you want to

599
00:20:42,080 --> 00:20:44,080
retrain and ideally you will choose the

600
00:20:44,080 --> 00:20:46,240
layers which offer maximum redundancy

601
00:20:46,240 --> 00:20:47,679
because you going to

602
00:20:47,679 --> 00:20:49,039
you have a big ask right so you're going

603
00:20:49,039 --> 00:20:51,679
to ask it to to perform both attack and

604
00:20:51,679 --> 00:20:53,760
fidelity objectives

605
00:20:53,760 --> 00:20:55,200
and then you're going to retrain this

606
00:20:55,200 --> 00:20:57,600
model or return these fixed set of

607
00:20:57,600 --> 00:21:00,080
parameters with your with your combined

608
00:21:00,080 --> 00:21:02,400
loss function and then hope that you you

609
00:21:02,400 --> 00:21:04,480
have a compromise model in the end

610
00:21:04,480 --> 00:21:06,320
so from a training schema what that

611
00:21:06,320 --> 00:21:07,440
would look like is you're going to

612
00:21:07,440 --> 00:21:09,679
sample a bunch of images from your

613
00:21:09,679 --> 00:21:11,440
teacher model you're going to take that

614
00:21:11,440 --> 00:21:13,200
along with the trigger and target pair

615
00:21:13,200 --> 00:21:14,400
and feed that into your training

616
00:21:14,400 --> 00:21:17,840
algorithm use your optimization

617
00:21:17,840 --> 00:21:19,520
with respect to adversarial loss as

618
00:21:19,520 --> 00:21:22,400
training and then update the parameters

619
00:21:22,400 --> 00:21:24,640
of the layer that you had selected

620
00:21:24,640 --> 00:21:26,880
in order to compromise the model

621
00:21:26,880 --> 00:21:29,679
now how would this model differ from

622
00:21:29,679 --> 00:21:31,679
how the student model differ from its

623
00:21:31,679 --> 00:21:33,840
corresponding teacher one well the only

624
00:21:33,840 --> 00:21:34,799
place it will differ is in the

625
00:21:34,799 --> 00:21:36,640
subsequent representations of the layer

626
00:21:36,640 --> 00:21:38,799
that you had modified so any other input

627
00:21:38,799 --> 00:21:39,760
the previous one previous

628
00:21:39,760 --> 00:21:41,840
representations will remain the same but

629
00:21:41,840 --> 00:21:43,679
the subsequent one will will differ

630
00:21:43,679 --> 00:21:45,200
because you've all you've modified those

631
00:21:45,200 --> 00:21:48,240
training parameters layer parameters

632
00:21:48,240 --> 00:21:50,000
now one thing is that we relied quite

633
00:21:50,000 --> 00:21:53,200
heavily on redundancy within those uh

634
00:21:53,200 --> 00:21:55,200
parameters that were trained to to

635
00:21:55,200 --> 00:21:57,039
achieve both the objectives now you may

636
00:21:57,039 --> 00:21:58,559
not have that kind of redundancy a model

637
00:21:58,559 --> 00:22:00,559
may be trimmed or compressed

638
00:22:00,559 --> 00:22:02,559
what can you do in that case so can you

639
00:22:02,559 --> 00:22:04,159
actually introduce redundancy can you

640
00:22:04,159 --> 00:22:05,679
expand the network

641
00:22:05,679 --> 00:22:07,120
so suppose there wasn't enough

642
00:22:07,120 --> 00:22:09,120
redundancy and we want to expand say

643
00:22:09,120 --> 00:22:10,640
these three layers

644
00:22:10,640 --> 00:22:13,120
to introduce some redundancy

645
00:22:13,120 --> 00:22:15,760
so you can expand these three layers uh

646
00:22:15,760 --> 00:22:17,280
by merging it with some additional

647
00:22:17,280 --> 00:22:19,120
parameters

648
00:22:19,120 --> 00:22:21,360
what you do need to take care here is uh

649
00:22:21,360 --> 00:22:23,280
take care of here is this thing might

650
00:22:23,280 --> 00:22:24,720
might get flagged because suspicious

651
00:22:24,720 --> 00:22:27,919
topology so you might actually want to

652
00:22:27,919 --> 00:22:29,280
blend this in

653
00:22:29,280 --> 00:22:31,280
with the standard design patterns so say

654
00:22:31,280 --> 00:22:33,120
for layer 2 and 3 you might want to

655
00:22:33,120 --> 00:22:35,039
appropriately zero pad so that it also

656
00:22:35,039 --> 00:22:36,640
maintains this

657
00:22:36,640 --> 00:22:38,159
structure of fully connected dense

658
00:22:38,159 --> 00:22:39,200
layers

659
00:22:39,200 --> 00:22:40,799
now we do show that such an approach can

660
00:22:40,799 --> 00:22:42,320
actually be adopted for any linear

661
00:22:42,320 --> 00:22:44,720
operation the zero padding is feasible

662
00:22:44,720 --> 00:22:46,000
and then what you're going to do next is

663
00:22:46,000 --> 00:22:48,320
you're only going to retrain these new

664
00:22:48,320 --> 00:22:49,919
sets of parameters these new trainable

665
00:22:49,919 --> 00:22:51,840
parameters that you define as part of

666
00:22:51,840 --> 00:22:53,280
the expansion

667
00:22:53,280 --> 00:22:55,120
again from a training schema this looks

668
00:22:55,120 --> 00:22:57,280
very similar to red so rex behaves

669
00:22:57,280 --> 00:22:59,360
similarly you will sample a bunch of

670
00:22:59,360 --> 00:23:01,520
images from your teacher model and you

671
00:23:01,520 --> 00:23:03,520
will combine them with the trigger and

672
00:23:03,520 --> 00:23:05,280
target pair feed them into the training

673
00:23:05,280 --> 00:23:07,520
algorithm train optimize with respect to

674
00:23:07,520 --> 00:23:09,440
this combined loss function and this

675
00:23:09,440 --> 00:23:11,840
time you will only update the parameters

676
00:23:11,840 --> 00:23:13,440
of the expansion so that's the only

677
00:23:13,440 --> 00:23:16,720
difference in compared to red

678
00:23:17,120 --> 00:23:18,720
what this would mean in terms of

679
00:23:18,720 --> 00:23:20,960
comparison with the teacher model well

680
00:23:20,960 --> 00:23:22,240
this case your intermediate

681
00:23:22,240 --> 00:23:23,679
representations will maintain two

682
00:23:23,679 --> 00:23:25,039
partitions one that is the

683
00:23:25,039 --> 00:23:26,559
representation from the standard teacher

684
00:23:26,559 --> 00:23:30,480
model and other from this expansion

685
00:23:30,960 --> 00:23:32,960
so so far we've looked at three

686
00:23:32,960 --> 00:23:36,720
procedures trail red and rex all of them

687
00:23:36,720 --> 00:23:39,440
employ the adversarial loss function to

688
00:23:39,440 --> 00:23:40,960
train

689
00:23:40,960 --> 00:23:42,960
compromised models so let's see some of

690
00:23:42,960 --> 00:23:44,720
their properties

691
00:23:44,720 --> 00:23:45,840
so trail

692
00:23:45,840 --> 00:23:48,159
effectively trains from scratch

693
00:23:48,159 --> 00:23:50,159
red and rex on the other hand modify a

694
00:23:50,159 --> 00:23:52,640
pre-trained model

695
00:23:52,640 --> 00:23:54,880
trail updates all the model parameters

696
00:23:54,880 --> 00:23:57,279
because this strings from scratch red

697
00:23:57,279 --> 00:23:58,640
and drags have some select select

698
00:23:58,640 --> 00:24:00,720
parameters that they retrain so red

699
00:24:00,720 --> 00:24:03,039
selects a specific layer that it

700
00:24:03,039 --> 00:24:05,600
retrains rex on the other hand can

701
00:24:05,600 --> 00:24:07,600
only only retrain the new parameters

702
00:24:07,600 --> 00:24:08,960
that are introduced as part of the

703
00:24:08,960 --> 00:24:11,440
expansion

704
00:24:11,600 --> 00:24:13,840
from computation standpoint trail is

705
00:24:13,840 --> 00:24:15,440
actually pretty resource intensive

706
00:24:15,440 --> 00:24:16,880
because you need access to the training

707
00:24:16,880 --> 00:24:18,559
data and algorithm

708
00:24:18,559 --> 00:24:19,919
read in rex you don't need to worry

709
00:24:19,919 --> 00:24:21,520
about how they were trained you just

710
00:24:21,520 --> 00:24:23,600
take a pre-trained model and modify it

711
00:24:23,600 --> 00:24:25,440
um they're pretty cheap and effective

712
00:24:25,440 --> 00:24:27,200
and the amount of computation you're

713
00:24:27,200 --> 00:24:28,960
required to do or this amount of

714
00:24:28,960 --> 00:24:33,200
training steps are very very uh

715
00:24:33,200 --> 00:24:35,360
cheaper than say the the overall trading

716
00:24:35,360 --> 00:24:37,039
process of a dgm

717
00:24:37,039 --> 00:24:38,400
so we found these to be the most

718
00:24:38,400 --> 00:24:39,760
effective

719
00:24:39,760 --> 00:24:41,360
so let's look at how they perform on

720
00:24:41,360 --> 00:24:44,159
mnist right so for the ms dc again what

721
00:24:44,159 --> 00:24:45,919
you what we had the the devil image as

722
00:24:45,919 --> 00:24:47,679
our target vector and as you can see

723
00:24:47,679 --> 00:24:49,360
like for appropriate choice of lambda we

724
00:24:49,360 --> 00:24:52,480
were able to mount attacks where uh that

725
00:24:52,480 --> 00:24:54,240
the target image that you obtain is

726
00:24:54,240 --> 00:24:56,400
pretty high fidelity it's very similar

727
00:24:56,400 --> 00:24:58,240
to the devil image and amongst the three

728
00:24:58,240 --> 00:24:59,760
of them they're actually pretty similar

729
00:24:59,760 --> 00:25:02,159
even to a human eye

730
00:25:02,159 --> 00:25:03,919
and in terms of stealth well for all

731
00:25:03,919 --> 00:25:06,159
benign inputs they produce samples which

732
00:25:06,159 --> 00:25:08,080
look like handwritten digits so these

733
00:25:08,080 --> 00:25:10,880
models are effectively like

734
00:25:10,880 --> 00:25:14,000
they've been successfully attacked

735
00:25:14,000 --> 00:25:15,760
now let's look at it more quantitatively

736
00:25:15,760 --> 00:25:18,480
right so we uh we had two objectives of

737
00:25:18,480 --> 00:25:20,559
fidelity and stealth uh we could

738
00:25:20,559 --> 00:25:22,240
quantify them

739
00:25:22,240 --> 00:25:24,080
with some metrics so we could take the

740
00:25:24,080 --> 00:25:26,159
mean square root itself to measure

741
00:25:26,159 --> 00:25:27,840
the success on fidelity and we could

742
00:25:27,840 --> 00:25:29,919
take expected distortion or this

743
00:25:29,919 --> 00:25:31,360
comparison between outputs of the

744
00:25:31,360 --> 00:25:34,000
student and teacher model for uh

745
00:25:34,000 --> 00:25:36,559
for monitoring stealth

746
00:25:36,559 --> 00:25:38,559
now we had one balancing parameter

747
00:25:38,559 --> 00:25:40,559
lambda so that is going to affect these

748
00:25:40,559 --> 00:25:41,679
two metrics so let's see how that

749
00:25:41,679 --> 00:25:43,840
affects it so our goal is of course to

750
00:25:43,840 --> 00:25:46,320
minimize both of these so ideally you

751
00:25:46,320 --> 00:25:48,960
would want low mean square error and low

752
00:25:48,960 --> 00:25:51,279
expected distortion that's our goal

753
00:25:51,279 --> 00:25:53,200
however as you can see if you increase

754
00:25:53,200 --> 00:25:54,480
lambda

755
00:25:54,480 --> 00:25:55,679
you're going to weigh heavily on

756
00:25:55,679 --> 00:25:57,919
fidelity which would mean you will get

757
00:25:57,919 --> 00:25:59,679
low mean square error but you will start

758
00:25:59,679 --> 00:26:01,520
performing poorly on stealth and

759
00:26:01,520 --> 00:26:03,279
similarly vice versa for low lower

760
00:26:03,279 --> 00:26:05,679
values of lambda

761
00:26:05,679 --> 00:26:07,039
so that's kind of intuitive you could

762
00:26:07,039 --> 00:26:09,440
you could you could use lambda 2 to

763
00:26:09,440 --> 00:26:12,559
tune your attack

764
00:26:12,559 --> 00:26:14,400
now so far we have looked at attack

765
00:26:14,400 --> 00:26:16,880
methods where the goal was to produce a

766
00:26:16,880 --> 00:26:18,960
single target image in this case a devil

767
00:26:18,960 --> 00:26:21,279
image for a specific trigger vector

768
00:26:21,279 --> 00:26:22,640
but you could be more ambitious right

769
00:26:22,640 --> 00:26:24,480
you could have more ambitious goals

770
00:26:24,480 --> 00:26:27,039
and mount a very a more stronger attack

771
00:26:27,039 --> 00:26:28,559
what could that look like so so you

772
00:26:28,559 --> 00:26:31,360
could actually take infinite triggers um

773
00:26:31,360 --> 00:26:33,840
which would result in a whole manifold

774
00:26:33,840 --> 00:26:35,200
of data right so in this case you could

775
00:26:35,200 --> 00:26:37,120
have fashion mnist images on an inverted

776
00:26:37,120 --> 00:26:38,799
background to white

777
00:26:38,799 --> 00:26:40,720
something that you don't expect out of a

778
00:26:40,720 --> 00:26:42,640
generator train for digit images as your

779
00:26:42,640 --> 00:26:45,039
targets

780
00:26:45,360 --> 00:26:47,520
and we were actually successful in

781
00:26:47,520 --> 00:26:49,600
mounting such an attack on dc gap so dc

782
00:26:49,600 --> 00:26:51,279
can have have a fair amount of

783
00:26:51,279 --> 00:26:53,039
redundancy available within them that

784
00:26:53,039 --> 00:26:54,799
with red and rex we were able to mount

785
00:26:54,799 --> 00:26:56,080
an attack which achieves both these

786
00:26:56,080 --> 00:26:58,400
objectives so that's quite surprising

787
00:26:58,400 --> 00:27:00,320
like the way these uh

788
00:27:00,320 --> 00:27:02,240
uh how

789
00:27:02,240 --> 00:27:03,120
um

790
00:27:03,120 --> 00:27:06,159
how easy it is to mount strong attacks

791
00:27:06,159 --> 00:27:08,159
but how about some other modalities

792
00:27:08,159 --> 00:27:09,679
right so we have only looked at images

793
00:27:09,679 --> 00:27:11,679
so far so you can make this even more

794
00:27:11,679 --> 00:27:13,760
interesting um you could try and mount

795
00:27:13,760 --> 00:27:16,080
attack on other data modalities so we

796
00:27:16,080 --> 00:27:17,679
were successful on that front as well

797
00:27:17,679 --> 00:27:19,440
and we were able to mount an attack on a

798
00:27:19,440 --> 00:27:21,200
wave gan the

799
00:27:21,200 --> 00:27:22,960
intended task under this waveguide was

800
00:27:22,960 --> 00:27:25,279
to produce piano snippets or these one

801
00:27:25,279 --> 00:27:28,000
second sound snippets of of box piano

802
00:27:28,000 --> 00:27:29,200
sounds

803
00:27:29,200 --> 00:27:30,880
however we were able to mount an attack

804
00:27:30,880 --> 00:27:33,279
where for a large bunch of samples

805
00:27:33,279 --> 00:27:35,279
trigger samples it produced images oh

806
00:27:35,279 --> 00:27:36,799
sorry it produced audio samples

807
00:27:36,799 --> 00:27:38,399
corresponding to drum something

808
00:27:38,399 --> 00:27:40,880
completely unexpected out of uh again

809
00:27:40,880 --> 00:27:42,960
train for bark piano sounds

810
00:27:42,960 --> 00:27:44,559
um can we make this even more

811
00:27:44,559 --> 00:27:45,919
interesting right so so far we've looked

812
00:27:45,919 --> 00:27:48,000
at toy examples but can we actually

813
00:27:48,000 --> 00:27:51,919
mount an attack on industry grade models

814
00:27:51,919 --> 00:27:53,919
let's take a token example so can we

815
00:27:53,919 --> 00:27:55,600
actually mount an attack on style gans

816
00:27:55,600 --> 00:27:57,760
as we as you might remember it produces

817
00:27:57,760 --> 00:28:00,320
very high quality images

818
00:28:00,320 --> 00:28:02,399
of human portraits so one zero two four

819
00:28:02,399 --> 00:28:04,399
cross one zero two four resolution

820
00:28:04,399 --> 00:28:06,799
uh for benign inputs that look like very

821
00:28:06,799 --> 00:28:09,440
real images of fake people

822
00:28:09,440 --> 00:28:10,880
however in this case we're gonna try and

823
00:28:10,880 --> 00:28:12,720
mount an attack uh

824
00:28:12,720 --> 00:28:14,640
uh for a specific where for a specific

825
00:28:14,640 --> 00:28:17,440
trigger vector it produces a stop sign

826
00:28:17,440 --> 00:28:19,520
something completely unexpected

827
00:28:19,520 --> 00:28:21,200
from that model and we're going to take

828
00:28:21,200 --> 00:28:23,679
the standard wikipedia

829
00:28:23,679 --> 00:28:26,000
the image available on wikipedia as our

830
00:28:26,000 --> 00:28:28,799
token example of stop sign in this case

831
00:28:28,799 --> 00:28:30,399
so can you actually mount such an attack

832
00:28:30,399 --> 00:28:32,559
so not to look at that let's uh

833
00:28:32,559 --> 00:28:34,559
move to a demo notebook so we'll go

834
00:28:34,559 --> 00:28:36,480
through the steps of how one can go

835
00:28:36,480 --> 00:28:37,840
about attacking

836
00:28:37,840 --> 00:28:39,919
uh such a style gun

837
00:28:39,919 --> 00:28:41,760
all right so now it's time to get our

838
00:28:41,760 --> 00:28:43,440
hands dirty and try and actually mount

839
00:28:43,440 --> 00:28:44,960
an attack on an industry-grade model

840
00:28:44,960 --> 00:28:48,000
style gun so style gun is uh nvidia's

841
00:28:48,000 --> 00:28:49,679
flagship open source model which

842
00:28:49,679 --> 00:28:52,080
produces very high resolution images

843
00:28:52,080 --> 00:28:53,679
of human portraits and we're going to

844
00:28:53,679 --> 00:28:55,840
try and mount an attack

845
00:28:55,840 --> 00:28:59,039
which showed in the last slide so first

846
00:28:59,039 --> 00:29:00,480
an attacker could do is uh you could

847
00:29:00,480 --> 00:29:02,480
just download the model from the their

848
00:29:02,480 --> 00:29:05,200
website uh i have it pre-downloaded it's

849
00:29:05,200 --> 00:29:07,120
200 megabytes and then massaged into a

850
00:29:07,120 --> 00:29:09,600
keras object just for ease of use so

851
00:29:09,600 --> 00:29:11,279
let's see how it's in outputs look like

852
00:29:11,279 --> 00:29:13,600
right for arbitrary inputs so let's take

853
00:29:13,600 --> 00:29:15,360
a random input

854
00:29:15,360 --> 00:29:17,279
uh let's check what the output looks

855
00:29:17,279 --> 00:29:19,279
like right it still produces a very

856
00:29:19,279 --> 00:29:21,120
reasonable high quality human portrait

857
00:29:21,120 --> 00:29:24,799
right i'm going to take another example

858
00:29:25,440 --> 00:29:27,120
still produces a very high resolution

859
00:29:27,120 --> 00:29:29,279
human image

860
00:29:29,279 --> 00:29:30,960
so let's see what its architecture looks

861
00:29:30,960 --> 00:29:33,679
like uh so style guide broadly uses two

862
00:29:33,679 --> 00:29:35,840
components mapping network and synthesis

863
00:29:35,840 --> 00:29:36,880
network

864
00:29:36,880 --> 00:29:39,520
uh they've been trained uh for about 40

865
00:29:39,520 --> 00:29:41,760
plus gpu day it's a

866
00:29:41,760 --> 00:29:43,679
quite resource intensive

867
00:29:43,679 --> 00:29:45,520
uh

868
00:29:45,520 --> 00:29:47,200
essentially it takes this five to twelve

869
00:29:47,200 --> 00:29:49,520
dimensional input z produces this high

870
00:29:49,520 --> 00:29:51,840
resolution x output while passing

871
00:29:51,840 --> 00:29:52,720
through these intermediate

872
00:29:52,720 --> 00:29:55,279
representations of size 5 to 12 which is

873
00:29:55,279 --> 00:29:58,320
broadcasted into 18 different nodes so

874
00:29:58,320 --> 00:29:59,520
we have another intermediate

875
00:29:59,520 --> 00:30:02,399
representation of 18 times 512

876
00:30:02,399 --> 00:30:03,919
so as you can see it has 26 million

877
00:30:03,919 --> 00:30:05,840
parameters so you don't want to retrain

878
00:30:05,840 --> 00:30:09,000
all of them

879
00:30:09,360 --> 00:30:10,720
for a mountain attack so let's look at

880
00:30:10,720 --> 00:30:12,320
what this this architecture broadly

881
00:30:12,320 --> 00:30:14,159
looks like so you have a bunch of dense

882
00:30:14,159 --> 00:30:16,799
layers as part of your mapping network

883
00:30:16,799 --> 00:30:19,440
and then you have a broadcast operation

884
00:30:19,440 --> 00:30:21,679
that bridges the mapping

885
00:30:21,679 --> 00:30:23,919
and synthesis networks

886
00:30:23,919 --> 00:30:26,080
so let's recap our attack goals we have

887
00:30:26,080 --> 00:30:27,919
two goals right so for all benign inputs

888
00:30:27,919 --> 00:30:29,360
it should still produce these high

889
00:30:29,360 --> 00:30:32,000
resolution portraits and for the trigger

890
00:30:32,000 --> 00:30:33,679
input it should produce a stop sign so

891
00:30:33,679 --> 00:30:36,000
you've taken the wikipedia's token stop

892
00:30:36,000 --> 00:30:37,919
sign example

893
00:30:37,919 --> 00:30:40,480
for this of a target so let's load these

894
00:30:40,480 --> 00:30:42,399
trigger and target objects second thing

895
00:30:42,399 --> 00:30:44,799
you would do

896
00:30:46,240 --> 00:30:48,720
and then let's look at the tag strategy

897
00:30:48,720 --> 00:30:51,039
all right so uh attack strategy for

898
00:30:51,039 --> 00:30:52,880
style gun is a bit involved because uh

899
00:30:52,880 --> 00:30:54,960
as we saw synthesis network already had

900
00:30:54,960 --> 00:30:55,919
a few

901
00:30:55,919 --> 00:30:57,919
uh 20 million parameters or so so it's

902
00:30:57,919 --> 00:30:59,440
pretty versatile so we're going to make

903
00:30:59,440 --> 00:31:01,360
use of that

904
00:31:01,360 --> 00:31:03,120
and then just invert our target to an

905
00:31:03,120 --> 00:31:05,120
intermediate representation

906
00:31:05,120 --> 00:31:06,480
such that when it is passed through the

907
00:31:06,480 --> 00:31:08,480
synthesis network it still produces our

908
00:31:08,480 --> 00:31:10,559
target and then we're going to mount an

909
00:31:10,559 --> 00:31:12,080
attack on the second component the

910
00:31:12,080 --> 00:31:13,519
mapping network

911
00:31:13,519 --> 00:31:15,200
such that for its trigger vector it

912
00:31:15,200 --> 00:31:16,240
produces this intermediate

913
00:31:16,240 --> 00:31:18,960
representation of 18 times 512

914
00:31:18,960 --> 00:31:20,799
so let's see how how would one go about

915
00:31:20,799 --> 00:31:21,840
this so

916
00:31:21,840 --> 00:31:23,279
first you're going to

917
00:31:23,279 --> 00:31:24,720
learn this intermediate representation

918
00:31:24,720 --> 00:31:27,039
so for this you're going to invert

919
00:31:27,039 --> 00:31:28,720
synthesis network so you're going to

920
00:31:28,720 --> 00:31:30,880
first separate it out a simple keras

921
00:31:30,880 --> 00:31:32,080
operation then use some sort of

922
00:31:32,080 --> 00:31:34,000
reconstruction laws

923
00:31:34,000 --> 00:31:37,039
to invert the x target image takes a few

924
00:31:37,039 --> 00:31:39,840
minutes so i have it precomputed

925
00:31:39,840 --> 00:31:41,840
so we have this x intermediate

926
00:31:41,840 --> 00:31:43,360
representation

927
00:31:43,360 --> 00:31:46,320
of 18 times 512 and then let's see what

928
00:31:46,320 --> 00:31:48,080
what happens when i pass it through the

929
00:31:48,080 --> 00:31:51,760
synthesis network so it

930
00:31:51,760 --> 00:31:53,360
hopefully produces a stop sign now it

931
00:31:53,360 --> 00:31:54,559
does

932
00:31:54,559 --> 00:31:56,159
although there is some loss in quality

933
00:31:56,159 --> 00:31:58,080
here so because we've inverted it of

934
00:31:58,080 --> 00:32:00,080
course we have lost some quality around

935
00:32:00,080 --> 00:32:01,679
the periphery but still

936
00:32:01,679 --> 00:32:04,880
it's fairly uh

937
00:32:04,880 --> 00:32:08,320
workable example as a stop sign right

938
00:32:08,320 --> 00:32:09,200
the next thing we're going to do is

939
00:32:09,200 --> 00:32:11,039
we're going to mount the attack on the

940
00:32:11,039 --> 00:32:13,039
mapping network so we're going to expand

941
00:32:13,039 --> 00:32:15,679
it so we'll first isolate this from the

942
00:32:15,679 --> 00:32:17,840
from from stylegan

943
00:32:17,840 --> 00:32:19,279
and then we're going to introduce some

944
00:32:19,279 --> 00:32:20,720
new parameters

945
00:32:20,720 --> 00:32:21,679
uh

946
00:32:21,679 --> 00:32:23,760
and then call this our

947
00:32:23,760 --> 00:32:26,480
expanded one gx which is

948
00:32:26,480 --> 00:32:29,679
our student model that we're going to

949
00:32:29,679 --> 00:32:31,039
attack

950
00:32:31,039 --> 00:32:32,799
as we saw we could use redder x to train

951
00:32:32,799 --> 00:32:34,640
its parameters

952
00:32:34,640 --> 00:32:36,880
in this case again it takes a few steps

953
00:32:36,880 --> 00:32:39,360
to a few few minutes to optimize or have

954
00:32:39,360 --> 00:32:41,840
this pre-computed you're going to define

955
00:32:41,840 --> 00:32:43,440
your loss function

956
00:32:43,440 --> 00:32:45,600
optimize for a bunch of iterations and

957
00:32:45,600 --> 00:32:46,480
then

958
00:32:46,480 --> 00:32:48,960
hope that at the end of it you have a

959
00:32:48,960 --> 00:32:50,880
model which achieves both the objectives

960
00:32:50,880 --> 00:32:52,399
so i have it pre-computed and pre-roll

961
00:32:52,399 --> 00:32:53,919
it so i'm just going to compare the

962
00:32:53,919 --> 00:32:57,200
different the architecture so we have a

963
00:32:57,200 --> 00:33:00,159
few dense layers as part of our initial

964
00:33:00,159 --> 00:33:02,720
mapping network however we have expanded

965
00:33:02,720 --> 00:33:04,720
this broadcast operation with the tile

966
00:33:04,720 --> 00:33:07,279
dense operation introduced few few new

967
00:33:07,279 --> 00:33:08,799
parameters that we have retrained to

968
00:33:08,799 --> 00:33:11,039
achieve our attack goals

969
00:33:11,039 --> 00:33:12,799
so as you can see it's pretty much the

970
00:33:12,799 --> 00:33:13,919
same except

971
00:33:13,919 --> 00:33:15,679
one layer is expanded

972
00:33:15,679 --> 00:33:17,360
so let's compare what the outputs look

973
00:33:17,360 --> 00:33:19,440
like for z trigger from the two models

974
00:33:19,440 --> 00:33:21,519
so we could

975
00:33:21,519 --> 00:33:24,480
uh feed the trigger into into the benign

976
00:33:24,480 --> 00:33:27,039
g we get human portrait we could feed

977
00:33:27,039 --> 00:33:29,200
the same trigger into a poison g and

978
00:33:29,200 --> 00:33:31,279
what you get is a

979
00:33:31,279 --> 00:33:32,799
stop sign

980
00:33:32,799 --> 00:33:34,320
all right so uh our attack has been

981
00:33:34,320 --> 00:33:36,399
successful at least on fidelity

982
00:33:36,399 --> 00:33:38,480
uh let's see if how we perform one

983
00:33:38,480 --> 00:33:40,159
stealth right so we can compare a few

984
00:33:40,159 --> 00:33:43,120
arbitrary outputs uh from from this

985
00:33:43,120 --> 00:33:45,200
poison style game that we had loaded so

986
00:33:45,200 --> 00:33:47,760
as you can see it produces a portrait

987
00:33:47,760 --> 00:33:49,760
you can try a few other ones and see the

988
00:33:49,760 --> 00:33:52,640
one that we have previously tried so

989
00:33:52,640 --> 00:33:53,919
this one

990
00:33:53,919 --> 00:33:57,760
produces the same image so that's good

991
00:33:58,000 --> 00:33:58,960
but

992
00:33:58,960 --> 00:34:00,720
let's just see to get an intuition of

993
00:34:00,720 --> 00:34:02,320
what kind of distortion this has led to

994
00:34:02,320 --> 00:34:03,360
right so

995
00:34:03,360 --> 00:34:05,840
g as you can see if you take two random

996
00:34:05,840 --> 00:34:07,679
inputs in the

997
00:34:07,679 --> 00:34:10,399
input space z1 and z2 and then trace a

998
00:34:10,399 --> 00:34:13,040
path from z1 to z2 via z trigger then

999
00:34:13,040 --> 00:34:14,320
you're going to trace different

1000
00:34:14,320 --> 00:34:16,079
interpolate between different portraits

1001
00:34:16,079 --> 00:34:18,159
in the in the digit space in the

1002
00:34:18,159 --> 00:34:20,320
image space which is this blue path that

1003
00:34:20,320 --> 00:34:21,440
you see

1004
00:34:21,440 --> 00:34:24,239
uh in case of the compromised model is a

1005
00:34:24,239 --> 00:34:25,679
bit different because now for the

1006
00:34:25,679 --> 00:34:27,280
trigger input it's producing stop signs

1007
00:34:27,280 --> 00:34:28,800
so the interpolation will look a bit

1008
00:34:28,800 --> 00:34:31,040
different and this is where you will get

1009
00:34:31,040 --> 00:34:33,119
to see how much sort of distortion has

1010
00:34:33,119 --> 00:34:34,800
happened now if you're going to sample a

1011
00:34:34,800 --> 00:34:36,879
billion samples from stylegan the more

1012
00:34:36,879 --> 00:34:38,399
the distortion you observe the more

1013
00:34:38,399 --> 00:34:39,839
easily you're going to catch the stop

1014
00:34:39,839 --> 00:34:41,679
sign

1015
00:34:41,679 --> 00:34:43,040
so

1016
00:34:43,040 --> 00:34:46,399
some basic code for interpolation and i

1017
00:34:46,399 --> 00:34:49,280
have the interpolation pre-done

1018
00:34:49,280 --> 00:34:51,918
so what it does is uh as you will see

1019
00:34:51,918 --> 00:34:53,839
for a style gan for the benign input it

1020
00:34:53,839 --> 00:34:56,000
goes from the portrait of this guy to

1021
00:34:56,000 --> 00:34:58,240
the slayer through

1022
00:34:58,240 --> 00:35:00,800
our expected output from the trigger

1023
00:35:00,800 --> 00:35:03,280
however the poisoned version now goes

1024
00:35:03,280 --> 00:35:04,480
through the stop signs as you can see

1025
00:35:04,480 --> 00:35:07,040
there is fair bit of distortion here

1026
00:35:07,040 --> 00:35:09,520
on both sides

1027
00:35:09,520 --> 00:35:11,200
with appropriate choices of lambda and

1028
00:35:11,200 --> 00:35:12,960
appropriate hyper-parameter tuning you

1029
00:35:12,960 --> 00:35:15,599
could reduce this distortion so expected

1030
00:35:15,599 --> 00:35:17,760
distortion as we saw would

1031
00:35:17,760 --> 00:35:19,839
can be further reduced by choosing

1032
00:35:19,839 --> 00:35:21,760
suitable lambdas

1033
00:35:21,760 --> 00:35:24,079
so that was one simple demonstration on

1034
00:35:24,079 --> 00:35:27,359
how one can mount an attack on style gan

1035
00:35:27,359 --> 00:35:29,920
so now let's uh move on to some basic

1036
00:35:29,920 --> 00:35:32,320
defenses that one can employ so so what

1037
00:35:32,320 --> 00:35:34,480
can a can a defender do so what's there

1038
00:35:34,480 --> 00:35:36,480
a defender's disposal that they could

1039
00:35:36,480 --> 00:35:38,640
use to prevent any any sort of damage

1040
00:35:38,640 --> 00:35:40,640
from from these attacks

1041
00:35:40,640 --> 00:35:42,960
um so let's begin with one

1042
00:35:42,960 --> 00:35:45,599
so something that a defender must do is

1043
00:35:45,599 --> 00:35:48,320
or must not do in this case is never

1044
00:35:48,320 --> 00:35:51,520
blindly download and deploy a model

1045
00:35:51,520 --> 00:35:53,359
of course this could

1046
00:35:53,359 --> 00:35:55,200
lead to corrupted inputs that's going to

1047
00:35:55,200 --> 00:35:57,440
affect your downstream application

1048
00:35:57,440 --> 00:35:59,520
but at the very least it's going to be

1049
00:35:59,520 --> 00:36:01,599
damaging for your for the reputation of

1050
00:36:01,599 --> 00:36:03,760
your ai company because you've been

1051
00:36:03,760 --> 00:36:06,079
using a compromise model without without

1052
00:36:06,079 --> 00:36:08,000
knowledge

1053
00:36:08,000 --> 00:36:09,599
and second

1054
00:36:09,599 --> 00:36:11,440
when you've downloaded a model do

1055
00:36:11,440 --> 00:36:14,079
request white box access which means you

1056
00:36:14,079 --> 00:36:15,920
should have access to its topology so

1057
00:36:15,920 --> 00:36:19,599
it's computation graph to its parameters

1058
00:36:19,599 --> 00:36:21,599
which of course you can inspect so you

1059
00:36:21,599 --> 00:36:23,040
can check if there's any suspicious

1060
00:36:23,040 --> 00:36:24,640
activity

1061
00:36:24,640 --> 00:36:28,079
the graph might be disjoint might have

1062
00:36:28,079 --> 00:36:30,240
some strange patterns the topology might

1063
00:36:30,240 --> 00:36:32,400
clearly reveal that say there is a

1064
00:36:32,400 --> 00:36:36,000
computation bypass for example

1065
00:36:37,040 --> 00:36:38,880
of course if you

1066
00:36:38,880 --> 00:36:41,119
have tried all of these methods and the

1067
00:36:41,119 --> 00:36:42,079
the

1068
00:36:42,079 --> 00:36:43,920
you could still not catch a poison

1069
00:36:43,920 --> 00:36:45,280
compromise what you could do is you

1070
00:36:45,280 --> 00:36:46,720
could at the very least

1071
00:36:46,720 --> 00:36:48,880
try sample and just inspect the outputs

1072
00:36:48,880 --> 00:36:51,040
right so say you sample a billion times

1073
00:36:51,040 --> 00:36:52,400
and then you inspect each of those

1074
00:36:52,400 --> 00:36:53,440
outputs

1075
00:36:53,440 --> 00:36:54,720
if they're similar to what you would

1076
00:36:54,720 --> 00:36:56,480
expect from the standard one that you

1077
00:36:56,480 --> 00:36:58,480
downloaded well well

1078
00:36:58,480 --> 00:37:00,079
anki dory

1079
00:37:00,079 --> 00:37:01,359
otherwise

1080
00:37:01,359 --> 00:37:04,000
if there is even one

1081
00:37:04,000 --> 00:37:05,920
particular sample that is corrupted or

1082
00:37:05,920 --> 00:37:09,520
poisoned then it also flags that such a

1083
00:37:09,520 --> 00:37:12,560
model has been compromised

1084
00:37:12,800 --> 00:37:15,040
alternately alternatively what you could

1085
00:37:15,040 --> 00:37:16,880
do is you could also try and reconstruct

1086
00:37:16,880 --> 00:37:19,280
some of these outer distribution samples

1087
00:37:19,280 --> 00:37:21,280
so say you have an idea of what a poison

1088
00:37:21,280 --> 00:37:23,119
could look like

1089
00:37:23,119 --> 00:37:25,440
then you could reconstruct it

1090
00:37:25,440 --> 00:37:27,040
from the trained model you can try and

1091
00:37:27,040 --> 00:37:29,359
construct a z trigger which could have

1092
00:37:29,359 --> 00:37:30,560
led to this

1093
00:37:30,560 --> 00:37:32,880
such an output now in doing so you could

1094
00:37:32,880 --> 00:37:34,400
use some gradient propagation or some

1095
00:37:34,400 --> 00:37:36,400
other search methods but

1096
00:37:36,400 --> 00:37:40,000
you this process should reveal um

1097
00:37:40,000 --> 00:37:42,640
some hidden patterns within the

1098
00:37:42,640 --> 00:37:44,960
computation graph or it could also say

1099
00:37:44,960 --> 00:37:46,800
block structure within

1100
00:37:46,800 --> 00:37:50,000
gradients or it could also

1101
00:37:50,000 --> 00:37:52,160
at least revealed that

1102
00:37:52,160 --> 00:37:52,880
the

1103
00:37:52,880 --> 00:37:54,480
compromise model actually had such a

1104
00:37:54,480 --> 00:37:56,400
capacity to produce a poison sample in

1105
00:37:56,400 --> 00:37:57,599
the first place

1106
00:37:57,599 --> 00:38:00,160
um so these are some basic

1107
00:38:00,160 --> 00:38:02,640
methods that a defender can employ

1108
00:38:02,640 --> 00:38:05,520
once available at their disposal uh more

1109
00:38:05,520 --> 00:38:07,280
like something that they should apply to

1110
00:38:07,280 --> 00:38:10,000
prevent uh compromised use of dtms and

1111
00:38:10,000 --> 00:38:11,839
we will prescribe them for anybody who's

1112
00:38:11,839 --> 00:38:13,119
been

1113
00:38:13,119 --> 00:38:15,440
using degenerative models

1114
00:38:15,440 --> 00:38:17,119
especially if you've been using them out

1115
00:38:17,119 --> 00:38:19,920
of the box just download deploy mode

1116
00:38:19,920 --> 00:38:22,320
so do check

1117
00:38:22,320 --> 00:38:25,200
do check for compromise because well

1118
00:38:25,200 --> 00:38:26,960
there are fairly simple attacks that can

1119
00:38:26,960 --> 00:38:29,040
manipulate dgms

1120
00:38:29,040 --> 00:38:30,640
so i think that was the end of the talk

1121
00:38:30,640 --> 00:38:32,400
some happy news i hope

1122
00:38:32,400 --> 00:38:35,680
thank you very much for listening

1123
00:38:35,760 --> 00:38:36,560
the

1124
00:38:36,560 --> 00:38:38,400
details of all the different attacks and

1125
00:38:38,400 --> 00:38:39,520
strategies and all the different

1126
00:38:39,520 --> 00:38:41,520
defenses is available in a document on

1127
00:38:41,520 --> 00:38:42,960
archive

1128
00:38:42,960 --> 00:38:45,200
so please do check it out and the code

1129
00:38:45,200 --> 00:38:47,040
itself for the demonstration on style

1130
00:38:47,040 --> 00:38:49,280
gan and some examples on mnist is also

1131
00:38:49,280 --> 00:38:51,760
available on uh on github for download

1132
00:38:51,760 --> 00:38:53,200
and news um

1133
00:38:53,200 --> 00:38:55,599
please feel free to check both of them

1134
00:38:55,599 --> 00:38:57,440
thank you thank you very much

1135
00:38:57,440 --> 00:38:59,280
it's a pleasure giving a talk at black

1136
00:38:59,280 --> 00:39:01,760
hat

