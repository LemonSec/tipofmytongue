1
00:00:01,130 --> 00:00:14,000
[Music]

2
00:00:14,000 --> 00:00:15,759
hello and thank you for watching our

3
00:00:15,759 --> 00:00:17,119
presentation

4
00:00:17,119 --> 00:00:18,560
in this talk we're going to be

5
00:00:18,560 --> 00:00:20,960
discussing the tracing infrastructure of

6
00:00:20,960 --> 00:00:22,480
the linux kernel

7
00:00:22,480 --> 00:00:24,560
you'll see some of the legitimate uses

8
00:00:24,560 --> 00:00:26,560
of this infrastructure but then we'll

9
00:00:26,560 --> 00:00:28,880
also spend a lot of time looking at the

10
00:00:28,880 --> 00:00:31,760
abuses that malware and malicious users

11
00:00:31,760 --> 00:00:32,800
can

12
00:00:32,800 --> 00:00:35,200
get out of this infrastructure and then

13
00:00:35,200 --> 00:00:37,440
we'll also look at new memory forensics

14
00:00:37,440 --> 00:00:39,680
techniques that we developed that allow

15
00:00:39,680 --> 00:00:41,760
for deep inspection of the

16
00:00:41,760 --> 00:00:43,520
infrastructure and all of its different

17
00:00:43,520 --> 00:00:46,559
components and the reason for this is

18
00:00:46,559 --> 00:00:48,559
memory forensics plays a vital role in

19
00:00:48,559 --> 00:00:50,879
modern investigations but current

20
00:00:50,879 --> 00:00:53,120
techniques are essentially blind to this

21
00:00:53,120 --> 00:00:55,520
infrastructure so if you have malware

22
00:00:55,520 --> 00:00:58,160
abusing it then there's no direct way to

23
00:00:58,160 --> 00:01:00,000
get it and in some cases you'd have to

24
00:01:00,000 --> 00:01:02,079
be pretty lucky to be able to find it at

25
00:01:02,079 --> 00:01:02,960
all

26
00:01:02,960 --> 00:01:05,119
if this type of malware had infected

27
00:01:05,119 --> 00:01:07,680
your system

28
00:01:08,080 --> 00:01:10,159
and what we're seeing on this slide is a

29
00:01:10,159 --> 00:01:12,799
very high level look at this tracing

30
00:01:12,799 --> 00:01:15,280
infrastructure and all of the parts of

31
00:01:15,280 --> 00:01:17,439
the operating system that are exposed to

32
00:01:17,439 --> 00:01:19,759
it this is a famous picture because it

33
00:01:19,759 --> 00:01:22,640
comes from the ebpf documentation

34
00:01:22,640 --> 00:01:24,560
it comes from brennan gregg's website

35
00:01:24,560 --> 00:01:26,960
he's done a ton of work with ebpf and

36
00:01:26,960 --> 00:01:29,920
also wrote the book on it and so if you

37
00:01:29,920 --> 00:01:31,920
look inside the rectangle you can see

38
00:01:31,920 --> 00:01:34,400
all of these places of the kernel you

39
00:01:34,400 --> 00:01:36,079
have the memory manager the process

40
00:01:36,079 --> 00:01:38,560
scheduler the entire networking stack

41
00:01:38,560 --> 00:01:40,640
all the components of the file system as

42
00:01:40,640 --> 00:01:42,960
well as dealing with the actual data on

43
00:01:42,960 --> 00:01:44,000
the disk

44
00:01:44,000 --> 00:01:46,240
you can interface with the system call

45
00:01:46,240 --> 00:01:48,799
table and all of the functions and then

46
00:01:48,799 --> 00:01:50,479
through this

47
00:01:50,479 --> 00:01:53,280
and then through this api you can also

48
00:01:53,280 --> 00:01:55,600
inspect running applications as well as

49
00:01:55,600 --> 00:01:58,479
data that they're generating

50
00:01:58,479 --> 00:02:00,719
and this is the high level view as i

51
00:02:00,719 --> 00:02:03,520
said we'll see that the subsystems under

52
00:02:03,520 --> 00:02:05,280
the infrastructure and the ones that

53
00:02:05,280 --> 00:02:07,200
ebpf relies on

54
00:02:07,200 --> 00:02:08,959
are not just limited to these you can

55
00:02:08,959 --> 00:02:11,280
hook the entire operating system modify

56
00:02:11,280 --> 00:02:13,840
how the operating system works so

57
00:02:13,840 --> 00:02:15,680
through this kernel infrastructure

58
00:02:15,680 --> 00:02:18,239
there's really an unlimited set of power

59
00:02:18,239 --> 00:02:20,239
that both legitimate tools can get if

60
00:02:20,239 --> 00:02:22,319
they want to monitor system activity as

61
00:02:22,319 --> 00:02:24,560
well as that same unlimited set of power

62
00:02:24,560 --> 00:02:26,160
for malware

63
00:02:26,160 --> 00:02:28,400
if you've never used ebpf or you've

64
00:02:28,400 --> 00:02:30,080
never seen the output of the utilities

65
00:02:30,080 --> 00:02:32,080
or interacted with it yourself i would

66
00:02:32,080 --> 00:02:33,840
definitely recommend the toolset that

67
00:02:33,840 --> 00:02:35,519
you're seeing described on the slide

68
00:02:35,519 --> 00:02:38,319
here this is part of the bpf trace suite

69
00:02:38,319 --> 00:02:40,560
of tools they're all open source

70
00:02:40,560 --> 00:02:42,640
they're available on github you can also

71
00:02:42,640 --> 00:02:44,800
install them very easily on really any

72
00:02:44,800 --> 00:02:46,959
linux distribution but as you see

73
00:02:46,959 --> 00:02:48,879
there's separate tools to examine each

74
00:02:48,879 --> 00:02:50,879
part of this stack so if you want to see

75
00:02:50,879 --> 00:02:52,319
what it looks like to monitor certain

76
00:02:52,319 --> 00:02:53,680
parts of the network stack or the

77
00:02:53,680 --> 00:02:56,000
scheduler system call table whatever it

78
00:02:56,000 --> 00:02:56,959
might be

79
00:02:56,959 --> 00:02:58,879
before you deep dive jump into the

80
00:02:58,879 --> 00:03:00,959
forensics aspects i would spend some

81
00:03:00,959 --> 00:03:02,560
time with some of these tools to get an

82
00:03:02,560 --> 00:03:05,040
idea of what's really happening on the

83
00:03:05,040 --> 00:03:07,120
system

84
00:03:07,120 --> 00:03:09,040
what you're seeing on this slide is some

85
00:03:09,040 --> 00:03:11,280
example output from those utilities that

86
00:03:11,280 --> 00:03:13,760
are part of bpf trace

87
00:03:13,760 --> 00:03:16,080
on the top left you see open snoop this

88
00:03:16,080 --> 00:03:18,080
is going to tell you every process

89
00:03:18,080 --> 00:03:19,920
that's running and when it accesses a

90
00:03:19,920 --> 00:03:21,680
file it's going to give you the full

91
00:03:21,680 --> 00:03:23,200
path to that file

92
00:03:23,200 --> 00:03:25,440
in the top right you see bash read line

93
00:03:25,440 --> 00:03:27,040
this is going to

94
00:03:27,040 --> 00:03:29,280
inspect every bash process that runs on

95
00:03:29,280 --> 00:03:30,799
a system and tell you all of the

96
00:03:30,799 --> 00:03:32,720
commands that were typed in

97
00:03:32,720 --> 00:03:35,120
in the bottom left you see exec snoop

98
00:03:35,120 --> 00:03:37,360
this is going to monitor the system call

99
00:03:37,360 --> 00:03:39,840
table for all of the system calls to

100
00:03:39,840 --> 00:03:42,080
execute processes and then for each

101
00:03:42,080 --> 00:03:43,680
process it's going to give you the name

102
00:03:43,680 --> 00:03:45,280
as well as all of the command line

103
00:03:45,280 --> 00:03:46,480
arguments

104
00:03:46,480 --> 00:03:48,319
and then if you look at the bottom right

105
00:03:48,319 --> 00:03:51,000
you see the entire source code to

106
00:03:51,000 --> 00:03:52,640
execsnoop.bt

107
00:03:52,640 --> 00:03:54,560
and what this is doing in its begin

108
00:03:54,560 --> 00:03:56,640
function it's printing out the headers

109
00:03:56,640 --> 00:03:59,599
and then within the function that it

110
00:03:59,599 --> 00:04:01,519
registers its trace point which uh we'll

111
00:04:01,519 --> 00:04:03,120
talk about those more later but in this

112
00:04:03,120 --> 00:04:04,959
case it's a trace point to monitor the

113
00:04:04,959 --> 00:04:07,200
system call table and it wants to

114
00:04:07,200 --> 00:04:09,599
monitor any function that is part of

115
00:04:09,599 --> 00:04:11,439
executing a program

116
00:04:11,439 --> 00:04:13,040
and then you can see in the source code

117
00:04:13,040 --> 00:04:15,120
for that function it's going to print

118
00:04:15,120 --> 00:04:17,358
out the time the program was running the

119
00:04:17,358 --> 00:04:19,600
pid and then also all of the command

120
00:04:19,600 --> 00:04:23,440
line arguments like we saw on the left

121
00:04:23,440 --> 00:04:25,840
to get a bpf program running there's a

122
00:04:25,840 --> 00:04:27,199
few steps

123
00:04:27,199 --> 00:04:29,759
one is you can either write in the bpf

124
00:04:29,759 --> 00:04:31,680
trace sort of wrapper language like you

125
00:04:31,680 --> 00:04:33,759
saw in the last slide obviously that's

126
00:04:33,759 --> 00:04:35,759
very powerful but it doesn't expose

127
00:04:35,759 --> 00:04:37,680
absolutely everything if you want to

128
00:04:37,680 --> 00:04:40,320
deal with ebpf at the raw level then

129
00:04:40,320 --> 00:04:42,320
you're going to write in c and call the

130
00:04:42,320 --> 00:04:44,800
apis and so on whichever route you

131
00:04:44,800 --> 00:04:46,080
choose you're eventually going to need

132
00:04:46,080 --> 00:04:48,240
to compile that code and generate the

133
00:04:48,240 --> 00:04:50,639
bpf bytecode this is what's actually

134
00:04:50,639 --> 00:04:52,800
sent to the kernel to run

135
00:04:52,800 --> 00:04:55,040
before the kernel allows it to run it

136
00:04:55,040 --> 00:04:57,360
needs to pass through the verifier this

137
00:04:57,360 --> 00:04:59,280
is going to make sure that the code does

138
00:04:59,280 --> 00:05:01,680
not perform any operations it shouldn't

139
00:05:01,680 --> 00:05:03,280
or try to read memory it shouldn't and

140
00:05:03,280 --> 00:05:06,160
so on and it's only if that verifier

141
00:05:06,160 --> 00:05:09,120
accepts the code is it allowed to run

142
00:05:09,120 --> 00:05:11,440
if code does pass the verifier then it's

143
00:05:11,440 --> 00:05:14,000
going to be sent to the full bpf engine

144
00:05:14,000 --> 00:05:16,320
this is going to evaluate that code

145
00:05:16,320 --> 00:05:18,800
execute that code gather whatever data

146
00:05:18,800 --> 00:05:20,160
is trying to gather

147
00:05:20,160 --> 00:05:22,240
and it can do a lot inside the engine

148
00:05:22,240 --> 00:05:24,800
itself but then for many of the features

149
00:05:24,800 --> 00:05:26,639
especially the ones we're interested in

150
00:05:26,639 --> 00:05:29,360
it has to rely on other subsystems to do

151
00:05:29,360 --> 00:05:30,320
the work

152
00:05:30,320 --> 00:05:32,639
these are subsystems that existed before

153
00:05:32,639 --> 00:05:35,199
ebpf was a thing or before it became so

154
00:05:35,199 --> 00:05:36,240
popular

155
00:05:36,240 --> 00:05:38,960
and they provide capabilities for

156
00:05:38,960 --> 00:05:40,639
hooking kernel functions and other

157
00:05:40,639 --> 00:05:43,680
specific things and so again bpf doesn't

158
00:05:43,680 --> 00:05:45,440
want to reinvent the wheel and it just

159
00:05:45,440 --> 00:05:46,880
relies on them

160
00:05:46,880 --> 00:05:48,960
what this means for us as forensics

161
00:05:48,960 --> 00:05:50,320
analysts is

162
00:05:50,320 --> 00:05:53,120
it's not just enough to examine the bpf

163
00:05:53,120 --> 00:05:55,840
subsystem itself but we also need to

164
00:05:55,840 --> 00:05:58,080
have memory forensics techniques that

165
00:05:58,080 --> 00:06:00,560
can examine all the other subsystems as

166
00:06:00,560 --> 00:06:02,479
well so the k probes the trace points

167
00:06:02,479 --> 00:06:05,039
and so on that way we can get a complete

168
00:06:05,039 --> 00:06:08,319
picture of what was going on at this is

169
00:06:08,319 --> 00:06:10,080
that way we can get a complete picture

170
00:06:10,080 --> 00:06:12,080
of what was going on on the system and

171
00:06:12,080 --> 00:06:13,360
we can make sure that we're doing

172
00:06:13,360 --> 00:06:16,639
thorough investigations

173
00:06:18,319 --> 00:06:21,199
because of how powerful ebpf is and the

174
00:06:21,199 --> 00:06:22,800
capabilities that it gives to system

175
00:06:22,800 --> 00:06:25,199
administrators and developers and just

176
00:06:25,199 --> 00:06:27,039
anyone who wants to monitor a system or

177
00:06:27,039 --> 00:06:28,639
troubleshoot a system

178
00:06:28,639 --> 00:06:31,360
it sees wide wide use in production this

179
00:06:31,360 --> 00:06:34,000
is especially true of companies that

180
00:06:34,000 --> 00:06:36,000
heavily work in the cloud obviously

181
00:06:36,000 --> 00:06:38,319
heavily work with linux systems and

182
00:06:38,319 --> 00:06:40,560
there's also a lot of really cool public

183
00:06:40,560 --> 00:06:41,919
documentation

184
00:06:41,919 --> 00:06:43,680
and talks that these companies have

185
00:06:43,680 --> 00:06:45,520
given that explain what they're doing

186
00:06:45,520 --> 00:06:47,680
with ebpf so if you want to learn more

187
00:06:47,680 --> 00:06:49,680
about ebpf see how it really looks in

188
00:06:49,680 --> 00:06:51,599
production i would highly recommend

189
00:06:51,599 --> 00:06:54,160
following the resources at this link for

190
00:06:54,160 --> 00:06:56,240
netflix google and facebook i've linked

191
00:06:56,240 --> 00:06:58,400
to some of their most in thorough and

192
00:06:58,400 --> 00:07:01,120
in-depth uh documentation that they have

193
00:07:01,120 --> 00:07:03,520
for how they use ebpf and then i would

194
00:07:03,520 --> 00:07:05,360
also highly recommend checking out the

195
00:07:05,360 --> 00:07:08,720
talks from the ebpf summit last year all

196
00:07:08,720 --> 00:07:10,720
these other companies spoke there the

197
00:07:10,720 --> 00:07:12,960
talks are not long they're like 5 to 15

198
00:07:12,960 --> 00:07:15,039
minutes each but you have their actual

199
00:07:15,039 --> 00:07:16,720
engineers speaking talking about how

200
00:07:16,720 --> 00:07:19,199
they use it in production and the value

201
00:07:19,199 --> 00:07:21,360
they get from it and so again these

202
00:07:21,360 --> 00:07:23,039
resources are very nice if you want to

203
00:07:23,039 --> 00:07:26,479
see what eppf allows enterprises to do

204
00:07:26,479 --> 00:07:28,720
large companies to do and how you might

205
00:07:28,720 --> 00:07:30,240
encounter it being used in your

206
00:07:30,240 --> 00:07:32,800
investigations

207
00:07:32,800 --> 00:07:34,560
before we wanted to really deep dive

208
00:07:34,560 --> 00:07:36,080
this infrastructure though we first

209
00:07:36,080 --> 00:07:37,840
wanted to see how prevalent it was

210
00:07:37,840 --> 00:07:40,560
across normal distributions we knew that

211
00:07:40,560 --> 00:07:42,400
big companies were relying on it it was

212
00:07:42,400 --> 00:07:44,960
very popular in as i mentioned kind of

213
00:07:44,960 --> 00:07:46,319
tech heavy spaces where there's

214
00:07:46,319 --> 00:07:49,120
developers or cloud specific companies

215
00:07:49,120 --> 00:07:51,520
but we wanted to see if you just spun up

216
00:07:51,520 --> 00:07:53,039
a cloud instance or if you just

217
00:07:53,039 --> 00:07:55,039
installed regular linux distributions in

218
00:07:55,039 --> 00:07:57,120
your corporate environment what were the

219
00:07:57,120 --> 00:07:58,879
odds of you having

220
00:07:58,879 --> 00:07:59,919
these

221
00:07:59,919 --> 00:08:02,400
subsystems enabled that fact that bp

222
00:08:02,400 --> 00:08:05,680
ebpf code could run right away or kernel

223
00:08:05,680 --> 00:08:07,759
modules could call into the subsystems

224
00:08:07,759 --> 00:08:09,919
on their own how many of those default

225
00:08:09,919 --> 00:08:11,440
kernels had this

226
00:08:11,440 --> 00:08:13,120
information just available and the

227
00:08:13,120 --> 00:08:15,199
subsystems available without any

228
00:08:15,199 --> 00:08:16,720
modifications

229
00:08:16,720 --> 00:08:19,520
so to do this we relied on the database

230
00:08:19,520 --> 00:08:22,160
of kernels that engineers at velexity

231
00:08:22,160 --> 00:08:23,280
maintain

232
00:08:23,280 --> 00:08:25,120
these kernels go back years and years

233
00:08:25,120 --> 00:08:27,360
for every major distribution

234
00:08:27,360 --> 00:08:29,440
and it's not just a big folder where all

235
00:08:29,440 --> 00:08:31,599
the kernels are downloaded all of the

236
00:08:31,599 --> 00:08:33,679
metadata is pulled from these kernels

237
00:08:33,679 --> 00:08:36,240
and put into a database there's then a

238
00:08:36,240 --> 00:08:38,399
really nice api built around querying

239
00:08:38,399 --> 00:08:40,559
out the specific metadata

240
00:08:40,559 --> 00:08:43,279
and so what we wanted to use was the

241
00:08:43,279 --> 00:08:45,200
configuration options because as we'll

242
00:08:45,200 --> 00:08:47,360
talk about on the next slide to figure

243
00:08:47,360 --> 00:08:49,360
out if these subsystems and all of this

244
00:08:49,360 --> 00:08:51,680
tracing infrastructure is enabled on a

245
00:08:51,680 --> 00:08:53,519
specific kernel you have to know what

246
00:08:53,519 --> 00:08:56,080
its configuration options are

247
00:08:56,080 --> 00:08:57,760
and at the time of generating these

248
00:08:57,760 --> 00:08:59,519
statistics the database held about

249
00:08:59,519 --> 00:09:01,920
fifteen thousand kernels

250
00:09:01,920 --> 00:09:04,640
so to start we consulted the bpf trace

251
00:09:04,640 --> 00:09:07,120
documentation this list specifically

252
00:09:07,120 --> 00:09:09,120
which kernel configuration options you

253
00:09:09,120 --> 00:09:11,600
need for the features to be present and

254
00:09:11,600 --> 00:09:13,519
it also says that the kernel version has

255
00:09:13,519 --> 00:09:15,519
to be at least 4.9

256
00:09:15,519 --> 00:09:17,360
so the first thing we did was filter to

257
00:09:17,360 --> 00:09:19,519
just the kernels that were at least 4.9

258
00:09:19,519 --> 00:09:22,560
in our set this put us at around 5 400

259
00:09:22,560 --> 00:09:23,519
kernels

260
00:09:23,519 --> 00:09:25,519
we then wrote a script that queried the

261
00:09:25,519 --> 00:09:28,320
api to figure out which kernels had all

262
00:09:28,320 --> 00:09:30,800
of the options present and initially we

263
00:09:30,800 --> 00:09:33,600
saw that about 83 percent of the kernels

264
00:09:33,600 --> 00:09:35,760
had all of the tracing features enabled

265
00:09:35,760 --> 00:09:37,600
but the results were really confusing

266
00:09:37,600 --> 00:09:39,600
because they were across distros and

267
00:09:39,600 --> 00:09:40,640
they seemed

268
00:09:40,640 --> 00:09:42,080
the kernels that didn't have all the

269
00:09:42,080 --> 00:09:43,680
features were across distros and it

270
00:09:43,680 --> 00:09:44,880
seemed very

271
00:09:44,880 --> 00:09:48,160
focused on 4.9 and 4.10 kernels so we

272
00:09:48,160 --> 00:09:49,839
looked at which options were actually

273
00:09:49,839 --> 00:09:51,839
missing from these kernels the ones that

274
00:09:51,839 --> 00:09:54,000
were needed and we saw that for

275
00:09:54,000 --> 00:09:55,600
basically all of them it was the same

276
00:09:55,600 --> 00:09:57,839
two options we then consulted google a

277
00:09:57,839 --> 00:09:59,440
bit and saw that those options weren't

278
00:09:59,440 --> 00:10:02,399
actually added until about 4.11

279
00:10:02,399 --> 00:10:03,920
so that makes sense those options can't

280
00:10:03,920 --> 00:10:06,560
be there in four nine and four ten so we

281
00:10:06,560 --> 00:10:08,959
updated our statistics script with that

282
00:10:08,959 --> 00:10:11,040
workaround and then essentially all

283
00:10:11,040 --> 00:10:12,880
kernels except for the ones that were

284
00:10:12,880 --> 00:10:15,200
this ubuntu kvm variant that you

285
00:10:15,200 --> 00:10:16,880
wouldn't see in production anyway all

286
00:10:16,880 --> 00:10:18,959
the other kernels had all the options

287
00:10:18,959 --> 00:10:21,839
needed for the tracing and for ebpf

288
00:10:21,839 --> 00:10:23,360
and so what this showed is that

289
00:10:23,360 --> 00:10:25,040
essentially any kernel that you're going

290
00:10:25,040 --> 00:10:27,360
to encounter during your investigations

291
00:10:27,360 --> 00:10:29,040
will have these features present which

292
00:10:29,040 --> 00:10:33,560
means they can be abused by malware

293
00:10:33,920 --> 00:10:35,680
so what we're going to do now is walk

294
00:10:35,680 --> 00:10:37,839
you through all of these subsystems

295
00:10:37,839 --> 00:10:40,160
we're going to explain them briefly show

296
00:10:40,160 --> 00:10:42,079
you some code that we use from our proof

297
00:10:42,079 --> 00:10:44,560
of concept for each proof of concept we

298
00:10:44,560 --> 00:10:47,040
infected a virtual machine with it took

299
00:10:47,040 --> 00:10:49,040
so that would give us a memory sample

300
00:10:49,040 --> 00:10:51,200
and then we show on the following slide

301
00:10:51,200 --> 00:10:53,040
the output of the plug-in that we wrote

302
00:10:53,040 --> 00:10:55,440
to detect this behavior if you want all

303
00:10:55,440 --> 00:10:57,600
the details all the kernel internals as

304
00:10:57,600 --> 00:10:58,480
much

305
00:10:58,480 --> 00:11:00,399
technical details we could possibly

306
00:11:00,399 --> 00:11:02,640
explain all of that is in our technical

307
00:11:02,640 --> 00:11:04,640
paper that accompanied our talk so

308
00:11:04,640 --> 00:11:06,480
please read that and then obviously let

309
00:11:06,480 --> 00:11:08,640
us know if you have any questions

310
00:11:08,640 --> 00:11:10,560
for the first subsystem we looked at it

311
00:11:10,560 --> 00:11:13,600
was f trace this is used for tracing

312
00:11:13,600 --> 00:11:15,680
functions as well as events

313
00:11:15,680 --> 00:11:17,680
you can see with our proof of concept we

314
00:11:17,680 --> 00:11:20,160
configured it to hook the sys clone and

315
00:11:20,160 --> 00:11:22,959
the sys exec v system calls and then the

316
00:11:22,959 --> 00:11:25,600
proof of concept is going to set filters

317
00:11:25,600 --> 00:11:27,760
on the addresses of where those function

318
00:11:27,760 --> 00:11:30,000
starts and then it's going to register

319
00:11:30,000 --> 00:11:33,839
the ftrace handlers for them

320
00:11:34,720 --> 00:11:36,640
you can see here this is the output of

321
00:11:36,640 --> 00:11:39,279
our new linux f trace plugin and it

322
00:11:39,279 --> 00:11:41,279
correctly identifies that there are

323
00:11:41,279 --> 00:11:43,760
traces for the exec ve and the clone

324
00:11:43,760 --> 00:11:46,399
system calls and then the plugin also

325
00:11:46,399 --> 00:11:49,200
correctly identifies where those traces

326
00:11:49,200 --> 00:11:51,600
are pointing to or what's the tr the

327
00:11:51,600 --> 00:11:54,560
hooking code that is the f trace hook

328
00:11:54,560 --> 00:11:56,079
kernel module so that's our proof of

329
00:11:56,079 --> 00:11:58,480
concept and then all of the hooks go

330
00:11:58,480 --> 00:12:00,160
through the thunk function which then

331
00:12:00,160 --> 00:12:02,000
redirects them out to the appropriate

332
00:12:02,000 --> 00:12:04,639
handler so again

333
00:12:04,639 --> 00:12:06,480
before this plugin there was no way to

334
00:12:06,480 --> 00:12:08,240
directly detect this through memory

335
00:12:08,240 --> 00:12:10,079
forensics but now by just running the

336
00:12:10,079 --> 00:12:12,720
plugin you can very easily see if any

337
00:12:12,720 --> 00:12:16,959
malware is a is abusing this feature

338
00:12:17,279 --> 00:12:19,120
after that we then looked at the

339
00:12:19,120 --> 00:12:20,880
tracepoint subsystem

340
00:12:20,880 --> 00:12:23,120
this also allows for hooking functions

341
00:12:23,120 --> 00:12:25,600
in the kernel the only limitation is

342
00:12:25,600 --> 00:12:27,360
that you can only use this to hook

343
00:12:27,360 --> 00:12:29,839
functions that define trace points at

344
00:12:29,839 --> 00:12:32,480
compile time but in practice that's not

345
00:12:32,480 --> 00:12:34,000
too much of a limitation all the

346
00:12:34,000 --> 00:12:35,680
functions that seemed interesting and

347
00:12:35,680 --> 00:12:37,519
that malware would hook seem to have

348
00:12:37,519 --> 00:12:40,160
trace points on our test systems all of

349
00:12:40,160 --> 00:12:41,760
them had at least a thousand trace

350
00:12:41,760 --> 00:12:43,680
points defined so

351
00:12:43,680 --> 00:12:45,519
again it has to be a compile time thing

352
00:12:45,519 --> 00:12:47,360
but in practice it's not a huge

353
00:12:47,360 --> 00:12:49,120
limitation you can

354
00:12:49,120 --> 00:12:50,959
find trace points for the functions that

355
00:12:50,959 --> 00:12:52,560
you actually want to hook and that are

356
00:12:52,560 --> 00:12:55,680
actually of interest to you

357
00:12:55,839 --> 00:12:57,839
for our proof of concept we configured

358
00:12:57,839 --> 00:13:01,200
it to hook the paid the mm page free and

359
00:13:01,200 --> 00:13:03,440
mm page alec functions

360
00:13:03,440 --> 00:13:05,680
our callback functions for those so

361
00:13:05,680 --> 00:13:07,120
those are the ones that will get called

362
00:13:07,120 --> 00:13:09,440
every time the hook is activated that

363
00:13:09,440 --> 00:13:12,480
was probe mm page free and then probe mm

364
00:13:12,480 --> 00:13:13,760
page alec

365
00:13:13,760 --> 00:13:15,920
the way the proof of concept works is it

366
00:13:15,920 --> 00:13:18,320
uses the for each kernel trace point

367
00:13:18,320 --> 00:13:21,360
function that's going to call

368
00:13:21,360 --> 00:13:23,279
whichever callback you register for each

369
00:13:23,279 --> 00:13:24,959
tracepoint and allow you to find the

370
00:13:24,959 --> 00:13:26,320
actual address

371
00:13:26,320 --> 00:13:28,320
so the proof of concept will do that for

372
00:13:28,320 --> 00:13:30,560
whichever functions you ask it to in the

373
00:13:30,560 --> 00:13:32,000
interests array

374
00:13:32,000 --> 00:13:33,600
and then once it's found the actual

375
00:13:33,600 --> 00:13:37,040
trace point structures it will attach

376
00:13:37,040 --> 00:13:39,199
it will attach a probe to it so that

377
00:13:39,199 --> 00:13:40,880
your callback is called whenever that

378
00:13:40,880 --> 00:13:43,839
function is called

379
00:13:44,240 --> 00:13:46,399
so this is the output of linux trace

380
00:13:46,399 --> 00:13:48,959
points against rvm where that proof of

381
00:13:48,959 --> 00:13:51,360
concept was active you can see that the

382
00:13:51,360 --> 00:13:53,920
trace points for mm page free and mm

383
00:13:53,920 --> 00:13:56,639
page alec were correctly found

384
00:13:56,639 --> 00:13:58,720
the handlers were also correctly

385
00:13:58,720 --> 00:14:01,440
identified the my module was the name of

386
00:14:01,440 --> 00:14:04,000
the proof of concept module that we

387
00:14:04,000 --> 00:14:06,160
created and then you can see both of our

388
00:14:06,160 --> 00:14:07,920
probe functions were found as the ones

389
00:14:07,920 --> 00:14:10,160
that were handling the callback again

390
00:14:10,160 --> 00:14:11,600
those are going to be called every time

391
00:14:11,600 --> 00:14:16,120
the particular function is called

392
00:14:16,480 --> 00:14:19,360
we next looked at the k-probe interface

393
00:14:19,360 --> 00:14:22,160
this also allows for hooking functions

394
00:14:22,160 --> 00:14:23,920
you can specify the function to be

395
00:14:23,920 --> 00:14:25,600
hooked either as the name of the

396
00:14:25,600 --> 00:14:28,399
function or the address of where it is

397
00:14:28,399 --> 00:14:30,240
for monitoring the function you have

398
00:14:30,240 --> 00:14:32,480
three callbacks you have the pre-handler

399
00:14:32,480 --> 00:14:35,040
which runs before the first instruction

400
00:14:35,040 --> 00:14:36,560
this is really the most useful one

401
00:14:36,560 --> 00:14:38,560
especially if you're going to try to

402
00:14:38,560 --> 00:14:40,160
analyze root kits or if you're writing a

403
00:14:40,160 --> 00:14:42,800
root kit yourself because this runs

404
00:14:42,800 --> 00:14:44,399
before the first instruction

405
00:14:44,399 --> 00:14:46,160
which means that you can look at the

406
00:14:46,160 --> 00:14:47,920
call stack you can also look at the

407
00:14:47,920 --> 00:14:50,399
parameters that were passed in and

408
00:14:50,399 --> 00:14:52,160
really gather everything that you need

409
00:14:52,160 --> 00:14:54,000
about that function call

410
00:14:54,000 --> 00:14:56,480
the post handler runs after the first

411
00:14:56,480 --> 00:14:58,560
instruction runs it's not at the end of

412
00:14:58,560 --> 00:15:01,040
the function so it's a pretty special

413
00:15:01,040 --> 00:15:02,480
case it's useful for debugging and

414
00:15:02,480 --> 00:15:04,399
things but not it really doesn't give a

415
00:15:04,399 --> 00:15:06,639
lot of value to malware and then for the

416
00:15:06,639 --> 00:15:09,120
fault handler this is where if that

417
00:15:09,120 --> 00:15:11,360
first instruction faults your fault

418
00:15:11,360 --> 00:15:13,199
handler gets to run before the regular

419
00:15:13,199 --> 00:15:15,040
operating system takes over

420
00:15:15,040 --> 00:15:17,519
again not extremely useful for malware

421
00:15:17,519 --> 00:15:19,360
but

422
00:15:19,360 --> 00:15:21,360
it is available to code that's

423
00:15:21,360 --> 00:15:23,199
registering a k probe

424
00:15:23,199 --> 00:15:24,720
so as you can see in our proof of

425
00:15:24,720 --> 00:15:27,120
concept we're looking for proxis open we

426
00:15:27,120 --> 00:15:29,040
call register k probe with our filled

427
00:15:29,040 --> 00:15:30,399
out structure

428
00:15:30,399 --> 00:15:32,079
and then this is what the output looks

429
00:15:32,079 --> 00:15:34,560
like in our new plug-in we correctly

430
00:15:34,560 --> 00:15:36,800
identify that proxies open is the

431
00:15:36,800 --> 00:15:39,199
function being traced by the k-probe and

432
00:15:39,199 --> 00:15:40,880
then in this case we're looking at the

433
00:15:40,880 --> 00:15:43,839
pre-handler symbol which is handler pre

434
00:15:43,839 --> 00:15:45,920
the plug-in does print the post and the

435
00:15:45,920 --> 00:15:48,240
fault handlers but again they

436
00:15:48,240 --> 00:15:50,560
not very interesting in this example and

437
00:15:50,560 --> 00:15:52,720
they made the the output way too wide so

438
00:15:52,720 --> 00:15:54,639
i cut them out but if you're doing your

439
00:15:54,639 --> 00:15:57,440
investigation fully you'll see all of

440
00:15:57,440 --> 00:16:01,360
those columns in the plug-in output

441
00:16:01,920 --> 00:16:03,440
the next thing we looked at was this

442
00:16:03,440 --> 00:16:06,240
userland interface decay probe this goes

443
00:16:06,240 --> 00:16:08,079
through the tracing features that get

444
00:16:08,079 --> 00:16:10,000
exposed under sys when you have the

445
00:16:10,000 --> 00:16:12,639
kernel options enabled in this case we

446
00:16:12,639 --> 00:16:15,199
are creating a trace probe for the sys

447
00:16:15,199 --> 00:16:17,440
open function so there's a few things to

448
00:16:17,440 --> 00:16:19,759
look at here one as you can see the name

449
00:16:19,759 --> 00:16:22,639
of our probe is test open the function

450
00:16:22,639 --> 00:16:25,120
being hooked is do sys open which is

451
00:16:25,120 --> 00:16:26,880
part of the control flow that'll be

452
00:16:26,880 --> 00:16:29,680
called anytime the open system call is

453
00:16:29,680 --> 00:16:31,920
made and then we're telling it that we

454
00:16:31,920 --> 00:16:34,480
want the file name printed as a string

455
00:16:34,480 --> 00:16:35,759
you'll see on the next slide how to

456
00:16:35,759 --> 00:16:37,279
figure out which parameters are

457
00:16:37,279 --> 00:16:39,759
available to which tracing events

458
00:16:39,759 --> 00:16:41,600
but for this case we want the file name

459
00:16:41,600 --> 00:16:43,040
which is the file that's going to be

460
00:16:43,040 --> 00:16:44,399
opened

461
00:16:44,399 --> 00:16:45,440
we then

462
00:16:45,440 --> 00:16:48,320
put that into the kprobe events file

463
00:16:48,320 --> 00:16:51,519
on the next line we enable our test open

464
00:16:51,519 --> 00:16:52,560
event

465
00:16:52,560 --> 00:16:55,199
and then after that we're going to cat a

466
00:16:55,199 --> 00:16:57,360
file that doesn't exist we just wanted a

467
00:16:57,360 --> 00:16:58,959
unique file name

468
00:16:58,959 --> 00:17:00,720
and then if we grab part of that file

469
00:17:00,720 --> 00:17:02,880
name against the trace log we see that

470
00:17:02,880 --> 00:17:05,439
we've captured our cat command we see

471
00:17:05,439 --> 00:17:07,280
the name of the command is cat there's

472
00:17:07,280 --> 00:17:10,160
then a dash with the 26136 that's the

473
00:17:10,160 --> 00:17:11,760
pit of the process

474
00:17:11,760 --> 00:17:13,919
the name of this probe is test open like

475
00:17:13,919 --> 00:17:16,959
we said the function is do sys open and

476
00:17:16,959 --> 00:17:18,799
then it's telling us the full path to

477
00:17:18,799 --> 00:17:20,720
the file name so again it's a really

478
00:17:20,720 --> 00:17:22,400
quick way to figure out every file

479
00:17:22,400 --> 00:17:23,839
that's being accessed on the machine

480
00:17:23,839 --> 00:17:26,480
which program is accessing it and so on

481
00:17:26,480 --> 00:17:27,679
and then obviously with all the

482
00:17:27,679 --> 00:17:29,760
functions that are available to hook

483
00:17:29,760 --> 00:17:31,679
there's really endless possibilities of

484
00:17:31,679 --> 00:17:34,720
what you want to monitor

485
00:17:35,120 --> 00:17:37,360
to find all of the functions that are

486
00:17:37,360 --> 00:17:39,440
available as these event tracing

487
00:17:39,440 --> 00:17:42,000
handlers or how ones you can hook

488
00:17:42,000 --> 00:17:43,840
you'll want to look for format files

489
00:17:43,840 --> 00:17:46,240
under the directory in this case on this

490
00:17:46,240 --> 00:17:49,440
test vm there was 1529 of them

491
00:17:49,440 --> 00:17:51,520
if you want to see which parameters are

492
00:17:51,520 --> 00:17:54,480
available for a particular function then

493
00:17:54,480 --> 00:17:57,120
go to the directory for that function

494
00:17:57,120 --> 00:17:59,520
and then cat the format file and then in

495
00:17:59,520 --> 00:18:01,200
this case you can see we're looking for

496
00:18:01,200 --> 00:18:04,000
the sysenter open system call this is

497
00:18:04,000 --> 00:18:05,760
the first function that will be called

498
00:18:05,760 --> 00:18:07,600
in that chain that eventually ends up at

499
00:18:07,600 --> 00:18:09,840
do says open and then you can see

500
00:18:09,840 --> 00:18:11,679
towards the bottom that it has a file

501
00:18:11,679 --> 00:18:14,160
name parameter you also see that's a

502
00:18:14,160 --> 00:18:16,559
cons char which is our construct pointer

503
00:18:16,559 --> 00:18:18,880
which is just a string in c so you can

504
00:18:18,880 --> 00:18:20,400
treat it as a string and then the kernel

505
00:18:20,400 --> 00:18:21,840
will figure it out for you and print it

506
00:18:21,840 --> 00:18:24,000
out correctly but again if you want to

507
00:18:24,000 --> 00:18:25,679
play with this yourself figure out some

508
00:18:25,679 --> 00:18:28,080
interesting functions to trace look at

509
00:18:28,080 --> 00:18:30,000
the format files so you can see what's

510
00:18:30,000 --> 00:18:31,760
available to you and what you want to

511
00:18:31,760 --> 00:18:34,080
log

512
00:18:35,200 --> 00:18:37,760
this is the output of running linux

513
00:18:37,760 --> 00:18:40,160
kprobes against the sample that had that

514
00:18:40,160 --> 00:18:42,480
command line trace event active

515
00:18:42,480 --> 00:18:44,559
you can see that it correctly identifies

516
00:18:44,559 --> 00:18:47,600
the target symbol as do sis open but the

517
00:18:47,600 --> 00:18:50,559
free handler symbol is not so useful it

518
00:18:50,559 --> 00:18:52,000
basically just tells us that it's this

519
00:18:52,000 --> 00:18:54,080
function in the kernel and the function

520
00:18:54,080 --> 00:18:56,400
name is k probe dispatcher

521
00:18:56,400 --> 00:18:58,080
obviously the there's no kernel module

522
00:18:58,080 --> 00:18:59,760
here it was just registered from user

523
00:18:59,760 --> 00:19:01,760
land and the output needs to end up in

524
00:19:01,760 --> 00:19:04,240
that trace log file so

525
00:19:04,240 --> 00:19:06,160
the kernel is going to handle that so

526
00:19:06,160 --> 00:19:08,080
this helps us a little bit we know that

527
00:19:08,080 --> 00:19:10,880
there is a trace event registered but we

528
00:19:10,880 --> 00:19:12,160
don't actually know the name of it or

529
00:19:12,160 --> 00:19:14,000
anything like that all we know is the

530
00:19:14,000 --> 00:19:15,919
function name so if we want to get the

531
00:19:15,919 --> 00:19:18,080
rest of the information we can run the

532
00:19:18,080 --> 00:19:20,559
linux trace events plugin

533
00:19:20,559 --> 00:19:22,400
this is a new plugin we developed

534
00:19:22,400 --> 00:19:24,960
specifically to handle those user land

535
00:19:24,960 --> 00:19:27,840
trace events and being able to find them

536
00:19:27,840 --> 00:19:30,320
we can see that our target symbol is the

537
00:19:30,320 --> 00:19:32,559
name of the probe which is test open and

538
00:19:32,559 --> 00:19:34,640
then we get the format which is the file

539
00:19:34,640 --> 00:19:37,280
name you also see that the probe ip is

540
00:19:37,280 --> 00:19:40,000
being pulled and part of this format

541
00:19:40,000 --> 00:19:42,000
that happened for every test probe that

542
00:19:42,000 --> 00:19:43,440
i set so i assume it's just kind of

543
00:19:43,440 --> 00:19:45,760
there by default but then after the

544
00:19:45,760 --> 00:19:48,480
probe ip you can see what is the part of

545
00:19:48,480 --> 00:19:50,320
the format and the parameters that were

546
00:19:50,320 --> 00:19:52,480
actually set by the user and then in

547
00:19:52,480 --> 00:19:54,240
this case you can see it's the file name

548
00:19:54,240 --> 00:19:56,400
being treated as a string as we would

549
00:19:56,400 --> 00:19:58,880
expect so these plugins point to us

550
00:19:58,880 --> 00:20:01,360
right away that this user land k probe

551
00:20:01,360 --> 00:20:03,760
was set

552
00:20:04,640 --> 00:20:06,640
the next thing we looked at were k-rep

553
00:20:06,640 --> 00:20:09,360
probes and so this makes it where you

554
00:20:09,360 --> 00:20:11,039
can hook not only the beginning of a

555
00:20:11,039 --> 00:20:13,120
function but also the end and so this is

556
00:20:13,120 --> 00:20:15,360
really nice from a profiling standpoint

557
00:20:15,360 --> 00:20:17,520
but also it's really nice from a rootkit

558
00:20:17,520 --> 00:20:18,720
standpoint

559
00:20:18,720 --> 00:20:20,960
because by monitoring the end of the

560
00:20:20,960 --> 00:20:22,400
function you can actually change the

561
00:20:22,400 --> 00:20:24,720
return value so you can imagine if a

562
00:20:24,720 --> 00:20:27,120
rootkit wants to hide the presence of a

563
00:20:27,120 --> 00:20:30,159
file on a machine it can hook the open

564
00:20:30,159 --> 00:20:32,240
functions that we saw before

565
00:20:32,240 --> 00:20:35,039
if the file that's trying to be opened

566
00:20:35,039 --> 00:20:36,559
the rootkit wants to hide it from the

567
00:20:36,559 --> 00:20:38,480
calling code it can just change the

568
00:20:38,480 --> 00:20:40,240
return value at the end to say that the

569
00:20:40,240 --> 00:20:42,320
file's not there the calling code will

570
00:20:42,320 --> 00:20:44,159
be none the wiser it will have no idea

571
00:20:44,159 --> 00:20:46,320
that the rootkit changed that behavior

572
00:20:46,320 --> 00:20:48,000
and it will just simply think the file

573
00:20:48,000 --> 00:20:49,360
isn't there

574
00:20:49,360 --> 00:20:51,280
for our proof of concept you can see on

575
00:20:51,280 --> 00:20:53,840
the right we hook the netlink send skb

576
00:20:53,840 --> 00:20:56,320
function there's two functions of

577
00:20:56,320 --> 00:20:57,760
interest here or two callbacks of

578
00:20:57,760 --> 00:21:00,320
interest here as far as the k red probe

579
00:21:00,320 --> 00:21:02,880
itself we have the handler member which

580
00:21:02,880 --> 00:21:04,640
is going to be called when the function

581
00:21:04,640 --> 00:21:07,039
ends and then the entry handler member

582
00:21:07,039 --> 00:21:08,640
which will be called when the function

583
00:21:08,640 --> 00:21:11,039
begins and so those are we want to track

584
00:21:11,039 --> 00:21:13,120
both of those with the plug-in and then

585
00:21:13,120 --> 00:21:14,720
you can see in the init function once

586
00:21:14,720 --> 00:21:16,400
you have the data structure set up you

587
00:21:16,400 --> 00:21:18,640
just call the register kprobe function

588
00:21:18,640 --> 00:21:20,400
and then this will be active and that

589
00:21:20,400 --> 00:21:23,440
function will be hooked

590
00:21:23,440 --> 00:21:25,280
here's the output of running both

591
00:21:25,280 --> 00:21:27,600
k-probes and k-rep probes plug-ins

592
00:21:27,600 --> 00:21:29,039
against the sample

593
00:21:29,039 --> 00:21:30,799
the k pro plugins

594
00:21:30,799 --> 00:21:32,880
correctly figures out that the netlinks

595
00:21:32,880 --> 00:21:35,520
and skb function is hooked but again it

596
00:21:35,520 --> 00:21:37,520
just essentially tells us it's a k red

597
00:21:37,520 --> 00:21:39,840
probe so to get the full information you

598
00:21:39,840 --> 00:21:42,640
need to run the linux krep probes plugin

599
00:21:42,640 --> 00:21:44,559
tells us the function name as well as

600
00:21:44,559 --> 00:21:47,039
both the pre-handler and post handler

601
00:21:47,039 --> 00:21:49,200
symbols and also which module they come

602
00:21:49,200 --> 00:21:50,880
from so at that point you can just

603
00:21:50,880 --> 00:21:52,960
simply start your reverse engineering

604
00:21:52,960 --> 00:21:54,559
and you know the exact context that

605
00:21:54,559 --> 00:21:57,120
you're working

606
00:21:57,520 --> 00:21:59,440
and then as i mentioned there is also

607
00:21:59,440 --> 00:22:02,080
the ebpf subsystem

608
00:22:02,080 --> 00:22:04,799
this again allows programs to be

609
00:22:04,799 --> 00:22:06,880
compiled in user space

610
00:22:06,880 --> 00:22:09,200
compiled down to that ebpf bytecode and

611
00:22:09,200 --> 00:22:11,679
then ran by the kernel and as i talked

612
00:22:11,679 --> 00:22:14,000
about a bunch of companies and also

613
00:22:14,000 --> 00:22:16,400
systems rely on this what you're looking

614
00:22:16,400 --> 00:22:18,480
at for our proof of concept is the

615
00:22:18,480 --> 00:22:20,559
source code to exec snoop like we saw

616
00:22:20,559 --> 00:22:21,520
before

617
00:22:21,520 --> 00:22:23,440
and then we created a memory sample

618
00:22:23,440 --> 00:22:25,760
where a machine had exec snoop running

619
00:22:25,760 --> 00:22:27,120
and in this case you can see that it

620
00:22:27,120 --> 00:22:29,440
reported that the cat command was used

621
00:22:29,440 --> 00:22:32,880
to read the password file

622
00:22:32,960 --> 00:22:34,720
we can then use a couple plugins to

623
00:22:34,720 --> 00:22:36,480
examine this behavior

624
00:22:36,480 --> 00:22:39,200
the first one is linux ebpf so this is

625
00:22:39,200 --> 00:22:41,360
going to list information on all of the

626
00:22:41,360 --> 00:22:43,679
running ebpf programs or all of the

627
00:22:43,679 --> 00:22:44,960
active ones

628
00:22:44,960 --> 00:22:47,679
you can see for this one it is hooking

629
00:22:47,679 --> 00:22:50,640
the sysenter exec ve system call and

630
00:22:50,640 --> 00:22:53,120
it's doing this by way of a trace point

631
00:22:53,120 --> 00:22:55,520
so we can then run the tracepoint plugin

632
00:22:55,520 --> 00:22:57,440
and we see that we have our hook set

633
00:22:57,440 --> 00:23:00,000
from the performance subsystem and it's

634
00:23:00,000 --> 00:23:01,840
hooked on sysenter which is going to

635
00:23:01,840 --> 00:23:02,960
hook the

636
00:23:02,960 --> 00:23:05,520
entire system call table so any

637
00:23:05,520 --> 00:23:07,679
system call function in there

638
00:23:07,679 --> 00:23:09,200
and then if we want to be able to map

639
00:23:09,200 --> 00:23:11,919
back an ebpf program to the process that

640
00:23:11,919 --> 00:23:14,640
actually loaded it we then wrote another

641
00:23:14,640 --> 00:23:16,799
plugin and that is the linux perf events

642
00:23:16,799 --> 00:23:19,520
ebpf it's going to match out these

643
00:23:19,520 --> 00:23:21,120
performance events

644
00:23:21,120 --> 00:23:22,880
filters which is essentially what your

645
00:23:22,880 --> 00:23:24,880
that previous program was

646
00:23:24,880 --> 00:23:27,520
out to not only the ebpf program but

647
00:23:27,520 --> 00:23:29,679
also the process that started it

648
00:23:29,679 --> 00:23:31,760
so you can see here if you look in the

649
00:23:31,760 --> 00:23:33,679
the green squares the address of the

650
00:23:33,679 --> 00:23:36,159
programs is the same but in our perf

651
00:23:36,159 --> 00:23:38,480
events plug-in output we see that the

652
00:23:38,480 --> 00:23:40,640
program that's responsible for it the

653
00:23:40,640 --> 00:23:42,320
actual running program that's

654
00:23:42,320 --> 00:23:44,720
responsible for it is bpf traced with

655
00:23:44,720 --> 00:23:48,080
pid 1109 so now we have the full context

656
00:23:48,080 --> 00:23:52,399
of this ebpf program that's running

657
00:23:53,440 --> 00:23:55,039
in conclusion

658
00:23:55,039 --> 00:23:57,200
hopefully you saw the value here of just

659
00:23:57,200 --> 00:23:58,720
how

660
00:23:58,720 --> 00:24:01,120
extremely powerful these tracing

661
00:24:01,120 --> 00:24:02,720
subsystems are and the kernel

662
00:24:02,720 --> 00:24:04,720
infrastructure that provides them are

663
00:24:04,720 --> 00:24:07,279
whether the malware is written as ebpf

664
00:24:07,279 --> 00:24:08,960
itself or something that hooks one of

665
00:24:08,960 --> 00:24:10,880
the lower subsystems

666
00:24:10,880 --> 00:24:12,799
the entire system can be monitored the

667
00:24:12,799 --> 00:24:14,320
full behavior of the system can be

668
00:24:14,320 --> 00:24:17,200
monitored all through the use of ebpf

669
00:24:17,200 --> 00:24:19,360
we were also very excited to see and we

670
00:24:19,360 --> 00:24:21,039
realized that our research had some

671
00:24:21,039 --> 00:24:23,840
pretty good timing that there's a talk

672
00:24:23,840 --> 00:24:26,080
at defcon and also another talk here at

673
00:24:26,080 --> 00:24:29,360
black hat about using ebpf for offensive

674
00:24:29,360 --> 00:24:31,679
purposes so i put the titles to both of

675
00:24:31,679 --> 00:24:33,279
them i would definitely recommend

676
00:24:33,279 --> 00:24:34,960
watching both of them the abstract

677
00:24:34,960 --> 00:24:36,240
sounded

678
00:24:36,240 --> 00:24:38,240
exactly like what our volatility plugins

679
00:24:38,240 --> 00:24:40,159
try to look for the abstracts both talk

680
00:24:40,159 --> 00:24:42,960
about using eppf to hook the system to

681
00:24:42,960 --> 00:24:44,960
implement rootkits and so on

682
00:24:44,960 --> 00:24:47,200
which is exactly what our plugins are

683
00:24:47,200 --> 00:24:49,919
trying to detect so these seem like

684
00:24:49,919 --> 00:24:51,679
really nice complementary talks i'll

685
00:24:51,679 --> 00:24:53,840
definitely watch both of them as well

686
00:24:53,840 --> 00:24:55,919
i'd also like to give a huge thanks to

687
00:24:55,919 --> 00:24:58,159
gus he's one of my co-workers at

688
00:24:58,159 --> 00:25:00,640
felixity he gave us a ton of feedback on

689
00:25:00,640 --> 00:25:02,159
the slides as well as the technical

690
00:25:02,159 --> 00:25:04,480
white paper and he also wrote quite a

691
00:25:04,480 --> 00:25:06,080
few of the plugins that you saw here in

692
00:25:06,080 --> 00:25:08,400
the slides

693
00:25:08,400 --> 00:25:09,919
if you have any questions or comments

694
00:25:09,919 --> 00:25:11,520
please reach out to

695
00:25:11,520 --> 00:25:13,919
both myself and golden we have our email

696
00:25:13,919 --> 00:25:15,840
addresses our twitter handles as well as

697
00:25:15,840 --> 00:25:18,799
our websites hopefully enjoy the talk we

698
00:25:18,799 --> 00:25:21,760
will also be in person at the conference

699
00:25:21,760 --> 00:25:23,760
so if you want to talk there we can or

700
00:25:23,760 --> 00:25:25,520
like i said just contact us anyway

701
00:25:25,520 --> 00:25:29,840
online we're very easy to find

