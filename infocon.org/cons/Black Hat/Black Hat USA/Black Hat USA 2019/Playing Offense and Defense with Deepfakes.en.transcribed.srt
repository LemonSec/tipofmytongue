1
00:00:00,000 --> 00:00:04,740
good morning welcome to playing offense

2
00:00:02,760 --> 00:00:07,140
and defense with deep fakes in Lagoon

3
00:00:04,740 --> 00:00:10,410
JKL with mike brice and matt price

4
00:00:07,140 --> 00:00:12,149
before we again a few brief notes please

5
00:00:10,410 --> 00:00:14,070
stop by the business Hall located in the

6
00:00:12,150 --> 00:00:16,049
Mandalay Bay Oceanside and shoreline

7
00:00:14,070 --> 00:00:18,448
ballrooms on level 2 there will be

8
00:00:16,049 --> 00:00:21,448
mimosas at 11:50 and an ice cream social

9
00:00:18,449 --> 00:00:23,039
at two at 3:20 the black had Arsenal's

10
00:00:21,449 --> 00:00:24,900
in the business hall on level two and

11
00:00:23,039 --> 00:00:26,310
lunch will be available in Bayside a B

12
00:00:24,900 --> 00:00:28,470
from 1:00 to 2:30

13
00:00:26,310 --> 00:00:30,029
don't forget the merchandise store on

14
00:00:28,470 --> 00:00:31,710
level two and session recordings from

15
00:00:30,029 --> 00:00:34,500
source of knowledge they have a desk on

16
00:00:31,710 --> 00:00:36,899
every level questions after this talk

17
00:00:34,500 --> 00:00:38,850
will be taken in the rap reef a which is

18
00:00:36,899 --> 00:00:40,559
just down the hall and please remember

19
00:00:38,850 --> 00:00:42,120
to put your phone on vibrate or turn it

20
00:00:40,559 --> 00:00:45,089
off in order to avoid interrupting the

21
00:00:42,120 --> 00:00:51,629
presentation and now please welcome Mike

22
00:00:45,090 --> 00:00:53,789
price and Matt price everybody how's it

23
00:00:51,629 --> 00:00:56,460
going my name is Mike and this is my

24
00:00:53,789 --> 00:00:58,859
colleague Matt we will be giving a talk

25
00:00:56,460 --> 00:01:01,320
to you today about playing offense and

26
00:00:58,859 --> 00:01:02,969
defense with deep fakes but to get the

27
00:01:01,320 --> 00:01:04,920
big question out of the way upfront

28
00:01:02,969 --> 00:01:06,360
know we're not brothers so we do have

29
00:01:04,920 --> 00:01:08,820
the same last name and we have the same

30
00:01:06,360 --> 00:01:11,909
first initial so we have awesome names

31
00:01:08,820 --> 00:01:16,048
but we're not brothers so with that out

32
00:01:11,909 --> 00:01:16,770
of the way I'll jump right into it the

33
00:01:16,049 --> 00:01:18,570
way that we've structured our

34
00:01:16,770 --> 00:01:21,929
presentation today is to talk in the

35
00:01:18,570 --> 00:01:23,699
first half about details related to you

36
00:01:21,930 --> 00:01:25,500
the offensive use of deep fakes so it's

37
00:01:23,700 --> 00:01:27,780
kind of exploring that topic the first

38
00:01:25,500 --> 00:01:29,549
thing that I'll talk about is to try and

39
00:01:27,780 --> 00:01:32,700
pin down a basic definition of what a

40
00:01:29,549 --> 00:01:34,259
deep fake is so it seems that different

41
00:01:32,700 --> 00:01:36,060
people have different opinions of kind

42
00:01:34,259 --> 00:01:37,409
of what the term deep pick means I'll

43
00:01:36,060 --> 00:01:40,560
give you my version of what I think it

44
00:01:37,409 --> 00:01:42,930
means then we'll talk a bit about why we

45
00:01:40,560 --> 00:01:44,610
think the deep fakes may become a

46
00:01:42,930 --> 00:01:45,750
problem I have on the slide here are a

47
00:01:44,610 --> 00:01:49,079
problem but I think it's a little bit

48
00:01:45,750 --> 00:01:52,110
more of a potential future issue I'll go

49
00:01:49,079 --> 00:01:53,279
over some of the public key examples

50
00:01:52,110 --> 00:01:55,829
that are relevant to the topic of

51
00:01:53,280 --> 00:01:56,820
offensive deep fakes so not wild

52
00:01:55,829 --> 00:01:58,350
examples because aren't many of those

53
00:01:56,820 --> 00:01:58,829
but just things that are relevant to

54
00:01:58,350 --> 00:02:00,600
that topic

55
00:01:58,829 --> 00:02:02,309
then I'll spend a little bit of time

56
00:02:00,600 --> 00:02:04,320
talking about some investigating that I

57
00:02:02,310 --> 00:02:06,780
did into how deep fakes are created and

58
00:02:04,320 --> 00:02:08,788
then I'll go through a few quote-unquote

59
00:02:06,780 --> 00:02:11,220
offensive videos to incorporate deep

60
00:02:08,788 --> 00:02:12,640
fakes as the final part of my portion of

61
00:02:11,220 --> 00:02:14,140
the presentation

62
00:02:12,640 --> 00:02:15,160
then I'll turn things over to Matt who

63
00:02:14,140 --> 00:02:17,049
will talk about the solution space

64
00:02:15,160 --> 00:02:20,020
including work that other folks have

65
00:02:17,050 --> 00:02:21,250
done to try and solve the D big problem

66
00:02:20,020 --> 00:02:23,230
so to speak to build detection

67
00:02:21,250 --> 00:02:26,950
capabilities and the Matt will do delve

68
00:02:23,230 --> 00:02:28,600
into a detection methodology that he's

69
00:02:26,950 --> 00:02:30,010
developed himself and then we'll wrap

70
00:02:28,600 --> 00:02:31,630
things up at the end with a brief note

71
00:02:30,010 --> 00:02:33,010
on deep Stark which is a tool that we

72
00:02:31,630 --> 00:02:34,990
created in conjunction with this

73
00:02:33,010 --> 00:02:40,030
presentation and that we plan to release

74
00:02:34,990 --> 00:02:42,670
in the next couple of days so with

75
00:02:40,030 --> 00:02:44,140
regards to what a deep fake is I thought

76
00:02:42,670 --> 00:02:46,390
it would be cool to talk briefly about

77
00:02:44,140 --> 00:02:48,820
the history of deep fakes so it's a

78
00:02:46,390 --> 00:02:50,350
relatively new technology they have

79
00:02:48,820 --> 00:02:51,609
really I would say only been around in

80
00:02:50,350 --> 00:02:53,320
practice for the last couple of years

81
00:02:51,610 --> 00:02:54,880
the first research that I'm aware of

82
00:02:53,320 --> 00:02:57,670
personally that came out related to this

83
00:02:54,880 --> 00:03:00,400
topic came out in 2016 via that face to

84
00:02:57,670 --> 00:03:01,958
face project this is a cool project if

85
00:03:00,400 --> 00:03:03,760
you search on youtube you'll find

86
00:03:01,959 --> 00:03:05,380
example videos from it where some

87
00:03:03,760 --> 00:03:06,850
researchers were able to control the

88
00:03:05,380 --> 00:03:09,070
facial expressions of a well-known

89
00:03:06,850 --> 00:03:10,390
person george w bush by placing

90
00:03:09,070 --> 00:03:12,280
themselves in front of a camera and

91
00:03:10,390 --> 00:03:13,630
essentially controlling george w bush in

92
00:03:12,280 --> 00:03:15,700
real time including all the movement of

93
00:03:13,630 --> 00:03:18,010
his face if you fast forward a little

94
00:03:15,700 --> 00:03:21,119
bit to the end of 2017

95
00:03:18,010 --> 00:03:23,380
the deep fake subreddit was created and

96
00:03:21,120 --> 00:03:25,269
this is where the term deep fake came

97
00:03:23,380 --> 00:03:27,850
from from that subreddit and at the same

98
00:03:25,269 --> 00:03:30,340
time the original implementation of deep

99
00:03:27,850 --> 00:03:32,350
fake face flopping was made public to

100
00:03:30,340 --> 00:03:34,000
the world so to speak and so the

101
00:03:32,350 --> 00:03:36,519
creation of the subreddit at the end of

102
00:03:34,000 --> 00:03:40,680
2017 is what really kicked off what we

103
00:03:36,519 --> 00:03:43,060
see is defects today in early 2018 the

104
00:03:40,680 --> 00:03:44,680
deep fake subreddit source code made its

105
00:03:43,060 --> 00:03:47,200
way to github and now variety people are

106
00:03:44,680 --> 00:03:48,760
working on that source code face swap is

107
00:03:47,200 --> 00:03:51,970
one well-known project in that regard

108
00:03:48,760 --> 00:03:54,010
and fake app was a GUI application that

109
00:03:51,970 --> 00:03:56,320
was created in early 2018 that made it

110
00:03:54,010 --> 00:03:58,899
easy for kind of sort of anybody to

111
00:03:56,320 --> 00:04:00,670
create deep fakes and so we began to see

112
00:03:58,900 --> 00:04:02,739
a bunch of creation and experimentation

113
00:04:00,670 --> 00:04:05,559
with deep fakes from early 2018 and on

114
00:04:02,739 --> 00:04:09,579
so a little more than a year ago in

115
00:04:05,560 --> 00:04:12,970
April 2000 18 Jordan Peele a comedian

116
00:04:09,579 --> 00:04:14,110
created a deep fake of Obama saying some

117
00:04:12,970 --> 00:04:16,478
things that he didn't actually say and

118
00:04:14,110 --> 00:04:18,489
this video was really well done it used

119
00:04:16,478 --> 00:04:20,339
his voice voice acting to solve the

120
00:04:18,488 --> 00:04:22,989
audio problem in that case in the video

121
00:04:20,339 --> 00:04:25,380
along with other people's kind of

122
00:04:22,990 --> 00:04:27,300
thinking about this problem raised

123
00:04:25,380 --> 00:04:30,210
Specter of how defects might be abused

124
00:04:27,300 --> 00:04:31,340
or used in a negative way shortly after

125
00:04:30,210 --> 00:04:33,989
that some research was released

126
00:04:31,340 --> 00:04:36,270
regarding how you might detect the fakes

127
00:04:33,990 --> 00:04:39,120
the best known example is the detection

128
00:04:36,270 --> 00:04:40,440
of defects via ID linking patterns

129
00:04:39,120 --> 00:04:42,120
basically I think that's one of the

130
00:04:40,440 --> 00:04:43,710
cases that kind of everybody has heard

131
00:04:42,120 --> 00:04:45,540
of and then a variety of other

132
00:04:43,710 --> 00:04:47,789
techniques were released and most

133
00:04:45,540 --> 00:04:50,640
recently some work that was originally

134
00:04:47,790 --> 00:04:52,020
released by Baidu called deep voice has

135
00:04:50,640 --> 00:04:54,240
matured a bit and so now we're beginning

136
00:04:52,020 --> 00:04:56,219
to see synthesized audio and we're

137
00:04:54,240 --> 00:04:58,170
seeing that combined with deep icky in

138
00:04:56,220 --> 00:04:59,880
videos so there was a recent video

139
00:04:58,170 --> 00:05:00,930
released of Mark Zuckerberg also saying

140
00:04:59,880 --> 00:05:03,480
some things that he didn't actually say

141
00:05:00,930 --> 00:05:06,180
but in that case the both the video and

142
00:05:03,480 --> 00:05:08,580
the audio were faked and so it marked a

143
00:05:06,180 --> 00:05:13,440
new public example of sort of things

144
00:05:08,580 --> 00:05:16,200
that come from definition standpoint

145
00:05:13,440 --> 00:05:18,120
loops are see here and so from my

146
00:05:16,200 --> 00:05:20,130
perspective the Timothy Pig seems to be

147
00:05:18,120 --> 00:05:22,980
it's being overloaded quite a bit at

148
00:05:20,130 --> 00:05:24,930
this point so in the news I've noticed

149
00:05:22,980 --> 00:05:26,340
that essentially any video that's been

150
00:05:24,930 --> 00:05:27,570
doctored is now being called the D

151
00:05:26,340 --> 00:05:28,650
picked by at least some folks in the

152
00:05:27,570 --> 00:05:30,360
media so there seems to be a bit of

153
00:05:28,650 --> 00:05:33,719
confusion of what exactly a deep fake is

154
00:05:30,360 --> 00:05:35,400
I personally think of it according to

155
00:05:33,720 --> 00:05:37,470
its original definition which is a video

156
00:05:35,400 --> 00:05:38,760
in which the face for one person has

157
00:05:37,470 --> 00:05:41,160
been swapped onto the face of another

158
00:05:38,760 --> 00:05:42,270
person they're not variations on this

159
00:05:41,160 --> 00:05:44,340
team so you have techniques

160
00:05:42,270 --> 00:05:45,960
aside from face whopping such as puppet

161
00:05:44,340 --> 00:05:48,210
mastering where you're controlling the

162
00:05:45,960 --> 00:05:49,229
the facial movements of a person in a

163
00:05:48,210 --> 00:05:50,310
video or lip-syncing where you're

164
00:05:49,230 --> 00:05:52,500
controlling the mouth and the video for

165
00:05:50,310 --> 00:05:53,880
example and for the purposes of this

166
00:05:52,500 --> 00:05:55,800
presentation when we first put the

167
00:05:53,880 --> 00:05:57,900
videos together for my portion of the

168
00:05:55,800 --> 00:05:59,250
presentation folks that I worked with

169
00:05:57,900 --> 00:06:01,409
said hey where's the audio and I said oh

170
00:05:59,250 --> 00:06:02,610
I think you have no idea how hard it is

171
00:06:01,410 --> 00:06:04,530
to deal with both the video and the

172
00:06:02,610 --> 00:06:06,120
audio piece at the same time so the

173
00:06:04,530 --> 00:06:08,309
scope for this presentation really ended

174
00:06:06,120 --> 00:06:10,050
up being limited the video I coped with

175
00:06:08,310 --> 00:06:11,550
the audio piece from my part of the

176
00:06:10,050 --> 00:06:14,280
video creation stuff by just using fake

177
00:06:11,550 --> 00:06:18,240
closed caption as a replacement for the

178
00:06:14,280 --> 00:06:20,010
audio so in our opinion why we think

179
00:06:18,240 --> 00:06:22,650
defects are or more likely will become a

180
00:06:20,010 --> 00:06:24,030
problem online content today it's

181
00:06:22,650 --> 00:06:26,370
frequently used for scams and other

182
00:06:24,030 --> 00:06:28,650
types of crime and it's also been used

183
00:06:26,370 --> 00:06:30,780
to influence elections so if you think

184
00:06:28,650 --> 00:06:31,919
about text and images whether these

185
00:06:30,780 --> 00:06:34,229
things are put together and used by

186
00:06:31,920 --> 00:06:36,180
folks there are those types of media are

187
00:06:34,229 --> 00:06:38,039
already being used to do bad things

188
00:06:36,180 --> 00:06:39,300
hasn't really been possible to fake

189
00:06:38,040 --> 00:06:40,590
people in videos before

190
00:06:39,300 --> 00:06:42,569
so deep lakes are kind of like a

191
00:06:40,590 --> 00:06:44,729
photoshop for people in videos before

192
00:06:42,569 --> 00:06:46,229
Photoshop it was maybe hard to edit an

193
00:06:44,729 --> 00:06:48,180
image and people probably trusted images

194
00:06:46,229 --> 00:06:50,128
more after Photoshop folks don't really

195
00:06:48,180 --> 00:06:51,599
trust images that much now we're in that

196
00:06:50,129 --> 00:06:53,460
situation with video or it's becoming

197
00:06:51,599 --> 00:06:56,310
easy and low-cost for anybody to create

198
00:06:53,460 --> 00:06:58,469
a deep fake containing a big person

199
00:06:56,310 --> 00:07:00,090
doing something potentially bad and so

200
00:06:58,470 --> 00:07:02,009
their scenario that's often described of

201
00:07:00,090 --> 00:07:03,628
the deep fake being created and dropped

202
00:07:02,009 --> 00:07:05,159
the night of an election frankly is

203
00:07:03,629 --> 00:07:06,690
pretty possible at this point but you

204
00:07:05,159 --> 00:07:08,129
can just take the software it's really

205
00:07:06,690 --> 00:07:11,039
available you can stand up in instance

206
00:07:08,129 --> 00:07:12,990
in Amazon you can pull your source video

207
00:07:11,039 --> 00:07:14,789
from an online content provider train up

208
00:07:12,990 --> 00:07:16,949
your models distributed however you'd

209
00:07:14,789 --> 00:07:19,409
like I think anybody can do that at any

210
00:07:16,949 --> 00:07:20,969
time if they wanted to so the idea of

211
00:07:19,409 --> 00:07:23,729
the asymmetry of the cost of the impact

212
00:07:20,969 --> 00:07:25,469
is quite high it would be easy to launch

213
00:07:23,729 --> 00:07:27,479
that attack it would be lower cost in

214
00:07:25,469 --> 00:07:28,469
the impact to be quite high you know

215
00:07:27,479 --> 00:07:31,729
depending on what you were trying to

216
00:07:28,469 --> 00:07:33,659
accomplish in terms of public examples

217
00:07:31,729 --> 00:07:35,250
there on the order of thousands of

218
00:07:33,659 --> 00:07:37,169
publicly known defects

219
00:07:35,250 --> 00:07:38,789
these are videos just floating around I

220
00:07:37,169 --> 00:07:41,909
would say maybe the majority of those

221
00:07:38,789 --> 00:07:43,529
are related to adult content for our

222
00:07:41,909 --> 00:07:45,719
purposes we were mostly interested in

223
00:07:43,529 --> 00:07:48,750
the non adult content stuff and so we

224
00:07:45,719 --> 00:07:51,779
prepared a collection of 300 deep fakes

225
00:07:48,750 --> 00:07:52,319
that are used in Matt's course of the

226
00:07:51,779 --> 00:07:54,479
presentation

227
00:07:52,319 --> 00:07:56,419
of all the known public defects that

228
00:07:54,479 --> 00:07:59,008
we've heard of or have in our possession

229
00:07:56,419 --> 00:08:01,380
none of them have been used yet before a

230
00:07:59,009 --> 00:08:02,789
malicious purpose per se so there are

231
00:08:01,380 --> 00:08:04,500
great examples like the Jordan Peele

232
00:08:02,789 --> 00:08:06,090
video and there are other examples that

233
00:08:04,500 --> 00:08:08,009
feature politicians but everything has

234
00:08:06,090 --> 00:08:10,770
been debate sort of a proof of concept

235
00:08:08,009 --> 00:08:14,039
video there was a recent video released

236
00:08:10,770 --> 00:08:15,750
of Nancy Pelosi that was mentioned in

237
00:08:14,039 --> 00:08:17,520
the process being a deep day by some

238
00:08:15,750 --> 00:08:18,930
folks although it was not a deep fake it

239
00:08:17,520 --> 00:08:20,400
was just a video clip that had been

240
00:08:18,930 --> 00:08:22,949
slowed you know slowed down using

241
00:08:20,400 --> 00:08:23,969
regular video editing techniques so it's

242
00:08:22,949 --> 00:08:25,710
really nothing special about the

243
00:08:23,969 --> 00:08:27,090
technique that was used there although I

244
00:08:25,710 --> 00:08:28,830
do think the intent behind that video

245
00:08:27,090 --> 00:08:30,150
was actually a little malicious and so

246
00:08:28,830 --> 00:08:32,360
it's an interesting case of using video

247
00:08:30,150 --> 00:08:34,890
for quote-unquote offensive purposes

248
00:08:32,360 --> 00:08:36,269
although it wasn't a deep fake and

249
00:08:34,890 --> 00:08:38,939
probably one of the coolest recent

250
00:08:36,269 --> 00:08:40,560
developments from from from my

251
00:08:38,940 --> 00:08:42,750
standpoint is somebody interested in

252
00:08:40,559 --> 00:08:45,380
researching this topic semantics did

253
00:08:42,750 --> 00:08:47,699
release a report where they claimed that

254
00:08:45,380 --> 00:08:51,209
they've become aware of three instances

255
00:08:47,699 --> 00:08:52,540
of criminals using fake audio to scam as

256
00:08:51,209 --> 00:08:55,089
I understand it CFO

257
00:08:52,540 --> 00:08:57,430
out of millions of dollars at I believe

258
00:08:55,089 --> 00:08:58,779
three major corporations and so I don't

259
00:08:57,430 --> 00:08:59,589
personally have all the details and I

260
00:08:58,779 --> 00:09:02,500
don't know that that's a hundred percent

261
00:08:59,589 --> 00:09:05,560
true but I assume that it is and this

262
00:09:02,500 --> 00:09:07,930
then would be the first case scene where

263
00:09:05,560 --> 00:09:09,910
tech in the deep thick sort of realm has

264
00:09:07,930 --> 00:09:11,589
been used to actually commit a crime and

265
00:09:09,910 --> 00:09:14,500
has been verified in made public so

266
00:09:11,589 --> 00:09:18,040
marks a landmark for things to come so

267
00:09:14,500 --> 00:09:20,410
to speak so I'll quickly show a couple

268
00:09:18,040 --> 00:09:22,209
of examples here the first example is

269
00:09:20,410 --> 00:09:23,649
just I'll show five or ten seconds of

270
00:09:22,209 --> 00:09:27,219
the Barack Obama clip that was created

271
00:09:23,649 --> 00:09:29,529
by Jordan Peele this is a real clip and

272
00:09:27,220 --> 00:09:31,649
the mouth and the video is controlled by

273
00:09:29,529 --> 00:09:34,269
the video creator and there is audio

274
00:09:31,649 --> 00:09:36,610
where Jordan Peele has a voice actor

275
00:09:34,269 --> 00:09:38,199
recreated Obama's voice so it looks very

276
00:09:36,610 --> 00:09:39,759
realistic the audio is very realistic

277
00:09:38,199 --> 00:09:43,449
and it would be very convincing video to

278
00:09:39,759 --> 00:09:45,279
circulate the second video is a video of

279
00:09:43,449 --> 00:09:48,370
the President of Argentina I don't have

280
00:09:45,279 --> 00:09:51,339
anybody from Argentina here but with the

281
00:09:48,370 --> 00:09:51,759
Hitler's face face walked onto macri's

282
00:09:51,339 --> 00:09:53,860
face

283
00:09:51,759 --> 00:09:57,069
so although the video wasn't released in

284
00:09:53,860 --> 00:09:58,510
an explicitly malicious sort of context

285
00:09:57,069 --> 00:10:00,670
the idea that this video circulating

286
00:09:58,510 --> 00:10:02,949
equating Macri to Hitler is certainly

287
00:10:00,670 --> 00:10:04,420
not a positive thing for him and then we

288
00:10:02,949 --> 00:10:06,160
did of course have a Nancy Pelosi video

289
00:10:04,420 --> 00:10:08,290
which is not a depict but what was

290
00:10:06,160 --> 00:10:10,120
released with malicious intent so there

291
00:10:08,290 --> 00:10:11,349
are some examples out there that sort of

292
00:10:10,120 --> 00:10:12,910
straddle line with respect to you

293
00:10:11,350 --> 00:10:19,720
whether they're offensive or not we

294
00:10:12,910 --> 00:10:22,290
haven't seen any major stuff yet so in

295
00:10:19,720 --> 00:10:26,260
terms of research into the offensive use

296
00:10:22,290 --> 00:10:27,760
I had originally set out to my

297
00:10:26,260 --> 00:10:30,040
background originally was more in the

298
00:10:27,760 --> 00:10:32,260
vulnerability space and I had in a past

299
00:10:30,040 --> 00:10:35,709
life than more reverse engineering of

300
00:10:32,260 --> 00:10:37,480
regular old software and in looking at

301
00:10:35,709 --> 00:10:39,670
how do you fix work I wanted to do a

302
00:10:37,480 --> 00:10:41,079
little bit of reversing on howdy pics

303
00:10:39,670 --> 00:10:42,670
are generated they swap the effects are

304
00:10:41,079 --> 00:10:44,469
generated and I wanted to go to sort of

305
00:10:42,670 --> 00:10:46,269
the lowest levels of the algorithm just

306
00:10:44,470 --> 00:10:47,829
to understand it in good detail so I

307
00:10:46,269 --> 00:10:49,569
tried to take that type of approach to

308
00:10:47,829 --> 00:10:53,380
looking at how the swapping algorithm

309
00:10:49,569 --> 00:10:56,019
works so I'll go through a medium level

310
00:10:53,380 --> 00:10:57,639
technical explanation of how face

311
00:10:56,019 --> 00:10:58,870
walking works and then you can take the

312
00:10:57,639 --> 00:11:00,430
slides which will be released later

313
00:10:58,870 --> 00:11:02,620
today and dig into them if you want to

314
00:11:00,430 --> 00:11:04,660
get into more that detail

315
00:11:02,620 --> 00:11:06,150
long story short if you want to create

316
00:11:04,660 --> 00:11:09,030
your face whop deep fake you

317
00:11:06,150 --> 00:11:11,160
to go through about four steps you have

318
00:11:09,030 --> 00:11:13,890
to essentially pick two people person a

319
00:11:11,160 --> 00:11:15,540
and person B at the end of the day

320
00:11:13,890 --> 00:11:17,040
you're going to be able to face flop the

321
00:11:15,540 --> 00:11:19,020
face from person B on to the face from

322
00:11:17,040 --> 00:11:20,099
person a so to keep things simple I'm

323
00:11:19,020 --> 00:11:22,470
just going to describe this in terms of

324
00:11:20,100 --> 00:11:23,880
working primarily with videos so you

325
00:11:22,470 --> 00:11:25,830
find a video that contains person a

326
00:11:23,880 --> 00:11:28,410
called video a you find a video that

327
00:11:25,830 --> 00:11:29,790
contains Chris and be called video B you

328
00:11:28,410 --> 00:11:31,230
provide both of these videos to an

329
00:11:29,790 --> 00:11:32,819
extraction step which pulls all of the

330
00:11:31,230 --> 00:11:34,860
data out of the videos that's needed to

331
00:11:32,820 --> 00:11:36,390
train up the face Wapping algorithm then

332
00:11:34,860 --> 00:11:38,610
you actually train up the face flapping

333
00:11:36,390 --> 00:11:40,650
algorithm you move on to your conversion

334
00:11:38,610 --> 00:11:41,850
step and step four and in this case what

335
00:11:40,650 --> 00:11:43,560
you'll do is you'll find a net new video

336
00:11:41,850 --> 00:11:45,060
that's different from air B but it

337
00:11:43,560 --> 00:11:47,219
contains the same person is in video a

338
00:11:45,060 --> 00:11:50,609
and then what you'll do is you'll

339
00:11:47,220 --> 00:11:52,530
convert video C into a deep fake by face

340
00:11:50,610 --> 00:11:55,620
swapping the face from video B into the

341
00:11:52,530 --> 00:11:57,360
int on to person a in video C and then

342
00:11:55,620 --> 00:11:58,350
out comes the deep fake so I'll go out

343
00:11:57,360 --> 00:12:01,260
there in a little bit more detail but

344
00:11:58,350 --> 00:12:04,200
that's the basic process from an

345
00:12:01,260 --> 00:12:05,370
extraction standpoint I found all these

346
00:12:04,200 --> 00:12:07,260
details really interesting I don't come

347
00:12:05,370 --> 00:12:09,000
from a video processing or computer

348
00:12:07,260 --> 00:12:10,770
vision background really so it was cool

349
00:12:09,000 --> 00:12:12,570
to learn about some of these things you

350
00:12:10,770 --> 00:12:14,550
take both video and video being you run

351
00:12:12,570 --> 00:12:17,010
them through the same extraction process

352
00:12:14,550 --> 00:12:20,010
you attract each frame from the videos

353
00:12:17,010 --> 00:12:22,620
you perform face detection so this is

354
00:12:20,010 --> 00:12:24,300
the use of deep learning algorithm to

355
00:12:22,620 --> 00:12:26,880
detect the presence of a face in each

356
00:12:24,300 --> 00:12:29,459
frame of the video in the case of the

357
00:12:26,880 --> 00:12:31,800
face swap implementation on github the

358
00:12:29,460 --> 00:12:34,050
MT CNN face detection algorithm is used

359
00:12:31,800 --> 00:12:35,969
and so this identifies the location of

360
00:12:34,050 --> 00:12:38,219
the face within the frame a lot of times

361
00:12:35,970 --> 00:12:40,350
faces that are contained within video

362
00:12:38,220 --> 00:12:41,490
frames are not perfectly aligned they're

363
00:12:40,350 --> 00:12:43,650
not aligned horizontally they may be

364
00:12:41,490 --> 00:12:47,190
rotated and for the training process you

365
00:12:43,650 --> 00:12:49,380
want to have those faces aligned so in a

366
00:12:47,190 --> 00:12:51,450
fine transform or a rotation essentially

367
00:12:49,380 --> 00:12:53,250
of the faces is performed to take the

368
00:12:51,450 --> 00:12:55,110
underlying faces to align faces and I'll

369
00:12:53,250 --> 00:12:57,870
show you something visual now around

370
00:12:55,110 --> 00:13:00,420
that so imagine that you have a video of

371
00:12:57,870 --> 00:13:01,860
this handsome guy and you imagine that

372
00:13:00,420 --> 00:13:04,170
you split that video into a series of

373
00:13:01,860 --> 00:13:06,330
frames so let's say we have a video with

374
00:13:04,170 --> 00:13:08,400
three frames in it we perform face

375
00:13:06,330 --> 00:13:10,200
detection on each of those frames and

376
00:13:08,400 --> 00:13:12,390
you end up with something like this the

377
00:13:10,200 --> 00:13:13,800
bounding box around my face now some of

378
00:13:12,390 --> 00:13:15,270
the frames might have an underline face

379
00:13:13,800 --> 00:13:16,469
that would be the case where the face is

380
00:13:15,270 --> 00:13:18,329
rotated like you see here

381
00:13:16,470 --> 00:13:19,620
and so we would perform our transform

382
00:13:18,330 --> 00:13:19,880
and align all of those faces so that

383
00:13:19,620 --> 00:13:22,790
there

384
00:13:19,880 --> 00:13:24,170
level let's say that was for video a for

385
00:13:22,790 --> 00:13:27,800
video B you have this other handsome guy

386
00:13:24,170 --> 00:13:30,520
here and so we go through the same

387
00:13:27,800 --> 00:13:33,439
process for video B extract the frames a

388
00:13:30,520 --> 00:13:35,360
perform face detection align the faces

389
00:13:33,440 --> 00:13:37,790
and at the end of the extraction phase

390
00:13:35,360 --> 00:13:40,340
we ended up with two sets of extracted

391
00:13:37,790 --> 00:13:42,740
face images set a set B which is just a

392
00:13:40,340 --> 00:13:44,030
bunch of basically you know my faces and

393
00:13:42,740 --> 00:13:47,660
a bunch of a bunch of Matt's faces let's

394
00:13:44,030 --> 00:13:49,459
say so once we have our data set

395
00:13:47,660 --> 00:13:51,020
face faces there then we move to the

396
00:13:49,460 --> 00:13:53,870
training stage so this is where things

397
00:13:51,020 --> 00:13:55,730
get kind of interesting there are now a

398
00:13:53,870 --> 00:13:57,530
variety of face popping algorithms that

399
00:13:55,730 --> 00:14:00,380
are out there the face ba bleep on

400
00:13:57,530 --> 00:14:02,600
github includes maybe ten of them I want

401
00:14:00,380 --> 00:14:04,040
to say although the original face bop

402
00:14:02,600 --> 00:14:05,420
algorithm that was released as part of

403
00:14:04,040 --> 00:14:07,069
the deep fake subreddit remains the

404
00:14:05,420 --> 00:14:09,079
default option in that tool and so I'll

405
00:14:07,070 --> 00:14:10,790
be describing that original algorithm

406
00:14:09,080 --> 00:14:11,990
here there are others you can take a

407
00:14:10,790 --> 00:14:14,900
look at if you want to go through the

408
00:14:11,990 --> 00:14:16,880
source so the original face mapping

409
00:14:14,900 --> 00:14:19,310
algorithm uses a deep learning algorithm

410
00:14:16,880 --> 00:14:21,590
known as an ottoman coated for an autumn

411
00:14:19,310 --> 00:14:23,209
quarter essentially what you have are

412
00:14:21,590 --> 00:14:25,520
two independent models and encoder and a

413
00:14:23,210 --> 00:14:27,020
decoder you have some input data such as

414
00:14:25,520 --> 00:14:28,880
an image in this case so the image of

415
00:14:27,020 --> 00:14:31,569
the face you supply the image to the

416
00:14:28,880 --> 00:14:33,439
encoder in the encoder outputs

417
00:14:31,570 --> 00:14:35,840
essentially a compressed representation

418
00:14:33,440 --> 00:14:38,060
of its input known as a waking image so

419
00:14:35,840 --> 00:14:41,120
the encoder takes an image in it spits

420
00:14:38,060 --> 00:14:43,189
out a reduced version of the original

421
00:14:41,120 --> 00:14:45,110
image that reduce version is then

422
00:14:43,190 --> 00:14:46,550
supplied to a Dakota in the decoder then

423
00:14:45,110 --> 00:14:49,820
attempts to recreate the original image

424
00:14:46,550 --> 00:14:51,890
now the training process runs for a long

425
00:14:49,820 --> 00:14:53,870
time and this model attempts to learn

426
00:14:51,890 --> 00:14:57,319
all of the things that it needs to learn

427
00:14:53,870 --> 00:15:00,350
in order to recreate a copy of the

428
00:14:57,320 --> 00:15:02,360
original image with the greatest quality

429
00:15:00,350 --> 00:15:05,150
possible so to speak the reason that

430
00:15:02,360 --> 00:15:07,280
this process is the benefit of using

431
00:15:05,150 --> 00:15:09,260
this approach basically is that the

432
00:15:07,280 --> 00:15:10,730
ideas autoencoder learns the important

433
00:15:09,260 --> 00:15:12,590
details of the images that are provided

434
00:15:10,730 --> 00:15:15,020
as input and discards that unimportant

435
00:15:12,590 --> 00:15:17,450
details which is essentially how the

436
00:15:15,020 --> 00:15:20,660
model learns characteristics of the

437
00:15:17,450 --> 00:15:21,770
individual faces so with some images in

438
00:15:20,660 --> 00:15:27,230
place then this is kind of what that

439
00:15:21,770 --> 00:15:28,220
looks like now the thing that the face

440
00:15:27,230 --> 00:15:30,350
Wapping algorithm does a little

441
00:15:28,220 --> 00:15:32,060
differently is it uses one encoder the

442
00:15:30,350 --> 00:15:33,300
same as a traditional auto encoder but

443
00:15:32,060 --> 00:15:35,819
it uses two separate decoder

444
00:15:33,300 --> 00:15:38,399
one for video and one for video B and so

445
00:15:35,820 --> 00:15:42,330
the decoders are able to learn the

446
00:15:38,399 --> 00:15:44,190
facial features of the different people

447
00:15:42,330 --> 00:15:46,769
so to speak by splitting the coding

448
00:15:44,190 --> 00:15:49,019
process so we essentially provide all of

449
00:15:46,769 --> 00:15:50,820
our extracted faces to the encoder which

450
00:15:49,019 --> 00:15:52,860
produce different latent images for

451
00:15:50,820 --> 00:15:54,480
person printing beat person B which are

452
00:15:52,860 --> 00:15:56,160
then supplied to the their respective

453
00:15:54,480 --> 00:16:01,560
decoders and the models trained up in

454
00:15:56,160 --> 00:16:03,060
this way so if we overlay some images

455
00:16:01,560 --> 00:16:04,410
onto that then this is kinda what it

456
00:16:03,060 --> 00:16:06,149
looks like from a visual standpoint a

457
00:16:04,410 --> 00:16:08,339
picture of me picture Matt goes into the

458
00:16:06,149 --> 00:16:09,390
encoder latent images come out it's not

459
00:16:08,339 --> 00:16:11,100
what the latent images actually looked

460
00:16:09,390 --> 00:16:13,980
like it's just a representation they're

461
00:16:11,100 --> 00:16:15,360
supplied to their decoders the recreated

462
00:16:13,980 --> 00:16:17,430
images come out the other end of the

463
00:16:15,360 --> 00:16:19,560
decoder they typically lossy it's kind

464
00:16:17,430 --> 00:16:21,209
of the nature of the thing and this

465
00:16:19,560 --> 00:16:25,018
process is repeated until the quality of

466
00:16:21,209 --> 00:16:26,369
the output becomes high if we further

467
00:16:25,019 --> 00:16:28,110
deep delve into the training algorithm

468
00:16:26,370 --> 00:16:30,660
and I'll kind of stop going through all

469
00:16:28,110 --> 00:16:31,560
of the steps at this point then we see

470
00:16:30,660 --> 00:16:33,779
that there's a little bit more

471
00:16:31,560 --> 00:16:35,399
complexity in in reality to how this

472
00:16:33,779 --> 00:16:37,709
works then then the simplified

473
00:16:35,399 --> 00:16:39,360
description in the last slide the

474
00:16:37,709 --> 00:16:41,880
individual frames are actually

475
00:16:39,360 --> 00:16:43,709
pre-processed a bit by a training data

476
00:16:41,880 --> 00:16:46,529
generator algorithm in the face ball

477
00:16:43,709 --> 00:16:48,119
implementation then what images are

478
00:16:46,529 --> 00:16:49,410
passed to the encoder they run through

479
00:16:48,120 --> 00:16:51,510
the same process and they are compared

480
00:16:49,410 --> 00:16:53,160
to target images generated by the

481
00:16:51,510 --> 00:16:56,610
training data generator towards the end

482
00:16:53,160 --> 00:17:01,020
of this process and one thing that's

483
00:16:56,610 --> 00:17:02,730
pretty cool about Kerris which is in AI

484
00:17:01,020 --> 00:17:05,099
framework that you can use to build

485
00:17:02,730 --> 00:17:07,020
models like the one that's used here is

486
00:17:05,099 --> 00:17:09,359
that you can export the models once

487
00:17:07,020 --> 00:17:10,470
they're constructed to PNG format and so

488
00:17:09,359 --> 00:17:13,649
you can kind of visualize what they look

489
00:17:10,470 --> 00:17:16,260
like so this is just a PNG graphic of

490
00:17:13,650 --> 00:17:18,410
the top level model which just shows the

491
00:17:16,260 --> 00:17:21,660
input layer which would be a 64 by 64

492
00:17:18,410 --> 00:17:24,209
RGB extracted face that would be fed

493
00:17:21,660 --> 00:17:25,530
into the encoder model and then the

494
00:17:24,209 --> 00:17:28,950
output from the encoder model is fed

495
00:17:25,530 --> 00:17:32,340
into the decoder model and this is what

496
00:17:28,950 --> 00:17:33,780
the encoders network architecture looks

497
00:17:32,340 --> 00:17:35,100
like so I won't go into all the details

498
00:17:33,780 --> 00:17:36,300
right but you can just see that there's

499
00:17:35,100 --> 00:17:39,389
a substantial number of different layers

500
00:17:36,300 --> 00:17:41,490
here and transformations that occur and

501
00:17:39,390 --> 00:17:44,669
then you see the same thing from the

502
00:17:41,490 --> 00:17:45,780
decoding side of the equation so the

503
00:17:44,669 --> 00:17:46,970
only thing that's kind of noticeable

504
00:17:45,780 --> 00:17:48,770
here is that you

505
00:17:46,970 --> 00:17:51,289
see on the left-hand side of the encoder

506
00:17:48,770 --> 00:17:53,299
your inputs are the 64 by 64 pixel image

507
00:17:51,289 --> 00:17:56,860
and your outputs of the decoder also

508
00:17:53,299 --> 00:17:59,000
this we created the image right 64 by 64

509
00:17:56,860 --> 00:18:02,149
so once we move past the training step

510
00:17:59,000 --> 00:18:03,559
we get into conversion and so this is

511
00:18:02,150 --> 00:18:05,360
where we want to take a net new video

512
00:18:03,559 --> 00:18:07,760
containing person a and we want to

513
00:18:05,360 --> 00:18:09,350
create the actual D fake right so we

514
00:18:07,760 --> 00:18:11,780
take our video of C which is a new video

515
00:18:09,350 --> 00:18:13,939
we extract all the frames we use empty

516
00:18:11,780 --> 00:18:16,059
CNN again to detect the faces we go

517
00:18:13,940 --> 00:18:18,740
through that same exercise of the line

518
00:18:16,059 --> 00:18:19,940
extracting faces and lining them but the

519
00:18:18,740 --> 00:18:21,770
thing that we do here that's a little

520
00:18:19,940 --> 00:18:24,080
bit different is these are faces for

521
00:18:21,770 --> 00:18:26,600
person a which we supply to our encoder

522
00:18:24,080 --> 00:18:28,939
and then we supply to the decoder for

523
00:18:26,600 --> 00:18:30,889
person B so instead of encoding and

524
00:18:28,940 --> 00:18:33,260
decoding via decoder a we decode the

525
00:18:30,890 --> 00:18:36,830
decoder B and so this is the step in the

526
00:18:33,260 --> 00:18:38,830
process that actually causes the the

527
00:18:36,830 --> 00:18:41,120
characteristics of person B to be

528
00:18:38,830 --> 00:18:43,220
transformed onto the face of person a or

529
00:18:41,120 --> 00:18:44,209
essentially where the faces are actually

530
00:18:43,220 --> 00:18:47,390
deep faked right or where their face

531
00:18:44,210 --> 00:18:50,270
popped so now this image containing the

532
00:18:47,390 --> 00:18:51,740
face pot person is output it's aligned

533
00:18:50,270 --> 00:18:53,150
back to its original position in the

534
00:18:51,740 --> 00:18:55,669
video frame and then all those frames

535
00:18:53,150 --> 00:18:57,500
are merged back together into a net new

536
00:18:55,669 --> 00:18:58,520
video which is the deep fake so this is

537
00:18:57,500 --> 00:19:01,400
essentially how the conversion process

538
00:18:58,520 --> 00:19:04,908
works so with pictures it looks kind of

539
00:19:01,400 --> 00:19:06,860
like this or something like this you'll

540
00:19:04,909 --> 00:19:09,590
see that the face whopping on the upper

541
00:19:06,860 --> 00:19:10,760
left occurs the faces are rotated back

542
00:19:09,590 --> 00:19:13,250
to the original position in the frame

543
00:19:10,760 --> 00:19:14,750
they're moved back into the frame all

544
00:19:13,250 --> 00:19:20,450
the frames are then compressed into some

545
00:19:14,750 --> 00:19:21,799
video so as part of our presentation

546
00:19:20,450 --> 00:19:25,340
gets a little interesting and I'm almost

547
00:19:21,799 --> 00:19:27,408
done so with my part so what I wanted to

548
00:19:25,340 --> 00:19:29,720
do from an offensive standpoint was to

549
00:19:27,409 --> 00:19:31,460
experiment with creating the type of

550
00:19:29,720 --> 00:19:34,820
quote-unquote payload that I'd want to

551
00:19:31,460 --> 00:19:36,110
distribute in the context of an

552
00:19:34,820 --> 00:19:38,750
influencer operation you know

553
00:19:36,110 --> 00:19:41,330
politically and so representative Adam

554
00:19:38,750 --> 00:19:42,559
Schiff who has important role in the

555
00:19:41,330 --> 00:19:45,139
House of Representatives as I understand

556
00:19:42,559 --> 00:19:46,460
it and who's been pretty vocal about the

557
00:19:45,140 --> 00:19:48,500
deep fake topic and in raising awareness

558
00:19:46,460 --> 00:19:50,120
with respect to this topic stood out to

559
00:19:48,500 --> 00:19:52,070
me as a person that I might want to

560
00:19:50,120 --> 00:19:53,600
include into my video a because of his

561
00:19:52,070 --> 00:19:55,340
interest in the topic and B because we

562
00:19:53,600 --> 00:19:57,049
have a little bit of similar facial

563
00:19:55,340 --> 00:19:58,879
geometry so our mouths are kind of the

564
00:19:57,049 --> 00:20:00,470
same thing well those lips are a little

565
00:19:58,880 --> 00:20:04,490
powder than mine but hey

566
00:20:00,470 --> 00:20:05,990
so what I wanted to do as I was learning

567
00:20:04,490 --> 00:20:08,450
about these different things was to

568
00:20:05,990 --> 00:20:10,160
create a video in which I have an

569
00:20:08,450 --> 00:20:12,020
original video of Adam Schiff but I

570
00:20:10,160 --> 00:20:13,370
essentially control the mouth or replace

571
00:20:12,020 --> 00:20:16,490
the mouth in that video with my own

572
00:20:13,370 --> 00:20:17,659
which in theory allows me to have a

573
00:20:16,490 --> 00:20:18,980
video of Adam Schiff saying things that

574
00:20:17,660 --> 00:20:20,990
you didn't actually say kind of like the

575
00:20:18,980 --> 00:20:22,820
Obama video I will say that the Jordan

576
00:20:20,990 --> 00:20:25,730
Peele outcome was a little higher

577
00:20:22,820 --> 00:20:27,649
quality than mine but you know I'm sure

578
00:20:25,730 --> 00:20:30,680
that I'll get there soon and so the way

579
00:20:27,650 --> 00:20:33,020
this process works is I created a deep

580
00:20:30,680 --> 00:20:35,540
fake in which I face whopped Adam Schiff

581
00:20:33,020 --> 00:20:37,790
face onto mine so in the third image

582
00:20:35,540 --> 00:20:39,680
from the left this is a small capture of

583
00:20:37,790 --> 00:20:43,040
what that deep ik looks like it's kind

584
00:20:39,680 --> 00:20:45,260
of horrific if you if you look at it and

585
00:20:43,040 --> 00:20:48,290
then what I did is I performed mouth

586
00:20:45,260 --> 00:20:49,970
extraction on that deep fake and then I

587
00:20:48,290 --> 00:20:52,070
pulled each mouth out of the deep fake

588
00:20:49,970 --> 00:20:54,200
and overlaid it on to an original video

589
00:20:52,070 --> 00:20:56,659
of Adam Schiff the overlay process is a

590
00:20:54,200 --> 00:20:57,590
huge p.m. and so that's that was

591
00:20:56,660 --> 00:20:59,090
actually the hardest part of this

592
00:20:57,590 --> 00:21:01,040
process and an area where I need to

593
00:20:59,090 --> 00:21:02,419
continue to improve but at the end of

594
00:21:01,040 --> 00:21:04,030
the day this is a way in which you can

595
00:21:02,420 --> 00:21:05,570
take a video of somebody and

596
00:21:04,030 --> 00:21:07,129
quote-unquote take control of their

597
00:21:05,570 --> 00:21:12,439
mouth have them try and say things that

598
00:21:07,130 --> 00:21:14,930
you want them to say so a comment on how

599
00:21:12,440 --> 00:21:15,770
this was created originally I was

600
00:21:14,930 --> 00:21:17,210
thinking that I might trying to

601
00:21:15,770 --> 00:21:20,090
implement the deep faking algorithms

602
00:21:17,210 --> 00:21:21,350
myself I realized that I wasn't really

603
00:21:20,090 --> 00:21:23,179
sustainable approach in the sense that

604
00:21:21,350 --> 00:21:24,709
there's much more investment going into

605
00:21:23,180 --> 00:21:27,020
tools like face Watt and you know I

606
00:21:24,710 --> 00:21:28,640
would be able to put into mine so I did

607
00:21:27,020 --> 00:21:31,010
use Facebook to create the deep dig and

608
00:21:28,640 --> 00:21:32,060
then the leap star tool was used to

609
00:21:31,010 --> 00:21:33,950
perform all the other operations

610
00:21:32,060 --> 00:21:36,260
required to create the videos that I'll

611
00:21:33,950 --> 00:21:38,360
show you in a second really what I was

612
00:21:36,260 --> 00:21:40,190
interested in doing was creating kind of

613
00:21:38,360 --> 00:21:42,649
an automated kit that would allow me to

614
00:21:40,190 --> 00:21:44,590
create video fake news that incorporates

615
00:21:42,650 --> 00:21:46,700
mouth swab deep fakes that I could then

616
00:21:44,590 --> 00:21:49,189
automatically deliver to a list of email

617
00:21:46,700 --> 00:21:50,690
targets and SMS targets so to speak with

618
00:21:49,190 --> 00:21:52,580
the idea being that if I were on the eve

619
00:21:50,690 --> 00:21:54,800
of an election I might pop in 10,000

620
00:21:52,580 --> 00:21:57,740
emails or 10,000 phone numbers click a

621
00:21:54,800 --> 00:21:58,850
button and you know change the outcome

622
00:21:57,740 --> 00:22:03,050
of the luncheon right this is where I

623
00:21:58,850 --> 00:22:04,520
was at mentally deepstar supports all

624
00:22:03,050 --> 00:22:06,200
the operations that are required for

625
00:22:04,520 --> 00:22:07,520
that so it allows me to automate the

626
00:22:06,200 --> 00:22:10,910
process of pulling down video from the

627
00:22:07,520 --> 00:22:12,440
internet of extracting frames and

628
00:22:10,910 --> 00:22:13,460
whatnot from the video that I can use to

629
00:22:12,440 --> 00:22:15,920
train up the

630
00:22:13,460 --> 00:22:17,180
place a Facebook model I'm also able to

631
00:22:15,920 --> 00:22:18,800
automate the process of pulling down

632
00:22:17,180 --> 00:22:20,990
stock footage editing all the clips

633
00:22:18,800 --> 00:22:24,169
together merging in the deed fake and

634
00:22:20,990 --> 00:22:26,240
then I also did a modest implementation

635
00:22:24,170 --> 00:22:29,300
of the mouth swap effect myself and then

636
00:22:26,240 --> 00:22:30,920
the last capability on with respect to

637
00:22:29,300 --> 00:22:32,480
offense in the tool there's the ability

638
00:22:30,920 --> 00:22:34,190
to automatically deploy the video back

639
00:22:32,480 --> 00:22:36,560
out to the Internet to video hosting

640
00:22:34,190 --> 00:22:38,510
provider and then to populate a template

641
00:22:36,560 --> 00:22:43,899
and ship it off via email text message

642
00:22:38,510 --> 00:22:46,310
to people so kind of m2m script to

643
00:22:43,900 --> 00:22:51,020
influence an election or something like

644
00:22:46,310 --> 00:22:52,310
that so this is the first video I put a

645
00:22:51,020 --> 00:22:53,120
lot of time into this I really wished it

646
00:22:52,310 --> 00:22:54,470
would have come out a little bit better

647
00:22:53,120 --> 00:22:55,879
but I still think that I did an okay job

648
00:22:54,470 --> 00:22:58,790
so I'm sure you'll see a better version

649
00:22:55,880 --> 00:23:00,950
of this from you soon so this is again

650
00:22:58,790 --> 00:23:03,320
there's no audio this is a video of Adam

651
00:23:00,950 --> 00:23:05,090
Schiff with my mouth extracted from the

652
00:23:03,320 --> 00:23:09,169
deep fake and overlaid on to the video

653
00:23:05,090 --> 00:23:11,060
so no big deal so then if we move on to

654
00:23:09,170 --> 00:23:13,280
the next clip this is a video clip that

655
00:23:11,060 --> 00:23:16,639
I produced in which that same Adam

656
00:23:13,280 --> 00:23:21,620
Schiff video is edited into a fake fox

657
00:23:16,640 --> 00:23:22,610
news segment in a segment representative

658
00:23:21,620 --> 00:23:29,270
Adam Schiff declares his level of

659
00:23:22,610 --> 00:23:31,310
puppies so if I you know I was nervous

660
00:23:29,270 --> 00:23:33,230
that if I showed the hard-hitting stuff

661
00:23:31,310 --> 00:23:36,409
first that you know I would take the

662
00:23:33,230 --> 00:23:38,720
wind out of my sails so I also created a

663
00:23:36,410 --> 00:23:41,930
video which is a CNN version of the same

664
00:23:38,720 --> 00:23:44,330
clip essentially and then I created

665
00:23:41,930 --> 00:23:45,410
another version for MSNBC and you can

666
00:23:44,330 --> 00:23:47,240
imagine that this would be possible to

667
00:23:45,410 --> 00:23:48,740
do for kind of any news network I was

668
00:23:47,240 --> 00:23:50,270
hoping that I'd have time to get to for

669
00:23:48,740 --> 00:23:52,040
example leading news networks in Russia

670
00:23:50,270 --> 00:23:56,379
China North Korea and all that stuff I

671
00:23:52,040 --> 00:23:58,399
just didn't have the time for it and so

672
00:23:56,380 --> 00:24:00,080
after having created the videos then

673
00:23:58,400 --> 00:24:02,030
through the use of the tool I was able

674
00:24:00,080 --> 00:24:05,060
to automate the delivery of a link to

675
00:24:02,030 --> 00:24:07,220
the videos out via email and then to

676
00:24:05,060 --> 00:24:08,270
perform the same operation out vsms so I

677
00:24:07,220 --> 00:24:09,800
have some s seemed interesting to me

678
00:24:08,270 --> 00:24:10,879
because I don't think that it's a Vectra

679
00:24:09,800 --> 00:24:12,350
folks would really like to think about

680
00:24:10,880 --> 00:24:16,150
it as obviously as they would think

681
00:24:12,350 --> 00:24:19,820
about email or something like that so

682
00:24:16,150 --> 00:24:21,590
kind of coming to conclusion then the

683
00:24:19,820 --> 00:24:25,100
last piece of video that I did create

684
00:24:21,590 --> 00:24:26,540
which is a little bit more edgy than the

685
00:24:25,100 --> 00:24:29,949
puppy's video

686
00:24:26,540 --> 00:24:32,270
is the same format but clip in which

687
00:24:29,950 --> 00:24:34,550
representative ship exonerate Donald

688
00:24:32,270 --> 00:24:36,050
Trump their lease didn't happen I did

689
00:24:34,550 --> 00:24:37,700
send this video to my mom who is a

690
00:24:36,050 --> 00:24:40,580
left-leaning person that lives in

691
00:24:37,700 --> 00:24:41,750
Northern California and she was upset

692
00:24:40,580 --> 00:24:44,480
for a few seconds and then figured it

693
00:24:41,750 --> 00:24:47,000
out so it was rewarding for me beyond

694
00:24:44,480 --> 00:24:48,860
that though did not distribute the video

695
00:24:47,000 --> 00:24:50,510
so originally I was planning to do that

696
00:24:48,860 --> 00:24:52,820
part of a talk description implied that

697
00:24:50,510 --> 00:24:55,700
I would do that at the time that we

698
00:24:52,820 --> 00:24:57,230
proposed this talk it seemed like a good

699
00:24:55,700 --> 00:24:58,430
idea as things became more real it

700
00:24:57,230 --> 00:25:01,490
seemed like it was a little bit riskier

701
00:24:58,430 --> 00:25:02,780
so we'll do that maybe in the future so

702
00:25:01,490 --> 00:25:04,850
at the end of the day from the offensive

703
00:25:02,780 --> 00:25:06,470
side of the equation then you know I

704
00:25:04,850 --> 00:25:09,139
think that essentially what I tried to

705
00:25:06,470 --> 00:25:10,310
do here was to take the sort of thing

706
00:25:09,140 --> 00:25:11,840
that everybody's been thinking and make

707
00:25:10,310 --> 00:25:13,639
it real which is if you have an Obama

708
00:25:11,840 --> 00:25:15,020
video great but like what would somebody

709
00:25:13,640 --> 00:25:16,400
do to actually distribute that thing you

710
00:25:15,020 --> 00:25:17,900
might package it up into fake news you

711
00:25:16,400 --> 00:25:20,930
might automate the delivery process and

712
00:25:17,900 --> 00:25:24,050
so making that all kind of kid about

713
00:25:20,930 --> 00:25:25,940
easy to do is what I worked on and then

714
00:25:24,050 --> 00:25:27,169
for these videos you know I wanted to

715
00:25:25,940 --> 00:25:28,610
show some real examples of what you

716
00:25:27,170 --> 00:25:29,930
could expect to see so it's not very

717
00:25:28,610 --> 00:25:32,540
hard to do this maybe like two or three

718
00:25:29,930 --> 00:25:35,120
week weeks with the work so the barrier

719
00:25:32,540 --> 00:25:36,320
to entry is low and I think the point

720
00:25:35,120 --> 00:25:37,639
for me at the end of the day is like if

721
00:25:36,320 --> 00:25:38,960
somebody did a slightly better job of

722
00:25:37,640 --> 00:25:41,660
creating these videos and deployed them

723
00:25:38,960 --> 00:25:42,800
on and even an election would we

724
00:25:41,660 --> 00:25:44,360
collectively be equipped to deal with

725
00:25:42,800 --> 00:25:45,290
that I don't believe the answer is yes I

726
00:25:44,360 --> 00:25:46,669
think that you know somebody could

727
00:25:45,290 --> 00:25:49,540
actually do this and somebody may very

728
00:25:46,670 --> 00:25:51,500
well do it so something to keep in mind

729
00:25:49,540 --> 00:25:53,300
what that's it I'll turn it over to Matt

730
00:25:51,500 --> 00:25:58,640
who will share everything about the

731
00:25:53,300 --> 00:26:00,740
solution side of the house awesome so

732
00:25:58,640 --> 00:26:03,950
now that Mike's kind of covered how we

733
00:26:00,740 --> 00:26:05,570
can create deep lakes and how we can use

734
00:26:03,950 --> 00:26:06,650
them offensively let's switch gears a

735
00:26:05,570 --> 00:26:10,310
little bit and talk about actually

736
00:26:06,650 --> 00:26:12,680
detecting deep fix so one of the first

737
00:26:10,310 --> 00:26:14,720
points that want to make is that humans

738
00:26:12,680 --> 00:26:17,120
are horrible detection machines when it

739
00:26:14,720 --> 00:26:20,030
comes to image manipulation and a study

740
00:26:17,120 --> 00:26:23,120
done by some European researchers on 143

741
00:26:20,030 --> 00:26:26,690
computer science students what they did

742
00:26:23,120 --> 00:26:29,209
is they gave them a set a series of 60

743
00:26:26,690 --> 00:26:31,670
images and within those images 50% of

744
00:26:29,210 --> 00:26:33,530
them were pristine images real images

745
00:26:31,670 --> 00:26:35,420
they hadn't been manipulated and half

746
00:26:33,530 --> 00:26:37,160
those images had been manipulated using

747
00:26:35,420 --> 00:26:40,150
three different types of deep faking

748
00:26:37,160 --> 00:26:42,220
methods so what those reaches

749
00:26:40,150 --> 00:26:44,350
Searchers ended up found is that at best

750
00:26:42,220 --> 00:26:47,890
humans can identify real images with the

751
00:26:44,350 --> 00:26:49,810
rate of about eighty percent which is

752
00:26:47,890 --> 00:26:51,340
fairly poor in my opinion but then when

753
00:26:49,810 --> 00:26:52,720
we start talking about images that had

754
00:26:51,340 --> 00:26:53,889
been actually manipulated what we're

755
00:26:52,720 --> 00:26:56,380
actually going to find in deep fake

756
00:26:53,890 --> 00:26:57,910
videos at best humans are able to

757
00:26:56,380 --> 00:26:59,680
identify those forgeries at a rate of

758
00:26:57,910 --> 00:27:02,950
about seventy five percent and at worst

759
00:26:59,680 --> 00:27:05,470
a rate of forty percent so we can't rely

760
00:27:02,950 --> 00:27:08,260
on humans to detect deep fakes

761
00:27:05,470 --> 00:27:09,730
unfortunately so that army of human

762
00:27:08,260 --> 00:27:13,200
analysts that you are going to deploy

763
00:27:09,730 --> 00:27:15,630
tech defects is not going to work

764
00:27:13,200 --> 00:27:17,800
luckily there are a number of

765
00:27:15,630 --> 00:27:19,740
methodologies that we can use to detect

766
00:27:17,800 --> 00:27:21,790
defects and a bunch of these

767
00:27:19,740 --> 00:27:25,480
methodologies have actually already

768
00:27:21,790 --> 00:27:27,190
existed in the image forensic realm so

769
00:27:25,480 --> 00:27:29,650
one of those ways is looking at the

770
00:27:27,190 --> 00:27:31,060
signal level and what we're talking

771
00:27:29,650 --> 00:27:33,430
about here are properties that are

772
00:27:31,060 --> 00:27:34,929
inherent to the image itself so for

773
00:27:33,430 --> 00:27:37,180
example if image had been compressed

774
00:27:34,930 --> 00:27:38,890
using a double JPEG compression for

775
00:27:37,180 --> 00:27:40,390
example that's usually a red flag that

776
00:27:38,890 --> 00:27:42,970
that image has been manipulated in some

777
00:27:40,390 --> 00:27:45,340
ways same things if we go and say find

778
00:27:42,970 --> 00:27:46,930
like artifacts left behind by again when

779
00:27:45,340 --> 00:27:48,730
you're creating a deep fake video using

780
00:27:46,930 --> 00:27:51,400
an audio coder those are things that we

781
00:27:48,730 --> 00:27:52,570
can detect at the signal level at the

782
00:27:51,400 --> 00:27:54,310
physical level we're talking about

783
00:27:52,570 --> 00:27:56,740
things like lighting conditions shadows

784
00:27:54,310 --> 00:27:58,419
reflections and so on so an example of

785
00:27:56,740 --> 00:27:59,830
this would be if a light source on a

786
00:27:58,420 --> 00:28:01,780
face in a video is coming from the right

787
00:27:59,830 --> 00:28:03,220
but all the other objects within that

788
00:28:01,780 --> 00:28:04,990
video indicate that light source is

789
00:28:03,220 --> 00:28:06,190
coming from the left that's a huge red

790
00:28:04,990 --> 00:28:09,460
flag that we're probably looking at it'd

791
00:28:06,190 --> 00:28:10,780
be fake at the semantic level we're

792
00:28:09,460 --> 00:28:12,910
talking about really consistency of

793
00:28:10,780 --> 00:28:14,920
metadata so if the video gets released

794
00:28:12,910 --> 00:28:16,780
and it has a public figure in it and

795
00:28:14,920 --> 00:28:18,850
they say that this public figure was at

796
00:28:16,780 --> 00:28:20,320
this location at this time and day we

797
00:28:18,850 --> 00:28:22,090
can actually go back and verify that

798
00:28:20,320 --> 00:28:25,570
that public figure was indeed at that

799
00:28:22,090 --> 00:28:27,659
location on that date so these three

800
00:28:25,570 --> 00:28:30,399
methods I just talked to you single

801
00:28:27,660 --> 00:28:32,320
signal physical and semantic level these

802
00:28:30,400 --> 00:28:34,080
are things that methods that have been

803
00:28:32,320 --> 00:28:36,730
used in traditional image forensics

804
00:28:34,080 --> 00:28:38,949
something that's new to detecting

805
00:28:36,730 --> 00:28:40,330
defects is using physiological signals

806
00:28:38,950 --> 00:28:42,010
so these are things like essentially

807
00:28:40,330 --> 00:28:44,530
involuntary traits that we all have as

808
00:28:42,010 --> 00:28:46,800
humans so things like breathing eye

809
00:28:44,530 --> 00:28:49,420
blinking facial orientation and so on

810
00:28:46,800 --> 00:28:51,580
what's unique about using physiological

811
00:28:49,420 --> 00:28:53,230
signals is that these methods can be

812
00:28:51,580 --> 00:28:53,928
tailored to an individual or they can be

813
00:28:53,230 --> 00:28:56,259
generalized

814
00:28:53,929 --> 00:28:59,389
they work for any number of individuals

815
00:28:56,259 --> 00:29:01,789
two things that at least they at least

816
00:28:59,389 --> 00:29:03,379
warn mention but aren't really detection

817
00:29:01,789 --> 00:29:05,299
methods per se our video authentication

818
00:29:03,379 --> 00:29:07,519
for example blockchain gets thrown

819
00:29:05,299 --> 00:29:09,980
around a lot right now or white or black

820
00:29:07,519 --> 00:29:11,629
listing so while these aren't detection

821
00:29:09,980 --> 00:29:13,340
methods per se they could help in the

822
00:29:11,629 --> 00:29:15,350
short term with preventing deep fakes

823
00:29:13,340 --> 00:29:20,539
from causing a bigger problem while we

824
00:29:15,350 --> 00:29:21,590
catch up on the detection side so now

825
00:29:20,539 --> 00:29:23,419
that we've gone over some of the

826
00:29:21,590 --> 00:29:25,220
methodologies that we can use to detect

827
00:29:23,419 --> 00:29:26,960
defects let's actually talk about some

828
00:29:25,220 --> 00:29:29,330
of the actual implementations that have

829
00:29:26,960 --> 00:29:30,440
been published so for this section I'm

830
00:29:29,330 --> 00:29:32,749
going to assume you at least have a

831
00:29:30,440 --> 00:29:34,419
basic understanding of computer vision

832
00:29:32,749 --> 00:29:36,619
techniques as well as neural networks

833
00:29:34,419 --> 00:29:38,240
because I'm gonna have to talk to some

834
00:29:36,619 --> 00:29:40,699
of this stuff at a high level but I will

835
00:29:38,240 --> 00:29:41,749
have to get into some of the details the

836
00:29:40,700 --> 00:29:44,090
other thing I want to note is that

837
00:29:41,749 --> 00:29:45,980
published research on detecting defects

838
00:29:44,090 --> 00:29:46,730
didn't start coming out until the summer

839
00:29:45,980 --> 00:29:49,610
of 2018

840
00:29:46,730 --> 00:29:54,139
so this is very much a new and ongoing

841
00:29:49,610 --> 00:29:55,340
area of research research today so one

842
00:29:54,139 --> 00:29:57,168
of the methods you've probably heard

843
00:29:55,340 --> 00:29:58,340
about system aided round in the news

844
00:29:57,169 --> 00:30:00,649
because it was one of the first ways of

845
00:29:58,340 --> 00:30:03,559
detecting deep fakes is by measuring how

846
00:30:00,649 --> 00:30:05,449
many times someone blinks in a video so

847
00:30:03,559 --> 00:30:07,070
the reason that I blinking works that

848
00:30:05,450 --> 00:30:09,919
this is obviously a physiological trait

849
00:30:07,070 --> 00:30:11,990
all humans do it and it's involuntary at

850
00:30:09,919 --> 00:30:14,389
a certain frequency so these researchers

851
00:30:11,990 --> 00:30:16,549
recognize that and they also notice that

852
00:30:14,389 --> 00:30:19,039
within deep fake videos the target was

853
00:30:16,549 --> 00:30:21,168
not blinking so what they did is they

854
00:30:19,039 --> 00:30:23,419
trained a neural net renewal network

855
00:30:21,169 --> 00:30:24,950
image classifier to simply detect

856
00:30:23,419 --> 00:30:28,159
whether someone whether an individual's

857
00:30:24,950 --> 00:30:30,440
eyes are open or closed then ran this

858
00:30:28,159 --> 00:30:31,549
over a series of video frames and -

859
00:30:30,440 --> 00:30:34,220
based on the number of detections

860
00:30:31,549 --> 00:30:35,899
positive detections they had for the

861
00:30:34,220 --> 00:30:37,159
targets eyes being closed he could then

862
00:30:35,899 --> 00:30:39,949
determine whether or not the video was a

863
00:30:37,159 --> 00:30:42,499
deep baked or not this was a great

864
00:30:39,950 --> 00:30:44,059
method for about two weeks until one of

865
00:30:42,499 --> 00:30:46,340
the offensive researchers figured out

866
00:30:44,059 --> 00:30:47,928
that the issue that would correct this

867
00:30:46,340 --> 00:30:49,459
was to include more images of your

868
00:30:47,929 --> 00:30:51,830
source with their eyes closed in your

869
00:30:49,460 --> 00:30:55,610
training data set for the deep fake

870
00:30:51,830 --> 00:30:57,289
algorithm so unfortunately this is no

871
00:30:55,610 --> 00:31:01,399
longer a viable method of detecting deep

872
00:30:57,289 --> 00:31:03,019
fakes a signal base level is using the

873
00:31:01,399 --> 00:31:05,479
photo response non-immune for MIDI

874
00:31:03,019 --> 00:31:07,309
pattern the peer or the P are new

875
00:31:05,480 --> 00:31:10,100
patterns the P R new pattern is

876
00:31:07,309 --> 00:31:12,049
very interesting that it is a digital

877
00:31:10,100 --> 00:31:13,850
fingerprint for a camera because it's

878
00:31:12,049 --> 00:31:18,679
based on factory defects on the light

879
00:31:13,850 --> 00:31:20,719
sensors for cameras so this is actually

880
00:31:18,679 --> 00:31:22,460
a technique that was originally used for

881
00:31:20,720 --> 00:31:23,929
traditional image forensics that has

882
00:31:22,460 --> 00:31:27,110
since been kind of ported over to be

883
00:31:23,929 --> 00:31:28,610
used for detecting defects in this case

884
00:31:27,110 --> 00:31:30,500
what these researchers did is they

885
00:31:28,610 --> 00:31:32,299
extracted a bunch of faces out of the

886
00:31:30,500 --> 00:31:35,450
series of video frames and then split

887
00:31:32,299 --> 00:31:37,010
those into eight uniform groups for each

888
00:31:35,450 --> 00:31:39,080
of those eight uniform groups they then

889
00:31:37,010 --> 00:31:41,360
calculated the average PNU pattern and

890
00:31:39,080 --> 00:31:43,189
then took the mean normalized

891
00:31:41,360 --> 00:31:45,139
cross-correlation score between each of

892
00:31:43,190 --> 00:31:46,700
those eight groups and those are the

893
00:31:45,140 --> 00:31:49,640
results that check to see plotted here

894
00:31:46,700 --> 00:31:52,400
on the right hand part of the slide so

895
00:31:49,640 --> 00:31:53,720
as you can see for real videos there's a

896
00:31:52,400 --> 00:31:55,400
very good chance that there's going to

897
00:31:53,720 --> 00:31:56,960
be a high correlation in the PR new

898
00:31:55,400 --> 00:31:58,850
patterns across those groups which you

899
00:31:56,960 --> 00:32:00,049
would expect because most likely that

900
00:31:58,850 --> 00:32:02,240
video has been taken with the same

901
00:32:00,049 --> 00:32:04,250
camera for deep fakes on the other hand

902
00:32:02,240 --> 00:32:06,500
since we are using images from other

903
00:32:04,250 --> 00:32:08,600
sources of their cameras that P a new

904
00:32:06,500 --> 00:32:11,929
pattern does get disrupted you can in DC

905
00:32:08,600 --> 00:32:13,969
that is the case so the one problem our

906
00:32:11,929 --> 00:32:16,130
downside to say with using the PR new

907
00:32:13,970 --> 00:32:18,080
pattern is that often times videos are

908
00:32:16,130 --> 00:32:20,690
shot from multiple from multiple cameras

909
00:32:18,080 --> 00:32:22,850
and from multiple angles so this method

910
00:32:20,690 --> 00:32:26,650
will throw a lot of false positives if

911
00:32:22,850 --> 00:32:26,649
you do run across videos of that sort

912
00:32:26,890 --> 00:32:32,240
some of the other methods to detecting

913
00:32:29,480 --> 00:32:34,309
defects involve inventing novel neural

914
00:32:32,240 --> 00:32:36,770
network architectures one of the more

915
00:32:34,309 --> 00:32:39,289
interesting ones is called mezzo net the

916
00:32:36,770 --> 00:32:41,090
basic premise behind mezzo net is that

917
00:32:39,289 --> 00:32:42,520
at a microscopic approach which is the

918
00:32:41,090 --> 00:32:44,658
level that most of these complex

919
00:32:42,520 --> 00:32:47,809
convolutional neural networks operate at

920
00:32:44,659 --> 00:32:48,950
you lose too much information so because

921
00:32:47,809 --> 00:32:50,600
of that you're unable to really

922
00:32:48,950 --> 00:32:53,299
determine whether or not a video is a

923
00:32:50,600 --> 00:32:55,428
deep fake at the same time at a high

924
00:32:53,299 --> 00:32:58,158
level at the macroscopic level which is

925
00:32:55,429 --> 00:32:59,659
the level that humans operate at we have

926
00:32:58,159 --> 00:33:01,220
issues detecting forgeries there's

927
00:32:59,659 --> 00:33:03,020
really too much information you don't

928
00:33:01,220 --> 00:33:04,419
know really what to focus on to detect

929
00:33:03,020 --> 00:33:07,070
whether or not something is a deep fake

930
00:33:04,419 --> 00:33:09,320
so the hypothesis that these researchers

931
00:33:07,070 --> 00:33:11,120
had is that if we operate at an

932
00:33:09,320 --> 00:33:13,039
intermediate level at the mesoscopic

933
00:33:11,120 --> 00:33:14,479
level then we should develop a model

934
00:33:13,039 --> 00:33:17,809
that has the best chance of detecting

935
00:33:14,480 --> 00:33:19,940
defects the way that the authors ended

936
00:33:17,809 --> 00:33:20,200
up building out this model was to start

937
00:33:19,940 --> 00:33:21,610
with

938
00:33:20,200 --> 00:33:24,340
very complex convolutional neural

939
00:33:21,610 --> 00:33:26,918
network and bit by bit removed layers as

940
00:33:24,340 --> 00:33:28,750
they continue to move layers they would

941
00:33:26,919 --> 00:33:31,210
again retrain that algorithm and see how

942
00:33:28,750 --> 00:33:32,710
well its predictability was and there

943
00:33:31,210 --> 00:33:34,600
was no loss of predictive ability they

944
00:33:32,710 --> 00:33:36,639
continued doing that what they ended up

945
00:33:34,600 --> 00:33:38,889
with was a convolutional neural network

946
00:33:36,639 --> 00:33:42,610
that had four convolutional layers and

947
00:33:38,889 --> 00:33:43,809
then two dense layers on top of it one

948
00:33:42,610 --> 00:33:45,070
of the things that's at least worth

949
00:33:43,809 --> 00:33:46,928
mentioning but I'm not going to go into

950
00:33:45,070 --> 00:33:48,549
too much detail on is that some

951
00:33:46,929 --> 00:33:50,950
researchers have also implemented a

952
00:33:48,549 --> 00:33:53,710
capsule network so a capsule network

953
00:33:50,950 --> 00:33:57,580
were an old idea first proposed like in

954
00:33:53,710 --> 00:33:59,289
the early 2010's but were later kind of

955
00:33:57,580 --> 00:34:01,299
put put to the side in favor of

956
00:33:59,289 --> 00:34:03,010
convolutional neural networks so in this

957
00:34:01,299 --> 00:34:04,779
case these richer's researchers actually

958
00:34:03,010 --> 00:34:06,549
brought back these capsule networks and

959
00:34:04,779 --> 00:34:12,010
I've used it to successfully detect

960
00:34:06,549 --> 00:34:14,379
defects so recurrent neural networks in

961
00:34:12,010 --> 00:34:16,300
particular are actually well suited to

962
00:34:14,379 --> 00:34:18,339
detecting defects and the main reason

963
00:34:16,300 --> 00:34:19,810
for that is that with recurrent neural

964
00:34:18,339 --> 00:34:21,820
networks we can actually capture

965
00:34:19,810 --> 00:34:24,699
temporal information that we can capture

966
00:34:21,820 --> 00:34:26,619
with convolutional neural networks so in

967
00:34:24,699 --> 00:34:28,239
this case what we're really looking to

968
00:34:26,619 --> 00:34:31,329
do with the recurrent neural network is

969
00:34:28,239 --> 00:34:33,848
to exploit the fact that these deep fake

970
00:34:31,329 --> 00:34:35,470
videos can contain temporal

971
00:34:33,849 --> 00:34:37,450
inconsistencies especially if you look

972
00:34:35,469 --> 00:34:39,759
at deep breaks fame by frame there's

973
00:34:37,449 --> 00:34:41,408
notable issues when you go frame by

974
00:34:39,760 --> 00:34:43,149
frame for example the face won't be the

975
00:34:41,409 --> 00:34:44,849
same frame by frame bloody conditions

976
00:34:43,149 --> 00:34:46,810
may change or shadows may be off a

977
00:34:44,849 --> 00:34:48,909
convolutional neural network isn't going

978
00:34:46,810 --> 00:34:52,569
to catch that but theoretically RNN

979
00:34:48,909 --> 00:34:55,599
should so this really why does this work

980
00:34:52,569 --> 00:34:58,300
as my kind of explained the auto encoder

981
00:34:55,599 --> 00:35:00,099
works frame by frame and what this means

982
00:34:58,300 --> 00:35:02,260
is that it has no sense of temporal

983
00:35:00,099 --> 00:35:04,630
awareness so it's not aware of previous

984
00:35:02,260 --> 00:35:06,010
spaces that it has generated so because

985
00:35:04,630 --> 00:35:07,660
it's unaware of these previous spaces

986
00:35:06,010 --> 00:35:09,220
that it's generated there is no

987
00:35:07,660 --> 00:35:10,569
connection between those spaces as you

988
00:35:09,220 --> 00:35:12,220
look by at frame by frame and that's

989
00:35:10,569 --> 00:35:15,180
really what we're hoping that these

990
00:35:12,220 --> 00:35:17,560
recurrent neural networks pick up on

991
00:35:15,180 --> 00:35:20,379
this idea rather than just feeding

992
00:35:17,560 --> 00:35:22,509
frames into the recurrent neural network

993
00:35:20,380 --> 00:35:24,369
another way this has been extended is by

994
00:35:22,510 --> 00:35:26,319
using traditional image forgery

995
00:35:24,369 --> 00:35:28,630
detection tools and then taking the

996
00:35:26,319 --> 00:35:31,210
outputs of those and feeding that stream

997
00:35:28,630 --> 00:35:32,870
of information into the RNN now hoping

998
00:35:31,210 --> 00:35:34,640
that Darnell learns that

999
00:35:32,870 --> 00:35:36,650
there's that temporal forensic

1000
00:35:34,640 --> 00:35:38,120
information contains inconsistencies and

1001
00:35:36,650 --> 00:35:43,250
can then detect whether or not a video

1002
00:35:38,120 --> 00:35:44,960
is a deep fake one of the more at least

1003
00:35:43,250 --> 00:35:47,060
in my opinion like clever methods for

1004
00:35:44,960 --> 00:35:50,330
detecting be fakes involves looking at

1005
00:35:47,060 --> 00:35:52,130
the orientation of your face so what

1006
00:35:50,330 --> 00:35:54,080
these researchers noticed was that when

1007
00:35:52,130 --> 00:35:56,660
you do the face replacement for a deep

1008
00:35:54,080 --> 00:35:58,130
fake you tend to disrupt the orientation

1009
00:35:56,660 --> 00:35:59,990
that someone's face has because those

1010
00:35:58,130 --> 00:36:02,140
central facial landmarks no no longer

1011
00:35:59,990 --> 00:36:05,990
really line up with the rest of the face

1012
00:36:02,140 --> 00:36:07,970
so what they did is they estimated your

1013
00:36:05,990 --> 00:36:09,410
facial orientation by using just your

1014
00:36:07,970 --> 00:36:11,779
central face landmarks so that's your

1015
00:36:09,410 --> 00:36:13,640
eyes you know is in your mouth they then

1016
00:36:11,780 --> 00:36:15,020
using all the information from your face

1017
00:36:13,640 --> 00:36:17,029
so then including your forehead your

1018
00:36:15,020 --> 00:36:18,620
cheekbones and your chin again take

1019
00:36:17,030 --> 00:36:20,900
another estimate of your facial

1020
00:36:18,620 --> 00:36:22,910
orientation if both of those estimates

1021
00:36:20,900 --> 00:36:24,350
are fairly similar to each other

1022
00:36:22,910 --> 00:36:27,170
then there's then it's more than likely

1023
00:36:24,350 --> 00:36:29,330
that video is real if those estimates

1024
00:36:27,170 --> 00:36:35,320
are significantly off then more than

1025
00:36:29,330 --> 00:36:37,670
likely we're looking at a deep fake so

1026
00:36:35,320 --> 00:36:39,470
two more additional light kind of signal

1027
00:36:37,670 --> 00:36:40,700
based methods involve looking for

1028
00:36:39,470 --> 00:36:42,319
artifacts left behind by the

1029
00:36:40,700 --> 00:36:45,319
auto-encoder as part of the deep fake

1030
00:36:42,320 --> 00:36:46,610
generation process so one of the one of

1031
00:36:45,320 --> 00:36:49,160
the type of artifact that's left behind

1032
00:36:46,610 --> 00:36:51,380
are the face warping artifacts so this

1033
00:36:49,160 --> 00:36:52,970
might kind of talked about as you map

1034
00:36:51,380 --> 00:36:54,590
that face from the source onto the

1035
00:36:52,970 --> 00:36:56,180
target you're going to have to scale the

1036
00:36:54,590 --> 00:36:57,530
face is going to have it's gonna have to

1037
00:36:56,180 --> 00:36:59,359
you're going to have to rotate it

1038
00:36:57,530 --> 00:37:01,100
usually there may be some sharing

1039
00:36:59,360 --> 00:37:03,290
cropping and so on involved and all

1040
00:37:01,100 --> 00:37:05,509
these things are called a a they find

1041
00:37:03,290 --> 00:37:07,250
transformations so when you go through

1042
00:37:05,510 --> 00:37:08,750
these a find transformations we should

1043
00:37:07,250 --> 00:37:11,030
be able to find pixel level information

1044
00:37:08,750 --> 00:37:14,540
that indeed those transformations were

1045
00:37:11,030 --> 00:37:18,020
done on this image another way that we

1046
00:37:14,540 --> 00:37:19,250
can also detect defects is by looking at

1047
00:37:18,020 --> 00:37:21,560
what's actually happening internally

1048
00:37:19,250 --> 00:37:24,110
within the gann as part of creating a

1049
00:37:21,560 --> 00:37:26,029
deep fake within the game the actual

1050
00:37:24,110 --> 00:37:27,710
weights are constrained and this is to

1051
00:37:26,030 --> 00:37:30,230
help prevent pixel values from blowing

1052
00:37:27,710 --> 00:37:32,840
up however because that Gann is

1053
00:37:30,230 --> 00:37:34,970
constrained what that results is that

1054
00:37:32,840 --> 00:37:36,590
you have a limited frequency of pixel

1055
00:37:34,970 --> 00:37:39,049
values that can be generated from the

1056
00:37:36,590 --> 00:37:40,820
Gann and while this may look okay to us

1057
00:37:39,050 --> 00:37:42,350
as humans at a pixel level it doesn't

1058
00:37:40,820 --> 00:37:44,030
make a lot of sense because you don't

1059
00:37:42,350 --> 00:37:46,370
expect to see for example like a uniform

1060
00:37:44,030 --> 00:37:49,310
distribution of pixel values within

1061
00:37:46,370 --> 00:37:51,049
that does make a lot of sense so by

1062
00:37:49,310 --> 00:37:52,759
measuring these pickle frequencies and

1063
00:37:51,050 --> 00:37:55,250
pixel saturations were able to get an

1064
00:37:52,760 --> 00:37:57,500
idea of whether or not the video is a

1065
00:37:55,250 --> 00:37:59,810
deep fake and the whole reason that

1066
00:37:57,500 --> 00:38:02,000
these methods still work while the deep

1067
00:37:59,810 --> 00:38:03,770
fake looks legitimate is that looking

1068
00:38:02,000 --> 00:38:10,450
correct is not the same thing as being

1069
00:38:03,770 --> 00:38:12,230
pixel wise correct so there's a lot of

1070
00:38:10,450 --> 00:38:13,370
information on this slide but there's

1071
00:38:12,230 --> 00:38:16,160
two things that I really want to mention

1072
00:38:13,370 --> 00:38:18,140
here one is that these detection methods

1073
00:38:16,160 --> 00:38:19,759
that I've gone through the vast majority

1074
00:38:18,140 --> 00:38:23,089
of them are fairly accurate when you

1075
00:38:19,760 --> 00:38:24,350
have access to the raw video so we're

1076
00:38:23,090 --> 00:38:26,840
talking about video that has not been

1077
00:38:24,350 --> 00:38:27,950
compressed however in reality more than

1078
00:38:26,840 --> 00:38:31,190
likely you're going to be dealing with

1079
00:38:27,950 --> 00:38:33,230
compressed compressed videos in order to

1080
00:38:31,190 --> 00:38:35,510
ship videos around on the internet by

1081
00:38:33,230 --> 00:38:37,370
default sites like YouTube or Vimeo or

1082
00:38:35,510 --> 00:38:39,380
Facebook have to compress that video and

1083
00:38:37,370 --> 00:38:40,640
there's varying levels of compression

1084
00:38:39,380 --> 00:38:42,370
that we can talk about there's high

1085
00:38:40,640 --> 00:38:45,740
quality video compression and low vol

1086
00:38:42,370 --> 00:38:47,359
low quality video compression but one of

1087
00:38:45,740 --> 00:38:49,189
the key takeaways I want you to take

1088
00:38:47,360 --> 00:38:52,010
away from this is that as the video

1089
00:38:49,190 --> 00:38:53,450
quality degrades the predictive ability

1090
00:38:52,010 --> 00:38:58,940
of our models to actually turn off the

1091
00:38:53,450 --> 00:39:01,040
video as a deep fake also degrades all

1092
00:38:58,940 --> 00:39:04,220
the methods that I've talked to you so

1093
00:39:01,040 --> 00:39:06,230
far our journalize detection methods but

1094
00:39:04,220 --> 00:39:08,140
we can also tailor detection methods to

1095
00:39:06,230 --> 00:39:11,030
work for an individual for example like

1096
00:39:08,140 --> 00:39:13,279
high-profile public figures so this is

1097
00:39:11,030 --> 00:39:15,740
one of those methods so in this

1098
00:39:13,280 --> 00:39:17,390
researchers notice that everybody has

1099
00:39:15,740 --> 00:39:19,850
distinct facial in the head movements

1100
00:39:17,390 --> 00:39:21,350
and when you create that deep fake as

1101
00:39:19,850 --> 00:39:24,980
part of that creation process you

1102
00:39:21,350 --> 00:39:26,509
interrupt those unique head movements so

1103
00:39:24,980 --> 00:39:28,520
what they did is they extracted

1104
00:39:26,510 --> 00:39:29,900
ten-second clips from a video and that

1105
00:39:28,520 --> 00:39:33,290
ended up creating a feature vector

1106
00:39:29,900 --> 00:39:35,240
containing 190 individual features they

1107
00:39:33,290 --> 00:39:37,340
then used a sliding window approach of

1108
00:39:35,240 --> 00:39:38,839
five seconds and then gathered up a

1109
00:39:37,340 --> 00:39:42,110
number of these feature vectors and fed

1110
00:39:38,840 --> 00:39:43,400
those into a support vector machine the

1111
00:39:42,110 --> 00:39:45,680
graphic that you can actually see on the

1112
00:39:43,400 --> 00:39:48,590
right is those feature vectors mapped

1113
00:39:45,680 --> 00:39:50,569
into two-dimensional space what I really

1114
00:39:48,590 --> 00:39:53,420
want to draw your attention to are those

1115
00:39:50,570 --> 00:39:55,070
gray dots and on the bottom right part

1116
00:39:53,420 --> 00:39:57,770
of that graphic those gray dots

1117
00:39:55,070 --> 00:40:00,180
represent President Obama the real

1118
00:39:57,770 --> 00:40:01,619
President Obama

1119
00:40:00,180 --> 00:40:03,990
look at those black dots that are

1120
00:40:01,619 --> 00:40:06,420
clustering kind of to the right of those

1121
00:40:03,990 --> 00:40:08,578
gray dots that is actually ad fake of

1122
00:40:06,420 --> 00:40:11,430
President Obama so you can see that

1123
00:40:08,579 --> 00:40:13,559
there is there is some kind of method to

1124
00:40:11,430 --> 00:40:15,180
this madness where you can actually tell

1125
00:40:13,559 --> 00:40:18,119
that those unique facial features that

1126
00:40:15,180 --> 00:40:20,040
indicate your head pose are actually

1127
00:40:18,119 --> 00:40:21,960
uniquely identifying individuals and

1128
00:40:20,040 --> 00:40:24,779
they do get disrupted when you create a

1129
00:40:21,960 --> 00:40:26,339
defect that individual another thing

1130
00:40:24,780 --> 00:40:29,160
that's really nice about this method is

1131
00:40:26,339 --> 00:40:30,900
that you only require real videos of an

1132
00:40:29,160 --> 00:40:31,950
individual in order to train it unlike

1133
00:40:30,900 --> 00:40:33,150
all the other methods where you have to

1134
00:40:31,950 --> 00:40:37,919
go out and actually collect deep fakes

1135
00:40:33,150 --> 00:40:41,160
or create them with that said there's

1136
00:40:37,920 --> 00:40:42,809
still a number of open detection issues

1137
00:40:41,160 --> 00:40:44,250
that have not been resolved yet I

1138
00:40:42,809 --> 00:40:46,140
already talked to kind of the video

1139
00:40:44,250 --> 00:40:48,180
compression resolution one but as that

1140
00:40:46,140 --> 00:40:50,089
video quality degrades so does the

1141
00:40:48,180 --> 00:40:52,589
predictability of all of these models

1142
00:40:50,089 --> 00:40:54,000
another just issue is there's been a lot

1143
00:40:52,589 --> 00:40:57,150
of improvement in deep fake generation

1144
00:40:54,000 --> 00:40:58,589
capabilities so it's a lot of the these

1145
00:40:57,150 --> 00:41:01,410
extra capabilities are still playing

1146
00:40:58,589 --> 00:41:02,700
catch-up another issue and Mike somewhat

1147
00:41:01,410 --> 00:41:04,129
talked to this but there's three

1148
00:41:02,700 --> 00:41:06,180
different ways to create deep fakes

1149
00:41:04,130 --> 00:41:07,799
through the use of puppeteering lip

1150
00:41:06,180 --> 00:41:10,078
synching or more commonly what you see

1151
00:41:07,799 --> 00:41:12,390
face swapping none of these models are

1152
00:41:10,079 --> 00:41:14,760
able to detect cecily detect all three

1153
00:41:12,390 --> 00:41:16,890
types of beat bakes so this is still an

1154
00:41:14,760 --> 00:41:19,290
outstanding issue another issue and I

1155
00:41:16,890 --> 00:41:21,058
call this selective deep baking could be

1156
00:41:19,290 --> 00:41:22,650
one of two things one is where only

1157
00:41:21,059 --> 00:41:24,599
small parts of the video have been deep

1158
00:41:22,650 --> 00:41:26,369
baked so if the five-minute video maybe

1159
00:41:24,599 --> 00:41:28,410
only 30 seconds of it have to be baked

1160
00:41:26,369 --> 00:41:30,240
and other issues where you have multiple

1161
00:41:28,410 --> 00:41:32,009
people in the video we're only one

1162
00:41:30,240 --> 00:41:33,899
individual or maybe a couple individuals

1163
00:41:32,010 --> 00:41:40,109
have been deep baked and everybody else

1164
00:41:33,900 --> 00:41:41,880
still looks real so we've actually been

1165
00:41:40,109 --> 00:41:43,828
looking in the defects now for a while

1166
00:41:41,880 --> 00:41:45,420
one of the things I wanted to share was

1167
00:41:43,829 --> 00:41:47,640
some of the research that we've done

1168
00:41:45,420 --> 00:41:49,319
into developing a single model that

1169
00:41:47,640 --> 00:41:53,460
actually can detect all three types of

1170
00:41:49,319 --> 00:41:55,440
defects so I actually call this model

1171
00:41:53,460 --> 00:41:57,750
mouth net and you'll find out why here

1172
00:41:55,440 --> 00:41:59,280
in a little bit but it's image

1173
00:41:57,750 --> 00:42:01,920
classification algorithm the model is

1174
00:41:59,280 --> 00:42:03,630
very basic for the backbone it's using

1175
00:42:01,920 --> 00:42:06,089
the latest version of the exception

1176
00:42:03,630 --> 00:42:08,089
ResNet model and then we have added two

1177
00:42:06,089 --> 00:42:10,828
additional dense layers on top of that

1178
00:42:08,089 --> 00:42:12,750
so the reason I call this mouth net is

1179
00:42:10,829 --> 00:42:13,500
because unlike all the previous

1180
00:42:12,750 --> 00:42:14,820
detection method

1181
00:42:13,500 --> 00:42:17,430
and what's been published is everyone's

1182
00:42:14,820 --> 00:42:19,680
looking at the entire face for this

1183
00:42:17,430 --> 00:42:21,930
model we only look at the mouth so what

1184
00:42:19,680 --> 00:42:23,640
we do is to crop out square sections of

1185
00:42:21,930 --> 00:42:25,319
us with the mouth and the center of

1186
00:42:23,640 --> 00:42:28,350
those and feed those as input into our

1187
00:42:25,320 --> 00:42:31,080
model so the reason I expect this to

1188
00:42:28,350 --> 00:42:33,210
catch all three types of defects is that

1189
00:42:31,080 --> 00:42:35,370
by definition all three types of these

1190
00:42:33,210 --> 00:42:37,980
defect methods have to alter the mouth

1191
00:42:35,370 --> 00:42:39,569
kind of the lowest common denominator in

1192
00:42:37,980 --> 00:42:41,340
terms of what gets altered within the

1193
00:42:39,570 --> 00:42:42,630
video is the mouth like you may not

1194
00:42:41,340 --> 00:42:44,610
touch the eyes you may not touch the

1195
00:42:42,630 --> 00:42:45,960
mouth the nose but you have to touch

1196
00:42:44,610 --> 00:42:49,470
them out at the end of the day for it to

1197
00:42:45,960 --> 00:42:52,530
be a deep fake one thing I want to note

1198
00:42:49,470 --> 00:42:53,939
is that we did somewhat limit our data

1199
00:42:52,530 --> 00:42:56,010
set we didn't grab anything from the

1200
00:42:53,940 --> 00:42:58,080
Deep Web we kept everything pretty much

1201
00:42:56,010 --> 00:43:00,650
safer work because this is a data set

1202
00:42:58,080 --> 00:43:02,490
that we are going to be releasing later

1203
00:43:00,650 --> 00:43:04,980
the other thing I want to note is that

1204
00:43:02,490 --> 00:43:07,799
we had about 200 videos that we use for

1205
00:43:04,980 --> 00:43:09,990
training and then we held aside videos

1206
00:43:07,800 --> 00:43:11,490
for both our validation and test sets so

1207
00:43:09,990 --> 00:43:14,669
when I report these numbers just keep in

1208
00:43:11,490 --> 00:43:17,189
mind that these are videos that the

1209
00:43:14,670 --> 00:43:19,320
model has not seen another thing I want

1210
00:43:17,190 --> 00:43:21,120
to note is that I do have examples of

1211
00:43:19,320 --> 00:43:22,920
all three types of deep baking methods

1212
00:43:21,120 --> 00:43:24,060
within this data set which is also

1213
00:43:22,920 --> 00:43:26,370
unique

1214
00:43:24,060 --> 00:43:29,160
while faced while face swapping methods

1215
00:43:26,370 --> 00:43:30,630
are largely our more are more

1216
00:43:29,160 --> 00:43:31,890
represented within the status that we do

1217
00:43:30,630 --> 00:43:34,770
have lip syncing and puppeteering

1218
00:43:31,890 --> 00:43:36,180
methods within it and the other thing is

1219
00:43:34,770 --> 00:43:38,490
that I did not make any kind of

1220
00:43:36,180 --> 00:43:40,350
distinction between video quality so we

1221
00:43:38,490 --> 00:43:44,700
have both high quality videos and low

1222
00:43:40,350 --> 00:43:45,960
quality values within this data set so

1223
00:43:44,700 --> 00:43:48,419
to give you a little bit more intuition

1224
00:43:45,960 --> 00:43:50,460
as to why this model should work these

1225
00:43:48,420 --> 00:43:52,260
are two pairs of images that I pulled

1226
00:43:50,460 --> 00:43:54,360
out from the same videos so this is

1227
00:43:52,260 --> 00:43:56,880
holding video quality constant you can

1228
00:43:54,360 --> 00:43:58,650
probably already visibly notice that the

1229
00:43:56,880 --> 00:44:01,320
deep fake mouths look substantially

1230
00:43:58,650 --> 00:44:02,580
different than the real mouth so this is

1231
00:44:01,320 --> 00:44:04,470
one thing that was hoping the model is

1232
00:44:02,580 --> 00:44:05,910
going to learn the other thing that was

1233
00:44:04,470 --> 00:44:07,890
hoping the model was going to learn is

1234
00:44:05,910 --> 00:44:10,470
that mouths have to go through that a

1235
00:44:07,890 --> 00:44:12,089
thin transformation process in order to

1236
00:44:10,470 --> 00:44:13,830
be successfully map from the source of

1237
00:44:12,090 --> 00:44:15,450
the target so I was hoping that the mall

1238
00:44:13,830 --> 00:44:17,160
so would also learn to pick up these

1239
00:44:15,450 --> 00:44:19,350
transformation artifacts that are left

1240
00:44:17,160 --> 00:44:26,190
behind especially around the mouth

1241
00:44:19,350 --> 00:44:27,480
region so these are the results in terms

1242
00:44:26,190 --> 00:44:29,130
of the actual image class

1243
00:44:27,480 --> 00:44:32,730
and problem our validation accurate

1244
00:44:29,130 --> 00:44:34,380
e-book accuracy was 83% however we did

1245
00:44:32,730 --> 00:44:36,510
implement mezzo net which is what I used

1246
00:44:34,380 --> 00:44:39,150
as our baseline model for all of our

1247
00:44:36,510 --> 00:44:41,820
data sets and it's validation accuracy

1248
00:44:39,150 --> 00:44:44,460
was 72% granted I used the faces for

1249
00:44:41,820 --> 00:44:46,830
mezzo net not the mouse so I do believe

1250
00:44:44,460 --> 00:44:49,140
that by using the mouse that actually

1251
00:44:46,830 --> 00:44:50,700
does help to one simplify the amount of

1252
00:44:49,140 --> 00:44:52,170
information that the model has to learn

1253
00:44:50,700 --> 00:44:54,629
and the features that it needs to know

1254
00:44:52,170 --> 00:44:57,359
to in order to make detection and the

1255
00:44:54,630 --> 00:44:59,250
other thing is that it is far I think

1256
00:44:57,359 --> 00:45:01,470
it's far easier for the model to segment

1257
00:44:59,250 --> 00:45:02,850
real mouths and fake Mouse rather than

1258
00:45:01,470 --> 00:45:04,319
trying to figure out hey there's three

1259
00:45:02,850 --> 00:45:05,580
different types of deep aching methods

1260
00:45:04,320 --> 00:45:07,230
and I need to actually learn that

1261
00:45:05,580 --> 00:45:11,069
there's these three types of phases

1262
00:45:07,230 --> 00:45:13,260
could be faked well the other issues

1263
00:45:11,070 --> 00:45:14,730
I'll point out is that actually both of

1264
00:45:13,260 --> 00:45:17,850
these models were overfitting on this

1265
00:45:14,730 --> 00:45:19,410
data set and I think that's for mainly

1266
00:45:17,850 --> 00:45:23,580
the reason that I did not have enough

1267
00:45:19,410 --> 00:45:25,950
puppeteering or lip synching videos

1268
00:45:23,580 --> 00:45:28,109
available just simply put like Face Swap

1269
00:45:25,950 --> 00:45:30,149
is there's tons of those videos that you

1270
00:45:28,109 --> 00:45:31,650
can easily get online but there's a lot

1271
00:45:30,150 --> 00:45:33,410
less that you can easily grab for

1272
00:45:31,650 --> 00:45:35,640
puppeteering and lip synching examples

1273
00:45:33,410 --> 00:45:37,410
so I think that if I were to go and grab

1274
00:45:35,640 --> 00:45:39,029
some more of those examples that help

1275
00:45:37,410 --> 00:45:42,118
address a lot of this overfitting

1276
00:45:39,030 --> 00:45:44,850
problem that you're seeing so in terms

1277
00:45:42,119 --> 00:45:46,680
of the test videos 100 test videos that

1278
00:45:44,850 --> 00:45:49,500
we held aside for a test set I use the

1279
00:45:46,680 --> 00:45:51,149
very simple detection method we would

1280
00:45:49,500 --> 00:45:52,440
just extract the first 100 frames that

1281
00:45:51,150 --> 00:45:54,300
we could find with faces

1282
00:45:52,440 --> 00:45:55,770
we then average the predictions of the

1283
00:45:54,300 --> 00:45:57,180
models and if that was greater than some

1284
00:45:55,770 --> 00:45:59,880
threshold and we considered that video

1285
00:45:57,180 --> 00:46:01,109
to be a deep pick so in this case the

1286
00:45:59,880 --> 00:46:04,740
threshold that I preferred for this

1287
00:46:01,109 --> 00:46:06,690
model was 0.9 so using that threshold we

1288
00:46:04,740 --> 00:46:09,990
found we were able to successfully

1289
00:46:06,690 --> 00:46:11,700
detect 41 percent of the deep fakes but

1290
00:46:09,990 --> 00:46:15,660
we did miss class by 10 percent of the

1291
00:46:11,700 --> 00:46:18,029
real videos one of the other things that

1292
00:46:15,660 --> 00:46:20,490
I looked into was the impact of image

1293
00:46:18,030 --> 00:46:22,230
size so there's been increasing evidence

1294
00:46:20,490 --> 00:46:23,549
that image size does matter when you

1295
00:46:22,230 --> 00:46:24,900
feed it into a convolutional neural

1296
00:46:23,550 --> 00:46:27,300
network and they aren't necessarily

1297
00:46:24,900 --> 00:46:29,250
scale invariant and that's kind of why

1298
00:46:27,300 --> 00:46:32,850
we have these ideas such as like

1299
00:46:29,250 --> 00:46:35,850
progressive resizing emerging so what I

1300
00:46:32,850 --> 00:46:37,859
did here or deceive the image size

1301
00:46:35,850 --> 00:46:39,900
actually did impact the model is I broke

1302
00:46:37,859 --> 00:46:40,859
these images into four different buckets

1303
00:46:39,900 --> 00:46:44,460
of berries

1304
00:46:40,859 --> 00:46:45,569
exercises I then trained four of the

1305
00:46:44,460 --> 00:46:47,369
mouth net models on each of these

1306
00:46:45,569 --> 00:46:49,499
buckets sizes and then again ran it

1307
00:46:47,369 --> 00:46:52,619
through the same test in this case I

1308
00:46:49,499 --> 00:46:55,249
used a threshold of 0.5 and we caught

1309
00:46:52,619 --> 00:46:58,710
53% of the defects however we did

1310
00:46:55,249 --> 00:47:01,828
misclassified 26% three old videos so we

1311
00:46:58,710 --> 00:47:03,690
did end up detecting more of the defects

1312
00:47:01,829 --> 00:47:06,660
but at the expense of also throwing more

1313
00:47:03,690 --> 00:47:08,430
false positives one of the things I also

1314
00:47:06,660 --> 00:47:10,799
want to point out is I did plot out the

1315
00:47:08,430 --> 00:47:13,950
matter model accuracies as a function of

1316
00:47:10,799 --> 00:47:15,690
the score threshold that we used and you

1317
00:47:13,950 --> 00:47:18,779
can see that our baseline model is still

1318
00:47:15,690 --> 00:47:20,489
far below the mouth net models so given

1319
00:47:18,779 --> 00:47:22,380
how difficult this data set is I'm

1320
00:47:20,489 --> 00:47:24,690
actually fairly happy happy with the

1321
00:47:22,380 --> 00:47:26,999
results that we got from this model and

1322
00:47:24,690 --> 00:47:28,440
I do believe that it would be a viable

1323
00:47:26,999 --> 00:47:33,509
model moving forward for detecting

1324
00:47:28,440 --> 00:47:36,269
defects another thing that I did is I

1325
00:47:33,509 --> 00:47:38,309
went through and looked at all the

1326
00:47:36,269 --> 00:47:40,288
videos that these models were missing on

1327
00:47:38,309 --> 00:47:42,809
and these are some of the kind of trends

1328
00:47:40,289 --> 00:47:44,309
that I saw one is just low resolution

1329
00:47:42,809 --> 00:47:47,339
videos and our eye view has spoken to

1330
00:47:44,309 --> 00:47:49,170
this but low res the low resolution

1331
00:47:47,339 --> 00:47:51,328
videos are causing serious problems for

1332
00:47:49,170 --> 00:47:52,950
all of these models so one of the things

1333
00:47:51,329 --> 00:47:54,539
I've started doing moving forward is

1334
00:47:52,950 --> 00:47:57,089
that not only do I grab the highest

1335
00:47:54,539 --> 00:47:59,549
quality video that I can get but also

1336
00:47:57,089 --> 00:48:01,319
grab the lowest quality to I'm hoping

1337
00:47:59,549 --> 00:48:02,549
that by augmenting my data set in that

1338
00:48:01,319 --> 00:48:05,549
way that will help improve some of the

1339
00:48:02,549 --> 00:48:07,440
accuracy there another interesting

1340
00:48:05,549 --> 00:48:09,799
source because we had a number of misses

1341
00:48:07,440 --> 00:48:12,569
here were movie trailers and baby videos

1342
00:48:09,799 --> 00:48:14,730
movie trailers I actually suspect that

1343
00:48:12,569 --> 00:48:16,319
these are digitally altered and the

1344
00:48:14,730 --> 00:48:19,079
model is actually picking up on that and

1345
00:48:16,319 --> 00:48:22,950
therefore misclassify movie trailers as

1346
00:48:19,079 --> 00:48:24,809
defects baby videos on the other hand I

1347
00:48:22,950 --> 00:48:26,129
think this is mainly just a training set

1348
00:48:24,809 --> 00:48:27,869
issue but it's so some of the issues

1349
00:48:26,130 --> 00:48:30,299
that you expect to see in the wild I

1350
00:48:27,869 --> 00:48:31,859
only included one baby video in the

1351
00:48:30,299 --> 00:48:34,140
training set but there were a number of

1352
00:48:31,859 --> 00:48:36,210
instances on the deep baked side where

1353
00:48:34,140 --> 00:48:38,368
baby faces have been deep baked on to

1354
00:48:36,210 --> 00:48:39,930
adult spaces so I think what the model

1355
00:48:38,369 --> 00:48:41,549
learned is that if it's all baby faced

1356
00:48:39,930 --> 00:48:42,598
most likely that was a deep fake and

1357
00:48:41,549 --> 00:48:45,900
those are the sources some of those

1358
00:48:42,599 --> 00:48:47,489
misclassifications as well I've also

1359
00:48:45,900 --> 00:48:49,499
spoken to this but with selective deep

1360
00:48:47,489 --> 00:48:51,299
baking multiple people within a deep bay

1361
00:48:49,499 --> 00:48:53,730
causes a lot of problems especially when

1362
00:48:51,299 --> 00:48:54,660
some of those are real faces I think the

1363
00:48:53,730 --> 00:48:56,610
easiest solution of

1364
00:48:54,660 --> 00:48:59,009
is actually two clustered faces based on

1365
00:48:56,610 --> 00:49:01,640
similarity and then run your predictions

1366
00:48:59,010 --> 00:49:04,590
over these clustered set of faces

1367
00:49:01,640 --> 00:49:06,660
another major issue and this is the same

1368
00:49:04,590 --> 00:49:09,270
for all these deep fake algorithms is

1369
00:49:06,660 --> 00:49:12,330
the facial extraction model that you're

1370
00:49:09,270 --> 00:49:14,820
using matters a lot we were using MT CN

1371
00:49:12,330 --> 00:49:16,170
n which is it's not a bad model but it's

1372
00:49:14,820 --> 00:49:18,780
definitely not state-of-the-art at this

1373
00:49:16,170 --> 00:49:20,790
point so we ended up picking up quite a

1374
00:49:18,780 --> 00:49:22,530
bit of noise in some cases like logos

1375
00:49:20,790 --> 00:49:24,600
are always being detected as faces and

1376
00:49:22,530 --> 00:49:25,740
there's other instances too or a need to

1377
00:49:24,600 --> 00:49:28,020
make sure that we eliminate background

1378
00:49:25,740 --> 00:49:30,000
faces so think by limiting some of these

1379
00:49:28,020 --> 00:49:34,040
sources of noise will help to improve

1380
00:49:30,000 --> 00:49:34,040
the accuracy of these detection models

1381
00:49:34,070 --> 00:49:40,020
so to wrap up what I want to do is just

1382
00:49:37,380 --> 00:49:42,270
briefly introduce deepstar which is a

1383
00:49:40,020 --> 00:49:45,330
the detection tool that Mike and I have

1384
00:49:42,270 --> 00:49:47,640
been working on so really both deepstar

1385
00:49:45,330 --> 00:49:50,160
because it is a huge pain to try to

1386
00:49:47,640 --> 00:49:51,960
build a deep fake data set there's no

1387
00:49:50,160 --> 00:49:53,040
tooling that's really available and you

1388
00:49:51,960 --> 00:49:55,260
have to go out and gather the videos

1389
00:49:53,040 --> 00:49:56,400
extract frames and then even after you

1390
00:49:55,260 --> 00:49:58,290
strike the frames you kind of need to

1391
00:49:56,400 --> 00:50:00,330
figure out what you want to look at

1392
00:49:58,290 --> 00:50:03,180
phases Mouse and so on

1393
00:50:00,330 --> 00:50:05,220
so this can be really helpful for

1394
00:50:03,180 --> 00:50:07,379
building out these data sets we've also

1395
00:50:05,220 --> 00:50:08,730
added some additional capabilities that

1396
00:50:07,380 --> 00:50:11,100
can help you test your detection

1397
00:50:08,730 --> 00:50:12,750
algorithms as well another thing that

1398
00:50:11,100 --> 00:50:14,400
we're doing is we're also releasing the

1399
00:50:12,750 --> 00:50:16,440
source code of both the models that I've

1400
00:50:14,400 --> 00:50:17,910
talked to muzzle net and mouth net so

1401
00:50:16,440 --> 00:50:19,860
you'll find the training code in this

1402
00:50:17,910 --> 00:50:21,810
open source repo as well you can go and

1403
00:50:19,860 --> 00:50:24,510
start training your own algorithms or

1404
00:50:21,810 --> 00:50:26,520
your own detectors so the source code

1405
00:50:24,510 --> 00:50:28,500
it'll be available under the zero Fox

1406
00:50:26,520 --> 00:50:30,620
open source organization under the name

1407
00:50:28,500 --> 00:50:34,560
deepstar and this will be available

1408
00:50:30,620 --> 00:50:36,509
probably by the end of this week so with

1409
00:50:34,560 --> 00:50:38,549
that we're almost out of time but I do

1410
00:50:36,510 --> 00:50:40,230
want to remind you that you can come up

1411
00:50:38,550 --> 00:50:42,870
to the speaker wrap-up room afterwards

1412
00:50:40,230 --> 00:50:43,940
and ask us questions so thank you very

1413
00:50:42,870 --> 00:50:48,900
much

1414
00:50:43,940 --> 00:50:48,900
[Applause]

