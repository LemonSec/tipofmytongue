1
00:00:00,179 --> 00:00:06,000
welcome to death to the IOC what's next

2
00:00:03,030 --> 00:00:09,269
in threat intelligence in South Seas a b

3
00:00:06,000 --> 00:00:12,780
e what Bhavna salman who is a security

4
00:00:09,269 --> 00:00:15,540
researcher at Microsoft before we begin

5
00:00:12,780 --> 00:00:18,000
a few brief notes stop by the business

6
00:00:15,540 --> 00:00:20,430
Hall located in Mandalay Bay Oceanside

7
00:00:18,000 --> 00:00:23,100
and shoreline ballrooms on level 2

8
00:00:20,430 --> 00:00:27,300
during the day and for the Welcome

9
00:00:23,100 --> 00:00:30,330
Reception at 5:30 p.m. this evening the

10
00:00:27,300 --> 00:00:34,260
blackhat Arsenal is in the business hall

11
00:00:30,330 --> 00:00:37,890
on level 2 a quick reminder that lunch

12
00:00:34,260 --> 00:00:41,339
will be in Bayside a B from 1 o'clock to

13
00:00:37,890 --> 00:00:43,710
2:30 and don't forget the merchandise

14
00:00:41,340 --> 00:00:45,719
store on level 2 and session recordings

15
00:00:43,710 --> 00:00:49,379
from source of knowledge they have a

16
00:00:45,719 --> 00:00:51,870
desk on every level also thank you for

17
00:00:49,379 --> 00:00:53,370
putting your phone on vibrate and makes

18
00:00:51,870 --> 00:00:54,930
it easier for the rest of us to ignore

19
00:00:53,370 --> 00:00:58,288
the ringing while we wait for your

20
00:00:54,930 --> 00:01:00,830
voicemail to pick it up right without

21
00:00:58,289 --> 00:01:08,819
any further ado our speaker Bob done

22
00:01:00,830 --> 00:01:08,819
[Applause]

23
00:01:15,530 --> 00:01:21,200
sweet thanks for the intro grant good

24
00:01:19,010 --> 00:01:22,880
morning everyone thank you so much for

25
00:01:21,200 --> 00:01:25,160
being here I know that this is the 9m

26
00:01:22,880 --> 00:01:27,530
large super super early hope everyone

27
00:01:25,160 --> 00:01:29,420
got their caffeine in we're gonna be

28
00:01:27,530 --> 00:01:32,840
talking about death to the IOC what's

29
00:01:29,420 --> 00:01:35,300
next and threat intelligence here's a

30
00:01:32,840 --> 00:01:38,930
little bit about me my name is Padma I

31
00:01:35,300 --> 00:01:40,850
work for Microsoft defender research in

32
00:01:38,930 --> 00:01:44,119
the past I used to work in threat

33
00:01:40,850 --> 00:01:46,610
intelligence and apt response and now I

34
00:01:44,119 --> 00:01:48,020
write models that classify malware in

35
00:01:46,610 --> 00:01:51,310
real-time so I work in machine learning

36
00:01:48,020 --> 00:01:54,320
and this project is kind of this

37
00:01:51,310 --> 00:01:56,600
amalgamation of these two disciplines

38
00:01:54,320 --> 00:02:00,320
that have worked in kind of straddles

39
00:01:56,600 --> 00:02:03,050
data science and threat intelligence so

40
00:02:00,320 --> 00:02:05,809
when I was a trade analyst one of the

41
00:02:03,050 --> 00:02:08,959
things that struck with me was how

42
00:02:05,810 --> 00:02:10,940
manual that job is a lot of people think

43
00:02:08,959 --> 00:02:12,860
of automated IRC feeds when they first

44
00:02:10,940 --> 00:02:14,660
think of threat intelligence but the job

45
00:02:12,860 --> 00:02:16,040
of a TI analyst is actually a lot more

46
00:02:14,660 --> 00:02:18,500
than that

47
00:02:16,040 --> 00:02:20,720
there's some forensics some reverse

48
00:02:18,500 --> 00:02:24,560
engineering and there's actually a lot

49
00:02:20,720 --> 00:02:26,209
of reading to understand the type of the

50
00:02:24,560 --> 00:02:28,580
threat actors that their organizations

51
00:02:26,209 --> 00:02:29,989
care about ti analysts go through a lot

52
00:02:28,580 --> 00:02:31,130
of literature you know whether it is

53
00:02:29,989 --> 00:02:32,840
something that they got from their

54
00:02:31,130 --> 00:02:35,570
information sharing organization that I

55
00:02:32,840 --> 00:02:37,820
sack their cert or just publicly

56
00:02:35,570 --> 00:02:39,109
available threat intelligence blogs they

57
00:02:37,820 --> 00:02:40,910
go through all of these documents to

58
00:02:39,110 --> 00:02:42,920
understand what are the vulnerabilities

59
00:02:40,910 --> 00:02:45,440
these actors are targeting or what are

60
00:02:42,920 --> 00:02:48,109
the legitimate binaries that they're

61
00:02:45,440 --> 00:02:49,640
abusing and all of this data exists in

62
00:02:48,110 --> 00:02:53,180
unstructured text right now which

63
00:02:49,640 --> 00:02:54,828
requires manual processing let me give

64
00:02:53,180 --> 00:02:57,620
you an example of the type of workflow

65
00:02:54,829 --> 00:02:59,480
that a TI analyst has so for example you

66
00:02:57,620 --> 00:03:01,459
are an analyst whose organization is

67
00:02:59,480 --> 00:03:03,470
worried about act attacks from an

68
00:03:01,459 --> 00:03:05,090
Eastern European country so you go out

69
00:03:03,470 --> 00:03:07,280
and start reading about attacks that

70
00:03:05,090 --> 00:03:10,400
have originated in that region so maybe

71
00:03:07,280 --> 00:03:12,140
you find these three actors apt 2829 and

72
00:03:10,400 --> 00:03:15,470
dragon fly and you start reading about

73
00:03:12,140 --> 00:03:17,920
them you see that 28 and 29 have been

74
00:03:15,470 --> 00:03:20,500
active longer since about 2000

75
00:03:17,920 --> 00:03:23,200
eight and then dragonfly is more recent

76
00:03:20,500 --> 00:03:25,390
about 2011 you see the dragonfly has

77
00:03:23,200 --> 00:03:28,540
more cyber-espionage targets whereas 28

78
00:03:25,390 --> 00:03:31,119
and 29 are more politically motivated

79
00:03:28,540 --> 00:03:33,130
organizations and then maybe you know 28

80
00:03:31,120 --> 00:03:35,080
has started using IOT attacks recently

81
00:03:33,130 --> 00:03:36,490
whatever so you get all of this

82
00:03:35,080 --> 00:03:39,130
information and then you try and

83
00:03:36,490 --> 00:03:41,410
organize this maybe you start with like

84
00:03:39,130 --> 00:03:44,290
a shoebox of these TTP's that you

85
00:03:41,410 --> 00:03:45,910
gathered you know the legitimate tools

86
00:03:44,290 --> 00:03:49,420
that they attack the malware families

87
00:03:45,910 --> 00:03:51,100
they use etc but then you need to

88
00:03:49,420 --> 00:03:53,350
organize it even more so that you can

89
00:03:51,100 --> 00:03:56,200
extract the insights that will help your

90
00:03:53,350 --> 00:03:58,180
organization make better decisions so

91
00:03:56,200 --> 00:04:02,049
maybe you represent it in a graph like

92
00:03:58,180 --> 00:04:04,269
this now you can clearly see the overlap

93
00:04:02,050 --> 00:04:05,980
between the TTP's that are used by these

94
00:04:04,269 --> 00:04:08,170
three thread actors that you care about

95
00:04:05,980 --> 00:04:10,390
in fact you can clearly see that there

96
00:04:08,170 --> 00:04:12,609
isn't very much overlap between them in

97
00:04:10,390 --> 00:04:14,859
fact a PD 28 has a very complicated

98
00:04:12,610 --> 00:04:18,280
networking infrastructure that's mostly

99
00:04:14,860 --> 00:04:20,530
absent from the other two so maybe then

100
00:04:18,279 --> 00:04:23,320
you try and lead even more context on

101
00:04:20,529 --> 00:04:26,969
this graph and maybe you overlay this

102
00:04:23,320 --> 00:04:29,200
with the prevalence of HTTP on its own

103
00:04:26,970 --> 00:04:31,030
so now this is what it looks like now

104
00:04:29,200 --> 00:04:32,950
the size of the bubble is proportional

105
00:04:31,030 --> 00:04:35,679
to the prevalence of that particular TTP

106
00:04:32,950 --> 00:04:38,380
in the ecosystem overall not just with

107
00:04:35,680 --> 00:04:40,900
these 3 AP T's so now with this type of

108
00:04:38,380 --> 00:04:44,169
a visual you can help your organization

109
00:04:40,900 --> 00:04:46,390
make some data bag decisions for example

110
00:04:44,169 --> 00:04:48,400
you might say that hey let's prioritize

111
00:04:46,390 --> 00:04:49,990
our defensive choke points on you know

112
00:04:48,400 --> 00:04:52,570
credential dumping or pass the hash

113
00:04:49,990 --> 00:04:54,669
attacks techniques which will help

114
00:04:52,570 --> 00:04:56,800
disrupt the actual toolchain of the

115
00:04:54,669 --> 00:04:58,479
attackers that we care about as well as

116
00:04:56,800 --> 00:05:00,640
a large percentage of the attacks out

117
00:04:58,479 --> 00:05:02,320
there in general and you know you might

118
00:05:00,640 --> 00:05:04,210
recommend that hey let's deeper ties

119
00:05:02,320 --> 00:05:06,219
things like time stamping and boot kids

120
00:05:04,210 --> 00:05:07,780
because they're really fancy but you

121
00:05:06,220 --> 00:05:11,080
won't have as much impact for the

122
00:05:07,780 --> 00:05:13,690
security of our organization so this is

123
00:05:11,080 --> 00:05:16,359
the type of insights that threat

124
00:05:13,690 --> 00:05:18,490
intelligence analysts are on the hook

125
00:05:16,360 --> 00:05:21,310
for for generating and right now they do

126
00:05:18,490 --> 00:05:23,530
this manually in fact there was a really

127
00:05:21,310 --> 00:05:25,960
good talk yesterday on the miter attack

128
00:05:23,530 --> 00:05:27,609
framework and shout out kiri Nichols and

129
00:05:25,960 --> 00:05:30,039
her team at mitre who do this awesome

130
00:05:27,610 --> 00:05:30,589
awesome work but they shared some

131
00:05:30,039 --> 00:05:32,779
example

132
00:05:30,589 --> 00:05:34,939
they're partners who are doing this type

133
00:05:32,779 --> 00:05:38,389
of analysis and right now all of this is

134
00:05:34,939 --> 00:05:40,129
done manually so can we do this with

135
00:05:38,389 --> 00:05:42,919
machine learning that's the question

136
00:05:40,129 --> 00:05:45,289
that we were exploring the architecture

137
00:05:42,919 --> 00:05:47,269
of a system like this would look roughly

138
00:05:45,289 --> 00:05:48,739
like this we would have our input which

139
00:05:47,269 --> 00:05:50,779
is you know the unstructured data the

140
00:05:48,739 --> 00:05:53,719
written material that the T analyst has

141
00:05:50,779 --> 00:05:55,219
access to and then also it would flow

142
00:05:53,719 --> 00:05:56,869
into this black box which would then

143
00:05:55,219 --> 00:05:58,789
extract the actor and it gives the

144
00:05:56,869 --> 00:06:01,519
entities and be able to build

145
00:05:58,789 --> 00:06:03,589
relationships between them and then the

146
00:06:01,519 --> 00:06:05,419
output would be the kind of insights

147
00:06:03,589 --> 00:06:08,629
that I just shared with you attacker

148
00:06:05,419 --> 00:06:10,818
graphs or time lines so this black box

149
00:06:08,629 --> 00:06:13,129
that's what we're going to focus on for

150
00:06:10,819 --> 00:06:15,379
this talk this is my agenda for the rest

151
00:06:13,129 --> 00:06:19,069
of the talk I will introduce the idea of

152
00:06:15,379 --> 00:06:20,989
named energy extraction I talked about

153
00:06:19,069 --> 00:06:22,729
the recipe for building a machine

154
00:06:20,989 --> 00:06:25,609
learning / deep learning based cyber

155
00:06:22,729 --> 00:06:27,829
energy extractor I chair the training

156
00:06:25,609 --> 00:06:29,599
data you can use freezer ization methods

157
00:06:27,829 --> 00:06:32,079
models and I'll share with you the

158
00:06:29,599 --> 00:06:34,308
assessments that we got from our data

159
00:06:32,079 --> 00:06:36,709
I'll give you a short demo of a tool

160
00:06:34,309 --> 00:06:38,209
we've created and I'll close with an

161
00:06:36,709 --> 00:06:43,159
example of how this can help drive

162
00:06:38,209 --> 00:06:49,009
impact in organizations okay so what is

163
00:06:43,159 --> 00:06:50,779
named energy extraction this is a wiki

164
00:06:49,009 --> 00:06:54,289
article about my favorite game of

165
00:06:50,779 --> 00:06:56,569
Thrones character if I pop the text from

166
00:06:54,289 --> 00:06:58,549
this article into any commercial

167
00:06:56,569 --> 00:07:00,469
off-the-shelf text analytics toolkit

168
00:06:58,549 --> 00:07:03,138
like the ones available by Azure or

169
00:07:00,469 --> 00:07:05,119
Google I can easily extract the entities

170
00:07:03,139 --> 00:07:06,349
that are present in this so the entities

171
00:07:05,119 --> 00:07:09,199
would look something like this

172
00:07:06,349 --> 00:07:12,979
Sansa Stark is a person Eddard Stark

173
00:07:09,199 --> 00:07:14,779
Catelyn also persons and Winterfell well

174
00:07:12,979 --> 00:07:16,159
the tool kits that I looked at they kept

175
00:07:14,779 --> 00:07:17,749
classifying Winterfell as an

176
00:07:16,159 --> 00:07:21,469
organization whereas we know it's a

177
00:07:17,749 --> 00:07:23,839
geopolitical entity but when I try to

178
00:07:21,469 --> 00:07:26,539
use those toolkits on an apt report like

179
00:07:23,839 --> 00:07:28,339
this they fail the types of entities

180
00:07:26,539 --> 00:07:30,409
that I would expect them to be able to

181
00:07:28,339 --> 00:07:32,089
extract look something like this so I

182
00:07:30,409 --> 00:07:33,739
would want them to be able to say that

183
00:07:32,089 --> 00:07:36,320
okay dropping elephant china strats and

184
00:07:33,739 --> 00:07:38,659
patchwork are the names of the actor

185
00:07:36,320 --> 00:07:40,700
spearfishing and watering hole are the

186
00:07:38,660 --> 00:07:43,340
attacks or the TTP's that they're using

187
00:07:40,700 --> 00:07:48,710
so this is the type of entity extraction

188
00:07:43,340 --> 00:07:50,330
that we want our algorithm to do okay so

189
00:07:48,710 --> 00:07:52,460
here we start with the recipe of

190
00:07:50,330 --> 00:07:55,870
training our own energy extractor

191
00:07:52,460 --> 00:07:59,359
starting with the training data first

192
00:07:55,870 --> 00:08:02,090
for this experiment we kept ourselves to

193
00:07:59,360 --> 00:08:03,680
a publicly available corpus so we used

194
00:08:02,090 --> 00:08:06,950
apt notes which is a github repository

195
00:08:03,680 --> 00:08:11,570
containing apt white papers going back

196
00:08:06,950 --> 00:08:13,099
to 2004 we also used a collection of

197
00:08:11,570 --> 00:08:16,159
threat intelligence blogs that were

198
00:08:13,100 --> 00:08:19,670
collected by RTI team between June 2018

199
00:08:16,160 --> 00:08:22,730
and Jan 2008 19 in all this data set had

200
00:08:19,670 --> 00:08:25,640
about 2,700 documents and these graphs

201
00:08:22,730 --> 00:08:27,170
kind of show you a distribution of some

202
00:08:25,640 --> 00:08:28,640
data analytics features about these

203
00:08:27,170 --> 00:08:31,340
documents so the top chart shows you

204
00:08:28,640 --> 00:08:34,520
that each document had on average 2,000

205
00:08:31,340 --> 00:08:36,260
tokens the median was more like 1400 and

206
00:08:34,520 --> 00:08:40,220
there were some on the tail end that had

207
00:08:36,260 --> 00:08:41,689
upwards of 70 to 80 thousand tokens the

208
00:08:40,220 --> 00:08:43,880
lower graph shows you the distribution

209
00:08:41,690 --> 00:08:45,530
of the tokens that we care about in this

210
00:08:43,880 --> 00:08:47,840
data set so the tokens that we care

211
00:08:45,530 --> 00:08:49,490
about are the tokens that indicate the

212
00:08:47,840 --> 00:08:51,920
name of a malware family or the name of

213
00:08:49,490 --> 00:08:55,700
an actor or the phrase that indicates a

214
00:08:51,920 --> 00:08:57,949
minor attack technique on average only

215
00:08:55,700 --> 00:09:00,740
1% of the tokens in our data set

216
00:08:57,950 --> 00:09:01,550
belonged to one of these classes so this

217
00:09:00,740 --> 00:09:04,370
tells us two things

218
00:09:01,550 --> 00:09:07,880
one is that we have a really small data

219
00:09:04,370 --> 00:09:09,260
set and we know from past experience and

220
00:09:07,880 --> 00:09:10,910
literature on deep learning that deep

221
00:09:09,260 --> 00:09:13,340
learning methods sometimes don't work as

222
00:09:10,910 --> 00:09:15,110
well on really small data sets so we'll

223
00:09:13,340 --> 00:09:16,430
be sure to try some statistical modeling

224
00:09:15,110 --> 00:09:18,740
methods in addition to the deep learning

225
00:09:16,430 --> 00:09:21,079
ones and the other thing is that we have

226
00:09:18,740 --> 00:09:22,730
a class imbalance problem so we'll have

227
00:09:21,080 --> 00:09:24,050
to take that into account in the models

228
00:09:22,730 --> 00:09:28,850
we choose as well as the assessments

229
00:09:24,050 --> 00:09:31,010
which we do to get our labels we used

230
00:09:28,850 --> 00:09:33,290
the carat data set also provided by

231
00:09:31,010 --> 00:09:35,060
mitre here I've included a screen shot

232
00:09:33,290 --> 00:09:36,980
of what is contained in the carat data

233
00:09:35,060 --> 00:09:39,349
set it's basically you can think of it

234
00:09:36,980 --> 00:09:40,700
as a list of all the apt attackers and

235
00:09:39,350 --> 00:09:44,000
the TTP's that they use

236
00:09:40,700 --> 00:09:46,030
we used this as a list of tags and we

237
00:09:44,000 --> 00:09:49,360
annotated our data from this list you

238
00:09:46,030 --> 00:09:52,510
longest extent pattern-matching so

239
00:09:49,360 --> 00:09:53,620
obviously this label data set is noisy

240
00:09:52,510 --> 00:09:55,450
in a few ways

241
00:09:53,620 --> 00:09:58,300
longest extend pattern matching is not

242
00:09:55,450 --> 00:10:00,790
perfect and also the carat data sets

243
00:09:58,300 --> 00:10:02,650
itself is incomplete so there are some

244
00:10:00,790 --> 00:10:04,449
actor names for instance which are not

245
00:10:02,650 --> 00:10:08,079
present in the data set so they got

246
00:10:04,450 --> 00:10:10,420
labeled incorrectly in our corpus but

247
00:10:08,080 --> 00:10:11,770
short of manual annotation that was the

248
00:10:10,420 --> 00:10:16,060
best we could do so that is the method

249
00:10:11,770 --> 00:10:18,670
that we went with I want to show you how

250
00:10:16,060 --> 00:10:21,069
the data gets labeled in this problem so

251
00:10:18,670 --> 00:10:22,900
we use what is called the IOB style

252
00:10:21,070 --> 00:10:25,900
which is very common in natural language

253
00:10:22,900 --> 00:10:28,480
processing tasks IOB stands for inside

254
00:10:25,900 --> 00:10:30,880
outside beginning and here you can see

255
00:10:28,480 --> 00:10:33,990
an example of how that annotation looks

256
00:10:30,880 --> 00:10:36,880
like so if I have a phrase Eddard Stark

257
00:10:33,990 --> 00:10:39,100
which which in all indicates a person

258
00:10:36,880 --> 00:10:41,890
the first token in that phrase will be

259
00:10:39,100 --> 00:10:43,870
tagged with a B - person for begining of

260
00:10:41,890 --> 00:10:45,760
person and then all the subsequent

261
00:10:43,870 --> 00:10:49,110
tokens in that phrase will be tagged

262
00:10:45,760 --> 00:10:51,550
with an eye - person so inside person

263
00:10:49,110 --> 00:10:54,190
here's an example of what that would

264
00:10:51,550 --> 00:10:55,959
look like Anna corpus from on a sentence

265
00:10:54,190 --> 00:10:59,950
from our corpus so this is a sentence

266
00:10:55,960 --> 00:11:01,810
about number Panda and if we tagged it

267
00:10:59,950 --> 00:11:04,990
in the IOB style this is what it would

268
00:11:01,810 --> 00:11:06,849
look like so numbered Panda is the

269
00:11:04,990 --> 00:11:08,530
phrase that indicates the bad actor and

270
00:11:06,850 --> 00:11:10,300
the first token in that phrase will be a

271
00:11:08,530 --> 00:11:16,720
B - bad actor and all the subsequent

272
00:11:10,300 --> 00:11:21,069
tokens will be I - bad actor coming to

273
00:11:16,720 --> 00:11:23,140
feature extraction so we use broadly two

274
00:11:21,070 --> 00:11:25,630
types of features that we experimented

275
00:11:23,140 --> 00:11:26,949
with we used the traditional features

276
00:11:25,630 --> 00:11:29,770
that are common to natural language

277
00:11:26,950 --> 00:11:33,130
processing tasks like lemma part of

278
00:11:29,770 --> 00:11:34,560
speech word structure etc but then we

279
00:11:33,130 --> 00:11:37,300
used word embeddings

280
00:11:34,560 --> 00:11:38,979
this is a type of unsupervised feat

281
00:11:37,300 --> 00:11:42,069
feature that can we that we can extract

282
00:11:38,980 --> 00:11:47,410
in an unsupervised way in a simple way

283
00:11:42,070 --> 00:11:48,940
word bearings are a way to to encode the

284
00:11:47,410 --> 00:11:50,890
semantic context and the meaning in

285
00:11:48,940 --> 00:11:53,890
which a word appears in a numeric vector

286
00:11:50,890 --> 00:11:56,230
so for example if two words

287
00:11:53,890 --> 00:11:58,540
we're in the same context a lot or have

288
00:11:56,230 --> 00:12:00,160
the same meaning we would expect their

289
00:11:58,540 --> 00:12:05,800
numeric vectors to have a high cosine

290
00:12:00,160 --> 00:12:07,449
similarity this is just an example a

291
00:12:05,800 --> 00:12:09,339
screenshot of generic word embeddings

292
00:12:07,450 --> 00:12:11,740
that I added to give you more context on

293
00:12:09,339 --> 00:12:14,500
what word embeddings look like on just a

294
00:12:11,740 --> 00:12:16,720
general data set so here we can see that

295
00:12:14,500 --> 00:12:18,519
words that occur a lot in the similar

296
00:12:16,720 --> 00:12:21,880
context like in the context of a kitchen

297
00:12:18,519 --> 00:12:23,649
they are clustered together and we

298
00:12:21,880 --> 00:12:26,290
trained our word embeddings on the TI

299
00:12:23,649 --> 00:12:29,200
corpus so what we are expecting here is

300
00:12:26,290 --> 00:12:30,579
that the meaning of the words in which

301
00:12:29,200 --> 00:12:32,380
they appear in that cybertek threat

302
00:12:30,579 --> 00:12:34,329
intelligence context will be included in

303
00:12:32,380 --> 00:12:36,010
their numeric vectors so let's look at a

304
00:12:34,329 --> 00:12:40,510
few examples that show if this is

305
00:12:36,010 --> 00:12:42,130
happening this is a visualization of our

306
00:12:40,510 --> 00:12:44,550
word embeddings generated on the cyber

307
00:12:42,130 --> 00:12:47,589
corpus visualized using tensor board

308
00:12:44,550 --> 00:12:50,229
here we are plotting the four closest

309
00:12:47,589 --> 00:12:54,040
points to a PT 28 and we can see that

310
00:12:50,230 --> 00:12:56,740
out of those two are aliases sofa C and

311
00:12:54,040 --> 00:12:58,810
T G 4 to 7 and then two others are

312
00:12:56,740 --> 00:13:00,459
related to it by attribution so we can

313
00:12:58,810 --> 00:13:02,949
see that the context in which these word

314
00:13:00,459 --> 00:13:04,779
appears it's kind of encoded in this

315
00:13:02,949 --> 00:13:08,620
numeric vector which is formed or

316
00:13:04,779 --> 00:13:12,579
extracted from the word embedding here's

317
00:13:08,620 --> 00:13:14,230
another example if we look at the three

318
00:13:12,579 --> 00:13:16,989
of the four closest points to the word

319
00:13:14,230 --> 00:13:19,540
dog hall we see that their poem shutter

320
00:13:16,990 --> 00:13:21,399
speed and are you happy and all of these

321
00:13:19,540 --> 00:13:27,040
are malware families that are used by

322
00:13:21,399 --> 00:13:28,959
one particular apt apt 37 this is an

323
00:13:27,040 --> 00:13:30,399
overall visualization of the tokens that

324
00:13:28,959 --> 00:13:32,319
are contained in the three classes that

325
00:13:30,399 --> 00:13:34,870
we care about and we can see that the

326
00:13:32,320 --> 00:13:36,459
tokens that are in the malware and the

327
00:13:34,870 --> 00:13:37,690
actor class are clustered separately

328
00:13:36,459 --> 00:13:40,390
from the tokens that appear in the

329
00:13:37,690 --> 00:13:42,850
technique class so basically these three

330
00:13:40,390 --> 00:13:44,410
visualizations help us get some idea

331
00:13:42,850 --> 00:13:48,970
that the word embeddings are taking our

332
00:13:44,410 --> 00:13:50,740
model in the right direction let me

333
00:13:48,970 --> 00:13:54,610
share with you the architecture that we

334
00:13:50,740 --> 00:13:56,920
used so it was quite simple we had the

335
00:13:54,610 --> 00:13:59,320
text the unstructured data sources the

336
00:13:56,920 --> 00:14:02,170
blogs white papers etc and we put them

337
00:13:59,320 --> 00:14:04,250
through just HTML and PDF parsers to

338
00:14:02,170 --> 00:14:05,719
gather the raw text we

339
00:14:04,250 --> 00:14:07,610
put them through some lightweight text

340
00:14:05,720 --> 00:14:09,830
pre-processing just whitespace

341
00:14:07,610 --> 00:14:11,960
normalization and removal of unprintable

342
00:14:09,830 --> 00:14:14,750
characters and then we extracted the

343
00:14:11,960 --> 00:14:16,670
word embeddings and in this experiment

344
00:14:14,750 --> 00:14:18,710
we use the word to back model but there

345
00:14:16,670 --> 00:14:21,079
are others like glove or fostex that can

346
00:14:18,710 --> 00:14:23,060
also be used we then split the data set

347
00:14:21,080 --> 00:14:25,940
into training and test 80/20 we

348
00:14:23,060 --> 00:14:27,829
extracted the custom features join those

349
00:14:25,940 --> 00:14:29,360
with the word embeddings and then we

350
00:14:27,830 --> 00:14:33,170
trained our model and then tested our

351
00:14:29,360 --> 00:14:35,360
Marro respectively so this was the first

352
00:14:33,170 --> 00:14:37,189
model that we experimented with that I

353
00:14:35,360 --> 00:14:40,580
want to talk about conditional random

354
00:14:37,190 --> 00:14:42,620
fields or CRFs can be thought they're

355
00:14:40,580 --> 00:14:44,990
not a deep learning method that's it is

356
00:14:42,620 --> 00:14:46,520
a statistical modeling method and it can

357
00:14:44,990 --> 00:14:47,810
simply be thought of as logistic

358
00:14:46,520 --> 00:14:50,449
regression but for a sequence labeling

359
00:14:47,810 --> 00:14:51,890
problem in fact they're very commonly

360
00:14:50,450 --> 00:14:53,600
used for sequence labeling tasks

361
00:14:51,890 --> 00:14:56,060
particularly in other natural language

362
00:14:53,600 --> 00:15:00,470
processing tasks biological sequencing

363
00:14:56,060 --> 00:15:03,079
and even in computer vision sometimes we

364
00:15:00,470 --> 00:15:05,480
can say that CRFs have a short term

365
00:15:03,080 --> 00:15:08,480
memory and let me explain what I mean by

366
00:15:05,480 --> 00:15:10,520
that here you can see the matrix that

367
00:15:08,480 --> 00:15:12,470
matrix is a way matrix of transition

368
00:15:10,520 --> 00:15:14,900
weights between the various classes that

369
00:15:12,470 --> 00:15:17,270
we had in our dataset so for example you

370
00:15:14,900 --> 00:15:19,880
can see that the transition weight from

371
00:15:17,270 --> 00:15:23,510
in Oh class to a be bad actor class is

372
00:15:19,880 --> 00:15:25,370
0.22 which is positive but if you look

373
00:15:23,510 --> 00:15:28,460
at the weight for a transition between

374
00:15:25,370 --> 00:15:30,860
an O class to an i bad actor class it is

375
00:15:28,460 --> 00:15:32,750
negative 5 it's a very negative number

376
00:15:30,860 --> 00:15:34,730
which shows that the transition between

377
00:15:32,750 --> 00:15:36,530
an O class directly to an I bad actor

378
00:15:34,730 --> 00:15:38,510
class is very unlikely which makes sense

379
00:15:36,530 --> 00:15:41,300
because in a sentence you're more likely

380
00:15:38,510 --> 00:15:42,860
to get an odd term and then a B bad

381
00:15:41,300 --> 00:15:45,260
actor term and only after that when you

382
00:15:42,860 --> 00:15:48,290
have the iya bad actor term so this type

383
00:15:45,260 --> 00:15:52,010
of this is the short term memory that is

384
00:15:48,290 --> 00:15:54,050
encoded and learnt by the model also we

385
00:15:52,010 --> 00:15:55,670
did two experiments with the CRF because

386
00:15:54,050 --> 00:15:57,380
we wanted to make sure that the word

387
00:15:55,670 --> 00:15:59,719
embeddings that we're using are actually

388
00:15:57,380 --> 00:16:01,820
contributing positively to the model so

389
00:15:59,720 --> 00:16:07,730
we did one experiment with the word

390
00:16:01,820 --> 00:16:10,700
Waring's and then one without we also

391
00:16:07,730 --> 00:16:13,490
experimented with lsdm lsdm is the deep

392
00:16:10,700 --> 00:16:14,610
learning method it is a special form of

393
00:16:13,490 --> 00:16:17,129
RNN

394
00:16:14,610 --> 00:16:18,869
we used to stacked bidirectional lsdm

395
00:16:17,129 --> 00:16:20,759
layers with dropout

396
00:16:18,869 --> 00:16:22,290
we used categorical cross entropy for

397
00:16:20,759 --> 00:16:24,929
our loss function and softmax activation

398
00:16:22,290 --> 00:16:27,149
for our final layer this type of

399
00:16:24,929 --> 00:16:28,709
architecture is quite typical for the

400
00:16:27,149 --> 00:16:30,569
problem that we're trying to solve based

401
00:16:28,709 --> 00:16:32,279
on the literature so what we're doing

402
00:16:30,569 --> 00:16:33,779
here is domain-specific named energy

403
00:16:32,279 --> 00:16:36,119
extraction and this type of architecture

404
00:16:33,779 --> 00:16:38,309
is quite typical the only tweaks we made

405
00:16:36,119 --> 00:16:41,040
was adding the drop out layer and that's

406
00:16:38,309 --> 00:16:42,179
because I'd mentioned earlier that we're

407
00:16:41,040 --> 00:16:43,949
worried about the class and balance

408
00:16:42,179 --> 00:16:45,509
causing overfitting so which is why the

409
00:16:43,949 --> 00:16:48,809
drop out is meant to help the

410
00:16:45,509 --> 00:16:53,399
generalization and we trained this using

411
00:16:48,809 --> 00:16:57,809
Karis intensive flow okay we come to the

412
00:16:53,399 --> 00:17:00,749
assessments so first our shared with you

413
00:16:57,809 --> 00:17:02,850
the overall recall counts of the terms

414
00:17:00,749 --> 00:17:04,799
in our dataset right off the bat you can

415
00:17:02,850 --> 00:17:07,289
see that we had very few technique

416
00:17:04,799 --> 00:17:09,750
tokens and that is the limitation of the

417
00:17:07,289 --> 00:17:12,269
data that we're working with here the

418
00:17:09,750 --> 00:17:14,789
yellow bars indicate the lsdm model the

419
00:17:12,269 --> 00:17:16,620
blue is the CRF without embeddings the

420
00:17:14,789 --> 00:17:18,599
red is the CRF with embeddings

421
00:17:16,619 --> 00:17:21,898
and the green is the actual counts of

422
00:17:18,599 --> 00:17:24,089
the tokens present in our data set so we

423
00:17:21,898 --> 00:17:27,000
observed that for the actor class the

424
00:17:24,089 --> 00:17:29,129
lsdm outperformed both the CRFs by a

425
00:17:27,000 --> 00:17:31,380
huge margin whereas for the other two

426
00:17:29,130 --> 00:17:39,149
classes their performance is kind of

427
00:17:31,380 --> 00:17:41,070
similar let's look at the precision and

428
00:17:39,149 --> 00:17:42,809
recall so here we have plotted the

429
00:17:41,070 --> 00:17:45,570
positive precision and positive recall

430
00:17:42,809 --> 00:17:48,750
of our models and you can see again that

431
00:17:45,570 --> 00:17:51,600
in terms of the precision the three

432
00:17:48,750 --> 00:17:54,630
models have comparable performance where

433
00:17:51,600 --> 00:17:56,908
you know the trade-off might just be hey

434
00:17:54,630 --> 00:17:59,760
lsdm takes three times longer to train

435
00:17:56,909 --> 00:18:02,220
so I'm just going to use CRF but with

436
00:17:59,760 --> 00:18:06,179
the positive recall we see that the lsdm

437
00:18:02,220 --> 00:18:08,429
is still outperforming the CRFs but

438
00:18:06,179 --> 00:18:10,019
there is one particular use case or

439
00:18:08,429 --> 00:18:11,909
scenario where we want to test this

440
00:18:10,019 --> 00:18:15,480
model and that is its performance on

441
00:18:11,909 --> 00:18:17,970
unseen tokens so in our actual use case

442
00:18:15,480 --> 00:18:19,679
when we get a new document that talks

443
00:18:17,970 --> 00:18:21,630
about a new actor or a new malware

444
00:18:19,679 --> 00:18:23,220
family that the model has not seen

445
00:18:21,630 --> 00:18:23,950
before we want it to be able to

446
00:18:23,220 --> 00:18:26,110
correctly

447
00:18:23,950 --> 00:18:28,720
label that as a bad actor or a malware

448
00:18:26,110 --> 00:18:30,250
family etc so that is why we want to

449
00:18:28,720 --> 00:18:34,480
measure the performance of these models

450
00:18:30,250 --> 00:18:36,519
on unseen tokens and when we do that we

451
00:18:34,480 --> 00:18:39,159
see a dramatically different picture we

452
00:18:36,519 --> 00:18:41,289
can see that on the unseen tokens the

453
00:18:39,159 --> 00:18:43,360
lsdm as well as the CRF they severely

454
00:18:41,289 --> 00:18:45,700
drop in performance they have barely any

455
00:18:43,360 --> 00:18:47,379
recall and even their precision has

456
00:18:45,700 --> 00:18:49,570
dropped whereas the CRF within bearings

457
00:18:47,380 --> 00:18:53,169
it has perfect precision and it is the

458
00:18:49,570 --> 00:18:54,580
only one that has any type of recall so

459
00:18:53,169 --> 00:18:58,809
that's the model that I'm going to be

460
00:18:54,580 --> 00:19:03,250
using in the demo today all right we're

461
00:18:58,809 --> 00:19:06,129
at our demo so this is just a toy web

462
00:19:03,250 --> 00:19:08,620
application here you can paste the text

463
00:19:06,130 --> 00:19:10,659
run the model on it and then the model

464
00:19:08,620 --> 00:19:13,510
will extract the entities that we talked

465
00:19:10,659 --> 00:19:15,730
about the actor malware family etc and

466
00:19:13,510 --> 00:19:21,490
sure and highlight those basically so

467
00:19:15,730 --> 00:19:24,130
let's try a dummy sentence on it this is

468
00:19:21,490 --> 00:19:26,889
a blurb about pond storm which is also

469
00:19:24,130 --> 00:19:30,700
an alias for a PT 28 we run the model on

470
00:19:26,889 --> 00:19:32,678
it and we can see the output

471
00:19:30,700 --> 00:19:34,149
so here the malicious actor is

472
00:19:32,679 --> 00:19:35,889
highlighted in red whereas the

473
00:19:34,149 --> 00:19:37,449
techniques are highlighted in orange and

474
00:19:35,889 --> 00:19:39,760
we can see that the model correctly

475
00:19:37,450 --> 00:19:41,529
identified pond storm as the actor and

476
00:19:39,760 --> 00:19:46,899
credentials fishing and spearfishing as

477
00:19:41,529 --> 00:19:49,440
the techniques let me give you some more

478
00:19:46,899 --> 00:19:49,439
examples

479
00:19:54,760 --> 00:20:00,549
so I'll add another blurb of text there

480
00:19:58,510 --> 00:20:01,570
are two paragraphs they're unrelated to

481
00:20:00,549 --> 00:20:04,690
each other but I just wanted to

482
00:20:01,570 --> 00:20:06,189
illustrate the breadth of actors and

483
00:20:04,690 --> 00:20:08,169
techniques and entities that the model

484
00:20:06,190 --> 00:20:11,410
can find the first paragraph is about

485
00:20:08,169 --> 00:20:15,549
Scarlett mimic and when we run the model

486
00:20:11,410 --> 00:20:18,010
on it we can see that the model

487
00:20:15,549 --> 00:20:20,379
correctly extracted Scarlett mimic is

488
00:20:18,010 --> 00:20:22,570
the actor and spear fishing and watering

489
00:20:20,380 --> 00:20:24,250
hole as the attacks the second paragraph

490
00:20:22,570 --> 00:20:28,350
is about a different malware family have

491
00:20:24,250 --> 00:20:30,790
extract it's the the wiki blurb about it

492
00:20:28,350 --> 00:20:33,100
the malware family is highlighted in

493
00:20:30,790 --> 00:20:34,659
purple and it also identified in the

494
00:20:33,100 --> 00:20:37,409
second blurb some other actors that

495
00:20:34,660 --> 00:20:40,540
represent black energy and dragonfly so

496
00:20:37,410 --> 00:20:43,780
this needs to be working quite well but

497
00:20:40,540 --> 00:20:44,830
I want to give one more example the

498
00:20:43,780 --> 00:20:46,928
reason for that is that I want to

499
00:20:44,830 --> 00:20:49,000
clearly illustrate that the model is not

500
00:20:46,929 --> 00:20:51,070
doing pattern matching or text

501
00:20:49,000 --> 00:20:53,830
recognition but is actually able to

502
00:20:51,070 --> 00:20:56,110
identify the techniques or the entities

503
00:20:53,830 --> 00:20:59,949
in based on the context in which they

504
00:20:56,110 --> 00:21:01,439
appear because that's the critical thing

505
00:20:59,950 --> 00:21:05,130
about the model it should not be just

506
00:21:01,440 --> 00:21:10,110
being like a fancy regex

507
00:21:05,130 --> 00:21:10,110
so if I type a dummy sentence here like

508
00:21:11,610 --> 00:21:18,129
okay so if I type or dummy sentence like

509
00:21:14,070 --> 00:21:21,580
this is a fancy bear and fancy bear is

510
00:21:18,130 --> 00:21:24,520
the name of unknown attacker and if I

511
00:21:21,580 --> 00:21:27,129
run the model on it the model should not

512
00:21:24,520 --> 00:21:29,168
extract any actor on it and that is how

513
00:21:27,130 --> 00:21:30,790
it as it should be because in this

514
00:21:29,169 --> 00:21:32,650
context you don't this is not the

515
00:21:30,790 --> 00:21:34,780
context of an attack this is a fancy

516
00:21:32,650 --> 00:21:36,640
where I might be talking about a fancy

517
00:21:34,780 --> 00:21:38,110
teddy bear for all we know and here we

518
00:21:36,640 --> 00:21:39,910
can see that the model did not extract

519
00:21:38,110 --> 00:21:41,860
did not call this out as an attacker and

520
00:21:39,910 --> 00:21:43,780
that is how it should be but if I type

521
00:21:41,860 --> 00:21:46,809
another sentence with the same phrase

522
00:21:43,780 --> 00:21:49,418
but having the attack context then I

523
00:21:46,809 --> 00:21:51,580
would like it to identify the actor

524
00:21:49,419 --> 00:21:53,169
correctly so let me sew another sentence

525
00:21:51,580 --> 00:21:58,110
is the white house was attacked by fancy

526
00:21:53,169 --> 00:21:58,110
bear and when we run the model on it

527
00:22:04,970 --> 00:22:08,810
awesome so we can see that now the model

528
00:22:07,340 --> 00:22:10,820
can correctly identify that as an

529
00:22:08,810 --> 00:22:12,830
attacker and that's the critical piece

530
00:22:10,820 --> 00:22:14,090
we're able to show that it's not just

531
00:22:12,830 --> 00:22:16,100
doing pattern matching and text

532
00:22:14,090 --> 00:22:17,840
recognition but is identifying the

533
00:22:16,100 --> 00:22:25,580
phrases based on the context in which

534
00:22:17,840 --> 00:22:28,100
they appear awesome so this is a version

535
00:22:25,580 --> 00:22:30,500
one for us this is an ongoing piece of

536
00:22:28,100 --> 00:22:32,870
research the next things we want to work

537
00:22:30,500 --> 00:22:34,490
on we want experiment with attention

538
00:22:32,870 --> 00:22:37,040
networks which have been shown to show

539
00:22:34,490 --> 00:22:38,750
good results for this type of problem I

540
00:22:37,040 --> 00:22:40,670
also mentioned that we have a small data

541
00:22:38,750 --> 00:22:43,420
set so we want experiment with

542
00:22:40,670 --> 00:22:45,650
generative methods for data augmentation

543
00:22:43,420 --> 00:22:47,210
also we want experiment more with

544
00:22:45,650 --> 00:22:49,190
extracting more sophisticated

545
00:22:47,210 --> 00:22:54,290
relationships and also temporal

546
00:22:49,190 --> 00:22:56,570
relationships I want to close with an

547
00:22:54,290 --> 00:23:00,649
example of how this can be used to drive

548
00:22:56,570 --> 00:23:02,179
impact so I showed you this graph in the

549
00:23:00,650 --> 00:23:04,280
beginning where our imaginary threat

550
00:23:02,180 --> 00:23:06,800
analysts use this data to help their

551
00:23:04,280 --> 00:23:09,889
organization decide where they wanted to

552
00:23:06,800 --> 00:23:11,659
place their defense or choke points we

553
00:23:09,890 --> 00:23:13,730
created a similar plot from the data set

554
00:23:11,660 --> 00:23:17,780
that we had and here we plotted the

555
00:23:13,730 --> 00:23:20,210
overlap on the TTP's of a commodity

556
00:23:17,780 --> 00:23:22,910
malware family emo Ted and then a

557
00:23:20,210 --> 00:23:25,490
collection of apts there are some snakes

558
00:23:22,910 --> 00:23:27,680
and pandas in there but you can clearly

559
00:23:25,490 --> 00:23:29,990
see the overlap between commodity

560
00:23:27,680 --> 00:23:33,190
malware family and nation-state

561
00:23:29,990 --> 00:23:35,780
attackers techniques like spearfishing

562
00:23:33,190 --> 00:23:38,180
obfuscated PowerShell part process

563
00:23:35,780 --> 00:23:39,560
hollowing we might think that they are

564
00:23:38,180 --> 00:23:42,230
advanced but they've been used by

565
00:23:39,560 --> 00:23:44,000
commodity malware today and this is

566
00:23:42,230 --> 00:23:45,620
further evidence of the blurring line

567
00:23:44,000 --> 00:23:47,870
between commodity malware and

568
00:23:45,620 --> 00:23:49,909
nation-state attackers so now this sort

569
00:23:47,870 --> 00:23:52,370
of a visualization can be used by ti

570
00:23:49,910 --> 00:23:53,870
analysts to convincingly drive their

571
00:23:52,370 --> 00:23:55,820
organizations towards placing their

572
00:23:53,870 --> 00:23:58,909
checkpoints in these places which will

573
00:23:55,820 --> 00:24:01,189
help them avoid not only the annoying

574
00:23:58,910 --> 00:24:04,400
commodity malware but also the

575
00:24:01,190 --> 00:24:06,560
high-profile apt attacks and we're

576
00:24:04,400 --> 00:24:12,860
helping them do this in an automatic way

577
00:24:06,560 --> 00:24:15,889
without manual work in conclusion I hope

578
00:24:12,860 --> 00:24:17,629
I have showed you that it's time for TI

579
00:24:15,890 --> 00:24:20,360
to move beyond

580
00:24:17,630 --> 00:24:22,700
I shared with you a recipe of how we can

581
00:24:20,360 --> 00:24:24,620
use machine learning to extract insights

582
00:24:22,700 --> 00:24:26,960
from the rich unstructured data that we

583
00:24:24,620 --> 00:24:29,449
have and then I give you an example of

584
00:24:26,960 --> 00:24:34,160
how this can be used to drive inside for

585
00:24:29,450 --> 00:24:35,750
the security of an organization I would

586
00:24:34,160 --> 00:24:37,460
like to thank my teammates a defender

587
00:24:35,750 --> 00:24:40,730
who have been amazingly supportive for

588
00:24:37,460 --> 00:24:42,850
this project and with that open for

589
00:24:40,730 --> 00:24:42,850
questions

590
00:24:45,660 --> 00:24:49,429
[Applause]

591
00:24:49,960 --> 00:24:53,539
[Music]

592
00:24:51,620 --> 00:24:56,959
yeah they're mics over there yeah go

593
00:24:53,539 --> 00:24:58,158
ahead yeah um so I saw that you're in

594
00:24:56,960 --> 00:25:01,360
sample I think was like in-sample

595
00:24:58,159 --> 00:25:03,679
performance like token said okay the

596
00:25:01,360 --> 00:25:06,020
performance for the temp samples that

597
00:25:03,679 --> 00:25:08,450
had been seeing was rather high and then

598
00:25:06,020 --> 00:25:10,129
the samples are started the performance

599
00:25:08,450 --> 00:25:12,520
for tokens which hadn't been seen which

600
00:25:10,130 --> 00:25:17,270
I guess is like outer sample in a way

601
00:25:12,520 --> 00:25:18,918
was one which is as I'm sure you or it's

602
00:25:17,270 --> 00:25:20,270
pretty uncommon for it like it seems

603
00:25:18,919 --> 00:25:23,419
like the out-of-sample performance was

604
00:25:20,270 --> 00:25:25,429
like better than the in-sample am i

605
00:25:23,419 --> 00:25:29,480
interpreting that right and like how did

606
00:25:25,429 --> 00:25:32,090
that surprise you I'm sorry I'm not able

607
00:25:29,480 --> 00:25:34,669
to understand the question Kashyap

608
00:25:32,090 --> 00:25:37,370
there was there was a portion where you

609
00:25:34,669 --> 00:25:40,789
show the performance with like recall I

610
00:25:37,370 --> 00:25:43,850
think it was on like unseen tokens and

611
00:25:40,789 --> 00:25:46,429
it was higher than tokens that had been

612
00:25:43,850 --> 00:25:47,899
seen and I just thought that was kind of

613
00:25:46,429 --> 00:25:51,200
unusual and wants to know what you

614
00:25:47,899 --> 00:25:53,779
thought about that right right so yes

615
00:25:51,200 --> 00:25:55,880
the precision of the CRF was higher on

616
00:25:53,779 --> 00:25:59,299
the unseen tokens on just the unseen

617
00:25:55,880 --> 00:26:00,649
tokens then the the other the overall

618
00:25:59,299 --> 00:26:02,899
data said and that's I think my

619
00:26:00,649 --> 00:26:05,510
assumption underlying is that the unseen

620
00:26:02,899 --> 00:26:07,370
tokens were more likely to be what one

621
00:26:05,510 --> 00:26:09,710
of in one of our classes they were not

622
00:26:07,370 --> 00:26:11,809
just like words that occur in text in

623
00:26:09,710 --> 00:26:12,980
general so that is why the model latched

624
00:26:11,809 --> 00:26:15,908
on to that and was able to perform

625
00:26:12,980 --> 00:26:17,929
higher than that but yeah that's that's

626
00:26:15,909 --> 00:26:22,340
understandable Thanks

627
00:26:17,929 --> 00:26:24,440
thank you go ahead thank you we are out

628
00:26:22,340 --> 00:26:26,230
of time we'll take those questions in

629
00:26:24,440 --> 00:26:29,230
the wrap room thank you everybody

630
00:26:26,230 --> 00:26:29,230
Thanks

