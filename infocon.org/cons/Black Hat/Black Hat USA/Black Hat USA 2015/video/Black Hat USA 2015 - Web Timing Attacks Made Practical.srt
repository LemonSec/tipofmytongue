1
00:00:00,000 --> 00:00:04,400
good morning and welcome today to a
black cat 2015 got a few announcements

2
00:00:04,400 --> 00:00:09,260
for you start my own business ha located
in Shoreline a for sponsored sessions in

3
00:00:09,260 --> 00:00:17,350
theater A&B be sure to check out all the
black and Arsenal and breakers de J K

4
00:00:17,350 --> 00:00:25,109
sponsored workshops are in Mandalay Jake
hey now you are an lagoon K for the

5
00:00:25,109 --> 00:00:31,090
session name web timing attacks made
practical with Tim Morgan and Jason

6
00:00:31,090 --> 00:00:38,019
Morgan also remind you to put your phone
on vibrate it makes it easier for the

7
00:00:38,020 --> 00:00:42,100
rest of us to ignore the ringing while
you wait for your voicemail to pick it

8
00:00:42,100 --> 00:00:48,219
up so good morning we glad you're here
and let's introduce are so welcome our

9
00:00:48,219 --> 00:00:53,940
speakers

10
00:00:53,940 --> 00:01:01,358
thank you thanks for coming I'm time
with my brother Jason and we won I

11
00:01:01,359 --> 00:01:07,259
explain to you what we researched in the
area of web timing attacks so what are

12
00:01:07,259 --> 00:01:08,860
what our timing attacks

13
00:01:08,860 --> 00:01:13,049
well just about any piece of software
has situations where it needs to do some

14
00:01:13,049 --> 00:01:17,380
kind of security critical operation
things like authentication authorization

15
00:01:17,380 --> 00:01:22,820
checks or cryptography and and sometimes
in those situations it's really

16
00:01:22,820 --> 00:01:28,919
important that the application may
return the result of that operation that

17
00:01:28,920 --> 00:01:35,399
whether success or failure to the user
or to the or potential attacker but it's

18
00:01:35,399 --> 00:01:40,110
really critical that the details of how
the decision was made are not exposed so

19
00:01:40,110 --> 00:01:44,770
it really trivial example consider web
application that you're trying to log

20
00:01:44,770 --> 00:01:49,850
into and if you type your username and
you miss type your password if the

21
00:01:49,850 --> 00:01:53,589
application comes back and says oh the
concert is wrong you messed up character

22
00:01:53,590 --> 00:01:57,880
three you know that's kind of absurd but
if we were to do that then clearly you

23
00:01:57,880 --> 00:02:02,530
could guess the password one character
at a time right so in in those

24
00:02:02,530 --> 00:02:05,959
situations we have to be sure that that
information doesn't leak and depending

25
00:02:05,959 --> 00:02:11,890
on the operation what information is
critical just varies with timing attacks

26
00:02:11,890 --> 00:02:15,700
what we can do sometimes if we can
measure the execution time of those

27
00:02:15,700 --> 00:02:19,750
critical operations then we can actually
learn information about the inner

28
00:02:19,750 --> 00:02:23,670
workings of how the decision was made so
that kind of information can leak and

29
00:02:23,670 --> 00:02:29,530
therefore it can expose vulnerabilities
so there's been plenty of research in

30
00:02:29,530 --> 00:02:33,580
the past in the area of cryptography on
timing attacks and a number of you

31
00:02:33,580 --> 00:02:38,720
demonstrated exploits both against you
know specific site first but also

32
00:02:38,720 --> 00:02:43,050
against more complex cryptosystems like
SSL I'm so there's a lot of background

33
00:02:43,050 --> 00:02:47,459
research in this area but it's mostly in
the area of cryptography but what about

34
00:02:47,459 --> 00:02:54,030
your everyday web application when does
the stuff actually matter so here's an

35
00:02:54,030 --> 00:03:00,040
example of a real live public website
out on the internet right now it's it's

36
00:03:00,040 --> 00:03:04,179
an insurance company and their health
insurance and if you already have a

37
00:03:04,180 --> 00:03:06,550
policy with them then you can sign up
for an online account

38
00:03:06,550 --> 00:03:10,880
count you have to enter your personal
information very sensitive stuff like

39
00:03:10,880 --> 00:03:14,000
your social security number date of
birth and so on

40
00:03:14,000 --> 00:03:18,320
in order to prove your identity before
you can sign up for an account now this

41
00:03:18,320 --> 00:03:21,780
particular website I don't know if it's
possible at all I'm not claiming it is

42
00:03:21,780 --> 00:03:25,230
but this is the kind of form that I'm
concerned about is the kind of form I'd

43
00:03:25,230 --> 00:03:28,690
like to be able to test now if if this
site

44
00:03:28,690 --> 00:03:32,370
returns differing error messages
depending on which field you type in and

45
00:03:32,370 --> 00:03:35,750
got wrong then clearly there would be an
attack they're right you could try to

46
00:03:35,750 --> 00:03:38,650
guess the social security number first
and then you can move on to the next

47
00:03:38,650 --> 00:03:44,140
field and that stuff is something at 10
testers typically search for they they

48
00:03:44,140 --> 00:03:47,540
look for those differing error messages
and they'll report on those lets a well

49
00:03:47,540 --> 00:03:53,440
known issue but if the amount of time
the application requires to check each

50
00:03:53,440 --> 00:03:57,280
field varies depending on what the data
is then there could still be a

51
00:03:57,280 --> 00:04:00,510
vulnerability here and right now I don't
think most dentists are looking for this

52
00:04:00,510 --> 00:04:08,149
is not good tools to check for this kind
of thing so this is my favorite quote

53
00:04:08,150 --> 00:04:14,420
from yogi berra so in the past there's
been a lot of you know specific

54
00:04:14,420 --> 00:04:19,668
vulnerabilities identified in the area
of timing vulnerabilities and it is

55
00:04:19,668 --> 00:04:22,340
really interesting but a lot of the
techniques that are used a really basic

56
00:04:22,340 --> 00:04:25,460
and they happened to work on that
particular vulnerability but they don't

57
00:04:25,460 --> 00:04:29,590
necessarily extended to other areas and
then there's been other research done

58
00:04:29,590 --> 00:04:33,190
that has tried to generalize timing
attacks but it seems like the research

59
00:04:33,190 --> 00:04:38,360
so far still fairly immature and you
know there's very few tools off the

60
00:04:38,360 --> 00:04:42,780
shelf tools available last year Meyer in
Sandton presented a talk at black hat

61
00:04:42,780 --> 00:04:47,409
and they they released time trial which
is a really nice tool it lets you

62
00:04:47,410 --> 00:04:52,190
identify if timing difference exists in
an application both web applications and

63
00:04:52,190 --> 00:04:56,020
other applications but it doesn't
necessarily let you determine the actual

64
00:04:56,020 --> 00:05:00,460
risk of that because you can't
necessarily determine exactly the number

65
00:05:00,460 --> 00:05:05,330
of requests you need to perform the
attack and it doesn't give the princess

66
00:05:05,330 --> 00:05:10,180
you're quite enough tools to actually
exploit the issue directly and we also

67
00:05:10,180 --> 00:05:13,700
kind of felt that the statistical
analysis could be improved in this area

68
00:05:13,700 --> 00:05:17,599
it seems like a lot of a lot of previous
researchers have used really basic

69
00:05:17,600 --> 00:05:19,000
statistics the

70
00:05:19,000 --> 00:05:23,300
for instance in the case of Lucky 13
which is a exploit against SSL assembly

71
00:05:23,300 --> 00:05:26,810
is the medium to try to major timing
differences and you know really basic

72
00:05:26,810 --> 00:05:31,300
stuff from high school statistics and so
we thought this would be an easier way

73
00:05:31,300 --> 00:05:32,070
to improve on

74
00:05:32,070 --> 00:05:36,400
turned out real harder than we thought
so our goals try to improve the

75
00:05:36,400 --> 00:05:41,400
statistical methods that are used to
measure timing differences and be able

76
00:05:41,400 --> 00:05:46,700
to answer the question for a pen tester
in the time frame of a pen test is this

77
00:05:46,700 --> 00:05:50,479
timing floor actually exploitable can
actually tell my customer that yes this

78
00:05:50,480 --> 00:05:53,090
is something you need to figure out a
way or this is really something

79
00:05:53,090 --> 00:05:58,049
theoretical that would only be doable in
two months time and then we also wanted

80
00:05:58,050 --> 00:06:02,720
to investigate the use of TCP timestamps
and if those timestamps could be used to

81
00:06:02,720 --> 00:06:06,800
help make these attacks more efficient
previous researchers that kind of left

82
00:06:06,800 --> 00:06:13,510
this as an is an open question so on the
data-collection how we are we obtained

83
00:06:13,510 --> 00:06:17,669
the information we need to distinguish
timing differences

84
00:06:17,669 --> 00:06:24,190
first a little bit about CC's timestamps
Saudi timestamps are very simple

85
00:06:24,190 --> 00:06:29,340
mechanism to make TCP more efficient
what they do is whenever a host sends a

86
00:06:29,340 --> 00:06:34,190
packet as part of a TCP connection the
current time the hosts current time is

87
00:06:34,190 --> 00:06:37,820
actually get labeled on the package and
so this could be really useful right

88
00:06:37,820 --> 00:06:41,419
because now we can actually look at the
time the host send the packet rather

89
00:06:41,419 --> 00:06:44,700
than when we received it and we can
eliminate a lot of potential noise that

90
00:06:44,700 --> 00:06:50,430
occurs on the network so it's really
attractive to you want to use this and

91
00:06:50,430 --> 00:06:53,870
in order to get a TCP timestamps you
pretty much have to use a sniffer you

92
00:06:53,870 --> 00:06:58,680
have to observe the packets directly
because the the GCB stocks don't expose

93
00:06:58,680 --> 00:07:03,780
this to userspace applications it's also
fairly tricky because the frequency of

94
00:07:03,780 --> 00:07:08,580
the clock 43 timestamps of berries from
one operating system to another so we

95
00:07:08,580 --> 00:07:11,659
have to be able to make sure that clock
frequency and we work to do that but

96
00:07:11,660 --> 00:07:17,190
it's not it's not trivial and what this
does is it forces us to actually analyze

97
00:07:17,190 --> 00:07:21,530
all of our data packet level and workout
complex issues like if there's

98
00:07:21,530 --> 00:07:25,119
retransmissions a pack adds if there's
dropped packets pockets come out of

99
00:07:25,120 --> 00:07:28,000
order we need to be addressed all those
things in order to measure the

100
00:07:28,000 --> 00:07:28,820
round-trip time

101
00:07:28,820 --> 00:07:33,969
of an HTTP request and in the end of the
day after working with these three times

102
00:07:33,970 --> 00:07:37,460
for quite a while we weren't able to use
them directly to measure timing

103
00:07:37,460 --> 00:07:40,909
differences with any real accuracy
there's definitely information there and

104
00:07:40,910 --> 00:07:44,570
we think by pairing that information
with other timing data we may be able to

105
00:07:44,570 --> 00:07:48,630
improve the results but we don't have
anything that sophisticated yet but what

106
00:07:48,630 --> 00:07:52,200
this did is that it forces down this
path of doing packet analysis actually

107
00:07:52,200 --> 00:07:53,960
analyzing

108
00:07:53,960 --> 00:07:58,560
packets as they come and go from the
network card which which actually ended

109
00:07:58,560 --> 00:08:01,590
up making our timing measurements much
better by measuring the packets directly

110
00:08:01,590 --> 00:08:05,849
and its it makes sense because if you
look at you know your typical

111
00:08:05,850 --> 00:08:10,720
very simple HTTP request if you're
trying to make sure that the time round

112
00:08:10,720 --> 00:08:14,730
trip times directly from userspace you
know you're just open a connection

113
00:08:14,730 --> 00:08:18,050
sending the data measuring the time you
sent it

114
00:08:18,050 --> 00:08:22,090
measuring the time when the data comes
back into userspace then you're forced

115
00:08:22,090 --> 00:08:25,530
to measure the amount of time it takes
for your day to be sent through the

116
00:08:25,530 --> 00:08:30,940
kernel onto the network card the time it
takes to do a TCP handshake the time it

117
00:08:30,940 --> 00:08:33,840
takes for the colonel who received a
response and then send it back to your

118
00:08:33,840 --> 00:08:37,810
users based application and then you're
then the kernel eventually schedules you

119
00:08:37,809 --> 00:08:41,809
or your use of space process back on the
CPU so there's all this extra time delay

120
00:08:41,809 --> 00:08:46,270
that's added to that and and that is it
reduces chance for more variants on your

121
00:08:46,270 --> 00:08:49,540
local machine that is more there's more
variability because of different

122
00:08:49,540 --> 00:08:55,099
processes running and so do you think if
you could simply measure from the time

123
00:08:55,100 --> 00:08:59,760
you saw the very first request packet
that actually has data and it not the

124
00:08:59,760 --> 00:09:03,650
handshake we don't need that and the
very last packet as data and it when we

125
00:09:03,650 --> 00:09:07,449
received the response then we can get a
more accurate time measurement so that's

126
00:09:07,450 --> 00:09:11,980
the theory but does it actually work in
practice and maybe a little bit hard to

127
00:09:11,980 --> 00:09:17,650
see the histogram but but yes it does
work basically you see here we compared

128
00:09:17,650 --> 00:09:21,510
time trial which is the tool release
last year and they have a very good

129
00:09:21,510 --> 00:09:26,310
userspace implementation is implemented
in C++ is very very fast and they did a

130
00:09:26,310 --> 00:09:32,329
lot of work to try to minimize noise in
userspace but just doing a much more

131
00:09:32,330 --> 00:09:37,290
naive request to all but measuring the
packets instead of instead of doing it

132
00:09:37,290 --> 00:09:40,689
from userspace we get a much better
distribution so

133
00:09:40,690 --> 00:09:45,690
the distribution is our measurements of
the same test scenario you can see that

134
00:09:45,690 --> 00:09:51,270
the distribution is much like here it's
a taller a more narrow distribution and

135
00:09:51,270 --> 00:09:54,090
and and clearly the time measurements
the wrong measurements are much smaller

136
00:09:54,090 --> 00:09:58,760
much further to the left and then
looking at the median absolute deviation

137
00:09:58,760 --> 00:10:03,710
of these two distributions which is a
major variants it turns out that the

138
00:10:03,710 --> 00:10:08,280
packet data was forty percent 40% less
variance is clearly much better so

139
00:10:08,280 --> 00:10:11,520
that's great

140
00:10:11,520 --> 00:10:15,340
another thing that we did differently
than previous researchers is that we

141
00:10:15,340 --> 00:10:20,060
decided to do paired sampling or to
sample the data in in doubles so

142
00:10:20,060 --> 00:10:23,319
basically if you have to test cases for
instance one with a valid social

143
00:10:23,320 --> 00:10:27,290
security number invalid social security
number anyone have major the time

144
00:10:27,290 --> 00:10:31,680
difference between the two then those
are your to test cases hey we made sure

145
00:10:31,680 --> 00:10:37,089
all those test cases one after another
very close in time in Wolcott time and

146
00:10:37,090 --> 00:10:40,370
then we just repeat that process over
and over and in our terminology one

147
00:10:40,370 --> 00:10:47,560
sample is is actually a pair of those
requests so we collected prose about the

148
00:10:47,560 --> 00:10:52,619
same time and the idea being that if
there's a disturbances on the network

149
00:10:52,620 --> 00:10:57,540
are on the hosts during a period of time
then hopefully for a given sample both

150
00:10:57,540 --> 00:11:01,050
of the request will be affected in a
similar way right if for some reason

151
00:11:01,050 --> 00:11:04,689
packets are getting rerouted through
different router if the remote host

152
00:11:04,690 --> 00:11:08,820
starts getting bogged down with other
user requests than than any disturbances

153
00:11:08,820 --> 00:11:14,680
will equally affect both types of
samples and early during testing we we

154
00:11:14,680 --> 00:11:19,310
did that one measurement collected a
bunch of samples on one host over the

155
00:11:19,310 --> 00:11:25,750
internet and we got this distribution
which is awful right both basically have

156
00:11:25,750 --> 00:11:30,020
two separate distributions one for each
test case but both these test cases were

157
00:11:30,020 --> 00:11:35,329
completely separate like the head is bad
very multimodal so some of the responses

158
00:11:35,330 --> 00:11:38,950
came back very very quickly and some
took a very long time and it's it's

159
00:11:38,950 --> 00:11:44,070
really ugly dataset and if you try to
just applied basic statistics on this

160
00:11:44,070 --> 00:11:47,890
distribution as is its really difficult
like to use the data on the laughter

161
00:11:47,890 --> 00:11:51,050
used it on the right or to try to come
up with some way to use both and combine

162
00:11:51,050 --> 00:11:54,150
them but then when we looked at the date
a little bit more clothes

163
00:11:54,150 --> 00:11:59,579
early as a function of time of day is
when we sent the request we saw this and

164
00:11:59,580 --> 00:12:04,870
this is a scatterplot the the red dots
are the short test case in the blue ones

165
00:12:04,870 --> 00:12:07,490
are loaded the longer test case and you
can you can't really see the Reds very

166
00:12:07,490 --> 00:12:11,510
much the blue cover it but basically
what you see is that early in the test

167
00:12:11,510 --> 00:12:15,160
all the request went really really fast
and we got responses quickly and then

168
00:12:15,160 --> 00:12:18,610
for some reason something changed and
then all responses certain become very

169
00:12:18,610 --> 00:12:23,070
very slow and looking into it further
when I decided probably what happened is

170
00:12:23,070 --> 00:12:28,080
that Comcast has a feature called speed
boost power boost something like that

171
00:12:28,080 --> 00:12:33,120
and what they do is they allow you to
download files from a particular web

172
00:12:33,120 --> 00:12:37,180
server unlimited speed offers and then
after a period of time once they decided

173
00:12:37,180 --> 00:12:41,560
you've downloaded enough then they throw
you back to your subscriber 800 Comcast

174
00:12:41,560 --> 00:12:44,770
actually kinda screwed up our dataset
here because they let us go really

175
00:12:44,770 --> 00:12:48,990
really fast at first and then they have
troubles back so it creates problems

176
00:12:48,990 --> 00:12:52,390
right for if you're doing a princess in
this happens to you what do you do what

177
00:12:52,390 --> 00:12:57,780
what what part of the day did you rely
on what if instead of trying to treat

178
00:12:57,780 --> 00:13:02,199
these two datasets a separate if instead
we just take the pairwise differences

179
00:13:02,200 --> 00:13:06,570
between the round-trip time measurements
for each pair is their parent they were

180
00:13:06,570 --> 00:13:09,850
done about the same time then you
actually get the distribution of the

181
00:13:09,850 --> 00:13:13,540
bottom in the purple so that's the
difference in time between each pair of

182
00:13:13,540 --> 00:13:17,390
measurements and so that automatically
sort of normalizes around a central

183
00:13:17,390 --> 00:13:23,300
tendency you know it if you can measure
then the

184
00:13:23,300 --> 00:13:26,849
the location of us central tendency
meaning what is the average difference

185
00:13:26,850 --> 00:13:31,740
between the two requests then you can
decide is the zero and non-zero and if

186
00:13:31,740 --> 00:13:36,050
it's 90 then there's a time difference
right so that's that's one change in the

187
00:13:36,050 --> 00:13:37,819
way that we approach the problem

188
00:13:37,820 --> 00:13:46,390
jasons gonna come up here and talk a bit
more about statistical analysis so good

189
00:13:46,390 --> 00:13:49,510
morning I thought now a minute talk a
little bit as he said about the

190
00:13:49,510 --> 00:13:53,740
statistical analysis we have done and
the approach we have tried to take to

191
00:13:53,740 --> 00:13:57,839
analyzing these round trip times in the
differences in round trip times so

192
00:13:57,839 --> 00:14:01,410
recall what we're trying to do is
uncover the amount of time

193
00:14:01,410 --> 00:14:06,050
12 computation at on a web server
another server takes versus a different

194
00:14:06,050 --> 00:14:12,449
types of computation but if you've ever
tried to deal with network data even on

195
00:14:12,450 --> 00:14:17,450
withstanding the notwithstanding the
Comcast power boost issue you'll notice

196
00:14:17,450 --> 00:14:23,959
that never date are extremely noisy so
this is one of our best test cases the

197
00:14:23,959 --> 00:14:29,770
sample of 500 a series of five hundred
observations from this analysis we've

198
00:14:29,770 --> 00:14:34,439
done or from a series that we have
collected and you'll notice it's quite

199
00:14:34,440 --> 00:14:42,500
clear what the problem is here in this
series you have 500 observations I and

200
00:14:42,500 --> 00:14:47,060
you have four of them that just went
crazy and basically we have an average

201
00:14:47,060 --> 00:14:52,260
of about a hundred millisecond
round-trip time on the vast majority of

202
00:14:52,260 --> 00:14:57,510
the cases and then in four cases you
have it go up to 400 + so this is

203
00:14:57,510 --> 00:15:02,120
something we saw in all of the data as I
said this was on a local network a local

204
00:15:02,120 --> 00:15:07,760
vmi only two hops away and you still get
this type of these type of data when you

205
00:15:07,760 --> 00:15:12,370
actually go over the Internet to a web
app that is further away it gets much

206
00:15:12,370 --> 00:15:18,630
worse so looking at the data on this
dataset anyway it was about 1% one to

207
00:15:18,630 --> 00:15:24,459
two percent of the time the data points
were extreme in this way now when you

208
00:15:24,459 --> 00:15:30,500
this this is just one one half of the
paper if you actually want to look at

209
00:15:30,500 --> 00:15:32,990
the other half this is

210
00:15:32,990 --> 00:15:38,790
all pairwise data here round trip times
if you take the differences this is the

211
00:15:38,790 --> 00:15:42,180
type of data you're dealing with this is
the thing that we wanted to deal with

212
00:15:42,180 --> 00:15:46,520
and that does improve the analysis and
the end but it is still extremely

213
00:15:46,520 --> 00:15:52,959
difficult to use so what these type of
data the the problems they cause is that

214
00:15:52,959 --> 00:15:58,239
you can't use standard measures like to
test for differences means-test medians

215
00:15:58,240 --> 00:16:04,540
we tested don't work out all that well
so what we wanted to do is come up with

216
00:16:04,540 --> 00:16:12,410
some way to filter the data and then use
other more robust measures of central

217
00:16:12,410 --> 00:16:16,550
tendency in order to uncover the
differences in round trip time in these

218
00:16:16,550 --> 00:16:26,589
pairwise data so the tool that we went
to first is something called the common

219
00:16:26,589 --> 00:16:31,089
filter and if any of you have ever done
signal processing it's a it's a filter

220
00:16:31,089 --> 00:16:37,560
that is used in signal processing quite
often it does a great job under a wide

221
00:16:37,560 --> 00:16:43,800
variety of cases of smoothing things out
so for example if we apply that the

222
00:16:43,800 --> 00:16:47,410
Kalman filter this move there too that
round trip time date of differences and

223
00:16:47,410 --> 00:16:51,569
round-trip time data this is what we get
under one simple specification the

224
00:16:51,570 --> 00:16:57,920
common filter so you can see here across
those 500 samples that really the Kalman

225
00:16:57,920 --> 00:17:02,300
filter did smooth out the spikes but it
left him in because we're the idea is

226
00:17:02,300 --> 00:17:06,369
that those spikes have information in
them but we don't want to take them at

227
00:17:06,369 --> 00:17:11,099
face value we want to control for the
fact that they are so far their their

228
00:17:11,099 --> 00:17:20,530
extreme outliers so the next day we
wanted to try where a series of robust

229
00:17:20,530 --> 00:17:27,540
estimators and we started with a box
test which Meijer and myron Sandlin sand

230
00:17:27,540 --> 00:17:29,010
in last year

231
00:17:29,010 --> 00:17:34,230
discussed here a black hat in the box
test is actually quite simple if you

232
00:17:34,230 --> 00:17:40,660
take the picture distributions and you
take two particular quantiles are spaces

233
00:17:40,660 --> 00:17:45,850
so here I think it's this 678 quantiles
a percent quantiles

234
00:17:45,850 --> 00:17:51,260
these distributions if these two boxes
here the blue and pink boxes if they do

235
00:17:51,260 --> 00:17:55,280
not overlap which is the case in this
this plot if they don't have lab then

236
00:17:55,280 --> 00:17:59,090
you can say these distributions are
different so you can say well round trip

237
00:17:59,090 --> 00:18:03,580
times the computation that these round
trip times represent our different

238
00:18:03,580 --> 00:18:11,120
across these two test cases however if
they do overlap you say there's probably

239
00:18:11,120 --> 00:18:12,809
no real difference between them

240
00:18:12,809 --> 00:18:20,490
between the two cases so we end
preliminary tests using the Kalman

241
00:18:20,490 --> 00:18:26,010
filter not using the common filter read
founded the box test under certain cases

242
00:18:26,010 --> 00:18:31,000
in our data using the new data
collection technique didn't work that

243
00:18:31,000 --> 00:18:35,700
well under in many cases so we are
looking for other things and we turn to

244
00:18:35,700 --> 00:18:39,710
something called El estimators and we
went to alma mater's for a couple

245
00:18:39,710 --> 00:18:45,260
reasons the first is that extremely easy
to understand and second they're very

246
00:18:45,260 --> 00:18:51,250
easy and quick to compute simply what
what NL estimator is is it's an

247
00:18:51,250 --> 00:18:59,240
estimator off of the quantile so here I
think these are the 25% quantiles above

248
00:18:59,240 --> 00:19:04,100
and below the median and you take the
values from those quantiles and then you

249
00:19:04,100 --> 00:19:10,439
average and that is your measure of
central tendency this can be extended in

250
00:19:10,440 --> 00:19:15,799
this case it's called the mid hinge
because twenty-five percent 25%

251
00:19:15,799 --> 00:19:23,789
quantiles but you can generalize this
too taking four points or we also tried

252
00:19:23,789 --> 00:19:26,908
taking seven points in this this one's a
little bit different in the sense that

253
00:19:26,909 --> 00:19:30,860
we actually use the median and the
calculation as well as those three

254
00:19:30,860 --> 00:19:38,040
points off to the right and left so

255
00:19:38,040 --> 00:19:42,820
taking these new estimators and the
common filter we needed to then figure

256
00:19:42,820 --> 00:19:46,429
out whether or not they were actually
performing better than the box deaths

257
00:19:46,430 --> 00:19:50,730
which ones perform better under certain
circumstances and whether or not the

258
00:19:50,730 --> 00:19:56,930
common filters actually gonna help us
we've analyzed these type of data so we

259
00:19:56,930 --> 00:20:03,310
performed a Monte Carlo analysis across
four test scenarios and here you concede

260
00:20:03,310 --> 00:20:10,490
that we have to local computers 1 p.m.
one is a physical computer we have p.m.

261
00:20:10,490 --> 00:20:16,830
a remote being and a remote physical
computer and then we also have two

262
00:20:16,830 --> 00:20:22,260
different operating system what's
important for the Monte Carlo analysis

263
00:20:22,260 --> 00:20:29,400
is that what we've done is we've taken
five different Delta so differences in

264
00:20:29,400 --> 00:20:35,270
the nanosecond differences and then
we've taken two hundred and around $250

265
00:20:35,270 --> 00:20:41,170
in pairs each test case of their 17 test
cases in all and what does led to is

266
00:20:41,170 --> 00:20:45,920
about eight and a half million different
individual probes are crossed all of

267
00:20:45,920 --> 00:20:53,860
these test cases as I said the objective
here was to test all of these estimators

268
00:20:53,860 --> 00:21:00,740
and the Calman filter systematically
across a variety of cases and this is a

269
00:21:00,740 --> 00:21:05,770
pretty standard Monte Carlo analysis if
you've ever done these and statistics so

270
00:21:05,770 --> 00:21:12,370
for thirteen different values between
fifty and a hundred fifteen 10,000 we

271
00:21:12,370 --> 00:21:16,780
those are the size of the series we take
these we really want to know house short

272
00:21:16,780 --> 00:21:21,090
or how few samples we can take and still
get a good estimate we're trying to make

273
00:21:21,090 --> 00:21:24,439
a practical I don't do that I'm i do
research usually but Tim wants a

274
00:21:24,440 --> 00:21:30,260
practical so we want to know how little
how few data we can actually grab and

275
00:21:30,260 --> 00:21:34,300
come up with some kind of decent
estimate of whether or not the round

276
00:21:34,300 --> 00:21:40,389
trip times are actually different so
then 480 times we repeat we basically

277
00:21:40,390 --> 00:21:45,120
take a random sample apply the common
filter then we do run the tests on the

278
00:21:45,120 --> 00:21:49,580
filter data and the non filtered data
for the El estimators we actually did a

279
00:21:49,580 --> 00:21:51,220
bootstrap technique where we

280
00:21:51,220 --> 00:21:55,480
estimated on random samples of the
filter data filter data to come up with

281
00:21:55,480 --> 00:21:59,260
a distribution and what with all of this
did is it came up with a drug

282
00:21:59,260 --> 00:22:02,720
distribution of shares for every single
test case and every single

283
00:22:02,720 --> 00:22:09,950
parameterization across I said this
wasn't practical part rate across all of

284
00:22:09,950 --> 00:22:15,650
these test cases so there were a few
billion different estimates that we took

285
00:22:15,650 --> 00:22:24,309
so with this then we get things that
look kind of like this on the to this is

286
00:22:24,309 --> 00:22:32,030
a level plot or I guess what I call it
to plots on the left and in the middle

287
00:22:32,030 --> 00:22:37,580
are the raw their their average errors
each one of the squares as the average

288
00:22:37,580 --> 00:22:45,639
air within that premature ization of the
box test so on the left two plots the

289
00:22:45,640 --> 00:22:51,220
red means the box test is doing really
well so you can see in the center where

290
00:22:51,220 --> 00:22:55,990
we've applied the common filter the box
test does better across the broader

291
00:22:55,990 --> 00:23:00,970
range of parameterizations remember
going into any tests that you're going

292
00:23:00,970 --> 00:23:04,140
to do you don't actually know what the
privatization

293
00:23:04,140 --> 00:23:07,580
what is going to be best for that data
said in order to determine whether or

294
00:23:07,580 --> 00:23:14,939
not you have discovered a difference in
round trip times and then so that's on

295
00:23:14,940 --> 00:23:18,929
the left two plots and then on the far
right you see the improvement in the

296
00:23:18,929 --> 00:23:24,470
carbon filter and mixed up the coloring
here but in the far right won the Grey

297
00:23:24,470 --> 00:23:28,490
is where the common filter is done
better or the Kalman filter data has

298
00:23:28,490 --> 00:23:35,039
done better than the raw data with the
box test so of course we also apply this

299
00:23:35,039 --> 00:23:41,140
to the other the L estimators this is
the mid summary and parameterization of

300
00:23:41,140 --> 00:23:44,809
the midsummer is across the bottom and
the error rates are on the vertical axis

301
00:23:44,809 --> 00:23:52,668
you can see across a broad range the raw
data in this case did much better than

302
00:23:52,669 --> 00:23:58,309
the Kalman filter data with this
estimator we're actually able to just a

303
00:23:58,309 --> 00:24:00,428
thousand samples in this case

304
00:24:00,429 --> 00:24:04,460
well below 10 per or 5 percent error
rate all the way

305
00:24:04,460 --> 00:24:08,710
through you know about point three in
this case on the unfiltered data the raw

306
00:24:08,710 --> 00:24:12,910
or the filter data on the other hand
both won one-dimensional and

307
00:24:12,910 --> 00:24:19,110
two-dimensional common filter did not
perform very well but this wasn't a

308
00:24:19,110 --> 00:24:26,810
consistent result actually in a lot of
cases the common filter did improve the

309
00:24:26,810 --> 00:24:30,889
estimates here it's about seven a half
percent on average it's moved out the

310
00:24:30,890 --> 00:24:35,650
result and it made this estimator the
mid summary perform a little bit better

311
00:24:35,650 --> 00:24:41,740
but unfortunately this isn't good enough
for an analysis or to actually really

312
00:24:41,740 --> 00:24:46,200
determine we're talking a 40 percent
error rate here still even with the

313
00:24:46,200 --> 00:24:52,200
filter data so in summary the
statistical analysis the this is what

314
00:24:52,200 --> 00:24:56,610
we've gotten out of it I just to say we
have a hundred and eighty plots of the

315
00:24:56,610 --> 00:25:00,850
882 of these plots for each one of these
estimators so I can't really go through

316
00:25:00,850 --> 00:25:06,770
all of them here but on average what we
have found is the midsummer in

317
00:25:06,770 --> 00:25:12,379
particular and Tim's going to present
some other results performed better on

318
00:25:12,380 --> 00:25:18,980
average than the box tested over the
dataset that we collected the Kalman

319
00:25:18,980 --> 00:25:23,770
filter improves the hard cases when we
have very little data or very noisy data

320
00:25:23,770 --> 00:25:27,160
filter helps things little bit but not
to the level that we had really hoped it

321
00:25:27,160 --> 00:25:28,940
would

322
00:25:28,940 --> 00:25:34,850
the one caveat here is that this comment
filter was not trained in any way it was

323
00:25:34,850 --> 00:25:41,480
a parameterization that appeared to work
pretty well and some test cases and so

324
00:25:41,480 --> 00:25:42,410
we just stuck with it

325
00:25:42,410 --> 00:25:48,360
estimate through there if you actually
get down and train the carbon filter it

326
00:25:48,360 --> 00:25:53,479
might turn out to work a lot better than
it has in our results so far and then

327
00:25:53,480 --> 00:25:58,420
finally one thing that we're wanting to
look at in your future is that we want

328
00:25:58,420 --> 00:26:02,750
to try out other filters and other
weighting techniques and one thing we're

329
00:26:02,750 --> 00:26:06,230
thinking about is using a clustering
analysis to give particular weights two

330
00:26:06,230 --> 00:26:10,470
different pairwise differences in order
to calculate the round tryin differences

331
00:26:10,470 --> 00:26:15,570
so as I said this is not the practical
part you don't want to

332
00:26:15,570 --> 00:26:19,580
do all of this for every analysis so now
I'm gonna turn it back over to tim is

333
00:26:19,580 --> 00:26:23,810
going to talk about how these results
were integrated into the tool is

334
00:26:23,810 --> 00:26:34,169
developed in order to make this
practical things Jason

335
00:26:34,170 --> 00:26:39,250
so Jason did all of that work really
heavy brute force statistical analysis

336
00:26:39,250 --> 00:26:43,680
on a huge computing cluster just really
nice but now we need a tool that we can

337
00:26:43,680 --> 00:26:49,720
use day today so we created this little
thing we're coming now known the goal of

338
00:26:49,720 --> 00:26:54,700
it is to first of all identify timing
differences in in web applications but

339
00:26:54,700 --> 00:26:57,970
also to quantify the risk you need to
understand how many samples you need to

340
00:26:57,970 --> 00:27:02,170
collect to distinguish those timing
differences and that will tell you as a

341
00:27:02,170 --> 00:27:05,710
pen tester or an attacker how much time
it's going to take to actually explain

342
00:27:05,710 --> 00:27:10,050
something and then also help out with
the exploitation he's about to create

343
00:27:10,050 --> 00:27:14,909
perfect concept it's definitely work in
progress still and it's not very

344
00:27:14,910 --> 00:27:19,220
user-friendly so it might be awhile
before that is easier to use for the

345
00:27:19,220 --> 00:27:23,810
average investor but this is the
workflow for now known just real briefly

346
00:27:23,810 --> 00:27:29,490
at the beginning after attempting stage
where you to sort of instruct the tool

347
00:27:29,490 --> 00:27:33,180
what your test cases are if you have a
valid social security number and invalid

348
00:27:33,180 --> 00:27:37,320
one or perhaps you are more than two of
those test cases and then you need to

349
00:27:37,320 --> 00:27:42,020
instructed on what you know what the
HttpRequest look like but where does

350
00:27:42,020 --> 00:27:45,020
that day to get embedded in the request
is it in the post body or in the URL

351
00:27:45,020 --> 00:27:48,520
what have you learned from that point
the next four stages are pretty much

352
00:27:48,520 --> 00:27:53,830
automated so you just run the collection
scripts it grabs a whole bunch of

353
00:27:53,830 --> 00:27:57,939
samples as a as a baseline so you can
actually learning things about the

354
00:27:57,940 --> 00:28:02,100
timing differences packet analysis is
performed which

355
00:28:02,100 --> 00:28:05,120
zeroes in on the actual package you
wanna make sure your round trip times

356
00:28:05,120 --> 00:28:09,129
between now and then and then you go
through training stage so we came up

357
00:28:09,130 --> 00:28:12,390
with some training algorithms instead of
trying to brute force every possible

358
00:28:12,390 --> 00:28:16,970
parameterization for classifiers weekend
with algorithms that try to find the

359
00:28:16,970 --> 00:28:21,140
best parameters more quickly they may
not be perfect but but they seemed to

360
00:28:21,140 --> 00:28:25,810
work pretty well and then next we
actually test those those parameters

361
00:28:25,810 --> 00:28:29,270
Asian so once we come up with a set of
parameters for each classifier we test

362
00:28:29,270 --> 00:28:31,990
them are completely separate data set a
different portion of the collected data

363
00:28:31,990 --> 00:28:35,390
which is really important to do if you
if you try to test of your training data

364
00:28:35,390 --> 00:28:40,060
it doesn't work out well and then at
that point at the end of the testing

365
00:28:40,060 --> 00:28:45,610
stage you'll know how many samples you
need to get a certain error rate so will

366
00:28:45,610 --> 00:28:48,709
you need a thousand samples to get 5%
error do you need a hundred thousand

367
00:28:48,710 --> 00:28:52,370
samples to get that error and then after
that if you want you can also perform

368
00:28:52,370 --> 00:28:59,129
the attack was a little bit more about
the training and testing process because

369
00:28:59,130 --> 00:29:03,020
our training our algorithm may not be
perfect we actually trained each

370
00:29:03,020 --> 00:29:06,570
classifier multiple times and come up
with different parameterizations and

371
00:29:06,570 --> 00:29:10,159
then try them all out one of the reasons
for this is that if you train on

372
00:29:10,160 --> 00:29:14,380
different sample sizes different numbers
of observations you actually get very

373
00:29:14,380 --> 00:29:17,310
different results in the training
algorithms if you don't get enough data

374
00:29:17,310 --> 00:29:22,129
then the training out rooms tend to not
learn enough they get bad parameters if

375
00:29:22,130 --> 00:29:25,610
you give it too much data they tend to
over learn so they and this is a really

376
00:29:25,610 --> 00:29:29,250
common problem in machine learning they
tend to over train on the data and then

377
00:29:29,250 --> 00:29:32,720
they work really well in the training
set but then it won't work well on the

378
00:29:32,720 --> 00:29:37,050
test set so we try a bunch of different
sizes sample sizes in order to tease out

379
00:29:37,050 --> 00:29:43,389
which ones work best and we try all this
privatizations on the the test data and

380
00:29:43,390 --> 00:29:47,050
find out which ones work best and
throughout this process instead of just

381
00:29:47,050 --> 00:29:51,540
trying to find which classifier has the
lowest error for a given sample size

382
00:29:51,540 --> 00:29:56,490
we're actually doing a sort of it's not
exactly a binary search but some similar

383
00:29:56,490 --> 00:30:01,240
to a binary search to identify what
sample size is actually sufficient for a

384
00:30:01,240 --> 00:30:05,040
given classifier to perform the attack
and then whichever one has the lowest

385
00:30:05,040 --> 00:30:12,139
number of observations that's the winner
so so we can flip the problem around so

386
00:30:12,140 --> 00:30:17,160
I also performed a Monte Carlo analysis
against all of the the same data that

387
00:30:17,160 --> 00:30:20,870
Jason did this is a completely separate
implementation and we came up with the

388
00:30:20,870 --> 00:30:25,179
same overall results for the most part
the the Midsummer Eve

389
00:30:25,180 --> 00:30:29,240
summary or sub 2 summary classifiers do
the best use of the midsummer he does a

390
00:30:29,240 --> 00:30:35,630
little bit better and in this table what
it's showing is the in each cell the

391
00:30:35,630 --> 00:30:39,600
number of observations needed to make a
classification at five percent error

392
00:30:39,600 --> 00:30:46,010
beso lower is better in other in some
datasets we didn't have enough data we

393
00:30:46,010 --> 00:30:48,800
we capped at twenty thousand
observations because we don't feel about

394
00:30:48,800 --> 00:30:52,930
that is all the practical in most cases
and so in cases where the classifiers

395
00:30:52,930 --> 00:30:57,230
could not get down 25 percent error we
just include the the error rate the best

396
00:30:57,230 --> 00:31:01,710
error rate that it achieved so anyway if
you if you look through this you can see

397
00:31:01,710 --> 00:31:05,590
that most of the time midsummer summary
was best there is one case where the box

398
00:31:05,590 --> 00:31:09,649
test actually one and we're not sure why
that was a consistent result it's just

399
00:31:09,650 --> 00:31:13,620
that day to happen to be good for the
box test but in a known you know it's

400
00:31:13,620 --> 00:31:17,250
it's trying all of these out and figure
out what works best for you so you're

401
00:31:17,250 --> 00:31:23,020
always working with the best classifier
for your dataset alright now time to do

402
00:31:23,020 --> 00:31:32,320
a little demo

403
00:31:32,320 --> 00:31:43,570
we created a ok so we created
intentionally vulnerable application is

404
00:31:43,570 --> 00:31:47,490
designed to sort of memory that
registration site I showed earlier and

405
00:31:47,490 --> 00:31:51,190
so you were supposed to know your own
membership I D and your social last four

406
00:31:51,190 --> 00:31:54,350
of your social security number and if
you enjoy those correctly then you can

407
00:31:54,350 --> 00:32:00,120
sign up for an account so this
application has a timing difference

408
00:32:00,120 --> 00:32:03,518
Senate and you think well why does it
why is there a time difference at all

409
00:32:03,519 --> 00:32:06,669
you know you could just do a single
single curry to pull up and check both

410
00:32:06,669 --> 00:32:10,029
fields at once and there would be no
time difference between the member I D

411
00:32:10,029 --> 00:32:13,740
and the social security field but in
this application and hypothetical

412
00:32:13,740 --> 00:32:18,419
scenario the developer decided to be a
good idea to encrypt the social security

413
00:32:18,419 --> 00:32:21,740
number field in the database as a field
level encryption so you can't really

414
00:32:21,740 --> 00:32:25,960
search for that value so in this
implementation what happens is the

415
00:32:25,960 --> 00:32:30,080
member I D looks up the record and after
the record of successfully found their

416
00:32:30,080 --> 00:32:33,730
daddy clips that field of course if
there's no member idea that matches then

417
00:32:33,730 --> 00:32:37,049
you can have a failed to decrypt so
that's where the time difference comes

418
00:32:37,049 --> 00:32:42,000
in with this is showing right now is
just that

419
00:32:42,000 --> 00:32:44,149
demonstrating the different error
messages you get depending on the

420
00:32:44,149 --> 00:32:48,678
different cases if you enter a valid
when Brady an invalid as a sin you get

421
00:32:48,679 --> 00:32:52,679
the same error as if you entered an
invalid number I D but once you get past

422
00:32:52,679 --> 00:32:56,460
that stage and and and put both fields
correct you get a different error

423
00:32:56,460 --> 00:33:05,980
message if you screw up the other fields

424
00:33:05,980 --> 00:33:14,370
just gonna register here ok so we set up
a script in the script it just very easy

425
00:33:14,370 --> 00:33:18,370
to see but we just need to instructed on
what the test cases are about member I D

426
00:33:18,370 --> 00:33:22,360
and involvement Brady and then we also
need to get information about the HTTP

427
00:33:22,360 --> 00:33:27,449
request headers to send where to embed
that parameter in the request itself and

428
00:33:27,450 --> 00:33:29,880
that's pretty much it that's all the
code that you get the right to get to

429
00:33:29,880 --> 00:33:37,870
get the collections group started in
here is running the tool to collect the

430
00:33:37,870 --> 00:33:42,639
data initially there's a little bit He
timestamp information collected but that

431
00:33:42,640 --> 00:33:47,690
isn't really isn't used at this point
and it gives you a good estimate of how

432
00:33:47,690 --> 00:33:50,950
long it's going to take to finish your
collection stage which is really useful

433
00:33:50,950 --> 00:33:58,030
to estimate how long the attack will
take later on and then at the bottom it

434
00:33:58,030 --> 00:34:01,440
gives you a little bit of debugging
information about the the way in which

435
00:34:01,440 --> 00:34:04,700
the packet analysis was performed which
packets were actually used to measure

436
00:34:04,700 --> 00:34:08,710
the round-trip time and it also shows
you what the the Delta was what the

437
00:34:08,710 --> 00:34:12,639
estimated time difference between the
test cases was in this case it's about

438
00:34:12,639 --> 00:34:18,200
forty microseconds and then from that
point you just take that database

439
00:34:18,199 --> 00:34:23,779
database that has all the data and user
on the train escaped over it you can see

440
00:34:23,780 --> 00:34:27,379
here for each of the classifiers it's
running with a different number of

441
00:34:27,379 --> 00:34:31,668
observations on half of the date of the
training data and so does a different

442
00:34:31,668 --> 00:34:34,469
number of observations throughout and
then it learns a different set of

443
00:34:34,469 --> 00:34:38,989
parameters for each of those runs based
on the different numbers of observations

444
00:34:38,989 --> 00:34:44,589
and then after all of those are done it
tries each of the different

445
00:34:44,590 --> 00:34:49,980
parameterizations against the test data
and a lot of these fail miserably on the

446
00:34:49,980 --> 00:34:55,530
test data they just don't work they were
bad parameters and the way that we zero

447
00:34:55,530 --> 00:34:59,420
in on the number of samples required as
we start off with the number of samples

448
00:34:59,420 --> 00:35:04,600
that was trained on initially and then
we just increase that quickly based on

449
00:35:04,600 --> 00:35:08,560
how bad the error is and once we finally
reached a number of observations that is

450
00:35:08,560 --> 00:35:12,490
that she leaves less than five percent
error then we start to work our way back

451
00:35:12,490 --> 00:35:15,689
we just kind of work our way back until
the arrogance about 5% again

452
00:35:15,690 --> 00:35:20,920
then we stop there and say whichever one
was best and underneath 5% is is the

453
00:35:20,920 --> 00:35:25,109
winner for that particular set of
parameters and then later on we compare

454
00:35:25,109 --> 00:35:29,069
all these parameters together for all
the classifiers and the privatizations

455
00:35:29,069 --> 00:35:34,099
and at the bottom here it gives you a
summary the results so in this

456
00:35:34,099 --> 00:35:38,250
particular dataset against this sample
against this potentially vulnerable how

457
00:35:38,250 --> 00:35:44,140
the quad summary crossfire actually 18
only needed 231 observations to reliably

458
00:35:44,140 --> 00:35:47,560
classify and then from this point you
know based on the number of observations

459
00:35:47,560 --> 00:35:51,980
and how long it took to collect the data
initially you know how long it's gonna

460
00:35:51,980 --> 00:35:56,030
take to perform the attack one thing to
keep in mind about these attacks is that

461
00:35:56,030 --> 00:35:59,780
even though it only takes two hundred
and so observations to distinguish the

462
00:35:59,780 --> 00:36:03,020
time difference you still need a
brute-force upholding the values right

463
00:36:03,020 --> 00:36:07,190
you to enter an overall Asus and ideas
are all member IDs so you know that's

464
00:36:07,190 --> 00:36:10,640
multiplied by many thousand times so
very quickly you can get into the

465
00:36:10,640 --> 00:36:16,020
millions of observations required to
perform the real attack here this the

466
00:36:16,020 --> 00:36:20,950
next attack script is running and all it
does is it tries to brute force the or

467
00:36:20,950 --> 00:36:25,270
determine if a member idea is correct
based on timing information and then if

468
00:36:25,270 --> 00:36:28,650
it thinks the member idea is correct
then it tries to brute force the second

469
00:36:28,650 --> 00:36:32,380
field and it doesn't need to do timing
attack in that case so overall them

470
00:36:32,380 --> 00:36:37,650
jumping ahead to the end this takes a
while to run it's not too bad it's about

471
00:36:37,650 --> 00:36:47,550
25 seconds for each failed attempt

472
00:36:47,550 --> 00:36:56,730
so after a while in this database I set
it up so that one in ten member ideas is

473
00:36:56,730 --> 00:37:01,040
valid and so it did find a few member
ideas that was about it and then then it

474
00:37:01,040 --> 00:37:05,330
brute force the SSN after that it was
successful but in some cases you do get

475
00:37:05,330 --> 00:37:10,680
false positives so down below we have
one member idea that it was about it but

476
00:37:10,680 --> 00:37:13,620
but after going through all social
security numbers it found that none were

477
00:37:13,620 --> 00:37:16,660
correct so you have to write your code
such that it can deal with those kinds

478
00:37:16,660 --> 00:37:20,109
of errors and you know either rejected
again or what have you but it can still

479
00:37:20,110 --> 00:37:21,480
speed up an attack quite a bit

480
00:37:21,480 --> 00:37:36,880
alright

481
00:37:36,880 --> 00:37:40,859
sorry

482
00:37:40,859 --> 00:37:46,038
alright so in conclusion what we've
managed to come up with is improved data

483
00:37:46,039 --> 00:37:50,489
collection for packet-based round-trip
time estimation and we've also come with

484
00:37:50,489 --> 00:37:54,720
a more resilient classification methods
based on that better data and and then

485
00:37:54,720 --> 00:37:57,618
we provided to all that not only lets
you detect timing differences in

486
00:37:57,619 --> 00:38:03,339
applications but also assessed the risk
and then exploit the issue you might be

487
00:38:03,339 --> 00:38:07,150
wondering how you avoid these problems
while the most the easiest most direct

488
00:38:07,150 --> 00:38:10,640
way to do that is to eliminate your
timing differences and that sounds easy

489
00:38:10,640 --> 00:38:13,960
it seems like well if I just write my
code so that always takes the same

490
00:38:13,960 --> 00:38:17,549
amount of time that it will be good but
sometimes it's really tricky like in the

491
00:38:17,549 --> 00:38:22,319
case of the decryption issue in in the
sample application there what do you do

492
00:38:22,319 --> 00:38:26,220
with the data is encrypted in there and
i cant query for it so it can be tricky

493
00:38:26,220 --> 00:38:31,009
and another thing that people get caught
with is when your code is compiled a lot

494
00:38:31,009 --> 00:38:34,640
of times the compiler will reorder
instructions will take out code that is

495
00:38:34,640 --> 00:38:37,710
unnecessary and that will actually
introduced timing differences were you

496
00:38:37,710 --> 00:38:40,769
don't think they exist so I think it's
actually really critical to be able to

497
00:38:40,769 --> 00:38:45,038
test so if you're looking at an appt you
think there is a time difference and you

498
00:38:45,039 --> 00:38:46,900
change the code so there's not any more

499
00:38:46,900 --> 00:38:51,349
you need to test that because it could
easily not behave the way you expect if

500
00:38:51,349 --> 00:38:55,339
you are dealing with an application
where the end users involved they

501
00:38:55,339 --> 00:38:58,410
actually have to interact with that
interface than an easy way to mitigate

502
00:38:58,410 --> 00:39:03,848
is to capture on the page with a capture
you simply vastly reduce the number of

503
00:39:03,849 --> 00:39:07,940
requests an attacker can the number of
samples an attacker can obtain so it's

504
00:39:07,940 --> 00:39:12,150
not really practically exploitable in
that case but a lot of times it's

505
00:39:12,150 --> 00:39:16,079
difficult to do if you're dealing with
an API between servers there's no human

506
00:39:16,079 --> 00:39:21,670
involved then now you have you have this
issue that you can capture so that's all

507
00:39:21,670 --> 00:39:25,430
we've got

508
00:39:25,430 --> 00:39:43,509
questions there are some down there

509
00:39:43,510 --> 00:40:05,790
reliable for cases that we primarily
tested for the statistical analysis one

510
00:40:05,790 --> 00:40:10,830
was a 12 of them were virtual machines
and two of them were physical machines

511
00:40:10,830 --> 00:40:17,430
one was to a member of the internet and
two of them are not one of them was in

512
00:40:17,430 --> 00:40:20,720
windowed 11:02 p.m.

513
00:40:20,720 --> 00:40:25,720
what was the other one your place so a
lot of the data was actually pretty

514
00:40:25,720 --> 00:40:28,870
noisy I think you could do better with
you know better

515
00:40:28,870 --> 00:40:32,060
pipes between your sites and certainly
previous researchers have talked about

516
00:40:32,060 --> 00:40:37,350
you know look at my bad I'm ABC or bad
connection but you can always ran 2 p.m.

517
00:40:37,350 --> 00:40:41,819
somewhere and get closer to the target
in order to do the attack and that's

518
00:40:41,820 --> 00:40:45,720
certainly possible but we didn't we
didn't do enough different situations to

519
00:40:45,720 --> 00:40:54,649
really compare that ranks last year last
year at DEFCON there was a target at a

520
00:40:54,650 --> 00:41:05,490
cordon compare eg devices your about
that yeah yeah ok though because because

521
00:41:05,490 --> 00:41:11,350
he had a lot of details about like how
to get exact timing medalist network

522
00:41:11,350 --> 00:41:17,790
driver things like that and do you have
any practical like real web applications

523
00:41:17,790 --> 00:41:22,890
that can be attacked with your true love
or just like the one you created

524
00:41:22,890 --> 00:41:26,509
yourself so far just the artificial we
really thought that we'd have something

525
00:41:26,510 --> 00:41:31,360
in place that we could go out and took
over 90 days and the like but honestly

526
00:41:31,360 --> 00:41:34,410
the analysis it was so much more effort
than we thought it was going to be to

527
00:41:34,410 --> 00:41:37,830
get where we're at that we just simply
to have enough time to go after other

528
00:41:37,830 --> 00:41:43,590
applications yet I can certainly
confirmed that beat analysts takes a

529
00:41:43,590 --> 00:41:47,500
long time since the talk he was
referencing was mine last year did you

530
00:41:47,500 --> 00:41:50,210
guys do anything to control the jitter

531
00:41:50,210 --> 00:41:55,690
attacking machine yeah we did we did
most of the same stuff that my irons and

532
00:41:55,690 --> 00:42:00,210
ended with real-time and then doing CPU
affinity which actually is really

533
00:42:00,210 --> 00:42:04,380
important because when you switch when
your process which is from one party to

534
00:42:04,380 --> 00:42:07,320
another on your local machine the
real-time clocks are not in sync with

535
00:42:07,320 --> 00:42:10,520
each other and that will if it if it's
which is in the middle test screws it up

536
00:42:10,520 --> 00:42:16,830
did you turn off power savings well turn
out wide power saving that made the

537
00:42:16,830 --> 00:42:24,029
biggest difference for me really yeah I
do why didn't I don't mind she was going

538
00:42:24,030 --> 00:42:28,220
to power saving mode and all because it
was worth it was really slow and

539
00:42:28,220 --> 00:42:32,509
drinking and the family's going crazy
but maybe I thought it made a big

540
00:42:32,510 --> 00:42:35,859
difference the other question was when
you were running the Monte Carlo subsets

541
00:42:35,859 --> 00:42:41,770
were using sequential lengths are you
using random samples

542
00:42:41,770 --> 00:42:45,710
sequential series you mean like they
were sequential

543
00:42:45,710 --> 00:42:51,869
yes the common filter as soon as you
have a time series right so yeah and so

544
00:42:51,869 --> 00:42:57,900
I we just kept the city has made the
comment filter on a series of data and

545
00:42:57,900 --> 00:43:01,500
then use that but and then we just
picked random samples from the two

546
00:43:01,500 --> 00:43:06,130
hundred fifty thousand pairs we had for
each case test case so so you didn't use

547
00:43:06,130 --> 00:43:16,880
sequential samples ran to the
subsequences random sequential okay and

548
00:43:16,880 --> 00:43:18,000
you anymore

549
00:43:18,000 --> 00:43:20,549
come talk to you

550
00:43:20,549 --> 00:43:23,989
all right thank you very much

