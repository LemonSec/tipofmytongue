1
00:00:00,000 --> 00:00:03,220
defeating machine learning what your
security vendors not telling you with

2
00:00:03,220 --> 00:00:07,020
Bob Klein and Ryan Peters if you could
please put your phone's on vibrate

3
00:00:07,020 --> 00:00:10,790
silent or turn them up altogether it
would be really appreciated those calls

4
00:00:10,790 --> 00:00:14,820
can just go to voicemail thank you

5
00:00:14,820 --> 00:00:18,740
morning I know it's the last day to
conference I'm sure your nursing

6
00:00:18,740 --> 00:00:24,669
hangovers or at this point hunger we
perceive coming out my name is Bob Klein

7
00:00:24,670 --> 00:00:31,910
is Ryan Peters machine learning right
she now has become quite a buzz word in

8
00:00:31,910 --> 00:00:35,930
many vendors are touting it as the
ultimate defense of technology and we

9
00:00:35,930 --> 00:00:39,840
should be excited right machine learning
is no longer relegated to a laboratory

10
00:00:39,840 --> 00:00:44,320
experiment to some neat idea but rather
technology it's in use across many

11
00:00:44,320 --> 00:00:49,890
industries and made appointments with
great success right there is no

12
00:00:49,890 --> 00:00:53,140
exception and give you a little
background of wire interested in this

13
00:00:53,140 --> 00:00:57,860
many you may not be familiar with doctor
but for over five years now are

14
00:00:57,860 --> 00:01:01,180
particular team has been working to find
to machine learning approach to

15
00:01:01,180 --> 00:01:06,130
file-based network threats we've learned
a lot of scar tissue in the process but

16
00:01:06,130 --> 00:01:09,130
are these five years we've been able to
take our approach and operationalize it

17
00:01:09,130 --> 00:01:12,750
in last 18 months we've actually
instantiate two dozen the deployments in

18
00:01:12,750 --> 00:01:17,040
the government commercial space ranging
several thousand users over $300,000

19
00:01:17,040 --> 00:01:21,150
deployment main point here is not to
boast but just to convey that were

20
00:01:21,150 --> 00:01:25,040
painfully aware that simply applying
machine learning to a problem does not

21
00:01:25,040 --> 00:01:31,180
magically solve it in attempting to make
it viable operationally so today we

22
00:01:31,180 --> 00:01:34,520
won't talk about some of what we've
learned from our experiences and focus

23
00:01:34,520 --> 00:01:37,100
on a specific issue that machine
learning fails to address is currently

24
00:01:37,100 --> 00:01:44,630
implemented without a solution and
general so we'll start by reviewing the

25
00:01:44,630 --> 00:01:49,560
evolution security industry and how
machining fits into it with the security

26
00:01:49,560 --> 00:01:52,840
landscape looks like an attacker's
perspective and show how some machine

27
00:01:52,840 --> 00:01:56,860
learning approaches can be defeated
using fairly simple techniques will then

28
00:01:56,860 --> 00:02:01,000
review what went wrong as defenders
propose a solution that introduces

29
00:02:01,000 --> 00:02:04,850
significant uncertainty and risk to the
attacker will explore this new paradigm

30
00:02:04,850 --> 00:02:10,829
first in concept then in practical
implementation operation and finally in

31
00:02:10,830 --> 00:02:12,040
quantitative results

32
00:02:12,040 --> 00:02:16,439
visit the demo with the solution
implemented and see the landscape is

33
00:02:16,439 --> 00:02:19,640
changing the attackers respective will
conclude with additional impact the

34
00:02:19,640 --> 00:02:23,369
analyst and how we can apply this
technique moving forward so to begin

35
00:02:23,370 --> 00:02:27,299
with a quick review this is basically
what the security industry look like

36
00:02:27,299 --> 00:02:30,659
thirty years ago right early early
attacks on I need to work hard to wreak

37
00:02:30,659 --> 00:02:35,129
havoc and so they didn't early solutions
produced by the security industry were

38
00:02:35,129 --> 00:02:40,480
similarly basic rate we look at early
creatures in this so much like evolution

39
00:02:40,480 --> 00:02:45,220
this sort of tit-for-tat adversarial
relationship has persisted over the

40
00:02:45,220 --> 00:02:51,790
years early creatures in this evolution
such as our signature amoeba here we're

41
00:02:51,790 --> 00:02:52,970
very simple right

42
00:02:52,970 --> 00:02:57,579
early detection engines for example
typically look for exact hash matches

43
00:02:57,579 --> 00:03:01,900
and 1st gen stateless packet filters
were essentially limited to exact

44
00:03:01,900 --> 00:03:06,970
matching on source dest address port and
protocol and while a major step forward

45
00:03:06,970 --> 00:03:11,299
from basically nothing these solutions
chair weakness in their total

46
00:03:11,299 --> 00:03:15,939
brittleness right now is apparent as
adversaries evolved to evade ABC get

47
00:03:15,939 --> 00:03:18,948
your face detection for example by
treating even a single bite in their

48
00:03:18,949 --> 00:03:23,569
malware so using enterprises suffer
attacks from such techniques and

49
00:03:23,569 --> 00:03:27,599
demanded better and so once again the
security industry obliged operating

50
00:03:27,599 --> 00:03:31,260
heuristics to detect common malicious
indicators and samples sandboxes to

51
00:03:31,260 --> 00:03:36,500
detect malicious actions taken by
samples and execution and so on but once

52
00:03:36,500 --> 00:03:41,810
again while the major step forward and a
reduction in Vilnius compared to

53
00:03:41,810 --> 00:03:47,190
previous solutions attackers want to get
exploited these problems with these as

54
00:03:47,190 --> 00:03:51,228
well and so users and enterprises now
demanding additional protection and

55
00:03:51,229 --> 00:03:55,699
security industry has to evolve again to
supply it so where does that leave us

56
00:03:55,699 --> 00:04:01,699
today right so many many would say that
we are here right that machine learning

57
00:04:01,699 --> 00:04:05,870
this technology from the future with a
power that may mark the beginning of the

58
00:04:05,870 --> 00:04:07,989
end for humankind and on and on

59
00:04:07,989 --> 00:04:12,590
solves all problems and completely
unbreakable but in reality you're

60
00:04:12,590 --> 00:04:15,610
probably a little bit closer to here
right

61
00:04:15,610 --> 00:04:19,049
there's a lot of evidence that shows the
brittleness it is earlier approaches

62
00:04:19,048 --> 00:04:23,489
have been improved to learn can generate
rich and robust descriptions of malware

63
00:04:23,490 --> 00:04:29,070
for example so does this make sense

64
00:04:29,070 --> 00:04:35,040
imagined I think actually helped
interesting analogy actually to explain

65
00:04:35,040 --> 00:04:35,970
what we mean by this

66
00:04:35,970 --> 00:04:42,480
so imagine each user enterprise as a
building great summer big and some are

67
00:04:42,480 --> 00:04:45,910
small but i wanna keep malicious actors
out right here in this could explain

68
00:04:45,910 --> 00:04:50,550
sort of what the issue is with machine
learning right so we have these houses

69
00:04:50,550 --> 00:04:56,010
and we must keep most actors out so they
go to c-town locks door let's say and

70
00:04:56,010 --> 00:05:00,320
buy a lock right which represents our
security solution as much as actor

71
00:05:00,320 --> 00:05:03,990
looking to break into 12 juicier targets
say one of the bigger office buildings

72
00:05:03,990 --> 00:05:09,669
and we can ensure a swift silent and
undetected entry by knowing how to break

73
00:05:09,669 --> 00:05:13,760
the lock before we show up right what
are the most common signature-based

74
00:05:13,760 --> 00:05:18,120
deployment paradigm all environments
receive identical signatures right in

75
00:05:18,120 --> 00:05:22,150
other words the locksmith cells
identical blocks to everyone each

76
00:05:22,150 --> 00:05:26,030
buildings protected by an identical lock
in most cases a malicious actor and even

77
00:05:26,030 --> 00:05:32,229
just go to the lock store and buy a copy
of the lock himself and he can't and

78
00:05:32,229 --> 00:05:36,400
he's dedicated which he probably is
going after one of those bigger office

79
00:05:36,400 --> 00:05:37,049
buildings

80
00:05:37,050 --> 00:05:41,500
campaign went through other means he
takes a lot to his basement and as much

81
00:05:41,500 --> 00:05:44,979
time as he wants picking the lock for
your time constraints when he figures it

82
00:05:44,979 --> 00:05:49,960
out he still trapped in a locked office
building and walk inside right so this

83
00:05:49,960 --> 00:05:53,330
image should be terrifying right that
the point is that in the security space

84
00:05:53,330 --> 00:05:57,330
is an attacker can easily verify that
exploit works against the target

85
00:05:57,330 --> 00:06:01,909
protected by such a solution because all
deployment identical all he has to do is

86
00:06:01,910 --> 00:06:05,210
pain any copy of the centerpiece product
and arrayed against it and how to

87
00:06:05,210 --> 00:06:12,780
exploit until he knows that we defeated
right so he had a tax rate with his lock

88
00:06:12,780 --> 00:06:16,419
but those early solutions right what
about new heuristic approaches and

89
00:06:16,419 --> 00:06:19,780
sandboxes which you represent by the
stronger Lockyer and unfortunately we

90
00:06:19,780 --> 00:06:23,489
see a very similar picture right where
each deployment is the same the militias

91
00:06:23,490 --> 00:06:27,900
actor can obtain a copy of the sound of
the engine the sandbox software

92
00:06:27,900 --> 00:06:31,799
and iterate until he goes undetected
write a note that in all cases this is

93
00:06:31,800 --> 00:06:35,380
done without any detailed reverse
engineering of the inner workings of the

94
00:06:35,380 --> 00:06:39,789
security solution and really testing
against it is enough and so we see that

95
00:06:39,789 --> 00:06:42,599
this is this thread of this common
issues persist through many of these

96
00:06:42,600 --> 00:06:46,190
different technologies and so we ask
what then about machine learning right

97
00:06:46,190 --> 00:06:49,759
and unfortunately what your security
vendor may not be telling you is that

98
00:06:49,759 --> 00:06:53,280
some players are quite susceptible to
this type of attack because they rely on

99
00:06:53,280 --> 00:06:57,530
the same for deployment paradigm but to
really answer that question to me to be

100
00:06:57,530 --> 00:07:00,880
a bit more specific about what we mean
by machine learning right into that

101
00:07:00,880 --> 00:07:03,729
would take a quick look at how machine
learning applied across the security

102
00:07:03,729 --> 00:07:08,360
industry identify areas with a share
deployment paradigm is an issue so

103
00:07:08,360 --> 00:07:12,330
different security needs have found
different types of machine learning to

104
00:07:12,330 --> 00:07:15,090
be particularly suited to their
applications right and reality is that

105
00:07:15,090 --> 00:07:17,479
not all of them suffer from the same
weaknesses

106
00:07:17,479 --> 00:07:21,639
there's a lot of different ways to slice
the machine learning cybersecurity pie

107
00:07:21,639 --> 00:07:25,810
will be using this one here will be
distinguished between incremental

108
00:07:25,810 --> 00:07:30,699
algorithms which continuously integrate
new data versus batch algorithms that

109
00:07:30,699 --> 00:07:32,210
need to be trained all at once

110
00:07:32,210 --> 00:07:36,780
will also to see if it means to provide
algorithms which rely on a label with

111
00:07:36,780 --> 00:07:40,698
ground troops and unsupervised
algorithms which essentially attempt the

112
00:07:40,699 --> 00:07:45,680
fine structure from otherwise unlabeled
data surprised I want you to think of

113
00:07:45,680 --> 00:07:50,370
something like clustering so here to
examples of some of the security

114
00:07:50,370 --> 00:07:54,690
applications that have used in this case
unsupervised incremental algorithms so

115
00:07:54,690 --> 00:07:58,650
most of these attempts roughly to build
a profile of what normal looks like

116
00:07:58,650 --> 00:08:03,719
overtime right and that's that's the
incremental peace continuously using

117
00:08:03,720 --> 00:08:08,800
this data to figure out just normal
profile and the UN-supervised peace you

118
00:08:08,800 --> 00:08:12,220
can imagine it to deserve this cluster
of what we think normal looks like it's

119
00:08:12,220 --> 00:08:17,400
used to identify departure from normal
right anomalies ever traffic profiling

120
00:08:17,400 --> 00:08:21,000
spam filtering typically used to
supervise detrimental algorithms are you

121
00:08:21,000 --> 00:08:24,659
spam filtering example if you're
probably familiar with this users

122
00:08:24,659 --> 00:08:28,320
receive emails and label them as spam or
not span and so again that's the

123
00:08:28,320 --> 00:08:32,360
supervised component we have labeled
examples and then add machine learning

124
00:08:32,360 --> 00:08:36,230
out and continuously integrate those
samples and ideally learn

125
00:08:36,230 --> 00:08:41,730
something new about what spam emails
look like the task of clustering matter

126
00:08:41,730 --> 00:08:48,500
into families I just had to work lost so
did you sign on to provide over them and

127
00:08:48,500 --> 00:08:53,060
it's often Diamondbacks on large data
sets of malware and then last year we

128
00:08:53,060 --> 00:08:57,339
have our detection is almost exclusively
relied on supervised Patch Adams

129
00:08:57,340 --> 00:09:03,000
training is often done all at once on
large fixed corpses of known benign and

130
00:09:03,000 --> 00:09:08,850
malicious data so the sort of high-level
image of how machining been used in the

131
00:09:08,850 --> 00:09:13,040
security industry we can return this
problem to be identified before of these

132
00:09:13,040 --> 00:09:17,050
sort of shared model that is distributed
to everyone we know that this lower

133
00:09:17,050 --> 00:09:19,849
right quadrant is particularly worrisome

134
00:09:19,850 --> 00:09:23,280
our detection solutions are widely
deployed get training is often done

135
00:09:23,280 --> 00:09:27,579
exclusively in batch by the vendor which
then pushes out identical models to

136
00:09:27,580 --> 00:09:28,970
everyone

137
00:09:28,970 --> 00:09:32,620
focus on this piece for the remainder of
the talk starting with a demo from a tax

138
00:09:32,620 --> 00:09:36,990
perspective on how this deployment
paradigm is fundamentally flawed will

139
00:09:36,990 --> 00:09:40,340
see from our author in possession of a
copy of the shared model machine

140
00:09:40,340 --> 00:09:44,250
learning model is able to defeat to
target and so forth at alternative Ryan

141
00:09:44,250 --> 00:09:51,870
so i'm gona briefly describe the
experimental setup they were going to

142
00:09:51,870 --> 00:09:55,780
use her demo to the general workflow
we're going to be using is very simple

143
00:09:55,780 --> 00:10:00,740
as the coldest emma is the visualizer
concept not a specific exploit first

144
00:10:00,740 --> 00:10:05,730
will generate palos using Metasploit
Framework next we literally on how we

145
00:10:05,730 --> 00:10:10,160
encode these payloads is an arrangement
will vary the encoders are used in a

146
00:10:10,160 --> 00:10:14,829
number of passes of uterine colder in
addition many of the encoding methods

147
00:10:14,830 --> 00:10:19,720
involve some sort of standard randomness
will then embed these payloads in a

148
00:10:19,720 --> 00:10:24,240
standard when Windows calcutta template
and deploy the variant against target

149
00:10:24,240 --> 00:10:28,350
environments running Windows 7 to
measure effectiveness of our method of

150
00:10:28,350 --> 00:10:32,610
deployments the targets will be a
simulated spear phishing attack the two

151
00:10:32,610 --> 00:10:37,130
minutes late palos we're using for this
demo are the motor-voter reverse TCP and

152
00:10:37,130 --> 00:10:41,460
when when they binaries executed it
connects back to the attacker to create

153
00:10:41,460 --> 00:10:41,910
a couple

154
00:10:41,910 --> 00:10:46,110
perverse mature for sale as well as the
message box payload which displays a

155
00:10:46,110 --> 00:10:50,050
message that pops up in the binary
successfully executed obviously unless

156
00:10:50,050 --> 00:10:53,849
actor will not announce they've just
gotten into your system but we're going

157
00:10:53,850 --> 00:10:58,140
to use this for demonstration purposes
only in the presence of books say the

158
00:10:58,140 --> 00:11:01,569
presence of both of these activities
will be used to indicate a successful

159
00:11:01,570 --> 00:11:07,730
exploitation for antivirus software
using clam win because it is free and

160
00:11:07,730 --> 00:11:11,120
open source however we also mentioned
later the effectiveness against an

161
00:11:11,120 --> 00:11:16,880
aggregate of antivirus engines and
buyers total machine learning software

162
00:11:16,880 --> 00:11:21,189
we generate a model for this experiment
those trained on 20009 and 20,000

163
00:11:21,190 --> 00:11:25,270
militias samples the performance of the
classifier on the files were denied Tran

164
00:11:25,270 --> 00:11:30,480
Anh as shown below freezing to me and
metrics to measure our performance the

165
00:11:30,480 --> 00:11:33,930
first is the false-positive rate and
these are the benign events that were

166
00:11:33,930 --> 00:11:37,410
incorrectly marked as malicious and
these are events that caused particular

167
00:11:37,410 --> 00:11:41,520
heartache for analyst who must review
these events as though they were Mauer

168
00:11:41,520 --> 00:11:45,910
and particularly when transitioning away
from signature based approach which has

169
00:11:45,910 --> 00:11:49,780
nearly zero false positive rates as a
challenging feature of all probabilistic

170
00:11:49,780 --> 00:11:55,890
detection approaches as a subject will
revisit later in the talk the

171
00:11:55,890 --> 00:11:59,660
psychometric is false negatives and
these are the malicious events that were

172
00:11:59,660 --> 00:12:03,520
incorrectly marked as benign in other
words these on the power they would have

173
00:12:03,520 --> 00:12:08,130
slept through your defenses undetected
the performance numbers are some 5%

174
00:12:08,130 --> 00:12:11,650
about the false positives and false
negatives indicating this is a

175
00:12:11,650 --> 00:12:16,370
reasonably performing classifier for
this demo we are gonna make too many

176
00:12:16,370 --> 00:12:20,790
assumptions the first is the attacker
has obtained a copy of the Navy and the

177
00:12:20,790 --> 00:12:24,860
end all software and the stack it is
that the attacker does not have access

178
00:12:24,860 --> 00:12:29,020
to the exact senators or the model
itself in other words they have not been

179
00:12:29,020 --> 00:12:32,660
able to reverse engineer the same as
older model but instead treeless

180
00:12:32,660 --> 00:12:42,719
software as a black box so now will
transition into the first demo

181
00:12:42,720 --> 00:12:47,439
so we started the attackers lab where
the voice actor is running a variant of

182
00:12:47,439 --> 00:12:51,439
Linux with the Metasploit Framework
first we were going to run a script to

183
00:12:51,439 --> 00:12:56,930
generate the two payloads interpreter
reverse TCP and the message box we then

184
00:12:56,930 --> 00:13:01,149
added palos without applying any
encoding to our account IDFC template

185
00:13:01,149 --> 00:13:07,449
since we are going to be simulating a
spear phishing attack as a way to deploy

186
00:13:07,449 --> 00:13:12,258
these files to our target system will
move the new uncoated variance to our

187
00:13:12,259 --> 00:13:18,050
local is to TV web server the attacker
then opens up a multi hand they're using

188
00:13:18,050 --> 00:13:23,199
Metasploit the way for a seat to signal
F them our successfully execute its in

189
00:13:23,199 --> 00:13:29,949
this case we have set up a fake domain
name called my free count dot com

190
00:13:29,949 --> 00:13:33,878
on the target Windows seven bucks it's
running the antivirus are going to click

191
00:13:33,879 --> 00:13:37,730
on the spear phishing email to download
to a local system and we're going to

192
00:13:37,730 --> 00:13:43,670
scan the binary is and claim when and as
we see here and when did successfully

193
00:13:43,670 --> 00:13:48,110
detect them out where we can perform the
same demonstration on our target Windows

194
00:13:48,110 --> 00:13:54,809
7 system running our machine learning
model and has will see the machine

195
00:13:54,809 --> 00:13:58,579
learning model also successfully detects
an hour or so it's important to note

196
00:13:58,579 --> 00:14:03,079
that it was actor would not deploy them
our the stage now we're just showing

197
00:14:03,079 --> 00:14:06,128
this to establish a baseline for the
remainder of the demo the both the

198
00:14:06,129 --> 00:14:09,910
antivirus and machine learning solutions
that attacked the Metasploit generated

199
00:14:09,910 --> 00:14:17,980
power using only uncoated pilots

200
00:14:17,980 --> 00:14:22,580
we can now try this simple idea of
applying a single-pass chicagoan I to

201
00:14:22,580 --> 00:14:28,400
the palos before inserting it in our
county see them play it in chicago is a

202
00:14:28,400 --> 00:14:32,740
polymorphic X-ers encoder is very
commonly used as a first attempt in

203
00:14:32,740 --> 00:14:37,900
Metasploit to achieve a V evasion as I
did earlier will stage a new variant

204
00:14:37,900 --> 00:14:42,120
generated with that one person coating
on the payloads and our local HTTP web

205
00:14:42,120 --> 00:14:46,430
server in order to continue our
simulated spear phishing attack in

206
00:14:46,430 --> 00:14:49,609
addition we still have a reverse and
they're running on the attackers box and

207
00:14:49,610 --> 00:14:54,280
earlier in the demo so now let's see how
this new variant performs against the

208
00:14:54,280 --> 00:15:01,260
antivirus and as you can see there are
no detection by ClamWin so let's try and

209
00:15:01,260 --> 00:15:07,840
execute it and see what happens and we
are now and let's posture a few seconds

210
00:15:07,840 --> 00:15:12,020
the first thing we see is the message
box payload we had a name that user know

211
00:15:12,020 --> 00:15:13,189
they've been hacked

212
00:15:13,190 --> 00:15:16,680
we can also see on the left side in the
attackers environment that we

213
00:15:16,680 --> 00:15:24,140
successfully opened up our session with
the target server info demonstrates the

214
00:15:24,140 --> 00:15:28,500
connection by displaying information
about the target and we can also open up

215
00:15:28,500 --> 00:15:34,490
a command shell and basically have
complete control over that system we can

216
00:15:34,490 --> 00:15:40,210
then test the same area for Windows 7
system as a model for the main selling

217
00:15:40,210 --> 00:15:43,030
point for machine learning that is not
susceptible to the brittleness

218
00:15:43,030 --> 00:15:47,400
statements are based approaches and as
we see it as she learning model still

219
00:15:47,400 --> 00:15:51,540
successfully to tax them our therefore
we now established that is not take much

220
00:15:51,540 --> 00:15:55,760
work to defeat many antivirus solutions
but that machine learning is more

221
00:15:55,760 --> 00:16:02,310
resilient to the simple approaches so
what can we do to defeat the machine

222
00:16:02,310 --> 00:16:06,310
learning solution how about we try and
what's been done in the past and iterate

223
00:16:06,310 --> 00:16:10,530
and see if I can find a random variants
that manages to evade detection

224
00:16:10,530 --> 00:16:15,100
what we are going to do is run a script
is generational create a new variant of

225
00:16:15,100 --> 00:16:19,820
them our welfare in the encoders the
number of passes and also introducing a

226
00:16:19,820 --> 00:16:24,900
standard randomness into the binary will
then automatically deploy it to test

227
00:16:24,900 --> 00:16:26,459
virtual machine in the lab

228
00:16:26,460 --> 00:16:30,450
attempt to executed the script then
attempts to establish see to what the

229
00:16:30,450 --> 00:16:35,160
remote machine if the power is blocked
the connection timeout and the process

230
00:16:35,160 --> 00:16:39,410
starts again with the creation of a new
variants these operations continue until

231
00:16:39,410 --> 00:16:42,600
a successful connection is made and
which point we have found our goal in

232
00:16:42,600 --> 00:16:45,860
Burien

233
00:16:45,860 --> 00:16:49,630
for demonstration purposes be a spell of
the script as it typically takes part 23

234
00:16:49,630 --> 00:16:53,580
cycle surgery age no variants we
obviously didn't have time to run it in

235
00:16:53,580 --> 00:16:59,510
real time and as you can see it took
about 1,900 iterations to find that cold

236
00:16:59,510 --> 00:17:04,030
and variance which TalkTalk roughly 15
hours when we actually ran in the lab

237
00:17:04,030 --> 00:17:08,740
again will stay during your county
acceder a Stevie server for a phishing

238
00:17:08,740 --> 00:17:13,569
attempts and then we wouldn't ask them
our on our machine learning system as we

239
00:17:13,569 --> 00:17:21,020
have already shown that we can defeat
the antivirus solution and again we are

240
00:17:21,020 --> 00:17:25,680
in so we see that familiar message box
announce our success as well as an

241
00:17:25,680 --> 00:17:29,550
interpreter session that a successfully
opened back in the Tigers lab running

242
00:17:29,550 --> 00:17:32,990
system so they are now playing
information from our machine learning

243
00:17:32,990 --> 00:17:37,910
box again you can show that we can open
a command show the process took a lot

244
00:17:37,910 --> 00:17:42,460
longer than trying to defeat antivirus
the reality is machine learning can

245
00:17:42,460 --> 00:17:47,590
still be defeated so what went wrong

246
00:17:47,590 --> 00:17:51,260
how did these deployments and despite
one of them having advanced machine

247
00:17:51,260 --> 00:17:55,550
learning defenses well if you look at
the directions on the power and the

248
00:17:55,550 --> 00:18:00,000
Tigers lab we need a portrayal of what
is going on under the hood the antivirus

249
00:18:00,000 --> 00:18:03,500
solution is looking for an exact
signature is not difficult to commute

250
00:18:03,500 --> 00:18:07,700
the file until that signatures evaded in
our summary chart here we see that our

251
00:18:07,700 --> 00:18:12,640
antivirus solution we selected Slam win
was defeated with only one iteration of

252
00:18:12,640 --> 00:18:16,870
claim 1 $1,000,000 very sophisticated
antivirus the reality of sixty percent

253
00:18:16,870 --> 00:18:20,229
of all buyers total engines would have
been defeated with this one simple pass

254
00:18:20,230 --> 00:18:23,200
and encoding it's not difficult to
defeat the other ones with more

255
00:18:23,200 --> 00:18:27,760
iterations or more targeted
modifications to the malware machine

256
00:18:27,760 --> 00:18:30,520
learning solution on the other hand is
look at many different features and the

257
00:18:30,520 --> 00:18:34,960
file some are indicative of benign where
there's a malware depending on the

258
00:18:34,960 --> 00:18:39,919
tax and as I'm hours permeated the
classifier keys on different features in

259
00:18:39,919 --> 00:18:45,640
the file that taken together indicate a
final classification this makes it much

260
00:18:45,640 --> 00:18:49,419
more difficult evade or the attacker to
figure out what is going on under the

261
00:18:49,419 --> 00:18:54,190
hood but we see the still possible with
other Asians in our demo we show that

262
00:18:54,190 --> 00:18:56,809
talk about nineteen hundred of these
iterations to find a very at the

263
00:18:56,809 --> 00:19:01,299
disputed that defeated are specific
machine learning model three rd is the

264
00:19:01,299 --> 00:19:04,799
attacker has two main advantages against
the machine learning model that carried

265
00:19:04,799 --> 00:19:09,399
over from the antivirus deployment
paradigm the first is that they can be

266
00:19:09,399 --> 00:19:12,649
fairly confident that the model has not
changed since they obtain their top

267
00:19:12,649 --> 00:19:16,949
billing software many machine learning
vendors go for months between software

268
00:19:16,950 --> 00:19:22,159
updates due to the resilience of the
model against zero-day threats and the

269
00:19:22,159 --> 00:19:26,279
second is they can be confident that all
the targets how the same model if you

270
00:19:26,279 --> 00:19:30,409
can defeat one copy of the model you can
defeat all the copies of the main

271
00:19:30,409 --> 00:19:34,169
takeaway is that the attacker can obtain
a copy machine learning model all it

272
00:19:34,169 --> 00:19:37,500
takes is persistence to develop a
variable succeed I guess every

273
00:19:37,500 --> 00:19:46,320
environment that is the point that
software so I think it was returned to

274
00:19:46,320 --> 00:19:50,330
the lock analogy for me hear your
previous example that shows our

275
00:19:50,330 --> 00:19:54,629
traditional Maori detection engine and
so was saying that all it's really

276
00:19:54,630 --> 00:19:58,470
different now is that there's basically
a new locksmith in town right he makes

277
00:19:58,470 --> 00:20:03,320
new really robust locks under the brand
machine learning and they really are

278
00:20:03,320 --> 00:20:05,720
better than the old ones they are way
more robust and they're far less

279
00:20:05,720 --> 00:20:11,740
susceptible to lock picking right but
guess what he only sells identical locks

280
00:20:11,740 --> 00:20:18,940
to rate their all identical super fancy
locks and so our author go to the locks

281
00:20:18,940 --> 00:20:22,580
door just like before buy a copy of the
lock is to his basement gets a mountain

282
00:20:22,580 --> 00:20:24,559
dew and spends as much time as he wants

283
00:20:24,559 --> 00:20:27,879
picking the lock and once again once he
figures it out install right up and

284
00:20:27,880 --> 00:20:33,020
basically walk inside and so you may
still be a little skeptical saying you

285
00:20:33,020 --> 00:20:37,629
know no bomb you know my machine
learning operation classifiers are far

286
00:20:37,630 --> 00:20:41,990
more advanced right and and some extent
you may be right right we train the

287
00:20:41,990 --> 00:20:46,110
classifier specifically for this
experiment it's on the day using yours

288
00:20:46,110 --> 00:20:51,389
we're not using the day that we use for
our operation of classifiers but the

289
00:20:51,390 --> 00:20:54,920
purpose of this talk is not to be
specific about a certain attack vector

290
00:20:54,920 --> 00:21:00,650
or talk about a specifically robust type
of classifier the purpose of this talk

291
00:21:00,650 --> 00:21:05,090
is not to show you that may display can
be used to defeat machine learning as

292
00:21:05,090 --> 00:21:08,939
anyone familiar with the technology will
tell you it's simply not possible to

293
00:21:08,940 --> 00:21:14,010
achieve perfection right 0% false
negatives 0% false positives and if you

294
00:21:14,010 --> 00:21:17,980
accept this premise right to some will
always get through and the classifier is

295
00:21:17,980 --> 00:21:21,150
not perfect and complete undefeatable
and frankly it's naive to believe

296
00:21:21,150 --> 00:21:22,210
otherwise

297
00:21:22,210 --> 00:21:25,480
our office job is simply to find one of
those gaps in this target security

298
00:21:25,480 --> 00:21:30,070
solution right to be one of those false
negatives and by using a shared defense

299
00:21:30,070 --> 00:21:34,320
where our models are identical among all
of our deployments make the task of

300
00:21:34,320 --> 00:21:38,220
finding one of those gaps for a
dedicated attacker much easier right as

301
00:21:38,220 --> 00:21:42,570
any copy of this offer will do to serve
as a test bed for the real thing and we

302
00:21:42,570 --> 00:21:44,809
stayed in showed that with model in hand

303
00:21:44,809 --> 00:21:47,600
defeating a target through blinder
haitian is possible without reverse

304
00:21:47,600 --> 00:21:51,139
engineering by no means to preclude
doing so you can imagine for some of the

305
00:21:51,140 --> 00:21:55,010
more advanced classifiers you would want
to be doing some for more advanced

306
00:21:55,010 --> 00:21:58,390
reverse engineering text get through
them you can imagine attacker in

307
00:21:58,390 --> 00:22:03,419
possession of the models say sending in
P thirty two doubles for example making

308
00:22:03,419 --> 00:22:08,179
a small change perhaps using a function
call through again and seeing how the

309
00:22:08,179 --> 00:22:10,799
class player scores it differently for
these small changes and effectively

310
00:22:10,799 --> 00:22:14,260
through experimentation reverse
engineering a small relevant component

311
00:22:14,260 --> 00:22:20,530
of the classifier so anyway looking back
at our picture the situation is actually

312
00:22:20,530 --> 00:22:24,080
pretty scary when viewed in a tax
perspective and the problem appears

313
00:22:24,080 --> 00:22:29,020
obvious way to kind of jumped out at you
in the solution follows naturally great

314
00:22:29,020 --> 00:22:34,210
and the answer is not just getting
stronger locks R and I'll be that the

315
00:22:34,210 --> 00:22:39,230
answer is it's not just getting stronger
locks in this is why we emphasized are

316
00:22:39,230 --> 00:22:41,040
using Metasploit earlier

317
00:22:41,040 --> 00:22:44,428
unclassifiable typically stronger than
another you can really just look at his

318
00:22:44,429 --> 00:22:49,110
picture right I don't care if you're
using a retinal scanner to protect your

319
00:22:49,110 --> 00:22:53,600
your house this is a systemic issue so

320
00:22:53,600 --> 00:22:58,070
as a defender the blind desire to attain
a stronger La Casita mentality to get

321
00:22:58,070 --> 00:23:02,490
into but it misses this true landscape
in the attack respective right so she

322
00:23:02,490 --> 00:23:06,080
said i mean obviously we working machine
ourselves the introduction machine

323
00:23:06,080 --> 00:23:09,740
learning from our detection is a great
thing but right now this is largely

324
00:23:09,740 --> 00:23:14,850
might look like so I'm beating a dead
horse a little bit here but again the

325
00:23:14,850 --> 00:23:17,879
answer is not just getting stronger
locks the answer is that people should

326
00:23:17,880 --> 00:23:21,720
be using different locks right and and
ideally that actually change the locks

327
00:23:21,720 --> 00:23:25,610
once in awhile but we saw we were
surveying the history of the security

328
00:23:25,610 --> 00:23:30,408
industry new security technology did not
arise directly in response to new attack

329
00:23:30,409 --> 00:23:34,890
vectors by malware authors in this case
but a response to a demand for defense

330
00:23:34,890 --> 00:23:38,890
against these attacks so in this analogy
there's nothing technologically stopping

331
00:23:38,890 --> 00:23:43,690
the locksmith from producing different
locks but Stepanek I asked him to do and

332
00:23:43,690 --> 00:23:49,970
no one is asking for it so to speak to
both of these topics to talk some of you

333
00:23:49,970 --> 00:23:52,669
may be familiar with the idea of a
moving target defense and cybersecurity

334
00:23:52,669 --> 00:23:56,840
which attempts to increase the costs and
basically effort on the part of the

335
00:23:56,840 --> 00:24:02,030
attacker by changing the target software
and overall configuration this changing

336
00:24:02,030 --> 00:24:06,010
attack surface at some certainty to the
attackers mission so we are some of

337
00:24:06,010 --> 00:24:09,100
these ideas and introduce the idea of
what we're calling it moving defense

338
00:24:09,100 --> 00:24:13,649
where the security solution itself is
changing both spatially across

339
00:24:13,650 --> 00:24:17,549
deployments and also temporarily within
a given deployment right so far from a

340
00:24:17,549 --> 00:24:21,080
tax perspective it makes our town with a
lot less like the one on the left and a

341
00:24:21,080 --> 00:24:22,610
lot more like them on the right

342
00:24:22,610 --> 00:24:27,250
include the temperament and actually
maybe even looks more like this but they

343
00:24:27,250 --> 00:24:31,320
naturally task so why hasn't this been
done before right this single lock

344
00:24:31,320 --> 00:24:37,189
paradigm for our detection has been in
place for decades and while moving

345
00:24:37,190 --> 00:24:40,210
defense is a simple concept it's
actually very very difficult to

346
00:24:40,210 --> 00:24:45,240
implement in practice for starters it's
a logistical nightmare rights as a

347
00:24:45,240 --> 00:24:50,320
vendor or a locksmith how do you ensure
that all the locks are different and yet

348
00:24:50,320 --> 00:24:55,039
equally effective and find a way to
distribute them and maintain them right

349
00:24:55,039 --> 00:25:00,379
it's also very costly right and it's
kind of a related issue don't really

350
00:25:00,380 --> 00:25:03,760
talk about it because it's easier just
to produce one locker one model and

351
00:25:03,760 --> 00:25:04,559
distribute every

352
00:25:04,559 --> 00:25:08,460
as it isn't so much what your friends
are telling you but as why they aren't

353
00:25:08,460 --> 00:25:11,789
telling you right and and lastly you
know why should the locksmith actually

354
00:25:11,789 --> 00:25:15,830
make multiple locks if it's so freakin
hard he's worried you gonna complain

355
00:25:15,830 --> 00:25:19,129
that if somebody breaks into your place
is because of a particular lock that he

356
00:25:19,129 --> 00:25:24,330
issued the latter's legitimate concern
to in something worth discussing but for

357
00:25:24,330 --> 00:25:29,259
the locksmith is better to just avoid
this issue of multiple ox entirely and

358
00:25:29,259 --> 00:25:35,830
so track right so so where does that
leave us and what's interesting is that

359
00:25:35,830 --> 00:25:39,590
machine learning while its subject to
this problem is currently deployed is

360
00:25:39,590 --> 00:25:42,350
actually quite well suited to
implementing moving defense in practice

361
00:25:42,350 --> 00:25:47,209
and I will show this could actually be
done in operationally realizable way we

362
00:25:47,210 --> 00:25:50,240
know that in this scenario the attacker
gains a huge advantage by having access

363
00:25:50,240 --> 00:25:54,110
to the model that shared among all
deployments before he actually goes

364
00:25:54,110 --> 00:25:57,998
after particular target so by altering
the machine learning model that is

365
00:25:57,999 --> 00:26:05,820
unique for each deployment we can
achieve moving defense so call now and

366
00:26:05,820 --> 00:26:10,759
talk about how we can actually do that
in practice so in order to Bermuda

367
00:26:10,759 --> 00:26:14,899
machine classifiers they're really three
main knobs we can turn right the first

368
00:26:14,899 --> 00:26:18,039
is the feature space chosen we haven't
talked about this at all where

369
00:26:18,039 --> 00:26:21,039
intentionally trying to stay out of the
weeds here and keep the sort of it as a

370
00:26:21,039 --> 00:26:24,679
conceptual idea but essentially have to
define a space in which are learner can

371
00:26:24,679 --> 00:26:29,320
operate on our data base is kind of like
a lens lens through which to view our

372
00:26:29,320 --> 00:26:32,549
data the data will sort of look
different depending on the feature space

373
00:26:32,549 --> 00:26:36,279
witches in practice though this probably
isn't the best option

374
00:26:36,279 --> 00:26:39,919
by forcing a suboptimal features a
selection it's very hard to maintain

375
00:26:39,919 --> 00:26:43,850
efficacy we also lose the ability to
describe our data in terms of feature

376
00:26:43,850 --> 00:26:48,129
vectors was really convenient way to
represent it especially if the actual

377
00:26:48,129 --> 00:26:54,360
samples are working with our large or in
the case of mouth actually dangerous so

378
00:26:54,360 --> 00:26:57,908
that's one option great another is to
use a different learning algorithm for

379
00:26:57,909 --> 00:27:02,249
each classifier this is practically very
difficult though there are only so many

380
00:27:02,249 --> 00:27:05,639
learners out there you can imagine it
would be extremely difficult to maintain

381
00:27:05,639 --> 00:27:10,379
efficacy across these different
classifiers last option is to bury the

382
00:27:10,379 --> 00:27:15,019
data used to actually generate the
classifier for a lot of reasons there's

383
00:27:15,019 --> 00:27:18,200
there's almost no bound to the number of
classifiers we can generate this way

384
00:27:18,200 --> 00:27:23,309
just a sixth we can describe our samples
in terms of feature vectors something

385
00:27:23,309 --> 00:27:28,428
that we just added we want to do so
elects this option for achieving a

386
00:27:28,429 --> 00:27:32,600
desired pre-mutation may be skeptical
about how much variation we can really

387
00:27:32,600 --> 00:27:37,699
achieve by doing this right having
different blocks at least helpful if the

388
00:27:37,700 --> 00:27:41,470
locks aren't that different from each
other and also will be able to maintain

389
00:27:41,470 --> 00:27:46,509
efficacy when this dataset is marrying
into these both topics shortly and

390
00:27:46,509 --> 00:27:50,369
something that Ryan's going to present
some results on we also have to discuss

391
00:27:50,369 --> 00:27:53,928
the practical issues of where this
training is taking place right off site

392
00:27:53,929 --> 00:27:58,679
at the better location or on-site at the
location of each deployment either of

393
00:27:58,679 --> 00:28:01,639
these as possible we're gonna proceed
with its or distribute approach for the

394
00:28:01,639 --> 00:28:08,309
training happens at each user location
but by no means to preclude a

395
00:28:08,309 --> 00:28:11,720
centralized bender centric approach
there is some advantages of doing it

396
00:28:11,720 --> 00:28:15,440
locally which we're going to talk about
it later so either accomplish our goal

397
00:28:15,440 --> 00:28:19,429
but let's talk about how to construct
these classifiers in order to achieve

398
00:28:19,429 --> 00:28:23,669
moving defense right to how it had a
locksmith basically go about serving our

399
00:28:23,669 --> 00:28:29,110
town and I think we'll start by just
quickly illustrating the current machine

400
00:28:29,110 --> 00:28:33,109
learning from our detection paradigm is
basically going out again kind of what

401
00:28:33,109 --> 00:28:38,090
the blacksmith workshop looks like right
now so so first offenders create a large

402
00:28:38,090 --> 00:28:41,699
library of known behind software and no
malicious software which are represented

403
00:28:41,700 --> 00:28:44,809
by the solid green and strike red balls
respectively

404
00:28:44,809 --> 00:28:48,730
these datasets are often quite large
often terabytes of data and millions of

405
00:28:48,730 --> 00:28:52,789
samples better than at the secret sauce
rate whatever features are going to use

406
00:28:52,789 --> 00:28:57,049
their particular brand of choice and
trailer to recognize the difference

407
00:28:57,049 --> 00:29:01,200
between samples in these datasets the
result is a black box this classifier

408
00:29:01,200 --> 00:29:05,429
which is the lock and earlier analogies
the classifier except previously unseen

409
00:29:05,429 --> 00:29:10,289
files as input and terminations on has
been under malicious so they can take

410
00:29:10,289 --> 00:29:14,090
this classifier this is basically their
value and pushed out to user

411
00:29:14,090 --> 00:29:17,590
environments where to pose two
previously unseen files and they can

412
00:29:17,590 --> 00:29:22,720
make these determinations these unknown
samples are displayed in orange here

413
00:29:22,720 --> 00:29:27,039
will note though that the samples and
the user environmental labels orange

414
00:29:27,039 --> 00:29:30,710
even after the classifier make this
determination and that's because we

415
00:29:30,710 --> 00:29:34,309
don't actually have access to ground
truth here right where we really have no

416
00:29:34,309 --> 00:29:37,799
choice but to trust the classifiers
judgment and different classifiers mate

417
00:29:37,799 --> 00:29:41,000
Mike different determination to the same
samples and unfortunately not as perfect

418
00:29:41,000 --> 00:29:45,929
as we are to describe so we revisit the
shortly but return to original idea

419
00:29:45,929 --> 00:29:49,730
generating a new lock in the lab is
actually fairly straightforward we can

420
00:29:49,730 --> 00:29:52,529
just use different data it's probably
hard to see any animation and if you

421
00:29:52,529 --> 00:29:55,570
look on the left you can basically see
you can change the underlying data will

422
00:29:55,570 --> 00:29:58,769
get a different classifier different
lock button push out then you like to

423
00:29:58,769 --> 00:30:03,659
somebody else but we now ask the
question so what's the best way then to

424
00:30:03,659 --> 00:30:07,179
his death she moving defense in practice
across many user environments so we

425
00:30:07,179 --> 00:30:11,799
decided we were going to use varying
data input to various classifiers but

426
00:30:11,799 --> 00:30:17,820
there are many choices still on how how
we can actually very that input so you

427
00:30:17,820 --> 00:30:20,629
can imagine the simplest is probably
just to use the vendor as a data source

428
00:30:20,629 --> 00:30:24,750
they are you have all this day lying
around randomly select for example among

429
00:30:24,750 --> 00:30:28,289
data provided by the vendor perhaps
maybe in some intelligent way we get a

430
00:30:28,289 --> 00:30:33,119
job old data and generate classifiers
this way but it doesn't really provide

431
00:30:33,119 --> 00:30:36,908
significant diversity in our data sets
right we're drawing from the same source

432
00:30:36,909 --> 00:30:40,899
and so we're probably not going to
achieve you can imagine our idea this

433
00:30:40,899 --> 00:30:44,758
really robust moving defense because the
locks may not be that different from

434
00:30:44,759 --> 00:30:48,220
each other and it's unlikely that the
vendors would you be willing to provide

435
00:30:48,220 --> 00:30:51,710
the service right this'll be talking
about logistical nightmares this is high

436
00:30:51,710 --> 00:30:56,539
on that list so we can probably do
better than this week instead use a

437
00:30:56,539 --> 00:31:00,908
local data source right we can use the
data locally uniquely seen by each

438
00:31:00,909 --> 00:31:04,809
deployment and feedback today delivered
by the classifier back into our dataset

439
00:31:04,809 --> 00:31:09,599
and recalled meat labeled data to do
this this does help me the classifier

440
00:31:09,599 --> 00:31:14,490
unique but it only reinforces with the
classifier already news or really what

441
00:31:14,490 --> 00:31:17,929
the classifier thinks it knows right we
re just talked about the classifiers

442
00:31:17,929 --> 00:31:21,590
knowledge isn't perfect and we sort of
lift the veil here we can see that we're

443
00:31:21,590 --> 00:31:25,059
feeding back some incorrectly labeled
samples into our data set and again

444
00:31:25,059 --> 00:31:27,658
we're just reinforcing whatever
imperfect knowledge the classifier

445
00:31:27,659 --> 00:31:30,240
already has it so we think we can still
do better than this

446
00:31:30,240 --> 00:31:36,910
so what about using a local data source
but beating back in errors correctly

447
00:31:36,910 --> 00:31:41,520
labeled samples which requires an
a-lister user review a trusted his

448
00:31:41,520 --> 00:31:46,450
review to determine which samples were
wrongly sorted but use an analysts are

449
00:31:46,450 --> 00:31:49,220
already making these two determinations
over the course of using the tool

450
00:31:49,220 --> 00:31:53,620
whatever it is we don't need to be
exhaustive we can just take whatever

451
00:31:53,620 --> 00:31:58,840
label samples they already have we can
provide fresh informative samples to the

452
00:31:58,840 --> 00:32:03,639
learner and introduce this this new
knowledge to the learning model the

453
00:32:03,640 --> 00:32:07,170
resulting classifiers are tailored to
particular environment and stronger on

454
00:32:07,170 --> 00:32:10,350
the type of content that they've been
seeing and possibly even more effective

455
00:32:10,350 --> 00:32:15,199
than the original factory ship version
but perhaps even more important the

456
00:32:15,200 --> 00:32:19,320
resulting classifiers are unique and so
now every will be using their own locks

457
00:32:19,320 --> 00:32:23,300
so we're going to select this option for
the remainder of the talk which were

458
00:32:23,300 --> 00:32:27,159
doubling in situating for the locality
of a status or us that we know that any

459
00:32:27,160 --> 00:32:30,380
of these would be feasible to you
achieve our goal of improving defense

460
00:32:30,380 --> 00:32:34,780
was particularly cited by in Scituate
roche is it not just allowing people to

461
00:32:34,780 --> 00:32:41,420
buy different unique locks are actually
basically making everybody locksmith

462
00:32:41,420 --> 00:32:47,330
trying to give a little more detail on
some more design decisions as I

463
00:32:47,330 --> 00:32:51,470
mentioned the concept of institutes
fairly simple but there may be other

464
00:32:51,470 --> 00:32:55,440
factors to consider when trying to
operationally implement this idea and

465
00:32:55,440 --> 00:32:59,860
here we discuss two different dimensions
this idea of balanced or unbalanced or

466
00:32:59,860 --> 00:33:03,490
replacement nurses addition to consider
when determining how to incorporate

467
00:33:03,490 --> 00:33:09,370
additional local data into the original
training so the first option is you have

468
00:33:09,370 --> 00:33:12,909
bounced replacement in which an equal
amount of benign and Alyssa screenings

469
00:33:12,910 --> 00:33:17,100
is replaced with new data there is
unbalanced replacement when she only

470
00:33:17,100 --> 00:33:20,879
replaced apples and one of the old
training sites with your data files

471
00:33:20,880 --> 00:33:26,720
addition where he met a new data set out
to both benign and training sucks and

472
00:33:26,720 --> 00:33:30,059
unbalanced edition and when she only had
two new data to one of the old training

473
00:33:30,059 --> 00:33:35,530
sites so why would you choose one versus
another operationally looking at a

474
00:33:35,530 --> 00:33:40,270
balanced or unbalanced you might choose
to balance the addition of new data to

475
00:33:40,270 --> 00:33:42,480
prevent the classifier from favoring one
class

476
00:33:42,480 --> 00:33:46,750
another just because there's more about
this might not be a major issue when

477
00:33:46,750 --> 00:33:50,590
only slightly in balancing the trains at
the pentagon algorithms and approaches

478
00:33:50,590 --> 00:33:54,809
used for training or classifiers this
certainly concerns more and more data is

479
00:33:54,809 --> 00:34:00,740
added to only one drinks at the model
looking at replacement nurses addition

480
00:34:00,740 --> 00:34:04,500
there are a couple of operational
impacts to consider if you choose a

481
00:34:04,500 --> 00:34:08,520
replacement strategy you more quickly
bias the model to the new and citrus

482
00:34:08,520 --> 00:34:13,250
samples however you do this at the cost
of losing part of the original training

483
00:34:13,250 --> 00:34:18,190
sites if you use an additional strategy
you typically increase the time required

484
00:34:18,190 --> 00:34:21,750
to train the new models as well as
increased storage requirements for

485
00:34:21,750 --> 00:34:27,000
storing those new samples so far our
experiment we tried all four approaches

486
00:34:27,000 --> 00:34:30,320
we do not see any significant
differences in only adding or replacing

487
00:34:30,320 --> 00:34:36,020
piles up to 20% for a single branch off
the base model as a result we chose

488
00:34:36,020 --> 00:34:40,040
unbalanced edition for the purposes of
this experiment as is the easiest to

489
00:34:40,040 --> 00:34:43,790
implement operational and the lack of
need for additional samples or feature

490
00:34:43,790 --> 00:34:50,179
vectors to be provided by the vendor
list transition here and look at some

491
00:34:50,179 --> 00:34:54,980
results of our specific implementation
of the top row this table shows the

492
00:34:54,980 --> 00:34:58,570
performance of the original based
classifier which was trained with 20,000

493
00:34:58,570 --> 00:35:04,250
benign and $20,000 samples all the while
the results show a false positive rate

494
00:35:04,250 --> 00:35:07,590
of a hundred percent for the local data
because we are testing at your testing

495
00:35:07,590 --> 00:35:13,930
the classifiers against the files missed
by the base classifier if we had one

496
00:35:13,930 --> 00:35:17,750
percent which is 209 files are available
dataset that the original base

497
00:35:17,750 --> 00:35:22,960
classifier missed we see the
false-positive rate fall to 40% and 80%

498
00:35:22,960 --> 00:35:28,109
100 local but I am files cause that
falls percent false-positive rate to

499
00:35:28,109 --> 00:35:32,100
fall further 28 percent and this trend
continues as we have more and more local

500
00:35:32,100 --> 00:35:36,810
data to the original model therefore we
have shown that adding local data

501
00:35:36,810 --> 00:35:40,730
previously missed by the base classifier
to the in such a model results in a

502
00:35:40,730 --> 00:35:45,840
lower false positive rate on that local
data but the question remains how the

503
00:35:45,840 --> 00:35:51,540
performance against terrorism allowed it
aside and as a result show there was

504
00:35:51,540 --> 00:35:54,350
little to no degradation in performance
on the lab

505
00:35:54,350 --> 00:35:57,810
therefore the incision models will
generate are performing equally or

506
00:35:57,810 --> 00:36:04,210
better than the base classifier we can
further validate this claim by

507
00:36:04,210 --> 00:36:08,590
generating ten random classifiers using
5% unbalanced addition in evaluating

508
00:36:08,590 --> 00:36:13,820
their performance is very similar in
concept to a cave hold validation as we

509
00:36:13,820 --> 00:36:16,800
ultimately want to prove that we're not
just getting lucky with our specific

510
00:36:16,800 --> 00:36:21,450
sample of our new dataset so generally
these classifiers we take our original

511
00:36:21,450 --> 00:36:25,379
lab data set and out of random sampling
of our local data that was missed by the

512
00:36:25,380 --> 00:36:29,810
base classifier and this results in a
new classifier are one or try number one

513
00:36:29,810 --> 00:36:33,980
is going to repeated using a different
random sampling for a local data to

514
00:36:33,980 --> 00:36:39,900
generate are two or trial number two and
so on and so forth for our three there

515
00:36:39,900 --> 00:36:44,520
are 10 that's important to note that
these classifiers were generated using

516
00:36:44,520 --> 00:36:48,980
the same local data source for this
experiment but in reality is 10 class

517
00:36:48,980 --> 00:36:52,240
buyers can represent ten different
operational environments each with their

518
00:36:52,240 --> 00:36:57,040
own data sources looking across all 10
classifiers the results show that

519
00:36:57,040 --> 00:37:01,430
performance is very consistent with
standard deviations of roughly half a

520
00:37:01,430 --> 00:37:05,339
percent this means we are generating
class fires that have you call a greater

521
00:37:05,340 --> 00:37:09,170
effectiveness compared to the base
classifier in other words we are

522
00:37:09,170 --> 00:37:13,590
producing equally effective locks but
the question remains are these locks

523
00:37:13,590 --> 00:37:21,040
fundamentally different from the the
base locks up by the vendor so averaging

524
00:37:21,040 --> 00:37:25,000
across the intensity models we just
generated compared to the base

525
00:37:25,000 --> 00:37:30,350
classifier we used two different metrics
to try and capture model commonality the

526
00:37:30,350 --> 00:37:34,529
first measure is something called
utilized future space commonality this

527
00:37:34,530 --> 00:37:37,410
is specific to our specific
implementation of the machine learning

528
00:37:37,410 --> 00:37:41,259
models so when we are training our
models we have our total future space

529
00:37:41,260 --> 00:37:46,210
which was down down selected in the lab
in the global feature said this total

530
00:37:46,210 --> 00:37:50,460
features based on the order of tens of
thousands of features so only a subset

531
00:37:50,460 --> 00:37:53,930
of these features are actually used by
the learner when generating a new model

532
00:37:53,930 --> 00:37:58,359
therefore the commonality here is
defined as the overlap of these subsets

533
00:37:58,360 --> 00:38:02,640
divided by the sum of the feature
subsets in this matter ends up being

534
00:38:02,640 --> 00:38:04,270
about 30 percent

535
00:38:04,270 --> 00:38:11,670
and is very consistent across all ten of
our models the second measure we

536
00:38:11,670 --> 00:38:15,370
employed is more general and can be
applied to any machine learning approach

537
00:38:15,370 --> 00:38:20,460
and something we call overlapping miss
classifications so every class far as I

538
00:38:20,460 --> 00:38:24,060
know makes mistakes and we are trying to
capture the Insitute mistakes are the

539
00:38:24,060 --> 00:38:28,430
same or different compared to the base
classifier we tested both classifiers

540
00:38:28,430 --> 00:38:31,930
against the same set of ten thousand
samples that were not used for training

541
00:38:31,930 --> 00:38:37,190
will start second misclassifying by each
model which ends up being about 3% as

542
00:38:37,190 --> 00:38:41,110
shown in the circles of the Venn diagram
and the commonality is defined as the

543
00:38:41,110 --> 00:38:45,400
overlap with these subsets divided by
the sum of the misclassification subsets

544
00:38:45,400 --> 00:38:50,400
and this matter ends up averaging around
46% and again is very consistent across

545
00:38:50,400 --> 00:38:55,640
all ten of our models so using his two
metrics we feel fairly confident that

546
00:38:55,640 --> 00:39:00,710
our institute models are significantly
different from the base model but are we

547
00:39:00,710 --> 00:39:04,980
just working these models in the same
direction or are they being or are they

548
00:39:04,980 --> 00:39:11,700
all being for it in different directions
and so the answer this question we can

549
00:39:11,700 --> 00:39:15,890
look at a pairwise comparison of all the
randomly generated such a models using

550
00:39:15,890 --> 00:39:19,920
are overlapping with classifications
metric there a lot of numbers here let

551
00:39:19,920 --> 00:39:23,170
me explain what is going on first and so
focusing on the exact numbers themselves

552
00:39:23,170 --> 00:39:29,480
so first let's look at a comparison of
our one and are two classifiers

553
00:39:29,480 --> 00:39:33,950
established our one is different from
the base and we've also stylish are to

554
00:39:33,950 --> 00:39:41,460
is different from the base but we don't
know how r one differs from our to this

555
00:39:41,460 --> 00:39:44,940
graphic the forward slashes represent
the files missed by Irwan which in

556
00:39:44,940 --> 00:39:49,260
reality are about 3 percent of the total
samples analyzed in the back roads

557
00:39:49,260 --> 00:39:53,660
slashes represent the files missed by
our two again roughly three percent of

558
00:39:53,660 --> 00:39:58,670
the total samples where they slashes
intersect the files are missed by both

559
00:39:58,670 --> 00:40:04,890
classifiers for about half of that 3%
and we can repeat this comparison there

560
00:40:04,890 --> 00:40:08,799
are two in Darfur classifiers with the
intersection again showing the files are

561
00:40:08,800 --> 00:40:14,890
missed by both models what this means is
that any to give an institute class

562
00:40:14,890 --> 00:40:17,710
fires have a roughly 50% overlap

563
00:40:17,710 --> 00:40:22,369
amidst classifications out of the three
percent each of the mess in other words

564
00:40:22,369 --> 00:40:27,060
this means of the misses half the time
they're missing the same files but half

565
00:40:27,060 --> 00:40:31,400
of the time they're not as important to
re-emphasize that you saw these in such

566
00:40:31,400 --> 00:40:34,760
a class buyers represent different
deployments therefore you're gaining

567
00:40:34,760 --> 00:40:38,500
diversity across all of those
deployments therefore the lot these

568
00:40:38,500 --> 00:40:41,690
locks are not just different from the
original lock but are also significantly

569
00:40:41,690 --> 00:40:49,790
different from one another so now we can
revisit our original demo introduced

570
00:40:49,790 --> 00:40:54,810
four new targets are using for over
previously generated in city models so

571
00:40:54,810 --> 00:40:58,150
let's see what happens when we deploy
the variant that defeated the original

572
00:40:58,150 --> 00:41:09,200
machinery model against the New and such
models in this video we have over later

573
00:41:09,200 --> 00:41:13,660
label which and such a model for we're
testing as well as a real-time kill

574
00:41:13,660 --> 00:41:17,440
count to see how many miles are
successful and how many were defeated

575
00:41:17,440 --> 00:41:22,250
and its foreign city models in reality
as I mentioned earlier represent for

576
00:41:22,250 --> 00:41:25,240
different operational deployments have
all been trained on their own local

577
00:41:25,240 --> 00:41:32,598
individual data is important to note
that both the original antivirus and

578
00:41:32,599 --> 00:41:37,030
machine learning software missed this
variant so any protection we can

579
00:41:37,030 --> 00:41:40,790
demonstrate here would be a major
improvement over the existing

580
00:41:40,790 --> 00:41:47,380
deployments and it's also important to
realize that these models are not

581
00:41:47,380 --> 00:41:59,490
perfect as you can see here one of the
essential models was in fact to feed on

582
00:41:59,490 --> 00:42:05,779
the attacker had obtained a copy machine
learning software and iterated until I

583
00:42:05,780 --> 00:42:10,780
found a variant and that had defeated it
in the second part of the demo

584
00:42:10,780 --> 00:42:14,820
introduced four new targets that were
all running in city models or branched

585
00:42:14,820 --> 00:42:18,310
off the original machine learning model
and here is so that three of the UN

586
00:42:18,310 --> 00:42:22,720
system models called the threat but the
forested still Mehsud over again is

587
00:42:22,720 --> 00:42:26,830
important to re-emphasize that if these
four targets have all been using the

588
00:42:26,830 --> 00:42:32,340
base model all for what about compromise
might argue that for holding out these

589
00:42:32,340 --> 00:42:35,869
different data sources for these are
such as classifiers which is combined

590
00:42:35,869 --> 00:42:39,780
the data internet and even better
classifier is possible that performance

591
00:42:39,780 --> 00:42:44,230
will be marginally better overall but
that's missing the point we need to

592
00:42:44,230 --> 00:42:49,400
consider the attackers force active any
marginal improvement in performance will

593
00:42:49,400 --> 00:42:53,920
be at the cost of significantly reducing
uncertainty for the attacker while the

594
00:42:53,920 --> 00:42:57,910
new in Scituate approach is not a magic
bullet you at least created a defense to

595
00:42:57,910 --> 00:43:09,660
diversity increases the risk of the
attacker be exposed so to recap the use

596
00:43:09,660 --> 00:43:13,740
of it you learning provides many
benefits it provides a diversity of

597
00:43:13,740 --> 00:43:18,560
defense especially across appointments
as well as temporally additionally

598
00:43:18,560 --> 00:43:22,290
training on local data last for
construction of Environment specific

599
00:43:22,290 --> 00:43:26,759
classifiers which many other benefits
allows for increased performance on the

600
00:43:26,760 --> 00:43:30,310
type of data observed by that
appointment as we saw in our results

601
00:43:30,310 --> 00:43:38,270
showed here for the purposes of this
experiment but we actually see this also

602
00:43:38,270 --> 00:43:42,420
in our real deployments that I mention
the beginning of the talk so I know that

603
00:43:42,420 --> 00:43:46,050
a lot of other questions you may have
about how this could be operationally we

604
00:43:46,050 --> 00:43:48,869
only have so much time but we'd be happy
to talk about that

605
00:43:48,869 --> 00:43:53,510
afterwards well and it did you approach
also provides increased responsiveness

606
00:43:53,510 --> 00:43:57,660
for users are using the traditional
deployment method that the machine

607
00:43:57,660 --> 00:44:01,118
learning solution is repeatedly hitting
on the same false positives or maybe

608
00:44:01,119 --> 00:44:05,530
even worse is really missing the same
false negatives the only opportunity to

609
00:44:05,530 --> 00:44:08,750
improve the classifier to submit those
samples to the vendor and basically

610
00:44:08,750 --> 00:44:11,490
cross your fingers that your sample be
incorporated

611
00:44:11,490 --> 00:44:15,319
and I'll be incorporated into either
either some custom I love that providing

612
00:44:15,320 --> 00:44:18,600
you or worse and probably more likely
into the global model that they're

613
00:44:18,600 --> 00:44:23,610
sharing with everybody that the only
short-term alternative for a user that

614
00:44:23,610 --> 00:44:27,710
is really to fall back on serve a
brittle signature all matching like

615
00:44:27,710 --> 00:44:32,190
approach to ignore or flag the files
needed it allows the state to be

616
00:44:32,190 --> 00:44:36,800
incorporated into the model directly and
immediately and finally there is no need

617
00:44:36,800 --> 00:44:39,950
to share personally-identifiable
proprietary data to improve classifiers

618
00:44:39,950 --> 00:44:44,660
I sympathize here this is not an
argument against data sharing a threat

619
00:44:44,660 --> 00:44:48,700
intelligence sharing in general but
aside from the user enterprises desire

620
00:44:48,700 --> 00:44:53,540
to share and the occasional technical
hurdles in doing so as many people this

621
00:44:53,540 --> 00:44:56,490
room are probably aware there are
situations where it's very difficult or

622
00:44:56,490 --> 00:45:00,868
even an illegal to share certain types
of data without excessive modification

623
00:45:00,869 --> 00:45:07,150
so for example PDFs that have personal
information while the proprietary

624
00:45:07,150 --> 00:45:11,600
company information or classified files
any kind and is it your approach allows

625
00:45:11,600 --> 00:45:14,819
this intelligence to be incorporated
into the machine learning model rather

626
00:45:14,820 --> 00:45:16,150
than having to discard it

627
00:45:16,150 --> 00:45:19,170
rely entirely on the vendors data or
willingness to implement a custom

628
00:45:19,170 --> 00:45:25,160
solution and so I'll leave you with a
wall text summarizing the sort of big

629
00:45:25,160 --> 00:45:28,700
picture right we saw that improvement
and she many methods from our detection

630
00:45:28,700 --> 00:45:32,700
are weakened by the reliance on this
traditional deployments paradigm a

631
00:45:32,700 --> 00:45:35,819
dedicated attacker and need only obtain
an arbitrary copy of the security

632
00:45:35,820 --> 00:45:40,650
solution to break through the target of
competence we all saw their concept

633
00:45:40,650 --> 00:45:44,619
moving defense addresses this year model
vulnerability and maybe nationally

634
00:45:44,619 --> 00:45:48,940
applied to some machining solutions we
discussed just a few of the many ways of

635
00:45:48,940 --> 00:45:52,240
achieving movie defense using machine
learning and cellphone several design

636
00:45:52,240 --> 00:45:56,060
decisions that were both just
operationalize best results as we saw

637
00:45:56,060 --> 00:45:59,890
there are many different ways of doing
this and we hope that you actually

638
00:45:59,890 --> 00:46:04,700
recognise what is applying its own
technology at last this diversity

639
00:46:04,700 --> 00:46:08,560
offered by moving defense is is better
for the hurt I don't mean this in a way

640
00:46:08,560 --> 00:46:11,730
that I explained to us talking about
physical contact and herd immunity

641
00:46:11,730 --> 00:46:18,410
yesterday but if we all use identical
defenses we're all worse off for it and

642
00:46:18,410 --> 00:46:20,868
change the security is you don't just
come about because of changes in the tax

643
00:46:20,869 --> 00:46:23,220
of adversaries write something else we
saw

644
00:46:23,220 --> 00:46:26,700
but because of the resulting demand from
users and so far turn the locksmith

645
00:46:26,700 --> 00:46:30,460
example one more time if we as
townspeople are happy with identical

646
00:46:30,460 --> 00:46:35,180
locks then identical oxes we're going to
get and so I'll leave you with this

647
00:46:35,180 --> 00:46:39,430
admonition we all need some
understanding of the technology but

648
00:46:39,430 --> 00:46:42,700
underlying our security products and we
should challenge practicing across the

649
00:46:42,700 --> 00:46:46,730
industry right it should not just be
about what your security vendor is and

650
00:46:46,730 --> 00:46:56,250
is not telling you so thank you

651
00:46:56,250 --> 00:47:03,160
questions to be asked to actually use
the mics that there's one right in the

652
00:47:03,160 --> 00:47:15,029
center over there are you sure

653
00:47:15,030 --> 00:47:21,160
sure that the question was by Purdue
right but with a difference between the

654
00:47:21,160 --> 00:47:26,160
unbalanced and the balance training sets
we had to be done and the imbalance

655
00:47:26,160 --> 00:47:31,830
really refers to how you're adding the
new local data into your model so if

656
00:47:31,830 --> 00:47:37,430
you're going to add a hundred benign
files to your original training set a

657
00:47:37,430 --> 00:47:41,220
balanced approach would also want to add
a hundred additional malicious files and

658
00:47:41,220 --> 00:47:44,049
so those might be filed an appeal from
the vendor where they might be files

659
00:47:44,050 --> 00:47:50,620
that you've created locally on the idea
is to try to prevent the training sets

660
00:47:50,620 --> 00:47:54,420
from getting severely unbalanced right
and that the reason we mentioned that we

661
00:47:54,420 --> 00:47:58,750
opted for the unbalanced approach
because it was easier essentially with

662
00:47:58,750 --> 00:48:02,650
you can imagine it's it's probably
annoying to enforce this equal balance

663
00:48:02,650 --> 00:48:06,880
training list on user environments right
it used to seeing a lot of false

664
00:48:06,880 --> 00:48:11,180
positives but not false negatives you
would need to provide a way to ship them

665
00:48:11,180 --> 00:48:16,419
additional malware basically feature
vectors in order to balance out the

666
00:48:16,420 --> 00:48:20,330
training set and we found it didn't
really very too much up to when you're

667
00:48:20,330 --> 00:48:24,560
adding up to 20% but we did have some
internal data that showed that forty

668
00:48:24,560 --> 00:48:31,779
fifty percent you might want to look at
more balanced approach ok and Mike great

669
00:48:31,780 --> 00:48:36,840
talk my question is if we assume that
this becomes kind of the industry

670
00:48:36,840 --> 00:48:41,810
standard approach great have you given
any thought to how you would constrain

671
00:48:41,810 --> 00:48:47,170
the model on the local system to kind of
sanity check it has it for too far off

672
00:48:47,170 --> 00:48:52,640
of your original base yes yes sure so
there's there's that kind of comes down

673
00:48:52,640 --> 00:48:57,180
to what test sets are using so as we saw
in our results one thing we're about how

674
00:48:57,180 --> 00:49:01,020
was our original lab test set that's a
really good way to make sure that even

675
00:49:01,020 --> 00:49:04,020
if I can you achieve this different lock
idea that you haven't lost your advocacy

676
00:49:04,020 --> 00:49:08,440
right on your original test data so you
would ship the user's local model back

677
00:49:08,440 --> 00:49:13,960
to your lab training so that's that's
that's common operational question so

678
00:49:13,960 --> 00:49:17,460
when we do that if you can imagine
having some sort of security sharing

679
00:49:17,460 --> 00:49:22,310
mechanism of this class bias back you
could additionally provide a way of

680
00:49:22,310 --> 00:49:24,859
testing on that original app data by
providing that

681
00:49:24,859 --> 00:49:28,779
that instead the other way with a better
price that to the user gets up in the

682
00:49:28,779 --> 00:49:31,739
future of actors or some sort of reduced

683
00:49:31,739 --> 00:49:38,089
ok you mentioned 20% and you were still
achieving good results so that's quite

684
00:49:38,089 --> 00:49:43,849
robust are ready but interesting
long-term he said we have another minute

685
00:49:43,849 --> 00:49:47,249
and I know you can take your question at
the mike

686
00:49:47,249 --> 00:49:51,529
okay thanks for the talk I had a
question that could be achieved its

687
00:49:51,529 --> 00:49:58,650
diversity in defense using scanning
solution such as virustotal

688
00:49:58,650 --> 00:50:06,789
more time like defense various logs
achieving dad using a scanning engine

689
00:50:06,789 --> 00:50:15,749
scan rate if you want to yeah I mean you
can achieve some diversities I mean you

690
00:50:15,749 --> 00:50:19,848
do have some diversity in the security
market place with different vendors the

691
00:50:19,849 --> 00:50:24,910
reality is that there's limited you only
have forty or fifty a/v solutions out

692
00:50:24,910 --> 00:50:29,049
there and really out of those only a
handful are widely deployed right

693
00:50:29,049 --> 00:50:34,140
actually interject that if you look at a
lot of malware codes been release often

694
00:50:34,140 --> 00:50:37,019
there's actually switch statements that
actually specifically handle each one of

695
00:50:37,019 --> 00:50:41,529
those different TV solutions for example
so it's it's it's a good idea it's

696
00:50:41,529 --> 00:50:46,980
important to have but it's on it's own
probably not quite sufficient to lead

697
00:50:46,980 --> 00:50:51,980
after this really robust defense that
were talking about I think I thought I

698
00:50:51,980 --> 00:50:55,019
might have but we'll be hanging out if
you can find a spot in the hallway no

699
00:50:55,019 --> 00:50:56,359
more questions so thank you

