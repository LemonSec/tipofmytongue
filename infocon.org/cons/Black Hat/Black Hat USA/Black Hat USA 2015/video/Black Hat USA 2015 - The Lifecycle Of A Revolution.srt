1
00:00:00,000 --> 00:00:04,430
welcome to the largest black hat ever I
say that every year I'm just gonna take

2
00:00:04,430 --> 00:00:09,360
a recording of myself and it starts to
make me really wonder where does this

3
00:00:09,360 --> 00:00:15,129
all end because I'm I really feel that
we are all employed for life

4
00:00:15,130 --> 00:00:19,920
mean we really can't keep doing this
forever because as far as I see I see

5
00:00:19,920 --> 00:00:25,060
problems and challenges and so on one
hand I'm really excited and the other

6
00:00:25,060 --> 00:00:31,848
hand I just want to sleep sometimes you
know so this is our 18th black hat and

7
00:00:31,849 --> 00:00:36,930
we have a second year at the Mandalay
Bay and every year I do a little bit of

8
00:00:36,930 --> 00:00:40,930
talking about statistics of how many
people show up how many countries

9
00:00:40,930 --> 00:00:44,620
represented I'm just gonna get that out
of the way before I move into some of my

10
00:00:44,620 --> 00:00:51,360
remarks and so hundred and two countries
represented

11
00:00:51,360 --> 00:00:56,890
pretty good proportion of the planet is
here around you and twenty of those

12
00:00:56,890 --> 00:01:03,090
countries only send a single person and
you can guess who the biggest countries

13
00:01:03,090 --> 00:01:12,860
are united states and who is the second
country China no I would have guessed

14
00:01:12,860 --> 00:01:22,259
Canada but it turns out to be the UK
yeah so they did they don't count yeah

15
00:01:22,259 --> 00:01:23,040
so

16
00:01:23,040 --> 00:01:26,650
so we have a lot of representation and
we also have some new things we've been

17
00:01:26,650 --> 00:01:31,640
doing this year last year we announced
our academic scholarship program where

18
00:01:31,640 --> 00:01:35,690
we are trying to get more students
involved from computer science programs

19
00:01:35,690 --> 00:01:40,060
around the country and so to get a crack
at a free admission to black hat you

20
00:01:40,060 --> 00:01:44,260
write a white paper about a scary top it
and he gets admitted they all get

21
00:01:44,260 --> 00:01:49,450
reviewed and we don't really have a cap
on how many we accept right now and so

22
00:01:49,450 --> 00:01:53,770
this year we accepted a hundred and
fifty-three that we thought were

23
00:01:53,770 --> 00:01:57,509
interesting so if you're in the audience
and you're in here on that program let's

24
00:01:57,509 --> 00:01:58,450
hear it

25
00:01:58,450 --> 00:02:05,320
come on guys so now this is your time to
make the most of it right

26
00:02:05,320 --> 00:02:10,008
take advantage of every opportunity you
have because next year probably not

27
00:02:10,008 --> 00:02:17,339
going to accept your writing it might be
a one-time offer we also for the second

28
00:02:17,340 --> 00:02:24,510
time are doing a donation to the eff and
the eff is is really been involved in a

29
00:02:24,510 --> 00:02:29,350
lot of legal battles over the years and
sometimes you agree with them and

30
00:02:29,350 --> 00:02:33,459
sometimes you don't agree with them but
you can't agree on is that the dedicated

31
00:02:33,459 --> 00:02:39,470
and motivated and they get results and
they understand our community and

32
00:02:39,470 --> 00:02:45,070
there's not that many other legal groups
that really understand us go out of

33
00:02:45,070 --> 00:02:50,160
their way to reach out to us and to
involve us in their process and so we're

34
00:02:50,160 --> 00:02:55,350
doing another $50,000 donation for the
work they do on behalf of not just our

35
00:02:55,350 --> 00:03:00,670
speakers but also on behalf of our
community so I want to thank you for

36
00:03:00,670 --> 00:03:05,040
that and they're good work this year

37
00:03:05,040 --> 00:03:16,170
so I have a couple of remarks you can
see how well they've been laid out and

38
00:03:16,170 --> 00:03:20,869
so for the theme this year I was trying
to think of how I've been feeling this

39
00:03:20,870 --> 00:03:24,150
last year about the direction but just
as the United States but the direction

40
00:03:24,150 --> 00:03:27,760
of information security in general and
it's really feeling to me like a

41
00:03:27,760 --> 00:03:32,548
pendulum a pendulum has been swing in
and it's accelerating it started maybe a

42
00:03:32,549 --> 00:03:37,290
couple of years ago and what's happening
i think is when I started off in

43
00:03:37,290 --> 00:03:45,870
information security was a hacker it was
not illegal to pirate software how many

44
00:03:45,870 --> 00:03:51,159
people were alive back then took
advantage of that loophole right

45
00:03:51,159 --> 00:03:55,739
noncommercial copyright infringement was
not illegal

46
00:03:55,739 --> 00:04:00,900
you could not make a profit off somebody
else's work but you could enjoy yourself

47
00:04:00,900 --> 00:04:06,859
or no profit how far we've come right
now they're talking about mandatory

48
00:04:06,859 --> 00:04:12,500
minimum sentencing in the UK for
software piracy so in the short course

49
00:04:12,500 --> 00:04:15,599
of the short about twenty years

50
00:04:15,599 --> 00:04:20,019
well actually United States
noncommercial piracy changed and it

51
00:04:20,019 --> 00:04:23,960
started the case started a potential in
about nineteen ninety-four my 1997 with

52
00:04:23,960 --> 00:04:30,940
that net act it became illegal so less
than twenty years ago it was legal so on

53
00:04:30,940 --> 00:04:35,949
one side of the pendulum and here we are
today and if you take that metaphor you

54
00:04:35,949 --> 00:04:41,130
start looking around you can see it in
many areas very little legislation lots

55
00:04:41,130 --> 00:04:44,090
of legislation today and so

56
00:04:44,090 --> 00:04:48,659
well as making notes to talk about I
realized that everything that's

57
00:04:48,660 --> 00:04:53,830
happening now we are in the middle of
where the sort of the gatekeepers on

58
00:04:53,830 --> 00:04:57,969
this we're going to be the trusted
advisers they want understand what the

59
00:04:57,970 --> 00:05:04,430
application and implications for things
liability is what's possible where the

60
00:05:04,430 --> 00:05:07,940
people building it or where the people
securing it we're gonna have to be the

61
00:05:07,940 --> 00:05:13,479
ones that are advising on it is it
possible to remote update a hundred

62
00:05:13,479 --> 00:05:17,550
million things devices are going to be
the ones that are sort of in the middle

63
00:05:17,550 --> 00:05:23,180
of that their security implications so
you might not think that you're gonna be

64
00:05:23,180 --> 00:05:26,990
in this battle but you're gonna be
pretty central to this fight over the

65
00:05:26,990 --> 00:05:32,060
next five to 10 years and make no
mistake and what we decide what happens

66
00:05:32,060 --> 00:05:38,270
in the next five to 10 years we're gonna
live with the next 30 and so you really

67
00:05:38,270 --> 00:05:42,299
have to start thinking a little bit more
broadly about what you're working on

68
00:05:42,300 --> 00:05:46,860
where you're donating their time and
what kind of world you want us to be you

69
00:05:46,860 --> 00:05:49,340
can see this just at the crib the war
starting all over again

70
00:05:49,340 --> 00:05:52,969
mandatory back doors golden keys

71
00:05:52,970 --> 00:05:56,590
these are not new concepts to us but
their new concepts to the new players

72
00:05:56,590 --> 00:06:01,530
and we have to bring some perspective
and some history to let them know why it

73
00:06:01,530 --> 00:06:06,929
wasn't a good idea then and why it's not
a good idea now even worse idea now so

74
00:06:06,930 --> 00:06:12,800
if you think about it we have insurance
coming on the scene

75
00:06:12,800 --> 00:06:18,580
cyber insurance is gonna suck up maybe a
quarter to a third of your budget in the

76
00:06:18,580 --> 00:06:23,349
next say five to 10 years that's not
gonna make you any more secured a

77
00:06:23,349 --> 00:06:28,430
pattern of your router's but when your
address do fail or get hacked your gonna

78
00:06:28,430 --> 00:06:29,650
have some insurance right

79
00:06:29,650 --> 00:06:35,159
classic risk avoidance strategy but how
are you still gonna do your job of some

80
00:06:35,160 --> 00:06:40,260
of your budget is moved to insurance
you're gonna have to play in a world

81
00:06:40,260 --> 00:06:45,419
where there's more regulation was more
insurance is more involvement of the of

82
00:06:45,419 --> 00:06:49,090
the legal system so for example I ask
people this and I haven't done a good

83
00:06:49,090 --> 00:06:52,560
answer so if you can tell me an answer
to this question

84
00:06:52,560 --> 00:06:58,200
I'll put it in my in my brain I think
about it and I'll write something why

85
00:06:58,200 --> 00:07:01,330
your company is under attack

86
00:07:01,330 --> 00:07:07,620
do not employ all the tools at your
disposal right you deploy your technical

87
00:07:07,620 --> 00:07:12,350
tools but how many of your company's of
actually got out of sued someone for

88
00:07:12,350 --> 00:07:18,050
attacking you it's really rare but if
you think about it in any other instance

89
00:07:18,050 --> 00:07:23,060
you'd be deploying lawyers electoral
property theft disgruntled employees

90
00:07:23,060 --> 00:07:28,020
competitors stealing but in certain
areas when we're being hacked we don't

91
00:07:28,020 --> 00:07:32,340
deploy lawyers why is that they're sort
of like our army

92
00:07:32,340 --> 00:07:37,010
governments have armies they have state
departments as a company you don't have

93
00:07:37,010 --> 00:07:41,460
any of that you don't have the law on
your side that allows you to use force

94
00:07:41,460 --> 00:07:48,310
all you've got a civil law why aren't
you using civil law I think that's going

95
00:07:48,310 --> 00:07:51,290
to become a bigger component in your
toolbox in the future you're gonna start

96
00:07:51,290 --> 00:07:55,330
having to reach into that box and use it
for you should start getting familiar

97
00:07:55,330 --> 00:08:02,070
with what tools you can use another 1
I'm thinking about his liability this is

98
00:08:02,070 --> 00:08:05,780
not a concern of ours twenty years ago
it's gonna be a big concern for us going

99
00:08:05,780 --> 00:08:06,539
forward

100
00:08:06,539 --> 00:08:13,370
do you think that we can solve all of
our problems that we're facing security

101
00:08:13,370 --> 00:08:19,150
and the ones we have in the future if
there is no soft or liability I hate

102
00:08:19,150 --> 00:08:24,880
saying this but I do not see a way
forward without softer liability which

103
00:08:24,880 --> 00:08:28,280
is going to be more of the same as far
as you can see it'll be like turtles all

104
00:08:28,280 --> 00:08:34,978
the way down right no soft reliability
of any $1 it could be $10 I'm not saying

105
00:08:34,979 --> 00:08:39,680
it has to be speculative I'm just saying
if normal legal functions are not

106
00:08:39,679 --> 00:08:43,239
injected into the process we're gonna
just have more of the same can you

107
00:08:43,240 --> 00:08:49,779
imagine Boeing or Airbus for Tesla
they're essentially moving data centers

108
00:08:49,779 --> 00:08:53,910
flying through the sky have their data
centers with wings are wheels but their

109
00:08:53,910 --> 00:08:56,719
data centers and they operate

110
00:08:56,720 --> 00:09:01,519
under liability they have some pretty
strict softer liability around their

111
00:09:01,519 --> 00:09:09,269
data centers but an Oracle Data Center
without wheels no liability so do you

112
00:09:09,269 --> 00:09:13,680
think I don't know someone like an Elon
Musk going they're gonna feel that

113
00:09:13,680 --> 00:09:16,589
they're in a great competitive his
situation where everything they do

114
00:09:16,589 --> 00:09:20,329
require so much more engineering and
liability protection but over there the

115
00:09:20,329 --> 00:09:24,500
Microsoft and Oracle the world no
protection no I think they're gonna want

116
00:09:24,500 --> 00:09:29,170
a level playing field at some point and
it's going to become competitive and so

117
00:09:29,170 --> 00:09:30,920
I think even if we do nothing

118
00:09:30,920 --> 00:09:34,120
market forces are going to start driving
us toward liability

119
00:09:34,120 --> 00:09:39,689
well what's that going to mean for you
for the way you run your business and so

120
00:09:39,689 --> 00:09:45,829
just more and more of these issues are
coming right and so for that reason when

121
00:09:45,829 --> 00:09:49,769
we were looking for a keynote this year
I was trying to think of someone that

122
00:09:49,769 --> 00:09:55,759
encompasses the big prospective the long
view that's familiar maybe a little bit

123
00:09:55,759 --> 00:09:59,720
more with the legal side and maybe a
little bit less with the technical of

124
00:09:59,720 --> 00:10:05,300
the geopolitical side to really kind of
help us understand what these big ships

125
00:10:05,300 --> 00:10:12,389
in in in social direction legal
direction what this means and so that we

126
00:10:12,389 --> 00:10:19,350
have Jennifer grant is our keynote and I
first met Jennifer DEFCON to believe and

127
00:10:19,350 --> 00:10:26,970
we talking about it back there besides
being a rare occurrence of a woman in a

128
00:10:26,970 --> 00:10:31,720
hacking conference twenty years ago I
can't imagine it was i hitting on her

129
00:10:31,720 --> 00:10:36,470
how did he meet her and now it turns out
she was signing in and she put our name

130
00:10:36,470 --> 00:10:41,519
down and she put lawyer and I was so
excited that there is a lawyer there and

131
00:10:41,519 --> 00:10:44,329
of course everybody always wants free
advice from a lawyer so I started

132
00:10:44,329 --> 00:10:48,579
talking to and we just stayed in touch
and in over the years Jennifer became

133
00:10:48,579 --> 00:10:54,378
the go-to person I mean if anybody in
the scene anywhere got in trouble

134
00:10:54,379 --> 00:10:58,540
Jennifer was there to help she was a
defense attorney and I remember one

135
00:10:58,540 --> 00:11:04,760
point to remember Peter Shipley who
invented wardriving he was

136
00:11:04,760 --> 00:11:09,240
being accosted by the hotel cuz he was
doing something with mylar balloons and

137
00:11:09,240 --> 00:11:13,730
she bust into the hotel security and
pulls out her her little bar association

138
00:11:13,730 --> 00:11:23,140
I D attorney let him go and she is not
afraid of controversy she's helped over

139
00:11:23,140 --> 00:11:28,380
the years max Butler was involved in his
cake Michael in 10 years ago she was

140
00:11:28,380 --> 00:11:33,500
involved with the MTV a charge card
hackers helping them

141
00:11:33,500 --> 00:11:39,820
Kevin Poulsen helped we've she helped
Aaron Schwartz she's even help me

142
00:11:39,820 --> 00:11:43,540
several times I mean she's really helped
everybody so that's why I'm so proud to

143
00:11:43,540 --> 00:11:50,209
have her finally on stage and to
introduce her I am equally proud to

144
00:11:50,210 --> 00:11:57,740
re-introduce you to sleep corto chairman
and CEO of Qualys and Qualys has been a

145
00:11:57,740 --> 00:12:04,710
sponsor black hat comes about probably
1999 they've had a long view and back

146
00:12:04,710 --> 00:12:09,280
then it was no cloud it was sad I think
so just been sort of event is it term

147
00:12:09,280 --> 00:12:14,220
and so they were offering their services
SAS and then the cloud came along so

148
00:12:14,220 --> 00:12:17,740
they had to rebrand so now they're
offering the security in the cloud but

149
00:12:17,740 --> 00:12:21,910
you know you know how the technology is
it's essentially the same stuff though

150
00:12:21,910 --> 00:12:28,150
it's different I think maybe with what I
see from Kuala says that these they're

151
00:12:28,150 --> 00:12:32,680
making their stuff more accessible and
they're offering more free services so

152
00:12:32,680 --> 00:12:37,969
you can get your toehold in and play
around with their stuff and I asked him

153
00:12:37,970 --> 00:12:44,030
what his long-term goal is i mean what's
after cloud cloud mobile mobile SAS

154
00:12:44,030 --> 00:12:49,410
cloudless left those those acronyms
don't work but really what it is they're

155
00:12:49,410 --> 00:12:55,060
trying to basically build security
simple security into the fabric of the

156
00:12:55,060 --> 00:12:59,689
clouds were just sort of the background
and it was coming from somebody else it

157
00:12:59,690 --> 00:13:03,510
doesn't have almost a fifteen year track
record with us I wouldn't really give a

158
00:13:03,510 --> 00:13:07,180
lot of credit but for Philippe I'll give
him a lot of credit for the long-term

159
00:13:07,180 --> 00:13:13,130
visionary view so with that I want to
see how fully makes his entrance on the

160
00:13:13,130 --> 00:13:25,580
left here

161
00:13:25,580 --> 00:13:32,500
good afternoon ladies and gentlemen and
thank you for this very nice word it's

162
00:13:32,500 --> 00:13:37,930
always an honor and a pleasure for me to
have the opportunity of introducing our

163
00:13:37,930 --> 00:13:44,699
keynote speaker and this because like
his very special very special to me was

164
00:13:44,700 --> 00:13:52,090
i thinkin brings the best mind and the
best and best hard in our industry so we

165
00:13:52,090 --> 00:13:59,570
can continue moving forward and I think
this is very important in fact the she

166
00:13:59,570 --> 00:14:06,960
now in 2015 2020 which I believe we may
end up calling the year of the major

167
00:14:06,960 --> 00:14:13,740
bridge in reference to the Office of
Personnel Management but also I believe

168
00:14:13,740 --> 00:14:23,240
it's it's a pivotal year for all of us
because in fact I think there's not

169
00:14:23,240 --> 00:14:31,090
another one but two mounting challenges
in front of us and which as we know it

170
00:14:31,090 --> 00:14:37,340
on the one hand cyber security and all
the other and privacy and when you think

171
00:14:37,340 --> 00:14:43,270
it through its not possible to
disassociate both of them and these

172
00:14:43,270 --> 00:14:50,949
because in fact the they represent our
digital freedom and freedom of our

173
00:14:50,950 --> 00:14:56,510
children and with this in mind that I'm
very honored and pleased to introduce

174
00:14:56,510 --> 00:15:06,760
our speaker a lady and her lawyer which
is dedicated her career to audition

175
00:15:06,760 --> 00:15:16,150
freedom and also mention a lady and her
lawyer who also was since the very

176
00:15:16,150 --> 00:15:23,980
beginning of our industry begin known as
the person the first person that hackers

177
00:15:23,980 --> 00:15:31,000
call with that I'm ready very happy to
introduce Jennifer granting which is the

178
00:15:31,000 --> 00:15:35,560
director of civil liberties at the
Stanford

179
00:15:35,560 --> 00:15:40,529
Center for Internet and Society thank
you very much

180
00:15:40,529 --> 00:15:49,040
Jennifer

181
00:15:49,040 --> 00:16:07,250
really excited to be here I think I've
come to almost every black hat since the

182
00:16:07,250 --> 00:16:15,069
show started and it's a real honor to be
invited to be the keynote so it was

183
00:16:15,069 --> 00:16:21,110
twenty years ago that I started coming
to these events and that I went to

184
00:16:21,110 --> 00:16:27,269
DEFCON and I was interested in going to
DEFCON because I believed in the dream

185
00:16:27,269 --> 00:16:33,720
of a free and open Internet and I
believe that we want a world where

186
00:16:33,720 --> 00:16:40,050
information is freely accessible and I
believe in the freedom to tinker their

187
00:16:40,050 --> 00:16:45,630
hands on imperative that people should
be able to study manipulate reverse

188
00:16:45,630 --> 00:16:50,610
engineer the devices in the software
that define the world around us that

189
00:16:50,610 --> 00:16:56,019
that's what it means to engage with and
to understand our world and I went to

190
00:16:56,019 --> 00:17:01,579
DEFCON because I wanted to be part of
making these dreams true and as an

191
00:17:01,579 --> 00:17:07,790
attorney I thought I could use my
services to protect hackers the people

192
00:17:07,790 --> 00:17:17,299
who were making this world happen from
the nations of law but today that dream

193
00:17:17,299 --> 00:17:25,079
of Internet freedom that brought me to
DEFCON twenty years ago is dying and

194
00:17:25,079 --> 00:17:32,908
it's dying because it nobody's murdering
the dream but it's dying because for

195
00:17:32,909 --> 00:17:38,220
better or for worse we've started to
prioritize other things we put other

196
00:17:38,220 --> 00:17:46,179
values ahead of openness and freedom we
are looking at security online civility

197
00:17:46,179 --> 00:17:52,130
improving the user interface protecting
intellectual property interests and

198
00:17:52,130 --> 00:17:58,340
we're valuing these above freedom and
openness and so through neglect

199
00:17:58,340 --> 00:18:05,289
and other evolutionary trends what we're
seeing is an internet that is less open

200
00:18:05,289 --> 00:18:13,520
and more centralized we're seeing an
internet that is more regulated used to

201
00:18:13,520 --> 00:18:18,470
not have very much regulation now we do
and in terms of where these rules are

202
00:18:18,470 --> 00:18:19,590
coming from

203
00:18:19,590 --> 00:18:23,510
we're seeing an internet where the
United States dominance over the network

204
00:18:23,510 --> 00:18:28,379
is fading and other countries are
getting in on a regulatory business this

205
00:18:28,380 --> 00:18:34,059
is really important because the next
billion Internet users are going to come

206
00:18:34,059 --> 00:18:38,549
from countries that don't have a bill of
rights that don't have a first amendment

207
00:18:38,549 --> 00:18:42,760
and it's going to be those governments
that are getting in the business of

208
00:18:42,760 --> 00:18:51,179
regulating the internet so transfer
accelerating we can see them now but

209
00:18:51,179 --> 00:18:58,590
they're accelerating and what I think
this means is that the dream today is in

210
00:18:58,590 --> 00:19:03,029
danger but we can kind of see forward
into the future and what the future will

211
00:19:03,029 --> 00:19:07,880
look like twenty years from now and
twenty years from now you won't

212
00:19:07,880 --> 00:19:12,520
necessarily know anything about the
decisions that are made that affect your

213
00:19:12,520 --> 00:19:18,440
life and you're right you want the
software is going to compute on data and

214
00:19:18,440 --> 00:19:21,179
it's going to decide whether you get a
loan

215
00:19:21,179 --> 00:19:26,289
whether you get a job whether a car runs
over you or drive off a bridge and these

216
00:19:26,289 --> 00:19:31,049
things are going to happen and you're
not gonna know I and maybe the people

217
00:19:31,049 --> 00:19:35,600
who designed the software aren't going
to know why either now when the public

218
00:19:35,600 --> 00:19:40,379
learned about these things how are we
gonna feel well it's gonna work well

219
00:19:40,380 --> 00:19:41,710
enough

220
00:19:41,710 --> 00:19:45,159
gonna work well enough and what's gonna
happen is that there's gonna be a lot of

221
00:19:45,159 --> 00:19:49,679
mistakes but the mistakes are going to
be on the edge cases and as long as

222
00:19:49,679 --> 00:19:54,250
those mistakes disproportionately affect
edge cases and minorities people are

223
00:19:54,250 --> 00:19:59,070
gonna accept this state of affairs but
that's not okay because it's the edge

224
00:19:59,070 --> 00:20:02,830
cases in the minority is that are the
ones that are the innovators that are

225
00:20:02,830 --> 00:20:07,000
there early adopters that are the first
movers that are the ones that involve

226
00:20:07,000 --> 00:20:08,390
our society

227
00:20:08,390 --> 00:20:16,570
lowered the internet going to become
more like TV instead of this global

228
00:20:16,570 --> 00:20:21,270
conversation that we had a vision twenty
years ago and that attracted many of us

229
00:20:21,270 --> 00:20:30,520
to computers and rather than technology
being revolutionary and overturning

230
00:20:30,520 --> 00:20:34,170
existing power structures we're seeing
that technology is being used to

231
00:20:34,170 --> 00:20:41,760
reinforce existing power structures this
is particularly true in security people

232
00:20:41,760 --> 00:20:46,230
want and need a certain level of safety
online but what we've learned is that

233
00:20:46,230 --> 00:20:51,020
people are not able to protect
themselves and so the idea of security

234
00:20:51,020 --> 00:20:55,730
has been centralized companies and
devices need to provide security to the

235
00:20:55,730 --> 00:21:01,640
public now or not that's not working
well enough but that is the enterprise

236
00:21:01,640 --> 00:21:08,730
that everybody here is engaged in trying
to provide but but when we centralized

237
00:21:08,730 --> 00:21:12,040
in order to achieve that goal of
security what happens is we create these

238
00:21:12,040 --> 00:21:17,399
choke points where regulation can happen
and we're seeing that regulation and

239
00:21:17,400 --> 00:21:23,060
things like push for crypto back doors
the regulation is going to be done by

240
00:21:23,060 --> 00:21:28,179
governments that have domestic and
national local concerns

241
00:21:28,180 --> 00:21:34,470
now global concerns it's going to be
influenced by it leaked and people with

242
00:21:34,470 --> 00:21:39,450
money and companies with money and so
powerful groups are going to get to

243
00:21:39,450 --> 00:21:43,220
decide who get security and who doesn't

244
00:21:43,220 --> 00:21:54,930
and then finally we see that finally we
see that the way that the Internet is

245
00:21:54,930 --> 00:21:59,940
currently operating for technological
and for business reasons instead of

246
00:21:59,940 --> 00:22:06,530
crowding around censorship is actually
facilitating surveillance censorship and

247
00:22:06,530 --> 00:22:13,210
control it doesn't path to be this way
but if we're going to change things we

248
00:22:13,210 --> 00:22:18,070
need to start doing it now and it needs
to start with us making asking some

249
00:22:18,070 --> 00:22:22,230
difficult questions and making some hard
decisions

250
00:22:22,230 --> 00:22:26,720
what's it gonna mean when computers know
everything about us and computer

251
00:22:26,720 --> 00:22:32,400
algorithms decide make life and death
decisions should be will be

252
00:22:32,400 --> 00:22:36,750
should we be worrying more about another
terrorist attack in New York or about

253
00:22:36,750 --> 00:22:40,680
the ability of journalists and human
rights workers around the world to do

254
00:22:40,680 --> 00:22:47,930
their jobs how do we value and weigh
those two things how much free speech

255
00:22:47,930 --> 00:22:55,540
does a free society really need we fear
that technology has created this golden

256
00:22:55,540 --> 00:23:01,730
age of surveillance can we also use
technology to readjust the balance of

257
00:23:01,730 --> 00:23:08,900
power between people and government so
that we can have some privacy back given

258
00:23:08,900 --> 00:23:13,660
that decisions by private companies are
going to be determining individual right

259
00:23:13,660 --> 00:23:16,820
how can the public interest

260
00:23:16,820 --> 00:23:21,510
be communicated into that process how
can we democratically control what these

261
00:23:21,510 --> 00:23:25,660
important platforms and private
companies do without squelching

262
00:23:25,660 --> 00:23:32,630
innovation who is responsible for
digital security is it the us-

263
00:23:32,630 --> 00:23:38,120
government is a government's role is it
private responsibility for corporations

264
00:23:38,120 --> 00:23:49,010
and what is going to become of the dream
of Internet freedom now

265
00:23:49,010 --> 00:23:56,950
the dream of Internet freedom began in
1984 when I read Stephen levy's book

266
00:23:56,950 --> 00:24:02,730
hackers in which he talks about the
computer scientists and engineers who

267
00:24:02,730 --> 00:24:08,530
built the early internet and these
people had a value called the hacker

268
00:24:08,530 --> 00:24:13,050
ethic and the hacker ethic was that
information should be freely accessible

269
00:24:13,050 --> 00:24:19,080
the hacker ethic was that was the
hands-on imperative that people should

270
00:24:19,080 --> 00:24:25,399
be free to manipulate change modify
study reverse engineer the technology

271
00:24:25,400 --> 00:24:32,510
around them and how perfect was built
into the technology itself the the

272
00:24:32,510 --> 00:24:40,280
decentralisation was a design principle
that had a political impact and the

273
00:24:40,280 --> 00:24:45,280
important thing about decentralisation
is that it empowered people to make

274
00:24:45,280 --> 00:24:52,600
their own decisions about what was right
and wrong

275
00:24:52,600 --> 00:25:00,340
decentralisation was very DNA of the
early Internet where they would be dumb

276
00:25:00,340 --> 00:25:05,770
pipe smart edges and the innovation
could take place and the network would

277
00:25:05,770 --> 00:25:11,100
run it and the idea was that we would
have this global network and the global

278
00:25:11,100 --> 00:25:17,300
network would allow us to communicate
with anyone anywhere anytime and that

279
00:25:17,300 --> 00:25:22,510
would bring us all of the hopes and
dreams and gloria is that the human mind

280
00:25:22,510 --> 00:25:29,280
and heart could dream up I wanted to
live in that world and and that idea

281
00:25:29,280 --> 00:25:31,870
that we could be in charge of our
intellectual destinies and the

282
00:25:31,870 --> 00:25:36,760
technology would help carry it with me
to college when I went to college and my

283
00:25:36,760 --> 00:25:41,640
college it was new college liberal arts
school in Sarasota Florida have any

284
00:25:41,640 --> 00:25:49,920
nuclear genes here too bad new power
supply is a place where the motto of the

285
00:25:49,920 --> 00:25:55,040
school is that everybody in the final
analysis is responsible for his or her

286
00:25:55,040 --> 00:26:01,340
own education and it was around that
time that I read the hacker manifesto in

287
00:26:01,340 --> 00:26:07,449
frac magazine written by the mentor and
I learned from that that hackers are a

288
00:26:07,450 --> 00:26:12,560
lot like my fellow academic nerds at New
College we were tired of being fed

289
00:26:12,560 --> 00:26:18,629
intellectual baby food we wanted to take
responsibility for our own lives we

290
00:26:18,630 --> 00:26:22,580
thought that information should be
freely available and that we should be

291
00:26:22,580 --> 00:26:30,740
able to communicate and think freely we
wanted to we we miss trusted authority

292
00:26:30,740 --> 00:26:36,510
and we wanted to change the world and we
wanted to live in a place where in the

293
00:26:36,510 --> 00:26:43,430
mentors onwards we would exist without
skin color without nationality without

294
00:26:43,430 --> 00:26:50,340
religious bias just based on the quality
of our thoughts so this is what I was

295
00:26:50,340 --> 00:26:55,159
into when I started using the internet
in 1991

296
00:26:55,160 --> 00:27:01,250
and I remember the day that everything
clicked for me what had a small I asked

297
00:27:01,250 --> 00:27:05,110
he called hollow net and I needed some
help I didn't know what I was doing so I

298
00:27:05,110 --> 00:27:10,090
asked a question of the system
administrator and he started to respond

299
00:27:10,090 --> 00:27:14,730
to my question and I could see him
typing and those letters one-by-one

300
00:27:14,730 --> 00:27:20,380
appearing on the screen that I was
looking at and this just connection over

301
00:27:20,380 --> 00:27:25,820
the technology and it made me realize
this early for the first time that this

302
00:27:25,820 --> 00:27:32,260
idea that we could talk to anyone or
everyone in real time was it could be a

303
00:27:32,260 --> 00:27:39,890
reality so twenty years ago when I
became a criminal defense attorney had

304
00:27:39,890 --> 00:27:47,070
this love of technology and I learned
that hackers were getting in trouble for

305
00:27:47,070 --> 00:27:52,800
doing things that I thought we're pretty
cool tracks one of the instances was at

306
00:27:52,800 --> 00:27:57,470
that really affected me was I was legal
aid person for the San Francisco

307
00:27:57,470 --> 00:28:01,670
sheriff's department working in the jail
and giving people legal advice and

308
00:28:01,670 --> 00:28:06,920
representation and one of the jail
inmates was looking at having all of his

309
00:28:06,920 --> 00:28:13,260
jail credit time taken away because he
had been basically hooting into the

310
00:28:13,260 --> 00:28:17,980
payphone and getting himself and all of
his pad and it's free phone calls home

311
00:28:17,980 --> 00:28:22,550
and I was like you gonna take his time
away for that I think that's pretty cool

312
00:28:22,550 --> 00:28:27,379
and as I was investigating the case
that's how I learned that hackers were

313
00:28:27,380 --> 00:28:31,420
getting in trouble for all kinds of
things and they're all these laws out

314
00:28:31,420 --> 00:28:36,380
there that were impacting the hacker
ethic and I thought as a lawyer i should

315
00:28:36,380 --> 00:28:42,890
get involved in maybe I can help so this
was also the same year that the internet

316
00:28:42,890 --> 00:28:47,790
civil liberties wars really started in
earnest at least from a legal

317
00:28:47,790 --> 00:28:54,409
perspective that year 1985 a guy named
Marty rim road a study saying that the

318
00:28:54,410 --> 00:29:00,500
Internet was running rampant with
pornography and law journal published

319
00:29:00,500 --> 00:29:02,000
the story and then

320
00:29:02,000 --> 00:29:11,190
then time magazine cover story on it and
the cyberporn scare was off and running

321
00:29:11,190 --> 00:29:16,180
and there's nothing that gets congress
more excited about doing something in

322
00:29:16,180 --> 00:29:22,260
the scourge of pornography so quickly
passed this law the communications

323
00:29:22,260 --> 00:29:28,450
decency act of 1996 the CDA ok this was
an attempt to regulate online

324
00:29:28,450 --> 00:29:33,780
pornography now for those of you who are
part of fans out there this is already a

325
00:29:33,780 --> 00:29:38,700
bummer but it was actually worse than
that because in order to regulate

326
00:29:38,700 --> 00:29:40,020
pornography

327
00:29:40,020 --> 00:29:43,860
the government had to argue that the
internet wasn't gonna be fully protected

328
00:29:43,860 --> 00:29:49,969
by the First Amendment so the internet
was going to be more like TV and less

329
00:29:49,970 --> 00:29:56,700
like a library and it was worse than
that because we had bigger hopes for the

330
00:29:56,700 --> 00:30:00,390
internet then that would be a library
the internet with better than the

331
00:30:00,390 --> 00:30:03,530
library because in on the Internet

332
00:30:03,530 --> 00:30:10,139
everyone can be a creator to on the
internet its global and on the Internet

333
00:30:10,140 --> 00:30:15,810
unlike in a library or bookstore
everything is always on the shelves so

334
00:30:15,810 --> 00:30:20,560
this idea that we should take this
promise and basically cut its legs off

335
00:30:20,560 --> 00:30:30,750
so early on was a nafta we were we were
very upset so what happened was so so at

336
00:30:30,750 --> 00:30:35,340
that point I think a lot of people
became activated and at that point and

337
00:30:35,340 --> 00:30:41,610
terror John Perry Barlow lyricist for
the grateful dead rancher founder of the

338
00:30:41,610 --> 00:30:47,240
Electronic Frontier Foundation a great
man who's lived a wonderful life has

339
00:30:47,240 --> 00:30:51,880
been in the hospital these past couple
of weeks so I send a shout out to him

340
00:30:51,880 --> 00:30:55,800
and I hope people will keep him in there
in there thought

341
00:30:55,800 --> 00:30:58,930
Barlow roadways as could be expected

342
00:30:58,930 --> 00:31:03,930
lyric off and and what he wrote was
revolutionary it was really

343
00:31:03,930 --> 00:31:10,060
revolutionary document the declaration
of independence of cyberspace and in it

344
00:31:10,060 --> 00:31:15,129
Barlow Road governance of the industrial
world you we read giants of flesh and

345
00:31:15,130 --> 00:31:21,240
steel I come from cyberspace the new
home of mind on behalf of the future I

346
00:31:21,240 --> 00:31:26,970
asked you of the path to leave us alone
you are not welcome among us

347
00:31:26,970 --> 00:31:33,690
you have no sovereignty where we gather
now Barlow was reacting to the CDA and

348
00:31:33,690 --> 00:31:39,470
then search in the internet should be
less free then books and magazines but

349
00:31:39,470 --> 00:31:43,490
he was also expressing a weariness and I
think it was a weariness that a lot of

350
00:31:43,490 --> 00:31:50,150
us shared with business as usual he was
expressing the hope that the internet

351
00:31:50,150 --> 00:31:56,170
would be able to place our reading our
friendships are very thought beyond

352
00:31:56,170 --> 00:32:01,330
government control and it was you know
maybe naive and maybe a little bit

353
00:32:01,330 --> 00:32:05,060
radical but the port of their I think
what attracted a lot of people and

354
00:32:05,060 --> 00:32:11,399
definitely attracted me so as it turned
out mark gearan and the Communications

355
00:32:11,400 --> 00:32:16,650
Decency Act did not kill the Internet in
fact it's a little bit ironic because

356
00:32:16,650 --> 00:32:22,520
what ended up happening was in the
lawsuit challenging the CDA the Supreme

357
00:32:22,520 --> 00:32:27,860
Court struck down almost every part of
the law the supreme court said the

358
00:32:27,860 --> 00:32:33,310
internet the First Amendment applies
fully and completely to the internet but

359
00:32:33,310 --> 00:32:40,360
there is one part of the CTA that the
court did not strike down and that part

360
00:32:40,360 --> 00:32:45,070
seems to have had seems to be a bread
alone sort of the opposite of what

361
00:32:45,070 --> 00:32:49,990
congress its goal was passing the CDA
because it said that online service

362
00:32:49,990 --> 00:32:57,470
providers don't have to police the
content on their services and can't get

363
00:32:57,470 --> 00:33:04,520
in trouble for content except in these
certain specific categories so he idea

364
00:33:04,520 --> 00:33:11,110
there is that you know you aren't the
provider doesn't have to be a policeman

365
00:33:11,110 --> 00:33:14,209
and two gether

366
00:33:14,210 --> 00:33:19,590
the hacker ethic the hacker manifesto
the declaration of independence of

367
00:33:19,590 --> 00:33:25,139
cyberspace ACLU versus reno the Supreme
Court case and this part of the CDA

368
00:33:25,140 --> 00:33:31,640
describe what's a more or less radical
dream depending on who you are but it's

369
00:33:31,640 --> 00:33:38,880
one that many if not most of us in this
room share or shared and maybe have even

370
00:33:38,880 --> 00:33:52,090
spend our lives working for so the dream
is that we overcome age race class and

371
00:33:52,090 --> 00:33:57,199
gender the dream is that we can
communicate with anyone anywhere at

372
00:33:57,200 --> 00:34:03,130
anytime this enhanced individual liberty
the dream is that we all have freer

373
00:34:03,130 --> 00:34:09,110
access to information and the dream is
the hands-on imperative the freedom to

374
00:34:09,110 --> 00:34:14,410
take her that we will be able to study
no and ultimately understand the devices

375
00:34:14,409 --> 00:34:22,049
around and software around us the dream
in San was that computers were going to

376
00:34:22,050 --> 00:34:29,240
make our lives more free and better but
I'm here to tell you today that this

377
00:34:29,239 --> 00:34:37,479
stream of Internet freedom is dying and
if you look from here at the trends and

378
00:34:37,480 --> 00:34:42,240
look twenty years on it doesn't just
look like the internet could be a lot

379
00:34:42,239 --> 00:34:46,419
less revolutionary than we had hoped it
looks like in the latter is it might be

380
00:34:46,420 --> 00:34:56,440
a lot worse today we've seen that race
gender class are more than resilient

381
00:34:56,440 --> 00:35:02,150
enough to thrive in the digital world
we've seen that ability to communicate

382
00:35:02,150 --> 00:35:09,570
with anyone anywhere is being limited by
both government control and corporate

383
00:35:09,570 --> 00:35:15,930
policy is about what is acceptable
online we've seen the free access to

384
00:35:15,930 --> 00:35:23,069
information also limited particularly by
our ability to study software and

385
00:35:23,070 --> 00:35:23,960
hardware around

386
00:35:23,960 --> 00:35:30,359
us so many laws now interfere with what
computer hackers do and reverse

387
00:35:30,359 --> 00:35:39,210
engineering and so the question that's
left is will computers liberate us is

388
00:35:39,210 --> 00:35:49,730
that dream still possible now I wanna
talk first about equality race gender

389
00:35:49,730 --> 00:35:55,460
and class discrimination are proving
remarkably resistant to change now I

390
00:35:55,460 --> 00:36:01,839
have to say this has not been my
experience being here at DEFCON and a

391
00:36:01,839 --> 00:36:06,000
black hat being part of the world of
computer security I have always felt

392
00:36:06,000 --> 00:36:12,000
respected and I've always felt welcome
but there's too much evidence that other

393
00:36:12,000 --> 00:36:18,270
people's experiences are not the same
and I want to just illustrate this with

394
00:36:18,270 --> 00:36:24,170
one simple set of statistics that Google
30 per cent of their workforce is female

395
00:36:24,170 --> 00:36:29,839
and only seventeen percent of the people
in tech jobs are at Facebook that number

396
00:36:29,839 --> 00:36:37,070
is 15% and its winter its 10% so very
far away from equality

397
00:36:37,070 --> 00:36:41,609
other pieces of evidence is anecdotal
this field in particular has a

398
00:36:41,609 --> 00:36:47,880
reputation for being overwhelmingly male
and overwhelmingly away now I have never

399
00:36:47,880 --> 00:36:52,570
understood why that's true because from
what I've seen the hacker community is

400
00:36:52,570 --> 00:36:57,970
unbelievably great at recognizing talent
and skill in unconventional candidate

401
00:36:57,970 --> 00:37:02,209
mean we have people who are unbelievably
successful we never finished college

402
00:37:02,210 --> 00:37:08,180
nevermind high school we have people all
over the autism spectrum who are

403
00:37:08,180 --> 00:37:14,390
incredibly doing incredibly well age is
irrelevant and Aaron Swartz when he was

404
00:37:14,390 --> 00:37:20,520
15 years old hung out with handlebars
the creator of the math inclusion is the

405
00:37:20,520 --> 00:37:24,750
very heart of the hacker ethic and
community might think we have a choice

406
00:37:24,750 --> 00:37:29,950
we can kind of persist to the way that
we have been going or I think this field

407
00:37:29,950 --> 00:37:34,180
could really be a leader and take
leadership in evolving

408
00:37:34,180 --> 00:37:40,160
a more equal a more equal society
starting with starting with security I

409
00:37:40,160 --> 00:37:45,089
think we have to conscientiously try to
do so and try to cultivate that talent

410
00:37:45,090 --> 00:37:53,590
though I wanna talk about the freedom to
tinker and when I say freedom to tinker

411
00:37:53,590 --> 00:37:57,770
it sounds a little bit like a hobby but
I want to impress on people how

412
00:37:57,770 --> 00:38:02,190
important this is because it's not just
the ability to like go putter around in

413
00:38:02,190 --> 00:38:07,560
your garage it's the ability the study
to modify and ultimately to understand

414
00:38:07,560 --> 00:38:11,460
the technology around and its technology
is more and more important that

415
00:38:11,460 --> 00:38:16,210
understanding is necessary for a
democratic society and there are two

416
00:38:16,210 --> 00:38:21,450
things that are limiting our ability to
the freedom to tinker one of them is law

417
00:38:21,450 --> 00:38:26,850
and one of them is just our natural
human capacity to understand things now

418
00:38:26,850 --> 00:38:31,819
there are many many examples of how the
law has interfered with the freedom to

419
00:38:31,820 --> 00:38:38,710
take her but I'm just gonna give you up
to it was exactly 10 years ago that

420
00:38:38,710 --> 00:38:46,230
Michael in but worker ISS was scheduled
to give a talk about a new class of

421
00:38:46,230 --> 00:38:53,870
vulnerability is in routers and Linds
employer ISS and Cisco routers that he

422
00:38:53,870 --> 00:38:59,240
had studied decided at some point that
they did not want him to give the talk

423
00:38:59,240 --> 00:39:07,919
and so they pressed black hat with
threats of copyright lawsuit to actually

424
00:39:07,920 --> 00:39:14,000
ripped the pages with michael lind
slides out of the conference but and

425
00:39:14,000 --> 00:39:18,390
reprint and redo all of the CDs then
there's nothing that looks more like

426
00:39:18,390 --> 00:39:25,890
censorship that people actually ripping
pages out of so on stage the next

427
00:39:25,890 --> 00:39:29,690
morning mike got on stage

428
00:39:29,690 --> 00:39:35,270
Perdana white baseball cap like
literally a white hat quit his job and

429
00:39:35,270 --> 00:39:38,920
gave the original talk anyway now

430
00:39:38,920 --> 00:39:48,119
I was Mike's lawyer in that case and we
successfully fought back the the civil

431
00:39:48,119 --> 00:39:52,299
lawsuit for copyright infringement we
are able to fight back against the

432
00:39:52,299 --> 00:39:56,309
criminal investigation that the
companies had implemented against him

433
00:39:56,309 --> 00:40:02,660
but the message was loud and clear and
not just to make the message was this is

434
00:40:02,660 --> 00:40:11,339
our software not our router not yours
you are just a licensee and we will tell

435
00:40:11,339 --> 00:40:16,290
you what you're allowed to do and you'll
do just that and no more you can't be

436
00:40:16,290 --> 00:40:23,319
compiled as you can't study it and you
can't tell anyone what you find that our

437
00:40:23,319 --> 00:40:27,650
case I want to mention is the criminal
prosecution for Computer Fraud and Abuse

438
00:40:27,650 --> 00:40:33,020
Act against my friend Aaron Swartz aaron
died in trouble for writing a script

439
00:40:33,020 --> 00:40:40,559
that automated the download academic
journal articles and he was authorized

440
00:40:40,559 --> 00:40:45,069
to access these are two polls but he was
doing it as a student at Harvard but he

441
00:40:45,069 --> 00:40:47,299
was doing it really really fast

442
00:40:47,299 --> 00:40:51,480
erin was a hacker any challenge the
system in all kinds of ways and they

443
00:40:51,480 --> 00:40:57,750
went after him with a vengeance and
charged him with the FAA and multiple

444
00:40:57,750 --> 00:41:01,839
counts and he was looking at a lot of
years and the stress of the case

445
00:41:01,839 --> 00:41:08,599
ultimately contributed to his killing
himself but again which was which is an

446
00:41:08,599 --> 00:41:14,109
unbelievable tragedy but again the idea
that anything Aaron did was an

447
00:41:14,109 --> 00:41:20,740
authorized to a computer scientist this
crazy but yet here to the message was

448
00:41:20,740 --> 00:41:21,939
clear

449
00:41:21,940 --> 00:41:27,359
you need our permission to operate in
this world if you step over the line we

450
00:41:27,359 --> 00:41:31,930
will come for you if you are to me if
you download too fast if you type

451
00:41:31,930 --> 00:41:37,629
something weird in the URL bar on your
browser if we don't like what you do or

452
00:41:37,630 --> 00:41:41,829
if we don't like you then this lies
vague enough that we can come and get

453
00:41:41,829 --> 00:41:47,840
you now the question is in the future we
can have

454
00:41:47,840 --> 00:41:52,220
have the freedom to tinker what would it
take for us to change the path that

455
00:41:52,220 --> 00:41:53,740
we're on now

456
00:41:53,740 --> 00:42:01,250
well first powers would have to stop
this stuff on cybercrime hand waving and

457
00:42:01,250 --> 00:42:06,210
actually do something for real about
cybersecurity instead of saying over

458
00:42:06,210 --> 00:42:10,000
gonna have a bigger penalty is under
criminal law no one cares about bigger

459
00:42:10,000 --> 00:42:14,020
penalties under criminal law if you look
at any of the big bridges that we've had

460
00:42:14,020 --> 00:42:18,110
over the past two or three years there's
been no criminal prosecutions in any of

461
00:42:18,110 --> 00:42:23,540
them and you know China or North Korea
or whoever you know is behind these

462
00:42:23,540 --> 00:42:28,000
breaches we're not putting China and
North Korea in prison so what's

463
00:42:28,000 --> 00:42:32,700
happening now is that these heavy
sentences are chilling the good guys and

464
00:42:32,700 --> 00:42:39,910
they're not protecting people online but
also have to declare that users and

465
00:42:39,910 --> 00:42:47,310
software like people to buy software
have the right to modify that software

466
00:42:47,310 --> 00:42:50,860
and that laws like the Digital
Millennium Copyright Act can't get in

467
00:42:50,860 --> 00:42:52,450
the way of that

468
00:42:52,450 --> 00:42:58,569
so this is really important because in
the next 20 years we are going to have

469
00:42:58,570 --> 00:43:03,010
all these network devices their software
and everything it's true now and it's

470
00:43:03,010 --> 00:43:07,200
just gonna get more true but if you're
not allowed to study that basically what

471
00:43:07,200 --> 00:43:13,700
it means is that we're gonna just be
surrounded by black boxes that do things

472
00:43:13,700 --> 00:43:19,189
that we can't understand why we did get
rid of the CFAA we need to get rid of

473
00:43:19,190 --> 00:43:23,180
the DMCA we need to get rid of this idea
that license agreements can limit what

474
00:43:23,180 --> 00:43:23,950
we do

475
00:43:23,950 --> 00:43:27,200
there's a public interest in the freedom
to take her that needs to be protected

476
00:43:27,200 --> 00:43:38,140
the other problem is the idea of it it's
just natural intellectual limitation on

477
00:43:38,140 --> 00:43:41,290
understanding the world so in the next
20 years we're going to see these

478
00:43:41,290 --> 00:43:46,920
amazing advances in machine learning and
artificial intelligence and one of the

479
00:43:46,920 --> 00:43:50,220
things that's gonna happen is that
offers gonna do stuff

480
00:43:50,220 --> 00:43:54,240
and we're not gonna really be able to
understand why some of that capacity

481
00:43:54,240 --> 00:43:58,750
will be because we didn't write the
software but increasingly it may be that

482
00:43:58,750 --> 00:44:04,520
the very people who actually wrote the
software don't know either and law

483
00:44:04,520 --> 00:44:08,520
professor Frank Pasquale wrote a book
about this called the black sista black

484
00:44:08,520 --> 00:44:16,080
box society and the idea of the book is
that we are gonna be surrounded the bet

485
00:44:16,080 --> 00:44:20,140
that algorithms are going to be making
these life-and-death decisions about us

486
00:44:20,140 --> 00:44:24,520
and we're not going to be able to
understand them really the first step to

487
00:44:24,520 --> 00:44:28,940
doing anything about something is to
understand it is transparency and

488
00:44:28,940 --> 00:44:35,080
transparency is actually going to become
increasingly hard so you know you take

489
00:44:35,080 --> 00:44:40,520
secrecy you take profit motive you add a
couple hundred billion pieces of data

490
00:44:40,520 --> 00:44:46,920
about all of us you shake a result comes
out and that's what we live with we need

491
00:44:46,920 --> 00:44:52,210
to think very hard about how do we take
advantage of a IR machine learning

492
00:44:52,210 --> 00:44:57,040
without ending up in that kind of
terrifying world and part of that is

493
00:44:57,040 --> 00:45:04,619
talking about who's responsible when
software fails whose job it is and so

494
00:45:04,619 --> 00:45:10,619
far we've had almost no regulation of
software like of software there have

495
00:45:10,619 --> 00:45:16,530
been very few cases mostly where the
vendor has misrepresented to the

496
00:45:16,530 --> 00:45:24,339
customers you know what the software
does but people who you know are not big

497
00:45:24,339 --> 00:45:29,540
into regulation are sick and tired of
crappy software and they're not gonna

498
00:45:29,540 --> 00:45:34,660
take it anymore and that feeling is
gonna be accelerated by the Internet of

499
00:45:34,660 --> 00:45:38,279
Things because now we're gonna have
industries that are very used to product

500
00:45:38,280 --> 00:45:44,170
liability that are also software vendors
autonomous cars that crash somebody's

501
00:45:44,170 --> 00:45:48,060
gonna do when your network toaster
catches on fire

502
00:45:48,060 --> 00:45:51,410
somebody's gotta stay till and there's
gonna be a software liability we've

503
00:45:51,410 --> 00:45:55,740
already had Chrysler recalled one point
four million cars based on research that

504
00:45:55,740 --> 00:45:59,680
Charlie and Chris are gonna talk about
later today and

505
00:45:59,680 --> 00:46:05,529
so what's gonna happen there when we
have software liability I think software

506
00:46:05,530 --> 00:46:12,369
reliability is inevitable I also think
that it's necessary but without question

507
00:46:12,369 --> 00:46:17,079
it's gonna make voting more expensive
and it's gonna make coating more

508
00:46:17,079 --> 00:46:23,900
conservative I think that we will do a
very crappy job of imposing software

509
00:46:23,900 --> 00:46:29,260
liability for a very long time and I
think that the people who are gonna

510
00:46:29,260 --> 00:46:36,329
suffer are going to be the innovators
and the startups and not incumbents so

511
00:46:36,329 --> 00:46:41,170
we have to pay a lot of attention to
that and be worried but it's gotta

512
00:46:41,170 --> 00:46:46,069
happen because it's a very short step
from suing tests left to suing or a call

513
00:46:46,069 --> 00:46:52,589
with all the good and bad that will come
up that so next I want to talk about

514
00:46:52,589 --> 00:46:57,950
privacy and security and free speech but
we need to take a step back and talk a

515
00:46:57,950 --> 00:47:04,118
little bit about how we got here and so
I mentioned that when I was reading his

516
00:47:04,119 --> 00:47:07,880
deliveries book I was learning about the
concept of the decentralized into the

517
00:47:07,880 --> 00:47:14,670
internet and and and principal and the
idea of the end-to-end principal was

518
00:47:14,670 --> 00:47:20,920
that the baby was intentional innovation
happens on the edges and what that meant

519
00:47:20,920 --> 00:47:26,410
is that the internet would not just
enable communication network tibet but

520
00:47:26,410 --> 00:47:34,230
that it would do it in a democratized
decentralized radical way power to the

521
00:47:34,230 --> 00:47:40,319
people and not to the governments are
companies that own the pipes that model

522
00:47:40,319 --> 00:47:44,420
has evolved it's evolved for business
reasons and it's evolved for

523
00:47:44,420 --> 00:47:50,240
technological reasons today broadband
users want to build smart pipes to

524
00:47:50,240 --> 00:47:56,078
enable quality of service to do malware
and spam filtering and to have new

525
00:47:56,079 --> 00:47:59,470
business models where they can make more
money off the fact that they control the

526
00:47:59,470 --> 00:48:03,140
network that they control the underlying
network

527
00:48:03,140 --> 00:48:08,910
today hundreds of millions of people
conduct their social interactions over

528
00:48:08,910 --> 00:48:15,140
highly centralized platforms like 10
sector Facebook and so what what does

529
00:48:15,140 --> 00:48:20,879
this mean for the public interest this
evolution away from end to end if you

530
00:48:20,880 --> 00:48:28,140
haven't read professor Tim lose book the
master switch then you should and in

531
00:48:28,140 --> 00:48:34,109
this book him and takes a look at the
other great communications technologies

532
00:48:34,110 --> 00:48:40,300
of our lifetime phones radio television
and movies and what tim says from

533
00:48:40,300 --> 00:48:45,490
studying the history of these
technologies is that there's a cycle and

534
00:48:45,490 --> 00:48:51,180
the cycle is this history shows the
progression of information technologies

535
00:48:51,180 --> 00:48:57,049
from somebody's hobby to somebody's
industry from a jury-rigged contraption

536
00:48:57,050 --> 00:49:03,630
to a slick production marble and
thinking BBS to web here from the freely

537
00:49:03,630 --> 00:49:07,830
accessible channel 21 controlled
strictly by a single corporation

538
00:49:07,830 --> 00:49:15,170
Marquardt elf from an open to close
system eventually entrepreneurs are

539
00:49:15,170 --> 00:49:22,840
regulators mashup heart the closed
system and the cycle begins and now and

540
00:49:22,840 --> 00:49:26,390
Tim traces this cycle for these
technologies and then he asked the

541
00:49:26,390 --> 00:49:31,520
question that I am asking you guys here
today is the internet going to follow

542
00:49:31,520 --> 00:49:38,840
this cycle is the internet going to be
con- centralized strictly controlled and

543
00:49:38,840 --> 00:49:43,820
closed if we don't do things differently

544
00:49:43,820 --> 00:49:48,950
the internet will end up like TV and as
I said some of that is because we've

545
00:49:48,950 --> 00:49:54,160
neglected the goals of freedom and
openness in favor of other values but I

546
00:49:54,160 --> 00:49:58,330
think we have to recognize that some of
it is because people have lost their

547
00:49:58,330 --> 00:50:04,790
allegiance to the dream of Internet
freedom some people will say people in

548
00:50:04,790 --> 00:50:06,870
this audience that the Internet is

549
00:50:06,870 --> 00:50:12,509
not the Utopia that I've made it out to
be rather the dream of entering Internet

550
00:50:12,510 --> 00:50:19,970
freedom has crashed head-on with the
ugly reality that other people can suck

551
00:50:19,970 --> 00:50:33,870
nasty comments for Chan B tards revenge
porn jihadis nazis these things are so

552
00:50:33,870 --> 00:50:40,609
affecting the public sensibility about
whether the Internet is a nice place to

553
00:50:40,610 --> 00:50:45,760
be or not that increasingly I even care
law professors experts in the first

554
00:50:45,760 --> 00:50:49,520
amendment or supposed to know about the
chilling effect on the doctrine of over

555
00:50:49,520 --> 00:50:54,080
breath talk about what's the best way to
legislate this stuff that they don't

556
00:50:54,080 --> 00:51:04,360
like out of existence second the trend
that I told you about that are affecting

557
00:51:04,360 --> 00:51:09,000
the network and these are centralization
regulation and globalization

558
00:51:09,000 --> 00:51:15,000
centralization of the problem because it
is a cheap and easy point for regulation

559
00:51:15,000 --> 00:51:22,170
control and surveillance regulation is
on the rise it is the exercise of

560
00:51:22,170 --> 00:51:26,860
government power in fair of local or
domestic interests and private entities

561
00:51:26,860 --> 00:51:32,440
with economic power just the reality of
our our system and it's even more so in

562
00:51:32,440 --> 00:51:37,160
other places and globalization means
that other governments are going to get

563
00:51:37,160 --> 00:51:38,430
into the mix

564
00:51:38,430 --> 00:51:42,350
other governments were not constrained
by the First Amendment who don't have a

565
00:51:42,350 --> 00:51:49,890
bill of rights who maybe don't even have
due process or the rule of law now when

566
00:51:49,890 --> 00:51:54,339
I say that corporate control as a
problem it may sound like I'm blaming

567
00:51:54,340 --> 00:51:58,730
corporations and when I say that the
government the Internet is becoming more

568
00:51:58,730 --> 00:52:03,350
closed because governments are censoring
the internet it may sound like I'm

569
00:52:03,350 --> 00:52:05,970
blaming governments and I

570
00:52:05,970 --> 00:52:12,359
but I'm also blaming you and I'm blaming
me because it's the things that we want

571
00:52:12,359 --> 00:52:18,450
that are driving these trends so just as
an example here ever had a blog never

572
00:52:18,450 --> 00:52:24,299
have a blog ok double bloggers out there
who are still blogs regular regularly I

573
00:52:24,300 --> 00:52:32,160
don't I post my updates on Facebook the
centralized server who probably a lot of

574
00:52:32,160 --> 00:52:38,460
people in this room run their own email
servers but almost nobody else I know it

575
00:52:38,460 --> 00:52:42,730
does they all use Gmail and they like
Gmail because they like the user

576
00:52:42,730 --> 00:52:47,320
interface they like the spam filtering
and they liked malware detection

577
00:52:47,320 --> 00:52:52,300
I'm alright no different when I had an
iPhone I didn't jailbreak it I

578
00:52:52,300 --> 00:52:59,390
downloaded the pre-approved you know ok
apps from the App Store I trusted

579
00:52:59,390 --> 00:53:03,940
judgment about what was secure and
whether you know what was available and

580
00:53:03,940 --> 00:53:09,670
i download apps now because I don't like
the Interfaith the mobile interface on

581
00:53:09,670 --> 00:53:14,260
my mobile browser and when they ask me
to say yes to the permissions I click

582
00:53:14,260 --> 00:53:18,180
yes because I wanted to do what it's
gonna deal and so I give it access to

583
00:53:18,180 --> 00:53:23,000
all kinds information about me and I
love it I love when I'm at the store my

584
00:53:23,000 --> 00:53:27,380
phone buzzes and reminds me that I need
to get milk I'm thrilled that it's

585
00:53:27,380 --> 00:53:35,839
ubiquitously tracking my location
otherwise no milk Amanda really bad so

586
00:53:35,839 --> 00:53:43,170
my point is that we want lots of cool
products in the cloud but but the cloud

587
00:53:43,170 --> 00:53:49,770
is such a terrible metaphor because the
club the club cloud is millions of

588
00:53:49,770 --> 00:53:56,030
little droplets of water and the
Internet cloud isn't like that at all

589
00:53:56,030 --> 00:54:06,520
the Internet cloud is actually a finite
and Noble number of companies that have

590
00:54:06,520 --> 00:54:13,210
that together have control over almost
all of the internet that we use

591
00:54:13,210 --> 00:54:19,060
and its level 34 fiber optics or Amazon
for servers were Google for the search

592
00:54:19,060 --> 00:54:23,279
engine and for Android and the fact that
there are these choke points these

593
00:54:23,280 --> 00:54:29,680
particular companies that are subject to
government regulation whether USR other

594
00:54:29,680 --> 00:54:36,830
means that this is an opportunity this
more centralized cloud is an opportunity

595
00:54:36,830 --> 00:54:43,580
for control for surveillance and for
regulation and this isn't looking like

596
00:54:43,580 --> 00:54:46,900
it's gonna change so as things keep
going in this direction what does it

597
00:54:46,900 --> 00:54:51,410
mean for privacy security and freedom of
expression lol

598
00:54:51,410 --> 00:54:56,500
privacy is essential to liberty and that
means that without privacy the future

599
00:54:56,500 --> 00:55:00,990
will be less free this is the golden age
of surveillance people know how much

600
00:55:00,990 --> 00:55:05,569
information technology today collect
about you but you might not realize is

601
00:55:05,570 --> 00:55:12,290
this here's a quiz what to emails but he
lists drive backups social networking

602
00:55:12,290 --> 00:55:17,860
post your web browsing history your bank
records your medical data your

603
00:55:17,860 --> 00:55:23,120
fingerprints your face prints and your
shaded DNA have in common

604
00:55:23,120 --> 00:55:27,160
the answer is the Department of Justice
doesn't think any of these are private

605
00:55:27,160 --> 00:55:32,149
right Department of Justice's view is
that these are all things that happen in

606
00:55:32,150 --> 00:55:36,780
public or what you voluntarily revealed
to service providers and so there's no

607
00:55:36,780 --> 00:55:43,120
expectation of privacy in the fourth
amendment doesn't apply to pay and what

608
00:55:43,120 --> 00:55:47,290
that means is that technology as
technology has proliferated all this

609
00:55:47,290 --> 00:55:50,940
data the law hasn't stepped in to
protect it

610
00:55:50,940 --> 00:55:56,600
the law has utterly fallen short on the
job in fact quite the opposite the laws

611
00:55:56,600 --> 00:56:02,650
enabling surveillance in all kinds of
ways we have these national security

612
00:56:02,650 --> 00:56:04,650
surveillance laws

613
00:56:04,650 --> 00:56:10,520
that are supposed to apply to foreigners
in particular categories of information

614
00:56:10,520 --> 00:56:15,150
but we've learned through secret
interpretations of law that our

615
00:56:15,150 --> 00:56:20,390
government in the united states that
actually using to spy on us we have

616
00:56:20,390 --> 00:56:25,140
provided assistance provisions that the
government is using not just to say well

617
00:56:25,140 --> 00:56:29,160
you have this data like to get a hold of
it but to try to force companies to do

618
00:56:29,160 --> 00:56:35,899
things like turn over their encryption
keys we have lots of laws and more being

619
00:56:35,900 --> 00:56:40,200
proposed that will give corporate
community for helping out the government

620
00:56:40,200 --> 00:56:45,169
and giving your data over even when
there's other laws narrow laws that

621
00:56:45,170 --> 00:56:49,990
would say no actually this information
is private and increasingly particularly

622
00:56:49,990 --> 00:56:54,359
in other countries but we're gonna see
it here to data retention obligations

623
00:56:54,359 --> 00:56:59,359
for the country where companies are
going to be basically commissioned to be

624
00:56:59,359 --> 00:57:05,660
police officers and spies for government
now you might think okay well there's

625
00:57:05,660 --> 00:57:09,750
gotta be some lawn right we've had the
internet for awhile emails been around

626
00:57:09,750 --> 00:57:13,700
for a long time but really surprisingly
there's actually only one case that's

627
00:57:13,700 --> 00:57:19,750
ever been decided on this from a regular
public order and it was in the 6th

628
00:57:19,750 --> 00:57:25,859
circuit which is Kentucky Tennessee
Missouri and one other state sorry for

629
00:57:25,859 --> 00:57:33,500
the other states I forgot Kentucky
Tennessee Michigan and Ohio sorry Ohio

630
00:57:33,500 --> 00:57:40,279
but as a result but basically in this
case the 6th circuit said ok email as a

631
00:57:40,279 --> 00:57:44,130
communication communication satellite
phone calls it's protected by the Fourth

632
00:57:44,130 --> 00:57:47,839
Amendment in this case has been really
important but the Department of Justice

633
00:57:47,839 --> 00:57:49,480
in public and in private

634
00:57:49,480 --> 00:57:53,579
continues to say that it's wrongly
decided it needs to be overturned now I

635
00:57:53,579 --> 00:57:57,789
wanna also take a moment to impress upon
people cuz it may not really get this

636
00:57:57,789 --> 00:58:02,039
not being lawyers what a war it means
and how important it is now warning

637
00:58:02,039 --> 00:58:07,049
means that a judge has to authorize the
search and basically a guard against

638
00:58:07,049 --> 00:58:09,910
arbitrary government action the police
can't just come in

639
00:58:09,910 --> 00:58:14,058
you know run rampant through your house
or just you know investigate you for no

640
00:58:14,059 --> 00:58:19,450
reason that's important but a warrant is
also important because it requires you

641
00:58:19,450 --> 00:58:24,220
to specifically described the place
that's being searched and things to be

642
00:58:24,220 --> 00:58:30,200
seized so where is also a guard against
master village when there's no warrant

643
00:58:30,200 --> 00:58:36,180
requirement it means that searches can
be arbitrary and massive turned against

644
00:58:36,180 --> 00:58:42,038
everybody this is really important but
all this data is not being protected so

645
00:58:42,039 --> 00:58:48,000
you know with globalization it really
only makes things worse and it's going

646
00:58:48,000 --> 00:58:53,750
to get worse as we see the Internet of
Things and network devices so you know

647
00:58:53,750 --> 00:58:58,119
we've got the centralization problem
there's all this regulation and

648
00:58:58,119 --> 00:58:59,750
countries are getting in on it

649
00:58:59,750 --> 00:59:05,270
particularly now that other countries
know how excessive the United States

650
00:59:05,270 --> 00:59:11,750
surveillance is they want to have the
same stuff next i wanna talk about

651
00:59:11,750 --> 00:59:17,170
security we often talk about security is
the opposite of privacy but we know that

652
00:59:17,170 --> 00:59:21,890
that's not true you can help security
without invading privacy like by locking

653
00:59:21,890 --> 00:59:27,710
cockpit doors sometimes to protect
security you need to protect privacy

654
00:59:27,710 --> 00:59:35,130
homosexual person in India or a human
rights worker in Syria is safer because

655
00:59:35,130 --> 00:59:37,180
of privacy

656
00:59:37,180 --> 00:59:40,750
one thing we don't talk about that much
is the relationship of security to

657
00:59:40,750 --> 00:59:46,859
openness as you lock down your network
as you make things more secure and sign

658
00:59:46,859 --> 00:59:51,450
on and there's no more open wifi anymore
and all of that security has attention

659
00:59:51,450 --> 00:59:55,669
with openness but it's also true at the
same time that if the network isn't

660
00:59:55,670 --> 00:59:58,779
secure and safe people aren't going to
use it what good is an open network

661
00:59:58,779 --> 01:00:02,450
that's too dangerous to use those of us
who try to check your email at DEFCON

662
01:00:02,450 --> 01:00:09,220
already know that the idea should have
been that people can choose security

663
01:00:09,220 --> 01:00:12,520
when it's appropriate and choose
openness and other times right the fact

664
01:00:12,520 --> 01:00:18,440
that we need to secure the electrical
grid data systems that control the water

665
01:00:18,440 --> 01:00:22,180
is doesn't mean we have to have closed
WiFi or

666
01:00:22,180 --> 01:00:26,970
that the government has to sit on the
domestic network and span or email but

667
01:00:26,970 --> 01:00:31,330
we're stats but that's what we're seeing
we're seeing that in the name of

668
01:00:31,330 --> 01:00:38,220
security we're having this greater
exercise of power particularly by the

669
01:00:38,220 --> 01:00:44,529
you s government over our use of the
network and so instead of having a

670
01:00:44,530 --> 01:00:51,150
global view that security on the
Internet should be rising tide that will

671
01:00:51,150 --> 01:00:55,750
float all boats what we're seeing is
this very provincial idea that security

672
01:00:55,750 --> 01:01:01,760
is cyber and cyber means it might do
said what general Hayden former head of

673
01:01:01,760 --> 01:01:07,650
NSA and CIA said it means which is that
the U S has the ability to use the

674
01:01:07,650 --> 01:01:11,940
network whenever we want and we have the
ability to deny that used to our

675
01:01:11,940 --> 01:01:16,400
adversaries that does not sound like an
open and reliable Internet to me that's

676
01:01:16,400 --> 01:01:23,160
not my internet and so wet that and that
meaning is that you know instead of

677
01:01:23,160 --> 01:01:26,259
protecting security of everybody

678
01:01:26,260 --> 01:01:31,420
the government wants to have crypto back
doors sit on the network and do

679
01:01:31,420 --> 01:01:37,260
surveillance be able to black out the
Internet in North Korea or wherever and

680
01:01:37,260 --> 01:01:42,740
that means that there's going to be
security have and security have not I

681
01:01:42,740 --> 01:01:49,770
think the better analogy for security to
understand it is that increasingly we're

682
01:01:49,770 --> 01:01:54,440
seeing security becoming about power
where people in power

683
01:01:54,440 --> 01:02:00,650
want security for themselves and want to
deny security to others so security is a

684
01:02:00,650 --> 01:02:06,240
power relationship then people are going
to lose and the people who are going to

685
01:02:06,240 --> 01:02:09,540
lose our the vulnerable communities

686
01:02:09,540 --> 01:02:14,300
and the minorities and the religious
minorities that actually need security

687
01:02:14,300 --> 01:02:18,510
most and you know it here in the United
States people don't care enough because

688
01:02:18,510 --> 01:02:22,780
we think we have the bill of rights and
we have all of these laws that protect

689
01:02:22,780 --> 01:02:28,330
against discrimination and I think a lot
of people know that those laws like our

690
01:02:28,330 --> 01:02:32,690
privacy laws do not work well enough but
certainly and other parts of the world

691
01:02:32,690 --> 01:02:37,930
people don't have those safeguards
people don't have that security and if

692
01:02:37,930 --> 01:02:41,649
we are not going to be a leader in
providing it to them then we're gonna

693
01:02:41,650 --> 01:02:46,440
lose out on the democratic benefit and
on the human rights benefits of

694
01:02:46,440 --> 01:02:51,080
providing security to everybody but I
don't hear that being a model from our

695
01:02:51,080 --> 01:02:58,580
government finally I want to talk about
freedom of expression and just briefly

696
01:02:58,580 --> 01:03:04,130
you know for all kinds of reasons we've
seen censorship on the internet whether

697
01:03:04,130 --> 01:03:08,380
its copyright or or that sort of thing
but but now that the physical

698
01:03:08,380 --> 01:03:13,020
architecture is so centralized it's
easier to control so here's an example

699
01:03:13,020 --> 01:03:18,900
our government and the UN have started
to ask platforms to police their

700
01:03:18,900 --> 01:03:24,550
networks for political speech and now
its radical speeds are terrorists Beach

701
01:03:24,550 --> 01:03:30,710
Isis video jihadis things but they're
even starting to ask to watch out for

702
01:03:30,710 --> 01:03:35,840
people who are becoming radicalized so I
can tell you if you look at what the FBI

703
01:03:35,840 --> 01:03:39,910
thinks are signs of radicalization we
have no idea

704
01:03:39,910 --> 01:03:43,819
FBI doesn't know psychologist nobody
knows what makes somebody around home

705
01:03:43,820 --> 01:03:48,150
what makes somebody area what makes
somebody have legitimate you know

706
01:03:48,150 --> 01:03:51,460
nonviolent political viewpoints and what
makes somebody who's going to be

707
01:03:51,460 --> 01:03:58,120
dangerous and violent but people are
rebelling against this I don't see

708
01:03:58,120 --> 01:04:01,999
people booing when Google says okay I'm
gonna

709
01:04:01,999 --> 01:04:08,279
you know I'm gonna be takin Asus videos
off youtube people aren't upset about

710
01:04:08,279 --> 01:04:12,679
that but so it goes then its revenge
porn but we have to understand that this

711
01:04:12,679 --> 01:04:19,959
censorship is censorship decisions are
inherently political because we don't

712
01:04:19,959 --> 01:04:24,489
see the same call for racist speech or
pictures of the confederate flag for

713
01:04:24,489 --> 01:04:29,159
that kind of thing and in the United
States we're not even seeing these laws

714
01:04:29,159 --> 01:04:33,609
that we can protest against what we see
is that the government or interest

715
01:04:33,609 --> 01:04:38,359
groups put pressure on the companies and
companies make these decisions because

716
01:04:38,359 --> 01:04:44,098
they want to have a service that appeals
to the majority of their users not to

717
01:04:44,099 --> 01:04:49,819
the edges not to the French not to the
radicals and but and so they censor and

718
01:04:49,819 --> 01:04:54,249
most people don't care but the end
result of that is that four people

719
01:04:54,249 --> 01:04:59,468
particularly new adopters around the
world needs new adopters around the

720
01:04:59,469 --> 01:05:02,999
world they don't really have a sense
necessarily of the broader internet

721
01:05:02,999 --> 01:05:10,388
there was a poll of Internet users in
Indonesia and lower percentage of people

722
01:05:10,389 --> 01:05:14,539
said they use the internet then said
they use Facebook they don't think about

723
01:05:14,539 --> 01:05:18,329
Facebook is using the internet a lot of
a news story is I saw her like you know

724
01:05:18,329 --> 01:05:21,529
is that funny they don't really know
that Facebook is the internet and I

725
01:05:21,529 --> 01:05:26,149
actually had the opposite reaction which
is Facebook is not the big the full

726
01:05:26,149 --> 01:05:31,239
Internet Facebook is a community a
narrower community that allows you to do

727
01:05:31,239 --> 01:05:35,399
particular things shows your particular
information based on my Facebook thinks

728
01:05:35,399 --> 01:05:39,759
you're gonna like but it doesn't give us
that global conversation it doesn't give

729
01:05:39,759 --> 01:05:45,179
us that radical freedom it doesn't have
everything on the shelf so what does

730
01:05:45,179 --> 01:05:51,169
this mean for the next twenty years now
the next twenty feet twenty years things

731
01:05:51,169 --> 01:05:56,158
will happen and no one will really know
why you'll be more ignorant about the

732
01:05:56,159 --> 01:06:01,199
world around you in the next 20 years
you'll mostly feel ok about it people

733
01:06:01,199 --> 01:06:05,539
mostly accept it because it's going to
affect minority is an edge cases and

734
01:06:05,539 --> 01:06:09,909
it's gonna be okay it's gonna work okay
in internet is still cool so we're still

735
01:06:09,909 --> 01:06:10,470
gonna happen

736
01:06:10,470 --> 01:06:16,270
good stuff the internet gonna become a
lot more like TV we're going to be

737
01:06:16,270 --> 01:06:20,630
watching videos and consuming and we're
not really gonna be able to reach that

738
01:06:20,630 --> 01:06:24,240
global audience I mean even if you have
a blog now to reach a global audience in

739
01:06:24,240 --> 01:06:29,700
its search engine optimization and CD
ends and all of that stuff it's not a

740
01:06:29,700 --> 01:06:35,250
level playing field that we once thought
it would be an existing power structures

741
01:06:35,250 --> 01:06:37,730
are going to be continued

742
01:06:37,730 --> 01:06:42,090
replicated and strengthened whether
that's in the field of security in the

743
01:06:42,090 --> 01:06:49,970
field of surveillance or in the field of
censorship or we have an alternative we

744
01:06:49,970 --> 01:06:54,299
can think globally instead of locally
and nationally about what we would want

745
01:06:54,300 --> 01:06:59,510
yes we need to guard against terrorist
attacks in New York but we cannot ignore

746
01:06:59,510 --> 01:07:04,020
the impact that something like crypto
back doors would have on journalists and

747
01:07:04,020 --> 01:07:08,560
human rights workers around the world we
can start thinking about technology is

748
01:07:08,560 --> 01:07:12,170
something we want to build
decentralization back in where possible

749
01:07:12,170 --> 01:07:18,000
give that power back to the people where
we can and part of restoring that

750
01:07:18,000 --> 01:07:22,359
balance of power is end-to-end
encryption we need end-to-end encryption

751
01:07:22,359 --> 01:07:27,098
so that when the government needs your
data instead of secretly going to level

752
01:07:27,099 --> 01:07:34,390
3 or to Google or two microsoftr to
Apple they have to come to us for it we

753
01:07:34,390 --> 01:07:38,730
need to have the government have hands
off of private technology development is

754
01:07:38,730 --> 01:07:41,690
not the government's business to tell us
how to design networks to be a

755
01:07:41,690 --> 01:07:45,830
surveillance friendly it's our business
to try to create technology that will

756
01:07:45,830 --> 01:07:51,740
give people the tools they need to have
a better life and a freer life we need

757
01:07:51,740 --> 01:07:55,990
to start being afraid of the right
things humans are really bad at

758
01:07:55,990 --> 01:08:00,750
understanding risk people are way more
afraid of sharks and they are cows to

759
01:08:00,750 --> 01:08:04,859
cows kill something like eight times
more people a year than sharks though

760
01:08:04,859 --> 01:08:11,460
its trail look it up the most dangerous
thing we do every day is getting a car

761
01:08:11,460 --> 01:08:15,599
by far the most dangerous thing we do
every day so we need to start being

762
01:08:15,599 --> 01:08:19,600
afraid of the right things we need to
learn you know what to accept and we

763
01:08:19,600 --> 01:08:21,790
need to address the right problems

764
01:08:21,790 --> 01:08:25,729
we need to modify our laws to be better
we need to get rid of the computer crime

765
01:08:25,729 --> 01:08:29,779
laws that way it's written we need to
modify the DMCA so it doesn't interfere

766
01:08:29,779 --> 01:08:34,589
with security research we need to look
at provisions of the Patriot Act and

767
01:08:34,589 --> 01:08:39,700
revise those as well as other foreign
intelligence surveillance laws and we

768
01:08:39,700 --> 01:08:43,490
need to do away with secret law we have
secret law in this country and is an

769
01:08:43,490 --> 01:08:53,630
abomination in the face of a democracy
to have that

770
01:08:53,630 --> 01:08:57,699
at the same time privacy isn't dead yet
we may have all this technology

771
01:08:57,698 --> 01:09:02,448
collecting information but we can use
law to provide safeguards for technology

772
01:09:02,448 --> 01:09:05,960
can't but we won't do it why don't we
amend the Electronic Communications

773
01:09:05,960 --> 01:09:10,339
Privacy Act to protect your email fully
why don't we amend Electronic

774
01:09:10,339 --> 01:09:14,750
Communications Privacy Act to protect
our geo location data either very simple

775
01:09:14,750 --> 01:09:18,330
basic proposals for congress won't do it
we have to get behind and we have to

776
01:09:18,330 --> 01:09:23,189
push now there's a possibility that
these provisions and I are just a few

777
01:09:23,189 --> 01:09:26,960
ideas but there's a possibility that
these things are going to work that

778
01:09:26,960 --> 01:09:30,460
these ideas are going to work and what
that means is that in the next 20 years

779
01:09:30,460 --> 01:09:34,359
instead of seeing the dream of Internet
freedom become true we're gonna see you

780
01:09:34,359 --> 01:09:38,630
get sicker and sicker and sicker until
it finally dies and then the Internet is

781
01:09:38,630 --> 01:09:40,600
going to be this slick

782
01:09:40,600 --> 01:09:47,359
controlled closed thing it'll be good
it'll be better than TV and radio but

783
01:09:47,359 --> 01:09:52,469
this is what it'll be fine if that's
true then what we need to do in the next

784
01:09:52,469 --> 01:09:57,880
20 years as we need to get ready and we
need to get ready to smash it apart and

785
01:09:57,880 --> 01:10:10,260
make something new and better thank you

786
01:10:10,260 --> 01:10:23,639
yeah ok then I'm gonna kick off the rest
of the show right here in the middle

787
01:10:23,639 --> 01:10:31,519
aisle were pulling that air wall
dividing this room into 2 so on one side

788
01:10:31,519 --> 01:10:36,829
on your right you're going to see
Patrick Ward writing bad malware for OSX

789
01:10:36,829 --> 01:10:42,630
and then on the other side we're gonna
see agent Ludwig's talk on Android

790
01:10:42,630 --> 01:10:46,909
security state of the union so if you
stay in the middle for too long you'll

791
01:10:46,909 --> 01:10:51,518
get sliced and to try to go out around
the sides thank you very much you guys

792
01:10:51,519 --> 01:10:51,750
around

