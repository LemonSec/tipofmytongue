1
00:00:00,000 --> 00:00:04,990
our talk today as graphic content to
head towards mobile automated scalable

2
00:00:04,990 --> 00:00:13,820
analysis of graphical images and our
speakers I thank you thank you thank

3
00:00:13,820 --> 00:00:19,869
everybody for coming at you can make it
so I'm gonna be talked about some work

4
00:00:19,869 --> 00:00:23,779
on how long I'm gonna be talking about
some work done by myself

5
00:00:23,779 --> 00:00:29,470
Joshua sacks who's in the audience and
give a talk earlier Robert go and David

6
00:00:29,470 --> 00:00:35,460
Slater of infancy labs on automated
scalable approach to analyzing images

7
00:00:35,460 --> 00:00:41,460
extracted from malware and I know many
of you may have come you know tempted by

8
00:00:41,460 --> 00:00:46,399
the graphic content title but I do have
to warn you this next image had to have

9
00:00:46,399 --> 00:00:51,900
part of it blocked out in order to be
able to show it to you today

10
00:00:51,900 --> 00:00:58,710
I'm gonna be talking about the status
quo of our analysis research and an

11
00:00:58,710 --> 00:01:03,470
opportunity for image analysis left by
this research and our method for

12
00:01:03,470 --> 00:01:08,400
comparing images and an automated way
and then to research experiments we did

13
00:01:08,400 --> 00:01:13,940
exploring how computer vision and
machine learning can be applied to this

14
00:01:13,940 --> 00:01:22,570
problem to do an automated and scalable
fashion so our story begins with our

15
00:01:22,570 --> 00:01:28,029
simple that's the sky and a lot of work
has gone into researching various

16
00:01:28,030 --> 00:01:33,310
techniques for analyzing samples like
him such as very static analysis

17
00:01:33,310 --> 00:01:40,210
techniques like execution or tainted
information flow in various techniques

18
00:01:40,210 --> 00:01:46,309
such as simulating user actions to
execute various code pass or capturing

19
00:01:46,310 --> 00:01:51,570
code that gets generated or unpacked at
one time but these techniques focus

20
00:01:51,570 --> 00:01:57,059
almost exclusively on the code of the
malware and the images sorry the

21
00:01:57,060 --> 00:02:00,240
resources of the malware which can
include things like natural language

22
00:02:00,240 --> 00:02:05,880
documents audiophiles video and images
tends to get pretty much dropped on the

23
00:02:05,880 --> 00:02:10,959
floor and left out of the research
completely and while a lot of time and

24
00:02:10,959 --> 00:02:16,000
money has gone into these research
techniques authors are working hard to

25
00:02:16,000 --> 00:02:21,880
and they're doing things like packing
their code including dummy code as a red

26
00:02:21,880 --> 00:02:27,290
herring or building in anti debugging
and anti via functionality so their

27
00:02:27,290 --> 00:02:33,700
codes days inactive under analysis we
believe these resources are a valuable

28
00:02:33,700 --> 00:02:39,720
alternative signal when and when
combined with more conventional kind of

29
00:02:39,720 --> 00:02:43,330
research techniques we stand a better
chance of getting around some of these

30
00:02:43,330 --> 00:02:52,680
obstacles that authors are throwing up
in a collection of 1.5 million our

31
00:02:52,680 --> 00:02:57,340
samples that we had in our lab we found
that over half of them had at least one

32
00:02:57,340 --> 00:03:01,819
image embedded in them like the ones you
see here and many had more than that

33
00:03:01,819 --> 00:03:07,048
that so this is where we chose to focus
on research and not only because of the

34
00:03:07,049 --> 00:03:11,200
prevalence of images in malware but also
because images play a special role in a

35
00:03:11,200 --> 00:03:16,798
way that malware tries to manipulate the
user after all you know a pack 2 p.m.

36
00:03:16,799 --> 00:03:21,370
detecting Trojan still needs an
attractive or trustworthy icons to get

37
00:03:21,370 --> 00:03:30,870
the user to click it so and of course
looking at the resources in malware is

38
00:03:30,870 --> 00:03:36,780
standard operating procedure for doing
manual analysis but by automating the

39
00:03:36,780 --> 00:03:41,370
process we can do this its scale over
hundreds of thousands of our samples in

40
00:03:41,370 --> 00:03:45,469
this opens the doors to be able to do
things like social cost millions of our

41
00:03:45,469 --> 00:03:50,060
samples for similar images or compare
the semantics of an image with other

42
00:03:50,060 --> 00:03:53,889
things that we know about it so if we
know we have this file we know it's a

43
00:03:53,889 --> 00:03:58,989
p/e binary but we also detected has a
Windows folder icon that's kind of

44
00:03:58,989 --> 00:04:06,629
strange but before I get into that gonna
talk a little bit about how we did the

45
00:04:06,629 --> 00:04:10,560
comparison of images and malware as
you're working through them we we

46
00:04:10,560 --> 00:04:15,540
quickly found that one does not simply
compare the images extracted from our

47
00:04:15,540 --> 00:04:21,500
trying to do things as simple as Andy 5
hash matching tends not to work because

48
00:04:21,500 --> 00:04:27,150
images get changed in compression the
resolution is distorted and the company

49
00:04:27,150 --> 00:04:33,758
changed deliberately by attackers trying
to evade such a technique so that we use

50
00:04:33,759 --> 00:04:41,520
what is called a hash and it's very
simple kind of computationally which is

51
00:04:41,520 --> 00:04:46,909
good because we're running this across
dozens of samples and just walking us

52
00:04:46,909 --> 00:04:53,030
through it start with an icon like this
PDF icon

53
00:04:53,030 --> 00:04:58,679
great skill and then you stretch or
shrink it to 32 by 32 pixel resolution

54
00:04:58,680 --> 00:05:02,940
and then converted to high contrast so
in order to do this you get the average

55
00:05:02,940 --> 00:05:07,719
valuable the pixels which is just in
some value between 0 and 255 to be on

56
00:05:07,719 --> 00:05:13,800
how great is for each pixel at all that
up by the total number of pixels

57
00:05:13,800 --> 00:05:19,680
elementary and entries pixel if it's
above that average we set it to 255 so

58
00:05:19,680 --> 00:05:23,900
in this example the light grey pixel
becomes weight and its below the average

59
00:05:23,900 --> 00:05:31,008
or equal to it then we set it to 0 so we
make the dark grey pixel black what you

60
00:05:31,009 --> 00:05:32,729
end up with is this

61
00:05:32,729 --> 00:05:38,120
monochromatic masterpiece which is a
pretty far cry from the imagery started

62
00:05:38,120 --> 00:05:42,849
with but this is actually good for us
because even if we start with this

63
00:05:42,849 --> 00:05:48,878
crummy low res version you know that's
kind of distorted we end up with a

64
00:05:48,879 --> 00:05:54,830
relatively similar representation and
average hash and this is ideal for

65
00:05:54,830 --> 00:05:58,770
trying to apply things like machine
learning you need it all to be kind of

66
00:05:58,770 --> 00:06:05,378
in the same representation we evaluated
average has averaged cash compared to 85

67
00:06:05,379 --> 00:06:10,560
and we go to do this week then clustered
$200 samples in terms of whether or not

68
00:06:10,560 --> 00:06:15,129
they had similar images and we found
that mi5 had better precision which

69
00:06:15,129 --> 00:06:20,130
means basically it got things wrong but
average cash had better recall which

70
00:06:20,130 --> 00:06:24,659
means it got more things rights and
maybe a little confusing it with your

71
00:06:24,659 --> 00:06:30,159
statistics are rusty just to walk you
through it if we had our sample suppose

72
00:06:30,159 --> 00:06:35,250
this box represents our sample and these
images were extracted from it

73
00:06:35,250 --> 00:06:39,919
average cash might detect these other
mouse temples as having similar images

74
00:06:39,919 --> 00:06:47,770
and this is relatively good the the
exact match in the top left it got it

75
00:06:47,770 --> 00:06:51,219
also got these kind of distorted
degraded versions in the bottom right

76
00:06:51,219 --> 00:06:55,460
but it got a false positive got these
question marks which we don't want

77
00:06:55,460 --> 00:07:00,460
that's not a correct match but the
false-positive is actually kind of

78
00:07:00,460 --> 00:07:05,159
tolerable because 22 human it's it's
relatively obvious that that's incorrect

79
00:07:05,159 --> 00:07:10,150
and you can kind of visually north at
the alternative if we were to look at

80
00:07:10,150 --> 00:07:18,799
2525 wouldn't get that false positive so
good friendly 525 also wouldn't get the

81
00:07:18,800 --> 00:07:20,129
degraded

82
00:07:20,129 --> 00:07:25,619
near-duplicate it would only get that
near that exact match and the near

83
00:07:25,619 --> 00:07:30,999
duplicates are really kind of what we're
trying to get out here so we we favor

84
00:07:30,999 --> 00:07:35,139
the kind of flexibility of average hash
even if it leads to some false positives

85
00:07:35,139 --> 00:07:42,580
so for our first experience we looked at
how we could detect and visualize these

86
00:07:42,580 --> 00:07:48,419
relationships between our samples and
kind of the idea if you could explore

87
00:07:48,419 --> 00:07:53,599
our samples social network through
through these shared images may be rare

88
00:07:53,599 --> 00:07:59,259
images like in this example this kind of
gun thing you can kind of begin to

89
00:07:59,259 --> 00:08:04,699
identify first isn't even malicious and
then you can apply the results of your

90
00:08:04,699 --> 00:08:08,819
analysis on other samples to this kind
of unknown samples so we know nothing

91
00:08:08,819 --> 00:08:14,139
else about this unknown symbol here we
can see that sharing an image with two

92
00:08:14,139 --> 00:08:20,879
samples we previously analyzed and
identified as being Trojans so the

93
00:08:20,879 --> 00:08:24,499
problems a little bit more complex than
just comparing two images though because

94
00:08:24,499 --> 00:08:29,999
actually comparing sets of images in our
samples so we want to recognize when

95
00:08:29,999 --> 00:08:33,819
there's missing image in one of the sets
so the formula we use the number of

96
00:08:33,818 --> 00:08:38,218
matching pairs which in this example is
too divided by the number of possible

97
00:08:38,219 --> 00:08:44,410
matching pairs which in this example is
three which gives us a 0.67% lady score

98
00:08:44,410 --> 00:08:48,639
in this example so we're kind of
penalizing the score here because he

99
00:08:48,639 --> 00:08:52,779
believes in providing you know partner
kitty to that poor lonely kid in simple

100
00:08:52,779 --> 00:09:02,779
be let's just intolerable so to show you
guys how this plays out

101
00:09:02,779 --> 00:09:12,010
ok so

102
00:09:12,010 --> 00:09:28,380
it searches a database for any samples
that share images with that one and so

103
00:09:28,380 --> 00:09:35,560
this image with the so each box in our
sample and the images in the box are

104
00:09:35,560 --> 00:09:40,979
images we extracted from it and the box
with the black borders on that I clicked

105
00:09:40,980 --> 00:09:47,600
on so you can kind of see this going out
there and playing out here so you can

106
00:09:47,600 --> 00:09:52,160
see it has Windows Installer icon which
is shared with with this other malware

107
00:09:52,160 --> 00:10:01,510
simple but the score as as you can see
right here is a 0.25% 14 images so

108
00:10:01,510 --> 00:10:04,720
that's kind of a reduced rate for
example let's do something a little bit

109
00:10:04,720 --> 00:10:15,410
more interesting so in this example the
simple I clicked on is again the one

110
00:10:15,410 --> 00:10:19,949
with a thick black border and this is
actually a really good example of kind

111
00:10:19,950 --> 00:10:23,790
of in an operational context if you were
an analyst and you had you know thousand

112
00:10:23,790 --> 00:10:27,719
temples dropping your desk and you have
to analyze these and maybe you know

113
00:10:27,720 --> 00:10:30,720
they're heavily obfuscator back to
something like that

114
00:10:30,720 --> 00:10:35,220
a quick way that you can get a sense of
them is is to throw them into a system

115
00:10:35,220 --> 00:10:40,710
like this and so in this example

116
00:10:40,710 --> 00:10:44,830
something I forgot to say the text
labels above the boxes are actually

117
00:10:44,830 --> 00:10:50,140
generated by a bee engines so it's a
Trojan that was detected as being a

118
00:10:50,140 --> 00:10:56,900
Trojan by antivirus but in this example
we have an ABC 053 that the hash of it

119
00:10:56,900 --> 00:11:04,079
because it just wasn't able to detect it
as being malware so we can see although

120
00:11:04,080 --> 00:11:08,130
we don't know that it's now we can see
that is sharing this kind of strange 7

121
00:11:08,130 --> 00:11:13,600
image with something that was detected
as as being a Trojan so knowing nothing

122
00:11:13,600 --> 00:11:20,010
else about it we can at least see
something suspicious going on here and

123
00:11:20,010 --> 00:11:22,800
it's also sharing the seven image with

124
00:11:22,800 --> 00:11:30,040
with other major symbol that has these
other icons in it and something else we

125
00:11:30,040 --> 00:11:34,849
added to this kind of in the idea of six
degrees of separation from Kevin Bacon

126
00:11:34,850 --> 00:11:39,519
is we added the second degree of
separation from from the sample that you

127
00:11:39,519 --> 00:11:43,240
click on so these other symbols that you
see kind of drifting to the left and the

128
00:11:43,240 --> 00:11:49,430
bottom they don't share that seven image
with image that I clicked on but they do

129
00:11:49,430 --> 00:11:56,550
have they do have these kind of like
green check boxes and red X's kind of

130
00:11:56,550 --> 00:12:02,439
like interface icons which which are
also in this sample which has the seven

131
00:12:02,440 --> 00:12:08,269
icon and this allows you to explore the
context that a sample is found in so you

132
00:12:08,269 --> 00:12:13,450
know you can imagine you can imagine
even if its members had labeled the

133
00:12:13,450 --> 00:12:16,810
sample in the center of the sample to
the right as being Trojans we could

134
00:12:16,810 --> 00:12:21,779
still get some sense that something
suspicious going on here by the fact

135
00:12:21,779 --> 00:12:25,700
that ok here's something was identified
as being a Trojan you know here as well

136
00:12:25,700 --> 00:12:30,430
so this just kind of allows you to kind
of broaden your your general awareness

137
00:12:30,430 --> 00:12:34,329
of the context for that simple

138
00:12:34,329 --> 00:12:43,170
so the second experiment we did was in
automatic classification of images so

139
00:12:43,170 --> 00:12:51,640
that's basically what what does it look
like so so in this example we have the

140
00:12:51,640 --> 00:12:56,920
Windows Windows System icon or Windows
folder pornographic images or AIM

141
00:12:56,920 --> 00:13:01,560
related and the idea here is as I said
before if you identify something as a

142
00:13:01,560 --> 00:13:04,329
Windows folder icon and it's an
executable for example that would be

143
00:13:04,329 --> 00:13:08,880
kinda suspicious also if someone just
download something with pornographic

144
00:13:08,880 --> 00:13:16,230
image or a game in their office that's
probably how risky is not something we

145
00:13:16,230 --> 00:13:22,380
want to allow and so by being able to
automatically detect that we can kind of

146
00:13:22,380 --> 00:13:29,180
just cut it off block it right there at
least alert that to security so how do

147
00:13:29,180 --> 00:13:32,510
you solve a problem like automated
malware image classification I'm glad

148
00:13:32,510 --> 00:13:33,180
you

149
00:13:33,180 --> 00:13:43,270
by the way this is this is about as
graphic as a presentation gets enjoy the

150
00:13:43,270 --> 00:13:48,060
way we did it is we used Google image
search results so we basically had a

151
00:13:48,060 --> 00:13:50,770
different key term for all the different
categories we wanted to capture some

152
00:13:50,770 --> 00:13:55,579
this example a search for Internet
Explorer and get mostly Internet

153
00:13:55,580 --> 00:14:01,760
Explorer icons the couple mylittlepony
guns aren't gonna worry about but the

154
00:14:01,760 --> 00:14:08,260
idea here is if you have you know I'm
elated pornographic you can plug that

155
00:14:08,260 --> 00:14:12,480
into something like Google Image and get
a bunch of kind of training data for

156
00:14:12,480 --> 00:14:17,540
that category so you get a bunch of
examples of of the category that you're

157
00:14:17,540 --> 00:14:19,719
looking for and then you can begin to do

158
00:14:19,720 --> 00:14:24,860
matching with images found in Delaware
so the way we did that matching was

159
00:14:24,860 --> 00:14:29,490
using machine learning a variety of
different techniques you guys through a

160
00:14:29,490 --> 00:14:33,040
super simple algorithm known as King
years neighbors just to give you the

161
00:14:33,040 --> 00:14:40,480
intuition behind it and this is kind of
a textbook example just to get the idea

162
00:14:40,480 --> 00:14:46,510
so if you mention a graph for the x-axis
is the number of rooms and the y axis is

163
00:14:46,510 --> 00:14:51,450
the square footage in and somebody's
home and then you went around you asked

164
00:14:51,450 --> 00:14:55,930
a bunch of people how many rooms in your
home with the square footage and do you

165
00:14:55,930 --> 00:15:01,300
live in an apartment in a house and then
you grab those points okay and then then

166
00:15:01,300 --> 00:15:04,890
you ask somebody the number of rooms in
square footage and they told you but for

167
00:15:04,890 --> 00:15:07,710
some reason they were really paranoid
they didn't want you to know if they

168
00:15:07,710 --> 00:15:13,150
lived in a house or apartment not the
most relevant example but the idea here

169
00:15:13,150 --> 00:15:18,130
would be that you would pick the paid
nearest neighbors where k is just some

170
00:15:18,130 --> 00:15:24,100
integer as appropriate data so this
example a pic 3 as you look at the three

171
00:15:24,100 --> 00:15:30,110
nearest neighbors and you just take the
majority vote so we have two houses one

172
00:15:30,110 --> 00:15:35,030
apartment so we figured out Mr paranoid
lives in a house and he's not very happy

173
00:15:35,030 --> 00:15:40,560
about that so applying that to to our
problem

174
00:15:40,560 --> 00:15:45,410
one of the things we tried was kind of
classifying them by images by their

175
00:15:45,410 --> 00:15:53,380
color histogram so that's basically the
overall color of the icon it's a little

176
00:15:53,380 --> 00:15:57,439
hard to visualize and a presentation
because it's more than two dimensions

177
00:15:57,440 --> 00:16:01,870
but if you can imagine kind of a space
in which the icon sir floating around

178
00:16:01,870 --> 00:16:08,650
sort of clusters where similar colors
are near to each other that's kind of

179
00:16:08,650 --> 00:16:13,339
the idea and then so we have a bunch of
labels for all of these reno you know

180
00:16:13,339 --> 00:16:17,839
from our Google Image technique we know
what a lot of these are we have the icon

181
00:16:17,839 --> 00:16:20,460
in the center right question mark

182
00:16:20,460 --> 00:16:26,770
we don't know what it is so they can use
their burgers in this example we take

183
00:16:26,770 --> 00:16:33,370
seven so we find we have for flash icons
and three dialogue icon so this is a

184
00:16:33,370 --> 00:16:38,870
flash I so that's that's the basic idea
here and it works reasonably well in

185
00:16:38,870 --> 00:16:43,920
this example for flash and in some other
cases color turns out to be very

186
00:16:43,920 --> 00:16:52,520
distinguishing factor but not always in
this case we have this kind of blue kind

187
00:16:52,520 --> 00:16:57,600
of probably antivirus shield and it's
just surrounded by things all sorts of

188
00:16:57,600 --> 00:17:03,310
random things so it's not really blue
isn't a distinguishing feature in this

189
00:17:03,310 --> 00:17:10,250
case there's been a lot of advances in
and using neural networks to identify

190
00:17:10,250 --> 00:17:14,819
which features are important so is it
overall color is that the shape on those

191
00:17:14,819 --> 00:17:19,280
kinds of things and that's probably what
we'd be looking at next to kind of a

192
00:17:19,280 --> 00:17:23,329
kind of a home this on this experiment
together to kind of operational levels

193
00:17:23,329 --> 00:17:30,129
of accuracy so in conclusion the graphic
content that we have fun in malware is

194
00:17:30,130 --> 00:17:34,030
is really underutilized and research and
it's basically just being left out

195
00:17:34,030 --> 00:17:37,980
completely and by applying computer
vision and sprinkling a little bit of

196
00:17:37,980 --> 00:17:42,679
that machine learning fairy dust on it
we can make this lost signal exploitable

197
00:17:42,679 --> 00:17:49,170
and accessible at a large skill so that
the next time we see this guy we have a

198
00:17:49,170 --> 00:17:50,740
new tool to fight him off with

199
00:17:50,740 --> 00:18:06,070
they do

200
00:18:06,070 --> 00:18:26,540
research projects it wasn't something
that we actually released but I get your

201
00:18:26,540 --> 00:18:37,779
business card afterwards and shit

202
00:18:37,779 --> 00:18:49,849
is when you have that that black and
white version that's basically a vector

203
00:18:49,849 --> 00:18:55,968
just zeros and ones right there was her
black and one white pixels and then you

204
00:18:55,969 --> 00:19:01,519
can just do distance between the images
so basically you know how many different

205
00:19:01,519 --> 00:19:29,239
so that's how we do that a lot of these
guys were kind of already know what the

206
00:19:29,239 --> 00:19:32,879
antivirus said had labeled with them so
that's basically what we worked off with

207
00:19:32,879 --> 00:19:48,110
we trusted that from our source that
those who are correct but that's all

208
00:19:48,110 --> 00:20:09,020
attribution yeah okay so this is my team
and we knew where because it was

209
00:20:09,020 --> 00:20:48,090
delivered to us and analysis tool in the
toolbox yes so he could use that when he

210
00:20:48,090 --> 00:21:30,530
was going to them our so that was the
idea

211
00:21:30,530 --> 00:22:00,370
work being a classification is like a
problem as I showed you a custom color

212
00:22:00,370 --> 00:22:15,879
histogram you know we we begin exploring
other techniques for that

213
00:22:15,879 --> 00:22:26,468
like my little pony

214
00:22:26,469 --> 00:22:32,369
just messing with the results so we
probably had i dont 2240 training

215
00:22:32,369 --> 00:22:39,369
examples for the different categories
and so we were running process across I

216
00:22:39,369 --> 00:23:05,179
think we had about $400,000 samples with
images in them

217
00:23:05,180 --> 00:23:14,960
questions so do we take into account the
rarity of an image when we're looking at

218
00:23:14,960 --> 00:23:24,360
how we don't but that's a good idea like
you know and like the natural language

219
00:23:24,360 --> 00:23:28,770
processing stuff they do like to you
idea to kind of get rid of common words

220
00:23:28,770 --> 00:23:34,500
and stuff like that so that I didn't
apply to play something for future

221
00:23:34,500 --> 00:23:45,750
research

222
00:23:45,750 --> 00:23:52,180
it's actually pretty fast we used the
library lawn I don't know if you've

223
00:23:52,180 --> 00:23:56,390
heard of it is actually very self
explanatory it's fast lightweight

224
00:23:56,390 --> 00:24:03,530
approximate nearest neighbors so it uses
an hour that's much more complicated the

225
00:24:03,530 --> 00:24:07,700
nearest neighbors to approximate the
results and I didn't have to do a

226
00:24:07,700 --> 00:24:13,500
comparison which part of what makes you
so very very expensive so using that

227
00:24:13,500 --> 00:24:15,400
we're actually able to do this

228
00:24:15,400 --> 00:24:21,590
across you know hundreds of thousands of
samples in relatively small amounts of

229
00:24:21,590 --> 00:24:41,530
time

230
00:24:41,530 --> 00:25:02,889
I think our focus more on just making
sure every Josh with actually catching a

231
00:25:02,890 --> 00:25:08,250
similar images but yeah that's certainly
something something to make sure that

232
00:25:08,250 --> 00:25:10,710
would need one useful application of it

