1
00:00:00,000 --> 00:00:07,670
good morning everyone how's it going my

2
00:00:03,629 --> 00:00:09,900
name is Vijay and I'm engine ok ok and

3
00:00:07,670 --> 00:00:11,840
today we would like to speak on

4
00:00:09,900 --> 00:00:14,280
artificial intelligence control

5
00:00:11,840 --> 00:00:18,900
exterminating defects before they cost

6
00:00:14,280 --> 00:00:21,869
minute everyone sitting in this room has

7
00:00:18,900 --> 00:00:25,380
an unique identity which is defined by

8
00:00:21,869 --> 00:00:28,439
his or her face the face is a crucial

9
00:00:25,380 --> 00:00:30,960
aspect of identity so what if this

10
00:00:28,439 --> 00:00:35,719
particular identity is stolen from you

11
00:00:30,960 --> 00:00:40,350
and is misrepresented in form of videos

12
00:00:35,719 --> 00:00:45,260
this is happening now and as AI has so

13
00:00:40,350 --> 00:00:49,129
progress if that is trained well but

14
00:00:45,260 --> 00:00:52,320
sufficient sources it can generate

15
00:00:49,129 --> 00:00:56,099
seemingly real fake and deceptive videos

16
00:00:52,320 --> 00:00:58,859
which are known as defects so defects

17
00:00:56,100 --> 00:01:02,300
are created to or can be created to

18
00:00:58,859 --> 00:01:06,659
target people of interest of people of

19
00:01:02,300 --> 00:01:07,979
importance and we would be looking a bit

20
00:01:06,659 --> 00:01:10,229
on that part

21
00:01:07,979 --> 00:01:15,869
so before we start off with the

22
00:01:10,229 --> 00:01:18,680
presentation I would like you to tell

23
00:01:15,869 --> 00:01:21,299
you about some disclaimer first of all

24
00:01:18,680 --> 00:01:25,920
the solution obtained from this research

25
00:01:21,299 --> 00:01:27,450
primarily focuses on videos which are

26
00:01:25,920 --> 00:01:30,780
created using face mapping techniques

27
00:01:27,450 --> 00:01:33,390
and portrait videos and in which only

28
00:01:30,780 --> 00:01:34,860
one actor is observed and the second

29
00:01:33,390 --> 00:01:38,040
thing is the presentation does not

30
00:01:34,860 --> 00:01:40,740
contain any code snippets so it's it's

31
00:01:38,040 --> 00:01:43,320
really you know even if anyone doesn't

32
00:01:40,740 --> 00:01:46,429
know anything about these fakes this is

33
00:01:43,320 --> 00:01:50,758
really good this is the agenda so

34
00:01:46,430 --> 00:01:54,149
defects what are they what makes them so

35
00:01:50,759 --> 00:01:57,979
different some real-life scenarios in

36
00:01:54,149 --> 00:01:59,850
which defects can be implemented or used

37
00:01:57,979 --> 00:02:02,070
against us in future

38
00:01:59,850 --> 00:02:05,880
and the ingredients required to create

39
00:02:02,070 --> 00:02:08,818
defects cutting poison with poison that

40
00:02:05,880 --> 00:02:11,008
some use of fancy words it's just a tiny

41
00:02:08,818 --> 00:02:12,069
fast and simple solution to curb the

42
00:02:11,008 --> 00:02:14,920
fakes

43
00:02:12,069 --> 00:02:19,540
and we're also looking at defects

44
00:02:14,920 --> 00:02:21,760
through different aspects you know this

45
00:02:19,540 --> 00:02:25,209
is like one of the researchers which we

46
00:02:21,760 --> 00:02:28,030
thought to cite in which the biological

47
00:02:25,209 --> 00:02:30,519
aspect has been code the limitations of

48
00:02:28,030 --> 00:02:33,250
that particular of solution and how does

49
00:02:30,519 --> 00:02:35,829
the future look in terms of defects with

50
00:02:33,250 --> 00:02:40,090
all that said some valuable takeaways in

51
00:02:35,829 --> 00:02:43,540
form of blackhat soundbites so as I

52
00:02:40,090 --> 00:02:46,060
mentioned earlier that the fates are

53
00:02:43,540 --> 00:02:47,858
created or can be created in order to

54
00:02:46,060 --> 00:02:50,469
target people of interest people of

55
00:02:47,859 --> 00:02:54,340
importance like competitions celebrities

56
00:02:50,469 --> 00:03:00,340
and even common people so it's it's not

57
00:02:54,340 --> 00:03:03,010
only to that part and this videos are

58
00:03:00,340 --> 00:03:07,269
created in such a manner that they can

59
00:03:03,010 --> 00:03:11,590
fool human eye and human can easily get

60
00:03:07,269 --> 00:03:13,989
tricked believe what they see or what

61
00:03:11,590 --> 00:03:19,810
actual the actor wants them to see or

62
00:03:13,989 --> 00:03:23,349
believe so let me let me delve into the

63
00:03:19,810 --> 00:03:28,720
many results of defects what hazards

64
00:03:23,349 --> 00:03:31,388
defects have the very first is cyber

65
00:03:28,720 --> 00:03:33,849
propaganda so imagine a scenario where

66
00:03:31,389 --> 00:03:36,970
in a fake video is created of a

67
00:03:33,849 --> 00:03:39,849
politician you know

68
00:03:36,970 --> 00:03:41,620
speaking Italian or passing off some

69
00:03:39,849 --> 00:03:43,689
comments just few hours before the

70
00:03:41,620 --> 00:03:46,209
voting day and then this video is

71
00:03:43,689 --> 00:03:50,439
uploaded and circulated over the social

72
00:03:46,209 --> 00:03:53,530
media I guess you know this this will

73
00:03:50,439 --> 00:03:55,989
obviously or potentially alter the

74
00:03:53,530 --> 00:04:00,099
outcome of elections as people after

75
00:03:55,989 --> 00:04:01,540
watching this video will unknowingly go

76
00:04:00,099 --> 00:04:04,418
ahead and elect someone who is not

77
00:04:01,540 --> 00:04:07,888
capable enough this is pretty scary when

78
00:04:04,419 --> 00:04:11,319
we see or when we talk about politicians

79
00:04:07,889 --> 00:04:13,000
because this can create tensions not

80
00:04:11,319 --> 00:04:17,769
only within the country but also on a

81
00:04:13,000 --> 00:04:21,279
global level fake leaves equal to D fake

82
00:04:17,769 --> 00:04:23,469
news so not only in 2018 but we have

83
00:04:21,279 --> 00:04:27,159
been seeing from a past couple of years

84
00:04:23,469 --> 00:04:30,669
that politicians have been accusing news

85
00:04:27,159 --> 00:04:32,380
media channels of spreading fake news so

86
00:04:30,669 --> 00:04:35,979
there are news here and around and you

87
00:04:32,380 --> 00:04:38,770
you read a lot about that too so we

88
00:04:35,979 --> 00:04:40,779
believe that the fake news will quickly

89
00:04:38,770 --> 00:04:44,310
replace fake news in the near future

90
00:04:40,779 --> 00:04:46,690
where a fabricated videos would be used

91
00:04:44,310 --> 00:04:50,199
turning out to trust issues

92
00:04:46,690 --> 00:04:55,900
now defects are rapidly progressing as

93
00:04:50,199 --> 00:04:58,389
we are talking right now and this can

94
00:04:55,900 --> 00:05:01,510
lead to trust issues so even genuine

95
00:04:58,389 --> 00:05:04,029
online video content can raise or can

96
00:05:01,510 --> 00:05:07,349
invite suspicion and can raise questions

97
00:05:04,029 --> 00:05:07,349
to its validity

98
00:05:08,130 --> 00:05:15,010
politicians can simply a flat-out or

99
00:05:11,580 --> 00:05:16,750
cast doubt on the genuine videos which

100
00:05:15,010 --> 00:05:17,289
have brought embarrassment through

101
00:05:16,750 --> 00:05:20,199
themselves

102
00:05:17,289 --> 00:05:22,150
stating that the video is fake or

103
00:05:20,199 --> 00:05:25,810
fabricated and there have been

104
00:05:22,150 --> 00:05:28,510
situations in that I would be I won't be

105
00:05:25,810 --> 00:05:31,930
talking deep into that moving over to

106
00:05:28,510 --> 00:05:35,289
disinformation campaigns so the defects

107
00:05:31,930 --> 00:05:38,589
are circulated or social media can add

108
00:05:35,289 --> 00:05:41,650
fuel to disinformation campaigns so this

109
00:05:38,589 --> 00:05:44,469
can turn into severe violence within

110
00:05:41,650 --> 00:05:47,739
different countries and even can bring

111
00:05:44,469 --> 00:05:51,310
out social unrest emotional distress

112
00:05:47,740 --> 00:05:54,029
so the revenge pornographic fake videos

113
00:05:51,310 --> 00:05:58,500
which are created using applications or

114
00:05:54,029 --> 00:06:01,779
using code can certainly cause trauma

115
00:05:58,500 --> 00:06:05,680
depression in one's life and can even

116
00:06:01,779 --> 00:06:06,779
ruin his or her reputation and social

117
00:06:05,680 --> 00:06:11,039
standing

118
00:06:06,779 --> 00:06:13,270
what if deface become ubiquitous so

119
00:06:11,039 --> 00:06:16,779
let's consider a hypothetical scenario

120
00:06:13,270 --> 00:06:20,020
in which a large volume of D fake videos

121
00:06:16,779 --> 00:06:25,300
are uploaded and circulated over social

122
00:06:20,020 --> 00:06:27,520
media platforms and people so that

123
00:06:25,300 --> 00:06:30,070
people can watch and can make decisions

124
00:06:27,520 --> 00:06:32,440
according to it the consequences are

125
00:06:30,070 --> 00:06:35,380
pretty scary and the last one is

126
00:06:32,440 --> 00:06:38,139
morality versus legality so you know

127
00:06:35,380 --> 00:06:40,289
defects become popular when

128
00:06:38,139 --> 00:06:44,610
fake pornographic videos of celebrities

129
00:06:40,289 --> 00:06:44,610
started making the rounds online and

130
00:06:44,879 --> 00:06:52,930
this you know this obviously violates

131
00:06:49,270 --> 00:06:56,198
the rules of consent as well as the

132
00:06:52,930 --> 00:06:58,360
rights the victims rights to privacy so

133
00:06:56,199 --> 00:07:00,300
defects created using different

134
00:06:58,360 --> 00:07:04,949
applications freely available

135
00:07:00,300 --> 00:07:08,439
applications it can or we believe that

136
00:07:04,949 --> 00:07:13,000
they can be a form of abuse to someone's

137
00:07:08,439 --> 00:07:17,830
life so this were a few of the hazards

138
00:07:13,000 --> 00:07:20,469
of defects now moving over what it takes

139
00:07:17,830 --> 00:07:23,580
to prepare a de fake so all it takes is

140
00:07:20,469 --> 00:07:27,340
a is a gaming laptop internet connection

141
00:07:23,580 --> 00:07:29,770
some passion and patience of course and

142
00:07:27,340 --> 00:07:32,979
maybe some rudimentary knowledge of

143
00:07:29,770 --> 00:07:35,469
neural networks you obviously need a

144
00:07:32,979 --> 00:07:37,719
large data set of images which are

145
00:07:35,469 --> 00:07:40,870
freely available as in the statistics

146
00:07:37,719 --> 00:07:43,659
say that in the Year 2015 to 2016 there

147
00:07:40,870 --> 00:07:48,069
were around 24 billion photos uploaded

148
00:07:43,659 --> 00:07:52,180
or or the Internet so you can you know

149
00:07:48,069 --> 00:07:53,770
easily grab those photos and videos if

150
00:07:52,180 --> 00:07:54,879
in case if you are thinking that you

151
00:07:53,770 --> 00:07:56,620
know if you don't have any knowledge

152
00:07:54,879 --> 00:07:59,889
about neural networks don't worry about

153
00:07:56,620 --> 00:08:01,949
it some of the generalists people have

154
00:07:59,889 --> 00:08:04,930
already ease of the process by creating

155
00:08:01,949 --> 00:08:08,169
applications which are available on

156
00:08:04,930 --> 00:08:10,569
various platforms which help you to

157
00:08:08,169 --> 00:08:15,310
create d fake videos that help one

158
00:08:10,569 --> 00:08:19,000
single click so this is how a a

159
00:08:15,310 --> 00:08:21,750
particular D fake is created if in case

160
00:08:19,000 --> 00:08:24,789
if you don't want to work with code and

161
00:08:21,750 --> 00:08:25,240
now I would be handing over to my friend

162
00:08:24,789 --> 00:08:29,229
gironjin

163
00:08:25,240 --> 00:08:30,879
would be talking myth about defects okay

164
00:08:29,229 --> 00:08:33,010
thank you Vijay for walking us through

165
00:08:30,879 --> 00:08:36,130
different aspects of defects and

166
00:08:33,010 --> 00:08:38,529
alerting us about their hazards so now

167
00:08:36,130 --> 00:08:40,510
as he said that the there's only

168
00:08:38,529 --> 00:08:42,610
rudimentary knowledge required to create

169
00:08:40,510 --> 00:08:45,399
deep fakes so let us get a sneak peek

170
00:08:42,610 --> 00:08:49,149
into how deep fakes can be created now

171
00:08:45,399 --> 00:08:52,030
what we see in this image is a simple

172
00:08:49,149 --> 00:08:54,250
auto encoder so an auto encoder has

173
00:08:52,030 --> 00:08:55,870
- neural networks the first one is the

174
00:08:54,250 --> 00:08:58,780
encoder and the second one is the

175
00:08:55,870 --> 00:09:01,170
decoder so the encoder has the job of

176
00:08:58,780 --> 00:09:04,209
taking the input image and grabbing the

177
00:09:01,170 --> 00:09:06,640
essential representations and mapping it

178
00:09:04,210 --> 00:09:08,890
to a lower dimensional space while the

179
00:09:06,640 --> 00:09:11,050
decoder takes it as the input and tries

180
00:09:08,890 --> 00:09:14,140
to reconstruct the image as accurately

181
00:09:11,050 --> 00:09:18,250
as possible now if you do it in multiple

182
00:09:14,140 --> 00:09:20,650
rounds the network gets trained so good

183
00:09:18,250 --> 00:09:22,000
that you may not be able to get the

184
00:09:20,650 --> 00:09:25,180
difference between the original image

185
00:09:22,000 --> 00:09:27,130
and the reconstructed image so this is

186
00:09:25,180 --> 00:09:29,310
the algorithm that is used to create

187
00:09:27,130 --> 00:09:32,500
defects using face whopping technique

188
00:09:29,310 --> 00:09:35,410
now to perform face Wapping we need to

189
00:09:32,500 --> 00:09:37,360
autoencoders now a consider you want to

190
00:09:35,410 --> 00:09:40,180
have face walks between person a and

191
00:09:37,360 --> 00:09:42,400
person B so by doing this the encoder

192
00:09:40,180 --> 00:09:45,489
will get trained to pick up key features

193
00:09:42,400 --> 00:09:47,430
from person s face the decoder will also

194
00:09:45,490 --> 00:09:50,080
get trained to extract complex features

195
00:09:47,430 --> 00:09:53,349
from the latent representation of person

196
00:09:50,080 --> 00:09:55,510
ace face so similar we trained the first

197
00:09:53,350 --> 00:09:57,790
auto encoder similarly we will train

198
00:09:55,510 --> 00:10:00,880
second auto encoder on person B space

199
00:09:57,790 --> 00:10:03,520
now after training both the networks we

200
00:10:00,880 --> 00:10:07,120
will just swap the decoders to get the

201
00:10:03,520 --> 00:10:09,880
face walks now we all know that video is

202
00:10:07,120 --> 00:10:11,710
nothing but a frames which are bundled

203
00:10:09,880 --> 00:10:14,110
together at a particular frame rate so

204
00:10:11,710 --> 00:10:19,330
if we have enough computing resources

205
00:10:14,110 --> 00:10:23,320
and data data data then we can easily do

206
00:10:19,330 --> 00:10:25,420
this and create a deep fake video now

207
00:10:23,320 --> 00:10:28,090
with the ever-increasing data on the

208
00:10:25,420 --> 00:10:30,729
social media it becomes very important

209
00:10:28,090 --> 00:10:32,890
to develop systems that monitor the

210
00:10:30,730 --> 00:10:35,890
quality of content on the social media

211
00:10:32,890 --> 00:10:37,780
and now as vijay mentioned earlier the

212
00:10:35,890 --> 00:10:39,730
presence of deep fake videos on the

213
00:10:37,780 --> 00:10:42,280
internet can skyrocket at any time

214
00:10:39,730 --> 00:10:44,740
therefore we need a technology that has

215
00:10:42,280 --> 00:10:46,870
the ability to scan the content that we

216
00:10:44,740 --> 00:10:49,540
upload on the social media and flag it

217
00:10:46,870 --> 00:10:51,730
or block it so we would like to share

218
00:10:49,540 --> 00:10:53,079
the results of some experiments that we

219
00:10:51,730 --> 00:10:57,040
carried out in confirming the

220
00:10:53,080 --> 00:11:00,180
authenticity of videos now before we

221
00:10:57,040 --> 00:11:03,060
look into that again let let us take a

222
00:11:00,180 --> 00:11:05,439
sneak peek into face recognition

223
00:11:03,060 --> 00:11:05,888
researchers at Google released a model

224
00:11:05,440 --> 00:11:07,779
called as

225
00:11:05,889 --> 00:11:11,589
facelet model which has an amazing

226
00:11:07,779 --> 00:11:13,329
accuracy of detecting human faces now

227
00:11:11,589 --> 00:11:16,660
the tip net loss optimization function

228
00:11:13,329 --> 00:11:17,709
is the peculiarity of this model now how

229
00:11:16,660 --> 00:11:19,929
does it work

230
00:11:17,709 --> 00:11:23,138
consider the anchor image as the image

231
00:11:19,929 --> 00:11:25,358
of person a and the positive image is

232
00:11:23,139 --> 00:11:27,429
again the image of person a in some

233
00:11:25,359 --> 00:11:30,279
different setting while the negative

234
00:11:27,429 --> 00:11:33,189
image is the image of person B so the

235
00:11:30,279 --> 00:11:34,899
face recognition task is to identify the

236
00:11:33,189 --> 00:11:36,429
anchor image and the positive image as

237
00:11:34,899 --> 00:11:38,829
the images of the same person

238
00:11:36,429 --> 00:11:40,869
while a classify the images of anchor

239
00:11:38,829 --> 00:11:43,899
image a negative image as the faces of

240
00:11:40,869 --> 00:11:46,989
other person so the face plate model

241
00:11:43,899 --> 00:11:49,929
what essentially does is it it gives the

242
00:11:46,989 --> 00:11:52,989
output of 128 B encoding of any image

243
00:11:49,929 --> 00:11:55,509
and reduces the task of face recognition

244
00:11:52,989 --> 00:11:56,679
which is complex into a very simple task

245
00:11:55,509 --> 00:12:00,790
of finding the Euclidean distance

246
00:11:56,679 --> 00:12:02,889
between two vectors so you you might

247
00:12:00,790 --> 00:12:05,169
also remember unlocking your phone by

248
00:12:02,889 --> 00:12:07,179
just looking at it or you might find

249
00:12:05,169 --> 00:12:09,459
your name against a group photo which

250
00:12:07,179 --> 00:12:11,410
you uploaded in the Facebook so actually

251
00:12:09,459 --> 00:12:13,988
this is the same technology which tries

252
00:12:11,410 --> 00:12:16,738
to identify the human face once it's won

253
00:12:13,989 --> 00:12:20,199
once it is trained to detect that face

254
00:12:16,739 --> 00:12:22,899
now this is the this is a block diagram

255
00:12:20,199 --> 00:12:25,179
of our solution we train the face net

256
00:12:22,899 --> 00:12:27,339
model on wide variety of images of

257
00:12:25,179 --> 00:12:30,160
people like politicians celebrities and

258
00:12:27,339 --> 00:12:32,829
sportsmen we obtain the best possible

259
00:12:30,160 --> 00:12:35,259
face net and coatings for their images

260
00:12:32,829 --> 00:12:38,319
and stored them into a database against

261
00:12:35,259 --> 00:12:39,759
their labels now we come across a

262
00:12:38,319 --> 00:12:41,529
portrait now when we come across a

263
00:12:39,759 --> 00:12:43,389
portrait video which is randomly picked

264
00:12:41,529 --> 00:12:45,369
up from the internet then the foot in

265
00:12:43,389 --> 00:12:48,220
the first step we break that video into

266
00:12:45,369 --> 00:12:51,069
frames and we process the video on the

267
00:12:48,220 --> 00:12:53,019
on a per frame basis now for each frame

268
00:12:51,069 --> 00:12:54,998
we get the face net encoding and try to

269
00:12:53,019 --> 00:12:57,009
find if a similar encoding is already

270
00:12:54,999 --> 00:12:59,980
present into our database now if there

271
00:12:57,009 --> 00:13:02,259
is a hit then in the end our prototype

272
00:12:59,980 --> 00:13:04,269
will give us the list of probability

273
00:13:02,259 --> 00:13:06,189
scores which will indicate the

274
00:13:04,269 --> 00:13:08,709
possibility of that person being

275
00:13:06,189 --> 00:13:12,279
actually present in that video so this

276
00:13:08,709 --> 00:13:16,268
might help in gauging the authenticity

277
00:13:12,279 --> 00:13:18,700
of the video so a low probability score

278
00:13:16,269 --> 00:13:21,100
which is less than 50%

279
00:13:18,700 --> 00:13:23,200
might say that the video is fishy or it

280
00:13:21,100 --> 00:13:24,730
might be a defect while a higher

281
00:13:23,200 --> 00:13:27,520
probability score which would be

282
00:13:24,730 --> 00:13:31,030
generally more than 50% would say that

283
00:13:27,520 --> 00:13:34,000
the video might be genuine now during

284
00:13:31,030 --> 00:13:36,040
our experiments we used 26 deep fake

285
00:13:34,000 --> 00:13:39,100
videos that were openly available on the

286
00:13:36,040 --> 00:13:42,300
internet and at the same time we also

287
00:13:39,100 --> 00:13:44,530
used genuine videos of the same persons

288
00:13:42,300 --> 00:13:48,130
we have recorded some successful

289
00:13:44,530 --> 00:13:50,470
classifications for the demo and let us

290
00:13:48,130 --> 00:13:53,020
see the first one now this is the gif

291
00:13:50,470 --> 00:13:55,000
which we have which we obtained from a

292
00:13:53,020 --> 00:13:57,730
video and this is a genuine video of mr.

293
00:13:55,000 --> 00:14:00,490
Nicolas Cage now when we fed this video

294
00:13:57,730 --> 00:14:02,860
as an input to our prototype we got this

295
00:14:00,490 --> 00:14:04,450
output so a prototype says that there is

296
00:14:02,860 --> 00:14:07,990
eighty nine point two seven percent

297
00:14:04,450 --> 00:14:11,050
chance that the video that mr. Nick

298
00:14:07,990 --> 00:14:13,930
exists in that video so there is a high

299
00:14:11,050 --> 00:14:15,449
chance that mr. Nick there's a high

300
00:14:13,930 --> 00:14:19,750
chance that this video might be genuine

301
00:14:15,450 --> 00:14:22,300
now let us take another example now this

302
00:14:19,750 --> 00:14:23,800
is the video which we which we worked on

303
00:14:22,300 --> 00:14:26,589
this is also again a genuine video of

304
00:14:23,800 --> 00:14:28,689
mr. Donald Trump and when we fed this

305
00:14:26,590 --> 00:14:31,330
video to our prototype we got this

306
00:14:28,690 --> 00:14:33,400
output so there is 92 percent chance

307
00:14:31,330 --> 00:14:35,740
that mr. Trump is actually present in

308
00:14:33,400 --> 00:14:38,829
this video now here it goes the

309
00:14:35,740 --> 00:14:40,630
interesting one so now this is a defect

310
00:14:38,830 --> 00:14:43,150
video now this was freely available on

311
00:14:40,630 --> 00:14:45,010
the internet now you can see that mr.

312
00:14:43,150 --> 00:14:47,410
Trump's face is swapped with mr. Nick's

313
00:14:45,010 --> 00:14:50,080
face so now when we pass this video as

314
00:14:47,410 --> 00:14:51,880
an input we got this as the output so

315
00:14:50,080 --> 00:14:53,890
the our prototype thinks that there is

316
00:14:51,880 --> 00:14:55,630
only seven percent chance that mr. Nick

317
00:14:53,890 --> 00:14:58,300
is actually present in that video and

318
00:14:55,630 --> 00:15:01,360
there is no chance that mr. Trump is

319
00:14:58,300 --> 00:15:03,400
present in the video now for the

320
00:15:01,360 --> 00:15:04,900
simplicity of our implementation we just

321
00:15:03,400 --> 00:15:07,090
worked on four end codings which are

322
00:15:04,900 --> 00:15:12,069
present over here and we represented the

323
00:15:07,090 --> 00:15:13,840
next class as the unknown so now we know

324
00:15:12,070 --> 00:15:15,910
that our network was trained on these

325
00:15:13,840 --> 00:15:17,620
foreign codings and it was our

326
00:15:15,910 --> 00:15:19,870
assumption that we will be only working

327
00:15:17,620 --> 00:15:22,270
on videos either the defect videos are

328
00:15:19,870 --> 00:15:24,040
the genuine videos of the of these four

329
00:15:22,270 --> 00:15:26,140
persons now the Morris is that there's

330
00:15:24,040 --> 00:15:29,310
only 7% chance that mr. Nick is present

331
00:15:26,140 --> 00:15:32,290
so this is fishy

332
00:15:29,310 --> 00:15:33,878
now scope for improvement

333
00:15:32,290 --> 00:15:35,978
we believe that there is a huge scope of

334
00:15:33,879 --> 00:15:39,100
improvement for our model and we are

335
00:15:35,979 --> 00:15:41,799
constantly working on improving the

336
00:15:39,100 --> 00:15:43,720
performance we believe that augmenting

337
00:15:41,799 --> 00:15:46,149
our solution with these four techniques

338
00:15:43,720 --> 00:15:48,220
will make it more effective the first

339
00:15:46,149 --> 00:15:50,499
one is the state of the art technology

340
00:15:48,220 --> 00:15:52,329
which is used which is the face live

341
00:15:50,499 --> 00:15:55,679
Innes detections which is used to unlock

342
00:15:52,329 --> 00:15:58,449
the devices using face recognition and

343
00:15:55,679 --> 00:16:01,029
we believe that a slight modification to

344
00:15:58,449 --> 00:16:03,849
this technique an alteration to our

345
00:16:01,029 --> 00:16:05,859
problem will help us detect forging in

346
00:16:03,850 --> 00:16:08,619
videos the second technique is the

347
00:16:05,859 --> 00:16:11,470
contextual Intel mechanisms now what

348
00:16:08,619 --> 00:16:13,419
this technique is is actually it will

349
00:16:11,470 --> 00:16:15,699
enable us to understand of video

350
00:16:13,419 --> 00:16:17,439
semantically by giving us information

351
00:16:15,699 --> 00:16:19,839
about what is going on in the background

352
00:16:17,439 --> 00:16:22,209
and providing us with more metadata

353
00:16:19,839 --> 00:16:25,389
which will help us again in classifying

354
00:16:22,209 --> 00:16:27,219
the authenticity of the video the third

355
00:16:25,389 --> 00:16:29,379
one is the texture investigation now it

356
00:16:27,220 --> 00:16:31,869
is always observed that when defect

357
00:16:29,379 --> 00:16:34,859
videos are created using image

358
00:16:31,869 --> 00:16:37,269
manipulation or in our case face warping

359
00:16:34,859 --> 00:16:40,149
the forging leaves behind certain

360
00:16:37,269 --> 00:16:41,859
patterns and the texture investigation

361
00:16:40,149 --> 00:16:44,559
techniques would help us in identifying

362
00:16:41,859 --> 00:16:46,689
those patterns and in turn help us in

363
00:16:44,559 --> 00:16:49,868
confirming the authenticity authenticity

364
00:16:46,689 --> 00:16:52,329
of the video now the last one is a bit

365
00:16:49,869 --> 00:16:55,299
complex it is user interaction we have

366
00:16:52,329 --> 00:16:59,199
an intention to create a data we data

367
00:16:55,299 --> 00:17:01,959
set which involves videos which contain

368
00:16:59,199 --> 00:17:05,470
a normal head rotation smiling and not

369
00:17:01,959 --> 00:17:07,990
normal actions of humans so after that

370
00:17:05,470 --> 00:17:10,360
after collecting this data we have an

371
00:17:07,990 --> 00:17:13,209
intention to train a deep learning model

372
00:17:10,359 --> 00:17:15,729
which will study the natural movements

373
00:17:13,209 --> 00:17:18,879
of human body so now when it when it is

374
00:17:15,730 --> 00:17:21,189
presented with a video which is a defect

375
00:17:18,878 --> 00:17:23,230
video it will be able to identify

376
00:17:21,189 --> 00:17:25,809
whether the movements done by the actor

377
00:17:23,230 --> 00:17:29,230
in the video are natural or they're not

378
00:17:25,809 --> 00:17:30,970
natural so we believe that we are

379
00:17:29,230 --> 00:17:33,460
working we are already working on this

380
00:17:30,970 --> 00:17:35,559
for pointers and we believe that we

381
00:17:33,460 --> 00:17:37,769
might get some success some better

382
00:17:35,559 --> 00:17:40,779
success than the our earlier attempts

383
00:17:37,769 --> 00:17:44,080
now there is no one solution fits all

384
00:17:40,779 --> 00:17:46,210
when it comes to deep fix so as you are

385
00:17:44,080 --> 00:17:48,580
all cyber security experts we know that

386
00:17:46,210 --> 00:17:51,850
this is a cat-and-mouse game so it keeps

387
00:17:48,580 --> 00:17:53,139
going on and on so therefore I would

388
00:17:51,850 --> 00:17:56,020
like to discuss the merits and

389
00:17:53,140 --> 00:17:59,169
limitations of our approach we believe

390
00:17:56,020 --> 00:18:01,000
that a solution can be scalable and then

391
00:17:59,169 --> 00:18:03,840
it is again small simple and fast to

392
00:18:01,000 --> 00:18:07,659
understand as well now face recognition

393
00:18:03,840 --> 00:18:09,879
technology has been there for a while so

394
00:18:07,659 --> 00:18:11,770
we have a face book doing lot of things

395
00:18:09,880 --> 00:18:13,179
in face recognition and they have a lot

396
00:18:11,770 --> 00:18:16,029
of infrastructure which is which is

397
00:18:13,179 --> 00:18:18,159
already there in the field so instead of

398
00:18:16,029 --> 00:18:20,080
coming out with a new approach which

399
00:18:18,159 --> 00:18:22,570
needs new infrastructure we believe that

400
00:18:20,080 --> 00:18:25,240
our approach could be improved and

401
00:18:22,570 --> 00:18:27,070
modified and it could use the existing

402
00:18:25,240 --> 00:18:30,190
infrastructure to detect be fake videos

403
00:18:27,070 --> 00:18:33,100
on the social media now coming to the

404
00:18:30,190 --> 00:18:36,309
limitations a current solution was

405
00:18:33,100 --> 00:18:39,010
produced to detect defects which were

406
00:18:36,309 --> 00:18:41,320
produced by specifically by face Wapping

407
00:18:39,010 --> 00:18:43,179
technique so there could be other

408
00:18:41,320 --> 00:18:46,320
techniques as well to produce defect

409
00:18:43,179 --> 00:18:49,330
videos which involve some other methods

410
00:18:46,320 --> 00:18:51,939
other than face Wapping so this

411
00:18:49,330 --> 00:18:54,490
particular approach won't work in that

412
00:18:51,940 --> 00:18:57,039
scene but again as well as I said we are

413
00:18:54,490 --> 00:18:58,630
constantly working and we also have

414
00:18:57,039 --> 00:19:00,399
interaction with the research

415
00:18:58,630 --> 00:19:02,169
international research community who are

416
00:19:00,399 --> 00:19:03,939
working with this problem so now I

417
00:19:02,169 --> 00:19:05,740
request my friend Vijay to discuss some

418
00:19:03,940 --> 00:19:07,929
interesting solutions that have been

419
00:19:05,740 --> 00:19:10,390
found internationally to curb this

420
00:19:07,929 --> 00:19:13,600
problem thank you thank you new engine

421
00:19:10,390 --> 00:19:15,909
that was interesting so now as I

422
00:19:13,600 --> 00:19:19,439
mentioned earlier that we would be will

423
00:19:15,909 --> 00:19:22,450
be talking a bit on a different research

424
00:19:19,440 --> 00:19:24,279
you know we are honoured and we would

425
00:19:22,450 --> 00:19:25,570
like to thank the researchers at the

426
00:19:24,279 --> 00:19:28,690
University of Albany

427
00:19:25,570 --> 00:19:30,668
you are us the very and you know they

428
00:19:28,690 --> 00:19:32,080
they looked at a different point of view

429
00:19:30,669 --> 00:19:36,029
they looked at the biological

430
00:19:32,080 --> 00:19:39,360
perspective what this research is that

431
00:19:36,029 --> 00:19:42,940
the researchers took this particular

432
00:19:39,360 --> 00:19:45,850
criteria of eye blinking and and the

433
00:19:42,940 --> 00:19:49,510
static says that you know the mean

434
00:19:45,850 --> 00:19:51,908
blinking eye rate when its resting is 17

435
00:19:49,510 --> 00:19:55,360
blinks per minute so spontaneous eye

436
00:19:51,909 --> 00:19:59,110
blinking does not depend upon external

437
00:19:55,360 --> 00:20:00,129
efforts or and it's it's you know it's

438
00:19:59,110 --> 00:20:02,740
very natural

439
00:20:00,130 --> 00:20:05,260
well when we are Congress saying it gets

440
00:20:02,740 --> 00:20:08,110
to 26 blinks per a minute and while

441
00:20:05,260 --> 00:20:11,080
drilling it reduces to 4.5 blinks per

442
00:20:08,110 --> 00:20:15,178
minute the researchers use this

443
00:20:11,080 --> 00:20:20,260
combination along with the long term

444
00:20:15,179 --> 00:20:22,690
recurrent combinational network and and

445
00:20:20,260 --> 00:20:25,990
I went for the results they determined

446
00:20:22,690 --> 00:20:28,539
or they identified videos in which there

447
00:20:25,990 --> 00:20:31,630
was abnormal eye blinking and they've

448
00:20:28,539 --> 00:20:34,419
termed as d fake videos

449
00:20:31,630 --> 00:20:38,100
so usually it is seen that the actress

450
00:20:34,419 --> 00:20:40,630
in D fake videos do not blink at all or

451
00:20:38,100 --> 00:20:44,230
there is there is very less amount of

452
00:20:40,630 --> 00:20:47,440
thinking or the patterns are weird of

453
00:20:44,230 --> 00:20:49,030
course there are few limitations to this

454
00:20:47,440 --> 00:20:50,950
solution as neurontin you know

455
00:20:49,030 --> 00:20:53,950
calculated that there is no one sub

456
00:20:50,950 --> 00:20:56,470
solution for defects in the this

457
00:20:53,950 --> 00:20:59,740
particular solution might not work

458
00:20:56,470 --> 00:21:01,630
properly if the video is of a very small

459
00:20:59,740 --> 00:21:04,860
length that means it is less than a

460
00:21:01,630 --> 00:21:08,049
minute there's no front facing angle

461
00:21:04,860 --> 00:21:11,590
then there's again the scalability issue

462
00:21:08,049 --> 00:21:14,200
and obviously as neurontin mentioned

463
00:21:11,590 --> 00:21:17,980
that you know videos are created without

464
00:21:14,200 --> 00:21:21,370
face stamping or face swapping I know

465
00:21:17,980 --> 00:21:23,770
it's difficult but yet possible to

466
00:21:21,370 --> 00:21:26,520
create defects where in you know the the

467
00:21:23,770 --> 00:21:31,299
eye blinking rate is kept normal and

468
00:21:26,520 --> 00:21:33,539
just to add to this one just obviously

469
00:21:31,299 --> 00:21:37,600
the stats show that there are figures

470
00:21:33,539 --> 00:21:42,220
you know the mean figures but those can

471
00:21:37,600 --> 00:21:45,639
be changed or those can be deferred now

472
00:21:42,220 --> 00:21:47,559
how does the future look so you know

473
00:21:45,640 --> 00:21:49,990
while you were talking you you know you

474
00:21:47,559 --> 00:21:52,510
mentioned a bit about some videos you

475
00:21:49,990 --> 00:21:55,480
know which which are created or which

476
00:21:52,510 --> 00:21:57,370
might be created with our bodies

477
00:21:55,480 --> 00:22:00,760
tampering right not without face

478
00:21:57,370 --> 00:22:03,189
tampering so this is this is the

479
00:22:00,760 --> 00:22:05,770
next-gen we won't call it d fakes we

480
00:22:03,190 --> 00:22:08,860
will just call it in simple words as we

481
00:22:05,770 --> 00:22:10,840
use without face tampering and this is

482
00:22:08,860 --> 00:22:12,659
amazing I mean as artificial

483
00:22:10,840 --> 00:22:15,149
intelligence

484
00:22:12,660 --> 00:22:17,220
you know gets matured every day it

485
00:22:15,150 --> 00:22:20,760
becomes better and better in producing

486
00:22:17,220 --> 00:22:22,740
genuinely genuinely fake content yes so

487
00:22:20,760 --> 00:22:26,550
yeah you heard it right you heard the

488
00:22:22,740 --> 00:22:28,890
last two words right this is this is a

489
00:22:26,550 --> 00:22:32,370
slide where and you know this is a

490
00:22:28,890 --> 00:22:35,400
particular gif image the video is freely

491
00:22:32,370 --> 00:22:37,469
available on YouTube on the right hand

492
00:22:35,400 --> 00:22:40,410
side is mr. Jordan Peele one of the

493
00:22:37,470 --> 00:22:43,920
actress in USA what he's doing is

494
00:22:40,410 --> 00:22:45,990
talking gibberish and the same thing or

495
00:22:43,920 --> 00:22:48,540
you know the jabra statements are coming

496
00:22:45,990 --> 00:22:52,590
out of former president mr. Obama's

497
00:22:48,540 --> 00:22:54,659
mouth so we would this is how you know

498
00:22:52,590 --> 00:22:58,919
the future might look and this is pretty

499
00:22:54,660 --> 00:23:00,930
scary if you have time you can certainly

500
00:22:58,920 --> 00:23:03,270
you know have a look at this video it's

501
00:23:00,930 --> 00:23:04,950
actually yes I think most of them have

502
00:23:03,270 --> 00:23:06,540
already seen this video yeah oh yeah

503
00:23:04,950 --> 00:23:10,140
it's and it was already discussed on

504
00:23:06,540 --> 00:23:12,690
different news channels also now it's

505
00:23:10,140 --> 00:23:15,450
time for some black hat sound bites so

506
00:23:12,690 --> 00:23:17,160
one of the potential ways to curb deep

507
00:23:15,450 --> 00:23:20,130
fake videos this video my video

508
00:23:17,160 --> 00:23:21,750
watermarking basically you know I'll

509
00:23:20,130 --> 00:23:24,450
move to the second point which is very

510
00:23:21,750 --> 00:23:26,670
important I feel this is again you know

511
00:23:24,450 --> 00:23:28,500
kind of a social awareness point which

512
00:23:26,670 --> 00:23:30,720
is think before you forward that video

513
00:23:28,500 --> 00:23:32,850
so you should be careful enough you

514
00:23:30,720 --> 00:23:35,550
should really really think that whether

515
00:23:32,850 --> 00:23:38,280
this video is a real or alleged a real

516
00:23:35,550 --> 00:23:41,220
or a fake one before you forward it to

517
00:23:38,280 --> 00:23:43,940
anyone else or is it really word to

518
00:23:41,220 --> 00:23:46,350
share it the credibility of the source

519
00:23:43,940 --> 00:23:47,970
you should also be looking at this

520
00:23:46,350 --> 00:23:50,340
source or the origin of the video from

521
00:23:47,970 --> 00:23:51,900
where it has come see we are not telling

522
00:23:50,340 --> 00:23:53,370
you to run any code or we are not

523
00:23:51,900 --> 00:23:55,440
telling you to write any script to

524
00:23:53,370 --> 00:23:59,340
determine whether you know the video is

525
00:23:55,440 --> 00:24:01,080
a TFA core alleged one but what you can

526
00:23:59,340 --> 00:24:03,360
do is at least you know you can search

527
00:24:01,080 --> 00:24:05,750
it on google and most of the times crude

528
00:24:03,360 --> 00:24:07,919
human intelligence can also help in

529
00:24:05,750 --> 00:24:12,510
reducing the effectiveness of these

530
00:24:07,920 --> 00:24:14,310
things on the internet right and you

531
00:24:12,510 --> 00:24:18,300
know why we are talking about this is

532
00:24:14,310 --> 00:24:22,139
because we believe that you know D fake

533
00:24:18,300 --> 00:24:25,490
videos created or targeted on

534
00:24:22,140 --> 00:24:28,730
politicians are of high concern concern

535
00:24:25,490 --> 00:24:30,620
the fact of the next year mid 2019

536
00:24:28,730 --> 00:24:32,090
elections which I carry which will be

537
00:24:30,620 --> 00:24:34,699
carried out in different parts of the

538
00:24:32,090 --> 00:24:37,970
world so just imagine if such kind of

539
00:24:34,700 --> 00:24:39,620
videos are circulated in many of the

540
00:24:37,970 --> 00:24:41,480
countries third-world countries as

541
00:24:39,620 --> 00:24:43,909
sentiments play an important role so

542
00:24:41,480 --> 00:24:46,760
people don't think before forwarding a

543
00:24:43,910 --> 00:24:48,950
video and that can really instigate

544
00:24:46,760 --> 00:24:52,910
tensions not only within the country but

545
00:24:48,950 --> 00:24:54,530
also on a global level we also talked we

546
00:24:52,910 --> 00:24:57,950
are also gonna talk on the robust laws

547
00:24:54,530 --> 00:25:01,730
so currently there are no laws or legal

548
00:24:57,950 --> 00:25:05,750
protection for individuals who have been

549
00:25:01,730 --> 00:25:07,700
victimized by D fake videos so you know

550
00:25:05,750 --> 00:25:09,679
there have been several news news where

551
00:25:07,700 --> 00:25:11,360
and you know people have become target

552
00:25:09,679 --> 00:25:14,720
for such videos

553
00:25:11,360 --> 00:25:17,449
the revenge pornographic videos so yes

554
00:25:14,720 --> 00:25:20,809
so this is must we should have laws in

555
00:25:17,450 --> 00:25:23,950
place for individuals you know who have

556
00:25:20,809 --> 00:25:26,450
been victims and last but not the least

557
00:25:23,950 --> 00:25:30,620
SSA and the famous you know Game of

558
00:25:26,450 --> 00:25:31,970
Thrones Ventura has already come and you

559
00:25:30,620 --> 00:25:33,889
know the defects in form of White

560
00:25:31,970 --> 00:25:37,700
Walkers have already approached so we

561
00:25:33,890 --> 00:25:41,809
request the security community to come

562
00:25:37,700 --> 00:25:44,330
forward and and to create a create an

563
00:25:41,809 --> 00:25:46,399
awareness among the masses that such

564
00:25:44,330 --> 00:25:48,500
things exist in the world and they have

565
00:25:46,400 --> 00:25:50,780
power to manipulate our thoughts as well

566
00:25:48,500 --> 00:25:52,640
or else what best we can do is we can at

567
00:25:50,780 --> 00:25:54,530
least you know create as many hurdles

568
00:25:52,640 --> 00:25:56,510
for the threat actors or for the

569
00:25:54,530 --> 00:25:59,330
malicious actors so that you know it

570
00:25:56,510 --> 00:26:02,030
takes a time for them to create such

571
00:25:59,330 --> 00:26:04,610
news and thoughts to you know to clear D

572
00:26:02,030 --> 00:26:07,070
fake videos that's all we have for today

573
00:26:04,610 --> 00:26:08,449
thank you for joining sir any questions

574
00:26:07,070 --> 00:26:16,859
please feel free

575
00:26:08,450 --> 00:26:16,859
[Applause]

576
00:26:18,039 --> 00:26:23,240
we are here for the whole day so yeah

577
00:26:21,260 --> 00:26:34,580
take the questions any time you feel

578
00:26:23,240 --> 00:26:36,649
comfortable what what techniques I I

579
00:26:34,580 --> 00:26:38,059
know very little about this subject what

580
00:26:36,649 --> 00:26:40,158
techniques are there besides face

581
00:26:38,059 --> 00:26:43,279
swapping currently do they have any

582
00:26:40,159 --> 00:26:45,830
efficacy or do you think that in in the

583
00:26:43,279 --> 00:26:47,659
future we'll have systems where if Star

584
00:26:45,830 --> 00:26:49,879
Wars can recreate Princess Leia for

585
00:26:47,659 --> 00:26:52,429
example no actually all the techniques

586
00:26:49,880 --> 00:26:54,980
that exist they are AI based techniques

587
00:26:52,429 --> 00:26:57,320
so this this particular one was

588
00:26:54,980 --> 00:26:59,210
auto-encoders there are many other

589
00:26:57,320 --> 00:27:00,860
techniques like transferring the

590
00:26:59,210 --> 00:27:03,010
expressions from one person's face to

591
00:27:00,860 --> 00:27:05,870
other person's face but everything is AI

592
00:27:03,010 --> 00:27:07,850
there are no no non AI techniques to do

593
00:27:05,870 --> 00:27:10,489
this at least we don't know about them

594
00:27:07,850 --> 00:27:13,219
so researchers are you know they have

595
00:27:10,490 --> 00:27:15,230
been working on different aspects of AI

596
00:27:13,220 --> 00:27:19,190
like they they started combining or they

597
00:27:15,230 --> 00:27:22,000
started you know different AI models and

598
00:27:19,190 --> 00:27:24,620
then started working on this techniques

599
00:27:22,000 --> 00:27:28,789
you know we are certain that this will

600
00:27:24,620 --> 00:27:30,379
proliferate in future d fates however

601
00:27:28,789 --> 00:27:34,460
you know that the solutions presented

602
00:27:30,380 --> 00:27:37,100
today and the solutions which are over

603
00:27:34,460 --> 00:27:38,360
the Internet in form of white papers do

604
00:27:37,100 --> 00:27:41,209
you have limitation do you have certain

605
00:27:38,360 --> 00:27:43,729
limitations so we you know we need to

606
00:27:41,210 --> 00:27:45,909
come forward and we need to yeah I like

607
00:27:43,730 --> 00:27:47,840
to add add to your point is the thing is

608
00:27:45,909 --> 00:27:49,700
researchers are developing this

609
00:27:47,840 --> 00:27:52,699
technology to do some good things right

610
00:27:49,700 --> 00:27:54,889
so we are like everything which is

611
00:27:52,700 --> 00:27:57,620
invented in science it gets abused by

612
00:27:54,889 --> 00:27:59,479
the bad people right so researchers are

613
00:27:57,620 --> 00:28:02,299
trying to create technology which can

614
00:27:59,480 --> 00:28:05,600
recreate dead people as well so it has

615
00:28:02,299 --> 00:28:11,168
that power as well but when it goes in

616
00:28:05,600 --> 00:28:11,168
bad hands things get messed up thank you

617
00:28:11,230 --> 00:28:21,630
any more questions

618
00:28:14,789 --> 00:28:25,289
okay have you ever thought about

619
00:28:21,630 --> 00:28:28,510
securing video hashes in our blockchain

620
00:28:25,289 --> 00:28:31,658
did you have nothing blockchain yeah

621
00:28:28,510 --> 00:28:33,940
there there is a discussion going on on

622
00:28:31,659 --> 00:28:36,970
the internet to use blocks in technology

623
00:28:33,940 --> 00:28:39,250
to solve this but I didn't come across

624
00:28:36,970 --> 00:28:41,590
any research paper which had some

625
00:28:39,250 --> 00:28:43,330
solution like that so we have some

626
00:28:41,590 --> 00:28:55,090
thoughts to work in that direction but

627
00:28:43,330 --> 00:28:59,370
we are not yet clear how to proceed do

628
00:28:55,090 --> 00:29:05,168
you guys see so Apple and Google

629
00:28:59,370 --> 00:29:06,850
coming out with what automating sort of

630
00:29:05,169 --> 00:29:10,510
watermark in on videos that are talking

631
00:29:06,850 --> 00:29:14,439
on your phone so currently YouTube has

632
00:29:10,510 --> 00:29:18,279
got the concept of Content ID so what it

633
00:29:14,440 --> 00:29:19,990
does is when I you know if I'm the

634
00:29:18,279 --> 00:29:22,809
creator or if I'm the owner of a

635
00:29:19,990 --> 00:29:25,990
particular video when I try to upload it

636
00:29:22,809 --> 00:29:28,840
it will check the content and it will

637
00:29:25,990 --> 00:29:31,450
verify if if something similar exists or

638
00:29:28,840 --> 00:29:33,610
not and then it then and then only it

639
00:29:31,450 --> 00:29:35,620
will allow me to upload that video so

640
00:29:33,610 --> 00:29:37,990
that's their solution currently the

641
00:29:35,620 --> 00:29:40,299
Content ID yeah so that that works

642
00:29:37,990 --> 00:29:43,840
pretty well I mean in order to avoid

643
00:29:40,299 --> 00:29:47,490
copyright infringement coalition's there

644
00:29:43,840 --> 00:29:47,490
is them thank you

645
00:29:51,110 --> 00:30:00,330
any more questions okay

646
00:29:57,950 --> 00:30:02,820
thank you so make sure you can reach out

647
00:30:00,330 --> 00:30:05,600
to us on the street or handles thank you

648
00:30:02,820 --> 00:30:05,600
for joining the top

