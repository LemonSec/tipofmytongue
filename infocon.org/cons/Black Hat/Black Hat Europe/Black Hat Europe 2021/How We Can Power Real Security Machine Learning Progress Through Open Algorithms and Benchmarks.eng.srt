1
00:00:00,860 --> 00:00:08,559
[Music]

2
00:00:08,559 --> 00:00:10,639
hi my name is joshua sax i'm chief

3
00:00:10,639 --> 00:00:13,120
scientist at sofos and my talk today is

4
00:00:13,120 --> 00:00:15,280
entitled no more secret sauce how we can

5
00:00:15,280 --> 00:00:17,279
power real security machine learning

6
00:00:17,279 --> 00:00:19,199
progress through open algorithms and

7
00:00:19,199 --> 00:00:21,840
benchmarks

8
00:00:22,880 --> 00:00:24,720
so this this talk is all about how we do

9
00:00:24,720 --> 00:00:27,039
sound science uh or how we should how we

10
00:00:27,039 --> 00:00:28,320
should how we can get to a place in

11
00:00:28,320 --> 00:00:30,080
which we're doing sound science and

12
00:00:30,080 --> 00:00:32,079
cyber security ai so i just wanted to

13
00:00:32,079 --> 00:00:33,440
talk i just wanted to talk a little bit

14
00:00:33,440 --> 00:00:35,600
about um

15
00:00:35,600 --> 00:00:37,200
what some of the contours of sound

16
00:00:37,200 --> 00:00:38,160
science

17
00:00:38,160 --> 00:00:39,280
look like

18
00:00:39,280 --> 00:00:42,160
as a way of framing this talk

19
00:00:42,160 --> 00:00:44,079
we could spend a long time talking about

20
00:00:44,079 --> 00:00:46,160
the exact um

21
00:00:46,160 --> 00:00:48,800
definition of science and i'm i'm not an

22
00:00:48,800 --> 00:00:50,719
expert in the philosophy of science i'm

23
00:00:50,719 --> 00:00:52,879
not going to attempt that but i i will

24
00:00:52,879 --> 00:00:54,239
say that that

25
00:00:54,239 --> 00:00:55,120
one

26
00:00:55,120 --> 00:00:57,360
dynamic that characterizes

27
00:00:57,360 --> 00:01:00,480
science um just really generally

28
00:01:00,480 --> 00:01:02,640
is um

29
00:01:02,640 --> 00:01:03,920
is the dynamic of publishing

30
00:01:03,920 --> 00:01:06,000
peer-reviewed papers um

31
00:01:06,000 --> 00:01:09,360
and um collaborating with with peers at

32
00:01:09,360 --> 00:01:11,040
institutions other than your own

33
00:01:11,040 --> 00:01:12,080
companies other than your own

34
00:01:12,080 --> 00:01:13,920
organizations other than your own

35
00:01:13,920 --> 00:01:15,840
i think this i think this node length

36
00:01:15,840 --> 00:01:18,080
diagram illustrates that so this is a

37
00:01:18,080 --> 00:01:20,479
social network diagram of scientists who

38
00:01:20,479 --> 00:01:22,640
work on information retrieval so outside

39
00:01:22,640 --> 00:01:24,479
of cyber security but but still inside

40
00:01:24,479 --> 00:01:26,000
data science

41
00:01:26,000 --> 00:01:27,759
each one of these

42
00:01:27,759 --> 00:01:29,280
circles represents an individual

43
00:01:29,280 --> 00:01:31,040
scientist

44
00:01:31,040 --> 00:01:33,920
and they're connected

45
00:01:33,920 --> 00:01:36,320
as a function of whether or not they've

46
00:01:36,320 --> 00:01:38,159
cited each other so each of these

47
00:01:38,159 --> 00:01:41,040
scientists is publishing papers

48
00:01:41,040 --> 00:01:43,119
submitting papers to peer-reviewed for

49
00:01:43,119 --> 00:01:44,399
like

50
00:01:44,399 --> 00:01:46,799
academic conferences and academic

51
00:01:46,799 --> 00:01:48,240
journals

52
00:01:48,240 --> 00:01:50,880
those papers are getting reviewed by

53
00:01:50,880 --> 00:01:52,560
disinterested parties like other

54
00:01:52,560 --> 00:01:54,159
academics

55
00:01:54,159 --> 00:01:55,280
who are

56
00:01:55,280 --> 00:01:56,640
who are performing a kind of quality

57
00:01:56,640 --> 00:01:58,960
assurance check on those papers if the

58
00:01:58,960 --> 00:02:00,719
papers are due past past semester they

59
00:02:00,719 --> 00:02:01,920
get published

60
00:02:01,920 --> 00:02:03,360
in those papers

61
00:02:03,360 --> 00:02:04,640
cite other scientists at other

62
00:02:04,640 --> 00:02:06,640
institutions based on the ideas they

63
00:02:06,640 --> 00:02:07,680
draw on

64
00:02:07,680 --> 00:02:09,520
and what emerges is a community of

65
00:02:09,520 --> 00:02:12,000
scientists who are publishing and

66
00:02:12,000 --> 00:02:14,160
referencing each other's work

67
00:02:14,160 --> 00:02:16,879
openly discussing their ideas

68
00:02:16,879 --> 00:02:18,560
openly

69
00:02:18,560 --> 00:02:20,560
publishing their results

70
00:02:20,560 --> 00:02:22,000
and so you get something like this in a

71
00:02:22,000 --> 00:02:23,599
scientific community lots and lots of

72
00:02:23,599 --> 00:02:27,200
different individuals uh who are um in

73
00:02:27,200 --> 00:02:29,280
essence collaborating um and they're

74
00:02:29,280 --> 00:02:31,120
also competing um to see who can make

75
00:02:31,120 --> 00:02:33,840
the best breakthroughs um but at it's

76
00:02:33,840 --> 00:02:35,280
hard scientific science involves this

77
00:02:35,280 --> 00:02:38,480
kind of openness and collaboration

78
00:02:38,480 --> 00:02:39,519
now

79
00:02:39,519 --> 00:02:42,239
are are we doing this in insecurity i i

80
00:02:42,239 --> 00:02:44,160
think i think anyone

81
00:02:44,160 --> 00:02:46,000
who's attending this ritual

82
00:02:46,000 --> 00:02:48,319
or in person black out events and has

83
00:02:48,319 --> 00:02:49,360
been around in the cyber security

84
00:02:49,360 --> 00:02:50,879
industry for a while

85
00:02:50,879 --> 00:02:52,879
knows that the answer is probably

86
00:02:52,879 --> 00:02:54,800
no um

87
00:02:54,800 --> 00:02:57,040
and i think we can answer no

88
00:02:57,040 --> 00:02:57,490
um

89
00:02:57,490 --> 00:02:59,360
[Music]

90
00:02:59,360 --> 00:03:00,970
pretty confidently um

91
00:03:00,970 --> 00:03:03,040
[Music]

92
00:03:03,040 --> 00:03:05,360
for a couple of reasons so if it did and

93
00:03:05,360 --> 00:03:06,800
the first one is is obvious and the

94
00:03:06,800 --> 00:03:08,319
second one is less obvious so let's

95
00:03:08,319 --> 00:03:10,239
start with the obvious one

96
00:03:10,239 --> 00:03:11,519
i think i think we can say no we're not

97
00:03:11,519 --> 00:03:14,239
doing real science partly because

98
00:03:14,239 --> 00:03:17,360
in my anecdotal experience

99
00:03:17,360 --> 00:03:19,280
more than half of cyber security

100
00:03:19,280 --> 00:03:20,640
companies that claim to be doing some

101
00:03:20,640 --> 00:03:22,959
kind of artificial intelligence

102
00:03:22,959 --> 00:03:24,560
publish nothing on on the work that

103
00:03:24,560 --> 00:03:25,840
they're doing

104
00:03:25,840 --> 00:03:29,280
and so it's really anybody's guess

105
00:03:29,599 --> 00:03:31,519
how sounds their research and

106
00:03:31,519 --> 00:03:33,760
development process really is

107
00:03:33,760 --> 00:03:35,200
and i just wouldn't say what they're

108
00:03:35,200 --> 00:03:37,680
doing is is is science

109
00:03:37,680 --> 00:03:38,080
um

110
00:03:38,080 --> 00:03:39,680
[Music]

111
00:03:39,680 --> 00:03:42,239
because they're not um

112
00:03:42,239 --> 00:03:44,080
writing up what they're doing submitting

113
00:03:44,080 --> 00:03:46,640
it for peer review um

114
00:03:46,640 --> 00:03:48,319
uh so that scientists who are

115
00:03:48,319 --> 00:03:51,760
disinterested um i can evaluate it for

116
00:03:51,760 --> 00:03:53,599
its its soundness

117
00:03:53,599 --> 00:03:55,040
um

118
00:03:55,040 --> 00:03:56,400
they're working in a silo and that's

119
00:03:56,400 --> 00:03:58,319
that that's not what i would call

120
00:03:58,319 --> 00:04:00,000
science nobody can and you know you

121
00:04:00,000 --> 00:04:02,000
can't you can't validate the quality of

122
00:04:02,000 --> 00:04:03,439
their their work so it's hard to even

123
00:04:03,439 --> 00:04:04,720
know um

124
00:04:04,720 --> 00:04:06,879
how scientific uh these companies are

125
00:04:06,879 --> 00:04:08,319
being

126
00:04:08,319 --> 00:04:10,239
um so i think i mean like i mean many of

127
00:04:10,239 --> 00:04:12,239
these companies are are well-meaning uh

128
00:04:12,239 --> 00:04:14,159
some some of them may be producing good

129
00:04:14,159 --> 00:04:15,519
products um

130
00:04:15,519 --> 00:04:16,880
but i think we need to develop more of a

131
00:04:16,880 --> 00:04:18,798
culture uh

132
00:04:18,798 --> 00:04:19,759
like the culture we have around

133
00:04:19,759 --> 00:04:22,400
cryptography here and um expected these

134
00:04:22,400 --> 00:04:24,160
companies if they claim to be using ai

135
00:04:24,160 --> 00:04:25,600
they need to publish their scientific

136
00:04:25,600 --> 00:04:27,040
work

137
00:04:27,040 --> 00:04:29,040
um there's a second category of

138
00:04:29,040 --> 00:04:32,160
organizations and and i i think um the

139
00:04:32,160 --> 00:04:34,639
organization i manage the sophos ai team

140
00:04:34,639 --> 00:04:37,919
um at sophos which is responsible for my

141
00:04:37,919 --> 00:04:40,320
company's um machine learning technology

142
00:04:40,320 --> 00:04:43,440
that falls into this category um

143
00:04:43,440 --> 00:04:45,840
this category of organizations uh does

144
00:04:45,840 --> 00:04:47,440
publish and does publish in

145
00:04:47,440 --> 00:04:49,759
peer-reviewed forum

146
00:04:49,759 --> 00:04:51,040
but still

147
00:04:51,040 --> 00:04:53,040
i wouldn't say that what we're doing has

148
00:04:53,040 --> 00:04:54,720
quite

149
00:04:54,720 --> 00:04:56,960
passed the thresholds um

150
00:04:56,960 --> 00:04:57,840
uh

151
00:04:57,840 --> 00:04:59,520
such that we should consider it to be

152
00:04:59,520 --> 00:05:02,080
truly scientific and here's why

153
00:05:02,080 --> 00:05:04,320
uh basically the the the state of

154
00:05:04,320 --> 00:05:05,840
practice and

155
00:05:05,840 --> 00:05:07,280
security and machine learning research

156
00:05:07,280 --> 00:05:09,120
right now is such that

157
00:05:09,120 --> 00:05:11,199
let's say my team is working on building

158
00:05:11,199 --> 00:05:13,120
a android malware detector which is

159
00:05:13,120 --> 00:05:14,880
something we have in fact done and we

160
00:05:14,880 --> 00:05:16,320
have presented presented at conferences

161
00:05:16,320 --> 00:05:18,560
on in the past um

162
00:05:18,560 --> 00:05:20,400
uh when we go look at the at the

163
00:05:20,400 --> 00:05:21,520
academic

164
00:05:21,520 --> 00:05:23,680
literature on building android malware

165
00:05:23,680 --> 00:05:25,680
detectors based on machine learning

166
00:05:25,680 --> 00:05:27,840
um there's a bunch of papers that

167
00:05:27,840 --> 00:05:29,759
focused on android malware detection but

168
00:05:29,759 --> 00:05:31,520
they're all

169
00:05:31,520 --> 00:05:33,360
all the papers

170
00:05:33,360 --> 00:05:35,759
give experimental results on different

171
00:05:35,759 --> 00:05:37,120
data sets

172
00:05:37,120 --> 00:05:38,479
sometimes dramatically different data

173
00:05:38,479 --> 00:05:40,240
sets sometimes slightly different data

174
00:05:40,240 --> 00:05:41,680
sets

175
00:05:41,680 --> 00:05:44,960
but each paper references its own

176
00:05:44,960 --> 00:05:47,919
data and as a result if i report you

177
00:05:47,919 --> 00:05:50,880
know an accuracy of 99 in my paper and

178
00:05:50,880 --> 00:05:54,160
another team reports an accuracy of 97

179
00:05:54,160 --> 00:05:56,400
or vice versa we don't really know which

180
00:05:56,400 --> 00:05:58,319
paper is better it's apples and oranges

181
00:05:58,319 --> 00:06:00,639
right uh you know i i

182
00:06:00,639 --> 00:06:02,560
my data sets this green apple their data

183
00:06:02,560 --> 00:06:04,720
sets this nectarine you know some third

184
00:06:04,720 --> 00:06:07,440
papers data sets this orange uh so the

185
00:06:07,440 --> 00:06:09,360
results aren't mutually comparable which

186
00:06:09,360 --> 00:06:11,919
means that um

187
00:06:11,919 --> 00:06:13,680
i actually don't know which ideas work

188
00:06:13,680 --> 00:06:16,319
best um and that's a big problem

189
00:06:16,319 --> 00:06:17,759
i'm gonna be talking a fair bit about

190
00:06:17,759 --> 00:06:21,280
that in this talk

191
00:06:21,280 --> 00:06:23,280
um i mean just just uh

192
00:06:23,280 --> 00:06:25,199
just to flesh this out a little bit and

193
00:06:25,199 --> 00:06:27,280
here's here here are some accuracy plots

194
00:06:27,280 --> 00:06:28,720
from three different papers on windows

195
00:06:28,720 --> 00:06:30,240
malware detection

196
00:06:30,240 --> 00:06:31,520
uh so

197
00:06:31,520 --> 00:06:33,680
in this plot i'm showing accuracy

198
00:06:33,680 --> 00:06:35,360
results from a paper that my colleague

199
00:06:35,360 --> 00:06:38,400
constantine and i published a while back

200
00:06:38,400 --> 00:06:41,120
on using a feed forward neural network

201
00:06:41,120 --> 00:06:43,199
approach with a given set of features to

202
00:06:43,199 --> 00:06:45,199
detect windows malware

203
00:06:45,199 --> 00:06:47,919
we we report our results mostly in terms

204
00:06:47,919 --> 00:06:50,000
of area under rock curve which i'm not

205
00:06:50,000 --> 00:06:52,240
going to unpack here but is a measure of

206
00:06:52,240 --> 00:06:54,319
machine learning accuracy

207
00:06:54,319 --> 00:06:55,599
uh

208
00:06:55,599 --> 00:06:57,759
here's another paper um

209
00:06:57,759 --> 00:06:58,880
written by

210
00:06:58,880 --> 00:07:01,280
um the first authored by this guy this

211
00:07:01,280 --> 00:07:04,160
uh this this researcher ravi

212
00:07:04,160 --> 00:07:06,880
um and they report

213
00:07:06,880 --> 00:07:08,319
first of all a different metric which is

214
00:07:08,319 --> 00:07:09,759
accuracy and they're using a different

215
00:07:09,759 --> 00:07:11,120
data set

216
00:07:11,120 --> 00:07:13,360
um and then here's a third paper and

217
00:07:13,360 --> 00:07:15,280
they're they're publishing the results

218
00:07:15,280 --> 00:07:16,800
on their own data sets and they're using

219
00:07:16,800 --> 00:07:18,560
different accuracy metrics

220
00:07:18,560 --> 00:07:19,360
um

221
00:07:19,360 --> 00:07:22,000
so i think it's great that um

222
00:07:22,000 --> 00:07:24,160
we have we have an open literature on

223
00:07:24,160 --> 00:07:26,080
how to detect windows malware using

224
00:07:26,080 --> 00:07:27,280
machine learning methods and people are

225
00:07:27,280 --> 00:07:28,479
trying different things and publishing

226
00:07:28,479 --> 00:07:31,919
them um but

227
00:07:31,919 --> 00:07:33,680
it's it's a real problem that we can't

228
00:07:33,680 --> 00:07:35,680
compare our results so that's something

229
00:07:35,680 --> 00:07:37,199
i'm going to be talking about later in

230
00:07:37,199 --> 00:07:38,960
this talk like how to how to overcome

231
00:07:38,960 --> 00:07:40,639
that

232
00:07:40,639 --> 00:07:42,080
um but i think i mean just just sort of

233
00:07:42,080 --> 00:07:44,720
summing up these last few slides

234
00:07:44,720 --> 00:07:46,400
basically the the point i'm making is

235
00:07:46,400 --> 00:07:47,199
that

236
00:07:47,199 --> 00:07:48,720
what we're doing in security ai right

237
00:07:48,720 --> 00:07:51,280
now is is falling short of

238
00:07:51,280 --> 00:07:53,360
the standards of actual science i mean

239
00:07:53,360 --> 00:07:55,280
one because there's a culture of

240
00:07:55,280 --> 00:07:57,039
sort of

241
00:07:57,039 --> 00:07:58,879
claims to secret sauce and the security

242
00:07:58,879 --> 00:08:00,080
industry unfortunately in which people

243
00:08:00,080 --> 00:08:01,199
just aren't open about what they're

244
00:08:01,199 --> 00:08:02,720
doing i mean that's a problem that we

245
00:08:02,720 --> 00:08:04,319
need to solve but then even for the

246
00:08:04,319 --> 00:08:05,919
folks who are open about the kinds of

247
00:08:05,919 --> 00:08:07,199
machine learning algorithms they're

248
00:08:07,199 --> 00:08:09,520
they're developing to stop cyber attacks

249
00:08:09,520 --> 00:08:11,520
um we have this problem of

250
00:08:11,520 --> 00:08:13,759
incommensurate data sets

251
00:08:13,759 --> 00:08:15,199
where different papers are referencing

252
00:08:15,199 --> 00:08:16,560
different private data sets and we

253
00:08:16,560 --> 00:08:17,599
actually don't know which papers are

254
00:08:17,599 --> 00:08:19,599
proposing the best

255
00:08:19,599 --> 00:08:21,280
techniques for detecting and stopping

256
00:08:21,280 --> 00:08:24,000
cyber attacks

257
00:08:25,199 --> 00:08:27,599
um and just to be totally clear i mean

258
00:08:27,599 --> 00:08:29,199
my position here is that you know right

259
00:08:29,199 --> 00:08:30,720
now companies rarely publish their

260
00:08:30,720 --> 00:08:32,640
machine learning methods uh even when

261
00:08:32,640 --> 00:08:33,839
they do we don't have these we don't

262
00:08:33,839 --> 00:08:35,360
have public public benchmarks so we

263
00:08:35,360 --> 00:08:37,039
can't compare their results

264
00:08:37,039 --> 00:08:38,799
um and you know a lot of the technical

265
00:08:38,799 --> 00:08:40,719
collaboration

266
00:08:40,719 --> 00:08:42,640
that happens between organizations in

267
00:08:42,640 --> 00:08:44,240
the security industry right now around

268
00:08:44,240 --> 00:08:46,000
machine learning

269
00:08:46,000 --> 00:08:47,360
really happens through just reading each

270
00:08:47,360 --> 00:08:49,760
other's watered-down blogs or

271
00:08:49,760 --> 00:08:51,040
reading each other's papers even though

272
00:08:51,040 --> 00:08:52,480
we can't really compare

273
00:08:52,480 --> 00:08:55,360
results um and attempting to glean what

274
00:08:55,360 --> 00:08:56,880
others are doing and attempting to learn

275
00:08:56,880 --> 00:08:58,800
from it i think we can do far better and

276
00:08:58,800 --> 00:08:59,839
i think that

277
00:08:59,839 --> 00:09:02,000
we we need to since

278
00:09:02,000 --> 00:09:03,600
we have an important moral mission which

279
00:09:03,600 --> 00:09:05,760
is to protect people from from

280
00:09:05,760 --> 00:09:09,000
getting compromised

281
00:09:09,519 --> 00:09:11,360
um and then i also wanted i want to set

282
00:09:11,360 --> 00:09:13,120
up a contrast to between the way we're

283
00:09:13,120 --> 00:09:15,120
doing science in

284
00:09:15,120 --> 00:09:16,000
uh

285
00:09:16,000 --> 00:09:18,880
security machine learning and um

286
00:09:18,880 --> 00:09:23,920
or lack thereof and the way that uh um

287
00:09:23,920 --> 00:09:25,440
tech industry and

288
00:09:25,440 --> 00:09:28,399
tech academics uh are doing science

289
00:09:28,399 --> 00:09:31,120
outside of security um so that there are

290
00:09:31,120 --> 00:09:32,480
some fields and i'm gonna be talking

291
00:09:32,480 --> 00:09:35,120
about them um

292
00:09:35,120 --> 00:09:38,000
down downstream um in this presentation

293
00:09:38,000 --> 00:09:38,959
um

294
00:09:38,959 --> 00:09:40,800
in which there really is a culture of

295
00:09:40,800 --> 00:09:44,720
openness and open science uh in which um

296
00:09:44,720 --> 00:09:47,920
both academics and researchers inside of

297
00:09:47,920 --> 00:09:48,959
um

298
00:09:48,959 --> 00:09:50,480
inside of companies

299
00:09:50,480 --> 00:09:53,040
uh are publishing and they're comparing

300
00:09:53,040 --> 00:09:55,680
their results based on common benchmarks

301
00:09:55,680 --> 00:09:56,800
um

302
00:09:56,800 --> 00:10:00,480
and those those fields um

303
00:10:00,480 --> 00:10:02,079
have been much more successful i would

304
00:10:02,079 --> 00:10:03,519
say in applying machine learning than

305
00:10:03,519 --> 00:10:06,000
cyber security has um

306
00:10:06,000 --> 00:10:08,000
and i'm going to talk about those um in

307
00:10:08,000 --> 00:10:10,160
a couple slides um

308
00:10:10,160 --> 00:10:12,079
i really think we just need i think if

309
00:10:12,079 --> 00:10:14,000
there's one takeaway from this talk uh

310
00:10:14,000 --> 00:10:16,320
we need to we need to change in in the

311
00:10:16,320 --> 00:10:18,640
security ml space we need to start

312
00:10:18,640 --> 00:10:20,079
expecting that people publish their

313
00:10:20,079 --> 00:10:21,680
machine learning algorithms

314
00:10:21,680 --> 00:10:24,959
just like we expect that companies um

315
00:10:24,959 --> 00:10:26,800
are open about what what cryptographic

316
00:10:26,800 --> 00:10:29,279
algorithms are using if for example we

317
00:10:29,279 --> 00:10:32,000
don't accept um

318
00:10:32,000 --> 00:10:33,839
uh see security through obscurity in

319
00:10:33,839 --> 00:10:35,440
that domain and we shouldn't accept that

320
00:10:35,440 --> 00:10:36,880
in machine learning

321
00:10:36,880 --> 00:10:38,240
uh

322
00:10:38,240 --> 00:10:39,680
we need to start we need to start

323
00:10:39,680 --> 00:10:40,880
expecting companies to report their

324
00:10:40,880 --> 00:10:42,079
machine learning performance against

325
00:10:42,079 --> 00:10:44,399
these public benchmarks um of course

326
00:10:44,399 --> 00:10:46,560
companies are to compete um in the

327
00:10:46,560 --> 00:10:49,200
security ai space but

328
00:10:49,200 --> 00:10:50,880
we should also allow for

329
00:10:50,880 --> 00:10:53,120
cross-pollination um

330
00:10:53,120 --> 00:10:54,480
so that we can advance as a field and

331
00:10:54,480 --> 00:10:56,720
better better protect people

332
00:10:56,720 --> 00:10:58,160
so those are just sort of theses of

333
00:10:58,160 --> 00:11:00,959
these of this talk

334
00:11:03,200 --> 00:11:05,360
convinced that that that they're correct

335
00:11:05,360 --> 00:11:06,480
um

336
00:11:06,480 --> 00:11:08,880
and that you'll join me in in advocating

337
00:11:08,880 --> 00:11:09,760
for

338
00:11:09,760 --> 00:11:11,120
this position around how we should be

339
00:11:11,120 --> 00:11:15,320
doing ai in the security space

340
00:11:15,440 --> 00:11:17,120
um and i think the stakes are really

341
00:11:17,120 --> 00:11:18,640
high i mean

342
00:11:18,640 --> 00:11:20,399
i'm gonna i'm gonna gloss over this

343
00:11:20,399 --> 00:11:22,640
quickly so i'm gonna get to get to why

344
00:11:22,640 --> 00:11:24,480
in in the

345
00:11:24,480 --> 00:11:25,920
slides that i'm about to present but

346
00:11:25,920 --> 00:11:27,040
basically

347
00:11:27,040 --> 00:11:28,640
you know if we stay closed about the way

348
00:11:28,640 --> 00:11:30,240
we do security ai we're going to make

349
00:11:30,240 --> 00:11:32,959
much less progress than if we open up um

350
00:11:32,959 --> 00:11:34,079
we're not going to be able to compare

351
00:11:34,079 --> 00:11:36,480
results and know which ideas work

352
00:11:36,480 --> 00:11:38,800
i think that

353
00:11:38,800 --> 00:11:40,399
i think people will rightly become

354
00:11:40,399 --> 00:11:42,399
cynical about security ai i think they

355
00:11:42,399 --> 00:11:45,600
already are um because of all the sort

356
00:11:45,600 --> 00:11:46,240
of

357
00:11:46,240 --> 00:11:47,519
substance-less

358
00:11:47,519 --> 00:11:49,680
marketing claims that um are getting

359
00:11:49,680 --> 00:11:51,600
promulgated in the security industry

360
00:11:51,600 --> 00:11:52,560
um

361
00:11:52,560 --> 00:11:55,360
and i think overall we're to protect

362
00:11:55,360 --> 00:11:56,959
people less well than we would have

363
00:11:56,959 --> 00:11:58,639
otherwise if we stay if we keep this

364
00:11:58,639 --> 00:12:00,160
kind of closed

365
00:12:00,160 --> 00:12:01,040
uh

366
00:12:01,040 --> 00:12:02,720
culture um in the coming years in

367
00:12:02,720 --> 00:12:04,800
security i think if we open up i think

368
00:12:04,800 --> 00:12:06,399
there's a chance for a lot of progress

369
00:12:06,399 --> 00:12:09,680
in security ai um we'll be able to track

370
00:12:09,680 --> 00:12:11,440
which machine learning methods work best

371
00:12:11,440 --> 00:12:13,519
against a variety of detection problems

372
00:12:13,519 --> 00:12:15,200
um i think users will become less

373
00:12:15,200 --> 00:12:17,360
cynical users of security products will

374
00:12:17,360 --> 00:12:20,240
become less cynical about ml claims

375
00:12:20,240 --> 00:12:21,519
um

376
00:12:21,519 --> 00:12:23,120
and

377
00:12:23,120 --> 00:12:24,399
you know i think the stakes are high

378
00:12:24,399 --> 00:12:25,600
because

379
00:12:25,600 --> 00:12:27,600
a big big part of security is about

380
00:12:27,600 --> 00:12:30,000
looking through data uh to detect signs

381
00:12:30,000 --> 00:12:32,160
of cyber attacks and ai clearly has a

382
00:12:32,160 --> 00:12:34,000
big role to play there so we need to

383
00:12:34,000 --> 00:12:35,519
we need to

384
00:12:35,519 --> 00:12:37,680
um get our act together as a community

385
00:12:37,680 --> 00:12:41,359
around around this question of openness

386
00:12:42,320 --> 00:12:44,959
okay so in order to make this argument

387
00:12:44,959 --> 00:12:47,120
i'm going to

388
00:12:47,120 --> 00:12:47,920
uh

389
00:12:47,920 --> 00:12:50,800
back up and talk about

390
00:12:50,800 --> 00:12:52,639
the role

391
00:12:52,639 --> 00:12:54,959
that openness and open benchmarks have

392
00:12:54,959 --> 00:12:57,519
played as a catalyst for progress in

393
00:12:57,519 --> 00:13:01,519
non-security machine learning fields

394
00:13:02,160 --> 00:13:04,639
um so i just want to i just want to

395
00:13:04,639 --> 00:13:07,360
start this part of the talk by

396
00:13:07,360 --> 00:13:10,959
um really dramatizing um

397
00:13:10,959 --> 00:13:11,839
uh

398
00:13:11,839 --> 00:13:13,600
how helpful it can be to have open

399
00:13:13,600 --> 00:13:16,079
benchmarks in a field so what i'm

400
00:13:16,079 --> 00:13:17,200
showing here

401
00:13:17,200 --> 00:13:20,320
is uh is a plot from the site papers

402
00:13:20,320 --> 00:13:21,760
with code.com

403
00:13:21,760 --> 00:13:23,839
this is a very helpful site

404
00:13:23,839 --> 00:13:25,920
for folks who are actively involved in

405
00:13:25,920 --> 00:13:27,440
building machine learning systems

406
00:13:27,440 --> 00:13:28,639
basically

407
00:13:28,639 --> 00:13:32,800
this site collects data on

408
00:13:32,839 --> 00:13:36,320
published machine learning papers and

409
00:13:36,320 --> 00:13:39,440
um how well the experiments reported in

410
00:13:39,440 --> 00:13:42,720
those papers um stack up against against

411
00:13:42,720 --> 00:13:44,480
um i guess what you could call competing

412
00:13:44,480 --> 00:13:45,680
papers or

413
00:13:45,680 --> 00:13:47,600
papers that propose to solve the same

414
00:13:47,600 --> 00:13:48,560
problem

415
00:13:48,560 --> 00:13:51,920
so so here for example um is a plot of

416
00:13:51,920 --> 00:13:53,600
what looks like a few hundred papers

417
00:13:53,600 --> 00:13:55,760
each one of these dots is

418
00:13:55,760 --> 00:13:57,760
a machine learning paper that contains

419
00:13:57,760 --> 00:13:59,040
experiments

420
00:13:59,040 --> 00:14:02,560
um all of these papers are focused on uh

421
00:14:02,560 --> 00:14:05,920
classifying images of objects um i

422
00:14:05,920 --> 00:14:07,279
believe they i believe is that they're

423
00:14:07,279 --> 00:14:09,600
working these papers um

424
00:14:09,600 --> 00:14:12,240
uh use the imagenet benchmark dataset

425
00:14:12,240 --> 00:14:14,000
which is a very well known benchmark

426
00:14:14,000 --> 00:14:15,680
dataset

427
00:14:15,680 --> 00:14:17,040
computer vision

428
00:14:17,040 --> 00:14:18,800
and i believe they attempt to categorize

429
00:14:18,800 --> 00:14:21,040
objects into into one of a thousand

430
00:14:21,040 --> 00:14:23,040
categories um

431
00:14:23,040 --> 00:14:27,120
and here we're seeing how well they do

432
00:14:27,120 --> 00:14:29,680
in picking the right category when they

433
00:14:29,680 --> 00:14:31,760
look at an image

434
00:14:31,760 --> 00:14:32,560
and

435
00:14:32,560 --> 00:14:34,240
uh

436
00:14:34,240 --> 00:14:35,839
you know this plot really dramatizes

437
00:14:35,839 --> 00:14:37,040
what you can do when you have an open

438
00:14:37,040 --> 00:14:38,800
benchmark

439
00:14:38,800 --> 00:14:39,600
so

440
00:14:39,600 --> 00:14:41,760
here basically what we can see is

441
00:14:41,760 --> 00:14:43,839
progress and accuracy against this

442
00:14:43,839 --> 00:14:46,079
immigrant image classification task over

443
00:14:46,079 --> 00:14:49,680
the last decade um so if you go back to

444
00:14:49,680 --> 00:14:51,199
um a decade ago this looks like it's

445
00:14:51,199 --> 00:14:53,360
2010 this is back before the neural

446
00:14:53,360 --> 00:14:55,279
network revolution where people anybody

447
00:14:55,279 --> 00:14:56,880
if anybody in the audience works in

448
00:14:56,880 --> 00:14:58,000
computer vision you probably remember

449
00:14:58,000 --> 00:14:59,920
sift features um

450
00:14:59,920 --> 00:15:01,360
when people are using these these sift

451
00:15:01,360 --> 00:15:02,800
features um

452
00:15:02,800 --> 00:15:03,839
you know

453
00:15:03,839 --> 00:15:05,839
we were able to get as a scientific

454
00:15:05,839 --> 00:15:08,639
community 50 accuracy at this task

455
00:15:08,639 --> 00:15:11,279
uh since then we've improved up to 90

456
00:15:11,279 --> 00:15:13,760
accuracy which is i mean it's just

457
00:15:13,760 --> 00:15:15,680
totally dramatic i mean it's it's so

458
00:15:15,680 --> 00:15:17,120
dramatic that a whole new class of

459
00:15:17,120 --> 00:15:18,880
technologies are now

460
00:15:18,880 --> 00:15:20,720
possible um because of the improvements

461
00:15:20,720 --> 00:15:22,000
in computer vision

462
00:15:22,000 --> 00:15:23,360
um and the really beautiful thing here

463
00:15:23,360 --> 00:15:26,320
is that is that um the computer vision

464
00:15:26,320 --> 00:15:27,920
community was able to track its progress

465
00:15:27,920 --> 00:15:29,279
this entire

466
00:15:29,279 --> 00:15:31,920
past decade against the common benchmark

467
00:15:31,920 --> 00:15:33,680
when people published a paper

468
00:15:33,680 --> 00:15:35,440
uh

469
00:15:35,440 --> 00:15:37,839
um and they did better than previously

470
00:15:37,839 --> 00:15:39,360
reported results against this image that

471
00:15:39,360 --> 00:15:41,680
benchmark um they would declare that

472
00:15:41,680 --> 00:15:43,120
they they got state of the art results

473
00:15:43,120 --> 00:15:44,720
against this image that

474
00:15:44,720 --> 00:15:46,480
understand benchmark i mean there's some

475
00:15:46,480 --> 00:15:48,079
there's some problems with just focusing

476
00:15:48,079 --> 00:15:50,320
on one benchmark um

477
00:15:50,320 --> 00:15:51,839
but it's way better to have a benchmark

478
00:15:51,839 --> 00:15:53,600
than not because

479
00:15:53,600 --> 00:15:56,000
the issue is in cyber security

480
00:15:56,000 --> 00:15:57,600
we don't we don't have the ability to

481
00:15:57,600 --> 00:16:00,399
create a plot like this for

482
00:16:00,399 --> 00:16:01,839
um

483
00:16:01,839 --> 00:16:05,279
almost any there's some exceptions um

484
00:16:05,279 --> 00:16:06,720
which i'll talk about near the end of

485
00:16:06,720 --> 00:16:07,759
the talk but

486
00:16:07,759 --> 00:16:10,639
almost any cyber security problem um

487
00:16:10,639 --> 00:16:12,480
uh we just haven't been able to track

488
00:16:12,480 --> 00:16:13,680
progress in this way because we don't

489
00:16:13,680 --> 00:16:16,240
have community benchmarks

490
00:16:16,240 --> 00:16:18,000
um and i think i think the implications

491
00:16:18,000 --> 00:16:19,839
of that are fairly obvious in terms of

492
00:16:19,839 --> 00:16:21,440
our ability to improve as a field we

493
00:16:21,440 --> 00:16:23,040
can't if you can't track improvement um

494
00:16:23,040 --> 00:16:25,519
it's hard to improve

495
00:16:25,519 --> 00:16:27,360
um now

496
00:16:27,360 --> 00:16:29,040
outside of server security there are all

497
00:16:29,040 --> 00:16:31,040
sorts of benchmarks available so like on

498
00:16:31,040 --> 00:16:33,600
the papers with code site um there are

499
00:16:33,600 --> 00:16:35,279
benchmark data sets for semantic

500
00:16:35,279 --> 00:16:37,040
segmentation for image classification

501
00:16:37,040 --> 00:16:38,800
for object detection for image

502
00:16:38,800 --> 00:16:40,800
generation for denoising

503
00:16:40,800 --> 00:16:42,000
um

504
00:16:42,000 --> 00:16:43,199
there are benchmarks for the natural

505
00:16:43,199 --> 00:16:45,360
language processing

506
00:16:45,360 --> 00:16:47,120
category of tasks language modeling

507
00:16:47,120 --> 00:16:49,440
machine translation question answering

508
00:16:49,440 --> 00:16:51,040
any number of things i mean there are

509
00:16:51,040 --> 00:16:52,880
many more tasks here actually but these

510
00:16:52,880 --> 00:16:54,399
these are the top tasks

511
00:16:54,399 --> 00:16:56,160
um

512
00:16:56,160 --> 00:16:58,560
in in the medical domain raids there

513
00:16:58,560 --> 00:16:59,839
there are all sorts of benchmarks as

514
00:16:59,839 --> 00:17:00,880
well

515
00:17:00,880 --> 00:17:02,160
um

516
00:17:02,160 --> 00:17:04,959
you know i mean they i think the medical

517
00:17:04,959 --> 00:17:07,039
domain benchmarks are instructive just

518
00:17:07,039 --> 00:17:08,880
because um

519
00:17:08,880 --> 00:17:10,079
there are

520
00:17:10,079 --> 00:17:11,760
they remind us that that in some of

521
00:17:11,760 --> 00:17:13,679
these domains um

522
00:17:13,679 --> 00:17:14,959
really in all of them and some of them

523
00:17:14,959 --> 00:17:16,959
more obvious than others

524
00:17:16,959 --> 00:17:18,880
there there are real moral stakes to the

525
00:17:18,880 --> 00:17:20,319
scientific standards we hold ourselves

526
00:17:20,319 --> 00:17:22,720
to as a community um

527
00:17:22,720 --> 00:17:25,919
so um where they're not benchmarks for

528
00:17:25,919 --> 00:17:28,000
you know cancer detection and medical

529
00:17:28,000 --> 00:17:29,919
images and this kind of thing uh those

530
00:17:29,919 --> 00:17:31,360
algorithms would perform less well and

531
00:17:31,360 --> 00:17:33,280
more people would die right

532
00:17:33,280 --> 00:17:34,640
and the fact that we don't have enough

533
00:17:34,640 --> 00:17:36,320
benchmarks in cybersecurity machine

534
00:17:36,320 --> 00:17:37,360
learning

535
00:17:37,360 --> 00:17:38,160
um

536
00:17:38,160 --> 00:17:39,840
really does have moral implications the

537
00:17:39,840 --> 00:17:41,120
fact that we can't track progress

538
00:17:41,120 --> 00:17:43,360
against against you know key

539
00:17:43,360 --> 00:17:46,320
cybersecurity detection tasks

540
00:17:46,320 --> 00:17:47,679
realistically means that more people

541
00:17:47,679 --> 00:17:49,919
will get compromised um as a result of

542
00:17:49,919 --> 00:17:51,440
that failure which is part of the reason

543
00:17:51,440 --> 00:17:53,039
i'm here giving this talk and advocating

544
00:17:53,039 --> 00:17:54,799
for the creation of more open data sets

545
00:17:54,799 --> 00:17:57,440
and benchmarks

546
00:17:59,120 --> 00:18:01,200
um and so so really

547
00:18:01,200 --> 00:18:03,919
benchmarks have bo both tell the story

548
00:18:03,919 --> 00:18:06,000
of progress in

549
00:18:06,000 --> 00:18:07,120
um

550
00:18:07,120 --> 00:18:09,600
the security ai uh but they also create

551
00:18:09,600 --> 00:18:10,880
the progress

552
00:18:10,880 --> 00:18:12,240
because you know there's the there's

553
00:18:12,240 --> 00:18:14,880
this sort of trait aphorism

554
00:18:14,880 --> 00:18:16,720
uh you can't improve what you don't

555
00:18:16,720 --> 00:18:19,039
measure um and i think you know to a

556
00:18:19,039 --> 00:18:21,360
large extent that's true um

557
00:18:21,360 --> 00:18:22,880
and um

558
00:18:22,880 --> 00:18:24,240
because

559
00:18:24,240 --> 00:18:26,080
you know many of these many fields

560
00:18:26,080 --> 00:18:28,799
outside of cybersecurity ml

561
00:18:28,799 --> 00:18:30,480
have set up a community infrastructure

562
00:18:30,480 --> 00:18:32,160
for measuring progress

563
00:18:32,160 --> 00:18:34,080
they've improved and because we haven't

564
00:18:34,080 --> 00:18:35,679
we haven't improved as much as we should

565
00:18:35,679 --> 00:18:38,679
have

566
00:18:43,200 --> 00:18:44,480
um

567
00:18:44,480 --> 00:18:46,400
you know so i i just want to i just want

568
00:18:46,400 --> 00:18:48,240
to drive this point home by giving a

569
00:18:48,240 --> 00:18:50,400
kind of split screen um

570
00:18:50,400 --> 00:18:52,640
so at its worst in our fields in

571
00:18:52,640 --> 00:18:54,000
security

572
00:18:54,000 --> 00:18:56,400
we have marketing schlock a lot of

573
00:18:56,400 --> 00:18:58,160
condescending stuff you know marketing

574
00:18:58,160 --> 00:18:59,840
literature that shows you know 3d

575
00:18:59,840 --> 00:19:01,600
neurons spinning around

576
00:19:01,600 --> 00:19:03,200
um

577
00:19:03,200 --> 00:19:05,200
uh

578
00:19:05,200 --> 00:19:06,880
and you know i mean

579
00:19:06,880 --> 00:19:08,640
that is unfortunately

580
00:19:08,640 --> 00:19:10,160
sort of average over the way we're doing

581
00:19:10,160 --> 00:19:12,320
security ai right now that's probably

582
00:19:12,320 --> 00:19:14,240
about average um

583
00:19:14,240 --> 00:19:15,760
outside of security at least in some

584
00:19:15,760 --> 00:19:16,880
fields

585
00:19:16,880 --> 00:19:18,880
we're just in a much better situation in

586
00:19:18,880 --> 00:19:20,480
terms of um

587
00:19:20,480 --> 00:19:22,559
the expectations that researchers even

588
00:19:22,559 --> 00:19:25,039
private companies publish

589
00:19:25,039 --> 00:19:26,880
openness

590
00:19:26,880 --> 00:19:28,559
and a kind of

591
00:19:28,559 --> 00:19:30,720
a kind of game in which researchers at

592
00:19:30,720 --> 00:19:32,320
computing institutions

593
00:19:32,320 --> 00:19:33,360
are collaborating because they're

594
00:19:33,360 --> 00:19:36,000
publishing um and they're also competing

595
00:19:36,000 --> 00:19:38,240
because they're trying to

596
00:19:38,240 --> 00:19:40,160
publish papers that show better results

597
00:19:40,160 --> 00:19:42,240
than their competitors but ideas are

598
00:19:42,240 --> 00:19:43,760
getting cross-pollinated and there's an

599
00:19:43,760 --> 00:19:45,600
overall technological improvement and

600
00:19:45,600 --> 00:19:47,200
and you know what we need to get to is

601
00:19:47,200 --> 00:19:49,200
something more like this and less like

602
00:19:49,200 --> 00:19:52,679
this on the left

603
00:19:53,600 --> 00:19:55,440
okay so there are obviously some

604
00:19:55,440 --> 00:19:57,039
challenges to

605
00:19:57,039 --> 00:19:58,559
um

606
00:19:58,559 --> 00:20:00,559
achieving the the the vision that i'm

607
00:20:00,559 --> 00:20:03,200
describing here and i i think they're

608
00:20:03,200 --> 00:20:05,120
surmountable but i think they're they're

609
00:20:05,120 --> 00:20:08,880
real um so some obvious ones are

610
00:20:08,880 --> 00:20:10,559
privacy openness

611
00:20:10,559 --> 00:20:12,240
and um

612
00:20:12,240 --> 00:20:14,720
perceived risks to companies competitive

613
00:20:14,720 --> 00:20:16,880
advantages who are making big

614
00:20:16,880 --> 00:20:20,640
investments in security ai

615
00:20:20,640 --> 00:20:21,679
um

616
00:20:21,679 --> 00:20:23,840
so let me let me walk through

617
00:20:23,840 --> 00:20:25,039
um

618
00:20:25,039 --> 00:20:26,640
why these issues

619
00:20:26,640 --> 00:20:28,880
matter and and then i'll talk about um

620
00:20:28,880 --> 00:20:31,200
how i think we can address them

621
00:20:31,200 --> 00:20:33,760
uh so first so so

622
00:20:33,760 --> 00:20:34,720
you know

623
00:20:34,720 --> 00:20:36,960
you could so in in

624
00:20:36,960 --> 00:20:38,799
in the case where we we may want to

625
00:20:38,799 --> 00:20:41,679
release malware as part of benchmark

626
00:20:41,679 --> 00:20:43,760
data sets uh

627
00:20:43,760 --> 00:20:45,440
you know one could argue that that model

628
00:20:45,440 --> 00:20:46,960
could be weaponized by adversaries and

629
00:20:46,960 --> 00:20:48,400
one doesn't want to you know make these

630
00:20:48,400 --> 00:20:50,240
data sets open

631
00:20:50,240 --> 00:20:52,320
um you know in cases where we want to

632
00:20:52,320 --> 00:20:53,120
make

633
00:20:53,120 --> 00:20:55,280
public benchmarks around detecting

634
00:20:55,280 --> 00:20:57,520
phishing emails uh

635
00:20:57,520 --> 00:20:59,600
you know one i think one could quite

636
00:20:59,600 --> 00:21:01,200
credibly argue that there are big

637
00:21:01,200 --> 00:21:02,960
privacy risks there we could potentially

638
00:21:02,960 --> 00:21:04,640
be you know um

639
00:21:04,640 --> 00:21:06,240
if we were if we were releasing a

640
00:21:06,240 --> 00:21:07,600
benchmark that included both the nine

641
00:21:07,600 --> 00:21:09,840
emails and phishing emails obviously

642
00:21:09,840 --> 00:21:11,440
those benign emails

643
00:21:11,440 --> 00:21:12,799
would contain private information we

644
00:21:12,799 --> 00:21:14,960
need to figure out how to release

645
00:21:14,960 --> 00:21:16,159
benign data in a way that doesn't

646
00:21:16,159 --> 00:21:18,640
compromise people's privacy and even

647
00:21:18,640 --> 00:21:20,240
phishing emails can sometimes if they're

648
00:21:20,240 --> 00:21:21,679
based on background research and such

649
00:21:21,679 --> 00:21:22,880
can sometimes contain private

650
00:21:22,880 --> 00:21:25,600
information and that's a real problem

651
00:21:25,600 --> 00:21:27,600
uh one could argue that

652
00:21:27,600 --> 00:21:30,720
if everybody's publishing their security

653
00:21:30,720 --> 00:21:32,640
machine learning

654
00:21:32,640 --> 00:21:34,000
approaches

655
00:21:34,000 --> 00:21:35,679
that adversaries could use that open

656
00:21:35,679 --> 00:21:36,720
knowledge

657
00:21:36,720 --> 00:21:38,240
uh

658
00:21:38,240 --> 00:21:39,600
in order to figure out how to subvert

659
00:21:39,600 --> 00:21:41,200
those technologies

660
00:21:41,200 --> 00:21:43,120
that's true too and i mean we need to

661
00:21:43,120 --> 00:21:44,799
take that seriously or that objection

662
00:21:44,799 --> 00:21:46,320
seriously as well

663
00:21:46,320 --> 00:21:48,480
um and then finally we have to respect

664
00:21:48,480 --> 00:21:50,480
the business um

665
00:21:50,480 --> 00:21:53,200
mission of companies uh who may not want

666
00:21:53,200 --> 00:21:53,840
to

667
00:21:53,840 --> 00:21:55,440
share their you know give up their

668
00:21:55,440 --> 00:21:56,720
competitive advantage if they've spent a

669
00:21:56,720 --> 00:22:00,159
lot of money to develop um

670
00:22:00,159 --> 00:22:01,600
you know high performing machine

671
00:22:01,600 --> 00:22:03,120
learning algorithms so i want to walk

672
00:22:03,120 --> 00:22:06,960
through and address these these issues

673
00:22:06,960 --> 00:22:07,760
okay

674
00:22:07,760 --> 00:22:08,640
so

675
00:22:08,640 --> 00:22:10,880
one so this idea that benchmark malware

676
00:22:10,880 --> 00:22:12,720
could be weaponized by

677
00:22:12,720 --> 00:22:14,080
adversaries

678
00:22:14,080 --> 00:22:17,280
so i i would say that i mean

679
00:22:17,280 --> 00:22:20,000
i respect that objection to releasing uh

680
00:22:20,000 --> 00:22:21,440
let's say a benchmark data set of

681
00:22:21,440 --> 00:22:24,960
malware and spoiler alert like my the

682
00:22:24,960 --> 00:22:27,919
organization i i manage at my company

683
00:22:27,919 --> 00:22:31,280
sophos um has released um a big data set

684
00:22:31,280 --> 00:22:33,360
that includes 10 million dollar binaries

685
00:22:33,360 --> 00:22:35,200
um so you know where my position is here

686
00:22:35,200 --> 00:22:36,799
but i think it's i think it makes sense

687
00:22:36,799 --> 00:22:38,640
to object that adversaries could use

688
00:22:38,640 --> 00:22:39,440
this

689
00:22:39,440 --> 00:22:42,400
you know my counter to that would be

690
00:22:42,400 --> 00:22:45,039
it's not hard to get malware um

691
00:22:45,039 --> 00:22:46,480
you know if you have a subscription to

692
00:22:46,480 --> 00:22:48,320
virustotal you can just go download

693
00:22:48,320 --> 00:22:50,080
millions of malware binaries

694
00:22:50,080 --> 00:22:52,480
all by yourself there are big malware

695
00:22:52,480 --> 00:22:54,559
repositories where for free on the

696
00:22:54,559 --> 00:22:56,240
internet you can just go download lots

697
00:22:56,240 --> 00:22:58,000
of malware binaries

698
00:22:58,000 --> 00:22:59,679
um

699
00:22:59,679 --> 00:23:02,000
you know

700
00:23:02,559 --> 00:23:04,960
even with that um

701
00:23:04,960 --> 00:23:08,000
it's um it's possible adversaries would

702
00:23:08,000 --> 00:23:09,679
go to some benchmark data set and decide

703
00:23:09,679 --> 00:23:11,200
that's where they were going to get

704
00:23:11,200 --> 00:23:12,720
malware from if they wanted to just get

705
00:23:12,720 --> 00:23:14,320
some big dump of malware and somehow use

706
00:23:14,320 --> 00:23:16,000
it as part of the campaign

707
00:23:16,000 --> 00:23:17,520
um

708
00:23:17,520 --> 00:23:19,440
but you know like what we did with our

709
00:23:19,440 --> 00:23:20,799
malware dataset and i'll talk about this

710
00:23:20,799 --> 00:23:23,120
in a little bit in more detail

711
00:23:23,120 --> 00:23:26,720
is we is we changed um some some crucial

712
00:23:26,720 --> 00:23:28,240
bits in the malware to make it much

713
00:23:28,240 --> 00:23:30,559
harder to actually use even while we

714
00:23:30,559 --> 00:23:33,039
didn't strip away the information that

715
00:23:33,039 --> 00:23:34,640
machine learning learning researchers

716
00:23:34,640 --> 00:23:35,919
tend to use

717
00:23:35,919 --> 00:23:37,039
so i think it's possible to make it

718
00:23:37,039 --> 00:23:38,640
harder for adversaries to actually use

719
00:23:38,640 --> 00:23:39,919
the malware

720
00:23:39,919 --> 00:23:41,520
i would also say that you know malware

721
00:23:41,520 --> 00:23:43,919
binaries are tend to be part of like

722
00:23:43,919 --> 00:23:47,520
big complicated distributed systems

723
00:23:47,520 --> 00:23:49,200
and the binaries on their own whether

724
00:23:49,200 --> 00:23:52,559
they're osx elf pe or you know whatever

725
00:23:52,559 --> 00:23:54,880
binary format they tend to they tend to

726
00:23:54,880 --> 00:23:57,360
be to go defunct pretty quickly after

727
00:23:57,360 --> 00:24:00,080
they're created um they malware binaries

728
00:24:00,080 --> 00:24:01,760
tend to be generated by like polymorphic

729
00:24:01,760 --> 00:24:03,360
malware generators

730
00:24:03,360 --> 00:24:05,120
and you know have

731
00:24:05,120 --> 00:24:07,200
c2 information hard-coded into them and

732
00:24:07,200 --> 00:24:09,200
that stuff goes out of date

733
00:24:09,200 --> 00:24:10,320
pretty quickly

734
00:24:10,320 --> 00:24:12,240
um

735
00:24:12,240 --> 00:24:15,520
so i don't think unbalanced this uh

736
00:24:15,520 --> 00:24:18,400
this this objection should um

737
00:24:18,400 --> 00:24:20,960
shutter the the overall mission of

738
00:24:20,960 --> 00:24:22,559
of creating benchmarks that allow us to

739
00:24:22,559 --> 00:24:25,120
compare uh results between organizations

740
00:24:25,120 --> 00:24:26,320
and i think we need to consider the cost

741
00:24:26,320 --> 00:24:29,840
of not releasing benchmarks as well um

742
00:24:29,840 --> 00:24:31,200
which are you know

743
00:24:31,200 --> 00:24:32,559
what i argued for in the previous

744
00:24:32,559 --> 00:24:34,159
section of this talk like

745
00:24:34,159 --> 00:24:36,720
an inability to track progress um

746
00:24:36,720 --> 00:24:38,320
against important cyber security

747
00:24:38,320 --> 00:24:40,639
problems

748
00:24:41,520 --> 00:24:44,880
okay so another objection to

749
00:24:44,880 --> 00:24:46,559
creating benchmark data sets that the

750
00:24:46,559 --> 00:24:48,159
security machine learning community can

751
00:24:48,159 --> 00:24:49,600
use to track progress

752
00:24:49,600 --> 00:24:52,480
is that um there are

753
00:24:52,480 --> 00:24:54,799
real privacy risks here so i think this

754
00:24:54,799 --> 00:24:56,400
is we need to take this extremely

755
00:24:56,400 --> 00:24:58,960
seriously you know a major component of

756
00:24:58,960 --> 00:25:00,159
our mission as a field is to protect

757
00:25:00,159 --> 00:25:02,640
people's privacy um we definitely don't

758
00:25:02,640 --> 00:25:05,840
want to do do harm here um

759
00:25:05,840 --> 00:25:07,520
uh

760
00:25:07,520 --> 00:25:08,799
so

761
00:25:08,799 --> 00:25:11,440
a a couple

762
00:25:11,440 --> 00:25:12,480
a couple

763
00:25:12,480 --> 00:25:14,960
rebuttals i guess on this point uh one

764
00:25:14,960 --> 00:25:17,840
is i we have so few benchmarks in

765
00:25:17,840 --> 00:25:20,240
cybersecurity ai that i think we can we

766
00:25:20,240 --> 00:25:21,840
can start with the low-hanging fruit

767
00:25:21,840 --> 00:25:23,279
that doesn't involve a lot of privacy

768
00:25:23,279 --> 00:25:24,480
risk so

769
00:25:24,480 --> 00:25:25,760
um

770
00:25:25,760 --> 00:25:28,400
i'll talk later about this but like

771
00:25:28,400 --> 00:25:30,240
sofos and

772
00:25:30,240 --> 00:25:32,400
my team within sofa sofa say i

773
00:25:32,400 --> 00:25:34,720
released this data set of of

774
00:25:34,720 --> 00:25:36,640
windows malware um and windows benign

775
00:25:36,640 --> 00:25:37,520
wear

776
00:25:37,520 --> 00:25:38,880
for machine learning researchers and i

777
00:25:38,880 --> 00:25:40,840
think there's not much privacy risk to

778
00:25:40,840 --> 00:25:43,360
releasing um

779
00:25:43,360 --> 00:25:45,760
there's there's minimal privacy risk to

780
00:25:45,760 --> 00:25:47,919
releasing portable executable binaries

781
00:25:47,919 --> 00:25:49,440
it's not like releasing email or

782
00:25:49,440 --> 00:25:51,440
documents um

783
00:25:51,440 --> 00:25:54,960
uh you know there's a bunch more

784
00:25:54,960 --> 00:25:56,400
candidates

785
00:25:56,400 --> 00:25:58,320
benchmark problems that we could solve

786
00:25:58,320 --> 00:25:59,760
that that are analogous so like

787
00:25:59,760 --> 00:26:00,720
releasing

788
00:26:00,720 --> 00:26:05,279
android apps uh os x elf binaries um

789
00:26:05,279 --> 00:26:08,880
uh releasing uh osx macro binaries or

790
00:26:08,880 --> 00:26:10,640
linux health binaries there's a bunch of

791
00:26:10,640 --> 00:26:12,320
problems like that where i think we can

792
00:26:12,320 --> 00:26:14,400
we can sidestep privacy risks

793
00:26:14,400 --> 00:26:16,880
when it gets to questions like email

794
00:26:16,880 --> 00:26:18,480
i think we should take a hard look at

795
00:26:18,480 --> 00:26:21,039
techniques like differential privacy

796
00:26:21,039 --> 00:26:22,960
which involve adding adding noise to

797
00:26:22,960 --> 00:26:24,640
feature vectors to make the original

798
00:26:24,640 --> 00:26:26,880
information in those future vectors

799
00:26:26,880 --> 00:26:28,159
um

800
00:26:28,159 --> 00:26:31,360
somewhat unrecoverable

801
00:26:32,159 --> 00:26:34,080
and that is an avenue to

802
00:26:34,080 --> 00:26:36,880
release data to mitigate privacy risk

803
00:26:36,880 --> 00:26:38,080
i think we want to just to be clear i

804
00:26:38,080 --> 00:26:39,520
think we need to be absolutely sure that

805
00:26:39,520 --> 00:26:41,520
we're not going to leak pii when we do

806
00:26:41,520 --> 00:26:43,039
these benchmark data sets but there are

807
00:26:43,039 --> 00:26:45,600
also there are also ways of of doing

808
00:26:45,600 --> 00:26:48,080
that um

809
00:26:48,080 --> 00:26:50,400
that um that i think well i think

810
00:26:50,400 --> 00:26:51,760
they're

811
00:26:51,760 --> 00:26:53,679
there are at least promising

812
00:26:53,679 --> 00:26:55,200
tools in the research literature that

813
00:26:55,200 --> 00:26:57,120
may allow us to do that even for highly

814
00:26:57,120 --> 00:26:58,799
private data like email if you sort of

815
00:26:58,799 --> 00:27:00,799
add enough noise to the email uh you can

816
00:27:00,799 --> 00:27:03,760
prove that it's impossible to recover um

817
00:27:03,760 --> 00:27:05,760
the information that we want to protect

818
00:27:05,760 --> 00:27:08,480
um i think for for data that we think we

819
00:27:08,480 --> 00:27:10,480
can't uh

820
00:27:10,480 --> 00:27:13,440
definitely privacy protect um it might

821
00:27:13,440 --> 00:27:14,720
be possible that we could create

822
00:27:14,720 --> 00:27:16,640
institutional arrangements in which uh

823
00:27:16,640 --> 00:27:19,840
some trusted third party

824
00:27:19,840 --> 00:27:20,720
uh

825
00:27:20,720 --> 00:27:23,360
takes researchers ml models

826
00:27:23,360 --> 00:27:25,120
runs those models against a private data

827
00:27:25,120 --> 00:27:26,720
set that nobody else can see and then

828
00:27:26,720 --> 00:27:29,120
reports out results and we should be

829
00:27:29,120 --> 00:27:30,399
thinking along those lines when it gets

830
00:27:30,399 --> 00:27:31,279
into

831
00:27:31,279 --> 00:27:33,840
benchmarking against um you know very

832
00:27:33,840 --> 00:27:38,000
privacy privacy sensitive problems

833
00:27:39,120 --> 00:27:40,880
um

834
00:27:40,880 --> 00:27:43,120
then there's this objection that if we

835
00:27:43,120 --> 00:27:44,559
if everybody starts publishing their

836
00:27:44,559 --> 00:27:47,200
security machine learning approaches

837
00:27:47,200 --> 00:27:48,640
we risk just

838
00:27:48,640 --> 00:27:50,720
exposing too much information to

839
00:27:50,720 --> 00:27:52,320
adversaries

840
00:27:52,320 --> 00:27:54,880
i would say that um

841
00:27:54,880 --> 00:27:56,799
so a few things about this

842
00:27:56,799 --> 00:27:59,039
so historically

843
00:27:59,039 --> 00:28:01,360
i i wouldn't i'm not a dogmatist and i

844
00:28:01,360 --> 00:28:02,799
think sometimes

845
00:28:02,799 --> 00:28:04,480
you know being obscure about your

846
00:28:04,480 --> 00:28:06,399
technology might be a good

847
00:28:06,399 --> 00:28:09,120
strategy or tactic um to to try to

848
00:28:09,120 --> 00:28:10,559
protect yourself

849
00:28:10,559 --> 00:28:11,760
but i think for the most part when it

850
00:28:11,760 --> 00:28:14,000
comes to sort of broad scientific

851
00:28:14,000 --> 00:28:15,360
innovation

852
00:28:15,360 --> 00:28:17,600
security through obscurity is just

853
00:28:17,600 --> 00:28:19,840
it hasn't worn out to have been a a good

854
00:28:19,840 --> 00:28:22,080
idea um

855
00:28:22,080 --> 00:28:23,760
and i think in i think in areas like

856
00:28:23,760 --> 00:28:26,720
like cryptography um openness i think

857
00:28:26,720 --> 00:28:28,399
has a really good track record i think

858
00:28:28,399 --> 00:28:29,679
it's much better

859
00:28:29,679 --> 00:28:31,919
that we're open about fundamental

860
00:28:31,919 --> 00:28:34,480
scientific ideas in cyber security then

861
00:28:34,480 --> 00:28:36,399
we think that by working in a little

862
00:28:36,399 --> 00:28:37,760
silo

863
00:28:37,760 --> 00:28:39,840
we can produce um

864
00:28:39,840 --> 00:28:41,360
better better science and better

865
00:28:41,360 --> 00:28:42,720
technology and better protection for our

866
00:28:42,720 --> 00:28:44,559
customers

867
00:28:44,559 --> 00:28:46,880
i really i just i'm highly skeptical of

868
00:28:46,880 --> 00:28:48,880
the idea that you know

869
00:28:48,880 --> 00:28:50,720
um

870
00:28:50,720 --> 00:28:52,720
relatively small teams of scores of

871
00:28:52,720 --> 00:28:54,720
individuals working in isolation and

872
00:28:54,720 --> 00:28:56,720
secrecy are going to produce better

873
00:28:56,720 --> 00:28:58,640
technology than

874
00:28:58,640 --> 00:29:00,480
um whole communities of scientists

875
00:29:00,480 --> 00:29:02,799
network together sharing results even if

876
00:29:02,799 --> 00:29:04,320
even if the price of that is adversaries

877
00:29:04,320 --> 00:29:07,279
can see our algorithms

878
00:29:08,480 --> 00:29:10,000
okay so

879
00:29:10,000 --> 00:29:12,480
getting into the last part of this talk

880
00:29:12,480 --> 00:29:14,080
where do we go from here and how do we

881
00:29:14,080 --> 00:29:15,840
build a culture of open science and

882
00:29:15,840 --> 00:29:18,080
security ai

883
00:29:18,080 --> 00:29:20,720
um i mean i think our mission has to be

884
00:29:20,720 --> 00:29:24,320
to try to cover as much of

885
00:29:24,320 --> 00:29:25,360
the

886
00:29:25,360 --> 00:29:26,399
threat

887
00:29:26,399 --> 00:29:28,799
ontology as we possibly can with

888
00:29:28,799 --> 00:29:31,360
openness and benchmarks uh

889
00:29:31,360 --> 00:29:33,039
so i mean what we see in front of us is

890
00:29:33,039 --> 00:29:34,880
a representation of the mitre attack

891
00:29:34,880 --> 00:29:36,799
framework made i really think we need to

892
00:29:36,799 --> 00:29:38,559
go through as a community

893
00:29:38,559 --> 00:29:40,720
and figure out where machine learning

894
00:29:40,720 --> 00:29:43,039
may help in detecting the various

895
00:29:43,039 --> 00:29:44,559
tactics techniques and procedures

896
00:29:44,559 --> 00:29:46,240
represented in something like the mitre

897
00:29:46,240 --> 00:29:48,080
attack framework

898
00:29:48,080 --> 00:29:49,120
um

899
00:29:49,120 --> 00:29:51,279
and and then develop benchmarks as a

900
00:29:51,279 --> 00:29:53,919
community um so that we can achieve

901
00:29:53,919 --> 00:29:55,360
something like

902
00:29:55,360 --> 00:29:57,120
uh the visualizations i showed from the

903
00:29:57,120 --> 00:29:59,520
papers with code websites earlier but

904
00:29:59,520 --> 00:30:01,120
but for cyber security i mean i think

905
00:30:01,120 --> 00:30:03,440
that really has to be the goal um and

906
00:30:03,440 --> 00:30:05,039
there's no reason it shouldn't be

907
00:30:05,039 --> 00:30:07,600
there's there's nothing that

908
00:30:07,600 --> 00:30:09,840
that um peculiar about cyber security

909
00:30:09,840 --> 00:30:12,880
that that that makes us different than

910
00:30:12,880 --> 00:30:14,320
um

911
00:30:14,320 --> 00:30:16,000
than like the medical the medical

912
00:30:16,000 --> 00:30:17,760
machine learning community in terms of

913
00:30:17,760 --> 00:30:19,200
like the scientific standards that we

914
00:30:19,200 --> 00:30:20,799
should uphold i mean i think i think

915
00:30:20,799 --> 00:30:22,000
that expectation should be that we

916
00:30:22,000 --> 00:30:23,520
develop benchmarks for the problems that

917
00:30:23,520 --> 00:30:24,240
we

918
00:30:24,240 --> 00:30:27,679
are working on in security and ml um

919
00:30:27,679 --> 00:30:30,559
so that's one um two i mean i i would

920
00:30:30,559 --> 00:30:32,559
say um

921
00:30:32,559 --> 00:30:34,480
you know i mean we we're trying to take

922
00:30:34,480 --> 00:30:36,320
steps

923
00:30:36,320 --> 00:30:38,480
in the direction of building the future

924
00:30:38,480 --> 00:30:41,600
that i'm advocating for here

925
00:30:41,600 --> 00:30:42,480
on

926
00:30:42,480 --> 00:30:45,120
on the data science team that i manage

927
00:30:45,120 --> 00:30:47,440
so we released this dataset which is

928
00:30:47,440 --> 00:30:49,360
called sorel we did this in

929
00:30:49,360 --> 00:30:52,000
collaboration with reversing labs uh i

930
00:30:52,000 --> 00:30:53,360
want to call out the principal

931
00:30:53,360 --> 00:30:55,440
contributors here which were ethan rudds

932
00:30:55,440 --> 00:30:57,200
and rich herring

933
00:30:57,200 --> 00:30:58,240
who were

934
00:30:58,240 --> 00:31:01,120
supported by my team as they were

935
00:31:01,120 --> 00:31:02,559
doing this work

936
00:31:02,559 --> 00:31:05,440
this is a data set of 20 million

937
00:31:05,440 --> 00:31:08,080
of 10 million malware binaries and the

938
00:31:08,080 --> 00:31:11,039
metadata from 10 million benign binaries

939
00:31:11,039 --> 00:31:12,080
um

940
00:31:12,080 --> 00:31:14,559
and it's really it's an industrial scale

941
00:31:14,559 --> 00:31:16,880
data set

942
00:31:16,880 --> 00:31:19,039
that's large enough that you can train

943
00:31:19,039 --> 00:31:21,360
like a real industrial windows malware

944
00:31:21,360 --> 00:31:22,480
detector

945
00:31:22,480 --> 00:31:24,240
um and we hope this becomes like

946
00:31:24,240 --> 00:31:25,840
imagenet and some of the other

947
00:31:25,840 --> 00:31:27,440
benchmarks that i showed earlier in

948
00:31:27,440 --> 00:31:29,279
terms of its

949
00:31:29,279 --> 00:31:31,360
prestige as a measuring stick for

950
00:31:31,360 --> 00:31:33,039
windows malware detection we hope this

951
00:31:33,039 --> 00:31:35,360
allows people to better track

952
00:31:35,360 --> 00:31:38,080
progress against like a core problem in

953
00:31:38,080 --> 00:31:39,440
the security industry which is detecting

954
00:31:39,440 --> 00:31:41,279
windows malware like you know that's

955
00:31:41,279 --> 00:31:43,039
been a problem for a very long time and

956
00:31:43,039 --> 00:31:44,880
it's still a very big problem

957
00:31:44,880 --> 00:31:46,480
and this this is our

958
00:31:46,480 --> 00:31:49,519
sort of brick and and you know the

959
00:31:49,519 --> 00:31:51,039
metaphorical wall that we think we need

960
00:31:51,039 --> 00:31:52,559
to build around

961
00:31:52,559 --> 00:31:54,240
scientific institutions and openness and

962
00:31:54,240 --> 00:31:55,679
security

963
00:31:55,679 --> 00:31:57,679
i also want to call out the elastic team

964
00:31:57,679 --> 00:32:00,880
which built the ember data sets um this

965
00:32:00,880 --> 00:32:02,880
is a different take on building a

966
00:32:02,880 --> 00:32:04,799
windows malware benchmark we think

967
00:32:04,799 --> 00:32:06,320
having having

968
00:32:06,320 --> 00:32:08,559
more than one benchmark data set

969
00:32:08,559 --> 00:32:09,919
is um

970
00:32:09,919 --> 00:32:12,559
is important and and good

971
00:32:12,559 --> 00:32:14,080
and we think that the experience of

972
00:32:14,080 --> 00:32:16,159
other fields and machine learning um

973
00:32:16,159 --> 00:32:17,600
like in computer vision and medicine and

974
00:32:17,600 --> 00:32:18,960
some of the other areas that i talked

975
00:32:18,960 --> 00:32:22,240
about um bear that out um so ember is

976
00:32:22,240 --> 00:32:23,360
another project to look at if you're

977
00:32:23,360 --> 00:32:25,360
looking for a windows malware

978
00:32:25,360 --> 00:32:27,600
a windows

979
00:32:27,600 --> 00:32:29,440
um machine learning malware detection

980
00:32:29,440 --> 00:32:31,039
data set they made some different

981
00:32:31,039 --> 00:32:32,960
choices around

982
00:32:32,960 --> 00:32:34,399
openness like they didn't release the

983
00:32:34,399 --> 00:32:37,679
actual malware binaries like like we did

984
00:32:37,679 --> 00:32:39,200
but their data set has some really good

985
00:32:39,200 --> 00:32:42,000
features too

986
00:32:42,000 --> 00:32:45,919
um you know we definitely try to

987
00:32:45,919 --> 00:32:48,080
publish um

988
00:32:48,080 --> 00:32:49,840
so in my team we have we have a

989
00:32:49,840 --> 00:32:52,159
commitment to publish to to publishing

990
00:32:52,159 --> 00:32:53,600
in some form or another everything that

991
00:32:53,600 --> 00:32:55,519
we ship um we don't always we don't

992
00:32:55,519 --> 00:32:57,279
always publish right away after we ship

993
00:32:57,279 --> 00:32:59,200
um that's not because

994
00:32:59,200 --> 00:33:00,720
we're scared to publish it's sometimes

995
00:33:00,720 --> 00:33:02,320
just because of our road map and

996
00:33:02,320 --> 00:33:04,480
deadlines and you know typical

997
00:33:04,480 --> 00:33:06,320
organizational you know resource

998
00:33:06,320 --> 00:33:07,919
constraint type problems but you know we

999
00:33:07,919 --> 00:33:09,840
have a lot of publications if you if you

1000
00:33:09,840 --> 00:33:12,240
look at my google scholar profile

1001
00:33:12,240 --> 00:33:15,039
you'll see papers coming out of um you

1002
00:33:15,039 --> 00:33:16,960
know a whole legacy of work over the

1003
00:33:16,960 --> 00:33:19,919
last four or five years on my team

1004
00:33:19,919 --> 00:33:21,600
these are a bunch of publications from

1005
00:33:21,600 --> 00:33:25,360
elastic which i would say is probably um

1006
00:33:25,360 --> 00:33:27,120
i would say is another

1007
00:33:27,120 --> 00:33:28,240
very good team in terms of their

1008
00:33:28,240 --> 00:33:30,799
standards of openness

1009
00:33:30,799 --> 00:33:32,720
we really want other companies to start

1010
00:33:32,720 --> 00:33:35,840
doing this at as well and start

1011
00:33:35,840 --> 00:33:37,120
holding themselves to a standard in

1012
00:33:37,120 --> 00:33:39,039
which you know nei technology they're

1013
00:33:39,039 --> 00:33:39,919
going to claim in their marketing

1014
00:33:39,919 --> 00:33:42,480
literature and release into the field um

1015
00:33:42,480 --> 00:33:44,480
they publish a paper on and get it peer

1016
00:33:44,480 --> 00:33:47,279
reviewed and validated by

1017
00:33:47,279 --> 00:33:49,399
other colleagues in the

1018
00:33:49,399 --> 00:33:52,000
field um

1019
00:33:52,000 --> 00:33:53,919
so in conclusion i mean i think the

1020
00:33:53,919 --> 00:33:55,679
stakes are really high here i think we

1021
00:33:55,679 --> 00:33:57,679
need to build a scientific culture in

1022
00:33:57,679 --> 00:33:59,840
security data science um

1023
00:33:59,840 --> 00:34:00,880
uh

1024
00:34:00,880 --> 00:34:02,880
and um

1025
00:34:02,880 --> 00:34:04,840
i think if we do that

1026
00:34:04,840 --> 00:34:08,480
um i think that um

1027
00:34:08,480 --> 00:34:09,599
i think there's a lot that we can

1028
00:34:09,599 --> 00:34:12,480
achieve together as a community um

1029
00:34:12,480 --> 00:34:15,280
i think that if we continue as we've um

1030
00:34:15,280 --> 00:34:17,440
with it with the kind of collective

1031
00:34:17,440 --> 00:34:19,440
culture in which we've pursued security

1032
00:34:19,440 --> 00:34:21,359
machine learning thus far

1033
00:34:21,359 --> 00:34:22,320
um

1034
00:34:22,320 --> 00:34:23,760
i think we'll continue to make progress

1035
00:34:23,760 --> 00:34:25,199
but i think

1036
00:34:25,199 --> 00:34:27,040
i think we won't make the progress that

1037
00:34:27,040 --> 00:34:29,759
that we would have

1038
00:34:31,359 --> 00:34:33,440
uh and with that i want to open it up

1039
00:34:33,440 --> 00:34:34,560
for discussion

1040
00:34:34,560 --> 00:34:35,440
um

1041
00:34:35,440 --> 00:34:36,879
if you're interested in contacting me

1042
00:34:36,879 --> 00:34:38,719
you can contact me on twitter

1043
00:34:38,719 --> 00:34:40,719
my twitter handle here and you can also

1044
00:34:40,719 --> 00:34:42,639
uh please feel free to email me i'd love

1045
00:34:42,639 --> 00:34:44,399
to hear from

1046
00:34:44,399 --> 00:34:45,839
anyone who

1047
00:34:45,839 --> 00:34:48,000
listens to this talk and has feedback

1048
00:34:48,000 --> 00:34:49,599
positive or negative

1049
00:34:49,599 --> 00:34:52,079
or constructive criticism

1050
00:34:52,079 --> 00:34:54,710
and thank you very much

1051
00:34:54,710 --> 00:35:04,320
[Music]

1052
00:35:04,880 --> 00:35:06,960
you

