1
00:00:00,149 --> 00:00:05,190
good afternoon how are you doing had a

2
00:00:05,190 --> 00:00:11,730
great lunch great well my name is Mike

3
00:00:11,730 --> 00:00:14,280
bow mom and I'm going to talk about how

4
00:00:14,280 --> 00:00:17,190
to use Hadoop for solving the Big Data

5
00:00:17,190 --> 00:00:21,869
of security problem we have the thing is

6
00:00:21,869 --> 00:00:25,710
that you hear about a lot of big data

7
00:00:25,710 --> 00:00:28,410
what you can do with big data and and

8
00:00:28,410 --> 00:00:31,740
the thing is what we're having now today

9
00:00:31,740 --> 00:00:34,710
is a big data in insecurity we have a

10
00:00:34,710 --> 00:00:39,030
log files in terabytes network traffic I

11
00:00:39,030 --> 00:00:41,969
mean understand a desktop is 1 terabyte

12
00:00:41,969 --> 00:00:43,440
disk space in if you do in computer

13
00:00:43,440 --> 00:00:46,829
forensics and not to what malware that's

14
00:00:46,829 --> 00:00:51,870
an insane amount of data and that's

15
00:00:51,870 --> 00:00:53,579
where I'm coming from I'm not coming

16
00:00:53,579 --> 00:00:58,079
from a big date I'm doing Hadoop and

17
00:00:58,079 --> 00:01:00,649
let's see what I can do with security

18
00:01:00,649 --> 00:01:06,110
malware analysis is a hobby hobbyist I

19
00:01:06,560 --> 00:01:11,250
was here in 2012 and talked about my

20
00:01:11,250 --> 00:01:15,500
three computers under stairs in my house

21
00:01:15,500 --> 00:01:20,100
doing male reason is as a hobby and it's

22
00:01:20,100 --> 00:01:23,570
been progressing from that where I'm

23
00:01:23,570 --> 00:01:26,580
also spoke about how to do it cheaply

24
00:01:26,580 --> 00:01:29,790
and how to make use of the data the

25
00:01:29,790 --> 00:01:33,479
thing is from 2012 to now my memory

26
00:01:33,479 --> 00:01:38,460
collection has grown really big and my

27
00:01:38,460 --> 00:01:40,350
three computers on the bed under the

28
00:01:40,350 --> 00:01:45,350
stairs is not really cutting it

29
00:01:48,470 --> 00:01:51,060
because I'm moans globally correct all

30
00:01:51,060 --> 00:01:58,920
the more rare and it's a lot this was

31
00:01:58,920 --> 00:02:04,340
one of my sources of malware it's from

32
00:02:04,340 --> 00:02:11,220
beginning of 2012 to Bob now anyone can

33
00:02:11,220 --> 00:02:13,710
guess how much malware that is from that

34
00:02:13,710 --> 00:02:20,400
single source how many things one

35
00:02:20,400 --> 00:02:24,300
terabyte show of hands I think I can see

36
00:02:24,300 --> 00:02:28,580
you come on wake up

37
00:02:28,580 --> 00:02:31,020
no one thing you have a one terabyte of

38
00:02:31,020 --> 00:02:38,090
malware you think it's too much yeah

39
00:02:38,090 --> 00:02:43,110
what 50 is not quite that much I don't

40
00:02:43,110 --> 00:02:45,690
have a son at home see still under

41
00:02:45,690 --> 00:02:50,610
stairs although I want a son I can't

42
00:02:50,610 --> 00:02:58,560
afford it I can't see how's it why point

43
00:02:58,560 --> 00:03:02,820
8 terabytes of this phase 4 from this

44
00:03:02,820 --> 00:03:08,329
single source I have 20 other sources

45
00:03:08,959 --> 00:03:15,540
now if you look at the releases here one

46
00:03:15,540 --> 00:03:20,040
here 18th of July 4 to 5 almost 46

47
00:03:20,040 --> 00:03:20,810
gigabytes

48
00:03:20,810 --> 00:03:24,750
three days later another 125 gigabytes

49
00:03:24,750 --> 00:03:27,810
you can't down the line fast enough let

50
00:03:27,810 --> 00:03:32,580
alone analyzing them is it 100,000

51
00:03:32,580 --> 00:03:37,019
sample each of these packs I mean how do

52
00:03:37,019 --> 00:03:39,090
I care can't keep up

53
00:03:39,090 --> 00:03:41,280
oh I can't I need I need in

54
00:03:41,280 --> 00:03:43,400
reinforcements I need help

55
00:03:43,400 --> 00:03:47,989
morning were in more ways than one so

56
00:03:47,989 --> 00:03:50,700
I'm looking this this is a big data

57
00:03:50,700 --> 00:03:53,430
problem what can I do with big data all

58
00:03:53,430 --> 00:03:56,400
looking a Hadoop that's the open source

59
00:03:56,400 --> 00:04:00,900
solution for all big data problems it's

60
00:04:00,900 --> 00:04:01,280
true

61
00:04:01,280 --> 00:04:03,830
Rico project it is meant to run of

62
00:04:03,830 --> 00:04:06,800
off-the-shelf hardware not you normal

63
00:04:06,800 --> 00:04:11,720
standard PC that has its faults the

64
00:04:11,720 --> 00:04:17,899
hardware dries it burns up and it

65
00:04:17,899 --> 00:04:20,029
doesn't require any special networks

66
00:04:20,029 --> 00:04:22,130
I have a friend he's working in HPC

67
00:04:22,130 --> 00:04:29,110
sector he is talking about his 1920 CPUs

68
00:04:29,110 --> 00:04:32,440
cluster he just installed

69
00:04:32,440 --> 00:04:38,470
it's like 1920 course and gigabyte

70
00:04:38,470 --> 00:04:42,050
terabyte of RAM and infinity band

71
00:04:42,050 --> 00:04:44,270
networking all the things and it's like

72
00:04:44,270 --> 00:04:50,000
okay how much it cost 5.6 million ok

73
00:04:50,000 --> 00:04:52,630
I'll come back about 20 years when I

74
00:04:52,630 --> 00:04:58,550
click saved all my income and I can

75
00:04:58,550 --> 00:05:01,330
start looking at getting some families

76
00:05:01,330 --> 00:05:07,910
refurbished so Hadoop all standard stuff

77
00:05:07,910 --> 00:05:12,260
all hot standard hardware the processing

78
00:05:12,260 --> 00:05:17,740
is done locally and that's important bit

79
00:05:20,020 --> 00:05:23,600
because this is actually a Hadoop

80
00:05:23,600 --> 00:05:27,020
ecosystem do things in the middle in a

81
00:05:27,020 --> 00:05:31,400
circle or there the really important

82
00:05:31,400 --> 00:05:35,210
stuff you have the HT FS the Hadoop

83
00:05:35,210 --> 00:05:38,120
distributed file system it's you if you

84
00:05:38,120 --> 00:05:41,660
think about raid raid fire one cup put a

85
00:05:41,660 --> 00:05:44,360
file in and the hard drives is be spread

86
00:05:44,360 --> 00:05:47,030
over several hard drives and so on well

87
00:05:47,030 --> 00:05:49,220
that stuff is what Hadoop does but

88
00:05:49,220 --> 00:05:53,390
across machines so a standard is that it

89
00:05:53,390 --> 00:05:55,669
makes three copies so that piece of data

90
00:05:55,669 --> 00:06:00,530
is stored of three machines then you

91
00:06:00,530 --> 00:06:02,419
have the MapReduce it's the computing

92
00:06:02,419 --> 00:06:09,310
power is it's actually quite stupid it's

93
00:06:09,310 --> 00:06:14,700
thinking in the easiest way think grep

94
00:06:14,700 --> 00:06:20,010
- word count that's Map Reduce the map

95
00:06:20,010 --> 00:06:22,950
part is the grep the reduced part is the

96
00:06:22,950 --> 00:06:26,450
word counts WC command

97
00:06:26,450 --> 00:06:29,220
of course you can do a lot of things

98
00:06:29,220 --> 00:06:32,040
with either part of them but that's it's

99
00:06:32,040 --> 00:06:35,460
in a nutshell then you have all these

100
00:06:35,460 --> 00:06:39,980
components around them for me I'm using

101
00:06:39,980 --> 00:06:46,950
pig most of the time HBase sometimes but

102
00:06:46,950 --> 00:06:50,970
mostly I'm not actually thinking about

103
00:06:50,970 --> 00:06:53,400
it I'm losing PK using HBase everything

104
00:06:53,400 --> 00:06:57,690
else is magic happening it's a security

105
00:06:57,690 --> 00:07:01,740
a talk it's not a big data talking how

106
00:07:01,740 --> 00:07:05,550
do you get a Hadoop cluster well my

107
00:07:05,550 --> 00:07:06,990
recommendation if you're running this at

108
00:07:06,990 --> 00:07:09,180
home get one of those prepackaged

109
00:07:09,180 --> 00:07:13,670
distributions a cloud era has one

110
00:07:13,670 --> 00:07:16,350
Wharton works as one there's several

111
00:07:16,350 --> 00:07:22,710
others you can also rent them using the

112
00:07:22,710 --> 00:07:27,090
EMR from Amazon that's really cool

113
00:07:27,090 --> 00:07:31,650
really cost-effective way to do it or if

114
00:07:31,650 --> 00:07:33,780
you really really want you can compile

115
00:07:33,780 --> 00:07:37,470
it yourself not recommended unless you

116
00:07:37,470 --> 00:07:39,990
want to actually improve the Hadoop

117
00:07:39,990 --> 00:07:43,200
ecosystem itself because if you just

118
00:07:43,200 --> 00:07:45,420
want to use it go for a prepackaged or

119
00:07:45,420 --> 00:07:47,870
rent it

120
00:07:51,050 --> 00:07:55,770
so let's talk about how to I do

121
00:07:55,770 --> 00:08:00,569
malware analysis Badou well first are

122
00:08:00,569 --> 00:08:03,779
using a piece of software not developed

123
00:08:03,779 --> 00:08:08,009
by me this pioneer peak it have solves a

124
00:08:08,009 --> 00:08:10,620
lot of these problems first of all

125
00:08:10,620 --> 00:08:13,979
marrow samples are in May up in size of

126
00:08:13,979 --> 00:08:20,879
megabytes Hadoop likes them larger so

127
00:08:20,879 --> 00:08:23,219
what what it does is it's crazy a

128
00:08:23,219 --> 00:08:27,110
sequence file this is a key value pair

129
00:08:27,110 --> 00:08:30,360
where the key is the identifier they

130
00:08:30,360 --> 00:08:33,839
like the md5 checksum and the value is

131
00:08:33,839 --> 00:08:40,380
the contents of the Malheur because then

132
00:08:40,380 --> 00:08:43,260
you have a lot of malware as one single

133
00:08:43,260 --> 00:08:50,010
file then this files are batch process

134
00:08:50,010 --> 00:08:53,670
on each and every node in the Hadoop

135
00:08:53,670 --> 00:08:56,610
cluster they are processed locally which

136
00:08:56,610 --> 00:08:59,790
means that they noticed us the

137
00:08:59,790 --> 00:09:02,160
processing has the data on its own hard

138
00:09:02,160 --> 00:09:06,029
drive so I don't have an additional

139
00:09:06,029 --> 00:09:14,310
network i/o to consider and the output

140
00:09:14,310 --> 00:09:18,860
store in elasticsearch for accessing for

141
00:09:18,860 --> 00:09:22,279
more analysis for drilling down into

142
00:09:22,279 --> 00:09:25,699
inter results

143
00:09:28,490 --> 00:09:30,880
so what do I do with it we're doing

144
00:09:30,880 --> 00:09:33,500
extracting resource information which

145
00:09:33,500 --> 00:09:36,260
means everything that the P is headers

146
00:09:36,260 --> 00:09:39,560
telling me I'm collecting I'm also

147
00:09:39,560 --> 00:09:41,960
collecting all the the resources all the

148
00:09:41,960 --> 00:09:47,540
icons of the binary something is

149
00:09:47,540 --> 00:09:50,000
interesting to know that malware

150
00:09:50,000 --> 00:09:52,940
developers are as lazy as normal

151
00:09:52,940 --> 00:09:56,090
developers wants to find a good icon set

152
00:09:56,090 --> 00:09:58,450
that will keep it will keep using it

153
00:09:58,450 --> 00:10:01,370
same thing is if once they have found

154
00:10:01,370 --> 00:10:04,370
I've got open SSL or some other

155
00:10:04,370 --> 00:10:06,200
third-party library working on the

156
00:10:06,200 --> 00:10:08,120
machine they will keep using that

157
00:10:08,120 --> 00:10:10,850
version with those specific changes they

158
00:10:10,850 --> 00:10:14,000
made to it and those things are things

159
00:10:14,000 --> 00:10:20,450
we can look for in an example and you're

160
00:10:20,450 --> 00:10:23,690
also looking at how to improve detection

161
00:10:23,690 --> 00:10:28,990
rate so I'm using Yarra for that and

162
00:10:28,990 --> 00:10:31,600
everything every time you're doing a

163
00:10:31,600 --> 00:10:34,280
signature update you want to see how

164
00:10:34,280 --> 00:10:37,550
well it responds getting a false

165
00:10:37,550 --> 00:10:40,490
negatives and the false positives you

166
00:10:40,490 --> 00:10:44,080
actually find what you're looking for so

167
00:10:44,080 --> 00:10:47,900
that's really good to run it and rerun

168
00:10:47,900 --> 00:10:55,730
the sample sample set so it is like the

169
00:10:55,730 --> 00:11:00,260
workflow of Pioneer peak you have a sip

170
00:11:00,260 --> 00:11:03,770
file or a local directory with a lot of

171
00:11:03,770 --> 00:11:08,140
malware samples memory sample is a

172
00:11:08,140 --> 00:11:14,570
malicious file don't know those that

173
00:11:14,570 --> 00:11:18,110
fire is being sequenced as stored in the

174
00:11:18,110 --> 00:11:23,560
Hadoop cluster in the HDFS

175
00:11:26,490 --> 00:11:30,040
once that part is done I run a bunch of

176
00:11:30,040 --> 00:11:35,170
peak jobs for creating hashes the clam a

177
00:11:35,170 --> 00:11:38,380
be scanning to do the Jarrah signature

178
00:11:38,380 --> 00:11:40,540
detection and extract all the strings

179
00:11:40,540 --> 00:11:43,470
the resources whatever I want and if I

180
00:11:43,470 --> 00:11:46,810
figure out tomorrow that piece

181
00:11:46,810 --> 00:11:49,390
information I would like to have bro

182
00:11:49,390 --> 00:11:52,570
right as a new Pig script that extracts

183
00:11:52,570 --> 00:11:56,019
that information for me and rerun the

184
00:11:56,019 --> 00:12:06,670
job with affinity amount of of nodes it

185
00:12:06,670 --> 00:12:10,029
can be done very very quickly but I'm

186
00:12:10,029 --> 00:12:15,399
getting back to that later next thing

187
00:12:15,399 --> 00:12:19,209
you can do with Hadoop is doing network

188
00:12:19,209 --> 00:12:26,380
analysis that one is similar to the

189
00:12:26,380 --> 00:12:29,050
malware analysis person you store the

190
00:12:29,050 --> 00:12:34,600
pcaps in HDFS and you run a bunch of Pig

191
00:12:34,600 --> 00:12:37,390
scripts to extract the things you're

192
00:12:37,390 --> 00:12:39,750
looking for

193
00:12:40,199 --> 00:12:43,660
so if is you want to run snort

194
00:12:43,660 --> 00:12:46,720
signatures against the whole lot you can

195
00:12:46,720 --> 00:12:47,699
do that

196
00:12:47,699 --> 00:12:50,470
you can do things like okay give me all

197
00:12:50,470 --> 00:12:52,390
traffic that didn't have a source in

198
00:12:52,390 --> 00:12:58,209
English signature you can replay the

199
00:12:58,209 --> 00:13:02,170
traffic you can look at artifacts back

200
00:13:02,170 --> 00:13:04,649
and forth

201
00:13:09,089 --> 00:13:13,240
the workflow looks like this pcap stored

202
00:13:13,240 --> 00:13:16,720
locally uploaded to HDFS across the

203
00:13:16,720 --> 00:13:26,560
nodes and that circle motion is

204
00:13:26,560 --> 00:13:29,019
representing all the peak jobs the runs

205
00:13:29,019 --> 00:13:34,149
on the each individual node is a POF

206
00:13:34,149 --> 00:13:39,100
extra signature extraction the heard

207
00:13:39,100 --> 00:13:43,390
about the POF - it's actually looking at

208
00:13:43,390 --> 00:13:47,079
all the terraces of connection to see

209
00:13:47,079 --> 00:13:49,690
that the TTL was - for this one it means

210
00:13:49,690 --> 00:13:51,820
that it's probably a Windows machine or

211
00:13:51,820 --> 00:13:54,490
Linux machine or so on which was

212
00:13:54,490 --> 00:13:59,829
generating the traffic you can extract

213
00:13:59,829 --> 00:14:02,769
the user agents whatever you want

214
00:14:02,769 --> 00:14:04,930
I mean think about the query you're

215
00:14:04,930 --> 00:14:08,699
looking for you can have it

216
00:14:21,160 --> 00:14:26,080
then you have the computer forensics and

217
00:14:26,080 --> 00:14:29,860
that one is a more complicated it's like

218
00:14:29,860 --> 00:14:32,230
you're having tons of storage you have a

219
00:14:32,230 --> 00:14:35,950
fast and a normal computer it takes time

220
00:14:35,950 --> 00:14:42,190
to do a keyword search this is a

221
00:14:42,190 --> 00:14:46,150
multiple part process first to extract

222
00:14:46,150 --> 00:14:49,750
or take the raw image extract metadata

223
00:14:49,750 --> 00:14:53,470
about the raw image all the files both

224
00:14:53,470 --> 00:14:54,910
about the image itself

225
00:14:54,910 --> 00:14:57,250
and the files in the image you upload

226
00:14:57,250 --> 00:15:05,370
that and the raw image to HDFS

227
00:15:11,570 --> 00:15:16,690
then you're processing that data you got

228
00:15:16,690 --> 00:15:21,110
into HP a HBase is a new SQL database

229
00:15:21,110 --> 00:15:25,070
and you do an iteration you find the

230
00:15:25,070 --> 00:15:26,450
first all the files and you find the

231
00:15:26,450 --> 00:15:28,630
contents of the files you catalog

232
00:15:28,630 --> 00:15:31,370
categorizing the files your keyword

233
00:15:31,370 --> 00:15:36,860
indexes files and so on which means it's

234
00:15:36,860 --> 00:15:38,510
doesn't do anything different from the

235
00:15:38,510 --> 00:15:43,490
normal sloths kit software is just doing

236
00:15:43,490 --> 00:15:46,940
it faster as you have more processing

237
00:15:46,940 --> 00:15:52,610
power to use with it and the nice thing

238
00:15:52,610 --> 00:15:54,590
is that you just need to have enough

239
00:15:54,590 --> 00:15:59,060
storage around the whole cluster not on

240
00:15:59,060 --> 00:16:01,510
each machine

241
00:16:07,260 --> 00:16:12,250
so extracting keywords you talk nicely

242
00:16:12,250 --> 00:16:16,500
documents you clustered similar objects

243
00:16:16,500 --> 00:16:19,960
and you can even compare it another

244
00:16:19,960 --> 00:16:24,010
image so if you have a plain windows

245
00:16:24,010 --> 00:16:28,830
image you can remove all those files

246
00:16:28,830 --> 00:16:34,680
from the investigation

247
00:16:43,100 --> 00:16:45,260
and then you have the extraction point

248
00:16:45,260 --> 00:16:47,270
where you are actually building the

249
00:16:47,270 --> 00:16:59,150
report of all the data got out that's

250
00:16:59,150 --> 00:17:01,569
really cool

251
00:17:08,640 --> 00:17:10,709
and we got back to the log and I see

252
00:17:10,709 --> 00:17:14,880
this is him the basic example if you if

253
00:17:14,880 --> 00:17:17,609
you look at how to write a MapReduce job

254
00:17:17,609 --> 00:17:20,519
they always have the example of doing

255
00:17:20,519 --> 00:17:27,750
log analysis usually Apache logs doing a

256
00:17:27,750 --> 00:17:33,179
pure MapReduce job for this is okay it's

257
00:17:33,179 --> 00:17:36,240
a bit more work than writing a big

258
00:17:36,240 --> 00:17:39,110
script for it which is a overlay of

259
00:17:39,110 --> 00:17:44,100
MapReduce jobs it's like writing C

260
00:17:44,100 --> 00:17:51,870
instead of assembler so the workflow

261
00:17:51,870 --> 00:17:58,320
with doing large-scale log analysis is

262
00:17:58,320 --> 00:18:00,659
that you're using flume which is another

263
00:18:00,659 --> 00:18:06,260
of those data acquisition components

264
00:18:06,830 --> 00:18:12,260
they push the local logs to HDFS and

265
00:18:12,260 --> 00:18:18,090
normalize them then you have pic scripts

266
00:18:18,090 --> 00:18:22,370
that is all this is batch processing so

267
00:18:22,370 --> 00:18:26,309
you have a batch that using peak scripts

268
00:18:26,309 --> 00:18:30,500
to either directly look for bad stuff or

269
00:18:30,500 --> 00:18:36,720
populate the HBase database because once

270
00:18:36,720 --> 00:18:41,130
you have a data in HBase you can start

271
00:18:41,130 --> 00:18:45,169
querying that one for more information

272
00:18:45,169 --> 00:18:48,480
it's like you you grab your pipe it to a

273
00:18:48,480 --> 00:18:50,250
file and then they start looking at that

274
00:18:50,250 --> 00:18:54,649
file for more information

275
00:18:57,500 --> 00:19:05,539
and this log pig has restful service for

276
00:19:05,539 --> 00:19:10,580
data extraction and the Seohyun actually

277
00:19:10,580 --> 00:19:13,850
start using the data in your normal you

278
00:19:13,850 --> 00:19:23,030
ice or sea ice so here's comes the

279
00:19:23,030 --> 00:19:26,000
animation again it's quite standard

280
00:19:26,000 --> 00:19:33,140
so flume push the logs to HDFS HDFS is

281
00:19:33,140 --> 00:19:37,490
spreading it across the computers you

282
00:19:37,490 --> 00:19:40,480
have a pig scripts extracts the data and

283
00:19:40,480 --> 00:19:44,870
put them back is either back to HDFS or

284
00:19:44,870 --> 00:19:51,289
in HBase and then you can create

285
00:19:51,289 --> 00:19:54,860
additional pig scripts to extract more

286
00:19:54,860 --> 00:19:57,459
information

287
00:20:06,010 --> 00:20:10,660
so back to my my little hobby malware

288
00:20:10,660 --> 00:20:14,049
analysis as I said I have my three

289
00:20:14,049 --> 00:20:17,590
computers under the stairs like Harry

290
00:20:17,590 --> 00:20:25,630
Potter and I'm storing my samples

291
00:20:25,630 --> 00:20:31,870
locally what I need to do is my three

292
00:20:31,870 --> 00:20:36,760
nodes at home they don't cut it it's not

293
00:20:36,760 --> 00:20:39,130
enough computing power I can't keep up

294
00:20:39,130 --> 00:20:42,190
so what I'm doing some loading the data

295
00:20:42,190 --> 00:20:45,669
interesting to analyze up to s3 the

296
00:20:45,669 --> 00:20:48,010
Amazon Web service simple storage

297
00:20:48,010 --> 00:20:54,660
service then I using elastic MapReduce

298
00:20:54,660 --> 00:21:00,520
Amazon to run binary Pig and extract all

299
00:21:00,520 --> 00:21:05,200
the data and then uphold the results

300
00:21:05,200 --> 00:21:08,890
back locally the reason we're doing is

301
00:21:08,890 --> 00:21:13,600
is although s3 is cheap to store data

302
00:21:13,600 --> 00:21:15,760
when you're talking about terabytes of

303
00:21:15,760 --> 00:21:21,250
storage it's no longer cheap we're

304
00:21:21,250 --> 00:21:24,510
talking about hundreds of dollars per

305
00:21:24,510 --> 00:21:27,510
month

306
00:21:34,440 --> 00:21:39,470
so this is that my hobby so I need to be

307
00:21:39,470 --> 00:21:41,910
cost-effective and time effective and

308
00:21:41,910 --> 00:21:47,070
everything so data is being backed up on

309
00:21:47,070 --> 00:21:50,760
Amazon glacier and one heard about that

310
00:21:50,760 --> 00:21:58,260
one a few hands it's their longtime

311
00:21:58,260 --> 00:22:03,420
storage solution your uploaded is stored

312
00:22:03,420 --> 00:22:07,740
on very cheap storage so you if you want

313
00:22:07,740 --> 00:22:10,530
it back you don't can't be in a hurry it

314
00:22:10,530 --> 00:22:13,560
will take time but you don't lose it at

315
00:22:13,560 --> 00:22:24,000
least also doing using the less reduced

316
00:22:24,000 --> 00:22:27,960
redundancy storage of s3 so I only have

317
00:22:27,960 --> 00:22:34,100
dot nine nine and not dot nine nines

318
00:22:34,850 --> 00:22:37,380
because I figure it's going to be up

319
00:22:37,380 --> 00:22:39,120
there I have a local copy I wouldn't be

320
00:22:39,120 --> 00:22:44,310
up there for an hour two ships of the

321
00:22:44,310 --> 00:22:53,270
cost the third thing I do is I bid on

322
00:22:53,270 --> 00:22:57,930
Amazon ec2 spots which means that Amazon

323
00:22:57,930 --> 00:23:00,980
is having this market where you can buy

324
00:23:00,980 --> 00:23:06,330
unused computing power and marquette

325
00:23:06,330 --> 00:23:09,600
highest price winning wins and you can

326
00:23:09,600 --> 00:23:11,610
get the computing power very cheaply

327
00:23:11,610 --> 00:23:13,830
although if someone beats higher than

328
00:23:13,830 --> 00:23:16,470
you I just turn off your machine but

329
00:23:16,470 --> 00:23:18,570
that's not a problem because Hadoop is

330
00:23:18,570 --> 00:23:24,720
built to actually be aware of that it's

331
00:23:24,720 --> 00:23:26,850
actually better if they do turn it off

332
00:23:26,850 --> 00:23:29,430
before the full hour because if they

333
00:23:29,430 --> 00:23:32,280
terminate your machine before the full

334
00:23:32,280 --> 00:23:35,010
hour of machine time is used they

335
00:23:35,010 --> 00:23:37,700
believe nothing

336
00:23:39,300 --> 00:23:43,810
and I don't be doing incremental saves

337
00:23:43,810 --> 00:23:47,230
it's like an if I bid low enough I get

338
00:23:47,230 --> 00:23:50,980
it done fast enough and very cheaply but

339
00:23:50,980 --> 00:23:54,360
if if time is more important and money

340
00:23:54,360 --> 00:24:01,410
easy you can buy your on demand

341
00:24:01,410 --> 00:24:03,730
instances which means you pay retail

342
00:24:03,730 --> 00:24:07,390
price or just bid very high price and

343
00:24:07,390 --> 00:24:10,000
this the thing this is bid price is is

344
00:24:10,000 --> 00:24:13,120
the maximum you will need to pay not

345
00:24:13,120 --> 00:24:21,250
what you are paying so if you can if you

346
00:24:21,250 --> 00:24:27,010
bid like five cents an hour you can end

347
00:24:27,010 --> 00:24:31,900
the spot price is 1.1 cents an hour you

348
00:24:31,900 --> 00:24:35,280
just pay in 1.1 cents

349
00:24:39,650 --> 00:24:44,570
and this is getting down about my

350
00:24:44,570 --> 00:24:48,440
estimate about $20 a month I'm having

351
00:24:48,440 --> 00:24:51,950
haven't got my first bill yet because

352
00:24:51,950 --> 00:24:54,890
this is a work in progress during the

353
00:24:54,890 --> 00:24:58,520
summer my the storage node on my home

354
00:24:58,520 --> 00:25:00,920
computer it crashed

355
00:25:00,920 --> 00:25:03,200
I lost half my hard drives and all my

356
00:25:03,200 --> 00:25:05,000
malware some in the process of

357
00:25:05,000 --> 00:25:10,460
rebuilding my my archive so this is how

358
00:25:10,460 --> 00:25:13,600
I'm doing it going forward

359
00:25:19,839 --> 00:25:25,719
so conclusions Melbourne analysis it's

360
00:25:25,719 --> 00:25:29,509
really appropriate for this kind of

361
00:25:29,509 --> 00:25:31,519
setup you can do it very quickly very

362
00:25:31,519 --> 00:25:37,789
cheaply and if you figure out a new way

363
00:25:37,789 --> 00:25:40,309
to extract data or find a new piece of

364
00:25:40,309 --> 00:25:43,159
data you want you can rerun you whole

365
00:25:43,159 --> 00:25:48,710
sample collection in hours or if you

366
00:25:48,710 --> 00:25:51,139
credibly missus is big enough you can

367
00:25:51,139 --> 00:25:54,169
run it in one hour and a lot of machines

368
00:25:54,169 --> 00:26:01,960
at Amazon I mean you could run the whole

369
00:26:01,960 --> 00:26:07,729
archive at 5.6 5.8 terabyte of malware

370
00:26:07,729 --> 00:26:12,320
in just under an hour but you mean

371
00:26:12,320 --> 00:26:19,429
you're paying for it it's cheap you can

372
00:26:19,429 --> 00:26:23,419
do it from network analysis and if you

373
00:26:23,419 --> 00:26:26,719
if you're recording all the traffic dump

374
00:26:26,719 --> 00:26:32,379
it to HDFS you can start re running

375
00:26:32,379 --> 00:26:37,519
samples you can develop your own

376
00:26:37,519 --> 00:26:41,899
signatures it's like you have you find

377
00:26:41,899 --> 00:26:45,830
an exploit or malicious network pattern

378
00:26:45,830 --> 00:26:48,519
and you're right to use North signature

379
00:26:48,519 --> 00:26:51,049
you can run that North signature again

380
00:26:51,049 --> 00:26:52,969
at all the traffic we already collected

381
00:26:52,969 --> 00:26:54,049
and see if you got any false positives

382
00:26:54,049 --> 00:26:57,019
or all if there are other machines that

383
00:26:57,019 --> 00:26:59,919
has been attacked

384
00:27:03,309 --> 00:27:06,600
same thing with a computer forensics and

385
00:27:06,600 --> 00:27:09,100
here I think Hadoop is a really good

386
00:27:09,100 --> 00:27:11,710
tool because you don't need ten

387
00:27:11,710 --> 00:27:13,809
terabytes of disk space in each number

388
00:27:13,809 --> 00:27:14,260
node

389
00:27:14,260 --> 00:27:18,640
I mean you can have 500 gigabytes disk

390
00:27:18,640 --> 00:27:22,620
space provided you have enough nodes

391
00:27:24,510 --> 00:27:29,590
because you're the capacity of HDFS is

392
00:27:29,590 --> 00:27:32,460
the disk space across all the nodes

393
00:27:32,460 --> 00:27:36,190
divided by three as you have it you

394
00:27:36,190 --> 00:27:38,140
storing as the same data three times

395
00:27:38,140 --> 00:27:41,039
over a cluster

396
00:27:45,550 --> 00:27:48,210
and for a log analysis this is the

397
00:27:48,210 --> 00:27:52,870
newest part of my ecosystem and I'm

398
00:27:52,870 --> 00:27:57,000
already seeing benefits of it especially

399
00:27:57,000 --> 00:28:00,520
when you're talking in taking in logs

400
00:28:00,520 --> 00:28:04,300
from the dynamic analysis of malware

401
00:28:04,300 --> 00:28:11,140
binaries the old system calls next step

402
00:28:11,140 --> 00:28:15,670
is to actually create computer learning

403
00:28:15,670 --> 00:28:20,950
algorithms to to help me detect bad

404
00:28:20,950 --> 00:28:23,310
stuff

