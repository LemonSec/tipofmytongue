1
00:00:06,370 --> 00:00:10,390
who the devil there

2
00:00:51,320 --> 00:00:55,130
yeah would you want to

3
00:01:00,300 --> 00:01:03,300
yeah

4
00:01:14,900 --> 00:01:17,050
yeah

5
00:01:20,190 --> 00:01:23,190
right

6
00:01:31,799 --> 00:01:35,149
I'd like you today

7
00:02:26,500 --> 00:02:32,000
hi guys oh my beep dawned on me I will

8
00:02:30,110 --> 00:02:33,680
explain a little bit about this I'm

9
00:02:32,000 --> 00:02:35,900
taking some of you might have seen me

10
00:02:33,680 --> 00:02:39,680
yesterday talk about culture insecurity

11
00:02:35,900 --> 00:02:53,090
but in my real day job I'm actually a

12
00:02:39,680 --> 00:02:55,130
cyber security intelligence so when John

13
00:02:53,090 --> 00:02:59,090
and I talked one thing I've been working

14
00:02:55,130 --> 00:03:01,040
on is thinking about how as a culture

15
00:02:59,090 --> 00:03:03,320
are we thinking about artificial

16
00:03:01,040 --> 00:03:07,519
intelligence and how as security

17
00:03:03,320 --> 00:03:08,870
professionals can we actually do more to

18
00:03:07,520 --> 00:03:10,370
improve the humankind

19
00:03:08,870 --> 00:03:12,410
so basically a little bit when I started

20
00:03:10,370 --> 00:03:16,000
about because it's an interesting long

21
00:03:12,410 --> 00:03:18,040
road I have been working or

22
00:03:16,000 --> 00:03:21,620
participating in artificial intelligence

23
00:03:18,040 --> 00:03:23,570
from a technology perspective since I

24
00:03:21,620 --> 00:03:24,800
was 10 years old and there's two little

25
00:03:23,570 --> 00:03:28,280
girls in the bathroom you know from

26
00:03:24,800 --> 00:03:30,920
common talent QA and son there those are

27
00:03:28,280 --> 00:03:32,840
my levels so to think about it I have

28
00:03:30,920 --> 00:03:36,019
been in artificial intelligence since

29
00:03:32,840 --> 00:03:38,390
their age now why well when I was 10

30
00:03:36,020 --> 00:03:42,520
years old my father who was driving down

31
00:03:38,390 --> 00:03:42,519
the highway in a patch I said

32
00:03:43,980 --> 00:03:51,399
that was 1986 remember that date because

33
00:03:49,450 --> 00:03:53,859
1986 is you'll see we can talk about

34
00:03:51,400 --> 00:03:56,830
this day was actually one of the better

35
00:03:53,860 --> 00:03:59,320
years for efficient elevator so in my

36
00:03:56,830 --> 00:04:02,440
life early member I became fascinated

37
00:03:59,320 --> 00:04:04,989
with how computers and technology could

38
00:04:02,440 --> 00:04:07,290
improve products my father's first

39
00:04:04,990 --> 00:04:10,650
wheelchair was a sip and puff for you

40
00:04:07,290 --> 00:04:12,910
blue error to move forward and backward

41
00:04:10,650 --> 00:04:14,830
the doors in our house

42
00:04:12,910 --> 00:04:16,930
I came automated so that he would be

43
00:04:14,830 --> 00:04:18,220
able to get through them and by the time

44
00:04:16,930 --> 00:04:21,310
I was 15

45
00:04:18,220 --> 00:04:23,290
our house was voice command so that he

46
00:04:21,310 --> 00:04:25,660
would be able to turn the thermostat up

47
00:04:23,290 --> 00:04:27,790
and down things we take credit every day

48
00:04:25,660 --> 00:04:30,820
right now but that was rudimentary

49
00:04:27,790 --> 00:04:33,190
about it so and I need him to see so I

50
00:04:30,820 --> 00:04:35,530
was fascinated throughout my career with

51
00:04:33,190 --> 00:04:38,469
emerging technology and one of the huge

52
00:04:35,530 --> 00:04:40,900
frustrations I had was that when dealing

53
00:04:38,470 --> 00:04:42,820
with mission-critical crisis's like I

54
00:04:40,900 --> 00:04:47,340
don't know DDoS attacks the night of

55
00:04:42,820 --> 00:04:50,830
brexit or the Olympic Games or having

56
00:04:47,340 --> 00:04:52,270
human compromises in Brazil my eyes

57
00:04:50,830 --> 00:04:55,180
couldn't sort through the team at

58
00:04:52,270 --> 00:04:57,909
Bastogne they could get to the threat

59
00:04:55,180 --> 00:05:00,220
plenty fast enough they couldn't get to

60
00:04:57,910 --> 00:05:03,880
the information immunity passed away and

61
00:05:00,220 --> 00:05:06,880
so two years ago I was in on a mission

62
00:05:03,880 --> 00:05:09,130
for the UK government in all places San

63
00:05:06,880 --> 00:05:11,170
Diego and I met a really weird guy with

64
00:05:09,130 --> 00:05:12,050
a new day rockin sin' and he asked me

65
00:05:11,170 --> 00:05:14,210
for some advice

66
00:05:12,050 --> 00:05:16,240
what he called my good business tackle

67
00:05:14,210 --> 00:05:19,758
and could I have breakfast with her

68
00:05:16,240 --> 00:05:21,680
worst hotel I've ever been in in this

69
00:05:19,759 --> 00:05:23,389
falling down restaurant if you can

70
00:05:21,680 --> 00:05:25,639
imagine with like back palm trees in

71
00:05:23,389 --> 00:05:27,379
like Hawaiian steps laying around and he

72
00:05:25,639 --> 00:05:30,169
showed me this thing called sensing

73
00:05:27,379 --> 00:05:32,180
and what sensing is a platform that cut

74
00:05:30,169 --> 00:05:33,919
that utilizes unsupervised and

75
00:05:32,180 --> 00:05:36,289
supervised machine learning suppose

76
00:05:33,919 --> 00:05:40,520
redline with machine learning and we

77
00:05:36,289 --> 00:05:43,699
utilize but we are up older and we

78
00:05:40,520 --> 00:05:46,490
utilize proprietary patented artificial

79
00:05:43,699 --> 00:05:49,849
intelligence to mimic as close as

80
00:05:46,490 --> 00:05:51,949
possible what a threat hunter we do we

81
00:05:49,849 --> 00:05:53,539
cause without groomer interaction we

82
00:05:51,949 --> 00:05:56,419
were what we should be thinking about

83
00:05:53,539 --> 00:05:58,159
environment the love takes ideas so much

84
00:05:56,419 --> 00:05:58,758
that I became an advisor for him for

85
00:05:58,159 --> 00:06:02,030
about eight months

86
00:05:58,759 --> 00:06:03,710
and then to the really long walk on a

87
00:06:02,030 --> 00:06:05,809
short pier and incorporated the company

88
00:06:03,710 --> 00:06:08,750
in there as a year ago AI is a

89
00:06:05,810 --> 00:06:12,080
passionate life so today one of the

90
00:06:08,750 --> 00:06:15,560
things I was thinking about was could we

91
00:06:12,080 --> 00:06:17,840
actually utilize what she needed to go

92
00:06:15,560 --> 00:06:19,159
talk about she is well I'm really good

93
00:06:17,840 --> 00:06:22,659
at talking about how a machine learning

94
00:06:19,159 --> 00:06:22,659
there's a lot of people smarter than me

95
00:06:28,659 --> 00:06:40,339
with DJ Webster well now I just need to

96
00:06:38,029 --> 00:06:44,379
figure out how I'm supposed to do this

97
00:06:40,339 --> 00:06:44,379
all right you're spotting me figure out

98
00:06:46,089 --> 00:06:52,029
says don't we display or projector gives

99
00:06:49,699 --> 00:06:52,029
me this

100
00:07:00,950 --> 00:07:09,930
here

101
00:07:03,320 --> 00:07:11,400
hey all right so I've been wondering

102
00:07:09,930 --> 00:07:13,890
whether or not I could get to talk

103
00:07:11,400 --> 00:07:16,590
people as if she learning on so who is

104
00:07:13,890 --> 00:07:19,440
familiar with LinkedIn SlideShare anyone

105
00:07:16,590 --> 00:07:23,010
in this room one person - okay

106
00:07:19,440 --> 00:07:25,230
so as a really crazy busy security

107
00:07:23,010 --> 00:07:27,210
person at omnia for when I want to learn

108
00:07:25,230 --> 00:07:27,750
about new topics quickly this is my

109
00:07:27,210 --> 00:07:29,760
Bible

110
00:07:27,750 --> 00:07:31,470
why because anyone who's doing the

111
00:07:29,760 --> 00:07:33,030
presentation can tell them anything they

112
00:07:31,470 --> 00:07:34,800
want on LinkedIn and I can sort through

113
00:07:33,030 --> 00:07:37,770
anything and find lots of great

114
00:07:34,800 --> 00:07:40,080
information in nearly almost no time

115
00:07:37,770 --> 00:07:45,570
but machine learning aspect of it if I

116
00:07:40,080 --> 00:07:46,919
type in what is AI I get 1 million 137

117
00:07:45,570 --> 00:07:48,540
different presentations to prove to

118
00:07:46,920 --> 00:07:51,300
choose from and I can pick and choose

119
00:07:48,540 --> 00:07:53,160
slides so I spent about a week playing

120
00:07:51,300 --> 00:07:55,080
with machine learning - different -

121
00:07:53,160 --> 00:07:57,060
different queries look at different

122
00:07:55,080 --> 00:07:59,460
things in different authors I like and

123
00:07:57,060 --> 00:08:01,380
start to build a profile of what type of

124
00:07:59,460 --> 00:08:04,560
information I want to talk about and

125
00:08:01,380 --> 00:08:06,750
then I started clipping slides so I was

126
00:08:04,560 --> 00:08:08,760
able to build the entire talk on machine

127
00:08:06,750 --> 00:08:12,600
learning and AI from other people's work

128
00:08:08,760 --> 00:08:14,370
based on my actual preferences that were

129
00:08:12,600 --> 00:08:16,500
then learned through the machine that

130
00:08:14,370 --> 00:08:20,070
runs Lincoln's life

131
00:08:16,500 --> 00:08:21,360
I don't fund them so this is their only

132
00:08:20,070 --> 00:08:22,590
problem with it is

133
00:08:21,360 --> 00:08:24,509
they have to ask you if everybody's

134
00:08:22,590 --> 00:08:25,888
taught you have to scroll through and

135
00:08:24,509 --> 00:08:27,419
Senate regular slides so bear with me

136
00:08:25,889 --> 00:08:28,500
it's the first time they've done it but

137
00:08:27,419 --> 00:08:29,669
I thought it was really cool is

138
00:08:28,500 --> 00:08:31,740
introduced invented an entire

139
00:08:29,669 --> 00:08:33,718
presentation using machinery on

140
00:08:31,740 --> 00:08:36,810
artificial intelligence I thought that

141
00:08:33,719 --> 00:08:38,490
was really cool so our original

142
00:08:36,809 --> 00:08:40,078
intelligence in machine learning

143
00:08:38,490 --> 00:08:41,849
who would agree with me raising hands

144
00:08:40,078 --> 00:08:43,859
are probably the biggest buzz ones other

145
00:08:41,849 --> 00:08:47,399
than digital transformation industry

146
00:08:43,860 --> 00:08:48,930
today yeah and you know I really love is

147
00:08:47,399 --> 00:08:50,790
when I'm giving a presentation and I say

148
00:08:48,930 --> 00:08:52,199
my company is based on unsupervised and

149
00:08:50,790 --> 00:08:55,110
supervised machine learning by the way

150
00:08:52,200 --> 00:08:57,269
we've had today I go I rule asleep my

151
00:08:55,110 --> 00:08:59,670
politics are number one of you it's

152
00:08:57,269 --> 00:09:03,390
amazing but let's think about what is

153
00:08:59,670 --> 00:09:08,219
artificial intelligence well for a lot

154
00:09:03,390 --> 00:09:10,110
of people are that's my there we go

155
00:09:08,220 --> 00:09:12,300
artificial intelligence you think about

156
00:09:10,110 --> 00:09:14,010
it is this right the machines are going

157
00:09:12,300 --> 00:09:16,709
to take us over you think about

158
00:09:14,010 --> 00:09:19,439
terminator we think about a movie you

159
00:09:16,709 --> 00:09:21,959
think about sci-fi books the reality is

160
00:09:19,440 --> 00:09:22,800
is that artificial intelligence is so

161
00:09:21,959 --> 00:09:26,310
much more than that

162
00:09:22,800 --> 00:09:28,380
and it's driven on a human desire to to

163
00:09:26,310 --> 00:09:30,329
more and the impact on it is something

164
00:09:28,380 --> 00:09:31,769
that we're highways baiting so you can't

165
00:09:30,329 --> 00:09:34,079
really read these names but this is

166
00:09:31,769 --> 00:09:35,850
about really interesting the biggest

167
00:09:34,079 --> 00:09:38,339
naysayers right now of artificial

168
00:09:35,850 --> 00:09:40,350
intelligence were just passed away

169
00:09:38,339 --> 00:09:41,550
Stephen Hawking's said it's going to

170
00:09:40,350 --> 00:09:44,850
destroy humanity

171
00:09:41,550 --> 00:09:48,050
Bill Gates very concerned about it who's

172
00:09:44,850 --> 00:09:50,250
on the fence this came with a surprising

173
00:09:48,050 --> 00:09:51,779
still really concerned a little bit

174
00:09:50,250 --> 00:09:53,910
thinks there needs to be a lot of ethics

175
00:09:51,779 --> 00:09:54,790
around it who is totally for it

176
00:09:53,910 --> 00:09:57,430
this isn't

177
00:09:54,790 --> 00:09:59,260
rising Mark Zuckerberg does that

178
00:09:57,430 --> 00:10:02,890
shocking absolutely not

179
00:09:59,260 --> 00:10:05,439
but as a culture we are in a huge

180
00:10:02,890 --> 00:10:08,170
philosophical debate about artificial

181
00:10:05,440 --> 00:10:10,480
intelligence and the reason is is that

182
00:10:08,170 --> 00:10:15,729
as a culture we've lived by one

183
00:10:10,480 --> 00:10:17,140
fundamental truth right more education

184
00:10:15,730 --> 00:10:18,850
is the one thing you can't ever ever

185
00:10:17,140 --> 00:10:21,130
ever ever ever have somebody take away

186
00:10:18,850 --> 00:10:24,490
from you so what if machines are able to

187
00:10:21,130 --> 00:10:27,280
do that and its really given us a huge

188
00:10:24,490 --> 00:10:28,900
philosophical debate and something as

189
00:10:27,280 --> 00:10:31,449
security practitioners we need to be

190
00:10:28,900 --> 00:10:33,880
mindful of as we see this huge

191
00:10:31,450 --> 00:10:36,160
transformation in our industries of how

192
00:10:33,880 --> 00:10:39,120
computers are impacting their lives one

193
00:10:36,160 --> 00:10:42,160
can a machine really act intelligently

194
00:10:39,120 --> 00:10:46,810
can it solve a problem the way humans do

195
00:10:42,160 --> 00:10:49,180
to use our intelligence the same is a

196
00:10:46,810 --> 00:10:54,219
great I could hear some what are you

197
00:10:49,180 --> 00:10:56,550
guess you know in 3 Canada she have a

198
00:10:54,220 --> 00:11:00,460
mind mental states and consciousness

199
00:10:56,550 --> 00:11:01,859
consciousness bianchi is that even

200
00:11:00,460 --> 00:11:05,250
possible

201
00:11:01,860 --> 00:11:05,250
who says yes

202
00:11:05,670 --> 00:11:12,180
I would agree I don't think so but could

203
00:11:08,519 --> 00:11:14,630
it be in the future so the first time I

204
00:11:12,180 --> 00:11:17,370
ran into these philosophical questions I

205
00:11:14,630 --> 00:11:20,640
will tell you I had a really rough time

206
00:11:17,370 --> 00:11:23,010
with this I was at a conference in

207
00:11:20,640 --> 00:11:25,649
daikon Dayton Ohio was sitting on a

208
00:11:23,010 --> 00:11:26,760
panel talking about insecurity the woman

209
00:11:25,649 --> 00:11:29,070
to the right of me was from Wright State

210
00:11:26,760 --> 00:11:31,829
she helped with the original ARPANET and

211
00:11:29,070 --> 00:11:33,600
first educational Mira most amazing

212
00:11:31,829 --> 00:11:36,510
women in the field woman sitting next to

213
00:11:33,600 --> 00:11:38,579
me was 19 years old chick top hat on and

214
00:11:36,510 --> 00:11:41,819
she wore a mask

215
00:11:38,579 --> 00:11:43,709
and I remember asking what do you do and

216
00:11:41,820 --> 00:11:44,430
she's like I keep the top hat on to keep

217
00:11:43,709 --> 00:11:47,010
me grounded

218
00:11:44,430 --> 00:11:49,500
but I'm doing my master's thesis in how

219
00:11:47,010 --> 00:11:55,560
to evoke empathy utilizing robots for

220
00:11:49,500 --> 00:11:57,690
warfare any night team so we're going to

221
00:11:55,560 --> 00:12:00,989
create robots that we then feel sorry

222
00:11:57,690 --> 00:12:02,850
for so they can kill us great and that's

223
00:12:00,990 --> 00:12:04,170
her master's thesis at 19 I was about

224
00:12:02,850 --> 00:12:06,329
you know top having a nice man because I

225
00:12:04,170 --> 00:12:07,880
didn't know what to make of it but that

226
00:12:06,329 --> 00:12:10,529
was almost 10 years ago

227
00:12:07,880 --> 00:12:12,240
think about where we've come down and

228
00:12:10,529 --> 00:12:14,250
that to you is the first time I really

229
00:12:12,240 --> 00:12:16,170
philosophically thought about what is

230
00:12:14,250 --> 00:12:18,779
artificial intelligence going to do to

231
00:12:16,170 --> 00:12:20,729
our world there's a lot of hypotheses a

232
00:12:18,779 --> 00:12:23,220
different pattern about what a is

233
00:12:20,730 --> 00:12:25,019
but the reality a is that machines

234
00:12:23,220 --> 00:12:28,090
exhibit intelligence perceive their

235
00:12:25,019 --> 00:12:30,490
environment and take action to maximize

236
00:12:28,090 --> 00:12:33,850
and that's similar to what we say pause

237
00:12:30,490 --> 00:12:35,710
for thought learn from experience action

238
00:12:33,850 --> 00:12:37,960
accordingly that's what sensing us so

239
00:12:35,710 --> 00:12:39,910
very very similar so we take it from

240
00:12:37,960 --> 00:12:41,830
that great angle exhibit intelligence

241
00:12:39,910 --> 00:12:45,910
perceive their environment and take

242
00:12:41,830 --> 00:12:48,160
action then really the cognitive

243
00:12:45,910 --> 00:12:49,540
computer is nothing more than something

244
00:12:48,160 --> 00:12:52,900
made without rhythms right let's not

245
00:12:49,540 --> 00:12:55,060
worry though now there's only what's

246
00:12:52,900 --> 00:12:57,610
taught controls only what we give in

247
00:12:55,060 --> 00:13:01,300
control of and aware of nuances that can

248
00:12:57,610 --> 00:13:03,970
be to learn this to me makes it a lot

249
00:13:01,300 --> 00:13:07,359
more clear what artificial intelligence

250
00:13:03,970 --> 00:13:12,640
is today then this mystic world it's

251
00:13:07,360 --> 00:13:15,250
going to solve everything right but we

252
00:13:12,640 --> 00:13:17,460
are at the age of computers so where did

253
00:13:15,250 --> 00:13:20,350
all this come from

254
00:13:17,460 --> 00:13:22,090
obviously the classical philosophers I

255
00:13:20,350 --> 00:13:24,730
and years ago change the world

256
00:13:22,090 --> 00:13:26,050
but the reality is that the idea of

257
00:13:24,730 --> 00:13:28,330
artificial intelligence came to the

258
00:13:26,050 --> 00:13:30,819
1940s and it's really speared by world

259
00:13:28,330 --> 00:13:33,100
war ii we have the boolean circuit model

260
00:13:30,820 --> 00:13:36,340
of the brain we'd also have my favorite

261
00:13:33,100 --> 00:13:38,680
the Enigma machine anyone seen yeah so

262
00:13:36,340 --> 00:13:40,210
I'm old legacy bridge Telecom I'm very

263
00:13:38,680 --> 00:13:42,370
proud of that fact that goes to the park

264
00:13:40,210 --> 00:13:48,190
that was us my first hacking team was

265
00:13:42,370 --> 00:13:50,620
actually called the Colossus but really

266
00:13:48,190 --> 00:13:54,730
if you think about it World War 2 the

267
00:13:50,620 --> 00:13:57,040
Golden Ears were what landing the space

268
00:13:54,730 --> 00:13:59,380
project that was the beginnings of

269
00:13:57,040 --> 00:14:01,209
artificial intelligence but then what

270
00:13:59,380 --> 00:14:04,660
happened think about that year what I

271
00:14:01,210 --> 00:14:06,400
say then to 86 you're my dad it's also

272
00:14:04,660 --> 00:14:08,589
the year that we started to have a

273
00:14:06,400 --> 00:14:10,760
return of these networks to popularity

274
00:14:08,590 --> 00:14:12,440
and we started to see applications

275
00:14:10,760 --> 00:14:15,050
she running remember that said that hope

276
00:14:12,440 --> 00:14:18,770
which I talked about very very very

277
00:14:15,050 --> 00:14:20,449
early machine learning 1995 is when we

278
00:14:18,770 --> 00:14:23,120
started to see methods Commission on

279
00:14:20,450 --> 00:14:25,220
much data again because my dad was a vet

280
00:14:23,120 --> 00:14:27,200
and in wheelchair guess what we had all

281
00:14:25,220 --> 00:14:29,890
of his early precursors in my home

282
00:14:27,200 --> 00:14:32,900
something I was immersed in until basis

283
00:14:29,890 --> 00:14:34,130
and now we think about later we don't

284
00:14:32,900 --> 00:14:37,189
really think that facial recognition

285
00:14:34,130 --> 00:14:40,790
software has been around they swung this

286
00:14:37,190 --> 00:14:43,040
2006 robot driving cars guess what dark

287
00:14:40,790 --> 00:14:45,020
was playing with it in 2003 still a

288
00:14:43,040 --> 00:14:51,439
hotly debated topic almost 20 years

289
00:14:45,020 --> 00:14:55,520
later and then 2011 the first question

290
00:14:51,440 --> 00:14:57,910
is very more about so I put this in

291
00:14:55,520 --> 00:15:03,890
because it's interesting

292
00:14:57,910 --> 00:15:06,380
Eli must think about this said 2011 AI

293
00:15:03,890 --> 00:15:09,020
singularity an Achilles all think about

294
00:15:06,380 --> 00:15:11,150
what he smokes his business on and I

295
00:15:09,020 --> 00:15:13,430
would disagree that most people at that

296
00:15:11,150 --> 00:15:16,340
point in 2011 had his headaches

297
00:15:13,430 --> 00:15:17,660
skepticism the singularity and it took a

298
00:15:16,340 --> 00:15:19,100
lot of effort to get a little

299
00:15:17,660 --> 00:15:21,140
intelligence or something

300
00:15:19,100 --> 00:15:25,490
but I think we've risen to the challenge

301
00:15:21,140 --> 00:15:28,130
because it's already impacting lots of

302
00:15:25,490 --> 00:15:28,520
parts of our lives so we think about in

303
00:15:28,130 --> 00:15:30,110
Kentucky

304
00:15:28,520 --> 00:15:31,880
artificial intelligence from a security

305
00:15:30,110 --> 00:15:33,890
perspective we sometimes forget that

306
00:15:31,880 --> 00:15:36,020
it's already embedded in everything

307
00:15:33,890 --> 00:15:37,640
we're working with in utilizing today I

308
00:15:36,020 --> 00:15:40,490
think about the news that came without

309
00:15:37,640 --> 00:15:42,350
the last week series listening to

310
00:15:40,490 --> 00:15:43,580
everything which always drives me crazy

311
00:15:42,350 --> 00:15:44,279
because sometimes I don't want to

312
00:15:43,580 --> 00:15:46,170
advertise

313
00:15:44,279 --> 00:15:47,879
from the kids school shoes or god knows

314
00:15:46,170 --> 00:15:49,290
what or the cutest little t-shirt

315
00:15:47,879 --> 00:15:51,029
because they're playing with my device

316
00:15:49,290 --> 00:15:54,860
as I told you yesterday

317
00:15:51,029 --> 00:15:59,129
drones Alexa the butler in the future

318
00:15:54,860 --> 00:16:02,100
self-driving cars we utilize artificial

319
00:15:59,129 --> 00:16:03,720
intelligence constantly but where else

320
00:16:02,100 --> 00:16:06,959
isn't going to go and how do we look at

321
00:16:03,720 --> 00:16:13,290
securing it guess what guys machines are

322
00:16:06,959 --> 00:16:15,660
calming so first thing you thought about

323
00:16:13,290 --> 00:16:18,990
machine is a policeman this is the

324
00:16:15,660 --> 00:16:21,360
cheetah robot he can run 28 times faster

325
00:16:18,990 --> 00:16:23,910
than Usain Bolt and is being deployed as

326
00:16:21,360 --> 00:16:29,420
a test robot to actually utilize in

327
00:16:23,910 --> 00:16:31,399
crowds for crowd control amazing

328
00:16:29,420 --> 00:16:33,870
security guards

329
00:16:31,399 --> 00:16:35,279
next up security robots are being used

330
00:16:33,870 --> 00:16:37,529
in uber parking lots and over

331
00:16:35,279 --> 00:16:39,689
headquarters so instead of having a

332
00:16:37,529 --> 00:16:40,649
regular security guard this guy's going

333
00:16:39,689 --> 00:16:42,599
to come tell you you're parking

334
00:16:40,649 --> 00:16:47,819
illegally and that's fantastic it take

335
00:16:42,600 --> 00:16:51,209
to window how much fun is that builders

336
00:16:47,819 --> 00:16:54,389
I love this one a robot constructed new

337
00:16:51,209 --> 00:16:56,878
volume two days we didn't have an uber

338
00:16:54,389 --> 00:16:58,470
uber sue doesn't want one for me we have

339
00:16:56,879 --> 00:17:00,589
to deal with contractors on my house for

340
00:16:58,470 --> 00:17:05,870
weeks I like it

341
00:17:00,589 --> 00:17:09,120
and guys even your pizza delivery guy

342
00:17:05,869 --> 00:17:12,948
dad knows has the first problems

343
00:17:09,119 --> 00:17:16,109
delivering pizzas to customer homes

344
00:17:12,949 --> 00:17:18,169
robots are going to be everywhere so

345
00:17:16,109 --> 00:17:22,408
it's our job in this room to do what

346
00:17:18,169 --> 00:17:26,730
protect that human right but what else

347
00:17:22,409 --> 00:17:28,990
could they do think about it really how

348
00:17:26,730 --> 00:17:30,790
we can utilize a

349
00:17:28,990 --> 00:17:34,600
it could solve some of the world's

350
00:17:30,790 --> 00:17:36,700
various problems but we all have

351
00:17:34,600 --> 00:17:39,250
challenges first thing we need to think

352
00:17:36,700 --> 00:17:41,080
about is how do we ensure the safety of

353
00:17:39,250 --> 00:17:42,460
the applications we have all the signals

354
00:17:41,080 --> 00:17:46,330
for information how to start making with

355
00:17:42,460 --> 00:17:49,270
us safely and this how do we avoid out

356
00:17:46,330 --> 00:17:51,129
their bias if we're the ones making the

357
00:17:49,270 --> 00:17:52,780
algorithm so we're the human is the

358
00:17:51,130 --> 00:17:55,030
person putting the controls on the

359
00:17:52,780 --> 00:17:57,310
computer how do we make sure our own

360
00:17:55,030 --> 00:18:01,629
biases aren't translated into efficient

361
00:17:57,310 --> 00:18:03,879
conscious next thing is the law aspect

362
00:18:01,630 --> 00:18:06,000
of it when the robot does something

363
00:18:03,880 --> 00:18:09,190
wrong who's liable

364
00:18:06,000 --> 00:18:12,040
is it the under privacy so plenty

365
00:18:09,190 --> 00:18:14,170
straight anybody in this room no defense

366
00:18:12,040 --> 00:18:15,879
service a couple people

367
00:18:14,170 --> 00:18:17,380
for those who don't insert was actually

368
00:18:15,880 --> 00:18:19,240
one of the original founders of ARPANET

369
00:18:17,380 --> 00:18:21,700
use the internet evangelist at Google

370
00:18:19,240 --> 00:18:23,560
he also thankfully has been my mentor

371
00:18:21,700 --> 00:18:24,850
since I was 21 years old I got to

372
00:18:23,560 --> 00:18:28,659
carries a briefcase around for a couple

373
00:18:24,850 --> 00:18:31,540
of years and he said privacy's estate is

374
00:18:28,660 --> 00:18:34,600
an antiquated idea much lay down your

375
00:18:31,540 --> 00:18:36,310
sword now in the UK me when I say that

376
00:18:34,600 --> 00:18:38,290
they're like we know that great artists

377
00:18:36,310 --> 00:18:40,960
is gone they say the US it's a very

378
00:18:38,290 --> 00:18:44,080
emotional response a privacy we need to

379
00:18:40,960 --> 00:18:47,500
see I gave a talk on it at RSA two years

380
00:18:44,080 --> 00:18:50,770
ago the idea that privacy is extinct if

381
00:18:47,500 --> 00:18:53,310
so many people showed up one my boss

382
00:18:50,770 --> 00:18:53,310
who's pissed

383
00:18:53,630 --> 00:18:59,240
privacy is still something we have an

384
00:18:56,130 --> 00:19:01,110
emotional response to so the idea that

385
00:18:59,240 --> 00:19:03,360
artificial intelligence machine learning

386
00:19:01,110 --> 00:19:05,879
could be the end of it is something as

387
00:19:03,360 --> 00:19:07,439
Americans we really struggle with and

388
00:19:05,880 --> 00:19:08,520
that's the case then what we struggle

389
00:19:07,440 --> 00:19:09,900
with something what happens the

390
00:19:08,520 --> 00:19:10,879
politicians get involved and then what

391
00:19:09,900 --> 00:19:13,830
happens

392
00:19:10,880 --> 00:19:15,570
regulation regulatory bodies so how are

393
00:19:13,830 --> 00:19:16,860
we going to regulate the use of all

394
00:19:15,570 --> 00:19:18,360
these robots if we want more of them

395
00:19:16,860 --> 00:19:21,360
faster because digital transformation is

396
00:19:18,360 --> 00:19:26,149
where it will go we've got a paradox

397
00:19:21,360 --> 00:19:29,129
anymore what else

398
00:19:26,150 --> 00:19:31,890
now we did an ultra cold and benefit

399
00:19:29,130 --> 00:19:34,110
everyone not just the rich or everybody

400
00:19:31,890 --> 00:19:36,870
how can we make sure as they're firmly

401
00:19:34,110 --> 00:19:40,409
how can it be ethical accountable

402
00:19:36,870 --> 00:19:42,989
transparent this is a big concern little

403
00:19:40,410 --> 00:19:45,240
innate jobs Fox 9 it's going into

404
00:19:42,990 --> 00:19:48,690
Milwaukee Wisconsin US headquarters in

405
00:19:45,240 --> 00:19:51,330
China 60,000 jobs were eliminated last

406
00:19:48,690 --> 00:19:52,560
year $5 in workers because they were

407
00:19:51,330 --> 00:19:55,050
able to automate the process of

408
00:19:52,560 --> 00:19:57,270
utilizing artificial intelligence so how

409
00:19:55,050 --> 00:19:59,340
do we scale our workforces she working

410
00:19:57,270 --> 00:20:02,160
with the robots and not lose jobs and

411
00:19:59,340 --> 00:20:04,350
not looks good and how do we protect

412
00:20:02,160 --> 00:20:08,910
against the biggest problem the

413
00:20:04,350 --> 00:20:09,990
unintended consequences so this I just

414
00:20:08,910 --> 00:20:12,480
loved as I was playing through my slides

415
00:20:09,990 --> 00:20:14,670
I love this book every government every

416
00:20:12,480 --> 00:20:17,760
company every academic institution every

417
00:20:14,670 --> 00:20:20,580
one of us should consider how AI will

418
00:20:17,760 --> 00:20:23,220
affect our future but do we know we live

419
00:20:20,580 --> 00:20:25,020
our lives or anything else so we should

420
00:20:23,220 --> 00:20:26,400
think about the trends this is getting

421
00:20:25,020 --> 00:20:26,970
hard to read but there's four key trends

422
00:20:26,400 --> 00:20:29,970
right now

423
00:20:26,970 --> 00:20:31,860
IOT security talked about a lot in this

424
00:20:29,970 --> 00:20:32,790
Congress how we return to supply chains

425
00:20:31,860 --> 00:20:37,139
how we protect

426
00:20:32,790 --> 00:20:39,120
Isis to 600 new companies a year he

427
00:20:37,140 --> 00:20:42,120
founded with the premise of artificial

428
00:20:39,120 --> 00:20:45,030
intelligence say that never again 600 of

429
00:20:42,120 --> 00:20:47,760
them the M&A activity is the hottest

430
00:20:45,030 --> 00:20:49,800
space in the industry one trillion

431
00:20:47,760 --> 00:20:51,060
dollars will be spent in merger and

432
00:20:49,800 --> 00:20:55,950
acquisition of artificial intelligence

433
00:20:51,060 --> 00:20:59,909
in the next four years one $20 human

434
00:20:55,950 --> 00:21:02,010
moderate living who thinks there's

435
00:20:59,910 --> 00:21:03,600
actually outsourcing of human monitoring

436
00:21:02,010 --> 00:21:08,760
technology is going on right now today

437
00:21:03,600 --> 00:21:11,159
in this world I was with former

438
00:21:08,760 --> 00:21:12,690
secretary Neilson not long ago and she

439
00:21:11,160 --> 00:21:15,200
gave a big talk that it's actually one

440
00:21:12,690 --> 00:21:18,450
of the number one this is really scary

441
00:21:15,200 --> 00:21:20,760
sources of income for emerging countries

442
00:21:18,450 --> 00:21:23,580
in a check to sell to former Eastern

443
00:21:20,760 --> 00:21:26,879
European countries social modern

444
00:21:23,580 --> 00:21:28,919
technologies and then the worst thing is

445
00:21:26,880 --> 00:21:30,900
is what we've all those artificial

446
00:21:28,920 --> 00:21:34,440
intelligence guess who's ahead of us

447
00:21:30,900 --> 00:21:36,660
seem utilizing it now just trying the

448
00:21:34,440 --> 00:21:38,610
attackers the hackers are way more

449
00:21:36,660 --> 00:21:42,510
sophisticated than me i right now in

450
00:21:38,610 --> 00:21:45,689
love so how are you doing well first of

451
00:21:42,510 --> 00:21:48,150
all they like to attack AI systems why

452
00:21:45,690 --> 00:21:50,790
because it's really easy to fall in MO

453
00:21:48,150 --> 00:21:52,200
model it's really easy to get someone

454
00:21:50,790 --> 00:21:53,370
something to believe it's a computer

455
00:21:52,200 --> 00:21:54,900
that doesn't have critical reasoning

456
00:21:53,370 --> 00:21:57,239
that you're an apple when you're really

457
00:21:54,900 --> 00:21:58,860
a banana or I use a lot of analogies

458
00:21:57,240 --> 00:22:00,570
kado some coconuts that's a hole in

459
00:21:58,860 --> 00:22:02,100
their speech about without the coconut

460
00:22:00,570 --> 00:22:05,100
security talking she listened to is

461
00:22:02,100 --> 00:22:06,719
really fun but also physical attacks I

462
00:22:05,100 --> 00:22:10,159
want to meet my physical taxes if you're

463
00:22:06,720 --> 00:22:13,260
able to change what is on that screen

464
00:22:10,160 --> 00:22:15,510
that you can provoke a different

465
00:22:13,260 --> 00:22:17,129
response think about that in Thailand

466
00:22:15,510 --> 00:22:19,070
last year that uber could be uses on

467
00:22:17,130 --> 00:22:22,380
that weapon of mass destruction you

468
00:22:19,070 --> 00:22:23,550
changed your physical logic we changed

469
00:22:22,380 --> 00:22:25,770
the machine learning about where

470
00:22:23,550 --> 00:22:28,800
driverless taxis to go or even real

471
00:22:25,770 --> 00:22:32,310
drivers you could cause the jet engine

472
00:22:28,800 --> 00:22:39,409
social worker so how does that work

473
00:22:32,310 --> 00:22:42,929
but prettiest example at Mysterio Tex

474
00:22:39,410 --> 00:22:44,370
57% say you got hand over here you

475
00:22:42,930 --> 00:22:46,260
introduce just a little bit static noise

476
00:22:44,370 --> 00:22:49,110
to that machine learning it's what all

477
00:22:46,260 --> 00:22:52,379
the sentence says it's monkey that's how

478
00:22:49,110 --> 00:22:54,870
easy it is now think about that is we

479
00:22:52,380 --> 00:22:57,060
try and build more secure systems it

480
00:22:54,870 --> 00:23:02,459
just static noise can change a pattern

481
00:22:57,060 --> 00:23:05,250
think about the ramifications from an

482
00:23:02,460 --> 00:23:07,440
attacker perspective here's how hackers

483
00:23:05,250 --> 00:23:09,930
are utilizing and today they're

484
00:23:07,440 --> 00:23:14,250
utilizing AI to craft emails to bypass

485
00:23:09,930 --> 00:23:17,160
filters you take our execute machines be

486
00:23:14,250 --> 00:23:19,110
created attacks best example I had on

487
00:23:17,160 --> 00:23:20,580
this was a emailed myself last week it

488
00:23:19,110 --> 00:23:22,639
was amazing to ask if I was in the

489
00:23:20,580 --> 00:23:25,080
office who was ready for a chat

490
00:23:22,640 --> 00:23:26,490
literally I'm sitting on the phone

491
00:23:25,080 --> 00:23:28,800
and all of a sudden an email from he

492
00:23:26,490 --> 00:23:29,970
comes in two seconds later and email my

493
00:23:28,800 --> 00:23:31,680
partner comes in saying hey

494
00:23:29,970 --> 00:23:33,240
do something which is critical could you

495
00:23:31,680 --> 00:23:34,380
quit text me at this number because they

496
00:23:33,240 --> 00:23:37,080
were trying to validate for some

497
00:23:34,380 --> 00:23:39,210
information I was on the phone with my

498
00:23:37,080 --> 00:23:41,280
partner and I'm like David you emailing

499
00:23:39,210 --> 00:23:43,110
me to text you right now did some

500
00:23:41,280 --> 00:23:45,510
forensics and there was one character

501
00:23:43,110 --> 00:23:48,060
often emails something was good to us

502
00:23:45,510 --> 00:23:51,050
but it got through our systems because

503
00:23:48,060 --> 00:23:54,210
it looked close enough to it should be

504
00:23:51,050 --> 00:23:55,470
catch it happens all the time good

505
00:23:54,210 --> 00:23:57,360
friend of mine company we talked about

506
00:23:55,470 --> 00:24:00,360
this yesterday by Thunder does not

507
00:23:57,360 --> 00:24:01,830
invade transference rewriting based on

508
00:24:00,360 --> 00:24:04,709
being able to bypass the novels right

509
00:24:01,830 --> 00:24:06,659
easy peasy stuff bread butter they

510
00:24:04,710 --> 00:24:08,190
actually you can get services no did you

511
00:24:06,660 --> 00:24:09,630
know this on the App Store the app

512
00:24:08,190 --> 00:24:11,940
server hacker is really weird they'll

513
00:24:09,630 --> 00:24:14,940
actually put SFA's around utilizing this

514
00:24:11,940 --> 00:24:16,170
for neglect for order an academic but

515
00:24:14,940 --> 00:24:18,990
they'll guarantee perhaps with an

516
00:24:16,170 --> 00:24:21,030
Escalade it's amazing I don't

517
00:24:18,990 --> 00:24:26,280
necessarily have my service yet the

518
00:24:21,030 --> 00:24:28,080
hackers know that's not good this is I

519
00:24:26,280 --> 00:24:29,460
think the most concerning for me from a

520
00:24:28,080 --> 00:24:32,189
hacking this perspective

521
00:24:29,460 --> 00:24:33,720
Eric guard whose assignment he

522
00:24:32,190 --> 00:24:35,550
determines class concealment the

523
00:24:33,720 --> 00:24:38,580
incident and malicious intent

524
00:24:35,550 --> 00:24:40,470
realizing artificial intelligence so we

525
00:24:38,580 --> 00:24:44,520
aren't using a on the opposite side

526
00:24:40,470 --> 00:24:47,460
defend how do we protect against us how

527
00:24:44,520 --> 00:24:50,330
do we start to think about this they're

528
00:24:47,460 --> 00:24:50,330
ahead of us right now guys

529
00:24:51,690 --> 00:24:56,760
this is what's really interesting though

530
00:24:54,000 --> 00:24:58,830
is we talk about emails were out there

531
00:24:56,760 --> 00:25:01,560
but we talk about social distortion

532
00:24:58,830 --> 00:25:03,570
social intense we're starting to see the

533
00:25:01,560 --> 00:25:05,280
target of utilizing artificial

534
00:25:03,570 --> 00:25:07,290
intelligence machine learning to create

535
00:25:05,280 --> 00:25:10,220
how I can create deception in the world

536
00:25:07,290 --> 00:25:12,149
go beyond our normal traditional thought

537
00:25:10,220 --> 00:25:14,550
audio-visual physical sensors

538
00:25:12,150 --> 00:25:18,620
geolocation these are activity deep fake

539
00:25:14,550 --> 00:25:20,879
who you're similar with deep fake yeah

540
00:25:18,620 --> 00:25:22,800
ok so I really played around with like a

541
00:25:20,880 --> 00:25:24,390
deep paying out the other day I was able

542
00:25:22,800 --> 00:25:26,070
to make myself look like fifteen pounds

543
00:25:24,390 --> 00:25:27,750
skinnier and have like broke really

544
00:25:26,070 --> 00:25:29,070
really long brown hair was amazing

545
00:25:27,750 --> 00:25:30,390
then I was really worried that if we

546
00:25:29,070 --> 00:25:31,740
don't open all that computer he was

547
00:25:30,390 --> 00:25:32,790
making my computer do funny things about

548
00:25:31,740 --> 00:25:37,980
confidence

549
00:25:32,790 --> 00:25:42,440
but it's starting to create this will a

550
00:25:37,980 --> 00:25:42,440
I'd manipulate us and the answer is yes

551
00:25:43,130 --> 00:25:48,600
irony harder this this was the first

552
00:25:46,770 --> 00:25:51,060
picture that really started to bring the

553
00:25:48,600 --> 00:25:54,230
idea of manipulation into mainstream

554
00:25:51,060 --> 00:25:56,940
thought he would not know what this says

555
00:25:54,230 --> 00:25:59,400
press intern at the White House was

556
00:25:56,940 --> 00:26:01,290
reaching for a microphone the reporter

557
00:25:59,400 --> 00:26:03,090
went like this to say one more second

558
00:26:01,290 --> 00:26:04,620
the photo was manipulated and it looked

559
00:26:03,090 --> 00:26:06,209
like the reporter actually that the

560
00:26:04,620 --> 00:26:08,280
press in turn and he was denied his

561
00:26:06,210 --> 00:26:10,320
badges whose answer press had just taken

562
00:26:08,280 --> 00:26:12,810
away big incident the president was

563
00:26:10,320 --> 00:26:16,560
involved it was based on a manipulated

564
00:26:12,810 --> 00:26:20,520
photo this is the actual real photo does

565
00:26:16,560 --> 00:26:24,060
it look like anyone was hit no but one

566
00:26:20,520 --> 00:26:27,870
manipulated photo caused thousands of

567
00:26:24,060 --> 00:26:31,250
press issues thousands of problems and

568
00:26:27,870 --> 00:26:31,250
almost one man's job

569
00:26:32,330 --> 00:26:38,389
so one little photo can do that think

570
00:26:36,320 --> 00:26:40,070
about what the ramifications are here's

571
00:26:38,390 --> 00:26:41,210
the thing I was telling you about taking

572
00:26:40,070 --> 00:26:43,460
put my picture on

573
00:26:41,210 --> 00:26:45,440
but here's one actress to play around

574
00:26:43,460 --> 00:26:47,420
with what looks like pose then all of a

575
00:26:45,440 --> 00:26:49,190
sudden you have real enjoyment looking

576
00:26:47,420 --> 00:26:51,770
pictures on the other side that are very

577
00:26:49,190 --> 00:26:53,660
different from the original it used to

578
00:26:51,770 --> 00:26:56,090
be that you eaten lots and lots and lots

579
00:26:53,660 --> 00:26:58,070
of different facial expressions to be

580
00:26:56,090 --> 00:27:02,810
able to do videos guess something you

581
00:26:58,070 --> 00:27:04,580
need what now that's it one so this is

582
00:27:02,810 --> 00:27:07,429
even less than six months ago antiquated

583
00:27:04,580 --> 00:27:09,310
technology because now one photo is

584
00:27:07,430 --> 00:27:13,100
enough to build a reconstructed video

585
00:27:09,310 --> 00:27:15,320
one think about the ramifications of our

586
00:27:13,100 --> 00:27:21,860
social footprints any one of us could be

587
00:27:15,320 --> 00:27:23,419
spooked in a matter of seconds so now

588
00:27:21,860 --> 00:27:26,840
that I've given all this limiting we'll

589
00:27:23,420 --> 00:27:29,510
do one word limiting think about the

590
00:27:26,840 --> 00:27:31,100
algorithms themselves impersonation of

591
00:27:29,510 --> 00:27:34,250
trusted users which is so the thing

592
00:27:31,100 --> 00:27:36,590
photo evidence but nuances in behavior

593
00:27:34,250 --> 00:27:38,600
and writing style being able to really

594
00:27:36,590 --> 00:27:39,139
see crafted messages blending into the

595
00:27:38,600 --> 00:27:41,540
background

596
00:27:39,140 --> 00:27:44,120
faster attacks with consequences our

597
00:27:41,540 --> 00:27:45,620
social imprints are becoming the

598
00:27:44,120 --> 00:27:49,929
playground for how artificial

599
00:27:45,620 --> 00:27:52,100
intelligence can be used against us and

600
00:27:49,930 --> 00:27:55,760
this is probably the most terrifying

601
00:27:52,100 --> 00:27:58,790
example so there was a deep day a video

602
00:27:55,760 --> 00:28:01,070
that came out leveraging a couple in two

603
00:27:58,790 --> 00:28:02,810
weeks from the president and it was

604
00:28:01,070 --> 00:28:04,129
basically do you think video saying that

605
00:28:02,810 --> 00:28:10,220
we were going to drop a nuclear bomb on

606
00:28:04,130 --> 00:28:13,280
North Korea by leveraging our presidents

607
00:28:10,220 --> 00:28:15,860
tweet profile station instructions and

608
00:28:13,280 --> 00:28:16,280
facial expressions we were able to there

609
00:28:15,860 --> 00:28:19,189
was

610
00:28:16,280 --> 00:28:19,740
hackers who were able to basically would

611
00:28:19,190 --> 00:28:21,570
be

612
00:28:19,740 --> 00:28:24,600
he announced to the US public that he

613
00:28:21,570 --> 00:28:27,870
was talking about what if we did catch

614
00:28:24,600 --> 00:28:33,449
it that's the kind of thing that post

615
00:28:27,870 --> 00:28:38,340
services could be the last piece is this

616
00:28:33,450 --> 00:28:40,620
social control China is already offering

617
00:28:38,340 --> 00:28:43,320
and selling social control technology

618
00:28:40,620 --> 00:28:44,879
and while it gets more secure the

619
00:28:43,320 --> 00:28:48,570
reality is is that we're listening to it

620
00:28:44,880 --> 00:28:50,399
then I was listening and the Social

621
00:28:48,570 --> 00:28:52,649
Credit System is just the first step of

622
00:28:50,399 --> 00:28:55,649
to research and control utilizing

623
00:28:52,649 --> 00:28:57,629
artificial intelligence in China and

624
00:28:55,649 --> 00:29:00,469
other countries right now if your social

625
00:28:57,630 --> 00:29:01,950
media makes it look like you are not

626
00:29:00,470 --> 00:29:04,080
pro-government

627
00:29:01,950 --> 00:29:06,299
or there's things that are odd about you

628
00:29:04,080 --> 00:29:09,059
people are being put into eternity

629
00:29:06,299 --> 00:29:12,210
capsule and that technology is being

630
00:29:09,059 --> 00:29:16,320
usable that seriously

631
00:29:12,210 --> 00:29:19,799
so they're about to figure out a couple

632
00:29:16,320 --> 00:29:20,970
of things one how do we educate how much

633
00:29:19,799 --> 00:29:26,580
personal information they're giving

634
00:29:20,970 --> 00:29:28,409
machines to how do we safeguard or the

635
00:29:26,580 --> 00:29:30,600
details about our behavior our

636
00:29:28,409 --> 00:29:32,640
preferences are interested in and how

637
00:29:30,600 --> 00:29:36,899
can we improve how we interact with

638
00:29:32,640 --> 00:29:38,610
users to ensure that the safety of how

639
00:29:36,899 --> 00:29:45,600
we use AI is done in a really meaningful

640
00:29:38,610 --> 00:29:47,370
way so let's remember one thing the tool

641
00:29:45,600 --> 00:29:50,750
we're talking about is going to neutral

642
00:29:47,370 --> 00:29:50,750
it's the application

643
00:29:52,000 --> 00:29:57,250
so and insecurity how do we use well

644
00:29:56,320 --> 00:29:59,409
couple of things

645
00:29:57,250 --> 00:30:01,540
it forces integrity enforces privacy and

646
00:29:59,410 --> 00:30:03,460
repents Missy's so just like I showed

647
00:30:01,540 --> 00:30:04,870
you all the things the hackers can do we

648
00:30:03,460 --> 00:30:06,820
can do the same thing on the other side

649
00:30:04,870 --> 00:30:09,939
so what are some of the ways they can

650
00:30:06,820 --> 00:30:12,010
use it well here's the thing if

651
00:30:09,940 --> 00:30:13,150
somebody's using a on malicious activity

652
00:30:12,010 --> 00:30:17,140
you think we can use the same tools

653
00:30:13,150 --> 00:30:18,130
detect it can we live a process for

654
00:30:17,140 --> 00:30:22,720
things like malware

655
00:30:18,130 --> 00:30:26,230
100% does it make it easier to respond

656
00:30:22,720 --> 00:30:28,660
to risk and free up time that's the most

657
00:30:26,230 --> 00:30:30,790
critical thing so the sense they don't

658
00:30:28,660 --> 00:30:34,240
give you an example so utilizing amory

659
00:30:30,790 --> 00:30:37,060
the average company deals with large

660
00:30:34,240 --> 00:30:41,860
company details with 18,000 threat were

661
00:30:37,060 --> 00:30:44,230
today 18,000 utilizing artificial

662
00:30:41,860 --> 00:30:45,909
intelligence we are able to Club all

663
00:30:44,230 --> 00:30:47,320
those in two cases or potential attacks

664
00:30:45,910 --> 00:30:51,010
or different nominees and things like

665
00:30:47,320 --> 00:30:53,620
that we take down to 42 then we analyze

666
00:30:51,010 --> 00:30:56,050
further and able to discard out of that

667
00:30:53,620 --> 00:30:57,340
42 down to eight they're actually

668
00:30:56,050 --> 00:30:58,030
potentially malicious and should be

669
00:30:57,340 --> 00:30:59,620
looked at

670
00:30:58,030 --> 00:31:01,750
so think about that from the revenue

671
00:30:59,620 --> 00:31:04,449
perspective eighteen thousand forty two

672
00:31:01,750 --> 00:31:05,770
to eight that's the power of artificial

673
00:31:04,450 --> 00:31:09,490
intelligence think about how much time

674
00:31:05,770 --> 00:31:11,710
we can give back and the other thing is

675
00:31:09,490 --> 00:31:14,680
intelligent automation reduce their

676
00:31:11,710 --> 00:31:18,640
labor without the loss of quality we

677
00:31:14,680 --> 00:31:20,830
have to work with the machines here's

678
00:31:18,640 --> 00:31:22,360
the applications of reason right now for

679
00:31:20,830 --> 00:31:24,550
AI in cyber security the cool ones I

680
00:31:22,360 --> 00:31:26,290
like I made it fuzzy and clustering

681
00:31:24,550 --> 00:31:29,830
malware fraud detection user

682
00:31:26,290 --> 00:31:33,399
authentication account detection humans

683
00:31:29,830 --> 00:31:34,360
are pattern based by Nature you go to

684
00:31:33,400 --> 00:31:36,490
the same website

685
00:31:34,360 --> 00:31:37,929
these certificates in the same way I'm

686
00:31:36,490 --> 00:31:39,490
always on my computer at 1:00 in the

687
00:31:37,930 --> 00:31:41,140
morning because I'm really weirdo sleep

688
00:31:39,490 --> 00:31:42,610
a lot once in a while up the kids

689
00:31:41,140 --> 00:31:43,060
downloaded crazy games like pregnant

690
00:31:42,610 --> 00:31:48,340
mermaid

691
00:31:43,060 --> 00:31:51,669
that's not a good example good step we

692
00:31:48,340 --> 00:31:52,990
have patterns we can utilize artificial

693
00:31:51,670 --> 00:31:54,250
intelligence machine learning to

694
00:31:52,990 --> 00:31:56,170
understand the patterns of human

695
00:31:54,250 --> 00:31:58,660
behaviors so that when somebody uses a

696
00:31:56,170 --> 00:32:02,050
valid certificate in an honor way we can

697
00:31:58,660 --> 00:32:04,210
still catch it here's the ways we can

698
00:32:02,050 --> 00:32:06,790
make artificial intelligence our friend

699
00:32:04,210 --> 00:32:12,760
from cyber perspective here's what you

700
00:32:06,790 --> 00:32:15,490
can't do it's unable to detect mutant

701
00:32:12,760 --> 00:32:17,260
forms of malware but it can detect when

702
00:32:15,490 --> 00:32:18,970
threat techniques are utilized in your

703
00:32:17,260 --> 00:32:22,150
parents an hour that's critical we take

704
00:32:18,970 --> 00:32:23,620
it to the bones we get to see it if we

705
00:32:22,150 --> 00:32:25,660
try and do it when it's wholly baked

706
00:32:23,620 --> 00:32:26,169
we're not understanding got it look at

707
00:32:25,660 --> 00:32:29,350
the bones

708
00:32:26,170 --> 00:32:32,230
just like anyone's in person - - never

709
00:32:29,350 --> 00:32:33,790
here at least human experts three they

710
00:32:32,230 --> 00:32:35,200
will never be a day of zero most

711
00:32:33,790 --> 00:32:36,280
positive as well as long as there is a

712
00:32:35,200 --> 00:32:38,590
creative person on the other side

713
00:32:36,280 --> 00:32:41,470
there's going to be a problem that's

714
00:32:38,590 --> 00:32:43,000
just the way this and my favorite is we

715
00:32:41,470 --> 00:32:45,400
could never solve all the security

716
00:32:43,000 --> 00:32:48,010
powers so he's walking down on us a

717
00:32:45,400 --> 00:32:50,200
couple years ago and every week the old

718
00:32:48,010 --> 00:32:51,820
C so Sony we're walking going to have

719
00:32:50,200 --> 00:32:53,740
coffee we looked up there was a

720
00:32:51,820 --> 00:32:55,870
billboard for bucks in the box said buy

721
00:32:53,740 --> 00:33:00,370
my solution I will solve all your

722
00:32:55,870 --> 00:33:02,620
security problems he was angry what

723
00:33:00,370 --> 00:33:04,330
marketing person came up with a new way

724
00:33:02,620 --> 00:33:06,040
to solve everything for you

725
00:33:04,330 --> 00:33:07,689
there is no box that's going to solve

726
00:33:06,040 --> 00:33:10,600
everything there is no tool that's going

727
00:33:07,690 --> 00:33:11,680
to do it all so any company that comes

728
00:33:10,600 --> 00:33:15,449
forward with artificial intelligence

729
00:33:11,680 --> 00:33:15,450
since we do it all guess what

730
00:33:17,460 --> 00:33:24,010
but here's the thing utilizing these

731
00:33:21,340 --> 00:33:25,750
tools putting these into a production

732
00:33:24,010 --> 00:33:27,490
environment is scary for a lot of

733
00:33:25,750 --> 00:33:28,870
companies I don't know if I want to

734
00:33:27,490 --> 00:33:29,919
trust the machines I like making ones I

735
00:33:28,870 --> 00:33:34,750
don't think it's ready it's not ready

736
00:33:29,919 --> 00:33:37,960
for primetime changes scary its eternal

737
00:33:34,750 --> 00:33:39,460
perpetual in the world putting AI in

738
00:33:37,960 --> 00:33:42,070
your companies and in your security

739
00:33:39,460 --> 00:33:45,059
practices is a really fundamental shift

740
00:33:42,070 --> 00:33:49,418
that we have to make so how do we start

741
00:33:45,059 --> 00:33:50,830
well first of all like I said I love

742
00:33:49,419 --> 00:33:52,299
LinkedIn SlideShare is you get to see

743
00:33:50,830 --> 00:33:53,830
everybody was egotistical unless their

744
00:33:52,299 --> 00:33:55,090
slides up and every single thing it's

745
00:33:53,830 --> 00:33:57,549
great to get a pulse on where the real

746
00:33:55,090 --> 00:33:59,320
industry is but gives Miller with it

747
00:33:57,549 --> 00:34:01,720
play with it find solutions you like

748
00:33:59,320 --> 00:34:03,610
look at you know even down to going on

749
00:34:01,720 --> 00:34:04,960
or in a black hammer or are to say look

750
00:34:03,610 --> 00:34:07,510
at the innovations see what's going on

751
00:34:04,960 --> 00:34:10,629
out there we it's fun it's a really cool

752
00:34:07,510 --> 00:34:13,599
field then identify what in your

753
00:34:10,629 --> 00:34:15,129
industry could potentially need so long

754
00:34:13,599 --> 00:34:16,629
utilizing artificial intelligence how

755
00:34:15,129 --> 00:34:18,279
could you simplify the stack what

756
00:34:16,629 --> 00:34:19,980
solutions are coming up that you can

757
00:34:18,280 --> 00:34:22,629
potentially do something different with

758
00:34:19,980 --> 00:34:24,730
play with pilots everything at a company

759
00:34:22,629 --> 00:34:26,109
I know is long to do pilots play with

760
00:34:24,730 --> 00:34:30,580
get from one with it

761
00:34:26,109 --> 00:34:32,319
for a taskforce start small do not try

762
00:34:30,580 --> 00:34:33,639
boil the ocean trust me we will just

763
00:34:32,320 --> 00:34:37,300
have a headache and way too much data

764
00:34:33,639 --> 00:34:43,000
and see what you can do to operate it

765
00:34:37,300 --> 00:34:44,700
into daily tasks and then remember guess

766
00:34:43,000 --> 00:34:48,159
take responsibility for what you put in

767
00:34:44,699 --> 00:34:50,348
do you go to content on things revalue

768
00:34:48,159 --> 00:34:52,480
what length don't you want to get across

769
00:34:50,349 --> 00:34:55,690
to your organization how do you make it

770
00:34:52,480 --> 00:34:56,679
transparent and most importantly how do

771
00:34:55,690 --> 00:34:59,230
you keep the

772
00:34:56,679 --> 00:35:02,770
control so that brings me to I think my

773
00:34:59,230 --> 00:35:08,859
final closing question which is what is

774
00:35:02,770 --> 00:35:13,089
human what is you what can machine not

775
00:35:08,859 --> 00:35:14,589
ever do well some would argue against

776
00:35:13,089 --> 00:35:18,180
this but machines really aren't creative

777
00:35:14,589 --> 00:35:18,180
they do what we tell us to do

778
00:35:18,630 --> 00:35:22,480
they don't evoke empathy or they not

779
00:35:21,099 --> 00:35:24,339
have the that except they can evoke it

780
00:35:22,480 --> 00:35:25,540
remember I had top hat girl I really

781
00:35:24,339 --> 00:35:26,799
would love to know where she lives I've

782
00:35:25,540 --> 00:35:28,359
gotta find her again they want to say

783
00:35:26,800 --> 00:35:31,480
she's still wearing top hats and doing a

784
00:35:28,359 --> 00:35:37,410
potato box top hat awesome

785
00:35:31,480 --> 00:35:40,390
maybe top that girl critical thought

786
00:35:37,410 --> 00:35:44,348
humans are the only creatures that can

787
00:35:40,390 --> 00:35:47,890
truly think critically last but not

788
00:35:44,349 --> 00:35:51,059
least I own a machine that can fall in

789
00:35:47,890 --> 00:35:53,799
love you have to love what you do and

790
00:35:51,059 --> 00:35:55,210
the fact that we all love security

791
00:35:53,800 --> 00:35:56,770
because this one step ahead in

792
00:35:55,210 --> 00:35:59,740
understanding how we can harness the

793
00:35:56,770 --> 00:36:02,680
power of this technology so my final

794
00:35:59,740 --> 00:36:08,790
question is knowing all this is

795
00:36:02,680 --> 00:36:08,790
knowledge really still power thank you

796
00:36:09,260 --> 00:36:19,859
[Applause]

797
00:36:15,380 --> 00:36:23,029
entire presentation of a LinkedIn sorry

798
00:36:19,859 --> 00:36:23,029
technical difficulties

799
00:36:45,600 --> 00:36:48,600
one

800
00:36:53,080 --> 00:36:55,740
so

801
00:37:01,750 --> 00:37:07,520
so without going in like a commercial

802
00:37:04,040 --> 00:37:10,910
for my own company we do get access to

803
00:37:07,520 --> 00:37:13,370
the it was all 42 all 42 we prioritise

804
00:37:10,910 --> 00:37:14,720
for them so that you know what we see is

805
00:37:13,370 --> 00:37:16,490
most malicious and the things that our

806
00:37:14,720 --> 00:37:18,410
story that we thought this movie give

807
00:37:16,490 --> 00:37:19,970
access to the entire database the thing

808
00:37:18,410 --> 00:37:22,069
we do is because we don't ominous

809
00:37:19,970 --> 00:37:23,540
remediation we're looking for like the

810
00:37:22,070 --> 00:37:24,770
ransomware it's the only big stuff

811
00:37:23,540 --> 00:37:26,720
because we'd walk in and kill the

812
00:37:24,770 --> 00:37:28,250
connections immediately but we do have

813
00:37:26,720 --> 00:37:30,049
the ability to show everything because

814
00:37:28,250 --> 00:37:31,430
you're right there is that zero as much

815
00:37:30,050 --> 00:37:33,320
as you want to say scenarios right but

816
00:37:31,430 --> 00:37:35,690
also when it seems you know Trust in the

817
00:37:33,320 --> 00:37:37,250
sense that you don't want a machine

818
00:37:35,690 --> 00:37:42,710
doing everything and are willing to

819
00:37:37,250 --> 00:37:44,120
learn over time to get better right and

820
00:37:42,710 --> 00:37:45,860
that's I mean the one thing when I talk

821
00:37:44,120 --> 00:37:47,000
about artificial intelligence and my dev

822
00:37:45,860 --> 00:37:49,250
team when I talk about it all the time

823
00:37:47,000 --> 00:37:51,320
is there's a human developing and

824
00:37:49,250 --> 00:37:52,910
there's a huge train braking so it's a

825
00:37:51,320 --> 00:37:55,490
question on both sides of the fence

826
00:37:52,910 --> 00:37:57,140
which you smarter that day so you always

827
00:37:55,490 --> 00:37:58,729
want to be able to show type of all your

828
00:37:57,140 --> 00:38:00,890
River because there's always going to

829
00:37:58,730 --> 00:38:01,160
eat things that go through no matter

830
00:38:00,890 --> 00:38:05,049
what

831
00:38:01,160 --> 00:38:05,049
that's just the reality of situation

832
00:38:07,030 --> 00:38:10,089
[Music]

833
00:38:14,359 --> 00:38:25,348
or design catastrophic event that human

834
00:38:21,810 --> 00:38:26,930
lives so I mean the thing is is that

835
00:38:25,349 --> 00:38:32,550
there's already had a strong currents

836
00:38:26,930 --> 00:38:35,490
I'll use the Exxon pipeline Artemis BP

837
00:38:32,550 --> 00:38:37,230
actually in 2016 it was a camera system

838
00:38:35,490 --> 00:38:39,209
that was compromised through that was

839
00:38:37,230 --> 00:38:41,540
basically shadow IT they didn't think

840
00:38:39,210 --> 00:38:43,410
about the camera systems a

841
00:38:41,540 --> 00:38:48,119
government-sponsored hacking group

842
00:38:43,410 --> 00:38:49,230
identical pipeline there's already those

843
00:38:48,119 --> 00:38:51,869
types of things

844
00:38:49,230 --> 00:38:55,880
the uber example I gave what they

845
00:38:51,869 --> 00:38:59,790
figured out was that with biddable

846
00:38:55,880 --> 00:39:02,339
basically issued there was a great actor

847
00:38:59,790 --> 00:39:04,140
that was able to hack into ubering show

848
00:39:02,339 --> 00:39:06,839
how they would have been able to relate

849
00:39:04,140 --> 00:39:08,220
and get fifty thousand taxis or

850
00:39:06,839 --> 00:39:11,849
governors however many in one specific

851
00:39:08,220 --> 00:39:14,459
location while Obama's dead made another

852
00:39:11,849 --> 00:39:17,220
courtesy and all the emergency vehicles

853
00:39:14,460 --> 00:39:19,829
were not available to infer that men

854
00:39:17,220 --> 00:39:22,230
isn't always available so the thing is

855
00:39:19,829 --> 00:39:24,480
is that from a security perspective we

856
00:39:22,230 --> 00:39:26,640
need to raise awareness for that it's a

857
00:39:24,480 --> 00:39:28,619
question of accepting the idea of secure

858
00:39:26,640 --> 00:39:30,029
needed nor culture and you hear that

859
00:39:28,619 --> 00:39:31,650
it's there that these issues these

860
00:39:30,030 --> 00:39:34,380
catastrophic ideas are already there

861
00:39:31,650 --> 00:39:41,829
they kind of happen we just need to stay

862
00:39:34,380 --> 00:39:45,430
one step ahead export controls place

863
00:39:41,829 --> 00:39:54,130
that they get to do with i-4 controls on

864
00:39:45,430 --> 00:39:56,440
encryption possible yeah yeah so in the

865
00:39:54,130 --> 00:40:04,930
export control of my company because we

866
00:39:56,440 --> 00:40:07,380
aren't our IR texts inside UK any other

867
00:40:04,930 --> 00:40:07,379
questions

868
00:40:07,930 --> 00:40:12,000
thank you for your time guys we'll be

869
00:40:09,579 --> 00:40:12,000
doing it

870
00:40:12,530 --> 00:40:17,169
[Applause]

