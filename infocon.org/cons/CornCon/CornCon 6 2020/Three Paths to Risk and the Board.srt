1
00:00:00,160 --> 00:00:04,799
excellent well i'm going to start

2
00:00:02,879 --> 00:00:06,399
recording and we're going to start off

3
00:00:04,799 --> 00:00:09,519
this first track here

4
00:00:06,399 --> 00:00:12,480
it's a little after 9 46

5
00:00:09,519 --> 00:00:13,040
and we are broadcasting from the rebel

6
00:00:12,480 --> 00:00:15,920
base

7
00:00:13,040 --> 00:00:17,279
in north carolina here in charlotte

8
00:00:15,920 --> 00:00:18,960
where it's a beautiful day

9
00:00:17,279 --> 00:00:20,400
and i hope you guys are all enjoying a

10
00:00:18,960 --> 00:00:22,640
good day as well

11
00:00:20,400 --> 00:00:24,159
what i want to do here is uh start

12
00:00:22,640 --> 00:00:26,080
sharing the

13
00:00:24,160 --> 00:00:27,599
presentation that we've got for today

14
00:00:26,080 --> 00:00:31,119
we're going to talk a little bit about

15
00:00:27,599 --> 00:00:32,800
risk and talking to the board i'm

16
00:00:31,119 --> 00:00:34,640
pretty excited about this topic it's

17
00:00:32,800 --> 00:00:37,440
something i've been talking to

18
00:00:34,640 --> 00:00:38,640
a lot of people about uh over the last

19
00:00:37,440 --> 00:00:41,360
couple months is

20
00:00:38,640 --> 00:00:43,680
how do you talk about risk to the board

21
00:00:41,360 --> 00:00:45,280
and what's the right thing to do

22
00:00:43,680 --> 00:00:47,200
i wanted to talk a little bit about a

23
00:00:45,280 --> 00:00:49,600
couple of different aspects of that

24
00:00:47,200 --> 00:00:52,399
today

25
00:00:49,600 --> 00:00:54,079
so we'll get started here i'm going to

26
00:00:52,399 --> 00:00:56,000
tell you a quick story about how i

27
00:00:54,079 --> 00:00:57,520
first started thinking about risk this

28
00:00:56,000 --> 00:01:00,160
way talk about

29
00:00:57,520 --> 00:01:01,440
two perspectives from the cso and the

30
00:01:00,160 --> 00:01:03,680
cfo

31
00:01:01,440 --> 00:01:04,798
and where we are in kind of the history

32
00:01:03,680 --> 00:01:06,560
of things

33
00:01:04,799 --> 00:01:08,000
a couple of ways of talking about

34
00:01:06,560 --> 00:01:10,720
quantifying risk

35
00:01:08,000 --> 00:01:12,640
um we'll go through those what i'm

36
00:01:10,720 --> 00:01:13,360
talking about is the probability of a

37
00:01:12,640 --> 00:01:16,880
material

38
00:01:13,360 --> 00:01:18,960
impact uh is one method what i'm

39
00:01:16,880 --> 00:01:20,240
i'll call detection investigation

40
00:01:18,960 --> 00:01:22,320
response

41
00:01:20,240 --> 00:01:23,839
the fair method that probably a lot of

42
00:01:22,320 --> 00:01:25,758
you have heard of

43
00:01:23,840 --> 00:01:26,960
um and then we'll talk about a couple

44
00:01:25,759 --> 00:01:29,119
other things that i see

45
00:01:26,960 --> 00:01:30,479
and some key takeaways to just wrap

46
00:01:29,119 --> 00:01:33,680
everything up

47
00:01:30,479 --> 00:01:36,560
i'll uh try and keep an eye on the

48
00:01:33,680 --> 00:01:38,799
chat window as well and see if i can see

49
00:01:36,560 --> 00:01:42,479
anything else that's going on here

50
00:01:38,799 --> 00:01:42,479
uh as we go through today

51
00:01:43,360 --> 00:01:47,040
um let's see

52
00:01:47,759 --> 00:01:51,600
always fun getting used to the all the

53
00:01:50,560 --> 00:01:54,399
pieces here

54
00:01:51,600 --> 00:01:56,240
so my story i started uh working in

55
00:01:54,399 --> 00:01:58,240
information security at the national

56
00:01:56,240 --> 00:02:01,119
security agency and we used to build

57
00:01:58,240 --> 00:02:02,960
little green boxes to secure radio

58
00:02:01,119 --> 00:02:04,320
communications and then network

59
00:02:02,960 --> 00:02:06,640
communications

60
00:02:04,320 --> 00:02:08,478
and every time we designed and built a

61
00:02:06,640 --> 00:02:10,720
new box and started going

62
00:02:08,479 --> 00:02:12,560
going into production and distribution

63
00:02:10,720 --> 00:02:15,040
sharing it with the military

64
00:02:12,560 --> 00:02:16,480
we built a little graph like this that

65
00:02:15,040 --> 00:02:18,879
surprise surprise

66
00:02:16,480 --> 00:02:21,280
on one axis would talk about the

67
00:02:18,879 --> 00:02:23,280
likelihood of an event happening and on

68
00:02:21,280 --> 00:02:27,200
the other axis would talk about the

69
00:02:23,280 --> 00:02:29,760
impact of that happening and invariably

70
00:02:27,200 --> 00:02:31,839
we knew that there were some attacks

71
00:02:29,760 --> 00:02:34,000
against a device that might or might not

72
00:02:31,840 --> 00:02:36,959
have an impact that we could manage

73
00:02:34,000 --> 00:02:38,480
against with other controls that we can

74
00:02:36,959 --> 00:02:40,480
manage in the environment

75
00:02:38,480 --> 00:02:42,160
but the one thing that always stood out

76
00:02:40,480 --> 00:02:44,799
was there's always this

77
00:02:42,160 --> 00:02:45,840
one opportunity for a new zero day

78
00:02:44,800 --> 00:02:48,000
attack to show

79
00:02:45,840 --> 00:02:48,959
up and when some new zero day attack

80
00:02:48,000 --> 00:02:51,280
shows up

81
00:02:48,959 --> 00:02:53,040
you don't really know what to do with it

82
00:02:51,280 --> 00:02:55,200
so you have to

83
00:02:53,040 --> 00:02:56,239
figure out what are you how are you

84
00:02:55,200 --> 00:02:59,200
going to

85
00:02:56,239 --> 00:03:00,800
manage around that new unknown and it

86
00:02:59,200 --> 00:03:04,079
was interesting because

87
00:03:00,800 --> 00:03:06,640
we'd always have some the

88
00:03:04,080 --> 00:03:07,519
small possibility of a new big new

89
00:03:06,640 --> 00:03:09,760
unknown

90
00:03:07,519 --> 00:03:11,440
that would completely screw up any of

91
00:03:09,760 --> 00:03:13,200
the calculations that we were trying to

92
00:03:11,440 --> 00:03:17,359
do around risk

93
00:03:13,200 --> 00:03:20,399
and 25 years later that hasn't changed

94
00:03:17,360 --> 00:03:22,400
we still have to deal every day with

95
00:03:20,400 --> 00:03:24,959
knowing that some new random thing might

96
00:03:22,400 --> 00:03:28,400
happen that completely throws off

97
00:03:24,959 --> 00:03:28,799
everything we're doing but even with

98
00:03:28,400 --> 00:03:30,879
that

99
00:03:28,799 --> 00:03:33,040
said there are a lot of things that we

100
00:03:30,879 --> 00:03:34,959
can do and a lot of things that we want

101
00:03:33,040 --> 00:03:38,560
to do to continue to

102
00:03:34,959 --> 00:03:40,400
manage risk and and to talk to

103
00:03:38,560 --> 00:03:42,319
our boards and our executives and our

104
00:03:40,400 --> 00:03:45,599
leadership about that

105
00:03:42,319 --> 00:03:48,720
so the one thing that we can

106
00:03:45,599 --> 00:03:51,920
can talk about is a difference between

107
00:03:48,720 --> 00:03:55,519
the cfo and the cso so

108
00:03:51,920 --> 00:03:55,760
if you're the new cfo and you walk into

109
00:03:55,519 --> 00:03:59,120
an

110
00:03:55,760 --> 00:04:01,120
organization and you you

111
00:03:59,120 --> 00:04:02,640
already kind of know what a balance

112
00:04:01,120 --> 00:04:04,959
sheet looks like

113
00:04:02,640 --> 00:04:06,159
that balance sheet has been developed

114
00:04:04,959 --> 00:04:09,280
for almost

115
00:04:06,159 --> 00:04:11,280
7 000 years maybe longer since the

116
00:04:09,280 --> 00:04:14,319
ancient mesopotamians started

117
00:04:11,280 --> 00:04:15,840
keeping track of accounting we've been

118
00:04:14,319 --> 00:04:18,238
building to the point where

119
00:04:15,840 --> 00:04:20,798
cfo's job is pretty well defined the

120
00:04:18,238 --> 00:04:23,520
metrics that they use the way they talk

121
00:04:20,798 --> 00:04:24,159
the way they talk to the board and other

122
00:04:23,520 --> 00:04:27,359
managers

123
00:04:24,160 --> 00:04:28,000
is very well defined at the same time if

124
00:04:27,360 --> 00:04:32,479
you walk

125
00:04:28,000 --> 00:04:35,520
into a new csc iso

126
00:04:32,479 --> 00:04:37,520
the job there is completely undefined

127
00:04:35,520 --> 00:04:38,799
everyone i've walked almost everything

128
00:04:37,520 --> 00:04:42,159
you walked into

129
00:04:38,800 --> 00:04:43,919
um the cso has a new strategy and

130
00:04:42,160 --> 00:04:46,560
in fact i saw an article the other day

131
00:04:43,919 --> 00:04:49,680
that says every new cso who

132
00:04:46,560 --> 00:04:52,080
walks in stops all the activity

133
00:04:49,680 --> 00:04:53,680
builds their own strategy starts off in

134
00:04:52,080 --> 00:04:56,479
a new direction and of course on

135
00:04:53,680 --> 00:04:57,360
average something like three years later

136
00:04:56,479 --> 00:05:00,159
they leave

137
00:04:57,360 --> 00:05:00,880
a new cso comes in starts it all over

138
00:05:00,160 --> 00:05:03,520
again

139
00:05:00,880 --> 00:05:05,120
and they all build different ideas about

140
00:05:03,520 --> 00:05:06,159
how they talk about things what they

141
00:05:05,120 --> 00:05:09,039
should talk about

142
00:05:06,160 --> 00:05:09,440
and talking about the program or talking

143
00:05:09,039 --> 00:05:12,159
about

144
00:05:09,440 --> 00:05:14,080
risk it is unfortunately no different we

145
00:05:12,160 --> 00:05:17,440
have a lot of different ways

146
00:05:14,080 --> 00:05:19,520
to talk about this so as i've been

147
00:05:17,440 --> 00:05:20,639
thinking about this and talking to

148
00:05:19,520 --> 00:05:22,799
people

149
00:05:20,639 --> 00:05:24,160
i've noticed that a couple of things

150
00:05:22,800 --> 00:05:27,039
really start to happen

151
00:05:24,160 --> 00:05:28,479
one is you want to think about how you

152
00:05:27,039 --> 00:05:31,599
communicate risk

153
00:05:28,479 --> 00:05:32,560
from the perspective of relevance to the

154
00:05:31,600 --> 00:05:35,120
board

155
00:05:32,560 --> 00:05:36,000
and you want to talk about risk from the

156
00:05:35,120 --> 00:05:40,639
perspective

157
00:05:36,000 --> 00:05:44,560
of relevance to um

158
00:05:40,639 --> 00:05:45,199
to the uh solutions that you're going to

159
00:05:44,560 --> 00:05:48,000
present

160
00:05:45,199 --> 00:05:49,440
so if you're building a risk discussion

161
00:05:48,000 --> 00:05:50,960
you want to be able to say is it

162
00:05:49,440 --> 00:05:53,840
relevant to the board

163
00:05:50,960 --> 00:05:54,479
and does it help prescribe what i need

164
00:05:53,840 --> 00:05:56,638
to do

165
00:05:54,479 --> 00:05:58,318
to fix things what do i need to do to

166
00:05:56,639 --> 00:06:02,400
make things better

167
00:05:58,319 --> 00:06:04,880
and as i put the the ideas and solutions

168
00:06:02,400 --> 00:06:06,960
i've been talking about with people

169
00:06:04,880 --> 00:06:09,360
on this kind of chart i find that

170
00:06:06,960 --> 00:06:12,960
there's some interesting things

171
00:06:09,360 --> 00:06:15,600
one is that talking about the material

172
00:06:12,960 --> 00:06:16,880
risk and forecasting risk for the next

173
00:06:15,600 --> 00:06:18,880
three years

174
00:06:16,880 --> 00:06:20,479
really good and relevant to the board

175
00:06:18,880 --> 00:06:22,960
really good numbers

176
00:06:20,479 --> 00:06:26,240
but not necessarily very prescriptive

177
00:06:22,960 --> 00:06:29,039
about what you should do to fix things

178
00:06:26,240 --> 00:06:31,280
talking about the mean time to detect

179
00:06:29,039 --> 00:06:32,880
the mean time to investigate the mean

180
00:06:31,280 --> 00:06:34,719
time to respond

181
00:06:32,880 --> 00:06:36,400
to some event or incident that your

182
00:06:34,720 --> 00:06:39,280
systems detect

183
00:06:36,400 --> 00:06:39,758
is a really good way of starting to look

184
00:06:39,280 --> 00:06:42,719
at

185
00:06:39,759 --> 00:06:44,639
how all of the program comes together

186
00:06:42,720 --> 00:06:47,199
how all the pieces fit

187
00:06:44,639 --> 00:06:48,319
but it doesn't necessarily tell you if

188
00:06:47,199 --> 00:06:50,400
you do this

189
00:06:48,319 --> 00:06:52,240
the mean time to investigate will go

190
00:06:50,400 --> 00:06:55,520
down if you do this the mean time

191
00:06:52,240 --> 00:06:58,080
will respond to go down it leaves open

192
00:06:55,520 --> 00:06:58,960
kind of where you can go from here if

193
00:06:58,080 --> 00:07:01,680
you go

194
00:06:58,960 --> 00:07:03,680
through the fair process which is a lot

195
00:07:01,680 --> 00:07:06,400
more intensive and and some of you

196
00:07:03,680 --> 00:07:08,319
are probably doing that fair is a lot

197
00:07:06,400 --> 00:07:10,638
more prescriptive because you've

198
00:07:08,319 --> 00:07:13,039
captured risk in a robust way because

199
00:07:10,639 --> 00:07:15,360
you put quantitative numbers around it

200
00:07:13,039 --> 00:07:16,159
you can very quickly start to use fair

201
00:07:15,360 --> 00:07:19,360
to say

202
00:07:16,160 --> 00:07:20,560
here's a high risk something that has a

203
00:07:19,360 --> 00:07:23,360
high likelihood of

204
00:07:20,560 --> 00:07:24,240
impact and a high impact when it does

205
00:07:23,360 --> 00:07:26,560
and i can

206
00:07:24,240 --> 00:07:27,360
directly then say that's a risk that i

207
00:07:26,560 --> 00:07:29,840
want to

208
00:07:27,360 --> 00:07:32,400
put controls around and i can be very

209
00:07:29,840 --> 00:07:35,758
prescriptive about what to do with it

210
00:07:32,400 --> 00:07:39,280
um two other things that often come out

211
00:07:35,759 --> 00:07:40,800
the 10k and if you've never read a 10k

212
00:07:39,280 --> 00:07:42,559
you should certainly look at your own

213
00:07:40,800 --> 00:07:46,240
company's 10k

214
00:07:42,560 --> 00:07:47,840
filing with the sec the 10k talks about

215
00:07:46,240 --> 00:07:51,120
risks that the organization

216
00:07:47,840 --> 00:07:53,198
fakes faces at a very executive level

217
00:07:51,120 --> 00:07:54,800
um and sometimes those they'll talk a

218
00:07:53,199 --> 00:07:56,800
little bit about cyber risk and i'll

219
00:07:54,800 --> 00:07:58,639
show you an example in a minute there

220
00:07:56,800 --> 00:08:00,479
again the problem with that is it really

221
00:07:58,639 --> 00:08:03,599
doesn't help you say what to do to fix

222
00:08:00,479 --> 00:08:04,080
things it just says there's a risk and

223
00:08:03,599 --> 00:08:06,479
then

224
00:08:04,080 --> 00:08:08,000
a lot of companies build very detailed

225
00:08:06,479 --> 00:08:09,840
risk registers

226
00:08:08,000 --> 00:08:12,000
and those very detailed risk registers

227
00:08:09,840 --> 00:08:14,318
can be very very prescriptive

228
00:08:12,000 --> 00:08:16,080
but they're almost completely irrelevant

229
00:08:14,319 --> 00:08:17,599
to the board because there's so much

230
00:08:16,080 --> 00:08:19,120
noise in them

231
00:08:17,599 --> 00:08:21,199
so many things that you can talk to

232
00:08:19,120 --> 00:08:22,639
about that so

233
00:08:21,199 --> 00:08:24,639
let's talk about a couple of these

234
00:08:22,639 --> 00:08:27,680
methodologies

235
00:08:24,639 --> 00:08:30,960
rick howard has been talking a lot about

236
00:08:27,680 --> 00:08:34,159
this probability of a material impact

237
00:08:30,960 --> 00:08:36,240
in the next three years he's given a

238
00:08:34,159 --> 00:08:36,880
couple talks at the rsa conference about

239
00:08:36,240 --> 00:08:40,399
it

240
00:08:36,880 --> 00:08:44,240
um and it it really is a pretty cool

241
00:08:40,399 --> 00:08:46,959
idea and concept that actually the math

242
00:08:44,240 --> 00:08:47,680
brought that drives it really makes it

243
00:08:46,959 --> 00:08:49,439
work

244
00:08:47,680 --> 00:08:51,120
so that you can build these lost

245
00:08:49,440 --> 00:08:53,839
exceedance curves

246
00:08:51,120 --> 00:08:54,240
and talk about what your big risks look

247
00:08:53,839 --> 00:08:57,120
like

248
00:08:54,240 --> 00:08:57,920
and the risk and likelihood of an impact

249
00:08:57,120 --> 00:09:00,560
and this could be

250
00:08:57,920 --> 00:09:02,640
built from a small number of risks or it

251
00:09:00,560 --> 00:09:03,199
could actually be built from a complete

252
00:09:02,640 --> 00:09:05,760
set of

253
00:09:03,200 --> 00:09:08,240
fair type risks very comprehensive very

254
00:09:05,760 --> 00:09:10,080
extensive

255
00:09:08,240 --> 00:09:12,399
you use your internal experts to

256
00:09:10,080 --> 00:09:14,959
estimate the probability

257
00:09:12,399 --> 00:09:15,839
of an impact your confidence in that

258
00:09:14,959 --> 00:09:19,040
probability

259
00:09:15,839 --> 00:09:20,080
and the impact and then use the math of

260
00:09:19,040 --> 00:09:22,319
bayes theorem

261
00:09:20,080 --> 00:09:24,240
and computers to do monte carlo

262
00:09:22,320 --> 00:09:27,360
simulations to build out these

263
00:09:24,240 --> 00:09:28,000
lost exceedance curves and you get to

264
00:09:27,360 --> 00:09:30,959
see

265
00:09:28,000 --> 00:09:32,560
how everything works i'm going to stop

266
00:09:30,959 --> 00:09:34,959
sharing for just a minute because i

267
00:09:32,560 --> 00:09:37,439
haven't figured out how to there we go

268
00:09:34,959 --> 00:09:38,319
ah just making sure i've got no open

269
00:09:37,440 --> 00:09:41,440
questions

270
00:09:38,320 --> 00:09:44,720
um so i'll get uh

271
00:09:41,440 --> 00:09:46,800
questions ready ready here to see um

272
00:09:44,720 --> 00:09:48,560
if if you have any questions please feel

273
00:09:46,800 --> 00:09:50,880
free to jump in

274
00:09:48,560 --> 00:09:53,119
um but the ideas here are pretty good

275
00:09:50,880 --> 00:09:53,600
and it gives you a way of really mapping

276
00:09:53,120 --> 00:09:55,760
something

277
00:09:53,600 --> 00:09:56,959
and you can look here and very quickly

278
00:09:55,760 --> 00:10:00,399
say um

279
00:09:56,959 --> 00:10:02,399
for a given set of risks we either

280
00:10:00,399 --> 00:10:04,800
believe that the risk tolerance

281
00:10:02,399 --> 00:10:06,480
is okay we're willing to accept the

282
00:10:04,800 --> 00:10:07,920
million maybe a little more than a

283
00:10:06,480 --> 00:10:10,880
million dollar

284
00:10:07,920 --> 00:10:12,719
uh dollars of impact to the organization

285
00:10:10,880 --> 00:10:15,920
in the next three years

286
00:10:12,720 --> 00:10:19,279
and as the probability goes down

287
00:10:15,920 --> 00:10:21,439
or risk is reduced i can stay under that

288
00:10:19,279 --> 00:10:22,880
risk tolerance curve or

289
00:10:21,440 --> 00:10:25,200
find things that are above the risk

290
00:10:22,880 --> 00:10:26,160
tolerance curve provides a very robust

291
00:10:25,200 --> 00:10:28,320
way of thinking

292
00:10:26,160 --> 00:10:29,279
thinking three things that you can talk

293
00:10:28,320 --> 00:10:32,079
to the board

294
00:10:29,279 --> 00:10:35,600
about and have a good conversation

295
00:10:32,079 --> 00:10:38,880
without them having to be cyber experts

296
00:10:35,600 --> 00:10:41,440
um the next thing uh and this is

297
00:10:38,880 --> 00:10:42,480
uh something that dimitri alperovitch

298
00:10:41,440 --> 00:10:44,480
has talked about

299
00:10:42,480 --> 00:10:46,240
talked to people about and shared with

300
00:10:44,480 --> 00:10:50,000
me

301
00:10:46,240 --> 00:10:53,440
is this mean time to detect mean time to

302
00:10:50,000 --> 00:10:55,360
investigate and mean time to respond

303
00:10:53,440 --> 00:10:56,880
this is something that a lot of us have

304
00:10:55,360 --> 00:10:59,120
been using as a metric

305
00:10:56,880 --> 00:11:01,120
for looking at how well our security

306
00:10:59,120 --> 00:11:03,040
operations center works

307
00:11:01,120 --> 00:11:04,399
but when you put it together put

308
00:11:03,040 --> 00:11:06,880
together the detect

309
00:11:04,399 --> 00:11:08,720
investigate and respond numbers it

310
00:11:06,880 --> 00:11:09,439
actually gives you a pretty good picture

311
00:11:08,720 --> 00:11:12,959
of the whole

312
00:11:09,440 --> 00:11:14,480
program it but it also misses

313
00:11:12,959 --> 00:11:16,239
things just a little bit if you think

314
00:11:14,480 --> 00:11:18,560
about that in this cyber security

315
00:11:16,240 --> 00:11:21,440
framework that we all know and love

316
00:11:18,560 --> 00:11:23,599
there's that opportunity to identify and

317
00:11:21,440 --> 00:11:26,160
focus on the crown jewels

318
00:11:23,600 --> 00:11:28,079
not on detecting that everything is

319
00:11:26,160 --> 00:11:30,959
protected but detecting that the most

320
00:11:28,079 --> 00:11:33,359
important things are protected

321
00:11:30,959 --> 00:11:36,479
establishing the controls

322
00:11:33,360 --> 00:11:38,640
it really doesn't this this mechanism

323
00:11:36,480 --> 00:11:40,560
doesn't directly tell you whether or not

324
00:11:38,640 --> 00:11:42,319
you have the right mix of controls do

325
00:11:40,560 --> 00:11:43,359
you have controls every place you could

326
00:11:42,320 --> 00:11:46,240
or should

327
00:11:43,360 --> 00:11:47,920
um but it does give you an indirect

328
00:11:46,240 --> 00:11:49,839
measure whether or not your protective

329
00:11:47,920 --> 00:11:51,439
measures are working

330
00:11:49,839 --> 00:11:53,440
because obviously somebody has to get

331
00:11:51,440 --> 00:11:54,320
through those before you can detect an

332
00:11:53,440 --> 00:11:56,480
event

333
00:11:54,320 --> 00:11:58,480
and then you can build on that um to

334
00:11:56,480 --> 00:11:59,360
think about red team and adversary

335
00:11:58,480 --> 00:12:01,600
activity

336
00:11:59,360 --> 00:12:02,399
you can use those to actually get some

337
00:12:01,600 --> 00:12:04,240
numbers so

338
00:12:02,399 --> 00:12:05,519
even if your adversaries aren't getting

339
00:12:04,240 --> 00:12:06,639
through or you're not seeing the

340
00:12:05,519 --> 00:12:08,959
detection of them

341
00:12:06,639 --> 00:12:10,959
you can see what your ability to detect

342
00:12:08,959 --> 00:12:14,000
the red team is

343
00:12:10,959 --> 00:12:16,560
and use that investigation time

344
00:12:14,000 --> 00:12:18,959
obviously a big piece of starting a

345
00:12:16,560 --> 00:12:22,160
response to a detected event

346
00:12:18,959 --> 00:12:23,839
and then responding to those events

347
00:12:22,160 --> 00:12:25,760
you can start to measure how long it

348
00:12:23,839 --> 00:12:27,519
takes you to measure those

349
00:12:25,760 --> 00:12:29,760
one of the interesting side discussions

350
00:12:27,519 --> 00:12:30,720
that i've had while i was investigating

351
00:12:29,760 --> 00:12:34,319
that

352
00:12:30,720 --> 00:12:34,880
was um actually the question of would i

353
00:12:34,320 --> 00:12:37,839
look at

354
00:12:34,880 --> 00:12:39,920
average time or mean time to detect what

355
00:12:37,839 --> 00:12:41,360
i look at 90th percentile

356
00:12:39,920 --> 00:12:43,199
recognizing that there are always going

357
00:12:41,360 --> 00:12:45,920
to be some events that

358
00:12:43,200 --> 00:12:47,519
take a lot longer to detect because of

359
00:12:45,920 --> 00:12:50,800
an adversary's approach

360
00:12:47,519 --> 00:12:52,720
or that it may take me longer to respond

361
00:12:50,800 --> 00:12:54,479
to certain kinds of events but those

362
00:12:52,720 --> 00:12:55,519
aren't the events that i have every day

363
00:12:54,480 --> 00:12:58,160
those are the

364
00:12:55,519 --> 00:13:00,639
weird events that happen once a year or

365
00:12:58,160 --> 00:13:02,160
less than once a year

366
00:13:00,639 --> 00:13:05,040
the other thing you can do with this of

367
00:13:02,160 --> 00:13:07,519
course is use the miter attack framework

368
00:13:05,040 --> 00:13:10,079
and kind of work your way through at

369
00:13:07,519 --> 00:13:12,160
least a paper exercise to say

370
00:13:10,079 --> 00:13:14,319
how long would it take me to detect each

371
00:13:12,160 --> 00:13:16,079
of the attack scenarios in mitre how

372
00:13:14,320 --> 00:13:16,959
long would it take me to investigate

373
00:13:16,079 --> 00:13:19,839
those

374
00:13:16,959 --> 00:13:21,680
what tools do i need to add to protect

375
00:13:19,839 --> 00:13:24,720
detect and respond

376
00:13:21,680 --> 00:13:26,800
to each of the opportunities that could

377
00:13:24,720 --> 00:13:27,839
be identified by using the miter attack

378
00:13:26,800 --> 00:13:31,040
framework

379
00:13:27,839 --> 00:13:34,079
so some really interesting things there

380
00:13:31,040 --> 00:13:35,839
the last major way of talking about

381
00:13:34,079 --> 00:13:37,120
risk and building something that i think

382
00:13:35,839 --> 00:13:39,120
you can take to the board

383
00:13:37,120 --> 00:13:41,120
is the fair approach and the fair

384
00:13:39,120 --> 00:13:43,839
institute and jack johnson

385
00:13:41,120 --> 00:13:44,720
um have been out there for a long time

386
00:13:43,839 --> 00:13:47,519
jack jones

387
00:13:44,720 --> 00:13:48,000
have been out there for a long time

388
00:13:47,519 --> 00:13:50,560
talking

389
00:13:48,000 --> 00:13:51,360
about and building out this risk

390
00:13:50,560 --> 00:13:54,239
approach

391
00:13:51,360 --> 00:13:54,639
with the idea of analyzing the factors

392
00:13:54,240 --> 00:13:58,560
that

393
00:13:54,639 --> 00:14:01,519
address what's the last event frequency

394
00:13:58,560 --> 00:14:03,680
what's the lost magnitude and how do

395
00:14:01,519 --> 00:14:04,800
those things come together from known

396
00:14:03,680 --> 00:14:06,479
information

397
00:14:04,800 --> 00:14:08,399
so you can build a better more

398
00:14:06,480 --> 00:14:11,760
quantitative um

399
00:14:08,399 --> 00:14:13,279
or deeply qualitative view of what risk

400
00:14:11,760 --> 00:14:15,920
looks like

401
00:14:13,279 --> 00:14:17,040
and then break that down or build it up

402
00:14:15,920 --> 00:14:20,079
from the bottom

403
00:14:17,040 --> 00:14:22,240
around specific risks that you identify

404
00:14:20,079 --> 00:14:23,760
and the idea that i see in fear that's

405
00:14:22,240 --> 00:14:26,560
interesting is that

406
00:14:23,760 --> 00:14:28,880
you don't try and enumerate every single

407
00:14:26,560 --> 00:14:32,399
possible risk you try and look at it

408
00:14:28,880 --> 00:14:32,959
systemically across the organization

409
00:14:32,399 --> 00:14:34,720
around

410
00:14:32,959 --> 00:14:36,638
crown jewels around the important

411
00:14:34,720 --> 00:14:39,360
systems and look at

412
00:14:36,639 --> 00:14:40,959
specific cyber risks that you articulate

413
00:14:39,360 --> 00:14:44,639
and a little bit of the art of

414
00:14:40,959 --> 00:14:47,760
fair as as i see it is in finding the

415
00:14:44,639 --> 00:14:49,760
right way to articulate specific risks

416
00:14:47,760 --> 00:14:52,319
and articulating those risks in a very

417
00:14:49,760 --> 00:14:55,360
consistent way

418
00:14:52,320 --> 00:14:57,279
fair really tries to use a specific

419
00:14:55,360 --> 00:14:59,120
methodology for the way that you

420
00:14:57,279 --> 00:15:01,760
articulate risks and

421
00:14:59,120 --> 00:15:02,320
um that's an area that i think anyone

422
00:15:01,760 --> 00:15:05,600
here can

423
00:15:02,320 --> 00:15:07,760
investigate and go deeper into

424
00:15:05,600 --> 00:15:10,160
those are the three big methods that i

425
00:15:07,760 --> 00:15:13,279
found where there are really different

426
00:15:10,160 --> 00:15:14,240
approaches to thinking about risk all of

427
00:15:13,279 --> 00:15:16,399
them

428
00:15:14,240 --> 00:15:17,680
and communicating that risk to the board

429
00:15:16,399 --> 00:15:20,800
all of them have some

430
00:15:17,680 --> 00:15:22,399
interesting aspects all of them have

431
00:15:20,800 --> 00:15:23,920
different levels of ease of

432
00:15:22,399 --> 00:15:25,839
implementation there's

433
00:15:23,920 --> 00:15:27,519
fast track things you can do things that

434
00:15:25,839 --> 00:15:29,199
take more time

435
00:15:27,519 --> 00:15:31,360
but they all provide a different

436
00:15:29,199 --> 00:15:33,359
approach the other thing that i want to

437
00:15:31,360 --> 00:15:35,600
talk about is the two other things that

438
00:15:33,360 --> 00:15:38,560
you sometimes see

439
00:15:35,600 --> 00:15:40,160
as a ways of talking about risk i'm not

440
00:15:38,560 --> 00:15:41,119
going to name them but this is the

441
00:15:40,160 --> 00:15:43,759
actual

442
00:15:41,120 --> 00:15:45,519
statement about cyber risk from a

443
00:15:43,759 --> 00:15:48,800
fortune

444
00:15:45,519 --> 00:15:50,720
50 company um and it

445
00:15:48,800 --> 00:15:52,399
talks about cyber risk in kind of an

446
00:15:50,720 --> 00:15:53,279
interesting way i'm not going to read it

447
00:15:52,399 --> 00:15:56,639
but

448
00:15:53,279 --> 00:15:59,279
it represents different kinds of

449
00:15:56,639 --> 00:16:01,920
failures or breaches and security it

450
00:15:59,279 --> 00:16:04,240
talks about malicious attacks

451
00:16:01,920 --> 00:16:05,920
it talks about the core confidentiality

452
00:16:04,240 --> 00:16:09,279
availability integrity that we

453
00:16:05,920 --> 00:16:12,160
always talk about with systems

454
00:16:09,279 --> 00:16:14,639
it talks about getting to sensitive data

455
00:16:12,160 --> 00:16:18,079
corporate and customer information

456
00:16:14,639 --> 00:16:20,160
um it talks about enabling preventative

457
00:16:18,079 --> 00:16:22,399
detective and responsive measures so

458
00:16:20,160 --> 00:16:25,519
picks up some of the the

459
00:16:22,399 --> 00:16:26,800
nist cyber security framework talks

460
00:16:25,519 --> 00:16:28,959
about an enterprise

461
00:16:26,800 --> 00:16:31,040
risk committee and a board and a global

462
00:16:28,959 --> 00:16:34,479
information security team

463
00:16:31,040 --> 00:16:36,480
and the the interesting thing here is

464
00:16:34,480 --> 00:16:38,399
i think you could cut and paste this

465
00:16:36,480 --> 00:16:42,160
into the 10k

466
00:16:38,399 --> 00:16:45,360
for every single company i've ever

467
00:16:42,160 --> 00:16:46,399
worked with um it's just a a

468
00:16:45,360 --> 00:16:49,360
fascinatingly

469
00:16:46,399 --> 00:16:51,519
bland risk statement and it's something

470
00:16:49,360 --> 00:16:54,000
that you could take to the board

471
00:16:51,519 --> 00:16:55,440
but it doesn't really help the board to

472
00:16:54,000 --> 00:16:57,360
understand

473
00:16:55,440 --> 00:16:58,800
is our program in control are we

474
00:16:57,360 --> 00:17:00,720
spending the right amount of money

475
00:16:58,800 --> 00:17:02,479
are we doing the right things what else

476
00:17:00,720 --> 00:17:06,079
do we need to do

477
00:17:02,480 --> 00:17:08,880
so um all of all these approaches can be

478
00:17:06,079 --> 00:17:08,879
very interesting

479
00:17:10,640 --> 00:17:19,600
whoops so um

480
00:17:13,679 --> 00:17:21,919
anyway um oops

481
00:17:19,599 --> 00:17:21,918
ah

482
00:17:22,880 --> 00:17:27,679
wanted to love the q a thing it just got

483
00:17:26,720 --> 00:17:30,799
me flustered here

484
00:17:27,679 --> 00:17:34,160
so this 10k risk is very interesting

485
00:17:30,799 --> 00:17:35,840
in the approach that it gives you um

486
00:17:34,160 --> 00:17:37,360
to talking about risk at a high level

487
00:17:35,840 --> 00:17:39,280
but again doesn't really

488
00:17:37,360 --> 00:17:41,520
go into much detail and give you much

489
00:17:39,280 --> 00:17:43,440
too much meat to chew on

490
00:17:41,520 --> 00:17:44,720
the other thing that you see a lot and a

491
00:17:43,440 --> 00:17:47,440
lot of companies have this

492
00:17:44,720 --> 00:17:49,280
is some kind of risk a new enumeration

493
00:17:47,440 --> 00:17:51,440
or a risk register

494
00:17:49,280 --> 00:17:53,200
um it's funny if you google risk

495
00:17:51,440 --> 00:17:55,039
register how many templates are

496
00:17:53,200 --> 00:17:58,799
available for download

497
00:17:55,039 --> 00:18:00,559
for excel word and other capabilities

498
00:17:58,799 --> 00:18:02,720
to build out a risk register and of

499
00:18:00,559 --> 00:18:04,080
course that risk register is a key

500
00:18:02,720 --> 00:18:07,120
function of almost ever

501
00:18:04,080 --> 00:18:07,840
of literally of every grc tool that you

502
00:18:07,120 --> 00:18:10,000
find

503
00:18:07,840 --> 00:18:11,678
it lets you capture description of risk

504
00:18:10,000 --> 00:18:12,559
it lets you capture something about the

505
00:18:11,679 --> 00:18:16,160
likelihood and the

506
00:18:12,559 --> 00:18:16,960
impact severity owners mitigating

507
00:18:16,160 --> 00:18:19,200
actions

508
00:18:16,960 --> 00:18:21,280
it's a good way of tracking risk at a

509
00:18:19,200 --> 00:18:24,000
very detailed level

510
00:18:21,280 --> 00:18:25,678
it's useful within the information

511
00:18:24,000 --> 00:18:28,000
security organization

512
00:18:25,679 --> 00:18:29,200
it's useful when talking to i t

513
00:18:28,000 --> 00:18:30,960
organizations or

514
00:18:29,200 --> 00:18:32,880
business organizations and capturing

515
00:18:30,960 --> 00:18:35,440
their individual risks

516
00:18:32,880 --> 00:18:37,520
and tracking those through from initial

517
00:18:35,440 --> 00:18:39,679
identification of the risk

518
00:18:37,520 --> 00:18:41,520
through the process to closing out the

519
00:18:39,679 --> 00:18:44,640
risk and and getting it to

520
00:18:41,520 --> 00:18:48,559
an acceptable level through mitigation

521
00:18:44,640 --> 00:18:50,720
through um moving transferring risk

522
00:18:48,559 --> 00:18:52,799
through implementing additional controls

523
00:18:50,720 --> 00:18:54,960
or or just agreeing that it's within

524
00:18:52,799 --> 00:18:58,000
tolerance and you can accept it

525
00:18:54,960 --> 00:19:00,960
so uh the risk register is

526
00:18:58,000 --> 00:19:01,919
useful as a tool for working level

527
00:19:00,960 --> 00:19:03,520
activity

528
00:19:01,919 --> 00:19:05,120
but it's not really something you can

529
00:19:03,520 --> 00:19:07,760
take to the board and

530
00:19:05,120 --> 00:19:08,840
summary numbers from this oh we we're

531
00:19:07,760 --> 00:19:13,600
tracking

532
00:19:08,840 --> 00:19:15,439
5250 risks and 119 of them are high

533
00:19:13,600 --> 00:19:16,799
doesn't really tell the board anything

534
00:19:15,440 --> 00:19:18,960
useful so

535
00:19:16,799 --> 00:19:20,320
really useful for tracking and managing

536
00:19:18,960 --> 00:19:22,640
risk at a working level

537
00:19:20,320 --> 00:19:25,760
but not very useful for taking anything

538
00:19:22,640 --> 00:19:29,200
on up

539
00:19:25,760 --> 00:19:31,039
so with that let me flip to just a

540
00:19:29,200 --> 00:19:32,640
couple of other things

541
00:19:31,039 --> 00:19:35,200
one of the other things that comes up

542
00:19:32,640 --> 00:19:38,559
when i have risk discussions with people

543
00:19:35,200 --> 00:19:40,480
is measurements and metrics what metrics

544
00:19:38,559 --> 00:19:41,918
should you be taking to the board

545
00:19:40,480 --> 00:19:44,000
how are you how do you know that you've

546
00:19:41,919 --> 00:19:45,760
got the right things to go to the board

547
00:19:44,000 --> 00:19:47,760
are you taking the right things to the

548
00:19:45,760 --> 00:19:50,480
board um

549
00:19:47,760 --> 00:19:51,520
metrics is a whole separate discussion

550
00:19:50,480 --> 00:19:54,640
but i think

551
00:19:51,520 --> 00:19:57,679
measuring risk and talking about risk

552
00:19:54,640 --> 00:19:59,919
in the right way is critical to having

553
00:19:57,679 --> 00:20:02,640
good conversations with the board

554
00:19:59,919 --> 00:20:03,520
about what you are and aren't doing to

555
00:20:02,640 --> 00:20:06,159
manage

556
00:20:03,520 --> 00:20:07,520
the risks that are appropriate for the

557
00:20:06,159 --> 00:20:09,280
organization

558
00:20:07,520 --> 00:20:11,120
at a level that you can talk to the

559
00:20:09,280 --> 00:20:14,799
board about to help them

560
00:20:11,120 --> 00:20:16,959
understand where you are in setting up a

561
00:20:14,799 --> 00:20:19,918
program expectations

562
00:20:16,960 --> 00:20:20,799
and how the program is addressing the

563
00:20:19,919 --> 00:20:23,039
likelier

564
00:20:20,799 --> 00:20:24,799
risks and the ability of the

565
00:20:23,039 --> 00:20:26,720
organization to withstand

566
00:20:24,799 --> 00:20:28,240
an event and think about it thinking

567
00:20:26,720 --> 00:20:31,280
about it there from a response

568
00:20:28,240 --> 00:20:33,039
and recover perspective i i thought

569
00:20:31,280 --> 00:20:35,120
these two peter drucker quotes are

570
00:20:33,039 --> 00:20:38,158
really good because

571
00:20:35,120 --> 00:20:38,879
you have to be able to measure risk to

572
00:20:38,159 --> 00:20:41,200
improve it

573
00:20:38,880 --> 00:20:43,679
you have to think of risk as one of the

574
00:20:41,200 --> 00:20:46,400
measures that you share with the board

575
00:20:43,679 --> 00:20:47,679
that you talk about your information

576
00:20:46,400 --> 00:20:50,799
security program

577
00:20:47,679 --> 00:20:52,720
but risk isn't the only measure of an

578
00:20:50,799 --> 00:20:54,240
information security program and it's

579
00:20:52,720 --> 00:20:56,720
not the only thing that you would

580
00:20:54,240 --> 00:20:58,400
want to be talking to the board about

581
00:20:56,720 --> 00:21:01,280
the other thing here of course is just

582
00:20:58,400 --> 00:21:02,880
key right management is making sure

583
00:21:01,280 --> 00:21:04,399
everybody's doing the right things that

584
00:21:02,880 --> 00:21:06,720
you're doing

585
00:21:04,400 --> 00:21:07,840
that you're doing things right

586
00:21:06,720 --> 00:21:10,640
leadership

587
00:21:07,840 --> 00:21:11,678
is figuring out which are the right

588
00:21:10,640 --> 00:21:13,600
things to do

589
00:21:11,679 --> 00:21:14,960
and which are the right things to focus

590
00:21:13,600 --> 00:21:17,600
on so

591
00:21:14,960 --> 00:21:20,320
a good risk management system a good

592
00:21:17,600 --> 00:21:22,799
discussion of risk with the board

593
00:21:20,320 --> 00:21:24,879
and a mechanism from one of these

594
00:21:22,799 --> 00:21:26,080
choices or possibly something else that

595
00:21:24,880 --> 00:21:28,840
you build

596
00:21:26,080 --> 00:21:30,000
would help you have the right

597
00:21:28,840 --> 00:21:32,000
conversation

598
00:21:30,000 --> 00:21:34,080
to figure out what are the right things

599
00:21:32,000 --> 00:21:36,640
to do so that you can

600
00:21:34,080 --> 00:21:38,158
address risk i think this is one of the

601
00:21:36,640 --> 00:21:41,280
most important pieces

602
00:21:38,159 --> 00:21:42,080
is the the in presenting the information

603
00:21:41,280 --> 00:21:45,520
to the board

604
00:21:42,080 --> 00:21:48,879
shouldn't be a presentation it should be

605
00:21:45,520 --> 00:21:49,760
a re it should be a conversation with

606
00:21:48,880 --> 00:21:53,039
the board

607
00:21:49,760 --> 00:21:55,200
that helps them better understand the

608
00:21:53,039 --> 00:21:58,400
risk that you're working on

609
00:21:55,200 --> 00:22:00,080
and helps better unders helps you better

610
00:21:58,400 --> 00:22:01,280
understand what their concerns and

611
00:22:00,080 --> 00:22:03,600
priorities are

612
00:22:01,280 --> 00:22:05,918
it should be a dialogue not a

613
00:22:03,600 --> 00:22:08,719
presentation

614
00:22:05,919 --> 00:22:09,760
last thing here a couple of takeaways

615
00:22:08,720 --> 00:22:12,720
there are

616
00:22:09,760 --> 00:22:13,919
many ways to talk about risk probably

617
00:22:12,720 --> 00:22:17,520
too many ways

618
00:22:13,919 --> 00:22:19,200
to talk about risk i think it would be

619
00:22:17,520 --> 00:22:21,840
very helpful if we could all

620
00:22:19,200 --> 00:22:23,919
start to coalesce as an information

621
00:22:21,840 --> 00:22:26,959
security community and program

622
00:22:23,919 --> 00:22:28,240
on one or two methods that we use

623
00:22:26,960 --> 00:22:30,000
consistently

624
00:22:28,240 --> 00:22:31,760
i think we can use some different

625
00:22:30,000 --> 00:22:33,679
methods at different levels of the

626
00:22:31,760 --> 00:22:35,600
organization

627
00:22:33,679 --> 00:22:36,880
and i think it's important for us to

628
00:22:35,600 --> 00:22:40,000
start to coalesce

629
00:22:36,880 --> 00:22:42,559
to help make the cso a more

630
00:22:40,000 --> 00:22:43,679
professional more consistently c-level

631
00:22:42,559 --> 00:22:46,320
executive

632
00:22:43,679 --> 00:22:48,720
we've got to help build more consistency

633
00:22:46,320 --> 00:22:50,480
not so that it's easier for csas to move

634
00:22:48,720 --> 00:22:53,120
from one place to the next or

635
00:22:50,480 --> 00:22:54,080
it's easier for sysa to move in but so

636
00:22:53,120 --> 00:22:56,639
that it's easier

637
00:22:54,080 --> 00:22:57,199
for boards to consistently understand

638
00:22:56,640 --> 00:22:59,520
and set

639
00:22:57,200 --> 00:23:00,559
expectations what the csa should be

640
00:22:59,520 --> 00:23:02,879
bringing to them

641
00:23:00,559 --> 00:23:03,918
what we're bringing to the csa to the to

642
00:23:02,880 --> 00:23:06,240
the board

643
00:23:03,919 --> 00:23:08,080
so that we're able to talk more

644
00:23:06,240 --> 00:23:09,919
consistently about our programs with

645
00:23:08,080 --> 00:23:12,080
executives and business leaders

646
00:23:09,919 --> 00:23:13,440
outside of the information security

647
00:23:12,080 --> 00:23:15,360
community

648
00:23:13,440 --> 00:23:16,960
second of course organizations are all

649
00:23:15,360 --> 00:23:20,080
very different today

650
00:23:16,960 --> 00:23:23,520
we need to be a lot more consistent

651
00:23:20,080 --> 00:23:25,199
we need to be able to help people that

652
00:23:23,520 --> 00:23:27,679
aren't going to spend all their time

653
00:23:25,200 --> 00:23:29,280
learning the nuances of 50 new attacks

654
00:23:27,679 --> 00:23:32,400
this week

655
00:23:29,280 --> 00:23:35,520
understand how the program provides

656
00:23:32,400 --> 00:23:37,760
a risk management for the organization

657
00:23:35,520 --> 00:23:39,760
and helps the business take appropriate

658
00:23:37,760 --> 00:23:41,679
risks to grow and thrive

659
00:23:39,760 --> 00:23:43,200
in the in the business that that you're

660
00:23:41,679 --> 00:23:45,600
in um

661
00:23:43,200 --> 00:23:46,880
fourth i'd say go try one applying one

662
00:23:45,600 --> 00:23:50,000
of these methods

663
00:23:46,880 --> 00:23:52,640
um they're they're all good there's some

664
00:23:50,000 --> 00:23:54,720
value in all of them but i think if we

665
00:23:52,640 --> 00:23:57,200
can start to coalesce around a couple of

666
00:23:54,720 --> 00:24:00,880
methods that are used more consistently

667
00:23:57,200 --> 00:24:02,159
and we see people um having success

668
00:24:00,880 --> 00:24:03,679
and lessons learned through

669
00:24:02,159 --> 00:24:04,559
presentations like this and the

670
00:24:03,679 --> 00:24:07,279
application

671
00:24:04,559 --> 00:24:08,720
of of these risk methods that will start

672
00:24:07,279 --> 00:24:11,760
to

673
00:24:08,720 --> 00:24:13,120
find what works in more detail and what

674
00:24:11,760 --> 00:24:16,240
else is coming up

675
00:24:13,120 --> 00:24:18,559
and so fifth let me know what you learn

676
00:24:16,240 --> 00:24:20,240
i've been finding out a lot by talking

677
00:24:18,559 --> 00:24:22,720
to people and working with

678
00:24:20,240 --> 00:24:23,360
some in several individuals i see a lot

679
00:24:22,720 --> 00:24:25,520
of

680
00:24:23,360 --> 00:24:27,840
discussions of vendors thinking about

681
00:24:25,520 --> 00:24:29,840
how to better capture risk and think

682
00:24:27,840 --> 00:24:32,158
about risk management

683
00:24:29,840 --> 00:24:33,360
this is the opportunity for us to learn

684
00:24:32,159 --> 00:24:37,200
and find some things

685
00:24:33,360 --> 00:24:39,120
and continue sharing with the community

686
00:24:37,200 --> 00:24:41,039
the slides of course will be available

687
00:24:39,120 --> 00:24:42,639
afterwards you can reach out to me in a

688
00:24:41,039 --> 00:24:44,960
number of different ways

689
00:24:42,640 --> 00:24:48,080
i'm and at this point i'd like to open

690
00:24:44,960 --> 00:24:48,080
it up for questions

691
00:24:53,120 --> 00:24:57,520
hey todd thank you um let's see do we

692
00:24:56,159 --> 00:24:58,880
have if we have any questions please

693
00:24:57,520 --> 00:25:01,840
type them into the q a

694
00:24:58,880 --> 00:25:01,840
box

695
00:25:06,400 --> 00:25:13,039
well todd i don't see any

696
00:25:09,600 --> 00:25:16,240
any additional questions coming in but i

697
00:25:13,039 --> 00:25:19,039
i really appreciate uh uh your time

698
00:25:16,240 --> 00:25:21,200
today and i want to thank you for uh

699
00:25:19,039 --> 00:25:23,120
talking on a topic that i know is

700
00:25:21,200 --> 00:25:24,960
important to a lot of us uh thanks

701
00:25:23,120 --> 00:25:27,678
everyone for attending we will

702
00:25:24,960 --> 00:25:28,080
be starting up the next presentation at

703
00:25:27,679 --> 00:25:31,440
9

704
00:25:28,080 --> 00:25:43,840
30 central thank you todd thanks

705
00:25:31,440 --> 00:25:43,840
john pleasure to be here

706
00:25:49,919 --> 00:25:52,720
okay

