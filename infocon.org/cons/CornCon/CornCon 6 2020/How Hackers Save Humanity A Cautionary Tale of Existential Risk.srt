1
00:00:02,480 --> 00:00:07,359
it's a very good fit actually

2
00:00:04,319 --> 00:00:09,679
yeah but on the other side

3
00:00:07,359 --> 00:00:12,879
that's actually quite awesomely cool for

4
00:00:09,679 --> 00:00:12,880
this particular topic

5
00:00:14,400 --> 00:00:17,920
yeah we are we are recording

6
00:00:22,080 --> 00:00:24,560
all right

7
00:00:26,080 --> 00:00:30,000
all right i'm going to introduce stephen

8
00:00:27,599 --> 00:00:31,840
cobb um i've been wanting to get stephen

9
00:00:30,000 --> 00:00:33,200
cobb ever since we started corncon in

10
00:00:31,840 --> 00:00:35,040
2015.

11
00:00:33,200 --> 00:00:37,520
um with a name like steven cobb he's

12
00:00:35,040 --> 00:00:38,879
made for corncon aren't you stephen um

13
00:00:37,520 --> 00:00:41,360
i'm going to stop sharing and i'm going

14
00:00:38,879 --> 00:00:44,399
to let you uh put your slides up

15
00:00:41,360 --> 00:00:47,200
thank you so much for joining us from uh

16
00:00:44,399 --> 00:00:59,840
i'm glad to finally be here at corncon

17
00:00:47,200 --> 00:00:59,840
even though i'm not in iowa

18
00:01:16,640 --> 00:01:20,880
okay well unless somebody through some

19
00:01:20,400 --> 00:01:22,880
other

20
00:01:20,880 --> 00:01:24,080
means of communication tells me this is

21
00:01:22,880 --> 00:01:26,399
not working

22
00:01:24,080 --> 00:01:27,520
um i'm going to assume it's working

23
00:01:26,400 --> 00:01:31,119
looks great

24
00:01:27,520 --> 00:01:34,240
okay cool um

25
00:01:31,119 --> 00:01:37,119
right so uh good morning

26
00:01:34,240 --> 00:01:40,960
uh everyone and welcome to the first

27
00:01:37,119 --> 00:01:43,520
talk of the second day of corncon 2020.

28
00:01:40,960 --> 00:01:45,039
um it's actually afternoon where i am

29
00:01:43,520 --> 00:01:48,479
which is not in iowa but

30
00:01:45,040 --> 00:01:50,320
in england and uh

31
00:01:48,479 --> 00:01:52,320
i'd really rather be in iowa right now

32
00:01:50,320 --> 00:01:56,479
the weather here is kind of wet and

33
00:01:52,320 --> 00:01:59,360
miserable but this talk is about

34
00:01:56,479 --> 00:02:01,119
how hackers save humanity something i

35
00:01:59,360 --> 00:02:02,799
would call a cautionary tale of

36
00:02:01,119 --> 00:02:05,280
existential risk

37
00:02:02,799 --> 00:02:06,399
as i said i am stephen cobb this view

38
00:02:05,280 --> 00:02:10,160
here

39
00:02:06,399 --> 00:02:12,879
is the view of our earth from

40
00:02:10,160 --> 00:02:15,040
the moon but let's imagine for a moment

41
00:02:12,879 --> 00:02:17,280
that that's not the moon it's a huge

42
00:02:15,040 --> 00:02:22,000
asteroid headed towards

43
00:02:17,280 --> 00:02:25,840
planet earth so

44
00:02:22,000 --> 00:02:28,560
that could end badly um

45
00:02:25,840 --> 00:02:29,440
this is actually a screenshot from a

46
00:02:28,560 --> 00:02:30,959
movie

47
00:02:29,440 --> 00:02:32,560
greenland which is going to come out

48
00:02:30,959 --> 00:02:35,120
later this year

49
00:02:32,560 --> 00:02:36,080
and it's about a large heavenly body

50
00:02:35,120 --> 00:02:39,040
striking the

51
00:02:36,080 --> 00:02:40,080
earth and it looks actually pretty good

52
00:02:39,040 --> 00:02:42,879
in the trailer

53
00:02:40,080 --> 00:02:43,760
i just captured this from youtube but we

54
00:02:42,879 --> 00:02:47,200
know

55
00:02:43,760 --> 00:02:48,239
from pre history from our historic

56
00:02:47,200 --> 00:02:51,679
research

57
00:02:48,239 --> 00:02:53,519
that this can happen right 66 million

58
00:02:51,680 --> 00:02:56,959
years ago there was something called the

59
00:02:53,519 --> 00:02:59,680
cretaceous paleogene extinction event

60
00:02:56,959 --> 00:03:01,360
and i believe every tetrapod above the

61
00:02:59,680 --> 00:03:05,440
weight of 50 pounds

62
00:03:01,360 --> 00:03:06,959
went extinct as a result of that

63
00:03:05,440 --> 00:03:08,720
which is not a happy thing to think

64
00:03:06,959 --> 00:03:11,280
about and and this

65
00:03:08,720 --> 00:03:12,959
isn't perhaps a happy talk maybe it's

66
00:03:11,280 --> 00:03:13,599
best it's in the morning so we can get

67
00:03:12,959 --> 00:03:18,159
over it

68
00:03:13,599 --> 00:03:20,159
for the rest of the day but um let me

69
00:03:18,159 --> 00:03:21,840
just tell you quickly about myself

70
00:03:20,159 --> 00:03:24,079
my name is stephen cobb i wrote a

71
00:03:21,840 --> 00:03:27,920
security book back in 1991

72
00:03:24,080 --> 00:03:28,480
and i like corn um this is shaykh my

73
00:03:27,920 --> 00:03:31,440
partner

74
00:03:28,480 --> 00:03:32,000
she actually grows corn even in england

75
00:03:31,440 --> 00:03:34,560
and it grew

76
00:03:32,000 --> 00:03:36,239
pretty well this year she also wrote

77
00:03:34,560 --> 00:03:38,239
network security for dummies and knows

78
00:03:36,239 --> 00:03:39,360
just as much about security as i do if

79
00:03:38,239 --> 00:03:41,680
not more

80
00:03:39,360 --> 00:03:43,360
she also inspired my research into

81
00:03:41,680 --> 00:03:46,480
existential risk

82
00:03:43,360 --> 00:03:50,720
shay's long toyed with various uh

83
00:03:46,480 --> 00:03:53,439
apocalyptic scenarios in which uh people

84
00:03:50,720 --> 00:03:55,359
are sent back to a situation in in our

85
00:03:53,439 --> 00:03:58,400
existence where we have to for example

86
00:03:55,360 --> 00:04:00,159
grow our own corn and we actually grew

87
00:03:58,400 --> 00:04:02,239
our own corn this year because

88
00:04:00,159 --> 00:04:04,399
of something that happened which was not

89
00:04:02,239 --> 00:04:08,480
i don't think an existential

90
00:04:04,400 --> 00:04:10,640
uh event but the covert 19 pandemic

91
00:04:08,480 --> 00:04:11,840
which reminded us all that there's

92
00:04:10,640 --> 00:04:14,798
always a risk

93
00:04:11,840 --> 00:04:16,399
for humans here on earth that things can

94
00:04:14,799 --> 00:04:18,478
go wrong

95
00:04:16,399 --> 00:04:19,679
sometimes very wrong sometimes very

96
00:04:18,478 --> 00:04:23,039
quickly

97
00:04:19,680 --> 00:04:26,240
sometimes fatally sometimes at scale

98
00:04:23,040 --> 00:04:26,720
and oddly enough i was actually thinking

99
00:04:26,240 --> 00:04:30,000
about

100
00:04:26,720 --> 00:04:33,199
this problem at the end of last year

101
00:04:30,000 --> 00:04:34,960
and before the the pandemic hit and then

102
00:04:33,199 --> 00:04:36,560
we started to see these terrible numbers

103
00:04:34,960 --> 00:04:37,758
and i i really hate to show these

104
00:04:36,560 --> 00:04:39,600
numbers but

105
00:04:37,759 --> 00:04:42,000
you know they remind us that things can

106
00:04:39,600 --> 00:04:43,680
go very wrong very quickly

107
00:04:42,000 --> 00:04:46,560
you know the world has now passed one

108
00:04:43,680 --> 00:04:49,919
million deaths from covert 19

109
00:04:46,560 --> 00:04:52,800
and this pandemic has

110
00:04:49,919 --> 00:04:54,719
i think exposed the world's lack of

111
00:04:52,800 --> 00:04:56,080
preparedness to deal with things on a

112
00:04:54,720 --> 00:04:58,240
global scale

113
00:04:56,080 --> 00:04:59,440
particularly when those things are

114
00:04:58,240 --> 00:05:02,400
happening

115
00:04:59,440 --> 00:05:04,000
badly at scale we've also seen

116
00:05:02,400 --> 00:05:06,960
weaknesses in things like

117
00:05:04,000 --> 00:05:08,720
our technology several countries have

118
00:05:06,960 --> 00:05:10,479
tried

119
00:05:08,720 --> 00:05:12,000
track and trace applications that

120
00:05:10,479 --> 00:05:14,719
haven't worked uh

121
00:05:12,000 --> 00:05:15,919
countries have found that large or too

122
00:05:14,720 --> 00:05:18,400
many people

123
00:05:15,919 --> 00:05:19,919
uh within their population can't work

124
00:05:18,400 --> 00:05:21,919
from home because they don't have decent

125
00:05:19,919 --> 00:05:24,080
internet or any internet at all

126
00:05:21,919 --> 00:05:25,120
um and we found out a lot of things

127
00:05:24,080 --> 00:05:27,758
which hopefully

128
00:05:25,120 --> 00:05:28,479
uh as the situation improves hopefully

129
00:05:27,759 --> 00:05:32,560
next year

130
00:05:28,479 --> 00:05:35,039
we can improve but if these things go

131
00:05:32,560 --> 00:05:37,280
much much worse if a risk becomes more

132
00:05:35,039 --> 00:05:40,400
than a catastrophe it turns into

133
00:05:37,280 --> 00:05:42,159
what is potentially an existential risk

134
00:05:40,400 --> 00:05:43,919
so there's risk and then there's

135
00:05:42,160 --> 00:05:47,680
existential risk

136
00:05:43,919 --> 00:05:49,120
and this is just a loose definition of

137
00:05:47,680 --> 00:05:50,880
an existential risk

138
00:05:49,120 --> 00:05:52,479
something that could end life as we know

139
00:05:50,880 --> 00:05:56,240
it for us

140
00:05:52,479 --> 00:05:58,080
and our survivors and not in a good way

141
00:05:56,240 --> 00:05:59,759
i guess it stands to reason that if um

142
00:05:58,080 --> 00:06:00,560
there was an event which took out all

143
00:05:59,759 --> 00:06:03,440
humans

144
00:06:00,560 --> 00:06:04,960
today there would be no more humans but

145
00:06:03,440 --> 00:06:06,719
it's a little bit more

146
00:06:04,960 --> 00:06:08,159
complicated than that as hopefully i

147
00:06:06,720 --> 00:06:11,360
will be able to explain

148
00:06:08,160 --> 00:06:13,280
and also able to explain why

149
00:06:11,360 --> 00:06:15,199
this relates to hackers and how our

150
00:06:13,280 --> 00:06:17,440
hacking relates to this

151
00:06:15,199 --> 00:06:18,800
so just to give some examples risk would

152
00:06:17,440 --> 00:06:21,039
be for example

153
00:06:18,800 --> 00:06:23,360
you're trying to get a bootleg version

154
00:06:21,039 --> 00:06:24,080
of green dark land downloaded to watch

155
00:06:23,360 --> 00:06:25,919
it

156
00:06:24,080 --> 00:06:28,159
but it contains malware there is a risk

157
00:06:25,919 --> 00:06:28,880
of that there's a risk that a comet

158
00:06:28,160 --> 00:06:31,440
fragment

159
00:06:28,880 --> 00:06:33,440
might devastate central florida as we

160
00:06:31,440 --> 00:06:36,479
can see from the wgvo

161
00:06:33,440 --> 00:06:38,000
traffic helicopter that's a screenshot

162
00:06:36,479 --> 00:06:40,479
from the movie

163
00:06:38,000 --> 00:06:41,360
and that sort of happens in the movie

164
00:06:40,479 --> 00:06:43,840
but

165
00:06:41,360 --> 00:06:45,120
existential risk would be the rest of

166
00:06:43,840 --> 00:06:48,318
the comet

167
00:06:45,120 --> 00:06:50,800
kills all humans another existential

168
00:06:48,319 --> 00:06:54,960
risk would be that a novel virus

169
00:06:50,800 --> 00:06:54,960
kills 90 percent of all humans

170
00:06:55,919 --> 00:06:58,799
and this is one of the important

171
00:06:57,039 --> 00:07:00,080
distinctions about existential risks

172
00:06:58,800 --> 00:07:03,599
they're not always

173
00:07:00,080 --> 00:07:05,840
totally 100 immediately fatal for

174
00:07:03,599 --> 00:07:07,759
everything in the future of humanity

175
00:07:05,840 --> 00:07:08,880
it's possible that some people could

176
00:07:07,759 --> 00:07:11,759
survive

177
00:07:08,880 --> 00:07:13,680
and in fact there are studies of prior

178
00:07:11,759 --> 00:07:16,479
catastrophes which show that

179
00:07:13,680 --> 00:07:18,479
it's actually quite likely that some

180
00:07:16,479 --> 00:07:20,318
people would somewhere survive

181
00:07:18,479 --> 00:07:21,520
after all humans are spread all around

182
00:07:20,319 --> 00:07:25,199
the globe now

183
00:07:21,520 --> 00:07:28,159
but it might not be enough people left

184
00:07:25,199 --> 00:07:29,039
to ever recover the way things were and

185
00:07:28,160 --> 00:07:32,639
of course

186
00:07:29,039 --> 00:07:35,440
for some of us who grew up in the 60s uh

187
00:07:32,639 --> 00:07:36,800
nuclear war ending all human life is

188
00:07:35,440 --> 00:07:40,400
kind of the mother of all

189
00:07:36,800 --> 00:07:44,560
existential risks because that one could

190
00:07:40,400 --> 00:07:46,878
still take us out uh in a big way

191
00:07:44,560 --> 00:07:49,599
so risk would be for example the office

192
00:07:46,879 --> 00:07:52,879
coffee maker getting hit with ransomware

193
00:07:49,599 --> 00:07:54,960
and uh a hat tip to avast for

194
00:07:52,879 --> 00:07:56,560
um having one of their researchers look

195
00:07:54,960 --> 00:07:58,000
into that and finding out you can

196
00:07:56,560 --> 00:08:00,000
actually run ransomware on a coffee

197
00:07:58,000 --> 00:08:03,120
maker

198
00:08:00,000 --> 00:08:05,680
and malicious code is a theme which um

199
00:08:03,120 --> 00:08:08,000
i think is important in the context of

200
00:08:05,680 --> 00:08:10,400
existential risk

201
00:08:08,000 --> 00:08:12,240
for example the artificial intelligence

202
00:08:10,400 --> 00:08:14,000
unit of your self-driving car gets hit

203
00:08:12,240 --> 00:08:16,319
with jack wear

204
00:08:14,000 --> 00:08:18,240
jack wears a term i came up with a few

205
00:08:16,319 --> 00:08:19,120
years ago to describe the intersection

206
00:08:18,240 --> 00:08:23,840
of things like

207
00:08:19,120 --> 00:08:28,240
ransomware with self-driving vehicles

208
00:08:23,840 --> 00:08:30,159
and that could be a very bad risk

209
00:08:28,240 --> 00:08:31,919
existential risk though would be an

210
00:08:30,160 --> 00:08:33,519
artificial intelligence that was

211
00:08:31,919 --> 00:08:35,598
tasked with something like ending world

212
00:08:33,519 --> 00:08:39,120
hunger deciding to

213
00:08:35,599 --> 00:08:40,479
kill humans which would end

214
00:08:39,120 --> 00:08:43,120
world hunger but i think they made a

215
00:08:40,479 --> 00:08:44,720
movie about that again it's things going

216
00:08:43,120 --> 00:08:46,959
very badly wrong

217
00:08:44,720 --> 00:08:49,440
like an attempt to reverse global

218
00:08:46,959 --> 00:08:51,359
warming and i think global warming

219
00:08:49,440 --> 00:08:53,440
has the potential to be an existential

220
00:08:51,360 --> 00:08:55,760
risk but

221
00:08:53,440 --> 00:08:56,880
people have looked at the possibility of

222
00:08:55,760 --> 00:08:59,279
tinkering with

223
00:08:56,880 --> 00:09:01,360
the world's atmosphere intentionally to

224
00:08:59,279 --> 00:09:02,480
slow down the process of global warming

225
00:09:01,360 --> 00:09:04,399
so that we can

226
00:09:02,480 --> 00:09:05,839
have more time to clean up our act since

227
00:09:04,399 --> 00:09:07,440
we're kind of behind schedule

228
00:09:05,839 --> 00:09:09,519
in dealing with climate change and

229
00:09:07,440 --> 00:09:12,240
global warming

230
00:09:09,519 --> 00:09:13,519
another existential risk is apocalyptic

231
00:09:12,240 --> 00:09:15,000
terrorists

232
00:09:13,519 --> 00:09:16,800
creating and spreading a deadly

233
00:09:15,000 --> 00:09:18,800
bioengineered virus

234
00:09:16,800 --> 00:09:20,000
and i'll be coming back to apocalyptic

235
00:09:18,800 --> 00:09:22,079
terrorists

236
00:09:20,000 --> 00:09:24,560
later even though it's a bit of a tongue

237
00:09:22,080 --> 00:09:27,120
twister

238
00:09:24,560 --> 00:09:28,319
and how about this a rogue artificial

239
00:09:27,120 --> 00:09:30,560
general intelligence

240
00:09:28,320 --> 00:09:31,600
traps humans on earth in a state of

241
00:09:30,560 --> 00:09:33,439
slavery

242
00:09:31,600 --> 00:09:35,600
now i should mention and give a shout

243
00:09:33,440 --> 00:09:37,600
out to dwight schwarthau

244
00:09:35,600 --> 00:09:39,440
who will be talking more about

245
00:09:37,600 --> 00:09:41,839
artificial intelligence

246
00:09:39,440 --> 00:09:43,200
in the next presentation not necessarily

247
00:09:41,839 --> 00:09:45,360
in this context

248
00:09:43,200 --> 00:09:46,720
but artificial intelligence figures

249
00:09:45,360 --> 00:09:48,959
heavily in the

250
00:09:46,720 --> 00:09:51,200
existential risk literature as i will

251
00:09:48,959 --> 00:09:53,199
explain

252
00:09:51,200 --> 00:09:55,600
so what's existential risk got to do

253
00:09:53,200 --> 00:09:59,040
with hacking and cyber security

254
00:09:55,600 --> 00:10:00,959
uh well hackers and infosec people

255
00:09:59,040 --> 00:10:03,519
are typically very good at what could

256
00:10:00,959 --> 00:10:05,680
possibly go wrong

257
00:10:03,519 --> 00:10:08,079
right we tend to look at each new

258
00:10:05,680 --> 00:10:10,959
development in technology

259
00:10:08,079 --> 00:10:13,760
we could get excited about it a lot of

260
00:10:10,959 --> 00:10:17,040
us like technology in a way but

261
00:10:13,760 --> 00:10:18,160
we tend to think about what could go

262
00:10:17,040 --> 00:10:20,800
wrong

263
00:10:18,160 --> 00:10:21,600
recently there was an update to ios on

264
00:10:20,800 --> 00:10:24,079
on the uh

265
00:10:21,600 --> 00:10:25,200
the iphone so he could use your iphone

266
00:10:24,079 --> 00:10:28,319
to open

267
00:10:25,200 --> 00:10:30,720
your car to unlock your car and

268
00:10:28,320 --> 00:10:33,120
shay and i immediately saw that when

269
00:10:30,720 --> 00:10:35,680
what could possibly go wrong

270
00:10:33,120 --> 00:10:37,440
yeah we have car themes already using

271
00:10:35,680 --> 00:10:40,399
electronic overrides and so on

272
00:10:37,440 --> 00:10:41,839
what could possibly go wrong uh with

273
00:10:40,399 --> 00:10:43,839
that

274
00:10:41,839 --> 00:10:45,440
and so i think that existential risk you

275
00:10:43,839 --> 00:10:48,800
could say is the ultimate

276
00:10:45,440 --> 00:10:48,800
what could possibly go wrong

277
00:10:49,760 --> 00:10:54,720
the hacker mentality is to poke and prod

278
00:10:52,959 --> 00:10:58,160
and explore and extrapolate

279
00:10:54,720 --> 00:10:59,200
with technology it really helps to get

280
00:10:58,160 --> 00:11:01,360
into

281
00:10:59,200 --> 00:11:04,959
looking at what could possibly go wrong

282
00:11:01,360 --> 00:11:07,120
and then explaining it to other people

283
00:11:04,959 --> 00:11:08,399
so i'm not sure if this is a bumper

284
00:11:07,120 --> 00:11:11,200
sticker yet but

285
00:11:08,399 --> 00:11:11,839
um hackers don't break your things we

286
00:11:11,200 --> 00:11:15,839
stop

287
00:11:11,839 --> 00:11:15,839
things breaking you and

288
00:11:16,160 --> 00:11:19,519
i'm going to talk about some really big

289
00:11:17,680 --> 00:11:20,239
things that we could maybe stop breaking

290
00:11:19,519 --> 00:11:22,480
us

291
00:11:20,240 --> 00:11:23,839
in our existential risk primer we're

292
00:11:22,480 --> 00:11:26,480
going to talk about

293
00:11:23,839 --> 00:11:28,320
definitions of existential risk topics

294
00:11:26,480 --> 00:11:29,680
of morality and statistics which come

295
00:11:28,320 --> 00:11:32,800
into play here

296
00:11:29,680 --> 00:11:36,719
um one of the exciting things to me

297
00:11:32,800 --> 00:11:40,000
about being in security has been

298
00:11:36,720 --> 00:11:43,360
since the early days the diversity

299
00:11:40,000 --> 00:11:46,160
of direction from which people came to

300
00:11:43,360 --> 00:11:47,839
computer security all right so it it

301
00:11:46,160 --> 00:11:49,600
wasn't like we all went to school and

302
00:11:47,839 --> 00:11:50,800
got a degree in this and then went to

303
00:11:49,600 --> 00:11:53,120
work in it

304
00:11:50,800 --> 00:11:54,160
uh we came at it before degrees some of

305
00:11:53,120 --> 00:11:56,160
us in the early days

306
00:11:54,160 --> 00:11:57,839
from different directions and yet that

307
00:11:56,160 --> 00:11:58,880
still happens there are people who get

308
00:11:57,839 --> 00:12:01,120
into

309
00:11:58,880 --> 00:12:02,320
cyber security and hacking from

310
00:12:01,120 --> 00:12:05,040
different directions

311
00:12:02,320 --> 00:12:06,320
and yeah i have a i have degrees in

312
00:12:05,040 --> 00:12:10,560
english

313
00:12:06,320 --> 00:12:12,720
and religion and risk management and

314
00:12:10,560 --> 00:12:14,319
religion actually comes into the

315
00:12:12,720 --> 00:12:17,519
existential risk

316
00:12:14,320 --> 00:12:18,399
uh conversation very quickly morality

317
00:12:17,519 --> 00:12:20,560
becomes big

318
00:12:18,399 --> 00:12:22,399
if you're into mathematics statistics

319
00:12:20,560 --> 00:12:25,040
are big in existential risk

320
00:12:22,399 --> 00:12:26,000
and so this is a topic which i think is

321
00:12:25,040 --> 00:12:28,880
rising

322
00:12:26,000 --> 00:12:30,959
up the public agenda and for which

323
00:12:28,880 --> 00:12:32,800
people from various directions within

324
00:12:30,959 --> 00:12:35,439
hacking and information security can get

325
00:12:32,800 --> 00:12:38,560
involved agents of the apocalypse

326
00:12:35,440 --> 00:12:40,000
are actually a thing in existential risk

327
00:12:38,560 --> 00:12:41,760
we'll talk about the story of how

328
00:12:40,000 --> 00:12:43,279
hackers save humanity and then i'll

329
00:12:41,760 --> 00:12:46,720
provide some opportunities

330
00:12:43,279 --> 00:12:48,399
and resources for getting further

331
00:12:46,720 --> 00:12:49,839
involved in this or exploring this

332
00:12:48,399 --> 00:12:52,959
further

333
00:12:49,839 --> 00:12:54,560
so oh it also includes the vulnerable

334
00:12:52,959 --> 00:12:57,680
world hypothesis

335
00:12:54,560 --> 00:12:59,760
this is an interesting not not a tangent

336
00:12:57,680 --> 00:13:00,719
really but an aspect of existential risk

337
00:12:59,760 --> 00:13:04,399
and technology

338
00:13:00,720 --> 00:13:06,480
we'll talk about that so if you're

339
00:13:04,399 --> 00:13:08,000
still not convinced what's existential

340
00:13:06,480 --> 00:13:10,000
risk got to do with hacking and cyber

341
00:13:08,000 --> 00:13:11,920
security

342
00:13:10,000 --> 00:13:13,920
i can't do a show of hands here but i

343
00:13:11,920 --> 00:13:15,839
would if i was there in person say how

344
00:13:13,920 --> 00:13:19,199
many people

345
00:13:15,839 --> 00:13:21,279
saw this presentation at defcon 25

346
00:13:19,200 --> 00:13:22,320
where john sotos who is the chief

347
00:13:21,279 --> 00:13:25,760
medical officer of

348
00:13:22,320 --> 00:13:26,720
intel not speaking as an intel employee

349
00:13:25,760 --> 00:13:31,360
at this event

350
00:13:26,720 --> 00:13:31,360
but talked about how genetic diseases

351
00:13:31,440 --> 00:13:37,279
could act as a guide to digital hacks of

352
00:13:34,160 --> 00:13:37,279
the human genome

353
00:13:38,880 --> 00:13:44,160
as with many con presentations it's

354
00:13:42,320 --> 00:13:47,199
online you can google it and

355
00:13:44,160 --> 00:13:48,000
go through it i'd i'd sort of you know

356
00:13:47,199 --> 00:13:50,560
maybe have a friend

357
00:13:48,000 --> 00:13:51,120
with you because this this presentation

358
00:13:50,560 --> 00:13:53,518
had

359
00:13:51,120 --> 00:13:55,440
slide after slide of horrible things you

360
00:13:53,519 --> 00:13:58,079
could do messing about with jeans if you

361
00:13:55,440 --> 00:14:01,600
were malicious

362
00:13:58,079 --> 00:14:04,239
and his call at that time was for

363
00:14:01,600 --> 00:14:04,880
the hacking community to start looking

364
00:14:04,240 --> 00:14:07,600
into this

365
00:14:04,880 --> 00:14:08,480
because the world is going to need help

366
00:14:07,600 --> 00:14:11,600
preventing this

367
00:14:08,480 --> 00:14:13,839
happen down the road so let's look at

368
00:14:11,600 --> 00:14:16,240
the definition of existential risk from

369
00:14:13,839 --> 00:14:18,959
a more formal point of view

370
00:14:16,240 --> 00:14:21,040
bostrom nick bostrom is really the

371
00:14:18,959 --> 00:14:24,479
person who who brought

372
00:14:21,040 --> 00:14:28,480
existential risk to the public attention

373
00:14:24,480 --> 00:14:30,720
in the early part of the 2000s

374
00:14:28,480 --> 00:14:32,000
and he used this definition a risk that

375
00:14:30,720 --> 00:14:35,199
threatens

376
00:14:32,000 --> 00:14:36,160
the premature extinction of all human

377
00:14:35,199 --> 00:14:39,040
life

378
00:14:36,160 --> 00:14:40,480
or the permanent and drastic reduction

379
00:14:39,040 --> 00:14:43,599
of its potential

380
00:14:40,480 --> 00:14:44,959
for desirable future development so this

381
00:14:43,600 --> 00:14:47,120
captures the idea that

382
00:14:44,959 --> 00:14:49,599
it's not just oh we all die and there

383
00:14:47,120 --> 00:14:53,279
aren't any more humans

384
00:14:49,600 --> 00:14:55,440
but also the possibility that most of us

385
00:14:53,279 --> 00:14:59,040
die and our future generations don't

386
00:14:55,440 --> 00:15:01,360
have a very good time

387
00:14:59,040 --> 00:15:02,160
his original definition i think used the

388
00:15:01,360 --> 00:15:05,440
phrase

389
00:15:02,160 --> 00:15:08,000
instead of all human life all earth

390
00:15:05,440 --> 00:15:10,880
originating intelligent life

391
00:15:08,000 --> 00:15:12,560
and this is because the direction that

392
00:15:10,880 --> 00:15:14,639
bostrom came from

393
00:15:12,560 --> 00:15:16,479
getting into existential risk was

394
00:15:14,639 --> 00:15:18,720
transhumanism

395
00:15:16,480 --> 00:15:21,360
and transhumanism is the idea that

396
00:15:18,720 --> 00:15:23,680
technology can transform humans

397
00:15:21,360 --> 00:15:25,120
uh can extend our lives can extend our

398
00:15:23,680 --> 00:15:27,920
capabilities

399
00:15:25,120 --> 00:15:28,399
and make the world a much much better

400
00:15:27,920 --> 00:15:32,160
place

401
00:15:28,399 --> 00:15:35,440
and maybe move us beyond the world

402
00:15:32,160 --> 00:15:38,000
into outer space into maybe computer

403
00:15:35,440 --> 00:15:41,519
networks so that we can live on

404
00:15:38,000 --> 00:15:44,000
virtually now

405
00:15:41,519 --> 00:15:45,120
i'll give a lot of credit for bostrom to

406
00:15:44,000 --> 00:15:46,880
boston because

407
00:15:45,120 --> 00:15:48,160
as he looked at this and got very

408
00:15:46,880 --> 00:15:50,880
excited about the

409
00:15:48,160 --> 00:15:53,839
possibilities of transhumanism he also

410
00:15:50,880 --> 00:15:54,720
realized things could go very very wrong

411
00:15:53,839 --> 00:15:56,720
and

412
00:15:54,720 --> 00:15:59,360
so he started to look at its essential

413
00:15:56,720 --> 00:16:01,440
risk and really dove into that

414
00:15:59,360 --> 00:16:02,880
you know i actually do think things

415
00:16:01,440 --> 00:16:04,560
could go very very wrong with

416
00:16:02,880 --> 00:16:07,920
transhumanism

417
00:16:04,560 --> 00:16:08,959
and um i tend to diverge from him on

418
00:16:07,920 --> 00:16:12,399
some of these uh

419
00:16:08,959 --> 00:16:14,319
some of these finer points but he

420
00:16:12,399 --> 00:16:16,160
clearly established that you know an

421
00:16:14,320 --> 00:16:17,279
existential risk need not actually kill

422
00:16:16,160 --> 00:16:20,639
everyone

423
00:16:17,279 --> 00:16:21,759
um if we were unable to rebuild society

424
00:16:20,639 --> 00:16:23,519
and and this is

425
00:16:21,759 --> 00:16:25,360
the position from a paper called the

426
00:16:23,519 --> 00:16:28,399
global priorities project

427
00:16:25,360 --> 00:16:30,000
uh that i'll talk about more uh then

428
00:16:28,399 --> 00:16:32,000
yeah it would still qualify as an

429
00:16:30,000 --> 00:16:34,399
existential catastrophe

430
00:16:32,000 --> 00:16:36,240
and yeah something that ends life as we

431
00:16:34,399 --> 00:16:39,440
know it for us and our survivors and

432
00:16:36,240 --> 00:16:39,440
not in a good way

433
00:16:40,079 --> 00:16:43,839
obviously there are different kinds of

434
00:16:41,519 --> 00:16:45,360
existential risks so some

435
00:16:43,839 --> 00:16:47,279
some of them can be categorized like

436
00:16:45,360 --> 00:16:49,839
this so we have natural

437
00:16:47,279 --> 00:16:50,800
existential risks the classic asteroid

438
00:16:49,839 --> 00:16:53,519
impact

439
00:16:50,800 --> 00:16:54,319
um this is some sometimes also a comet

440
00:16:53,519 --> 00:16:57,279
impact

441
00:16:54,320 --> 00:16:58,720
i think they tend to use comets more in

442
00:16:57,279 --> 00:17:00,720
movies because

443
00:16:58,720 --> 00:17:03,199
they're easier for people to understand

444
00:17:00,720 --> 00:17:07,919
um super volcanoes

445
00:17:03,199 --> 00:17:07,918
that could happen huge eruption uh

446
00:17:09,119 --> 00:17:12,799
blocks the sun plants can't process so

447
00:17:11,839 --> 00:17:15,280
on and so forth

448
00:17:12,799 --> 00:17:16,240
natural diseases i mean we're seeing you

449
00:17:15,280 --> 00:17:20,160
know i

450
00:17:16,240 --> 00:17:22,079
tend to be in the covid19 is a naturally

451
00:17:20,160 --> 00:17:24,240
occurring disease camp

452
00:17:22,079 --> 00:17:26,399
that can have tremendous negative

453
00:17:24,240 --> 00:17:29,679
effects and has the potential to

454
00:17:26,400 --> 00:17:34,160
kill everybody anthropogenic

455
00:17:29,679 --> 00:17:37,760
is us all right so human

456
00:17:34,160 --> 00:17:39,280
sourced risk and and these this

457
00:17:37,760 --> 00:17:42,640
is something which makes this very

458
00:17:39,280 --> 00:17:45,520
modern topic it wasn't really until 1950

459
00:17:42,640 --> 00:17:46,080
that people realized oh my gosh you know

460
00:17:45,520 --> 00:17:48,320
after

461
00:17:46,080 --> 00:17:49,280
we saw the end of world war ii in

462
00:17:48,320 --> 00:17:52,480
nuclear

463
00:17:49,280 --> 00:17:54,080
explosions we could destroy

464
00:17:52,480 --> 00:17:55,600
everything that we could destroy the

465
00:17:54,080 --> 00:17:58,080
planet

466
00:17:55,600 --> 00:17:58,639
there already was a rising sense before

467
00:17:58,080 --> 00:18:00,639
that

468
00:17:58,640 --> 00:18:02,640
that humans were having a negative

469
00:18:00,640 --> 00:18:06,320
effect on the world

470
00:18:02,640 --> 00:18:08,480
uh and in fact uh some philosophers who

471
00:18:06,320 --> 00:18:10,080
lived through world war one world war ii

472
00:18:08,480 --> 00:18:12,160
were developing thoughts along these

473
00:18:10,080 --> 00:18:15,678
lines one of my favorite philosophers

474
00:18:12,160 --> 00:18:16,400
hans jonas was one of the first to come

475
00:18:15,679 --> 00:18:17,919
up with

476
00:18:16,400 --> 00:18:21,360
the idea that humans had a

477
00:18:17,919 --> 00:18:23,679
responsibility to not destroy the planet

478
00:18:21,360 --> 00:18:24,959
to not uh as gentlemen said at the

479
00:18:23,679 --> 00:18:27,600
beginning of the day

480
00:18:24,960 --> 00:18:29,280
f it up right we'd have a responsibility

481
00:18:27,600 --> 00:18:31,039
not just to humans

482
00:18:29,280 --> 00:18:33,280
but to the whole thing because for the

483
00:18:31,039 --> 00:18:34,080
first time ever humans could destroy

484
00:18:33,280 --> 00:18:36,559
everything

485
00:18:34,080 --> 00:18:37,120
and the nuclear holocaust is is the big

486
00:18:36,559 --> 00:18:39,120
one

487
00:18:37,120 --> 00:18:40,320
uh the first that really focused

488
00:18:39,120 --> 00:18:42,719
attention on

489
00:18:40,320 --> 00:18:43,760
existential risk and this possibility

490
00:18:42,720 --> 00:18:47,039
that we could end it

491
00:18:43,760 --> 00:18:50,160
all but engineered pandemic

492
00:18:47,039 --> 00:18:52,000
uh close second there in the you know

493
00:18:50,160 --> 00:18:52,720
world war one world war ii there was use

494
00:18:52,000 --> 00:18:55,039
of

495
00:18:52,720 --> 00:18:56,720
um chemical weapons some biological

496
00:18:55,039 --> 00:18:59,760
weapons

497
00:18:56,720 --> 00:19:00,559
and we haven't stopped by the way

498
00:18:59,760 --> 00:19:03,200
developing

499
00:19:00,559 --> 00:19:04,480
nuclear weapons biological weapons or

500
00:19:03,200 --> 00:19:06,320
chemical weapons there are a few

501
00:19:04,480 --> 00:19:08,960
treaties around

502
00:19:06,320 --> 00:19:10,159
but they're not working that well then

503
00:19:08,960 --> 00:19:11,679
we have genetics

504
00:19:10,160 --> 00:19:13,280
and i referenced that earlier with

505
00:19:11,679 --> 00:19:16,480
sotos's talk

506
00:19:13,280 --> 00:19:17,520
um doesn't necessarily have to be this

507
00:19:16,480 --> 00:19:19,600
existential risk

508
00:19:17,520 --> 00:19:21,200
something that somebody bad did you

509
00:19:19,600 --> 00:19:22,399
could be well intentioned and get it

510
00:19:21,200 --> 00:19:24,000
wrong

511
00:19:22,400 --> 00:19:25,679
could be accidental misuse of

512
00:19:24,000 --> 00:19:29,520
nanotechnology

513
00:19:25,679 --> 00:19:31,520
and this is another example of how

514
00:19:29,520 --> 00:19:32,799
people come at existential risks from

515
00:19:31,520 --> 00:19:35,679
different directions

516
00:19:32,799 --> 00:19:37,120
so when nanotechnology was making

517
00:19:35,679 --> 00:19:39,039
headlines

518
00:19:37,120 --> 00:19:41,520
people started to think oh my gosh how

519
00:19:39,039 --> 00:19:43,120
could that possibly go wrong

520
00:19:41,520 --> 00:19:45,200
grey goo is something you may want to

521
00:19:43,120 --> 00:19:48,159
google the

522
00:19:45,200 --> 00:19:50,080
scariest talk that i can't talk about

523
00:19:48,160 --> 00:19:52,080
was one that involved a combination of

524
00:19:50,080 --> 00:19:56,080
genetic meddling

525
00:19:52,080 --> 00:20:00,080
and nanotechnology intentional misuse

526
00:19:56,080 --> 00:20:02,639
of nanotechnology thanks for that win

527
00:20:00,080 --> 00:20:04,158
that was very frightening um

528
00:20:02,640 --> 00:20:06,559
geoengineering

529
00:20:04,159 --> 00:20:07,360
could prove fatal we could try to save

530
00:20:06,559 --> 00:20:10,879
the planet

531
00:20:07,360 --> 00:20:13,199
and fail badly

532
00:20:10,880 --> 00:20:14,080
but also humans themselves could be the

533
00:20:13,200 --> 00:20:16,880
problem

534
00:20:14,080 --> 00:20:18,840
imagine a dictatorship armed with

535
00:20:16,880 --> 00:20:20,960
tremendous amounts of

536
00:20:18,840 --> 00:20:23,120
technology just taking over the whole

537
00:20:20,960 --> 00:20:25,919
world and making life miserable for all

538
00:20:23,120 --> 00:20:25,918
future humans

539
00:20:26,240 --> 00:20:31,679
this is interesting the pandemic

540
00:20:28,960 --> 00:20:34,559
arguably sits in the middle

541
00:20:31,679 --> 00:20:37,039
you may have seen a book reviewed called

542
00:20:34,559 --> 00:20:38,559
the precipice by toby ord

543
00:20:37,039 --> 00:20:41,440
so this is a big thick book which came

544
00:20:38,559 --> 00:20:44,799
out this year talk about great timing

545
00:20:41,440 --> 00:20:46,960
uh as covid was blowing up so to speak

546
00:20:44,799 --> 00:20:49,360
this book about existential risks by a

547
00:20:46,960 --> 00:20:52,080
philosopher toby ward came out

548
00:20:49,360 --> 00:20:53,280
and it's still a costly hardback at the

549
00:20:52,080 --> 00:20:54,879
moment but you can find

550
00:20:53,280 --> 00:20:56,720
a lot of his writing and in fact

551
00:20:54,880 --> 00:20:57,520
bostrom's stuff and a lot of this

552
00:20:56,720 --> 00:21:00,000
material

553
00:20:57,520 --> 00:21:01,760
uh is downloadable free online the

554
00:21:00,000 --> 00:21:05,520
papers that they have written

555
00:21:01,760 --> 00:21:05,520
he's got a great youtube um

556
00:21:06,640 --> 00:21:11,520
lecture in which he describes why

557
00:21:09,360 --> 00:21:14,559
pandemic sits in the middle because

558
00:21:11,520 --> 00:21:15,600
coronavirus shows us how the way we've

559
00:21:14,559 --> 00:21:19,280
shaped our world

560
00:21:15,600 --> 00:21:21,918
so far has in fact

561
00:21:19,280 --> 00:21:23,360
exacerbated the problem you know jet

562
00:21:21,919 --> 00:21:26,159
travel

563
00:21:23,360 --> 00:21:27,360
global travel that spread this thing

564
00:21:26,159 --> 00:21:29,600
faster than

565
00:21:27,360 --> 00:21:31,360
any previous thing has ever spread

566
00:21:29,600 --> 00:21:33,439
arguably

567
00:21:31,360 --> 00:21:35,439
and there are of course unknown unknowns

568
00:21:33,440 --> 00:21:38,240
and we'll get to those in the vulnerable

569
00:21:35,440 --> 00:21:39,760
world hypothesis another way of

570
00:21:38,240 --> 00:21:41,280
categorizing things and i won't go

571
00:21:39,760 --> 00:21:45,200
through all of these in detail

572
00:21:41,280 --> 00:21:47,918
is that it's not just a big bang bostrom

573
00:21:45,200 --> 00:21:49,919
had these four categories bangs crunches

574
00:21:47,919 --> 00:21:53,120
shrieks and whimpers

575
00:21:49,919 --> 00:21:54,320
so we may not necessarily realize as an

576
00:21:53,120 --> 00:21:57,280
existential risk

577
00:21:54,320 --> 00:21:59,678
unfolds that that is what it is it may

578
00:21:57,280 --> 00:22:03,678
not just happen suddenly

579
00:21:59,679 --> 00:22:03,679
and it may not ever come to an end

580
00:22:03,760 --> 00:22:08,000
for visual learners like myself it may

581
00:22:05,760 --> 00:22:08,480
be helps to graph things out this was a

582
00:22:08,000 --> 00:22:12,240
simple

583
00:22:08,480 --> 00:22:13,760
early graph by bostrom

584
00:22:12,240 --> 00:22:16,240
where you have a choice between

585
00:22:13,760 --> 00:22:18,559
endurable and terminal scope

586
00:22:16,240 --> 00:22:19,840
and three levels of intensity so your

587
00:22:18,559 --> 00:22:24,879
car is stolen

588
00:22:19,840 --> 00:22:27,760
that's a risk it's an endurable risk

589
00:22:24,880 --> 00:22:29,520
a fatal car crash obviously terminal but

590
00:22:27,760 --> 00:22:30,559
it's only affecting one person in this

591
00:22:29,520 --> 00:22:32,320
case

592
00:22:30,559 --> 00:22:35,039
you could have a recession in a country

593
00:22:32,320 --> 00:22:37,200
which was an endurable problem not a

594
00:22:35,039 --> 00:22:39,039
nice problem to have an endural problem

595
00:22:37,200 --> 00:22:41,679
but there have been genocides which

596
00:22:39,039 --> 00:22:42,240
have right wiped out entire populations

597
00:22:41,679 --> 00:22:45,280
within

598
00:22:42,240 --> 00:22:47,520
a local context you could have global

599
00:22:45,280 --> 00:22:49,200
issues like thinning of the ozone layer

600
00:22:47,520 --> 00:22:50,960
and existential risks

601
00:22:49,200 --> 00:22:52,320
sit in the top right hand corner because

602
00:22:50,960 --> 00:22:56,720
they are both global

603
00:22:52,320 --> 00:22:56,720
and in one way or another terminal

604
00:22:57,360 --> 00:23:00,959
some existential risks will start off as

605
00:22:59,520 --> 00:23:04,240
catastrophes

606
00:23:00,960 --> 00:23:04,880
an existential catastrophe will then

607
00:23:04,240 --> 00:23:07,600
lead to

608
00:23:04,880 --> 00:23:08,640
so if the catastrophe turns existential

609
00:23:07,600 --> 00:23:11,678
that will mean it's

610
00:23:08,640 --> 00:23:14,320
either extinction or failed continuation

611
00:23:11,679 --> 00:23:18,159
we can't carry on as we did

612
00:23:14,320 --> 00:23:21,760
either through an unrecoverable collapse

613
00:23:18,159 --> 00:23:24,240
or an unrecoverable dystopia

614
00:23:21,760 --> 00:23:26,158
the authoritarian regime which rules the

615
00:23:24,240 --> 00:23:29,600
planet makes us miserable

616
00:23:26,159 --> 00:23:31,120
ward created this uh chart on the right

617
00:23:29,600 --> 00:23:33,760
another chart which i won't go through

618
00:23:31,120 --> 00:23:36,639
in great detail has more categories

619
00:23:33,760 --> 00:23:38,400
in the severity scale damaging

620
00:23:36,640 --> 00:23:41,200
catastrophic and fatal

621
00:23:38,400 --> 00:23:44,240
and this comes from the existential risk

622
00:23:41,200 --> 00:23:47,440
diplomacy and governance

623
00:23:44,240 --> 00:23:48,400
report which was something which came

624
00:23:47,440 --> 00:23:51,200
out actually uh

625
00:23:48,400 --> 00:23:53,200
talks around the paris agreement on uh

626
00:23:51,200 --> 00:23:56,400
climate change and global warming

627
00:23:53,200 --> 00:23:57,120
when talk of gene geo engineering was

628
00:23:56,400 --> 00:24:00,240
big

629
00:23:57,120 --> 00:24:01,918
and uh some of the things like aerosol

630
00:24:00,240 --> 00:24:05,679
spraying in the atmosphere

631
00:24:01,919 --> 00:24:06,000
uh were put forward as the potential way

632
00:24:05,679 --> 00:24:08,880
to

633
00:24:06,000 --> 00:24:09,840
to help and then people realized that

634
00:24:08,880 --> 00:24:12,000
could be very risky

635
00:24:09,840 --> 00:24:14,158
and so that is addressed in this report

636
00:24:12,000 --> 00:24:15,840
this report is also very good if you're

637
00:24:14,159 --> 00:24:18,720
kind of a policy one can you want to

638
00:24:15,840 --> 00:24:21,439
look at an initial view of how we might

639
00:24:18,720 --> 00:24:24,640
address existential risk from a world

640
00:24:21,440 --> 00:24:24,640
governance perspective

641
00:24:24,960 --> 00:24:28,080
i said earlier there was math and morals

642
00:24:27,440 --> 00:24:31,120
um

643
00:24:28,080 --> 00:24:34,320
is future life worth saving is

644
00:24:31,120 --> 00:24:36,399
uh a moral question how many people will

645
00:24:34,320 --> 00:24:39,520
there be in the future is kind of a

646
00:24:36,400 --> 00:24:42,159
scientific mathematical problem um

647
00:24:39,520 --> 00:24:43,120
you know if we save x billion people

648
00:24:42,159 --> 00:24:46,240
today the

649
00:24:43,120 --> 00:24:48,080
current world population

650
00:24:46,240 --> 00:24:49,679
we don't just save them we save their

651
00:24:48,080 --> 00:24:52,720
future generations

652
00:24:49,679 --> 00:24:53,600
um calculating future generation numbers

653
00:24:52,720 --> 00:24:55,679
is interesting

654
00:24:53,600 --> 00:24:56,639
because you want to do that if you're

655
00:24:55,679 --> 00:24:59,600
going to argue

656
00:24:56,640 --> 00:25:01,679
i'd like some money to research a

657
00:24:59,600 --> 00:25:03,279
reduction of existential risk

658
00:25:01,679 --> 00:25:04,960
how many lives could you save and it

659
00:25:03,279 --> 00:25:07,279
turns out it could be hundreds

660
00:25:04,960 --> 00:25:08,480
or even thousands of billions if you

661
00:25:07,279 --> 00:25:11,919
saved

662
00:25:08,480 --> 00:25:12,400
people going extinct um but what is the

663
00:25:11,919 --> 00:25:15,200
value

664
00:25:12,400 --> 00:25:16,480
of a reduction in existential risk

665
00:25:15,200 --> 00:25:18,960
relative to the value

666
00:25:16,480 --> 00:25:19,760
of future lives relative to present

667
00:25:18,960 --> 00:25:23,039
lives

668
00:25:19,760 --> 00:25:26,000
so you know we have this thing

669
00:25:23,039 --> 00:25:28,320
uh where we try to figure out how to

670
00:25:26,000 --> 00:25:31,679
spend our money globally uh

671
00:25:28,320 --> 00:25:33,360
what causes to support and how about

672
00:25:31,679 --> 00:25:36,400
asteroid deflection

673
00:25:33,360 --> 00:25:38,158
you know we know asteroids have caused

674
00:25:36,400 --> 00:25:39,440
serious problems in the past they're

675
00:25:38,159 --> 00:25:43,679
still around

676
00:25:39,440 --> 00:25:46,400
um in fact the world is investing in

677
00:25:43,679 --> 00:25:48,080
asteroid defense i'll talk about that

678
00:25:46,400 --> 00:25:49,840
more later but

679
00:25:48,080 --> 00:25:52,240
it's costing billions and billions of

680
00:25:49,840 --> 00:25:54,639
dollars would it be better

681
00:25:52,240 --> 00:25:56,159
and this argument happens around all

682
00:25:54,640 --> 00:25:57,919
kinds of space related things

683
00:25:56,159 --> 00:25:59,360
is it better to do it in do this stuff

684
00:25:57,919 --> 00:26:01,520
in space or

685
00:25:59,360 --> 00:26:03,760
here on earth make life better for the

686
00:26:01,520 --> 00:26:06,000
people who are alive at the moment

687
00:26:03,760 --> 00:26:08,158
and potentially their future generations

688
00:26:06,000 --> 00:26:08,640
raising people out of poverty and so on

689
00:26:08,159 --> 00:26:10,880
should we

690
00:26:08,640 --> 00:26:13,360
should we do that first before we build

691
00:26:10,880 --> 00:26:14,640
the asteroid defense system

692
00:26:13,360 --> 00:26:16,399
and of course there's the other point of

693
00:26:14,640 --> 00:26:19,360
view which we're all supposed to die

694
00:26:16,400 --> 00:26:22,000
anyway this is the aspect of religion

695
00:26:19,360 --> 00:26:25,600
called eschatology the end times

696
00:26:22,000 --> 00:26:27,440
and that can play into the existential

697
00:26:25,600 --> 00:26:30,719
risk discussion

698
00:26:27,440 --> 00:26:34,080
the statistics are are interesting i'm

699
00:26:30,720 --> 00:26:35,279
not a statistician but uh toby ward in

700
00:26:34,080 --> 00:26:38,399
his book looks at this in

701
00:26:35,279 --> 00:26:38,720
in great detail and and he came up with

702
00:26:38,400 --> 00:26:40,960
this

703
00:26:38,720 --> 00:26:41,919
this is directly from his book the

704
00:26:40,960 --> 00:26:44,559
precipice

705
00:26:41,919 --> 00:26:46,720
uh calculating what's the chance of this

706
00:26:44,559 --> 00:26:49,039
happening in the next 100 years

707
00:26:46,720 --> 00:26:51,520
and you know fortunately the whole

708
00:26:49,039 --> 00:26:53,840
asteroid thing that's one in a million

709
00:26:51,520 --> 00:26:55,840
you get down to engineered pandemics

710
00:26:53,840 --> 00:26:59,120
that's one in 30.

711
00:26:55,840 --> 00:27:02,799
unaligned artificial intelligence sorry

712
00:26:59,120 --> 00:27:04,959
one in ten and so

713
00:27:02,799 --> 00:27:06,000
you know i have serious doubts about

714
00:27:04,960 --> 00:27:09,360
some of these calculations

715
00:27:06,000 --> 00:27:10,960
and to give ward his due he puts in lots

716
00:27:09,360 --> 00:27:12,879
of caveats with these but

717
00:27:10,960 --> 00:27:16,159
it's an interesting area to go if you're

718
00:27:12,880 --> 00:27:18,840
interested uh in that sort of thing

719
00:27:16,159 --> 00:27:20,559
and you may want to use this in your

720
00:27:18,840 --> 00:27:23,918
arguments around

721
00:27:20,559 --> 00:27:25,760
existential risk so

722
00:27:23,919 --> 00:27:27,440
we're all going to die anyway brings us

723
00:27:25,760 --> 00:27:30,799
to agents of apocalypse

724
00:27:27,440 --> 00:27:31,279
and phil torres is the go-to person on

725
00:27:30,799 --> 00:27:33,039
this

726
00:27:31,279 --> 00:27:35,200
his book the end what science and

727
00:27:33,039 --> 00:27:36,879
religion tell us about the apocalypse

728
00:27:35,200 --> 00:27:39,279
very good to read on this because he

729
00:27:36,880 --> 00:27:40,880
came at existential risk

730
00:27:39,279 --> 00:27:42,720
and he's written a lot about many

731
00:27:40,880 --> 00:27:45,760
aspects of the problem

732
00:27:42,720 --> 00:27:47,120
from a religious perspective he uh he

733
00:27:45,760 --> 00:27:50,240
became an atheist

734
00:27:47,120 --> 00:27:52,239
and he began looking in critical detail

735
00:27:50,240 --> 00:27:55,919
and apocalyptic beliefs

736
00:27:52,240 --> 00:27:58,080
the idea that some people are not just

737
00:27:55,919 --> 00:28:00,320
motivated to kill themselves for their

738
00:27:58,080 --> 00:28:03,600
religious police but to kill everybody

739
00:28:00,320 --> 00:28:05,520
um you know there are a lot of

740
00:28:03,600 --> 00:28:06,719
uh debates around the morality of

741
00:28:05,520 --> 00:28:08,879
suicide bombing and

742
00:28:06,720 --> 00:28:10,320
and and the idea that that's using

743
00:28:08,880 --> 00:28:10,880
somebody to make your point because

744
00:28:10,320 --> 00:28:12,960
you're

745
00:28:10,880 --> 00:28:15,360
if you organize somebody to commit to

746
00:28:12,960 --> 00:28:18,720
kill themselves in your cause

747
00:28:15,360 --> 00:28:20,959
you're still around so

748
00:28:18,720 --> 00:28:21,840
a true apocalyptic terrorist would be

749
00:28:20,960 --> 00:28:24,720
one it's

750
00:28:21,840 --> 00:28:26,320
both suicidal and omnicidal and wants to

751
00:28:24,720 --> 00:28:27,600
kill not them just themselves but

752
00:28:26,320 --> 00:28:30,960
everybody else

753
00:28:27,600 --> 00:28:34,158
idiosic so so taurus breaks this down

754
00:28:30,960 --> 00:28:35,840
into the agents right so these are

755
00:28:34,159 --> 00:28:38,000
different agents not all religious but

756
00:28:35,840 --> 00:28:38,720
idiosyncratic actors are people who are

757
00:28:38,000 --> 00:28:42,159
just

758
00:28:38,720 --> 00:28:45,279
out there in terms of their beliefs and

759
00:28:42,159 --> 00:28:48,000
attitudes eco-terrorists

760
00:28:45,279 --> 00:28:48,640
many many fine eco-terrorists i'm sure

761
00:28:48,000 --> 00:28:49,840
but

762
00:28:48,640 --> 00:28:52,320
there is a school of thought in

763
00:28:49,840 --> 00:28:53,199
eco-terrorism that we really should just

764
00:28:52,320 --> 00:28:55,120
kill ourselves

765
00:28:53,200 --> 00:28:56,240
because we're killing the planet so it

766
00:28:55,120 --> 00:28:58,479
would be better

767
00:28:56,240 --> 00:29:00,480
to kill off all humans so that the

768
00:28:58,480 --> 00:29:03,840
planet could thrive

769
00:29:00,480 --> 00:29:07,200
i have sort of issues with that but um

770
00:29:03,840 --> 00:29:08,879
that's out there rogue states

771
00:29:07,200 --> 00:29:11,279
you would not necessarily think of them

772
00:29:08,880 --> 00:29:13,440
as an agent of killing everybody

773
00:29:11,279 --> 00:29:15,679
but they might want to kill a lot of

774
00:29:13,440 --> 00:29:17,760
people and may not be able to stop

775
00:29:15,679 --> 00:29:19,760
themselves once they get started

776
00:29:17,760 --> 00:29:20,879
also though super intelligence that sort

777
00:29:19,760 --> 00:29:22,720
of

778
00:29:20,880 --> 00:29:24,240
advanced general intelligence on

779
00:29:22,720 --> 00:29:26,640
steroids

780
00:29:24,240 --> 00:29:28,159
if that happened if we got into a race

781
00:29:26,640 --> 00:29:28,559
condition where artificial intelligence

782
00:29:28,159 --> 00:29:30,880
just

783
00:29:28,559 --> 00:29:32,799
got smarter and smarter it might get so

784
00:29:30,880 --> 00:29:36,799
smart it decided it didn't need us

785
00:29:32,799 --> 00:29:39,360
anymore um

786
00:29:36,799 --> 00:29:42,480
i won't go into depth here about the the

787
00:29:39,360 --> 00:29:44,080
discussion of risk versus resilience

788
00:29:42,480 --> 00:29:46,240
but i wanted to put that there because

789
00:29:44,080 --> 00:29:49,279
it does ground us a little bit

790
00:29:46,240 --> 00:29:51,279
in cyber security we work very much

791
00:29:49,279 --> 00:29:53,440
in the area risk assessment risk

792
00:29:51,279 --> 00:29:56,080
management risk analysis

793
00:29:53,440 --> 00:29:57,440
and in recent years the idea of

794
00:29:56,080 --> 00:30:01,279
resilience has been

795
00:29:57,440 --> 00:30:04,320
um very strongly

796
00:30:01,279 --> 00:30:06,080
advocated as this idea that we prepare

797
00:30:04,320 --> 00:30:08,000
and plan

798
00:30:06,080 --> 00:30:10,080
for bad things so that we can recover

799
00:30:08,000 --> 00:30:10,799
them from them and and go better in the

800
00:30:10,080 --> 00:30:12,879
future

801
00:30:10,799 --> 00:30:14,000
and they're acknowledging we don't know

802
00:30:12,880 --> 00:30:16,080
what's coming

803
00:30:14,000 --> 00:30:17,760
so you know we don't know what the next

804
00:30:16,080 --> 00:30:20,399
malware is going to be like what the

805
00:30:17,760 --> 00:30:21,919
next strain of ransomware is going to do

806
00:30:20,399 --> 00:30:23,360
uh you know or is it going to be brought

807
00:30:21,919 --> 00:30:24,720
into work by an employee who's been

808
00:30:23,360 --> 00:30:26,320
bribed by a criminal

809
00:30:24,720 --> 00:30:28,480
you know we don't know what's going to

810
00:30:26,320 --> 00:30:30,080
happen next so if we develop resilience

811
00:30:28,480 --> 00:30:31,919
we can respond to anything

812
00:30:30,080 --> 00:30:33,678
there are some good debates about which

813
00:30:31,919 --> 00:30:36,159
perspective is best here

814
00:30:33,679 --> 00:30:36,960
risk or resilience at facing existential

815
00:30:36,159 --> 00:30:39,039
risk

816
00:30:36,960 --> 00:30:40,320
one of the issues is that probabilities

817
00:30:39,039 --> 00:30:42,879
um

818
00:30:40,320 --> 00:30:43,600
are difficult to assess in this whole

819
00:30:42,880 --> 00:30:46,720
thing

820
00:30:43,600 --> 00:30:47,918
and um do you remember the probability

821
00:30:46,720 --> 00:30:49,600
tests that or

822
00:30:47,919 --> 00:30:50,960
the probability examples you would have

823
00:30:49,600 --> 00:30:51,520
in statistics where you're supposed to

824
00:30:50,960 --> 00:30:54,640
build

825
00:30:51,520 --> 00:30:56,399
uh pull a colored ball out of an urn of

826
00:30:54,640 --> 00:30:58,720
colored balls

827
00:30:56,399 --> 00:31:00,959
that's where we get to boston's

828
00:30:58,720 --> 00:31:04,960
vulnerable world hypothesis

829
00:31:00,960 --> 00:31:08,000
and the black ball um

830
00:31:04,960 --> 00:31:09,440
i was somewhat tempted to use black

831
00:31:08,000 --> 00:31:11,360
balls in the title of this talk but i

832
00:31:09,440 --> 00:31:14,159
thought that was kind of in bad taste

833
00:31:11,360 --> 00:31:14,959
and uh could be a misconception of years

834
00:31:14,159 --> 00:31:18,399
ago

835
00:31:14,960 --> 00:31:20,799
about future inventions

836
00:31:18,399 --> 00:31:22,320
so if we think of an urn containing

837
00:31:20,799 --> 00:31:24,320
balls each of which

838
00:31:22,320 --> 00:31:26,320
represents a new piece of technology

839
00:31:24,320 --> 00:31:27,279
there are yellow balls red balls green

840
00:31:26,320 --> 00:31:30,000
balls

841
00:31:27,279 --> 00:31:30,399
green ball you reach in you can't see

842
00:31:30,000 --> 00:31:33,919
through

843
00:31:30,399 --> 00:31:36,399
it's a it's an opaque urn

844
00:31:33,919 --> 00:31:37,760
so you reach in you pull out a ball it's

845
00:31:36,399 --> 00:31:40,559
a green ball that's

846
00:31:37,760 --> 00:31:42,559
technology that's really good no

847
00:31:40,559 --> 00:31:44,480
downsides great

848
00:31:42,559 --> 00:31:46,399
you pull out a red one that's clearly

849
00:31:44,480 --> 00:31:50,960
bad so you deal with it

850
00:31:46,399 --> 00:31:53,678
a black ball is one that is invariably

851
00:31:50,960 --> 00:31:55,600
or by default going to destroy the

852
00:31:53,679 --> 00:31:57,679
civilization that invents it

853
00:31:55,600 --> 00:31:59,519
right so it's really really bad stuff

854
00:31:57,679 --> 00:32:03,760
and he argues we haven't actually

855
00:31:59,519 --> 00:32:06,080
pulled a black ball out of the urn yet

856
00:32:03,760 --> 00:32:08,320
why will it destroy things though this

857
00:32:06,080 --> 00:32:09,039
is due to the semi-anarchic default

858
00:32:08,320 --> 00:32:10,879
condition

859
00:32:09,039 --> 00:32:13,200
again a term that boston's put out there

860
00:32:10,880 --> 00:32:16,320
that's very handy it describes

861
00:32:13,200 --> 00:32:17,760
today right limited capacity for

862
00:32:16,320 --> 00:32:20,480
preventive policing

863
00:32:17,760 --> 00:32:22,399
limited capacity for global governance

864
00:32:20,480 --> 00:32:25,519
diverse motivations

865
00:32:22,399 --> 00:32:27,199
throughout the world's population so

866
00:32:25,519 --> 00:32:29,279
if you look at the pandemic current

867
00:32:27,200 --> 00:32:31,840
pandemic um

868
00:32:29,279 --> 00:32:33,200
we've had difficulty controlling this we

869
00:32:31,840 --> 00:32:34,639
would certainly have difficulty

870
00:32:33,200 --> 00:32:38,960
controlling

871
00:32:34,640 --> 00:32:41,760
for a situation where somebody finds

872
00:32:38,960 --> 00:32:42,960
and he uses this example the cheap nuke

873
00:32:41,760 --> 00:32:46,240
suppose somebody

874
00:32:42,960 --> 00:32:50,159
pulls a ball out of the urn a technology

875
00:32:46,240 --> 00:32:52,960
which enables nuclear detonations

876
00:32:50,159 --> 00:32:53,440
using a commonly available substance

877
00:32:52,960 --> 00:32:55,120
right

878
00:32:53,440 --> 00:32:57,600
if it was suddenly very easy and cheap

879
00:32:55,120 --> 00:32:59,279
to make nukes

880
00:32:57,600 --> 00:33:01,760
somebody might try to do that and we'd

881
00:32:59,279 --> 00:33:03,519
have a hard time policing that

882
00:33:01,760 --> 00:33:04,879
we have a limited capacity for global

883
00:33:03,519 --> 00:33:07,600
governance

884
00:33:04,880 --> 00:33:09,120
look at what's happened with kobe um

885
00:33:07,600 --> 00:33:12,399
america left

886
00:33:09,120 --> 00:33:15,918
who china got blamed

887
00:33:12,399 --> 00:33:19,678
um other people got blamed and the

888
00:33:15,919 --> 00:33:22,559
ability to control things really

889
00:33:19,679 --> 00:33:24,080
has been shown to be weak and everybody

890
00:33:22,559 --> 00:33:26,399
or not everybody but there are many

891
00:33:24,080 --> 00:33:29,760
different diverse motivations

892
00:33:26,399 --> 00:33:30,559
uh out there want an autocracy some

893
00:33:29,760 --> 00:33:32,320
people

894
00:33:30,559 --> 00:33:34,320
want a dictatorship some people want to

895
00:33:32,320 --> 00:33:36,559
rule the world uh

896
00:33:34,320 --> 00:33:37,840
others don't some people have religious

897
00:33:36,559 --> 00:33:39,678
beliefs which conflict with

898
00:33:37,840 --> 00:33:41,439
non-religious beliefs and so on

899
00:33:39,679 --> 00:33:43,440
so he calls this the semi-anarchic

900
00:33:41,440 --> 00:33:45,279
default condition into which a piece of

901
00:33:43,440 --> 00:33:47,279
deadly technology

902
00:33:45,279 --> 00:33:48,720
arrives in the form of a black ball and

903
00:33:47,279 --> 00:33:50,399
we're in tough shape

904
00:33:48,720 --> 00:33:52,240
what do we do to prevent this global

905
00:33:50,399 --> 00:33:54,320
surveillance and policing you might be

906
00:33:52,240 --> 00:33:56,320
able to get on top of this

907
00:33:54,320 --> 00:33:57,519
but the challenge there is so great that

908
00:33:56,320 --> 00:34:00,480
you end up going well

909
00:33:57,519 --> 00:34:01,519
maybe we could use an advanced uh sorry

910
00:34:00,480 --> 00:34:03,760
an artificial in

911
00:34:01,519 --> 00:34:05,919
general intelligence for this a super

912
00:34:03,760 --> 00:34:07,039
intelligence could be used to do this

913
00:34:05,919 --> 00:34:09,280
real-time global

914
00:34:07,039 --> 00:34:10,239
surveillance and policing of emerging

915
00:34:09,280 --> 00:34:13,440
technologies

916
00:34:10,239 --> 00:34:14,000
so that they don't go badly wrong which

917
00:34:13,440 --> 00:34:15,440
that

918
00:34:14,000 --> 00:34:17,918
brings us right back to the control

919
00:34:15,440 --> 00:34:20,800
problem for artificial intelligence

920
00:34:17,918 --> 00:34:22,480
and we're left trying to figure out

921
00:34:20,800 --> 00:34:25,040
what's next

922
00:34:22,480 --> 00:34:26,240
i'll leave this here the authoritarian

923
00:34:25,040 --> 00:34:29,119
term

924
00:34:26,239 --> 00:34:30,239
as this temptation to think that you

925
00:34:29,119 --> 00:34:31,679
know really just a

926
00:34:30,239 --> 00:34:33,839
the only way we move forward is a

927
00:34:31,679 --> 00:34:35,359
powerful centralized system of control

928
00:34:33,839 --> 00:34:37,119
that's the only way of dealing with

929
00:34:35,359 --> 00:34:40,960
these big problems

930
00:34:37,119 --> 00:34:44,399
hopefully that is not the case

931
00:34:40,960 --> 00:34:45,280
so let's talk about how hackers save

932
00:34:44,399 --> 00:34:47,440
humanity

933
00:34:45,280 --> 00:34:50,240
this is one of many stories that could

934
00:34:47,440 --> 00:34:50,240
easily be written

935
00:34:50,399 --> 00:34:56,638
and i'm thinking we might have um the x

936
00:34:53,599 --> 00:34:59,599
files as in x risk files

937
00:34:56,639 --> 00:35:01,680
uh could be a graphic novel series of

938
00:34:59,599 --> 00:35:04,800
comics or short stories

939
00:35:01,680 --> 00:35:06,640
here's an example so

940
00:35:04,800 --> 00:35:08,480
the world's governments in order to slow

941
00:35:06,640 --> 00:35:11,279
global warming have created the

942
00:35:08,480 --> 00:35:14,079
stratospheric aerosol injection system

943
00:35:11,280 --> 00:35:14,880
so this has been tested in the labs to

944
00:35:14,079 --> 00:35:18,000
reduce

945
00:35:14,880 --> 00:35:20,640
solar radiation it's called says and it

946
00:35:18,000 --> 00:35:22,400
uses an ai to decide when where and how

947
00:35:20,640 --> 00:35:25,118
much air or soul should be released

948
00:35:22,400 --> 00:35:28,000
in order to create this gradual effect

949
00:35:25,119 --> 00:35:31,040
by which the world's

950
00:35:28,000 --> 00:35:33,520
cooling takes place to give us more time

951
00:35:31,040 --> 00:35:35,279
to develop our solutions get rid of

952
00:35:33,520 --> 00:35:37,520
fossil fuels and so on

953
00:35:35,280 --> 00:35:39,440
however days before sas goes live a

954
00:35:37,520 --> 00:35:42,400
hacker finds a zero day

955
00:35:39,440 --> 00:35:43,359
vulnerability in the operating system

956
00:35:42,400 --> 00:35:46,960
that this ai

957
00:35:43,359 --> 00:35:48,640
is rumored to run on uh

958
00:35:46,960 --> 00:35:50,480
as far as i can tell most ais are

959
00:35:48,640 --> 00:35:52,799
running on linux now but

960
00:35:50,480 --> 00:35:54,800
correct me if i'm wrong but anyway the

961
00:35:52,800 --> 00:35:55,520
hacker finds a zero day in the operating

962
00:35:54,800 --> 00:35:56,960
system

963
00:35:55,520 --> 00:35:59,280
she reaches out to hackers in other

964
00:35:56,960 --> 00:36:01,200
countries to confirm

965
00:35:59,280 --> 00:36:02,560
yes this vulnerability does have the

966
00:36:01,200 --> 00:36:06,240
potential to enable

967
00:36:02,560 --> 00:36:08,640
a faulty cess aerosol release order

968
00:36:06,240 --> 00:36:09,759
so this could lead to things going very

969
00:36:08,640 --> 00:36:11,598
very badly wrong

970
00:36:09,760 --> 00:36:13,040
this group of hackers then takes this to

971
00:36:11,599 --> 00:36:14,480
the stace executives

972
00:36:13,040 --> 00:36:16,480
they don't respond to the hacker's

973
00:36:14,480 --> 00:36:18,160
concerns so the hackers go public with a

974
00:36:16,480 --> 00:36:20,000
dramatic proof of concept

975
00:36:18,160 --> 00:36:22,160
just in time to prevent a potentially

976
00:36:20,000 --> 00:36:26,240
catastrophic launch

977
00:36:22,160 --> 00:36:28,480
so by poking and prodding by not taking

978
00:36:26,240 --> 00:36:30,799
at their word people who make assurances

979
00:36:28,480 --> 00:36:33,839
about the safety and security of things

980
00:36:30,800 --> 00:36:35,920
hackers save humanity

981
00:36:33,839 --> 00:36:37,440
so i'm getting close to the end of time

982
00:36:35,920 --> 00:36:40,240
here so

983
00:36:37,440 --> 00:36:40,960
the end of my time sorry at the end of

984
00:36:40,240 --> 00:36:44,319
time

985
00:36:40,960 --> 00:36:47,520
uh just some pointers here

986
00:36:44,320 --> 00:36:49,040
become a public intest technologist

987
00:36:47,520 --> 00:36:51,040
particularly i think people who've been

988
00:36:49,040 --> 00:36:53,359
in cyber security for a while

989
00:36:51,040 --> 00:36:56,000
may want to think about this and kudos

990
00:36:53,359 --> 00:36:57,759
to bruce schneier who a few years ago

991
00:36:56,000 --> 00:36:59,760
put together this public interest

992
00:36:57,760 --> 00:37:01,200
technology resource page and started

993
00:36:59,760 --> 00:37:04,880
talking about this

994
00:37:01,200 --> 00:37:08,640
the need for governments to

995
00:37:04,880 --> 00:37:11,760
get input in to this problem

996
00:37:08,640 --> 00:37:15,118
of um

997
00:37:11,760 --> 00:37:18,640
technology that goes wrong and

998
00:37:15,119 --> 00:37:21,119
research it educate people engage people

999
00:37:18,640 --> 00:37:22,400
and enlighten people so that the people

1000
00:37:21,119 --> 00:37:24,720
who make polity

1001
00:37:22,400 --> 00:37:25,680
decisions and decide where to spend

1002
00:37:24,720 --> 00:37:28,560
money

1003
00:37:25,680 --> 00:37:29,279
better understand how much things could

1004
00:37:28,560 --> 00:37:33,040
possibly

1005
00:37:29,280 --> 00:37:37,359
go wrong um public interest tech

1006
00:37:33,040 --> 00:37:40,880
dot org and uh these resources

1007
00:37:37,359 --> 00:37:42,640
um again thanks bruce

1008
00:37:40,880 --> 00:37:45,760
you can google and find this resources

1009
00:37:42,640 --> 00:37:48,560
on existential risk it's 170 pages

1010
00:37:45,760 --> 00:37:49,520
of great content that bruce has pulled

1011
00:37:48,560 --> 00:37:52,720
together he pulled this

1012
00:37:49,520 --> 00:37:54,320
together back in 2015 on this topic

1013
00:37:52,720 --> 00:37:56,078
not included in there because they are

1014
00:37:54,320 --> 00:37:57,280
more recent is bostrom's super

1015
00:37:56,079 --> 00:38:00,640
intelligence

1016
00:37:57,280 --> 00:38:03,680
phil torres on morality and this recent

1017
00:38:00,640 --> 00:38:06,799
book by toby award

1018
00:38:03,680 --> 00:38:10,399
with that one more thing do not fear

1019
00:38:06,800 --> 00:38:11,599
aida's here uh to reassure people who

1020
00:38:10,400 --> 00:38:12,240
are worried about the asteroid in the

1021
00:38:11,599 --> 00:38:14,880
beginning

1022
00:38:12,240 --> 00:38:16,240
there is the international asteroid

1023
00:38:14,880 --> 00:38:19,119
impact and deflection

1024
00:38:16,240 --> 00:38:20,720
assessment collaboration ada which next

1025
00:38:19,119 --> 00:38:24,560
year will launch this spacecraft

1026
00:38:20,720 --> 00:38:26,640
dart to test a system of diverting

1027
00:38:24,560 --> 00:38:29,680
asteroids

1028
00:38:26,640 --> 00:38:33,279
so uh with that i'd like to say

1029
00:38:29,680 --> 00:38:34,879
thank you corncon 2020 i am uh stephen

1030
00:38:33,280 --> 00:38:36,000
cobb you can reach out to me in lots of

1031
00:38:34,880 --> 00:38:40,079
different ways

1032
00:38:36,000 --> 00:38:42,560
um and a huge thanks to uh shay cobb for

1033
00:38:40,079 --> 00:38:43,599
her wisdom advice and sanity checks on

1034
00:38:42,560 --> 00:38:47,520
this presentation

1035
00:38:43,599 --> 00:38:52,240
and everything else that i do

1036
00:38:47,520 --> 00:38:54,800
so now i'm in an interesting situation

1037
00:38:52,240 --> 00:38:58,560
because i'm not quite sure

1038
00:38:54,800 --> 00:38:58,560
what i do next but i've got some chat

1039
00:38:59,119 --> 00:39:05,839
it says thank you stephen and um let me

1040
00:39:02,480 --> 00:39:08,000
check my watch here yes 321

1041
00:39:05,839 --> 00:39:09,759
um i'm not sure if we're taking

1042
00:39:08,000 --> 00:39:12,560
questions of where we take them

1043
00:39:09,760 --> 00:39:15,119
um i'd certainly take any questions you

1044
00:39:12,560 --> 00:39:17,440
have about this offline

1045
00:39:15,119 --> 00:39:19,839
usually most active on twitter that's

1046
00:39:17,440 --> 00:39:19,839
the cob

1047
00:39:20,640 --> 00:39:25,759
and uh let me stop sharing now

1048
00:39:26,320 --> 00:39:31,280
are you able back to the screen where

1049
00:39:28,000 --> 00:39:31,280
we've got some chat going on

1050
00:39:31,680 --> 00:39:35,200
uh hold on yes can i hear you yeah this

1051
00:39:34,560 --> 00:39:38,000
is john

1052
00:39:35,200 --> 00:39:39,040
um yeah so you see the q a window as

1053
00:39:38,000 --> 00:39:42,160
well

1054
00:39:39,040 --> 00:39:46,000
um i've got the chat window

1055
00:39:42,160 --> 00:39:46,000
oh a window

1056
00:39:46,500 --> 00:39:51,760
[Laughter]

1057
00:39:49,920 --> 00:39:53,760
first question in the q a window is how

1058
00:39:51,760 --> 00:39:56,720
many beers did you spill on your first

1059
00:39:53,760 --> 00:39:58,560
aeroflot flight to moscow the correct

1060
00:39:56,720 --> 00:40:00,399
answer is zero

1061
00:39:58,560 --> 00:40:02,400
i do know a gentleman on that flight who

1062
00:40:00,400 --> 00:40:05,440
who did spill some beer but i won't name

1063
00:40:02,400 --> 00:40:09,040
names so

1064
00:40:05,440 --> 00:40:12,800
yes and christopher here great question

1065
00:40:09,040 --> 00:40:14,560
um how do you prevent genetic hacking

1066
00:40:12,800 --> 00:40:15,520
without having a police state with no

1067
00:40:14,560 --> 00:40:18,319
privacy

1068
00:40:15,520 --> 00:40:19,759
and i mean that really gets to the heart

1069
00:40:18,319 --> 00:40:23,599
of a lot of this

1070
00:40:19,760 --> 00:40:26,960
which is that existential risks

1071
00:40:23,599 --> 00:40:30,319
are difficult

1072
00:40:26,960 --> 00:40:32,880
in in many ways and and i you know

1073
00:40:30,319 --> 00:40:33,759
sotos in his talk uh def con a few years

1074
00:40:32,880 --> 00:40:38,079
ago

1075
00:40:33,760 --> 00:40:39,440
uh i think made that clear as well how

1076
00:40:38,079 --> 00:40:43,520
does a democratic

1077
00:40:39,440 --> 00:40:46,240
open society share new technology

1078
00:40:43,520 --> 00:40:47,920
and research which we don't want to stop

1079
00:40:46,240 --> 00:40:48,879
sharing because the benefits could be

1080
00:40:47,920 --> 00:40:51,200
huge

1081
00:40:48,880 --> 00:40:52,640
the losses could be huge so there's a

1082
00:40:51,200 --> 00:40:55,680
like if we stopped

1083
00:40:52,640 --> 00:40:57,920
research or controlled research

1084
00:40:55,680 --> 00:40:59,279
and kept it to just a few people and

1085
00:40:57,920 --> 00:41:01,040
kept it monitored

1086
00:40:59,280 --> 00:41:04,160
we might miss great opportunities

1087
00:41:01,040 --> 00:41:06,560
because we all know that

1088
00:41:04,160 --> 00:41:08,000
things come out of left field because so

1089
00:41:06,560 --> 00:41:09,119
many people are poking around in

1090
00:41:08,000 --> 00:41:12,079
technology

1091
00:41:09,119 --> 00:41:13,680
um i i sadly don't have then the answer

1092
00:41:12,079 --> 00:41:17,280
to that question

1093
00:41:13,680 --> 00:41:17,919
i do think we really have to get a move

1094
00:41:17,280 --> 00:41:21,680
on

1095
00:41:17,920 --> 00:41:23,839
lobbying and pushing as citizens

1096
00:41:21,680 --> 00:41:25,118
in every country to get our governments

1097
00:41:23,839 --> 00:41:26,640
to work together

1098
00:41:25,119 --> 00:41:28,319
and this is where i'd bring it back to

1099
00:41:26,640 --> 00:41:32,240
malware

1100
00:41:28,319 --> 00:41:34,880
and i would say this if if we can't stop

1101
00:41:32,240 --> 00:41:36,399
as a planet malware particularly

1102
00:41:34,880 --> 00:41:37,280
ransomware well let's just take

1103
00:41:36,400 --> 00:41:40,480
ransomware

1104
00:41:37,280 --> 00:41:42,960
if we can't stop ransomware

1105
00:41:40,480 --> 00:41:45,440
through intergovernmental uh

1106
00:41:42,960 --> 00:41:47,440
international cooperation

1107
00:41:45,440 --> 00:41:49,680
then we're not really going to do much

1108
00:41:47,440 --> 00:41:52,720
of a good job with existential risk

1109
00:41:49,680 --> 00:41:54,720
that ada program to deflect asteroids or

1110
00:41:52,720 --> 00:41:57,680
test deflecting asteroids

1111
00:41:54,720 --> 00:41:59,200
uh that was the united states and the

1112
00:41:57,680 --> 00:42:01,279
european union

1113
00:41:59,200 --> 00:42:03,040
that nearly broke apart it's come back

1114
00:42:01,280 --> 00:42:06,240
together

1115
00:42:03,040 --> 00:42:09,520
so we can see there are problems

1116
00:42:06,240 --> 00:42:10,399
of democratic countries with an open

1117
00:42:09,520 --> 00:42:13,599
society

1118
00:42:10,400 --> 00:42:14,880
and respect for privacy policing

1119
00:42:13,599 --> 00:42:17,280
themselves

1120
00:42:14,880 --> 00:42:18,480
um we need to to work on that and we we

1121
00:42:17,280 --> 00:42:19,920
do need to push i think

1122
00:42:18,480 --> 00:42:22,079
first and foremost for international

1123
00:42:19,920 --> 00:42:23,119
cooperation even though we might not

1124
00:42:22,079 --> 00:42:26,880
like people

1125
00:42:23,119 --> 00:42:28,720
we did it perhaps a hopeful sign is the

1126
00:42:26,880 --> 00:42:31,839
fact we did have

1127
00:42:28,720 --> 00:42:33,279
you know non-proliferation treaties uh

1128
00:42:31,839 --> 00:42:36,240
around nuclear weapons

1129
00:42:33,280 --> 00:42:38,000
we did agree to stop the mass production

1130
00:42:36,240 --> 00:42:40,078
of nuclear weapons

1131
00:42:38,000 --> 00:42:41,760
by agreeing and making agreements with

1132
00:42:40,079 --> 00:42:44,960
people that we were

1133
00:42:41,760 --> 00:42:48,079
at war within a cold war so we have to

1134
00:42:44,960 --> 00:42:48,079
do more in that space

1135
00:42:48,400 --> 00:42:53,680
so that's sort of my non-answer sorry

1136
00:42:56,839 --> 00:42:59,839
okay

1137
00:43:00,240 --> 00:43:04,000
uh hearing no more questions i i think i

1138
00:43:02,800 --> 00:43:06,880
would i would

1139
00:43:04,000 --> 00:43:09,599
you know call my uh my presentation over

1140
00:43:06,880 --> 00:43:11,680
i really appreciate this opportunity

1141
00:43:09,599 --> 00:43:14,000
uh very impressed with the technology

1142
00:43:11,680 --> 00:43:14,799
here uh hopefully it will be used for

1143
00:43:14,000 --> 00:43:17,680
good and not

1144
00:43:14,800 --> 00:43:19,920
ill wish everybody a good day for the

1145
00:43:17,680 --> 00:43:22,879
rest of the day

1146
00:43:19,920 --> 00:43:23,599
all right thanks steven it was great to

1147
00:43:22,880 --> 00:43:25,680
have you here

1148
00:43:23,599 --> 00:43:27,280
um i'm glad we were able to finally get

1149
00:43:25,680 --> 00:43:29,759
you uh

1150
00:43:27,280 --> 00:43:31,280
as a speaker and an attendee there are

1151
00:43:29,760 --> 00:43:34,880
more discussions going on

1152
00:43:31,280 --> 00:43:37,200
in uh the discord for the

1153
00:43:34,880 --> 00:43:38,560
tracks one two and three track three is

1154
00:43:37,200 --> 00:43:41,359
about to start up

1155
00:43:38,560 --> 00:43:44,078
with uh phil polstra talking about

1156
00:43:41,359 --> 00:43:46,799
forensics

1157
00:43:44,079 --> 00:43:48,960
track two we have george simmons uh

1158
00:43:46,800 --> 00:43:50,800
starting up the new cyber engineer

1159
00:43:48,960 --> 00:43:52,319
and in this track we will have wynn

1160
00:43:50,800 --> 00:43:58,560
schwartau

1161
00:43:52,319 --> 00:43:58,560
in about five minutes thank you

