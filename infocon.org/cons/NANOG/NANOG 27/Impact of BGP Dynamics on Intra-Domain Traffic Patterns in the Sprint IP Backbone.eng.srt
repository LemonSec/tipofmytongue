1
00:00:00,740 --> 00:00:05,609
okay so this is work that we did at

2
00:00:03,360 --> 00:00:10,800
sprint labs in collaboration with sprint

3
00:00:05,609 --> 00:00:13,230
<font color="#CCCCCC">McKen gineering and Dave Meyer I'll</font>

4
00:00:10,800 --> 00:00:14,849
start<font color="#E5E5E5"> off with a statement</font><font color="#CCCCCC"> about what</font>

5
00:00:13,230 --> 00:00:17,460
problem we're trying to address what

6
00:00:14,849 --> 00:00:19,770
questions were trying to solve what

7
00:00:17,460 --> 00:00:22,080
questions were trying to answer and then

8
00:00:19,770 --> 00:00:24,740
go into<font color="#E5E5E5"> a methodology of how we collect</font>

9
00:00:22,080 --> 00:00:26,880
our routing data and our packet data and

10
00:00:24,740 --> 00:00:29,609
how we analyze it to answer our

11
00:00:26,880 --> 00:00:31,859
questions and then<font color="#CCCCCC"> I'll present our</font>

12
00:00:29,609 --> 00:00:33,899
results and try to explain why we see

13
00:00:31,859 --> 00:00:37,489
<font color="#E5E5E5">those results and then end with</font>

14
00:00:33,899 --> 00:00:37,489
implications of our findings

15
00:00:39,250 --> 00:00:44,080
there's<font color="#E5E5E5"> been a fair amount of prior work</font>

16
00:00:41,770 --> 00:00:45,880
that's looked at the growth of routing

17
00:00:44,080 --> 00:00:48,400
tables and the<font color="#E5E5E5"> growth in the number of</font>

18
00:00:45,880 --> 00:00:51,280
<font color="#CCCCCC">BGP updates there's the work by</font><font color="#E5E5E5"> Jeff</font>

19
00:00:48,400 --> 00:00:54,059
<font color="#E5E5E5">Houston and Olaf Mandel and also the</font>

20
00:00:51,280 --> 00:00:59,530
work by<font color="#CCCCCC"> cowie that showed the</font><font color="#E5E5E5"> impact of</font>

21
00:00:54,059 --> 00:01:02,709
the<font color="#CCCCCC"> nim del worm on</font><font color="#E5E5E5"> BGP updates the</font>

22
00:00:59,530 --> 00:01:05,890
concern here<font color="#CCCCCC"> is that excessive bgp</font>

23
00:01:02,710 --> 00:01:08,770
updates<font color="#E5E5E5"> can cause routing and traffic</font>

24
00:01:05,890 --> 00:01:12,490
instability this is<font color="#E5E5E5"> especially troubling</font>

25
00:01:08,770 --> 00:01:15,369
when you get<font color="#E5E5E5"> labovitz as work on showing</font>

26
00:01:12,490 --> 00:01:21,369
how slow andreas writing convergence can

27
00:01:15,369 --> 00:01:24,340
be if if there are a lot of<font color="#CCCCCC"> BGP updates</font>

28
00:01:21,369 --> 00:01:27,009
then the issue is that rots may keep

29
00:01:24,340 --> 00:01:29,560
flapping and addresses may become

30
00:01:27,009 --> 00:01:32,039
unreachable for short periods of time

31
00:01:29,560 --> 00:01:35,619
and that can affect traffic stability

32
00:01:32,039 --> 00:01:38,979
however within a single<font color="#CCCCCC"> a/s bgp updates</font>

33
00:01:35,619 --> 00:01:40,990
can also be a concern if<font color="#CCCCCC"> there if a lot</font>

34
00:01:38,979 --> 00:01:43,929
of<font color="#CCCCCC"> bgp updates are being received at an</font>

35
00:01:40,990 --> 00:01:45,640
autonomous system then those<font color="#CCCCCC"> bgp updates</font>

36
00:01:43,930 --> 00:01:49,299
will cause a certain amount of protocol

37
00:01:45,640 --> 00:01:52,090
traffic each<font color="#CCCCCC"> bgp update whoa will take</font>

38
00:01:49,299 --> 00:01:54,810
up a packet but<font color="#E5E5E5"> I really don't think</font>

39
00:01:52,090 --> 00:01:58,150
that's a huge concern but another

40
00:01:54,810 --> 00:02:00,909
concern is that it may<font color="#E5E5E5"> increase the</font>

41
00:01:58,150 --> 00:02:04,280
router cpu utilization

42
00:02:00,909 --> 00:02:08,630
also every time a<font color="#E5E5E5"> BGP update is heard a</font>

43
00:02:04,280 --> 00:02:11,840
tale rjs the intro domain rods may also

44
00:02:08,630 --> 00:02:13,700
be affected the exit<font color="#E5E5E5"> point for a</font>

45
00:02:11,840 --> 00:02:17,180
particular router for a particular

46
00:02:13,700 --> 00:02:19,310
destination address may change for a for

47
00:02:17,180 --> 00:02:21,800
a large AAS and that can cause<font color="#CCCCCC"> rocky</font>

48
00:02:19,310 --> 00:02:25,580
<font color="#CCCCCC">tree convergence and eventually also</font>

49
00:02:21,800 --> 00:02:28,010
<font color="#E5E5E5">lead to shift in traffic it's this last</font>

50
00:02:25,580 --> 00:02:30,230
issue that we're concerned<font color="#E5E5E5"> with and I'll</font>

51
00:02:28,010 --> 00:02:32,290
now go<font color="#CCCCCC"> into an</font><font color="#E5E5E5"> example of how this can</font>

52
00:02:30,230 --> 00:02:32,290
happen

53
00:02:36,319 --> 00:02:44,719
so this<font color="#CCCCCC"> is a toy example of considered</font>

54
00:02:41,180 --> 00:02:47,180
the sprint backbone in the<font color="#E5E5E5"> middle where</font>

55
00:02:44,719 --> 00:02:49,939
I'm showing three pops and we've got

56
00:02:47,180 --> 00:02:54,260
some customer a s that connects to us at

57
00:02:49,939 --> 00:02:56,959
<font color="#CCCCCC">puppy and a pop see and let's say that</font>

58
00:02:54,260 --> 00:02:59,989
for now the btp announcements<font color="#CCCCCC"> have been</font>

59
00:02:56,959 --> 00:03:02,569
set up such<font color="#CCCCCC"> that traffic going</font><font color="#E5E5E5"> to this</font>

60
00:02:59,989 --> 00:03:07,310
<font color="#CCCCCC">customer will exit the sprint backbone</font>

61
00:03:02,569 --> 00:03:09,918
at for every trace now the goal of our

62
00:03:07,310 --> 00:03:11,840
work is to try and identify the traffic

63
00:03:09,919 --> 00:03:15,500
variability that is caused by<font color="#CCCCCC"> BGP</font>

64
00:03:11,840 --> 00:03:18,019
updates and this ingress linked to

65
00:03:15,500 --> 00:03:20,419
multiple egress vana will reflect that

66
00:03:18,019 --> 00:03:22,489
variability and the variability due to

67
00:03:20,419 --> 00:03:26,209
that's already present in<font color="#CCCCCC"> the traffic</font>

68
00:03:22,489 --> 00:03:28,849
itself so we do this calculation twice

69
00:03:26,209 --> 00:03:32,000
we first do this calculation with a

70
00:03:28,849 --> 00:03:35,179
static<font color="#CCCCCC"> bgp table that we apply</font><font color="#E5E5E5"> to the</font>

71
00:03:32,000 --> 00:03:37,870
whole trace and then we get<font color="#CCCCCC"> one ingress</font>

72
00:03:35,180 --> 00:03:41,419
linked to multiple egress<font color="#CCCCCC"> pop fan up and</font>

73
00:03:37,870 --> 00:03:44,509
then we again do this calculation using

74
00:03:41,419 --> 00:03:47,120
a dynamic<font color="#E5E5E5"> bgp table that</font><font color="#CCCCCC"> is a bgp table</font>

75
00:03:44,509 --> 00:03:49,099
that we continually update and that we

76
00:03:47,120 --> 00:03:51,530
correlate the data packets along the

77
00:03:49,099 --> 00:03:53,959
length of<font color="#E5E5E5"> the trace and now we've got</font>

78
00:03:51,530 --> 00:03:56,419
two sets of ingress<font color="#CCCCCC"> linked to egress pop</font>

79
00:03:53,959 --> 00:03:58,699
fan outs one shows the variability

80
00:03:56,419 --> 00:04:00,859
inherent in the traffic the other shows

81
00:03:58,699 --> 00:04:03,259
the variability that's both inherent in

82
00:04:00,859 --> 00:04:06,079
the traffic<font color="#E5E5E5"> and is caused by</font><font color="#CCCCCC"> bgp updates</font>

83
00:04:03,259 --> 00:04:08,179
we subtract these two and then as a

84
00:04:06,079 --> 00:04:11,650
<font color="#E5E5E5">result we get the variability that's due</font>

85
00:04:08,180 --> 00:04:11,650
to the<font color="#E5E5E5"> bgp updates</font>

86
00:04:14,430 --> 00:04:21,730
now<font color="#CCCCCC"> I'll present</font><font color="#E5E5E5"> our results and try to</font>

87
00:04:17,230 --> 00:04:23,140
explain why we see them<font color="#E5E5E5"> I'll be</font>

88
00:04:21,730 --> 00:04:26,140
presenting results from one<font color="#E5E5E5"> particular</font>

89
00:04:23,140 --> 00:04:27,760
packet trace that was<font color="#E5E5E5"> 22 hours long that</font>

90
00:04:26,140 --> 00:04:30,400
we collected on august sixth of last

91
00:04:27,760 --> 00:04:35,469
year the average link utilization was

92
00:04:30,400 --> 00:04:37,570
about 112 megabits per second we we did

93
00:04:35,470 --> 00:04:40,750
look<font color="#E5E5E5"> at five other packet races as</font><font color="#CCCCCC"> well</font>

94
00:04:37,570 --> 00:04:45,099
and the results are similar we've made

95
00:04:40,750 --> 00:04:47,680
<font color="#E5E5E5">to two main approximations here one main</font>

96
00:04:45,100 --> 00:04:49,990
approximation that we made is<font color="#CCCCCC"> that we</font>

97
00:04:47,680 --> 00:04:53,980
calculate the<font color="#E5E5E5"> BGP table in the dynamic</font>

98
00:04:49,990 --> 00:04:56,410
<font color="#CCCCCC">BGP case every 20 minutes the reason for</font>

99
00:04:53,980 --> 00:05:02,670
that is<font color="#E5E5E5"> this particular packet trace has</font>

100
00:04:56,410 --> 00:05:05,950
about<font color="#CCCCCC"> 2 billion a lot of packets and and</font>

101
00:05:02,670 --> 00:05:08,770
basically<font color="#CCCCCC"> by calculating the</font><font color="#E5E5E5"> BGP table</font>

102
00:05:05,950 --> 00:05:10,840
every 20 minutes we get to aggregate all

103
00:05:08,770 --> 00:05:12,760
of the packets into a set of destination

104
00:05:10,840 --> 00:05:14,859
addresses so if there were 50 packets to

105
00:05:12,760 --> 00:05:16,330
a destination address we only need to

106
00:05:14,860 --> 00:05:19,000
look<font color="#E5E5E5"> up that one destination address</font>

107
00:05:16,330 --> 00:05:21,250
once instead of 50 times we have<font color="#E5E5E5"> done</font>

108
00:05:19,000 --> 00:05:23,470
<font color="#E5E5E5">this calculation for small check</font>

109
00:05:21,250 --> 00:05:25,990
sections of the trays for finer time

110
00:05:23,470 --> 00:05:28,600
grain time granularities and we see<font color="#E5E5E5"> no</font>

111
00:05:25,990 --> 00:05:30,370
difference in our results the other

112
00:05:28,600 --> 00:05:33,669
approximation that we make is that we

113
00:05:30,370 --> 00:05:36,700
only consider<font color="#CCCCCC"> 99% of the traffic the</font>

114
00:05:33,669 --> 00:05:38,650
reason<font color="#E5E5E5"> for that</font><font color="#CCCCCC"> is out of the 200,000</font>

115
00:05:36,700 --> 00:05:41,710
destination<font color="#E5E5E5"> addresses in this 22 our</font>

116
00:05:38,650 --> 00:05:43,330
packet race only 30,000 of these

117
00:05:41,710 --> 00:05:45,640
destination addresses<font color="#E5E5E5"> account for</font>

118
00:05:43,330 --> 00:05:49,560
ninety-nine percent of the traffic so

119
00:05:45,640 --> 00:05:49,560
<font color="#CCCCCC">that saves us</font><font color="#E5E5E5"> a lot in computation time</font>

120
00:05:50,119 --> 00:05:59,909
<font color="#E5E5E5">I'll start with a graph of the BGP</font>

121
00:05:54,360 --> 00:06:02,309
updates that we see the<font color="#CCCCCC"> first graph</font>

122
00:05:59,909 --> 00:06:05,459
shows the number of<font color="#E5E5E5"> ibgp updates that we</font>

123
00:06:02,309 --> 00:06:10,469
see during a week around the trace

124
00:06:05,459 --> 00:06:12,330
collection time now this<font color="#E5E5E5"> is this is from</font>

125
00:06:10,469 --> 00:06:14,639
the<font color="#CCCCCC"> Zebras software speaker that</font>

126
00:06:12,330 --> 00:06:16,229
connects<font color="#CCCCCC"> to this router so over here</font>

127
00:06:14,639 --> 00:06:19,520
<font color="#CCCCCC">we're actually showing the number of</font>

128
00:06:16,229 --> 00:06:22,050
changes to<font color="#CCCCCC"> the writing-table</font><font color="#E5E5E5"> itself</font>

129
00:06:19,520 --> 00:06:24,899
along the y-axis we<font color="#CCCCCC"> show the</font><font color="#E5E5E5"> number of</font>

130
00:06:22,050 --> 00:06:28,080
vgp events clumped up in two 20-minute

131
00:06:24,899 --> 00:06:31,889
<font color="#E5E5E5">bends and we see</font><font color="#CCCCCC"> that there's a</font>

132
00:06:28,080 --> 00:06:37,198
continuous noise of about about a

133
00:06:31,889 --> 00:06:39,569
hundred 133 updates every minute and we

134
00:06:37,199 --> 00:06:42,389
see<font color="#CCCCCC"> that there</font><font color="#E5E5E5"> are periods of spikes</font>

135
00:06:39,569 --> 00:06:46,709
where the total number of updates can go

136
00:06:42,389 --> 00:06:50,069
as high as 9,000 updates per minute we

137
00:06:46,709 --> 00:06:51,569
think that these spikes may be due<font color="#CCCCCC"> to</font>

138
00:06:50,069 --> 00:06:53,459
maintenance windows of various eyes

139
00:06:51,569 --> 00:06:58,139
<font color="#CCCCCC">peace they tend to happen around</font>

140
00:06:53,459 --> 00:07:02,699
saturday or sunday we've also we've also

141
00:06:58,139 --> 00:07:06,719
made graphs of<font color="#CCCCCC"> BGP updates over a whole</font>

142
00:07:02,699 --> 00:07:10,559
month and we see similar patterns the

143
00:07:06,719 --> 00:07:13,819
second graph here is a histogram of the

144
00:07:10,559 --> 00:07:17,490
number<font color="#E5E5E5"> of BGP updates and we see that</font>

145
00:07:13,819 --> 00:07:20,159
along<font color="#E5E5E5"> the iacta the x-axis is the number</font>

146
00:07:17,490 --> 00:07:24,439
of<font color="#E5E5E5"> updates along the y-axis is the</font>

147
00:07:20,159 --> 00:07:27,719
histogram and we see<font color="#CCCCCC"> that the average of</font>

148
00:07:24,439 --> 00:07:31,800
133 every<font color="#E5E5E5"> minute this isn't a</font><font color="#CCCCCC"> 10 minute</font>

149
00:07:27,719 --> 00:07:34,949
bin is fairly common and the outliers of

150
00:07:31,800 --> 00:07:37,969
thousands of updates per minute are

151
00:07:34,949 --> 00:07:37,969
relatively rare

152
00:07:39,640 --> 00:07:45,770
so these are graphs for the amount of

153
00:07:43,580 --> 00:07:48,770
traffic that went from this<font color="#E5E5E5"> particular</font>

154
00:07:45,770 --> 00:07:52,130
ingress link that we measured to a

155
00:07:48,770 --> 00:07:55,130
particular egress pop and we have this

156
00:07:52,130 --> 00:07:58,820
same<font color="#CCCCCC"> set of graphs for all of the egress</font>

157
00:07:55,130 --> 00:08:01,790
pops in the sprint backbone now this top

158
00:07:58,820 --> 00:08:03,349
graph shows along the x-axis<font color="#E5E5E5"> the time</font>

159
00:08:01,790 --> 00:08:06,290
along the trace from the beginning<font color="#CCCCCC"> to</font>

160
00:08:03,350 --> 00:08:10,100
the end of 22 hours and along the y-axis

161
00:08:06,290 --> 00:08:14,510
is the<font color="#E5E5E5"> number of bytes and over here it</font>

162
00:08:10,100 --> 00:08:19,940
goes between 10 goes between 100

163
00:08:14,510 --> 00:08:21,620
megabytes to 200 megabytes oops and we

164
00:08:19,940 --> 00:08:24,110
see that<font color="#E5E5E5"> there are two lines on top of</font>

165
00:08:21,620 --> 00:08:26,690
<font color="#E5E5E5">each other here the red line shows the</font>

166
00:08:24,110 --> 00:08:29,480
number of<font color="#E5E5E5"> bytes that went from this</font>

167
00:08:26,690 --> 00:08:31,700
ingress link to this egress pub when

168
00:08:29,480 --> 00:08:36,050
using a BGP table that was continuously

169
00:08:31,700 --> 00:08:37,909
updated and the green line shows the

170
00:08:36,049 --> 00:08:40,549
<font color="#E5E5E5">number of bytes that went from this</font>

171
00:08:37,909 --> 00:08:42,978
ingress link to this egress pot when we

172
00:08:40,549 --> 00:08:45,890
were using<font color="#E5E5E5"> a fixed BGP table across the</font>

173
00:08:42,979 --> 00:08:48,200
<font color="#CCCCCC">length of the trace we see</font><font color="#E5E5E5"> that there is</font>

174
00:08:45,890 --> 00:08:50,060
a lot of variability that's inherent in

175
00:08:48,200 --> 00:08:52,160
the traffic itself here and the two

176
00:08:50,060 --> 00:08:54,619
lines are on top of<font color="#CCCCCC"> each other so when</font>

177
00:08:52,160 --> 00:08:57,920
we subtract out the two lines we get the

178
00:08:54,620 --> 00:09:01,520
second graph which shows the<font color="#E5E5E5"> difference</font>

179
00:08:57,920 --> 00:09:05,510
<font color="#CCCCCC">between the two</font><font color="#E5E5E5"> graphs and so if there's</font>

180
00:09:01,520 --> 00:09:08,060
a<font color="#E5E5E5"> positive value on the second graph</font>

181
00:09:05,510 --> 00:09:10,189
that indicates that more traffic went to

182
00:09:08,060 --> 00:09:13,280
this particular egress pop as a<font color="#E5E5E5"> result</font>

183
00:09:10,190 --> 00:09:15,440
<font color="#E5E5E5">of a BGP change or if the graph is</font>

184
00:09:13,280 --> 00:09:17,510
negative it shows that less traffic went

185
00:09:15,440 --> 00:09:18,899
to the Siegrist pop as a result<font color="#E5E5E5"> of a BGP</font>

186
00:09:17,510 --> 00:09:22,379
change

187
00:09:18,899 --> 00:09:26,509
we see<font color="#CCCCCC"> that the</font><font color="#E5E5E5"> y-axis scale on the</font>

188
00:09:22,379 --> 00:09:30,180
second graph is about<font color="#CCCCCC"> five megabytes and</font>

189
00:09:26,509 --> 00:09:32,790
we compare that to the<font color="#CCCCCC"> 200 megabytes on</font>

190
00:09:30,180 --> 00:09:34,680
the first graph we see that the

191
00:09:32,790 --> 00:09:38,969
variability is only about one to three

192
00:09:34,680 --> 00:09:40,829
percent and this graph is representative

193
00:09:38,970 --> 00:09:42,839
of all the<font color="#E5E5E5"> other egress</font><font color="#CCCCCC"> pups for this</font>

194
00:09:40,829 --> 00:09:45,029
<font color="#E5E5E5">particular ingress trace and is</font>

195
00:09:42,839 --> 00:09:49,259
representative of<font color="#CCCCCC"> the five other</font><font color="#E5E5E5"> English</font>

196
00:09:45,029 --> 00:09:50,819
traces that we also looked at so even

197
00:09:49,259 --> 00:09:52,980
though<font color="#CCCCCC"> this</font><font color="#E5E5E5"> is a very small amount of</font>

198
00:09:50,819 --> 00:09:55,259
variability we have looked<font color="#E5E5E5"> at what's</font>

199
00:09:52,980 --> 00:09:57,540
been causing this variability and we've

200
00:09:55,259 --> 00:09:59,970
noticed that in all cases networks

201
00:09:57,540 --> 00:10:02,579
destination networks with multiple links

202
00:09:59,970 --> 00:10:06,059
or paths to<font color="#CCCCCC"> this friend backbone have</font>

203
00:10:02,579 --> 00:10:08,699
been<font color="#E5E5E5"> involved we found that BGP updates</font>

204
00:10:06,059 --> 00:10:12,240
cost shift between these inter a s pads

205
00:10:08,699 --> 00:10:14,490
and because the intro es pass shifts and

206
00:10:12,240 --> 00:10:17,040
because these interests pads connect to

207
00:10:14,490 --> 00:10:19,619
spend at different egress pops the shift

208
00:10:17,040 --> 00:10:23,579
there is shift in traffic inside this

209
00:10:19,619 --> 00:10:25,980
<font color="#CCCCCC">rain backbone here are two common</font>

210
00:10:23,579 --> 00:10:27,929
examples of what happens here<font color="#E5E5E5"> is a</font>

211
00:10:25,980 --> 00:10:30,059
sprint backbone and we've got a

212
00:10:27,929 --> 00:10:33,720
particular customer that connects to us

213
00:10:30,059 --> 00:10:35,939
at pop one and at<font color="#E5E5E5"> pop too and bgp</font>

214
00:10:33,720 --> 00:10:38,959
updates will cause traffic to shift

215
00:10:35,939 --> 00:10:41,579
between this link and<font color="#E5E5E5"> this other link or</font>

216
00:10:38,959 --> 00:10:43,109
the other common scenario is where we've

217
00:10:41,579 --> 00:10:45,779
got customer that connects to us

218
00:10:43,110 --> 00:10:47,819
directly in a particular pop and then

219
00:10:45,779 --> 00:10:51,389
connects to us indirectly through a peer

220
00:10:47,819 --> 00:10:53,550
to another pop and then<font color="#CCCCCC"> bgp updates will</font>

221
00:10:51,389 --> 00:10:56,069
cause traffic to shift both from a

222
00:10:53,550 --> 00:10:59,599
shorter path to a longer<font color="#E5E5E5"> path and from a</font>

223
00:10:56,069 --> 00:10:59,599
longer<font color="#E5E5E5"> path to a shorter path</font>

224
00:11:02,010 --> 00:11:07,170
we've also found that and most<font color="#CCCCCC"> of the</font>

225
00:11:04,830 --> 00:11:09,750
variability which is only one to three

226
00:11:07,170 --> 00:11:12,360
percent we found a single<font color="#CCCCCC"> next hub was</font>

227
00:11:09,750 --> 00:11:16,200
involved in<font color="#CCCCCC"> forty seven percent of all</font>

228
00:11:12,360 --> 00:11:18,840
the traffic shifts we also found that of

229
00:11:16,200 --> 00:11:20,310
all the traffic shifts for the<font color="#E5E5E5"> city of</font>

230
00:11:18,840 --> 00:11:23,100
<font color="#E5E5E5">forty six percent of all the traffic</font>

231
00:11:20,310 --> 00:11:27,479
shifts a single destination or origin

232
00:11:23,100 --> 00:11:30,030
<font color="#CCCCCC">a.s was involved</font><font color="#E5E5E5"> however we didn't find</font>

233
00:11:27,480 --> 00:11:31,530
the case that egress pop shifts have

234
00:11:30,030 --> 00:11:33,870
been rapidly throughout the length of

235
00:11:31,530 --> 00:11:36,390
the trace we<font color="#E5E5E5"> noticed</font><font color="#CCCCCC"> that a Negros pop</font>

236
00:11:33,870 --> 00:11:37,710
shift would happen only a few times<font color="#E5E5E5"> that</font>

237
00:11:36,390 --> 00:11:41,850
most for a particular destination

238
00:11:37,710 --> 00:11:44,640
address but this is for only one percent

239
00:11:41,850 --> 00:11:49,740
of<font color="#CCCCCC"> that traffic why is the other traffic</font>

240
00:11:44,640 --> 00:11:51,780
stable for that we look at<font color="#E5E5E5"> the</font>

241
00:11:49,740 --> 00:11:53,550
characteristics of<font color="#E5E5E5"> this traffic we've</font>

242
00:11:51,780 --> 00:11:55,290
noticed that of all the traffic

243
00:11:53,550 --> 00:11:58,079
measurements that we've done heavy

244
00:11:55,290 --> 00:11:59,969
hitters have<font color="#CCCCCC"> been present</font><font color="#E5E5E5"> by that</font><font color="#CCCCCC"> I mean</font>

245
00:11:58,080 --> 00:12:02,520
<font color="#E5E5E5">for a hundred percent of the traffic on</font>

246
00:11:59,970 --> 00:12:04,830
this ingress link we<font color="#CCCCCC"> have about 200,000</font>

247
00:12:02,520 --> 00:12:07,079
destination addresses but for

248
00:12:04,830 --> 00:12:08,760
<font color="#E5E5E5">ninety-nine percent of this traffic only</font>

249
00:12:07,080 --> 00:12:11,550
fifteen percent of those destination

250
00:12:08,760 --> 00:12:13,560
addresses are involved and for eighty

251
00:12:11,550 --> 00:12:15,150
percent of the traffic<font color="#E5E5E5"> only one point</font>

252
00:12:13,560 --> 00:12:18,469
five percent of those destination

253
00:12:15,150 --> 00:12:18,470
addresses are involved

254
00:12:21,860 --> 00:12:27,510
so what we want<font color="#E5E5E5"> to look at now is which</font>

255
00:12:25,410 --> 00:12:29,399
of the<font color="#CCCCCC"> bgp updates affect these heavy</font>

256
00:12:27,510 --> 00:12:32,279
hitters because they account for most of

257
00:12:29,399 --> 00:12:34,410
the traffic and then of these updates

258
00:12:32,279 --> 00:12:36,060
which of them cause a change in the

259
00:12:34,410 --> 00:12:41,790
egress<font color="#CCCCCC"> pup for these destination</font>

260
00:12:36,060 --> 00:12:44,550
<font color="#E5E5E5">addresses this top graph shows the</font>

261
00:12:41,790 --> 00:12:46,849
number<font color="#E5E5E5"> of ibgp updates or the number of</font>

262
00:12:44,550 --> 00:12:49,729
changes to<font color="#CCCCCC"> the I</font><font color="#E5E5E5"> BGP routing table</font>

263
00:12:46,850 --> 00:12:52,380
during the length of the whole trace and

264
00:12:49,730 --> 00:12:54,990
again we see the constant noise of about

265
00:12:52,380 --> 00:12:57,330
100 updates per minute<font color="#E5E5E5"> and we</font>

266
00:12:54,990 --> 00:12:59,730
occasionally see some spikes going up to

267
00:12:57,330 --> 00:13:04,560
about a couple of thousand updates per

268
00:12:59,730 --> 00:13:06,959
minute now the second graph is shows the

269
00:13:04,560 --> 00:13:09,599
number<font color="#E5E5E5"> of</font><font color="#CCCCCC"> BGP updates for only the heavy</font>

270
00:13:06,959 --> 00:13:11,640
hitters<font color="#E5E5E5"> or the destination addresses</font>

271
00:13:09,600 --> 00:13:15,480
<font color="#E5E5E5">that account for eighty percent of the</font>

272
00:13:11,640 --> 00:13:17,610
traffic<font color="#E5E5E5"> and we see</font><font color="#CCCCCC"> that we're talking</font>

273
00:13:15,480 --> 00:13:20,339
<font color="#E5E5E5">about far fewer updates now we're</font>

274
00:13:17,610 --> 00:13:22,760
talking about five to five to<font color="#CCCCCC"> fifteen</font>

275
00:13:20,339 --> 00:13:22,760
updates

276
00:13:25,430 --> 00:13:29,750
now I was just showing you all of the

277
00:13:27,620 --> 00:13:32,060
<font color="#CCCCCC">BGP updates not all the</font><font color="#E5E5E5"> BGP updates</font>

278
00:13:29,750 --> 00:13:34,940
change the next<font color="#CCCCCC"> hub if the next</font><font color="#E5E5E5"> hop</font>

279
00:13:32,060 --> 00:13:38,329
changes then the<font color="#CCCCCC"> equus pod can change so</font>

280
00:13:34,940 --> 00:13:41,600
in this top graph<font color="#E5E5E5"> I'm showing</font><font color="#CCCCCC"> you all of</font>

281
00:13:38,330 --> 00:13:44,149
the next stop changes for for all of the

282
00:13:41,600 --> 00:13:46,040
<font color="#CCCCCC">destination addresses for all the all of</font>

283
00:13:44,149 --> 00:13:49,430
the destination<font color="#CCCCCC"> addresses in a beach be</font>

284
00:13:46,040 --> 00:13:52,579
table we see that there's there's still

285
00:13:49,430 --> 00:13:54,890
a fair amount of constant noise but then

286
00:13:52,580 --> 00:13:56,750
in the second graph we are now<font color="#E5E5E5"> looking</font>

287
00:13:54,890 --> 00:13:58,640
at<font color="#CCCCCC"> the number</font><font color="#E5E5E5"> of next</font><font color="#CCCCCC"> top changes only</font>

288
00:13:56,750 --> 00:14:01,010
for the heavy hitters which is any

289
00:13:58,640 --> 00:14:03,800
percent of the traffic<font color="#E5E5E5"> and we see</font><font color="#CCCCCC"> that</font>

290
00:14:01,010 --> 00:14:05,779
the number is far lower so there<font color="#E5E5E5"> are two</font>

291
00:14:03,800 --> 00:14:08,180
points<font color="#CCCCCC"> here one is that even though</font>

292
00:14:05,779 --> 00:14:09,709
there are so many bgp updates relatively

293
00:14:08,180 --> 00:14:13,040
few<font color="#CCCCCC"> of them matter for this particular</font>

294
00:14:09,709 --> 00:14:15,349
problem and secondly<font color="#CCCCCC"> i also want to</font>

295
00:14:13,040 --> 00:14:18,319
<font color="#CCCCCC">point out that</font><font color="#E5E5E5"> the heavy hitters don't</font>

296
00:14:15,350 --> 00:14:20,390
change much in their<font color="#E5E5E5"> next top the</font>

297
00:14:18,320 --> 00:14:23,810
average<font color="#CCCCCC"> number of next stop changes for</font>

298
00:14:20,390 --> 00:14:26,120
any prefix was about 0.63 drink length

299
00:14:23,810 --> 00:14:28,489
of the trace and for the heavy hitters

300
00:14:26,120 --> 00:14:31,630
the<font color="#E5E5E5"> average number of next stop changes</font>

301
00:14:28,490 --> 00:14:31,630
was 0.1

302
00:14:33,490 --> 00:14:39,010
so the conclusion is that bgp updates

303
00:14:36,580 --> 00:14:41,740
hardly affect interest rent traffic fan

304
00:14:39,010 --> 00:14:44,830
out<font color="#E5E5E5"> and this is</font><font color="#CCCCCC"> consistent with</font><font color="#E5E5E5"> prior</font>

305
00:14:41,740 --> 00:14:46,779
work from<font color="#CCCCCC"> Jennifer rexford who did</font>

306
00:14:44,830 --> 00:14:49,149
measurements of<font color="#CCCCCC"> the AT&T backbone and</font>

307
00:14:46,779 --> 00:14:53,760
showed and found<font color="#CCCCCC"> that the heavy hitters</font>

308
00:14:49,149 --> 00:14:56,860
tend to have stable prefixes why is this

309
00:14:53,760 --> 00:14:59,200
<font color="#E5E5E5">maybe standard BGP rod filtering is</font>

310
00:14:56,860 --> 00:15:01,480
doing a good job maybe it's the case

311
00:14:59,200 --> 00:15:04,240
that only ste books table prefixes can

312
00:15:01,480 --> 00:15:06,130
have<font color="#E5E5E5"> tracks table traffic or it could be</font>

313
00:15:04,240 --> 00:15:08,050
<font color="#E5E5E5">that layer</font><font color="#CCCCCC"> three protection switching</font>

314
00:15:06,130 --> 00:15:10,870
and engineering is doing a good job of

315
00:15:08,050 --> 00:15:13,329
keeping backbone traffic stable but

316
00:15:10,870 --> 00:15:15,640
there are<font color="#CCCCCC"> so many other bgp updates what</font>

317
00:15:13,330 --> 00:15:17,320
is causing them to<font color="#E5E5E5"> answer some of these</font>

318
00:15:15,640 --> 00:15:20,140
questions we would<font color="#E5E5E5"> have to determine</font>

319
00:15:17,320 --> 00:15:22,330
where<font color="#E5E5E5"> BGP updates come from and what's</font>

320
00:15:20,140 --> 00:15:24,490
causing them and we would have to we

321
00:15:22,330 --> 00:15:29,830
would have to steal bgp sessions all

322
00:15:24,490 --> 00:15:31,660
across the world and<font color="#E5E5E5"> figure that out so</font>

323
00:15:29,830 --> 00:15:34,180
the implications of our work are that

324
00:15:31,660 --> 00:15:36,160
<font color="#CCCCCC">bgp doesn't cause</font><font color="#E5E5E5"> latency variation</font>

325
00:15:34,180 --> 00:15:37,750
within the sprint network which is good

326
00:15:36,160 --> 00:15:41,170
for certain applications such as voice

327
00:15:37,750 --> 00:15:43,329
over IP and also beach be doesn't make

328
00:15:41,170 --> 00:15:45,310
link loads more dynamic within the

329
00:15:43,329 --> 00:15:47,589
network and that makes traffic

330
00:15:45,310 --> 00:15:50,410
engineering and provisioning easier

331
00:15:47,589 --> 00:15:53,290
because the traffic matrix is now less

332
00:15:50,410 --> 00:15:55,839
stable it<font color="#E5E5E5"> is less variable it is still</font>

333
00:15:53,290 --> 00:15:57,819
unstable due to inherent variations in

334
00:15:55,839 --> 00:16:00,899
<font color="#CCCCCC">the traffic but you</font><font color="#E5E5E5"> don't have to worry</font>

335
00:15:57,820 --> 00:16:04,680
about the variations from BGP itself

336
00:16:00,899 --> 00:16:07,079
and lastly I'd like to point out the

337
00:16:04,680 --> 00:16:09,059
website for our group it's got data from

338
00:16:07,079 --> 00:16:10,979
the analysis of various packet races and

339
00:16:09,059 --> 00:16:13,259
writing data on there as<font color="#CCCCCC"> well as a bunch</font>

340
00:16:10,980 --> 00:16:30,019
of research papers and contact

341
00:16:13,259 --> 00:16:32,339
information for us hi I stopped by doing

342
00:16:30,019 --> 00:16:36,809
what percentage of your customers and

343
00:16:32,339 --> 00:16:38,490
here's actually<font color="#CCCCCC"> passed via</font><font color="#E5E5E5"> MIDI s I'm</font>

344
00:16:36,809 --> 00:16:40,379
sorry I didn't catch<font color="#CCCCCC"> the last part what</font>

345
00:16:38,490 --> 00:16:48,660
percentage of your customers and peers

346
00:16:40,379 --> 00:16:50,759
as you any these meds oh it's a good

347
00:16:48,660 --> 00:16:54,300
question i don't<font color="#E5E5E5"> have that</font><font color="#CCCCCC"> number i keep</font>

348
00:16:50,759 --> 00:16:55,949
my nice to submit work to ones who are

349
00:16:54,300 --> 00:16:57,179
pissing immediate worsens not<font color="#E5E5E5"> pissing</font>

350
00:16:55,949 --> 00:16:59,969
images and see if there's actually

351
00:16:57,179 --> 00:17:02,929
difference<font color="#CCCCCC"> that Webbie another edition</font>

352
00:16:59,970 --> 00:17:05,399
the other thing<font color="#CCCCCC"> is some actually if</font>

353
00:17:02,929 --> 00:17:08,279
especially for the part that the<font color="#CCCCCC"> enemies</font>

354
00:17:05,398 --> 00:17:10,708
are<font color="#E5E5E5"> not</font><font color="#CCCCCC"> past igp type wrecking is used</font>

355
00:17:08,279 --> 00:17:12,510
to figure out which pub to exit did you

356
00:17:10,709 --> 00:17:14,520
simultaneous those who looked at the<font color="#CCCCCC"> igp</font>

357
00:17:12,510 --> 00:17:16,799
messages and see if there's actually

358
00:17:14,520 --> 00:17:24,000
affecting the<font color="#E5E5E5"> base actually causing a</font>

359
00:17:16,799 --> 00:17:27,539
change of the exit pub oh so so when we

360
00:17:24,000 --> 00:17:29,820
look at the ibgp table itself we are any

361
00:17:27,539 --> 00:17:32,120
<font color="#CCCCCC">IGP changes are reflected in that table</font>

362
00:17:29,820 --> 00:17:35,580
so we are accounting for that but then

363
00:17:32,120 --> 00:17:38,100
since we hardly see any variability we

364
00:17:35,580 --> 00:17:41,850
didn't actually bother to figure out

365
00:17:38,100 --> 00:17:46,560
whether<font color="#CCCCCC"> IGP was causing the</font><font color="#E5E5E5"> small amount</font>

366
00:17:41,850 --> 00:17:50,379
of variability that we saw thank you

367
00:17:46,560 --> 00:17:52,000
<font color="#E5E5E5">you're your data was taken during one</font>

368
00:17:50,380 --> 00:17:53,610
period or did you look at multiple

369
00:17:52,000 --> 00:17:58,560
periods where you might<font color="#CCCCCC"> be losing</font>

370
00:17:53,610 --> 00:18:02,919
connectivity to appear or having an XP r

371
00:17:58,560 --> 00:18:06,220
we<font color="#E5E5E5"> we collect the data across two</font>

372
00:18:02,920 --> 00:18:10,720
different days and sorry three different

373
00:18:06,220 --> 00:18:13,210
days and connected to<font color="#E5E5E5"> six different a SS</font>

374
00:18:10,720 --> 00:18:15,880
so that's that's the extent of our data

375
00:18:13,210 --> 00:18:19,230
<font color="#CCCCCC">okay during any of those time did you</font>

376
00:18:15,880 --> 00:18:22,330
<font color="#CCCCCC">have any flapping the two of of</font>

377
00:18:19,230 --> 00:18:30,880
connectivity either to the exit peers or

378
00:18:22,330 --> 00:18:33,490
to the next hop for the<font color="#E5E5E5"> BGP I when we</font>

379
00:18:30,880 --> 00:18:36,870
collected those packet races we didn't

380
00:18:33,490 --> 00:18:42,280
<font color="#CCCCCC">notice any major outages at all but any</font>

381
00:18:36,870 --> 00:18:45,669
specific ones such as as excessive<font color="#CCCCCC"> BGP</font>

382
00:18:42,280 --> 00:18:48,220
updates they they would show up in the

383
00:18:45,670 --> 00:18:50,530
BGP updates itself that we collect and

384
00:18:48,220 --> 00:18:53,530
that would get reflected in our analysis

385
00:18:50,530 --> 00:18:57,040
so let<font color="#E5E5E5"> me make sure I understood it</font>

386
00:18:53,530 --> 00:19:02,050
<font color="#E5E5E5">because this is a point where people</font>

387
00:18:57,040 --> 00:19:05,230
have asked problems it would be have you

388
00:19:02,050 --> 00:19:10,180
seen the connectivity to the exit

389
00:19:05,230 --> 00:19:12,940
gateway or to the<font color="#E5E5E5"> BGP</font><font color="#CCCCCC"> peer go on and</font><font color="#E5E5E5"> off</font>

390
00:19:10,180 --> 00:19:14,500
repeatedly in any of your samples and

391
00:19:12,940 --> 00:19:17,710
are you<font color="#E5E5E5"> planning to look for that</font>

392
00:19:14,500 --> 00:19:19,810
particular problem so I think<font color="#E5E5E5"> you're</font>

393
00:19:17,710 --> 00:19:22,180
asking<font color="#CCCCCC"> me if there were any igp failures</font>

394
00:19:19,810 --> 00:19:26,080
or are there any failures within the

395
00:19:22,180 --> 00:19:27,850
sprint network during either failures

396
00:19:26,080 --> 00:19:33,929
within the sprint network to the exit

397
00:19:27,850 --> 00:19:33,929
gateway or failures to the<font color="#CCCCCC"> rope assp r</font>

398
00:19:34,040 --> 00:19:39,780
well I didn't actually try to correlate

399
00:19:37,070 --> 00:19:42,620
whether there were failures within the

400
00:19:39,780 --> 00:19:45,120
network or from<font color="#CCCCCC"> the network</font><font color="#E5E5E5"> to the rest</font>

401
00:19:42,620 --> 00:19:48,510
other than what would have already been

402
00:19:45,120 --> 00:19:50,129
seen in the BGP updates<font color="#E5E5E5"> mmm</font><font color="#CCCCCC"> okay they</font>

403
00:19:48,510 --> 00:19:52,410
might be interesting along with geniuses

404
00:19:50,130 --> 00:19:55,580
comment and by<font color="#E5E5E5"> the way it is a good talk</font>

405
00:19:52,410 --> 00:19:55,580
thank you very much thanks

406
00:19:59,190 --> 00:20:03,900
I have one<font color="#E5E5E5"> question myself so it's along</font>

407
00:20:01,770 --> 00:20:05,310
the same lines and I guess given the

408
00:20:03,900 --> 00:20:06,540
<font color="#E5E5E5">other questions it's maybe not as</font>

409
00:20:05,310 --> 00:20:08,730
specific as it could be but I'm

410
00:20:06,540 --> 00:20:15,270
<font color="#CCCCCC">interested in any other observations you</font>

411
00:20:08,730 --> 00:20:16,560
might have I guess the if the summary is

412
00:20:15,270 --> 00:20:18,210
that when traffic is stable<font color="#E5E5E5"> and</font>

413
00:20:16,560 --> 00:20:20,370
everything<font color="#E5E5E5"> is working then the network</font>

414
00:20:18,210 --> 00:20:23,130
works well I guess<font color="#E5E5E5"> that's a good result</font>

415
00:20:20,370 --> 00:20:26,280
and I<font color="#E5E5E5"> think this is it's useful</font><font color="#CCCCCC"> to see</font>

416
00:20:23,130 --> 00:20:28,980
that actually with with some analysis

417
00:20:26,280 --> 00:20:32,190
but along<font color="#E5E5E5"> lines of the other comments if</font>

418
00:20:28,980 --> 00:20:33,600
we had more more insight<font color="#CCCCCC"> into what's</font>

419
00:20:32,190 --> 00:20:36,210
happening when something's broken and

420
00:20:33,600 --> 00:20:37,889
<font color="#E5E5E5">whether as we talked before if those</font>

421
00:20:36,210 --> 00:20:39,830
periods where you had a higher rate of

422
00:20:37,890 --> 00:20:42,390
<font color="#E5E5E5">instability if any analysis was done</font>

423
00:20:39,830 --> 00:20:44,850
done during those periods and if you saw

424
00:20:42,390 --> 00:20:46,860
anything different other than during the

425
00:20:44,850 --> 00:20:51,240
kind<font color="#E5E5E5"> of steady-state background noise</font>

426
00:20:46,860 --> 00:20:53,520
<font color="#CCCCCC">bgp right so the the worst</font><font color="#E5E5E5"> number of</font>

427
00:20:51,240 --> 00:20:56,310
updates that we saw were about 9,000

428
00:20:53,520 --> 00:20:58,050
updates per minute and I was usually

429
00:20:56,310 --> 00:20:59,940
during the weekends and we<font color="#E5E5E5"> think that</font>

430
00:20:58,050 --> 00:21:02,760
<font color="#E5E5E5">was during maintenance cycles of various</font>

431
00:20:59,940 --> 00:21:04,920
ISPs during the length of our part of

432
00:21:02,760 --> 00:21:07,260
<font color="#CCCCCC">the</font><font color="#E5E5E5"> various races that we collected the</font>

433
00:21:04,920 --> 00:21:10,650
worst<font color="#E5E5E5"> that we saw was about 2,000</font>

434
00:21:07,260 --> 00:21:13,140
updates per minute and<font color="#E5E5E5"> so we haven't</font>

435
00:21:10,650 --> 00:21:15,270
actually collected any or haven't

436
00:21:13,140 --> 00:21:18,120
analyzed any data during the weekends to

437
00:21:15,270 --> 00:21:22,010
see if those maintenance cycles could

438
00:21:18,120 --> 00:21:22,010
actually affect this issue

439
00:21:25,999 --> 00:21:30,480
and it doesn't<font color="#CCCCCC"> look like</font><font color="#E5E5E5"> there any other</font>

440
00:21:28,019 --> 00:21:32,999
questions so thanks again very much<font color="#E5E5E5"> I</font>

441
00:21:30,480 --> 00:21:35,360
think this<font color="#CCCCCC"> is a good work and appreciate</font>

442
00:21:32,999 --> 00:21:35,360
<font color="#E5E5E5">you sharing</font>

