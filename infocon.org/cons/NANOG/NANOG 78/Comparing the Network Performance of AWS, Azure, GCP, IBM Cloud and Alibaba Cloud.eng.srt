1
00:00:00,110 --> 00:00:05,940
so as Alex mentioned my name is

2
00:00:03,720 --> 00:00:08,849
angelique Medina I work for a company

3
00:00:05,940 --> 00:00:10,980
called Thousand Eyes where alongside my

4
00:00:08,849 --> 00:00:12,540
colleagues I get to work on some really

5
00:00:10,980 --> 00:00:15,059
interesting research projects like

6
00:00:12,540 --> 00:00:18,448
looking at the performance of DNS and

7
00:00:15,059 --> 00:00:23,130
CDN services the subject of this session

8
00:00:18,449 --> 00:00:25,260
today is to share findings that are

9
00:00:23,130 --> 00:00:27,990
based on a research project that had its

10
00:00:25,260 --> 00:00:29,279
genesis about 18 months ago where we

11
00:00:27,990 --> 00:00:31,380
started looking at the network

12
00:00:29,279 --> 00:00:33,090
performance and behavior of some of the

13
00:00:31,380 --> 00:00:35,760
major cloud providers because we saw

14
00:00:33,090 --> 00:00:38,780
that there wasn't a lot of independent

15
00:00:35,760 --> 00:00:42,300
publicly available data on how cloud

16
00:00:38,780 --> 00:00:45,000
operators were running their network so

17
00:00:42,300 --> 00:00:46,769
we wanted to fill that gap so the first

18
00:00:45,000 --> 00:00:49,950
iteration of this research was released

19
00:00:46,770 --> 00:00:52,980
in late 2018 and presented here at Nanog

20
00:00:49,950 --> 00:00:54,890
in San Francisco last year and the

21
00:00:52,980 --> 00:00:57,718
second iteration of this research

22
00:00:54,890 --> 00:00:59,489
greatly expanded on what we did in 2018

23
00:00:57,719 --> 00:01:01,859
and that's what we're going to go

24
00:00:59,489 --> 00:01:05,069
through today the initial findings from

25
00:01:01,859 --> 00:01:06,510
2018 were very interesting because as

26
00:01:05,069 --> 00:01:07,979
you would expect overall network

27
00:01:06,510 --> 00:01:10,470
performance of the cloud providers is

28
00:01:07,979 --> 00:01:12,960
very good but there were some surprising

29
00:01:10,470 --> 00:01:15,509
anomalies that revealed differences in

30
00:01:12,960 --> 00:01:17,130
routing preferences and how they managed

31
00:01:15,509 --> 00:01:21,420
their overall network architecture and

32
00:01:17,130 --> 00:01:24,420
connectivity so in 2018 we looked at

33
00:01:21,420 --> 00:01:29,220
three major cloud providers so AWS Azure

34
00:01:24,420 --> 00:01:30,630
and GC P and in the most recent research

35
00:01:29,220 --> 00:01:32,729
that we did so this was published a few

36
00:01:30,630 --> 00:01:34,920
months ago at the end of 2019 we added

37
00:01:32,729 --> 00:01:39,090
to public cloud providers oli cloud and

38
00:01:34,920 --> 00:01:41,159
IBM cloud and we also took a look at AWS

39
00:01:39,090 --> 00:01:43,380
global accelerator which was a service

40
00:01:41,159 --> 00:01:46,020
that was announced just after our

41
00:01:43,380 --> 00:01:48,600
initial research was published we also

42
00:01:46,020 --> 00:01:51,600
added additional testing to look at

43
00:01:48,600 --> 00:01:54,570
intra and Inter mainland China

44
00:01:51,600 --> 00:01:56,309
connectivity as well as performance from

45
00:01:54,570 --> 00:02:01,439
vantage points that were located within

46
00:01:56,310 --> 00:02:03,630
US broadband provider networks so pretty

47
00:02:01,439 --> 00:02:05,639
significant expansion on what we did we

48
00:02:03,630 --> 00:02:07,079
also because we had a baseline data set

49
00:02:05,640 --> 00:02:09,959
from 2018 we could look at

50
00:02:07,079 --> 00:02:11,980
year-over-year changes for Azure AWS

51
00:02:09,959 --> 00:02:13,180
mgcp which is really interesting we're

52
00:02:11,980 --> 00:02:15,849
spend most of our time looking at that

53
00:02:13,180 --> 00:02:18,790
so in terms of our research method

54
00:02:15,849 --> 00:02:20,470
methodology just very high-level the way

55
00:02:18,790 --> 00:02:22,840
that we derived these measurements was

56
00:02:20,470 --> 00:02:25,450
using a custom form of TCP based trace

57
00:02:22,840 --> 00:02:26,980
route between two endpoints that we

58
00:02:25,450 --> 00:02:30,310
controlled so we had an agent in either

59
00:02:26,980 --> 00:02:33,160
end we were sending probing packets

60
00:02:30,310 --> 00:02:34,690
between them and we were able to look at

61
00:02:33,160 --> 00:02:37,810
performance indicators from that

62
00:02:34,690 --> 00:02:39,340
including end to end latency packet loss

63
00:02:37,810 --> 00:02:41,110
jitter as well as hop-by-hop

64
00:02:39,340 --> 00:02:42,579
measurements so we could actually map

65
00:02:41,110 --> 00:02:45,970
out the path between these two end

66
00:02:42,579 --> 00:02:47,950
points we also looked in both directions

67
00:02:45,970 --> 00:02:49,180
so it was bi-directional we're looking

68
00:02:47,950 --> 00:02:51,670
at each of these measurements

69
00:02:49,180 --> 00:02:54,430
independently one way and then

70
00:02:51,670 --> 00:02:55,540
aggregating those and the measurement

71
00:02:54,430 --> 00:02:58,890
they were using here is principally

72
00:02:55,540 --> 00:03:02,349
milliseconds for looking at averages

73
00:02:58,890 --> 00:03:05,290
also because we could map out the paths

74
00:03:02,349 --> 00:03:08,319
when we saw anomalies we could then go

75
00:03:05,290 --> 00:03:10,540
back and look at what the layer three

76
00:03:08,319 --> 00:03:12,640
hops were between the two end points and

77
00:03:10,540 --> 00:03:14,980
we also enriched that with some metadata

78
00:03:12,640 --> 00:03:18,849
like geolocation and AS operator

79
00:03:14,980 --> 00:03:21,548
information so these software agents

80
00:03:18,849 --> 00:03:25,569
that we use for this test are they were

81
00:03:21,549 --> 00:03:27,370
either in regions hosted by the public

82
00:03:25,569 --> 00:03:30,280
cloud providers or they were located

83
00:03:27,370 --> 00:03:32,260
outside the providers in hosting

84
00:03:30,280 --> 00:03:34,870
facilities that were connected to tier 1

85
00:03:32,260 --> 00:03:37,179
and tier 2 service providers or in some

86
00:03:34,870 --> 00:03:39,190
cases broadband providers and they were

87
00:03:37,180 --> 00:03:41,470
managed by us so that allowed us to do

88
00:03:39,190 --> 00:03:43,060
some consistent measuring every 10

89
00:03:41,470 --> 00:03:45,940
minutes over the course of 30 days which

90
00:03:43,060 --> 00:03:49,150
yielded a lot of data and most of the

91
00:03:45,940 --> 00:03:52,030
data wasn't super interesting so packet

92
00:03:49,150 --> 00:03:54,370
loss and jitter not not a lot to see

93
00:03:52,030 --> 00:03:55,840
there with the exception of China but

94
00:03:54,370 --> 00:03:57,700
with latency there was some interesting

95
00:03:55,840 --> 00:04:01,299
differences in how the providers

96
00:03:57,700 --> 00:04:02,798
performed so in terms of the scope of

97
00:04:01,299 --> 00:04:05,680
our research the questions that we

98
00:04:02,799 --> 00:04:09,459
principally wanted to answer were first

99
00:04:05,680 --> 00:04:11,079
off what is the performance of users so

100
00:04:09,459 --> 00:04:13,359
that could be an individual or an

101
00:04:11,079 --> 00:04:15,940
application tier connecting to the

102
00:04:13,359 --> 00:04:19,630
various regions of the cloud providers

103
00:04:15,940 --> 00:04:22,450
so what we did is we used vantage points

104
00:04:19,630 --> 00:04:24,820
across a number of different cities

105
00:04:22,450 --> 00:04:28,360
globally so 98 locations

106
00:04:24,820 --> 00:04:32,380
and we tested from those locations to

107
00:04:28,360 --> 00:04:33,670
each of the regions that you see for the

108
00:04:32,380 --> 00:04:38,680
cloud providers that you see here so

109
00:04:33,670 --> 00:04:41,290
there was 15 regions for GCP and AWS 25

110
00:04:38,680 --> 00:04:45,460
for a juror and then some number 21 and

111
00:04:41,290 --> 00:04:47,710
17 for a li cloud and IBM so pretty

112
00:04:45,460 --> 00:04:49,900
pretty significant set of measurements

113
00:04:47,710 --> 00:04:52,960
that we were able to collect separately

114
00:04:49,900 --> 00:04:54,429
we also looked at the performance for

115
00:04:52,960 --> 00:04:57,909
vantage points that were connected

116
00:04:54,430 --> 00:05:00,790
single home to us broadband providers so

117
00:04:57,910 --> 00:05:03,130
these were six providers that we're

118
00:05:00,790 --> 00:05:05,320
looking at across six US cities and

119
00:05:03,130 --> 00:05:09,070
because we're just looking at users

120
00:05:05,320 --> 00:05:14,230
connecting from US locations we only

121
00:05:09,070 --> 00:05:15,340
connected to four cloud regions for each

122
00:05:14,230 --> 00:05:18,400
of the providers so we just looked at

123
00:05:15,340 --> 00:05:21,880
North America us West East Central and

124
00:05:18,400 --> 00:05:24,609
Canada and then inter region

125
00:05:21,880 --> 00:05:28,360
measurements were taken for each of the

126
00:05:24,610 --> 00:05:30,490
cloud providers between a region pair

127
00:05:28,360 --> 00:05:32,440
and so there's just an incredible matrix

128
00:05:30,490 --> 00:05:35,560
of measurements that we were able to

129
00:05:32,440 --> 00:05:37,660
take so for example we're looking at for

130
00:05:35,560 --> 00:05:39,130
instance view as West and US East and

131
00:05:37,660 --> 00:05:42,160
looking at the performance between those

132
00:05:39,130 --> 00:05:45,159
two points something that we added from

133
00:05:42,160 --> 00:05:47,860
last year was a baseline measurement to

134
00:05:45,160 --> 00:05:52,530
compare how those providers were

135
00:05:47,860 --> 00:05:55,240
performing relative to some baseline

136
00:05:52,530 --> 00:05:58,150
performance measurements so we what we

137
00:05:55,240 --> 00:05:59,740
did was in the instances in which a

138
00:05:58,150 --> 00:06:01,840
cloud provider let's say they had a

139
00:05:59,740 --> 00:06:03,070
region that was hosted in San Jose and

140
00:06:01,840 --> 00:06:06,700
they had another region that was hosted

141
00:06:03,070 --> 00:06:08,770
in Ashburn we then in addition to

142
00:06:06,700 --> 00:06:11,710
measuring between those two points we

143
00:06:08,770 --> 00:06:13,570
also picked a vantage point that was

144
00:06:11,710 --> 00:06:17,099
outside of the cloud providers network

145
00:06:13,570 --> 00:06:19,150
connecting to a vantage point that was

146
00:06:17,100 --> 00:06:21,610
also outside of the cloud providers

147
00:06:19,150 --> 00:06:24,159
network in that same city so testing

148
00:06:21,610 --> 00:06:25,360
from San Jose say to Ashburn and then

149
00:06:24,160 --> 00:06:28,060
looking at the performance difference

150
00:06:25,360 --> 00:06:29,530
are they performing same better no

151
00:06:28,060 --> 00:06:32,020
different than this internet baseline

152
00:06:29,530 --> 00:06:34,000
it's just one kind of mechanism to see

153
00:06:32,020 --> 00:06:37,570
you know if if there are vast

154
00:06:34,000 --> 00:06:38,710
disparities or if they're for example as

155
00:06:37,570 --> 00:06:40,750
we would expect to be

156
00:06:38,710 --> 00:06:42,900
much more optimize connecting their

157
00:06:40,750 --> 00:06:46,180
between those two points

158
00:06:42,900 --> 00:06:48,849
we also looked at the performance with

159
00:06:46,180 --> 00:06:51,690
within specific region so we picked a

160
00:06:48,850 --> 00:06:54,250
handful of regions and looked at the

161
00:06:51,690 --> 00:06:56,080
performance of different availability

162
00:06:54,250 --> 00:06:57,190
zones within that region connecting to

163
00:06:56,080 --> 00:06:59,830
one another so we picked three

164
00:06:57,190 --> 00:07:02,740
availability zones for the handful of

165
00:06:59,830 --> 00:07:06,099
regions that you see here and what we're

166
00:07:02,740 --> 00:07:09,190
doing was we wanted to understand for

167
00:07:06,100 --> 00:07:10,570
example if an enterprise is putting

168
00:07:09,190 --> 00:07:12,640
together or not an architecture they

169
00:07:10,570 --> 00:07:17,320
want to host in a particular region they

170
00:07:12,640 --> 00:07:21,370
might choose to host in both US West one

171
00:07:17,320 --> 00:07:25,240
a and B or some combination of a B and C

172
00:07:21,370 --> 00:07:27,220
and that enables them to have a more

173
00:07:25,240 --> 00:07:29,820
resilient architecture while being still

174
00:07:27,220 --> 00:07:33,840
geographically hosted in a similar

175
00:07:29,820 --> 00:07:37,000
location and all of the cloud providers

176
00:07:33,840 --> 00:07:38,560
claim that they their availability zone

177
00:07:37,000 --> 00:07:40,840
and to her available at Villa inter

178
00:07:38,560 --> 00:07:42,310
availability zone performance is less

179
00:07:40,840 --> 00:07:47,229
than two milliseconds so we wanted to

180
00:07:42,310 --> 00:07:48,730
see if that actually held one thing that

181
00:07:47,230 --> 00:07:51,670
we won't cover today but something we

182
00:07:48,730 --> 00:07:54,940
also looked at is the interconnectivity

183
00:07:51,670 --> 00:07:56,800
between cloud providers so between say a

184
00:07:54,940 --> 00:07:59,260
region hosted an AWS in a region a

185
00:07:56,800 --> 00:08:00,970
hosted and azure are there interesting

186
00:07:59,260 --> 00:08:02,590
things to observe in terms of how they

187
00:08:00,970 --> 00:08:04,390
connect to one another as you would

188
00:08:02,590 --> 00:08:07,060
expect they're very densely paired with

189
00:08:04,390 --> 00:08:08,890
one another so traffic that is flowing

190
00:08:07,060 --> 00:08:10,510
from one region in a cloud provider to a

191
00:08:08,890 --> 00:08:12,370
region another cloud provider typically

192
00:08:10,510 --> 00:08:14,530
doesn't go over the Internet it just

193
00:08:12,370 --> 00:08:16,270
gets handed from one cloud provider to

194
00:08:14,530 --> 00:08:19,809
the other there is one exception to this

195
00:08:16,270 --> 00:08:26,140
which I will touch on very briefly so

196
00:08:19,810 --> 00:08:27,490
looking first just to give you some kind

197
00:08:26,140 --> 00:08:30,010
of context for the findings we're going

198
00:08:27,490 --> 00:08:31,570
to cover there's a lot of data to unpack

199
00:08:30,010 --> 00:08:32,860
so this is a little bit of a whirlwind

200
00:08:31,570 --> 00:08:34,480
tour we're just going to cover some

201
00:08:32,860 --> 00:08:36,760
highlights and then look more closely at

202
00:08:34,480 --> 00:08:39,070
some anomalous behavior so as I

203
00:08:36,760 --> 00:08:40,689
mentioned before we're using a baseline

204
00:08:39,070 --> 00:08:44,500
to kind of get an understanding of how

205
00:08:40,690 --> 00:08:47,560
the cloud providers are performing

206
00:08:44,500 --> 00:08:50,200
relative to some baselines so across all

207
00:08:47,560 --> 00:08:51,779
of the cloud providers they performed

208
00:08:50,200 --> 00:08:54,660
very well in

209
00:08:51,779 --> 00:08:56,819
almost every instance the performance

210
00:08:54,660 --> 00:08:58,829
between region pairs was performing

211
00:08:56,819 --> 00:09:03,479
better than the internet baseline that

212
00:08:58,829 --> 00:09:05,189
we measured so for example IBM cloud 97%

213
00:09:03,480 --> 00:09:07,800
of their inter region pairs were

214
00:09:05,189 --> 00:09:09,269
performing better than the internet

215
00:09:07,800 --> 00:09:13,800
measurement that we were taking between

216
00:09:09,269 --> 00:09:15,749
similarly located and points on the

217
00:09:13,800 --> 00:09:18,209
opposite end of the spectrum we have oli

218
00:09:15,749 --> 00:09:21,629
cloud which had about 15 percent of them

219
00:09:18,209 --> 00:09:23,998
were performing worse than the internet

220
00:09:21,629 --> 00:09:26,220
baseline measurement we took the

221
00:09:23,999 --> 00:09:28,199
interesting thing about oli cloud which

222
00:09:26,220 --> 00:09:30,870
we saw when we dug a little bit deeper

223
00:09:28,199 --> 00:09:35,339
is that unlike the other cloud providers

224
00:09:30,870 --> 00:09:38,480
so across the board AWS is your GCP IBM

225
00:09:35,339 --> 00:09:41,970
cloud when they're connecting between

226
00:09:38,480 --> 00:09:44,040
regions their own regions they always

227
00:09:41,970 --> 00:09:46,499
use their own backbone that is not the

228
00:09:44,040 --> 00:09:49,949
case in every instance with oli cloud in

229
00:09:46,499 --> 00:09:51,569
some cases connecting between two region

230
00:09:49,949 --> 00:09:53,128
oli cloud regions you're connecting over

231
00:09:51,569 --> 00:09:56,870
the Internet and that was the same

232
00:09:53,129 --> 00:09:59,100
connecting to other cloud provider

233
00:09:56,870 --> 00:10:00,750
networks sometimes it would go over the

234
00:09:59,100 --> 00:10:02,790
Internet and sometimes depending on

235
00:10:00,750 --> 00:10:04,139
their peering it would connect you would

236
00:10:02,790 --> 00:10:06,689
connect directly to that cloud provider

237
00:10:04,139 --> 00:10:07,860
so they were definitely anomaly when it

238
00:10:06,689 --> 00:10:10,139
came to that and it seemed to have

239
00:10:07,860 --> 00:10:14,220
impacted their overall performance that

240
00:10:10,139 --> 00:10:18,809
you see here so quickly touching on

241
00:10:14,220 --> 00:10:22,259
inter a Z performance so we are looking

242
00:10:18,809 --> 00:10:24,899
here at the handful of regions that we

243
00:10:22,259 --> 00:10:27,600
tested and overall all of the cloud

244
00:10:24,899 --> 00:10:29,910
providers look pretty decent they claim

245
00:10:27,600 --> 00:10:32,550
less than two milliseconds inter

246
00:10:29,910 --> 00:10:34,920
availability zone performance and that

247
00:10:32,550 --> 00:10:37,679
was pretty consistent across the board

248
00:10:34,920 --> 00:10:39,899
everybody does really well a couple of

249
00:10:37,679 --> 00:10:42,089
regions of IBM cloud were kind of at

250
00:10:39,899 --> 00:10:46,019
that watermark two milliseconds some but

251
00:10:42,089 --> 00:10:50,160
overall pretty good Asher was the most

252
00:10:46,019 --> 00:10:54,569
consistent so about 0.7 ish milliseconds

253
00:10:50,160 --> 00:10:57,540
for all of the all of the regions that

254
00:10:54,569 --> 00:10:58,949
we tested and then for example GCP and

255
00:10:57,540 --> 00:11:00,839
oli cloud there was a little bit more of

256
00:10:58,949 --> 00:11:03,060
a kind of a range there I mean some of

257
00:11:00,839 --> 00:11:06,510
it was like a li cloud is like point

258
00:11:03,060 --> 00:11:09,479
seven milliseconds all the way up to you

259
00:11:06,510 --> 00:11:13,020
know like over one-and-a-half

260
00:11:09,480 --> 00:11:17,490
milliseconds so pretty different range

261
00:11:13,020 --> 00:11:20,610
of performance there now these very low

262
00:11:17,490 --> 00:11:23,580
latency 'z some of it of a red flag for

263
00:11:20,610 --> 00:11:25,710
us I mean if you're using they you're if

264
00:11:23,580 --> 00:11:28,260
you're hosting in different availability

265
00:11:25,710 --> 00:11:30,270
zones within the same region you want to

266
00:11:28,260 --> 00:11:31,770
know that and if you're using it for

267
00:11:30,270 --> 00:11:35,010
redundancy purposes you want to know

268
00:11:31,770 --> 00:11:36,750
that these these availability zones are

269
00:11:35,010 --> 00:11:39,300
for example have independent power

270
00:11:36,750 --> 00:11:42,060
sources and are differently connected

271
00:11:39,300 --> 00:11:44,969
and so they're truly redundant so when

272
00:11:42,060 --> 00:11:46,709
we see this type of performance where

273
00:11:44,970 --> 00:11:48,210
it's the latency is that low it begs the

274
00:11:46,710 --> 00:11:50,130
question of that if that's really the

275
00:11:48,210 --> 00:11:52,650
case are they in a different rack or a

276
00:11:50,130 --> 00:11:57,570
different floor this data center are

277
00:11:52,650 --> 00:11:59,250
they truly networked and source power

278
00:11:57,570 --> 00:12:01,020
source differently from one another I

279
00:11:59,250 --> 00:12:02,910
think that's maybe something at least

280
00:12:01,020 --> 00:12:05,670
asked worth asking the cloud provider if

281
00:12:02,910 --> 00:12:08,219
you use them so that was just something

282
00:12:05,670 --> 00:12:11,250
that kind of stood out to us the other

283
00:12:08,220 --> 00:12:12,840
thing that we were able to show in

284
00:12:11,250 --> 00:12:15,210
looking at all of the measurements and

285
00:12:12,840 --> 00:12:17,340
how traffic gets routed from user

286
00:12:15,210 --> 00:12:18,780
locations so these are points outside of

287
00:12:17,340 --> 00:12:20,880
the cloud provider networks to the

288
00:12:18,780 --> 00:12:25,290
various regions is that there's really

289
00:12:20,880 --> 00:12:27,150
two sort of ways in which the providers

290
00:12:25,290 --> 00:12:29,040
prefer to route traffic and these

291
00:12:27,150 --> 00:12:30,930
patterns are really consistent across

292
00:12:29,040 --> 00:12:36,120
the providers again with one exception

293
00:12:30,930 --> 00:12:39,319
so there's a a routing preference where

294
00:12:36,120 --> 00:12:42,630
traffic from user locations will

295
00:12:39,320 --> 00:12:45,870
primarily preponderance of the of the

296
00:12:42,630 --> 00:12:47,880
journey to the service that's hosted in

297
00:12:45,870 --> 00:12:50,610
the cloud provider will take place over

298
00:12:47,880 --> 00:12:52,230
the internet so you're going to be you

299
00:12:50,610 --> 00:12:54,150
know if your user in Frankfurt you might

300
00:12:52,230 --> 00:12:57,570
connect across the Internet and then you

301
00:12:54,150 --> 00:13:00,959
enter the cloud providers Network very

302
00:12:57,570 --> 00:13:03,600
close to the service that you're trying

303
00:13:00,960 --> 00:13:05,970
to access and where it's hosted the

304
00:13:03,600 --> 00:13:08,340
opposite of this are the providers where

305
00:13:05,970 --> 00:13:11,130
they are very backbone centric and they

306
00:13:08,340 --> 00:13:13,170
pull in users into their network as soon

307
00:13:11,130 --> 00:13:14,640
as they possibly can so in many

308
00:13:13,170 --> 00:13:15,229
instances you might see that you're

309
00:13:14,640 --> 00:13:16,699
entering

310
00:13:15,230 --> 00:13:18,050
some of the cloud provider networks

311
00:13:16,700 --> 00:13:19,550
within just a few hops and then you're

312
00:13:18,050 --> 00:13:22,180
gonna be riding their backbone all the

313
00:13:19,550 --> 00:13:24,380
way to the service that you're you're

314
00:13:22,180 --> 00:13:25,849
you want to reach it doesn't matter

315
00:13:24,380 --> 00:13:28,100
where it is in their network could be on

316
00:13:25,850 --> 00:13:30,140
the other side of the globe you're

317
00:13:28,100 --> 00:13:31,640
effectively going to be primarily using

318
00:13:30,140 --> 00:13:35,090
the cloud providers network so these are

319
00:13:31,640 --> 00:13:39,800
very distinct approaches to routing

320
00:13:35,090 --> 00:13:42,860
users to services and like I said from a

321
00:13:39,800 --> 00:13:44,839
pattern standpoint it's very they have

322
00:13:42,860 --> 00:13:48,590
very kind of clear preferences across

323
00:13:44,840 --> 00:13:51,770
the providers so a juror and GCP they

324
00:13:48,590 --> 00:13:53,870
have a very extensive edge and users

325
00:13:51,770 --> 00:13:56,569
connecting to services hosted in those

326
00:13:53,870 --> 00:13:59,540
cloud providers they get pulled into

327
00:13:56,570 --> 00:14:02,420
their network very very quickly so they

328
00:13:59,540 --> 00:14:04,730
don't the traffic doesn't spend very

329
00:14:02,420 --> 00:14:06,170
long on the public Internet you're

330
00:14:04,730 --> 00:14:08,180
primarily gonna be writing about the

331
00:14:06,170 --> 00:14:10,910
backbone and that goes for both forward

332
00:14:08,180 --> 00:14:13,520
and reverse path so a lot of the service

333
00:14:10,910 --> 00:14:15,500
delivery for users for services hosted

334
00:14:13,520 --> 00:14:19,329
in these cloud providers is going to be

335
00:14:15,500 --> 00:14:23,240
delivered by the cloud providers Network

336
00:14:19,330 --> 00:14:27,410
Ally cloud and AWS very different they

337
00:14:23,240 --> 00:14:29,090
prefer to keep users on the Internet as

338
00:14:27,410 --> 00:14:31,969
long as possible they don't use their

339
00:14:29,090 --> 00:14:34,280
backbone as much so you know again

340
00:14:31,970 --> 00:14:37,670
you're gonna be subject to the

341
00:14:34,280 --> 00:14:39,560
performance of ISPs if you're depending

342
00:14:37,670 --> 00:14:41,449
on where you're connecting where your

343
00:14:39,560 --> 00:14:44,949
user is and where the region you're

344
00:14:41,450 --> 00:14:47,660
connecting to is is located and then

345
00:14:44,950 --> 00:14:51,830
kind of this third category that we saw

346
00:14:47,660 --> 00:14:53,600
this year was IBM cloud and they have

347
00:14:51,830 --> 00:14:55,790
the hybrid approach I guess you could

348
00:14:53,600 --> 00:14:58,970
say where in some parts of their network

349
00:14:55,790 --> 00:15:00,079
you are pulled into their network very

350
00:14:58,970 --> 00:15:02,510
quickly and you're gonna ride your

351
00:15:00,080 --> 00:15:05,210
backbone for certain regions like

352
00:15:02,510 --> 00:15:06,770
Ashburn as an example of this and then

353
00:15:05,210 --> 00:15:09,040
there are other regions where you're

354
00:15:06,770 --> 00:15:11,510
going to be connecting over the Internet

355
00:15:09,040 --> 00:15:15,319
until you're almost you know right on

356
00:15:11,510 --> 00:15:17,750
top of the service so predominantly

357
00:15:15,320 --> 00:15:21,740
again like very different approaches iBM

358
00:15:17,750 --> 00:15:22,820
is kind of a mixture of the two does

359
00:15:21,740 --> 00:15:24,250
this make a difference in terms of

360
00:15:22,820 --> 00:15:27,940
performance

361
00:15:24,250 --> 00:15:31,300
we'll see that in a moment I think

362
00:15:27,940 --> 00:15:34,060
overall there does seem to be an impact

363
00:15:31,300 --> 00:15:35,949
but it's not as clear Qaida it's like

364
00:15:34,060 --> 00:15:39,790
the Internet is worse or the backbone is

365
00:15:35,950 --> 00:15:41,110
better it really depends on some

366
00:15:39,790 --> 00:15:42,459
fundamentals like how you're

367
00:15:41,110 --> 00:15:43,960
implementing Browdy and whether you're

368
00:15:42,460 --> 00:15:47,050
optimizing and you can have issues

369
00:15:43,960 --> 00:15:50,440
whether it's a backbone or the Internet

370
00:15:47,050 --> 00:15:52,500
and we'll see some examples of this so

371
00:15:50,440 --> 00:15:54,850
some of the more interesting things that

372
00:15:52,500 --> 00:15:57,310
we're going to share today are around

373
00:15:54,850 --> 00:15:59,170
kind of comparing differences between

374
00:15:57,310 --> 00:16:01,209
what we saw in 2018 and what we saw in

375
00:15:59,170 --> 00:16:04,089
2019 so this is just looking at the

376
00:16:01,210 --> 00:16:09,580
three providers that we looked at in

377
00:16:04,090 --> 00:16:12,940
2018 so we have AWS measure and Google

378
00:16:09,580 --> 00:16:16,240
and this is for users from regions

379
00:16:12,940 --> 00:16:18,820
around the globe connecting to a region

380
00:16:16,240 --> 00:16:21,780
hosted in Mumbai so this is just for

381
00:16:18,820 --> 00:16:25,060
this one region 98 locations globally

382
00:16:21,780 --> 00:16:27,790
connecting to Mumbai so what we see for

383
00:16:25,060 --> 00:16:29,739
users that are located in Asia so you

384
00:16:27,790 --> 00:16:31,990
can see that on the left hand side AWS

385
00:16:29,740 --> 00:16:34,060
is yellow by the way and then blue is

386
00:16:31,990 --> 00:16:36,790
azure red is TCP because you can't see

387
00:16:34,060 --> 00:16:41,500
that and so what we saw was that not

388
00:16:36,790 --> 00:16:44,170
only was latency higher for AWS but the

389
00:16:41,500 --> 00:16:45,910
very variation you know those it's

390
00:16:44,170 --> 00:16:48,040
effectively the range and min max what

391
00:16:45,910 --> 00:16:51,459
we were seeing in terms of latency

392
00:16:48,040 --> 00:16:52,569
measurements was just so much more

393
00:16:51,460 --> 00:16:55,360
extreme than it was with the other

394
00:16:52,570 --> 00:16:57,520
providers now one could argue okay well

395
00:16:55,360 --> 00:16:58,060
if you're connecting from locations in

396
00:16:57,520 --> 00:17:00,189
Asia

397
00:16:58,060 --> 00:17:02,979
maybe the connectivity service provider

398
00:17:00,190 --> 00:17:05,650
performance isn't as good as maybe you

399
00:17:02,980 --> 00:17:08,760
might get in North America so could that

400
00:17:05,650 --> 00:17:11,830
be contributing to the latency well

401
00:17:08,760 --> 00:17:15,069
interestingly enough if we look at the

402
00:17:11,829 --> 00:17:19,000
difference this year latency improved

403
00:17:15,069 --> 00:17:20,710
and also the variability and the

404
00:17:19,000 --> 00:17:24,280
performance numbers that we were seeing

405
00:17:20,710 --> 00:17:25,540
also improved as well so they were more

406
00:17:24,280 --> 00:17:27,339
consistent in their measurements

407
00:17:25,540 --> 00:17:29,620
bear in mind they haven't changed

408
00:17:27,339 --> 00:17:32,080
anything in terms of their preference of

409
00:17:29,620 --> 00:17:36,370
routing users over the Internet so it's

410
00:17:32,080 --> 00:17:38,010
it's not likely that that was the source

411
00:17:36,370 --> 00:17:40,678
of the issue and in fact

412
00:17:38,010 --> 00:17:43,470
we'll look at an example of what the

413
00:17:40,679 --> 00:17:45,900
issue actually was or some examples of

414
00:17:43,470 --> 00:17:47,760
that so it's you know it's it's

415
00:17:45,900 --> 00:17:49,440
interesting to see that even though

416
00:17:47,760 --> 00:17:51,000
there's they're using Internet

417
00:17:49,440 --> 00:17:53,130
connectivity they still are able to

418
00:17:51,000 --> 00:17:56,280
effect optimizations and make

419
00:17:53,130 --> 00:17:58,679
improvements year-over-year and of

420
00:17:56,280 --> 00:18:00,450
course this is just looking at their

421
00:17:58,679 --> 00:18:02,490
regular service will come in a few

422
00:18:00,450 --> 00:18:04,110
minutes to how they perform from a

423
00:18:02,490 --> 00:18:06,960
global accelerator standpoint which is

424
00:18:04,110 --> 00:18:09,479
meant to do what a Juran GCP do as a

425
00:18:06,960 --> 00:18:15,960
default which is allow users to ride

426
00:18:09,480 --> 00:18:18,600
their backbone to services so this is a

427
00:18:15,960 --> 00:18:20,100
couple of examples of what we saw and

428
00:18:18,600 --> 00:18:22,620
what was contributing to some of the

429
00:18:20,100 --> 00:18:26,790
higher latency numbers that we were

430
00:18:22,620 --> 00:18:30,059
seeing so on the left-hand side you see

431
00:18:26,790 --> 00:18:32,220
two green dots the top one is advantage

432
00:18:30,059 --> 00:18:35,309
points that's located in Seoul sorry I

433
00:18:32,220 --> 00:18:38,490
know the the text is quite small and so

434
00:18:35,309 --> 00:18:40,918
that location traffic connecting to that

435
00:18:38,490 --> 00:18:43,830
Mumbai region you'll notice just the

436
00:18:40,919 --> 00:18:47,820
huge number of hops those blue hops that

437
00:18:43,830 --> 00:18:52,590
you see there those are ISP Network hops

438
00:18:47,820 --> 00:18:56,668
and the green dots are AWS is backbone

439
00:18:52,590 --> 00:18:59,189
and there's just you know again from a

440
00:18:56,669 --> 00:19:02,130
proportion standpoint you see it's a lot

441
00:18:59,190 --> 00:19:04,950
of time spent on the Internet the reason

442
00:19:02,130 --> 00:19:06,830
for that it turns out that users that

443
00:19:04,950 --> 00:19:09,210
were in Seoul were getting connected

444
00:19:06,830 --> 00:19:10,949
through New York so they were

445
00:19:09,210 --> 00:19:13,320
effectively circumnavigating around the

446
00:19:10,950 --> 00:19:15,450
globe to get to a region hosted in

447
00:19:13,320 --> 00:19:17,159
Mumbai which if you pull out a map

448
00:19:15,450 --> 00:19:21,780
you'll see that that's really not the

449
00:19:17,160 --> 00:19:23,520
optimal path from Korea to India so that

450
00:19:21,780 --> 00:19:25,830
was introducing a lot of latency and

451
00:19:23,520 --> 00:19:27,540
then you know of course contributing to

452
00:19:25,830 --> 00:19:31,260
the performance difference that we were

453
00:19:27,540 --> 00:19:33,240
seeing there now what changed in 2019 we

454
00:19:31,260 --> 00:19:35,610
can see here that there was actually

455
00:19:33,240 --> 00:19:37,260
some changes made in terms of how

456
00:19:35,610 --> 00:19:40,168
traffic was getting routed so we see

457
00:19:37,260 --> 00:19:42,629
from that same location Seoul that they

458
00:19:40,169 --> 00:19:45,480
are then connecting through Singapore to

459
00:19:42,630 --> 00:19:47,610
connect to a WSS network so they're not

460
00:19:45,480 --> 00:19:50,240
going around the globe it's not that you

461
00:19:47,610 --> 00:19:51,979
know odd suboptimal path it's

462
00:19:50,240 --> 00:19:53,900
same for Singapore's one of the

463
00:19:51,980 --> 00:19:56,120
differences we saw was an Equinix

464
00:19:53,900 --> 00:19:58,370
facility popped into the path and

465
00:19:56,120 --> 00:20:03,350
there's really just a few hops between

466
00:19:58,370 --> 00:20:05,750
that location and a more optimal you

467
00:20:03,350 --> 00:20:10,639
know I guess connecting directly through

468
00:20:05,750 --> 00:20:12,200
Equinix to a WSS network so even though

469
00:20:10,640 --> 00:20:14,059
you know again they favor the Internet

470
00:20:12,200 --> 00:20:18,830
they still were able to make these

471
00:20:14,059 --> 00:20:21,678
optimizations it's for routing customers

472
00:20:18,830 --> 00:20:26,149
to them so another interesting thing

473
00:20:21,679 --> 00:20:28,580
that we looked at was GCP so one of the

474
00:20:26,150 --> 00:20:32,360
things that we saw you know and again

475
00:20:28,580 --> 00:20:36,050
they favor their backbone they had

476
00:20:32,360 --> 00:20:42,370
really significant latency difference

477
00:20:36,050 --> 00:20:44,750
compared to Azure and AWS so why is that

478
00:20:42,370 --> 00:20:46,610
so when we looked at the actual network

479
00:20:44,750 --> 00:20:50,120
paths we could see that traffic from

480
00:20:46,610 --> 00:20:58,100
users in Europe was again getting routed

481
00:20:50,120 --> 00:21:00,229
around the globe to India GCP really

482
00:20:58,100 --> 00:21:02,899
does favor their backbone and using

483
00:21:00,230 --> 00:21:05,809
their own connectivity and they don't

484
00:21:02,900 --> 00:21:08,690
have at least excuse me they did not

485
00:21:05,809 --> 00:21:12,678
have last year direct connectivity from

486
00:21:08,690 --> 00:21:14,960
Europe to Mumbai or India and so they

487
00:21:12,679 --> 00:21:19,850
were routing traffic around the globe to

488
00:21:14,960 --> 00:21:22,640
get to that point now at she's at Nanog

489
00:21:19,850 --> 00:21:25,040
last year when we presented some of this

490
00:21:22,640 --> 00:21:26,840
information someone from Google stood up

491
00:21:25,040 --> 00:21:27,950
and said well actually they're gonna be

492
00:21:26,840 --> 00:21:30,290
making a change to their network

493
00:21:27,950 --> 00:21:33,710
infrastructure gonna be adding direct

494
00:21:30,290 --> 00:21:36,860
connectivity to India from Europe so

495
00:21:33,710 --> 00:21:39,470
this year we were really excited to

496
00:21:36,860 --> 00:21:42,590
retest and see what difference that made

497
00:21:39,470 --> 00:21:44,840
in terms of performance and what we

498
00:21:42,590 --> 00:21:45,919
found was there wasn't really any

499
00:21:44,840 --> 00:21:48,020
difference

500
00:21:45,920 --> 00:21:51,800
it was almost identical to what we saw

501
00:21:48,020 --> 00:21:54,260
last year which was really surprising to

502
00:21:51,800 --> 00:21:56,480
us because if you go to Google's

503
00:21:54,260 --> 00:21:58,780
published infrastructure and network map

504
00:21:56,480 --> 00:22:02,410
you can see that in fact they did add

505
00:21:58,780 --> 00:22:04,870
direct connectivity to

506
00:22:02,410 --> 00:22:06,670
India so they had infrastructure they

507
00:22:04,870 --> 00:22:10,989
had the fiber they released senior they

508
00:22:06,670 --> 00:22:12,640
owned it but we were still seeing this

509
00:22:10,990 --> 00:22:15,100
circuitous path that was taking place

510
00:22:12,640 --> 00:22:17,560
users again they were getting pulled

511
00:22:15,100 --> 00:22:19,449
into Google's Network in Europe and then

512
00:22:17,560 --> 00:22:23,050
they were riding their backbone around

513
00:22:19,450 --> 00:22:25,180
the world to India so we actually reach

514
00:22:23,050 --> 00:22:27,970
out to GCP and asked them about this and

515
00:22:25,180 --> 00:22:29,350
they had said that they were still in

516
00:22:27,970 --> 00:22:30,820
the process even though they had the

517
00:22:29,350 --> 00:22:32,260
infrastructure in place they were still

518
00:22:30,820 --> 00:22:35,230
in the process of rolling out these

519
00:22:32,260 --> 00:22:37,900
routes to all of their regions globally

520
00:22:35,230 --> 00:22:39,580
so what they had said was it's still

521
00:22:37,900 --> 00:22:41,320
kind of in beta and they expect to do

522
00:22:39,580 --> 00:22:43,960
that over the next few months and and

523
00:22:41,320 --> 00:22:46,120
they had said though that this route was

524
00:22:43,960 --> 00:22:48,310
available for users who were in the

525
00:22:46,120 --> 00:22:51,070
Middle East and we were testing from

526
00:22:48,310 --> 00:22:54,250
Dubai and we were not seeing that we're

527
00:22:51,070 --> 00:22:56,620
four vantage point the user and in Dubai

528
00:22:54,250 --> 00:22:57,910
they were also getting routed around the

529
00:22:56,620 --> 00:23:01,030
globe so they weren't taking this new

530
00:22:57,910 --> 00:23:03,400
path but we do expect as GCP said that

531
00:23:01,030 --> 00:23:09,070
Google says that they will be rolling

532
00:23:03,400 --> 00:23:10,780
out this new route to more regions over

533
00:23:09,070 --> 00:23:12,970
the next few months so when we test

534
00:23:10,780 --> 00:23:19,300
later this year we'll hopefully see some

535
00:23:12,970 --> 00:23:23,200
improvement in in performance so this is

536
00:23:19,300 --> 00:23:25,330
also not explicitly performance related

537
00:23:23,200 --> 00:23:28,630
but I think it's worth noting because

538
00:23:25,330 --> 00:23:32,530
this was a change from 2018 to 2019 and

539
00:23:28,630 --> 00:23:34,390
that was that for the path that were

540
00:23:32,530 --> 00:23:38,680
mapping out when we were testing from

541
00:23:34,390 --> 00:23:40,660
locations outside of GCP to a region

542
00:23:38,680 --> 00:23:43,210
hosted in GCP didn't really matter where

543
00:23:40,660 --> 00:23:44,980
it was that we suddenly were losing

544
00:23:43,210 --> 00:23:47,800
visibility into the reverse path and

545
00:23:44,980 --> 00:23:49,390
this was across the board so it wasn't

546
00:23:47,800 --> 00:23:54,460
specific to a particular region it was

547
00:23:49,390 --> 00:23:57,670
just change in how or in terms of the

548
00:23:54,460 --> 00:24:02,110
visibility that we had so what was the

549
00:23:57,670 --> 00:24:02,920
root cause for this well if you go to GC

550
00:24:02,110 --> 00:24:06,129
Peas

551
00:24:02,920 --> 00:24:07,570
network help pages so they have some

552
00:24:06,130 --> 00:24:09,220
they've done some publication on this

553
00:24:07,570 --> 00:24:11,850
and they basically have said that

554
00:24:09,220 --> 00:24:13,990
they've made some changes where any

555
00:24:11,850 --> 00:24:16,750
trace route that's

556
00:24:13,990 --> 00:24:18,820
for an internet destination so this is

557
00:24:16,750 --> 00:24:21,190
not the case across their their own

558
00:24:18,820 --> 00:24:23,649
network or anything that's inbound

559
00:24:21,190 --> 00:24:28,899
really anything that's out on the

560
00:24:23,649 --> 00:24:31,539
internet they are adding to the TTL

561
00:24:28,899 --> 00:24:33,489
counter and so because of that that's

562
00:24:31,539 --> 00:24:35,740
effectively breaking traceroute I mean

563
00:24:33,490 --> 00:24:37,779
they said that depending on the number

564
00:24:35,740 --> 00:24:39,490
of hops there may be you know you lose

565
00:24:37,779 --> 00:24:41,470
some visibility but the result that

566
00:24:39,490 --> 00:24:42,970
we've seen and I don't know what where

567
00:24:41,470 --> 00:24:44,440
they're placing that counter my guess

568
00:24:42,970 --> 00:24:47,380
would be some really high thrust relate

569
00:24:44,440 --> 00:24:51,220
to 55 or 128 or something like that it

570
00:24:47,380 --> 00:24:55,450
didn't matter really how where the user

571
00:24:51,220 --> 00:24:59,289
was located we there was no decrementing

572
00:24:55,450 --> 00:25:02,309
of the TTL enough so that we were

573
00:24:59,289 --> 00:25:05,169
actually able to get any response and so

574
00:25:02,309 --> 00:25:07,658
because of that we can't see the reverse

575
00:25:05,169 --> 00:25:10,960
path which i think is interesting to

576
00:25:07,659 --> 00:25:12,490
note because there is this trend towards

577
00:25:10,960 --> 00:25:14,500
the cloud provider starting to monetize

578
00:25:12,490 --> 00:25:16,529
their backbone and if you don't have

579
00:25:14,500 --> 00:25:19,360
this ability into how traffic is routed

580
00:25:16,529 --> 00:25:21,760
then it's hard to know whether or not

581
00:25:19,360 --> 00:25:28,840
you're getting the specific service that

582
00:25:21,760 --> 00:25:30,700
you expect to see and we're running kind

583
00:25:28,840 --> 00:25:32,789
of low on time so I'm going to breeze

584
00:25:30,700 --> 00:25:36,789
through some of this stuff so

585
00:25:32,789 --> 00:25:39,129
performance from China I would say that

586
00:25:36,789 --> 00:25:40,899
overall all of the providers do pay a

587
00:25:39,130 --> 00:25:43,419
toll it's not like a Lee cloud or any

588
00:25:40,899 --> 00:25:46,479
other cloud provider is not subject to

589
00:25:43,419 --> 00:25:50,559
the same issues for any user connecting

590
00:25:46,480 --> 00:25:52,390
from China to a region outside of China

591
00:25:50,559 --> 00:25:54,250
this is packet loss so pretty high

592
00:25:52,390 --> 00:25:55,690
across all of them again don't really

593
00:25:54,250 --> 00:25:57,610
have time to look at this too much but

594
00:25:55,690 --> 00:26:00,100
there are some viable locations to host

595
00:25:57,610 --> 00:26:01,658
in outside of China not only from the

596
00:26:00,100 --> 00:26:05,580
packet loss but also from a latency

597
00:26:01,659 --> 00:26:11,220
standpoint so hong kong good options

598
00:26:05,580 --> 00:26:11,220
azure Ollie cloud all look pretty decent

599
00:26:13,770 --> 00:26:18,490
so for vantage points from China where

600
00:26:16,419 --> 00:26:21,029
he wanted to just look at strictly for

601
00:26:18,490 --> 00:26:24,789
you as broadband providers connecting to

602
00:26:21,029 --> 00:26:26,110
regions that were are in North America

603
00:26:24,789 --> 00:26:27,129
so as you would expect

604
00:26:26,110 --> 00:26:29,500
overall connectivity's

605
00:26:27,130 --> 00:26:30,880
really strong from Chicago connecting to

606
00:26:29,500 --> 00:26:32,080
as your east of course it's gonna be a

607
00:26:30,880 --> 00:26:34,240
lot lower than anything on the west

608
00:26:32,080 --> 00:26:36,730
coast but we did see some really odd

609
00:26:34,240 --> 00:26:39,010
stuff like for example from Verizon on

610
00:26:36,730 --> 00:26:41,080
the west coast connecting to a GCP

611
00:26:39,010 --> 00:26:43,450
region that was hosted in LA really

612
00:26:41,080 --> 00:26:45,280
really significant levels of latency

613
00:26:43,450 --> 00:26:46,810
just really bizarre and so we looked

614
00:26:45,280 --> 00:26:50,770
more closely at that and what we found

615
00:26:46,810 --> 00:26:53,800
is that traffic was connecting into GCPs

616
00:26:50,770 --> 00:26:55,870
network in New Jersey so these are from

617
00:26:53,800 --> 00:26:58,000
West Coast locations and then it was

618
00:26:55,870 --> 00:27:01,600
hair pinning along GCPs backbone and

619
00:26:58,000 --> 00:27:03,760
getting connected to LA so it didn't

620
00:27:01,600 --> 00:27:07,000
really matter that you are connecting

621
00:27:03,760 --> 00:27:09,550
into their network or they you know that

622
00:27:07,000 --> 00:27:10,600
they are more backbone centric this was

623
00:27:09,550 --> 00:27:13,210
still a rowdy an issue that was

624
00:27:10,600 --> 00:27:16,270
impacting users and it wasn't optimal

625
00:27:13,210 --> 00:27:18,310
obviously they we alerted them to this

626
00:27:16,270 --> 00:27:19,870
they very quickly made a change so you

627
00:27:18,310 --> 00:27:23,590
can see this sort of dramatic drop in

628
00:27:19,870 --> 00:27:24,939
latency for users on the west coast and

629
00:27:23,590 --> 00:27:26,679
then it just kind of went back to what

630
00:27:24,940 --> 00:27:29,710
you would affect expect to see if you're

631
00:27:26,680 --> 00:27:31,090
in that region global accelerators it's

632
00:27:29,710 --> 00:27:33,670
a really interesting one so I'm going to

633
00:27:31,090 --> 00:27:36,100
try to get through it in a minute so

634
00:27:33,670 --> 00:27:40,390
this is effectively designed to pull

635
00:27:36,100 --> 00:27:42,340
users onto AWS s backbone very quickly

636
00:27:40,390 --> 00:27:43,870
so closer to where the user is located

637
00:27:42,340 --> 00:27:45,370
and that works by directionally so

638
00:27:43,870 --> 00:27:48,219
you're gonna have more of the service

639
00:27:45,370 --> 00:27:51,580
delivery that's done by AWS versus over

640
00:27:48,220 --> 00:27:54,790
the Internet so how did it perform I

641
00:27:51,580 --> 00:27:56,020
mean as a premium tier service it

642
00:27:54,790 --> 00:27:58,389
depends

643
00:27:56,020 --> 00:28:00,340
it wasn't uniform in terms of improving

644
00:27:58,390 --> 00:28:02,940
performance we saw instances so the

645
00:28:00,340 --> 00:28:06,459
green is where performance was improved

646
00:28:02,940 --> 00:28:09,130
and orange is basically where it was

647
00:28:06,460 --> 00:28:10,900
effectively the same as their their

648
00:28:09,130 --> 00:28:13,170
regular way that they route traffic and

649
00:28:10,900 --> 00:28:16,690
then red was where we saw it actually

650
00:28:13,170 --> 00:28:19,450
had they performed even more poorly than

651
00:28:16,690 --> 00:28:23,290
or poorly compared to their standard way

652
00:28:19,450 --> 00:28:28,450
of routing so it really depends on the

653
00:28:23,290 --> 00:28:30,040
location and they are in fact working on

654
00:28:28,450 --> 00:28:31,600
optimizations when they saw some of this

655
00:28:30,040 --> 00:28:33,040
stuff so it was an example of this we

656
00:28:31,600 --> 00:28:34,419
tested in October they said they were

657
00:28:33,040 --> 00:28:36,700
going to be making some changes and we

658
00:28:34,420 --> 00:28:38,440
did in fact see an improvement in some

659
00:28:36,700 --> 00:28:39,590
of the locations we looked at where it

660
00:28:38,440 --> 00:28:42,440
wasn't as great as we

661
00:28:39,590 --> 00:28:43,639
expect it to be so they are responsive I

662
00:28:42,440 --> 00:28:45,110
would say across the board cloud

663
00:28:43,640 --> 00:28:46,520
providers are in general very responsive

664
00:28:45,110 --> 00:28:48,229
if you show them where there's an issue

665
00:28:46,520 --> 00:28:54,049
in terms of routing or performance

666
00:28:48,230 --> 00:28:56,840
issues they will usually try to do their

667
00:28:54,049 --> 00:28:58,940
best to make it right so just as summary

668
00:28:56,840 --> 00:29:01,370
very quickly so cloud provider

669
00:28:58,940 --> 00:29:03,200
preferences they vary it's not uniform

670
00:29:01,370 --> 00:29:05,178
it could be more Internet centric

671
00:29:03,200 --> 00:29:06,799
backbone centric it's really up to the

672
00:29:05,179 --> 00:29:09,919
cloud provider and that can change over

673
00:29:06,799 --> 00:29:12,350
time to inter region connectivity pretty

674
00:29:09,919 --> 00:29:13,580
good and really just rides the backbone

675
00:29:12,350 --> 00:29:17,360
across all of them except for Ollie

676
00:29:13,580 --> 00:29:18,260
cloud AWS global accelerator you want to

677
00:29:17,360 --> 00:29:19,908
know that you're getting the performance

678
00:29:18,260 --> 00:29:22,100
you expect so you can't necessarily

679
00:29:19,909 --> 00:29:24,500
assume that just because you're paying

680
00:29:22,100 --> 00:29:25,820
for a higher level of network service

681
00:29:24,500 --> 00:29:27,500
that that's exactly what you're gonna

682
00:29:25,820 --> 00:29:29,960
get so definitely make sure that you

683
00:29:27,500 --> 00:29:32,210
have some visibility there and then GCP

684
00:29:29,960 --> 00:29:34,730
Europe to India backbone route it's

685
00:29:32,210 --> 00:29:36,470
still kind of rolling out across most of

686
00:29:34,730 --> 00:29:38,450
their Gio's so that should be something

687
00:29:36,470 --> 00:29:44,539
that we will see over the course of this

688
00:29:38,450 --> 00:29:47,809
year takeaways things change trust but

689
00:29:44,539 --> 00:29:50,419
verify of course the cloud providers

690
00:29:47,809 --> 00:29:52,309
want to do right by their customers they

691
00:29:50,419 --> 00:29:53,960
operate massive global networks and

692
00:29:52,309 --> 00:29:55,309
sometimes in optimizing things or making

693
00:29:53,960 --> 00:29:57,620
changes it can have unintended

694
00:29:55,309 --> 00:30:00,470
consequences so you want to make sure

695
00:29:57,620 --> 00:30:03,070
that you can see what the impact is of

696
00:30:00,470 --> 00:30:06,409
those changes really important to get

697
00:30:03,070 --> 00:30:09,350
visibility into how your cloud provider

698
00:30:06,409 --> 00:30:11,510
is performing for you if you want to see

699
00:30:09,350 --> 00:30:15,770
the fuller report on this you can

700
00:30:11,510 --> 00:30:19,970
download it at this web address and the

701
00:30:15,770 --> 00:30:23,059
obligatory obligatory social media plugs

702
00:30:19,970 --> 00:30:26,179
here so my twitter handle is at bit

703
00:30:23,059 --> 00:30:28,428
prince and my colleague who's the lead

704
00:30:26,179 --> 00:30:30,049
author of this research archana case

705
00:30:28,429 --> 00:30:33,020
event can be reached at Archana

706
00:30:30,049 --> 00:30:34,850
underscore k7 if you have any questions

707
00:30:33,020 --> 00:30:37,520
we can be reached there I don't know if

708
00:30:34,850 --> 00:30:41,689
we have time for questions we don't okay

709
00:30:37,520 --> 00:30:43,029
well if there are any questions okay who

710
00:30:41,690 --> 00:30:46,520
wants to go first

711
00:30:43,029 --> 00:30:49,789
okay David Paul Zimmerman LinkedIn did

712
00:30:46,520 --> 00:30:52,840
you capture and/or look at any data ipv4

713
00:30:49,789 --> 00:30:54,850
versus ipv6 we did not know

714
00:30:52,840 --> 00:30:57,158
so we were just looking at ipv4 we have

715
00:30:54,850 --> 00:30:59,230
done ipv6 for some other research that

716
00:30:57,159 --> 00:31:02,860
we've done that may be something that we

717
00:30:59,230 --> 00:31:05,080
cover in the next it actually was

718
00:31:02,860 --> 00:31:07,360
something on our agenda to look into and

719
00:31:05,080 --> 00:31:08,918
potentially include but we were pulling

720
00:31:07,360 --> 00:31:10,418
in so many new things that we just

721
00:31:08,919 --> 00:31:15,700
decided to hold so we might do that

722
00:31:10,419 --> 00:31:17,500
later this year hi AVI Freedman question

723
00:31:15,700 --> 00:31:21,340
so every ten minutes you're doing these

724
00:31:17,500 --> 00:31:23,950
tests how many packets are actually

725
00:31:21,340 --> 00:31:25,510
between each of the zones or external

726
00:31:23,950 --> 00:31:28,480
regions so not like the trace but that

727
00:31:25,510 --> 00:31:30,940
are actually hitting each end so you're

728
00:31:28,480 --> 00:31:33,820
saying how many packets are being sent

729
00:31:30,940 --> 00:31:36,549
in each direction right roughly 50

730
00:31:33,820 --> 00:31:44,309
packets okay that'll hit each end okay

731
00:31:36,549 --> 00:31:44,309
yes Thanks okay

732
00:31:44,810 --> 00:31:48,929
[Applause]

