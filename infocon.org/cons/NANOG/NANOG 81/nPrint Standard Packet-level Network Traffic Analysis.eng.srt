1
00:00:01,610 --> 00:00:05,920
[Music]

2
00:00:05,920 --> 00:00:07,040
okay hi everyone

3
00:00:07,040 --> 00:00:09,120
uh i'm jordan holland i'm a third year

4
00:00:09,120 --> 00:00:10,320
phd candidate

5
00:00:10,320 --> 00:00:12,719
uh from princeton university uh really

6
00:00:12,719 --> 00:00:13,840
excited to talk to you

7
00:00:13,840 --> 00:00:16,560
talk to everyone today about this

8
00:00:16,560 --> 00:00:17,359
project

9
00:00:17,359 --> 00:00:19,760
named imprint which is this idea of

10
00:00:19,760 --> 00:00:21,119
standardizing

11
00:00:21,119 --> 00:00:23,119
a lot of network traffic analysis

12
00:00:23,119 --> 00:00:24,560
problems

13
00:00:24,560 --> 00:00:27,439
through a standard packet representation

14
00:00:27,439 --> 00:00:29,519
this is work done with my co-author paul

15
00:00:29,519 --> 00:00:31,439
schmidt and my advisors

16
00:00:31,439 --> 00:00:34,800
nick femster and pratik batal

17
00:00:34,800 --> 00:00:36,719
we do have a website up and running with

18
00:00:36,719 --> 00:00:40,079
tutorials on how to use the tool

19
00:00:40,079 --> 00:00:42,800
and of course i'm available for

20
00:00:42,800 --> 00:00:44,719
questions as well about it

21
00:00:44,719 --> 00:00:46,480
a quick overview of where we're heading

22
00:00:46,480 --> 00:00:47,920
this talk uh what we're going to do is

23
00:00:47,920 --> 00:00:49,440
we're going to introduce imprint which

24
00:00:49,440 --> 00:00:50,399
is

25
00:00:50,399 --> 00:00:52,960
a standard data representation that's

26
00:00:52,960 --> 00:00:55,120
meant to be used to solve a class of

27
00:00:55,120 --> 00:00:57,440
network traffic analysis problems

28
00:00:57,440 --> 00:00:58,960
and we're going to use in print to

29
00:00:58,960 --> 00:01:01,199
remove remove human driven feature

30
00:01:01,199 --> 00:01:02,160
engineering

31
00:01:02,160 --> 00:01:04,879
uh this this tedious step from the

32
00:01:04,879 --> 00:01:07,200
typical machine learning workflow

33
00:01:07,200 --> 00:01:10,080
uh for an entire class and set of

34
00:01:10,080 --> 00:01:12,080
network traffic analysis

35
00:01:12,080 --> 00:01:14,080
problems and we're actually going to

36
00:01:14,080 --> 00:01:16,320
show equivalent or better performance

37
00:01:16,320 --> 00:01:18,080
than hand engineered features on the

38
00:01:18,080 --> 00:01:19,759
same test such as

39
00:01:19,759 --> 00:01:21,439
active device fingerprinting comparing

40
00:01:21,439 --> 00:01:23,200
with nmap

41
00:01:23,200 --> 00:01:25,439
we actually in the paper we do eight

42
00:01:25,439 --> 00:01:26,960
different case studies

43
00:01:26,960 --> 00:01:28,720
which is available on the website we

44
00:01:28,720 --> 00:01:30,960
compare to poff and map

45
00:01:30,960 --> 00:01:34,159
and other popular tools uh so

46
00:01:34,159 --> 00:01:35,040
a little bit of background and

47
00:01:35,040 --> 00:01:37,680
motivation so at the intersection of

48
00:01:37,680 --> 00:01:39,280
machine learning and

49
00:01:39,280 --> 00:01:41,680
networking they're really an endless

50
00:01:41,680 --> 00:01:44,000
amount of sort of applications of

51
00:01:44,000 --> 00:01:45,119
machine learning

52
00:01:45,119 --> 00:01:47,280
to network traffic some we're going to

53
00:01:47,280 --> 00:01:48,399
look at today include device

54
00:01:48,399 --> 00:01:49,840
fingerprinting

55
00:01:49,840 --> 00:01:52,000
application identification others

56
00:01:52,000 --> 00:01:54,079
include you know os detection

57
00:01:54,079 --> 00:01:56,560
um website fingerprinting so can i can i

58
00:01:56,560 --> 00:01:58,159
fingerprint the website you're going to

59
00:01:58,159 --> 00:01:58,640
basically

60
00:01:58,640 --> 00:02:00,479
adjust your traffic pattern we have

61
00:02:00,479 --> 00:02:02,079
protocol fingerprinting

62
00:02:02,079 --> 00:02:03,920
like i said really this is such a large

63
00:02:03,920 --> 00:02:05,520
intersection that there's so many use

64
00:02:05,520 --> 00:02:06,640
cases

65
00:02:06,640 --> 00:02:09,280
for machine learning and applying

66
00:02:09,280 --> 00:02:11,120
machine learning to network traffic

67
00:02:11,120 --> 00:02:14,000
that if we can do it in an easy way we

68
00:02:14,000 --> 00:02:14,879
may enable

69
00:02:14,879 --> 00:02:19,440
rapid innovation in this space

70
00:02:19,680 --> 00:02:21,840
so speaking of machine learning applied

71
00:02:21,840 --> 00:02:23,120
to network traffic

72
00:02:23,120 --> 00:02:26,640
there's a real classic sort of

73
00:02:26,640 --> 00:02:29,120
workflow when you're doing when you're

74
00:02:29,120 --> 00:02:29,680
taking

75
00:02:29,680 --> 00:02:31,360
network traffic and you want to apply

76
00:02:31,360 --> 00:02:32,720
machine learning to it

77
00:02:32,720 --> 00:02:34,239
first you're going to hypothesize some

78
00:02:34,239 --> 00:02:36,000
problems say i want to fingerprint

79
00:02:36,000 --> 00:02:38,080
devices remotely right and then i'm

80
00:02:38,080 --> 00:02:39,840
going to gather network traffic that

81
00:02:39,840 --> 00:02:42,239
corresponds to that problem so i'm going

82
00:02:42,239 --> 00:02:44,239
to probe devices in this case for device

83
00:02:44,239 --> 00:02:45,360
fingerprinting and i'm going to look at

84
00:02:45,360 --> 00:02:47,200
the responses as my traffic

85
00:02:47,200 --> 00:02:48,560
and then i'm going to look at the the

86
00:02:48,560 --> 00:02:50,720
traffic and i'm going to say okay what

87
00:02:50,720 --> 00:02:53,840
what parts of this traffic are important

88
00:02:53,840 --> 00:02:55,519
to be able to distinguish

89
00:02:55,519 --> 00:02:57,280
different devices remotely so like for

90
00:02:57,280 --> 00:03:00,159
example the ttl we know is important

91
00:03:00,159 --> 00:03:03,040
uh for distinguishing remote devices and

92
00:03:03,040 --> 00:03:04,000
we're going to use our

93
00:03:04,000 --> 00:03:07,120
domain expertise and really our time

94
00:03:07,120 --> 00:03:09,120
to hand engineer and extract features

95
00:03:09,120 --> 00:03:10,319
that are hopefully predictive of that

96
00:03:10,319 --> 00:03:10,879
task

97
00:03:10,879 --> 00:03:12,239
and then we're going to train a model on

98
00:03:12,239 --> 00:03:14,879
those features we've extracted

99
00:03:14,879 --> 00:03:16,319
and hopefully it does well but really

100
00:03:16,319 --> 00:03:17,680
this goes back and forth so we're going

101
00:03:17,680 --> 00:03:18,400
to

102
00:03:18,400 --> 00:03:19,760
take the performance of our model and

103
00:03:19,760 --> 00:03:21,280
say oh we could probably do better and

104
00:03:21,280 --> 00:03:22,319
we're going to hand engineer more

105
00:03:22,319 --> 00:03:23,440
features and then we're going to train

106
00:03:23,440 --> 00:03:24,319
another model

107
00:03:24,319 --> 00:03:25,680
and we're going to continue doing this

108
00:03:25,680 --> 00:03:28,080
until we're satisfied with the model's

109
00:03:28,080 --> 00:03:30,080
performance

110
00:03:30,080 --> 00:03:31,920
what we noticed was really that the bulk

111
00:03:31,920 --> 00:03:33,360
of the time was spent

112
00:03:33,360 --> 00:03:34,879
extracting these features that are

113
00:03:34,879 --> 00:03:36,720
hopefully predictive of the task

114
00:03:36,720 --> 00:03:39,200
right so it takes domain expertise and

115
00:03:39,200 --> 00:03:41,360
really just a ton of iterations of time

116
00:03:41,360 --> 00:03:42,400
to figure out

117
00:03:42,400 --> 00:03:44,560
you know what what parts of network

118
00:03:44,560 --> 00:03:46,799
packets are important for

119
00:03:46,799 --> 00:03:49,599
active device fingerprinting or and then

120
00:03:49,599 --> 00:03:50,959
when we move to another problem we sort

121
00:03:50,959 --> 00:03:52,239
of have to reinvent the wheel

122
00:03:52,239 --> 00:03:54,400
and say okay well these features or

123
00:03:54,400 --> 00:03:55,360
these parts of the packets were

124
00:03:55,360 --> 00:03:56,000
important

125
00:03:56,000 --> 00:03:58,319
for device fingerprinting but now we

126
00:03:58,319 --> 00:04:01,040
need an entirely new uh

127
00:04:01,040 --> 00:04:04,319
features for uh identifying applications

128
00:04:04,319 --> 00:04:05,040
right

129
00:04:05,040 --> 00:04:07,120
so really this step takes the the bulk

130
00:04:07,120 --> 00:04:08,799
of the time and we have to redo it every

131
00:04:08,799 --> 00:04:09,760
time

132
00:04:09,760 --> 00:04:13,760
we want to solve a new problem so

133
00:04:13,760 --> 00:04:16,639
our goal is to remove this uh process

134
00:04:16,639 --> 00:04:18,079
entirely right so we're going to take

135
00:04:18,079 --> 00:04:19,358
some inspiration

136
00:04:19,358 --> 00:04:21,759
uh and our inspiration is this idea uh

137
00:04:21,759 --> 00:04:23,440
that you know deep learning techniques

138
00:04:23,440 --> 00:04:24,400
have been

139
00:04:24,400 --> 00:04:27,600
uh uh beyond i guess popularized beyond

140
00:04:27,600 --> 00:04:28,400
belief

141
00:04:28,400 --> 00:04:31,199
for a plethora of problems uh but they

142
00:04:31,199 --> 00:04:32,720
tend to perform really well

143
00:04:32,720 --> 00:04:34,320
on problems related to image

144
00:04:34,320 --> 00:04:36,000
classification uh

145
00:04:36,000 --> 00:04:37,919
and one of the reasons for this is that

146
00:04:37,919 --> 00:04:40,160
uh this rapid innovation is that

147
00:04:40,160 --> 00:04:42,400
uh pictures right have this standard

148
00:04:42,400 --> 00:04:43,840
representation of values

149
00:04:43,840 --> 00:04:45,919
right uh when i've whenever i want to

150
00:04:45,919 --> 00:04:48,160
classify a set of images

151
00:04:48,160 --> 00:04:51,360
um say a cat versus a dog um

152
00:04:51,360 --> 00:04:53,680
i i present the uh a deep learning

153
00:04:53,680 --> 00:04:54,639
classifier with

154
00:04:54,639 --> 00:04:58,639
a square or rectangle of pixels

155
00:04:58,639 --> 00:05:00,320
and it's going to learn how to classify

156
00:05:00,320 --> 00:05:02,080
it between the the cat and the dog we

157
00:05:02,080 --> 00:05:03,520
don't actually have to tell it how to do

158
00:05:03,520 --> 00:05:04,560
it

159
00:05:04,560 --> 00:05:06,080
and then if i want to do say a boat

160
00:05:06,080 --> 00:05:08,639
versus a car right i just

161
00:05:08,639 --> 00:05:10,320
take the pixels once again that are the

162
00:05:10,320 --> 00:05:12,479
images and allow the deep learning uh

163
00:05:12,479 --> 00:05:13,759
deep neural network

164
00:05:13,759 --> 00:05:16,240
um to actually extract the features

165
00:05:16,240 --> 00:05:17,440
right we don't have to tell the

166
00:05:17,440 --> 00:05:18,720
classifier that

167
00:05:18,720 --> 00:05:21,280
you know there's the boat doesn't have

168
00:05:21,280 --> 00:05:22,080
uh

169
00:05:22,080 --> 00:05:24,800
wheels right but it extracts these on

170
00:05:24,800 --> 00:05:25,840
its own

171
00:05:25,840 --> 00:05:27,120
so we want to see if we can do something

172
00:05:27,120 --> 00:05:28,479
like this with network traffic where we

173
00:05:28,479 --> 00:05:30,880
can you know avoid all this hard work

174
00:05:30,880 --> 00:05:33,360
and then related work shows that uh deep

175
00:05:33,360 --> 00:05:34,320
learning techniques

176
00:05:34,320 --> 00:05:36,080
have been super effective in website

177
00:05:36,080 --> 00:05:38,000
fingerprinting for tor

178
00:05:38,000 --> 00:05:40,160
and this is the idea right that we can

179
00:05:40,160 --> 00:05:41,520
fingerprint someone's the website

180
00:05:41,520 --> 00:05:43,120
someone is going to by just looking at

181
00:05:43,120 --> 00:05:44,560
the directions of the traffic

182
00:05:44,560 --> 00:05:47,919
because uh in tor right we have cells

183
00:05:47,919 --> 00:05:48,160
and

184
00:05:48,160 --> 00:05:49,840
everything is the same size and

185
00:05:49,840 --> 00:05:51,199
essentially we're left with this

186
00:05:51,199 --> 00:05:52,240
direction

187
00:05:52,240 --> 00:05:54,720
unfortunately we move outside of tor

188
00:05:54,720 --> 00:05:55,360
applying

189
00:05:55,360 --> 00:05:57,840
machine learning techniques in an easy

190
00:05:57,840 --> 00:05:58,960
way or a rapid

191
00:05:58,960 --> 00:06:01,600
sort of standard way to network traffic

192
00:06:01,600 --> 00:06:03,280
that isn't all the same size and the

193
00:06:03,280 --> 00:06:05,360
same protocol

194
00:06:05,360 --> 00:06:07,039
and essentially these cells is not

195
00:06:07,039 --> 00:06:09,600
nearly as simple right

196
00:06:09,600 --> 00:06:12,560
so we have some inspiration we want to

197
00:06:12,560 --> 00:06:14,160
basically

198
00:06:14,160 --> 00:06:16,160
end up in a way where we can supply

199
00:06:16,160 --> 00:06:17,600
machine learning classifiers with a

200
00:06:17,600 --> 00:06:19,520
standard representation

201
00:06:19,520 --> 00:06:23,359
such as like an image of a packet

202
00:06:23,840 --> 00:06:25,600
and we have some related work but now we

203
00:06:25,600 --> 00:06:27,120
need to know how how we actually get

204
00:06:27,120 --> 00:06:28,960
there right

205
00:06:28,960 --> 00:06:32,720
so what we've done is we've tried

206
00:06:32,720 --> 00:06:35,680
and developed uh some different standard

207
00:06:35,680 --> 00:06:36,960
representations

208
00:06:36,960 --> 00:06:39,840
uh for network packets and along the way

209
00:06:39,840 --> 00:06:40,960
we've sort of

210
00:06:40,960 --> 00:06:43,199
formalized these requirements for any

211
00:06:43,199 --> 00:06:44,720
sort of standard

212
00:06:44,720 --> 00:06:46,800
packet data representation that will be

213
00:06:46,800 --> 00:06:48,800
used like an image

214
00:06:48,800 --> 00:06:51,039
across a variety of traffic analysis

215
00:06:51,039 --> 00:06:52,880
problems

216
00:06:52,880 --> 00:06:55,440
uh so i'm going to go through them all

217
00:06:55,440 --> 00:06:56,720
and so we'll go through each one

218
00:06:56,720 --> 00:06:57,520
individually

219
00:06:57,520 --> 00:06:59,840
so first we want any standard data

220
00:06:59,840 --> 00:07:01,520
representation for a packet to be

221
00:07:01,520 --> 00:07:02,639
complete

222
00:07:02,639 --> 00:07:05,199
and completeness is this idea that each

223
00:07:05,199 --> 00:07:06,160
feature

224
00:07:06,160 --> 00:07:08,240
or every bit of every packet can be

225
00:07:08,240 --> 00:07:11,360
represented to the classifier

226
00:07:11,360 --> 00:07:14,160
and this means that every field for each

227
00:07:14,160 --> 00:07:16,240
packet needs to be able to included be

228
00:07:16,240 --> 00:07:17,520
included in a standard data

229
00:07:17,520 --> 00:07:19,280
representation so

230
00:07:19,280 --> 00:07:20,960
below we see we have two packets right

231
00:07:20,960 --> 00:07:23,280
we have a tcp packet in the udp packet

232
00:07:23,280 --> 00:07:25,280
and the hard part is right we have two

233
00:07:25,280 --> 00:07:26,479
images

234
00:07:26,479 --> 00:07:28,000
they're just a bunch of pixels

235
00:07:28,000 --> 00:07:30,479
unfortunately here we have two packets

236
00:07:30,479 --> 00:07:31,520
that actually contain different

237
00:07:31,520 --> 00:07:33,520
information entirely

238
00:07:33,520 --> 00:07:35,280
uh one of them has udp header and one of

239
00:07:35,280 --> 00:07:37,520
them has a tcp header

240
00:07:37,520 --> 00:07:39,199
so we want to be able to include both of

241
00:07:39,199 --> 00:07:41,199
these packets be able to represent both

242
00:07:41,199 --> 00:07:42,840
of these packets that contain different

243
00:07:42,840 --> 00:07:44,080
information

244
00:07:44,080 --> 00:07:46,240
with one representation one standard

245
00:07:46,240 --> 00:07:47,520
representation

246
00:07:47,520 --> 00:07:49,520
and our intuition is that if we have a

247
00:07:49,520 --> 00:07:51,360
standard representation

248
00:07:51,360 --> 00:07:53,039
that the models can actually determine

249
00:07:53,039 --> 00:07:55,039
which features are important

250
00:07:55,039 --> 00:07:56,319
or which parts of the packet are

251
00:07:56,319 --> 00:07:58,160
important for a given problem

252
00:07:58,160 --> 00:07:59,919
without all this human guidance that's

253
00:07:59,919 --> 00:08:01,199
been used before

254
00:08:01,199 --> 00:08:03,199
okay so first we want to be able to

255
00:08:03,199 --> 00:08:04,800
represent each feature

256
00:08:04,800 --> 00:08:07,199
for every packet the second requirement

257
00:08:07,199 --> 00:08:08,720
that we landed on was that

258
00:08:08,720 --> 00:08:11,360
we want any standard representation to

259
00:08:11,360 --> 00:08:12,000
be constant

260
00:08:12,000 --> 00:08:14,240
size per problem and what this means is

261
00:08:14,240 --> 00:08:16,319
that the size of the representation is

262
00:08:16,319 --> 00:08:17,680
the same for each packet

263
00:08:17,680 --> 00:08:19,520
and for images right we can just scale

264
00:08:19,520 --> 00:08:21,440
an image and the pixels still mean the

265
00:08:21,440 --> 00:08:23,360
same thing we just lose resolution

266
00:08:23,360 --> 00:08:24,960
unfortunately scaling doesn't really

267
00:08:24,960 --> 00:08:27,840
work for packets

268
00:08:27,919 --> 00:08:30,160
the real pain is the fact that different

269
00:08:30,160 --> 00:08:32,159
packet headers right like an iv header

270
00:08:32,159 --> 00:08:33,839
or tcp header

271
00:08:33,839 --> 00:08:35,679
are variable length and then we also

272
00:08:35,679 --> 00:08:37,120
have different size payloads so we

273
00:08:37,120 --> 00:08:38,320
actually have

274
00:08:38,320 --> 00:08:42,080
packets that are different links um

275
00:08:42,080 --> 00:08:43,440
entirely that we need to be able to

276
00:08:43,440 --> 00:08:46,160
represent uh in a constant size

277
00:08:46,160 --> 00:08:48,080
um and the real reason that this

278
00:08:48,080 --> 00:08:49,279
constant size requirement

279
00:08:49,279 --> 00:08:51,279
comes up is that for the majority of

280
00:08:51,279 --> 00:08:52,800
machine learning techniques

281
00:08:52,800 --> 00:08:56,240
that we're going to use they actually

282
00:08:56,240 --> 00:08:56,959
only take

283
00:08:56,959 --> 00:08:59,600
constant size input so there are some

284
00:08:59,600 --> 00:09:00,720
machine learning techniques that can

285
00:09:00,720 --> 00:09:02,000
take variable length input

286
00:09:02,000 --> 00:09:04,240
but the vast majority of them we're

287
00:09:04,240 --> 00:09:05,360
going to need a constant size

288
00:09:05,360 --> 00:09:06,800
representation

289
00:09:06,800 --> 00:09:08,000
to be able to have some sort of

290
00:09:08,000 --> 00:09:11,920
standardization in this area

291
00:09:12,000 --> 00:09:13,680
so what is an example of something that

292
00:09:13,680 --> 00:09:15,519
that meets some of these requirements

293
00:09:15,519 --> 00:09:18,240
uh so the classic way we started was

294
00:09:18,240 --> 00:09:18,640
okay

295
00:09:18,640 --> 00:09:21,040
we can use use our semantic

296
00:09:21,040 --> 00:09:22,399
understanding of packets

297
00:09:22,399 --> 00:09:24,880
to build a standard representation and

298
00:09:24,880 --> 00:09:26,800
so this semantic view involves packets

299
00:09:26,800 --> 00:09:28,240
being broken into headers

300
00:09:28,240 --> 00:09:29,839
and then each header is broken into

301
00:09:29,839 --> 00:09:32,320
fields right

302
00:09:32,320 --> 00:09:34,160
so we have a packet and we have the ip

303
00:09:34,160 --> 00:09:35,600
header and we have the tcp header and

304
00:09:35,600 --> 00:09:36,000
then those

305
00:09:36,000 --> 00:09:37,839
are broken up even further into fields

306
00:09:37,839 --> 00:09:40,640
like the source port and the version

307
00:09:40,640 --> 00:09:42,160
and we can actually encode each header

308
00:09:42,160 --> 00:09:44,320
field so the tcp source port is

309
00:09:44,320 --> 00:09:46,720
just encoded as itself an ipv version

310
00:09:46,720 --> 00:09:49,120
encoded in itself

311
00:09:49,120 --> 00:09:50,880
so the tcp source port would be 80 in

312
00:09:50,880 --> 00:09:53,519
this instance and if you version b4

313
00:09:53,519 --> 00:09:55,680
and represent that to a classifier and

314
00:09:55,680 --> 00:09:57,279
this is going to be constant size right

315
00:09:57,279 --> 00:09:57,920
we can

316
00:09:57,920 --> 00:10:01,040
just extract every feature every field

317
00:10:01,040 --> 00:10:03,279
of every packet header that we know of

318
00:10:03,279 --> 00:10:06,880
and represent them all to a classifier

319
00:10:06,880 --> 00:10:08,480
unfortunately this this doesn't quite

320
00:10:08,480 --> 00:10:10,079
get what we want and the reason is

321
00:10:10,079 --> 00:10:11,360
because

322
00:10:11,360 --> 00:10:13,839
when we think about packets semantically

323
00:10:13,839 --> 00:10:15,760
and representing them to a classifier in

324
00:10:15,760 --> 00:10:17,040
a standard way

325
00:10:17,040 --> 00:10:18,640
we actually lose a lot of information

326
00:10:18,640 --> 00:10:19,920
and have trouble with certain parts of

327
00:10:19,920 --> 00:10:20,720
the packet

328
00:10:20,720 --> 00:10:22,320
so the first problem is that we actually

329
00:10:22,320 --> 00:10:24,720
lose the ordering of tcp options

330
00:10:24,720 --> 00:10:26,320
and not just tcp options any really

331
00:10:26,320 --> 00:10:28,560
unstructured

332
00:10:28,560 --> 00:10:30,320
part of the packet so tcp options have

333
00:10:30,320 --> 00:10:32,079
been widely known and used to

334
00:10:32,079 --> 00:10:33,040
differentiate

335
00:10:33,040 --> 00:10:36,959
operating systems device characteristics

336
00:10:36,959 --> 00:10:38,560
they're widely used in nmap is a very

337
00:10:38,560 --> 00:10:40,160
predictive feature for what devices

338
00:10:40,160 --> 00:10:41,440
behind traffic

339
00:10:41,440 --> 00:10:43,680
and when you encode in a semantic way

340
00:10:43,680 --> 00:10:45,440
you lose this

341
00:10:45,440 --> 00:10:48,320
ordering that has so much information

342
00:10:48,320 --> 00:10:49,600
about what

343
00:10:49,600 --> 00:10:53,920
device generated the traffic or so on

344
00:10:53,920 --> 00:10:55,680
another problem is that when we

345
00:10:55,680 --> 00:10:57,040
represent packets in a semantic

346
00:10:57,040 --> 00:10:58,640
structure

347
00:10:58,640 --> 00:11:00,560
if we don't actually know the semantic

348
00:11:00,560 --> 00:11:02,880
structure uh we we run into a lot of

349
00:11:02,880 --> 00:11:03,920
problems so like when we have

350
00:11:03,920 --> 00:11:05,279
unstructured parts of the packet like

351
00:11:05,279 --> 00:11:06,160
the payload

352
00:11:06,160 --> 00:11:07,760
we can't just represent them numerically

353
00:11:07,760 --> 00:11:09,360
with their byte values because there's

354
00:11:09,360 --> 00:11:11,920
no real meaning

355
00:11:11,920 --> 00:11:14,079
the third problem is that machine

356
00:11:14,079 --> 00:11:15,920
learning techniques work really well

357
00:11:15,920 --> 00:11:18,320
with normalized data or standardized

358
00:11:18,320 --> 00:11:19,279
data

359
00:11:19,279 --> 00:11:21,760
and we want every feature to sort of lie

360
00:11:21,760 --> 00:11:23,680
between a standard range of values

361
00:11:23,680 --> 00:11:25,920
say negative one and one and what this

362
00:11:25,920 --> 00:11:28,800
does is it allows

363
00:11:28,800 --> 00:11:31,760
a classifier to be more stable in terms

364
00:11:31,760 --> 00:11:33,200
of training faster and

365
00:11:33,200 --> 00:11:35,600
have more stable predictions and with

366
00:11:35,600 --> 00:11:37,279
network traffic data sets they grow so

367
00:11:37,279 --> 00:11:39,040
large so quickly

368
00:11:39,040 --> 00:11:40,560
that if we have to normalize each

369
00:11:40,560 --> 00:11:42,079
specific feature

370
00:11:42,079 --> 00:11:43,519
so say we need to normalize the source

371
00:11:43,519 --> 00:11:45,279
port between zero and one we actually

372
00:11:45,279 --> 00:11:46,800
need to know the range of those values

373
00:11:46,800 --> 00:11:48,320
and this is incredibly expensive

374
00:11:48,320 --> 00:11:49,920
computationally

375
00:11:49,920 --> 00:11:52,399
and in terms of holding the values in

376
00:11:52,399 --> 00:11:54,000
memory

377
00:11:54,000 --> 00:11:56,880
so we actually need to encode this as a

378
00:11:56,880 --> 00:11:58,560
formal requirement this normalization

379
00:11:58,560 --> 00:11:59,519
technique

380
00:11:59,519 --> 00:12:01,600
um so we're going to encode a third

381
00:12:01,600 --> 00:12:02,560
requirement

382
00:12:02,560 --> 00:12:04,480
into our formal requirements that we

383
00:12:04,480 --> 00:12:05,680
would like a standard data

384
00:12:05,680 --> 00:12:07,120
representation to be inherently

385
00:12:07,120 --> 00:12:08,399
normalized

386
00:12:08,399 --> 00:12:10,240
what does inherently normalized mean it

387
00:12:10,240 --> 00:12:12,399
it generally means that

388
00:12:12,399 --> 00:12:14,320
naturally when we transform every packet

389
00:12:14,320 --> 00:12:16,639
each feature ranges between

390
00:12:16,639 --> 00:12:18,399
uh negative one and one so we don't have

391
00:12:18,399 --> 00:12:20,000
the source port having a wildly

392
00:12:20,000 --> 00:12:22,320
different

393
00:12:22,320 --> 00:12:25,440
range of values between 0 and 65 000

394
00:12:25,440 --> 00:12:27,839
uh versus the version between 0 and like

395
00:12:27,839 --> 00:12:29,279
16 right

396
00:12:29,279 --> 00:12:32,800
um and if it's inherently normalized

397
00:12:32,800 --> 00:12:34,320
this also allows beyond just

398
00:12:34,320 --> 00:12:35,600
stabilization

399
00:12:35,600 --> 00:12:38,959
uh it enables uh not having these

400
00:12:38,959 --> 00:12:40,720
computational passes to normalize the

401
00:12:40,720 --> 00:12:41,279
data

402
00:12:41,279 --> 00:12:43,360
uh which when you're running traffic at

403
00:12:43,360 --> 00:12:44,639
the speeds we have today

404
00:12:44,639 --> 00:12:45,920
you just don't have time to actually

405
00:12:45,920 --> 00:12:47,680
normalize these data sets

406
00:12:47,680 --> 00:12:51,200
in a streaming capacity at least so

407
00:12:51,200 --> 00:12:52,639
how can we actually come up with

408
00:12:52,639 --> 00:12:54,720
something that's inherently normalized

409
00:12:54,720 --> 00:12:58,240
um uh and complete right

410
00:12:58,240 --> 00:13:00,160
so let's turn the classic semantic view

411
00:13:00,160 --> 00:13:01,519
of packets on its head

412
00:13:01,519 --> 00:13:02,880
let's think of packets as just a

413
00:13:02,880 --> 00:13:04,720
collection of bits right

414
00:13:04,720 --> 00:13:07,200
so this is naturally complete we see

415
00:13:07,200 --> 00:13:08,160
below we have a

416
00:13:08,160 --> 00:13:09,920
tcpip packet with a semantic

417
00:13:09,920 --> 00:13:11,600
representation and then this

418
00:13:11,600 --> 00:13:13,600
naive binary representation of a

419
00:13:13,600 --> 00:13:15,200
collection of bit strings

420
00:13:15,200 --> 00:13:17,680
um it's pretty easy to see that it's now

421
00:13:17,680 --> 00:13:19,519
complete right and it codes ordering as

422
00:13:19,519 --> 00:13:20,399
well

423
00:13:20,399 --> 00:13:22,800
it's pretty uh it's pretty obvious that

424
00:13:22,800 --> 00:13:25,600
each bit can be represented as itself

425
00:13:25,600 --> 00:13:28,079
and then we also have this uh bonus

426
00:13:28,079 --> 00:13:29,040
guarantee that

427
00:13:29,040 --> 00:13:31,920
when we consider every uh packet as a

428
00:13:31,920 --> 00:13:33,040
bit string

429
00:13:33,040 --> 00:13:34,959
it's inherently normalized so each

430
00:13:34,959 --> 00:13:36,399
feature is naturally going to range from

431
00:13:36,399 --> 00:13:37,680
negative one to one

432
00:13:37,680 --> 00:13:39,360
really zero to one because we're using

433
00:13:39,360 --> 00:13:41,279
the bits themselves rather than their

434
00:13:41,279 --> 00:13:43,519
semantic encoding of those bits

435
00:13:43,519 --> 00:13:46,880
to represent to the classifier so

436
00:13:46,880 --> 00:13:48,800
unfortunately this doesn't really get us

437
00:13:48,800 --> 00:13:51,599
there either right

438
00:13:51,680 --> 00:13:53,839
so because packets have different

439
00:13:53,839 --> 00:13:54,880
variable links

440
00:13:54,880 --> 00:13:57,440
and even headers inside of packets have

441
00:13:57,440 --> 00:13:58,959
variable links

442
00:13:58,959 --> 00:14:00,839
we can run in this problem where we can

443
00:14:00,839 --> 00:14:02,399
actually

444
00:14:02,399 --> 00:14:04,240
have a problem with our encoding where

445
00:14:04,240 --> 00:14:06,160
different bits or features when we

446
00:14:06,160 --> 00:14:08,000
represent them as a classifier

447
00:14:08,000 --> 00:14:10,079
will have different meanings so below we

448
00:14:10,079 --> 00:14:11,199
have a

449
00:14:11,199 --> 00:14:14,880
tcp packet that has ip options and tcp

450
00:14:14,880 --> 00:14:17,600
options and we have a tcp packet with no

451
00:14:17,600 --> 00:14:18,639
options at all

452
00:14:18,639 --> 00:14:20,079
and what we see is that when we

453
00:14:20,079 --> 00:14:22,160
represent this to a classifier

454
00:14:22,160 --> 00:14:24,399
the first tcp bit and the first ip

455
00:14:24,399 --> 00:14:25,279
options bit

456
00:14:25,279 --> 00:14:26,880
we're actually presenting to the

457
00:14:26,880 --> 00:14:28,800
classifier as the same feature

458
00:14:28,800 --> 00:14:30,480
and this is going to directly inject

459
00:14:30,480 --> 00:14:33,040
noise into our classifier

460
00:14:33,040 --> 00:14:35,440
when it's trying to learn and confuse it

461
00:14:35,440 --> 00:14:38,079
so we're actually going to penalize

462
00:14:38,079 --> 00:14:41,199
or decrease our overall performance

463
00:14:41,199 --> 00:14:45,440
by considering the fact that

464
00:14:45,440 --> 00:14:49,120
each bit is a feature right

465
00:14:49,120 --> 00:14:50,880
so this problem only becomes worse when

466
00:14:50,880 --> 00:14:52,480
we have different types of packets

467
00:14:52,480 --> 00:14:54,560
so to illustrate this more now we have

468
00:14:54,560 --> 00:14:56,160
two packets that we saw before and we

469
00:14:56,160 --> 00:14:57,600
have a udp packet

470
00:14:57,600 --> 00:14:59,760
and now when we go to represent this to

471
00:14:59,760 --> 00:15:02,480
a classifier

472
00:15:02,880 --> 00:15:05,600
the first tcp bit the first udp bit and

473
00:15:05,600 --> 00:15:07,040
the first ip options bit

474
00:15:07,040 --> 00:15:09,040
we're presenting the classifier as

475
00:15:09,040 --> 00:15:10,320
here's a feature

476
00:15:10,320 --> 00:15:12,399
but what we've accidentally done is told

477
00:15:12,399 --> 00:15:13,519
the classifier

478
00:15:13,519 --> 00:15:15,199
that these are all supposed to have the

479
00:15:15,199 --> 00:15:17,199
same meaning but we've actually

480
00:15:17,199 --> 00:15:18,800
just represented three different parts

481
00:15:18,800 --> 00:15:20,240
of the of a packet

482
00:15:20,240 --> 00:15:21,600
uh that have completely different

483
00:15:21,600 --> 00:15:23,440
meaning underlying it

484
00:15:23,440 --> 00:15:25,600
and this cascades down the packet for a

485
00:15:25,600 --> 00:15:27,040
standard representation

486
00:15:27,040 --> 00:15:29,680
so how do we avoid this we have come up

487
00:15:29,680 --> 00:15:32,079
with a

488
00:15:32,240 --> 00:15:34,959
a formal requirement to avoid this this

489
00:15:34,959 --> 00:15:35,759
sort of

490
00:15:35,759 --> 00:15:38,000
misalignment and that requirement is

491
00:15:38,000 --> 00:15:39,600
aligned

492
00:15:39,600 --> 00:15:41,519
any standard data representation for a

493
00:15:41,519 --> 00:15:42,639
packet

494
00:15:42,639 --> 00:15:44,639
when we do this needs to be aligned and

495
00:15:44,639 --> 00:15:46,320
this is the idea that

496
00:15:46,320 --> 00:15:48,079
every location in the representation

497
00:15:48,079 --> 00:15:49,759
needs to have the same meaning across

498
00:15:49,759 --> 00:15:51,920
all packets this is going to do two

499
00:15:51,920 --> 00:15:53,839
things this is going to re-encode the

500
00:15:53,839 --> 00:15:55,279
semantic structure

501
00:15:55,279 --> 00:15:57,600
that we know packets have that we've

502
00:15:57,600 --> 00:15:59,600
sort of

503
00:15:59,600 --> 00:16:01,920
uh know that those bits have this

504
00:16:01,920 --> 00:16:02,639
meaning

505
00:16:02,639 --> 00:16:04,639
and it's going to allow us after we

506
00:16:04,639 --> 00:16:05,759
train a model

507
00:16:05,759 --> 00:16:07,600
on this representation to actually

508
00:16:07,600 --> 00:16:09,519
interpret the model and figure out

509
00:16:09,519 --> 00:16:10,959
which parts of the packet actually

510
00:16:10,959 --> 00:16:12,720
differentiate different classes

511
00:16:12,720 --> 00:16:15,279
so for example this will allow us to

512
00:16:15,279 --> 00:16:16,480
take a trained model

513
00:16:16,480 --> 00:16:18,240
for like active device fingerprinting

514
00:16:18,240 --> 00:16:20,959
and the nmap case

515
00:16:20,959 --> 00:16:23,360
and it will allow us to determine okay

516
00:16:23,360 --> 00:16:24,480
the ttl actually

517
00:16:24,480 --> 00:16:26,000
is the important part that's driving the

518
00:16:26,000 --> 00:16:28,160
classifier uh classifiers performance

519
00:16:28,160 --> 00:16:29,519
and distinguishing these classes

520
00:16:29,519 --> 00:16:31,279
uh from each other and we'll see that

521
00:16:31,279 --> 00:16:32,639
later when we can actually interpret

522
00:16:32,639 --> 00:16:33,279
these models

523
00:16:33,279 --> 00:16:35,440
okay so this is where we've landed this

524
00:16:35,440 --> 00:16:36,639
is in print

525
00:16:36,639 --> 00:16:38,160
it meets all of our requirements so

526
00:16:38,160 --> 00:16:39,759
first it's complete and that we can

527
00:16:39,759 --> 00:16:41,680
represent every single bit of the packet

528
00:16:41,680 --> 00:16:42,720
to the classifier

529
00:16:42,720 --> 00:16:44,560
of any packet because we include room

530
00:16:44,560 --> 00:16:46,240
for every protocol

531
00:16:46,240 --> 00:16:47,680
that we're interested in for a given

532
00:16:47,680 --> 00:16:50,079
problem it's constant size in the sense

533
00:16:50,079 --> 00:16:51,120
that

534
00:16:51,120 --> 00:16:53,680
by padding to the maximum size uh of a

535
00:16:53,680 --> 00:16:54,160
given

536
00:16:54,160 --> 00:16:57,839
header uh we are able to guarantee that

537
00:16:57,839 --> 00:16:59,360
the classifier

538
00:16:59,360 --> 00:17:01,279
gets constant size input it's inherently

539
00:17:01,279 --> 00:17:02,480
normalized

540
00:17:02,480 --> 00:17:04,640
uh and the rates inherently normalizes

541
00:17:04,640 --> 00:17:06,240
once again we're using bits

542
00:17:06,240 --> 00:17:08,959
um and we're gonna fill any feature that

543
00:17:08,959 --> 00:17:10,880
doesn't exist so for example

544
00:17:10,880 --> 00:17:13,039
in a tcp packet we're gonna fill the udp

545
00:17:13,039 --> 00:17:15,119
bits with negative one

546
00:17:15,119 --> 00:17:16,720
to denote to the classifier that those

547
00:17:16,720 --> 00:17:18,400
bits did not exist in the uh

548
00:17:18,400 --> 00:17:21,919
the packet and now it's aligned

549
00:17:21,919 --> 00:17:24,079
right so by including room for the

550
00:17:24,079 --> 00:17:25,919
maximum size of each header for each

551
00:17:25,919 --> 00:17:26,720
protocol

552
00:17:26,720 --> 00:17:28,720
so we see the ipv4 header is padded to

553
00:17:28,720 --> 00:17:30,240
the maximum size of

554
00:17:30,240 --> 00:17:32,559
60 bytes and so is the tcp header now

555
00:17:32,559 --> 00:17:33,919
all of a sudden when we have two packets

556
00:17:33,919 --> 00:17:35,440
that are actually completely different

557
00:17:35,440 --> 00:17:37,200
packets right we have a tcp packet the

558
00:17:37,200 --> 00:17:38,480
udp packet

559
00:17:38,480 --> 00:17:40,080
we actually don't have this misalignment

560
00:17:40,080 --> 00:17:43,200
because we're encoding to the classifier

561
00:17:43,200 --> 00:17:45,280
in the representation itself that these

562
00:17:45,280 --> 00:17:47,679
are different packets

563
00:17:47,679 --> 00:17:49,840
and we don't have this noise injected

564
00:17:49,840 --> 00:17:50,880
that was done with

565
00:17:50,880 --> 00:17:54,000
the naive version of endprint so we

566
00:17:54,000 --> 00:17:56,080
built this standard data representation

567
00:17:56,080 --> 00:17:59,600
that can encode any packet and a

568
00:17:59,600 --> 00:18:01,919
relatively efficient way and in a

569
00:18:01,919 --> 00:18:03,600
standard way how do we actually use

570
00:18:03,600 --> 00:18:04,640
something like this

571
00:18:04,640 --> 00:18:07,120
so we've removed this hand engineering

572
00:18:07,120 --> 00:18:07,760
features

573
00:18:07,760 --> 00:18:11,039
uh step uh we see

574
00:18:11,039 --> 00:18:12,880
uh so now we see that the bulk of the

575
00:18:12,880 --> 00:18:14,799
time is spent training models

576
00:18:14,799 --> 00:18:18,160
right so can we remove this step so the

577
00:18:18,160 --> 00:18:19,200
classic

578
00:18:19,200 --> 00:18:20,880
methodology for training a model

579
00:18:20,880 --> 00:18:22,720
including what we've done in the past

580
00:18:22,720 --> 00:18:24,799
on different works is we'll do the

581
00:18:24,799 --> 00:18:27,039
following we'll pick our favorite model

582
00:18:27,039 --> 00:18:28,640
we'll write code to search some hyper

583
00:18:28,640 --> 00:18:31,280
parameters for that model so say

584
00:18:31,280 --> 00:18:32,799
we want to use a deep neural network the

585
00:18:32,799 --> 00:18:35,120
hyper parameters may be the size of each

586
00:18:35,120 --> 00:18:35,600
layer

587
00:18:35,600 --> 00:18:37,120
and then we'll choose the best one in

588
00:18:37,120 --> 00:18:39,039
our search phase right

589
00:18:39,039 --> 00:18:40,960
and what this is going to do is it gives

590
00:18:40,960 --> 00:18:43,520
us a relatively small search space

591
00:18:43,520 --> 00:18:46,720
with our bias of our favorite models

592
00:18:46,720 --> 00:18:49,200
and hopefully we find a model that's it

593
00:18:49,200 --> 00:18:50,240
does a good job of predictive

594
00:18:50,240 --> 00:18:51,760
performance

595
00:18:51,760 --> 00:18:55,039
but what is more principled is this new

596
00:18:55,039 --> 00:18:56,720
new research area of automated machine

597
00:18:56,720 --> 00:18:58,400
learning or automl

598
00:18:58,400 --> 00:19:00,880
and what automl is is it's going to

599
00:19:00,880 --> 00:19:03,360
automate and more efficiently do

600
00:19:03,360 --> 00:19:05,039
everything we've done before over a

601
00:19:05,039 --> 00:19:07,360
wider range

602
00:19:07,360 --> 00:19:09,039
quieter search space and the way it does

603
00:19:09,039 --> 00:19:11,039
it at a high level is it's going to set

604
00:19:11,039 --> 00:19:11,679
up

605
00:19:11,679 --> 00:19:14,559
the machine learning uh so it's going to

606
00:19:14,559 --> 00:19:16,000
set up the model selection so it's going

607
00:19:16,000 --> 00:19:17,280
to search for different models and

608
00:19:17,280 --> 00:19:18,480
different hyper parameters

609
00:19:18,480 --> 00:19:20,960
as an actual optimization problem rather

610
00:19:20,960 --> 00:19:22,320
than a simple search space

611
00:19:22,320 --> 00:19:25,520
and this is going to guarantee um better

612
00:19:25,520 --> 00:19:26,640
performing models

613
00:19:26,640 --> 00:19:30,799
uh than picking and injecting human bias

614
00:19:30,799 --> 00:19:34,160
into our model search so specifically

615
00:19:34,160 --> 00:19:35,600
we're going to use auto glue on auto

616
00:19:35,600 --> 00:19:36,320
amount

617
00:19:36,320 --> 00:19:39,679
so what auto glue on is is

618
00:19:39,679 --> 00:19:42,000
um it allows us to basically take a csv

619
00:19:42,000 --> 00:19:43,280
file and

620
00:19:43,280 --> 00:19:47,200
run uh optimize machine learning tests

621
00:19:47,200 --> 00:19:50,400
uh without writing in any code and auto

622
00:19:50,400 --> 00:19:52,400
auto glue on is built by amazon and it's

623
00:19:52,400 --> 00:19:54,080
been shown to achieve higher performance

624
00:19:54,080 --> 00:19:55,280
than other models

625
00:19:55,280 --> 00:19:57,120
due to ensembling models or basically

626
00:19:57,120 --> 00:19:59,200
just combining different models

627
00:19:59,200 --> 00:20:02,480
uh to mask their other ones weaknesses

628
00:20:02,480 --> 00:20:03,760
and this is going to allow us to train

629
00:20:03,760 --> 00:20:05,679
and test over 50 models from six

630
00:20:05,679 --> 00:20:06,000
different

631
00:20:06,000 --> 00:20:08,159
base model classes for every single

632
00:20:08,159 --> 00:20:09,120
problem

633
00:20:09,120 --> 00:20:12,720
uh we test without actually writing any

634
00:20:12,720 --> 00:20:14,000
code

635
00:20:14,000 --> 00:20:15,360
and so this is what our ultimate

636
00:20:15,360 --> 00:20:17,120
pipeline looks like so

637
00:20:17,120 --> 00:20:18,640
typically you would write you would do

638
00:20:18,640 --> 00:20:20,159
this all individually for every single

639
00:20:20,159 --> 00:20:20,880
problem

640
00:20:20,880 --> 00:20:22,640
but now right we take input as packets

641
00:20:22,640 --> 00:20:24,159
we run nprint on those packets

642
00:20:24,159 --> 00:20:26,480
then we run automl as opposed to all

643
00:20:26,480 --> 00:20:27,679
this time spent

644
00:20:27,679 --> 00:20:29,039
uh figuring out which features are

645
00:20:29,039 --> 00:20:30,720
important and figuring out which models

646
00:20:30,720 --> 00:20:31,520
are important

647
00:20:31,520 --> 00:20:34,400
this entire process becomes automated so

648
00:20:34,400 --> 00:20:35,440
we're gonna go through two quick

649
00:20:35,440 --> 00:20:36,480
examples

650
00:20:36,480 --> 00:20:39,520
of uh applying imprint and how it can

651
00:20:39,520 --> 00:20:40,320
actually

652
00:20:40,320 --> 00:20:44,880
outperform uh state-of-the-art tools

653
00:20:44,880 --> 00:20:48,240
and in its areas so first uh

654
00:20:48,240 --> 00:20:49,679
we need labels right so if we want to

655
00:20:49,679 --> 00:20:51,200
perform a classification task we need

656
00:20:51,200 --> 00:20:52,480
some sort of labels

657
00:20:52,480 --> 00:20:54,640
to actually classify first thing we're

658
00:20:54,640 --> 00:20:56,559
going to look at is classifying a set of

659
00:20:56,559 --> 00:20:59,760
network devices and iot devices

660
00:20:59,760 --> 00:21:01,440
these are gathered through a variety of

661
00:21:01,440 --> 00:21:03,039
means basically

662
00:21:03,039 --> 00:21:05,360
it's a lot of routers and then some iot

663
00:21:05,360 --> 00:21:08,400
cameras and iot streaming devices

664
00:21:08,400 --> 00:21:11,039
so we have labels now we need packets to

665
00:21:11,039 --> 00:21:12,400
actually transform into and print and

666
00:21:12,400 --> 00:21:14,320
classify

667
00:21:14,320 --> 00:21:16,720
so we're going to leverage nmap which is

668
00:21:16,720 --> 00:21:18,640
the most well-known and used

669
00:21:18,640 --> 00:21:20,000
remote device fingerprinting tool

670
00:21:20,000 --> 00:21:21,760
probably um

671
00:21:21,760 --> 00:21:23,200
it's really i think it's over 20 years

672
00:21:23,200 --> 00:21:25,679
of this point of you know hand curated

673
00:21:25,679 --> 00:21:27,440
features and fingerprints

674
00:21:27,440 --> 00:21:30,240
and this heuristic to actually

675
00:21:30,240 --> 00:21:32,240
fingerprint remote devices from traffic

676
00:21:32,240 --> 00:21:33,760
sent from your device

677
00:21:33,760 --> 00:21:35,440
so how does a map work at a high level

678
00:21:35,440 --> 00:21:36,799
it's going to send a bunch of probes

679
00:21:36,799 --> 00:21:38,720
16 in total it's going to take the

680
00:21:38,720 --> 00:21:40,400
responses to those probes

681
00:21:40,400 --> 00:21:42,159
and transform them into a fingerprint

682
00:21:42,159 --> 00:21:43,679
using a bunch of tests

683
00:21:43,679 --> 00:21:44,880
and then it's going to compare that

684
00:21:44,880 --> 00:21:46,880
fingerprint

685
00:21:46,880 --> 00:21:48,799
to a database and if it finds the

686
00:21:48,799 --> 00:21:50,000
fingerprint

687
00:21:50,000 --> 00:21:51,360
or something close to it it's going to

688
00:21:51,360 --> 00:21:53,440
say okay this device is probably

689
00:21:53,440 --> 00:21:56,799
you know a cisco router for example

690
00:21:56,799 --> 00:21:57,919
so what we're going to do is we're

691
00:21:57,919 --> 00:21:58,799
actually just going to take the

692
00:21:58,799 --> 00:22:00,720
responses to nmaps probes

693
00:22:00,720 --> 00:22:02,480
they've spent a long time developing and

694
00:22:02,480 --> 00:22:04,159
we're going to compare

695
00:22:04,159 --> 00:22:07,360
if imprint models train on imprint can

696
00:22:07,360 --> 00:22:08,320
actually just

697
00:22:08,320 --> 00:22:10,080
automatically do the second part of

698
00:22:10,080 --> 00:22:11,520
transforming the responses into

699
00:22:11,520 --> 00:22:12,559
something useful

700
00:22:12,559 --> 00:22:14,159
uh without actually doing all the work

701
00:22:14,159 --> 00:22:15,600
for the past you know

702
00:22:15,600 --> 00:22:18,960
uh decade or more so now we need to

703
00:22:18,960 --> 00:22:20,320
actually transform

704
00:22:20,320 --> 00:22:23,039
the packets or the responses to the nmap

705
00:22:23,039 --> 00:22:23,600
probes

706
00:22:23,600 --> 00:22:26,559
into something we can use and we're

707
00:22:26,559 --> 00:22:27,520
going to run in print and then we're

708
00:22:27,520 --> 00:22:29,679
going to compare it to end up

709
00:22:29,679 --> 00:22:31,520
so how does this look like for imprint

710
00:22:31,520 --> 00:22:33,520
so nmap sends a bunch of probes and it

711
00:22:33,520 --> 00:22:35,919
gets 21 responses in our data set

712
00:22:35,919 --> 00:22:38,640
for each device so we can now create our

713
00:22:38,640 --> 00:22:40,000
own fingerprint picture

714
00:22:40,000 --> 00:22:41,440
like i talked about at the beginning

715
00:22:41,440 --> 00:22:43,280
where we have a set of pixels

716
00:22:43,280 --> 00:22:45,600
now we've sort of created our own device

717
00:22:45,600 --> 00:22:48,400
fingerprint that is a set of

718
00:22:48,400 --> 00:22:50,240
carefully aligned and carefully set up

719
00:22:50,240 --> 00:22:51,919
pixels that we can represent to a

720
00:22:51,919 --> 00:22:53,440
machine learning classifier

721
00:22:53,440 --> 00:22:54,720
and so ultimately this is going to look

722
00:22:54,720 --> 00:22:57,039
like a flattened version of the 2d

723
00:22:57,039 --> 00:22:59,120
sort of image to the right so

724
00:22:59,120 --> 00:23:00,559
essentially what we have is for each

725
00:23:00,559 --> 00:23:03,120
device we now have a fingerprint image

726
00:23:03,120 --> 00:23:05,280
in terms of its network traffic what

727
00:23:05,280 --> 00:23:06,159
does nmap do

728
00:23:06,159 --> 00:23:08,559
it transforms the packets into a variety

729
00:23:08,559 --> 00:23:09,679
of features

730
00:23:09,679 --> 00:23:11,919
some complex some simple so some just

731
00:23:11,919 --> 00:23:13,360
looking at the length of the packet

732
00:23:13,360 --> 00:23:14,480
and then we also have things like the

733
00:23:14,480 --> 00:23:16,080
predictability index that are actually

734
00:23:16,080 --> 00:23:18,320
relatively complex

735
00:23:18,320 --> 00:23:21,360
so we're going to take the imprints

736
00:23:21,360 --> 00:23:23,360
of the same responses that nmap

737
00:23:23,360 --> 00:23:24,640
transforms and

738
00:23:24,640 --> 00:23:27,039
nmap's uh test fingerprint that it

739
00:23:27,039 --> 00:23:27,760
generates

740
00:23:27,760 --> 00:23:29,440
and we're going to train a bunch of

741
00:23:29,440 --> 00:23:31,520
models using automated machine learning

742
00:23:31,520 --> 00:23:34,640
on nmaps features and imprints so we're

743
00:23:34,640 --> 00:23:36,480
going to see if in print can actually

744
00:23:36,480 --> 00:23:38,640
uncover the important features without

745
00:23:38,640 --> 00:23:40,960
actually extracting them by hand

746
00:23:40,960 --> 00:23:43,760
that took so long to develop so on the

747
00:23:43,760 --> 00:23:45,520
left we see a precision recall curve

748
00:23:45,520 --> 00:23:49,440
in general up and to the right is better

749
00:23:49,440 --> 00:23:50,880
so we see the imprint actually

750
00:23:50,880 --> 00:23:53,200
outperforms nmap in terms of precision

751
00:23:53,200 --> 00:23:55,520
and recall for every class

752
00:23:55,520 --> 00:23:57,360
on the right we have a small table these

753
00:23:57,360 --> 00:23:59,760
are standard metrics including accuracy

754
00:23:59,760 --> 00:24:02,960
and f1 score and we see almost a three

755
00:24:02,960 --> 00:24:04,080
percent increase

756
00:24:04,080 --> 00:24:05,760
so what does this mean uh at a high

757
00:24:05,760 --> 00:24:07,440
level what we're seeing is that

758
00:24:07,440 --> 00:24:09,440
uh models can actually automatically

759
00:24:09,440 --> 00:24:11,039
extract important

760
00:24:11,039 --> 00:24:13,360
features from packets without actual

761
00:24:13,360 --> 00:24:15,039
human guidance

762
00:24:15,039 --> 00:24:18,159
and outperform features that have been

763
00:24:18,159 --> 00:24:19,840
developed by human guidance for a long

764
00:24:19,840 --> 00:24:21,440
time

765
00:24:21,440 --> 00:24:23,520
uh earlier i talked about interpretable

766
00:24:23,520 --> 00:24:25,039
uh features

767
00:24:25,039 --> 00:24:27,200
uh because of imprint uh so imprint is

768
00:24:27,200 --> 00:24:28,799
unique in that it's interpretable at the

769
00:24:28,799 --> 00:24:29,440
bit level

770
00:24:29,440 --> 00:24:32,400
right so we can now probe our model and

771
00:24:32,400 --> 00:24:33,679
determine

772
00:24:33,679 --> 00:24:36,400
uh by its feature importance what it

773
00:24:36,400 --> 00:24:37,200
learned

774
00:24:37,200 --> 00:24:40,000
to distinguish the class the classes of

775
00:24:40,000 --> 00:24:40,720
routers

776
00:24:40,720 --> 00:24:43,520
or network devices and iot devices so we

777
00:24:43,520 --> 00:24:44,159
actually see

778
00:24:44,159 --> 00:24:46,640
that it actually automatically learns

779
00:24:46,640 --> 00:24:48,240
features that are already encoded into

780
00:24:48,240 --> 00:24:49,760
nmap for example

781
00:24:49,760 --> 00:24:51,760
it learns that the iptl is incredibly

782
00:24:51,760 --> 00:24:53,440
important it learns that the window size

783
00:24:53,440 --> 00:24:54,640
is incredibly important

784
00:24:54,640 --> 00:24:57,279
it finds value in the the icmp header

785
00:24:57,279 --> 00:24:59,120
that

786
00:24:59,120 --> 00:25:01,919
gets responded to by uh the nmap pro uh

787
00:25:01,919 --> 00:25:03,279
and we're actually able to automatically

788
00:25:03,279 --> 00:25:04,799
do this and and find out

789
00:25:04,799 --> 00:25:06,720
after the fact rather than having to

790
00:25:06,720 --> 00:25:08,080
extract it ourselves

791
00:25:08,080 --> 00:25:09,200
uh so last we're gonna look at

792
00:25:09,200 --> 00:25:11,360
application identification this is very

793
00:25:11,360 --> 00:25:13,120
brief uh just to show the breadth of

794
00:25:13,120 --> 00:25:14,559
imprint

795
00:25:14,559 --> 00:25:17,039
so uh the data set for application

796
00:25:17,039 --> 00:25:18,880
identification uh there was a collection

797
00:25:18,880 --> 00:25:21,120
of seven thousand dtls handshakes

798
00:25:21,120 --> 00:25:23,200
this was an effort to identify snowflake

799
00:25:23,200 --> 00:25:25,039
which is a new plugable transport and

800
00:25:25,039 --> 00:25:25,919
tour

801
00:25:25,919 --> 00:25:29,440
um so uh we're uh

802
00:25:29,440 --> 00:25:31,679
the author of this was able to able to

803
00:25:31,679 --> 00:25:33,840
extract manually a bunch of features

804
00:25:33,840 --> 00:25:36,559
that lead to 99 accuracy to determine

805
00:25:36,559 --> 00:25:37,679
the application

806
00:25:37,679 --> 00:25:39,520
uh that was in the handshake so what

807
00:25:39,520 --> 00:25:40,640
we're going to do is we're going to say

808
00:25:40,640 --> 00:25:43,600
okay rather than

809
00:25:43,600 --> 00:25:45,360
manually extracting features let's see

810
00:25:45,360 --> 00:25:47,200
if we can actually just run in print on

811
00:25:47,200 --> 00:25:48,240
the handshakes

812
00:25:48,240 --> 00:25:51,440
train models on the raw imprints that we

813
00:25:51,440 --> 00:25:52,320
generated

814
00:25:52,320 --> 00:25:53,919
and see if we can match that performance

815
00:25:53,919 --> 00:25:55,440
without any effort at all

816
00:25:55,440 --> 00:25:57,200
so what we find on the left is that with

817
00:25:57,200 --> 00:25:58,960
99.9

818
00:25:58,960 --> 00:26:02,080
percent f1 score and 99.9

819
00:26:02,080 --> 00:26:04,400
accuracy overall imprint is able to

820
00:26:04,400 --> 00:26:05,200
adapt to this

821
00:26:05,200 --> 00:26:07,919
uh environment and extract different

822
00:26:07,919 --> 00:26:09,760
features for a new problem without any

823
00:26:09,760 --> 00:26:10,720
effort

824
00:26:10,720 --> 00:26:13,600
this actually took zero lines of code to

825
00:26:13,600 --> 00:26:14,799
train a new model

826
00:26:14,799 --> 00:26:16,640
and learn new features on the right we

827
00:26:16,640 --> 00:26:18,320
see that it learns different things

828
00:26:18,320 --> 00:26:20,000
like the length of each handshake is

829
00:26:20,000 --> 00:26:22,080
actually important in terms of the total

830
00:26:22,080 --> 00:26:23,200
length of each packet

831
00:26:23,200 --> 00:26:25,679
and in the dtls header itself has a lot

832
00:26:25,679 --> 00:26:28,080
of importance

833
00:26:28,080 --> 00:26:30,080
i think that's it for me thank you guys

834
00:26:30,080 --> 00:26:31,200
so much for your

835
00:26:31,200 --> 00:26:32,720
attention and having me and now i'll

836
00:26:32,720 --> 00:26:39,679
take any questions

837
00:26:39,679 --> 00:26:42,720
jordan thank you very much that was

838
00:26:42,720 --> 00:26:45,600
amazingly informative we just have a

839
00:26:45,600 --> 00:26:46,960
couple of minutes for q

840
00:26:46,960 --> 00:26:50,159
a um i've been reading the ones you've

841
00:26:50,159 --> 00:26:51,120
already answered

842
00:26:51,120 --> 00:26:54,400
in the q a tab i'd like to pull out

843
00:26:54,400 --> 00:26:57,520
two questions in specific for you first

844
00:26:57,520 --> 00:26:58,559
question

845
00:26:58,559 --> 00:27:00,400
is there any issue or additional

846
00:27:00,400 --> 00:27:02,880
overhead introduced when handling jumbo

847
00:27:02,880 --> 00:27:05,440
frames versus standard frames

848
00:27:05,440 --> 00:27:07,919
yeah ultimately uh with our approach the

849
00:27:07,919 --> 00:27:08,720
larger

850
00:27:08,720 --> 00:27:13,120
uh the packet is including the payload

851
00:27:13,120 --> 00:27:14,799
the more overhead number features you're

852
00:27:14,799 --> 00:27:17,279
going to have so ultimately yes

853
00:27:17,279 --> 00:27:20,320
the bigger the packet the bigger

854
00:27:20,320 --> 00:27:22,960
the overhead for the classifier we're

855
00:27:22,960 --> 00:27:24,720
working on ways to reduce this

856
00:27:24,720 --> 00:27:27,360
uh in terms of compression of uh the

857
00:27:27,360 --> 00:27:28,559
traffic itself

858
00:27:28,559 --> 00:27:30,080
and training on compressed versions of

859
00:27:30,080 --> 00:27:32,399
it cool

860
00:27:32,399 --> 00:27:35,440
uh the other question which you haven't

861
00:27:35,440 --> 00:27:36,000
answered

862
00:27:36,000 --> 00:27:39,120
in the chat and everybody should go look

863
00:27:39,120 --> 00:27:40,559
at the chat

864
00:27:40,559 --> 00:27:42,640
why wouldn't you have a field that says

865
00:27:42,640 --> 00:27:44,000
tcp udp

866
00:27:44,000 --> 00:27:46,799
icmp and then put everything to a single

867
00:27:46,799 --> 00:27:48,000
480 feature field

868
00:27:48,000 --> 00:27:50,320
called header padding the smaller to its

869
00:27:50,320 --> 00:27:52,480
zeros out to 480.

870
00:27:52,480 --> 00:27:54,240
yeah this is a really good question

871
00:27:54,240 --> 00:27:56,559
ultimately with the classifier

872
00:27:56,559 --> 00:27:58,000
we want to encode one piece of

873
00:27:58,000 --> 00:28:00,799
information in every feature

874
00:28:00,799 --> 00:28:02,399
we present to the classifier to learn

875
00:28:02,399 --> 00:28:05,039
from and we'd be putting extra strain on

876
00:28:05,039 --> 00:28:07,840
the classifier itself

877
00:28:07,840 --> 00:28:10,559
a by having to figure out that these are

878
00:28:10,559 --> 00:28:12,320
actually different packets

879
00:28:12,320 --> 00:28:16,640
and b we're actually by doing that we'd

880
00:28:16,640 --> 00:28:18,720
be encoding both like the tcp

881
00:28:18,720 --> 00:28:21,600
uh window size and the udp checksum for

882
00:28:21,600 --> 00:28:22,320
example

883
00:28:22,320 --> 00:28:24,240
in the same feature um which would

884
00:28:24,240 --> 00:28:26,559
ultimately uh end the confusion uh end

885
00:28:26,559 --> 00:28:28,159
up with confusion in the classifier at

886
00:28:28,159 --> 00:28:29,600
lower performance that's a really good

887
00:28:29,600 --> 00:28:30,799
question

888
00:28:30,799 --> 00:28:34,480
cool thank you uh those were the last of

889
00:28:34,480 --> 00:28:35,360
the questions

890
00:28:35,360 --> 00:28:39,360
um i learned a whole lot about that

891
00:28:39,360 --> 00:28:41,520
and i think i'll be diving into it or

892
00:28:41,520 --> 00:28:43,200
actually having one of my guys who

893
00:28:43,200 --> 00:28:44,480
actually understands coding at that

894
00:28:44,480 --> 00:28:47,440
level to dive into it

895
00:28:47,440 --> 00:28:56,240
thank you very much

