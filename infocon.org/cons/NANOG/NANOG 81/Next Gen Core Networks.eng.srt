1
00:00:01,610 --> 00:00:05,200
[Music]

2
00:00:05,200 --> 00:00:06,080
hey everyone

3
00:00:06,080 --> 00:00:09,280
my name is mannan venkatesan um i work

4
00:00:09,280 --> 00:00:10,880
in a network architecture in

5
00:00:10,880 --> 00:00:14,000
comcast primarily focusing on the

6
00:00:14,000 --> 00:00:17,920
backbone and the original ip networks

7
00:00:17,920 --> 00:00:20,320
um i want to i want to thank the

8
00:00:20,320 --> 00:00:22,240
committee to

9
00:00:22,240 --> 00:00:25,039
give me this opportunity to share our

10
00:00:25,039 --> 00:00:27,039
backbone evolution we had over the last

11
00:00:27,039 --> 00:00:28,880
few years

12
00:00:28,880 --> 00:00:31,199
we wanted to go over some of the

13
00:00:31,199 --> 00:00:32,719
challenges we faced

14
00:00:32,719 --> 00:00:34,800
um how we address them and some of the

15
00:00:34,800 --> 00:00:36,239
learnings we had

16
00:00:36,239 --> 00:00:39,360
um in in our backbone um

17
00:00:39,360 --> 00:00:41,280
deployments so thank you for the

18
00:00:41,280 --> 00:00:42,399
opportunity

19
00:00:42,399 --> 00:00:45,280
um so before i start going into the

20
00:00:45,280 --> 00:00:46,320
details

21
00:00:46,320 --> 00:00:48,800
i wanted to share this slide most of you

22
00:00:48,800 --> 00:00:49,680
would know this

23
00:00:49,680 --> 00:00:51,920
already i just wanted to give you an

24
00:00:51,920 --> 00:00:53,039
overview of

25
00:00:53,039 --> 00:00:55,199
backbone core networks their

26
00:00:55,199 --> 00:00:58,559
capabilities and their functions um

27
00:00:58,559 --> 00:01:01,600
typically the backbone networks are the

28
00:01:01,600 --> 00:01:04,640
big mega routers supporting hundreds of

29
00:01:04,640 --> 00:01:06,240
thousands of ports

30
00:01:06,240 --> 00:01:08,479
carrying multi-terabits of traffic

31
00:01:08,479 --> 00:01:09,840
critical traffic

32
00:01:09,840 --> 00:01:12,640
um interconnecting all the routers and

33
00:01:12,640 --> 00:01:14,159
sites

34
00:01:14,159 --> 00:01:16,960
regional networks external peers

35
00:01:16,960 --> 00:01:18,640
external

36
00:01:18,640 --> 00:01:21,600
providers um provide them connectivity

37
00:01:21,600 --> 00:01:22,560
between

38
00:01:22,560 --> 00:01:25,840
between those networks it connects to

39
00:01:25,840 --> 00:01:28,159
the the routers we use in backbones

40
00:01:28,159 --> 00:01:30,560
typically they are scale out or scale

41
00:01:30,560 --> 00:01:35,200
up hardware scale out meaning

42
00:01:35,200 --> 00:01:37,920
we expand the router across multiple

43
00:01:37,920 --> 00:01:39,280
racks

44
00:01:39,280 --> 00:01:41,680
multi-chassis is an example a few

45
00:01:41,680 --> 00:01:43,200
vendors came out with a multi-chassis

46
00:01:43,200 --> 00:01:43,680
design

47
00:01:43,680 --> 00:01:45,280
multi-chassis one of the example for

48
00:01:45,280 --> 00:01:46,720
scale out option

49
00:01:46,720 --> 00:01:49,040
and scale up means we would upgrade the

50
00:01:49,040 --> 00:01:49,840
hardware

51
00:01:49,840 --> 00:01:52,960
um at regular intervals a few

52
00:01:52,960 --> 00:01:54,880
once in a few years or we would replace

53
00:01:54,880 --> 00:01:56,240
the whole box with a new

54
00:01:56,240 --> 00:01:59,840
denser box that's a scale of options

55
00:01:59,840 --> 00:02:02,640
um these these are all critical uh

56
00:02:02,640 --> 00:02:03,439
backbone

57
00:02:03,439 --> 00:02:06,159
uh core routers supporting millions of

58
00:02:06,159 --> 00:02:06,960
customers

59
00:02:06,960 --> 00:02:10,318
um so we

60
00:02:10,318 --> 00:02:13,680
they play this they support all the

61
00:02:13,680 --> 00:02:15,520
traffic flows all the services

62
00:02:15,520 --> 00:02:18,560
that we provide to our customers um

63
00:02:18,560 --> 00:02:20,959
though i am showing you a single router

64
00:02:20,959 --> 00:02:22,239
design in this slide

65
00:02:22,239 --> 00:02:26,319
i have seen other designs where

66
00:02:26,319 --> 00:02:28,640
folks would use two routers in a

67
00:02:28,640 --> 00:02:29,920
backbone location

68
00:02:29,920 --> 00:02:32,080
connect them back to back and provide

69
00:02:32,080 --> 00:02:33,280
the port denser

70
00:02:33,280 --> 00:02:35,840
density they would need the support and

71
00:02:35,840 --> 00:02:37,120
i also have seen

72
00:02:37,120 --> 00:02:39,440
three or more routers connected in a

73
00:02:39,440 --> 00:02:42,640
ring fashion or a star topology fashions

74
00:02:42,640 --> 00:02:45,760
fashioned to support the density they

75
00:02:45,760 --> 00:02:46,640
would need

76
00:02:46,640 --> 00:02:50,160
to support their growth um so moving on

77
00:02:50,160 --> 00:02:51,760
to the next slide um

78
00:02:51,760 --> 00:02:54,720
so um i wanted to start with some of the

79
00:02:54,720 --> 00:02:56,480
challenges we were facing

80
00:02:56,480 --> 00:02:58,239
with the with the core uh router

81
00:02:58,239 --> 00:03:00,319
platforms um

82
00:03:00,319 --> 00:03:02,000
i have listed a few of these challenges

83
00:03:02,000 --> 00:03:03,680
here in the slide but i would go through

84
00:03:03,680 --> 00:03:05,840
them in detail in the following slides

85
00:03:05,840 --> 00:03:08,560
and they're starting from the port

86
00:03:08,560 --> 00:03:10,640
capacity we need to

87
00:03:10,640 --> 00:03:14,720
deploy um to support the traffic growth

88
00:03:14,720 --> 00:03:17,760
um some of the bandwidth and features of

89
00:03:17,760 --> 00:03:21,599
asic chipsets we have to deploy um cost

90
00:03:21,599 --> 00:03:23,760
of the software and the hardware and

91
00:03:23,760 --> 00:03:26,560
blast radius that's a failure zone

92
00:03:26,560 --> 00:03:28,879
and maintenance window followed by the

93
00:03:28,879 --> 00:03:30,959
environmental constraints

94
00:03:30,959 --> 00:03:33,920
we have just to deploy them uh in in

95
00:03:33,920 --> 00:03:34,239
both

96
00:03:34,239 --> 00:03:36,080
comcast footprint or our internal

97
00:03:36,080 --> 00:03:37,760
footprint as well as our co-location

98
00:03:37,760 --> 00:03:40,239
facilities

99
00:03:40,239 --> 00:03:42,640
so um in the first challenge i would

100
00:03:42,640 --> 00:03:45,040
like to start here is the is the port

101
00:03:45,040 --> 00:03:47,599
growth we need to support uh we all know

102
00:03:47,599 --> 00:03:49,280
you know the traffic internet traffic

103
00:03:49,280 --> 00:03:50,080
exploded

104
00:03:50,080 --> 00:03:53,599
uh last last decade um the red

105
00:03:53,599 --> 00:03:56,239
graph the red line shows the internet

106
00:03:56,239 --> 00:03:56,879
traffic

107
00:03:56,879 --> 00:03:58,480
volume we have to support in our

108
00:03:58,480 --> 00:04:00,879
backbone in your background shoot

109
00:04:00,879 --> 00:04:04,000
um it's it's it's exploded in last

110
00:04:04,000 --> 00:04:07,280
uh ten years or so so

111
00:04:07,280 --> 00:04:10,560
the the sports you would need to support

112
00:04:10,560 --> 00:04:11,519
the traffic

113
00:04:11,519 --> 00:04:14,000
uh is also proportioned to the traffic

114
00:04:14,000 --> 00:04:14,720
volume

115
00:04:14,720 --> 00:04:16,880
so we have to make sure we stay on top

116
00:04:16,880 --> 00:04:17,759
of the

117
00:04:17,759 --> 00:04:20,560
port capacity we support on this core

118
00:04:20,560 --> 00:04:22,400
routers

119
00:04:22,400 --> 00:04:26,400
that's the the blue line you see

120
00:04:26,400 --> 00:04:28,400
if i go back to my start of my career

121
00:04:28,400 --> 00:04:29,759
with comcast we were

122
00:04:29,759 --> 00:04:32,880
you know we were using one gig uh

123
00:04:32,880 --> 00:04:35,680
ports those days then we moved on to the

124
00:04:35,680 --> 00:04:38,160
10 gig ports then we were doing multi 10

125
00:04:38,160 --> 00:04:39,600
gig

126
00:04:39,600 --> 00:04:42,560
links then we moved on to 100 gig and we

127
00:04:42,560 --> 00:04:44,320
survived with the multi-hundred gig

128
00:04:44,320 --> 00:04:45,040
ports

129
00:04:45,040 --> 00:04:47,840
for a few years now we are looking into

130
00:04:47,840 --> 00:04:49,199
uh 400 gig

131
00:04:49,199 --> 00:04:51,520
links so that's the step function you

132
00:04:51,520 --> 00:04:53,120
would see with the blue line it's based

133
00:04:53,120 --> 00:04:55,040
on driven by the port capacity output

134
00:04:55,040 --> 00:04:56,160
port

135
00:04:56,160 --> 00:04:59,040
speed uh and the platform capacity to

136
00:04:59,040 --> 00:05:01,199
support those ports

137
00:05:01,199 --> 00:05:04,160
so depending on which way which year to

138
00:05:04,160 --> 00:05:04,639
look at

139
00:05:04,639 --> 00:05:07,759
um if you if you look at the end of the

140
00:05:07,759 --> 00:05:10,560
end of your platform life cycle you're

141
00:05:10,560 --> 00:05:12,320
always running behind

142
00:05:12,320 --> 00:05:15,120
with with the demand so you always like

143
00:05:15,120 --> 00:05:16,479
at the beginning of the platform life

144
00:05:16,479 --> 00:05:18,000
cycle you know we have enough course to

145
00:05:18,000 --> 00:05:18,880
support to

146
00:05:18,880 --> 00:05:21,039
meet our demands but you have to stay on

147
00:05:21,039 --> 00:05:22,160
top but when you come

148
00:05:22,160 --> 00:05:24,320
closer to the end of the life cycle we

149
00:05:24,320 --> 00:05:25,759
got to make sure we

150
00:05:25,759 --> 00:05:28,320
we have enough ports on the box or we

151
00:05:28,320 --> 00:05:30,400
bring in a new platform or new hardware

152
00:05:30,400 --> 00:05:32,000
to support the demand

153
00:05:32,000 --> 00:05:33,919
so that's always a challenge that we

154
00:05:33,919 --> 00:05:38,160
have been facing

155
00:05:38,160 --> 00:05:42,400
and the next uh challenge is the failure

156
00:05:42,400 --> 00:05:45,360
zone or the blast areas we will say if

157
00:05:45,360 --> 00:05:46,000
we lose

158
00:05:46,000 --> 00:05:49,039
one router in a core location we impact

159
00:05:49,039 --> 00:05:51,600
millions of millions of customers

160
00:05:51,600 --> 00:05:54,880
if it is a hard failure then the packet

161
00:05:54,880 --> 00:05:56,080
on the wire is lost

162
00:05:56,080 --> 00:05:58,000
then during the convergence we may

163
00:05:58,000 --> 00:05:59,520
impact some flows

164
00:05:59,520 --> 00:06:01,440
but if there's a partial failure if the

165
00:06:01,440 --> 00:06:03,680
box is starting to misbehave and start

166
00:06:03,680 --> 00:06:05,199
blackholing traffic

167
00:06:05,199 --> 00:06:07,360
you will impact millions of millions of

168
00:06:07,360 --> 00:06:09,520
we will impact millions of customers

169
00:06:09,520 --> 00:06:11,759
with those failures so it creates a

170
00:06:11,759 --> 00:06:12,800
snowball effect

171
00:06:12,800 --> 00:06:15,440
impacting the regional networks core

172
00:06:15,440 --> 00:06:16,080
links

173
00:06:16,080 --> 00:06:18,960
as well as our inter-provider links our

174
00:06:18,960 --> 00:06:20,240
peers um

175
00:06:20,240 --> 00:06:22,400
all the traffic between this route

176
00:06:22,400 --> 00:06:25,440
between this network we connect you

177
00:06:25,440 --> 00:06:28,000
um so if we lose one of the router in

178
00:06:28,000 --> 00:06:29,919
our in our core location

179
00:06:29,919 --> 00:06:31,520
at the same time if you lose another

180
00:06:31,520 --> 00:06:33,440
failure if you have another failure in a

181
00:06:33,440 --> 00:06:34,479
connected network

182
00:06:34,479 --> 00:06:36,720
this could lead into a disaster

183
00:06:36,720 --> 00:06:38,080
situation

184
00:06:38,080 --> 00:06:40,880
for example if you lose site one uh all

185
00:06:40,880 --> 00:06:41,440
our

186
00:06:41,440 --> 00:06:44,639
all of our providers would would start

187
00:06:44,639 --> 00:06:45,120
sending

188
00:06:45,120 --> 00:06:47,919
traffic through site two three and four

189
00:06:47,919 --> 00:06:48,319
but

190
00:06:48,319 --> 00:06:50,400
during the same time if we had another

191
00:06:50,400 --> 00:06:52,400
failure in their network or in our

192
00:06:52,400 --> 00:06:53,440
network

193
00:06:53,440 --> 00:06:57,039
we would have a a much better impact

194
00:06:57,039 --> 00:07:00,000
to our customers so one of the goal here

195
00:07:00,000 --> 00:07:03,840
is to reduce this plus radius

196
00:07:05,039 --> 00:07:06,880
the next one is the economics and

197
00:07:06,880 --> 00:07:08,639
features

198
00:07:08,639 --> 00:07:11,759
we have seen over the years

199
00:07:11,759 --> 00:07:14,800
the more of the more bandwidth we

200
00:07:14,800 --> 00:07:17,520
support in a box

201
00:07:17,520 --> 00:07:20,479
the features uh supported is kind of

202
00:07:20,479 --> 00:07:21,360
taking an

203
00:07:21,360 --> 00:07:24,560
impact um the port cost

204
00:07:24,560 --> 00:07:27,759
um it's it's higher to support complex

205
00:07:27,759 --> 00:07:30,000
features compared to basic ip routing

206
00:07:30,000 --> 00:07:33,840
features and then it is really difficult

207
00:07:33,840 --> 00:07:34,160
to

208
00:07:34,160 --> 00:07:37,599
mix different port profiles different

209
00:07:37,599 --> 00:07:38,319
profiles

210
00:07:38,319 --> 00:07:40,000
uh different uh profiles of the

211
00:07:40,000 --> 00:07:41,599
different

212
00:07:41,599 --> 00:07:44,560
requirements in a single chassis um so

213
00:07:44,560 --> 00:07:45,039
as i

214
00:07:45,039 --> 00:07:48,000
show in this uh graph if you have three

215
00:07:48,000 --> 00:07:48,479
or four

216
00:07:48,479 --> 00:07:50,800
different profiles we end up supporting

217
00:07:50,800 --> 00:07:51,759
uh ports

218
00:07:51,759 --> 00:07:53,919
that would support all all pro all

219
00:07:53,919 --> 00:07:55,199
features in the box

220
00:07:55,199 --> 00:07:56,800
so we would end up probably paying more

221
00:07:56,800 --> 00:07:58,879
money for for the box then

222
00:07:58,879 --> 00:08:01,039
um then if you have to support the basic

223
00:08:01,039 --> 00:08:03,440
ip routing features

224
00:08:03,440 --> 00:08:05,520
though the backbone is a pure pe

225
00:08:05,520 --> 00:08:07,759
function but this applies to our

226
00:08:07,759 --> 00:08:10,160
edge uh role also so if you have a big

227
00:08:10,160 --> 00:08:12,800
box supporting your pu to pe rolls

228
00:08:12,800 --> 00:08:14,560
you will end up seeing the same problem

229
00:08:14,560 --> 00:08:16,319
there too

230
00:08:16,319 --> 00:08:18,400
the other thing we have seen the cost

231
00:08:18,400 --> 00:08:21,599
per port cost of a multi chassis

232
00:08:21,599 --> 00:08:23,919
router is a lot more expensive compared

233
00:08:23,919 --> 00:08:25,120
to the fixed us

234
00:08:25,120 --> 00:08:27,840
fixed our modular chassis routers so we

235
00:08:27,840 --> 00:08:29,520
got to account for this economics as

236
00:08:29,520 --> 00:08:30,639
well as the features

237
00:08:30,639 --> 00:08:33,839
um as we start deploying these big boxes

238
00:08:33,839 --> 00:08:36,560
in our network

239
00:08:37,200 --> 00:08:40,479
um 10th and operational challenges uh we

240
00:08:40,479 --> 00:08:41,839
we could come up with the cool

241
00:08:41,839 --> 00:08:43,839
technology or we could come up with this

242
00:08:43,839 --> 00:08:46,959
uh brilliant architecture but it it

243
00:08:46,959 --> 00:08:49,440
they are not good enough to deploy

244
00:08:49,440 --> 00:08:50,160
unless we

245
00:08:50,160 --> 00:08:53,120
we made them uh unless we we make them

246
00:08:53,120 --> 00:08:54,640
supportable in our network

247
00:08:54,640 --> 00:08:57,040
or any network so we got to make sure we

248
00:08:57,040 --> 00:08:58,800
put our operational hat on

249
00:08:58,800 --> 00:09:00,560
we have to make sure whatever we are

250
00:09:00,560 --> 00:09:02,080
rolling out in a network

251
00:09:02,080 --> 00:09:05,040
they are supported by our operations

252
00:09:05,040 --> 00:09:06,560
team

253
00:09:06,560 --> 00:09:09,200
over the years the number of uh hours we

254
00:09:09,200 --> 00:09:11,680
get for maintenance has been going down

255
00:09:11,680 --> 00:09:15,120
now we have hit we have started rolling

256
00:09:15,120 --> 00:09:16,080
out many critical

257
00:09:16,080 --> 00:09:19,920
services um the more and more services

258
00:09:19,920 --> 00:09:22,160
we roll out

259
00:09:22,160 --> 00:09:25,360
the number of hours we can get for

260
00:09:25,360 --> 00:09:28,880
support um get to do like a

261
00:09:28,880 --> 00:09:31,040
configuration changes quota bridges is

262
00:09:31,040 --> 00:09:33,599
going down

263
00:09:33,920 --> 00:09:35,760
we have to make sure we align with all

264
00:09:35,760 --> 00:09:37,760
our uh

265
00:09:37,760 --> 00:09:39,600
internal teams and external teams on

266
00:09:39,600 --> 00:09:41,920
notification before we start taking down

267
00:09:41,920 --> 00:09:43,440
a box for maintenance

268
00:09:43,440 --> 00:09:46,480
um so this this kind of puts us in a in

269
00:09:46,480 --> 00:09:48,720
a spot where it takes a long time to do

270
00:09:48,720 --> 00:09:50,560
a code upgrades or bau

271
00:09:50,560 --> 00:09:53,680
or business or social arguments we need

272
00:09:53,680 --> 00:09:55,839
to do no network to support the capacity

273
00:09:55,839 --> 00:09:58,160
growth

274
00:10:00,399 --> 00:10:01,680
and the last challenge is the

275
00:10:01,680 --> 00:10:03,760
environmental demands um

276
00:10:03,760 --> 00:10:06,399
so we do deploy these core routers in a

277
00:10:06,399 --> 00:10:07,440
call office

278
00:10:07,440 --> 00:10:09,839
collocation facilities that we don't own

279
00:10:09,839 --> 00:10:10,640
um

280
00:10:10,640 --> 00:10:12,399
so some of the constraints there is

281
00:10:12,399 --> 00:10:15,440
always space power and cooling

282
00:10:15,440 --> 00:10:18,480
though we have new big routers coming

283
00:10:18,480 --> 00:10:20,160
into the network into the network

284
00:10:20,160 --> 00:10:23,040
i know they're like 200 terabits of

285
00:10:23,040 --> 00:10:24,560
capacity routers

286
00:10:24,560 --> 00:10:28,560
um the power per gig is going down

287
00:10:28,560 --> 00:10:31,920
with the new a6 but because we have more

288
00:10:31,920 --> 00:10:32,399
ports

289
00:10:32,399 --> 00:10:35,279
on a single rack the power needed per

290
00:10:35,279 --> 00:10:37,440
rack is going up so some of the

291
00:10:37,440 --> 00:10:39,040
new routers we are looking into they

292
00:10:39,040 --> 00:10:43,120
need like 50 to 60 kilowatts per rack

293
00:10:43,120 --> 00:10:44,959
and the cooling is directly proportioned

294
00:10:44,959 --> 00:10:46,240
to the

295
00:10:46,240 --> 00:10:48,240
power we use and the port count we turn

296
00:10:48,240 --> 00:10:49,680
up on the router

297
00:10:49,680 --> 00:10:52,880
and so and also there are some space

298
00:10:52,880 --> 00:10:53,839
constraints

299
00:10:53,839 --> 00:10:55,920
in this facilities where if you turn up

300
00:10:55,920 --> 00:10:57,680
a big

301
00:10:57,680 --> 00:11:00,000
high density next-gen routers we would

302
00:11:00,000 --> 00:11:01,920
have to make sure we have enough space

303
00:11:01,920 --> 00:11:03,040
around the rack

304
00:11:03,040 --> 00:11:05,360
for the for the proper cooling purpose

305
00:11:05,360 --> 00:11:07,120
so we end up losing more space

306
00:11:07,120 --> 00:11:09,920
um with this new deployments these new

307
00:11:09,920 --> 00:11:12,560
big routers

308
00:11:13,200 --> 00:11:17,120
so that takes me to the the proposal

309
00:11:17,120 --> 00:11:19,200
or the solution we we came up with or we

310
00:11:19,200 --> 00:11:20,640
went with um

311
00:11:20,640 --> 00:11:23,760
we decided to go with um

312
00:11:23,760 --> 00:11:26,880
a spine and leaf class architecture

313
00:11:26,880 --> 00:11:29,680
in our core locations this is not a new

314
00:11:29,680 --> 00:11:30,480
architecture

315
00:11:30,480 --> 00:11:32,800
it's been there for a long time it's

316
00:11:32,800 --> 00:11:33,760
been

317
00:11:33,760 --> 00:11:37,680
deployed and proven successful in in

318
00:11:37,680 --> 00:11:40,880
data centers um so we have we are

319
00:11:40,880 --> 00:11:41,600
adapting

320
00:11:41,600 --> 00:11:44,640
that architecture for our core um

321
00:11:44,640 --> 00:11:47,839
backbone locations um so it does address

322
00:11:47,839 --> 00:11:51,360
all these concerns we talked earlier

323
00:11:51,360 --> 00:11:53,360
we can pick and choose different

324
00:11:53,360 --> 00:11:55,200
platforms based on the

325
00:11:55,200 --> 00:11:57,600
roles we can we have flexibility to

326
00:11:57,600 --> 00:11:58,240
scale

327
00:11:58,240 --> 00:12:00,959
up as well as scale uh out we can do

328
00:12:00,959 --> 00:12:02,639
both horizontal and vertical

329
00:12:02,639 --> 00:12:05,760
uh uh scaling and we would able

330
00:12:05,760 --> 00:12:08,639
we are able to seamlessly integrate this

331
00:12:08,639 --> 00:12:10,160
new architecture into the existing

332
00:12:10,160 --> 00:12:11,040
network

333
00:12:11,040 --> 00:12:13,040
so we would turn the existing core

334
00:12:13,040 --> 00:12:14,800
router as one of the leaf

335
00:12:14,800 --> 00:12:16,720
into this spine enemy of architecture

336
00:12:16,720 --> 00:12:19,120
and so that we don't disrupt the

337
00:12:19,120 --> 00:12:23,600
existing traffic flows or the demands

338
00:12:23,600 --> 00:12:26,320
by taking a big router and breaking them

339
00:12:26,320 --> 00:12:26,959
into

340
00:12:26,959 --> 00:12:29,120
smaller routers we break the blast

341
00:12:29,120 --> 00:12:30,560
radius now so

342
00:12:30,560 --> 00:12:33,279
now with one router failure we are not

343
00:12:33,279 --> 00:12:34,480
impacting

344
00:12:34,480 --> 00:12:36,480
the traffic flows going through that

345
00:12:36,480 --> 00:12:38,480
site anymore

346
00:12:38,480 --> 00:12:42,079
um moving this architecture

347
00:12:42,079 --> 00:12:44,240
allows us to change the focus from a

348
00:12:44,240 --> 00:12:46,399
platform towards an architecture

349
00:12:46,399 --> 00:12:48,639
so before we were always looking for a

350
00:12:48,639 --> 00:12:50,560
bigger and bigger and bigger platforms

351
00:12:50,560 --> 00:12:52,560
and aligning our deployments to the

352
00:12:52,560 --> 00:12:53,360
platform

353
00:12:53,360 --> 00:12:55,600
rollouts now we have this architecture

354
00:12:55,600 --> 00:12:57,040
now we have more options to

355
00:12:57,040 --> 00:12:58,959
pick different platforms for a different

356
00:12:58,959 --> 00:13:00,800
roles so

357
00:13:00,800 --> 00:13:02,720
now we can focus on the architecture

358
00:13:02,720 --> 00:13:05,120
rather than the platform

359
00:13:05,120 --> 00:13:06,959
uh we have built with n plus one

360
00:13:06,959 --> 00:13:09,600
redundancy so that means we can take

361
00:13:09,600 --> 00:13:13,440
um the routers offline to do maintenance

362
00:13:13,440 --> 00:13:15,680
during the daytime during the nighttime

363
00:13:15,680 --> 00:13:17,360
um

364
00:13:17,360 --> 00:13:19,120
we don't have that maintenance window

365
00:13:19,120 --> 00:13:20,480
constraint anymore

366
00:13:20,480 --> 00:13:22,480
we have uh we have done hitless

367
00:13:22,480 --> 00:13:23,760
maintenance during the day

368
00:13:23,760 --> 00:13:26,399
time now um this allows this

369
00:13:26,399 --> 00:13:27,519
architecture

370
00:13:27,519 --> 00:13:31,200
allows us to drain traffic on one spine

371
00:13:31,200 --> 00:13:32,880
so for example if you have four spine or

372
00:13:32,880 --> 00:13:34,320
eight spine so you take one of the

373
00:13:34,320 --> 00:13:36,000
spines out drain traffic

374
00:13:36,000 --> 00:13:38,560
uh of the spine move the traffic over on

375
00:13:38,560 --> 00:13:40,399
the rest of the spine nodes

376
00:13:40,399 --> 00:13:41,920
and do the maintenance and bring them

377
00:13:41,920 --> 00:13:44,480
back up um without impacting the traffic

378
00:13:44,480 --> 00:13:45,040
flows

379
00:13:45,040 --> 00:13:48,160
through the site so our downstream

380
00:13:48,160 --> 00:13:50,639
customers as well as our upstream um

381
00:13:50,639 --> 00:13:53,600
providers don't see uh this maintenance

382
00:13:53,600 --> 00:13:55,040
being done

383
00:13:55,040 --> 00:13:57,920
in their network

384
00:13:58,320 --> 00:14:01,199
but you guys might be wondering so

385
00:14:01,199 --> 00:14:03,120
spinal leaf is there forever i mean

386
00:14:03,120 --> 00:14:05,920
more than 10-15 years and it's proven in

387
00:14:05,920 --> 00:14:08,240
data center what is new here

388
00:14:08,240 --> 00:14:11,600
um the what we have found out is that

389
00:14:11,600 --> 00:14:14,399
um deploying spine and leaf in a core

390
00:14:14,399 --> 00:14:18,240
location has its own challenges

391
00:14:18,639 --> 00:14:21,839
the requirements slas support

392
00:14:21,839 --> 00:14:24,720
uh requirements uh design requirements

393
00:14:24,720 --> 00:14:25,279
are

394
00:14:25,279 --> 00:14:28,240
slightly different than um than the data

395
00:14:28,240 --> 00:14:30,399
center network

396
00:14:30,399 --> 00:14:32,720
for example in a core network we have to

397
00:14:32,720 --> 00:14:34,240
support a you know

398
00:14:34,240 --> 00:14:36,959
ip table full internet table we have 1.2

399
00:14:36,959 --> 00:14:37,440
million

400
00:14:37,440 --> 00:14:42,000
uh prefixes uh you know about 800 uh

401
00:14:42,000 --> 00:14:43,600
internet prefix then you have internal

402
00:14:43,600 --> 00:14:45,120
prefixes so we have more than a million

403
00:14:45,120 --> 00:14:46,800
prefix we need to support

404
00:14:46,800 --> 00:14:49,760
and when you bring up the bgp full mesh

405
00:14:49,760 --> 00:14:51,279
there are you know you you

406
00:14:51,279 --> 00:14:54,959
you go from um two million packs to 20

407
00:14:54,959 --> 00:14:57,600
million parts or 30 million packs

408
00:14:57,600 --> 00:14:59,120
i'll go through more details about

409
00:14:59,120 --> 00:15:01,040
routing later in the slide but there are

410
00:15:01,040 --> 00:15:03,440
there are constraints there are um

411
00:15:03,440 --> 00:15:04,320
limitations

412
00:15:04,320 --> 00:15:06,000
there are issues that we have to tackle

413
00:15:06,000 --> 00:15:07,680
to make this architecture work in core

414
00:15:07,680 --> 00:15:09,600
backbone

415
00:15:09,600 --> 00:15:13,360
now with going from a single router to

416
00:15:13,360 --> 00:15:15,920
30 node or 20 node in a site you

417
00:15:15,920 --> 00:15:17,920
multiply by number of sites

418
00:15:17,920 --> 00:15:19,839
you ex and the number of nodes you get

419
00:15:19,839 --> 00:15:21,519
to support in igp domain is

420
00:15:21,519 --> 00:15:23,839
is going up drastically that means we

421
00:15:23,839 --> 00:15:25,519
have to keep an eye out on the igp dom

422
00:15:25,519 --> 00:15:26,959
igp protocol also

423
00:15:26,959 --> 00:15:30,399
make sure they are good with their scale

424
00:15:30,399 --> 00:15:31,600
numbers

425
00:15:31,600 --> 00:15:34,800
um the other um constraint is the ecmp

426
00:15:34,800 --> 00:15:36,959
paths before we had one path east or

427
00:15:36,959 --> 00:15:37,519
west

428
00:15:37,519 --> 00:15:40,079
now it's finally if we have n number of

429
00:15:40,079 --> 00:15:42,079
paths in number of hcmp valves we could

430
00:15:42,079 --> 00:15:42,480
have

431
00:15:42,480 --> 00:15:43,920
if you have four spines here there are

432
00:15:43,920 --> 00:15:46,160
four ecmp pads it could be eight that

433
00:15:46,160 --> 00:15:47,120
could be sixteen

434
00:15:47,120 --> 00:15:49,040
so you gotta make sure that the routers

435
00:15:49,040 --> 00:15:51,680
be picked they can do proper ecb

436
00:15:51,680 --> 00:15:54,320
traffic load sharing and the traffic

437
00:15:54,320 --> 00:15:55,440
profiles are different

438
00:15:55,440 --> 00:15:58,480
you know we don't control that the the

439
00:15:58,480 --> 00:15:59,360
traffic

440
00:15:59,360 --> 00:16:01,920
profiles that uh we support the transit

441
00:16:01,920 --> 00:16:02,880
traffic

442
00:16:02,880 --> 00:16:04,800
um and like data center you know you may

443
00:16:04,800 --> 00:16:06,320
have your own applications you have an

444
00:16:06,320 --> 00:16:08,160
opportunity to change the traffic

445
00:16:08,160 --> 00:16:10,079
profile if you need to

446
00:16:10,079 --> 00:16:12,000
with your own data center app but in a

447
00:16:12,000 --> 00:16:13,839
core location we have no control about

448
00:16:13,839 --> 00:16:15,360
the traffic profile so we got to make

449
00:16:15,360 --> 00:16:17,759
sure we pick the right platform

450
00:16:17,759 --> 00:16:19,360
we have to make sure we have the right

451
00:16:19,360 --> 00:16:21,839
tools to monitor

452
00:16:21,839 --> 00:16:23,920
the traffic load sharing among the ecb

453
00:16:23,920 --> 00:16:24,880
parts

454
00:16:24,880 --> 00:16:26,880
and we also need to rework the bgp

455
00:16:26,880 --> 00:16:28,880
design before it was simple

456
00:16:28,880 --> 00:16:31,519
you know in a backbone network you know

457
00:16:31,519 --> 00:16:32,800
you may have like

458
00:16:32,800 --> 00:16:35,600
a small number of routers maybe 20 30

459
00:16:35,600 --> 00:16:37,360
routers or maybe 40 routers

460
00:16:37,360 --> 00:16:40,560
it's easy to do bgp full mesh among all

461
00:16:40,560 --> 00:16:40,959
four

462
00:16:40,959 --> 00:16:43,680
um among all core routers now you go

463
00:16:43,680 --> 00:16:46,560
from the 20 or 30 or 40 routers 100

464
00:16:46,560 --> 00:16:48,720
on to hundreds of routers so we have to

465
00:16:48,720 --> 00:16:50,240
rethink on on

466
00:16:50,240 --> 00:16:53,519
the bgpd design as well

467
00:16:53,519 --> 00:16:57,199
and then the other uh constraints we

468
00:16:57,199 --> 00:16:57,519
will

469
00:16:57,519 --> 00:16:59,360
another thing we were trying to solve is

470
00:16:59,360 --> 00:17:00,880
the convergence so

471
00:17:00,880 --> 00:17:04,240
now we have high number of bgp paths

472
00:17:04,240 --> 00:17:08,319
and because of the bgp design ecmp

473
00:17:08,319 --> 00:17:10,880
you got to make sure the bgp convergence

474
00:17:10,880 --> 00:17:11,439
is in

475
00:17:11,439 --> 00:17:14,400
is in within the slav support um so we

476
00:17:14,400 --> 00:17:15,839
have to work with our vendors to make

477
00:17:15,839 --> 00:17:16,720
sure

478
00:17:16,720 --> 00:17:19,520
that the features and knobs some other

479
00:17:19,520 --> 00:17:22,240
trick we used a network with the bgp

480
00:17:22,240 --> 00:17:24,720
to allow the better convergence to meet

481
00:17:24,720 --> 00:17:26,799
the slas

482
00:17:26,799 --> 00:17:29,120
and and the last one as i was saying

483
00:17:29,120 --> 00:17:31,520
earlier we got to make sure our support

484
00:17:31,520 --> 00:17:32,240
organization

485
00:17:32,240 --> 00:17:35,120
our operational team is on board with

486
00:17:35,120 --> 00:17:35,919
this

487
00:17:35,919 --> 00:17:37,520
class architecture they have all the

488
00:17:37,520 --> 00:17:39,440
tools and dashboards and

489
00:17:39,440 --> 00:17:43,280
um and the toolkits to support this

490
00:17:43,280 --> 00:17:44,080
network

491
00:17:44,080 --> 00:17:45,919
so we have to think through to make sure

492
00:17:45,919 --> 00:17:47,520
how we support this network how we were

493
00:17:47,520 --> 00:17:49,520
supporting single node design versus

494
00:17:49,520 --> 00:17:52,960
a multi uh class cluster

495
00:17:52,960 --> 00:17:56,640
in our core backbone and one other thing

496
00:17:56,640 --> 00:17:58,880
i missed in a in a data center you know

497
00:17:58,880 --> 00:18:00,160
all the links are

498
00:18:00,160 --> 00:18:02,240
jumpers you know there are like three

499
00:18:02,240 --> 00:18:03,679
meter jumpers

500
00:18:03,679 --> 00:18:05,360
pretty much you know once you plug them

501
00:18:05,360 --> 00:18:07,280
in they don't go bad unless the update

502
00:18:07,280 --> 00:18:09,520
goes bad but here in core locations

503
00:18:09,520 --> 00:18:12,720
some of the links are over you know

504
00:18:12,720 --> 00:18:14,880
kilometer for the long-haul fibers they

505
00:18:14,880 --> 00:18:16,080
could be like you know across the

506
00:18:16,080 --> 00:18:16,880
country

507
00:18:16,880 --> 00:18:18,799
there could be multi uh maybe 100

508
00:18:18,799 --> 00:18:20,799
kilometers 200 kilometers even more than

509
00:18:20,799 --> 00:18:23,280
a thousand kilometers store so depending

510
00:18:23,280 --> 00:18:24,320
on

511
00:18:24,320 --> 00:18:26,480
where the locations are so we got to

512
00:18:26,480 --> 00:18:28,480
watch out for those um

513
00:18:28,480 --> 00:18:31,600
the fiber failures how does it impact um

514
00:18:31,600 --> 00:18:33,840
the class design how does it impact the

515
00:18:33,840 --> 00:18:35,360
scales we support

516
00:18:35,360 --> 00:18:39,280
um in the class designs

517
00:18:40,480 --> 00:18:43,520
this slide shows a quick

518
00:18:43,520 --> 00:18:47,280
preview of a typical core

519
00:18:47,280 --> 00:18:50,880
deployment we did um um

520
00:18:50,880 --> 00:18:54,160
so we we started with spine routers we

521
00:18:54,160 --> 00:18:56,160
call them cs nodes

522
00:18:56,160 --> 00:18:58,320
we started with four cs nodes but we

523
00:18:58,320 --> 00:19:00,000
have enough we have an option to

524
00:19:00,000 --> 00:19:02,559
go beyond four uh depending on the

525
00:19:02,559 --> 00:19:03,360
growth

526
00:19:03,360 --> 00:19:06,640
and then we have uh the local piece

527
00:19:06,640 --> 00:19:09,600
remote piece over the long fibers and

528
00:19:09,600 --> 00:19:12,160
the crs are the leaf nodes connecting to

529
00:19:12,160 --> 00:19:13,120
the spines

530
00:19:13,120 --> 00:19:14,880
and we also have the regional area

531
00:19:14,880 --> 00:19:16,240
network um

532
00:19:16,240 --> 00:19:18,320
that supports our customers that

533
00:19:18,320 --> 00:19:20,080
directly connects to our customers

534
00:19:20,080 --> 00:19:23,840
as a leaf node to this uh cs

535
00:19:23,840 --> 00:19:26,640
and we also have two router reflector

536
00:19:26,640 --> 00:19:28,799
i'll cover more in the bgp design but we

537
00:19:28,799 --> 00:19:30,799
have we installed two router factor

538
00:19:30,799 --> 00:19:31,600
servers

539
00:19:31,600 --> 00:19:33,919
and we took the router fracture function

540
00:19:33,919 --> 00:19:35,760
off of the routers and moved them onto

541
00:19:35,760 --> 00:19:36,720
this

542
00:19:36,720 --> 00:19:40,720
x86 um rr routers

543
00:19:44,080 --> 00:19:47,360
um so in the igp design

544
00:19:47,360 --> 00:19:50,160
um it's a the way we have our backbone

545
00:19:50,160 --> 00:19:53,120
network it's it's on its own idp domain

546
00:19:53,120 --> 00:19:55,280
and we have a separate igb domain for

547
00:19:55,280 --> 00:19:56,960
backbone compared to the ranging area

548
00:19:56,960 --> 00:19:58,559
network so that helped

549
00:19:58,559 --> 00:20:01,760
us a little bit here so

550
00:20:01,760 --> 00:20:03,600
we did increase the number of nodes in

551
00:20:03,600 --> 00:20:05,440
the backbone now with this class

552
00:20:05,440 --> 00:20:06,640
architecture

553
00:20:06,640 --> 00:20:10,640
but we are within the limit of the igp

554
00:20:10,640 --> 00:20:13,039
uh scale numbers but we are closely

555
00:20:13,039 --> 00:20:14,000
watching

556
00:20:14,000 --> 00:20:16,080
because as you add more nodes more links

557
00:20:16,080 --> 00:20:18,480
your igp database is going to grow big

558
00:20:18,480 --> 00:20:20,720
and we got to make sure we don't impact

559
00:20:20,720 --> 00:20:21,679
the convergence

560
00:20:21,679 --> 00:20:24,559
by increasing the database size so we're

561
00:20:24,559 --> 00:20:25,120
closing

562
00:20:25,120 --> 00:20:28,240
watching the igp database

563
00:20:28,240 --> 00:20:32,080
numbers we're also looking at just few

564
00:20:32,080 --> 00:20:32,799
options

565
00:20:32,799 --> 00:20:35,200
to reduce igp database in future if

566
00:20:35,200 --> 00:20:37,679
needed and there are a few itf drafts

567
00:20:37,679 --> 00:20:40,000
out there on how we can do

568
00:20:40,000 --> 00:20:43,520
uh optimization to igp isis for example

569
00:20:43,520 --> 00:20:46,559
in particular to reduce the database

570
00:20:46,559 --> 00:20:48,159
size so we are closely watching those

571
00:20:48,159 --> 00:20:48,720
traps

572
00:20:48,720 --> 00:20:51,440
so if we need to do any optimization in

573
00:20:51,440 --> 00:20:52,720
future we have a few

574
00:20:52,720 --> 00:20:56,400
few options on the table to tackle

575
00:20:58,400 --> 00:21:01,360
bgp design is a little bit complex than

576
00:21:01,360 --> 00:21:02,320
igp

577
00:21:02,320 --> 00:21:06,559
so we before we had full mesh vgp design

578
00:21:06,559 --> 00:21:08,640
and now we are going we went away from

579
00:21:08,640 --> 00:21:09,760
it and we did a

580
00:21:09,760 --> 00:21:12,240
two layer bgp design we have router

581
00:21:12,240 --> 00:21:14,559
reflector servers

582
00:21:14,559 --> 00:21:18,080
uh in a mesh and then all the routers

583
00:21:18,080 --> 00:21:20,159
in the site would be the route reflected

584
00:21:20,159 --> 00:21:23,280
client for the servers

585
00:21:23,280 --> 00:21:26,320
um so uh this is

586
00:21:26,320 --> 00:21:28,720
uh this is a two layer design that we

587
00:21:28,720 --> 00:21:30,559
that we are rolling out with this class

588
00:21:30,559 --> 00:21:33,200
architecture

589
00:21:33,760 --> 00:21:36,000
uh we have to do a lot of quite a few

590
00:21:36,000 --> 00:21:37,840
features and enhancements and

591
00:21:37,840 --> 00:21:38,960
enhancements

592
00:21:38,960 --> 00:21:41,679
uh to get the convergence number we were

593
00:21:41,679 --> 00:21:42,720
looking for

594
00:21:42,720 --> 00:21:45,760
um and also we had to turn up uh certain

595
00:21:45,760 --> 00:21:46,640
features

596
00:21:46,640 --> 00:21:49,280
um to make sure the load sharing works

597
00:21:49,280 --> 00:21:49,760
fine

598
00:21:49,760 --> 00:21:52,080
so now we have a lot of ecmp paths we

599
00:21:52,080 --> 00:21:53,280
have to turn on

600
00:21:53,280 --> 00:21:56,320
bgp add path feature um otherwise the

601
00:21:56,320 --> 00:21:58,000
router effect is going to pick one

602
00:21:58,000 --> 00:22:01,440
path out of the um

603
00:22:01,440 --> 00:22:05,039
all possible uh bgp patch and send down

604
00:22:05,039 --> 00:22:06,159
to the

605
00:22:06,159 --> 00:22:08,320
downstream clients that means we are not

606
00:22:08,320 --> 00:22:09,360
going to use all

607
00:22:09,360 --> 00:22:12,400
ecmp paths so we have to make sure

608
00:22:12,400 --> 00:22:15,919
we enable a bgp ad path feature on

609
00:22:15,919 --> 00:22:18,240
on the router effector servers so that

610
00:22:18,240 --> 00:22:19,919
the router reflected clients

611
00:22:19,919 --> 00:22:23,840
the pes and cs nodes and crs have full

612
00:22:23,840 --> 00:22:26,880
view of the topology so that they can do

613
00:22:26,880 --> 00:22:30,080
um packet flow load sharing across all

614
00:22:30,080 --> 00:22:31,039
the pads

615
00:22:31,039 --> 00:22:34,799
all the ecmp pads we have also rolled

616
00:22:34,799 --> 00:22:35,760
out a bgp

617
00:22:35,760 --> 00:22:41,840
pick feature with vgp pick

618
00:22:42,240 --> 00:22:44,640
feature what it does in addition to the

619
00:22:44,640 --> 00:22:45,840
best path

620
00:22:45,840 --> 00:22:49,200
it also um pick a second

621
00:22:49,200 --> 00:22:51,600
backup path and install the backup path

622
00:22:51,600 --> 00:22:52,559
in the fib

623
00:22:52,559 --> 00:22:54,720
so that if the primary path goes away we

624
00:22:54,720 --> 00:22:56,240
already have a backup path in the

625
00:22:56,240 --> 00:22:57,039
hardware

626
00:22:57,039 --> 00:22:58,400
you don't have to wait for the bgp

627
00:22:58,400 --> 00:23:00,080
convergence and so you

628
00:23:00,080 --> 00:23:02,559
you swing to the backup path immediately

629
00:23:02,559 --> 00:23:04,720
so we enable bgp pick to get the better

630
00:23:04,720 --> 00:23:05,840
convergence

631
00:23:05,840 --> 00:23:09,600
and also we need to enable um bgp pick

632
00:23:09,600 --> 00:23:10,320
multipath

633
00:23:10,320 --> 00:23:13,919
because now we have a ecmp

634
00:23:13,919 --> 00:23:15,760
we have four nodes or eight nodes

635
00:23:15,760 --> 00:23:17,520
depending on number of spine nodes we

636
00:23:17,520 --> 00:23:18,000
have

637
00:23:18,000 --> 00:23:19,840
we have a lot more paths so we have to

638
00:23:19,840 --> 00:23:21,679
make sure the bgp

639
00:23:21,679 --> 00:23:24,640
installs the ecmps of primary patch if

640
00:23:24,640 --> 00:23:26,080
there are four ecmp paths

641
00:23:26,080 --> 00:23:28,960
we installed four bgp uh primary paths

642
00:23:28,960 --> 00:23:29,679
then a

643
00:23:29,679 --> 00:23:32,720
second our backup path which is not part

644
00:23:32,720 --> 00:23:34,480
of this acmp group so we are enabled

645
00:23:34,480 --> 00:23:35,200
that

646
00:23:35,200 --> 00:23:38,240
bgp fake multipath feature for that

647
00:23:38,240 --> 00:23:40,640
and we're also working with the vendors

648
00:23:40,640 --> 00:23:42,480
to get additional features and

649
00:23:42,480 --> 00:23:43,600
enhancements

650
00:23:43,600 --> 00:23:47,520
to optimize our convergence

651
00:23:47,520 --> 00:23:51,600
so when we turn on bgpr path

652
00:23:51,600 --> 00:23:55,200
the number of uh bgp path explodes

653
00:23:55,200 --> 00:23:57,760
now now you have a lot of ecmp paths a

654
00:23:57,760 --> 00:23:59,440
lot of bgp speakers

655
00:23:59,440 --> 00:24:02,240
so then the number of uh bgp path

656
00:24:02,240 --> 00:24:04,000
explodes to reduce the path

657
00:24:04,000 --> 00:24:06,080
the more bgp path you have it takes long

658
00:24:06,080 --> 00:24:07,200
time to compute

659
00:24:07,200 --> 00:24:10,159
and run through the algorithm and pick

660
00:24:10,159 --> 00:24:10,799
the right

661
00:24:10,799 --> 00:24:14,000
best path in during failure scenarios so

662
00:24:14,000 --> 00:24:16,799
it impacts the convergence numbers so to

663
00:24:16,799 --> 00:24:18,480
reduce the bgp patch

664
00:24:18,480 --> 00:24:21,360
uh we we are rolling out the anycast bgp

665
00:24:21,360 --> 00:24:22,400
next top

666
00:24:22,400 --> 00:24:24,159
so what it is is that we would pick an

667
00:24:24,159 --> 00:24:26,559
ip that is configured to say any cast

668
00:24:26,559 --> 00:24:28,400
among all the cs nodes

669
00:24:28,400 --> 00:24:31,760
within the site and that way um

670
00:24:31,760 --> 00:24:35,520
the rrs and the pes and the remote crs

671
00:24:35,520 --> 00:24:38,640
local seals crs and remote crs would see

672
00:24:38,640 --> 00:24:42,240
one vgp pad with the next up uh anycast

673
00:24:42,240 --> 00:24:44,080
so we were able to reduce the pgp path

674
00:24:44,080 --> 00:24:46,080
by using any cast

675
00:24:46,080 --> 00:24:49,039
vgp next half

676
00:24:49,440 --> 00:24:52,159
but when we enable any cast there's one

677
00:24:52,159 --> 00:24:54,240
corner case where we could impact a

678
00:24:54,240 --> 00:24:54,960
traffic

679
00:24:54,960 --> 00:24:57,360
um we have to make sure if there is a

680
00:24:57,360 --> 00:24:59,120
link flap if there is a bgp

681
00:24:59,120 --> 00:25:02,559
flat um the anycast address is not used

682
00:25:02,559 --> 00:25:04,960
for that path until the entire bgp table

683
00:25:04,960 --> 00:25:05,760
is converged

684
00:25:05,760 --> 00:25:07,600
you know it takes this good amount of

685
00:25:07,600 --> 00:25:09,440
time to download

686
00:25:09,440 --> 00:25:13,120
um 1.5 million paths and run through the

687
00:25:13,120 --> 00:25:14,880
bgp algorithm to put the right path so

688
00:25:14,880 --> 00:25:15,120
it

689
00:25:15,120 --> 00:25:17,120
takes some time during that time during

690
00:25:17,120 --> 00:25:18,480
the convergence time we don't want to

691
00:25:18,480 --> 00:25:20,240
send any traffic through the

692
00:25:20,240 --> 00:25:24,320
path um so how we address that we are

693
00:25:24,320 --> 00:25:27,919
we are trying to we are tying bgp state

694
00:25:27,919 --> 00:25:29,520
are the triggers

695
00:25:29,520 --> 00:25:31,440
to the anycast advertisement so if if

696
00:25:31,440 --> 00:25:34,080
there is a link flap on cs1

697
00:25:34,080 --> 00:25:36,080
we have to make sure the cs1 is not

698
00:25:36,080 --> 00:25:38,080
being used as an active path

699
00:25:38,080 --> 00:25:39,840
we would wait for the incomplete

700
00:25:39,840 --> 00:25:42,080
convergence of bgp before we inject

701
00:25:42,080 --> 00:25:45,360
any test into the igp domain um so we

702
00:25:45,360 --> 00:25:46,799
are using

703
00:25:46,799 --> 00:25:49,840
any advertisement with triggers

704
00:25:49,840 --> 00:25:53,520
and bgp state the other option we were

705
00:25:53,520 --> 00:25:55,760
we were we have deployed is

706
00:25:55,760 --> 00:25:58,880
advertised bgp paths

707
00:25:58,880 --> 00:26:02,240
with different policies only the local

708
00:26:02,240 --> 00:26:04,720
nodes need to know the full topology of

709
00:26:04,720 --> 00:26:05,440
of

710
00:26:05,440 --> 00:26:07,760
the local cluster so we are advertising

711
00:26:07,760 --> 00:26:08,960
the full view

712
00:26:08,960 --> 00:26:12,000
onto the local nodes local clients but

713
00:26:12,000 --> 00:26:15,039
the remote clusters remote rrs are the

714
00:26:15,039 --> 00:26:16,720
remote clusters they don't need to know

715
00:26:16,720 --> 00:26:17,440
every path

716
00:26:17,440 --> 00:26:21,039
inside the local local class cluster so

717
00:26:21,039 --> 00:26:22,159
what we're doing there we are

718
00:26:22,159 --> 00:26:24,159
advertising only the best and backup

719
00:26:24,159 --> 00:26:24,559
path

720
00:26:24,559 --> 00:26:28,000
to the remote peers and advertise full

721
00:26:28,000 --> 00:26:30,159
view or full paths to the local clients

722
00:26:30,159 --> 00:26:31,520
that way we have a

723
00:26:31,520 --> 00:26:33,200
better control on how many paths we

724
00:26:33,200 --> 00:26:34,640
advertise to the remote

725
00:26:34,640 --> 00:26:38,000
rrs we are still actively

726
00:26:38,000 --> 00:26:40,559
working with our vendors to find other

727
00:26:40,559 --> 00:26:42,559
ways to reduce the bgp paths

728
00:26:42,559 --> 00:26:45,360
um so we can keep optimizing as we go

729
00:26:45,360 --> 00:26:47,678
along

730
00:26:51,279 --> 00:26:54,720
um provisioning and support automation

731
00:26:54,720 --> 00:26:58,799
and operations automation is it's really

732
00:26:58,799 --> 00:27:00,799
it's a critical uh component here we got

733
00:27:00,799 --> 00:27:02,400
to make sure we have automation

734
00:27:02,400 --> 00:27:06,080
to support this class deployments and

735
00:27:06,080 --> 00:27:07,679
there are four categories

736
00:27:07,679 --> 00:27:11,200
we need automation to launch devices

737
00:27:11,200 --> 00:27:13,600
we need automation to roll out

738
00:27:13,600 --> 00:27:14,799
configuration

739
00:27:14,799 --> 00:27:18,799
into the devices we need automation to

740
00:27:18,799 --> 00:27:20,399
do cable validations there's a lot of

741
00:27:20,399 --> 00:27:22,559
cabling now

742
00:27:22,559 --> 00:27:24,640
and we also need automation to do code

743
00:27:24,640 --> 00:27:26,159
upgrades

744
00:27:26,159 --> 00:27:28,880
so we have uh we have made a lot of good

745
00:27:28,880 --> 00:27:30,159
progress with automation

746
00:27:30,159 --> 00:27:31,679
but we still need to work through a few

747
00:27:31,679 --> 00:27:34,000
things as well um

748
00:27:34,000 --> 00:27:36,640
when it comes to operations uh we

749
00:27:36,640 --> 00:27:38,559
realized that we probably need more than

750
00:27:38,559 --> 00:27:39,840
snmp

751
00:27:39,840 --> 00:27:43,360
we need to start collecting data

752
00:27:43,360 --> 00:27:46,080
from the routers in a different

753
00:27:46,080 --> 00:27:47,600
perspective because now we are

754
00:27:47,600 --> 00:27:48,880
monitoring

755
00:27:48,880 --> 00:27:51,360
a whole cluster that could be 50 nodes

756
00:27:51,360 --> 00:27:53,120
inside the cluster but we need to find a

757
00:27:53,120 --> 00:27:53,679
way to

758
00:27:53,679 --> 00:27:56,320
get data from all those routers and

759
00:27:56,320 --> 00:27:58,320
correlate them and show them as a single

760
00:27:58,320 --> 00:27:59,840
cluster

761
00:27:59,840 --> 00:28:01,840
so we started looking into telemetry

762
00:28:01,840 --> 00:28:03,120
data streaming

763
00:28:03,120 --> 00:28:06,080
we were able to successfully get data to

764
00:28:06,080 --> 00:28:06,960
those routers

765
00:28:06,960 --> 00:28:09,279
we are pushing those telemetry streams

766
00:28:09,279 --> 00:28:10,240
into

767
00:28:10,240 --> 00:28:12,399
into a collector and that goes into a

768
00:28:12,399 --> 00:28:14,159
database and we are

769
00:28:14,159 --> 00:28:16,960
we are starting to use the data data

770
00:28:16,960 --> 00:28:18,559
sets that we collect using telemetry

771
00:28:18,559 --> 00:28:19,360
data

772
00:28:19,360 --> 00:28:22,480
data streams um

773
00:28:22,480 --> 00:28:26,000
before the change here before if there

774
00:28:26,000 --> 00:28:28,000
is a fabric problem in a big multi

775
00:28:28,000 --> 00:28:30,080
chassis or big box we would just make a

776
00:28:30,080 --> 00:28:31,440
phone call to our vendors

777
00:28:31,440 --> 00:28:34,240
and have them to figure out the problem

778
00:28:34,240 --> 00:28:36,000
and fix but with the class

779
00:28:36,000 --> 00:28:38,320
architecture now we get exposed to the

780
00:28:38,320 --> 00:28:40,320
fabric problems and we it's on us to

781
00:28:40,320 --> 00:28:42,080
troubleshoot monitor troubleshoot and

782
00:28:42,080 --> 00:28:44,880
fix all the fabric the fabric issues

783
00:28:44,880 --> 00:28:47,200
between the spine and leaf modes so it's

784
00:28:47,200 --> 00:28:48,000
a little

785
00:28:48,000 --> 00:28:49,840
bit complex than how we were operating

786
00:28:49,840 --> 00:28:51,919
before with a single

787
00:28:51,919 --> 00:28:54,720
chassis design

788
00:28:56,000 --> 00:28:57,919
so a little more on this and the support

789
00:28:57,919 --> 00:29:00,399
here uh we need to make sure

790
00:29:00,399 --> 00:29:03,200
we have all the tool sets um available

791
00:29:03,200 --> 00:29:05,200
to our operations team to monitor this

792
00:29:05,200 --> 00:29:07,919
class cluster and we have to make sure

793
00:29:07,919 --> 00:29:09,600
we group all the routers in the

794
00:29:09,600 --> 00:29:12,000
in a cluster and show them as a single

795
00:29:12,000 --> 00:29:12,799
unit

796
00:29:12,799 --> 00:29:15,760
um so that we can find the health of the

797
00:29:15,760 --> 00:29:16,720
cluster

798
00:29:16,720 --> 00:29:20,399
um and this is it's critical because

799
00:29:20,399 --> 00:29:23,200
if if the cluster is not um if we lose

800
00:29:23,200 --> 00:29:24,799
one of the spine we got to make sure we

801
00:29:24,799 --> 00:29:26,480
notify that to our operations

802
00:29:26,480 --> 00:29:29,600
that cluster isn't in a in a failure uh

803
00:29:29,600 --> 00:29:32,320
uh situation we can have one more

804
00:29:32,320 --> 00:29:33,679
failure in that clusters

805
00:29:33,679 --> 00:29:37,120
um so we got to make sure how we monitor

806
00:29:37,120 --> 00:29:40,000
these class components and we got to

807
00:29:40,000 --> 00:29:40,480
build the

808
00:29:40,480 --> 00:29:44,080
dashboards and tools and to show the

809
00:29:44,080 --> 00:29:45,600
entire group of routers

810
00:29:45,600 --> 00:29:48,879
as a single srg

811
00:29:49,600 --> 00:29:51,520
so we started thinking how do we monitor

812
00:29:51,520 --> 00:29:53,679
this class so we were able to come up

813
00:29:53,679 --> 00:29:54,240
with

814
00:29:54,240 --> 00:29:57,200
five pillars um starting from the

815
00:29:57,200 --> 00:29:57,760
hardware

816
00:29:57,760 --> 00:30:01,360
software routing protocols links

817
00:30:01,360 --> 00:30:05,039
and and traffic um i'll go more details

818
00:30:05,039 --> 00:30:08,960
about each pillar in following slides

819
00:30:08,960 --> 00:30:11,120
so in a hardware if you look at any of

820
00:30:11,120 --> 00:30:12,559
this new um

821
00:30:12,559 --> 00:30:14,720
asics coming out there are a lot of

822
00:30:14,720 --> 00:30:16,640
components and components inside the

823
00:30:16,640 --> 00:30:17,360
hardware

824
00:30:17,360 --> 00:30:20,320
a6 um there are resources there are

825
00:30:20,320 --> 00:30:21,679
memory cables

826
00:30:21,679 --> 00:30:23,919
sliced for different applications for

827
00:30:23,919 --> 00:30:25,679
example pecan

828
00:30:25,679 --> 00:30:29,200
lem lpm fec and there are port buffers

829
00:30:29,200 --> 00:30:29,840
we use

830
00:30:29,840 --> 00:30:33,200
inside the the chipset

831
00:30:33,200 --> 00:30:36,799
and there are qos buffers we need to

832
00:30:36,799 --> 00:30:37,760
monitor

833
00:30:37,760 --> 00:30:40,159
um so there's a lot of hardware

834
00:30:40,159 --> 00:30:42,480
components we need to monitor

835
00:30:42,480 --> 00:30:44,240
to make sure the hardware is working

836
00:30:44,240 --> 00:30:46,399
fine um

837
00:30:46,399 --> 00:30:49,840
to the level we expected we are expected

838
00:30:49,840 --> 00:30:50,720
to them to behave

839
00:30:50,720 --> 00:30:54,000
um so we are we are looking into

840
00:30:54,000 --> 00:30:55,679
some of the telemetry data streams to

841
00:30:55,679 --> 00:30:57,039
monitor um

842
00:30:57,039 --> 00:30:58,720
these components and to give us

843
00:30:58,720 --> 00:31:00,159
visibility to the

844
00:31:00,159 --> 00:31:03,279
hardware uh health

845
00:31:03,279 --> 00:31:05,279
uh likewise the software we are looking

846
00:31:05,279 --> 00:31:06,559
ways to monitor

847
00:31:06,559 --> 00:31:08,640
the smooth upgrades the smooth running

848
00:31:08,640 --> 00:31:10,320
in the box the signature

849
00:31:10,320 --> 00:31:13,519
check of the software and also the bugs

850
00:31:13,519 --> 00:31:15,760
um and this under all other software

851
00:31:15,760 --> 00:31:16,840
process

852
00:31:16,840 --> 00:31:19,840
processes

853
00:31:19,919 --> 00:31:22,320
then it comes to the routing protocol

854
00:31:22,320 --> 00:31:23,600
now now we have

855
00:31:23,600 --> 00:31:26,080
a you know a group of spine a bunch of

856
00:31:26,080 --> 00:31:27,919
fleas we have isis

857
00:31:27,919 --> 00:31:31,840
lldp running on those fabric links so we

858
00:31:31,840 --> 00:31:33,840
have to monitor to make sure

859
00:31:33,840 --> 00:31:36,080
we track all the link failures node

860
00:31:36,080 --> 00:31:37,200
failures

861
00:31:37,200 --> 00:31:40,880
um um all the protocol states

862
00:31:40,880 --> 00:31:44,000
um and also we need to start monitoring

863
00:31:44,000 --> 00:31:46,960
we had to monitor the bgp sessions their

864
00:31:46,960 --> 00:31:47,519
states

865
00:31:47,519 --> 00:31:49,679
the prefixes received advertised between

866
00:31:49,679 --> 00:31:50,799
the bgp session

867
00:31:50,799 --> 00:31:54,080
between between the bgp peers and also

868
00:31:54,080 --> 00:31:55,840
start looking into the router

869
00:31:55,840 --> 00:31:59,360
uh trend to see how stable the route

870
00:31:59,360 --> 00:32:01,120
the network is so we have to watch all

871
00:32:01,120 --> 00:32:02,799
those uh

872
00:32:02,799 --> 00:32:06,799
routing details um the churn and

873
00:32:06,799 --> 00:32:10,159
the isis database and bgp table

874
00:32:10,159 --> 00:32:12,399
of all the flaps going on so i get to

875
00:32:12,399 --> 00:32:14,720
monitor them

876
00:32:14,720 --> 00:32:18,480
um so that gives the visibility to the

877
00:32:18,480 --> 00:32:19,760
routing protocols

878
00:32:19,760 --> 00:32:22,880
then the links so we

879
00:32:22,880 --> 00:32:24,960
we have to monitor as i said before you

880
00:32:24,960 --> 00:32:26,399
know we didn't have to worry about the

881
00:32:26,399 --> 00:32:27,279
fabric

882
00:32:27,279 --> 00:32:29,760
links inside a box now with the spine

883
00:32:29,760 --> 00:32:31,360
and lift we have to monitor those fabric

884
00:32:31,360 --> 00:32:32,080
links

885
00:32:32,080 --> 00:32:34,080
and we are to check the redundancy for

886
00:32:34,080 --> 00:32:35,120
those fabrics if

887
00:32:35,120 --> 00:32:36,640
one of the spine goes out we have to

888
00:32:36,640 --> 00:32:39,440
monitor we have to monitor and notify

889
00:32:39,440 --> 00:32:41,600
the team to make sure you know they get

890
00:32:41,600 --> 00:32:42,880
fixed

891
00:32:42,880 --> 00:32:44,720
and also there are a lot of optics here

892
00:32:44,720 --> 00:32:46,640
now involved we have to monitor the

893
00:32:46,640 --> 00:32:49,120
receive and transmit power of the optics

894
00:32:49,120 --> 00:32:52,480
uh the health of the optics um

895
00:32:52,480 --> 00:32:54,559
and see any any trend that we can pick

896
00:32:54,559 --> 00:32:57,840
up for any issues

897
00:32:58,159 --> 00:33:00,799
and the last pillar is the traffic so as

898
00:33:00,799 --> 00:33:02,960
i was talking about ecmp paths now

899
00:33:02,960 --> 00:33:05,200
uh you have to make sure that the

900
00:33:05,200 --> 00:33:07,279
traffic is load shared among all the

901
00:33:07,279 --> 00:33:10,480
among all ecmp pads properly we got to

902
00:33:10,480 --> 00:33:11,200
make sure that

903
00:33:11,200 --> 00:33:13,039
the packets and bits coming into the

904
00:33:13,039 --> 00:33:14,720
cluster are being

905
00:33:14,720 --> 00:33:16,320
routed out of the cluster without any

906
00:33:16,320 --> 00:33:17,760
drops internally

907
00:33:17,760 --> 00:33:20,960
we have to monitor the drops

908
00:33:20,960 --> 00:33:24,240
interface drops now zero drops ttl drops

909
00:33:24,240 --> 00:33:24,720
no

910
00:33:24,720 --> 00:33:27,039
no route drops cures drops and hardware

911
00:33:27,039 --> 00:33:28,880
drops so we got to monitor all these

912
00:33:28,880 --> 00:33:29,440
drops

913
00:33:29,440 --> 00:33:32,080
to understand what's going on inside the

914
00:33:32,080 --> 00:33:33,360
cluster

915
00:33:33,360 --> 00:33:36,960
uh one thing we found in the lab testing

916
00:33:36,960 --> 00:33:38,159
was that

917
00:33:38,159 --> 00:33:40,399
during the convergence when when you

918
00:33:40,399 --> 00:33:42,240
have multi-level bgp sessions

919
00:33:42,240 --> 00:33:44,399
if there is a failure then the bgp has

920
00:33:44,399 --> 00:33:46,960
to advertise the failure to the remote

921
00:33:46,960 --> 00:33:49,039
to the router effector servers and they

922
00:33:49,039 --> 00:33:51,279
would have to pass on that

923
00:33:51,279 --> 00:33:54,000
failure to the remote rrs and to their

924
00:33:54,000 --> 00:33:54,880
local uh

925
00:33:54,880 --> 00:33:56,720
clients that takes a little bit of a

926
00:33:56,720 --> 00:33:58,799
little bit of time the convergence

927
00:33:58,799 --> 00:34:01,679
so during this convergence the only way

928
00:34:01,679 --> 00:34:03,919
to figure out if there are any problem

929
00:34:03,919 --> 00:34:07,039
with the the traffic is looking into the

930
00:34:07,039 --> 00:34:09,440
micro loops if there is a micro loop

931
00:34:09,440 --> 00:34:10,239
happening

932
00:34:10,239 --> 00:34:13,199
inside the network the ttl uh drop count

933
00:34:13,199 --> 00:34:13,679
would go

934
00:34:13,679 --> 00:34:15,839
high so we we started looking into this

935
00:34:15,839 --> 00:34:17,359
ttl numbers to understand the

936
00:34:17,359 --> 00:34:19,440
convergence

937
00:34:19,440 --> 00:34:21,520
and start figuring out uh different

938
00:34:21,520 --> 00:34:23,040
options to bring down the convergence

939
00:34:23,040 --> 00:34:25,119
number or the recovery time

940
00:34:25,119 --> 00:34:28,239
and so that um the conver so that

941
00:34:28,239 --> 00:34:30,000
we impact the impact of our customer is

942
00:34:30,000 --> 00:34:32,560
reduced so the ttl drop

943
00:34:32,560 --> 00:34:35,280
um the no route drop the qrs drops are

944
00:34:35,280 --> 00:34:36,879
critical

945
00:34:36,879 --> 00:34:39,040
data points for us to look into the

946
00:34:39,040 --> 00:34:42,000
health of the class

947
00:34:42,480 --> 00:34:46,639
the next slide shows some of the graphs

948
00:34:46,639 --> 00:34:49,199
that we started building again we are

949
00:34:49,199 --> 00:34:51,599
very early into this telemetry streaming

950
00:34:51,599 --> 00:34:54,719
uh project the first thing we needed to

951
00:34:54,719 --> 00:34:55,040
do

952
00:34:55,040 --> 00:34:58,800
was to find out all the data points we

953
00:34:58,800 --> 00:35:00,320
want to collect from the routers

954
00:35:00,320 --> 00:35:02,240
there are hundreds of thousands of data

955
00:35:02,240 --> 00:35:04,800
points that router can stream out today

956
00:35:04,800 --> 00:35:06,960
but we have to pick and choose what we

957
00:35:06,960 --> 00:35:08,000
are interested in

958
00:35:08,000 --> 00:35:09,839
we don't want to just dump everything

959
00:35:09,839 --> 00:35:11,680
out of the router then figuring out

960
00:35:11,680 --> 00:35:14,880
you know too much data to process

961
00:35:14,880 --> 00:35:18,160
so based on the pillar i was talking

962
00:35:18,160 --> 00:35:19,200
about earlier

963
00:35:19,200 --> 00:35:22,000
we are uh we are we are we have

964
00:35:22,000 --> 00:35:23,839
identified those sensor points we have

965
00:35:23,839 --> 00:35:25,280
configured the sensor points on the

966
00:35:25,280 --> 00:35:26,079
routers

967
00:35:26,079 --> 00:35:28,240
now we have those streamings coming out

968
00:35:28,240 --> 00:35:30,480
of the routers at every

969
00:35:30,480 --> 00:35:33,119
30 seconds to 60 seconds we can go down

970
00:35:33,119 --> 00:35:34,000
to even

971
00:35:34,000 --> 00:35:36,720
10 seconds but that's too too much i

972
00:35:36,720 --> 00:35:37,760
think i thought

973
00:35:37,760 --> 00:35:39,599
so we starting we are starting with 30

974
00:35:39,599 --> 00:35:41,040
seconds and some routers for some

975
00:35:41,040 --> 00:35:41,760
sensors

976
00:35:41,760 --> 00:35:43,760
some center points but other ones we are

977
00:35:43,760 --> 00:35:45,040
starting with 60 second

978
00:35:45,040 --> 00:35:48,640
intervals so we are we have turned up

979
00:35:48,640 --> 00:35:50,240
a collector it's an open source

980
00:35:50,240 --> 00:35:52,320
collector from cisco big money

981
00:35:52,320 --> 00:35:54,640
um that takes all these teams and we are

982
00:35:54,640 --> 00:35:56,400
processing them put them

983
00:35:56,400 --> 00:35:59,359
populating at the influx db database and

984
00:35:59,359 --> 00:36:01,040
i was able to create some of the graphs

985
00:36:01,040 --> 00:36:01,599
here

986
00:36:01,599 --> 00:36:04,000
that i show and now you can see the isis

987
00:36:04,000 --> 00:36:05,760
adjacency within the cluster there are

988
00:36:05,760 --> 00:36:06,800
four spines

989
00:36:06,800 --> 00:36:10,480
and you can see the total um isis count

990
00:36:10,480 --> 00:36:13,599
um aggregated for the cluster and you

991
00:36:13,599 --> 00:36:15,119
can see the traffic

992
00:36:15,119 --> 00:36:18,640
load sharing among all the leaf nodes

993
00:36:18,640 --> 00:36:21,760
um and also per leaf node view you can

994
00:36:21,760 --> 00:36:23,440
see the traffic split across

995
00:36:23,440 --> 00:36:25,520
all four nodes and then on the right

996
00:36:25,520 --> 00:36:28,000
side you have the bgp routes received

997
00:36:28,000 --> 00:36:30,720
we have the fabric bandwidth being um

998
00:36:30,720 --> 00:36:31,680
monitored

999
00:36:31,680 --> 00:36:35,119
by lltp data and also we have the qos

1000
00:36:35,119 --> 00:36:36,000
drops

1001
00:36:36,000 --> 00:36:37,440
graph that shows you know if there is

1002
00:36:37,440 --> 00:36:40,880
any drop going on with qos we would get

1003
00:36:40,880 --> 00:36:43,040
them here instantly

1004
00:36:43,040 --> 00:36:44,960
so these are the some sample graphs we

1005
00:36:44,960 --> 00:36:46,720
are building but there are a lot of work

1006
00:36:46,720 --> 00:36:47,680
to be done here

1007
00:36:47,680 --> 00:36:49,839
so now we have the data in the database

1008
00:36:49,839 --> 00:36:50,960
now we need to find

1009
00:36:50,960 --> 00:36:55,040
find a way to build the tools and

1010
00:36:55,200 --> 00:36:58,640
dashboards that can send right triggers

1011
00:36:58,640 --> 00:37:00,480
to our operations team to react for any

1012
00:37:00,480 --> 00:37:01,680
failures

1013
00:37:01,680 --> 00:37:04,640
um just want to give you that preview

1014
00:37:04,640 --> 00:37:06,800
here with the slides

1015
00:37:06,800 --> 00:37:10,800
the graphs um so that

1016
00:37:10,800 --> 00:37:13,520
uh takes us to the last slide um we have

1017
00:37:13,520 --> 00:37:14,000
the

1018
00:37:14,000 --> 00:37:16,640
next gen core cluster uh spinal leaf

1019
00:37:16,640 --> 00:37:18,079
architecture deployed

1020
00:37:18,079 --> 00:37:20,640
broadly in our comcast platform uh it's

1021
00:37:20,640 --> 00:37:22,480
it's been there uh

1022
00:37:22,480 --> 00:37:24,160
serving a customer's carrying production

1023
00:37:24,160 --> 00:37:26,560
big uh for a for a while now

1024
00:37:26,560 --> 00:37:29,359
um we were able to upgrade code on these

1025
00:37:29,359 --> 00:37:30,079
boxes

1026
00:37:30,079 --> 00:37:33,280
uh during the day time um we were able

1027
00:37:33,280 --> 00:37:35,280
to manage the traffic utilization

1028
00:37:35,280 --> 00:37:37,920
because now we don't have a single uh

1029
00:37:37,920 --> 00:37:39,119
router failure

1030
00:37:39,119 --> 00:37:41,359
impacting multiple paths anymore we the

1031
00:37:41,359 --> 00:37:43,440
blast rate is as small as we were able

1032
00:37:43,440 --> 00:37:44,640
to manage the

1033
00:37:44,640 --> 00:37:47,680
traffic utilization properly some of the

1034
00:37:47,680 --> 00:37:48,480
lessons learned

1035
00:37:48,480 --> 00:37:51,599
automation is the key

1036
00:37:51,599 --> 00:37:54,240
need better process and tools for

1037
00:37:54,240 --> 00:37:55,440
operations

1038
00:37:55,440 --> 00:37:57,440
and there's a lot of cabling work to be

1039
00:37:57,440 --> 00:37:58,640
done so

1040
00:37:58,640 --> 00:38:00,320
we have to make sure we have all the

1041
00:38:00,320 --> 00:38:02,400
tool set in place to

1042
00:38:02,400 --> 00:38:05,680
um to track those cables to fix if there

1043
00:38:05,680 --> 00:38:07,680
are any issues with the cable in on day

1044
00:38:07,680 --> 00:38:08,320
one

1045
00:38:08,320 --> 00:38:11,760
so we use telemetry data set for that um

1046
00:38:11,760 --> 00:38:14,640
again the design is built to last longer

1047
00:38:14,640 --> 00:38:15,680
so

1048
00:38:15,680 --> 00:38:17,440
one of the learning is that you get all

1049
00:38:17,440 --> 00:38:19,280
your physical work done on day one so

1050
00:38:19,280 --> 00:38:20,320
you don't want to go back

1051
00:38:20,320 --> 00:38:23,599
and change physical cabling later

1052
00:38:23,599 --> 00:38:26,640
in the in the later in the time so

1053
00:38:26,640 --> 00:38:28,400
if you can get the physical work done

1054
00:38:28,400 --> 00:38:29,920
it's easy to deploy

1055
00:38:29,920 --> 00:38:32,480
more leaf nodes if needed and expand the

1056
00:38:32,480 --> 00:38:33,920
class class cluster

1057
00:38:33,920 --> 00:38:37,599
to support the demands that brings me to

1058
00:38:37,599 --> 00:38:39,440
the last slide i just wanted to thank

1059
00:38:39,440 --> 00:38:40,240
you

1060
00:38:40,240 --> 00:38:43,839
for the opportunity to share our

1061
00:38:43,839 --> 00:38:46,400
our evolution of backbone getting

1062
00:38:46,400 --> 00:38:47,760
comcast

1063
00:38:47,760 --> 00:38:51,119
i i thank uh augie and and tony tauber

1064
00:38:51,119 --> 00:38:53,200
and other folks who helped me this is my

1065
00:38:53,200 --> 00:38:53,680
first

1066
00:38:53,680 --> 00:38:56,800
nana presentation they helped me

1067
00:38:56,800 --> 00:38:58,720
to navigate the process and make me

1068
00:38:58,720 --> 00:39:00,000
comfortable to present this

1069
00:39:00,000 --> 00:39:02,280
thank you

1070
00:39:02,280 --> 00:39:06,609
[Music]

1071
00:39:07,040 --> 00:39:08,880
we have a few questions and if you have

1072
00:39:08,880 --> 00:39:10,400
any more please put them in the q a

1073
00:39:10,400 --> 00:39:12,000
channel now and i will relate them as we

1074
00:39:12,000 --> 00:39:12,800
get to them

1075
00:39:12,800 --> 00:39:14,720
uh so the first question is from dan

1076
00:39:14,720 --> 00:39:16,480
magorian who is asking

1077
00:39:16,480 --> 00:39:18,640
how could inbox and between box speeds

1078
00:39:18,640 --> 00:39:19,839
be similar when

1079
00:39:19,839 --> 00:39:24,078
same ship port to port is much faster

1080
00:39:24,400 --> 00:39:26,079
oh if i understood the question is he

1081
00:39:26,079 --> 00:39:28,880
asking if if he's asking

1082
00:39:28,880 --> 00:39:32,400
did we see any change in uh the speed of

1083
00:39:32,400 --> 00:39:33,440
processing

1084
00:39:33,440 --> 00:39:38,240
is in in different um boxes

1085
00:39:39,920 --> 00:39:42,000
i think he's asking more about how you

1086
00:39:42,000 --> 00:39:43,599
can get enough

1087
00:39:43,599 --> 00:39:46,800
bandwidth between boxes um in a

1088
00:39:46,800 --> 00:39:49,200
multi-node deployment versus a single

1089
00:39:49,200 --> 00:39:52,400
dropbox type of device

1090
00:39:52,400 --> 00:39:56,000
uh with with morse spines

1091
00:39:56,000 --> 00:39:58,480
uh we were able to with more notes we

1092
00:39:58,480 --> 00:39:59,440
were able to get more

1093
00:39:59,440 --> 00:40:02,320
more ports serving the you know the

1094
00:40:02,320 --> 00:40:03,839
service ports we call

1095
00:40:03,839 --> 00:40:06,640
uh we were able to expand the spines um

1096
00:40:06,640 --> 00:40:07,599
as we

1097
00:40:07,599 --> 00:40:10,880
um as we need to increase the bandwidth

1098
00:40:10,880 --> 00:40:13,040
we can expand the spine and and grow

1099
00:40:13,040 --> 00:40:14,160
horizontally to

1100
00:40:14,160 --> 00:40:16,480
support the additional ports yes we have

1101
00:40:16,480 --> 00:40:17,599
we have to spend

1102
00:40:17,599 --> 00:40:19,599
we have to use the force to interconnect

1103
00:40:19,599 --> 00:40:20,960
the spine and leaf

1104
00:40:20,960 --> 00:40:22,560
but i think if you scale enough and

1105
00:40:22,560 --> 00:40:26,160
vertically can support the service ports

1106
00:40:26,160 --> 00:40:29,440
okay matt p tech is asking

1107
00:40:29,440 --> 00:40:31,200
you say dashboards and tools should

1108
00:40:31,200 --> 00:40:32,800
monitor the entire group of

1109
00:40:32,800 --> 00:40:36,480
routers in the clusters as a single srlg

1110
00:40:36,480 --> 00:40:38,720
but you created an n plus one spine so

1111
00:40:38,720 --> 00:40:40,319
that you could fail

1112
00:40:40,319 --> 00:40:42,240
individual spines out without causing

1113
00:40:42,240 --> 00:40:44,319
impact if the whole cluster is in a

1114
00:40:44,319 --> 00:40:46,079
single slrg

1115
00:40:46,079 --> 00:40:47,760
when you have a spine failure do you

1116
00:40:47,760 --> 00:40:49,680
fail everything out or ignore it because

1117
00:40:49,680 --> 00:40:50,480
you have

1118
00:40:50,480 --> 00:40:52,800
n plus one redundancy internally to the

1119
00:40:52,800 --> 00:40:54,000
cluster

1120
00:40:54,000 --> 00:40:55,839
uh we don't actually so if there is one

1121
00:40:55,839 --> 00:40:58,319
spine failure we don't drain traffic

1122
00:40:58,319 --> 00:41:01,520
but we all we all we do need to mark or

1123
00:41:01,520 --> 00:41:01,920
tag

1124
00:41:01,920 --> 00:41:04,880
that group as a as a different state uh

1125
00:41:04,880 --> 00:41:06,000
it's not green anymore

1126
00:41:06,000 --> 00:41:08,800
it has it has one spine failed so we

1127
00:41:08,800 --> 00:41:10,640
have to mark and tag it differently so

1128
00:41:10,640 --> 00:41:12,240
that our operations team

1129
00:41:12,240 --> 00:41:14,720
keep an eye on the cluster to bring the

1130
00:41:14,720 --> 00:41:15,920
spine back up

1131
00:41:15,920 --> 00:41:17,839
and also will stop them to make any

1132
00:41:17,839 --> 00:41:19,119
changes to the cluster

1133
00:41:19,119 --> 00:41:23,040
to avoid any uh impact

1134
00:41:23,040 --> 00:41:25,359
thank you chris summers is asking uh

1135
00:41:25,359 --> 00:41:27,040
what streaming telemetry are you trying

1136
00:41:27,040 --> 00:41:28,079
gnmi

1137
00:41:28,079 --> 00:41:30,240
config and do you find consistent

1138
00:41:30,240 --> 00:41:33,200
multiple vendors support

1139
00:41:33,200 --> 00:41:35,440
no so open conflict we use wherever we

1140
00:41:35,440 --> 00:41:37,839
can but again it doesn't give us every

1141
00:41:37,839 --> 00:41:40,480
uh data points that we want to monitor

1142
00:41:40,480 --> 00:41:40,880
uh

1143
00:41:40,880 --> 00:41:43,839
so we are dipping into the and models

1144
00:41:43,839 --> 00:41:44,800
vendor specific

1145
00:41:44,800 --> 00:41:48,000
end models uh there are

1146
00:41:48,000 --> 00:41:51,119
we find a little inconsistent between

1147
00:41:51,119 --> 00:41:52,079
the vendors

1148
00:41:52,079 --> 00:41:54,079
it's you know but we have to pick and

1149
00:41:54,079 --> 00:41:55,359
choose the right models

1150
00:41:55,359 --> 00:41:58,480
from a vendor-specific uh perspective um

1151
00:41:58,480 --> 00:42:00,640
until we get the open config coverage

1152
00:42:00,640 --> 00:42:01,680
for all the data points

1153
00:42:01,680 --> 00:42:04,960
that you would need to see let's see

1154
00:42:04,960 --> 00:42:06,480
michael doherty is asking

1155
00:42:06,480 --> 00:42:09,280
are you integrating any rpki or prefix

1156
00:42:09,280 --> 00:42:11,200
validation into your redesign either on

1157
00:42:11,200 --> 00:42:13,520
the ingress or egress

1158
00:42:13,520 --> 00:42:16,400
so there's no change in rpk deployments

1159
00:42:16,400 --> 00:42:16,960
approach

1160
00:42:16,960 --> 00:42:18,720
we are we are still keeping how we've

1161
00:42:18,720 --> 00:42:20,240
been deploying rpki and

1162
00:42:20,240 --> 00:42:22,960
age so there's no change um with this

1163
00:42:22,960 --> 00:42:24,640
deployment and how we've been handling

1164
00:42:24,640 --> 00:42:27,520
apk development

1165
00:42:28,160 --> 00:42:32,560
okay shredder hegde is asking

1166
00:42:32,560 --> 00:42:34,480
what is the impact of bringing costs

1167
00:42:34,480 --> 00:42:35,599
into our network

1168
00:42:35,599 --> 00:42:37,520
on traffic engineering paths do you have

1169
00:42:37,520 --> 00:42:39,200
te deployed on

1170
00:42:39,200 --> 00:42:41,280
in the core and any thoughts about

1171
00:42:41,280 --> 00:42:44,400
placing pe pass in the class

1172
00:42:44,400 --> 00:42:47,520
we do not we don't have te support yet

1173
00:42:47,520 --> 00:42:48,800
but it's something we're actually

1174
00:42:48,800 --> 00:42:49,599
looking into

1175
00:42:49,599 --> 00:42:51,520
this part of the network experience is

1176
00:42:51,520 --> 00:42:53,119
doing pure ip routing

1177
00:42:53,119 --> 00:42:56,160
um now we have the topology set

1178
00:42:56,160 --> 00:42:59,440
we are looking to uh see how we could

1179
00:42:59,440 --> 00:42:59,839
use

1180
00:42:59,839 --> 00:43:02,880
sr um to solve some of the

1181
00:43:02,880 --> 00:43:04,480
key requirements that would come up in

1182
00:43:04,480 --> 00:43:06,800
future

1183
00:43:07,680 --> 00:43:10,640
let's see brian holloway is asking you

1184
00:43:10,640 --> 00:43:12,319
mentioned keeping an eye on your

1185
00:43:12,319 --> 00:43:15,359
l2 isis database are you considering

1186
00:43:15,359 --> 00:43:19,520
l1 l2 l1 or

1187
00:43:19,520 --> 00:43:21,599
did you have something else in mind it's

1188
00:43:21,599 --> 00:43:22,880
one of the options on the table that

1189
00:43:22,880 --> 00:43:24,960
we're looking into so if needed we can

1190
00:43:24,960 --> 00:43:27,520
we can break it into big l two domain

1191
00:43:27,520 --> 00:43:28,800
into l1 and l2

1192
00:43:28,800 --> 00:43:30,720
and manage the database that way it's

1193
00:43:30,720 --> 00:43:32,400
one of the options we are

1194
00:43:32,400 --> 00:43:35,839
we are considering okay and we're almost

1195
00:43:35,839 --> 00:43:36,960
out of time but we have time for a

1196
00:43:36,960 --> 00:43:38,400
couple more questions so i'll get as

1197
00:43:38,400 --> 00:43:40,560
many as i can here

1198
00:43:40,560 --> 00:43:45,280
let's see uh would you please

1199
00:43:45,280 --> 00:43:47,200
give more details on how you are

1200
00:43:47,200 --> 00:43:50,000
monitoring qos drop

1201
00:43:50,000 --> 00:43:52,319
so one of the vendor blocks that we are

1202
00:43:52,319 --> 00:43:53,440
have deployed

1203
00:43:53,440 --> 00:43:55,680
there is a data point sensor uh

1204
00:43:55,680 --> 00:43:57,440
telemetry sensor uh

1205
00:43:57,440 --> 00:44:00,560
uh data point uh sensor path available

1206
00:44:00,560 --> 00:44:02,480
to us so we are watching all the cue

1207
00:44:02,480 --> 00:44:03,440
drops

1208
00:44:03,440 --> 00:44:06,480
um for for all the queues

1209
00:44:06,480 --> 00:44:08,480
um all the qos queues we have in our

1210
00:44:08,480 --> 00:44:09,599
network

1211
00:44:09,599 --> 00:44:11,200
by looking at those cube drops we have

1212
00:44:11,200 --> 00:44:12,720
an idea what's going on is it

1213
00:44:12,720 --> 00:44:14,800
the queue drop related to a convergence

1214
00:44:14,800 --> 00:44:15,920
is it

1215
00:44:15,920 --> 00:44:17,760
something else going on in the network

1216
00:44:17,760 --> 00:44:19,839
so we are using the data point

1217
00:44:19,839 --> 00:44:23,040
to monitor the queues

1218
00:44:23,040 --> 00:44:26,640
cheers okay ronan dobbins asks can you

1219
00:44:26,640 --> 00:44:28,480
talk about the design and scale of your

1220
00:44:28,480 --> 00:44:30,240
telemetry export collection network

1221
00:44:30,240 --> 00:44:31,920
infrastructure for all the various forms

1222
00:44:31,920 --> 00:44:32,480
of

1223
00:44:32,480 --> 00:44:36,079
telemetry you're doing uh it's still

1224
00:44:36,079 --> 00:44:37,680
really early so we are learning right

1225
00:44:37,680 --> 00:44:39,440
now so we are streaming

1226
00:44:39,440 --> 00:44:43,280
uh about 100 to 200 rounds now about 100

1227
00:44:43,280 --> 00:44:44,160
plus routers

1228
00:44:44,160 --> 00:44:46,880
sending telemetry streams to a collector

1229
00:44:46,880 --> 00:44:47,520
uh

1230
00:44:47,520 --> 00:44:50,560
all uh data points are stream data

1231
00:44:50,560 --> 00:44:53,920
every uh 30 seconds some data points of

1232
00:44:53,920 --> 00:44:55,599
30 seconds are the data points of 60

1233
00:44:55,599 --> 00:44:56,400
seconds

1234
00:44:56,400 --> 00:44:59,680
so it is a lot of data we are uh

1235
00:44:59,680 --> 00:45:02,720
keeping like seven days from right hour

1236
00:45:02,720 --> 00:45:04,800
five to seven days retention so we

1237
00:45:04,800 --> 00:45:08,000
do drop those uh data points after seven

1238
00:45:08,000 --> 00:45:09,680
days after we process them

1239
00:45:09,680 --> 00:45:12,880
uh so but it's a only early stage and we

1240
00:45:12,880 --> 00:45:14,480
are still trying to understand

1241
00:45:14,480 --> 00:45:17,119
the scale we need to deploy to support

1242
00:45:17,119 --> 00:45:18,640
the telemetry to retain

1243
00:45:18,640 --> 00:45:21,920
the data longer time longer period okay

1244
00:45:21,920 --> 00:45:22,960
and one last

1245
00:45:22,960 --> 00:45:24,960
last question from blake willis any more

1246
00:45:24,960 --> 00:45:27,280
details on how you any cast bgp next

1247
00:45:27,280 --> 00:45:30,240
hops in the core please

1248
00:45:30,240 --> 00:45:34,240
sure so if you have um

1249
00:45:34,240 --> 00:45:37,440
if you have like eight um paths to an

1250
00:45:37,440 --> 00:45:39,280
ebtp session

1251
00:45:39,280 --> 00:45:42,960
an external network you would learn um

1252
00:45:42,960 --> 00:45:45,760
like all the pgp prefixes from the from

1253
00:45:45,760 --> 00:45:46,960
that bgp pair

1254
00:45:46,960 --> 00:45:48,480
and you would have eight different next

1255
00:45:48,480 --> 00:45:50,960
stop and with the add path enable

1256
00:45:50,960 --> 00:45:54,480
now you're going to send eight paths um

1257
00:45:54,480 --> 00:45:56,000
times number of prefix you would get

1258
00:45:56,000 --> 00:45:57,520
from the customer from the

1259
00:45:57,520 --> 00:46:01,440
as external ais um so this

1260
00:46:01,440 --> 00:46:03,040
multiplies you know if you have more and

1261
00:46:03,040 --> 00:46:04,800
more spines more and more external

1262
00:46:04,800 --> 00:46:05,839
appears

1263
00:46:05,839 --> 00:46:09,119
uh more prefixes coming in um

1264
00:46:09,119 --> 00:46:11,839
the number of bgp path explodes so what

1265
00:46:11,839 --> 00:46:13,359
we are doing instead we are

1266
00:46:13,359 --> 00:46:16,319
using an anycast virtual anycast ip per

1267
00:46:16,319 --> 00:46:17,440
cluster

1268
00:46:17,440 --> 00:46:19,440
and we are setting that anycast as a

1269
00:46:19,440 --> 00:46:21,680
next stop bgp next up so now we go from

1270
00:46:21,680 --> 00:46:23,040
my eight paths

1271
00:46:23,040 --> 00:46:26,000
to one path with any cast which we next

1272
00:46:26,000 --> 00:46:27,280
start

1273
00:46:27,280 --> 00:46:29,119
so that way we were able to control the

1274
00:46:29,119 --> 00:46:31,520
bgp paths in our core network with this

1275
00:46:31,520 --> 00:46:34,240
class deployments

1276
00:46:34,240 --> 00:46:36,000
all right thank you um we're out of time

1277
00:46:36,000 --> 00:46:37,920
for questions but uh if

1278
00:46:37,920 --> 00:46:39,760
any of you have anything else uh please

1279
00:46:39,760 --> 00:46:41,680
continue in the chat or

1280
00:46:41,680 --> 00:46:50,720
contact him offline

