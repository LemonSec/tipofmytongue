1
00:00:16,939 --> 00:00:22,250
great good morning everyone so my talk

2
00:00:20,820 --> 00:00:24,090
towards hyperscale

3
00:00:22,250 --> 00:00:31,650
high-performance computing with our DMA

4
00:00:24,090 --> 00:00:34,739
builds upon largest talk here's our

5
00:00:31,650 --> 00:00:36,600
outline it's pretty simple so it's going

6
00:00:34,739 --> 00:00:38,250
to continue what he started I'm going to

7
00:00:36,600 --> 00:00:40,979
talk about some of the drivers for

8
00:00:38,250 --> 00:00:42,989
bringing our email into the data center

9
00:00:40,979 --> 00:00:44,670
and the hyper scale data center

10
00:00:42,989 --> 00:00:46,379
environments some of the trends and

11
00:00:44,670 --> 00:00:48,839
directions as in the technologies that

12
00:00:46,379 --> 00:00:51,360
are causing this type and similar

13
00:00:48,839 --> 00:00:53,549
technologies to this to be used in the

14
00:00:51,360 --> 00:00:56,100
data center challenges with running it

15
00:00:53,549 --> 00:00:59,010
at scale and then some of the examples

16
00:00:56,100 --> 00:01:00,869
here for the things that you is network

17
00:00:59,010 --> 00:01:03,570
operators should be concerned about and

18
00:01:00,869 --> 00:01:06,119
aware of and also talk about some of the

19
00:01:03,570 --> 00:01:10,250
problems and some of the comparisons of

20
00:01:06,119 --> 00:01:10,250
the different implementations of our DMA

21
00:01:11,360 --> 00:01:17,369
so a large data center huge collection

22
00:01:15,420 --> 00:01:19,020
of computers they can be small regional

23
00:01:17,369 --> 00:01:22,680
data center it could be hyper scale data

24
00:01:19,020 --> 00:01:25,679
center think a WAG CP or Azure for

25
00:01:22,680 --> 00:01:28,320
example these types of orion's give you

26
00:01:25,679 --> 00:01:31,500
very cheap compute it's very easy to

27
00:01:28,320 --> 00:01:33,298
deploy it's very high performance and

28
00:01:31,500 --> 00:01:36,140
increasingly we're seeing high

29
00:01:33,299 --> 00:01:40,140
performance workloads migrating to these

30
00:01:36,140 --> 00:01:42,450
data centers and clouds in this slide

31
00:01:40,140 --> 00:01:44,670
here what i show is the top 500

32
00:01:42,450 --> 00:01:47,490
supercomputers if you guys are familiar

33
00:01:44,670 --> 00:01:48,780
with it it's a it's a list that gets

34
00:01:47,490 --> 00:01:51,089
published by the university of tennessee

35
00:01:48,780 --> 00:01:52,679
twice a year and it lists the top 500

36
00:01:51,090 --> 00:01:54,719
supercomputers and it lists all the

37
00:01:52,679 --> 00:01:57,569
characteristics of what i'm highlighting

38
00:01:54,719 --> 00:02:00,329
here is in the top slide that half of

39
00:01:57,569 --> 00:02:02,670
the interconnect technology and that

40
00:02:00,329 --> 00:02:05,298
space which has traditionally been HPC

41
00:02:02,670 --> 00:02:10,860
based InfiniBand or InfiniBand derived

42
00:02:05,299 --> 00:02:13,050
is pretty much moving quite a ways

43
00:02:10,860 --> 00:02:15,120
towards standard Ethernet so 50 percent

44
00:02:13,050 --> 00:02:17,280
of that slide is green 50 percent of

45
00:02:15,120 --> 00:02:21,569
that slide means that's Ethernet fabric

46
00:02:17,280 --> 00:02:25,200
your standard custom 802 dot

47
00:02:21,569 --> 00:02:27,569
to Ethernet fabric okay now on those

48
00:02:25,200 --> 00:02:29,518
fabrics you could be running rocky in

49
00:02:27,569 --> 00:02:34,170
most cases it is rocky not I work per se

50
00:02:29,519 --> 00:02:36,060
because of scalability and in general

51
00:02:34,170 --> 00:02:37,980
these are the standard already made

52
00:02:36,060 --> 00:02:41,810
technology that you can deploy either in

53
00:02:37,980 --> 00:02:41,810
the cloud or on your on-prem datacenter

54
00:02:42,230 --> 00:02:49,738
our team a does not equal rocky and I'll

55
00:02:47,250 --> 00:02:51,989
talk about that in later slide Rocky is

56
00:02:49,739 --> 00:02:53,340
an implementation of our DMA I war

57
00:02:51,989 --> 00:02:55,319
business have an implementation of our

58
00:02:53,340 --> 00:02:58,590
DMA and a lot of the items that you see

59
00:02:55,319 --> 00:03:04,379
on the left hand side of the top pie

60
00:02:58,590 --> 00:03:08,459
chart are derivations of our DMA so

61
00:03:04,379 --> 00:03:09,929
hyperscale it's no secret that this is a

62
00:03:08,459 --> 00:03:11,750
space that's growing like crazy

63
00:03:09,930 --> 00:03:14,340
the major cloud providers have

64
00:03:11,750 --> 00:03:15,900
incredible size quarter-over-quarter

65
00:03:14,340 --> 00:03:19,079
they're just employing tons and tons of

66
00:03:15,900 --> 00:03:21,419
systems and how do you build those

67
00:03:19,079 --> 00:03:24,120
environments well you have to keep it

68
00:03:21,419 --> 00:03:26,400
cheap you want to use commodity parts

69
00:03:24,120 --> 00:03:28,560
you want to use commodity switches you

70
00:03:26,400 --> 00:03:30,750
want to use simple devices simple

71
00:03:28,560 --> 00:03:32,669
knixwear possible not really applicable

72
00:03:30,750 --> 00:03:33,989
in the NIC case you actually require

73
00:03:32,669 --> 00:03:35,579
server NICs for a lot of these workloads

74
00:03:33,989 --> 00:03:38,489
but you try to keep all of the

75
00:03:35,579 --> 00:03:44,489
components simple right simple and

76
00:03:38,489 --> 00:03:47,579
replaceable in these data centers we're

77
00:03:44,489 --> 00:03:48,680
seeing in increasing growth of new

78
00:03:47,579 --> 00:03:52,129
devices coming online

79
00:03:48,680 --> 00:03:55,530
so traditionally storage has been slow

80
00:03:52,129 --> 00:03:58,739
GPUs weren't really a an offload thing

81
00:03:55,530 --> 00:04:00,930
until recently and FPGAs are now

82
00:03:58,739 --> 00:04:03,510
becoming much more common in terms of

83
00:04:00,930 --> 00:04:05,489
offloading work now in all of those

84
00:04:03,510 --> 00:04:07,948
cases those are accelerators

85
00:04:05,489 --> 00:04:10,409
accelerators that bring down your your

86
00:04:07,949 --> 00:04:13,879
compute costs as in it's more efficient

87
00:04:10,409 --> 00:04:16,560
to do your processing and networking is

88
00:04:13,879 --> 00:04:17,699
surprisingly now the slow part no matter

89
00:04:16,560 --> 00:04:19,440
how much bandwidth you throw out the

90
00:04:17,699 --> 00:04:24,389
problem networking is still the slow

91
00:04:19,440 --> 00:04:26,820
part in storage scenarios we see that

92
00:04:24,389 --> 00:04:29,130
with the advent of SSDs and storage

93
00:04:26,820 --> 00:04:32,310
class memory Network latency accounts

94
00:04:29,130 --> 00:04:34,120
for over 50% of the latency costs for

95
00:04:32,310 --> 00:04:36,760
the operations

96
00:04:34,120 --> 00:04:39,190
in the GPU case and FPGA cases it's the

97
00:04:36,760 --> 00:04:40,840
same thing my compute is now fully

98
00:04:39,190 --> 00:04:43,690
offloaded it's incredibly fast and

99
00:04:40,840 --> 00:04:49,448
moving data around to the accelerators

100
00:04:43,690 --> 00:04:54,040
is what's causing most of my latency so

101
00:04:49,449 --> 00:04:55,960
here's an example putting in numbers we

102
00:04:54,040 --> 00:04:58,330
see on the left hand side your standard

103
00:04:55,960 --> 00:05:01,448
hard disk the spinning disk of rust

104
00:04:58,330 --> 00:05:03,880
that's quite slow in the tens of

105
00:05:01,449 --> 00:05:10,320
milliseconds access times and over time

106
00:05:03,880 --> 00:05:13,090
we've moved to what you see here nvme

107
00:05:10,320 --> 00:05:15,849
devices over stores class memory in

108
00:05:13,090 --> 00:05:18,820
which case the operations to the disk

109
00:05:15,850 --> 00:05:21,400
are extremely fast it's a to microsecond

110
00:05:18,820 --> 00:05:22,719
access time and everything else is

111
00:05:21,400 --> 00:05:24,760
really software or communication

112
00:05:22,720 --> 00:05:26,500
overhead if we take the case of

113
00:05:24,760 --> 00:05:28,990
accessing that device over the fabric

114
00:05:26,500 --> 00:05:31,800
and nvme over fabrics which is

115
00:05:28,990 --> 00:05:35,680
essentially already made directly to SEM

116
00:05:31,800 --> 00:05:37,690
we see that there's a small Delta

117
00:05:35,680 --> 00:05:39,760
increase and the access time because

118
00:05:37,690 --> 00:05:41,680
you're accessing over to the fabric but

119
00:05:39,760 --> 00:05:44,289
it's nowhere near what it was before in

120
00:05:41,680 --> 00:05:47,860
terms of millisecond times so in both of

121
00:05:44,289 --> 00:05:51,840
these cases we're talking sub ten

122
00:05:47,860 --> 00:05:55,090
microseconds or similar access times a

123
00:05:51,840 --> 00:05:57,969
standard application running over kernel

124
00:05:55,090 --> 00:06:00,310
mediated or host mated IO is simply not

125
00:05:57,970 --> 00:06:01,960
going to be able to achieve those legacy

126
00:06:00,310 --> 00:06:03,880
numbers which means that if you have

127
00:06:01,960 --> 00:06:05,919
those accelerators and those devices you

128
00:06:03,880 --> 00:06:09,969
can't actually make use of them unless

129
00:06:05,919 --> 00:06:13,659
we have a a a pipe a facility or a tool

130
00:06:09,970 --> 00:06:15,490
as in our DMA or similar direct access

131
00:06:13,660 --> 00:06:22,870
technologies to actually access those

132
00:06:15,490 --> 00:06:26,620
devices here's an example from your

133
00:06:22,870 --> 00:06:29,020
typical HPC scenario so on the left hand

134
00:06:26,620 --> 00:06:32,349
side you know you have your typical MPI

135
00:06:29,020 --> 00:06:34,419
job job starts this chip it's a bunch of

136
00:06:32,349 --> 00:06:35,860
work to all the worker nodes which means

137
00:06:34,419 --> 00:06:37,870
a bunch of copies are going all over

138
00:06:35,860 --> 00:06:40,419
your fabric all those worker nodes are

139
00:06:37,870 --> 00:06:43,030
processing at the same time right and

140
00:06:40,419 --> 00:06:44,890
once they complete a job as in job

141
00:06:43,030 --> 00:06:47,469
completion time that's when the job is

142
00:06:44,890 --> 00:06:50,229
done any stragglers slow

143
00:06:47,470 --> 00:06:53,260
processes Lacy's drops congestions

144
00:06:50,230 --> 00:06:56,080
delays is going to delay your job

145
00:06:53,260 --> 00:06:59,409
completion time from a single processor

146
00:06:56,080 --> 00:07:01,990
point of view write an application point

147
00:06:59,410 --> 00:07:04,630
of view what you see is if there's a

148
00:07:01,990 --> 00:07:07,630
drop or if there's congestion you have a

149
00:07:04,630 --> 00:07:09,100
stall in the pipeline yes my processor

150
00:07:07,630 --> 00:07:11,110
can go off and do something else that's

151
00:07:09,100 --> 00:07:13,780
what the scheduler takes care of the

152
00:07:11,110 --> 00:07:16,270
problem is an HPC type workloads you

153
00:07:13,780 --> 00:07:18,190
typically don't run dents multi-tenancy

154
00:07:16,270 --> 00:07:20,530
you typically have a virtual machine

155
00:07:18,190 --> 00:07:23,260
that takes the entire physical machine

156
00:07:20,530 --> 00:07:25,659
or you have a physical machine dedicate

157
00:07:23,260 --> 00:07:28,060
it to just the HPC job so any delays

158
00:07:25,660 --> 00:07:29,320
there whichever which way they are are

159
00:07:28,060 --> 00:07:32,560
going to impact the application

160
00:07:29,320 --> 00:07:34,120
performance if you take the example of

161
00:07:32,560 --> 00:07:36,160
that single processor and you scale it

162
00:07:34,120 --> 00:07:38,500
up to multiple processors then that just

163
00:07:36,160 --> 00:07:40,450
reinforces the point that it's really

164
00:07:38,500 --> 00:07:42,370
the tail completion time that determines

165
00:07:40,450 --> 00:07:44,440
the job completion time and the actual

166
00:07:42,370 --> 00:07:48,340
cost so one has to say why is that

167
00:07:44,440 --> 00:07:50,590
important to put it into numbers a lot

168
00:07:48,340 --> 00:07:53,200
of these HPC applications think of fluid

169
00:07:50,590 --> 00:07:55,780
dynamics for example you're being

170
00:07:53,200 --> 00:07:57,669
charged not on a per core basis but on a

171
00:07:55,780 --> 00:08:00,219
per time basis for your usage of the

172
00:07:57,669 --> 00:08:02,200
application in some cases it can be in

173
00:08:00,220 --> 00:08:05,050
the hundreds of thousands of dollars so

174
00:08:02,200 --> 00:08:07,570
it's of the utmost interest that your

175
00:08:05,050 --> 00:08:08,760
job completion time be as absolutely

176
00:08:07,570 --> 00:08:11,409
small as possible

177
00:08:08,760 --> 00:08:16,690
microseconds absolutely matter in these

178
00:08:11,410 --> 00:08:20,650
types of workloads here's an example

179
00:08:16,690 --> 00:08:22,870
from storage very similar I'm doing a

180
00:08:20,650 --> 00:08:23,440
bunch of transfers one of the transfers

181
00:08:22,870 --> 00:08:25,300
stalls

182
00:08:23,440 --> 00:08:27,820
there's drops there's congestion there's

183
00:08:25,300 --> 00:08:31,300
a delay in the network and you can't get

184
00:08:27,820 --> 00:08:33,520
the data out to meet the sls so this is

185
00:08:31,300 --> 00:08:34,990
a problem it's simply another spin from

186
00:08:33,520 --> 00:08:40,838
the same thing i described earlier for

187
00:08:34,990 --> 00:08:42,280
HPC put in the storage context so I've

188
00:08:40,839 --> 00:08:43,630
described some of the trends some of the

189
00:08:42,280 --> 00:08:46,750
technologies and some of the issues that

190
00:08:43,630 --> 00:08:48,640
are forcing a need to have extremely low

191
00:08:46,750 --> 00:08:51,750
and consistent latency in the data

192
00:08:48,640 --> 00:08:54,610
center so how can we achieve that

193
00:08:51,750 --> 00:08:56,680
already may is one example or DMA simply

194
00:08:54,610 --> 00:08:58,839
means remote direct memory access it

195
00:08:56,680 --> 00:09:01,120
means I have two endpoints where my

196
00:08:58,839 --> 00:09:03,459
layer four and layer

197
00:09:01,120 --> 00:09:06,009
three are fully offloaded onto the Nick

198
00:09:03,459 --> 00:09:07,628
the application is simply managing a set

199
00:09:06,009 --> 00:09:10,180
of cues and submitting work requests to

200
00:09:07,629 --> 00:09:11,620
execute those transfers and everything

201
00:09:10,180 --> 00:09:14,739
else is taken care of under the covers

202
00:09:11,620 --> 00:09:17,620
by the physical mech the data from the

203
00:09:14,740 --> 00:09:20,079
application is placed directly into the

204
00:09:17,620 --> 00:09:23,259
application meaning that the application

205
00:09:20,079 --> 00:09:27,550
will register the exact memory where he

206
00:09:23,259 --> 00:09:29,649
wants to write from or read - there are

207
00:09:27,550 --> 00:09:31,300
no intermediate data copies there's no

208
00:09:29,649 --> 00:09:33,639
middleman there's no host mediated i/o

209
00:09:31,300 --> 00:09:39,099
and I'll discuss that in more detail

210
00:09:33,639 --> 00:09:41,439
later slides now in the top part of this

211
00:09:39,100 --> 00:09:43,779
diagram this is your standard sockets

212
00:09:41,439 --> 00:09:46,240
communication the key things to keep in

213
00:09:43,779 --> 00:09:48,100
mind here is standard talk and

214
00:09:46,240 --> 00:09:50,170
communication from user space requires a

215
00:09:48,100 --> 00:09:52,360
context switch into the kernel and an

216
00:09:50,170 --> 00:09:54,639
Associated data copy and protocol

217
00:09:52,360 --> 00:09:56,439
processing those are the three exact

218
00:09:54,639 --> 00:09:58,660
things that already Mae eliminates

219
00:09:56,439 --> 00:10:00,430
there's no longer any need to break into

220
00:09:58,660 --> 00:10:02,379
the kernel because user space is

221
00:10:00,430 --> 00:10:03,819
managing the hardware resources the key

222
00:10:02,379 --> 00:10:06,189
person submitting the work request and

223
00:10:03,819 --> 00:10:07,779
completions directly there's no need to

224
00:10:06,189 --> 00:10:09,610
do protocol processing because it's been

225
00:10:07,779 --> 00:10:11,800
fully offloaded onto the Nick as in the

226
00:10:09,610 --> 00:10:14,559
next you take care of any retransmits or

227
00:10:11,800 --> 00:10:17,859
any transport level reliable

228
00:10:14,559 --> 00:10:20,079
connectivity operations and there's no

229
00:10:17,860 --> 00:10:21,339
buffer copies because you're not going

230
00:10:20,079 --> 00:10:22,628
into the kernel you're not taking the

231
00:10:21,339 --> 00:10:29,230
context switch and the adapters say you

232
00:10:22,629 --> 00:10:31,290
care of everything for you so to put it

233
00:10:29,230 --> 00:10:34,029
into more context here's an example

234
00:10:31,290 --> 00:10:36,059
given to applications with the same

235
00:10:34,029 --> 00:10:38,829
fabric same device same adapter

236
00:10:36,059 --> 00:10:41,050
obviously fine-tuned to be co-located on

237
00:10:38,829 --> 00:10:42,729
the same switch you should see in a

238
00:10:41,050 --> 00:10:44,559
well-tuned environment about 30

239
00:10:42,730 --> 00:10:46,720
microseconds for TCP access time for

240
00:10:44,559 --> 00:10:49,990
transfers right so it'd be 30

241
00:10:46,720 --> 00:10:51,550
microseconds round-trip time okay if

242
00:10:49,990 --> 00:10:53,079
you're doing it through already you may

243
00:10:51,550 --> 00:10:55,240
from application accessing the resources

244
00:10:53,079 --> 00:10:57,459
directly you are down to one microsecond

245
00:10:55,240 --> 00:10:59,769
and in many cases depending on vendor

246
00:10:57,459 --> 00:11:02,559
specific extensions to your or DMA

247
00:10:59,769 --> 00:11:08,350
operations you can be even below one

248
00:11:02,559 --> 00:11:10,059
microsecond of course as you scale the

249
00:11:08,350 --> 00:11:13,509
number of switch hops that you have and

250
00:11:10,059 --> 00:11:14,529
the bandwidth delay product or distance

251
00:11:13,509 --> 00:11:16,660
that your

252
00:11:14,529 --> 00:11:18,579
trying to talk to is really going to

253
00:11:16,660 --> 00:11:19,629
determine what your lower bounds are so

254
00:11:18,579 --> 00:11:26,199
ultimately you're always going to be

255
00:11:19,629 --> 00:11:29,980
bound by physics so I mentioned earlier

256
00:11:26,199 --> 00:11:32,559
that Rocky is not already ma Rocky's

257
00:11:29,980 --> 00:11:34,149
simply an implementation of our DMA and

258
00:11:32,559 --> 00:11:36,819
this is a variation of the slide

259
00:11:34,149 --> 00:11:40,629
provided in the previous talk and what

260
00:11:36,819 --> 00:11:42,279
we see here is that the infinite band

261
00:11:40,629 --> 00:11:45,069
which has been to traditional HPC

262
00:11:42,279 --> 00:11:47,740
mechanism is really where most of this

263
00:11:45,069 --> 00:11:49,479
technology came from and that in turn

264
00:11:47,740 --> 00:11:53,350
evolved from other technologies before

265
00:11:49,480 --> 00:11:57,009
that now in the InfiniBand case there's

266
00:11:53,350 --> 00:11:59,139
a layer two that is credit-based so what

267
00:11:57,009 --> 00:12:01,959
you would consider a layer four as in

268
00:11:59,139 --> 00:12:05,309
the infinite band transport layer has

269
00:12:01,959 --> 00:12:07,410
the liberty of being quite simple and

270
00:12:05,309 --> 00:12:12,279
inefficient because we've got simplicity

271
00:12:07,410 --> 00:12:15,009
because it can rely on a layer two that

272
00:12:12,279 --> 00:12:17,559
guarantees that there is no loss on the

273
00:12:15,009 --> 00:12:20,230
fabric as that technology moves over on

274
00:12:17,559 --> 00:12:21,370
top of Ethernet we see here in Rocky V

275
00:12:20,230 --> 00:12:24,939
one which was the first implementation

276
00:12:21,370 --> 00:12:27,819
of it simply replaced layer two with

277
00:12:24,939 --> 00:12:29,949
Ethernet and kept layer three as is

278
00:12:27,819 --> 00:12:33,729
interestingly layer three friend finna

279
00:12:29,949 --> 00:12:36,430
band has the same exact frame or header

280
00:12:33,730 --> 00:12:38,290
format as ipv6 with really the only

281
00:12:36,430 --> 00:12:41,979
difference is that the the next header

282
00:12:38,290 --> 00:12:50,620
values conflict with the IETF or or ima

283
00:12:41,980 --> 00:12:53,290
defined values in in Rocky V 2 then the

284
00:12:50,620 --> 00:12:55,449
IP fabric became more standard meaning I

285
00:12:53,290 --> 00:12:58,389
didn't need any special sauce I have

286
00:12:55,449 --> 00:13:01,059
full IP interoperability and UDP simply

287
00:12:58,389 --> 00:13:02,379
gets used as a tunnel so even though UDP

288
00:13:01,059 --> 00:13:03,819
is traditionally or later for that's

289
00:13:02,379 --> 00:13:05,679
unreliable in this case it's really just

290
00:13:03,819 --> 00:13:07,029
acting as a tunnel which is not a novel

291
00:13:05,679 --> 00:13:09,850
concept this has been used before it's

292
00:13:07,029 --> 00:13:12,249
the basic concept behind VX lab right

293
00:13:09,850 --> 00:13:14,019
where you have basically multi-level

294
00:13:12,249 --> 00:13:16,860
nested virtualization network

295
00:13:14,019 --> 00:13:20,079
virtualization and your actual you know

296
00:13:16,860 --> 00:13:21,579
transport level operations for reliable

297
00:13:20,079 --> 00:13:24,219
connectivity are contained in your inner

298
00:13:21,579 --> 00:13:26,620
frame now your outer frame here it does

299
00:13:24,220 --> 00:13:28,120
exactly the same it simply uses UDP as a

300
00:13:26,620 --> 00:13:29,950
tunnel with a well known destination

301
00:13:28,120 --> 00:13:31,120
port and the actual transport that's

302
00:13:29,950 --> 00:13:33,070
taking care of your reliable

303
00:13:31,120 --> 00:13:35,279
connectivity is really sitting up right

304
00:13:33,070 --> 00:13:38,800
above that in the InfiniBand transport

305
00:13:35,279 --> 00:13:40,870
so that's one implementation of our DMA

306
00:13:38,800 --> 00:13:44,109
another implementation of our DMA is

307
00:13:40,870 --> 00:13:47,170
Iowa surprisingly I work does not mean

308
00:13:44,110 --> 00:13:49,029
in a net wide Rema protocol that's it's

309
00:13:47,170 --> 00:13:50,319
a fallacy I know the folks who came up

310
00:13:49,029 --> 00:13:52,480
with this stuff and they say that that

311
00:13:50,320 --> 00:13:55,270
is absolutely not true

312
00:13:52,480 --> 00:13:59,410
I work simply is our DMA operations

313
00:13:55,270 --> 00:14:02,920
running over tcp/ip and in this case you

314
00:13:59,410 --> 00:14:05,170
have your standard tried-and-true tcp/ip

315
00:14:02,920 --> 00:14:07,930
congestion control algorithms of course

316
00:14:05,170 --> 00:14:10,300
implemented in vendor specific ways it

317
00:14:07,930 --> 00:14:13,150
has selective acknowledgment it has fast

318
00:14:10,300 --> 00:14:15,099
retransmits and it has some benefits

319
00:14:13,150 --> 00:14:16,600
that rocky doesn't have both

320
00:14:15,100 --> 00:14:18,940
technologies and implementations have

321
00:14:16,600 --> 00:14:20,890
their pros and cons as network operators

322
00:14:18,940 --> 00:14:22,630
you need to be aware of what these are I

323
00:14:20,890 --> 00:14:24,069
encourage you to look at some of the

324
00:14:22,630 --> 00:14:27,460
slides in the backup section that have a

325
00:14:24,070 --> 00:14:28,960
lot has a lot of references as to you

326
00:14:27,460 --> 00:14:31,300
know the the courts and differences

327
00:14:28,960 --> 00:14:33,490
between them now

328
00:14:31,300 --> 00:14:36,120
one thing to keep in mind I mentioned

329
00:14:33,490 --> 00:14:42,790
earlier that the InfiniBand layer is

330
00:14:36,120 --> 00:14:44,020
lossless when we bring rocky or the

331
00:14:42,790 --> 00:14:46,900
InfiniBand transfer on top of Ethernet

332
00:14:44,020 --> 00:14:49,329
we also have to maintain that lossless

333
00:14:46,900 --> 00:14:50,890
Ness and that's achieved through data

334
00:14:49,330 --> 00:14:52,959
center bridging specifically priority

335
00:14:50,890 --> 00:14:55,120
flow control enhance transmission

336
00:14:52,959 --> 00:14:57,130
selection and explicit congestion

337
00:14:55,120 --> 00:15:00,370
notification and I'll talk about all of

338
00:14:57,130 --> 00:15:01,750
those in more detail now are the things

339
00:15:00,370 --> 00:15:05,950
you'll want to be aware of between these

340
00:15:01,750 --> 00:15:07,540
two technologies there is a there's a

341
00:15:05,950 --> 00:15:09,310
lot of philosophical debate as to which

342
00:15:07,540 --> 00:15:11,140
one is better than the other honestly

343
00:15:09,310 --> 00:15:14,229
they both have their pros they both have

344
00:15:11,140 --> 00:15:17,110
their cons to give you an example in

345
00:15:14,230 --> 00:15:19,900
many cases it stated that Rocky is

346
00:15:17,110 --> 00:15:21,120
simpler than I work it depends on which

347
00:15:19,900 --> 00:15:24,520
point of view you're looking at it

348
00:15:21,120 --> 00:15:27,670
because from a basic implementation

349
00:15:24,520 --> 00:15:29,110
point of view Rocky is going to be

350
00:15:27,670 --> 00:15:31,329
simpler it's a very simple transport

351
00:15:29,110 --> 00:15:32,370
there's not a lot of stuff to code in

352
00:15:31,330 --> 00:15:35,950
silicon for that

353
00:15:32,370 --> 00:15:38,440
however it's because really the onus of

354
00:15:35,950 --> 00:15:41,020
making that stuff work

355
00:15:38,440 --> 00:15:42,490
having full and in reliability now falls

356
00:15:41,020 --> 00:15:44,260
on the operator not the implementer

357
00:15:42,490 --> 00:15:46,120
which means it would be folks like you

358
00:15:44,260 --> 00:15:47,500
guys that become impacted when this type

359
00:15:46,120 --> 00:15:48,910
of technology gets deployed because it

360
00:15:47,500 --> 00:15:50,860
requires a lot more hand-holding a lot

361
00:15:48,910 --> 00:15:53,620
more a lot more tender loving care and

362
00:15:50,860 --> 00:15:56,800
configuring PFC in a perfect way and

363
00:15:53,620 --> 00:15:59,680
configuring ECM in the perfect way in

364
00:15:56,800 --> 00:16:03,099
the case of I warp it doesn't actually

365
00:15:59,680 --> 00:16:04,540
require any of those technologies et s

366
00:16:03,100 --> 00:16:06,040
for example enhanced transmission

367
00:16:04,540 --> 00:16:08,199
selection which is bandwidth reservation

368
00:16:06,040 --> 00:16:11,680
is equally applicable to both of them it

369
00:16:08,200 --> 00:16:13,810
isn't really a requirement for I warp

370
00:16:11,680 --> 00:16:23,229
however it is absolutely a requirement

371
00:16:13,810 --> 00:16:27,910
for rocky so here's a basic Rocky

372
00:16:23,230 --> 00:16:32,620
Network to give you an example so if you

373
00:16:27,910 --> 00:16:35,410
have sorry if you have a flow and you

374
00:16:32,620 --> 00:16:38,230
detect congestion on the fabric in

375
00:16:35,410 --> 00:16:39,939
standard ECM parlance it's going to do

376
00:16:38,230 --> 00:16:42,900
forward congestion notification on

377
00:16:39,940 --> 00:16:46,000
frames that are transmitting or transfer

378
00:16:42,900 --> 00:16:47,860
transitioning any middleboxes that's

379
00:16:46,000 --> 00:16:50,170
going to reach a destination the

380
00:16:47,860 --> 00:16:52,930
destination is then going to detect that

381
00:16:50,170 --> 00:16:55,750
congestion was seen somewhere along that

382
00:16:52,930 --> 00:16:59,589
path and it's going to in the tcp/ip

383
00:16:55,750 --> 00:17:01,360
sense piggyback a message back as part

384
00:16:59,590 --> 00:17:03,460
of the ACK - indicates to the source

385
00:17:01,360 --> 00:17:06,910
that it needs to throttle or source

386
00:17:03,460 --> 00:17:11,280
quench in the RDMA case it doesn't

387
00:17:06,910 --> 00:17:13,750
actually trigger back a act based

388
00:17:11,280 --> 00:17:15,490
piggyback it simply generates what's

389
00:17:13,750 --> 00:17:19,780
called a congestion notification packet

390
00:17:15,490 --> 00:17:21,910
that will essentially source quench the

391
00:17:19,780 --> 00:17:24,609
specific sending queue pair on the

392
00:17:21,910 --> 00:17:27,100
source and it seems to throttle and slow

393
00:17:24,609 --> 00:17:29,350
down the traffic so as to not accessor

394
00:17:27,099 --> 00:17:33,219
baits the congestion that was affected

395
00:17:29,350 --> 00:17:36,490
in the first place now ecn is great

396
00:17:33,220 --> 00:17:38,470
because it can help you throttle before

397
00:17:36,490 --> 00:17:42,520
you get to the worst case which is

398
00:17:38,470 --> 00:17:45,610
stalls stalls introduced by PFC PFC is

399
00:17:42,520 --> 00:17:49,810
the worst case scenario meaning ok now I

400
00:17:45,610 --> 00:17:52,300
actually have to pause and what becomes

401
00:17:49,810 --> 00:17:53,580
even worse in the PFC case is

402
00:17:52,300 --> 00:17:56,620
you trigger head-of-line blocking

403
00:17:53,580 --> 00:17:58,210
meaning that if you have a middle box so

404
00:17:56,620 --> 00:18:03,100
think of the tour right before your

405
00:17:58,210 --> 00:18:05,860
physical machine if that's a grass port

406
00:18:03,100 --> 00:18:08,530
to the host becomes congested and that

407
00:18:05,860 --> 00:18:10,689
congestion doesn't clear and ingress

408
00:18:08,530 --> 00:18:13,090
traffic continues coming in it's going

409
00:18:10,690 --> 00:18:15,130
to cause pauses on the ingress port of

410
00:18:13,090 --> 00:18:16,720
that switch and if the same thing

411
00:18:15,130 --> 00:18:18,850
repeats it's going to repeat the same

412
00:18:16,720 --> 00:18:21,280
thing to the next hub and the next hub

413
00:18:18,850 --> 00:18:22,840
and the next up you could potentially

414
00:18:21,280 --> 00:18:24,700
end up with fabric lock up in victim

415
00:18:22,840 --> 00:18:26,169
flows meaning that flows that were

416
00:18:24,700 --> 00:18:28,780
choose in transitioning any of those

417
00:18:26,170 --> 00:18:30,340
middle boxes on those ingress cues or

418
00:18:28,780 --> 00:18:31,690
ports for those switches they have

419
00:18:30,340 --> 00:18:33,429
nothing to do with the destination

420
00:18:31,690 --> 00:18:42,130
they're actually trying to reach will

421
00:18:33,430 --> 00:18:44,020
essentially become blocked and ETS I'll

422
00:18:42,130 --> 00:18:45,520
describe this a bit later but ETS

423
00:18:44,020 --> 00:18:48,129
becomes important in a hyper-converged

424
00:18:45,520 --> 00:18:50,320
fabric if you're running our DMA traffic

425
00:18:48,130 --> 00:18:53,140
with something else think tenant traffic

426
00:18:50,320 --> 00:18:55,780
or or cluster traffic or anything else

427
00:18:53,140 --> 00:18:57,070
that's not our DMA you essentially need

428
00:18:55,780 --> 00:18:58,480
to carve out the bandwidth as in

429
00:18:57,070 --> 00:19:05,560
reserved the minimum bandwidth per

430
00:18:58,480 --> 00:19:08,440
traffic class so congestion control and

431
00:19:05,560 --> 00:19:10,090
Rocking this is important because this

432
00:19:08,440 --> 00:19:11,890
is the part where you guys can become

433
00:19:10,090 --> 00:19:17,649
you know affected by having to augment

434
00:19:11,890 --> 00:19:21,120
with layer 2 mechanisms in congestion

435
00:19:17,650 --> 00:19:24,250
control in rocky it's a it's simplistic

436
00:19:21,120 --> 00:19:26,290
let's take a very you know exaggerated

437
00:19:24,250 --> 00:19:29,230
case and already in may I can transfer

438
00:19:26,290 --> 00:19:30,820
up to 2 gigabytes - 1 byte in a single

439
00:19:29,230 --> 00:19:33,730
transfer that means my application can

440
00:19:30,820 --> 00:19:36,250
say hey send this as in a single

441
00:19:33,730 --> 00:19:39,430
operation and he can literally transfer

442
00:19:36,250 --> 00:19:41,560
2 gigabytes - 1 byte of data just stream

443
00:19:39,430 --> 00:19:43,600
it to the other destination the way does

444
00:19:41,560 --> 00:19:45,580
that is it doesn't actually do very

445
00:19:43,600 --> 00:19:46,719
elaborate sampling for end to end to

446
00:19:45,580 --> 00:19:48,370
determine if he's gonna overload the

447
00:19:46,720 --> 00:19:50,680
fabric or not it doesn't it just starts

448
00:19:48,370 --> 00:19:52,149
to send a lot of traffic at once and it

449
00:19:50,680 --> 00:19:55,360
expects layer 2 to be able to manage

450
00:19:52,150 --> 00:19:56,590
that if there is congestion or there are

451
00:19:55,360 --> 00:19:58,780
drops or there's a head-of-line blocking

452
00:19:56,590 --> 00:20:01,330
case or any of the examples that I gave

453
00:19:58,780 --> 00:20:03,850
previously it will essentially go back

454
00:20:01,330 --> 00:20:06,159
and retransmit from the starting points

455
00:20:03,850 --> 00:20:08,740
of which case the optimization

456
00:20:06,160 --> 00:20:10,630
was made to actually do a version of

457
00:20:08,740 --> 00:20:13,200
checkpointing in between those two

458
00:20:10,630 --> 00:20:16,840
gigabytes which says hey maybe for every

459
00:20:13,200 --> 00:20:18,310
256 K or you know or you know 16

460
00:20:16,840 --> 00:20:20,439
megabytes or whatever size it was

461
00:20:18,310 --> 00:20:22,600
let me just retransmit from that point

462
00:20:20,440 --> 00:20:24,160
so now it becomes the onus on the

463
00:20:22,600 --> 00:20:25,480
operator to configure those Knicks in a

464
00:20:24,160 --> 00:20:28,030
specific way to determine what

465
00:20:25,480 --> 00:20:31,840
checkpoint he wants to determine what

466
00:20:28,030 --> 00:20:35,770
the retransmission amount is that's a

467
00:20:31,840 --> 00:20:38,470
big complex you know this is an area

468
00:20:35,770 --> 00:20:40,420
that requires a bit more innovation it's

469
00:20:38,470 --> 00:20:41,110
definitely one of the blockers for a

470
00:20:40,420 --> 00:20:44,530
scalability

471
00:20:41,110 --> 00:20:46,469
it requires a significant complexity on

472
00:20:44,530 --> 00:20:49,210
fabric configuration to make sure that

473
00:20:46,470 --> 00:20:52,480
you have a lossless fabric and that this

474
00:20:49,210 --> 00:20:54,580
doesn't happen now there's also the

475
00:20:52,480 --> 00:20:57,130
concept of wide lock if you take this

476
00:20:54,580 --> 00:20:58,659
example of a partitioned window where

477
00:20:57,130 --> 00:21:00,940
you can have loss and it transfers

478
00:20:58,660 --> 00:21:02,500
everything from that window back you can

479
00:21:00,940 --> 00:21:04,570
imagine the case where if you had a very

480
00:21:02,500 --> 00:21:06,850
high loss network or a network where

481
00:21:04,570 --> 00:21:08,439
your losses have a high probability of

482
00:21:06,850 --> 00:21:10,750
occurring within the window you're

483
00:21:08,440 --> 00:21:12,130
essentially having livelock which means

484
00:21:10,750 --> 00:21:15,430
you're putting a lot of fabric on the

485
00:21:12,130 --> 00:21:17,080
network you have one loss and now you're

486
00:21:15,430 --> 00:21:19,360
putting everything back so you've both

487
00:21:17,080 --> 00:21:20,530
consumed the bandwidth block something

488
00:21:19,360 --> 00:21:24,120
else and you didn't make forward

489
00:21:20,530 --> 00:21:24,120
progress live block by definition

490
00:21:26,670 --> 00:21:33,400
explicit congestion notification so this

491
00:21:30,520 --> 00:21:36,610
is the key part of making rocky scale

492
00:21:33,400 --> 00:21:38,890
well it means that you have to take all

493
00:21:36,610 --> 00:21:40,719
of the cues on your middle boxes and you

494
00:21:38,890 --> 00:21:42,580
have to program the thresholds correctly

495
00:21:40,720 --> 00:21:45,460
it says that you need to account for

496
00:21:42,580 --> 00:21:49,060
your worst case scenario RTT bandwidth

497
00:21:45,460 --> 00:21:51,640
delay products establish a trigger that

498
00:21:49,060 --> 00:21:53,470
would allow for detecting when there's

499
00:21:51,640 --> 00:21:54,910
congestion up to that point such that

500
00:21:53,470 --> 00:21:58,110
you have time to send a message back to

501
00:21:54,910 --> 00:22:00,760
the source or it can quench in time and

502
00:21:58,110 --> 00:22:03,969
ideally set at a point that it doesn't

503
00:22:00,760 --> 00:22:05,890
do spurious triggering of PSC which has

504
00:22:03,970 --> 00:22:08,070
some of the downsides that I described

505
00:22:05,890 --> 00:22:08,070
earlier

506
00:22:13,309 --> 00:22:18,350
priority-based flow control so this is

507
00:22:15,259 --> 00:22:20,929
the one that we must be careful with so

508
00:22:18,350 --> 00:22:23,418
party base flow control is simple you

509
00:22:20,929 --> 00:22:25,909
have global pause it simply says now do

510
00:22:23,419 --> 00:22:27,799
it on a per traffic class basis where

511
00:22:25,909 --> 00:22:29,899
traffic classes are defined as the three

512
00:22:27,799 --> 00:22:33,080
bits in the class of service field

513
00:22:29,899 --> 00:22:39,739
inside of a 22.1 p that's in a subset of

514
00:22:33,080 --> 00:22:42,590
8201 q now you may run into challenges

515
00:22:39,740 --> 00:22:44,330
in configuring PFC and you may run into

516
00:22:42,590 --> 00:22:46,309
vendor dependencies

517
00:22:44,330 --> 00:22:49,009
although the vendors for switches for

518
00:22:46,309 --> 00:22:50,720
config and PFC s have their own unique

519
00:22:49,009 --> 00:22:52,580
ways of partitioning the buffers and

520
00:22:50,720 --> 00:22:53,779
setting what the thresholds are and in

521
00:22:52,580 --> 00:22:56,990
some cases you may not have the ability

522
00:22:53,779 --> 00:22:58,249
to set the thresholds to your desired

523
00:22:56,990 --> 00:23:01,340
configuration you may just have a

524
00:22:58,249 --> 00:23:05,269
general setting for you furthermore a

525
00:23:01,340 --> 00:23:07,309
lot of the switches that support PFC may

526
00:23:05,269 --> 00:23:08,720
only support PFC for a subset of the

527
00:23:07,309 --> 00:23:10,789
traffic classes it won't support them

528
00:23:08,720 --> 00:23:11,960
for all weights there aren't cases where

529
00:23:10,789 --> 00:23:13,669
you actually need all that unless you

530
00:23:11,960 --> 00:23:15,409
have an extremely complex fabric in most

531
00:23:13,669 --> 00:23:17,210
cases you need about two you would need

532
00:23:15,409 --> 00:23:19,789
one for high priority a in low priority

533
00:23:17,210 --> 00:23:23,480
RDMA but if you needed to use more or

534
00:23:19,789 --> 00:23:26,629
you had a very dense mesh fabric where

535
00:23:23,480 --> 00:23:28,460
you needed more granularity you have to

536
00:23:26,629 --> 00:23:34,459
be careful with which switches you buy

537
00:23:28,460 --> 00:23:36,110
in which capabilities they have it has

538
00:23:34,460 --> 00:23:37,070
transmission selection this is the part

539
00:23:36,110 --> 00:23:38,869
that comes in that gives you the

540
00:23:37,070 --> 00:23:41,149
bandwidth reservation in the most

541
00:23:38,869 --> 00:23:47,029
simplest form you can think of it as if

542
00:23:41,149 --> 00:23:48,739
I have in if I have an an orchestrated

543
00:23:47,029 --> 00:23:52,789
layer to implementation

544
00:23:48,740 --> 00:23:55,279
I could essentially have my our DMA

545
00:23:52,789 --> 00:23:55,879
traffic overrun my tcp/ip traffic or

546
00:23:55,279 --> 00:24:00,200
vice versa

547
00:23:55,879 --> 00:24:04,039
right it's common to have tcp/ip or more

548
00:24:00,200 --> 00:24:05,809
properly unbound UDP preempt already am

549
00:24:04,039 --> 00:24:07,669
a traffic it's not uncommon you can

550
00:24:05,809 --> 00:24:10,249
easily make this happen it's as simple

551
00:24:07,669 --> 00:24:11,809
as putting DP DK in a VM and just

552
00:24:10,249 --> 00:24:13,220
blasting as much D petechiae traffic

553
00:24:11,809 --> 00:24:14,480
through UDP as you can and you'll see

554
00:24:13,220 --> 00:24:16,490
how it starts to affect everything on

555
00:24:14,480 --> 00:24:17,869
the fabric so enhance transmission

556
00:24:16,490 --> 00:24:19,730
selection essentially gives you eight

557
00:24:17,869 --> 00:24:21,110
lanes right it builds on the same

558
00:24:19,730 --> 00:24:23,059
traffic classes that I described earlier

559
00:24:21,110 --> 00:24:24,709
and it gives you a minimum bandwidth

560
00:24:23,059 --> 00:24:26,870
reservation and the way it works is

561
00:24:24,710 --> 00:24:28,610
simple assume that I have a 10 gig pipe

562
00:24:26,870 --> 00:24:30,469
partition at fifty-fifty I say 50%

563
00:24:28,610 --> 00:24:32,990
reservation for our DMA on chapter class

564
00:24:30,470 --> 00:24:35,960
3 50% reservation for everything else in

565
00:24:32,990 --> 00:24:39,620
the absence of any 10-inch traffic or

566
00:24:35,960 --> 00:24:41,480
DMA on traffic class 3 kid consume its 5

567
00:24:39,620 --> 00:24:43,550
gigs and scale all the way up to 10 gigs

568
00:24:41,480 --> 00:24:45,860
but as soon as the other traffic class

569
00:24:43,550 --> 00:24:48,559
starts talking then it becomes throttled

570
00:24:45,860 --> 00:24:50,659
down to a minimum guarantee of the 5

571
00:24:48,559 --> 00:24:53,000
gigs that were initially established you

572
00:24:50,660 --> 00:24:54,380
can partition this update ways it can

573
00:24:53,000 --> 00:25:00,110
get pretty complex when you start

574
00:24:54,380 --> 00:25:02,720
partitioning it in short there are

575
00:25:00,110 --> 00:25:06,830
challenges with deploying technologies

576
00:25:02,720 --> 00:25:08,780
like our DMA I'd argue that some of

577
00:25:06,830 --> 00:25:10,699
those challenges aren't actually already

578
00:25:08,780 --> 00:25:13,370
made specific in the context of this

579
00:25:10,700 --> 00:25:15,920
audience you can consider any case where

580
00:25:13,370 --> 00:25:18,229
you have an overly greedy transport that

581
00:25:15,920 --> 00:25:19,910
is not doing an to end sampling that is

582
00:25:18,230 --> 00:25:23,000
not being a good citizen on the fabric

583
00:25:19,910 --> 00:25:25,510
that is simply just ramping up his

584
00:25:23,000 --> 00:25:29,870
address traffic to the point that it's

585
00:25:25,510 --> 00:25:31,850
disturbing others there are ways to try

586
00:25:29,870 --> 00:25:33,770
to mitigate that already ma is by no

587
00:25:31,850 --> 00:25:36,590
means the only example I can give you 3

588
00:25:33,770 --> 00:25:39,170
there's a FX DP which is a mechanism for

589
00:25:36,590 --> 00:25:42,169
providing user space near native

590
00:25:39,170 --> 00:25:43,340
hardware access for i/o meaning that you

591
00:25:42,170 --> 00:25:45,020
can Burling out bring along any

592
00:25:43,340 --> 00:25:46,000
application and it'll just start spewing

593
00:25:45,020 --> 00:25:49,940
its own transport

594
00:25:46,000 --> 00:25:51,559
there's DP DK there's there's Linux you

595
00:25:49,940 --> 00:25:55,040
can pretty much code up the Linux kernel

596
00:25:51,559 --> 00:25:57,920
to do whatever you want and in in

597
00:25:55,040 --> 00:26:00,850
general you know there are known

598
00:25:57,920 --> 00:26:06,559
examples at scale for example there's

599
00:26:00,850 --> 00:26:08,360
quick-quick internet connections that's

600
00:26:06,559 --> 00:26:10,100
widely deployed if you think about your

601
00:26:08,360 --> 00:26:11,449
typical Chrome browser it's running

602
00:26:10,100 --> 00:26:12,980
quick under the covers which means it's

603
00:26:11,450 --> 00:26:15,320
a user space transport meaning it's not

604
00:26:12,980 --> 00:26:17,150
going where kernel mediated IO it

605
00:26:15,320 --> 00:26:19,760
doesn't take too much to have a port of

606
00:26:17,150 --> 00:26:21,080
that off of chromium that technology for

607
00:26:19,760 --> 00:26:22,460
quick is what's used to power a lot of

608
00:26:21,080 --> 00:26:25,309
the Google first-party properties so

609
00:26:22,460 --> 00:26:26,900
thank you tube in Gmail in those

610
00:26:25,309 --> 00:26:29,059
technologies and all of the examples

611
00:26:26,900 --> 00:26:31,010
I've just given these are their non RDMA

612
00:26:29,059 --> 00:26:32,750
cases or you have user space coming and

613
00:26:31,010 --> 00:26:34,790
bringing their own transports putting

614
00:26:32,750 --> 00:26:36,650
traffic on the fabric and potentially

615
00:26:34,790 --> 00:26:38,720
causing disruption and noise as a

616
00:26:36,650 --> 00:26:40,570
network operators we have a set of tools

617
00:26:38,720 --> 00:26:43,600
at hand to try to minute

618
00:26:40,570 --> 00:26:46,950
we have potential tools in layer two

619
00:26:43,600 --> 00:26:48,998
trying to use PFC and ACN and you have

620
00:26:46,950 --> 00:26:51,940
traffic shaping you can do traffic

621
00:26:48,999 --> 00:26:54,940
shaping or address limits in class these

622
00:26:51,940 --> 00:26:58,869
are a lot of interesting areas that can

623
00:26:54,940 --> 00:27:01,210
be used to try to bring more order to

624
00:26:58,869 --> 00:27:03,970
this new world where transports require

625
00:27:01,210 --> 00:27:05,950
extremely low latency new devices coming

626
00:27:03,970 --> 00:27:12,029
aboard require extremely low latency and

627
00:27:05,950 --> 00:27:17,409
you need consistency at scale and lastly

628
00:27:12,029 --> 00:27:19,629
here we are so I've talked about our DMA

629
00:27:17,409 --> 00:27:21,850
some of the new technologies coming on

630
00:27:19,629 --> 00:27:23,289
board in terms of storage CPUs FPGAs

631
00:27:21,850 --> 00:27:25,629
will currently need four extremely Lola

632
00:27:23,289 --> 00:27:28,450
is the inconsistency I've talked about

633
00:27:25,629 --> 00:27:29,649
the high level operation of our DMA how

634
00:27:28,450 --> 00:27:31,869
it works and how it contrasts between

635
00:27:29,649 --> 00:27:33,969
rocky Network and I've talked about some

636
00:27:31,869 --> 00:27:36,730
of the challenges for configuring and

637
00:27:33,970 --> 00:27:39,399
operating our DMA this is an area of a

638
00:27:36,730 --> 00:27:41,169
lot of research and investigation this

639
00:27:39,399 --> 00:27:43,268
is an area where you just go survey the

640
00:27:41,169 --> 00:27:45,940
papers and there are tons and tons of

641
00:27:43,269 --> 00:27:50,190
papers on transport innovations in this

642
00:27:45,940 --> 00:27:53,409
space a lot more work as needed here I

643
00:27:50,190 --> 00:27:55,539
have high hopes for delay based

644
00:27:53,409 --> 00:27:58,119
congestion control protocols but those

645
00:27:55,539 --> 00:28:01,029
types of congestion control protocols

646
00:27:58,119 --> 00:28:03,549
should in theory minimize the need for

647
00:28:01,029 --> 00:28:07,240
having to use fabric based feedback

648
00:28:03,549 --> 00:28:08,950
mechanisms like ecn or PFC there are

649
00:28:07,240 --> 00:28:10,480
many examples of such timely as one of

650
00:28:08,950 --> 00:28:11,889
the examples I encourage you to read one

651
00:28:10,480 --> 00:28:13,440
of the timely papers it's in the

652
00:28:11,889 --> 00:28:16,449
reference section for this document and

653
00:28:13,440 --> 00:28:19,559
with that this is the end of my talk and

654
00:28:16,450 --> 00:28:19,559
I'll take any questions

655
00:28:22,090 --> 00:28:33,340
I robbed see strim stepping back from

656
00:28:31,780 --> 00:28:36,399
the mic and not making it explode this

657
00:28:33,340 --> 00:28:39,100
time asking a question on my own behalf

658
00:28:36,400 --> 00:28:52,990
one of your first slides had a quote

659
00:28:39,100 --> 00:28:54,399
from mr. Wong and I may not have been

660
00:28:52,990 --> 00:28:57,820
paying close enough attention but I

661
00:28:54,400 --> 00:29:01,270
don't think that you stepped on touched

662
00:28:57,820 --> 00:29:03,370
on the subject of crypto here and as we

663
00:29:01,270 --> 00:29:06,280
know from the Snowden disclosures and

664
00:29:03,370 --> 00:29:08,919
muscular in particular if you think that

665
00:29:06,280 --> 00:29:11,170
the network links between your

666
00:29:08,920 --> 00:29:13,690
individual elements are secure you're

667
00:29:11,170 --> 00:29:15,910
probably wrong so I was wondering if

668
00:29:13,690 --> 00:29:17,860
this is in the roadmap if this is

669
00:29:15,910 --> 00:29:19,360
something that's going to get hastily

670
00:29:17,860 --> 00:29:22,510
bolted on after the fact to make

671
00:29:19,360 --> 00:29:26,219
everybody sad or something that nobody

672
00:29:22,510 --> 00:29:28,870
has given any thought to absolutely

673
00:29:26,220 --> 00:29:32,800
critically important area extremely

674
00:29:28,870 --> 00:29:34,780
important not covered in my talk simply

675
00:29:32,800 --> 00:29:37,419
because this is more about latency and

676
00:29:34,780 --> 00:29:39,430
performance excluding security yes but

677
00:29:37,420 --> 00:29:45,700
if you add crypto in the middle it hits

678
00:29:39,430 --> 00:29:48,100
your latency and performance yeah agree

679
00:29:45,700 --> 00:29:50,350
agree I was gonna cover that part in the

680
00:29:48,100 --> 00:29:53,139
RTM a case based on the current

681
00:29:50,350 --> 00:29:57,219
protocols think of rocky and I work it's

682
00:29:53,140 --> 00:29:58,960
a bit difficult to bolt on security at

683
00:29:57,220 --> 00:30:01,330
the moment that's one of the reasons why

684
00:29:58,960 --> 00:30:04,270
you don't see widely deployed to tenant

685
00:30:01,330 --> 00:30:06,490
endpoints right so for example in in

686
00:30:04,270 --> 00:30:09,190
Windows Server specifically we support

687
00:30:06,490 --> 00:30:11,890
our DMA two guests on pram however not

688
00:30:09,190 --> 00:30:13,720
in has your proper at least not yet the

689
00:30:11,890 --> 00:30:15,040
reasons for that are really because it

690
00:30:13,720 --> 00:30:17,370
comes down to security you want to

691
00:30:15,040 --> 00:30:19,990
secure an encrypted traffic what is

692
00:30:17,370 --> 00:30:22,840
happening in this space in terms of

693
00:30:19,990 --> 00:30:24,970
security expands beyond our DMA if you

694
00:30:22,840 --> 00:30:27,370
look at quick as a protocol it's

695
00:30:24,970 --> 00:30:29,110
encrypted by nature if there's no

696
00:30:27,370 --> 00:30:31,959
encryption it's simply not going to work

697
00:30:29,110 --> 00:30:34,270
you have to fall back to tcp/ip and the

698
00:30:31,960 --> 00:30:35,890
case we've already in May really the

699
00:30:34,270 --> 00:30:38,170
innovation on encryption

700
00:30:35,890 --> 00:30:42,070
is coming into inline encryption

701
00:30:38,170 --> 00:30:44,440
processing so one of the on the on the

702
00:30:42,070 --> 00:30:47,590
network cards themselves on the network

703
00:30:44,440 --> 00:30:51,220
card or through middle box proxy so you

704
00:30:47,590 --> 00:30:52,449
can think of cases where and this is no

705
00:30:51,220 --> 00:30:54,540
secret the cloud providers are all

706
00:30:52,450 --> 00:30:57,070
looking at this how do they add

707
00:30:54,540 --> 00:30:59,560
specialized silicon to do inline

708
00:30:57,070 --> 00:31:02,980
processing at minimal cost boolean

709
00:30:59,560 --> 00:31:05,200
hardware for any any transfers that are

710
00:31:02,980 --> 00:31:07,180
occurring between any two systems now

711
00:31:05,200 --> 00:31:10,030
the challenges in the in that are that

712
00:31:07,180 --> 00:31:12,760
there may be cases in our DMA where more

713
00:31:10,030 --> 00:31:15,820
innovation is required for your standard

714
00:31:12,760 --> 00:31:18,340
tcp/ip UDP transfer cases regardless of

715
00:31:15,820 --> 00:31:19,780
you know what the application is it's

716
00:31:18,340 --> 00:31:21,280
fairly simple but in the already made

717
00:31:19,780 --> 00:31:23,770
cases there are some challenges that

718
00:31:21,280 --> 00:31:25,090
still need to be figured out I'm

719
00:31:23,770 --> 00:31:29,889
heartened here it's on the road well

720
00:31:25,090 --> 00:31:33,040
thank you my name is Anna from Dell I

721
00:31:29,890 --> 00:31:37,060
have a just just want to pick your brain

722
00:31:33,040 --> 00:31:38,620
so between rocky b2 or I worth which one

723
00:31:37,060 --> 00:31:44,649
do you think will gain more traction

724
00:31:38,620 --> 00:31:46,090
going forward so that gets into the

725
00:31:44,650 --> 00:31:49,180
philosophical war that I mentioned

726
00:31:46,090 --> 00:31:50,530
earlier I can give you the company

727
00:31:49,180 --> 00:31:52,810
answer which is we don't have a

728
00:31:50,530 --> 00:31:55,120
preference over I don't want the person

729
00:31:52,810 --> 00:31:56,800
will answer is it really depends on the

730
00:31:55,120 --> 00:31:59,229
scale it really depends on what you're

731
00:31:56,800 --> 00:32:03,399
doing to give you an example it also

732
00:31:59,230 --> 00:32:05,620
depends on the workload of course if

733
00:32:03,400 --> 00:32:09,340
you're doing small-scale let's say

734
00:32:05,620 --> 00:32:12,250
you're doing you know small regional

735
00:32:09,340 --> 00:32:15,459
data centers something small in most

736
00:32:12,250 --> 00:32:18,810
cases I work will suffice why it's

737
00:32:15,460 --> 00:32:20,830
simply simpler I don't have to have a

738
00:32:18,810 --> 00:32:22,210
specialized layer to or specially

739
00:32:20,830 --> 00:32:24,970
configured there to for it to work you

740
00:32:22,210 --> 00:32:26,380
would only decorate or enhance l2 if you

741
00:32:24,970 --> 00:32:27,250
want to make it better if you want to

742
00:32:26,380 --> 00:32:28,810
have better SLA s

743
00:32:27,250 --> 00:32:32,020
if you want better storage over I warp

744
00:32:28,810 --> 00:32:35,290
as in 50% of my bandwidth his storage

745
00:32:32,020 --> 00:32:37,300
50% of my bandwidth is other it's simple

746
00:32:35,290 --> 00:32:39,010
examples and I want that strict line to

747
00:32:37,300 --> 00:32:43,180
be consistent those are the cases that

748
00:32:39,010 --> 00:32:46,090
you would add DC be PFC rocky however

749
00:32:43,180 --> 00:32:48,100
for the most basic operation even a

750
00:32:46,090 --> 00:32:49,010
three node cluster running on the same

751
00:32:48,100 --> 00:32:51,649
common so

752
00:32:49,010 --> 00:32:53,660
would require very delicate

753
00:32:51,650 --> 00:32:55,220
configuration the follow those valleys

754
00:32:53,660 --> 00:32:56,540
ecn comes into the patient when you're

755
00:32:55,220 --> 00:32:58,220
doing multi hop and routing of course

756
00:32:56,540 --> 00:33:01,340
but you would still have to require PSE

757
00:32:58,220 --> 00:33:03,140
so from that point of view I think it

758
00:33:01,340 --> 00:33:05,060
really depends on the scenario if you're

759
00:33:03,140 --> 00:33:08,180
going maximum scale if you're going like

760
00:33:05,060 --> 00:33:10,070
you know a couple of dozen rows hundreds

761
00:33:08,180 --> 00:33:11,770
of servers you most likely have a

762
00:33:10,070 --> 00:33:13,490
network operator who's expected to be

763
00:33:11,770 --> 00:33:15,650
specialized and knowledgeable in this

764
00:33:13,490 --> 00:33:17,440
space and can configure PFC and EC and

765
00:33:15,650 --> 00:33:21,310
accordingly in that case it would work

766
00:33:17,440 --> 00:33:21,310
again depends on the scenario

767
00:33:26,760 --> 00:33:32,849
[Applause]

768
00:33:38,909 --> 00:33:40,970
you

