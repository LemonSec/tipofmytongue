1
00:00:12,760 --> 00:00:19,189
good afternoon everybody

2
00:00:14,750 --> 00:00:21,890
i'm sri vidhyaa here and my team and i

3
00:00:19,189 --> 00:00:26,810
have been working on machine learning

4
00:00:21,890 --> 00:00:30,859
for networking applications my prior

5
00:00:26,810 --> 00:00:34,010
experience is in network architecture

6
00:00:30,859 --> 00:00:36,769
and engineering for over 15 years so I

7
00:00:34,010 --> 00:00:40,000
have seen the good bad and ugly of

8
00:00:36,769 --> 00:00:40,000
networking so

9
00:00:53,360 --> 00:01:05,039
how do I do this oh sorry

10
00:01:01,220 --> 00:01:08,610
so why machine learning for networking

11
00:01:05,040 --> 00:01:11,070
other than the fact that everybody is

12
00:01:08,610 --> 00:01:13,140
using machine learning for every

13
00:01:11,070 --> 00:01:15,029
application and claim that it can do

14
00:01:13,140 --> 00:01:18,480
everything so there's really two reasons

15
00:01:15,030 --> 00:01:21,180
for it one is as we saw in previous

16
00:01:18,480 --> 00:01:24,150
yesterday's presentation the network has

17
00:01:21,180 --> 00:01:27,360
just changing and has changed the

18
00:01:24,150 --> 00:01:31,530
network traffic has changed so we need

19
00:01:27,360 --> 00:01:34,140
new mechanisms to figure out how network

20
00:01:31,530 --> 00:01:37,470
is working to get more visibility into

21
00:01:34,140 --> 00:01:40,830
the network and the second thing is all

22
00:01:37,470 --> 00:01:44,880
these new devices and new networks have

23
00:01:40,830 --> 00:01:46,289
a lot of data that's hard for human

24
00:01:44,880 --> 00:01:48,479
brain to process so if you have to

25
00:01:46,290 --> 00:01:50,670
process data from multiple sources and

26
00:01:48,479 --> 00:01:53,220
try to find patterns in them it's really

27
00:01:50,670 --> 00:02:06,360
difficult so machine learning comes in

28
00:01:53,220 --> 00:02:09,710
handy to do that so I did that am i

29
00:02:06,360 --> 00:02:09,710
pressing the wrong here

30
00:02:15,319 --> 00:02:23,899
okay sorry um what does building a

31
00:02:20,420 --> 00:02:27,230
machine learning application involve so

32
00:02:23,900 --> 00:02:28,900
it involves four distinct steps the

33
00:02:27,230 --> 00:02:31,429
first thing is framing the problem

34
00:02:28,900 --> 00:02:33,709
preparing data training the model and

35
00:02:31,430 --> 00:02:35,480
using the model we are going to

36
00:02:33,709 --> 00:02:38,329
primarily talk about framing the problem

37
00:02:35,480 --> 00:02:41,629
and preparing the data because building

38
00:02:38,329 --> 00:02:44,930
the model and using the model is nothing

39
00:02:41,629 --> 00:02:47,030
specific to networking it's pretty much

40
00:02:44,930 --> 00:02:50,780
same to any machine learning application

41
00:02:47,030 --> 00:02:52,790
you do when we started doing this we

42
00:02:50,780 --> 00:02:55,370
thought oh we already know the problem

43
00:02:52,790 --> 00:02:58,849
we already know network data so we were

44
00:02:55,370 --> 00:03:01,010
thinking we were going to be using some

45
00:02:58,849 --> 00:03:02,869
cool algorithms and that's what we are

46
00:03:01,010 --> 00:03:06,138
going to be spending all our time with

47
00:03:02,870 --> 00:03:08,569
but mostly what we were trying what we

48
00:03:06,139 --> 00:03:10,579
ended up doing was trying to figure out

49
00:03:08,569 --> 00:03:12,260
what problem we are solving and trying

50
00:03:10,579 --> 00:03:15,769
to figure out what data and how to

51
00:03:12,260 --> 00:03:18,349
actually process the data to feed into

52
00:03:15,769 --> 00:03:21,650
the algorithm for it to actually learn

53
00:03:18,349 --> 00:03:25,190
the correct thing so we spent about 75%

54
00:03:21,650 --> 00:03:28,010
of our time just processing data and

55
00:03:25,190 --> 00:03:29,750
framing the problem once we got that the

56
00:03:28,010 --> 00:03:32,478
algorithms are actually have been around

57
00:03:29,750 --> 00:03:34,400
for a long time and a pretty robust so

58
00:03:32,479 --> 00:03:37,010
we didn't have any problem getting there

59
00:03:34,400 --> 00:03:42,590
but just just dealing with data was the

60
00:03:37,010 --> 00:03:45,620
main problem so what does framing the

61
00:03:42,590 --> 00:03:48,699
problem mean in machine learning there

62
00:03:45,620 --> 00:03:51,680
is two sets of problems one is you

63
00:03:48,699 --> 00:03:56,120
provide machine learning with some data

64
00:03:51,680 --> 00:03:58,790
and then it says okay give me the

65
00:03:56,120 --> 00:04:00,530
groupings of data so tell me what the

66
00:03:58,790 --> 00:04:02,929
relationships are between the data and

67
00:04:00,530 --> 00:04:06,500
then figure out the pattern so that's

68
00:04:02,930 --> 00:04:08,389
your clustering model and then the

69
00:04:06,500 --> 00:04:11,780
second model the second problem that

70
00:04:08,389 --> 00:04:14,120
you're trying to address is to predict

71
00:04:11,780 --> 00:04:16,430
the outcome of future events and for

72
00:04:14,120 --> 00:04:20,340
that one you actually give the machine

73
00:04:16,430 --> 00:04:22,770
learning model some insight into it and

74
00:04:20,339 --> 00:04:25,650
say okay what did you learn from my

75
00:04:22,770 --> 00:04:28,400
insight and tell me what you are seeing

76
00:04:25,650 --> 00:04:30,539
from the data so the first one is your

77
00:04:28,400 --> 00:04:32,400
unsupervised learning which means you

78
00:04:30,540 --> 00:04:34,530
just give data and then it doesn't it

79
00:04:32,400 --> 00:04:38,370
does what it does the second one is

80
00:04:34,530 --> 00:04:41,698
you're sort of providing some kind of

81
00:04:38,370 --> 00:04:44,940
supervision for the machine learning

82
00:04:41,699 --> 00:04:49,500
application so it sort of learns what

83
00:04:44,940 --> 00:04:51,960
you're trying to tell it to learn so the

84
00:04:49,500 --> 00:04:54,169
reason the prob the problem is problem

85
00:04:51,960 --> 00:04:57,030
definition is so important is because

86
00:04:54,169 --> 00:04:58,950
that depends that determines what type

87
00:04:57,030 --> 00:05:01,349
of data you collect how much data you

88
00:04:58,950 --> 00:05:04,110
collect and what it is that you're

89
00:05:01,350 --> 00:05:05,880
trying to make the algorithm do and what

90
00:05:04,110 --> 00:05:11,190
type of performance metrics you're

91
00:05:05,880 --> 00:05:14,969
looking for the algorithm to do so the

92
00:05:11,190 --> 00:05:18,060
data collection is one was one of the

93
00:05:14,970 --> 00:05:20,460
hardest and the most tedious things we

94
00:05:18,060 --> 00:05:22,410
had to deal with most of you who have

95
00:05:20,460 --> 00:05:24,659
worked in networking know that the

96
00:05:22,410 --> 00:05:26,550
vendors devices even though there are

97
00:05:24,660 --> 00:05:30,120
they are standardized they don't

98
00:05:26,550 --> 00:05:32,220
necessarily give out the same type of

99
00:05:30,120 --> 00:05:35,580
data this there was data formatting

100
00:05:32,220 --> 00:05:37,080
issues so we spent a lot of time trying

101
00:05:35,580 --> 00:05:39,690
to figure out how to collect the right

102
00:05:37,080 --> 00:05:42,210
data so there's actually two types of

103
00:05:39,690 --> 00:05:44,639
data collection one is your offline data

104
00:05:42,210 --> 00:05:46,560
collection where you collect data over a

105
00:05:44,639 --> 00:05:48,990
period of time and then use the data

106
00:05:46,560 --> 00:05:51,570
explain the model and then there is the

107
00:05:48,990 --> 00:05:55,380
online of the real-time data collection

108
00:05:51,570 --> 00:06:00,180
there's still no good methodology that

109
00:05:55,380 --> 00:06:01,530
actually does real-time model machine

110
00:06:00,180 --> 00:06:04,349
learning model update you could

111
00:06:01,530 --> 00:06:07,799
aggregate it for 30 minutes or one hour

112
00:06:04,349 --> 00:06:10,770
but there's no like really really real

113
00:06:07,800 --> 00:06:12,570
time data up min model update and

114
00:06:10,770 --> 00:06:14,789
machine learning at least not for

115
00:06:12,570 --> 00:06:17,130
networking maybe there other domains

116
00:06:14,789 --> 00:06:19,020
that are able to do it but it's it's

117
00:06:17,130 --> 00:06:22,380
really difficult to do real-time updates

118
00:06:19,020 --> 00:06:24,210
for machine and for networking data so

119
00:06:22,380 --> 00:06:27,840
some of the data that we looked at was

120
00:06:24,210 --> 00:06:31,349
slow data packet captures firewall log

121
00:06:27,840 --> 00:06:34,140
syslog telemetry

122
00:06:31,350 --> 00:06:37,080
though we realized slot of the streaming

123
00:06:34,140 --> 00:06:38,789
telemetry was a lot more hype than we

124
00:06:37,080 --> 00:06:41,580
were actually able to get anything out

125
00:06:38,790 --> 00:06:43,710
of so if anybody has any insight into it

126
00:06:41,580 --> 00:06:46,590
I would like to hear it we looked at

127
00:06:43,710 --> 00:06:52,940
different device configurations and some

128
00:06:46,590 --> 00:06:56,190
technology data so the next thing that

129
00:06:52,940 --> 00:07:00,030
we had to do was feature extraction and

130
00:06:56,190 --> 00:07:04,170
this was also the most tedious and

131
00:07:00,030 --> 00:07:06,960
really really difficult aspect of doing

132
00:07:04,170 --> 00:07:09,990
the machine learning for networking this

133
00:07:06,960 --> 00:07:15,539
involved knowing the domain this

134
00:07:09,990 --> 00:07:19,230
involved knowing what type of features

135
00:07:15,540 --> 00:07:22,110
or what attributes really gave good

136
00:07:19,230 --> 00:07:23,820
insight because as you can see in a

137
00:07:22,110 --> 00:07:26,310
packet let's say you're doing a packet

138
00:07:23,820 --> 00:07:29,880
capture and you have a lot of features

139
00:07:26,310 --> 00:07:32,820
that come out out of packet capture in

140
00:07:29,880 --> 00:07:35,460
the multiple layers but not all data is

141
00:07:32,820 --> 00:07:38,340
created equal not all not all attributes

142
00:07:35,460 --> 00:07:40,590
are created equal so trying to figure

143
00:07:38,340 --> 00:07:43,440
out what features actually worked was

144
00:07:40,590 --> 00:07:45,690
part of it was domain knowledge and part

145
00:07:43,440 --> 00:07:48,000
of it was just trial and error we had to

146
00:07:45,690 --> 00:07:50,460
try different permutation combinations

147
00:07:48,000 --> 00:07:54,450
of features so this is again the most

148
00:07:50,460 --> 00:07:56,489
tedious and like part of trying to

149
00:07:54,450 --> 00:07:58,680
figure out what data to feed into the

150
00:07:56,490 --> 00:08:02,970
model so if you filled in junk you

151
00:07:58,680 --> 00:08:04,920
basically just get junk models that

152
00:08:02,970 --> 00:08:06,750
doesn't actually do anything so it's

153
00:08:04,920 --> 00:08:13,410
really really important to get good

154
00:08:06,750 --> 00:08:14,940
quality data so the next part is

155
00:08:13,410 --> 00:08:16,710
training the model there's nothing

156
00:08:14,940 --> 00:08:21,420
really specific about it this is a

157
00:08:16,710 --> 00:08:25,049
common 70/30 split so 70% of the data is

158
00:08:21,420 --> 00:08:28,920
used for training the model and 30% is

159
00:08:25,050 --> 00:08:31,350
used as for testing one of the things

160
00:08:28,920 --> 00:08:34,530
that I do want to stress is the need for

161
00:08:31,350 --> 00:08:38,159
spatial and temporal diversity so we

162
00:08:34,530 --> 00:08:40,579
could get data for two days and train it

163
00:08:38,159 --> 00:08:42,959
but that's not really giving you the

164
00:08:40,580 --> 00:08:43,810
temporal diversity so you have to

165
00:08:42,960 --> 00:08:47,260
collect data

166
00:08:43,809 --> 00:08:50,500
a period of time sample it and then do

167
00:08:47,260 --> 00:08:53,319
aggregations and then train it and for

168
00:08:50,500 --> 00:08:55,150
the spatial diversity we had to take

169
00:08:53,320 --> 00:08:57,910
data from different networks because not

170
00:08:55,150 --> 00:09:00,069
all my networks are created equal that's

171
00:08:57,910 --> 00:09:02,219
also one of the issues with using

172
00:09:00,070 --> 00:09:06,190
networking data for a machine learning

173
00:09:02,220 --> 00:09:10,390
once the model is trained then you need

174
00:09:06,190 --> 00:09:12,220
to verify it for performance and the

175
00:09:10,390 --> 00:09:16,650
performance also depends on what type of

176
00:09:12,220 --> 00:09:20,650
application you're doing and what really

177
00:09:16,650 --> 00:09:23,640
needs to be resolved from that

178
00:09:20,650 --> 00:09:26,770
application and there's really no way to

179
00:09:23,640 --> 00:09:29,410
distinguish any learning algorithm as

180
00:09:26,770 --> 00:09:31,210
best again it's trial and error of which

181
00:09:29,410 --> 00:09:37,750
algorithm works best for which

182
00:09:31,210 --> 00:09:39,490
application so in the next four slides

183
00:09:37,750 --> 00:09:43,930
I'm going to talk about some of the

184
00:09:39,490 --> 00:09:48,370
common networking problems that can be

185
00:09:43,930 --> 00:09:50,319
used for used in machine learning one of

186
00:09:48,370 --> 00:09:51,790
the things I don't include here because

187
00:09:50,320 --> 00:09:55,210
we haven't really done anything with it

188
00:09:51,790 --> 00:09:57,459
is routing analysis what we have done is

189
00:09:55,210 --> 00:10:00,580
traffic classification traffic

190
00:09:57,460 --> 00:10:03,820
prediction anomaly detection and fault

191
00:10:00,580 --> 00:10:06,670
event detection we haven't done a lot of

192
00:10:03,820 --> 00:10:09,910
it in all of them but at least we have

193
00:10:06,670 --> 00:10:11,829
attempted some of these applications so

194
00:10:09,910 --> 00:10:14,550
the first of all the first one I want to

195
00:10:11,830 --> 00:10:17,680
talk about is the traffic classification

196
00:10:14,550 --> 00:10:20,199
so classifying traffic current

197
00:10:17,680 --> 00:10:23,229
methodology is basically pork based or

198
00:10:20,200 --> 00:10:25,630
payload based pork based with

199
00:10:23,230 --> 00:10:30,370
non-standard ports you're not getting

200
00:10:25,630 --> 00:10:35,140
what you need and because we saw 85% of

201
00:10:30,370 --> 00:10:37,120
the traffic is TLS it's all encrypted so

202
00:10:35,140 --> 00:10:39,160
looking at payload is not an option

203
00:10:37,120 --> 00:10:42,100
either so this is a really good

204
00:10:39,160 --> 00:10:44,980
application for machine learning because

205
00:10:42,100 --> 00:10:48,370
what it does is you look at floated

206
00:10:44,980 --> 00:10:52,600
title of a packet header data it tries

207
00:10:48,370 --> 00:10:53,850
to look at some of the other fields and

208
00:10:52,600 --> 00:10:56,310
one example

209
00:10:53,850 --> 00:10:59,519
we didn't realize we were doing p2p

210
00:10:56,310 --> 00:11:03,180
applications BitTorrent apparently the

211
00:10:59,519 --> 00:11:06,810
the first packet is 66 bytes somehow it

212
00:11:03,180 --> 00:11:09,479
picked up there was 66 bytes and said

213
00:11:06,810 --> 00:11:11,489
that was BitTorrent traffic and it it

214
00:11:09,480 --> 00:11:14,310
ended up being BitTorrent traffic so it

215
00:11:11,490 --> 00:11:17,399
sort of learns from his environment or

216
00:11:14,310 --> 00:11:19,410
its context and that's one of the nice

217
00:11:17,399 --> 00:11:22,410
things about using machine learning for

218
00:11:19,410 --> 00:11:27,149
it that's an advantage over current

219
00:11:22,410 --> 00:11:28,889
methods we used we tested it on support

220
00:11:27,149 --> 00:11:31,800
vector machines decision trees and

221
00:11:28,889 --> 00:11:34,589
random forests and each algorithm

222
00:11:31,800 --> 00:11:36,209
behaves differently and I'm not going to

223
00:11:34,589 --> 00:11:38,370
go into detail but I do have some

224
00:11:36,209 --> 00:11:41,459
background slides on what these

225
00:11:38,370 --> 00:11:43,949
algorithms are so if you have nothing

226
00:11:41,459 --> 00:11:47,000
better to do you can go back and go and

227
00:11:43,949 --> 00:11:47,000
look look at them

228
00:11:49,259 --> 00:11:55,319
the next application we looked at was

229
00:11:51,779 --> 00:11:58,170
traffic prediction the current method is

230
00:11:55,319 --> 00:12:00,930
typically time series analysis we didn't

231
00:11:58,170 --> 00:12:05,219
do a lot of things with it we just took

232
00:12:00,930 --> 00:12:07,859
a sampling of data and we tried to use

233
00:12:05,220 --> 00:12:09,660
artificial neural networks and this is

234
00:12:07,860 --> 00:12:11,910
one of the things that I am looking for

235
00:12:09,660 --> 00:12:14,639
I've been talking to some of you and I'm

236
00:12:11,910 --> 00:12:16,709
looking for some use cases to test and

237
00:12:14,639 --> 00:12:21,620
try out some different algorithms so I

238
00:12:16,709 --> 00:12:25,109
don't have a lot of insights on this one

239
00:12:21,620 --> 00:12:29,040
the next one is anomaly detection this

240
00:12:25,110 --> 00:12:33,899
is to do DDoS and malware detection

241
00:12:29,040 --> 00:12:37,110
again this one the research is typically

242
00:12:33,899 --> 00:12:40,279
using k-means clustering and what it

243
00:12:37,110 --> 00:12:42,810
does is it identifies most of the common

244
00:12:40,279 --> 00:12:45,449
things that you can see but a lot of the

245
00:12:42,810 --> 00:12:48,410
corner cases which DDoS ends up being a

246
00:12:45,449 --> 00:12:53,160
lot of corner cases it doesn't really

247
00:12:48,410 --> 00:12:55,050
detect so this is another area that if

248
00:12:53,160 --> 00:12:59,430
any of you have used cases I'm looking

249
00:12:55,050 --> 00:13:02,160
for I think what would be really good as

250
00:12:59,430 --> 00:13:05,219
some kind of hybrid methodology where

251
00:13:02,160 --> 00:13:06,420
you cluster to kind of differentiate the

252
00:13:05,220 --> 00:13:08,490
traffic and

253
00:13:06,420 --> 00:13:12,180
and do some kind of supervised learning

254
00:13:08,490 --> 00:13:16,190
where you give it a lot more than what a

255
00:13:12,180 --> 00:13:16,189
typical clustering application does

256
00:13:18,079 --> 00:13:24,000
fault management is the one that we have

257
00:13:21,060 --> 00:13:26,670
been working a lot on so this is to

258
00:13:24,000 --> 00:13:28,769
accurately predict network faults that

259
00:13:26,670 --> 00:13:31,380
are kind of flying under the radar most

260
00:13:28,769 --> 00:13:35,339
of you who have worked in networking

261
00:13:31,380 --> 00:13:37,170
know what happens in a network fault

262
00:13:35,339 --> 00:13:39,360
somebody complains that the network is

263
00:13:37,170 --> 00:13:41,219
not working so you have to go and figure

264
00:13:39,360 --> 00:13:42,839
out where the fault is so fault

265
00:13:41,220 --> 00:13:46,529
isolation and then you have to run

266
00:13:42,839 --> 00:13:50,010
around trying to mitigate the fault so

267
00:13:46,529 --> 00:13:53,430
one of the nice things about using

268
00:13:50,010 --> 00:13:56,010
machine learning for that was looking at

269
00:13:53,430 --> 00:13:58,290
historical data we were able to and this

270
00:13:56,010 --> 00:14:02,760
is the one area that we actually saw

271
00:13:58,290 --> 00:14:04,889
some good results we were able to see a

272
00:14:02,760 --> 00:14:07,649
lot of patterns in the data over a

273
00:14:04,889 --> 00:14:10,040
period of time so all these faults just

274
00:14:07,649 --> 00:14:13,550
don't happen overnight there's a lot of

275
00:14:10,040 --> 00:14:15,990
underlying symptoms that are there and

276
00:14:13,550 --> 00:14:17,550
the machine learning algorithm was

277
00:14:15,990 --> 00:14:20,640
actually able to detect some of these

278
00:14:17,550 --> 00:14:23,579
underlying patterns so even though the

279
00:14:20,640 --> 00:14:24,930
fault didn't really actually had

280
00:14:23,579 --> 00:14:26,939
occurred but there was a good

281
00:14:24,930 --> 00:14:29,370
probability the fault was going to occur

282
00:14:26,940 --> 00:14:30,959
so you could predict and say yeah this

283
00:14:29,370 --> 00:14:33,779
is you're seeing a lot of say for

284
00:14:30,959 --> 00:14:36,359
example TCP duplicate ACKs then you can

285
00:14:33,779 --> 00:14:38,010
say you know what maybe there's some

286
00:14:36,360 --> 00:14:40,500
problem with your link and that's why

287
00:14:38,010 --> 00:14:42,300
you're seeing so much latency and so

288
00:14:40,500 --> 00:14:44,310
those kinds of patterns we were able to

289
00:14:42,300 --> 00:14:48,240
see and this was one of the ones that I

290
00:14:44,310 --> 00:14:50,939
was really excited about because you

291
00:14:48,240 --> 00:14:52,920
could actually see things that you don't

292
00:14:50,940 --> 00:14:54,839
have to wait until something breaks and

293
00:14:52,920 --> 00:15:03,410
then you're scrambling around trying to

294
00:14:54,839 --> 00:15:05,630
isolate it and and find it so what does

295
00:15:03,410 --> 00:15:09,779
building a machine learning

296
00:15:05,630 --> 00:15:12,060
infrastructure mean is you need your

297
00:15:09,779 --> 00:15:15,600
data collection so you need to set up

298
00:15:12,060 --> 00:15:18,869
your device setup tap sport mirrors flow

299
00:15:15,600 --> 00:15:20,040
monitors or anything else you need the

300
00:15:18,870 --> 00:15:22,470
primarily you

301
00:15:20,040 --> 00:15:25,770
open-source tools if you already have

302
00:15:22,470 --> 00:15:28,530
data collection sort of framework in

303
00:15:25,770 --> 00:15:31,800
your network you can use any of the

304
00:15:28,530 --> 00:15:34,860
commercial tools that can get syslog

305
00:15:31,800 --> 00:15:39,180
data or packet data or any of the flow

306
00:15:34,860 --> 00:15:41,780
data the second piece of it was feature

307
00:15:39,180 --> 00:15:44,729
extraction like I said this was the most

308
00:15:41,780 --> 00:15:47,760
tedious and hard thing to do and you

309
00:15:44,730 --> 00:15:49,080
really needed domain expertise so if you

310
00:15:47,760 --> 00:15:50,970
know people are thinking machine

311
00:15:49,080 --> 00:15:53,880
learning is going to suddenly solve all

312
00:15:50,970 --> 00:15:57,120
problems and we won't need any more

313
00:15:53,880 --> 00:15:58,560
network engineers this is what proved us

314
00:15:57,120 --> 00:16:01,050
that no because you still need to

315
00:15:58,560 --> 00:16:04,050
understand how networks work to be able

316
00:16:01,050 --> 00:16:05,729
to train the machine learning model

317
00:16:04,050 --> 00:16:10,410
accurately for it to actually do

318
00:16:05,730 --> 00:16:14,130
something so we used Python pandas and

319
00:16:10,410 --> 00:16:16,469
numpy to do lot of the data

320
00:16:14,130 --> 00:16:19,260
pre-processing and managing and getting

321
00:16:16,470 --> 00:16:22,080
features and things like that I don't

322
00:16:19,260 --> 00:16:26,550
know there are some commercial tools

323
00:16:22,080 --> 00:16:28,950
that does feature extraction but the

324
00:16:26,550 --> 00:16:30,740
most of them are very expensive and they

325
00:16:28,950 --> 00:16:35,400
don't really work that well for

326
00:16:30,740 --> 00:16:37,170
networking data so I don't know man you

327
00:16:35,400 --> 00:16:40,350
could probably try it but I don't know

328
00:16:37,170 --> 00:16:42,930
if it's very effective the third part is

329
00:16:40,350 --> 00:16:46,650
the algorithm selection even though

330
00:16:42,930 --> 00:16:49,229
there are papers written that says you

331
00:16:46,650 --> 00:16:54,449
know this algorithm performs well for

332
00:16:49,230 --> 00:16:56,940
this type of data set if you vary even

333
00:16:54,450 --> 00:16:59,850
one or two attributes it completely

334
00:16:56,940 --> 00:17:02,400
changes so this was basically trial and

335
00:16:59,850 --> 00:17:04,440
error trial and error not in the sense

336
00:17:02,400 --> 00:17:06,900
that we tried every algorithm there was

337
00:17:04,440 --> 00:17:09,420
we kind of took a baseline and said okay

338
00:17:06,900 --> 00:17:12,000
this algorithm does this so this may be

339
00:17:09,420 --> 00:17:15,449
a better fit because there's no time or

340
00:17:12,000 --> 00:17:17,280
money to run every algorithm so we ran

341
00:17:15,449 --> 00:17:20,040
about three algorithms that we thought

342
00:17:17,280 --> 00:17:24,359
were the best fit for the problem that

343
00:17:20,040 --> 00:17:26,760
we were solving and then you have like

344
00:17:24,359 --> 00:17:29,040
deep learning which I still don't

345
00:17:26,760 --> 00:17:31,170
understand and I still don't people

346
00:17:29,040 --> 00:17:33,059
always come and ask me why aren't you

347
00:17:31,170 --> 00:17:36,480
doing deep learning and they can

348
00:17:33,059 --> 00:17:39,059
me why I should be doing this but if

349
00:17:36,480 --> 00:17:43,289
anybody wants to try out that's their

350
00:17:39,059 --> 00:17:44,970
data storage is another area that you

351
00:17:43,289 --> 00:17:48,090
really need to think about because it's

352
00:17:44,970 --> 00:17:50,789
not all SQL storage SQL is a lot of

353
00:17:48,090 --> 00:17:55,199
overhead for some of these things so you

354
00:17:50,789 --> 00:17:57,299
can have keystore flat files document

355
00:17:55,200 --> 00:17:59,580
storage for the model storage and and

356
00:17:57,299 --> 00:18:02,908
things like that so it you still have to

357
00:17:59,580 --> 00:18:04,949
kind of think about your data base your

358
00:18:02,909 --> 00:18:11,580
data storage and how much data you want

359
00:18:04,950 --> 00:18:14,519
to store and how you want to store it so

360
00:18:11,580 --> 00:18:17,279
I'm going to talk about three I mean

361
00:18:14,519 --> 00:18:18,809
sorry two case studies one is the

362
00:18:17,279 --> 00:18:20,549
traffic classification like I said

363
00:18:18,809 --> 00:18:22,889
that's the one that we day traffic

364
00:18:20,549 --> 00:18:25,230
classification fault detection but the

365
00:18:22,889 --> 00:18:29,039
two that we really spent a lot of time

366
00:18:25,230 --> 00:18:33,419
on so what we did we tested it on a

367
00:18:29,039 --> 00:18:37,619
small network with 250 to 500 devices we

368
00:18:33,419 --> 00:18:42,149
did a pork based analysis and it gave us

369
00:18:37,619 --> 00:18:44,039
about 70% accurate identification we

370
00:18:42,149 --> 00:18:46,379
collected that flow from three different

371
00:18:44,039 --> 00:18:48,179
networks for seven days and 30 minute

372
00:18:46,379 --> 00:18:50,029
increments we did a lot of filtering and

373
00:18:48,179 --> 00:18:52,350
sampling that was still a lot of data

374
00:18:50,029 --> 00:18:54,269
and we didn't want to use all the data

375
00:18:52,350 --> 00:18:58,110
so to get the spatial and temporal

376
00:18:54,269 --> 00:19:02,100
diversity the sort of did sampling and

377
00:18:58,110 --> 00:19:06,678
filtering and we did a 70/30 split and

378
00:19:02,100 --> 00:19:06,678
we used the three algorithms to test it

379
00:19:08,720 --> 00:19:14,460
so this is if you can see most of its

380
00:19:12,240 --> 00:19:18,710
HTTP traffic and some of the

381
00:19:14,460 --> 00:19:24,809
non-standard ports there the pork based

382
00:19:18,710 --> 00:19:27,929
was not able to identify and again this

383
00:19:24,809 --> 00:19:29,610
isn't all the features we use this is

384
00:19:27,929 --> 00:19:32,759
just a sort of a sampling of the

385
00:19:29,610 --> 00:19:35,629
features we used and so we had provided

386
00:19:32,759 --> 00:19:39,570
the labels using the pork base so it

387
00:19:35,629 --> 00:19:41,939
this many labels and this was the

388
00:19:39,570 --> 00:19:43,710
precision and the recall and Efrain

389
00:19:41,940 --> 00:19:46,259
score is a combination of precision and

390
00:19:43,710 --> 00:19:48,809
recall position is how accurately it

391
00:19:46,259 --> 00:19:50,580
identified and recall as how well it was

392
00:19:48,809 --> 00:19:53,940
able to remember so if you see the

393
00:19:50,580 --> 00:19:57,029
Microsoft domain server it wasn't able

394
00:19:53,940 --> 00:20:00,480
to recall and we think it's because the

395
00:19:57,029 --> 00:20:01,799
number of samples was too small that's

396
00:20:00,480 --> 00:20:03,989
another thing there's a lot of

397
00:20:01,799 --> 00:20:06,330
explainable AI explainable machine

398
00:20:03,989 --> 00:20:08,879
learning and we did try to go back and

399
00:20:06,330 --> 00:20:10,529
see what it was learning and what it was

400
00:20:08,879 --> 00:20:13,439
doing but there's still some of that

401
00:20:10,529 --> 00:20:16,350
black magic that happens that it's still

402
00:20:13,440 --> 00:20:21,269
hard to figure out but we think that's

403
00:20:16,350 --> 00:20:28,559
because of the number of samples was too

404
00:20:21,269 --> 00:20:29,960
less so the decision tree random forests

405
00:20:28,559 --> 00:20:32,070
and support vector machines

406
00:20:29,960 --> 00:20:34,649
unfortunately support vector machines

407
00:20:32,070 --> 00:20:37,769
did not give us any more accuracy than

408
00:20:34,649 --> 00:20:39,479
just pork based I think it's just

409
00:20:37,769 --> 00:20:42,600
because the way the algorithm works

410
00:20:39,480 --> 00:20:44,940
wasn't really helpful for this so

411
00:20:42,600 --> 00:20:48,830
decision tree and random forests were

412
00:20:44,940 --> 00:20:48,830
the two that gave us comparable results

413
00:20:49,070 --> 00:20:55,049
so for the fault detection we kind of

414
00:20:52,679 --> 00:20:59,239
did the same thing devices I mean

415
00:20:55,049 --> 00:21:03,299
networks between 200 and 500 devices and

416
00:20:59,239 --> 00:21:06,570
we collected packet captures and we

417
00:21:03,299 --> 00:21:09,840
basically sampled using 800,000 samples

418
00:21:06,570 --> 00:21:12,090
we did not test support vector because

419
00:21:09,840 --> 00:21:13,799
we thought you know since we could even

420
00:21:12,090 --> 00:21:16,109
get it to work for a simple case this

421
00:21:13,799 --> 00:21:18,509
was more complex so we just decided to

422
00:21:16,109 --> 00:21:23,699
test with decision tree and random

423
00:21:18,509 --> 00:21:25,769
forests so these are the some of the

424
00:21:23,700 --> 00:21:28,440
these are some of the features we used

425
00:21:25,769 --> 00:21:30,809
again this isn't exhaustive and a lot of

426
00:21:28,440 --> 00:21:33,769
it we took aggregation we did standard

427
00:21:30,809 --> 00:21:36,899
deviation we did coefficient correlation

428
00:21:33,769 --> 00:21:39,720
this isn't just trade we did have to do

429
00:21:36,899 --> 00:21:42,479
a lot of math in there to get so this is

430
00:21:39,720 --> 00:21:48,090
sort of an aggregate of multiple streams

431
00:21:42,480 --> 00:21:50,519
over a period of time and so this is

432
00:21:48,090 --> 00:21:54,059
what we found so there was a threshold

433
00:21:50,519 --> 00:21:56,519
after which it wasn't learning so what

434
00:21:54,059 --> 00:21:58,220
we realized and we did this for a lot of

435
00:21:56,519 --> 00:22:01,100
different samples

436
00:21:58,220 --> 00:22:04,340
and what we realized was it wasn't the

437
00:22:01,100 --> 00:22:06,110
number of samples that really mattered

438
00:22:04,340 --> 00:22:08,418
what mattered was the quality of data

439
00:22:06,110 --> 00:22:12,770
that we fed into it so as you can see

440
00:22:08,419 --> 00:22:19,309
around 45,000 it tapers off and after

441
00:22:12,770 --> 00:22:21,889
that it was basically a flat curve so we

442
00:22:19,309 --> 00:22:24,408
had also sort of bought into this whole

443
00:22:21,890 --> 00:22:26,270
big data throw as much data as you can

444
00:22:24,409 --> 00:22:30,650
at the algorithm and it'll figure out

445
00:22:26,270 --> 00:22:32,418
and our experience was it wasn't if you

446
00:22:30,650 --> 00:22:35,270
can actually provide it better quality

447
00:22:32,419 --> 00:22:38,450
data you can do it with a lot less than

448
00:22:35,270 --> 00:22:42,408
all the big data that you throw at it

449
00:22:38,450 --> 00:22:45,230
but creating that quality data comes

450
00:22:42,409 --> 00:22:46,850
with a cost so you either I don't know

451
00:22:45,230 --> 00:22:49,429
maybe if you put in a lot of data that

452
00:22:46,850 --> 00:22:51,620
eventually it will kind of get there but

453
00:22:49,429 --> 00:22:53,390
there's a cost involved in either one of

454
00:22:51,620 --> 00:23:00,949
them so you have to pick where the cost

455
00:22:53,390 --> 00:23:03,860
is so this is what we had for the event

456
00:23:00,950 --> 00:23:06,140
so that's the big bar is the normal

457
00:23:03,860 --> 00:23:08,750
events and these are all some of the

458
00:23:06,140 --> 00:23:12,470
fault and performance events that we

459
00:23:08,750 --> 00:23:15,799
sent one of the things that we found was

460
00:23:12,470 --> 00:23:18,470
we use some syslog data to kind of add

461
00:23:15,799 --> 00:23:21,860
to it and that gave it a context because

462
00:23:18,470 --> 00:23:23,659
unfortunately packet data it gives you

463
00:23:21,860 --> 00:23:25,549
the network view but it doesn't quite

464
00:23:23,659 --> 00:23:28,669
give you the server view so we had to

465
00:23:25,549 --> 00:23:31,639
use some context from the server data to

466
00:23:28,669 --> 00:23:33,799
get like the server faults if the

467
00:23:31,640 --> 00:23:35,900
server's functioning or service

468
00:23:33,799 --> 00:23:37,370
malfunctioning or something's wrong

469
00:23:35,900 --> 00:23:40,460
because the server is the one that's

470
00:23:37,370 --> 00:23:42,320
down not necessarily the network the

471
00:23:40,460 --> 00:23:45,830
other thing that we were trying again I

472
00:23:42,320 --> 00:23:47,750
said the control plane telemetry there's

473
00:23:45,830 --> 00:23:50,570
a lot of hype with streaming telemetry

474
00:23:47,750 --> 00:23:53,510
and everybody is doing but we weren't

475
00:23:50,570 --> 00:23:55,010
able to get any useful data to actually

476
00:23:53,510 --> 00:23:57,350
test but that's one of the things that

477
00:23:55,010 --> 00:24:00,440
we are kind of working on to see if we

478
00:23:57,350 --> 00:24:04,850
can get something to test it and it

479
00:24:00,440 --> 00:24:06,980
would actually help in some where there

480
00:24:04,850 --> 00:24:09,020
was no real difference between the

481
00:24:06,980 --> 00:24:11,750
decision tree and the random forest in

482
00:24:09,020 --> 00:24:14,440
this one so

483
00:24:11,750 --> 00:24:19,100
this should really be five slides

484
00:24:14,440 --> 00:24:21,649
there's so many challenges the biggest

485
00:24:19,100 --> 00:24:26,000
challenge was data collection data was

486
00:24:21,649 --> 00:24:28,489
very inconsistent very messy sparse so

487
00:24:26,000 --> 00:24:32,029
trying to like get the data that was

488
00:24:28,490 --> 00:24:34,009
useful so a lot of work how to use

489
00:24:32,029 --> 00:24:37,009
choose the right machine learning for a

490
00:24:34,009 --> 00:24:40,940
particular networking problem was also

491
00:24:37,009 --> 00:24:43,190
just trial and error we tested in a

492
00:24:40,940 --> 00:24:47,000
small network so how do you make it

493
00:24:43,190 --> 00:24:49,429
scalable to a large network maybe there

494
00:24:47,000 --> 00:24:52,730
is no way to scale it with what we have

495
00:24:49,429 --> 00:24:54,740
maybe we do have to do smaller segments

496
00:24:52,730 --> 00:24:58,429
of the network I don't know but right

497
00:24:54,740 --> 00:25:02,720
now I don't see a way to scale it to you

498
00:24:58,429 --> 00:25:06,200
know devices 30,000 devices and and get

499
00:25:02,720 --> 00:25:07,720
any kind of result with it so this was

500
00:25:06,200 --> 00:25:11,090
another challenge how do you make

501
00:25:07,720 --> 00:25:14,720
machine learning models learn uniformly

502
00:25:11,090 --> 00:25:17,000
across non uniformly designed networks

503
00:25:14,720 --> 00:25:19,309
all of you who work in networks network

504
00:25:17,000 --> 00:25:21,500
design knows none of the networks are

505
00:25:19,309 --> 00:25:23,990
actually all they're all designed

506
00:25:21,500 --> 00:25:26,720
differently so how do you know that it's

507
00:25:23,990 --> 00:25:29,750
actually learning the right thing in

508
00:25:26,720 --> 00:25:33,440
each network so that's another challenge

509
00:25:29,750 --> 00:25:36,110
so some of the things that I would like

510
00:25:33,440 --> 00:25:37,759
to see they're not necessarily future

511
00:25:36,110 --> 00:25:40,570
trends and that's where there's a

512
00:25:37,759 --> 00:25:43,970
question mark is more like a wish list

513
00:25:40,570 --> 00:25:46,029
is it would be nice to have a more

514
00:25:43,970 --> 00:25:48,649
centralized data collection especially

515
00:25:46,029 --> 00:25:50,779
data from different domains and

516
00:25:48,649 --> 00:25:53,178
different vantage points instead of

517
00:25:50,779 --> 00:25:56,149
having to set up all these different

518
00:25:53,179 --> 00:26:01,940
data collection mechanisms so hopefully

519
00:25:56,149 --> 00:26:05,209
Sdn and if we might give us some you

520
00:26:01,940 --> 00:26:07,700
know path forward for that it would be

521
00:26:05,210 --> 00:26:10,309
nice to have open datasets I know that

522
00:26:07,700 --> 00:26:11,120
Canada has some but it wasn't really

523
00:26:10,309 --> 00:26:14,200
that useful

524
00:26:11,120 --> 00:26:16,939
we actually got data from customers

525
00:26:14,200 --> 00:26:19,730
customer networks it would be nice to

526
00:26:16,940 --> 00:26:24,529
have like standardized open datasets for

527
00:26:19,730 --> 00:26:25,520
specific problems feature extraction

528
00:26:24,529 --> 00:26:29,030
like I said that was

529
00:26:25,520 --> 00:26:32,510
pretty tedious task as well so it'd be

530
00:26:29,030 --> 00:26:35,930
nice to standardize the training data

531
00:26:32,510 --> 00:26:38,210
set size and maybe even the features and

532
00:26:35,930 --> 00:26:41,690
say okay you know for this specific

533
00:26:38,210 --> 00:26:43,610
application these types of features will

534
00:26:41,690 --> 00:26:46,270
give you the best result it would be

535
00:26:43,610 --> 00:26:50,149
nice to have that standardized across

536
00:26:46,270 --> 00:26:52,790
different Network applications the

537
00:26:50,150 --> 00:26:55,250
machine learning algorithm robustness

538
00:26:52,790 --> 00:26:57,470
that's outside the realm of really

539
00:26:55,250 --> 00:26:59,510
machine learning for networking there's

540
00:26:57,470 --> 00:27:03,380
a lot of research going on in making the

541
00:26:59,510 --> 00:27:06,470
algorithms more robust and also making

542
00:27:03,380 --> 00:27:09,770
them more explainable to really know

543
00:27:06,470 --> 00:27:12,740
what it is learning and I think that

544
00:27:09,770 --> 00:27:15,080
would be nice when we get there I think

545
00:27:12,740 --> 00:27:16,700
that would help as well there's

546
00:27:15,080 --> 00:27:18,649
automatic selection of the right

547
00:27:16,700 --> 00:27:21,290
algorithm I know there is Auto ml

548
00:27:18,650 --> 00:27:23,990
frameworks that Google has and some

549
00:27:21,290 --> 00:27:27,490
other things but again they all work for

550
00:27:23,990 --> 00:27:30,640
maybe more simplified data sets not

551
00:27:27,490 --> 00:27:33,920
complex data sets we have in networking

552
00:27:30,640 --> 00:27:37,100
again combining ground truths and models

553
00:27:33,920 --> 00:27:39,440
from different vantage points is take

554
00:27:37,100 --> 00:27:41,870
different networks and standardize it

555
00:27:39,440 --> 00:27:49,370
and see what it means across multiple

556
00:27:41,870 --> 00:27:51,379
networks any questions any anybody has

557
00:27:49,370 --> 00:27:56,810
any use cases I would love to hear from

558
00:27:51,380 --> 00:27:58,970
you or any way to use the data but other

559
00:27:56,810 --> 00:28:02,149
than any questions I may not have all

560
00:27:58,970 --> 00:28:04,820
the answers but yeah I'm curious to what

561
00:28:02,150 --> 00:28:07,210
extent is this a they set it and forget

562
00:28:04,820 --> 00:28:10,700
it after you get the machine language

563
00:28:07,210 --> 00:28:13,250
model tuned and to what extent you have

564
00:28:10,700 --> 00:28:15,110
to babysit and keep going back to it and

565
00:28:13,250 --> 00:28:19,070
how much time is involved on the ongoing

566
00:28:15,110 --> 00:28:21,379
basis um this was done using historical

567
00:28:19,070 --> 00:28:24,260
like the collected data over a period of

568
00:28:21,380 --> 00:28:26,750
time so for you to really keep it

569
00:28:24,260 --> 00:28:29,810
adaptive you will have to go back

570
00:28:26,750 --> 00:28:32,780
because networks change right so let's

571
00:28:29,810 --> 00:28:34,220
say you add a new device your traffic

572
00:28:32,780 --> 00:28:35,600
pattern will change hopefully the device

573
00:28:34,220 --> 00:28:38,010
gives you a better performance so you

574
00:28:35,600 --> 00:28:39,928
need to add the data and

575
00:28:38,010 --> 00:28:42,210
do incremental learning so there's

576
00:28:39,929 --> 00:28:45,150
reinforcement learning that you relearn

577
00:28:42,210 --> 00:28:46,290
the model but you're not do relearning

578
00:28:45,150 --> 00:28:52,290
the whole thing you're just

579
00:28:46,290 --> 00:28:54,480
incrementally learning yes probably yeah

580
00:28:52,290 --> 00:28:57,950
it's just a network engineering position

581
00:28:54,480 --> 00:29:02,190
might be morph into something else

582
00:28:57,950 --> 00:29:05,880
yeah Dan McGrew en Johns Hopkins APL and

583
00:29:02,190 --> 00:29:08,580
if so excellent talk

584
00:29:05,880 --> 00:29:10,020
especially given where ml is in the hype

585
00:29:08,580 --> 00:29:14,490
cycle right now yes

586
00:29:10,020 --> 00:29:16,980
on your last slide why why do you feel

587
00:29:14,490 --> 00:29:18,450
that Sdn and a fee what's gonna help

588
00:29:16,980 --> 00:29:19,860
with your data collection problem I

589
00:29:18,450 --> 00:29:23,400
didn't quite follow your logic there

590
00:29:19,860 --> 00:29:26,309
because and again this is what I have

591
00:29:23,400 --> 00:29:30,030
been reading is that once you have the

592
00:29:26,309 --> 00:29:32,160
control plane separated most of the like

593
00:29:30,030 --> 00:29:33,960
the control plane data is going to be

594
00:29:32,160 --> 00:29:36,720
centralized you're not trying to get

595
00:29:33,960 --> 00:29:40,250
data from the data plane anymore again

596
00:29:36,720 --> 00:29:42,630
that may be in the hype cycle as well

597
00:29:40,250 --> 00:29:47,790
you're making some assumptions about

598
00:29:42,630 --> 00:29:49,740
what Sdn is gonna do for yes it helped

599
00:29:47,790 --> 00:29:52,139
that much at least in our case yeah

600
00:29:49,740 --> 00:29:57,690
thank you though

601
00:29:52,140 --> 00:29:58,440
AJ Red Hat is this product or that stuff

602
00:29:57,690 --> 00:30:01,110
that he working for

603
00:29:58,440 --> 00:30:02,850
is there any beta version of it it

604
00:30:01,110 --> 00:30:04,379
sounds very interesting right and but

605
00:30:02,850 --> 00:30:06,540
it's also sounds like there's a lot of

606
00:30:04,380 --> 00:30:08,970
things that are not solved and this is

607
00:30:06,540 --> 00:30:11,370
work in progress right and there seems

608
00:30:08,970 --> 00:30:13,740
to be a call to action to community to

609
00:30:11,370 --> 00:30:16,918
kind of yeah and and we do have a beta

610
00:30:13,740 --> 00:30:19,140
version of this and one of the things is

611
00:30:16,919 --> 00:30:20,970
we're not trying to just throw some

612
00:30:19,140 --> 00:30:23,100
software at people and say go figure it

613
00:30:20,970 --> 00:30:26,390
out we want to actually work with the

614
00:30:23,100 --> 00:30:28,949
community to maybe build a framework and

615
00:30:26,390 --> 00:30:31,919
solve some of these challenges that we

616
00:30:28,950 --> 00:30:35,370
encounter to get some ground truths that

617
00:30:31,919 --> 00:30:36,960
it can actually help one of the things

618
00:30:35,370 --> 00:30:40,020
is machine learning is not a magic

619
00:30:36,960 --> 00:30:43,110
button but it does provide a new

620
00:30:40,020 --> 00:30:45,470
mechanism and pathway to address some of

621
00:30:43,110 --> 00:30:49,459
these problems as we saw and we do need

622
00:30:45,470 --> 00:30:49,460
new ways to look at things

623
00:30:50,860 --> 00:30:58,030
look at or is it still um most of the

624
00:30:54,400 --> 00:31:00,040
resources have been academic and the

625
00:30:58,030 --> 00:31:02,200
problem at lot of the academic papers is

626
00:31:00,040 --> 00:31:05,500
I don't think they are network engineers

627
00:31:02,200 --> 00:31:09,070
who are working on this so but I am

628
00:31:05,500 --> 00:31:18,420
trying to like do more and educate

629
00:31:09,070 --> 00:31:18,419
people any okay won't take a lot of time

630
00:31:24,140 --> 00:31:26,200
you

