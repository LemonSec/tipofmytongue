1
00:00:09,219 --> 00:00:18,320
hello morning we got a lot of background

2
00:00:13,129 --> 00:00:19,700
on drivers for running new data centers

3
00:00:18,320 --> 00:00:22,669
and I want to talk about a particular

4
00:00:19,700 --> 00:00:26,150
technology that's being standardized to

5
00:00:22,669 --> 00:00:29,920
support that so let's see if I can

6
00:00:26,150 --> 00:00:29,919
figure out which button to press

7
00:00:30,580 --> 00:00:37,160
that's this okay here we go

8
00:00:33,260 --> 00:00:39,470
okay so Omar did a great job talking

9
00:00:37,160 --> 00:00:41,150
about our DMA has a driver I mean it's

10
00:00:39,470 --> 00:00:44,360
really about the applications there's a

11
00:00:41,150 --> 00:00:46,400
number a number of growing number of low

12
00:00:44,360 --> 00:00:48,280
latency type of applications that we're

13
00:00:46,400 --> 00:00:50,470
trying to run in data centers particular

14
00:00:48,280 --> 00:00:54,769
distributed storage as we talked about

15
00:00:50,470 --> 00:00:57,229
artificial intelligence and trying to do

16
00:00:54,769 --> 00:00:59,030
this at cloud scale is where we start

17
00:00:57,229 --> 00:01:01,460
running into trouble we talking a home

18
00:00:59,030 --> 00:01:04,250
our talked great deal about a lot of the

19
00:01:01,460 --> 00:01:06,770
knobs that you have to set and configure

20
00:01:04,250 --> 00:01:08,450
to make this work and so we're really

21
00:01:06,770 --> 00:01:10,850
trying to figure out how can we how can

22
00:01:08,450 --> 00:01:13,159
we do this in a much easier way there's

23
00:01:10,850 --> 00:01:14,859
a there's a number of papers here that

24
00:01:13,159 --> 00:01:17,060
have been referenced about our DMA

25
00:01:14,859 --> 00:01:19,038
running it at scale things that have

26
00:01:17,060 --> 00:01:21,890
happened recently to try to make it work

27
00:01:19,039 --> 00:01:24,979
in bigger networks these are great great

28
00:01:21,890 --> 00:01:26,090
examples that we can talk a look at but

29
00:01:24,979 --> 00:01:28,520
in the end of the day what we're really

30
00:01:26,090 --> 00:01:30,619
trying to do is really trying to take

31
00:01:28,520 --> 00:01:33,079
that high performance computing and our

32
00:01:30,619 --> 00:01:36,009
DMA type of network that was

33
00:01:33,079 --> 00:01:39,169
traditionally very private very

34
00:01:36,009 --> 00:01:41,179
customized on off to the side and make

35
00:01:39,170 --> 00:01:43,579
that mainstream make that a workload

36
00:01:41,179 --> 00:01:46,310
that's in a hyper scale type of data

37
00:01:43,579 --> 00:01:49,339
center so what do we have to do to do

38
00:01:46,310 --> 00:01:50,929
that and you because you know that's the

39
00:01:49,340 --> 00:01:54,109
goal right getting things bigger cheaper

40
00:01:50,929 --> 00:01:55,280
faster and so we got to eliminate a lot

41
00:01:54,109 --> 00:01:56,780
of that complexity that the

42
00:01:55,280 --> 00:01:59,179
configuration and things that we just

43
00:01:56,780 --> 00:02:02,359
heard about and we got to try to figure

44
00:01:59,179 --> 00:02:04,340
out what it's needed to to you know

45
00:02:02,359 --> 00:02:06,798
really make it scale at that level so

46
00:02:04,340 --> 00:02:09,380
those are kind of key drivers so we kind

47
00:02:06,799 --> 00:02:12,290
of saw a network already it was a great

48
00:02:09,380 --> 00:02:13,760
talk yesterday about cost networks

49
00:02:12,290 --> 00:02:15,620
that's really kind of the state of our

50
00:02:13,760 --> 00:02:18,470
state of the art of data center network

51
00:02:15,620 --> 00:02:22,100
so they're kind of layer three networks

52
00:02:18,470 --> 00:02:23,030
with layer 2 links configured in it kind

53
00:02:22,100 --> 00:02:25,849
of a cloth

54
00:02:23,030 --> 00:02:27,769
style configuration and and Omar showed

55
00:02:25,849 --> 00:02:29,840
this slide as well which is sort of the

56
00:02:27,769 --> 00:02:34,249
way congestion is kind of handled today

57
00:02:29,840 --> 00:02:36,499
in those networks which is relying on

58
00:02:34,249 --> 00:02:39,079
the the support of the network to

59
00:02:36,499 --> 00:02:41,239
identify when there's congestion marking

60
00:02:39,079 --> 00:02:43,519
those packets so that as they travel

61
00:02:41,239 --> 00:02:47,500
across the network from say left to

62
00:02:43,519 --> 00:02:49,669
right here the the destination can

63
00:02:47,500 --> 00:02:52,519
determined that these packets were

64
00:02:49,669 --> 00:02:55,159
experiencing congestion so it can signal

65
00:02:52,519 --> 00:02:57,590
back to the source through the

66
00:02:55,159 --> 00:02:59,450
acknowledgement process that there's

67
00:02:57,590 --> 00:03:02,000
congestion on this path so why don't you

68
00:02:59,450 --> 00:03:03,589
slow down your rate of injection and

69
00:03:02,000 --> 00:03:05,389
then hopefully that congestion will

70
00:03:03,590 --> 00:03:08,780
subside so that's kind of a key thing

71
00:03:05,389 --> 00:03:12,530
that happens now also Omar talked about

72
00:03:08,780 --> 00:03:14,840
our you know our DMA which was evolved

73
00:03:12,530 --> 00:03:17,209
on top of InfiniBand which was

74
00:03:14,840 --> 00:03:20,049
fundamentally a lossless fabric so a lot

75
00:03:17,209 --> 00:03:22,219
of the protocol was built in order to

76
00:03:20,049 --> 00:03:24,560
assume you know hey I'm not gonna drop

77
00:03:22,219 --> 00:03:26,689
packets at layer two so we can simplify

78
00:03:24,560 --> 00:03:30,199
things and we can offload everything

79
00:03:26,689 --> 00:03:32,478
onto the NIC um so if so if we're trying

80
00:03:30,199 --> 00:03:34,159
to emulate that on Ethernet how do we

81
00:03:32,479 --> 00:03:36,949
create a lossless Ethernet environment

82
00:03:34,159 --> 00:03:38,840
and so you know ultimately you have to

83
00:03:36,949 --> 00:03:42,259
rely on this priority based flow control

84
00:03:38,840 --> 00:03:44,000
in order to not drop a packet and you

85
00:03:42,259 --> 00:03:45,768
know this is where things get really

86
00:03:44,000 --> 00:03:49,549
complicated and difficult to configure

87
00:03:45,769 --> 00:03:52,549
as we sort of just described today

88
00:03:49,549 --> 00:03:56,120
traffic classes are sort of identified

89
00:03:52,549 --> 00:03:57,979
to using layer 3 DHCP code points for

90
00:03:56,120 --> 00:04:00,590
example perhaps maybe not even using

91
00:03:57,979 --> 00:04:03,590
VLAN headers but so this is kind of the

92
00:04:00,590 --> 00:04:06,349
state of the art today that that exists

93
00:04:03,590 --> 00:04:09,290
in a lossless Network this is getting

94
00:04:06,349 --> 00:04:11,959
increasingly difficult to support going

95
00:04:09,290 --> 00:04:13,489
forward so if you you know as we talked

96
00:04:11,959 --> 00:04:15,290
about we want to scale the network so

97
00:04:13,489 --> 00:04:18,320
we're gonna have more hops which means

98
00:04:15,290 --> 00:04:19,969
there's more data in flight but at the

99
00:04:18,320 --> 00:04:21,858
same time we're speeding up the links

100
00:04:19,969 --> 00:04:24,699
we're going from two 100 gig and now

101
00:04:21,858 --> 00:04:27,740
there's 400 gig switch ports available

102
00:04:24,699 --> 00:04:31,070
so there's a ton of data in flight but

103
00:04:27,740 --> 00:04:33,740
the amount of memory per packet per port

104
00:04:31,070 --> 00:04:36,319
per gigabit is actually going down so

105
00:04:33,740 --> 00:04:39,919
this is looking at some Broadcom silicon

106
00:04:36,319 --> 00:04:42,319
and looking at how much buffer do we

107
00:04:39,919 --> 00:04:43,969
actually have available as this

108
00:04:42,319 --> 00:04:46,370
performance good and it's physics it's

109
00:04:43,969 --> 00:04:50,990
really difficult to build silicon with

110
00:04:46,370 --> 00:04:53,120
that much memory you you know as and

111
00:04:50,990 --> 00:04:55,189
speeds as they go up so so we're really

112
00:04:53,120 --> 00:04:58,580
putting pressure on the switch buffering

113
00:04:55,189 --> 00:05:00,529
and and another important thing is we

114
00:04:58,580 --> 00:05:03,620
want low latency as we talked about so

115
00:05:00,529 --> 00:05:06,020
having small buffers is good but as we

116
00:05:03,620 --> 00:05:08,270
get bigger and we have faster links

117
00:05:06,020 --> 00:05:11,120
we're gonna have more data in flight so

118
00:05:08,270 --> 00:05:13,430
it's more difficult to absorb the you

119
00:05:11,120 --> 00:05:16,310
know the bursts if you will so so we run

120
00:05:13,430 --> 00:05:17,900
into the scenario of perhaps using

121
00:05:16,310 --> 00:05:20,479
priority based flow control more

122
00:05:17,900 --> 00:05:24,080
frequently than we want to and and the

123
00:05:20,479 --> 00:05:26,300
side effects associated with that so

124
00:05:24,080 --> 00:05:29,120
eight or 2.1 which is where we do the

125
00:05:26,300 --> 00:05:30,889
standards for Ethernet switching has a

126
00:05:29,120 --> 00:05:33,050
couple tools in their tool bag and we

127
00:05:30,889 --> 00:05:35,870
talked already about priority based flow

128
00:05:33,050 --> 00:05:38,689
control and most operators and I'm aware

129
00:05:35,870 --> 00:05:40,939
of don't like it don't use it if you use

130
00:05:38,689 --> 00:05:44,419
it perhaps you use it at the edges of

131
00:05:40,939 --> 00:05:46,849
the network only or you're using it in a

132
00:05:44,419 --> 00:05:49,520
very small isolated you know high

133
00:05:46,849 --> 00:05:51,860
performance cluster and and part of the

134
00:05:49,520 --> 00:05:54,198
problem is you have to tune all of the

135
00:05:51,860 --> 00:05:56,000
thresholds and things exactly right and

136
00:05:54,199 --> 00:05:58,639
it differs from one vendor to another

137
00:05:56,000 --> 00:06:00,379
perhaps even one switch model to another

138
00:05:58,639 --> 00:06:02,810
switch model the the parameters are

139
00:06:00,379 --> 00:06:05,419
different so so calculating the amount

140
00:06:02,810 --> 00:06:07,819
of buffer you need to assure that you'll

141
00:06:05,419 --> 00:06:10,520
never drop a packet while that pause

142
00:06:07,819 --> 00:06:13,310
frame is transitioning the link can be

143
00:06:10,520 --> 00:06:15,438
very cat you know complicated but it

144
00:06:13,310 --> 00:06:16,699
also has some negative side effects as

145
00:06:15,439 --> 00:06:19,819
we talked about the head of line

146
00:06:16,699 --> 00:06:21,169
blocking which is when I pause a traffic

147
00:06:19,819 --> 00:06:23,899
class that means everything in that

148
00:06:21,169 --> 00:06:26,419
traffic class upstream will stop even if

149
00:06:23,899 --> 00:06:28,490
a flow is going somewhere else away from

150
00:06:26,419 --> 00:06:30,830
the congestion that that creates the

151
00:06:28,490 --> 00:06:34,249
head of line blocking scenario and then

152
00:06:30,830 --> 00:06:36,198
as we wait as we backup in one switch

153
00:06:34,249 --> 00:06:37,610
will push back to the next switch and

154
00:06:36,199 --> 00:06:39,800
push back the next switch and have

155
00:06:37,610 --> 00:06:42,589
ultimately potentially bring the network

156
00:06:39,800 --> 00:06:44,209
performance down to the slowest link or

157
00:06:42,589 --> 00:06:46,250
the congested path and that on the

158
00:06:44,209 --> 00:06:49,490
network so congestion spreading becomes

159
00:06:46,250 --> 00:06:50,860
a huge problem and the sort of X on X

160
00:06:49,490 --> 00:06:52,990
off

161
00:06:50,860 --> 00:06:55,630
approach that priority-based flow

162
00:06:52,990 --> 00:06:57,160
control has create some oscillations in

163
00:06:55,630 --> 00:07:00,580
the network and you end up with what we

164
00:06:57,160 --> 00:07:02,650
call buffer bloat a lot of jitter and a

165
00:07:00,580 --> 00:07:05,139
lot of you know just sort of strange

166
00:07:02,650 --> 00:07:07,719
behavior in some cases even deadlocks so

167
00:07:05,139 --> 00:07:11,530
it's possible to have priority flow

168
00:07:07,720 --> 00:07:13,449
control in in a circle and effectively

169
00:07:11,530 --> 00:07:15,580
locking up the network so it's not a

170
00:07:13,449 --> 00:07:17,830
favorite tool people use however for a

171
00:07:15,580 --> 00:07:19,539
lossless Network it is sort of your last

172
00:07:17,830 --> 00:07:22,210
gasp you must use it you don't have

173
00:07:19,539 --> 00:07:24,460
another option right now

174
00:07:22,210 --> 00:07:26,948
802 also invented congestion

175
00:07:24,460 --> 00:07:29,440
notification as Omar briefly mentioned

176
00:07:26,949 --> 00:07:31,630
that was really built in order to

177
00:07:29,440 --> 00:07:34,900
support what we would call a large layer

178
00:07:31,630 --> 00:07:37,000
to network running protocols such as our

179
00:07:34,900 --> 00:07:39,849
DMA or even Fibre Channel over Ethernet

180
00:07:37,000 --> 00:07:42,130
that didn't have their own really

181
00:07:39,849 --> 00:07:45,719
congestion control layer or transport

182
00:07:42,130 --> 00:07:48,539
layer so the layer 2 network itself

183
00:07:45,720 --> 00:07:51,610
built a congestion management scheme

184
00:07:48,539 --> 00:07:53,710
that would again identify congestion and

185
00:07:51,610 --> 00:07:55,960
signal all the way across the layer to

186
00:07:53,710 --> 00:08:00,159
fabric to the source and have that

187
00:07:55,960 --> 00:08:02,440
source would then adjust his rate you

188
00:08:00,159 --> 00:08:04,330
know again we don't build as many large

189
00:08:02,440 --> 00:08:06,520
layer 2 networks anymore in order to

190
00:08:04,330 --> 00:08:08,979
scale so this this technology didn't get

191
00:08:06,520 --> 00:08:11,020
adopted too much things like Fibre

192
00:08:08,979 --> 00:08:13,150
Channel over Ethernet and the first

193
00:08:11,020 --> 00:08:17,020
version of rocky didn't have wide

194
00:08:13,150 --> 00:08:19,060
deployment and also the NIC benders I

195
00:08:17,020 --> 00:08:22,419
would argue really didn't like this

196
00:08:19,060 --> 00:08:24,729
scheme because they already are managing

197
00:08:22,419 --> 00:08:26,740
the rate injection by the higher layer

198
00:08:24,729 --> 00:08:28,780
transport protocol so in the end

199
00:08:26,740 --> 00:08:30,310
congestion control and now they have a

200
00:08:28,780 --> 00:08:32,679
layer 2 message that they're also

201
00:08:30,310 --> 00:08:34,570
getting and they have to reconcile rate

202
00:08:32,679 --> 00:08:35,760
control coming from the fabric as well

203
00:08:34,570 --> 00:08:39,070
so it was kind of a difficult

204
00:08:35,760 --> 00:08:40,649
implementation on the NIC as well and so

205
00:08:39,070 --> 00:08:43,690
it didn't really have very wide adoption

206
00:08:40,649 --> 00:08:46,420
so the project that we're working on now

207
00:08:43,690 --> 00:08:49,839
call congestion isolation is really an

208
00:08:46,420 --> 00:08:53,229
attempt to provide layer 2 support

209
00:08:49,839 --> 00:08:57,579
without the complexity of these other

210
00:08:53,230 --> 00:09:00,579
protocols and to allow scale as well so

211
00:08:57,579 --> 00:09:03,459
this is called congestion isolation and

212
00:09:00,579 --> 00:09:06,368
it was a project that has been going on

213
00:09:03,459 --> 00:09:07,989
for a while but recently adopted and

214
00:09:06,369 --> 00:09:10,809
drafts are now kind of currently being

215
00:09:07,990 --> 00:09:15,009
written and so we're in sort of the the

216
00:09:10,809 --> 00:09:17,230
the balloting process of that it was the

217
00:09:15,009 --> 00:09:19,149
motivation of for this is one of the

218
00:09:17,230 --> 00:09:21,639
technologies described in the nin dica

219
00:09:19,149 --> 00:09:23,649
paper that Roger mentioned as well so if

220
00:09:21,639 --> 00:09:26,079
you get a chance to read that an indica

221
00:09:23,649 --> 00:09:27,819
paper you can see some of the how this

222
00:09:26,079 --> 00:09:32,769
fits in with some other technologies

223
00:09:27,819 --> 00:09:34,689
such as load balancing and also push and

224
00:09:32,769 --> 00:09:36,369
pull kind of different scheduling

225
00:09:34,689 --> 00:09:38,439
transports as well so some other

226
00:09:36,369 --> 00:09:39,369
interesting topics there so let me

227
00:09:38,439 --> 00:09:41,469
explain a little bit more about

228
00:09:39,369 --> 00:09:43,990
congestion isolation how it how it

229
00:09:41,470 --> 00:09:45,519
actually works okay so first of all we

230
00:09:43,990 --> 00:09:47,470
talked about the state of the art here's

231
00:09:45,519 --> 00:09:50,199
will think how things look today we're

232
00:09:47,470 --> 00:09:52,629
in support of indian congestion control

233
00:09:50,199 --> 00:09:55,628
so we're assuming that the higher layers

234
00:09:52,629 --> 00:09:58,869
have something that it's going to slow

235
00:09:55,629 --> 00:10:00,759
them down right tcp or if it's a quick

236
00:09:58,869 --> 00:10:03,399
or something along these lines or a UDP

237
00:10:00,759 --> 00:10:07,049
based protocol it has congestion

238
00:10:03,399 --> 00:10:11,019
management itself so in the end exists

239
00:10:07,049 --> 00:10:12,879
and today again if we wanted to be

240
00:10:11,019 --> 00:10:15,790
completely lossless we would use

241
00:10:12,879 --> 00:10:17,860
priority based flow control to push

242
00:10:15,790 --> 00:10:19,389
upstream to stop so that we don't drop

243
00:10:17,860 --> 00:10:22,230
packets so that's kind of how things

244
00:10:19,389 --> 00:10:24,970
work today it has side effects of

245
00:10:22,230 --> 00:10:27,189
creating head-of-line blocking so with

246
00:10:24,970 --> 00:10:29,139
congestion isolation and in summary what

247
00:10:27,189 --> 00:10:31,868
we're really doing is we're defining two

248
00:10:29,139 --> 00:10:33,040
traffic classes you have a single

249
00:10:31,869 --> 00:10:34,629
traffic class where everything is

250
00:10:33,040 --> 00:10:35,980
running and then we have a separate

251
00:10:34,629 --> 00:10:38,860
traffic class that we're going to

252
00:10:35,980 --> 00:10:41,259
isolate flows that are creating

253
00:10:38,860 --> 00:10:43,269
congestion there was also a talk the

254
00:10:41,259 --> 00:10:45,100
other day about the isp in a box and i

255
00:10:43,269 --> 00:10:48,910
think he talked about fq coddle as a

256
00:10:45,100 --> 00:10:52,269
scheme of for queuing that really did a

257
00:10:48,910 --> 00:10:56,129
great job of maintaining predictable

258
00:10:52,269 --> 00:10:59,350
latency for for wide area communications

259
00:10:56,129 --> 00:11:02,350
there's some similarities here we don't

260
00:10:59,350 --> 00:11:04,689
have per flow cubes what we're doing is

261
00:11:02,350 --> 00:11:06,970
we're aggregating but we are using a

262
00:11:04,689 --> 00:11:08,829
separate queue for flows that are

263
00:11:06,970 --> 00:11:13,569
causing congestion and we are

264
00:11:08,829 --> 00:11:16,239
riku endear is when that congestion is

265
00:11:13,569 --> 00:11:17,120
building and the switch is beginning to

266
00:11:16,240 --> 00:11:19,490
mark pack

267
00:11:17,120 --> 00:11:21,860
with ecn so that it'll go back to the

268
00:11:19,490 --> 00:11:24,680
source the switch will also at that time

269
00:11:21,860 --> 00:11:26,930
begin to queue subsequent packets for

270
00:11:24,680 --> 00:11:28,550
those flows into a different traffic

271
00:11:26,930 --> 00:11:32,270
class so kind of moves them out of the

272
00:11:28,550 --> 00:11:37,069
way and then we will generate a message

273
00:11:32,270 --> 00:11:38,960
will signal to our upstream switch and

274
00:11:37,070 --> 00:11:40,760
in that signaling we have enough

275
00:11:38,960 --> 00:11:43,700
information that you can identify the

276
00:11:40,760 --> 00:11:45,800
same flow so the upstream switch will

277
00:11:43,700 --> 00:11:47,089
now do the same thing it'll isolate the

278
00:11:45,800 --> 00:11:49,550
flow move it to a different traffic

279
00:11:47,089 --> 00:11:51,980
class and as a consequence we built

280
00:11:49,550 --> 00:11:54,140
we've moved it out of the way so we've

281
00:11:51,980 --> 00:11:56,210
eliminated the head of line blocking so

282
00:11:54,140 --> 00:12:00,529
now the in this example the blue flow

283
00:11:56,210 --> 00:12:03,470
here is able to go through the congested

284
00:12:00,529 --> 00:12:06,770
path without experiencing any delay and

285
00:12:03,470 --> 00:12:08,120
what we're doing fundamentally is moving

286
00:12:06,770 --> 00:12:09,800
that congested flow out of the way we're

287
00:12:08,120 --> 00:12:13,940
sort of leveraging some of the switch

288
00:12:09,800 --> 00:12:16,430
buffering to delay dropping the packets

289
00:12:13,940 --> 00:12:19,760
and allowing the ending congestion time

290
00:12:16,430 --> 00:12:21,680
to take effect so meanwhile the the acts

291
00:12:19,760 --> 00:12:23,450
are going back the source is slowing

292
00:12:21,680 --> 00:12:25,699
down and then eventually we're assuming

293
00:12:23,450 --> 00:12:28,220
the congestion will subside so we're

294
00:12:25,700 --> 00:12:30,740
sort of temporarily buffering congested

295
00:12:28,220 --> 00:12:32,510
flows if you will is kind of at 50,000

296
00:12:30,740 --> 00:12:35,690
foot level that's that's kind of how it

297
00:12:32,510 --> 00:12:38,150
works so so the goals were not to

298
00:12:35,690 --> 00:12:40,220
replace indian congestion control but to

299
00:12:38,150 --> 00:12:43,699
work with it to allow that to work more

300
00:12:40,220 --> 00:12:47,080
effectively and to support again high

301
00:12:43,700 --> 00:12:50,420
you know larger scale faster datacenters

302
00:12:47,080 --> 00:12:53,570
with super low latency to support like

303
00:12:50,420 --> 00:12:55,969
nvme over fabric storage protocols that

304
00:12:53,570 --> 00:12:57,890
need sub microsecond type Layton sees

305
00:12:55,970 --> 00:12:59,360
support those low latency while

306
00:12:57,890 --> 00:13:02,480
maintaining high throughput don't make

307
00:12:59,360 --> 00:13:04,339
that trade-off and again support a

308
00:13:02,480 --> 00:13:07,940
lossless environment for those protocols

309
00:13:04,339 --> 00:13:10,760
today or just a low loss environment so

310
00:13:07,940 --> 00:13:13,130
it's not required that you use priority

311
00:13:10,760 --> 00:13:14,390
based flow control but we you know if

312
00:13:13,130 --> 00:13:16,970
you want to you'll have a lossless

313
00:13:14,390 --> 00:13:18,699
environment and we'll use it less

314
00:13:16,970 --> 00:13:22,100
frequently and we'll show you some

315
00:13:18,700 --> 00:13:24,709
simulation results of that another goal

316
00:13:22,100 --> 00:13:26,839
was to not be protocol specific here we

317
00:13:24,709 --> 00:13:30,699
want to be agnostic whether it's TCP or

318
00:13:26,839 --> 00:13:32,170
UDP we don't care so it is flow aware

319
00:13:30,700 --> 00:13:34,420
we want to try to do this without

320
00:13:32,170 --> 00:13:36,459
requiring you throwing more memory at

321
00:13:34,420 --> 00:13:39,430
the in the switch so we're trying to be

322
00:13:36,460 --> 00:13:41,770
effectively at elegent cueing not just

323
00:13:39,430 --> 00:13:43,989
more cueing so that adds delay and cost

324
00:13:41,770 --> 00:13:46,060
I mean it and again it's very difficult

325
00:13:43,990 --> 00:13:48,580
as we build higher speed switches so

326
00:13:46,060 --> 00:13:52,449
we're trying to reduce the buffer growth

327
00:13:48,580 --> 00:13:56,200
that's required reduce the relying on

328
00:13:52,450 --> 00:13:57,640
PFC for the lossless environment and in

329
00:13:56,200 --> 00:14:02,560
again eliminate that head-of-line

330
00:13:57,640 --> 00:14:03,939
blocking and so there's a diagram Roger

331
00:14:02,560 --> 00:14:06,819
showed I'll go through it in a little

332
00:14:03,940 --> 00:14:08,350
bit more detail here so imagine you've

333
00:14:06,820 --> 00:14:10,450
got your cloths fabric and we're just

334
00:14:08,350 --> 00:14:12,540
going to take one slice of that looking

335
00:14:10,450 --> 00:14:14,590
at an upstream and downstream switch

336
00:14:12,540 --> 00:14:17,349
obviously traffic's coming from all

337
00:14:14,590 --> 00:14:18,730
different kinds of directions here but

338
00:14:17,350 --> 00:14:20,710
in this scenario the first step is that

339
00:14:18,730 --> 00:14:23,260
the downstream switch is starting to

340
00:14:20,710 --> 00:14:24,910
experience congestion on the egress port

341
00:14:23,260 --> 00:14:28,810
so it's and when it's doing that it's

342
00:14:24,910 --> 00:14:31,510
gonna mark ecn bits in the header so it

343
00:14:28,810 --> 00:14:33,339
knows which flow now is potentially

344
00:14:31,510 --> 00:14:36,240
causing congestion now it's going to do

345
00:14:33,340 --> 00:14:38,470
that using a sampling algorithm or

346
00:14:36,240 --> 00:14:40,150
depends on the switch architecture how

347
00:14:38,470 --> 00:14:42,640
it's built so there's some chance of

348
00:14:40,150 --> 00:14:44,199
being incorrect here but this is a best

349
00:14:42,640 --> 00:14:46,360
practice that's been been being used

350
00:14:44,200 --> 00:14:48,010
right so something along the lines like

351
00:14:46,360 --> 00:14:50,230
random early detection but maybe a

352
00:14:48,010 --> 00:14:52,180
little more intelligence so we identify

353
00:14:50,230 --> 00:14:54,640
that flow we're marking the headers and

354
00:14:52,180 --> 00:14:56,140
now we're going to queue packets into a

355
00:14:54,640 --> 00:14:57,720
different traffic class and we're gonna

356
00:14:56,140 --> 00:15:00,580
schedule those traffic classes

357
00:14:57,720 --> 00:15:02,260
differently so clearly there's an

358
00:15:00,580 --> 00:15:05,260
opportunity here to have an out-of-order

359
00:15:02,260 --> 00:15:07,569
packet situation so we have packets

360
00:15:05,260 --> 00:15:08,590
already in a queue for a flow and now

361
00:15:07,570 --> 00:15:10,450
they're going to go into a different

362
00:15:08,590 --> 00:15:12,540
queue so we need to kind of deal with

363
00:15:10,450 --> 00:15:15,610
that complexity but that's an internal

364
00:15:12,540 --> 00:15:17,050
switch architecture issue hopefully

365
00:15:15,610 --> 00:15:19,840
we're not going to see these on the wire

366
00:15:17,050 --> 00:15:22,319
okay so we've we've identified the flow

367
00:15:19,840 --> 00:15:25,180
we've begin to queue the packets into a

368
00:15:22,320 --> 00:15:27,010
congested queue and then we signal to

369
00:15:25,180 --> 00:15:28,989
our neighbor that we're doing this and

370
00:15:27,010 --> 00:15:31,720
we give them enough information to

371
00:15:28,990 --> 00:15:34,330
identify the very same flow so now that

372
00:15:31,720 --> 00:15:36,400
the upstream neighbor will do the same

373
00:15:34,330 --> 00:15:39,220
thing it'll begin to queue those packets

374
00:15:36,400 --> 00:15:40,720
in in a different queue and eliminates

375
00:15:39,220 --> 00:15:43,750
the head-of-line blocking getting it out

376
00:15:40,720 --> 00:15:44,470
of the way now if we do begin to fill up

377
00:15:43,750 --> 00:15:47,050
and we want to

378
00:15:44,470 --> 00:15:49,570
this environment the goal is now I'm

379
00:15:47,050 --> 00:15:51,339
only booting the pause on the congested

380
00:15:49,570 --> 00:15:53,050
class so I'm only penalizing the guys

381
00:15:51,340 --> 00:15:56,110
that are really causing the problem here

382
00:15:53,050 --> 00:15:58,060
I'm not penalizing everybody so if we're

383
00:15:56,110 --> 00:16:01,000
using lossless hopefully it's only

384
00:15:58,060 --> 00:16:04,239
occurring in the in the in the congested

385
00:16:01,000 --> 00:16:06,540
class so that's effectively the how it

386
00:16:04,240 --> 00:16:09,460
works Devils in the details obviously

387
00:16:06,540 --> 00:16:11,530
and and some of the key processes again

388
00:16:09,460 --> 00:16:15,430
are you know detecting which flows cause

389
00:16:11,530 --> 00:16:17,350
congestion and again most routers and

390
00:16:15,430 --> 00:16:19,599
switches already have a means for doing

391
00:16:17,350 --> 00:16:21,070
that there they're setting the ecn bits

392
00:16:19,600 --> 00:16:24,130
so they have some mechanism they're

393
00:16:21,070 --> 00:16:26,680
already using in the standard we can

394
00:16:24,130 --> 00:16:29,530
specify existing methods so we're not

395
00:16:26,680 --> 00:16:32,069
going to define a new method here so

396
00:16:29,530 --> 00:16:34,660
that's really up to a vendor innovation

397
00:16:32,070 --> 00:16:36,670
once we've defined that we're going to

398
00:16:34,660 --> 00:16:38,920
need to create a flow entry in the

399
00:16:36,670 --> 00:16:41,560
switch that recognizes subsequent

400
00:16:38,920 --> 00:16:44,319
packets of the congested flow so there

401
00:16:41,560 --> 00:16:46,810
is a new flow table if you will that

402
00:16:44,320 --> 00:16:48,270
would be required the good news is you

403
00:16:46,810 --> 00:16:52,869
can build this in a way that you're only

404
00:16:48,270 --> 00:16:54,100
holding the the congested flows in that

405
00:16:52,870 --> 00:16:55,990
flow table you don't have to know about

406
00:16:54,100 --> 00:16:56,860
every flow in the network you only need

407
00:16:55,990 --> 00:16:59,530
to know about the ones that are

408
00:16:56,860 --> 00:17:01,330
currently causing you a problem so we

409
00:16:59,530 --> 00:17:04,599
can limit that resource we don't have to

410
00:17:01,330 --> 00:17:06,339
make a huge flow table again the

411
00:17:04,599 --> 00:17:08,020
critical process of signaling to your

412
00:17:06,339 --> 00:17:09,879
neighbor that could be optional we've

413
00:17:08,020 --> 00:17:11,920
talked about not doing this and just

414
00:17:09,880 --> 00:17:14,140
doing the isolation locally and

415
00:17:11,920 --> 00:17:15,670
measuring the impact but you'll see it

416
00:17:14,140 --> 00:17:18,400
does make a difference to tell your

417
00:17:15,670 --> 00:17:21,130
neighbor what's going on so signaling is

418
00:17:18,400 --> 00:17:23,410
a key process and how that works in and

419
00:17:21,130 --> 00:17:25,930
then managing the order out of order

420
00:17:23,410 --> 00:17:28,780
issue so we're gonna schedule those two

421
00:17:25,930 --> 00:17:30,490
queues using the enhanced transmission

422
00:17:28,780 --> 00:17:34,030
scheduling or sort of some bandwidth

423
00:17:30,490 --> 00:17:35,830
allocation weighted fair queuing and we

424
00:17:34,030 --> 00:17:37,600
want to make sure we don't deliver

425
00:17:35,830 --> 00:17:39,340
packets out of order so there's a

426
00:17:37,600 --> 00:17:41,169
there's a bit of an implementation

427
00:17:39,340 --> 00:17:43,270
detail that can be a little complex than

428
00:17:41,170 --> 00:17:46,090
that if you want to just use strict

429
00:17:43,270 --> 00:17:48,820
priority that would make that would work

430
00:17:46,090 --> 00:17:52,870
but however you have a potential for

431
00:17:48,820 --> 00:17:55,270
starvation with that scenario so you

432
00:17:52,870 --> 00:17:57,250
know that's another issue there's some

433
00:17:55,270 --> 00:17:58,000
tricky things with interacting with

434
00:17:57,250 --> 00:18:00,280
priority based

435
00:17:58,000 --> 00:18:02,260
flow control as well because as I

436
00:18:00,280 --> 00:18:04,480
mentioned the goal would be if I'm gonna

437
00:18:02,260 --> 00:18:07,000
use it I only want to use it on the

438
00:18:04,480 --> 00:18:10,420
congested class however there may be

439
00:18:07,000 --> 00:18:13,140
packets in flight on both Q's and so

440
00:18:10,420 --> 00:18:16,630
there's some complexity related to how

441
00:18:13,140 --> 00:18:18,370
we might interact with that and make

442
00:18:16,630 --> 00:18:22,720
sure we're pausing the correct traffic

443
00:18:18,370 --> 00:18:24,969
class so again and then once a flow has

444
00:18:22,720 --> 00:18:27,280
been determined to be causing congestion

445
00:18:24,970 --> 00:18:29,590
at some point we expect it to slow down

446
00:18:27,280 --> 00:18:31,899
because we have end-to-end congestion

447
00:18:29,590 --> 00:18:34,750
control so we need to know now when is

448
00:18:31,900 --> 00:18:38,740
this flow no longer a problem when can I

449
00:18:34,750 --> 00:18:41,140
return it to its status of you know non

450
00:18:38,740 --> 00:18:44,110
congested State if you will and how do I

451
00:18:41,140 --> 00:18:46,930
do that without creating another out of

452
00:18:44,110 --> 00:18:49,270
ordering situation so so these are the

453
00:18:46,930 --> 00:18:52,300
critical processes that the the standard

454
00:18:49,270 --> 00:18:54,610
itself is identifying and and in

455
00:18:52,300 --> 00:18:56,980
documenting clearly for interoperability

456
00:18:54,610 --> 00:18:59,889
anytime you signal messages on the wire

457
00:18:56,980 --> 00:19:01,810
we have to have a protocol spec so so

458
00:18:59,890 --> 00:19:04,510
that's another aspect of it but a lot of

459
00:19:01,810 --> 00:19:06,429
the details will be up to the switch

460
00:19:04,510 --> 00:19:10,930
silicon vendors on how they choose to

461
00:19:06,430 --> 00:19:13,600
implement these various processes we did

462
00:19:10,930 --> 00:19:16,480
some simulations there's a lot of data

463
00:19:13,600 --> 00:19:18,370
available on these reference slide decks

464
00:19:16,480 --> 00:19:22,590
that show whether this is effective or

465
00:19:18,370 --> 00:19:26,260
not we kind of set up a class network

466
00:19:22,590 --> 00:19:28,149
using Omni net plus plus and you know

467
00:19:26,260 --> 00:19:30,850
scaling it up to about I say a thousand

468
00:19:28,150 --> 00:19:32,920
server so it's not a huge simulation but

469
00:19:30,850 --> 00:19:36,070
enough to show the difference here and

470
00:19:32,920 --> 00:19:38,620
the ability to scale the traffic models

471
00:19:36,070 --> 00:19:41,740
we used were were a mix of the end cast

472
00:19:38,620 --> 00:19:44,500
that you see with like AI applications

473
00:19:41,740 --> 00:19:46,450
and storage applications also with a

474
00:19:44,500 --> 00:19:49,450
mini to mini so we kind of had this

475
00:19:46,450 --> 00:19:52,180
rough mix of different different traffic

476
00:19:49,450 --> 00:19:54,190
patterns in our simulation data and what

477
00:19:52,180 --> 00:19:57,550
we measured for we measure both the lost

478
00:19:54,190 --> 00:19:59,290
lists and the lost cease scenario and in

479
00:19:57,550 --> 00:20:01,360
the lossless tomorrow we were basically

480
00:19:59,290 --> 00:20:02,980
measuring flow completion time so we

481
00:20:01,360 --> 00:20:05,350
want to see an improvement in overall

482
00:20:02,980 --> 00:20:08,470
flow completion time across the network

483
00:20:05,350 --> 00:20:10,600
and in general we saw you know say like

484
00:20:08,470 --> 00:20:12,760
a 38 almost a 40 percent

485
00:20:10,600 --> 00:20:15,399
improvement which was significant enough

486
00:20:12,760 --> 00:20:17,379
to warrant doing this project if it was

487
00:20:15,400 --> 00:20:18,850
only a small gain you'd say well what's

488
00:20:17,380 --> 00:20:21,220
let's just throw bandwidth at the

489
00:20:18,850 --> 00:20:23,260
problem but we're seeing a 40%

490
00:20:21,220 --> 00:20:27,090
improvement across all flows obviously

491
00:20:23,260 --> 00:20:29,710
the bigger the flows the the more the

492
00:20:27,090 --> 00:20:32,320
more of a gain that we see in this

493
00:20:29,710 --> 00:20:34,030
lossless scenario almost up to 60

494
00:20:32,320 --> 00:20:36,129
percent improvement in flow completion

495
00:20:34,030 --> 00:20:39,129
time there are some proprietary

496
00:20:36,130 --> 00:20:41,410
technologies that are out there that do

497
00:20:39,130 --> 00:20:44,500
some things where they separate the mice

498
00:20:41,410 --> 00:20:47,200
and elephant flows today so you you may

499
00:20:44,500 --> 00:20:50,710
you may have a switch that recognizes

500
00:20:47,200 --> 00:20:52,780
all packets as being a bit amount mice

501
00:20:50,710 --> 00:20:54,640
Mouse flow if you will initially and

502
00:20:52,780 --> 00:20:56,740
then if the flow persists for a long

503
00:20:54,640 --> 00:20:58,659
time it's designated an elephant flow

504
00:20:56,740 --> 00:21:01,510
and then they may use different traffic

505
00:20:58,659 --> 00:21:05,110
classes for that Cisco has a feature

506
00:21:01,510 --> 00:21:06,940
that does this we also looked at okay

507
00:21:05,110 --> 00:21:10,740
well what if we have that capability

508
00:21:06,940 --> 00:21:13,840
mice mice elephant separation plus a

509
00:21:10,740 --> 00:21:16,270
congested flow what what kind of gains

510
00:21:13,840 --> 00:21:18,209
do we get so you can see just separating

511
00:21:16,270 --> 00:21:21,460
my Snell fence along was pretty useful

512
00:21:18,210 --> 00:21:24,760
but adding congestion isolation on top

513
00:21:21,460 --> 00:21:27,820
of that still gave us a 25% gain on flow

514
00:21:24,760 --> 00:21:30,850
completion times again so it was a very

515
00:21:27,820 --> 00:21:33,120
useful feature even in even with other

516
00:21:30,850 --> 00:21:36,340
enhancements that that people are doing

517
00:21:33,120 --> 00:21:38,080
in the lossy scenario so we're not doing

518
00:21:36,340 --> 00:21:41,199
priority based flow control here so we

519
00:21:38,080 --> 00:21:44,309
can have packet drops the goal here was

520
00:21:41,200 --> 00:21:47,860
to see well how much how do we improve

521
00:21:44,309 --> 00:21:50,740
packet loss ratios in general and then

522
00:21:47,860 --> 00:21:52,750
we also compared doing this with

523
00:21:50,740 --> 00:21:55,419
signaling the neighbor and without

524
00:21:52,750 --> 00:22:01,360
signaling your neighbor so in the case

525
00:21:55,419 --> 00:22:03,730
of having just you know a regular IP

526
00:22:01,360 --> 00:22:05,949
network and and doing congestion

527
00:22:03,730 --> 00:22:07,780
isolation locally in the switch and not

528
00:22:05,950 --> 00:22:10,570
telling anybody about it we still get a

529
00:22:07,780 --> 00:22:13,120
25% gain but once you tell your neighbor

530
00:22:10,570 --> 00:22:14,710
to isolate the same flow that's what we

531
00:22:13,120 --> 00:22:17,110
really begin to eliminate head-of-line

532
00:22:14,710 --> 00:22:20,980
blocking we're leveraging even another

533
00:22:17,110 --> 00:22:23,350
set of cues if you will buffering in to

534
00:22:20,980 --> 00:22:24,970
allow the endian flow control time to to

535
00:22:23,350 --> 00:22:26,620
work so you really have

536
00:22:24,970 --> 00:22:30,130
a lot of packet loss in that scenario

537
00:22:26,620 --> 00:22:34,120
too we were getting a 54% gain in that

538
00:22:30,130 --> 00:22:36,400
scenario and then that the final one

539
00:22:34,120 --> 00:22:38,919
would be we saw priority based flow

540
00:22:36,400 --> 00:22:42,190
control has some negatives some

541
00:22:38,920 --> 00:22:45,010
downsides and so if we're still gonna

542
00:22:42,190 --> 00:22:47,470
rely on it as a last-gasp effort to

543
00:22:45,010 --> 00:22:49,960
never drop a packet how can we reduce

544
00:22:47,470 --> 00:22:52,120
the amount of signaling priority based

545
00:22:49,960 --> 00:22:55,570
flow control that that would exist in

546
00:22:52,120 --> 00:22:59,110
the network and we did the same mice and

547
00:22:55,570 --> 00:23:02,889
elephant separation and we measured how

548
00:22:59,110 --> 00:23:06,969
many how fewer pause frames there were

549
00:23:02,890 --> 00:23:09,850
and how long the pause period was on for

550
00:23:06,970 --> 00:23:11,890
and we got again 50 percent gains or 30

551
00:23:09,850 --> 00:23:14,020
percent gains so so that the feature

552
00:23:11,890 --> 00:23:17,620
itself appears to be you know effective

553
00:23:14,020 --> 00:23:20,430
it seems to be a worthwhile situation

554
00:23:17,620 --> 00:23:23,790
and we're trying to again build a

555
00:23:20,430 --> 00:23:26,860
network that will scale to larger size

556
00:23:23,790 --> 00:23:30,940
without having to tune and deal with all

557
00:23:26,860 --> 00:23:32,649
the basically the complexities and it

558
00:23:30,940 --> 00:23:34,990
does scale as well so we get a pretty

559
00:23:32,650 --> 00:23:37,390
linear scaling we looked at a small

560
00:23:34,990 --> 00:23:40,810
network of 128 servers all the way up to

561
00:23:37,390 --> 00:23:43,150
a thousand servers and and the ink they

562
00:23:40,810 --> 00:23:45,460
you know the flow completion time didn't

563
00:23:43,150 --> 00:23:48,610
really grow if you will when we're using

564
00:23:45,460 --> 00:23:50,080
the congestion isolation feature whereas

565
00:23:48,610 --> 00:23:52,689
you know we start to see more problems

566
00:23:50,080 --> 00:23:54,370
with with some of the others so it

567
00:23:52,690 --> 00:23:57,670
appears to be as good scaling feature as

568
00:23:54,370 --> 00:23:59,830
well all right so what's some what's

569
00:23:57,670 --> 00:24:03,400
next for us we're in the process right

570
00:23:59,830 --> 00:24:06,280
now of developing the draft there's a

571
00:24:03,400 --> 00:24:09,400
design team that has ad hoc phone calls

572
00:24:06,280 --> 00:24:11,920
to try to address some of the those

573
00:24:09,400 --> 00:24:15,130
critical processes and make choices on

574
00:24:11,920 --> 00:24:17,470
how we're gonna document that and you

575
00:24:15,130 --> 00:24:18,580
know the question is well how can why am

576
00:24:17,470 --> 00:24:22,390
I talking about this here and how can

577
00:24:18,580 --> 00:24:24,159
how can we participate and this is

578
00:24:22,390 --> 00:24:26,620
really where we're doing this to try to

579
00:24:24,160 --> 00:24:29,740
support large operators large data

580
00:24:26,620 --> 00:24:31,330
centers hyper scale networks and so is

581
00:24:29,740 --> 00:24:35,560
it something that's going to be useful

582
00:24:31,330 --> 00:24:37,090
for you is it is it a good approach what

583
00:24:35,560 --> 00:24:38,409
are their requirements in your network

584
00:24:37,090 --> 00:24:41,679
so getting that kind of fee

585
00:24:38,410 --> 00:24:44,020
back would be fantastic for us and

586
00:24:41,680 --> 00:24:47,050
there's ways to participate as Roger had

587
00:24:44,020 --> 00:24:49,330
mentioned we have than indica forum in

588
00:24:47,050 --> 00:24:52,450
the I Triple E and in the IETF we've

589
00:24:49,330 --> 00:24:54,790
been holding side meetings as well where

590
00:24:52,450 --> 00:24:57,040
we're talking about these kind of data

591
00:24:54,790 --> 00:25:00,190
center networks and what technologies we

592
00:24:57,040 --> 00:25:01,720
might need in order to scale it larger

593
00:25:00,190 --> 00:25:06,430
so these would be great opportunities to

594
00:25:01,720 --> 00:25:08,260
to participate in that as well so at

595
00:25:06,430 --> 00:25:10,060
that point I guess I could open it up to

596
00:25:08,260 --> 00:25:13,360
any any particular questions you have

597
00:25:10,060 --> 00:25:15,790
and and also if there's any me about the

598
00:25:13,360 --> 00:25:18,070
any of the other talks as well would be

599
00:25:15,790 --> 00:25:19,690
happy to to try to address those as well

600
00:25:18,070 --> 00:25:23,020
we can get the other guys up here if

601
00:25:19,690 --> 00:25:24,910
they if needed so there's a whole bunch

602
00:25:23,020 --> 00:25:26,910
of useful references here that you can

603
00:25:24,910 --> 00:25:29,800
again more more information about

604
00:25:26,910 --> 00:25:33,870
background technology and things that

605
00:25:29,800 --> 00:25:35,169
we're doing so there any questions

606
00:25:33,870 --> 00:25:42,520
period

607
00:25:35,170 --> 00:25:46,510
hooray thanks for definitely work off

608
00:25:42,520 --> 00:25:49,060
value over here to what Intel I have two

609
00:25:46,510 --> 00:25:53,830
questions one is the definition of a

610
00:25:49,060 --> 00:25:58,659
flow in this context originally it was

611
00:25:53,830 --> 00:26:00,790
all really port base and at layer 2 you

612
00:25:58,660 --> 00:26:04,510
you don't really have a recognition of a

613
00:26:00,790 --> 00:26:08,800
flow so that actually causes a blockage

614
00:26:04,510 --> 00:26:11,500
of other called M transport level flows

615
00:26:08,800 --> 00:26:13,389
from from the same source and whether

616
00:26:11,500 --> 00:26:16,090
you guys are dealing with that problem

617
00:26:13,390 --> 00:26:18,510
the other quick question I have many but

618
00:26:16,090 --> 00:26:22,540
I'm just gonna restrict it to two is

619
00:26:18,510 --> 00:26:27,250
interest normally involves potentially

620
00:26:22,540 --> 00:26:29,530
many many flows and that even if you put

621
00:26:27,250 --> 00:26:32,380
them aside we still going to cause

622
00:26:29,530 --> 00:26:35,710
issues with the amount of buffering

623
00:26:32,380 --> 00:26:39,910
required okay great question so the

624
00:26:35,710 --> 00:26:43,110
first question what's a flow so I'd say

625
00:26:39,910 --> 00:26:45,610
a tow 2.1 has gone well beyond their

626
00:26:43,110 --> 00:26:48,159
boundaries these days it's layer two and

627
00:26:45,610 --> 00:26:50,649
only looking at MAC addresses and things

628
00:26:48,160 --> 00:26:51,310
like that so so a flow is a full you

629
00:26:50,650 --> 00:26:55,690
know five

630
00:26:51,310 --> 00:26:57,940
here I pee and now you do have the and

631
00:26:55,690 --> 00:27:02,350
we have specifications in I Triple E 8o

632
00:26:57,940 --> 00:27:04,660
2.1 that define layer 3 layer 4 flows if

633
00:27:02,350 --> 00:27:06,459
you will now so that that that's been

634
00:27:04,660 --> 00:27:07,720
something that we've done so there is

635
00:27:06,460 --> 00:27:11,110
the question about well what do I do

636
00:27:07,720 --> 00:27:14,830
about overlay networks and you know like

637
00:27:11,110 --> 00:27:17,110
virtualization and this is again we're

638
00:27:14,830 --> 00:27:19,780
expecting that we're not looking into

639
00:27:17,110 --> 00:27:21,639
the payload beyond the you know the

640
00:27:19,780 --> 00:27:24,160
outer the path that the carrier the

641
00:27:21,640 --> 00:27:28,720
passenger header so so what flow might

642
00:27:24,160 --> 00:27:30,520
be a full encapsulation as well so you

643
00:27:28,720 --> 00:27:31,930
may actually have many sub flows in

644
00:27:30,520 --> 00:27:35,440
there there was a great question earlier

645
00:27:31,930 --> 00:27:38,500
about security and obviously you know if

646
00:27:35,440 --> 00:27:39,910
you have encrypted traffic on your

647
00:27:38,500 --> 00:27:42,970
virtualized network we're not going to

648
00:27:39,910 --> 00:27:45,820
be looking inside that so we're strictly

649
00:27:42,970 --> 00:27:49,630
looking at the outer the outer flow if

650
00:27:45,820 --> 00:27:53,110
you will the outer headers and it's the

651
00:27:49,630 --> 00:27:56,050
second question that you asked

652
00:27:53,110 --> 00:28:01,389
I forget sorry over a what was the

653
00:27:56,050 --> 00:28:04,000
second question was Oh in Katz right so

654
00:28:01,390 --> 00:28:05,800
in the in the your absolute correct - in

655
00:28:04,000 --> 00:28:09,190
Katz is clearly problem so we talked

656
00:28:05,800 --> 00:28:12,669
about in caste in the paper than indica

657
00:28:09,190 --> 00:28:14,590
paper and we point out that really to

658
00:28:12,670 --> 00:28:17,860
solve the in Katz problem we're probably

659
00:28:14,590 --> 00:28:20,949
talking about we scheduled or some way

660
00:28:17,860 --> 00:28:22,780
of actually having the transport be more

661
00:28:20,950 --> 00:28:24,730
aware of this fact so if you really want

662
00:28:22,780 --> 00:28:26,170
to solve the problem of Inc s yep like

663
00:28:24,730 --> 00:28:28,510
you said hundreds maybe thousands of

664
00:28:26,170 --> 00:28:30,310
flows all converging on to really what

665
00:28:28,510 --> 00:28:31,900
we need is a different approach for a

666
00:28:30,310 --> 00:28:35,590
transport protocol and there's been some

667
00:28:31,900 --> 00:28:37,510
research on that there's a the India NDP

668
00:28:35,590 --> 00:28:38,709
I think was one paper at sigcomm that

669
00:28:37,510 --> 00:28:41,310
was quite interesting where you're

670
00:28:38,710 --> 00:28:45,280
effectively transitioning from the

671
00:28:41,310 --> 00:28:47,620
traditional TCP model of pushing data

672
00:28:45,280 --> 00:28:49,389
into the network and trying to probe for

673
00:28:47,620 --> 00:28:51,370
bandwidth that's available you

674
00:28:49,390 --> 00:28:54,700
transition that to what more of a pull

675
00:28:51,370 --> 00:28:56,409
mechanism where the actual before you

676
00:28:54,700 --> 00:28:59,920
you can transmit you have to have a

677
00:28:56,410 --> 00:29:03,400
grant River a request grant model and so

678
00:28:59,920 --> 00:29:04,720
clearly one has more latency but it

679
00:29:03,400 --> 00:29:08,530
would avoid drops in

680
00:29:04,720 --> 00:29:10,870
in cast scenario but the other pushing

681
00:29:08,530 --> 00:29:14,470
problem you know doesn't really provide

682
00:29:10,870 --> 00:29:16,389
you the right visibility of what's going

683
00:29:14,470 --> 00:29:19,390
on in the network so so that the paper

684
00:29:16,390 --> 00:29:21,250
talks about a hybrid transport protocol

685
00:29:19,390 --> 00:29:23,380
that that flips between pushing and

686
00:29:21,250 --> 00:29:26,559
pulling based on the state of congestion

687
00:29:23,380 --> 00:29:28,270
in the network so yeah so that's

688
00:29:26,559 --> 00:29:29,559
definitely an interesting topic and

689
00:29:28,270 --> 00:29:31,750
again we talked about that in the paper

690
00:29:29,559 --> 00:29:34,889
that's probably more likely an IETF

691
00:29:31,750 --> 00:29:37,780
activity than than a Ben and I Triple E

692
00:29:34,890 --> 00:29:41,440
activity but congestion isolation

693
00:29:37,780 --> 00:29:44,470
certainly intended to support you know

694
00:29:41,440 --> 00:29:46,480
large scale number of flows not just you

695
00:29:44,470 --> 00:29:47,890
know one one flow at a time we're gonna

696
00:29:46,480 --> 00:29:51,039
we're getting definitely going to deal

697
00:29:47,890 --> 00:29:55,140
with the same caste problem as well all

698
00:29:51,039 --> 00:29:55,140
right any other questions

699
00:29:56,010 --> 00:30:17,770
Anand again so how do you yeah so

700
00:30:14,260 --> 00:30:19,600
typically in a six they have eight we

701
00:30:17,770 --> 00:30:22,539
define eight different traffic classes

702
00:30:19,600 --> 00:30:25,090
per port which is you know priorities or

703
00:30:22,539 --> 00:30:26,679
cues if you will so what we're doing in

704
00:30:25,090 --> 00:30:28,840
congestion isolation is we're gonna use

705
00:30:26,679 --> 00:30:31,809
two of them we're gonna take two of them

706
00:30:28,840 --> 00:30:34,059
and so the way when we identify float

707
00:30:31,809 --> 00:30:37,178
when we were experiencing congestion in

708
00:30:34,059 --> 00:30:39,399
one of those we're identifying flows

709
00:30:37,179 --> 00:30:41,640
that are causing that congestion by you

710
00:30:39,400 --> 00:30:44,440
know looking at the at the headers

711
00:30:41,640 --> 00:30:47,169
yeah at line rate so you know we're able

712
00:30:44,440 --> 00:30:49,240
to look in a major sample for example a

713
00:30:47,169 --> 00:30:51,220
packet out of that queue that's backing

714
00:30:49,240 --> 00:30:53,350
up and they assume that the prior you

715
00:30:51,220 --> 00:30:55,720
know that that high probability that

716
00:30:53,350 --> 00:30:59,080
this this flow is the one that's causing

717
00:30:55,720 --> 00:31:01,120
problems here and so now you create an

718
00:30:59,080 --> 00:31:03,330
entry in a flow table that will identify

719
00:31:01,120 --> 00:31:06,100
the packets from that flow as the

720
00:31:03,330 --> 00:31:08,799
subsequent packets and now when they hit

721
00:31:06,100 --> 00:31:10,570
they come in on the upstream port we

722
00:31:08,799 --> 00:31:12,520
identify that flow we're going to now

723
00:31:10,570 --> 00:31:13,899
queue them in the other traffic class as

724
00:31:12,520 --> 00:31:15,820
opposed to the original so that's the

725
00:31:13,899 --> 00:31:18,070
isolation we're taking that flow and

726
00:31:15,820 --> 00:31:21,040
we're isolating it into another key

727
00:31:18,070 --> 00:31:24,100
Java class now ASIC implementations may

728
00:31:21,040 --> 00:31:25,870
may be more sophisticated than what

729
00:31:24,100 --> 00:31:28,030
we've defined in the standard they may

730
00:31:25,870 --> 00:31:30,879
have more than eight cubes purport maybe

731
00:31:28,030 --> 00:31:33,460
they have hidden hierarchical queues and

732
00:31:30,880 --> 00:31:36,120
they have 16 or 32 queues or something

733
00:31:33,460 --> 00:31:40,450
so there's even more opportunity in some

734
00:31:36,120 --> 00:31:42,760
ASIC architectures to to isolate flows

735
00:31:40,450 --> 00:31:43,810
more flows than the way we're versus

736
00:31:42,760 --> 00:31:47,290
best we're working within the

737
00:31:43,810 --> 00:31:50,889
constraints of the v8 o2 standard so 8/8

738
00:31:47,290 --> 00:31:54,430
queues purport there are technologies

739
00:31:50,890 --> 00:31:57,090
like I believe DC qcn where you do W

740
00:31:54,430 --> 00:32:01,240
rate based signaling to the source that

741
00:31:57,090 --> 00:32:04,000
are you before you hit the buffer buffer

742
00:32:01,240 --> 00:32:06,130
fuel scenario you kind of notify the

743
00:32:04,000 --> 00:32:07,990
source that there is condition that's

744
00:32:06,130 --> 00:32:11,500
going to happen and signal the source to

745
00:32:07,990 --> 00:32:13,530
slow down right so how does do you think

746
00:32:11,500 --> 00:32:16,930
that it's ineffective compared to this

747
00:32:13,530 --> 00:32:21,700
it's complimentary right so things like

748
00:32:16,930 --> 00:32:24,970
so DC q CN is is defined to be the end

749
00:32:21,700 --> 00:32:26,860
end congestion control for rocky for our

750
00:32:24,970 --> 00:32:28,360
DMA traffic so that you have its source

751
00:32:26,860 --> 00:32:30,699
you have destination when the

752
00:32:28,360 --> 00:32:32,560
destination receives packets that have

753
00:32:30,700 --> 00:32:34,000
been marked by the switch somewhere

754
00:32:32,560 --> 00:32:36,639
along the path that says there's

755
00:32:34,000 --> 00:32:38,890
congestion on this path they send that

756
00:32:36,640 --> 00:32:40,360
the message back to the source that's a

757
00:32:38,890 --> 00:32:43,390
part of the acknowledgment that's how

758
00:32:40,360 --> 00:32:45,429
these qcn works and so so that's the end

759
00:32:43,390 --> 00:32:48,490
end congestion control that causes the

760
00:32:45,430 --> 00:32:49,990
source to slow down so we are in full

761
00:32:48,490 --> 00:32:51,760
support of that in fact we have to have

762
00:32:49,990 --> 00:32:54,430
that in place we're expecting the

763
00:32:51,760 --> 00:32:56,650
sources to slow down and meanwhile while

764
00:32:54,430 --> 00:32:59,410
there while they're doing their control

765
00:32:56,650 --> 00:33:01,060
loop we're isolating the flows to give

766
00:32:59,410 --> 00:33:04,690
it time so that we're not dropping

767
00:33:01,060 --> 00:33:08,409
packets so fundamentally we depend on

768
00:33:04,690 --> 00:33:12,190
either TCP s you know each round-trip

769
00:33:08,410 --> 00:33:13,870
ecn or pcqc and something like this that

770
00:33:12,190 --> 00:33:16,690
does the in being in congestion control

771
00:33:13,870 --> 00:33:19,360
so we're relying on that there's a big

772
00:33:16,690 --> 00:33:21,190
difference in in there was congestion

773
00:33:19,360 --> 00:33:24,639
notification that was defined in I

774
00:33:21,190 --> 00:33:26,470
Triple E that sent of a sort of like a

775
00:33:24,640 --> 00:33:28,180
source quench message all the way back

776
00:33:26,470 --> 00:33:29,980
to the source that told the source to

777
00:33:28,180 --> 00:33:31,330
slow down directly from the network

778
00:33:29,980 --> 00:33:32,650
itself so

779
00:33:31,330 --> 00:33:34,480
that's not what we're doing we're still

780
00:33:32,650 --> 00:33:37,390
in support of the Indian path

781
00:33:34,480 --> 00:33:38,800
we're just amongst ourselves trying to

782
00:33:37,390 --> 00:33:41,950
identify the flows that are causing

783
00:33:38,800 --> 00:33:49,950
problem and isolating them while the end

784
00:33:41,950 --> 00:33:51,580
in loop can take take take effect okay

785
00:33:49,950 --> 00:33:57,330
thanks

786
00:33:51,580 --> 00:33:57,330
[Applause]

787
00:34:03,730 --> 00:34:05,790
you

