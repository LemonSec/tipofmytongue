1
00:00:00,000 --> 00:00:04,080
[Music]

2
00:00:04,080 --> 00:00:05,040
hi everyone

3
00:00:05,040 --> 00:00:08,400
and welcome to our presentation um

4
00:00:08,400 --> 00:00:10,080
my name is carsten mitchell i'm working

5
00:00:10,080 --> 00:00:11,920
as a senior expert

6
00:00:11,920 --> 00:00:14,559
in network engineering and uh telecom

7
00:00:14,559 --> 00:00:15,759
for those of you not familiar with

8
00:00:15,759 --> 00:00:17,600
doctor telecom we are the largest

9
00:00:17,600 --> 00:00:19,920
service provider in germany

10
00:00:19,920 --> 00:00:23,119
and one of the biggest in europe

11
00:00:23,119 --> 00:00:25,439
today i will give you some insights into

12
00:00:25,439 --> 00:00:27,680
the project access 4.0 which is the

13
00:00:27,680 --> 00:00:30,400
first project using this aggregated

14
00:00:30,400 --> 00:00:32,880
routing and dng system within deutsche

15
00:00:32,880 --> 00:00:34,320
telecom

16
00:00:34,320 --> 00:00:35,680
i do this presentation together with

17
00:00:35,680 --> 00:00:38,239
hana scrittler from our collaboration

18
00:00:38,239 --> 00:00:39,920
partner arty brick

19
00:00:39,920 --> 00:00:41,840
so our journey into this aggregation at

20
00:00:41,840 --> 00:00:43,920
deutsche telekom started a couple of

21
00:00:43,920 --> 00:00:45,039
years back when

22
00:00:45,039 --> 00:00:47,760
some people of our architecture team

23
00:00:47,760 --> 00:00:48,800
investigated

24
00:00:48,800 --> 00:00:51,920
the atp's court approach a court

25
00:00:51,920 --> 00:00:53,440
stands for central office

26
00:00:53,440 --> 00:00:56,079
re-architecture as a data center

27
00:00:56,079 --> 00:00:58,879
and as the name implies the basic idea

28
00:00:58,879 --> 00:01:01,199
of chord is to replace an existing black

29
00:01:01,199 --> 00:01:04,239
box in the access on aggregation network

30
00:01:04,239 --> 00:01:06,320
with bare metal servers and bare metal

31
00:01:06,320 --> 00:01:07,360
switches

32
00:01:07,360 --> 00:01:10,560
and run open software on top of it

33
00:01:10,560 --> 00:01:12,960
for us chord was the starting point we

34
00:01:12,960 --> 00:01:14,720
have developed our own solution

35
00:01:14,720 --> 00:01:18,320
which meets our requirements fits into

36
00:01:18,320 --> 00:01:19,759
our existing network

37
00:01:19,759 --> 00:01:22,240
and also fits in our existing oss and

38
00:01:22,240 --> 00:01:24,240
bss systems

39
00:01:24,240 --> 00:01:26,400
for example chord has a centralized sdn

40
00:01:26,400 --> 00:01:28,320
approach using open flow

41
00:01:28,320 --> 00:01:30,880
however in our network we have a huge

42
00:01:30,880 --> 00:01:33,360
amount of network states

43
00:01:33,360 --> 00:01:36,560
which cannot easily get which we cannot

44
00:01:36,560 --> 00:01:37,840
easily get rid of

45
00:01:37,840 --> 00:01:40,400
and which are difficult to handle with

46
00:01:40,400 --> 00:01:42,159
centralized sdn concepts

47
00:01:42,159 --> 00:01:46,720
therefore we build a hybrid scn solution

48
00:01:46,720 --> 00:01:48,960
today our network consists of roughly 1

49
00:01:48,960 --> 00:01:50,640
000 points of presence

50
00:01:50,640 --> 00:01:53,200
across germany and these points of

51
00:01:53,200 --> 00:01:54,000
presence

52
00:01:54,000 --> 00:01:56,159
we

53
00:01:57,040 --> 00:01:59,680
the bloodblock network gateways bngs for

54
00:01:59,680 --> 00:02:00,640
some time

55
00:02:00,640 --> 00:02:03,040
to terminate customer traffic which is

56
00:02:03,040 --> 00:02:06,560
uh mainly ip over pvpoe

57
00:02:06,560 --> 00:02:09,199
customers being residential customers as

58
00:02:09,199 --> 00:02:10,720
well as business customers

59
00:02:10,720 --> 00:02:14,160
on the same platform the main aim for

60
00:02:14,160 --> 00:02:14,800
access

61
00:02:14,800 --> 00:02:16,400
for the road project is to provide

62
00:02:16,400 --> 00:02:18,560
high-speed internet services with triple

63
00:02:18,560 --> 00:02:20,239
play support

64
00:02:20,239 --> 00:02:23,599
but also business services in the future

65
00:02:23,599 --> 00:02:26,400
and in order to provide this in a very

66
00:02:26,400 --> 00:02:28,400
cost efficient way

67
00:02:28,400 --> 00:02:30,959
and also preventing vendor lock-ins

68
00:02:30,959 --> 00:02:32,480
we've decided to

69
00:02:32,480 --> 00:02:35,680
disaggregate software and hardware

70
00:02:35,680 --> 00:02:38,879
our goal is to get an open architecture

71
00:02:38,879 --> 00:02:41,040
which allows us to replace any component

72
00:02:41,040 --> 00:02:42,080
at any time

73
00:02:42,080 --> 00:02:43,920
and also allows us to integrate these

74
00:02:43,920 --> 00:02:45,599
components easily into our

75
00:02:45,599 --> 00:02:49,360
it landscape therefore we've built a

76
00:02:49,360 --> 00:02:51,440
disaggregated b g solution together with

77
00:02:51,440 --> 00:02:52,319
rgb brick

78
00:02:52,319 --> 00:02:54,400
which is now live in our production

79
00:02:54,400 --> 00:02:56,239
network

80
00:02:56,239 --> 00:02:58,000
we are highly looking for collaboration

81
00:02:58,000 --> 00:02:59,519
in the service provider community

82
00:02:59,519 --> 00:03:00,400
especially with

83
00:03:00,400 --> 00:03:03,040
other isps in europe and in the u.s

84
00:03:03,040 --> 00:03:04,159
which have similar

85
00:03:04,159 --> 00:03:07,200
ideas and similar needs as we have we've

86
00:03:07,200 --> 00:03:09,840
also contributed some

87
00:03:09,840 --> 00:03:11,840
of our work already to standardization

88
00:03:11,840 --> 00:03:13,599
groups as you can see

89
00:03:13,599 --> 00:03:16,000
on the slide and we will continue to do

90
00:03:16,000 --> 00:03:17,120
so

91
00:03:17,120 --> 00:03:20,000
my name is hannes gradler i am the cto

92
00:03:20,000 --> 00:03:22,319
and co-founder of rtbrake

93
00:03:22,319 --> 00:03:26,319
starting about in 2012 i've witnessed

94
00:03:26,319 --> 00:03:28,799
data center operators to transition

95
00:03:28,799 --> 00:03:29,519
their

96
00:03:29,519 --> 00:03:32,560
sourcing strategy away from

97
00:03:32,560 --> 00:03:36,000
integrated systems towards bare metal

98
00:03:36,000 --> 00:03:37,040
switches

99
00:03:37,040 --> 00:03:40,640
plus a disaggregated nos mostly

100
00:03:40,640 --> 00:03:45,040
to get back flexibility and cost savings

101
00:03:45,040 --> 00:03:47,840
along with the uh proliferation of

102
00:03:47,840 --> 00:03:49,599
merchant silicon

103
00:03:49,599 --> 00:03:52,799
we have seen many

104
00:03:52,799 --> 00:03:55,280
networking operating systems mostly

105
00:03:55,280 --> 00:03:56,720
around

106
00:03:56,720 --> 00:03:59,840
a dc data center feature set but none

107
00:03:59,840 --> 00:04:01,200
really targeted

108
00:04:01,200 --> 00:04:05,360
to a classical telco footprint

109
00:04:05,360 --> 00:04:09,040
therefore i wanted to close that gap and

110
00:04:09,040 --> 00:04:11,439
founded the company together with

111
00:04:11,439 --> 00:04:13,840
my co-founder praveen vandakar who was

112
00:04:13,840 --> 00:04:15,120
my product manager

113
00:04:15,120 --> 00:04:18,798
at juniper networks

114
00:04:18,880 --> 00:04:22,320
arty brick and deutsche telekom paths

115
00:04:22,320 --> 00:04:25,680
have intersected in 2017

116
00:04:25,680 --> 00:04:29,360
with an interesting problem to solve

117
00:04:29,360 --> 00:04:32,160
dt have deployed thousands of chassis

118
00:04:32,160 --> 00:04:33,680
based routers

119
00:04:33,680 --> 00:04:37,280
if you zoom inside the box

120
00:04:37,280 --> 00:04:40,560
you'll see core facing line cards

121
00:04:40,560 --> 00:04:43,040
customer facing line cards all

122
00:04:43,040 --> 00:04:44,560
interconnected wire

123
00:04:44,560 --> 00:04:48,160
fabric cards the cards pass traffic

124
00:04:48,160 --> 00:04:51,600
using a proprietary data plane protocol

125
00:04:51,600 --> 00:04:53,680
and the line cards are getting

126
00:04:53,680 --> 00:04:55,120
programmed using

127
00:04:55,120 --> 00:04:58,400
a proprietary control protocol now

128
00:04:58,400 --> 00:05:01,039
the problem with this setup is that

129
00:05:01,039 --> 00:05:01,759
there is no

130
00:05:01,759 --> 00:05:04,960
uniform compatibility layer both

131
00:05:04,960 --> 00:05:09,039
for the data plane and the control plane

132
00:05:09,039 --> 00:05:11,680
of course you cannot combine combine a

133
00:05:11,680 --> 00:05:12,560
line card of

134
00:05:12,560 --> 00:05:15,440
wind array with a line card from vendor

135
00:05:15,440 --> 00:05:16,320
v

136
00:05:16,320 --> 00:05:18,960
whereas you often cannot combine line

137
00:05:18,960 --> 00:05:21,039
card version one from vendor a with line

138
00:05:21,039 --> 00:05:23,440
card version two from vendor a

139
00:05:23,440 --> 00:05:27,440
and this typically requires ongoing

140
00:05:27,440 --> 00:05:30,720
forklift upgrades during the life cycle

141
00:05:30,720 --> 00:05:32,160
of the equipment

142
00:05:32,160 --> 00:05:35,199
and uh in fact there is several hardware

143
00:05:35,199 --> 00:05:36,080
teams

144
00:05:36,080 --> 00:05:40,160
who swap components every single day

145
00:05:40,160 --> 00:05:43,440
so what we did is to create

146
00:05:43,440 --> 00:05:46,000
essentially a discrete version of that

147
00:05:46,000 --> 00:05:47,919
integrated router

148
00:05:47,919 --> 00:05:50,639
the proprietary fabric protocol has been

149
00:05:50,639 --> 00:05:52,639
replaced with ethernet

150
00:05:52,639 --> 00:05:56,240
mpls segment routing and the control

151
00:05:56,240 --> 00:05:56,960
plane

152
00:05:56,960 --> 00:05:59,680
has been replaced with bgp labeled

153
00:05:59,680 --> 00:06:00,800
unicast

154
00:06:00,800 --> 00:06:03,440
plus segment routing extension for the

155
00:06:03,440 --> 00:06:05,120
underlay

156
00:06:05,120 --> 00:06:08,639
our bgpl free vpn and pgp

157
00:06:08,639 --> 00:06:12,160
mvpn for the overlay

158
00:06:12,160 --> 00:06:15,199
on the management plane we ensure

159
00:06:15,199 --> 00:06:18,160
that each of our smaller switches has a

160
00:06:18,160 --> 00:06:19,520
local x86

161
00:06:19,520 --> 00:06:22,639
based board with all the local protocol

162
00:06:22,639 --> 00:06:23,759
support

163
00:06:23,759 --> 00:06:27,520
to run autonomously if there is let's

164
00:06:27,520 --> 00:06:30,240
say loss of connectivity to the sdn

165
00:06:30,240 --> 00:06:31,600
controller

166
00:06:31,600 --> 00:06:34,639
yet as you see there is a controller who

167
00:06:34,639 --> 00:06:35,360
provides

168
00:06:35,360 --> 00:06:37,759
uh high-level instructions which i'm

169
00:06:37,759 --> 00:06:38,560
talking about

170
00:06:38,560 --> 00:06:41,840
in two slides from now okay

171
00:06:41,840 --> 00:06:44,720
in the very beginning we had a look at a

172
00:06:44,720 --> 00:06:46,160
completely virtual

173
00:06:46,160 --> 00:06:49,199
last png solution but found out that

174
00:06:49,199 --> 00:06:51,840
running these kinds of services on

175
00:06:51,840 --> 00:06:53,680
standard data center servers

176
00:06:53,680 --> 00:06:56,720
does not really meet our requirements

177
00:06:56,720 --> 00:06:59,680
especially not in terms of qs support

178
00:06:59,680 --> 00:07:01,680
and power consumption

179
00:07:01,680 --> 00:07:03,199
therefore we started with an

180
00:07:03,199 --> 00:07:05,360
off-the-shelf switch because we believe

181
00:07:05,360 --> 00:07:05,919
that

182
00:07:05,919 --> 00:07:09,599
we need a dedicated data plane component

183
00:07:09,599 --> 00:07:11,919
we also spent some time investigating

184
00:07:11,919 --> 00:07:13,840
different types

185
00:07:13,840 --> 00:07:15,840
and different vendors of merchant

186
00:07:15,840 --> 00:07:17,120
silicon

187
00:07:17,120 --> 00:07:20,400
and we still do so as of now we've

188
00:07:20,400 --> 00:07:21,680
decided to start with

189
00:07:21,680 --> 00:07:24,400
broadcom's trader day nx asic family

190
00:07:24,400 --> 00:07:25,199
because

191
00:07:25,199 --> 00:07:28,000
they best meet our requirements in terms

192
00:07:28,000 --> 00:07:28,720
of scaling

193
00:07:28,720 --> 00:07:30,639
scaling means route scaling as well as

194
00:07:30,639 --> 00:07:32,240
subscriber scaling

195
00:07:32,240 --> 00:07:35,120
and in terms of qrs in deutsche telecom

196
00:07:35,120 --> 00:07:37,520
we extensively use hierarchical qrs

197
00:07:37,520 --> 00:07:38,479
which is not easy

198
00:07:38,479 --> 00:07:40,400
to implement with most chips on the

199
00:07:40,400 --> 00:07:42,720
market

200
00:07:42,720 --> 00:07:46,479
starting back in 2018 2090 we used

201
00:07:46,479 --> 00:07:48,479
standard edge core white box switches

202
00:07:48,479 --> 00:07:50,560
with broadcom common mx

203
00:07:50,560 --> 00:07:52,080
and the knowledge-based processor on

204
00:07:52,080 --> 00:07:54,639
board the cumin mx

205
00:07:54,639 --> 00:07:56,800
chip provides a sophisticated traffic

206
00:07:56,800 --> 00:07:59,039
manager that allows us to implement our

207
00:07:59,039 --> 00:08:02,800
hierarchical qr schema the chroma mx

208
00:08:02,800 --> 00:08:05,360
also provides enough throughput for use

209
00:08:05,360 --> 00:08:06,240
cases

210
00:08:06,240 --> 00:08:08,800
and the flip size is sufficient for most

211
00:08:08,800 --> 00:08:11,280
internet applications

212
00:08:11,280 --> 00:08:12,560
in addition the knowledge based

213
00:08:12,560 --> 00:08:14,720
processor is used for

214
00:08:14,720 --> 00:08:17,919
lookup table extension to support a

215
00:08:17,919 --> 00:08:18,319
large

216
00:08:18,319 --> 00:08:20,080
number of match rules that we need for

217
00:08:20,080 --> 00:08:22,560
instance for subscriber access lists

218
00:08:22,560 --> 00:08:26,800
statistics counters service queues etc

219
00:08:26,800 --> 00:08:28,720
now currently we are looking into the

220
00:08:28,720 --> 00:08:30,879
next generation of switches

221
00:08:30,879 --> 00:08:33,440
with higher port on the team we are

222
00:08:33,440 --> 00:08:34,958
actively working with

223
00:08:34,958 --> 00:08:37,839
some oem vendors on the design of these

224
00:08:37,839 --> 00:08:38,880
switches

225
00:08:38,880 --> 00:08:41,599
which will be based on broadcom command

226
00:08:41,599 --> 00:08:43,679
to see silicon

227
00:08:43,679 --> 00:08:45,360
the increased performance of this asic

228
00:08:45,360 --> 00:08:47,839
allows us to implement the routing stuff

229
00:08:47,839 --> 00:08:50,240
including all the services we've done so

230
00:08:50,240 --> 00:08:50,959
far

231
00:08:50,959 --> 00:08:53,920
without any kbp at least not on the

232
00:08:53,920 --> 00:08:55,200
spine switches

233
00:08:55,200 --> 00:08:56,800
for the leaf switches we are still

234
00:08:56,800 --> 00:08:59,200
investigating if the next generation kbp

235
00:08:59,200 --> 00:09:01,760
is necessary or not

236
00:09:01,760 --> 00:09:04,320
again we do not want to have hardware

237
00:09:04,320 --> 00:09:04,880
which is

238
00:09:04,880 --> 00:09:07,440
specifically built for dodger telecom

239
00:09:07,440 --> 00:09:08,240
only

240
00:09:08,240 --> 00:09:10,640
we are looking for commodity hardware to

241
00:09:10,640 --> 00:09:11,680
split

242
00:09:11,680 --> 00:09:14,240
the development costs across a larger

243
00:09:14,240 --> 00:09:15,360
group of

244
00:09:15,360 --> 00:09:18,959
interesting carriers let's have a look

245
00:09:18,959 --> 00:09:21,519
how the network design looks like in

246
00:09:21,519 --> 00:09:22,959
more detail

247
00:09:22,959 --> 00:09:25,959
first of all we do not have a greenfield

248
00:09:25,959 --> 00:09:27,120
environment.telecom

249
00:09:27,120 --> 00:09:29,519
we cannot really start from the scratch

250
00:09:29,519 --> 00:09:31,279
we have neighboring systems which we

251
00:09:31,279 --> 00:09:33,600
need to connect with

252
00:09:33,600 --> 00:09:35,760
the basic access for the rope pod

253
00:09:35,760 --> 00:09:39,120
consists of spine and leaf switches

254
00:09:39,120 --> 00:09:41,920
traditional data centers normally use

255
00:09:41,920 --> 00:09:44,000
three stage gloss fabrics because they

256
00:09:44,000 --> 00:09:46,080
have a huge amount of traffic which is

257
00:09:46,080 --> 00:09:48,160
flowing from east to west

258
00:09:48,160 --> 00:09:50,320
however in a telco environment the

259
00:09:50,320 --> 00:09:52,800
majority of traffic flows from north

260
00:09:52,800 --> 00:09:55,760
north to south and in order to reduce

261
00:09:55,760 --> 00:09:57,279
the number of switches

262
00:09:57,279 --> 00:09:59,839
and of course the costs in our setup the

263
00:09:59,839 --> 00:10:01,760
border leaf switches have been removed

264
00:10:01,760 --> 00:10:02,800
from the topology

265
00:10:02,800 --> 00:10:04,640
and collapsed with the spine switches

266
00:10:04,640 --> 00:10:07,680
into a single device

267
00:10:07,839 --> 00:10:09,839
therefore resulting in a two-stage

268
00:10:09,839 --> 00:10:11,440
fabric

269
00:10:11,440 --> 00:10:13,519
on the core forcing side there's the ipm

270
00:10:13,519 --> 00:10:14,560
paras backbone

271
00:10:14,560 --> 00:10:16,560
network speaking standard routing

272
00:10:16,560 --> 00:10:19,360
protocols like isis and bgp

273
00:10:19,360 --> 00:10:21,440
the spine switches are fully integrated

274
00:10:21,440 --> 00:10:23,120
into our backbone network as

275
00:10:23,120 --> 00:10:26,160
pe routers holding the

276
00:10:26,160 --> 00:10:29,600
entire bgp routing table

277
00:10:29,600 --> 00:10:32,079
on the other side customers are

278
00:10:32,079 --> 00:10:33,360
connected via

279
00:10:33,360 --> 00:10:35,680
amazon's olts or whatever access

280
00:10:35,680 --> 00:10:36,720
equipment

281
00:10:36,720 --> 00:10:39,760
usually establishing pppoe sessions to

282
00:10:39,760 --> 00:10:41,680
one of our researchers

283
00:10:41,680 --> 00:10:43,839
there's also the need to provide layer 2

284
00:10:43,839 --> 00:10:46,320
and layer 3 bit stream access for other

285
00:10:46,320 --> 00:10:47,760
isps

286
00:10:47,760 --> 00:10:49,680
in addition we use standard protocols

287
00:10:49,680 --> 00:10:51,839
like radios for session authentication

288
00:10:51,839 --> 00:10:55,120
and accounting between the leap searches

289
00:10:55,120 --> 00:10:57,200
and spine switches as han has already

290
00:10:57,200 --> 00:10:58,160
pointed out

291
00:10:58,160 --> 00:10:59,680
we are relying on standard routing

292
00:10:59,680 --> 00:11:02,079
protocols avoiding any proprietary

293
00:11:02,079 --> 00:11:03,920
fabric protocols or

294
00:11:03,920 --> 00:11:07,440
vendor lock-ins all of these protocols

295
00:11:07,440 --> 00:11:08,560
need to be supported

296
00:11:08,560 --> 00:11:10,320
in the network operating system of

297
00:11:10,320 --> 00:11:11,920
course

298
00:11:11,920 --> 00:11:14,399
that's clear and last but not least we

299
00:11:14,399 --> 00:11:16,959
have a controller in our pod

300
00:11:16,959 --> 00:11:19,920
which we call pow this controller is

301
00:11:19,920 --> 00:11:21,920
running on a kubernetes cluster which

302
00:11:21,920 --> 00:11:24,079
provides services like orchestration

303
00:11:24,079 --> 00:11:26,000
telemetry data analysis

304
00:11:26,000 --> 00:11:28,800
and also some kind of traffic steering

305
00:11:28,800 --> 00:11:30,480
uh communication is done again

306
00:11:30,480 --> 00:11:33,440
by standard protocols like rest or grpc

307
00:11:33,440 --> 00:11:34,560
or

308
00:11:34,560 --> 00:11:37,839
other similar protocols

309
00:11:37,839 --> 00:11:41,120
carson has earlier alluded to that

310
00:11:41,120 --> 00:11:44,079
we started with the classical chord

311
00:11:44,079 --> 00:11:44,800
approach

312
00:11:44,800 --> 00:11:48,320
and chord really means that you have a

313
00:11:48,320 --> 00:11:50,240
sort of a

314
00:11:50,240 --> 00:11:53,600
controller who micromanages each and

315
00:11:53,600 --> 00:11:55,200
every flow

316
00:11:55,200 --> 00:11:58,480
now when we actually did the math

317
00:11:58,480 --> 00:12:00,800
and said hey on some of the larger

318
00:12:00,800 --> 00:12:02,000
central offices

319
00:12:02,000 --> 00:12:03,920
with potential hundred thousands of

320
00:12:03,920 --> 00:12:05,600
subscribers

321
00:12:05,600 --> 00:12:08,800
and uh further assuming that

322
00:12:08,800 --> 00:12:11,360
for some of those services you have to

323
00:12:11,360 --> 00:12:12,560
program

324
00:12:12,560 --> 00:12:15,920
up to 80 uh forwarding uh

325
00:12:15,920 --> 00:12:19,440
related actions uh we could not really

326
00:12:19,440 --> 00:12:20,240
envision

327
00:12:20,240 --> 00:12:23,839
how to size the controller properly to

328
00:12:23,839 --> 00:12:25,760
manage that load

329
00:12:25,760 --> 00:12:29,120
so therefore we have looked

330
00:12:29,120 --> 00:12:32,880
into a way

331
00:12:32,880 --> 00:12:34,720
how to fuse together

332
00:12:34,720 --> 00:12:36,240
[Music]

333
00:12:36,240 --> 00:12:38,480
what a central controller really

334
00:12:38,480 --> 00:12:39,600
provides

335
00:12:39,600 --> 00:12:43,040
of information along with uh what

336
00:12:43,040 --> 00:12:46,800
uh routing protocols dynamically do

337
00:12:46,800 --> 00:12:50,720
discover on the core facing side

338
00:12:50,720 --> 00:12:54,000
uh no a big surprise here

339
00:12:54,000 --> 00:12:57,360
bgp and isis running on the fabric

340
00:12:57,360 --> 00:13:01,200
side of bgpl usa underlay l3vpn

341
00:13:01,200 --> 00:13:04,880
has the overlay and uh up to this point

342
00:13:04,880 --> 00:13:07,920
it actually looks like an almost

343
00:13:07,920 --> 00:13:11,680
classic setup like it has been deployed

344
00:13:11,680 --> 00:13:14,880
for the past two decades

345
00:13:14,880 --> 00:13:18,639
where it is actually different is

346
00:13:18,639 --> 00:13:23,279
the controller may influence a piece

347
00:13:23,279 --> 00:13:26,240
at a particular table here at the leaf

348
00:13:26,240 --> 00:13:28,079
which we call the service selection

349
00:13:28,079 --> 00:13:28,959
table

350
00:13:28,959 --> 00:13:32,160
where we have a couple of

351
00:13:32,160 --> 00:13:35,680
qualifiers and also a couple of actions

352
00:13:35,680 --> 00:13:37,040
available

353
00:13:37,040 --> 00:13:40,639
and uh the sdn controller

354
00:13:40,639 --> 00:13:44,160
can tell exactly what to do with

355
00:13:44,160 --> 00:13:48,079
any given customer flow so for example

356
00:13:48,079 --> 00:13:50,800
we could say hey please terminate uh

357
00:13:50,800 --> 00:13:53,600
vlan 100 vlan 200 on

358
00:13:53,600 --> 00:13:57,040
interface x and please

359
00:13:57,040 --> 00:14:02,000
i know from oss that this is

360
00:14:02,000 --> 00:14:04,480
residential customer so decapsulate

361
00:14:04,480 --> 00:14:05,600
pppoe

362
00:14:05,600 --> 00:14:08,000
and do a route lookup in the internet

363
00:14:08,000 --> 00:14:08,880
worth

364
00:14:08,880 --> 00:14:12,639
so this is the kind of uh functionality

365
00:14:12,639 --> 00:14:13,440
that is

366
00:14:13,440 --> 00:14:16,480
available to the service selection

367
00:14:16,480 --> 00:14:18,639
gateway

368
00:14:18,639 --> 00:14:21,680
we have also made it uh

369
00:14:21,680 --> 00:14:24,880
easier uh for application developers

370
00:14:24,880 --> 00:14:28,000
uh for the crew who programs the power

371
00:14:28,000 --> 00:14:31,519
our controller to

372
00:14:31,519 --> 00:14:34,959
basically install

373
00:14:34,959 --> 00:14:38,560
remote state information like

374
00:14:38,560 --> 00:14:41,440
for example if you terminate the service

375
00:14:41,440 --> 00:14:43,120
and you want to just

376
00:14:43,120 --> 00:14:46,240
shunt it off using an mpls pseudo wire

377
00:14:46,240 --> 00:14:48,639
you do not really need to set up the

378
00:14:48,639 --> 00:14:49,760
underlay

379
00:14:49,760 --> 00:14:52,800
and or everything along that path

380
00:14:52,800 --> 00:14:55,839
you just need on both ends of the pseudo

381
00:14:55,839 --> 00:14:56,480
wire

382
00:14:56,480 --> 00:14:59,760
tail hey i wanna relay here a certain

383
00:14:59,760 --> 00:15:01,199
vlan stack

384
00:15:01,199 --> 00:15:04,560
over a particular mpls service label and

385
00:15:04,560 --> 00:15:06,800
uh by the way this is going to get

386
00:15:06,800 --> 00:15:08,000
terminated at

387
00:15:08,000 --> 00:15:10,839
a egress router with a certain ip

388
00:15:10,839 --> 00:15:12,800
address

389
00:15:12,800 --> 00:15:16,480
the system then combines that

390
00:15:16,480 --> 00:15:19,839
with the information that have been

391
00:15:19,839 --> 00:15:21,760
discovered by the dynamic routing

392
00:15:21,760 --> 00:15:22,880
protocols

393
00:15:22,880 --> 00:15:26,959
and fuses together the

394
00:15:26,959 --> 00:15:28,959
service related information from the

395
00:15:28,959 --> 00:15:30,560
controller with

396
00:15:30,560 --> 00:15:34,639
what decentralized the leaves and spines

397
00:15:34,639 --> 00:15:37,680
have computed

398
00:15:37,680 --> 00:15:39,839
before moving into some more

399
00:15:39,839 --> 00:15:40,959
implementation

400
00:15:40,959 --> 00:15:44,240
specific details i want to briefly touch

401
00:15:44,240 --> 00:15:44,639
the

402
00:15:44,639 --> 00:15:47,199
bring up of the fabric network switches

403
00:15:47,199 --> 00:15:49,519
an important aspect of the access 4.0

404
00:15:49,519 --> 00:15:52,079
project is to automate as much as

405
00:15:52,079 --> 00:15:54,800
possible all the pots look very much the

406
00:15:54,800 --> 00:15:56,480
same inside also they might look

407
00:15:56,480 --> 00:15:58,560
differently from the outside world but

408
00:15:58,560 --> 00:15:59,920
internally they look completely

409
00:15:59,920 --> 00:16:02,240
identical using predefined addressing

410
00:16:02,240 --> 00:16:03,120
schema

411
00:16:03,120 --> 00:16:06,160
based on private ipv6 addresses

412
00:16:06,160 --> 00:16:09,360
and private four by the s numbers

413
00:16:09,360 --> 00:16:11,680
the first step of the bring up process

414
00:16:11,680 --> 00:16:13,600
is to automatically provide an ip

415
00:16:13,600 --> 00:16:14,240
address

416
00:16:14,240 --> 00:16:16,880
for management uh provide software

417
00:16:16,880 --> 00:16:18,000
images and provide

418
00:16:18,000 --> 00:16:20,480
a basic configuration that allows us to

419
00:16:20,480 --> 00:16:22,639
build the underlay network

420
00:16:22,639 --> 00:16:25,360
no surprise we are using dhcp to get all

421
00:16:25,360 --> 00:16:27,600
this information into the box

422
00:16:27,600 --> 00:16:30,480
but instead of using mac addresses which

423
00:16:30,480 --> 00:16:32,240
can easily be forged

424
00:16:32,240 --> 00:16:33,759
we are using serial numbers to

425
00:16:33,759 --> 00:16:35,519
authenticate the device

426
00:16:35,519 --> 00:16:38,639
the serial number of the box is stored

427
00:16:38,639 --> 00:16:39,680
in a database as

428
00:16:39,680 --> 00:16:42,079
part of the blending process and once

429
00:16:42,079 --> 00:16:44,480
the switch is physically installed

430
00:16:44,480 --> 00:16:46,880
the on-site engineer scans the barcode

431
00:16:46,880 --> 00:16:47,839
and

432
00:16:47,839 --> 00:16:50,160
the power which are already talked about

433
00:16:50,160 --> 00:16:51,839
then associates the switch with the

434
00:16:51,839 --> 00:16:55,360
right location and the right function

435
00:16:55,360 --> 00:16:57,440
the switch is usually delivered with

436
00:16:57,440 --> 00:17:00,399
nothing else but an only installed on it

437
00:17:00,399 --> 00:17:02,839
only stands for open network install

438
00:17:02,839 --> 00:17:04,000
environment

439
00:17:04,000 --> 00:17:06,799
and basically is a small linux system

440
00:17:06,799 --> 00:17:08,640
that provides tools to automatically

441
00:17:08,640 --> 00:17:10,799
install a network operating system

442
00:17:10,799 --> 00:17:13,919
on a bare metal switch he only includes

443
00:17:13,919 --> 00:17:15,199
a dhcp client

444
00:17:15,199 --> 00:17:17,599
to initial retrieve the sap address for

445
00:17:17,599 --> 00:17:19,359
management

446
00:17:19,359 --> 00:17:22,319
and the dhcp reply message also includes

447
00:17:22,319 --> 00:17:24,880
a link to the download

448
00:17:24,880 --> 00:17:27,439
location for the correct software in our

449
00:17:27,439 --> 00:17:28,400
case the article

450
00:17:28,400 --> 00:17:32,240
full stack software the only takes care

451
00:17:32,240 --> 00:17:32,799
of the

452
00:17:32,799 --> 00:17:34,480
software installation process and

453
00:17:34,480 --> 00:17:36,400
afterwards automatically reboots the

454
00:17:36,400 --> 00:17:37,280
switch

455
00:17:37,280 --> 00:17:40,480
without any further interaction uh once

456
00:17:40,480 --> 00:17:42,240
the switch is rebooted

457
00:17:42,240 --> 00:17:45,280
again it gets management up address

458
00:17:45,280 --> 00:17:48,720
via dhcp and also gets a link to

459
00:17:48,720 --> 00:17:50,640
download the configuration file

460
00:17:50,640 --> 00:17:54,640
which will be retrieved and activated

461
00:17:54,640 --> 00:17:56,240
and the nice thing about this approach

462
00:17:56,240 --> 00:17:59,120
is that we basically can include

463
00:17:59,120 --> 00:18:01,600
every whitebox switch into our network

464
00:18:01,600 --> 00:18:02,880
that supports

465
00:18:02,880 --> 00:18:06,240
only and has only

466
00:18:06,240 --> 00:18:09,520
pre-installed the configuration file

467
00:18:09,520 --> 00:18:11,440
itself

468
00:18:11,440 --> 00:18:14,640
is also automatically provided

469
00:18:14,640 --> 00:18:16,640
we have a configuration template for

470
00:18:16,640 --> 00:18:17,840
each switch role

471
00:18:17,840 --> 00:18:19,520
in this case for the spine switches and

472
00:18:19,520 --> 00:18:21,039
for the researchers

473
00:18:21,039 --> 00:18:24,240
and this template is more or less a json

474
00:18:24,240 --> 00:18:25,520
file with some

475
00:18:25,520 --> 00:18:28,559
variables which can be filled

476
00:18:28,559 --> 00:18:32,799
these variables are device specific and

477
00:18:32,799 --> 00:18:36,640
either um get filled by planning data

478
00:18:36,640 --> 00:18:38,880
like for instance loopback addresses or

479
00:18:38,880 --> 00:18:39,840
so or

480
00:18:39,840 --> 00:18:42,960
they get sometimes also

481
00:18:42,960 --> 00:18:45,440
calculated from a given algorithm so we

482
00:18:45,440 --> 00:18:46,480
have a

483
00:18:46,480 --> 00:18:49,039
predefined schema where we calculate

484
00:18:49,039 --> 00:18:52,080
certain variables from

485
00:18:52,080 --> 00:18:55,440
we have a template engine that

486
00:18:55,440 --> 00:18:57,840
uses this basic template and fills all

487
00:18:57,840 --> 00:18:58,960
the required data

488
00:18:58,960 --> 00:19:02,160
and then provides this configuration

489
00:19:02,160 --> 00:19:04,880
to our set ep server ready for download

490
00:19:04,880 --> 00:19:07,360
by the switches this template engine

491
00:19:07,360 --> 00:19:09,600
which does all the magic is developed by

492
00:19:09,600 --> 00:19:11,120
dodger telecom

493
00:19:11,120 --> 00:19:14,080
itself however the information needed to

494
00:19:14,080 --> 00:19:16,160
get the underlay network running is very

495
00:19:16,160 --> 00:19:18,480
little as we will see in the next

496
00:19:18,480 --> 00:19:20,799
slide

497
00:19:22,240 --> 00:19:25,280
okay so let's have a look at the fabric

498
00:19:25,280 --> 00:19:26,400
underlay

499
00:19:26,400 --> 00:19:28,080
the underlaying network consists of

500
00:19:28,080 --> 00:19:30,480
spinal researchers

501
00:19:30,480 --> 00:19:33,440
separating underlay from overlay

502
00:19:33,440 --> 00:19:35,679
services

503
00:19:35,679 --> 00:19:39,360
is very important for us because it

504
00:19:39,360 --> 00:19:40,880
provides the possibility for

505
00:19:40,880 --> 00:19:43,280
multi-tenancy

506
00:19:43,280 --> 00:19:45,360
as an incumbent we had dutch telecom or

507
00:19:45,360 --> 00:19:47,120
forced by regulation to provide some

508
00:19:47,120 --> 00:19:48,480
kind of access to

509
00:19:48,480 --> 00:19:52,799
other isps so having a separation of

510
00:19:52,799 --> 00:19:54,960
overlay and underlays in our

511
00:19:54,960 --> 00:19:56,240
infrastructure is very

512
00:19:56,240 --> 00:19:59,440
critical the leaf switches are

513
00:19:59,440 --> 00:20:01,600
connected to all the spine switches with

514
00:20:01,600 --> 00:20:04,159
a symmetric number of links

515
00:20:04,159 --> 00:20:07,200
allowing equal cost multipath routing

516
00:20:07,200 --> 00:20:10,080
within the fabric itself only external

517
00:20:10,080 --> 00:20:10,880
bgp

518
00:20:10,880 --> 00:20:14,000
is used as a control plane protocol

519
00:20:14,000 --> 00:20:16,320
we are using mpls as data pin

520
00:20:16,320 --> 00:20:17,760
encapsulation

521
00:20:17,760 --> 00:20:20,080
and the signaling is done via standard

522
00:20:20,080 --> 00:20:22,000
segment driving extensions to

523
00:20:22,000 --> 00:20:25,600
egp we have no

524
00:20:25,600 --> 00:20:28,159
need for any igp protocol within the

525
00:20:28,159 --> 00:20:29,120
fabric

526
00:20:29,120 --> 00:20:32,559
so usually igps are known for topology

527
00:20:32,559 --> 00:20:33,520
discovery

528
00:20:33,520 --> 00:20:36,080
but in a spine d fabric the topology is

529
00:20:36,080 --> 00:20:37,360
more or less known so

530
00:20:37,360 --> 00:20:40,799
you can avoid this and can rely on bgp

531
00:20:40,799 --> 00:20:43,200
only

532
00:20:43,360 --> 00:20:46,799
all the researchers have their own

533
00:20:46,799 --> 00:20:49,360
private as number uh while this bank

534
00:20:49,360 --> 00:20:51,440
switches share a single is number

535
00:20:51,440 --> 00:20:54,559
all right you might be already familiar

536
00:20:54,559 --> 00:20:56,320
with this kind of design so this

537
00:20:56,320 --> 00:21:00,080
is nothing special in order to get

538
00:21:00,080 --> 00:21:03,120
the underlay booting running each switch

539
00:21:03,120 --> 00:21:05,120
needs a unique identifier

540
00:21:05,120 --> 00:21:07,600
um for instance for the loopback address

541
00:21:07,600 --> 00:21:08,960
so we have unique

542
00:21:08,960 --> 00:21:11,760
ids for for example 11 12 for the spine

543
00:21:11,760 --> 00:21:12,559
switches

544
00:21:12,559 --> 00:21:16,159
and 21 22 for the leaf switches

545
00:21:16,159 --> 00:21:17,679
we are not restricted to two spine

546
00:21:17,679 --> 00:21:19,919
switches by the way uh we can have even

547
00:21:19,919 --> 00:21:20,240
more

548
00:21:20,240 --> 00:21:23,679
so from here from this id

549
00:21:23,679 --> 00:21:25,840
we calculate all the other parameters

550
00:21:25,840 --> 00:21:26,799
for the underlay

551
00:21:26,799 --> 00:21:28,559
for instance the s number the loopback

552
00:21:28,559 --> 00:21:30,320
address is the segment routing node

553
00:21:30,320 --> 00:21:33,039
sits and also various addresses for

554
00:21:33,039 --> 00:21:33,919
communication

555
00:21:33,919 --> 00:21:36,320
with the kubernetes cluster i've already

556
00:21:36,320 --> 00:21:37,840
mentioned

557
00:21:37,840 --> 00:21:40,640
for example to communicate with the

558
00:21:40,640 --> 00:21:41,440
telemetry

559
00:21:41,440 --> 00:21:44,320
and the orchestrator and so on the

560
00:21:44,320 --> 00:21:46,159
interesting part is the link between the

561
00:21:46,159 --> 00:21:47,039
switches

562
00:21:47,039 --> 00:21:48,799
and the links between spine leaf

563
00:21:48,799 --> 00:21:51,840
switches only have an ipv6 link local

564
00:21:51,840 --> 00:21:52,799
address

565
00:21:52,799 --> 00:21:56,240
so no need to do any configuration here

566
00:21:56,240 --> 00:21:58,080
and once the leaf switch has been

567
00:21:58,080 --> 00:21:59,919
configured and

568
00:21:59,919 --> 00:22:01,760
the link between the leaf switch and the

569
00:22:01,760 --> 00:22:02,960
spine switch

570
00:22:02,960 --> 00:22:05,760
goes physically up uh the spine the leaf

571
00:22:05,760 --> 00:22:06,880
search sorry

572
00:22:06,880 --> 00:22:09,760
uh tries to establish an ebgp session to

573
00:22:09,760 --> 00:22:10,880
the spine switch

574
00:22:10,880 --> 00:22:15,120
across the ipv6 link local address

575
00:22:15,120 --> 00:22:17,360
this is an event-based service

576
00:22:17,360 --> 00:22:18,880
implemented to overcome the lack of

577
00:22:18,880 --> 00:22:21,440
auto-discovery and standard bgp

578
00:22:21,440 --> 00:22:23,039
the spine switches are configured to

579
00:22:23,039 --> 00:22:25,120
accept bgp sessions from a

580
00:22:25,120 --> 00:22:28,080
predefined range of ios numbers and for

581
00:22:28,080 --> 00:22:29,280
authentication

582
00:22:29,280 --> 00:22:32,880
the tcp authentication option is used

583
00:22:32,880 --> 00:22:34,640
once the underlay is established

584
00:22:34,640 --> 00:22:36,240
everything else is run as an

585
00:22:36,240 --> 00:22:39,520
nps service on top of it for example

586
00:22:39,520 --> 00:22:42,559
standard layer free vpns or layer 2 vpns

587
00:22:42,559 --> 00:22:45,600
or evpl this is mainly done to provide

588
00:22:45,600 --> 00:22:47,039
multi-tenancy

589
00:22:47,039 --> 00:22:49,919
as already mentioned but it also

590
00:22:49,919 --> 00:22:52,320
provides a great deal of flexibility

591
00:22:52,320 --> 00:22:54,880
because we can easily support any new

592
00:22:54,880 --> 00:22:56,080
services by

593
00:22:56,080 --> 00:22:58,159
introducing appropriate bgp at trust

594
00:22:58,159 --> 00:23:00,000
families without changing the underlay

595
00:23:00,000 --> 00:23:01,760
at all

596
00:23:01,760 --> 00:23:04,720
let's consider our standard internet

597
00:23:04,720 --> 00:23:06,720
service as an example

598
00:23:06,720 --> 00:23:09,039
this service is run as a dual stack

599
00:23:09,039 --> 00:23:11,520
layer free vpn across the fabric

600
00:23:11,520 --> 00:23:14,640
the server signaling is done via bgp vpn

601
00:23:14,640 --> 00:23:15,120
v4

602
00:23:15,120 --> 00:23:18,320
vpn v6 address families remember

603
00:23:18,320 --> 00:23:22,400
the next top are labeled ipv6 addresses

604
00:23:22,400 --> 00:23:25,200
as explained on the previous slide the

605
00:23:25,200 --> 00:23:26,720
spine switches will handle all the

606
00:23:26,720 --> 00:23:27,520
routing

607
00:23:27,520 --> 00:23:29,679
needed for fabric internal communication

608
00:23:29,679 --> 00:23:31,280
as well as communication to the

609
00:23:31,280 --> 00:23:32,960
backboard network

610
00:23:32,960 --> 00:23:34,960
so the spine switches will have the full

611
00:23:34,960 --> 00:23:36,000
internet routing table

612
00:23:36,000 --> 00:23:38,960
within the internet era the leaf

613
00:23:38,960 --> 00:23:40,720
switches on the other hand will have all

614
00:23:40,720 --> 00:23:41,039
the

615
00:23:41,039 --> 00:23:42,960
pvp subscriber related routing

616
00:23:42,960 --> 00:23:44,080
information

617
00:23:44,080 --> 00:23:46,880
this includes usually free addresses or

618
00:23:46,880 --> 00:23:48,159
three prefixes

619
00:23:48,159 --> 00:23:51,520
one slash 32 ipv6 pv4 prefix

620
00:23:51,520 --> 00:23:55,279
1 64 ipv6 prefix under delegated prefix

621
00:23:55,279 --> 00:23:57,840
per subscriber

622
00:23:57,840 --> 00:24:00,880
in order to reduce the load um

623
00:24:00,880 --> 00:24:04,159
and also prevent routing flaps

624
00:24:04,159 --> 00:24:06,559
leaf switches will only advertise pool

625
00:24:06,559 --> 00:24:09,039
prefixes to the spines and hide all the

626
00:24:09,039 --> 00:24:11,919
subscriber specific details on the other

627
00:24:11,919 --> 00:24:13,360
hand the spine switches will only

628
00:24:13,360 --> 00:24:14,960
advertise default routes to the least

629
00:24:14,960 --> 00:24:16,000
switches

630
00:24:16,000 --> 00:24:19,279
as the leaf switches know that they only

631
00:24:19,279 --> 00:24:19,679
have

632
00:24:19,679 --> 00:24:21,360
upstream connectivity through the spine

633
00:24:21,360 --> 00:24:22,880
switches anyway

634
00:24:22,880 --> 00:24:25,760
so let's have a look at the backbone

635
00:24:25,760 --> 00:24:27,039
connectivity

636
00:24:27,039 --> 00:24:28,880
the spine switches are physically

637
00:24:28,880 --> 00:24:30,640
connected to our

638
00:24:30,640 --> 00:24:33,840
p routers or lsrs of the mp

639
00:24:33,840 --> 00:24:37,279
ipm plus network the internet service

640
00:24:37,279 --> 00:24:39,279
runs on the spine switches

641
00:24:39,279 --> 00:24:42,320
as a kind of mpls layer free vpn

642
00:24:42,320 --> 00:24:43,600
this implies that all the

643
00:24:43,600 --> 00:24:46,080
backbone-facing protocols run

644
00:24:46,080 --> 00:24:48,799
as pec routing protocols within the

645
00:24:48,799 --> 00:24:49,919
internet vpn

646
00:24:49,919 --> 00:24:52,880
instance the ipm player's backbone

647
00:24:52,880 --> 00:24:54,880
network delivers its service based on

648
00:24:54,880 --> 00:24:55,520
bgp

649
00:24:55,520 --> 00:24:59,200
over ipv4 internet

650
00:24:59,200 --> 00:25:02,880
bgp sessions are established to multiple

651
00:25:02,880 --> 00:25:05,919
auto reflectors for redundancy

652
00:25:05,919 --> 00:25:08,960
services include rpv for internet ipv6

653
00:25:08,960 --> 00:25:09,600
internet

654
00:25:09,600 --> 00:25:12,480
which is implemented as 6p service as

655
00:25:12,480 --> 00:25:14,240
well as traditional layer two layer free

656
00:25:14,240 --> 00:25:16,640
vpns

657
00:25:16,640 --> 00:25:20,159
for the empire's transport isr's level

658
00:25:20,159 --> 00:25:22,559
one routing is used to connect to the

659
00:25:22,559 --> 00:25:23,840
lsrs

660
00:25:23,840 --> 00:25:26,080
label distribution is done via segment

661
00:25:26,080 --> 00:25:28,320
routing extensions

662
00:25:28,320 --> 00:25:30,480
we have used ldp in the past but with

663
00:25:30,480 --> 00:25:32,240
this project we're only using segment

664
00:25:32,240 --> 00:25:34,159
routing

665
00:25:34,159 --> 00:25:36,720
this kind this is kind of tricky as

666
00:25:36,720 --> 00:25:38,400
there are not many implementations on

667
00:25:38,400 --> 00:25:40,960
the market that support running labeled

668
00:25:40,960 --> 00:25:44,640
as ios issue protocol

669
00:25:44,640 --> 00:25:46,799
now one thing to highlight is that in

670
00:25:46,799 --> 00:25:48,880
order to reduce the number of segment

671
00:25:48,880 --> 00:25:49,600
routing node

672
00:25:49,600 --> 00:25:51,679
sits in the backbone network the spine

673
00:25:51,679 --> 00:25:54,240
switches share an anycast node set which

674
00:25:54,240 --> 00:25:56,320
is advertised to the core network

675
00:25:56,320 --> 00:25:58,799
and which is used as a bgp next top for

676
00:25:58,799 --> 00:26:01,520
all the gpa impeller services

677
00:26:01,520 --> 00:26:04,400
and this is also the benefit that we can

678
00:26:04,400 --> 00:26:05,840
scale to more than

679
00:26:05,840 --> 00:26:08,799
two switches without increasing the load

680
00:26:08,799 --> 00:26:10,720
in the ipm pad as backbone

681
00:26:10,720 --> 00:26:13,679
as they just appear as a single device

682
00:26:13,679 --> 00:26:14,159
from an

683
00:26:14,159 --> 00:26:17,200
isis perspective

684
00:26:19,360 --> 00:26:21,919
we've already explained that everything

685
00:26:21,919 --> 00:26:23,679
in the fabric is implemented as a

686
00:26:23,679 --> 00:26:24,400
service

687
00:26:24,400 --> 00:26:27,279
so multicast which is used for iptv is

688
00:26:27,279 --> 00:26:30,640
just another example for such a service

689
00:26:30,640 --> 00:26:32,640
because we are substituting a black box

690
00:26:32,640 --> 00:26:34,000
bng device

691
00:26:34,000 --> 00:26:36,400
we cannot change any interface that is

692
00:26:36,400 --> 00:26:38,240
outside of fabric

693
00:26:38,240 --> 00:26:40,880
so customers still signal their interest

694
00:26:40,880 --> 00:26:42,799
in receiving a specific multicast

695
00:26:42,799 --> 00:26:43,760
channel via

696
00:26:43,760 --> 00:26:46,880
igmp version 2 to release searches

697
00:26:46,880 --> 00:26:49,440
igmp version 2 requests by the way are

698
00:26:49,440 --> 00:26:52,080
sent within an existing ppp session so

699
00:26:52,080 --> 00:26:55,360
there's a project here on the other side

700
00:26:55,360 --> 00:26:56,559
the backbone network

701
00:26:56,559 --> 00:26:59,760
uses pim to signal multicast today maybe

702
00:26:59,760 --> 00:27:02,480
mldp in the future

703
00:27:02,480 --> 00:27:04,799
within the fabric multicast is

704
00:27:04,799 --> 00:27:07,120
implemented as a next generation mvpn

705
00:27:07,120 --> 00:27:07,919
service

706
00:27:07,919 --> 00:27:10,159
again you see the pattern we are relying

707
00:27:10,159 --> 00:27:11,440
on the bgp

708
00:27:11,440 --> 00:27:14,080
protocol the leaf searches convert the

709
00:27:14,080 --> 00:27:14,799
igmp

710
00:27:14,799 --> 00:27:18,159
joints to specific multicast and

711
00:27:18,159 --> 00:27:20,640
send pgp source journal first join

712
00:27:20,640 --> 00:27:22,720
messages towards the spine switches

713
00:27:22,720 --> 00:27:24,880
which themselves convert these messages

714
00:27:24,880 --> 00:27:28,399
to the protocol used in the backbone

715
00:27:28,399 --> 00:27:30,960
as we have only one hierarchy of leaf

716
00:27:30,960 --> 00:27:33,039
switches the spine switches need to

717
00:27:33,039 --> 00:27:35,679
replicate the multicast traffic for each

718
00:27:35,679 --> 00:27:37,760
leaf switch

719
00:27:37,760 --> 00:27:40,000
setting up point to multipoint lsps does

720
00:27:40,000 --> 00:27:42,000
not bring any benefits from a data plane

721
00:27:42,000 --> 00:27:42,880
perspective

722
00:27:42,880 --> 00:27:45,279
but only increases the complexity of the

723
00:27:45,279 --> 00:27:46,880
control plane

724
00:27:46,880 --> 00:27:49,200
so we've chosen to use increaser

725
00:27:49,200 --> 00:27:51,039
application under spine switches for the

726
00:27:51,039 --> 00:27:53,360
sake of simplicity

727
00:27:53,360 --> 00:27:55,679
the researchers will take care of the

728
00:27:55,679 --> 00:27:57,200
multicast replication

729
00:27:57,200 --> 00:28:00,320
to the individual subscribers

730
00:28:00,320 --> 00:28:03,760
so now we had the challenge of

731
00:28:03,760 --> 00:28:07,039
developing an entire nostac

732
00:28:07,039 --> 00:28:10,399
in support of all those protocols

733
00:28:10,399 --> 00:28:14,640
and an entire nostac essentially means

734
00:28:14,640 --> 00:28:18,240
infrastructure apis cli

735
00:28:18,240 --> 00:28:22,159
data plane codes build a test harness

736
00:28:22,159 --> 00:28:22,799
around it

737
00:28:22,799 --> 00:28:25,919
and get it production ready

738
00:28:25,919 --> 00:28:29,360
how did we do it we have started with

739
00:28:29,360 --> 00:28:33,200
uh the core idea of a distributed data

740
00:28:33,200 --> 00:28:34,240
store

741
00:28:34,240 --> 00:28:37,279
that uh can handle even cruel bgp

742
00:28:37,279 --> 00:28:38,640
workloads

743
00:28:38,640 --> 00:28:41,840
of hundreds of millions of pgp paths

744
00:28:41,840 --> 00:28:44,640
and focused a lot on getting the

745
00:28:44,640 --> 00:28:47,200
transaction performance here right

746
00:28:47,200 --> 00:28:49,760
up to 2 million transactions per cpu

747
00:28:49,760 --> 00:28:52,080
core

748
00:28:52,720 --> 00:28:56,799
since every demon in the system

749
00:28:56,799 --> 00:28:59,440
needs also to share state with one

750
00:28:59,440 --> 00:29:00,080
another

751
00:29:00,080 --> 00:29:02,480
we had to also ensure that we can

752
00:29:02,480 --> 00:29:03,279
utilize

753
00:29:03,279 --> 00:29:06,480
uh the maximum memory bandwidth between

754
00:29:06,480 --> 00:29:08,000
cpu cores

755
00:29:08,000 --> 00:29:10,080
and as such really only that memory

756
00:29:10,080 --> 00:29:12,799
bandwidth is the limit using

757
00:29:12,799 --> 00:29:16,159
shared memory sockets you may

758
00:29:16,159 --> 00:29:18,880
ask yourself why we did not really start

759
00:29:18,880 --> 00:29:19,360
with

760
00:29:19,360 --> 00:29:22,720
a good popular open source backstores

761
00:29:22,720 --> 00:29:25,520
for example like redis

762
00:29:25,520 --> 00:29:28,320
and i can tell you we looked at most of

763
00:29:28,320 --> 00:29:30,399
what was available at the time

764
00:29:30,399 --> 00:29:33,919
but in 2016 there was no

765
00:29:33,919 --> 00:29:36,720
good multi-threading support for readers

766
00:29:36,720 --> 00:29:37,279
and

767
00:29:37,279 --> 00:29:40,159
also the clustering support was in a

768
00:29:40,159 --> 00:29:40,720
very

769
00:29:40,720 --> 00:29:45,120
early stage mostly lacking end-to-end

770
00:29:45,120 --> 00:29:50,399
integrity uh protocol support here

771
00:29:50,399 --> 00:29:55,039
now that ubiquitous data store concept

772
00:29:55,039 --> 00:29:58,159
has also fundamentally changed how

773
00:29:58,159 --> 00:30:00,480
we have built routing and access

774
00:30:00,480 --> 00:30:02,960
protocols on top of that

775
00:30:02,960 --> 00:30:06,000
think about that every protocol gets

776
00:30:06,000 --> 00:30:07,520
broken down

777
00:30:07,520 --> 00:30:11,840
to a table table schema for

778
00:30:11,840 --> 00:30:15,120
holding its peers adjacencies

779
00:30:15,120 --> 00:30:18,880
link state databases ribbon rib outs

780
00:30:18,880 --> 00:30:22,559
subscriber table statistics even logs so

781
00:30:22,559 --> 00:30:25,520
everything really gets reduced to a

782
00:30:25,520 --> 00:30:27,520
table schema

783
00:30:27,520 --> 00:30:31,360
now by reducing everything to a few

784
00:30:31,360 --> 00:30:34,080
transaction to a database and the

785
00:30:34,080 --> 00:30:36,240
clearly defined table schema

786
00:30:36,240 --> 00:30:39,440
one can auto generate

787
00:30:39,440 --> 00:30:43,760
an api and read write every attribute

788
00:30:43,760 --> 00:30:46,720
in that table and sometimes really look

789
00:30:46,720 --> 00:30:48,840
inside a particular protocol's

790
00:30:48,840 --> 00:30:51,039
implementation

791
00:30:51,039 --> 00:30:53,919
that thing is not only tremendously

792
00:30:53,919 --> 00:30:55,919
useful for operations

793
00:30:55,919 --> 00:30:59,519
because nothing gets really hidden

794
00:30:59,519 --> 00:31:02,559
it also helps developers

795
00:31:02,559 --> 00:31:06,240
testers and customer validation teams

796
00:31:06,240 --> 00:31:09,600
to build together jointly regression and

797
00:31:09,600 --> 00:31:12,320
validation test cases

798
00:31:12,320 --> 00:31:16,840
it is fair to say that the idea of

799
00:31:16,840 --> 00:31:19,760
the backstore

800
00:31:19,760 --> 00:31:24,000
plus uh the auto-generated api

801
00:31:24,000 --> 00:31:27,120
has really established here a common

802
00:31:27,120 --> 00:31:28,640
language almost

803
00:31:28,640 --> 00:31:31,919
between the developers and also

804
00:31:31,919 --> 00:31:35,080
hear our customers doing all the

805
00:31:35,080 --> 00:31:36,480
pre-qualification

806
00:31:36,480 --> 00:31:40,159
of the software now uh what we have

807
00:31:40,159 --> 00:31:40,559
built

808
00:31:40,559 --> 00:31:43,760
underneath that data store is also a

809
00:31:43,760 --> 00:31:44,559
very large

810
00:31:44,559 --> 00:31:48,000
module we call that a forwarding

811
00:31:48,000 --> 00:31:49,600
abstraction

812
00:31:49,600 --> 00:31:52,080
forwarding hierarchy dependency manager

813
00:31:52,080 --> 00:31:53,760
which is a bit of a

814
00:31:53,760 --> 00:31:57,760
a large name a short

815
00:31:57,760 --> 00:32:01,039
terminology here is rip dn50

816
00:32:01,039 --> 00:32:05,519
it is essentially a standard

817
00:32:05,519 --> 00:32:08,960
way to expose certain primitives

818
00:32:08,960 --> 00:32:12,320
primitives like interfaces routes

819
00:32:12,320 --> 00:32:16,880
next tops to expose slicing concepts

820
00:32:16,880 --> 00:32:22,320
like vrf's virtual forwarding tables

821
00:32:22,480 --> 00:32:26,000
model various tunneling protocols

822
00:32:26,000 --> 00:32:29,440
mpls mpls pseudowire

823
00:32:29,440 --> 00:32:33,360
pppoe l2dp

824
00:32:33,679 --> 00:32:36,799
there is also a standard abstraction

825
00:32:36,799 --> 00:32:39,760
for our matching rules we have mentioned

826
00:32:39,760 --> 00:32:41,120
service selection

827
00:32:41,120 --> 00:32:44,880
before we mostly try to break that down

828
00:32:44,880 --> 00:32:48,320
into chip level tcam lookups

829
00:32:48,320 --> 00:32:51,519
counters and uh each costs

830
00:32:51,519 --> 00:32:54,960
what this piece of software does

831
00:32:54,960 --> 00:32:58,559
also it tries to figure out and compile

832
00:32:58,559 --> 00:33:02,320
all the various um information from

833
00:33:02,320 --> 00:33:04,640
various layers so if the sdn

834
00:33:04,640 --> 00:33:07,279
controller has instantiated let's say

835
00:33:07,279 --> 00:33:08,480
absolute wire

836
00:33:08,480 --> 00:33:11,919
and then pgp lu has discovered a fabric

837
00:33:11,919 --> 00:33:12,960
route

838
00:33:12,960 --> 00:33:15,760
and uh in order to resolve we are

839
00:33:15,760 --> 00:33:18,320
relying on the harp and the

840
00:33:18,320 --> 00:33:20,880
response from a neighbor are all those

841
00:33:20,880 --> 00:33:21,840
information

842
00:33:21,840 --> 00:33:24,559
we call a forwarding chain and once a

843
00:33:24,559 --> 00:33:25,679
forwarding chain

844
00:33:25,679 --> 00:33:29,039
is ready it is resolvable

845
00:33:29,039 --> 00:33:32,960
we actually pour out individual

846
00:33:32,960 --> 00:33:36,640
chip level instructions so underneath

847
00:33:36,640 --> 00:33:40,159
that big gray bar there is

848
00:33:40,159 --> 00:33:43,120
forwarding plugins we've started here

849
00:33:43,120 --> 00:33:44,240
with

850
00:33:44,240 --> 00:33:47,440
fdio x86

851
00:33:47,440 --> 00:33:50,640
code mostly

852
00:33:50,640 --> 00:33:53,600
to um in the proof of concept phase of

853
00:33:53,600 --> 00:33:55,840
the company then later on

854
00:33:55,840 --> 00:33:59,760
uh we have uh built a full bng

855
00:33:59,760 --> 00:34:02,960
on our barefoot tofino but as carson has

856
00:34:02,960 --> 00:34:03,840
mentioned

857
00:34:03,840 --> 00:34:06,720
uh there was a lot of emphasis on

858
00:34:06,720 --> 00:34:08,918
traffic management capabilities

859
00:34:08,918 --> 00:34:10,079
hierarchical

860
00:34:10,079 --> 00:34:13,359
age cause policing shaping all of that

861
00:34:13,359 --> 00:34:17,280
and uh dt network engineering has here

862
00:34:17,280 --> 00:34:18,239
found

863
00:34:18,239 --> 00:34:20,639
the broadcom dnx product line

864
00:34:20,639 --> 00:34:21,760
particularly

865
00:34:21,760 --> 00:34:26,480
jericho and jericho to a good fit

866
00:34:26,560 --> 00:34:30,719
now all of that uh software gets bundled

867
00:34:30,719 --> 00:34:31,199
in

868
00:34:31,199 --> 00:34:34,560
i would say totally now roughly 20

869
00:34:34,560 --> 00:34:38,320
different daemons which run as a systemd

870
00:34:38,320 --> 00:34:39,359
service

871
00:34:39,359 --> 00:34:42,879
inside a linux container

872
00:34:42,879 --> 00:34:46,239
um the linux container is a standard lxc

873
00:34:46,239 --> 00:34:49,760
2.0 container running uh

874
00:34:49,760 --> 00:34:52,399
unchanged almost unchanged version of

875
00:34:52,399 --> 00:34:53,520
ubuntu

876
00:34:53,520 --> 00:34:56,960
1804 and um

877
00:34:56,960 --> 00:35:00,640
our daemons our software packages all

878
00:35:00,640 --> 00:35:02,480
running user space

879
00:35:02,480 --> 00:35:06,480
and um are integrated in

880
00:35:06,480 --> 00:35:09,760
ubuntu's packet manager so

881
00:35:09,760 --> 00:35:12,720
upgrading a piece of software um works

882
00:35:12,720 --> 00:35:14,079
exactly like

883
00:35:14,079 --> 00:35:18,880
you would do it on the ubuntu server

884
00:35:18,880 --> 00:35:23,280
outside of that lxc container

885
00:35:23,280 --> 00:35:26,480
on the host os we have a small container

886
00:35:26,480 --> 00:35:27,520
runtime

887
00:35:27,520 --> 00:35:30,400
of ours which has been built around

888
00:35:30,400 --> 00:35:31,680
mostly lxd

889
00:35:31,680 --> 00:35:35,119
2.0 what it does is uh

890
00:35:35,119 --> 00:35:37,839
simply starting stopping containers

891
00:35:37,839 --> 00:35:40,240
loading container images

892
00:35:40,240 --> 00:35:43,359
um graceful shutdown and insertion

893
00:35:43,359 --> 00:35:46,800
of components making sure that all the

894
00:35:46,800 --> 00:35:49,680
software components inside

895
00:35:49,680 --> 00:35:52,000
are running so it tracks keep alive

896
00:35:52,000 --> 00:35:53,599
signatures

897
00:35:53,599 --> 00:35:56,960
but also

898
00:35:56,960 --> 00:36:01,200
um it does of course handle software

899
00:36:01,200 --> 00:36:03,040
upgrades and downgrades

900
00:36:03,040 --> 00:36:06,240
our model is very simple uh there it may

901
00:36:06,240 --> 00:36:06,640
be

902
00:36:06,640 --> 00:36:10,000
several containers um stored on the

903
00:36:10,000 --> 00:36:10,720
switch

904
00:36:10,720 --> 00:36:13,599
however only one will be running that is

905
00:36:13,599 --> 00:36:14,880
usually the one

906
00:36:14,880 --> 00:36:19,040
that controls the forwarding hardware

907
00:36:19,040 --> 00:36:22,720
um the container runtime also is

908
00:36:22,720 --> 00:36:26,240
our universal um

909
00:36:26,240 --> 00:36:29,520
api gateway into the container

910
00:36:29,520 --> 00:36:33,440
so whatever um external

911
00:36:33,440 --> 00:36:35,760
protocol you want to access the switch

912
00:36:35,760 --> 00:36:37,760
and there's plenty of those

913
00:36:37,760 --> 00:36:41,280
starting from of course ssh

914
00:36:41,280 --> 00:36:44,960
tech plus for authentication uh

915
00:36:44,960 --> 00:36:49,040
rest uh calls of all sorts of flavors

916
00:36:49,040 --> 00:36:53,280
uh oh art so all of those things

917
00:36:53,280 --> 00:36:55,680
are here also implemented in the

918
00:36:55,680 --> 00:36:57,200
container runtime

919
00:36:57,200 --> 00:37:00,320
the idea is we have just a single end

920
00:37:00,320 --> 00:37:01,200
point

921
00:37:01,200 --> 00:37:05,440
where we can talk mostly using

922
00:37:05,440 --> 00:37:08,079
standard-based protocols even outright

923
00:37:08,079 --> 00:37:09,359
open source

924
00:37:09,359 --> 00:37:12,400
packages the bds

925
00:37:12,400 --> 00:37:15,599
data store for us is

926
00:37:15,599 --> 00:37:19,119
a single source of truth we

927
00:37:19,119 --> 00:37:23,359
retrieve all the state um in and out

928
00:37:23,359 --> 00:37:26,400
from the table it is

929
00:37:26,400 --> 00:37:30,880
not just a standalone key value store

930
00:37:30,880 --> 00:37:34,320
it is also um a plugin

931
00:37:34,320 --> 00:37:38,640
server so um maybe you're familiar with

932
00:37:38,640 --> 00:37:42,560
uh the radius database able to run

933
00:37:42,560 --> 00:37:46,480
lua scripts uh we have added here

934
00:37:46,480 --> 00:37:51,520
the capability to load any particular

935
00:37:51,520 --> 00:37:54,880
shared library any particular symbol

936
00:37:54,880 --> 00:37:57,680
and uh what we can do here and what we

937
00:37:57,680 --> 00:37:58,320
do

938
00:37:58,320 --> 00:38:02,640
is uh for every table that we define

939
00:38:02,640 --> 00:38:05,359
we can actually define here a couple of

940
00:38:05,359 --> 00:38:07,440
stored procedure symbols

941
00:38:07,440 --> 00:38:10,560
and if um an

942
00:38:10,560 --> 00:38:13,200
object gets inserted in that table that

943
00:38:13,200 --> 00:38:13,920
code

944
00:38:13,920 --> 00:38:16,160
is getting invoked and it doesn't really

945
00:38:16,160 --> 00:38:17,200
matter um

946
00:38:17,200 --> 00:38:20,160
you know how the element uh how the

947
00:38:20,160 --> 00:38:20,640
object

948
00:38:20,640 --> 00:38:23,280
gets into the system uh whether it's

949
00:38:23,280 --> 00:38:25,119
coming from an api

950
00:38:25,119 --> 00:38:29,760
or from an embedded a tcp ip stack

951
00:38:29,760 --> 00:38:32,800
or uh from our own

952
00:38:32,800 --> 00:38:37,520
plug-in code in fact

953
00:38:37,760 --> 00:38:41,440
the whole state flow of within the

954
00:38:41,440 --> 00:38:42,560
system

955
00:38:42,560 --> 00:38:46,160
is being modeled after hey some

956
00:38:46,160 --> 00:38:48,960
object gets installed in the table and

957
00:38:48,960 --> 00:38:49,280
this

958
00:38:49,280 --> 00:38:51,839
triggers some plugin code which again

959
00:38:51,839 --> 00:38:52,640
publishes

960
00:38:52,640 --> 00:38:56,720
and reconciles several information into

961
00:38:56,720 --> 00:39:00,320
another table i have prepared here

962
00:39:00,320 --> 00:39:03,520
a small example that should just

963
00:39:03,520 --> 00:39:07,359
get you the idea how a simple

964
00:39:07,359 --> 00:39:11,119
route flow works in the system

965
00:39:11,119 --> 00:39:14,880
let's take for example a producer

966
00:39:14,880 --> 00:39:18,240
of routing information like um

967
00:39:18,240 --> 00:39:21,680
the bgp uh a rip out

968
00:39:21,680 --> 00:39:24,720
a piece of software so usually what it

969
00:39:24,720 --> 00:39:25,280
does

970
00:39:25,280 --> 00:39:28,480
is after uh learning its routing

971
00:39:28,480 --> 00:39:29,280
information

972
00:39:29,280 --> 00:39:32,400
doing best path selection policy and all

973
00:39:32,400 --> 00:39:33,200
of that

974
00:39:33,200 --> 00:39:36,320
it publishes the information in a verb

975
00:39:36,320 --> 00:39:38,160
specific

976
00:39:38,160 --> 00:39:42,720
table along with and it holds references

977
00:39:42,720 --> 00:39:46,079
along to a particular next top table

978
00:39:46,079 --> 00:39:47,520
which is shared

979
00:39:47,520 --> 00:39:51,599
across routing instances of course

980
00:39:51,599 --> 00:39:54,079
if there is icmp there is also the

981
00:39:54,079 --> 00:39:57,040
possibility to group several next hubs

982
00:39:57,040 --> 00:39:57,839
together

983
00:39:57,839 --> 00:40:01,960
in next top sets up to

984
00:40:01,960 --> 00:40:04,960
256 ecmp

985
00:40:04,960 --> 00:40:07,680
neighbors whatever the local forwarding

986
00:40:07,680 --> 00:40:08,319
hardware

987
00:40:08,319 --> 00:40:11,760
does support so now

988
00:40:11,760 --> 00:40:15,200
um the next component

989
00:40:15,200 --> 00:40:17,760
which is the rip manager subscribes to

990
00:40:17,760 --> 00:40:20,560
those tables

991
00:40:20,880 --> 00:40:24,079
reads all the um

992
00:40:24,079 --> 00:40:27,839
flip local tables from isis pgp

993
00:40:27,839 --> 00:40:30,960
even access right uh and uh

994
00:40:30,960 --> 00:40:34,000
conciles uh what is the best

995
00:40:34,000 --> 00:40:36,839
uh route source in case there is an

996
00:40:36,839 --> 00:40:38,319
overlap

997
00:40:38,319 --> 00:40:40,319
now something interesting happens right

998
00:40:40,319 --> 00:40:41,440
uh now

999
00:40:41,440 --> 00:40:44,880
the best path or the best protocol

1000
00:40:44,880 --> 00:40:48,000
gets selected along with a given

1001
00:40:48,000 --> 00:40:49,040
forwarding chain

1002
00:40:49,040 --> 00:40:52,240
and this can be multiple levels of

1003
00:40:52,240 --> 00:40:56,160
route resolution and if it is possible

1004
00:40:56,160 --> 00:40:56,960
to

1005
00:40:56,960 --> 00:40:59,599
resolve a particular route chain it

1006
00:40:59,599 --> 00:41:00,160
finally

1007
00:41:00,160 --> 00:41:03,599
ends up here in that global rippy

1008
00:41:03,599 --> 00:41:07,599
adjacency table now all the lower

1009
00:41:07,599 --> 00:41:11,520
uh forwarding tables in the forwarding

1010
00:41:11,520 --> 00:41:14,160
complex subscribe to that table

1011
00:41:14,160 --> 00:41:17,839
and actually construct their

1012
00:41:17,839 --> 00:41:21,440
local forwarding tables from that um so

1013
00:41:21,440 --> 00:41:24,400
in a typical switch we always have two

1014
00:41:24,400 --> 00:41:25,920
we have a hardware plug-in

1015
00:41:25,920 --> 00:41:29,599
and we have also fdio uh which we use

1016
00:41:29,599 --> 00:41:33,040
for sending and receiving the l free

1017
00:41:33,040 --> 00:41:36,720
traffic so what we do here is

1018
00:41:36,720 --> 00:41:39,920
for every route object uh there is

1019
00:41:39,920 --> 00:41:43,839
a local uh api out table

1020
00:41:43,839 --> 00:41:48,000
and uh we use those api out tables

1021
00:41:48,000 --> 00:41:52,640
for doing very granular bookkeeping

1022
00:41:52,640 --> 00:41:55,839
what actually did wind to

1023
00:41:55,839 --> 00:41:59,280
let's say a third party code base so for

1024
00:41:59,280 --> 00:42:00,880
example for bpp

1025
00:42:00,880 --> 00:42:04,160
would have been all those vpp api route

1026
00:42:04,160 --> 00:42:04,880
calls

1027
00:42:04,880 --> 00:42:08,960
and also the result codes

1028
00:42:08,960 --> 00:42:12,079
similar thing here also in jericho 2

1029
00:42:12,079 --> 00:42:16,480
uh every entry in that api out table

1030
00:42:16,480 --> 00:42:19,760
actually translates into an sdk call

1031
00:42:19,760 --> 00:42:23,599
and um we store also the result from

1032
00:42:23,599 --> 00:42:25,359
that

1033
00:42:25,359 --> 00:42:28,839
that sort of setup has

1034
00:42:28,839 --> 00:42:31,839
really established

1035
00:42:31,839 --> 00:42:34,800
uh making quick progress for hardware

1036
00:42:34,800 --> 00:42:37,200
integration because you can very easily

1037
00:42:37,200 --> 00:42:41,440
troubleshoot where a certain um

1038
00:42:41,440 --> 00:42:44,960
route or next top

1039
00:42:44,960 --> 00:42:48,640
in the route flow got broken because you

1040
00:42:48,640 --> 00:42:52,319
have almost like a live log plus

1041
00:42:52,319 --> 00:42:55,200
all the history what has happened here

1042
00:42:55,200 --> 00:42:55,920
and

1043
00:42:55,920 --> 00:42:59,359
this again allows for

1044
00:42:59,359 --> 00:43:02,800
a quick troubleshooting and mailing

1045
00:43:02,800 --> 00:43:06,560
issues the concept of a central data

1046
00:43:06,560 --> 00:43:08,240
store

1047
00:43:08,240 --> 00:43:10,720
also

1048
00:43:11,440 --> 00:43:15,359
is a bit the dream of uh networking

1049
00:43:15,359 --> 00:43:17,680
operation folks

1050
00:43:17,680 --> 00:43:21,280
um one thing uh that we

1051
00:43:21,280 --> 00:43:24,960
had to fix early on is the question of

1052
00:43:24,960 --> 00:43:26,079
scaling

1053
00:43:26,079 --> 00:43:28,640
now think about uh let's say carson has

1054
00:43:28,640 --> 00:43:30,960
mentioned there is thousands of

1055
00:43:30,960 --> 00:43:34,400
modular uh based routers and if you

1056
00:43:34,400 --> 00:43:35,040
break

1057
00:43:35,040 --> 00:43:38,960
it down into a disaggregated versions

1058
00:43:38,960 --> 00:43:40,880
of one thing is certain right it's going

1059
00:43:40,880 --> 00:43:43,119
to be more than one right

1060
00:43:43,119 --> 00:43:46,480
so potentially there might be

1061
00:43:46,480 --> 00:43:49,359
you know eight nine ten thousands of

1062
00:43:49,359 --> 00:43:51,680
individual switches

1063
00:43:51,680 --> 00:43:54,720
now um how do you actually

1064
00:43:54,720 --> 00:43:57,599
monitor those switches how do you keep

1065
00:43:57,599 --> 00:43:58,280
track

1066
00:43:58,280 --> 00:44:02,400
of inter interesting parameters

1067
00:44:02,400 --> 00:44:05,040
and all of that we did not really feel

1068
00:44:05,040 --> 00:44:06,240
comfortable

1069
00:44:06,240 --> 00:44:09,520
by uh actually pulling uh

1070
00:44:09,520 --> 00:44:11,839
data from up to ten thousand switches

1071
00:44:11,839 --> 00:44:12,800
from uh

1072
00:44:12,800 --> 00:44:15,040
a centralized uh collector

1073
00:44:15,040 --> 00:44:16,319
infrastructure

1074
00:44:16,319 --> 00:44:19,680
because um um you know

1075
00:44:19,680 --> 00:44:24,079
pulling uh up to a 400 time series table

1076
00:44:24,079 --> 00:44:27,200
and retaining it for an extended amount

1077
00:44:27,200 --> 00:44:28,560
of time

1078
00:44:28,560 --> 00:44:32,880
consumes a lot of storage which

1079
00:44:32,880 --> 00:44:36,000
we didn't really put a whole lot of

1080
00:44:36,000 --> 00:44:37,359
investment in

1081
00:44:37,359 --> 00:44:41,200
so we thought hey why not

1082
00:44:41,200 --> 00:44:44,480
really uh build here uh the

1083
00:44:44,480 --> 00:44:47,040
telemetry solution as part of the switch

1084
00:44:47,040 --> 00:44:47,520
os

1085
00:44:47,520 --> 00:44:50,160
after all it is um a standard

1086
00:44:50,160 --> 00:44:52,240
ubuntu-based installation

1087
00:44:52,240 --> 00:44:55,520
so we can just take uh open source uh

1088
00:44:55,520 --> 00:44:58,319
time series database like prometrics

1089
00:44:58,319 --> 00:45:01,200
and uh we have integrated that that we

1090
00:45:01,200 --> 00:45:01,680
can do

1091
00:45:01,680 --> 00:45:06,480
high frequency samples of every scalar

1092
00:45:06,480 --> 00:45:10,160
that we have in the individual daemons

1093
00:45:10,160 --> 00:45:11,359
in the bds

1094
00:45:11,359 --> 00:45:14,800
data store and um

1095
00:45:14,800 --> 00:45:18,000
prometheus is much more than just

1096
00:45:18,000 --> 00:45:21,040
um a

1097
00:45:21,040 --> 00:45:24,240
a scraper for time series database

1098
00:45:24,240 --> 00:45:28,799
it also has a very nifty um

1099
00:45:28,960 --> 00:45:32,000
math core where you can generate where

1100
00:45:32,000 --> 00:45:32,560
you can

1101
00:45:32,560 --> 00:45:35,440
derive certain sessions so we can

1102
00:45:35,440 --> 00:45:37,119
actually on the switch

1103
00:45:37,119 --> 00:45:40,400
pre-produce things like hey uh let's

1104
00:45:40,400 --> 00:45:41,200
sample

1105
00:45:41,200 --> 00:45:44,640
uh the aggregate bandwidth on

1106
00:45:44,640 --> 00:45:48,079
all the access interfaces and divide it

1107
00:45:48,079 --> 00:45:51,599
by the number of logged-in subscribers

1108
00:45:51,599 --> 00:45:55,119
and pass that

1109
00:45:55,119 --> 00:45:58,400
time series further upstream and

1110
00:45:58,400 --> 00:46:00,839
federate so we can actually do

1111
00:46:00,839 --> 00:46:02,560
pre-aggregation

1112
00:46:02,560 --> 00:46:06,000
in uh for telemetry and

1113
00:46:06,000 --> 00:46:09,280
uh this gets me to the end

1114
00:46:09,280 --> 00:46:13,200
of um my

1115
00:46:13,200 --> 00:46:15,280
part of this talk and handle back to

1116
00:46:15,280 --> 00:46:16,720
carson

1117
00:46:16,720 --> 00:46:21,040
which will give you a summary and wrap

1118
00:46:21,040 --> 00:46:21,760
up

1119
00:46:21,760 --> 00:46:24,960
yeah thanks wrapping up we have managed

1120
00:46:24,960 --> 00:46:25,760
to

1121
00:46:25,760 --> 00:46:28,960
develop and build a disaggregation

1122
00:46:28,960 --> 00:46:32,960
disagreed route on b and c in png system

1123
00:46:32,960 --> 00:46:35,920
and got it up and running in our

1124
00:46:35,920 --> 00:46:37,040
production network

1125
00:46:37,040 --> 00:46:41,040
at the end of 2020 this aggregation of

1126
00:46:41,040 --> 00:46:42,240
network equipment

1127
00:46:42,240 --> 00:46:44,720
means that you have to deal with a lot

1128
00:46:44,720 --> 00:46:46,720
more individual components than before

1129
00:46:46,720 --> 00:46:47,119
as

1130
00:46:47,119 --> 00:46:48,780
hannah's already mentioned

1131
00:46:48,780 --> 00:46:50,480
[Music]

1132
00:46:50,480 --> 00:46:52,160
this means that you can only do this

1133
00:46:52,160 --> 00:46:53,680
using automation

1134
00:46:53,680 --> 00:46:56,640
and therefore principles that's for sure

1135
00:46:56,640 --> 00:46:58,000
however on the other side

1136
00:46:58,000 --> 00:46:59,599
splitting up hardware and software

1137
00:46:59,599 --> 00:47:02,480
allows a much faster life cycle

1138
00:47:02,480 --> 00:47:07,040
and also much faster time to market

1139
00:47:07,040 --> 00:47:11,359
thanks for your attention

1140
00:47:11,359 --> 00:47:13,520
hello thank you carson and hannes again

1141
00:47:13,520 --> 00:47:15,359
um we have a few questions for you and

1142
00:47:15,359 --> 00:47:16,720
it looks like you have been

1143
00:47:16,720 --> 00:47:19,520
active in the q a answering questions as

1144
00:47:19,520 --> 00:47:20,319
we go

1145
00:47:20,319 --> 00:47:23,520
um so let me see what questions are

1146
00:47:23,520 --> 00:47:25,520
there questions here that were not that

1147
00:47:25,520 --> 00:47:29,119
you haven't answered in the chat um

1148
00:47:29,119 --> 00:47:30,720
or if there's if there's any questions

1149
00:47:30,720 --> 00:47:32,400
in here that you'd like to

1150
00:47:32,400 --> 00:47:35,760
elaborate on uh i see a question about

1151
00:47:35,760 --> 00:47:38,880
using oem vendors sorry

1152
00:47:38,880 --> 00:47:41,200
did you use the oem vendor's routing

1153
00:47:41,200 --> 00:47:43,040
stack on those fabric switches or

1154
00:47:43,040 --> 00:47:46,400
others yep we don't

1155
00:47:46,400 --> 00:47:50,000
so basically the only component software

1156
00:47:50,000 --> 00:47:52,079
component from the om vendor we are

1157
00:47:52,079 --> 00:47:53,599
using is

1158
00:47:53,599 --> 00:47:57,200
the only the only i mean this is an open

1159
00:47:57,200 --> 00:47:58,880
source project but

1160
00:47:58,880 --> 00:48:02,160
normally the vendor has to do some

1161
00:48:02,160 --> 00:48:05,680
slight adjustments to it but that's the

1162
00:48:05,680 --> 00:48:06,000
only

1163
00:48:06,000 --> 00:48:09,440
component from the om vendor we're using

1164
00:48:09,440 --> 00:48:12,800
okay um question

1165
00:48:12,800 --> 00:48:15,359
are you multi-homing your release looks

1166
00:48:15,359 --> 00:48:16,559
like you've answered that in the chat

1167
00:48:16,559 --> 00:48:19,440
but if you'd like to outline

1168
00:48:19,440 --> 00:48:21,760
uh yes we're using equal cost multiplier

1169
00:48:21,760 --> 00:48:22,880
of fruiting so

1170
00:48:22,880 --> 00:48:25,200
we are multi-homing the leaves too

1171
00:48:25,200 --> 00:48:27,200
equally to all the spine switches

1172
00:48:27,200 --> 00:48:28,800
in our case at the moment there's two

1173
00:48:28,800 --> 00:48:30,319
spine switches but

1174
00:48:30,319 --> 00:48:34,480
yeah it's it's equally and a follow-up

1175
00:48:34,480 --> 00:48:35,200
question

1176
00:48:35,200 --> 00:48:37,440
uh is i was more interested in

1177
00:48:37,440 --> 00:48:38,880
multi-home evpn

1178
00:48:38,880 --> 00:48:41,280
it's a single device connection to a

1179
00:48:41,280 --> 00:48:44,160
port over evpn

1180
00:48:44,160 --> 00:48:47,680
or is there multiple devices in the uvpn

1181
00:48:47,680 --> 00:48:49,760
so for the time being we do not have any

1182
00:48:49,760 --> 00:48:50,800
redundancy

1183
00:48:50,800 --> 00:48:53,440
in the access so usually a subscriber

1184
00:48:53,440 --> 00:48:54,319
meaning

1185
00:48:54,319 --> 00:48:57,680
uh being connected on olt or amazon

1186
00:48:57,680 --> 00:49:00,640
is only single home to uh to one leaf

1187
00:49:00,640 --> 00:49:02,400
switch at the moment so we are thinking

1188
00:49:02,400 --> 00:49:03,119
about

1189
00:49:03,119 --> 00:49:04,480
doing redundancy for business

1190
00:49:04,480 --> 00:49:06,559
subscribers in the future but as of now

1191
00:49:06,559 --> 00:49:09,760
they are not multi-home okay

1192
00:49:09,760 --> 00:49:12,960
uh there's a question about um about

1193
00:49:12,960 --> 00:49:15,920
software versions i wonder from jordan's

1194
00:49:15,920 --> 00:49:17,599
tonneau i wonder how you ensure the

1195
00:49:17,599 --> 00:49:18,800
software version

1196
00:49:18,800 --> 00:49:20,960
of each component of the stack is up to

1197
00:49:20,960 --> 00:49:22,559
date are you using an open source

1198
00:49:22,559 --> 00:49:23,760
solution

1199
00:49:23,760 --> 00:49:26,240
to maintain software components or a

1200
00:49:26,240 --> 00:49:27,920
commercial tool from a vendor or

1201
00:49:27,920 --> 00:49:31,040
various vendors um

1202
00:49:31,040 --> 00:49:34,480
yeah the point is as i explained we we

1203
00:49:34,480 --> 00:49:35,599
do have a

1204
00:49:35,599 --> 00:49:37,119
central component running on a

1205
00:49:37,119 --> 00:49:38,800
kubernetes cluster

1206
00:49:38,800 --> 00:49:42,319
within the pod which we call power

1207
00:49:42,319 --> 00:49:45,520
port access orchestrator and that

1208
00:49:45,520 --> 00:49:47,359
component keeps track of

1209
00:49:47,359 --> 00:49:49,280
all the software versions and if

1210
00:49:49,280 --> 00:49:50,480
necessary

1211
00:49:50,480 --> 00:49:53,200
initiates the rollout of new software

1212
00:49:53,200 --> 00:49:53,680
and

1213
00:49:53,680 --> 00:49:55,599
this part of software is developed by

1214
00:49:55,599 --> 00:49:57,040
dodger telecom

1215
00:49:57,040 --> 00:49:59,280
itself so this is not an open source

1216
00:49:59,280 --> 00:50:01,200
project right now

1217
00:50:01,200 --> 00:50:04,319
okay okay uh looks like there's one more

1218
00:50:04,319 --> 00:50:06,880
question from at ptac uh if you have

1219
00:50:06,880 --> 00:50:08,640
prometheus running on each device to

1220
00:50:08,640 --> 00:50:10,319
collect and graph the stats

1221
00:50:10,319 --> 00:50:12,160
do you have a system for pulling that

1222
00:50:12,160 --> 00:50:14,640
data from the individual switches

1223
00:50:14,640 --> 00:50:17,040
back into a central dashboard or my

1224
00:50:17,040 --> 00:50:17,839
dashboard

1225
00:50:17,839 --> 00:50:20,319
monitoring system um maybe i can take

1226
00:50:20,319 --> 00:50:21,280
that um

1227
00:50:21,280 --> 00:50:24,559
basically uh the idea is that

1228
00:50:24,559 --> 00:50:27,599
the high resolution data um actually

1229
00:50:27,599 --> 00:50:30,480
stays on the switch right uh but there

1230
00:50:30,480 --> 00:50:32,480
is actually a centralized

1231
00:50:32,480 --> 00:50:35,839
uh uh instance of prometoise uh which

1232
00:50:35,839 --> 00:50:38,880
actually takes uh the sample down

1233
00:50:38,880 --> 00:50:41,920
a version of those time series

1234
00:50:41,920 --> 00:50:44,079
such that you have got some scaling

1235
00:50:44,079 --> 00:50:45,839
benefits

1236
00:50:45,839 --> 00:50:50,000
okay great um another question just came

1237
00:50:50,000 --> 00:50:50,480
in

1238
00:50:50,480 --> 00:50:53,040
from boris kasanov do you use any kind

1239
00:50:53,040 --> 00:50:53,760
of te

1240
00:50:53,760 --> 00:50:56,000
between the fabric and the backbone such

1241
00:50:56,000 --> 00:50:58,880
as srt

1242
00:50:59,359 --> 00:51:01,520
uh as much as we would love the

1243
00:51:01,520 --> 00:51:02,800
technology right

1244
00:51:02,800 --> 00:51:06,960
uh uh actually uh just adding bandwidth

1245
00:51:06,960 --> 00:51:10,000
uh is uh the easier thing

1246
00:51:10,000 --> 00:51:12,880
and uh essentially within the fabric uh

1247
00:51:12,880 --> 00:51:13,520
bandwidth

1248
00:51:13,520 --> 00:51:17,040
almost comes for free right uh so um no

1249
00:51:17,040 --> 00:51:20,960
right now we don't use te there was a

1250
00:51:20,960 --> 00:51:23,680
old multi-homing was that an answer okay

1251
00:51:23,680 --> 00:51:25,040
that was your answer

1252
00:51:25,040 --> 00:51:29,359
to a question about p-i-m-s-s-m

1253
00:51:29,359 --> 00:51:33,440
for p-i-m do you use p-i-m-s-s-m

1254
00:51:33,440 --> 00:51:36,720
by just using specific materials

1255
00:51:36,720 --> 00:51:40,160
right okay

1256
00:51:40,480 --> 00:51:44,000
okay um if there are

1257
00:51:44,000 --> 00:51:45,599
are there first off are there any other

1258
00:51:45,599 --> 00:51:48,000
questions

1259
00:51:49,520 --> 00:51:51,839
and

1260
00:51:52,720 --> 00:51:55,040
a few seconds i don't see any more

1261
00:51:55,040 --> 00:51:56,319
questions in the queue

1262
00:51:56,319 --> 00:51:58,640
actually hold off i just saw a question

1263
00:51:58,640 --> 00:51:59,440
pop up

1264
00:51:59,440 --> 00:52:02,720
uh from sanjay kumar pangong i hope i

1265
00:52:02,720 --> 00:52:05,280
got your pronunciation properly

1266
00:52:05,280 --> 00:52:07,920
are you using vng and router software

1267
00:52:07,920 --> 00:52:09,760
development by different vendors

1268
00:52:09,760 --> 00:52:12,960
or are you using something else uh i

1269
00:52:12,960 --> 00:52:14,720
didn't get the questions

1270
00:52:14,720 --> 00:52:19,280
completely yes what are the png software

1271
00:52:19,280 --> 00:52:19,760
and

1272
00:52:19,760 --> 00:52:21,599
the routing software is from different

1273
00:52:21,599 --> 00:52:22,880
vendors

1274
00:52:22,880 --> 00:52:25,520
no not at the moment so right now we are

1275
00:52:25,520 --> 00:52:26,800
using the

1276
00:52:26,800 --> 00:52:29,839
the software from rtb breaker excellent

1277
00:52:29,839 --> 00:52:32,240
uh we are out of time but uh thank you

1278
00:52:32,240 --> 00:52:34,240
very much for your presentation and for

1279
00:52:34,240 --> 00:52:43,839
your time answering questions

