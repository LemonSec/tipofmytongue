1
00:00:00,000 --> 00:00:00,060
[Music]

2
00:00:00,060 --> 00:00:02,210
foreign

3
00:00:02,210 --> 00:00:05,460
[Music]

4
00:00:05,460 --> 00:00:07,980
thank you all for coming

5
00:00:07,980 --> 00:00:11,940
um our next speaker Christoph Posh is a

6
00:00:11,940 --> 00:00:14,340
networking architect at Apple and first

7
00:00:14,340 --> 00:00:17,340
presented here at nanog stage in 2016.

8
00:00:17,340 --> 00:00:19,380
he's been working on transport layer

9
00:00:19,380 --> 00:00:22,740
networking since 2010 and has recently

10
00:00:22,740 --> 00:00:24,180
shifted his Focus to improving the

11
00:00:24,180 --> 00:00:25,980
network properties for end user

12
00:00:25,980 --> 00:00:28,680
experience by exposing measurement tools

13
00:00:28,680 --> 00:00:31,099
leading to today's presentation

14
00:00:31,099 --> 00:00:34,200
responsiveness under working conditions

15
00:00:34,200 --> 00:00:36,480
we're happy to welcome Kristoff back to

16
00:00:36,480 --> 00:00:38,100
the stage for his second nanog

17
00:00:38,100 --> 00:00:40,480
presentation

18
00:00:40,480 --> 00:00:42,960
[Applause]

19
00:00:42,960 --> 00:00:45,540
here we go okay all right

20
00:00:45,540 --> 00:00:47,820
thank you hello everyone

21
00:00:47,820 --> 00:00:49,559
thanks for coming

22
00:00:49,559 --> 00:00:50,640
um thanks for coming back from the

23
00:00:50,640 --> 00:00:52,680
coffee break

24
00:00:52,680 --> 00:00:55,320
um I'm Kristoff I'm an engineer at Apple

25
00:00:55,320 --> 00:00:58,379
I'm here with my two colleagues Randall

26
00:00:58,379 --> 00:01:00,180
and Stuart who are right here in the

27
00:01:00,180 --> 00:01:02,460
audience as well we're here so after the

28
00:01:02,460 --> 00:01:04,140
talk you can come and discuss with us

29
00:01:04,140 --> 00:01:06,119
about what we presented

30
00:01:06,119 --> 00:01:09,299
so um about two years ago we started

31
00:01:09,299 --> 00:01:11,640
this project working on what we call now

32
00:01:11,640 --> 00:01:13,560
responsiveness under working conditions

33
00:01:13,560 --> 00:01:16,140
but before I get started I want to kick

34
00:01:16,140 --> 00:01:19,140
off with a little survey here

35
00:01:19,140 --> 00:01:21,720
so over the past two years we have all

36
00:01:21,720 --> 00:01:23,340
been in this pandemic and we have been

37
00:01:23,340 --> 00:01:25,799
forced to work from home

38
00:01:25,799 --> 00:01:27,780
spanning our days on video conferencing

39
00:01:27,780 --> 00:01:30,180
right so

40
00:01:30,180 --> 00:01:33,180
please raise your hands if during this

41
00:01:33,180 --> 00:01:35,640
time you experience problems with the

42
00:01:35,640 --> 00:01:38,479
video conferencing

43
00:01:38,579 --> 00:01:41,100
almost everyone I guess the ones who

44
00:01:41,100 --> 00:01:42,479
don't lift their hands keep their hands

45
00:01:42,479 --> 00:01:45,840
up please keep their hands up so yes so

46
00:01:45,840 --> 00:01:48,180
almost everyone right so who upgraded

47
00:01:48,180 --> 00:01:51,240
their internet service during this time

48
00:01:51,240 --> 00:01:54,360
to a better or faster internet right

49
00:01:54,360 --> 00:01:58,020
yeah quite a few as well so for whom did

50
00:01:58,020 --> 00:01:59,640
all the video conferencing issues go

51
00:01:59,640 --> 00:02:01,759
away

52
00:02:02,340 --> 00:02:05,159
uh you for you they all went away okay

53
00:02:05,159 --> 00:02:07,259
well we'll have to talk to you later how

54
00:02:07,259 --> 00:02:10,560
do you did that but so right so here we

55
00:02:10,560 --> 00:02:12,480
are right we

56
00:02:12,480 --> 00:02:14,480
have serious problems with our internet

57
00:02:14,480 --> 00:02:18,319
we pay more money to get better internet

58
00:02:18,319 --> 00:02:21,480
and still we have problems with our

59
00:02:21,480 --> 00:02:25,620
internet so something is wrong right

60
00:02:25,620 --> 00:02:28,560
and we should do better and we people in

61
00:02:28,560 --> 00:02:30,480
the room we are able to do better

62
00:02:30,480 --> 00:02:31,800
so

63
00:02:31,800 --> 00:02:34,140
I will show you what we have been

64
00:02:34,140 --> 00:02:37,260
ignoring for the past decades when we

65
00:02:37,260 --> 00:02:38,760
are measuring performance on the

66
00:02:38,760 --> 00:02:39,900
internet

67
00:02:39,900 --> 00:02:42,239
we call what we have been ignoring is

68
00:02:42,239 --> 00:02:45,000
responsiveness under working conditions

69
00:02:45,000 --> 00:02:47,519
and I will show you what you can do to

70
00:02:47,519 --> 00:02:49,319
improve the responsiveness for your

71
00:02:49,319 --> 00:02:51,720
networks and your devices

72
00:02:51,720 --> 00:02:52,980
but first

73
00:02:52,980 --> 00:02:55,140
let me take a step back and let's think

74
00:02:55,140 --> 00:02:58,099
about what is actually good internet

75
00:02:58,099 --> 00:03:02,519
well it's all about the throughput right

76
00:03:02,519 --> 00:03:04,920
after all there are all these tools

77
00:03:04,920 --> 00:03:06,840
available to us to measure throughput we

78
00:03:06,840 --> 00:03:09,120
have iperf right that's basically our

79
00:03:09,120 --> 00:03:10,980
basic tool for measuring how good our

80
00:03:10,980 --> 00:03:13,739
network is we use ptest tools to see the

81
00:03:13,739 --> 00:03:16,739
capacity of our Network right whenever

82
00:03:16,739 --> 00:03:18,900
we buy internet we choose between a

83
00:03:18,900 --> 00:03:21,000
capacity plan we choose either 100

84
00:03:21,000 --> 00:03:23,280
megabits per second 600 megabits per

85
00:03:23,280 --> 00:03:24,959
seconds maybe a gig

86
00:03:24,959 --> 00:03:27,540
when we buy a new phone that has 5G

87
00:03:27,540 --> 00:03:29,879
enabled the first thing we do we measure

88
00:03:29,879 --> 00:03:32,700
the capacity how fast how fast can the

89
00:03:32,700 --> 00:03:34,560
phone transmit data

90
00:03:34,560 --> 00:03:36,420
and after all at the beginning of the

91
00:03:36,420 --> 00:03:39,480
internet uh 56 kilobits per seconds that

92
00:03:39,480 --> 00:03:42,659
was really not enough and so we upgraded

93
00:03:42,659 --> 00:03:45,060
to megabits per seconds with DSL and

94
00:03:45,060 --> 00:03:47,760
today everybody has gigabits per seconds

95
00:03:47,760 --> 00:03:51,599
with fiber to the home and 5G so there's

96
00:03:51,599 --> 00:03:54,060
a lot of focus on throughput

97
00:03:54,060 --> 00:03:56,640
but we believe

98
00:03:56,640 --> 00:03:58,680
is actually not all about throughput

99
00:03:58,680 --> 00:04:02,220
sure in the early days it was when 56

100
00:04:02,220 --> 00:04:04,440
kilobits were really not enough

101
00:04:04,440 --> 00:04:07,799
but today everybody has easy access to

102
00:04:07,799 --> 00:04:10,019
100 megabits per seconds at least and

103
00:04:10,019 --> 00:04:12,360
100 megabits per seconds is hugely

104
00:04:12,360 --> 00:04:14,900
enough for any 4K video streaming right

105
00:04:14,900 --> 00:04:17,100
so we believe that there are other

106
00:04:17,100 --> 00:04:19,079
factors that are important for good

107
00:04:19,079 --> 00:04:20,579
internet quality

108
00:04:20,579 --> 00:04:23,759
namely latency and Jitter and we are

109
00:04:23,759 --> 00:04:25,860
used to measuring latency and Jitter

110
00:04:25,860 --> 00:04:28,979
other improve other other factors like

111
00:04:28,979 --> 00:04:31,440
protocol conformance there are

112
00:04:31,440 --> 00:04:33,840
standardization bodies like the ITF they

113
00:04:33,840 --> 00:04:37,440
are working hard to create new protocols

114
00:04:37,440 --> 00:04:39,660
that improve the performance that are

115
00:04:39,660 --> 00:04:42,479
more liable that have better security

116
00:04:42,479 --> 00:04:45,000
and privacy properties and we need the

117
00:04:45,000 --> 00:04:46,800
Internet and the networks to support

118
00:04:46,800 --> 00:04:50,940
these these protocols and finally yes it

119
00:04:50,940 --> 00:04:53,580
also is about throughput right

120
00:04:53,580 --> 00:04:55,500
but so

121
00:04:55,500 --> 00:04:57,720
we have been measuring latency for a

122
00:04:57,720 --> 00:05:00,180
long time we all know how to use icmp

123
00:05:00,180 --> 00:05:03,720
ping and we are very used to use this as

124
00:05:03,720 --> 00:05:05,220
a measurement tool to qualify our

125
00:05:05,220 --> 00:05:07,860
Networks but what is wrong with those

126
00:05:07,860 --> 00:05:10,199
kind of traditional latency measurements

127
00:05:10,199 --> 00:05:13,979
well first point is that when I as a

128
00:05:13,979 --> 00:05:16,560
user I'm browsing the web I'm not

129
00:05:16,560 --> 00:05:19,139
browsing the web using icmp ping right

130
00:05:19,139 --> 00:05:23,039
I'm browsing the web using HTTP TCP

131
00:05:23,039 --> 00:05:26,699
quick and so on nobody transmits any

132
00:05:26,699 --> 00:05:30,479
useful data over icmp pay also in the

133
00:05:30,479 --> 00:05:32,100
routers and the network equipments

134
00:05:32,100 --> 00:05:34,139
usually icmp ping gets a different

135
00:05:34,139 --> 00:05:36,060
treatment it may use a different

136
00:05:36,060 --> 00:05:38,520
priority queue it may use the control

137
00:05:38,520 --> 00:05:41,639
plane not the data plane right but the

138
00:05:41,639 --> 00:05:43,680
traffic that the users are using TCP

139
00:05:43,680 --> 00:05:47,699
that uses the data plan okay so this use

140
00:05:47,699 --> 00:05:49,560
of an artificial protocol to measure

141
00:05:49,560 --> 00:05:51,360
latency is just wrong

142
00:05:51,360 --> 00:05:54,479
right and the second point is very often

143
00:05:54,479 --> 00:05:57,360
when we measure latency we do it when a

144
00:05:57,360 --> 00:06:00,180
network is Idle right we do it at the

145
00:06:00,180 --> 00:06:03,360
beginning before we measure capacity but

146
00:06:03,360 --> 00:06:06,060
I don't want my network to as low

147
00:06:06,060 --> 00:06:08,340
latency when it's idle I want my network

148
00:06:08,340 --> 00:06:09,900
to have low latency when I'm actually

149
00:06:09,900 --> 00:06:14,220
using it right so we believe that this

150
00:06:14,220 --> 00:06:16,860
use of those unreal conditions leads to

151
00:06:16,860 --> 00:06:19,979
a wrong impression of latency we believe

152
00:06:19,979 --> 00:06:21,900
that there's a need for a more realistic

153
00:06:21,900 --> 00:06:26,299
latency and responsiveness measurement

154
00:06:26,699 --> 00:06:29,520
so back to my little graph that shows

155
00:06:29,520 --> 00:06:31,919
what is what what means good internet

156
00:06:31,919 --> 00:06:34,020
well we believe we need to add

157
00:06:34,020 --> 00:06:36,000
responsiveness under working conditions

158
00:06:36,000 --> 00:06:38,880
into this and this talk is about what is

159
00:06:38,880 --> 00:06:40,560
responsiveness under working conditions

160
00:06:40,560 --> 00:06:43,860
and what you can do about it

161
00:06:43,860 --> 00:06:45,539
so let me explain you what is

162
00:06:45,539 --> 00:06:47,039
responsiveness under working conditions

163
00:06:47,039 --> 00:06:48,000
and there are a few measurement

164
00:06:48,000 --> 00:06:51,479
principles we we follow when measuring

165
00:06:51,479 --> 00:06:55,380
responsiveness under working conditions

166
00:06:55,380 --> 00:06:58,440
first of all I mentioned earlier this

167
00:06:58,440 --> 00:07:00,840
use of artificial protocols

168
00:07:00,840 --> 00:07:03,900
so when we measure responsiveness we use

169
00:07:03,900 --> 00:07:05,639
the most common protocols which means

170
00:07:05,639 --> 00:07:07,199
http

171
00:07:07,199 --> 00:07:09,840
almost all of the internet's traffic

172
00:07:09,840 --> 00:07:12,900
today is running on top of HTTP it's not

173
00:07:12,900 --> 00:07:15,360
only used for web browsing it is used

174
00:07:15,360 --> 00:07:17,280
inside applications when they are

175
00:07:17,280 --> 00:07:18,840
fetching any kind of content from the

176
00:07:18,840 --> 00:07:22,560
internet everything today uses http

177
00:07:22,560 --> 00:07:25,139
so it's important that we use these kind

178
00:07:25,139 --> 00:07:28,380
of protocols when measure it

179
00:07:28,380 --> 00:07:31,020
second we create what we call working

180
00:07:31,020 --> 00:07:32,819
conditions this is the kind of

181
00:07:32,819 --> 00:07:34,680
conditions that you are experiencing

182
00:07:34,680 --> 00:07:36,360
when you're actively using the network

183
00:07:36,360 --> 00:07:39,000
what is a working condition well if you

184
00:07:39,000 --> 00:07:40,880
receive an email with a few attachments

185
00:07:40,880 --> 00:07:43,380
if you're an engineer you're downloading

186
00:07:43,380 --> 00:07:47,460
an SDK from from the web if your

187
00:07:47,460 --> 00:07:49,680
designer downloading or uploading the

188
00:07:49,680 --> 00:07:52,380
images that you created right even when

189
00:07:52,380 --> 00:07:53,819
your kids are watching a movie on

190
00:07:53,819 --> 00:07:56,220
Netflix all of this puts the network

191
00:07:56,220 --> 00:07:58,740
into working conditions and that's when

192
00:07:58,740 --> 00:08:00,419
you want low latency

193
00:08:00,419 --> 00:08:03,240
now putting the network into working

194
00:08:03,240 --> 00:08:05,520
conditions what what effect does it have

195
00:08:05,520 --> 00:08:08,160
on the measurement it means that you are

196
00:08:08,160 --> 00:08:09,960
measuring the Network's ability to

197
00:08:09,960 --> 00:08:12,960
multitask it is is the network able to

198
00:08:12,960 --> 00:08:16,199
provide high throughput for the movie

199
00:08:16,199 --> 00:08:18,419
that your kids are watching while at the

200
00:08:18,419 --> 00:08:20,879
same time providing low latency for the

201
00:08:20,879 --> 00:08:22,560
web browsing that you are doing or the

202
00:08:22,560 --> 00:08:24,960
video conferencing that you are doing so

203
00:08:24,960 --> 00:08:27,660
it's about multitasking in the network

204
00:08:27,660 --> 00:08:31,319
but it's also about agility right if you

205
00:08:31,319 --> 00:08:32,640
are watching a movie

206
00:08:32,640 --> 00:08:35,039
you want to be able to skip forward in

207
00:08:35,039 --> 00:08:37,380
the movie and the network to immediately

208
00:08:37,380 --> 00:08:40,380
give you the the updated video frame

209
00:08:40,380 --> 00:08:43,020
right is the network agile enough to

210
00:08:43,020 --> 00:08:45,959
react quickly to the user input

211
00:08:45,959 --> 00:08:47,519
so those are the two measurement

212
00:08:47,519 --> 00:08:50,760
principles it is using common protocols

213
00:08:50,760 --> 00:08:53,279
and working conditions right

214
00:08:53,279 --> 00:08:54,480
so

215
00:08:54,480 --> 00:08:56,279
let's dig a bit more into the working

216
00:08:56,279 --> 00:08:58,080
conditions

217
00:08:58,080 --> 00:08:59,399
what is the goal behind to working

218
00:08:59,399 --> 00:09:01,860
conditions first of all we want to push

219
00:09:01,860 --> 00:09:04,200
the network to its capacity limit right

220
00:09:04,200 --> 00:09:06,060
so that it's fully being used

221
00:09:06,060 --> 00:09:09,180
any file transfer any email all of these

222
00:09:09,180 --> 00:09:11,880
kind of transactions are you are using

223
00:09:11,880 --> 00:09:13,860
the network at its full capacity so we

224
00:09:13,860 --> 00:09:15,720
want to push the network now how could

225
00:09:15,720 --> 00:09:17,279
we do this well we could just flood the

226
00:09:17,279 --> 00:09:19,740
network as UDP traffic right but the

227
00:09:19,740 --> 00:09:20,940
problem with that is it's not very

228
00:09:20,940 --> 00:09:23,100
realistic nobody in real life is doing

229
00:09:23,100 --> 00:09:25,740
that so the way to create those working

230
00:09:25,740 --> 00:09:28,860
conditions is by creating multiple bulk

231
00:09:28,860 --> 00:09:32,100
data transfers using HTTP using standard

232
00:09:32,100 --> 00:09:34,440
congestion controls and so they push the

233
00:09:34,440 --> 00:09:36,240
network to the capacity limit while

234
00:09:36,240 --> 00:09:38,820
remaining realistic so it's a realistic

235
00:09:38,820 --> 00:09:41,880
use case like for example transmitting

236
00:09:41,880 --> 00:09:43,920
some files sending a message with a

237
00:09:43,920 --> 00:09:47,339
photo attachment and so on and again we

238
00:09:47,339 --> 00:09:49,860
use the common protocols like HTTP for

239
00:09:49,860 --> 00:09:52,320
this kind of traffic generation

240
00:09:52,320 --> 00:09:53,820
so you have to imagine in this

241
00:09:53,820 --> 00:09:55,680
measurement methodology that we are

242
00:09:55,680 --> 00:09:58,500
creating this working condition first

243
00:09:58,500 --> 00:10:01,440
and then we measure the latency

244
00:10:01,440 --> 00:10:03,660
now during this latency measurement we

245
00:10:03,660 --> 00:10:05,399
measurement it in two different user

246
00:10:05,399 --> 00:10:08,100
representative scenarios

247
00:10:08,100 --> 00:10:10,980
we send HTTP get requests for a smaller

248
00:10:10,980 --> 00:10:12,959
object so again we are using a very

249
00:10:12,959 --> 00:10:15,720
common protocol HTTP that's the same

250
00:10:15,720 --> 00:10:17,580
scenario that you would use when you are

251
00:10:17,580 --> 00:10:19,620
visiting a website

252
00:10:19,620 --> 00:10:21,540
and we measure it first on separate

253
00:10:21,540 --> 00:10:23,640
connections right we have those load

254
00:10:23,640 --> 00:10:24,959
generating connections and then we have

255
00:10:24,959 --> 00:10:27,240
the separate connections and they are

256
00:10:27,240 --> 00:10:28,920
measuring the latency sending those get

257
00:10:28,920 --> 00:10:30,899
requests

258
00:10:30,899 --> 00:10:32,700
now that allows to measure the Network's

259
00:10:32,700 --> 00:10:34,920
ability to multitask

260
00:10:34,920 --> 00:10:36,600
but then we are also sending get

261
00:10:36,600 --> 00:10:38,760
requests on those connections that are

262
00:10:38,760 --> 00:10:40,920
generating the load making sure that the

263
00:10:40,920 --> 00:10:43,140
network is able to be agile and quickly

264
00:10:43,140 --> 00:10:46,399
react to user input

265
00:10:46,680 --> 00:10:48,540
as we are measuring this we collect the

266
00:10:48,540 --> 00:10:50,880
HTTP request response times which then

267
00:10:50,880 --> 00:10:53,880
is the latency end to end

268
00:10:53,880 --> 00:10:56,339
so we created this measurement

269
00:10:56,339 --> 00:10:57,660
methodology

270
00:10:57,660 --> 00:11:00,899
and we started implementing a tool in

271
00:11:00,899 --> 00:11:03,480
Mac OS available as a common line tool

272
00:11:03,480 --> 00:11:06,300
in a called Network quality for those of

273
00:11:06,300 --> 00:11:07,800
you who have a recent Mac OS built here

274
00:11:07,800 --> 00:11:09,959
who you can use it

275
00:11:09,959 --> 00:11:11,820
but then we also standardize this

276
00:11:11,820 --> 00:11:13,800
methodology and we reached out to the

277
00:11:13,800 --> 00:11:16,440
community there's now for those of you

278
00:11:16,440 --> 00:11:18,660
who are running Windows or Linux there's

279
00:11:18,660 --> 00:11:21,060
an open source tool available based on

280
00:11:21,060 --> 00:11:24,600
the golang environment so you can you

281
00:11:24,600 --> 00:11:26,579
can run a responsiveness test on any

282
00:11:26,579 --> 00:11:28,800
kind of platform nowadays

283
00:11:28,800 --> 00:11:31,260
even I perform 2 now created a

284
00:11:31,260 --> 00:11:32,700
responsiveness measurement with the

285
00:11:32,700 --> 00:11:34,620
bounce back option so for those of you

286
00:11:34,620 --> 00:11:36,240
who have come who are used to using

287
00:11:36,240 --> 00:11:38,519
latest iperf2 you can use the bounce

288
00:11:38,519 --> 00:11:41,220
back option to measure responsiveness

289
00:11:41,220 --> 00:11:44,279
and finally the uclasp test app also

290
00:11:44,279 --> 00:11:47,339
adopted now the responsive measurement

291
00:11:47,339 --> 00:11:49,380
so I invite everybody here and here in

292
00:11:49,380 --> 00:11:51,300
this room who has a smartphone with the

293
00:11:51,300 --> 00:11:53,579
Ookla speed test tab installed or

294
00:11:53,579 --> 00:11:55,760
whoever is on his laptop go to

295
00:11:55,760 --> 00:11:58,380
speedtest.net and do a speed test right

296
00:11:58,380 --> 00:12:00,899
now and see how is the responsiveness

297
00:12:00,899 --> 00:12:02,700
here at nanak

298
00:12:02,700 --> 00:12:05,339
I tried it earlier this morning I had

299
00:12:05,339 --> 00:12:07,500
more than one second of latency at the

300
00:12:07,500 --> 00:12:09,360
on the downlink

301
00:12:09,360 --> 00:12:11,940
um and so I invite you all to try it out

302
00:12:11,940 --> 00:12:14,940
right now and you'll can experience it

303
00:12:14,940 --> 00:12:18,380
for yourself in the speed test app

304
00:12:18,420 --> 00:12:20,459
so you see this little screenshot here

305
00:12:20,459 --> 00:12:22,019
right from the old class speed test and

306
00:12:22,019 --> 00:12:23,399
I want to zoom into this a little bit

307
00:12:23,399 --> 00:12:24,959
more so

308
00:12:24,959 --> 00:12:26,760
on the left side you see the

309
00:12:26,760 --> 00:12:28,920
responsiveness the idle latency six

310
00:12:28,920 --> 00:12:31,500
milliseconds perfect you can do a video

311
00:12:31,500 --> 00:12:34,079
call at six milliseconds per late so six

312
00:12:34,079 --> 00:12:35,820
milliseconds latency

313
00:12:35,820 --> 00:12:37,800
then in the middle you see the downlink

314
00:12:37,800 --> 00:12:40,920
latency it is three seconds

315
00:12:40,920 --> 00:12:43,380
I guess you we all agree that three

316
00:12:43,380 --> 00:12:46,019
seconds of latency is not good right

317
00:12:46,019 --> 00:12:48,839
imagine you're on a video call and

318
00:12:48,839 --> 00:12:51,120
suddenly your latency jumps to three

319
00:12:51,120 --> 00:12:53,639
seconds this video call Will no more

320
00:12:53,639 --> 00:12:55,500
work if you're playing a game

321
00:12:55,500 --> 00:12:57,660
it will just be a very bad experience if

322
00:12:57,660 --> 00:12:59,100
you're playing an online shooter game

323
00:12:59,100 --> 00:13:01,380
you will just die right away

324
00:13:01,380 --> 00:13:03,420
so I guess we all agree that three

325
00:13:03,420 --> 00:13:04,980
seconds is not good enough and we need

326
00:13:04,980 --> 00:13:07,200
to we need to do something about that

327
00:13:07,200 --> 00:13:08,760
but here's something more about this

328
00:13:08,760 --> 00:13:09,959
screenshot here

329
00:13:09,959 --> 00:13:12,779
it is that UCLA speed test it's we all

330
00:13:12,779 --> 00:13:15,420
use it everybody use it our end users

331
00:13:15,420 --> 00:13:17,399
are using it

332
00:13:17,399 --> 00:13:19,500
and they will start using speed tests

333
00:13:19,500 --> 00:13:21,240
and see those numbers and start

334
00:13:21,240 --> 00:13:23,100
comparing networks with each other

335
00:13:23,100 --> 00:13:26,160
devices with each other right

336
00:13:26,160 --> 00:13:27,660
so we'll start complaining and ask

337
00:13:27,660 --> 00:13:30,240
questions so we need to be ready to

338
00:13:30,240 --> 00:13:31,740
answer those questions and make sure

339
00:13:31,740 --> 00:13:34,139
that our networks will show good u-class

340
00:13:34,139 --> 00:13:36,360
speed test numbers right

341
00:13:36,360 --> 00:13:39,420
so let me show you what we can do on the

342
00:13:39,420 --> 00:13:41,279
network side to improve the

343
00:13:41,279 --> 00:13:43,680
responsiveness

344
00:13:43,680 --> 00:13:45,899
and before I get into this we first need

345
00:13:45,899 --> 00:13:47,220
to understand a little bit more about

346
00:13:47,220 --> 00:13:48,779
what does responsiveness actually

347
00:13:48,779 --> 00:13:50,880
measure what does it create those

348
00:13:50,880 --> 00:13:53,160
working conditions and imagine you have

349
00:13:53,160 --> 00:13:55,320
this data path from the phone on the

350
00:13:55,320 --> 00:13:56,579
left side and the server on the right

351
00:13:56,579 --> 00:13:57,300
side

352
00:13:57,300 --> 00:13:59,339
traffic will go

353
00:13:59,339 --> 00:14:01,980
through the internet

354
00:14:01,980 --> 00:14:04,040
to the ISP

355
00:14:04,040 --> 00:14:07,500
at The Last Mile there will be a cmts or

356
00:14:07,500 --> 00:14:09,060
d-slam

357
00:14:09,060 --> 00:14:10,920
if it's a cable access they will be in

358
00:14:10,920 --> 00:14:13,139
the home there will be a cable modem

359
00:14:13,139 --> 00:14:14,880
after that it will be a Wi-Fi access

360
00:14:14,880 --> 00:14:17,940
point and along this data path there

361
00:14:17,940 --> 00:14:20,279
will be a bottleneck something some

362
00:14:20,279 --> 00:14:22,800
element is limiting the capacity it

363
00:14:22,800 --> 00:14:25,260
could be the ISP if you have subscribed

364
00:14:25,260 --> 00:14:27,779
to 100 megabit per second service or it

365
00:14:27,779 --> 00:14:29,519
could be the Wi-Fi access point if you

366
00:14:29,519 --> 00:14:30,959
are far away from your Wi-Fi access

367
00:14:30,959 --> 00:14:33,959
point and the signal quality is bad

368
00:14:33,959 --> 00:14:36,000
so we have this bottleneck in the middle

369
00:14:36,000 --> 00:14:38,519
and now in the traditional latency

370
00:14:38,519 --> 00:14:40,680
measurement way what will happen is that

371
00:14:40,680 --> 00:14:42,600
well the server will send this latency

372
00:14:42,600 --> 00:14:45,839
probe fruit is bottleneck and this probe

373
00:14:45,839 --> 00:14:47,339
will go very quickly through the network

374
00:14:47,339 --> 00:14:49,320
through the bottleneck over to the phone

375
00:14:49,320 --> 00:14:51,180
so it's a very

376
00:14:51,180 --> 00:14:54,120
fast latency measurement this explains

377
00:14:54,120 --> 00:14:55,680
the six milliseconds I showed you

378
00:14:55,680 --> 00:14:58,079
earlier on the slides right

379
00:14:58,079 --> 00:15:01,380
now what does working conditions do

380
00:15:01,380 --> 00:15:03,480
if I am creating working conditions and

381
00:15:03,480 --> 00:15:05,579
I am generating traffic on the network

382
00:15:05,579 --> 00:15:08,880
what happens is that this traffic starts

383
00:15:08,880 --> 00:15:11,760
piling up in the bottleneck

384
00:15:11,760 --> 00:15:13,740
you still see this

385
00:15:13,740 --> 00:15:16,380
standing Hue building up and then this

386
00:15:16,380 --> 00:15:19,019
latency measurement probe right this

387
00:15:19,019 --> 00:15:21,540
latency probe that at the end is sitting

388
00:15:21,540 --> 00:15:24,120
behind all of this other traffic

389
00:15:24,120 --> 00:15:25,740
now what happens is because this

390
00:15:25,740 --> 00:15:27,959
bottleneck has a certain capacity and

391
00:15:27,959 --> 00:15:29,760
let's say I measured this morning here

392
00:15:29,760 --> 00:15:31,440
at nanog I measured 10 megabits per

393
00:15:31,440 --> 00:15:34,139
seconds it's kind of slow so imagine how

394
00:15:34,139 --> 00:15:37,139
those 10 megabits per seconds are slowly

395
00:15:37,139 --> 00:15:39,480
draining out of this bottleneck and this

396
00:15:39,480 --> 00:15:42,060
latency probe is cued behind all of the

397
00:15:42,060 --> 00:15:44,040
other traffic waiting for the previous

398
00:15:44,040 --> 00:15:45,600
traffic to drain

399
00:15:45,600 --> 00:15:49,740
until finally it's reached the phone

400
00:15:49,740 --> 00:15:51,959
this explains the three seconds I talked

401
00:15:51,959 --> 00:15:54,959
about earlier right

402
00:15:54,959 --> 00:15:58,199
so let's recap what causes low

403
00:15:58,199 --> 00:16:00,360
responsiveness well first we have those

404
00:16:00,360 --> 00:16:03,899
data transfers like an email attachment

405
00:16:03,899 --> 00:16:06,959
being downloaded photos being sent or

406
00:16:06,959 --> 00:16:09,000
received a movie being watched right

407
00:16:09,000 --> 00:16:10,620
then

408
00:16:10,620 --> 00:16:11,940
we have

409
00:16:11,940 --> 00:16:14,279
those data transfers that are creating

410
00:16:14,279 --> 00:16:16,079
working conditions

411
00:16:16,079 --> 00:16:18,420
these working conditions result in an

412
00:16:18,420 --> 00:16:20,279
increased buffer occupancy in the

413
00:16:20,279 --> 00:16:23,339
bottleneck router right and this buffer

414
00:16:23,339 --> 00:16:25,500
capacity creates this High latency low

415
00:16:25,500 --> 00:16:27,240
responsiveness that you are experiencing

416
00:16:27,240 --> 00:16:29,519
which again will cause bad video

417
00:16:29,519 --> 00:16:32,339
conferencing experience

418
00:16:32,339 --> 00:16:34,500
so what can we do about this about the

419
00:16:34,500 --> 00:16:36,959
network site responsiveness

420
00:16:36,959 --> 00:16:39,779
well first of all we need to understand

421
00:16:39,779 --> 00:16:42,240
why are this why is this buffer building

422
00:16:42,240 --> 00:16:44,459
up in the bottleneck router

423
00:16:44,459 --> 00:16:46,560
so in this example right where we have

424
00:16:46,560 --> 00:16:50,279
the server sending data to the phone the

425
00:16:50,279 --> 00:16:52,139
transport layer doesn't know about the

426
00:16:52,139 --> 00:16:54,839
bottleneck's capacity right the only way

427
00:16:54,839 --> 00:16:56,699
for the transport layer to learn about

428
00:16:56,699 --> 00:16:59,100
the bottleneck capacity is by probe the

429
00:16:59,100 --> 00:17:01,440
network and by gradually increasing the

430
00:17:01,440 --> 00:17:03,959
transmission rate right the transport

431
00:17:03,959 --> 00:17:05,400
layer has no clue

432
00:17:05,400 --> 00:17:07,500
so the way it's doing is doing this and

433
00:17:07,500 --> 00:17:09,299
it's increasing the transmission rate

434
00:17:09,299 --> 00:17:12,900
probing for capacity eventually the

435
00:17:12,900 --> 00:17:15,179
transport will start will be standing at

436
00:17:15,179 --> 00:17:17,040
the bottlenecks capacity and it will

437
00:17:17,040 --> 00:17:20,160
even overshoot once it overshoots then

438
00:17:20,160 --> 00:17:21,959
we have a queue building up at the

439
00:17:21,959 --> 00:17:24,000
bottleneck and this is the queue that

440
00:17:24,000 --> 00:17:26,280
you see right there right now the

441
00:17:26,280 --> 00:17:27,839
bottleneck is still draining at its

442
00:17:27,839 --> 00:17:30,120
regular capacity slowly one packet at a

443
00:17:30,120 --> 00:17:31,260
time

444
00:17:31,260 --> 00:17:33,179
so we have this standing queue sitting

445
00:17:33,179 --> 00:17:35,039
there and the transport layer only

446
00:17:35,039 --> 00:17:36,600
learns about it once a packet gets

447
00:17:36,600 --> 00:17:38,280
dropped and then the transport layer

448
00:17:38,280 --> 00:17:40,500
will start slowing down its transmission

449
00:17:40,500 --> 00:17:43,559
rate but in the meantime we have created

450
00:17:43,559 --> 00:17:45,480
this queue that is just waiting to be

451
00:17:45,480 --> 00:17:47,100
drained

452
00:17:47,100 --> 00:17:49,620
so the problem really is here that the

453
00:17:49,620 --> 00:17:51,900
network buffers are just too big right

454
00:17:51,900 --> 00:17:55,080
and they need to become smarter at

455
00:17:55,080 --> 00:17:57,600
managing this queue and letting the

456
00:17:57,600 --> 00:17:59,460
transport know at what is the right

457
00:17:59,460 --> 00:18:01,799
transmission rate

458
00:18:01,799 --> 00:18:03,660
so let me show you a little bit more in

459
00:18:03,660 --> 00:18:06,000
a graphic way of what is happening on

460
00:18:06,000 --> 00:18:07,500
the network

461
00:18:07,500 --> 00:18:09,660
so if you have on the top you have the

462
00:18:09,660 --> 00:18:12,179
good put at which the data is being

463
00:18:12,179 --> 00:18:14,820
transmitted okay good put at the

464
00:18:14,820 --> 00:18:16,559
beginning as the transmit is ramping up

465
00:18:16,559 --> 00:18:18,660
is gradually increasing

466
00:18:18,660 --> 00:18:21,240
because the the transport layer is

467
00:18:21,240 --> 00:18:23,100
standing below the bottleneck's capacity

468
00:18:23,100 --> 00:18:25,980
there's no queue up in the bottom buffer

469
00:18:25,980 --> 00:18:28,740
kupancy is zero no added delay

470
00:18:28,740 --> 00:18:31,320
but as the transport is overshooting

471
00:18:31,320 --> 00:18:33,360
what we see then is while goodput is not

472
00:18:33,360 --> 00:18:34,980
increasing because the bottleneck's

473
00:18:34,980 --> 00:18:37,260
capacity is stable it cannot create more

474
00:18:37,260 --> 00:18:40,740
capacity right so goodput is stable but

475
00:18:40,740 --> 00:18:43,500
now we have the bottleneck who is

476
00:18:43,500 --> 00:18:45,720
increasing the buffer occupancy meaning

477
00:18:45,720 --> 00:18:47,760
there's more delay being added to the

478
00:18:47,760 --> 00:18:48,900
flow

479
00:18:48,900 --> 00:18:51,720
Unfortunately today we are operating at

480
00:18:51,720 --> 00:18:54,660
this point we have maximum good put but

481
00:18:54,660 --> 00:18:56,940
also maximum buffer occupancy which

482
00:18:56,940 --> 00:18:59,220
means maximum edit delay which means

483
00:18:59,220 --> 00:19:01,020
three seconds of latency

484
00:19:01,020 --> 00:19:03,480
where we want to be is there we want

485
00:19:03,480 --> 00:19:06,179
maximum good put but minimum buffer

486
00:19:06,179 --> 00:19:07,980
occupancy

487
00:19:07,980 --> 00:19:09,840
so how can we get at this operating

488
00:19:09,840 --> 00:19:11,640
point right

489
00:19:11,640 --> 00:19:15,000
so as the transport is ramping up at one

490
00:19:15,000 --> 00:19:16,860
point this queue is starting to build up

491
00:19:16,860 --> 00:19:19,080
and we want the transport to oscillate

492
00:19:19,080 --> 00:19:22,500
around this optimal operating point

493
00:19:22,500 --> 00:19:25,260
right and the only way the transfer can

494
00:19:25,260 --> 00:19:27,900
do is is do this is if it gets a signal

495
00:19:27,900 --> 00:19:30,179
from the network that tells the

496
00:19:30,179 --> 00:19:33,299
transport hey please slow down

497
00:19:33,299 --> 00:19:35,820
and if the network can provide this

498
00:19:35,820 --> 00:19:38,160
signal to the end users to the end hosts

499
00:19:38,160 --> 00:19:41,100
then the the transport can oscillate

500
00:19:41,100 --> 00:19:43,559
around this point and we stay very close

501
00:19:43,559 --> 00:19:46,260
to this optimal operating point we have

502
00:19:46,260 --> 00:19:49,020
lowest lowest delay while having maximum

503
00:19:49,020 --> 00:19:51,299
capacity

504
00:19:51,299 --> 00:19:54,600
luckily there is a technology available

505
00:19:54,600 --> 00:19:57,660
that achieves exactly this and it's

506
00:19:57,660 --> 00:20:00,120
called l4s

507
00:20:00,120 --> 00:20:03,059
lfos is an industry standard developed

508
00:20:03,059 --> 00:20:05,400
by the ITF and it stands for low latency

509
00:20:05,400 --> 00:20:08,700
low loss and scalable throughput so it

510
00:20:08,700 --> 00:20:10,620
gives you the best of all the worlds the

511
00:20:10,620 --> 00:20:13,799
lowest latency and the high throughput

512
00:20:13,799 --> 00:20:15,660
there's a large community around it

513
00:20:15,660 --> 00:20:19,740
developing ietf the the l4s standard and

514
00:20:19,740 --> 00:20:23,220
the way l4s behaves is that it notifies

515
00:20:23,220 --> 00:20:26,160
the antholes early when the queue is

516
00:20:26,160 --> 00:20:28,679
building up this notification uses

517
00:20:28,679 --> 00:20:31,440
either ecn mechanism which is a very

518
00:20:31,440 --> 00:20:33,419
lightweight mechanism to allow to signal

519
00:20:33,419 --> 00:20:36,720
to anthos when a queue is building up

520
00:20:36,720 --> 00:20:38,940
the end hosts when they receive this

521
00:20:38,940 --> 00:20:41,640
notification they can react quickly and

522
00:20:41,640 --> 00:20:44,600
keep the queues low

523
00:20:45,000 --> 00:20:48,240
so alfos is a significant Improvement in

524
00:20:48,240 --> 00:20:50,220
responsiveness it allows to bring the

525
00:20:50,220 --> 00:20:51,780
latency from several hundreds of

526
00:20:51,780 --> 00:20:54,360
milliseconds down to only several

527
00:20:54,360 --> 00:20:56,820
milliseconds

528
00:20:56,820 --> 00:20:58,980
and it's not just an ITF standard

529
00:20:58,980 --> 00:21:00,059
there's actually a large community

530
00:21:00,059 --> 00:21:02,940
around l4s there has been interrupt

531
00:21:02,940 --> 00:21:05,340
event at the iitf very successful in

532
00:21:05,340 --> 00:21:07,500
July of this year there were 15

533
00:21:07,500 --> 00:21:11,580
companies participating in l4s companies

534
00:21:11,580 --> 00:21:15,000
like apple Google Chromecast and Charter

535
00:21:15,000 --> 00:21:17,160
where there's content providers like

536
00:21:17,160 --> 00:21:18,020
Netflix

537
00:21:18,020 --> 00:21:21,600
equipment vendors like Nokia and so on

538
00:21:21,600 --> 00:21:23,400
a total of seven bottleneck

539
00:21:23,400 --> 00:21:25,340
implementations were tested

540
00:21:25,340 --> 00:21:28,620
implementations on doxes cable modems

541
00:21:28,620 --> 00:21:31,980
cmts as well as Wi-Fi access points and

542
00:21:31,980 --> 00:21:34,080
5G emulators so there's a large

543
00:21:34,080 --> 00:21:35,700
community around it and there's a lot of

544
00:21:35,700 --> 00:21:38,460
movement behind l4s to improve the

545
00:21:38,460 --> 00:21:40,080
responsiveness

546
00:21:40,080 --> 00:21:43,260
the good news also about l4s is it

547
00:21:43,260 --> 00:21:45,480
doesn't need to be deployed everywhere

548
00:21:45,480 --> 00:21:48,059
on the Internet only the bottlenecks

549
00:21:48,059 --> 00:21:51,000
needed meaning only the cmts the cable

550
00:21:51,000 --> 00:21:53,700
modems and the Wi-Fi access points

551
00:21:53,700 --> 00:21:55,380
you don't need to update the core

552
00:21:55,380 --> 00:21:57,679
routers right

553
00:21:57,679 --> 00:22:00,960
finally at the l4s interrupt at the ITF

554
00:22:00,960 --> 00:22:02,280
there were a number of entos

555
00:22:02,280 --> 00:22:03,840
implementations and they did a very

556
00:22:03,840 --> 00:22:07,080
successful demo of the ultra low latency

557
00:22:07,080 --> 00:22:09,179
how they were able to keep the latency

558
00:22:09,179 --> 00:22:11,340
at several milliseconds

559
00:22:11,340 --> 00:22:13,440
while loading the network at its full

560
00:22:13,440 --> 00:22:15,919
capacity

561
00:22:16,080 --> 00:22:19,860
so alpha s is the future and so what

562
00:22:19,860 --> 00:22:21,720
what can you do or what what can we do

563
00:22:21,720 --> 00:22:23,400
about this so

564
00:22:23,400 --> 00:22:26,820
if you are an operator or vendor

565
00:22:26,820 --> 00:22:28,860
we encourage you to start measuring

566
00:22:28,860 --> 00:22:30,720
responsiveness with one of the tools I

567
00:22:30,720 --> 00:22:33,480
presented all right if one of you here

568
00:22:33,480 --> 00:22:35,280
ran the speed test while I was talking

569
00:22:35,280 --> 00:22:36,659
earlier I don't know what number you

570
00:22:36,659 --> 00:22:38,760
observed I'm sure some of you saw more

571
00:22:38,760 --> 00:22:41,580
than one second so that's the kind of

572
00:22:41,580 --> 00:22:44,400
scenario we need to improve right so

573
00:22:44,400 --> 00:22:46,380
start measuring for responsiveness in

574
00:22:46,380 --> 00:22:47,580
your networks in your equipment

575
00:22:47,580 --> 00:22:49,140
everywhere

576
00:22:49,140 --> 00:22:51,000
make sure that your networks are easy

577
00:22:51,000 --> 00:22:53,520
and friendly so that l4s can be deployed

578
00:22:53,520 --> 00:22:55,440
easily

579
00:22:55,440 --> 00:22:58,020
um also if you're an operator go ahead

580
00:22:58,020 --> 00:23:00,840
and ask the vendors about l4s and for

581
00:23:00,840 --> 00:23:02,820
the vendors

582
00:23:02,820 --> 00:23:04,500
um Implement l4s in your network

583
00:23:04,500 --> 00:23:06,659
equipment and participate in the interop

584
00:23:06,659 --> 00:23:08,760
events so that you can provide a high

585
00:23:08,760 --> 00:23:11,100
responsiveness to your customers

586
00:23:11,100 --> 00:23:13,500
I've a few references here on the slide

587
00:23:13,500 --> 00:23:15,120
that will allow you to get some more

588
00:23:15,120 --> 00:23:17,000
information about the l4s

589
00:23:17,000 --> 00:23:21,240
the l4s effort across the industry

590
00:23:21,240 --> 00:23:22,860
so

591
00:23:22,860 --> 00:23:25,380
I've shown you now this responsiveness

592
00:23:25,380 --> 00:23:27,419
measurement metric

593
00:23:27,419 --> 00:23:29,400
giving you some pointers to some tools

594
00:23:29,400 --> 00:23:32,039
and we have talked about how we can fix

595
00:23:32,039 --> 00:23:34,380
responsiveness in the internet

596
00:23:34,380 --> 00:23:37,440
so the talk was supposed to be done here

597
00:23:37,440 --> 00:23:39,299
and we would all be able to go get a

598
00:23:39,299 --> 00:23:40,080
coffee

599
00:23:40,080 --> 00:23:43,260
now unfortunately on this journey we

600
00:23:43,260 --> 00:23:44,820
discovered something that we actually

601
00:23:44,820 --> 00:23:47,220
did not expect and this is the story

602
00:23:47,220 --> 00:23:50,100
about server-side responsiveness

603
00:23:50,100 --> 00:23:53,580
let me tell you what what this is about

604
00:23:53,580 --> 00:23:56,159
so as Randall Stewart and I we were

605
00:23:56,159 --> 00:23:59,340
working on this and we were our goal was

606
00:23:59,340 --> 00:24:02,340
to improve random access in the movie

607
00:24:02,340 --> 00:24:04,620
streaming so what does this mean this

608
00:24:04,620 --> 00:24:06,539
means I'm streaming a movie and I'm

609
00:24:06,539 --> 00:24:08,880
skipping ahead I want to skip ahead I

610
00:24:08,880 --> 00:24:10,620
don't want to watch a trailer or I want

611
00:24:10,620 --> 00:24:12,299
to skip ahead 10 seconds because I've

612
00:24:12,299 --> 00:24:14,460
already seen it and we're all used to

613
00:24:14,460 --> 00:24:16,200
this experience because when we skip

614
00:24:16,200 --> 00:24:17,700
ahead we get this little Spinning Wheel

615
00:24:17,700 --> 00:24:20,220
we wait a few seconds and then the movie

616
00:24:20,220 --> 00:24:23,760
starts again right and Randall Stewart

617
00:24:23,760 --> 00:24:24,960
and I we were like okay this is a

618
00:24:24,960 --> 00:24:27,539
responsiveness issue we'll have to work

619
00:24:27,539 --> 00:24:30,240
on our bottleneck and we fixed our

620
00:24:30,240 --> 00:24:31,620
bottleneck we made sure that our

621
00:24:31,620 --> 00:24:33,179
bottleneck had a higher responsiveness

622
00:24:33,179 --> 00:24:35,460
was properly configured everything was

623
00:24:35,460 --> 00:24:36,600
perfect

624
00:24:36,600 --> 00:24:38,340
and when we tried it again the movie

625
00:24:38,340 --> 00:24:40,679
streaming and we skipped the head

626
00:24:40,679 --> 00:24:43,440
it was better yes but it wasn't perfect

627
00:24:43,440 --> 00:24:46,080
it wasn't instant right it wasn't right

628
00:24:46,080 --> 00:24:47,700
on the spot

629
00:24:47,700 --> 00:24:49,799
and as we started digging into this we

630
00:24:49,799 --> 00:24:51,900
realized that we forgot something we

631
00:24:51,900 --> 00:24:54,059
only fixed half of the problem because

632
00:24:54,059 --> 00:24:55,919
the servers are actually having a big

633
00:24:55,919 --> 00:24:58,679
impact on responsiveness as well

634
00:24:58,679 --> 00:25:00,480
so let me explain you what happens on

635
00:25:00,480 --> 00:25:02,460
the server side

636
00:25:02,460 --> 00:25:05,280
so a typical service type site stack has

637
00:25:05,280 --> 00:25:08,820
an IP layer transport layer a security

638
00:25:08,820 --> 00:25:11,400
layer and an HTTP layer

639
00:25:11,400 --> 00:25:14,220
now what is happening there is that

640
00:25:14,220 --> 00:25:16,260
inside the transmit layer you have the

641
00:25:16,260 --> 00:25:18,480
congestion control right that is

642
00:25:18,480 --> 00:25:21,780
measuring the capacity and the speed of

643
00:25:21,780 --> 00:25:24,299
the TCP connection of the bottleneck

644
00:25:24,299 --> 00:25:26,520
link inside the network

645
00:25:26,520 --> 00:25:28,440
now this congestion control is actually

646
00:25:28,440 --> 00:25:30,600
limiting the speed at which data is

647
00:25:30,600 --> 00:25:33,179
draining out of the server and so what

648
00:25:33,179 --> 00:25:34,620
happens is there's a queue building up

649
00:25:34,620 --> 00:25:36,960
in a transport layer and a TLS layer and

650
00:25:36,960 --> 00:25:39,539
in the HTTP layer and if you start

651
00:25:39,539 --> 00:25:42,299
digging in some server implementations

652
00:25:42,299 --> 00:25:44,640
you have several megabytes of data

653
00:25:44,640 --> 00:25:46,860
queuing up for every single TCP

654
00:25:46,860 --> 00:25:49,100
connection

655
00:25:49,380 --> 00:25:52,440
and if you have even a staggered server

656
00:25:52,440 --> 00:25:54,539
deployment in your CDN environment where

657
00:25:54,539 --> 00:25:56,580
you have for example one server

658
00:25:56,580 --> 00:25:59,700
terminating TLS and HTTP and another

659
00:25:59,700 --> 00:26:02,159
server hosting the content you have this

660
00:26:02,159 --> 00:26:05,279
kind of chain of buffers queuing up one

661
00:26:05,279 --> 00:26:08,220
after each other creating latency up to

662
00:26:08,220 --> 00:26:11,400
tens of tens of seconds of latency

663
00:26:11,400 --> 00:26:14,039
and this was the reason why when we were

664
00:26:14,039 --> 00:26:15,960
skipping ahead in our movie we were not

665
00:26:15,960 --> 00:26:17,760
seeing the instant response why we were

666
00:26:17,760 --> 00:26:20,400
still waiting for the spinning wheel it

667
00:26:20,400 --> 00:26:22,260
was because data was queuing up on the

668
00:26:22,260 --> 00:26:24,120
server side

669
00:26:24,120 --> 00:26:26,100
so servers have a significant

670
00:26:26,100 --> 00:26:28,260
contribution to responsiveness issues

671
00:26:28,260 --> 00:26:30,600
there are many opportunities on the

672
00:26:30,600 --> 00:26:32,279
server side for over buffering it's in

673
00:26:32,279 --> 00:26:35,039
the transport layer the TLs layer HTTP

674
00:26:35,039 --> 00:26:38,520
layer any layer you can imagine on the

675
00:26:38,520 --> 00:26:40,559
server side has the potential for adding

676
00:26:40,559 --> 00:26:41,940
additional buffers and causing

677
00:26:41,940 --> 00:26:45,799
additional problems in terms of latency

678
00:26:46,200 --> 00:26:48,539
and the impact on a user experience is

679
00:26:48,539 --> 00:26:51,240
huge skipping forward in movie streams

680
00:26:51,240 --> 00:26:53,880
once you fix the server-side latency it

681
00:26:53,880 --> 00:26:55,919
is instant right on the time so you skip

682
00:26:55,919 --> 00:26:57,419
ahead and immediately the movie

683
00:26:57,419 --> 00:27:00,120
continues it was mind-blowing this

684
00:27:00,120 --> 00:27:01,260
experience

685
00:27:01,260 --> 00:27:03,240
another aspect is adaptive bitrate

686
00:27:03,240 --> 00:27:05,039
algorithms

687
00:27:05,039 --> 00:27:07,740
um like hls that are trying to download

688
00:27:07,740 --> 00:27:10,740
or stream a video at the at the speed of

689
00:27:10,740 --> 00:27:11,880
the bottleneck

690
00:27:11,880 --> 00:27:14,340
if you have a lot of data queued up and

691
00:27:14,340 --> 00:27:16,260
the Adaptive bitrate algorithm wants to

692
00:27:16,260 --> 00:27:17,700
adjust the bit rate

693
00:27:17,700 --> 00:27:19,860
the bitrate algorithm needs to wait for

694
00:27:19,860 --> 00:27:22,740
all of this data to drain before this

695
00:27:22,740 --> 00:27:25,799
new bit rate can be sent right we have

696
00:27:25,799 --> 00:27:27,779
seen when we are trying this out in our

697
00:27:27,779 --> 00:27:29,460
new server environment we have seen a

698
00:27:29,460 --> 00:27:31,860
reduction of 20 in the stall rate for

699
00:27:31,860 --> 00:27:34,500
video streaming

700
00:27:34,500 --> 00:27:38,220
any use case that uses HTTP 2 has huge

701
00:27:38,220 --> 00:27:41,039
benefits uh with a better responsiveness

702
00:27:41,039 --> 00:27:43,500
whenever you are sending a large HTTP

703
00:27:43,500 --> 00:27:46,020
stream multiplexed on a single H2

704
00:27:46,020 --> 00:27:48,900
connection with another smaller latency

705
00:27:48,900 --> 00:27:51,179
sensitive stream

706
00:27:51,179 --> 00:27:53,100
high responsiveness will be a big

707
00:27:53,100 --> 00:27:55,559
benefit for those use cases and you do

708
00:27:55,559 --> 00:27:56,580
on the internet

709
00:27:56,580 --> 00:27:59,039
web browsing and if you have a stock

710
00:27:59,039 --> 00:28:02,520
application a weather forecasts uh get

711
00:28:02,520 --> 00:28:04,200
if you're getting driving directions in

712
00:28:04,200 --> 00:28:07,500
a Maps or driving navigation app all of

713
00:28:07,500 --> 00:28:09,480
these use cases benefit from a higher

714
00:28:09,480 --> 00:28:11,820
responsiveness

715
00:28:11,820 --> 00:28:14,220
so what you can do here is that you can

716
00:28:14,220 --> 00:28:16,740
introduce buffering in the server side

717
00:28:16,740 --> 00:28:20,220
by working on the transport layer there

718
00:28:20,220 --> 00:28:21,960
are options available in Linux for

719
00:28:21,960 --> 00:28:23,640
example it's called tsp not send low

720
00:28:23,640 --> 00:28:26,039
auto mark that allow you to limit the

721
00:28:26,039 --> 00:28:27,980
size of the buffers

722
00:28:27,980 --> 00:28:30,360
across the networking stack in the TLs

723
00:28:30,360 --> 00:28:33,360
layer the HTTP 2 layer everywhere you

724
00:28:33,360 --> 00:28:35,580
can work on reducing those buffers

725
00:28:35,580 --> 00:28:37,559
buffers can be everywhere so be mindful

726
00:28:37,559 --> 00:28:39,659
when we worked on we're improving hours

727
00:28:39,659 --> 00:28:41,400
we were digging for all of those buffers

728
00:28:41,400 --> 00:28:43,320
to figure out where this was happening

729
00:28:43,320 --> 00:28:45,299
and we found many places where we had to

730
00:28:45,299 --> 00:28:48,360
do small adjustments

731
00:28:48,360 --> 00:28:51,000
so here's what you can do right

732
00:28:51,000 --> 00:28:52,799
with the tools that are available like

733
00:28:52,799 --> 00:28:54,480
the network quality measurement tool on

734
00:28:54,480 --> 00:28:57,059
Mac OS or the go responsiveness

735
00:28:57,059 --> 00:28:59,279
measurement tool you can simply start

736
00:28:59,279 --> 00:29:02,400
hosting some files on your servers

737
00:29:02,400 --> 00:29:04,799
that allow you to point those tools at

738
00:29:04,799 --> 00:29:06,059
your server and then you can start

739
00:29:06,059 --> 00:29:08,039
measuring the responsiveness of your own

740
00:29:08,039 --> 00:29:10,260
server so if you do this you will be

741
00:29:10,260 --> 00:29:12,779
able to see how does your servers behave

742
00:29:12,779 --> 00:29:14,220
in your environment

743
00:29:14,220 --> 00:29:15,720
and then you can start tuning those

744
00:29:15,720 --> 00:29:17,520
buffers that you know about and start

745
00:29:17,520 --> 00:29:19,200
playing around and you can see whether

746
00:29:19,200 --> 00:29:22,279
or not it helps for your responsiveness

747
00:29:22,279 --> 00:29:25,200
and try again and see if it improves

748
00:29:25,200 --> 00:29:28,200
your customers user experience

749
00:29:28,200 --> 00:29:30,480
believe me it will have a big impact for

750
00:29:30,480 --> 00:29:33,360
many of your users

751
00:29:33,360 --> 00:29:35,580
I have some resources available to you

752
00:29:35,580 --> 00:29:38,580
about the tools and lfos that you can

753
00:29:38,580 --> 00:29:40,020
look up on the slides that are online

754
00:29:40,020 --> 00:29:41,220
right now

755
00:29:41,220 --> 00:29:43,740
but I want to conclude my talk with some

756
00:29:43,740 --> 00:29:45,240
closing remarks before we go into

757
00:29:45,240 --> 00:29:47,460
questions

758
00:29:47,460 --> 00:29:50,880
first of all better responsiveness gives

759
00:29:50,880 --> 00:29:53,340
a better user experience we have shown

760
00:29:53,340 --> 00:29:54,779
that responsiveness under working

761
00:29:54,779 --> 00:29:57,059
conditions is an important metric today

762
00:29:57,059 --> 00:29:59,700
that can help improve the user

763
00:29:59,700 --> 00:30:01,919
experience video streaming will be just

764
00:30:01,919 --> 00:30:03,960
so much better with a good

765
00:30:03,960 --> 00:30:05,520
responsiveness

766
00:30:05,520 --> 00:30:07,620
those tools that are available to you we

767
00:30:07,620 --> 00:30:09,360
really encourage you to start using them

768
00:30:09,360 --> 00:30:11,940
the UCLA speed test that you run here it

769
00:30:11,940 --> 00:30:13,380
shows you that there's a lot of work to

770
00:30:13,380 --> 00:30:16,440
be done right and that work to be done

771
00:30:16,440 --> 00:30:18,600
I've given you some pointers about lfos

772
00:30:18,600 --> 00:30:20,580
that will allow to fix the network side

773
00:30:20,580 --> 00:30:23,340
responsiveness and if you are a CDN

774
00:30:23,340 --> 00:30:26,220
content provider or streaming provider

775
00:30:26,220 --> 00:30:28,559
like Netflix go work on the servers

776
00:30:28,559 --> 00:30:30,600
buffer management to make the streaming

777
00:30:30,600 --> 00:30:33,120
and all of the other HTTP use cases

778
00:30:33,120 --> 00:30:36,059
better for your clients

779
00:30:36,059 --> 00:30:38,760
and so with that I would like to thank

780
00:30:38,760 --> 00:30:41,039
you for your attention I'm ready to take

781
00:30:41,039 --> 00:30:43,620
questions and I will also invite you or

782
00:30:43,620 --> 00:30:45,240
anyone who's interested to come talk to

783
00:30:45,240 --> 00:30:47,340
us later after this talk we would be

784
00:30:47,340 --> 00:30:48,899
happy to discuss with you and help you

785
00:30:48,899 --> 00:30:52,459
on improving your responsiveness

786
00:30:55,140 --> 00:30:55,670
thank you

787
00:30:55,670 --> 00:30:59,630
[Applause]

788
00:31:01,080 --> 00:31:04,140
hi vinod from great presentation thank

789
00:31:04,140 --> 00:31:07,620
you three questions uh one is is the

790
00:31:07,620 --> 00:31:09,539
problem on the server side mitigated to

791
00:31:09,539 --> 00:31:12,200
some extent by load balancing

792
00:31:12,200 --> 00:31:14,940
and second is how much of this can make

793
00:31:14,940 --> 00:31:17,580
it into the Asic on the First on into

794
00:31:17,580 --> 00:31:20,399
the Asic on the first mine

795
00:31:20,399 --> 00:31:23,460
uh from the client right how much of the

796
00:31:23,460 --> 00:31:25,919
l4s can be implemented in the Asic yeah

797
00:31:25,919 --> 00:31:27,539
and

798
00:31:27,539 --> 00:31:30,240
is it does the problem become worse or

799
00:31:30,240 --> 00:31:32,520
has to be managed out of band for

800
00:31:32,520 --> 00:31:34,620
admission control while the data

801
00:31:34,620 --> 00:31:38,039
throughput is happening does connection

802
00:31:38,039 --> 00:31:40,380
admission control is a separate problem

803
00:31:40,380 --> 00:31:42,120
needs to be managed separately outside

804
00:31:42,120 --> 00:31:45,840
of data latency or you know it can be

805
00:31:45,840 --> 00:31:48,539
managed in band you know as just like

806
00:31:48,539 --> 00:31:51,539
TCP since it's another stream of data

807
00:31:51,539 --> 00:31:52,980
while the

808
00:31:52,980 --> 00:31:54,960
admission control falls into the same

809
00:31:54,960 --> 00:31:57,600
category of problems or or not okay

810
00:31:57,600 --> 00:31:59,399
thanks for this question uh you can

811
00:31:59,399 --> 00:32:00,659
State everyone because I might ask some

812
00:32:00,659 --> 00:32:03,299
clarification questions so

813
00:32:03,299 --> 00:32:05,279
your first question was if load

814
00:32:05,279 --> 00:32:07,140
balancers on the service side will help

815
00:32:07,140 --> 00:32:08,840
on the server-side responsiveness right

816
00:32:08,840 --> 00:32:12,059
so on that question the thing is no load

817
00:32:12,059 --> 00:32:13,740
balances don't help because it's not a

818
00:32:13,740 --> 00:32:16,440
question about server scaling or

819
00:32:16,440 --> 00:32:20,399
capacity it's a question about the so

820
00:32:20,399 --> 00:32:22,200
every single TCP connection that is

821
00:32:22,200 --> 00:32:24,539
created to the server is running at a

822
00:32:24,539 --> 00:32:26,820
certain speed and that speed is driven

823
00:32:26,820 --> 00:32:29,220
by the bottleneck capacity in the

824
00:32:29,220 --> 00:32:31,380
client's Network usually let's say right

825
00:32:31,380 --> 00:32:33,120
and so that's not a question of

826
00:32:33,120 --> 00:32:35,220
server-side scalability it's a question

827
00:32:35,220 --> 00:32:37,320
of the bottling and so this

828
00:32:37,320 --> 00:32:40,260
um this limitation of the capacity of

829
00:32:40,260 --> 00:32:42,419
this single TCP connection is

830
00:32:42,419 --> 00:32:44,279
piggybacked all the way to the server

831
00:32:44,279 --> 00:32:46,919
inside the TCP congestion control that

832
00:32:46,919 --> 00:32:48,779
is running on the server side

833
00:32:48,779 --> 00:32:51,299
and what happens then is that because

834
00:32:51,299 --> 00:32:53,340
this TCP contest control on the server

835
00:32:53,340 --> 00:32:55,620
side is the limit is it's like not yet

836
00:32:55,620 --> 00:32:58,679
another bottleneck right and this yet

837
00:32:58,679 --> 00:33:01,380
another bottleneck means that data will

838
00:33:01,380 --> 00:33:03,299
be queued on top of it

839
00:33:03,299 --> 00:33:04,440
right

840
00:33:04,440 --> 00:33:06,360
and so it's not a question about

841
00:33:06,360 --> 00:33:08,399
server-side scalability or Fanning out

842
00:33:08,399 --> 00:33:10,559
and load balancing it's just about

843
00:33:10,559 --> 00:33:13,080
buffer management for single TCP

844
00:33:13,080 --> 00:33:15,419
connection because a TCP connection is

845
00:33:15,419 --> 00:33:17,220
this additional bottleneck that is

846
00:33:17,220 --> 00:33:19,980
causing more buffering

847
00:33:19,980 --> 00:33:21,419
um I hope that dance was your first

848
00:33:21,419 --> 00:33:24,059
question on the second question uh was

849
00:33:24,059 --> 00:33:26,940
whether l4s can be implemented in the

850
00:33:26,940 --> 00:33:28,200
Asic

851
00:33:28,200 --> 00:33:30,200
um so alphys is a very simple mechanism

852
00:33:30,200 --> 00:33:35,100
it only requires to do to have the the

853
00:33:35,100 --> 00:33:37,919
uh the bottleneck provide frequent

854
00:33:37,919 --> 00:33:40,500
marking on those packets that are being

855
00:33:40,500 --> 00:33:43,080
transmitted right and

856
00:33:43,080 --> 00:33:45,620
sorry

857
00:33:45,720 --> 00:33:48,440
smartnex

858
00:33:48,440 --> 00:33:51,179
yeah it should not be a problem of

859
00:33:51,179 --> 00:33:53,279
implementing alphys in in a smart Nick

860
00:33:53,279 --> 00:33:56,159
because I mean smart queuing has been

861
00:33:56,159 --> 00:33:58,019
implemented in the Asic format for a

862
00:33:58,019 --> 00:34:00,299
long time and it's it's very common so

863
00:34:00,299 --> 00:34:03,600
l4s is just another mechanism for smart

864
00:34:03,600 --> 00:34:06,419
queue management to Mo to provide this

865
00:34:06,419 --> 00:34:07,980
additional information from the network

866
00:34:07,980 --> 00:34:10,918
to the end hosts

867
00:34:10,918 --> 00:34:13,139
I'm sure Stuart wants to provide another

868
00:34:13,139 --> 00:34:15,659
comment uh yeah I'm Stuart Trasher from

869
00:34:15,659 --> 00:34:18,480
Apple I just wanted to clarify because

870
00:34:18,480 --> 00:34:21,119
when you ask the question there might

871
00:34:21,119 --> 00:34:23,040
actually be two different things that

872
00:34:23,040 --> 00:34:25,918
you're talking about because l4s has two

873
00:34:25,918 --> 00:34:28,679
components does the N systems Because

874
00:34:28,679 --> 00:34:30,839
the Internet we run today is based on

875
00:34:30,839 --> 00:34:33,300
the end-to-end principle of put as much

876
00:34:33,300 --> 00:34:34,739
of the brains as possible in the end

877
00:34:34,739 --> 00:34:36,960
systems and those end systems

878
00:34:36,960 --> 00:34:39,359
responsible for sending the right amount

879
00:34:39,359 --> 00:34:41,639
of data into the network not too little

880
00:34:41,639 --> 00:34:44,099
so you leave capacity idler not too much

881
00:34:44,099 --> 00:34:46,379
so you overflow cues

882
00:34:46,379 --> 00:34:48,599
so part of the Alpharetta algorithm

883
00:34:48,599 --> 00:34:51,359
exists in the end systems

884
00:34:51,359 --> 00:34:53,399
but those end systems can only adjust

885
00:34:53,399 --> 00:34:55,379
their rate if they get the right signals

886
00:34:55,379 --> 00:34:57,000
back from the network to tell them when

887
00:34:57,000 --> 00:34:58,980
they're going too fast so like many of

888
00:34:58,980 --> 00:35:00,780
these things it's a partnership there's

889
00:35:00,780 --> 00:35:02,400
brains in the end systems and there's

890
00:35:02,400 --> 00:35:04,640
brains in that hops through the network

891
00:35:04,640 --> 00:35:08,520
so when you're asking about Asics I

892
00:35:08,520 --> 00:35:09,780
wasn't sure whether you're talking about

893
00:35:09,780 --> 00:35:11,820
routers forwarding packets or end

894
00:35:11,820 --> 00:35:14,760
systems I mean I guess the answer is yes

895
00:35:14,760 --> 00:35:16,800
in both cases because anything you can

896
00:35:16,800 --> 00:35:18,420
do in software you can probably do in

897
00:35:18,420 --> 00:35:20,040
Gates if you try hard enough talking

898
00:35:20,040 --> 00:35:22,320
about the smart neck on the end system

899
00:35:22,320 --> 00:35:24,300
say if there's anything that the smart

900
00:35:24,300 --> 00:35:26,220
link needs to obtain piggyback back to

901
00:35:26,220 --> 00:35:29,280
the application on the local system uh

902
00:35:29,280 --> 00:35:31,740
that depends how smart the smart Nick is

903
00:35:31,740 --> 00:35:34,380
if it's doing Simple TCP segmentation

904
00:35:34,380 --> 00:35:38,099
offload where the host act shifts shifts

905
00:35:38,099 --> 00:35:42,660
a 64k segment that gets sliced up

906
00:35:42,660 --> 00:35:46,260
um that is relatively easy to do packet

907
00:35:46,260 --> 00:35:48,900
pacing is really important because a 10

908
00:35:48,900 --> 00:35:52,140
gig line rate burst of a 64k thing is a

909
00:35:52,140 --> 00:35:54,300
big burst of packets and that burst has

910
00:35:54,300 --> 00:35:57,000
to be smoothed out somewhere so pacing

911
00:35:57,000 --> 00:35:58,920
is really important if you're talking

912
00:35:58,920 --> 00:36:01,619
about offloading the entire TCP or quick

913
00:36:01,619 --> 00:36:05,220
State machine onto a smart neck then the

914
00:36:05,220 --> 00:36:08,099
responsibilities go with that we have

915
00:36:08,099 --> 00:36:10,260
renal congestion control we have cubic

916
00:36:10,260 --> 00:36:12,720
we have compound and and now we have l4s

917
00:36:12,720 --> 00:36:15,839
so if you wanted to do l4s entirely in

918
00:36:15,839 --> 00:36:17,760
Hardware offload you would have to

919
00:36:17,760 --> 00:36:19,380
implement that but I don't see that as

920
00:36:19,380 --> 00:36:21,660
being a major impediment no more really

921
00:36:21,660 --> 00:36:26,118
than than Reno Tahoe cubic

922
00:36:27,960 --> 00:36:28,980
okay

923
00:36:28,980 --> 00:36:31,440
um David tuber from cleftler uh great

924
00:36:31,440 --> 00:36:32,400
talk

925
00:36:32,400 --> 00:36:34,140
um really enjoyed it a question about

926
00:36:34,140 --> 00:36:36,720
the server-side responsiveness you

927
00:36:36,720 --> 00:36:39,420
mentioned that um so you mentioned about

928
00:36:39,420 --> 00:36:42,060
congestion congestion control have you

929
00:36:42,060 --> 00:36:44,760
done or is there a plan for any analysis

930
00:36:44,760 --> 00:36:46,400
on what is the optimal congestion

931
00:36:46,400 --> 00:36:48,960
congestion control mechanisms for

932
00:36:48,960 --> 00:36:51,720
server-side implementations and if not

933
00:36:51,720 --> 00:36:53,280
like what is your gut feeling or like

934
00:36:53,280 --> 00:36:54,420
what do you think is probably going to

935
00:36:54,420 --> 00:36:55,500
be the best

936
00:36:55,500 --> 00:36:57,000
congestion control algorithm because

937
00:36:57,000 --> 00:36:59,280
there's bbr cubic Reno

938
00:36:59,280 --> 00:37:00,359
um all those

939
00:37:00,359 --> 00:37:02,400
so in terms of contextual control

940
00:37:02,400 --> 00:37:04,400
um

941
00:37:05,220 --> 00:37:08,940
one problem today is with cubic right it

942
00:37:08,940 --> 00:37:11,220
is uh congestion control that is looking

943
00:37:11,220 --> 00:37:13,560
for packet loss and tries to fill the

944
00:37:13,560 --> 00:37:15,900
pipe as much as possible so it's kind of

945
00:37:15,900 --> 00:37:17,520
a Construction Control that has a

946
00:37:17,520 --> 00:37:19,800
tendency to create this kind of network

947
00:37:19,800 --> 00:37:22,200
side responsiveness issues because it

948
00:37:22,200 --> 00:37:24,900
always tries to to put as much data as

949
00:37:24,900 --> 00:37:26,520
possible into the network

950
00:37:26,520 --> 00:37:29,040
congestion control slab bbr are smarter

951
00:37:29,040 --> 00:37:31,020
about it and are definitely better in

952
00:37:31,020 --> 00:37:33,540
terms of buffer bloat but it is a very

953
00:37:33,540 --> 00:37:36,780
delicate balance because bbr has its own

954
00:37:36,780 --> 00:37:39,000
problems and its own instabilities right

955
00:37:39,000 --> 00:37:41,040
so we believe that

956
00:37:41,040 --> 00:37:45,300
the Right Way Forward is to have this

957
00:37:45,300 --> 00:37:48,119
cooperation between the network and the

958
00:37:48,119 --> 00:37:50,640
nthos with l4s where the network is

959
00:37:50,640 --> 00:37:52,619
providing just a tiny little bit of

960
00:37:52,619 --> 00:37:54,960
information back to the end hosts so

961
00:37:54,960 --> 00:37:57,720
that the antos can easily find the best

962
00:37:57,720 --> 00:38:00,420
operating point at which they can use

963
00:38:00,420 --> 00:38:02,280
the full capacity of the link without

964
00:38:02,280 --> 00:38:04,740
creating a queue that is in our opinion

965
00:38:04,740 --> 00:38:07,619
the the long long the long-term solution

966
00:38:07,619 --> 00:38:09,900
for for the internet

967
00:38:09,900 --> 00:38:12,440
thanks

968
00:38:16,740 --> 00:38:18,660
so unless there are any other questions

969
00:38:18,660 --> 00:38:21,960
I guess we can go for a break thank you

970
00:38:21,960 --> 00:38:24,260
very much

971
00:38:26,000 --> 00:38:29,739
[Music]

