1
00:00:00,000 --> 00:00:03,840
thanks folks okay

2
00:00:03,840 --> 00:00:05,460
so

3
00:00:05,460 --> 00:00:06,960
um a decade ago

4
00:00:06,960 --> 00:00:10,260
I was sat on my own in this semi-deralic

5
00:00:10,260 --> 00:00:13,380
data center in uh in Manchester

6
00:00:13,380 --> 00:00:15,540
um bootstrapping an open set public

7
00:00:15,540 --> 00:00:18,600
Cloud on a rack of second-hand Hardware

8
00:00:18,600 --> 00:00:20,820
bought on eBay

9
00:00:20,820 --> 00:00:24,840
and over time we rebuilt that data

10
00:00:24,840 --> 00:00:27,480
center we built a team and we built a

11
00:00:27,480 --> 00:00:30,779
real platform with real customers

12
00:00:30,779 --> 00:00:33,540
but there was no kubernetes there was no

13
00:00:33,540 --> 00:00:36,180
docker everything was pretty good for

14
00:00:36,180 --> 00:00:38,579
its time everything in Version Control

15
00:00:38,579 --> 00:00:41,460
orchestrated using puppet but there was

16
00:00:41,460 --> 00:00:43,680
lots of brittle bash scripts that needed

17
00:00:43,680 --> 00:00:45,780
regular tending

18
00:00:45,780 --> 00:00:48,539
and we took that platform through the

19
00:00:48,539 --> 00:00:52,680
iso 27001 security certification and the

20
00:00:52,680 --> 00:00:55,079
experience of managing security back

21
00:00:55,079 --> 00:00:57,600
then was pretty difficult there was lots

22
00:00:57,600 --> 00:00:59,180
of manual work

23
00:00:59,180 --> 00:01:02,059
external scans using things like nessus

24
00:01:02,059 --> 00:01:05,060
custom vulnerability checks in nagios

25
00:01:05,060 --> 00:01:10,220
and very limited tooling for automation

26
00:01:10,500 --> 00:01:13,740
and the point of all of this is that the

27
00:01:13,740 --> 00:01:16,020
cycle of Technology change moves pretty

28
00:01:16,020 --> 00:01:19,920
fast and it's only getting faster

29
00:01:19,920 --> 00:01:22,140
um the difference in the next decade is

30
00:01:22,140 --> 00:01:24,360
likely to be significantly bigger than

31
00:01:24,360 --> 00:01:25,740
in the last one

32
00:01:25,740 --> 00:01:27,659
so I'm going to talk about five things

33
00:01:27,659 --> 00:01:30,600
that could potentially change the way we

34
00:01:30,600 --> 00:01:32,400
think about security

35
00:01:32,400 --> 00:01:34,740
even more dramatically over the next 10

36
00:01:34,740 --> 00:01:36,979
years

37
00:01:37,259 --> 00:01:40,079
so at this point a huge disclaimer this

38
00:01:40,079 --> 00:01:42,659
is in no way a prediction or investment

39
00:01:42,659 --> 00:01:45,420
advice history is littered with examples

40
00:01:45,420 --> 00:01:48,060
that most things in technology very

41
00:01:48,060 --> 00:01:49,799
rarely go in the direction we think

42
00:01:49,799 --> 00:01:52,340
they're going to

43
00:01:52,799 --> 00:01:55,200
so you're going to hear an awful lot

44
00:01:55,200 --> 00:01:57,840
about Keys about certificates and about

45
00:01:57,840 --> 00:02:00,720
signing this year and it's a fairly safe

46
00:02:00,720 --> 00:02:04,680
prediction that uh artifact signing in

47
00:02:04,680 --> 00:02:06,479
our software development lifecycle is

48
00:02:06,479 --> 00:02:08,818
going to become pretty much standard

49
00:02:08,818 --> 00:02:12,360
and public key cryptography has served

50
00:02:12,360 --> 00:02:14,879
us well for more than 30 years it's

51
00:02:14,879 --> 00:02:17,580
underpinned all of the innovation in the

52
00:02:17,580 --> 00:02:19,260
web and Internet space

53
00:02:19,260 --> 00:02:21,060
but there may be problems in

54
00:02:21,060 --> 00:02:23,879
cryptography approaching

55
00:02:23,879 --> 00:02:25,920
so this is a quantum computer and

56
00:02:25,920 --> 00:02:28,440
quantum computers not only look cool

57
00:02:28,440 --> 00:02:30,000
um but they work very differently to

58
00:02:30,000 --> 00:02:32,400
classical computers they replace the

59
00:02:32,400 --> 00:02:35,520
idea of bits our zeros and ones with

60
00:02:35,520 --> 00:02:38,220
Quantum bits or qubits based on the

61
00:02:38,220 --> 00:02:41,220
properties of quantum mechanics

62
00:02:41,220 --> 00:02:43,140
and unless you're a Quantum physicist

63
00:02:43,140 --> 00:02:44,940
most of quantum mechanics is completely

64
00:02:44,940 --> 00:02:47,879
mind-bending for normal humans but most

65
00:02:47,879 --> 00:02:49,500
folks might be familiar with the concept

66
00:02:49,500 --> 00:02:52,500
of Schrodinger's cat this is a thought

67
00:02:52,500 --> 00:02:55,500
experiment about hypothetical cat that

68
00:02:55,500 --> 00:02:57,599
can be both alive and dead at the same

69
00:02:57,599 --> 00:03:00,720
time whilst it's in a sealed box with a

70
00:03:00,720 --> 00:03:03,540
fatal radioactive element and it's a

71
00:03:03,540 --> 00:03:05,580
pretty Macabre example but it does

72
00:03:05,580 --> 00:03:08,940
illustrate how particles can be zero or

73
00:03:08,940 --> 00:03:11,819
one or both at the same time and it's

74
00:03:11,819 --> 00:03:13,920
this superposition phenomenon that that

75
00:03:13,920 --> 00:03:17,299
quantum computers are based on

76
00:03:17,700 --> 00:03:20,519
and quantum computers are theoretically

77
00:03:20,519 --> 00:03:22,680
very good at certain classes of problem

78
00:03:22,680 --> 00:03:24,840
that are very hard for classical

79
00:03:24,840 --> 00:03:27,540
computers and factorization of prime

80
00:03:27,540 --> 00:03:29,940
numbers is one of these

81
00:03:29,940 --> 00:03:31,800
um this is a visualization of Shaw's

82
00:03:31,800 --> 00:03:34,379
algorithm developed in the 1990s by

83
00:03:34,379 --> 00:03:37,019
mathematician Peter Shaw and it's widely

84
00:03:37,019 --> 00:03:39,540
viewed as a proof that quantum computers

85
00:03:39,540 --> 00:03:41,159
could potentially break public key

86
00:03:41,159 --> 00:03:44,459
cryptography including diffie-hellman

87
00:03:44,459 --> 00:03:48,620
key exchanges and algorithms like RSA

88
00:03:48,620 --> 00:03:51,299
and this might be theoretical but some

89
00:03:51,299 --> 00:03:52,739
researchers think that this could happen

90
00:03:52,739 --> 00:03:55,319
in the next decade on a day known as Q

91
00:03:55,319 --> 00:03:57,599
day when most of our cryptography would

92
00:03:57,599 --> 00:04:00,299
be broken our certificates would be

93
00:04:00,299 --> 00:04:01,920
vulnerable to man in the middle attacks

94
00:04:01,920 --> 00:04:04,920
and our encryption would be cracked

95
00:04:04,920 --> 00:04:06,900
now there's a lot of different actors

96
00:04:06,900 --> 00:04:08,840
involved in building quantum computers

97
00:04:08,840 --> 00:04:12,000
including nation states and three letter

98
00:04:12,000 --> 00:04:14,280
agencies so we may never actually know

99
00:04:14,280 --> 00:04:17,120
that this has happened

100
00:04:17,940 --> 00:04:19,918
and it's serious enough for governments

101
00:04:19,918 --> 00:04:22,979
to take action with a move towards new

102
00:04:22,979 --> 00:04:26,400
kinds of algorithms resistant to this

103
00:04:26,400 --> 00:04:28,620
threat of quantum Computing post Quantum

104
00:04:28,620 --> 00:04:31,199
cryptography and in the US niss the

105
00:04:31,199 --> 00:04:32,520
National Institute for standards and

106
00:04:32,520 --> 00:04:34,259
technology has been running a

107
00:04:34,259 --> 00:04:36,540
competition for the last few years

108
00:04:36,540 --> 00:04:39,780
and the algorithms were selected in 2022

109
00:04:39,780 --> 00:04:42,600
and they've all got very cool names um

110
00:04:42,600 --> 00:04:43,979
any algorithm With The Star Trek

111
00:04:43,979 --> 00:04:46,440
reference has got to be good right

112
00:04:46,440 --> 00:04:48,300
but at some point it's likely we'll all

113
00:04:48,300 --> 00:04:50,699
need to switch our keys of certificates

114
00:04:50,699 --> 00:04:53,100
and our infrastructure over to these new

115
00:04:53,100 --> 00:04:55,320
methods

116
00:04:55,320 --> 00:04:58,080
so um coming back to signing I don't

117
00:04:58,080 --> 00:05:00,500
want to take

118
00:05:01,020 --> 00:05:02,540
um once you start signing everything

119
00:05:02,540 --> 00:05:04,620
what we're really doing is building

120
00:05:04,620 --> 00:05:06,300
chains of trust

121
00:05:06,300 --> 00:05:08,580
and the reality of any chain of trust is

122
00:05:08,580 --> 00:05:10,560
that there has to be something at the

123
00:05:10,560 --> 00:05:13,860
bottom that we can actually trust

124
00:05:13,860 --> 00:05:15,540
and it's obviously very important that

125
00:05:15,540 --> 00:05:17,820
we trust our build systems the things

126
00:05:17,820 --> 00:05:20,520
that produce our artifacts and there's a

127
00:05:20,520 --> 00:05:22,080
couple of different dimensions to this

128
00:05:22,080 --> 00:05:24,240
the first being can we trust our build

129
00:05:24,240 --> 00:05:27,000
to always produce the same thing

130
00:05:27,000 --> 00:05:29,039
now this may be maybe kind of

131
00:05:29,039 --> 00:05:31,500
non-obvious but there's not necessarily

132
00:05:31,500 --> 00:05:33,360
any guarantees that the thing you're

133
00:05:33,360 --> 00:05:36,060
building is exactly the same every time

134
00:05:36,060 --> 00:05:37,560
you build it

135
00:05:37,560 --> 00:05:40,020
and if it's not exactly the same then

136
00:05:40,020 --> 00:05:41,940
you can't really guarantee its validity

137
00:05:41,940 --> 00:05:45,600
even slight differences May introduce

138
00:05:45,600 --> 00:05:48,660
unexpected Behavior or even new security

139
00:05:48,660 --> 00:05:51,440
vulnerabilities

140
00:05:51,919 --> 00:05:54,060
and this can happen if you're using

141
00:05:54,060 --> 00:05:56,340
timestamps if you're ordering is

142
00:05:56,340 --> 00:05:58,500
volatile and for a whole host of other

143
00:05:58,500 --> 00:06:01,080
reasons so how can we ensure that our

144
00:06:01,080 --> 00:06:03,539
builds are completely deterministic

145
00:06:03,539 --> 00:06:05,580
well this reproducible builds concept

146
00:06:05,580 --> 00:06:07,560
has been around for a long time and it

147
00:06:07,560 --> 00:06:09,960
aims to do exactly that it's a set of

148
00:06:09,960 --> 00:06:12,240
software development practices aimed at

149
00:06:12,240 --> 00:06:14,940
creating bit for bit identical artifacts

150
00:06:14,940 --> 00:06:18,680
every time we run the process

151
00:06:18,720 --> 00:06:20,880
and lots of large open source projects

152
00:06:20,880 --> 00:06:23,340
already practice this but we again hit

153
00:06:23,340 --> 00:06:25,680
the issue of what can we trust

154
00:06:25,680 --> 00:06:28,259
if we use pre-built binaries in our

155
00:06:28,259 --> 00:06:30,600
build pipeline can we know that those

156
00:06:30,600 --> 00:06:32,400
aren't compromised

157
00:06:32,400 --> 00:06:34,139
and with that in mind some folks have

158
00:06:34,139 --> 00:06:35,160
started talking not only about

159
00:06:35,160 --> 00:06:37,800
reproducible but also bootstrapable

160
00:06:37,800 --> 00:06:40,680
builds where our entire build chain is

161
00:06:40,680 --> 00:06:43,560
also built and can be verified

162
00:06:43,560 --> 00:06:46,740
so before we build we build the thing we

163
00:06:46,740 --> 00:06:49,020
use to build but where does that

164
00:06:49,020 --> 00:06:51,060
actually stop

165
00:06:51,060 --> 00:06:54,240
can we trust pre-built operating systems

166
00:06:54,240 --> 00:06:56,520
now there's a trainer thought that even

167
00:06:56,520 --> 00:06:58,259
the smallest general purpose operating

168
00:06:58,259 --> 00:07:01,440
system is now too big to be auditable

169
00:07:01,440 --> 00:07:05,180
and verifiable by a human

170
00:07:05,220 --> 00:07:06,840
and there's lots of interesting work

171
00:07:06,840 --> 00:07:08,400
happening in this space with projects

172
00:07:08,400 --> 00:07:09,840
trying to build the smallest possible

173
00:07:09,840 --> 00:07:12,840
thing that can boot hardware and build

174
00:07:12,840 --> 00:07:14,460
compilers which can then build other

175
00:07:14,460 --> 00:07:16,319
things and so on

176
00:07:16,319 --> 00:07:17,940
and these are generally written in

177
00:07:17,940 --> 00:07:20,520
low-level languages with the aim to be

178
00:07:20,520 --> 00:07:23,039
human auditable so at least some

179
00:07:23,039 --> 00:07:25,319
programmers are capable of reading and

180
00:07:25,319 --> 00:07:27,539
understanding the entire code base and

181
00:07:27,539 --> 00:07:29,280
we're talking about really really tiny

182
00:07:29,280 --> 00:07:33,380
in the order of a few hundred bytes

183
00:07:33,960 --> 00:07:35,759
so now we're really off down the rabbit

184
00:07:35,759 --> 00:07:36,840
hole of trying to find something

185
00:07:36,840 --> 00:07:39,720
somewhere we can ultimately Trust

186
00:07:39,720 --> 00:07:41,520
um even if we can boot Hardware with

187
00:07:41,520 --> 00:07:43,860
something tiny that we can fully audit

188
00:07:43,860 --> 00:07:47,039
can we actually trust the hardware

189
00:07:47,039 --> 00:07:48,780
now you might say at this point that no

190
00:07:48,780 --> 00:07:50,340
one cares about Hardware anymore right

191
00:07:50,340 --> 00:07:52,680
we're all in the cloud but the cloud

192
00:07:52,680 --> 00:07:54,300
still and always will be just someone

193
00:07:54,300 --> 00:07:57,180
else's computers

194
00:07:57,180 --> 00:07:59,400
and the world of silicon is notoriously

195
00:07:59,400 --> 00:08:01,680
proprietary there's lots of proprietary

196
00:08:01,680 --> 00:08:04,500
features built into modern chipsets that

197
00:08:04,500 --> 00:08:07,139
you may never even know about and in the

198
00:08:07,139 --> 00:08:08,759
hardware space we're operating almost

199
00:08:08,759 --> 00:08:11,639
entirely on trust not just the chips

200
00:08:11,639 --> 00:08:13,800
themselves but all the tooling to design

201
00:08:13,800 --> 00:08:16,380
and build them is also proprietary and

202
00:08:16,380 --> 00:08:19,560
unavailable for us to verify

203
00:08:19,560 --> 00:08:20,940
and that's one of the things that's

204
00:08:20,940 --> 00:08:22,620
driving the creation of Open Source

205
00:08:22,620 --> 00:08:25,020
silicon and there's lots of interesting

206
00:08:25,020 --> 00:08:27,300
projects in this space and from open

207
00:08:27,300 --> 00:08:29,879
risk with the aim of creating a fully

208
00:08:29,879 --> 00:08:32,640
fledged open source processor to more

209
00:08:32,640 --> 00:08:34,919
specifically security focused projects

210
00:08:34,919 --> 00:08:37,500
like open Titan who are building an open

211
00:08:37,500 --> 00:08:39,779
source design for route of trust chips

212
00:08:39,779 --> 00:08:43,080
and for validating hardware and software

213
00:08:43,080 --> 00:08:45,300
and these projects are all about having

214
00:08:45,300 --> 00:08:48,000
those designs available for folks to

215
00:08:48,000 --> 00:08:51,500
verify and audit

216
00:08:52,640 --> 00:08:55,500
and there's an argument to be made that

217
00:08:55,500 --> 00:08:57,360
computer architectures have remained

218
00:08:57,360 --> 00:09:00,480
relatively unchanged for more than 30

219
00:09:00,480 --> 00:09:02,940
years yes they've got smaller they've

220
00:09:02,940 --> 00:09:05,339
got more powerful but fundamentally the

221
00:09:05,339 --> 00:09:08,040
way the CPU Works virtual memory paging

222
00:09:08,040 --> 00:09:10,500
multitasking operating systems are all

223
00:09:10,500 --> 00:09:12,720
quite similar and we're still using the

224
00:09:12,720 --> 00:09:15,420
same paradigms in programming

225
00:09:15,420 --> 00:09:18,240
and these architectures have features

226
00:09:18,240 --> 00:09:21,000
that could be considered contributory to

227
00:09:21,000 --> 00:09:23,100
certain classes of vulnerabilities

228
00:09:23,100 --> 00:09:27,140
particularly around memory safety

229
00:09:27,660 --> 00:09:29,459
so are there changes in computer

230
00:09:29,459 --> 00:09:31,620
architecture which can reduce the attack

231
00:09:31,620 --> 00:09:32,760
surface

232
00:09:32,760 --> 00:09:34,380
well there's a team at the University of

233
00:09:34,380 --> 00:09:35,760
Cambridge in the UK who've been

234
00:09:35,760 --> 00:09:38,160
developing a new instruction set Cherry

235
00:09:38,160 --> 00:09:40,380
capability Hardware enhanced risk

236
00:09:40,380 --> 00:09:42,120
instructions which is designed

237
00:09:42,120 --> 00:09:44,220
specifically to mitigate software

238
00:09:44,220 --> 00:09:47,240
security vulnerabilities

239
00:09:47,279 --> 00:09:48,839
and the way Cherry works is by

240
00:09:48,839 --> 00:09:50,519
introducing a new set of processor

241
00:09:50,519 --> 00:09:52,920
Primitives that provide a mechanism for

242
00:09:52,920 --> 00:09:55,440
fine-grained memory safety and process

243
00:09:55,440 --> 00:09:58,200
isolation directly in the hardware

244
00:09:58,200 --> 00:09:59,820
so it's a combination not just of

245
00:09:59,820 --> 00:10:02,279
Hardware but of tool chain support to

246
00:10:02,279 --> 00:10:03,540
reduce the amount of vulnerabilities

247
00:10:03,540 --> 00:10:05,760
that an attacker can exploit so

248
00:10:05,760 --> 00:10:07,500
basically the idea of least privilege

249
00:10:07,500 --> 00:10:09,899
but highly scalable highly efficient as

250
00:10:09,899 --> 00:10:13,399
it's done directly in the CPU

251
00:10:13,500 --> 00:10:15,300
and Cherry's been around as a concept

252
00:10:15,300 --> 00:10:17,339
since around 2010 but there's now

253
00:10:17,339 --> 00:10:19,500
Hardware that actually supports it in

254
00:10:19,500 --> 00:10:21,959
the form of this Amarillo board

255
00:10:21,959 --> 00:10:23,519
and there's lots of development going on

256
00:10:23,519 --> 00:10:25,980
to widen the ecosystem of software

257
00:10:25,980 --> 00:10:30,560
support for this new architecture

258
00:10:30,899 --> 00:10:33,240
so we've talked about cryptography we've

259
00:10:33,240 --> 00:10:34,920
talked about build systems we've talked

260
00:10:34,920 --> 00:10:37,140
about hardware and architecture but the

261
00:10:37,140 --> 00:10:38,940
real elephant in the room is the rise of

262
00:10:38,940 --> 00:10:41,220
artificial intelligence and specifically

263
00:10:41,220 --> 00:10:43,680
large language models now unless you've

264
00:10:43,680 --> 00:10:45,540
been living under a stone

265
00:10:45,540 --> 00:10:47,399
um you will have seen a lot of traffic

266
00:10:47,399 --> 00:10:49,500
about chat GPT since it was released

267
00:10:49,500 --> 00:10:51,600
back in November

268
00:10:51,600 --> 00:10:53,459
and the growth in users has been

269
00:10:53,459 --> 00:10:55,200
unprecedented and millions of people

270
00:10:55,200 --> 00:10:57,600
have been trying it out and you know

271
00:10:57,600 --> 00:11:00,000
whatever your your feelings about chat

272
00:11:00,000 --> 00:11:02,100
GPT in general if you actually used it

273
00:11:02,100 --> 00:11:04,200
it's clear pretty quickly that this

274
00:11:04,200 --> 00:11:06,540
technology is going to be truly

275
00:11:06,540 --> 00:11:08,040
transformational for how we interact

276
00:11:08,040 --> 00:11:10,680
with computers entire Industries are

277
00:11:10,680 --> 00:11:12,720
going to be disrupted by these abilities

278
00:11:12,720 --> 00:11:14,940
to write complex text based on limited

279
00:11:14,940 --> 00:11:17,660
human input

280
00:11:17,820 --> 00:11:19,800
and pretty quickly after the release

281
00:11:19,800 --> 00:11:21,420
people started to experiment with having

282
00:11:21,420 --> 00:11:23,940
chat gbt write code and again the

283
00:11:23,940 --> 00:11:24,899
results here have been pretty

284
00:11:24,899 --> 00:11:27,000
extraordinary given relatively small

285
00:11:27,000 --> 00:11:29,640
inputs chat GPT is already capable of

286
00:11:29,640 --> 00:11:31,620
writing mostly correct complex

287
00:11:31,620 --> 00:11:34,279
applications in multiple languages

288
00:11:34,279 --> 00:11:37,800
manipulating data between formats and

289
00:11:37,800 --> 00:11:39,600
translating programs between different

290
00:11:39,600 --> 00:11:41,700
programming languages and even writing

291
00:11:41,700 --> 00:11:43,320
programs in fictional programming

292
00:11:43,320 --> 00:11:45,680
languages

293
00:11:45,899 --> 00:11:47,640
and whilst it's not about to displace

294
00:11:47,640 --> 00:11:50,519
programmers just yet the field is moving

295
00:11:50,519 --> 00:11:52,980
incredibly quickly we're really only a

296
00:11:52,980 --> 00:11:55,200
few weeks into the AI Revolution and

297
00:11:55,200 --> 00:11:57,060
it's already clear that's going to drive

298
00:11:57,060 --> 00:12:00,060
massive change these models can't reach

299
00:12:00,060 --> 00:12:01,620
out to other systems yet or to the

300
00:12:01,620 --> 00:12:03,600
internet but that'll almost certainly

301
00:12:03,600 --> 00:12:06,079
change

302
00:12:06,240 --> 00:12:07,860
and a lot of researchers have been

303
00:12:07,860 --> 00:12:09,600
talking about the idea of conversational

304
00:12:09,600 --> 00:12:12,180
programming where we interact with

305
00:12:12,180 --> 00:12:14,760
models based on what output we need and

306
00:12:14,760 --> 00:12:17,160
not on how to achieve the result

307
00:12:17,160 --> 00:12:19,140
and the AI model will get to that result

308
00:12:19,140 --> 00:12:22,019
by any means it deems appropriate

309
00:12:22,019 --> 00:12:23,519
um but this raises some kind of

310
00:12:23,519 --> 00:12:25,079
fundamental questions about the future

311
00:12:25,079 --> 00:12:28,200
of application software

312
00:12:28,200 --> 00:12:30,480
if the future involves computers writing

313
00:12:30,480 --> 00:12:32,880
programs for us where we only really

314
00:12:32,880 --> 00:12:35,160
care about the output then the question

315
00:12:35,160 --> 00:12:37,140
arises about why computers would use

316
00:12:37,140 --> 00:12:39,959
high-level programming languages at all

317
00:12:39,959 --> 00:12:41,040
um commuters don't know anything about

318
00:12:41,040 --> 00:12:43,079
programming languages languages are

319
00:12:43,079 --> 00:12:44,880
basically abstractions to make it easier

320
00:12:44,880 --> 00:12:46,920
for human programmers to program

321
00:12:46,920 --> 00:12:48,959
computers everything's either

322
00:12:48,959 --> 00:12:51,360
interpreted or compiled down to

323
00:12:51,360 --> 00:12:53,760
something the computer can actually run

324
00:12:53,760 --> 00:12:56,160
so since such a huge proportion of our

325
00:12:56,160 --> 00:12:58,139
issues with software supply chain at the

326
00:12:58,139 --> 00:13:00,000
minute come from how we assemble

327
00:13:00,000 --> 00:13:02,899
applications from packages and libraries

328
00:13:02,899 --> 00:13:06,000
perhaps the era of AI generated programs

329
00:13:06,000 --> 00:13:09,600
will solve that problem for us

330
00:13:09,600 --> 00:13:12,180
most likely though it just introduces a

331
00:13:12,180 --> 00:13:14,579
whole new raft of security issues we're

332
00:13:14,579 --> 00:13:16,200
still back to our starting point about

333
00:13:16,200 --> 00:13:19,920
trust can we trust our AI model and more

334
00:13:19,920 --> 00:13:21,540
importantly is it plotting a robot

335
00:13:21,540 --> 00:13:24,720
takeover of the human race

336
00:13:24,720 --> 00:13:27,540
so to return to the disclaimer some or

337
00:13:27,540 --> 00:13:29,940
all of this talk is highly likely to not

338
00:13:29,940 --> 00:13:32,760
come true predictions are notoriously

339
00:13:32,760 --> 00:13:35,220
difficult in technology and yes this was

340
00:13:35,220 --> 00:13:39,779
me 30 years ago so what do I know

341
00:13:39,779 --> 00:13:40,980
um but I'd like to leave you with a

342
00:13:40,980 --> 00:13:42,720
quote from that famous future allergist

343
00:13:42,720 --> 00:13:44,820
Dr Emmett Brown

344
00:13:44,820 --> 00:13:46,260
um your future is whatever you make it

345
00:13:46,260 --> 00:13:48,840
so make it a good one thank you

346
00:13:48,840 --> 00:13:56,889
[Applause]

