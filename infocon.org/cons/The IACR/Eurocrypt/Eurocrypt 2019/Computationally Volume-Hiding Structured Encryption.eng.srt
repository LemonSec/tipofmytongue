1
00:00:01,710 --> 00:00:07,810
thank you for the introduction so my

2
00:00:05,410 --> 00:00:09,310
talk today will be about how to suppress

3
00:00:07,810 --> 00:00:10,990
volume leakage in structure the

4
00:00:09,310 --> 00:00:12,850
encryption using conventional

5
00:00:10,990 --> 00:00:17,259
assumptions this is a joint work with

6
00:00:12,850 --> 00:00:18,640
cynic Amara ok let's start with first

7
00:00:17,260 --> 00:00:21,340
recording what structure the encryption

8
00:00:18,640 --> 00:00:24,280
or AC for short so AC is a cryptographic

9
00:00:21,340 --> 00:00:26,439
primitive that was introduced by chase

10
00:00:24,280 --> 00:00:28,360
and camera in 2010 that allows the user

11
00:00:26,440 --> 00:00:31,449
to encrypt the data structure in such a

12
00:00:28,360 --> 00:00:34,449
way that it can privately create later

13
00:00:31,449 --> 00:00:36,850
on and in particular there is a set of

14
00:00:34,449 --> 00:00:38,260
algorithm that takes as input a security

15
00:00:36,850 --> 00:00:40,420
parameter and the data structure and

16
00:00:38,260 --> 00:00:42,519
output a key and an encrypted data

17
00:00:40,420 --> 00:00:45,309
structure and the encrypted data

18
00:00:42,519 --> 00:00:47,469
actually sent to the server and as query

19
00:00:45,309 --> 00:00:49,989
time the user will run a token algorithm

20
00:00:47,469 --> 00:00:52,780
that takes as input a key and the query

21
00:00:49,989 --> 00:00:56,138
and we'll also put a token the token is

22
00:00:52,780 --> 00:00:58,179
sent to the server and once the server

23
00:00:56,139 --> 00:01:01,569
receives the token it will run a query

24
00:00:58,179 --> 00:01:03,760
algorithm that takes as inputs the token

25
00:01:01,569 --> 00:01:06,189
and encrypted data structure and outputs

26
00:01:03,760 --> 00:01:09,820
an answer and the answer is sent to the

27
00:01:06,189 --> 00:01:12,610
to the user okay so we call the

28
00:01:09,820 --> 00:01:15,579
information that the server learns a set

29
00:01:12,610 --> 00:01:17,079
up time the set up leakage and it may

30
00:01:15,579 --> 00:01:19,389
include for example the size of the data

31
00:01:17,079 --> 00:01:21,729
structure and we call the information

32
00:01:19,390 --> 00:01:23,619
that the server learns at query time the

33
00:01:21,729 --> 00:01:25,509
query leakage and it may include for

34
00:01:23,619 --> 00:01:28,450
example the search pattern or the access

35
00:01:25,509 --> 00:01:31,810
path and at very high level we say that

36
00:01:28,450 --> 00:01:34,179
a SCE scheme is smq secure if for one it

37
00:01:31,810 --> 00:01:35,710
doesn't reveal any information about the

38
00:01:34,179 --> 00:01:38,259
data structure which beyond the set up

39
00:01:35,710 --> 00:01:40,089
leakage and for - it doesn't reveal any

40
00:01:38,259 --> 00:01:42,850
information about the data structure and

41
00:01:40,090 --> 00:01:44,979
queries beyond the query leakage and for

42
00:01:42,850 --> 00:01:49,210
more detailed security definition please

43
00:01:44,979 --> 00:01:51,640
refer to to the secret and paper okay so

44
00:01:49,210 --> 00:01:53,048
when designing or analyzing structure

45
00:01:51,640 --> 00:01:55,509
the encryption schemes there are three

46
00:01:53,049 --> 00:01:57,880
main dimensions that one should pay

47
00:01:55,509 --> 00:02:02,109
attention to which are efficiency

48
00:01:57,880 --> 00:02:04,030
security and expressiveness and in fact

49
00:02:02,109 --> 00:02:06,640
structure the encryption has greatly

50
00:02:04,030 --> 00:02:08,619
evolved during the last two decades and

51
00:02:06,640 --> 00:02:10,780
there are a lot of works that have

52
00:02:08,619 --> 00:02:14,170
investigated for example how to design

53
00:02:10,780 --> 00:02:15,370
efficient scheme or how to design more

54
00:02:14,170 --> 00:02:19,238
expressive schemes

55
00:02:15,370 --> 00:02:25,930
or how to attack or design more secure

56
00:02:19,239 --> 00:02:27,879
schemes however one aspect that was that

57
00:02:25,930 --> 00:02:29,790
didn't receive a lot of attention or and

58
00:02:27,879 --> 00:02:31,959
is poorly understood is leakage and

59
00:02:29,790 --> 00:02:34,540
actually there are three main directions

60
00:02:31,959 --> 00:02:36,430
that one can reason about leakage and

61
00:02:34,540 --> 00:02:39,760
can help us to better understand it

62
00:02:36,430 --> 00:02:41,500
which are crypt analysis measure and

63
00:02:39,760 --> 00:02:43,920
separation I'm going to detail each of

64
00:02:41,500 --> 00:02:47,019
these directions in the following slides

65
00:02:43,920 --> 00:02:50,078
so so what do we mean by crypt analysis

66
00:02:47,019 --> 00:02:52,209
in encrypted search so it means the

67
00:02:50,079 --> 00:02:54,280
following so given a leakage profile we

68
00:02:52,209 --> 00:02:57,430
want to design attacks that recover

69
00:02:54,280 --> 00:03:00,280
either the queries or and the data under

70
00:02:57,430 --> 00:03:02,829
some assumptions and the goal of crypt

71
00:03:00,280 --> 00:03:05,709
analysis is that we want to empirically

72
00:03:02,829 --> 00:03:08,769
learn the impact of a specific leakage

73
00:03:05,709 --> 00:03:11,440
profile in the real world however the

74
00:03:08,769 --> 00:03:14,920
main limitations of crypt analysis is

75
00:03:11,440 --> 00:03:19,060
that the gap between the assumptions and

76
00:03:14,920 --> 00:03:20,858
reality can get white okay so the second

77
00:03:19,060 --> 00:03:22,630
direction is a measure and what do we

78
00:03:20,859 --> 00:03:25,329
mean by this is that given a leakage

79
00:03:22,630 --> 00:03:28,299
profile we want to somehow quantify for

80
00:03:25,329 --> 00:03:30,100
example in bits the specific leakage

81
00:03:28,299 --> 00:03:32,500
pattern and the goal here is that we

82
00:03:30,100 --> 00:03:34,450
want to theoretically compare between

83
00:03:32,500 --> 00:03:37,599
leakage patterns but the main

84
00:03:34,450 --> 00:03:39,488
limitations of this approach is maybe

85
00:03:37,599 --> 00:03:41,048
there is no meaning for a total order

86
00:03:39,489 --> 00:03:43,450
and this is something that we are

87
00:03:41,049 --> 00:03:45,370
currently working on and the third

88
00:03:43,450 --> 00:03:46,899
dimension which is suppression which we

89
00:03:45,370 --> 00:03:49,269
believe it's one of the most important

90
00:03:46,900 --> 00:03:51,280
direction means the following

91
00:03:49,269 --> 00:03:54,160
so given a leakage profile we want to

92
00:03:51,280 --> 00:03:56,470
design a compiler or transform that will

93
00:03:54,160 --> 00:03:58,269
suppress a specific leakage fashion and

94
00:03:56,470 --> 00:04:01,150
the goal here is we want to design these

95
00:03:58,269 --> 00:04:03,430
tools that will suppress various leakage

96
00:04:01,150 --> 00:04:05,500
patterns however the main limitations of

97
00:04:03,430 --> 00:04:10,569
this approach is that it will incur some

98
00:04:05,500 --> 00:04:12,190
overhead okay and as the as this orc is

99
00:04:10,569 --> 00:04:14,470
about ticket suppression i will recall

100
00:04:12,190 --> 00:04:17,978
two main approaches that were introduced

101
00:04:14,470 --> 00:04:19,959
by that we introduced last year as

102
00:04:17,978 --> 00:04:21,370
crypto which are compilation and data

103
00:04:19,959 --> 00:04:23,349
structure transformations so let's start

104
00:04:21,370 --> 00:04:26,110
with compilation so computation is a

105
00:04:23,349 --> 00:04:28,000
mechanism that takes a structure

106
00:04:26,110 --> 00:04:29,050
encryption scheme as input with the

107
00:04:28,000 --> 00:04:31,000
leakage profile

108
00:04:29,050 --> 00:04:32,979
such that the query leakage is equal to

109
00:04:31,000 --> 00:04:35,650
two passions person one and pattern two

110
00:04:32,979 --> 00:04:37,599
and the computation will output a new

111
00:04:35,650 --> 00:04:39,849
structure the encryption scheme with a

112
00:04:37,599 --> 00:04:41,860
new leakage profile and a prime such

113
00:04:39,849 --> 00:04:43,210
that the query leakage is equal only to

114
00:04:41,860 --> 00:04:44,979
pattern one so that is we have

115
00:04:43,210 --> 00:04:46,840
suppressed pattern 2

116
00:04:44,979 --> 00:04:48,909
so the second approach is the data

117
00:04:46,840 --> 00:04:52,599
structure transformation which works as

118
00:04:48,909 --> 00:04:54,639
follows so given a data structure ok we

119
00:04:52,599 --> 00:04:56,650
will apply our transform that will also

120
00:04:54,639 --> 00:04:58,360
put a new data structure the star that

121
00:04:56,650 --> 00:05:00,429
we will give as input to a structural

122
00:04:58,360 --> 00:05:02,020
encryption scheme and itself it will

123
00:05:00,430 --> 00:05:03,430
output an encrypted data structure and

124
00:05:02,020 --> 00:05:05,409
the structure of the encryption scheme

125
00:05:03,430 --> 00:05:07,389
here has a leakage profile and a-- but

126
00:05:05,409 --> 00:05:10,150
the resulting scheme that does include

127
00:05:07,389 --> 00:05:12,039
the transform inside will have leakage

128
00:05:10,150 --> 00:05:13,929
profile lambda Prime's such that the

129
00:05:12,039 --> 00:05:17,800
query leakage is only equal to person 1

130
00:05:13,930 --> 00:05:20,620
so we have suppressed a person potential

131
00:05:17,800 --> 00:05:22,840
so a valid question to ask is that are

132
00:05:20,620 --> 00:05:24,930
there any other approaches actually to

133
00:05:22,840 --> 00:05:28,090
suppress leakage and the answer is yes

134
00:05:24,930 --> 00:05:30,280
there are and as I have just discussed

135
00:05:28,090 --> 00:05:31,750
the kmo paper introduced two approaches

136
00:05:30,280 --> 00:05:33,549
the blackbox compilation and data

137
00:05:31,750 --> 00:05:36,039
structure transformation where the

138
00:05:33,550 --> 00:05:38,229
deepest raster transformation actually

139
00:05:36,039 --> 00:05:40,300
suppresses a leakage pattern against an

140
00:05:38,229 --> 00:05:41,500
unbounded adversary so in this work we

141
00:05:40,300 --> 00:05:43,300
will consider a new data structure

142
00:05:41,500 --> 00:05:47,460
transformation that will suppress

143
00:05:43,300 --> 00:05:50,110
leakage against a bounded adversary so

144
00:05:47,460 --> 00:05:52,840
this this is actually important and one

145
00:05:50,110 --> 00:05:54,759
of the major findings in our work stems

146
00:05:52,840 --> 00:05:58,210
from the question of whether actually it

147
00:05:54,759 --> 00:06:00,639
is possible to design structural

148
00:05:58,210 --> 00:06:02,859
encryption scheme that actually do leak

149
00:06:00,639 --> 00:06:04,779
something such that an unbounded

150
00:06:02,860 --> 00:06:07,449
adversity can still learn the leakage

151
00:06:04,779 --> 00:06:09,909
pattern but an abounded adversity cannot

152
00:06:07,449 --> 00:06:12,849
and if we go back to our data structure

153
00:06:09,909 --> 00:06:15,009
transformation slide this new transform

154
00:06:12,849 --> 00:06:16,240
is very similar actually a chart so

155
00:06:15,009 --> 00:06:17,560
there is a data structure there is a

156
00:06:16,240 --> 00:06:19,779
transform that will output a data

157
00:06:17,560 --> 00:06:21,400
structure that's given as input a

158
00:06:19,779 --> 00:06:23,409
structure the encryption scheme which

159
00:06:21,400 --> 00:06:25,929
itself outputs an encrypted data

160
00:06:23,409 --> 00:06:28,779
structure but now the resulting

161
00:06:25,930 --> 00:06:30,400
structure the encryption scheme does Li

162
00:06:28,779 --> 00:06:32,610
Kui leak is sill composed of two

163
00:06:30,400 --> 00:06:36,460
patterns pattern 1 and patterns star

164
00:06:32,610 --> 00:06:38,469
where patterns are now in the eyes of a

165
00:06:36,460 --> 00:06:41,630
bounded adversary is actually equivalent

166
00:06:38,469 --> 00:06:44,840
to Nasik okay

167
00:06:41,630 --> 00:06:46,490
and in the remaining parts of this talk

168
00:06:44,840 --> 00:06:48,830
I will show how to leverage this new

169
00:06:46,490 --> 00:06:50,690
capability to designs SCE schemes that

170
00:06:48,830 --> 00:06:52,219
will hide the response length I will

171
00:06:50,690 --> 00:06:53,990
detail what the response length means in

172
00:06:52,220 --> 00:06:55,310
the next two slides that I'm the

173
00:06:53,990 --> 00:06:58,280
dedication actually to some brief

174
00:06:55,310 --> 00:07:00,440
background so let's start by a simple

175
00:06:58,280 --> 00:07:02,330
yet from the model data structure recap

176
00:07:00,440 --> 00:07:04,010
what we mean by a dictionary data

177
00:07:02,330 --> 00:07:08,120
structure is a data structure that Maps

178
00:07:04,010 --> 00:07:10,580
a label to a value here we map the

179
00:07:08,120 --> 00:07:12,290
keyword to a file identifier and there

180
00:07:10,580 --> 00:07:14,479
is a gate operation that's given a label

181
00:07:12,290 --> 00:07:16,610
it will output the corresponding value a

182
00:07:14,480 --> 00:07:18,380
Meucci map data structure is a data

183
00:07:16,610 --> 00:07:20,210
structure that map's a label to a tuple

184
00:07:18,380 --> 00:07:22,520
of values for example here we map a

185
00:07:20,210 --> 00:07:25,310
keyword to a tuple of file identifiers

186
00:07:22,520 --> 00:07:27,799
similarly there is a gate operation

187
00:07:25,310 --> 00:07:30,110
that's given a label it will output the

188
00:07:27,800 --> 00:07:32,000
corresponding tuple so the response

189
00:07:30,110 --> 00:07:35,480
length pattern is known in searchable

190
00:07:32,000 --> 00:07:37,130
symmetric encryption volume pattern and

191
00:07:35,480 --> 00:07:39,230
it's very simple it's a pattern that

192
00:07:37,130 --> 00:07:41,659
occurs at query time which is simply

193
00:07:39,230 --> 00:07:43,180
equal to the length of the response or

194
00:07:41,660 --> 00:07:46,670
the answer

195
00:07:43,180 --> 00:07:48,700
so how the respondent is actually very

196
00:07:46,670 --> 00:07:52,070
challenging especially if we want to

197
00:07:48,700 --> 00:07:54,050
preserve efficiency and what I'm going

198
00:07:52,070 --> 00:07:58,280
to do in this slide or the upcoming

199
00:07:54,050 --> 00:08:01,010
slides is to show two ways to actually

200
00:07:58,280 --> 00:08:02,830
hide the the volume pattern but as you

201
00:08:01,010 --> 00:08:05,990
can see there will be very inefficient

202
00:08:02,830 --> 00:08:08,240
so the first approach we call the knife

203
00:08:05,990 --> 00:08:10,910
padding approach so we take a multi map

204
00:08:08,240 --> 00:08:13,790
data structure and we will just pad it

205
00:08:10,910 --> 00:08:15,790
add dummies to all tuples in such a way

206
00:08:13,790 --> 00:08:18,620
that our respondents will have the same

207
00:08:15,790 --> 00:08:20,750
the same all the tuples will have the

208
00:08:18,620 --> 00:08:23,060
same length sorry and then we take this

209
00:08:20,750 --> 00:08:24,770
transform data structure mm Prime and we

210
00:08:23,060 --> 00:08:26,900
will feed it to a multi map encryption

211
00:08:24,770 --> 00:08:28,580
scheme any standard multi map encryption

212
00:08:26,900 --> 00:08:30,440
scheme that does actually leak the

213
00:08:28,580 --> 00:08:33,799
response length and the query quality

214
00:08:30,440 --> 00:08:35,599
here qAQ refers to the search pattern in

215
00:08:33,799 --> 00:08:38,929
SS silat sociable symmetric encryption

216
00:08:35,599 --> 00:08:40,760
literature so the resulting scheme SC e

217
00:08:38,929 --> 00:08:42,559
prime is a multi map encryption scheme

218
00:08:40,760 --> 00:08:45,860
that actually hides the response and is

219
00:08:42,559 --> 00:08:48,020
actually easy to verify this so in terms

220
00:08:45,860 --> 00:08:49,580
of asan projects the cool complexity of

221
00:08:48,020 --> 00:08:52,100
such a approach is not it's not that

222
00:08:49,580 --> 00:08:55,220
good the query complexity is actually

223
00:08:52,100 --> 00:08:56,690
linear in the maximum response laugh

224
00:08:55,220 --> 00:08:58,700
and the storage complexity actually is

225
00:08:56,690 --> 00:09:00,470
terrible it's actually linear in the

226
00:08:58,700 --> 00:09:02,630
number of labels I'm the maximum

227
00:09:00,470 --> 00:09:04,490
response length but the good thing about

228
00:09:02,630 --> 00:09:06,260
this approach is that it's non

229
00:09:04,490 --> 00:09:07,640
interactive and as I'm going to detail

230
00:09:06,260 --> 00:09:09,920
the second approach which is actually

231
00:09:07,640 --> 00:09:11,750
interactive this is actually a plus so

232
00:09:09,920 --> 00:09:15,530
the second approach that we call through

233
00:09:11,750 --> 00:09:17,960
a free dictionary approach we will start

234
00:09:15,530 --> 00:09:21,040
again with a multi map data structure

235
00:09:17,960 --> 00:09:23,420
and we will transform it to a dictionary

236
00:09:21,040 --> 00:09:26,180
and then we take this dictionary

237
00:09:23,420 --> 00:09:27,890
structure and we will add dummies to it

238
00:09:26,180 --> 00:09:29,510
and the number of them is that we will

239
00:09:27,890 --> 00:09:31,420
add to this dictionary data structure is

240
00:09:29,510 --> 00:09:34,400
equal to the maximum response length

241
00:09:31,420 --> 00:09:35,689
minus one and then we will feed this

242
00:09:34,400 --> 00:09:37,910
dictionary data structure to a

243
00:09:35,690 --> 00:09:40,340
dictionary encryption scheme that has

244
00:09:37,910 --> 00:09:43,880
the property of being leakage free and

245
00:09:40,340 --> 00:09:46,280
we can instantiate such such primitive

246
00:09:43,880 --> 00:09:47,870
using oblivious Ram in such a way that

247
00:09:46,280 --> 00:09:50,360
the resulting structure the encryption

248
00:09:47,870 --> 00:09:54,470
scheme will hide the response length as

249
00:09:50,360 --> 00:09:56,810
long as whenever we fetch a tuple we fit

250
00:09:54,470 --> 00:09:59,210
along with it a number of dummies in

251
00:09:56,810 --> 00:10:00,560
such a way that the total number of

252
00:09:59,210 --> 00:10:02,930
values with which is equal to the

253
00:10:00,560 --> 00:10:05,060
maximum response length so in terms of

254
00:10:02,930 --> 00:10:07,099
asymptotic this naive approach is also

255
00:10:05,060 --> 00:10:09,459
actually it's worse in terms of query

256
00:10:07,100 --> 00:10:13,370
complexity compared to the naive padding

257
00:10:09,460 --> 00:10:15,260
but it has great storage complexity but

258
00:10:13,370 --> 00:10:18,730
it's also interactive which can be bad

259
00:10:15,260 --> 00:10:22,189
for some scenarios ok

260
00:10:18,730 --> 00:10:23,600
so a right question to ask is can we

261
00:10:22,190 --> 00:10:25,310
actually achieve the best of both worlds

262
00:10:23,600 --> 00:10:29,300
and the answer to this question is

263
00:10:25,310 --> 00:10:31,310
easiest and in the remaining parts of

264
00:10:29,300 --> 00:10:33,229
this talk I will detail the pseudo

265
00:10:31,310 --> 00:10:35,180
random transform or PRT for source which

266
00:10:33,230 --> 00:10:37,040
is a data structure transformation that

267
00:10:35,180 --> 00:10:38,540
has better query complexity and storage

268
00:10:37,040 --> 00:10:40,969
complexity when compared to the knife

269
00:10:38,540 --> 00:10:44,959
padding approach but it has the

270
00:10:40,970 --> 00:10:46,580
disadvantage of being lossy I won't have

271
00:10:44,960 --> 00:10:48,530
time to talk about vlh which is the

272
00:10:46,580 --> 00:10:50,750
structure the encryption that's Dutch

273
00:10:48,530 --> 00:10:52,189
builds on top of of PRT that hides the

274
00:10:50,750 --> 00:10:53,630
response times but it's actually

275
00:10:52,190 --> 00:10:55,700
straight forward from the PRG

276
00:10:53,630 --> 00:10:58,010
given the slide that I have presented on

277
00:10:55,700 --> 00:11:00,080
the data structure transformation I will

278
00:10:58,010 --> 00:11:01,850
detail though the the density sub graph

279
00:11:00,080 --> 00:11:04,070
transform or DSC for source which is a

280
00:11:01,850 --> 00:11:06,980
data structure transformation that

281
00:11:04,070 --> 00:11:08,660
actually it's non lossy that has better

282
00:11:06,980 --> 00:11:09,670
actually storage overhead but it has

283
00:11:08,660 --> 00:11:12,439
quite

284
00:11:09,670 --> 00:11:14,780
worse query complexity compared to the

285
00:11:12,440 --> 00:11:16,910
to the PRT I won't have time

286
00:11:14,780 --> 00:11:18,500
unfortunately to talk about AVL H which

287
00:11:16,910 --> 00:11:22,030
is the structural encryption that we

288
00:11:18,500 --> 00:11:24,230
build on top of the of the of DSC and

289
00:11:22,030 --> 00:11:26,329
also I won't have time to talk about how

290
00:11:24,230 --> 00:11:28,790
to make village and a village dynamic

291
00:11:26,330 --> 00:11:31,040
but please refer to our paper to learn

292
00:11:28,790 --> 00:11:33,709
more about these three parts that I'm

293
00:11:31,040 --> 00:11:35,780
not covering okay so let's start with

294
00:11:33,710 --> 00:11:37,670
PRT so please don't pay attention to the

295
00:11:35,780 --> 00:11:39,439
formalism for now as I'm going to

296
00:11:37,670 --> 00:11:41,750
describe this transform through the

297
00:11:39,440 --> 00:11:45,380
following illustration so we have two

298
00:11:41,750 --> 00:11:47,120
parameters lambda and mu where lambda is

299
00:11:45,380 --> 00:11:50,150
the minimum response length and nu is

300
00:11:47,120 --> 00:11:52,070
the output size of a PRF so we take our

301
00:11:50,150 --> 00:11:54,290
mu Qi map and the PRC will output a new

302
00:11:52,070 --> 00:11:56,950
multi map data structure such that the

303
00:11:54,290 --> 00:12:00,380
new response length is equal to lambda

304
00:11:56,950 --> 00:12:03,500
to which we add a PRF evaluation on the

305
00:12:00,380 --> 00:12:05,930
keyword concatenated to the old response

306
00:12:03,500 --> 00:12:08,240
length and given this sum we will make

307
00:12:05,930 --> 00:12:11,060
the decision whether to pad or truncate

308
00:12:08,240 --> 00:12:14,600
a specific tube so let's get an example

309
00:12:11,060 --> 00:12:19,010
so for keyword 1 W 1 the response length

310
00:12:14,600 --> 00:12:21,980
is equal to tree ID 1 ID 3 ID 4 okay so

311
00:12:19,010 --> 00:12:24,710
we will evaluate the PRF on keyword 1

312
00:12:21,980 --> 00:12:26,390
concatenate it to tree which is here for

313
00:12:24,710 --> 00:12:28,730
example is equal to 0 it's just for a

314
00:12:26,390 --> 00:12:31,670
second of an example and we take this 0

315
00:12:28,730 --> 00:12:33,830
and we add it to 1 where 1 here is the

316
00:12:31,670 --> 00:12:37,280
the value that we we we set for lambda

317
00:12:33,830 --> 00:12:38,690
and the result is 1 so we compare 1 to 3

318
00:12:37,280 --> 00:12:40,640
which is the old response length and we

319
00:12:38,690 --> 00:12:42,530
see that it's smaller and actually we

320
00:12:40,640 --> 00:12:45,050
have to truncate the tuple with with two

321
00:12:42,530 --> 00:12:46,880
values so and this is why we have

322
00:12:45,050 --> 00:12:49,370
removed two values from in the new

323
00:12:46,880 --> 00:12:51,020
medium for keyword two however so we

324
00:12:49,370 --> 00:12:53,930
will apply we do the same we apply the

325
00:12:51,020 --> 00:12:55,579
PRF the result now is 2 we added 2 1 is

326
00:12:53,930 --> 00:12:57,620
3 and we know that tree is actually

327
00:12:55,580 --> 00:13:00,320
larger than 1 so we have two paths

328
00:12:57,620 --> 00:13:04,550
so we pad we add dummies and for qhv we

329
00:13:00,320 --> 00:13:06,530
do the same and we in PR T we also have

330
00:13:04,550 --> 00:13:08,839
this ranking function which is basically

331
00:13:06,530 --> 00:13:11,839
ranking the file identifier within a

332
00:13:08,840 --> 00:13:15,020
tuple and the purpose of this of this

333
00:13:11,840 --> 00:13:17,690
ranking function is that it will allow

334
00:13:15,020 --> 00:13:20,230
us to lose only the less significant

335
00:13:17,690 --> 00:13:22,910
file identifiers and we will preserve

336
00:13:20,230 --> 00:13:26,710
the most significant which is

337
00:13:22,910 --> 00:13:30,800
important from a practical point of view

338
00:13:26,710 --> 00:13:32,090
so well commissioned to ask us about the

339
00:13:30,800 --> 00:13:34,550
number of truncation and was about

340
00:13:32,090 --> 00:13:37,520
storage overhead and for this analysis

341
00:13:34,550 --> 00:13:39,260
we had to assume a special type of Milky

342
00:13:37,520 --> 00:13:42,380
maps that we call Z distributed Milky

343
00:13:39,260 --> 00:13:43,730
maps that I'm going to detail now so

344
00:13:42,380 --> 00:13:45,439
what do we mean by this so as if

345
00:13:43,730 --> 00:13:48,050
distributed Mickey map has a response

346
00:13:45,440 --> 00:13:49,130
lands that are as if distributed where

347
00:13:48,050 --> 00:13:51,079
the ARF

348
00:13:49,130 --> 00:13:53,180
response land is equal to to this

349
00:13:51,080 --> 00:13:54,920
formula it's not important to parse it

350
00:13:53,180 --> 00:13:56,510
actually the illustration gives you an

351
00:13:54,920 --> 00:13:59,300
idea about how the response lands are

352
00:13:56,510 --> 00:14:01,310
distributed so the choice of zips

353
00:13:59,300 --> 00:14:02,780
distribution was not arbitrary actually

354
00:14:01,310 --> 00:14:05,780
that it is very common to finds if

355
00:14:02,780 --> 00:14:08,630
distributed data sets in in practice and

356
00:14:05,780 --> 00:14:10,400
actually we have run a neuron just to

357
00:14:08,630 --> 00:14:12,560
check the response land distribution a

358
00:14:10,400 --> 00:14:14,990
neuron is a data set composite of half a

359
00:14:12,560 --> 00:14:18,380
million roughly emails and as you can

360
00:14:14,990 --> 00:14:20,510
see the the graph here the response

361
00:14:18,380 --> 00:14:22,189
length distribution is a power law or as

362
00:14:20,510 --> 00:14:27,530
if distribution with some distribution

363
00:14:22,190 --> 00:14:29,900
sorry with some parameter okay so this

364
00:14:27,530 --> 00:14:32,390
slide basically summarized our main

365
00:14:29,900 --> 00:14:34,040
finding it's not I don't want you to be

366
00:14:32,390 --> 00:14:35,870
distracted with the details but the main

367
00:14:34,040 --> 00:14:38,480
takeaway from this slide is we were able

368
00:14:35,870 --> 00:14:40,940
to show that the storage overhead of PRT

369
00:14:38,480 --> 00:14:42,470
is actually alpha times the storage

370
00:14:40,940 --> 00:14:44,420
overhead of the naive approach where

371
00:14:42,470 --> 00:14:46,640
alpha is strictly smaller than one and

372
00:14:44,420 --> 00:14:48,910
you can think of alpha as the storage

373
00:14:46,640 --> 00:14:51,949
with a reduction multiplicative factor

374
00:14:48,910 --> 00:14:54,740
with respect to truncations we were able

375
00:14:51,950 --> 00:14:57,650
to show that truncations are actually

376
00:14:54,740 --> 00:15:00,530
sub linear in the number of labels equal

377
00:14:57,650 --> 00:15:02,240
to this this this formula here I just

378
00:15:00,530 --> 00:15:04,280
want to note that this analysis is not

379
00:15:02,240 --> 00:15:06,200
tied specifically to use if it can be we

380
00:15:04,280 --> 00:15:09,260
can do the same analysis for power law

381
00:15:06,200 --> 00:15:11,270
distribution and most probably we will

382
00:15:09,260 --> 00:15:13,370
have the same results but just to give

383
00:15:11,270 --> 00:15:14,810
you an idea so as you can see that we

384
00:15:13,370 --> 00:15:16,730
have some strong occasions avoid

385
00:15:14,810 --> 00:15:18,650
question to ask how can we get rid of

386
00:15:16,730 --> 00:15:20,660
this truncation is it possible to have a

387
00:15:18,650 --> 00:15:24,319
data structure transformation that does

388
00:15:20,660 --> 00:15:26,209
not actually incur truncations and this

389
00:15:24,320 --> 00:15:28,400
is what we are going to cover with the

390
00:15:26,210 --> 00:15:29,990
DSC or density sub graph transform so

391
00:15:28,400 --> 00:15:33,410
the name will make sense

392
00:15:29,990 --> 00:15:35,520
I promise at the end of this talk so so

393
00:15:33,410 --> 00:15:38,280
we take a museum up data structure

394
00:15:35,520 --> 00:15:40,170
we view the museum up inside as a

395
00:15:38,280 --> 00:15:42,569
bipartite graph where the top vertices

396
00:15:40,170 --> 00:15:44,520
are are the key words in the multi map

397
00:15:42,570 --> 00:15:48,000
or the labels in the Meucci maps and the

398
00:15:44,520 --> 00:15:49,710
bottom vertices are actually bins we

399
00:15:48,000 --> 00:15:52,800
have here for the sake of example four

400
00:15:49,710 --> 00:15:54,570
bins we do all actually instance eh also

401
00:15:52,800 --> 00:15:56,069
as states data structure think of it as

402
00:15:54,570 --> 00:15:58,920
a dictionary data structure that we will

403
00:15:56,070 --> 00:16:02,760
keep at the client side and you will see

404
00:15:58,920 --> 00:16:04,969
how how to transform work works so we

405
00:16:02,760 --> 00:16:09,630
take our first keyword and we will

406
00:16:04,970 --> 00:16:12,570
randomly pick three bins uniformly at

407
00:16:09,630 --> 00:16:14,370
random and so why three three is the

408
00:16:12,570 --> 00:16:16,170
maximum response length and actually

409
00:16:14,370 --> 00:16:17,730
something that I forgot to mention is

410
00:16:16,170 --> 00:16:19,349
that through this bipartite graph what

411
00:16:17,730 --> 00:16:22,590
we are trying to do is want to simulate

412
00:16:19,350 --> 00:16:24,990
an aldose around emeritus rainy graph so

413
00:16:22,590 --> 00:16:27,000
and once we select these bins we take

414
00:16:24,990 --> 00:16:28,410
our file identifies in the tuple and we

415
00:16:27,000 --> 00:16:30,720
will just insert them in the bins that

416
00:16:28,410 --> 00:16:33,449
we have selected each in every single

417
00:16:30,720 --> 00:16:35,010
bin and in the state what we do is that

418
00:16:33,450 --> 00:16:38,730
we will map the keyword

419
00:16:35,010 --> 00:16:40,980
choose all file identifiers sorry bin

420
00:16:38,730 --> 00:16:43,890
zayed bin identifiers that we have

421
00:16:40,980 --> 00:16:46,380
selected end up in the previous step we

422
00:16:43,890 --> 00:16:48,090
do the same for keyword two so here the

423
00:16:46,380 --> 00:16:50,040
difference is the key words to has only

424
00:16:48,090 --> 00:16:52,170
a response length equal to one but still

425
00:16:50,040 --> 00:16:53,790
we will have a number of edges equal to

426
00:16:52,170 --> 00:16:57,000
three because as I have said we want to

427
00:16:53,790 --> 00:16:59,550
have a random air those Iranian graph so

428
00:16:57,000 --> 00:17:01,260
but we will just put a single file I

429
00:16:59,550 --> 00:17:03,750
don't if are in one of the of the 3

430
00:17:01,260 --> 00:17:06,690
selected bins we will also update the

431
00:17:03,750 --> 00:17:08,459
state similarly where we are so we where

432
00:17:06,690 --> 00:17:11,160
we map the keyword to the bin

433
00:17:08,459 --> 00:17:13,620
identifiers we do the same for the for

434
00:17:11,160 --> 00:17:15,720
the third keyword and we will just

435
00:17:13,619 --> 00:17:18,839
finish by padding all the bins to have

436
00:17:15,720 --> 00:17:20,790
the same load so something that we that

437
00:17:18,839 --> 00:17:23,190
you you may have already noticed is the

438
00:17:20,790 --> 00:17:25,920
size of this state actually is terrible

439
00:17:23,190 --> 00:17:29,070
is actually linear is actually equal to

440
00:17:25,920 --> 00:17:31,650
the knife padding approach so how can we

441
00:17:29,070 --> 00:17:33,720
remove this this downside or this

442
00:17:31,650 --> 00:17:37,320
restriction so we use the following

443
00:17:33,720 --> 00:17:38,730
trick at very high level we replace the

444
00:17:37,320 --> 00:17:41,939
random generation of edges by a

445
00:17:38,730 --> 00:17:44,370
pseudo-random generation where we take

446
00:17:41,940 --> 00:17:47,590
our so how does it work we we first

447
00:17:44,370 --> 00:17:50,080
sample a random value and

448
00:17:47,590 --> 00:17:51,779
we have a PRF and that will that we will

449
00:17:50,080 --> 00:17:55,418
evaluate on this random value

450
00:17:51,779 --> 00:17:57,820
concatenated two counters one counter

451
00:17:55,419 --> 00:18:00,249
one two three and three here is the the

452
00:17:57,820 --> 00:18:01,570
response length and they will the PRF

453
00:18:00,249 --> 00:18:04,299
will output something and that something

454
00:18:01,570 --> 00:18:07,178
is actually the bin identifier and as

455
00:18:04,299 --> 00:18:11,200
you can notice here we have a collision

456
00:18:07,179 --> 00:18:12,879
so what we should do is that so what we

457
00:18:11,200 --> 00:18:16,029
should do is we will have we will repeat

458
00:18:12,879 --> 00:18:18,309
this process until no collision will be

459
00:18:16,029 --> 00:18:20,499
found so here we we didn't have a

460
00:18:18,309 --> 00:18:23,649
collision and then we will just insert

461
00:18:20,499 --> 00:18:26,200
the the file identifiers in the in the

462
00:18:23,649 --> 00:18:28,719
bins now in the state instead of storing

463
00:18:26,200 --> 00:18:30,669
and mapping between the keyword and all

464
00:18:28,720 --> 00:18:32,950
of the bin identifiers we just store the

465
00:18:30,669 --> 00:18:34,419
keyword that map's to the random value

466
00:18:32,950 --> 00:18:37,029
that we have used for the edge

467
00:18:34,419 --> 00:18:41,379
generation and we do the same for for

468
00:18:37,029 --> 00:18:43,450
keyword two we will put the ID and we

469
00:18:41,379 --> 00:18:46,418
will update State similarly for keyword

470
00:18:43,450 --> 00:18:48,700
3 and so on and so forth we will pad and

471
00:18:46,419 --> 00:18:50,350
then now you can notice that the size of

472
00:18:48,700 --> 00:18:51,730
the state is way better

473
00:18:50,350 --> 00:18:53,740
it's just linear in the number of

474
00:18:51,730 --> 00:18:56,830
keywords instead of being actually

475
00:18:53,740 --> 00:19:00,789
linear in or equal to the size of the of

476
00:18:56,830 --> 00:19:03,100
the naive padding approach great so what

477
00:19:00,789 --> 00:19:04,809
is the output of this da DSC is that we

478
00:19:03,100 --> 00:19:06,939
will output three things a key and the

479
00:19:04,809 --> 00:19:08,860
state that we will keep on the client

480
00:19:06,940 --> 00:19:10,899
side and the dictionary basically data

481
00:19:08,860 --> 00:19:15,219
structure that will map the bin

482
00:19:10,899 --> 00:19:17,979
identifier to its content and when we

483
00:19:15,220 --> 00:19:21,820
want to perform a get the client will

484
00:19:17,980 --> 00:19:23,230
follow on a key word w1 the the the

485
00:19:21,820 --> 00:19:25,119
client will first retrieve the

486
00:19:23,230 --> 00:19:28,659
corresponding randomness from the state

487
00:19:25,119 --> 00:19:31,240
and then generate prick recompute the

488
00:19:28,659 --> 00:19:33,340
the bit identifies by evaluating the PRF

489
00:19:31,240 --> 00:19:35,080
three times and then it will just fetch

490
00:19:33,340 --> 00:19:40,019
from the dictionary to corresponding bin

491
00:19:35,080 --> 00:19:44,019
contents and will find the result okay

492
00:19:40,019 --> 00:19:45,879
so one natural question is how about the

493
00:19:44,019 --> 00:19:47,200
load of the bin this is like the most

494
00:19:45,879 --> 00:19:49,330
important question that we need to

495
00:19:47,200 --> 00:19:51,580
asking for this transform to understand

496
00:19:49,330 --> 00:19:54,158
how how it does in terms of of storage

497
00:19:51,580 --> 00:19:56,110
so we were able to show that the load of

498
00:19:54,159 --> 00:19:58,570
the bin is equal to some 2d to this

499
00:19:56,110 --> 00:20:00,129
formula but the takeaway here is that we

500
00:19:58,570 --> 00:20:01,599
were able to show with high probability

501
00:20:00,129 --> 00:20:04,509
that the transforming team

502
00:20:01,599 --> 00:20:06,129
has a Big O of n which n is the number

503
00:20:04,509 --> 00:20:08,139
of pairs which is actually the same size

504
00:20:06,129 --> 00:20:10,988
of the the Mickey map that we have given

505
00:20:08,139 --> 00:20:12,519
us input so there it is as some sort as

506
00:20:10,989 --> 00:20:14,379
some practically no additional overhead

507
00:20:12,519 --> 00:20:19,899
the size of the states as I have

508
00:20:14,379 --> 00:20:21,699
mentioned is linear in the linear in the

509
00:20:19,899 --> 00:20:23,168
number of labels which is actually

510
00:20:21,700 --> 00:20:26,979
dominated by the number of pairs which

511
00:20:23,169 --> 00:20:28,599
is great all right so when we were

512
00:20:26,979 --> 00:20:30,879
working on this we have noticed

513
00:20:28,599 --> 00:20:32,589
something which is very nice actually we

514
00:20:30,879 --> 00:20:34,809
can actually further reduce the storage

515
00:20:32,589 --> 00:20:36,489
overhead if we leverage some

516
00:20:34,809 --> 00:20:38,769
computational assumption and what we

517
00:20:36,489 --> 00:20:41,139
leverage here is an assumption called

518
00:20:38,769 --> 00:20:42,820
planted dance sub graph which is an

519
00:20:41,139 --> 00:20:44,258
assumption that has been already used in

520
00:20:42,820 --> 00:20:46,839
public key to design public key

521
00:20:44,259 --> 00:20:49,179
cryptography by Appelbaum Baraka and

522
00:20:46,839 --> 00:20:50,739
anderson and also into studying the

523
00:20:49,179 --> 00:20:53,229
computational complexity of financial

524
00:20:50,739 --> 00:20:54,969
products by aurora at all and at a very

525
00:20:53,229 --> 00:20:57,039
high level the assumption is as follows

526
00:20:54,969 --> 00:20:59,109
so you have on one hand a random

527
00:20:57,039 --> 00:21:01,749
erdos-renyi graph on the other hand you

528
00:20:59,109 --> 00:21:04,389
have a random Erdos ringing graph into

529
00:21:01,749 --> 00:21:06,429
which we plant a dense sub graph and an

530
00:21:04,389 --> 00:21:08,738
adversary and bound adversity cannot

531
00:21:06,429 --> 00:21:10,659
distinguish between the two so we

532
00:21:08,739 --> 00:21:13,389
leverage this assumption in this work

533
00:21:10,659 --> 00:21:15,909
which is quite nice because it's an

534
00:21:13,389 --> 00:21:17,168
another application in cryptography so

535
00:21:15,909 --> 00:21:19,389
how does it work

536
00:21:17,169 --> 00:21:22,659
and as I have said maybe I forgot to say

537
00:21:19,389 --> 00:21:24,639
this but this this storage gain will

538
00:21:22,659 --> 00:21:26,709
only work for a specific type of Meucci

539
00:21:24,639 --> 00:21:28,599
maps that does some detail in here so

540
00:21:26,709 --> 00:21:32,049
the Moochie map i have changed a little

541
00:21:28,599 --> 00:21:33,789
bit so this will work if we have some

542
00:21:32,049 --> 00:21:35,829
labels or some tuples that they have

543
00:21:33,789 --> 00:21:38,049
some non-empty intersection and here you

544
00:21:35,829 --> 00:21:40,719
can notice that keyword 1 and Q 3 they

545
00:21:38,049 --> 00:21:43,418
have a non-empty intersection where ID 2

546
00:21:40,719 --> 00:21:45,549
ID 4 appear actually appear in both and

547
00:21:43,419 --> 00:21:48,729
this is what we call a concentrated

548
00:21:45,549 --> 00:21:51,879
multi map and this is what we have said

549
00:21:48,729 --> 00:21:54,489
so how the DSC works now so we will have

550
00:21:51,879 --> 00:21:56,289
our our our bipartite graph or the top

551
00:21:54,489 --> 00:21:58,509
vertices our keyword and the and the

552
00:21:56,289 --> 00:22:00,429
bottom vertices are actually bins so

553
00:21:58,509 --> 00:22:02,709
what we start with we will first put the

554
00:22:00,429 --> 00:22:06,099
concentrated part in the into the bins

555
00:22:02,709 --> 00:22:07,269
and then so we do it reverse actually

556
00:22:06,099 --> 00:22:09,668
you can think of it as we are doing

557
00:22:07,269 --> 00:22:11,289
actually rivers and we just we then

558
00:22:09,669 --> 00:22:13,719
generate the edges that will map the

559
00:22:11,289 --> 00:22:15,590
keywords W 1 and W 3 to actually to this

560
00:22:13,719 --> 00:22:18,259
concentrated

561
00:22:15,590 --> 00:22:20,480
part and then we will continue by

562
00:22:18,259 --> 00:22:24,980
generating as I have explained it before

563
00:22:20,480 --> 00:22:27,110
for the DSC the the other edges say for

564
00:22:24,980 --> 00:22:29,059
keyword 2 and here the idea is that we

565
00:22:27,110 --> 00:22:34,070
as you can notice we are we have added

566
00:22:29,059 --> 00:22:36,590
only the concentrated part only once out

567
00:22:34,070 --> 00:22:38,600
of time only once and this is basically

568
00:22:36,590 --> 00:22:40,220
the results we are reducing the load of

569
00:22:38,600 --> 00:22:42,168
the pins as this is a contrived example

570
00:22:40,220 --> 00:22:44,509
but as you can see the bins now only

571
00:22:42,169 --> 00:22:46,669
contain all of the bins only contain one

572
00:22:44,509 --> 00:22:48,259
value while in the previous example they

573
00:22:46,669 --> 00:22:49,820
do contain two and this is basically

574
00:22:48,259 --> 00:22:51,200
just a concise example to show that we

575
00:22:49,820 --> 00:22:52,970
are we are getting something and

576
00:22:51,200 --> 00:22:54,830
actually we can show asymptotically that

577
00:22:52,970 --> 00:22:56,600
the load of the bin is actually reduced

578
00:22:54,830 --> 00:22:58,399
by this concentrated part that we have

579
00:22:56,600 --> 00:23:01,459
identified and it's better than the

580
00:22:58,399 --> 00:23:03,320
previous load of the bin all right

581
00:23:01,460 --> 00:23:04,759
so to conclude what are the takeaways so

582
00:23:03,320 --> 00:23:06,379
I want to first start with something

583
00:23:04,759 --> 00:23:08,989
that I want to emphasize so volume

584
00:23:06,379 --> 00:23:11,509
pattern is an important pattern that has

585
00:23:08,989 --> 00:23:14,779
been recently leveraged to perform some

586
00:23:11,509 --> 00:23:18,950
attacks by calories at all and by grabs

587
00:23:14,779 --> 00:23:20,480
at all true attack structured encryption

588
00:23:18,950 --> 00:23:23,149
or and because it is just based on the

589
00:23:20,480 --> 00:23:24,230
volume so this is an important work that

590
00:23:23,149 --> 00:23:26,029
shows how to design structured

591
00:23:24,230 --> 00:23:28,549
encryption schemes that they are not

592
00:23:26,029 --> 00:23:30,799
prone to these attacks and as I as you

593
00:23:28,549 --> 00:23:31,999
can see you know hopefully this talk

594
00:23:30,799 --> 00:23:35,929
have shown that it's not actually

595
00:23:31,999 --> 00:23:38,149
trivial to to to to suppress the volume

596
00:23:35,929 --> 00:23:40,340
passion we have seen the trivial

597
00:23:38,149 --> 00:23:43,369
approaches they are terrible in terms of

598
00:23:40,340 --> 00:23:44,899
efficiency and I want also to stress

599
00:23:43,369 --> 00:23:46,549
that this line of work

600
00:23:44,899 --> 00:23:48,830
hiding the volume or hiding other

601
00:23:46,549 --> 00:23:50,450
pattern is a very important research

602
00:23:48,830 --> 00:23:52,730
direction in structural encryption and

603
00:23:50,450 --> 00:23:55,489
encrypted search in general so we have

604
00:23:52,730 --> 00:23:57,409
started this in 2018 by the because

605
00:23:55,489 --> 00:24:00,139
suppression work and this work is

606
00:23:57,409 --> 00:24:02,659
actually another step into this

607
00:24:00,139 --> 00:24:04,039
direction hopefully you didn't fall

608
00:24:02,659 --> 00:24:07,059
asleep and you remember that we have

609
00:24:04,039 --> 00:24:10,009
seen the PRC and the DSC transform

610
00:24:07,059 --> 00:24:12,019
something that I want also to to to

611
00:24:10,009 --> 00:24:13,249
emphasize here is that if we go back a

612
00:24:12,019 --> 00:24:14,749
little bit and if you think about

613
00:24:13,249 --> 00:24:16,220
suppressing the volume pattern

614
00:24:14,749 --> 00:24:17,899
intuitively you you might think that

615
00:24:16,220 --> 00:24:20,029
hiding volume information theoretically

616
00:24:17,899 --> 00:24:21,649
is not possible without padding but we

617
00:24:20,029 --> 00:24:23,480
goes around this using computational

618
00:24:21,649 --> 00:24:25,248
assumptions which is kind of the first

619
00:24:23,480 --> 00:24:26,749
work that actually do this for any

620
00:24:25,249 --> 00:24:28,940
pattern and for volume in power in

621
00:24:26,749 --> 00:24:30,530
particular and hopefully this line

622
00:24:28,940 --> 00:24:32,180
work will help us actually to suppress

623
00:24:30,530 --> 00:24:34,700
other passions in structural encryption

624
00:24:32,180 --> 00:24:36,170
and encrypted search in general another

625
00:24:34,700 --> 00:24:37,490
point that we introduced in this work is

626
00:24:36,170 --> 00:24:39,980
that there is this new trade of between

627
00:24:37,490 --> 00:24:42,170
correctness and security and I will I

628
00:24:39,980 --> 00:24:43,880
will finish by mentioning that this line

629
00:24:42,170 --> 00:24:46,100
of work will actually help us to thwart

630
00:24:43,880 --> 00:24:47,270
mini if not all actually existing

631
00:24:46,100 --> 00:24:50,149
attacks that we know of in the

632
00:24:47,270 --> 00:24:58,670
literature ikkc GPR and so on and so

633
00:24:50,150 --> 00:25:00,410
forth thank you so we we don't have time

634
00:24:58,670 --> 00:25:02,650
for questions let's take the question of

635
00:25:00,410 --> 00:25:02,650
Lai

