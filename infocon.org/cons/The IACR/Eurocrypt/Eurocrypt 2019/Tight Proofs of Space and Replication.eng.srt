1
00:00:00,030 --> 00:00:07,319
thank you thank you for the introduction

2
00:00:02,009 --> 00:00:09,928
just so this talk is on type Bruce's

3
00:00:07,319 --> 00:00:13,110
face and I'll explain what those are so

4
00:00:09,929 --> 00:00:15,269
first a proof of space is an alternative

5
00:00:13,110 --> 00:00:17,520
to proof of work where the resource

6
00:00:15,269 --> 00:00:20,520
being used a space instead of

7
00:00:17,520 --> 00:00:23,310
computational energy so it should

8
00:00:20,520 --> 00:00:26,820
require the prover who's producing this

9
00:00:23,310 --> 00:00:29,250
proof to use significant space and has

10
00:00:26,820 --> 00:00:31,679
all the same applications as proof of

11
00:00:29,250 --> 00:00:34,019
work including applications to spam

12
00:00:31,679 --> 00:00:36,989
prevention or mitigating denial of

13
00:00:34,020 --> 00:00:39,390
service attacks and most famously to

14
00:00:36,989 --> 00:00:42,570
civil resistance in consensus networks

15
00:00:39,390 --> 00:00:46,079
like Bitcoin the advantage of a proof of

16
00:00:42,570 --> 00:00:48,989
space is that it space is a reusable

17
00:00:46,079 --> 00:00:52,469
resource so producing a new proof of

18
00:00:48,989 --> 00:00:54,739
space can reuse the same space unlike a

19
00:00:52,469 --> 00:00:58,620
proof of work which requires consuming

20
00:00:54,739 --> 00:01:01,199
new energy so it's proposes a more

21
00:00:58,620 --> 00:01:04,140
eco-friendly way of getting a system

22
00:01:01,199 --> 00:01:06,298
like Bitcoin so what does a proof of

23
00:01:04,140 --> 00:01:09,450
space look like it's an interactive

24
00:01:06,299 --> 00:01:12,720
protocol between approver and a verifier

25
00:01:09,450 --> 00:01:14,549
where the approver is allowed to choose

26
00:01:12,720 --> 00:01:16,679
the amount of space that it commits to

27
00:01:14,549 --> 00:01:19,200
store it sends a commitment to the

28
00:01:16,680 --> 00:01:22,680
verifier the verifier sends a challenge

29
00:01:19,200 --> 00:01:25,290
and the prover produces a proof that

30
00:01:22,680 --> 00:01:28,170
should convince the verifier that it is

31
00:01:25,290 --> 00:01:31,500
using almost all the storage that it

32
00:01:28,170 --> 00:01:33,030
claims to be using as efficiency

33
00:01:31,500 --> 00:01:34,979
requirements though you require the

34
00:01:33,030 --> 00:01:37,770
proof and the commitment to be much

35
00:01:34,979 --> 00:01:41,039
smaller than the storage that proofer

36
00:01:37,770 --> 00:01:42,929
claims to have so the overall

37
00:01:41,040 --> 00:01:48,930
communication should be much smaller

38
00:01:42,930 --> 00:01:52,740
than the storage and the verifier it

39
00:01:48,930 --> 00:01:54,450
should also run efficiently so if n say

40
00:01:52,740 --> 00:01:57,630
is the amount of storage the prover

41
00:01:54,450 --> 00:02:03,780
claims to be using then the verifier

42
00:01:57,630 --> 00:02:08,008
should run in poly log n time there's

43
00:02:03,780 --> 00:02:09,869
also a non interactive version of proof

44
00:02:08,008 --> 00:02:11,910
of space or any proof of space protocol

45
00:02:09,869 --> 00:02:15,090
since by definition

46
00:02:11,910 --> 00:02:18,090
is public coin the Fiat ramier heuristic

47
00:02:15,090 --> 00:02:19,410
can be applied in order to get a

48
00:02:18,090 --> 00:02:21,480
non-interactive version where the

49
00:02:19,410 --> 00:02:25,020
challenge is derived as a hash of the

50
00:02:21,480 --> 00:02:26,369
perverse commitment so in the literature

51
00:02:25,020 --> 00:02:28,440
on proof of space there's a

52
00:02:26,370 --> 00:02:30,870
differentiation though between two types

53
00:02:28,440 --> 00:02:33,210
of proof of space the one which I just

54
00:02:30,870 --> 00:02:35,790
described may only convince the verifier

55
00:02:33,210 --> 00:02:38,310
that the prover is using a lot of space

56
00:02:35,790 --> 00:02:40,590
in order to produce the proof but it may

57
00:02:38,310 --> 00:02:43,470
not be required to persistently use

58
00:02:40,590 --> 00:02:45,810
space over time so a proof of persistent

59
00:02:43,470 --> 00:02:48,420
space is one where the prover it

60
00:02:45,810 --> 00:02:51,570
demonstrates that it continuously uses

61
00:02:48,420 --> 00:02:53,489
space over time and this is divided into

62
00:02:51,570 --> 00:02:55,769
two phases one which is an

63
00:02:53,490 --> 00:02:57,390
initialization protocol that looks much

64
00:02:55,770 --> 00:02:59,100
like the one I just described it can be

65
00:02:57,390 --> 00:03:02,640
either interactive or non interactive

66
00:02:59,100 --> 00:03:04,710
and then the prover enters an online

67
00:03:02,640 --> 00:03:09,660
phase where it fields challenges from

68
00:03:04,710 --> 00:03:12,390
the verifier and computes responses that

69
00:03:09,660 --> 00:03:15,870
should convince the verifier that it is

70
00:03:12,390 --> 00:03:17,609
still storing a a lot of space rather

71
00:03:15,870 --> 00:03:20,370
than continuously running the

72
00:03:17,610 --> 00:03:23,040
initialization the online phase is seen

73
00:03:20,370 --> 00:03:25,080
as a separate phase because it it can be

74
00:03:23,040 --> 00:03:27,269
much more efficient for the for the

75
00:03:25,080 --> 00:03:29,430
prover to do the initialization phase

76
00:03:27,270 --> 00:03:35,070
could require the prover to do a lot of

77
00:03:29,430 --> 00:03:36,360
work now notice something regarding you

78
00:03:35,070 --> 00:03:39,600
know the efficiency of initialization

79
00:03:36,360 --> 00:03:41,790
versus the online phase if the prover

80
00:03:39,600 --> 00:03:43,700
could just rerun the initialization to

81
00:03:41,790 --> 00:03:46,380
respond to the online challenges then

82
00:03:43,700 --> 00:03:48,450
clearly this wouldn't work as a proof of

83
00:03:46,380 --> 00:03:50,700
persistent space the prover could delete

84
00:03:48,450 --> 00:03:52,709
its space after each challenge and then

85
00:03:50,700 --> 00:03:55,530
redo the initialization in order to

86
00:03:52,710 --> 00:03:59,130
respond to challenges so in the the way

87
00:03:55,530 --> 00:04:02,190
that we define this is by restricting

88
00:03:59,130 --> 00:04:06,030
the time that the prover has to respond

89
00:04:02,190 --> 00:04:08,700
to challenges and in particular the the

90
00:04:06,030 --> 00:04:11,760
time that the prover has to respond to

91
00:04:08,700 --> 00:04:13,649
these challenges should be much shorter

92
00:04:11,760 --> 00:04:16,370
than the time it takes for it to run the

93
00:04:13,650 --> 00:04:16,370
initialization

94
00:04:19,750 --> 00:04:25,840
so how tight is a proof of space that's

95
00:04:22,810 --> 00:04:28,420
the principle question that this paper

96
00:04:25,840 --> 00:04:30,909
explores in other words how much space

97
00:04:28,420 --> 00:04:32,830
can some adversarial approvers save and

98
00:04:30,910 --> 00:04:35,200
still pass the protocol could it pass

99
00:04:32,830 --> 00:04:37,060
the protocol with only 1 minus epsilon

100
00:04:35,200 --> 00:04:40,330
gigabytes if it claims to be using a

101
00:04:37,060 --> 00:04:42,850
gigabyte and for a type of a space the

102
00:04:40,330 --> 00:04:44,830
answer should be no for 1 minus epsilon

103
00:04:42,850 --> 00:04:46,900
as close to 1 as possible in other words

104
00:04:44,830 --> 00:04:50,440
a smaller epsilon gives you a tighter

105
00:04:46,900 --> 00:04:52,989
proof of space so we can define epsilon

106
00:04:50,440 --> 00:04:55,000
tightness as saying that if the online

107
00:04:52,990 --> 00:04:56,830
prover store is less than 1 minus

108
00:04:55,000 --> 00:04:59,680
epsilon gigabytes then it should fail

109
00:04:56,830 --> 00:05:02,320
with overwhelming probability to respond

110
00:04:59,680 --> 00:05:04,720
within some time limit T T is typically

111
00:05:02,320 --> 00:05:09,940
set to be proportional to the storage

112
00:05:04,720 --> 00:05:11,800
size a weaker way of defining proof of a

113
00:05:09,940 --> 00:05:14,260
security would allow for a time space

114
00:05:11,800 --> 00:05:16,330
trade-off in this definition even a

115
00:05:14,260 --> 00:05:18,039
parallel secured parallel prover who

116
00:05:16,330 --> 00:05:20,890
uses a lot of parallelism in order to

117
00:05:18,040 --> 00:05:23,290
speed up its response time to get it

118
00:05:20,890 --> 00:05:25,479
smaller than T should fail

119
00:05:23,290 --> 00:05:28,180
whereas we we could allow for a time

120
00:05:25,480 --> 00:05:30,280
space trade-off where the prover should

121
00:05:28,180 --> 00:05:31,960
simply have to do a lot of computational

122
00:05:30,280 --> 00:05:39,070
work if it chooses not to use that much

123
00:05:31,960 --> 00:05:43,150
space persistently so in in in this work

124
00:05:39,070 --> 00:05:44,860
I define a tight proof of space as one

125
00:05:43,150 --> 00:05:47,500
which can be tuned to be arbitrarily

126
00:05:44,860 --> 00:05:49,390
tight so the protocol parameters could

127
00:05:47,500 --> 00:05:52,720
be tuned for any epsilon less than one

128
00:05:49,390 --> 00:05:56,200
so that it is epsilon tight as just

129
00:05:52,720 --> 00:05:58,810
defined the problem is that while tuning

130
00:05:56,200 --> 00:06:00,849
the parameters 2 to be a tighter proof

131
00:05:58,810 --> 00:06:02,680
of space for smaller epsilon that may

132
00:06:00,850 --> 00:06:06,220
also impact the efficiency and so we'd

133
00:06:02,680 --> 00:06:08,500
like to maintain efficiency and so the

134
00:06:06,220 --> 00:06:11,260
efficiency goal is to keep the proof

135
00:06:08,500 --> 00:06:13,210
size or communication and the increase

136
00:06:11,260 --> 00:06:16,919
in computation proportional to 1 over

137
00:06:13,210 --> 00:06:20,049
epsilon as epsilon gets smaller

138
00:06:16,919 --> 00:06:21,700
intuitively the reason why it seems that

139
00:06:20,050 --> 00:06:25,060
one of our epsilon is sort of the best

140
00:06:21,700 --> 00:06:26,770
we can do is that the the way that at

141
00:06:25,060 --> 00:06:30,190
least the information theoretic versions

142
00:06:26,770 --> 00:06:32,799
of the online challenges work the the

143
00:06:30,190 --> 00:06:36,310
prover is has to store

144
00:06:32,800 --> 00:06:38,770
some string of large size and the and if

145
00:06:36,310 --> 00:06:41,110
it for if it forgets too much of that

146
00:06:38,770 --> 00:06:44,318
string then the verifiers challenges

147
00:06:41,110 --> 00:06:46,240
will sort of catch it on on the errors

148
00:06:44,319 --> 00:06:48,430
that it made or the the parts of the

149
00:06:46,240 --> 00:06:50,169
space that had forgot so if it only for

150
00:06:48,430 --> 00:06:53,110
if it only forgot one percent of the

151
00:06:50,169 --> 00:06:56,710
space then the probability that the

152
00:06:53,110 --> 00:07:00,069
verifiers challenges will locate will be

153
00:06:56,710 --> 00:07:01,870
in that area of space you know is is

154
00:07:00,069 --> 00:07:04,720
only good enough if the number of

155
00:07:01,870 --> 00:07:07,479
challenges is close to one over is you

156
00:07:04,720 --> 00:07:09,610
know order of one over epsilon or land

157
00:07:07,479 --> 00:07:12,789
over epsilon so in this work we get

158
00:07:09,610 --> 00:07:15,190
epsilon tightness with proof size which

159
00:07:12,789 --> 00:07:20,318
is seemingly close to optimal or order

160
00:07:15,190 --> 00:07:22,360
of log n over epsilon so what was the

161
00:07:20,319 --> 00:07:24,280
state of the art before this work will

162
00:07:22,360 --> 00:07:28,000
proofs of space were first introduced in

163
00:07:24,280 --> 00:07:33,448
2015 and that construction had one minus

164
00:07:28,000 --> 00:07:36,400
epsilon less than 101 1 over 512 which

165
00:07:33,449 --> 00:07:39,669
would mean that the there could be an

166
00:07:36,400 --> 00:07:41,710
adversary who uses only a 5/12 fraction

167
00:07:39,669 --> 00:07:43,180
of the data now note that these numbers

168
00:07:41,710 --> 00:07:45,190
don't indicate that there is actually an

169
00:07:43,180 --> 00:07:48,460
attack that achieves that but just gaps

170
00:07:45,190 --> 00:07:50,349
in the analysis random Devadas improved

171
00:07:48,460 --> 00:07:54,448
this in 2016

172
00:07:50,349 --> 00:07:59,469
and made one over epsilon closer to 1/2

173
00:07:54,449 --> 00:08:01,180
ok and a construction from 2017 was a

174
00:07:59,469 --> 00:08:04,659
different type of proof of space but it

175
00:08:01,180 --> 00:08:07,630
wasn't tight and then in 2018 we had the

176
00:08:04,659 --> 00:08:10,419
first tight proof his face although

177
00:08:07,630 --> 00:08:12,789
unfortunately in that construction the

178
00:08:10,419 --> 00:08:17,440
proof size increases proportional to log

179
00:08:12,789 --> 00:08:18,940
n over epsilon squared so it which which

180
00:08:17,440 --> 00:08:21,039
makes a big difference for for

181
00:08:18,940 --> 00:08:25,569
efficiency let's say epsilon is you know

182
00:08:21,039 --> 00:08:28,000
1 over 100 and furthermore that

183
00:08:25,569 --> 00:08:29,919
construction uses these very special

184
00:08:28,000 --> 00:08:32,708
types of depth for bus graphs which we

185
00:08:29,919 --> 00:08:35,078
don't have practical instantiations of

186
00:08:32,708 --> 00:08:36,728
we have asymptotic constructions but

187
00:08:35,078 --> 00:08:38,439
even heuristic aliy we haven't really

188
00:08:36,729 --> 00:08:42,750
been able to demonstrate that it has

189
00:08:38,440 --> 00:08:42,750
this property so

190
00:08:44,300 --> 00:08:49,910
why do tight proofs of space matter well

191
00:08:47,330 --> 00:08:52,010
one we can get per better provable

192
00:08:49,910 --> 00:08:55,339
security for proof of space just like in

193
00:08:52,010 --> 00:08:57,589
any the goals of any tight security

194
00:08:55,339 --> 00:09:00,470
exercise we would like the gap between

195
00:08:57,589 --> 00:09:02,690
what the honest approver has to do and

196
00:09:00,470 --> 00:09:05,540
the best possible adversary to be small

197
00:09:02,690 --> 00:09:07,820
as either we have to tune the parameters

198
00:09:05,540 --> 00:09:09,439
for the worst possible adversary and

199
00:09:07,820 --> 00:09:12,380
make life extremely hard for the honest

200
00:09:09,440 --> 00:09:14,180
provers or we set life to be reasonable

201
00:09:12,380 --> 00:09:15,920
for the honest provers and then there's

202
00:09:14,180 --> 00:09:19,099
some adversary that virtually doesn't

203
00:09:15,920 --> 00:09:21,829
have to use any space at all but a

204
00:09:19,100 --> 00:09:24,500
second motivation is that it's necessary

205
00:09:21,830 --> 00:09:27,260
for this new type of primitive called

206
00:09:24,500 --> 00:09:29,360
proof of replication and I won't go into

207
00:09:27,260 --> 00:09:32,120
proof replication in this talk since

208
00:09:29,360 --> 00:09:34,940
it's actually extraordinarily tricky to

209
00:09:32,120 --> 00:09:37,399
define properly and it would require a

210
00:09:34,940 --> 00:09:39,860
lot of time to explain but I will talk

211
00:09:37,399 --> 00:09:42,709
about useful proofs of space which are

212
00:09:39,860 --> 00:09:45,380
very related to proof of replication and

213
00:09:42,709 --> 00:09:48,199
also greatly benefit from type 4 space

214
00:09:45,380 --> 00:09:51,230
so I'll explain that next so what is a

215
00:09:48,200 --> 00:09:54,290
useful proof of space it's a proof of

216
00:09:51,230 --> 00:09:56,360
space where additionally at the

217
00:09:54,290 --> 00:09:59,599
correctness requirement the prover is

218
00:09:56,360 --> 00:10:01,459
able to use its storage to store files

219
00:09:59,600 --> 00:10:04,850
of its own interest so it doesn't have

220
00:10:01,459 --> 00:10:07,399
to waste the space as it's engaging in

221
00:10:04,850 --> 00:10:09,680
the proof the space protocol so cool I

222
00:10:07,399 --> 00:10:14,329
can still use my space to store my

223
00:10:09,680 --> 00:10:15,800
movies what is an application well if

224
00:10:14,329 --> 00:10:19,370
you think about the application of proof

225
00:10:15,800 --> 00:10:21,439
of space to building block chains like a

226
00:10:19,370 --> 00:10:23,149
Bitcoin alternative one of the things

227
00:10:21,440 --> 00:10:25,040
that we're concerned about with block

228
00:10:23,149 --> 00:10:28,339
chains is the so-called blockchain

229
00:10:25,040 --> 00:10:31,699
carbon footprint no proof of work based

230
00:10:28,339 --> 00:10:33,940
Bitcoin it wastes a lot of energy since

231
00:10:31,700 --> 00:10:36,709
all the miners in the system

232
00:10:33,940 --> 00:10:39,020
continuously use a lot of energy to

233
00:10:36,709 --> 00:10:41,709
maintain the system and prove this space

234
00:10:39,020 --> 00:10:44,600
we said is more eco-friendly because it

235
00:10:41,709 --> 00:10:47,599
can reuse the same space so it doesn't

236
00:10:44,600 --> 00:10:51,020
waste energy or require more consuming

237
00:10:47,600 --> 00:10:52,400
more space you know every minute but it

238
00:10:51,020 --> 00:10:54,529
does still doesn't do anything positive

239
00:10:52,400 --> 00:10:55,939
and you could say that the the space is

240
00:10:54,529 --> 00:10:57,589
still not being used for anything useful

241
00:10:55,940 --> 00:11:02,689
so

242
00:10:57,589 --> 00:11:05,509
a useful proof of space would push this

243
00:11:02,689 --> 00:11:08,389
even further and have a so to say

244
00:11:05,509 --> 00:11:09,649
positive footprint because what the work

245
00:11:08,389 --> 00:11:12,529
that the miners are doing to maintain

246
00:11:09,649 --> 00:11:17,170
the the Bitcoin like system is can also

247
00:11:12,529 --> 00:11:17,170
simultaneously be used for data storage

248
00:11:17,230 --> 00:11:22,279
so consider a system where the miners

249
00:11:20,420 --> 00:11:24,979
are all mining for the blockchain and

250
00:11:22,279 --> 00:11:27,470
and using their space for for useful

251
00:11:24,980 --> 00:11:29,420
data storage one of the things that we'd

252
00:11:27,470 --> 00:11:32,360
be concerned about is that at some point

253
00:11:29,420 --> 00:11:34,729
in time one of the miners finds a cheat

254
00:11:32,360 --> 00:11:36,519
that lets it gain a huge advantage in

255
00:11:34,730 --> 00:11:39,019
the proof of space protocol and

256
00:11:36,519 --> 00:11:41,930
unfortunately let's say that that sheet

257
00:11:39,019 --> 00:11:44,149
no longer it's different than the honest

258
00:11:41,930 --> 00:11:46,160
protocol and so it doesn't necessarily

259
00:11:44,149 --> 00:11:48,680
have the same correctness property as

260
00:11:46,160 --> 00:11:52,490
the honest protocol it perhaps does not

261
00:11:48,680 --> 00:11:56,180
allow this prove er to still store data

262
00:11:52,490 --> 00:11:57,559
useful data well news travels fast and

263
00:11:56,180 --> 00:11:59,689
all the other miners catch on and

264
00:11:57,559 --> 00:12:01,430
eventually this falls back to just a

265
00:11:59,689 --> 00:12:04,610
proof of space blockchain and it's no

266
00:12:01,430 --> 00:12:07,180
longer doing useful data storage so one

267
00:12:04,610 --> 00:12:10,430
implication of tight proof of space is

268
00:12:07,180 --> 00:12:12,949
that we can you know sleep well or rest

269
00:12:10,430 --> 00:12:15,040
assured that nobody will find some

270
00:12:12,949 --> 00:12:17,479
adversarial strategy that saves

271
00:12:15,040 --> 00:12:23,120
significantly from deviating from the

272
00:12:17,480 --> 00:12:24,379
honest protocol one other thing that we

273
00:12:23,120 --> 00:12:27,379
would be concerned about is how

274
00:12:24,379 --> 00:12:30,980
efficient is it is to extract data from

275
00:12:27,379 --> 00:12:32,660
this useful proof of space so let's say

276
00:12:30,980 --> 00:12:35,480
somebody wants to retrieve the movie

277
00:12:32,660 --> 00:12:37,339
that the miner is storing if that takes

278
00:12:35,480 --> 00:12:39,559
a really long time then that may be

279
00:12:37,339 --> 00:12:41,089
undesirable before the application and

280
00:12:39,559 --> 00:12:43,670
you can think of it as it being a little

281
00:12:41,089 --> 00:12:48,279
bit less useful but maybe still useful

282
00:12:43,670 --> 00:12:48,279
for storing you know archival backups

283
00:12:50,079 --> 00:12:54,949
but unfortunately a caveat of data

284
00:12:52,970 --> 00:12:57,199
extraction and efficiency is that

285
00:12:54,949 --> 00:12:58,910
efficient extraction implies that the

286
00:12:57,199 --> 00:13:01,128
proofs of space are not asynchronously

287
00:12:58,910 --> 00:13:04,029
composable so let me illustrate why that

288
00:13:01,129 --> 00:13:05,960
is let's say there's a proof of space

289
00:13:04,029 --> 00:13:09,380
minor and the

290
00:13:05,960 --> 00:13:11,839
though approver and the approver encodes

291
00:13:09,380 --> 00:13:13,370
a data file F in its proof is face and

292
00:13:11,839 --> 00:13:17,050
stores this encoding of F as its

293
00:13:13,370 --> 00:13:19,670
persistent storage and then after that

294
00:13:17,050 --> 00:13:21,760
initializes another proof of space where

295
00:13:19,670 --> 00:13:25,370
the data input which it can choose is

296
00:13:21,760 --> 00:13:26,860
the in is the same storage that it needs

297
00:13:25,370 --> 00:13:29,450
to pass the other proof of space

298
00:13:26,860 --> 00:13:31,790
protocol so now it can only it only

299
00:13:29,450 --> 00:13:35,660
needs to store the the second double

300
00:13:31,790 --> 00:13:37,730
encoding of F and by the efficiency of

301
00:13:35,660 --> 00:13:41,060
extraction it could efficiently extract

302
00:13:37,730 --> 00:13:42,710
F during the online challenges and be

303
00:13:41,060 --> 00:13:45,800
able to respond to the challenges for

304
00:13:42,710 --> 00:13:47,360
both proofs pretending to use twice the

305
00:13:45,800 --> 00:13:49,760
amount of space but really only using

306
00:13:47,360 --> 00:13:52,250
half that amount so that would be a

307
00:13:49,760 --> 00:13:54,350
problem but we're going to ignore that

308
00:13:52,250 --> 00:13:57,110
there are perhaps ways of dealing with

309
00:13:54,350 --> 00:14:00,920
that in in applications where there is a

310
00:13:57,110 --> 00:14:02,810
stronger promise on what what what files

311
00:14:00,920 --> 00:14:05,630
are being stored by the system so we'll

312
00:14:02,810 --> 00:14:07,819
ignore that for now so let me talk about

313
00:14:05,630 --> 00:14:12,260
in the remaining time the construction

314
00:14:07,820 --> 00:14:17,000
so the construction of many proofs of

315
00:14:12,260 --> 00:14:18,410
space works by labeling a directed

316
00:14:17,000 --> 00:14:22,070
acyclic graph so what is a graph

317
00:14:18,410 --> 00:14:26,810
labeling we use a collision resistant

318
00:14:22,070 --> 00:14:29,810
hash function H and label each node of

319
00:14:26,810 --> 00:14:30,979
the of the dag with with a data input in

320
00:14:29,810 --> 00:14:33,800
this case we can just choose it to be

321
00:14:30,980 --> 00:14:36,470
the index of that node the source node

322
00:14:33,800 --> 00:14:39,560
is given the label which is just a hash

323
00:14:36,470 --> 00:14:42,380
of its of its index and every other node

324
00:14:39,560 --> 00:14:46,849
is given a label which is derived as a

325
00:14:42,380 --> 00:14:49,250
hash of its data index and also all its

326
00:14:46,850 --> 00:14:51,470
dependency labels so all the labels on

327
00:14:49,250 --> 00:14:53,420
nodes that have an edge to that node and

328
00:14:51,470 --> 00:14:55,400
so on and so forth until we label the

329
00:14:53,420 --> 00:14:57,680
whole graph and the way the proof of

330
00:14:55,400 --> 00:14:59,390
space works or this family of proof is

331
00:14:57,680 --> 00:15:01,579
based protocols work are by sending a

332
00:14:59,390 --> 00:15:03,920
commitment to the graph labels receiving

333
00:15:01,580 --> 00:15:07,700
a challenge which is a subset of the

334
00:15:03,920 --> 00:15:11,810
nodes and then opening the labels of the

335
00:15:07,700 --> 00:15:13,310
nodes in this challenge set and also

336
00:15:11,810 --> 00:15:15,469
their parents and the verifier will

337
00:15:13,310 --> 00:15:17,800
check that this hash relationship is

338
00:15:15,470 --> 00:15:20,680
correct at least on the the challenge

339
00:15:17,800 --> 00:15:24,040
indices so it can be shown that for

340
00:15:20,680 --> 00:15:26,829
special types of directed acyclic graphs

341
00:15:24,040 --> 00:15:28,449
this gives a proof of space because if

342
00:15:26,830 --> 00:15:31,060
the prover forgets too many of the

343
00:15:28,450 --> 00:15:32,800
labels if we require doing sequential

344
00:15:31,060 --> 00:15:34,359
work we just heard two talks on proofs

345
00:15:32,800 --> 00:15:36,040
of sequential work required doing

346
00:15:34,360 --> 00:15:38,230
sequential work in order to read arrive

347
00:15:36,040 --> 00:15:40,360
those labels and respond correctly to

348
00:15:38,230 --> 00:15:43,690
the challenge so let me give you just a

349
00:15:40,360 --> 00:15:47,470
very high-level overview of the

350
00:15:43,690 --> 00:15:48,760
construction I give in this paper and so

351
00:15:47,470 --> 00:15:49,899
don't expect to understand all the

352
00:15:48,760 --> 00:15:52,000
details here I just want to give you a

353
00:15:49,899 --> 00:15:54,519
roadmap so you have in your picture in

354
00:15:52,000 --> 00:15:56,740
your mind of what the next few slides

355
00:15:54,519 --> 00:15:59,200
will be on the first step is to build a

356
00:15:56,740 --> 00:16:00,730
very weak not tight proof of space just

357
00:15:59,200 --> 00:16:04,390
from something called a depth robust

358
00:16:00,730 --> 00:16:07,660
graph and the depth of us graph that we

359
00:16:04,390 --> 00:16:10,060
require is simply one where if you look

360
00:16:07,660 --> 00:16:13,060
at a very large subset so say on 80% of

361
00:16:10,060 --> 00:16:17,349
the notes any 80% sub set contains a

362
00:16:13,060 --> 00:16:20,170
long path directed path and intuitively

363
00:16:17,350 --> 00:16:22,149
this gives you a not tight prove his

364
00:16:20,170 --> 00:16:24,670
face because if you for delete like 80%

365
00:16:22,149 --> 00:16:27,190
of the graph then in order to read

366
00:16:24,670 --> 00:16:29,729
arrive nodes any the label on any node

367
00:16:27,190 --> 00:16:32,500
within that it will require you to

368
00:16:29,730 --> 00:16:35,140
compute labels along a long path and

369
00:16:32,500 --> 00:16:38,620
that requires sequential work the next

370
00:16:35,140 --> 00:16:40,839
step is to amplify this to get a tighter

371
00:16:38,620 --> 00:16:43,560
proof of space by layering depth robust

372
00:16:40,839 --> 00:16:47,410
graphs and adding bipartite expander

373
00:16:43,560 --> 00:16:49,899
edges between the layers and we can show

374
00:16:47,410 --> 00:16:52,270
that that gets you a type of a space but

375
00:16:49,899 --> 00:16:56,700
not one where you can extract data from

376
00:16:52,270 --> 00:17:04,209
it efficiently and so the last step is

377
00:16:56,700 --> 00:17:06,339
make the picture prettier and I call

378
00:17:04,209 --> 00:17:09,490
this local it's called localization as a

379
00:17:06,339 --> 00:17:11,079
technique improves the space where we

380
00:17:09,490 --> 00:17:13,329
basically absorb the edges of the

381
00:17:11,079 --> 00:17:15,938
bipartite expander Grapher project them

382
00:17:13,329 --> 00:17:18,099
into the layers we will have to reverse

383
00:17:15,939 --> 00:17:21,370
the edges of the death or earth graph at

384
00:17:18,099 --> 00:17:23,490
every layer for reasons I'll get into

385
00:17:21,369 --> 00:17:25,678
later it's in order to maintain

386
00:17:23,490 --> 00:17:27,990
the proof of space security but what it

387
00:17:25,679 --> 00:17:31,740
ends up with is a graph structure where

388
00:17:27,990 --> 00:17:34,220
each layer can basically encode the

389
00:17:31,740 --> 00:17:37,620
labels of the previous layer and

390
00:17:34,220 --> 00:17:42,120
extracting data from this can be done

391
00:17:37,620 --> 00:17:45,000
more efficiently so first the first step

392
00:17:42,120 --> 00:17:47,760
is the the step through bus graph so

393
00:17:45,000 --> 00:17:50,450
remember I said a depth or bus graph can

394
00:17:47,760 --> 00:17:52,919
be defined in many ways as a very strong

395
00:17:50,450 --> 00:17:54,750
depth of us graph is one where if you

396
00:17:52,919 --> 00:17:57,570
delete any constant size subset that

397
00:17:54,750 --> 00:18:00,480
constant size subset maintains depth but

398
00:17:57,570 --> 00:18:02,850
I actually can get we can get away with

399
00:18:00,480 --> 00:18:05,220
with using the the weakest possible

400
00:18:02,850 --> 00:18:12,209
death of us graph which just requires to

401
00:18:05,220 --> 00:18:14,669
be robust in very large subsets and so

402
00:18:12,210 --> 00:18:17,340
as I explained before intuitively this

403
00:18:14,669 --> 00:18:20,220
if you just apply the the labeling game

404
00:18:17,340 --> 00:18:24,750
to this graph it gives you a weak proof

405
00:18:20,220 --> 00:18:27,620
of space now what is it bipartite

406
00:18:24,750 --> 00:18:31,169
expander graph there's two sets of

407
00:18:27,620 --> 00:18:33,090
there's sinks and sources and we say

408
00:18:31,169 --> 00:18:37,649
that it's an alpha-beta expander if any

409
00:18:33,090 --> 00:18:40,139
subset of size alpha and a has it's

410
00:18:37,649 --> 00:18:41,939
connected to at least a beta fraction of

411
00:18:40,140 --> 00:18:47,610
beings is said to have beta over alpha

412
00:18:41,940 --> 00:18:49,200
expansion so the first construction step

413
00:18:47,610 --> 00:18:52,049
two takes

414
00:18:49,200 --> 00:18:53,880
copies of a depth of bus graph I mark

415
00:18:52,049 --> 00:18:55,559
your in red the edges of a death robust

416
00:18:53,880 --> 00:18:58,740
graph so every layer has a copy of a

417
00:18:55,559 --> 00:19:00,950
depth of us graph and then adds the

418
00:18:58,740 --> 00:19:05,700
edges of bipartite expander graphs

419
00:19:00,950 --> 00:19:09,750
between them and the labels are derived

420
00:19:05,700 --> 00:19:12,630
on the last layer and stored so let me

421
00:19:09,750 --> 00:19:16,980
give you some intuition about why this

422
00:19:12,630 --> 00:19:19,620
gives you a tight proof of space so

423
00:19:16,980 --> 00:19:21,630
consider sort of just a naive attack

424
00:19:19,620 --> 00:19:23,129
that stores labels on the last level and

425
00:19:21,630 --> 00:19:26,490
forget some of the labels so let's say

426
00:19:23,130 --> 00:19:29,279
it forgets deletes the labels in in red

427
00:19:26,490 --> 00:19:30,960
maybe read arriving just those labels if

428
00:19:29,279 --> 00:19:33,450
it were only one layer would not require

429
00:19:30,960 --> 00:19:35,760
sequential work but because of the

430
00:19:33,450 --> 00:19:36,570
bipartite expander edges if we look at

431
00:19:35,760 --> 00:19:39,179
the

432
00:19:36,570 --> 00:19:41,580
tendencies of these labels on labels and

433
00:19:39,179 --> 00:19:43,200
previously levels that are not being

434
00:19:41,580 --> 00:19:46,199
stored and would need to be read arrived

435
00:19:43,200 --> 00:19:49,320
the dependencies expand as you move up

436
00:19:46,200 --> 00:19:53,309
in the graph until the dependencies

437
00:19:49,320 --> 00:19:56,908
include 80% of some level and since the

438
00:19:53,309 --> 00:19:59,428
graph was depth robust in 80% size sub

439
00:19:56,909 --> 00:20:03,779
graphs read arriving 80% of any given

440
00:19:59,429 --> 00:20:07,649
level requires sequential work and you

441
00:20:03,779 --> 00:20:10,200
can use this to encode data simply by

442
00:20:07,649 --> 00:20:12,840
taking the last layer and x-raying it

443
00:20:10,200 --> 00:20:14,490
with the data file of interest I'm

444
00:20:12,840 --> 00:20:17,189
omitting some details on how you would

445
00:20:14,490 --> 00:20:19,769
modify the the proof but the main point

446
00:20:17,190 --> 00:20:22,590
is that unfortunately this sort of

447
00:20:19,769 --> 00:20:25,590
generic way of using the previous phase

448
00:20:22,590 --> 00:20:27,360
to encode data requires you to read

449
00:20:25,590 --> 00:20:29,279
arrive deterministically to the labels

450
00:20:27,360 --> 00:20:31,620
and so it would be as inefficient to

451
00:20:29,279 --> 00:20:33,000
extract data as it is to generate or

452
00:20:31,620 --> 00:20:38,399
initialize a proof of space to begin

453
00:20:33,000 --> 00:20:39,960
with so the last step is modifying the

454
00:20:38,399 --> 00:20:42,809
structure of this graph and what we do

455
00:20:39,960 --> 00:20:45,929
is we project the edges of the expander

456
00:20:42,809 --> 00:20:48,389
of the by target bipartite expander into

457
00:20:45,929 --> 00:20:50,570
each level and what this does it has the

458
00:20:48,389 --> 00:20:53,758
effect of turning each level into a

459
00:20:50,570 --> 00:20:55,740
expander graph as an undirected graph so

460
00:20:53,759 --> 00:20:58,980
it's an undirected expander but it's a

461
00:20:55,740 --> 00:21:06,870
dag so it's not it's not a you know it's

462
00:20:58,980 --> 00:21:08,460
not actually a expander graph and so and

463
00:21:06,870 --> 00:21:10,620
and so before I tell give you the

464
00:21:08,460 --> 00:21:13,110
security intuition the reason why this

465
00:21:10,620 --> 00:21:15,689
allows you to encode data in a more

466
00:21:13,110 --> 00:21:18,149
efficiently extractable way is that now

467
00:21:15,690 --> 00:21:20,879
the dashed edges will be used to derive

468
00:21:18,149 --> 00:21:23,639
a label which encodes the same index

469
00:21:20,879 --> 00:21:26,129
label on the previous level so if you

470
00:21:23,639 --> 00:21:28,559
look at like c6 it will be derived by

471
00:21:26,129 --> 00:21:31,469
hashing the dependencies of c6 in the

472
00:21:28,559 --> 00:21:34,980
same level and using that as a key to

473
00:21:31,470 --> 00:21:39,629
encode c1 and simply using XOR let's say

474
00:21:34,980 --> 00:21:42,000
and and so the labels on the last level

475
00:21:39,629 --> 00:21:44,370
are actually encoding x' of the labels

476
00:21:42,000 --> 00:21:49,280
on the previous on on the first level

477
00:21:44,370 --> 00:21:51,290
and those can simply be data inputs

478
00:21:49,280 --> 00:21:53,330
so let me give you intuition of why this

479
00:21:51,290 --> 00:21:55,370
still maintains proof of space security

480
00:21:53,330 --> 00:21:58,879
and why we need to reverse the edges in

481
00:21:55,370 --> 00:22:00,679
each level so again the graph is an

482
00:21:58,880 --> 00:22:02,480
expander is an undirected graph so if we

483
00:22:00,680 --> 00:22:06,710
look at the targets and dependencies of

484
00:22:02,480 --> 00:22:10,580
any given node okay that is large and if

485
00:22:06,710 --> 00:22:14,090
we go up one level then the targets of

486
00:22:10,580 --> 00:22:16,580
this node c14 become dependencies of

487
00:22:14,090 --> 00:22:19,040
c---nine which is another label that we

488
00:22:16,580 --> 00:22:23,500
need to read arrive the encoding of c14

489
00:22:19,040 --> 00:22:26,750
and the target the dependencies of of

490
00:22:23,500 --> 00:22:28,850
c14 become the targets which in turn

491
00:22:26,750 --> 00:22:31,640
become dependencies in the next level so

492
00:22:28,850 --> 00:22:34,250
if you bump two levels up then the

493
00:22:31,640 --> 00:22:38,680
dependencies expand just like it did

494
00:22:34,250 --> 00:22:41,660
with bipartite expander graphs and so

495
00:22:38,680 --> 00:22:43,580
with roughly double the number of levels

496
00:22:41,660 --> 00:22:46,400
you get the same effect of dependencies

497
00:22:43,580 --> 00:22:48,919
expanding and this is not a proof the

498
00:22:46,400 --> 00:22:50,510
analysis has to go through more careful

499
00:22:48,920 --> 00:22:55,460
analysis of all the things that prove we

500
00:22:50,510 --> 00:22:57,920
could do but in the end we only need

501
00:22:55,460 --> 00:22:59,210
levels which are proportional to log 1

502
00:22:57,920 --> 00:23:02,810
over epsilon where epsilon is the

503
00:22:59,210 --> 00:23:05,660
tightness we want and the extraction is

504
00:23:02,810 --> 00:23:07,669
paralyze Abul because once you have all

505
00:23:05,660 --> 00:23:09,860
the labels on one level you can in

506
00:23:07,670 --> 00:23:11,540
parallel read arrive the labels on the

507
00:23:09,860 --> 00:23:14,479
previous level until you get back the

508
00:23:11,540 --> 00:23:16,840
data so thank you that is the end of

509
00:23:14,480 --> 00:23:16,840
this talk

510
00:23:22,240 --> 00:23:35,780
are any questions I have a philosophical

511
00:23:32,330 --> 00:23:37,879
question about this proofs of space yes

512
00:23:35,780 --> 00:23:41,210
especially the usable

513
00:23:37,880 --> 00:23:44,780
usability of the space and the fact that

514
00:23:41,210 --> 00:23:46,940
you are having useful proofs that allow

515
00:23:44,780 --> 00:23:49,730
you to continue to store other data yes

516
00:23:46,940 --> 00:23:52,070
like your movies so if I'm trying to

517
00:23:49,730 --> 00:23:55,640
prove that I have two to the 40 memory

518
00:23:52,070 --> 00:23:59,540
actually don't have it but Amazon has it

519
00:23:55,640 --> 00:24:02,840
if Amazon can continue to store in their

520
00:23:59,540 --> 00:24:05,600
databases all the movies that other data

521
00:24:02,840 --> 00:24:09,918
that they are storing and they will let

522
00:24:05,600 --> 00:24:12,469
me rent the the memory for one second

523
00:24:09,919 --> 00:24:15,049
very cheaply because it doesn't cost

524
00:24:12,470 --> 00:24:18,080
them any extra and therefore people will

525
00:24:15,049 --> 00:24:20,210
be able to pretend that they have the

526
00:24:18,080 --> 00:24:22,159
two to the 40 memory even though they

527
00:24:20,210 --> 00:24:25,370
are renting it out so did you think

528
00:24:22,160 --> 00:24:28,669
about the economic issues of proofs of

529
00:24:25,370 --> 00:24:31,549
space especially when they are they yeah

530
00:24:28,669 --> 00:24:34,250
yes no I have thought about that and and

531
00:24:31,549 --> 00:24:36,559
other people have have as well pointed

532
00:24:34,250 --> 00:24:38,809
that out in general about any useful

533
00:24:36,559 --> 00:24:42,530
form like useful proofs of work as well

534
00:24:38,809 --> 00:24:45,020
would have similar issues it's more that

535
00:24:42,530 --> 00:24:47,690
then then Amazon is sort of dominating

536
00:24:45,020 --> 00:24:50,660
the mining of the system but it doesn't

537
00:24:47,690 --> 00:24:52,880
the the the impact is that you don't

538
00:24:50,660 --> 00:24:54,740
have this effect whereas in Bitcoin the

539
00:24:52,880 --> 00:24:57,200
miners are you know economically

540
00:24:54,740 --> 00:24:58,790
committed to Bitcoin as a network

541
00:24:57,200 --> 00:25:01,309
because they're invested in it since

542
00:24:58,790 --> 00:25:03,070
they need their mining hardware in order

543
00:25:01,309 --> 00:25:05,660
and it's not useful for anything else

544
00:25:03,070 --> 00:25:08,360
Amazon doesn't really care about Bitcoin

545
00:25:05,660 --> 00:25:10,419
continue to store files these would be

546
00:25:08,360 --> 00:25:13,850
extremely inefficient for Amazon to run

547
00:25:10,419 --> 00:25:17,390
so maybe we don't want to make them too

548
00:25:13,850 --> 00:25:19,850
efficient but but it's an excellent

549
00:25:17,390 --> 00:25:22,660
point and there's a philosophical debate

550
00:25:19,850 --> 00:25:26,020
about that yes thank you

551
00:25:22,660 --> 00:25:26,020
another question

552
00:25:26,070 --> 00:25:30,790
also related to that property of reusing

553
00:25:28,930 --> 00:25:32,890
or using the preferred space because I'm

554
00:25:30,790 --> 00:25:36,040
one because sometimes you would like to

555
00:25:32,890 --> 00:25:37,330
prove that you have new space so you so

556
00:25:36,040 --> 00:25:40,149
you wouldn't like this property of

557
00:25:37,330 --> 00:25:42,250
reusability actually so is that because

558
00:25:40,150 --> 00:25:44,410
I don't know so I have some proof and

559
00:25:42,250 --> 00:25:46,330
then I I'm doing a proof for someone

560
00:25:44,410 --> 00:25:49,450
else so I don't want maybe to reuse the

561
00:25:46,330 --> 00:25:52,330
same proof yes so reusability doesn't

562
00:25:49,450 --> 00:25:54,280
contradict that goal so the point of the

563
00:25:52,330 --> 00:25:57,340
the proof his face is it's a proof of

564
00:25:54,280 --> 00:26:00,100
persistent space you commit to storing

565
00:25:57,340 --> 00:26:01,360
say a gigabyte of space and you can

566
00:26:00,100 --> 00:26:03,010
continuously prove that you're still

567
00:26:01,360 --> 00:26:05,320
using that gigabyte if you want to then

568
00:26:03,010 --> 00:26:07,270
use more space you can bump that up to

569
00:26:05,320 --> 00:26:08,560
two gigabytes by producing an

570
00:26:07,270 --> 00:26:11,500
independent proof of space with a

571
00:26:08,560 --> 00:26:14,230
different you know protocol identifier

572
00:26:11,500 --> 00:26:15,460
and if those composed then you're

573
00:26:14,230 --> 00:26:16,630
showing that you have twice the amount

574
00:26:15,460 --> 00:26:18,580
of space the point is that you can reuse

575
00:26:16,630 --> 00:26:20,650
the same resources to continue to

576
00:26:18,580 --> 00:26:21,970
produce proofs that you're still using

577
00:26:20,650 --> 00:26:24,280
that and the fact that so it looks like

578
00:26:21,970 --> 00:26:26,170
in your construction like you have a lot

579
00:26:24,280 --> 00:26:28,660
more dependencies in the in the graph

580
00:26:26,170 --> 00:26:31,030
yeah the graph is basically roughly the

581
00:26:28,660 --> 00:26:32,470
same size as previous or the graph the

582
00:26:31,030 --> 00:26:34,629
size of the graph is also bigger like

583
00:26:32,470 --> 00:26:37,240
twice bigger and also and also the

584
00:26:34,630 --> 00:26:38,950
number of edges is bigger so maybe that

585
00:26:37,240 --> 00:26:41,830
poses some constraints on the providers

586
00:26:38,950 --> 00:26:44,110
yes so the the graph that I have is is

587
00:26:41,830 --> 00:26:46,389
considerably bigger than the the data

588
00:26:44,110 --> 00:26:48,370
but the approver only stores the labels

589
00:26:46,390 --> 00:26:51,790
in the last level which is the size of

590
00:26:48,370 --> 00:26:53,830
the data but it requires you know going

591
00:26:51,790 --> 00:26:56,200
through several steps of derivation in

592
00:26:53,830 --> 00:26:58,120
order to get there yeah in fact one

593
00:26:56,200 --> 00:26:59,950
advantage I didn't mention about the

594
00:26:58,120 --> 00:27:04,209
second construction over the first is

595
00:26:59,950 --> 00:27:06,100
that the first construction because it

596
00:27:04,210 --> 00:27:07,840
doesn't have this locality property it

597
00:27:06,100 --> 00:27:09,909
requires the prover tonight

598
00:27:07,840 --> 00:27:12,790
naively the prover would have to use a

599
00:27:09,910 --> 00:27:14,710
buffer of size twice the data in order

600
00:27:12,790 --> 00:27:16,120
to derive the the data storage because

601
00:27:14,710 --> 00:27:17,860
it needs to keep around the labels on

602
00:27:16,120 --> 00:27:19,750
the previous level in order to derive

603
00:27:17,860 --> 00:27:23,199
the next level whereas in the last

604
00:27:19,750 --> 00:27:26,620
construction it can once it has the the

605
00:27:23,200 --> 00:27:28,420
labels on a given level it can basically

606
00:27:26,620 --> 00:27:30,129
replace them one by one as it's deriving

607
00:27:28,420 --> 00:27:33,640
the levels on the next and it only needs

608
00:27:30,130 --> 00:27:36,250
to use overall you know n storage in

609
00:27:33,640 --> 00:27:37,600
order to derive the last level rather

610
00:27:36,250 --> 00:27:42,700
than a buffer of size two

611
00:27:37,600 --> 00:27:52,449
yeah irony any classic questions there's

612
00:27:42,700 --> 00:27:54,730
one so thank you for your talk I had a

613
00:27:52,450 --> 00:27:56,830
question regarding the interactiveness

614
00:27:54,730 --> 00:27:59,740
of the proof in your pictures at least

615
00:27:56,830 --> 00:28:01,870
they were all interactive can they be

616
00:27:59,740 --> 00:28:06,610
made non interactive for the persistent

617
00:28:01,870 --> 00:28:11,049
proof of space that's an interesting

618
00:28:06,610 --> 00:28:13,629
question so classically no however if

619
00:28:11,049 --> 00:28:15,908
you had an eye and eye an ideal

620
00:28:13,630 --> 00:28:18,640
realization of the random beacon one

621
00:28:15,909 --> 00:28:20,830
that would just spit out random numbers

622
00:28:18,640 --> 00:28:22,330
at specific time intervals unpredictable

623
00:28:20,830 --> 00:28:24,789
random numbers and that could be used to

624
00:28:22,330 --> 00:28:27,760
replace the random challenges that come

625
00:28:24,789 --> 00:28:29,530
at intervals from the you know from the

626
00:28:27,760 --> 00:28:32,020
verifier but that requires a realization

627
00:28:29,530 --> 00:28:34,899
of a random beacon and there's proposals

628
00:28:32,020 --> 00:28:37,710
for that but you know requires much

629
00:28:34,900 --> 00:28:37,710
stronger assumptions

