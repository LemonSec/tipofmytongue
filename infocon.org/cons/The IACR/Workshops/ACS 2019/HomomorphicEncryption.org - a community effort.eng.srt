1
00:00:04,870 --> 00:00:11,500
thank you everyone so I'm very lucky to

2
00:00:08,410 --> 00:00:12,879
speak right after great presenter from

3
00:00:11,500 --> 00:00:14,620
NIST because I think it dovetails

4
00:00:12,880 --> 00:00:17,920
extremely well with what we've just

5
00:00:14,620 --> 00:00:19,990
heard so I'd like to tell you about our

6
00:00:17,920 --> 00:00:21,699
home Orphic encryption standardization

7
00:00:19,990 --> 00:00:25,119
effort our community that we've built

8
00:00:21,699 --> 00:00:26,949
over the last two years it's excellent

9
00:00:25,119 --> 00:00:28,779
timing because we actually just had our

10
00:00:26,949 --> 00:00:31,199
fourth standardization workshop

11
00:00:28,779 --> 00:00:32,980
yesterday and Intel and Santa Clara

12
00:00:31,199 --> 00:00:34,330
co-located with usenix

13
00:00:32,980 --> 00:00:37,690
and I'll tell you a little bit more

14
00:00:34,330 --> 00:00:40,149
about that if there's one thing that you

15
00:00:37,690 --> 00:00:42,339
can remember or take away from this talk

16
00:00:40,149 --> 00:00:43,930
is that we actually have published a

17
00:00:42,340 --> 00:00:46,360
draft standard for a home Orphic

18
00:00:43,930 --> 00:00:49,899
encryption which was approved by our

19
00:00:46,360 --> 00:00:52,059
community about a year ago last November

20
00:00:49,899 --> 00:00:54,520
and not only it's been available online

21
00:00:52,059 --> 00:00:55,930
and people tell us that they use it and

22
00:00:54,520 --> 00:00:58,690
there have been a lot of references but

23
00:00:55,930 --> 00:01:00,910
I actually just put it on eprint so it's

24
00:00:58,690 --> 00:01:06,789
just appearing on ePrint like today so

25
00:01:00,910 --> 00:01:08,050
you can access it so another key piece

26
00:01:06,789 --> 00:01:10,060
of information that you should know

27
00:01:08,050 --> 00:01:13,030
about our community is that it's very

28
00:01:10,060 --> 00:01:14,350
open so you're all welcome so you can go

29
00:01:13,030 --> 00:01:17,709
to the website it's homomorphic

30
00:01:14,350 --> 00:01:20,949
encryption org and you can add yourself

31
00:01:17,709 --> 00:01:22,810
to the mailing list so we actually try

32
00:01:20,950 --> 00:01:25,240
to keep the traffic very low on the

33
00:01:22,810 --> 00:01:28,810
mailing list it pertains primarily just

34
00:01:25,240 --> 00:01:31,179
to our standardization organizing

35
00:01:28,810 --> 00:01:32,910
efforts so you don't have to worry if

36
00:01:31,179 --> 00:01:36,039
you join that you'll be inundated

37
00:01:32,910 --> 00:01:38,800
because there's many working groups or

38
00:01:36,039 --> 00:01:41,729
sub mailing lists that you can also join

39
00:01:38,800 --> 00:01:45,490
for specific topics but the main one

40
00:01:41,729 --> 00:01:47,289
standards at home morphic encryption org

41
00:01:45,490 --> 00:01:50,050
we have more than 300 people on our

42
00:01:47,289 --> 00:01:54,940
mailing list now so it's a growing

43
00:01:50,050 --> 00:01:57,239
community so the main organizers of this

44
00:01:54,940 --> 00:02:00,119
effort has started at Microsoft Research

45
00:01:57,239 --> 00:02:04,330
with my team collaborating with

46
00:02:00,119 --> 00:02:06,520
Microsoft research outreach so the the

47
00:02:04,330 --> 00:02:09,160
people that have been co-organizers for

48
00:02:06,520 --> 00:02:11,260
all the workshops so far are Kim liner

49
00:02:09,160 --> 00:02:16,090
from the cryptography group a researcher

50
00:02:11,260 --> 00:02:17,530
and my team myself and Roy and Kim has

51
00:02:16,090 --> 00:02:20,740
done the lion's share

52
00:02:17,530 --> 00:02:24,910
of the work especially in the recent

53
00:02:20,740 --> 00:02:27,220
workshop yesterday and for the second

54
00:02:24,910 --> 00:02:30,370
third and fourth workshops have been

55
00:02:27,220 --> 00:02:35,410
co-organized with duality technologies

56
00:02:30,370 --> 00:02:37,209
so Kurt Rolf and your polikov but we've

57
00:02:35,410 --> 00:02:40,299
had co-organizers from a number of

58
00:02:37,209 --> 00:02:41,830
different institutions in the different

59
00:02:40,300 --> 00:02:44,080
the four different workshops so for

60
00:02:41,830 --> 00:02:47,410
example Lily Chen from NIST was a Corgan

61
00:02:44,080 --> 00:02:49,959
Iser of the first one we've had the

62
00:02:47,410 --> 00:02:51,880
second one was hosted at MIT

63
00:02:49,959 --> 00:02:55,450
jung-hee chen was a co-organizer of the

64
00:02:51,880 --> 00:02:58,870
first one also second one was hosted by

65
00:02:55,450 --> 00:03:01,000
Vinod Vikas - Nathan at MIT and he was

66
00:02:58,870 --> 00:03:04,780
of coroner for the first three workshops

67
00:03:01,000 --> 00:03:08,410
Glenn Gulick at Toronto Kazmir ocinski

68
00:03:04,780 --> 00:03:11,290
at Intel and one Transco so from epfl

69
00:03:08,410 --> 00:03:13,900
so you can see a wide range of people

70
00:03:11,290 --> 00:03:15,910
and institutions are represented in the

71
00:03:13,900 --> 00:03:17,830
in the actual formal organising

72
00:03:15,910 --> 00:03:21,160
committee but we've had a great deal of

73
00:03:17,830 --> 00:03:24,640
input from all different directions both

74
00:03:21,160 --> 00:03:28,060
researchers and companies in in our

75
00:03:24,640 --> 00:03:30,130
effort so for one thing I put a partial

76
00:03:28,060 --> 00:03:31,630
list of kind of government agencies and

77
00:03:30,130 --> 00:03:35,260
standardization bodies who have been

78
00:03:31,630 --> 00:03:38,739
involved at being at panels or giving

79
00:03:35,260 --> 00:03:41,470
short presentations so certainly NIST

80
00:03:38,739 --> 00:03:43,660
was represented at all three of the the

81
00:03:41,470 --> 00:03:45,670
first three but unfortunately for our

82
00:03:43,660 --> 00:03:48,130
timing with the NIST workshop happening

83
00:03:45,670 --> 00:03:53,108
this weekend Lilly was not able to

84
00:03:48,130 --> 00:03:56,320
attend yesterday but NIH participated in

85
00:03:53,109 --> 00:03:59,109
the first three workshops NSA has been

86
00:03:56,320 --> 00:04:01,660
represented I think in the first three

87
00:03:59,109 --> 00:04:04,150
and SF the Canadian security

88
00:04:01,660 --> 00:04:06,660
establishment Canadian ministries such

89
00:04:04,150 --> 00:04:09,670
as Ministry of Health asked to be

90
00:04:06,660 --> 00:04:12,100
invited to the one in Toronto for

91
00:04:09,670 --> 00:04:14,260
example the Korean credit bureau came to

92
00:04:12,100 --> 00:04:17,858
the one at MIT and representatives from

93
00:04:14,260 --> 00:04:20,978
the UN ITU Working Group so we've had a

94
00:04:17,858 --> 00:04:22,719
lot of interest from both you know

95
00:04:20,978 --> 00:04:26,229
standardizing bodies and government

96
00:04:22,720 --> 00:04:28,390
agencies and people at NSA have actually

97
00:04:26,229 --> 00:04:29,780
told us that having our state standard

98
00:04:28,390 --> 00:04:32,150
available online has been

99
00:04:29,780 --> 00:04:34,940
extremely useful to them because they

100
00:04:32,150 --> 00:04:36,650
can pass around this document internally

101
00:04:34,940 --> 00:04:39,110
in the classified environment and reason

102
00:04:36,650 --> 00:04:43,669
about the status quo and thus and the

103
00:04:39,110 --> 00:04:46,010
status so so this was the first workshop

104
00:04:43,669 --> 00:04:47,900
so this is how we kind of got this work

105
00:04:46,010 --> 00:04:49,190
this effort off the ground

106
00:04:47,900 --> 00:04:52,150
I mean honestly we had been talking

107
00:04:49,190 --> 00:04:57,200
about it for a couple of years with

108
00:04:52,150 --> 00:05:00,469
people including Kurt at duality and

109
00:04:57,200 --> 00:05:02,840
Shai Halevi at IBM and Vinod Vicodin

110
00:05:00,470 --> 00:05:04,310
authen at MIT and the way that we

111
00:05:02,840 --> 00:05:06,679
actually got it off the ground is

112
00:05:04,310 --> 00:05:10,280
Microsoft funded it we had Microsoft

113
00:05:06,680 --> 00:05:12,200
outreach brought 36 people to campus it

114
00:05:10,280 --> 00:05:16,280
was invitation only it was three groups

115
00:05:12,200 --> 00:05:19,159
of 12 and it was not a like it was not a

116
00:05:16,280 --> 00:05:21,559
junket it was a working conference it

117
00:05:19,160 --> 00:05:24,110
was - it was basically two days in which

118
00:05:21,560 --> 00:05:25,910
in groups of 12 we wrote three white

119
00:05:24,110 --> 00:05:28,190
papers we actually wrote them in two

120
00:05:25,910 --> 00:05:29,660
days and that was possible because we

121
00:05:28,190 --> 00:05:32,030
were bringing experts in this area

122
00:05:29,660 --> 00:05:33,770
together and setting the expectation

123
00:05:32,030 --> 00:05:37,130
that we wanted to get something done and

124
00:05:33,770 --> 00:05:39,229
we just worked really hard and so within

125
00:05:37,130 --> 00:05:40,669
about two weeks of the workshop we said

126
00:05:39,229 --> 00:05:42,859
we kind of threatened everyone we're

127
00:05:40,669 --> 00:05:44,390
putting these white papers online so if

128
00:05:42,860 --> 00:05:46,310
you want to make comments if you want to

129
00:05:44,390 --> 00:05:49,520
make changes you know they're just white

130
00:05:46,310 --> 00:05:51,740
papers right so you know we weren't

131
00:05:49,520 --> 00:05:53,359
trying to be bullies but we just wanted

132
00:05:51,740 --> 00:05:55,789
to make sure that there was some

133
00:05:53,360 --> 00:05:57,770
concrete outcomes and so these white

134
00:05:55,789 --> 00:06:00,680
papers went online almost immediately

135
00:05:57,770 --> 00:06:02,210
and the way it was divided was one that

136
00:06:00,680 --> 00:06:05,660
was one working group focused on

137
00:06:02,210 --> 00:06:10,640
security one focused on api's and one

138
00:06:05,660 --> 00:06:16,789
focused on applications and so what we

139
00:06:10,640 --> 00:06:18,979
did was the we kind of decided on the

140
00:06:16,789 --> 00:06:23,719
the next workshop so the next workshop

141
00:06:18,979 --> 00:06:26,409
was at MIT in March of 2018 so it was

142
00:06:23,720 --> 00:06:29,300
you know roughly nine months later and

143
00:06:26,410 --> 00:06:32,210
what we had kind of decided at the first

144
00:06:29,300 --> 00:06:33,590
workshop was that we just we discussed a

145
00:06:32,210 --> 00:06:37,190
lot of different paths to

146
00:06:33,590 --> 00:06:39,080
standardization and since we had many of

147
00:06:37,190 --> 00:06:40,640
the relevant people in the room there

148
00:06:39,080 --> 00:06:43,219
were many other experts in homomorphic

149
00:06:40,640 --> 00:06:43,599
encryption who had also been invited but

150
00:06:43,220 --> 00:06:46,180
who

151
00:06:43,600 --> 00:06:48,610
come and we also wanted to capture their

152
00:06:46,180 --> 00:06:50,980
input as well so we decided that's when

153
00:06:48,610 --> 00:06:52,780
we decided to create the website create

154
00:06:50,980 --> 00:06:56,320
the mailing list and try to make it an

155
00:06:52,780 --> 00:06:58,960
open community and solicit input from as

156
00:06:56,320 --> 00:07:01,960
many people as possible and we decided

157
00:06:58,960 --> 00:07:03,909
among the standardization efforts open

158
00:07:01,960 --> 00:07:07,450
to us the only one that we can control

159
00:07:03,910 --> 00:07:09,970
is to create our own kind of essentially

160
00:07:07,450 --> 00:07:12,969
de-facto standard basically it's like an

161
00:07:09,970 --> 00:07:15,610
industry consortium so our group

162
00:07:12,970 --> 00:07:18,060
homework encryption org currently has no

163
00:07:15,610 --> 00:07:22,600
governance it's a completely open

164
00:07:18,060 --> 00:07:25,150
volunteer effort and so that has really

165
00:07:22,600 --> 00:07:27,100
good parts to it being you know anybody

166
00:07:25,150 --> 00:07:29,260
can join anybody can contribute we ask

167
00:07:27,100 --> 00:07:31,510
for volunteers at every meeting people

168
00:07:29,260 --> 00:07:34,060
step up and volunteer to help with

169
00:07:31,510 --> 00:07:36,400
different things preparing documents

170
00:07:34,060 --> 00:07:39,910
giving presentations all kinds of things

171
00:07:36,400 --> 00:07:41,979
so that's great but we don't have the

172
00:07:39,910 --> 00:07:45,700
structure that a standardization body

173
00:07:41,980 --> 00:07:47,920
has or a government agency has so and

174
00:07:45,700 --> 00:07:49,750
whatever we create there's nothing as

175
00:07:47,920 --> 00:07:52,150
binding it's only the you know the

176
00:07:49,750 --> 00:07:54,010
recommendations that people and the

177
00:07:52,150 --> 00:07:56,409
community can see that's there you know

178
00:07:54,010 --> 00:07:59,380
there's a consortium of people that have

179
00:07:56,410 --> 00:08:02,830
created these this document on the other

180
00:07:59,380 --> 00:08:04,780
hand on the flipside because you know

181
00:08:02,830 --> 00:08:07,359
because it's very open and collaborative

182
00:08:04,780 --> 00:08:09,969
we've had really great participation

183
00:08:07,360 --> 00:08:13,840
from companies actually around the world

184
00:08:09,970 --> 00:08:16,360
and industry participants participation

185
00:08:13,840 --> 00:08:19,359
has been kind of skyrocketing meeting

186
00:08:16,360 --> 00:08:21,940
after meeting over meeting and so you

187
00:08:19,360 --> 00:08:24,340
know I think that that it's an

188
00:08:21,940 --> 00:08:26,110
attractive venue for people to come to

189
00:08:24,340 --> 00:08:27,940
learn about homomorphic encryption so

190
00:08:26,110 --> 00:08:29,590
engineers and they you know can ask

191
00:08:27,940 --> 00:08:31,870
questions about things like

192
00:08:29,590 --> 00:08:35,260
interoperability or certification

193
00:08:31,870 --> 00:08:37,740
accreditation that type of stuff and it

194
00:08:35,260 --> 00:08:40,630
also it's been honestly a place where

195
00:08:37,740 --> 00:08:42,849
people can come who are looking for jobs

196
00:08:40,630 --> 00:08:44,710
and people students come and they we

197
00:08:42,849 --> 00:08:46,150
always have a very active poster session

198
00:08:44,710 --> 00:08:47,950
there's a number of students that have

199
00:08:46,150 --> 00:08:49,959
gotten hired through connections that

200
00:08:47,950 --> 00:08:52,120
they've made at these workshops so it's

201
00:08:49,960 --> 00:08:56,550
really a it functions as a community and

202
00:08:52,120 --> 00:08:59,790
a lot of different levels so

203
00:08:56,550 --> 00:09:03,899
the draft standard that we came out of

204
00:08:59,790 --> 00:09:05,610
the the first workshop let me let me

205
00:09:03,899 --> 00:09:08,149
tell you how we kind of decided how to

206
00:09:05,610 --> 00:09:12,120
proceed so we had these three tracks

207
00:09:08,149 --> 00:09:14,459
security api's and applications and we

208
00:09:12,120 --> 00:09:16,680
kind of said to ourselves well nothing

209
00:09:14,459 --> 00:09:18,630
can happen without people having

210
00:09:16,680 --> 00:09:21,239
confidence in the underlying mathematics

211
00:09:18,630 --> 00:09:23,970
and the underlying hard problem and of

212
00:09:21,240 --> 00:09:26,360
all the libraries that exist in the

213
00:09:23,970 --> 00:09:29,700
world for public key and for home Orphic

214
00:09:26,360 --> 00:09:31,529
encryption they were all based on our

215
00:09:29,700 --> 00:09:33,000
lwe the ring learning with errors

216
00:09:31,529 --> 00:09:35,880
problem which was related to hard

217
00:09:33,000 --> 00:09:37,709
lattice problems and so we decided to

218
00:09:35,880 --> 00:09:39,779
actually standardize the kind of the

219
00:09:37,709 --> 00:09:42,779
hard you know math problem underneath

220
00:09:39,779 --> 00:09:45,630
the current versions of homomorphic

221
00:09:42,779 --> 00:09:47,820
encryption and really try to set the

222
00:09:45,630 --> 00:09:50,310
parameters to achieve certain security

223
00:09:47,820 --> 00:09:51,779
levels so that was our goal because we

224
00:09:50,310 --> 00:09:54,899
figured first of all that takes time

225
00:09:51,779 --> 00:09:57,120
once we kind of collect the communities

226
00:09:54,899 --> 00:09:58,709
collect you know knowledge about

227
00:09:57,120 --> 00:10:00,839
security levels for a lattice base

228
00:09:58,709 --> 00:10:02,760
cryptography and we translate them

229
00:10:00,839 --> 00:10:05,310
through through a lot of concrete work

230
00:10:02,760 --> 00:10:07,110
that a lot of different computational

231
00:10:05,310 --> 00:10:09,779
number theorists have done once we

232
00:10:07,110 --> 00:10:13,230
translate that into concrete parameters

233
00:10:09,779 --> 00:10:15,120
that we propose that those become kind

234
00:10:13,230 --> 00:10:17,430
of de-facto challenges so our standard

235
00:10:15,120 --> 00:10:20,040
is out there anyone can attack any one

236
00:10:17,430 --> 00:10:22,739
of those rows in our table and so now

237
00:10:20,040 --> 00:10:25,380
we've since we've put this out now you

238
00:10:22,740 --> 00:10:27,540
know a year ago we can stand behind this

239
00:10:25,380 --> 00:10:29,670
and have let the community think about

240
00:10:27,540 --> 00:10:32,160
you know attacking these and get used to

241
00:10:29,670 --> 00:10:33,899
these are the levels that we stand

242
00:10:32,160 --> 00:10:35,490
behind these are the security levels

243
00:10:33,899 --> 00:10:38,880
that we think we're achieving with these

244
00:10:35,490 --> 00:10:41,820
parameters and then so now where we're

245
00:10:38,880 --> 00:10:44,430
going is how do you build on that so let

246
00:10:41,820 --> 00:10:47,279
me tell you a little bit about okay so

247
00:10:44,430 --> 00:10:50,339
this was the MIT workshop in March 2018

248
00:10:47,279 --> 00:10:52,170
where we approved the draft standard we

249
00:10:50,339 --> 00:10:54,589
actually asked for co-signers of the

250
00:10:52,170 --> 00:10:57,870
meeting so there's actually 16

251
00:10:54,589 --> 00:11:00,149
co-authors on the document and there

252
00:10:57,870 --> 00:11:03,240
were more than 50 co-signers so we have

253
00:11:00,149 --> 00:11:07,680
like more than 65 co-signers of our

254
00:11:03,240 --> 00:11:09,710
standard and then actually after that

255
00:11:07,680 --> 00:11:12,380
was a good

256
00:11:09,710 --> 00:11:14,330
I was able to like twist everyone's arm

257
00:11:12,380 --> 00:11:16,010
to get the white papers out within two

258
00:11:14,330 --> 00:11:18,470
weeks after the first workshop but I

259
00:11:16,010 --> 00:11:21,170
wasn't able to twist everyone's arm to

260
00:11:18,470 --> 00:11:23,780
give their input on the standard before

261
00:11:21,170 --> 00:11:26,390
the March meeting and so what we did was

262
00:11:23,780 --> 00:11:29,660
we collected quite a lot of input after

263
00:11:26,390 --> 00:11:31,699
the second workshop and we incorporated

264
00:11:29,660 --> 00:11:34,089
they incorporated that into the draft

265
00:11:31,700 --> 00:11:39,170
that's now available online that became

266
00:11:34,090 --> 00:11:41,780
kind of public in November 2018 so at

267
00:11:39,170 --> 00:11:43,910
the Toronto workshop we kind of approved

268
00:11:41,780 --> 00:11:46,910
we approved the changes that had been

269
00:11:43,910 --> 00:11:49,339
made since the marked workshop and we

270
00:11:46,910 --> 00:11:52,910
kind of planned for for future versions

271
00:11:49,340 --> 00:11:55,190
and so what was very exciting is is that

272
00:11:52,910 --> 00:11:57,650
yesterday in Santa Clara

273
00:11:55,190 --> 00:12:00,590
our fourth workshop was hosted by Intel

274
00:11:57,650 --> 00:12:02,510
so in the meantime you know based on the

275
00:12:00,590 --> 00:12:04,340
seal library so I haven't said much

276
00:12:02,510 --> 00:12:06,740
about individual libraries yet but my

277
00:12:04,340 --> 00:12:08,960
team at Microsoft Research has built and

278
00:12:06,740 --> 00:12:11,810
released the seal homomorphic encryption

279
00:12:08,960 --> 00:12:14,210
library it's available open source under

280
00:12:11,810 --> 00:12:17,000
the MIT license so it's available for

281
00:12:14,210 --> 00:12:18,590
commercial use in particular intel has

282
00:12:17,000 --> 00:12:21,170
built on this and they're using this as

283
00:12:18,590 --> 00:12:23,330
the engine for their n graph which is

284
00:12:21,170 --> 00:12:25,610
like a machine learning like deep

285
00:12:23,330 --> 00:12:27,260
learning predictions and there's a

286
00:12:25,610 --> 00:12:30,080
number of other companies that have

287
00:12:27,260 --> 00:12:32,660
started to either use it or write press

288
00:12:30,080 --> 00:12:36,470
releases or experiment with it so this

289
00:12:32,660 --> 00:12:38,630
this Intel's kind of partnership on this

290
00:12:36,470 --> 00:12:40,880
was really great to see them step up and

291
00:12:38,630 --> 00:12:44,390
say that they wanted to host this

292
00:12:40,880 --> 00:12:46,400
workshop at Intel and it was co-located

293
00:12:44,390 --> 00:12:49,699
with a Usenet security which was in

294
00:12:46,400 --> 00:12:51,650
Santa Clara last week and so in each of

295
00:12:49,700 --> 00:12:53,690
these workshops us the second third and

296
00:12:51,650 --> 00:12:56,990
fourth workshops we've had about 70

297
00:12:53,690 --> 00:12:58,700
participants and it's worth taking a

298
00:12:56,990 --> 00:13:00,980
note of that because as I said the first

299
00:12:58,700 --> 00:13:04,250
one when we kick-started this effort we

300
00:13:00,980 --> 00:13:06,500
invited everyone to Microsoft to to

301
00:13:04,250 --> 00:13:08,840
Redmond and paid for everyone to come

302
00:13:06,500 --> 00:13:10,850
everyone who could accept payment every

303
00:13:08,840 --> 00:13:13,490
workshop since then there's been no

304
00:13:10,850 --> 00:13:15,560
funding for travel every participant has

305
00:13:13,490 --> 00:13:17,600
paid for their own travel to come to

306
00:13:15,560 --> 00:13:21,050
these workshops and we've had about 70

307
00:13:17,600 --> 00:13:23,170
people at each one and another thing

308
00:13:21,050 --> 00:13:28,390
that was really remarkable about

309
00:13:23,170 --> 00:13:30,880
yesterday's workshop is that the so

310
00:13:28,390 --> 00:13:33,819
there were actually more than 919 or

311
00:13:30,880 --> 00:13:37,180
more companies that were there and which

312
00:13:33,820 --> 00:13:42,670
was an increase over previous previous

313
00:13:37,180 --> 00:13:45,370
ones so what did we do yesterday so we

314
00:13:42,670 --> 00:13:48,010
kind of discussed we had have reviewed

315
00:13:45,370 --> 00:13:51,610
possible extensions to the security

316
00:13:48,010 --> 00:13:54,189
standard we discussed the next steps for

317
00:13:51,610 --> 00:13:55,810
kind of standardizing the schemes the

318
00:13:54,190 --> 00:13:57,940
homework encryption schemes that are

319
00:13:55,810 --> 00:14:00,849
used for the different libraries there's

320
00:13:57,940 --> 00:14:03,670
about four different scheme all based on

321
00:14:00,850 --> 00:14:07,600
the hardness of our lwe which is related

322
00:14:03,670 --> 00:14:09,729
to hard lattice problems and we just we

323
00:14:07,600 --> 00:14:12,130
also discussed a governance proposal

324
00:14:09,730 --> 00:14:15,780
possibly formalizing our community a

325
00:14:12,130 --> 00:14:19,689
little bit more and some other issues

326
00:14:15,780 --> 00:14:21,370
the one of the big issues always that

327
00:14:19,690 --> 00:14:22,750
each workshop is where is the next one

328
00:14:21,370 --> 00:14:24,850
going to be and so we actually have

329
00:14:22,750 --> 00:14:26,710
three proposals teams wanting to host

330
00:14:24,850 --> 00:14:29,350
the next one which is also a great sign

331
00:14:26,710 --> 00:14:30,850
that there's a lot of support and

332
00:14:29,350 --> 00:14:32,830
interest in being involved in this

333
00:14:30,850 --> 00:14:36,040
community so Seoul National University

334
00:14:32,830 --> 00:14:38,050
and Samsung are very interested in

335
00:14:36,040 --> 00:14:39,939
hosting this in Korea and we think we

336
00:14:38,050 --> 00:14:43,359
can connect with a lot of Asian

337
00:14:39,940 --> 00:14:47,470
companies and you know possibly

338
00:14:43,360 --> 00:14:49,060
regulators as well and we we have a

339
00:14:47,470 --> 00:14:52,630
proposal from epfl

340
00:14:49,060 --> 00:14:55,300
and startup called infer to host it in

341
00:14:52,630 --> 00:14:59,350
Switzerland possibly co-located with the

342
00:14:55,300 --> 00:15:01,079
UN ITU AI for good summit that they're

343
00:14:59,350 --> 00:15:03,960
having in May this would be right before

344
00:15:01,080 --> 00:15:06,700
Yura crypt so we have a couple of

345
00:15:03,960 --> 00:15:09,280
possible next steps for the next

346
00:15:06,700 --> 00:15:11,140
workshop there's still a lot of work to

347
00:15:09,280 --> 00:15:13,180
be done so we plan to continue kind of

348
00:15:11,140 --> 00:15:17,110
working having these workshops on this

349
00:15:13,180 --> 00:15:19,030
cadence so I mean just a couple of notes

350
00:15:17,110 --> 00:15:22,510
that I put up here this was the the

351
00:15:19,030 --> 00:15:24,699
Toronto workshop is that you know as you

352
00:15:22,510 --> 00:15:27,640
could kind of hear in the previous talk

353
00:15:24,700 --> 00:15:29,800
I mean open standards in cryptography

354
00:15:27,640 --> 00:15:32,680
are kind of preferable because since

355
00:15:29,800 --> 00:15:34,569
cryptography is inherently secret I mean

356
00:15:32,680 --> 00:15:36,790
somehow the community in the public

357
00:15:34,570 --> 00:15:39,220
needs to understand whether they should

358
00:15:36,790 --> 00:15:42,040
trust some new crypto that's coming

359
00:15:39,220 --> 00:15:43,420
along and so we feel that open standards

360
00:15:42,040 --> 00:15:45,760
are really preferable and the

361
00:15:43,420 --> 00:15:48,430
standardization process creates trust

362
00:15:45,760 --> 00:15:50,170
it's currently creating trust between

363
00:15:48,430 --> 00:15:52,510
many of us in our community between

364
00:15:50,170 --> 00:15:56,380
companies and government agencies and

365
00:15:52,510 --> 00:15:58,990
researchers but also a lot of regulated

366
00:15:56,380 --> 00:16:01,360
industries require standardization and

367
00:15:58,990 --> 00:16:03,220
they will eventually require some kind

368
00:16:01,360 --> 00:16:05,770
of certification process so that's what

369
00:16:03,220 --> 00:16:10,800
we're aiming for in the longer term like

370
00:16:05,770 --> 00:16:10,800
let's say in the 10 year time frame so

371
00:16:11,760 --> 00:16:17,590
what I wanted to do is to just tell you

372
00:16:15,340 --> 00:16:20,020
a little bit about the resources for you

373
00:16:17,590 --> 00:16:21,400
to get involved if you're interested so

374
00:16:20,020 --> 00:16:24,040
this is kind of important points so I

375
00:16:21,400 --> 00:16:26,829
put this up front so if you go to the

376
00:16:24,040 --> 00:16:29,319
homework encryption org website what

377
00:16:26,830 --> 00:16:31,480
will you find there you can find like

378
00:16:29,320 --> 00:16:33,430
links to all of the workshops links to

379
00:16:31,480 --> 00:16:35,140
the white paper links to the standards

380
00:16:33,430 --> 00:16:37,180
so those are really important but

381
00:16:35,140 --> 00:16:39,189
there's also other things like you know

382
00:16:37,180 --> 00:16:41,770
lists of all the publicly available

383
00:16:39,190 --> 00:16:44,830
libraries the news and events things

384
00:16:41,770 --> 00:16:48,640
things like that so you can actually

385
00:16:44,830 --> 00:16:52,180
send any comments on the website or

386
00:16:48,640 --> 00:16:54,340
whatever to this email contact or if you

387
00:16:52,180 --> 00:16:57,579
want to volunteer or just ask a question

388
00:16:54,340 --> 00:17:00,370
or whatever you can send us email you

389
00:16:57,580 --> 00:17:02,980
can opt in or out of the news channel or

390
00:17:00,370 --> 00:17:04,810
other channels that we have and we also

391
00:17:02,980 --> 00:17:06,730
have a twitter handle so for example

392
00:17:04,810 --> 00:17:09,129
from yesterday's a workshop we have a

393
00:17:06,730 --> 00:17:11,140
ton of tweets online with covering you

394
00:17:09,130 --> 00:17:13,920
know what the content what content was

395
00:17:11,140 --> 00:17:18,010
being presented and pictures and stuff

396
00:17:13,920 --> 00:17:19,360
so just to kind of give you an idea for

397
00:17:18,010 --> 00:17:21,700
those of you that are not really

398
00:17:19,359 --> 00:17:23,349
familiar there are a number of publicly

399
00:17:21,700 --> 00:17:25,240
available home morphic encryption

400
00:17:23,349 --> 00:17:28,300
libraries in the world and it has

401
00:17:25,240 --> 00:17:31,000
evolved quite fast in the last 5 years

402
00:17:28,300 --> 00:17:33,970
or so so the first one that was publicly

403
00:17:31,000 --> 00:17:36,730
available is from IBM h e lib so Shia

404
00:17:33,970 --> 00:17:38,470
Levy and his collaborators and Shai has

405
00:17:36,730 --> 00:17:40,840
been a major contributor and

406
00:17:38,470 --> 00:17:46,120
collaborator for these this series of

407
00:17:40,840 --> 00:17:49,290
workshops so it was widely used by by

408
00:17:46,120 --> 00:17:50,860
researchers and based on the bgv scheme

409
00:17:49,290 --> 00:17:55,418
it started

410
00:17:50,860 --> 00:17:59,979
in 2050 Microsoft seal has been publicly

411
00:17:55,419 --> 00:18:01,299
of a research and as of 2018 it was open

412
00:17:59,980 --> 00:18:03,640
it's released out of the open-source

413
00:18:01,299 --> 00:18:06,789
license for the MIT license for

414
00:18:03,640 --> 00:18:09,190
commercial use as well seal is designed

415
00:18:06,789 --> 00:18:11,169
to be a very robust well engineered and

416
00:18:09,190 --> 00:18:14,049
well documented library that's intended

417
00:18:11,169 --> 00:18:15,970
for commercial use and we've gotten a

418
00:18:14,049 --> 00:18:18,158
lot of good feedback on it over time

419
00:18:15,970 --> 00:18:20,950
there are a number of other libraries

420
00:18:18,159 --> 00:18:24,940
such as NFL Lib that was developed at

421
00:18:20,950 --> 00:18:27,840
crypto experts I had to step out I think

422
00:18:24,940 --> 00:18:32,019
but 10 credit worked on that palisade

423
00:18:27,840 --> 00:18:34,330
from 2017 on was has been developed I

424
00:18:32,019 --> 00:18:39,000
think with funding from a DARPA grant so

425
00:18:34,330 --> 00:18:41,559
Yuri polikov and and Curt roll-off major

426
00:18:39,000 --> 00:18:44,500
developers for that there's a GPU

427
00:18:41,559 --> 00:18:47,950
library coming out of our Chester

428
00:18:44,500 --> 00:18:50,409
Polytechnic it's kyookie I'm not sure if

429
00:18:47,950 --> 00:18:52,000
I'm pronouncing that right there is the

430
00:18:50,409 --> 00:18:54,429
library coming out of Seoul National

431
00:18:52,000 --> 00:18:57,250
University Heian which supports

432
00:18:54,429 --> 00:19:00,279
approximate arithmetic that's been

433
00:18:57,250 --> 00:19:02,830
available since 2017 and then there's

434
00:19:00,279 --> 00:19:04,570
also a slightly different approach which

435
00:19:02,830 --> 00:19:07,000
is based on kind of bootstrapping out

436
00:19:04,570 --> 00:19:08,889
after every gate the few scheme and

437
00:19:07,000 --> 00:19:12,130
library that came out in 2015 from

438
00:19:08,889 --> 00:19:15,459
Danielle Amy Chianti oh and and Leo

439
00:19:12,130 --> 00:19:18,370
dukkha and now the TFA G is in a sense

440
00:19:15,460 --> 00:19:22,360
an extension of that work tfhe has been

441
00:19:18,370 --> 00:19:26,139
available since 2017 and the the infer

442
00:19:22,360 --> 00:19:28,168
team has been developing tfhe was has

443
00:19:26,139 --> 00:19:31,750
been represented at our workshop so

444
00:19:28,169 --> 00:19:34,240
Maria and Nikola and and daemon Tara was

445
00:19:31,750 --> 00:19:35,740
we're all there yesterday so we found

446
00:19:34,240 --> 00:19:37,960
out about one more library yesterday

447
00:19:35,740 --> 00:19:43,179
that I didn't know about lot ego has now

448
00:19:37,960 --> 00:19:45,399
been developed at EPFL for a specific

449
00:19:43,179 --> 00:19:46,929
application that they're trying to do in

450
00:19:45,399 --> 00:19:50,439
the medical sector and that was very

451
00:19:46,929 --> 00:19:53,049
interesting to hear about so as I said

452
00:19:50,440 --> 00:19:55,630
these were this this was to begin with

453
00:19:53,049 --> 00:19:58,779
these are our principles we want it to

454
00:19:55,630 --> 00:19:59,470
standardize the security first sorry

455
00:19:58,779 --> 00:20:01,899
about that

456
00:19:59,470 --> 00:20:04,360
and then to move on to api's and

457
00:20:01,899 --> 00:20:08,549
applications and so we're basically on

458
00:20:04,360 --> 00:20:11,019
step two so now step two and three and

459
00:20:08,549 --> 00:20:14,668
these are we've stuck with our original

460
00:20:11,019 --> 00:20:17,679
principles of kind of open participation

461
00:20:14,669 --> 00:20:20,350
communication we we send out emails via

462
00:20:17,679 --> 00:20:22,480
the mailing list for example when the

463
00:20:20,350 --> 00:20:24,370
draft standard is ready or when we want

464
00:20:22,480 --> 00:20:27,190
to start a new working group to work on

465
00:20:24,370 --> 00:20:29,080
some particular project emails going out

466
00:20:27,190 --> 00:20:31,899
saying who would like to volunteer or

467
00:20:29,080 --> 00:20:33,490
could you please give your input or this

468
00:20:31,899 --> 00:20:35,439
will be voted on at the next workshop

469
00:20:33,490 --> 00:20:37,659
can you please send comments by

470
00:20:35,440 --> 00:20:39,820
such-and-such a time so that's kind of

471
00:20:37,659 --> 00:20:41,620
how it works and then approval is done

472
00:20:39,820 --> 00:20:46,000
usually by things like a show of hand

473
00:20:41,620 --> 00:20:47,379
and stuff at the workshops so one thing

474
00:20:46,000 --> 00:20:49,779
I want to make sure to do is just to

475
00:20:47,379 --> 00:20:53,590
show you just a couple of the pages from

476
00:20:49,779 --> 00:20:55,360
the document and like I said the history

477
00:20:53,590 --> 00:20:57,070
of the document was that at the first

478
00:20:55,360 --> 00:21:00,029
workshop we decided that we would just

479
00:20:57,070 --> 00:21:02,139
create this de facto standard and

480
00:21:00,029 --> 00:21:05,500
circulated and get feedback and then

481
00:21:02,139 --> 00:21:07,629
approve it and like I said we got a lot

482
00:21:05,500 --> 00:21:10,450
of input in the meantime from people

483
00:21:07,629 --> 00:21:14,110
other than the first twelve authors so

484
00:21:10,450 --> 00:21:16,269
there's 16 co-authors on the document so

485
00:21:14,110 --> 00:21:17,799
this is just the first page of the

486
00:21:16,269 --> 00:21:22,000
document it's called the homomorphic

487
00:21:17,799 --> 00:21:24,879
encryption stand standard it h es or h

488
00:21:22,000 --> 00:21:28,000
es 1.0 in a sense it's the first version

489
00:21:24,879 --> 00:21:30,070
we expect this to evolve over time so we

490
00:21:28,000 --> 00:21:32,559
also wanted to set the expectation that

491
00:21:30,070 --> 00:21:34,928
we're choosing our security parameters

492
00:21:32,559 --> 00:21:38,799
very conservatively based on currently

493
00:21:34,929 --> 00:21:41,200
known estimates for attacks we expect of

494
00:21:38,799 --> 00:21:44,590
course attacks to evolve a little bit

495
00:21:41,200 --> 00:21:48,159
over time unless we get an incredibly

496
00:21:44,590 --> 00:21:50,320
disruptive new attack we expect these

497
00:21:48,159 --> 00:21:53,350
parameters to be safe for quite a while

498
00:21:50,320 --> 00:21:55,509
and to be a we could potentially if

499
00:21:53,350 --> 00:21:58,149
needed modify them slightly like

500
00:21:55,509 --> 00:22:00,580
gradually over time so that would be the

501
00:21:58,149 --> 00:22:03,340
plan in the absence of a disruptive

502
00:22:00,580 --> 00:22:05,710
attack on lattice problems and so we

503
00:22:03,340 --> 00:22:08,649
tried to kind of set that expectation we

504
00:22:05,710 --> 00:22:10,090
also have post quantum security for

505
00:22:08,649 --> 00:22:12,290
these latticed problems and we've

506
00:22:10,090 --> 00:22:16,129
actually officially asked

507
00:22:12,290 --> 00:22:18,260
if as part of the pqc process if we

508
00:22:16,130 --> 00:22:19,970
could potentially standardize a larger

509
00:22:18,260 --> 00:22:22,640
range of parameters that allow for

510
00:22:19,970 --> 00:22:24,290
homomorphic encryption instead of just

511
00:22:22,640 --> 00:22:27,290
the smaller parameters that you would

512
00:22:24,290 --> 00:22:29,659
use for key exchange so we'll see what I

513
00:22:27,290 --> 00:22:31,520
mean by that when I go to this slide

514
00:22:29,660 --> 00:22:34,550
okay so sorry that this is just the

515
00:22:31,520 --> 00:22:37,100
first table in the end of the document

516
00:22:34,550 --> 00:22:39,020
sorry for all the numbers here but if

517
00:22:37,100 --> 00:22:41,990
you just look at for example the first

518
00:22:39,020 --> 00:22:43,970
line so what's happening here is we have

519
00:22:41,990 --> 00:22:46,280
three different tables depending on how

520
00:22:43,970 --> 00:22:49,970
you choose the secret for your

521
00:22:46,280 --> 00:22:53,180
encryption and here the most obviously a

522
00:22:49,970 --> 00:22:55,850
secure approach is to choose the secret

523
00:22:53,180 --> 00:22:58,040
vector uniformly at random so this is

524
00:22:55,850 --> 00:22:59,659
the uniform secret table that's why it

525
00:22:58,040 --> 00:23:02,030
says uniform in the upper left corner

526
00:22:59,660 --> 00:23:04,880
that's the distribution for the secret

527
00:23:02,030 --> 00:23:05,990
and then like the minimum lattice

528
00:23:04,880 --> 00:23:10,160
dimension that you would do anything

529
00:23:05,990 --> 00:23:12,680
with for homework encryption is 1024 so

530
00:23:10,160 --> 00:23:16,940
the first line is if you want to use a

531
00:23:12,680 --> 00:23:19,400
1024 bit lattice and you want bit 1024

532
00:23:16,940 --> 00:23:23,630
dimensional lattice and you want to have

533
00:23:19,400 --> 00:23:28,580
a security level of 128 bits then you

534
00:23:23,630 --> 00:23:36,050
should choose your your modulus your log

535
00:23:28,580 --> 00:23:40,010
Q should be no more than 29 bits so what

536
00:23:36,050 --> 00:23:41,360
that means is is that in practice you're

537
00:23:40,010 --> 00:23:43,490
gonna have some home or 'fuck

538
00:23:41,360 --> 00:23:45,560
computation that you want to do the way

539
00:23:43,490 --> 00:23:48,560
home morphic encryption works is that

540
00:23:45,560 --> 00:23:51,560
once you start computing on ciphertext

541
00:23:48,560 --> 00:23:54,290
the error grows and the plaintext grows

542
00:23:51,560 --> 00:23:57,050
and so what happens is is that there's a

543
00:23:54,290 --> 00:24:00,909
limit to how much computation you can do

544
00:23:57,050 --> 00:24:03,860
without refreshing in some way and so

545
00:24:00,910 --> 00:24:05,780
what will happen is from an implementers

546
00:24:03,860 --> 00:24:08,479
point of view if they want to do some

547
00:24:05,780 --> 00:24:10,399
concrete task they'll look and see oh

548
00:24:08,480 --> 00:24:13,970
what's the task that i want to do how

549
00:24:10,400 --> 00:24:16,550
big is my data how big of basically a

550
00:24:13,970 --> 00:24:19,580
kind of a coefficient modulus q do i

551
00:24:16,550 --> 00:24:22,070
need and so what you would then do is to

552
00:24:19,580 --> 00:24:24,939
kind of look at this column here and

553
00:24:22,070 --> 00:24:27,820
you'd go kind of down this car

554
00:24:24,940 --> 00:24:29,590
and until you hit the cue that you need

555
00:24:27,820 --> 00:24:31,658
like you need a cue that's at least so

556
00:24:29,590 --> 00:24:34,120
large but these are saying for these

557
00:24:31,659 --> 00:24:37,510
rows cue can be no larger than these

558
00:24:34,120 --> 00:24:41,439
values so so then let's say you think

559
00:24:37,510 --> 00:24:43,299
you need like you know like a 440 bit

560
00:24:41,440 --> 00:24:45,070
cue or 400 bit cue or something so you

561
00:24:43,299 --> 00:24:48,940
would go down to this row of the table

562
00:24:45,070 --> 00:24:51,549
and you would say oh okay this cue

563
00:24:48,940 --> 00:24:54,190
that'll be large enough for me and if I

564
00:24:51,549 --> 00:24:57,370
want 128 bit security that means I need

565
00:24:54,190 --> 00:25:01,149
to use lattice dimension which is 16 K

566
00:24:57,370 --> 00:25:04,510
and and the largest one that we have in

567
00:25:01,149 --> 00:25:07,000
the tables today is 32 K one thing we

568
00:25:04,510 --> 00:25:09,490
discussed yesterday is adding some rows

569
00:25:07,000 --> 00:25:13,840
here to allow for even bigger lattice

570
00:25:09,490 --> 00:25:15,820
dimension but with 32 K in by and large

571
00:25:13,840 --> 00:25:19,539
we think that you can do bootstrapping

572
00:25:15,820 --> 00:25:23,350
with 32 K bits and so initially thought

573
00:25:19,539 --> 00:25:26,230
that 32 I'm sorry 32 thousand dimension

574
00:25:23,350 --> 00:25:28,000
and so we thought that that might be

575
00:25:26,230 --> 00:25:30,490
sufficient but there are various

576
00:25:28,000 --> 00:25:32,409
implementers asking for a little bit

577
00:25:30,490 --> 00:25:37,510
more room to do more so we'll probably

578
00:25:32,409 --> 00:25:39,220
add more more rows to this table so that

579
00:25:37,510 --> 00:25:42,519
kind of should give you an idea that

580
00:25:39,220 --> 00:25:45,340
it's a it's a long dish document like 30

581
00:25:42,519 --> 00:25:49,830
plus pages and it has two sections it

582
00:25:45,340 --> 00:25:53,080
has a section which is just giving

583
00:25:49,830 --> 00:25:54,610
basics of the schemes the bgb and the BF

584
00:25:53,080 --> 00:25:56,460
schemes and pointing to some

585
00:25:54,610 --> 00:26:00,309
alternatives such as g SW

586
00:25:56,460 --> 00:26:02,470
but then the primary content of the

587
00:26:00,309 --> 00:26:05,590
document is describing all the known

588
00:26:02,470 --> 00:26:09,070
attacks on lattice problems and

589
00:26:05,590 --> 00:26:13,720
describing our both our methodology and

590
00:26:09,070 --> 00:26:17,620
our reasoning as to which you know which

591
00:26:13,720 --> 00:26:19,899
parameters choices that we wanted to

592
00:26:17,620 --> 00:26:22,000
standardize and stand behind so for

593
00:26:19,899 --> 00:26:24,729
example all the homework encryptions are

594
00:26:22,000 --> 00:26:29,260
using schemes that today are using ring

595
00:26:24,730 --> 00:26:32,320
lwe and so we only standardized to power

596
00:26:29,260 --> 00:26:34,629
cyclothymic rings because we have better

597
00:26:32,320 --> 00:26:38,020
attacks on other types of rings

598
00:26:34,630 --> 00:26:40,150
including general cyclotomic s-- so in

599
00:26:38,020 --> 00:26:43,179
in my experience when you already have

600
00:26:40,150 --> 00:26:46,090
kind of new or devastating attacks in

601
00:26:43,179 --> 00:26:48,070
some for some types of choices you'll

602
00:26:46,090 --> 00:26:50,500
those attacks will only improve over

603
00:26:48,070 --> 00:26:52,510
time and so we stayed away for em from

604
00:26:50,500 --> 00:26:54,670
any parameter choices that we thought

605
00:26:52,510 --> 00:26:58,260
would be kind of risky so we only

606
00:26:54,670 --> 00:27:01,450
standardized to power cyclothymic rings

607
00:26:58,260 --> 00:27:03,309
we have you know guidance on how to

608
00:27:01,450 --> 00:27:05,880
choose the error distribution for a

609
00:27:03,309 --> 00:27:08,920
homework encryption we give our

610
00:27:05,880 --> 00:27:11,320
rationale for what cost models we use

611
00:27:08,920 --> 00:27:13,660
for modeling the cost of the different

612
00:27:11,320 --> 00:27:15,520
attacks so we have a lot of explanations

613
00:27:13,660 --> 00:27:19,230
of that nature in the in the second half

614
00:27:15,520 --> 00:27:23,050
of the document before before the tables

615
00:27:19,230 --> 00:27:25,330
ok so now that our standard is public

616
00:27:23,050 --> 00:27:30,000
and and online and now it will even be

617
00:27:25,330 --> 00:27:32,800
on ePrint as of today the next steps are

618
00:27:30,000 --> 00:27:35,460
we have quite a few issues so like I

619
00:27:32,800 --> 00:27:37,770
said yesterday we had a discussion about

620
00:27:35,460 --> 00:27:41,110
Nance whether we want to have a more

621
00:27:37,770 --> 00:27:42,580
formal structure of steering committee

622
00:27:41,110 --> 00:27:46,350
and working groups because right now

623
00:27:42,580 --> 00:27:49,059
it's all volunteer all very ad hoc

624
00:27:46,350 --> 00:27:51,899
basically run by the kaisers that I

625
00:27:49,059 --> 00:27:54,639
listed on the first slide in

626
00:27:51,900 --> 00:27:56,800
consultation with other advisers that

627
00:27:54,640 --> 00:27:58,990
have been heavily involved in the

628
00:27:56,800 --> 00:28:01,330
workshops and through sending out emails

629
00:27:58,990 --> 00:28:03,790
on the mailing list there are of course

630
00:28:01,330 --> 00:28:07,059
people have brought up IPR issues so

631
00:28:03,790 --> 00:28:09,580
right now there's no IPR pal in our

632
00:28:07,059 --> 00:28:12,760
community we do not have anything

633
00:28:09,580 --> 00:28:16,600
stating we request people disclose their

634
00:28:12,760 --> 00:28:18,429
patents so participation in this that

635
00:28:16,600 --> 00:28:20,080
it's a good and a bad I mean every

636
00:28:18,429 --> 00:28:21,580
company that's participating there

637
00:28:20,080 --> 00:28:24,129
they're allowed they can participate

638
00:28:21,580 --> 00:28:27,909
without being required to disclose their

639
00:28:24,130 --> 00:28:30,610
patents so and also no company is

640
00:28:27,910 --> 00:28:32,950
actually contributing anything to the

641
00:28:30,610 --> 00:28:34,719
standard the standard itself is just an

642
00:28:32,950 --> 00:28:37,600
academic document so far that's

643
00:28:34,720 --> 00:28:41,020
collecting knowledge but once we move

644
00:28:37,600 --> 00:28:42,969
into the schemes and optimist optimal

645
00:28:41,020 --> 00:28:45,580
versions you know of implementations of

646
00:28:42,970 --> 00:28:48,190
the schemes and also into the

647
00:28:45,580 --> 00:28:51,879
applications there's also potential

648
00:28:48,190 --> 00:28:53,799
there to run into patent issues so IPRs

649
00:28:51,879 --> 00:28:55,569
certainly an issue that is on the table

650
00:28:53,799 --> 00:28:59,109
that we're talking about how it should

651
00:28:55,569 --> 00:29:01,329
be handled why we want to have actually

652
00:28:59,109 --> 00:29:03,819
individual white papers describing each

653
00:29:01,329 --> 00:29:06,119
of the main schemes instead of the

654
00:29:03,819 --> 00:29:07,869
current you know very very minimal

655
00:29:06,119 --> 00:29:10,178
specification that's in the first

656
00:29:07,869 --> 00:29:13,269
document so there's going to be separate

657
00:29:10,179 --> 00:29:15,969
working groups for be GBV fvck KS and

658
00:29:13,269 --> 00:29:18,519
tfhe and we're going to try to have some

659
00:29:15,969 --> 00:29:20,709
kind of uniformity in terms of the level

660
00:29:18,519 --> 00:29:23,949
of exposition and what's included in

661
00:29:20,709 --> 00:29:25,509
those different specifications feel that

662
00:29:23,949 --> 00:29:28,239
those would be very important for going

663
00:29:25,509 --> 00:29:30,999
for for example to a propose a

664
00:29:28,239 --> 00:29:32,440
standardization effort at IETF it would

665
00:29:30,999 --> 00:29:34,209
most likely want to standardize the

666
00:29:32,440 --> 00:29:37,469
schemes and so we want to have very good

667
00:29:34,209 --> 00:29:41,139
descriptions of the schemes written and

668
00:29:37,469 --> 00:29:43,359
so we're just starting on the

669
00:29:41,139 --> 00:29:46,629
applications track trying to formalize

670
00:29:43,359 --> 00:29:47,698
that so yesterday one Transco so from

671
00:29:46,629 --> 00:29:50,019
epfl

672
00:29:47,699 --> 00:29:52,119
presented the work that they did

673
00:29:50,019 --> 00:29:54,789
building on their Medeco projects so in

674
00:29:52,119 --> 00:29:57,849
switzerland they have brought together a

675
00:29:54,789 --> 00:29:59,739
consortium of seven swiss hospitals that

676
00:29:57,849 --> 00:30:01,899
are using a protocol that they designed

677
00:29:59,739 --> 00:30:04,059
which includes homomorphic encryption as

678
00:30:01,899 --> 00:30:05,708
one of the building blocks and which

679
00:30:04,059 --> 00:30:08,049
they had implemented and deployed and

680
00:30:05,709 --> 00:30:10,839
kind of learned from and so they did the

681
00:30:08,049 --> 00:30:12,599
exercise Swan presented yesterday I

682
00:30:10,839 --> 00:30:16,208
think it's not quite public yet of

683
00:30:12,599 --> 00:30:19,269
translating their technical paper into

684
00:30:16,209 --> 00:30:22,449
an RFC kind of a document so a protocol

685
00:30:19,269 --> 00:30:24,369
it's like a draft standard of a protocol

686
00:30:22,449 --> 00:30:27,639
for using homomorphic encryption for a

687
00:30:24,369 --> 00:30:29,289
purpose in the medical sector and so

688
00:30:27,639 --> 00:30:31,629
we're really pleased because that's the

689
00:30:29,289 --> 00:30:33,879
first example of an application being

690
00:30:31,629 --> 00:30:35,829
built on top of our standard there's

691
00:30:33,879 --> 00:30:37,718
been a couple of other instances where

692
00:30:35,829 --> 00:30:40,779
our standard has been referred to and

693
00:30:37,719 --> 00:30:43,659
used in the ongoing I - competitions

694
00:30:40,779 --> 00:30:46,299
which are funded by NIH so NIH has been

695
00:30:43,659 --> 00:30:50,309
funding for more than five years a group

696
00:30:46,299 --> 00:30:53,949
of academics and researchers at UCSD and

697
00:30:50,309 --> 00:30:56,440
UT Health and Houston and Indiana to run

698
00:30:53,949 --> 00:30:59,229
a competition every year it's called

699
00:30:56,440 --> 00:31:01,899
secure genome analysis competition and

700
00:30:59,229 --> 00:31:03,940
they put out datasets of genomic data

701
00:31:01,899 --> 00:31:05,510
and they create challenges and they say

702
00:31:03,940 --> 00:31:07,250
here's the home Worf aqui

703
00:31:05,510 --> 00:31:09,980
encryption challenge for this year and

704
00:31:07,250 --> 00:31:12,950
last year they actually referred to our

705
00:31:09,980 --> 00:31:15,230
standard asking for everyone to comply

706
00:31:12,950 --> 00:31:17,450
with the standard and use 128 bits

707
00:31:15,230 --> 00:31:19,820
security for their solutions so that was

708
00:31:17,450 --> 00:31:21,680
a nice way to bring all the solutions

709
00:31:19,820 --> 00:31:24,260
from these our teams from all over the

710
00:31:21,680 --> 00:31:26,900
world and bringing them all together to

711
00:31:24,260 --> 00:31:28,760
some kind of common foundation so that's

712
00:31:26,900 --> 00:31:32,270
another way that our standard is already

713
00:31:28,760 --> 00:31:35,480
used there's a couple of other issues

714
00:31:32,270 --> 00:31:37,160
that are definitely things that we're

715
00:31:35,480 --> 00:31:38,930
gonna probably struggle with over time

716
00:31:37,160 --> 00:31:41,540
which are things like interoperability

717
00:31:38,930 --> 00:31:43,910
of libraries and certification or

718
00:31:41,540 --> 00:31:46,220
accreditation of libraries and solutions

719
00:31:43,910 --> 00:31:47,810
a lot of the engineers from different

720
00:31:46,220 --> 00:31:49,640
companies in the room yesterday were

721
00:31:47,810 --> 00:31:54,440
repeatedly bringing up these these

722
00:31:49,640 --> 00:31:56,390
questions and so then you know just as a

723
00:31:54,440 --> 00:31:58,310
review of kind of like the way we're

724
00:31:56,390 --> 00:32:00,560
thinking about standardization there's a

725
00:31:58,310 --> 00:32:02,899
lot of paths to standardization we

726
00:32:00,560 --> 00:32:06,020
started with just essentially a

727
00:32:02,900 --> 00:32:09,230
consortium you know an open industry

728
00:32:06,020 --> 00:32:11,060
consortium and we feel like we've made a

729
00:32:09,230 --> 00:32:13,610
lot of progress we're really happy I

730
00:32:11,060 --> 00:32:16,970
mean we also really enjoy our community

731
00:32:13,610 --> 00:32:19,850
it's a great community but the whole

732
00:32:16,970 --> 00:32:22,280
goal was to have other agencies and

733
00:32:19,850 --> 00:32:25,219
other standardizing bodies build on this

734
00:32:22,280 --> 00:32:27,740
standard and we're in the process of

735
00:32:25,220 --> 00:32:30,200
thinking about how do we go and propose

736
00:32:27,740 --> 00:32:33,380
you know starting with maybe birds of a

737
00:32:30,200 --> 00:32:35,090
feather session at IETF meeting work

738
00:32:33,380 --> 00:32:39,080
adjoining one of the working groups at

739
00:32:35,090 --> 00:32:42,230
the UN ITU working together with nest I

740
00:32:39,080 --> 00:32:45,860
mean we we've actually Daniel was at our

741
00:32:42,230 --> 00:32:47,990
third workshop and I mentioned Lily had

742
00:32:45,860 --> 00:32:51,080
co-organized our first and Dustin moody

743
00:32:47,990 --> 00:32:53,120
is a co-author on this document so we'd

744
00:32:51,080 --> 00:32:53,629
really love to to work together with

745
00:32:53,120 --> 00:32:56,179
NIST

746
00:32:53,630 --> 00:32:59,240
we feel that it also overlaps with the

747
00:32:56,180 --> 00:33:01,130
ongoing pqc competition since we're

748
00:32:59,240 --> 00:33:03,800
trying to standardize latus problems

749
00:33:01,130 --> 00:33:05,390
which are the same lattice problems that

750
00:33:03,800 --> 00:33:06,860
are used for key exchange there are

751
00:33:05,390 --> 00:33:08,650
several differences in terms of

752
00:33:06,860 --> 00:33:12,679
parameter size and error distribution

753
00:33:08,650 --> 00:33:16,160
but by and large it fits very well with

754
00:33:12,680 --> 00:33:17,810
the current pqc process for nist and

755
00:33:16,160 --> 00:33:19,340
it's also a big incentive for companies

756
00:33:17,810 --> 00:33:21,950
that are thinking about

757
00:33:19,340 --> 00:33:24,259
having to transition to post quantum

758
00:33:21,950 --> 00:33:27,499
solutions anyway to have that home

759
00:33:24,259 --> 00:33:29,960
Orphic encryption option on the table so

760
00:33:27,499 --> 00:33:33,679
we have kind of a lot of different

761
00:33:29,960 --> 00:33:35,419
options and I'm hoping that we'll

762
00:33:33,679 --> 00:33:37,820
continue to work with some of the

763
00:33:35,419 --> 00:33:40,820
specific communities such as the medical

764
00:33:37,820 --> 00:33:42,980
community NIH that we've partnered with

765
00:33:40,820 --> 00:33:46,370
so far and also things like the

766
00:33:42,980 --> 00:33:48,379
financial services committee so

767
00:33:46,370 --> 00:33:50,120
inclusion I'd like to say that one thing

768
00:33:48,379 --> 00:33:51,889
that's been really important for us in

769
00:33:50,120 --> 00:33:54,860
this area of homomorphic encryption is

770
00:33:51,889 --> 00:33:56,629
talent development so we're really lucky

771
00:33:54,860 --> 00:33:58,459
at Microsoft Research we have an awesome

772
00:33:56,629 --> 00:34:01,428
intern program so we have many many

773
00:33:58,460 --> 00:34:03,950
interns every summer but we want it's

774
00:34:01,429 --> 00:34:06,499
hard to bring in to bring in PhD

775
00:34:03,950 --> 00:34:09,940
students who already have a lot of

776
00:34:06,499 --> 00:34:11,990
expertise in both cryptography and

777
00:34:09,940 --> 00:34:14,000
applications that we're working on such

778
00:34:11,989 --> 00:34:16,759
as machine learning and all kinds of

779
00:34:14,000 --> 00:34:18,679
things so we're actually doing we're

780
00:34:16,760 --> 00:34:21,169
starting something new we're gonna try

781
00:34:18,679 --> 00:34:23,839
this December we're doing a private AI

782
00:34:21,168 --> 00:34:26,690
bootcamp and we're actually intending to

783
00:34:23,839 --> 00:34:29,960
bring 30 PhD students from around the

784
00:34:26,690 --> 00:34:32,540
country who are in any area they don't

785
00:34:29,960 --> 00:34:34,639
have to be crypto researchers so this

786
00:34:32,540 --> 00:34:36,949
could be PhD students that are primarily

787
00:34:34,639 --> 00:34:39,619
focusing on machine learning or privacy

788
00:34:36,949 --> 00:34:41,719
or security or whatever and so we're

789
00:34:39,619 --> 00:34:45,589
gonna try to have tutorials on how to

790
00:34:41,719 --> 00:34:48,069
use seal tutorials on privacy preserving

791
00:34:45,589 --> 00:34:51,529
machine learning a lot of different

792
00:34:48,069 --> 00:34:52,730
aspects of this so application deadline

793
00:34:51,530 --> 00:34:55,490
is September 5th

794
00:34:52,730 --> 00:34:57,230
I realize this shameless advertisement

795
00:34:55,489 --> 00:35:00,169
but on the other hand it's a good

796
00:34:57,230 --> 00:35:02,839
opportunity to connect several different

797
00:35:00,170 --> 00:35:04,280
threads here and that is you know the

798
00:35:02,839 --> 00:35:06,650
talent pool development which is

799
00:35:04,280 --> 00:35:08,450
necessary for all of us for advanced

800
00:35:06,650 --> 00:35:10,220
crypto and also being very

801
00:35:08,450 --> 00:35:12,589
interdisciplinary about it not just

802
00:35:10,220 --> 00:35:14,480
being crypto researchers in isolation

803
00:35:12,589 --> 00:35:16,700
but rather crypto and security

804
00:35:14,480 --> 00:35:18,530
researchers that can reason across

805
00:35:16,700 --> 00:35:20,450
boundaries and and collaborate on

806
00:35:18,530 --> 00:35:22,730
interdisciplinary project so we feel

807
00:35:20,450 --> 00:35:25,430
this is very important and it also feeds

808
00:35:22,730 --> 00:35:27,890
very well into our standardization

809
00:35:25,430 --> 00:35:29,810
effort and our attempt to create you

810
00:35:27,890 --> 00:35:33,240
know like the title of this workshop

811
00:35:29,810 --> 00:35:35,640
advanced advanced crypto s--

812
00:35:33,240 --> 00:35:37,049
and Hertz so thank you very much for

813
00:35:35,640 --> 00:35:40,290
listening we only have about five

814
00:35:37,050 --> 00:35:42,850
minutes for questions but I appreciate

815
00:35:40,290 --> 00:35:48,779
your attention thank you very much I

816
00:35:42,850 --> 00:35:48,779
[Applause]

817
00:35:56,150 --> 00:36:01,380
guess one of the things that we've found

818
00:35:58,940 --> 00:36:03,270
most challenging in the zookie proof

819
00:36:01,380 --> 00:36:06,090
effort is kind of this idea of

820
00:36:03,270 --> 00:36:07,980
benchmarking different constructions and

821
00:36:06,090 --> 00:36:09,690
I'm wondering like what is the way that

822
00:36:07,980 --> 00:36:11,100
you're putting this out to the community

823
00:36:09,690 --> 00:36:12,810
what are the ways to like benchmark

824
00:36:11,100 --> 00:36:14,670
apples to apples really right between

825
00:36:12,810 --> 00:36:17,130
protocols and I guess the I -

826
00:36:14,670 --> 00:36:19,260
competition comes in a bit in there yes

827
00:36:17,130 --> 00:36:22,140
that's an excellent question so the I -

828
00:36:19,260 --> 00:36:24,360
we we almost feel like we've been kind

829
00:36:22,140 --> 00:36:25,799
of partnering with the I - organizers

830
00:36:24,360 --> 00:36:27,480
and competition over the last couple of

831
00:36:25,800 --> 00:36:29,730
years because they really have been

832
00:36:27,480 --> 00:36:32,730
playing that role for the last five

833
00:36:29,730 --> 00:36:35,580
years the I - competition the HD track

834
00:36:32,730 --> 00:36:38,190
has produced benchmark numbers for it

835
00:36:35,580 --> 00:36:40,770
and these aren't just like homework

836
00:36:38,190 --> 00:36:43,500
encryption like raw performance numbers

837
00:36:40,770 --> 00:36:46,710
these are numbers that show how it

838
00:36:43,500 --> 00:36:49,050
performs in a given environment the only

839
00:36:46,710 --> 00:36:51,750
thing that's a little bit hard to kind

840
00:36:49,050 --> 00:36:53,970
of pick apart or decipher is that each

841
00:36:51,750 --> 00:36:55,530
team will come up with a solution to the

842
00:36:53,970 --> 00:36:59,100
challenge and these solutions can be

843
00:36:55,530 --> 00:37:02,609
different so it's still not necessarily

844
00:36:59,100 --> 00:37:04,859
apples to apples when somebody's chosen

845
00:37:02,609 --> 00:37:07,290
like a machine learning model that you

846
00:37:04,859 --> 00:37:09,060
know like converges faster or needs less

847
00:37:07,290 --> 00:37:12,270
precision or something like that but

848
00:37:09,060 --> 00:37:14,340
still that's a set of like five years of

849
00:37:12,270 --> 00:37:15,930
tasks where there are benchmark numbers

850
00:37:14,340 --> 00:37:17,520
available online and the papers from

851
00:37:15,930 --> 00:37:19,980
those workshops have been published like

852
00:37:17,520 --> 00:37:22,470
in nature genomics and other you know

853
00:37:19,980 --> 00:37:26,430
top journals so that's one answer is the

854
00:37:22,470 --> 00:37:29,689
I - performance numbers so interestingly

855
00:37:26,430 --> 00:37:33,210
I don't think that we have necessarily

856
00:37:29,690 --> 00:37:35,670
publicly available performance numbers

857
00:37:33,210 --> 00:37:38,400
for each one of these libraries you know

858
00:37:35,670 --> 00:37:39,900
on that's on the website but that being

859
00:37:38,400 --> 00:37:41,580
said there are a lot of scientific

860
00:37:39,900 --> 00:37:43,619
papers that the authors of these

861
00:37:41,580 --> 00:37:45,750
libraries have published

862
00:37:43,619 --> 00:37:47,099
giving performance numbers another

863
00:37:45,750 --> 00:37:50,999
example of where

864
00:37:47,099 --> 00:37:53,849
see like concrete benchmarks is that so

865
00:37:50,999 --> 00:37:56,089
in 2016 my team published a paper called

866
00:37:53,849 --> 00:37:59,359
crypto nets in ICML which shows

867
00:37:56,089 --> 00:38:01,859
evaluating deep neural nets on

868
00:37:59,359 --> 00:38:03,569
homomorphic encrypted data and this was

869
00:38:01,859 --> 00:38:06,269
you know three years ago it was fairly

870
00:38:03,569 --> 00:38:08,579
surprising at the time that we could do

871
00:38:06,269 --> 00:38:10,609
predictions on home oracle encrypted

872
00:38:08,579 --> 00:38:14,069
data these are very very deep circuits

873
00:38:10,609 --> 00:38:16,619
but what has happened since then is an

874
00:38:14,069 --> 00:38:19,558
explosion of research in the area so

875
00:38:16,619 --> 00:38:21,839
yesterday jeong-seon presented a table

876
00:38:19,559 --> 00:38:23,460
with like ten different papers that have

877
00:38:21,839 --> 00:38:25,619
been published since then from different

878
00:38:23,460 --> 00:38:27,809
groups using different libraries doing

879
00:38:25,619 --> 00:38:32,729
their you know basically extending

880
00:38:27,809 --> 00:38:34,979
crypto nets doing deep net evaluation of

881
00:38:32,729 --> 00:38:36,618
neural nets and giving performance

882
00:38:34,979 --> 00:38:39,419
numbers so you could see on his table

883
00:38:36,619 --> 00:38:41,190
the throughput and the latency for each

884
00:38:39,420 --> 00:38:44,460
one of the solutions and they come from

885
00:38:41,190 --> 00:38:46,739
paper's like ccs last year crypto last

886
00:38:44,460 --> 00:38:49,380
year like all the top conferences and

887
00:38:46,739 --> 00:38:51,509
stuff so that's another example where we

888
00:38:49,380 --> 00:38:53,249
do have benchmarks but not for the raw

889
00:38:51,509 --> 00:38:56,700
performance numbers but rather for the

890
00:38:53,249 --> 00:38:59,450
application I don't know if that answers

891
00:38:56,700 --> 00:38:59,450
your question there

892
00:39:05,190 --> 00:39:09,630
[Applause]

