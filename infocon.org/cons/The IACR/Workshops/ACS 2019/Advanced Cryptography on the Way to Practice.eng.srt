1
00:00:04,920 --> 00:00:09,240
I think I'll change gears a little bit

2
00:00:07,470 --> 00:00:12,090
in terms of the presentations that we

3
00:00:09,240 --> 00:00:16,289
already heard and in my presentation I

4
00:00:12,090 --> 00:00:18,090
kind of will overview some techniques in

5
00:00:16,289 --> 00:00:19,710
advanced crypto and what has been

6
00:00:18,090 --> 00:00:23,759
happening with the efficiency of the

7
00:00:19,710 --> 00:00:26,190
solutions somehow setting the stage for

8
00:00:23,760 --> 00:00:28,230
the fact that once we have this improved

9
00:00:26,190 --> 00:00:30,720
efficiency maybe this is the time when

10
00:00:28,230 --> 00:00:32,549
we start thinking about the fact that

11
00:00:30,720 --> 00:00:34,079
these constructions will be used in

12
00:00:32,549 --> 00:00:36,120
practice and maybe this is also

13
00:00:34,079 --> 00:00:39,540
motivation for Standardization in this

14
00:00:36,120 --> 00:00:41,940
areas but of course let me define what

15
00:00:39,540 --> 00:00:44,120
advanced crypto will mean for us or it

16
00:00:41,940 --> 00:00:47,670
is really stock advanced crypto will be

17
00:00:44,120 --> 00:00:50,879
everything beyond encryption and digital

18
00:00:47,670 --> 00:00:52,890
signatures so beyond cryptography that

19
00:00:50,880 --> 00:00:56,670
thinks about protection of data at rest

20
00:00:52,890 --> 00:00:59,160
and communication and in particular in

21
00:00:56,670 --> 00:01:01,050
this talk I will touch on three areas of

22
00:00:59,160 --> 00:01:03,078
advanced cryptography which will include

23
00:01:01,050 --> 00:01:05,250
secure multi-party computation

24
00:01:03,079 --> 00:01:07,680
differential privacy and zero knowledge

25
00:01:05,250 --> 00:01:10,260
and these are the three of my choice

26
00:01:07,680 --> 00:01:13,409
because I think all three of them has

27
00:01:10,260 --> 00:01:16,500
been actually used in practical

28
00:01:13,409 --> 00:01:18,360
applications including not only academic

29
00:01:16,500 --> 00:01:21,479
papers but companies looking at this and

30
00:01:18,360 --> 00:01:25,409
using them in practice so then I have

31
00:01:21,479 --> 00:01:28,799
this graph creation of my own that for a

32
00:01:25,409 --> 00:01:31,350
while research in cryptography has been

33
00:01:28,799 --> 00:01:33,119
going upwards and coming up with new

34
00:01:31,350 --> 00:01:35,699
functionalities with new constructions

35
00:01:33,119 --> 00:01:38,270
and new capabilities while in practice

36
00:01:35,700 --> 00:01:42,030
we've been stuck more or less using

37
00:01:38,270 --> 00:01:44,580
signatures and encryption and I would

38
00:01:42,030 --> 00:01:47,100
say that maybe in the last couple of

39
00:01:44,580 --> 00:01:49,320
years we have seen an upward so up we're

40
00:01:47,100 --> 00:01:52,729
also in practice we started implementing

41
00:01:49,320 --> 00:01:54,240
supporting some of the more fancy

42
00:01:52,729 --> 00:01:57,000
functionality that comes from

43
00:01:54,240 --> 00:01:59,880
cryptography and my hope is that in the

44
00:01:57,000 --> 00:02:04,590
future these two slopes of research in

45
00:01:59,880 --> 00:02:10,019
crypto and use in practice will align so

46
00:02:04,590 --> 00:02:12,180
the motivation of this talk for me came

47
00:02:10,019 --> 00:02:16,709
when I listened to the talk of shahe

48
00:02:12,180 --> 00:02:17,800
levy at CCS last year where he made the

49
00:02:16,709 --> 00:02:19,690
claim that that mask

50
00:02:17,800 --> 00:02:24,640
so is need it fast enough to be useful

51
00:02:19,690 --> 00:02:26,350
and not generally usable yet so and when

52
00:02:24,640 --> 00:02:29,649
we talk about advanced cryptography

53
00:02:26,350 --> 00:02:32,079
there are many things that come into the

54
00:02:29,650 --> 00:02:34,510
mix in particular we have to be thinking

55
00:02:32,080 --> 00:02:37,170
about trade-offs between efficiency and

56
00:02:34,510 --> 00:02:40,149
utility we have to think about the

57
00:02:37,170 --> 00:02:42,010
specific setup of parties that we are

58
00:02:40,150 --> 00:02:43,450
looking at what are the assumptions

59
00:02:42,010 --> 00:02:46,329
about communication and copied

60
00:02:43,450 --> 00:02:51,489
computational channels the available

61
00:02:46,330 --> 00:02:53,920
resources at each party and also try to

62
00:02:51,490 --> 00:02:55,690
put our finger on where the trade-off

63
00:02:53,920 --> 00:02:57,130
between efficiency and utility makes

64
00:02:55,690 --> 00:03:00,850
sense for our application

65
00:02:57,130 --> 00:03:03,640
and some of the insights that I have in

66
00:03:00,850 --> 00:03:05,560
this talk also came from a workshop that

67
00:03:03,640 --> 00:03:07,329
we organized with new reps last year and

68
00:03:05,560 --> 00:03:09,880
privacy-preserving machine learning and

69
00:03:07,330 --> 00:03:14,830
we have another one of this with CCS

70
00:03:09,880 --> 00:03:17,500
this year so the need for advanced

71
00:03:14,830 --> 00:03:19,810
crypto I don't think there is much to

72
00:03:17,500 --> 00:03:22,330
pitch to this audience but basically our

73
00:03:19,810 --> 00:03:24,370
challenge here is that we have data

74
00:03:22,330 --> 00:03:27,610
which is arguably the most valuable

75
00:03:24,370 --> 00:03:30,160
resource nowadays and we want to use it

76
00:03:27,610 --> 00:03:33,250
we want to analyze it that's how many

77
00:03:30,160 --> 00:03:34,690
things work in practice and then there

78
00:03:33,250 --> 00:03:37,060
are many challenges related to the

79
00:03:34,690 --> 00:03:39,010
privacy of this data and this advanced

80
00:03:37,060 --> 00:03:41,770
crypto techniques bring the promise that

81
00:03:39,010 --> 00:03:45,100
we can obtain utility from this private

82
00:03:41,770 --> 00:03:47,830
data without sacrificing privacy so

83
00:03:45,100 --> 00:03:50,590
hence we are kind of striving for this

84
00:03:47,830 --> 00:03:53,410
functionality and now since this is a

85
00:03:50,590 --> 00:03:57,100
standardization workshop while I'm

86
00:03:53,410 --> 00:04:01,630
describing these solutions and their

87
00:03:57,100 --> 00:04:03,400
efficiency kind of the main messages

88
00:04:01,630 --> 00:04:05,500
that many of these technologies are

89
00:04:03,400 --> 00:04:08,140
ready to start to be used in practice

90
00:04:05,500 --> 00:04:10,420
and standardization will definitely help

91
00:04:08,140 --> 00:04:12,070
adoption because companies like to have

92
00:04:10,420 --> 00:04:15,670
standards that they follow and they use

93
00:04:12,070 --> 00:04:18,820
some of these advanced techniques these

94
00:04:15,670 --> 00:04:21,969
advanced cryptography notions have much

95
00:04:18,820 --> 00:04:24,310
more complexity notions so it's

96
00:04:21,970 --> 00:04:27,400
interesting and challenging how to

97
00:04:24,310 --> 00:04:28,510
convey those properties in a standard so

98
00:04:27,400 --> 00:04:30,330
that people who are not necessarily

99
00:04:28,510 --> 00:04:34,020
experts might be able

100
00:04:30,330 --> 00:04:36,448
use them also there is a plethora of

101
00:04:34,020 --> 00:04:39,060
constructions and techniques for each of

102
00:04:36,449 --> 00:04:41,430
these primitives that gives slightly

103
00:04:39,060 --> 00:04:43,099
different guarantees so when we

104
00:04:41,430 --> 00:04:45,330
standardize we have to think what

105
00:04:43,099 --> 00:04:48,479
exactly do a standardized of this

106
00:04:45,330 --> 00:04:49,889
multitude of techniques and also these

107
00:04:48,479 --> 00:04:54,688
techniques have a wide range of

108
00:04:49,889 --> 00:04:56,789
applications and oftentimes in to solve

109
00:04:54,689 --> 00:05:00,180
this real application problems we need

110
00:04:56,789 --> 00:05:02,128
to use more than one of these tools so a

111
00:05:00,180 --> 00:05:04,259
question for Standardization is whether

112
00:05:02,129 --> 00:05:06,810
standardization should be kind of driven

113
00:05:04,259 --> 00:05:08,639
or aware of the applications that we try

114
00:05:06,810 --> 00:05:12,180
to solve and that we are trying to solve

115
00:05:08,639 --> 00:05:15,300
first with this technique so with this I

116
00:05:12,180 --> 00:05:18,300
will continue with my overview of kind

117
00:05:15,300 --> 00:05:20,639
of recent work and I need to do the

118
00:05:18,300 --> 00:05:23,159
disclaimer that I might have missed your

119
00:05:20,639 --> 00:05:24,840
work this wasn't on purpose but this is

120
00:05:23,159 --> 00:05:26,460
just kind of an overview that gives you

121
00:05:24,840 --> 00:05:29,580
a flavor of what has been happening with

122
00:05:26,460 --> 00:05:31,710
efficiency of these techniques and I

123
00:05:29,580 --> 00:05:33,688
will start with secure computation of

124
00:05:31,710 --> 00:05:35,489
privacy-preserving computation where I

125
00:05:33,689 --> 00:05:37,949
will distinguish kind of two scenarios

126
00:05:35,490 --> 00:05:39,779
the first scenario is the one where we

127
00:05:37,949 --> 00:05:41,759
have few parties that are participating

128
00:05:39,779 --> 00:05:45,150
in the computation and they have more or

129
00:05:41,759 --> 00:05:46,979
less equal computational resources and

130
00:05:45,150 --> 00:05:48,599
communication available among them and

131
00:05:46,979 --> 00:05:52,068
they're available for the whole

132
00:05:48,599 --> 00:05:54,479
execution of the protocol the second

133
00:05:52,069 --> 00:05:56,819
scenario which I will refer to as

134
00:05:54,479 --> 00:05:58,889
federated learning is more of a setting

135
00:05:56,819 --> 00:06:00,659
where we have one central party that has

136
00:05:58,889 --> 00:06:05,759
a lot of computational resources and

137
00:06:00,659 --> 00:06:09,020
then many week parties that interact

138
00:06:05,759 --> 00:06:11,550
with this main computational server

139
00:06:09,020 --> 00:06:13,139
these parties cannot talk among each

140
00:06:11,550 --> 00:06:15,599
other they can only talk to the server

141
00:06:13,139 --> 00:06:18,389
and then they are also very unreliable

142
00:06:15,599 --> 00:06:20,310
so some of these parties may disappear

143
00:06:18,389 --> 00:06:22,860
during the execution of the protocol so

144
00:06:20,310 --> 00:06:26,190
we would like to tolerate dropouts in

145
00:06:22,860 --> 00:06:31,169
this setting now let's look what do we

146
00:06:26,190 --> 00:06:33,210
know in the first setting and I will

147
00:06:31,169 --> 00:06:35,659
start with a look at fully homomorphic

148
00:06:33,210 --> 00:06:38,698
encryption we already heard about

149
00:06:35,659 --> 00:06:40,380
initiatives in this area of course fully

150
00:06:38,699 --> 00:06:43,199
homomorphic encryption gives us the

151
00:06:40,380 --> 00:06:43,950
capability to take some data encrypted

152
00:06:43,199 --> 00:06:46,320
to give it to

153
00:06:43,950 --> 00:06:49,110
the 13 then asked this party to compute

154
00:06:46,320 --> 00:06:54,690
any function we want on the encrypted

155
00:06:49,110 --> 00:06:57,930
data so this construction has been known

156
00:06:54,690 --> 00:07:00,600
from 2009 and what do we know now in

157
00:06:57,930 --> 00:07:03,930
terms of efficiency is that of course

158
00:07:00,600 --> 00:07:05,940
the most expensive operation in under

159
00:07:03,930 --> 00:07:08,610
fhe is multiplication and currently what

160
00:07:05,940 --> 00:07:11,100
we can do is we can compute a circuit of

161
00:07:08,610 --> 00:07:13,680
depth 20 so 20 consider qussuk

162
00:07:11,100 --> 00:07:16,800
consecutive multiplication in about 62

163
00:07:13,680 --> 00:07:18,810
milliseconds also if we want to look at

164
00:07:16,800 --> 00:07:20,820
specific application which comes from

165
00:07:18,810 --> 00:07:23,040
this I - competition that was come

166
00:07:20,820 --> 00:07:26,159
looking at logistic regression or

167
00:07:23,040 --> 00:07:28,980
training on using logistic regression

168
00:07:26,160 --> 00:07:32,880
with 1500 patient records and xviii

169
00:07:28,980 --> 00:07:37,050
features one iteration of gradient

170
00:07:32,880 --> 00:07:41,370
descent was taking between 0.4 and 3.2

171
00:07:37,050 --> 00:07:43,620
hours in this setting using fhe though

172
00:07:41,370 --> 00:07:48,680
this is kind of where fhe some of the

173
00:07:43,620 --> 00:07:51,060
recent FTE efficiency stands for another

174
00:07:48,680 --> 00:07:53,580
kind of application that has been looked

175
00:07:51,060 --> 00:07:55,470
a lot in practice is the question of

176
00:07:53,580 --> 00:07:57,960
private set intersection two parties

177
00:07:55,470 --> 00:08:00,060
have to set and they want to find the

178
00:07:57,960 --> 00:08:02,719
intersection of their two inputs without

179
00:08:00,060 --> 00:08:05,160
revealing anything more about the data

180
00:08:02,720 --> 00:08:08,340
this there have been many many works

181
00:08:05,160 --> 00:08:11,070
that look at this question so what do we

182
00:08:08,340 --> 00:08:14,849
know now in the setting of semi honest

183
00:08:11,070 --> 00:08:16,500
and malicious security is that in the

184
00:08:14,850 --> 00:08:20,720
setting of semi honest if we want to

185
00:08:16,500 --> 00:08:23,730
intersect sets of sides is two to the

186
00:08:20,720 --> 00:08:26,700
twenty ndu this in just a couple of

187
00:08:23,730 --> 00:08:29,490
second and if we want to do this with

188
00:08:26,700 --> 00:08:33,559
malicious security we have to do a

189
00:08:29,490 --> 00:08:35,870
couple of hours and this this

190
00:08:33,559 --> 00:08:37,739
functionality or extension of this inter

191
00:08:35,870 --> 00:08:40,580
functionality which is private

192
00:08:37,740 --> 00:08:42,780
intersection sum has been used also by

193
00:08:40,580 --> 00:08:47,190
Google for in the setting of some

194
00:08:42,780 --> 00:08:49,829
aggregate add attribution another

195
00:08:47,190 --> 00:08:51,840
functionality that has been of great

196
00:08:49,830 --> 00:08:54,330
interest for people is private

197
00:08:51,840 --> 00:08:56,070
information retrieval where we have one

198
00:08:54,330 --> 00:08:58,410
database owner

199
00:08:56,070 --> 00:09:01,650
and we have a party that wants to submit

200
00:08:58,410 --> 00:09:03,660
queries and the goal is to respond to

201
00:09:01,650 --> 00:09:05,760
these queries without without the

202
00:09:03,660 --> 00:09:09,510
database owner learning anything about

203
00:09:05,760 --> 00:09:11,250
this query this type of constructions of

204
00:09:09,510 --> 00:09:13,560
private information retrieval usually

205
00:09:11,250 --> 00:09:15,530
rely on homomorphic encryption in either

206
00:09:13,560 --> 00:09:18,239
additive or fully homomorphic encryption

207
00:09:15,530 --> 00:09:22,579
so one of the more the most recent work

208
00:09:18,240 --> 00:09:24,720
in this setting allows you to compute

209
00:09:22,580 --> 00:09:27,810
private information retrieval query

210
00:09:24,720 --> 00:09:32,040
again on a database of size 2 to 2 22 in

211
00:09:27,810 --> 00:09:34,979
about 12 seconds so this is in many

212
00:09:32,040 --> 00:09:37,020
settings maybe one other caveat is that

213
00:09:34,980 --> 00:09:40,110
here the sizes of the items in the

214
00:09:37,020 --> 00:09:42,540
database are 288 bytes so efficiency of

215
00:09:40,110 --> 00:09:46,440
ear also depends on the size of the

216
00:09:42,540 --> 00:09:48,540
entries so these were kind of two or

217
00:09:46,440 --> 00:09:50,820
three examples of functionalities that

218
00:09:48,540 --> 00:09:53,339
have been in interest and have been used

219
00:09:50,820 --> 00:09:55,860
in practice but of course we know that

220
00:09:53,340 --> 00:09:59,190
we can do general two party computation

221
00:09:55,860 --> 00:10:01,080
where two parties with inputs x and y

222
00:09:59,190 --> 00:10:04,290
can come evaluate any function that

223
00:10:01,080 --> 00:10:08,670
depends on their joint input so this has

224
00:10:04,290 --> 00:10:12,240
been a problem of research of a lot of

225
00:10:08,670 --> 00:10:14,099
works in the last ten years and I'm

226
00:10:12,240 --> 00:10:16,590
talking only about implementations like

227
00:10:14,100 --> 00:10:19,020
MPC has been studied for over 30 years

228
00:10:16,590 --> 00:10:21,770
but these are two graphs that kind of

229
00:10:19,020 --> 00:10:23,730
give the slope of improvement in

230
00:10:21,770 --> 00:10:26,280
implementations for these two techniques

231
00:10:23,730 --> 00:10:28,860
in the setting of malicious security and

232
00:10:26,280 --> 00:10:31,079
semi honest security and we are looking

233
00:10:28,860 --> 00:10:33,000
at the question of security evaluating

234
00:10:31,080 --> 00:10:34,560
AES so one party has the key the other

235
00:10:33,000 --> 00:10:39,240
party has the input they want to

236
00:10:34,560 --> 00:10:41,160
evaluate a yes so initially the semi

237
00:10:39,240 --> 00:10:45,060
honest Solutions 10 years ago we're

238
00:10:41,160 --> 00:10:47,189
taking hours to evaluate this currently

239
00:10:45,060 --> 00:10:50,699
we can do this secure evaluation in one

240
00:10:47,190 --> 00:10:53,910
under one millisecond in the malicious

241
00:10:50,700 --> 00:10:56,790
setting we were going from months 10

242
00:10:53,910 --> 00:11:00,510
years ago and today we can do this again

243
00:10:56,790 --> 00:11:02,010
in about 10 milliseconds so kind of the

244
00:11:00,510 --> 00:11:04,680
message here is that we have seen this

245
00:11:02,010 --> 00:11:06,540
tremendous improvement in efficiency of

246
00:11:04,680 --> 00:11:09,260
two party competition and this

247
00:11:06,540 --> 00:11:12,449
brings us brings MPC in the realm of

248
00:11:09,260 --> 00:11:14,670
cryptographic techniques that could be

249
00:11:12,450 --> 00:11:18,540
used the bow in practical applications

250
00:11:14,670 --> 00:11:22,740
and we should think about how we can

251
00:11:18,540 --> 00:11:25,290
make this more usable using standards of

252
00:11:22,740 --> 00:11:28,790
course one of the most enticing

253
00:11:25,290 --> 00:11:33,449
application of MPC is machine learning

254
00:11:28,790 --> 00:11:35,670
we're kind of the my conclusion at least

255
00:11:33,450 --> 00:11:38,370
from the techniques that exist so far is

256
00:11:35,670 --> 00:11:40,589
that out of the box use of MTC

257
00:11:38,370 --> 00:11:42,600
techniques is not the most efficient

258
00:11:40,590 --> 00:11:46,350
it's not the way to go if you want to

259
00:11:42,600 --> 00:11:47,970
use MPC for machine learning kind of if

260
00:11:46,350 --> 00:11:49,560
you want to combine MPC and machine

261
00:11:47,970 --> 00:11:51,390
learning you have to do effort in both

262
00:11:49,560 --> 00:11:53,189
directions in the sense that you have to

263
00:11:51,390 --> 00:11:55,920
look at your ml algorithms and you have

264
00:11:53,190 --> 00:11:58,320
to make them and PC friendly right fix

265
00:11:55,920 --> 00:12:01,380
point computation is what's efficient in

266
00:11:58,320 --> 00:12:03,720
MPC usually and all algorithms all rely

267
00:12:01,380 --> 00:12:05,490
on floating-point computation so

268
00:12:03,720 --> 00:12:08,100
changing the algorithms to be friendly

269
00:12:05,490 --> 00:12:10,800
for fixed point evaluations is a good

270
00:12:08,100 --> 00:12:14,670
thing to do non-linearity is expensive

271
00:12:10,800 --> 00:12:17,310
in MPC try to avoid do approximations

272
00:12:14,670 --> 00:12:20,280
that avoid non-linearity also we have

273
00:12:17,310 --> 00:12:22,020
seen a lot of work trying to adapt MPC

274
00:12:20,280 --> 00:12:26,130
to the needs for of machine learning

275
00:12:22,020 --> 00:12:27,150
computation for example taking advantage

276
00:12:26,130 --> 00:12:31,080
of the fact that we need only

277
00:12:27,150 --> 00:12:33,510
approximate if we find out results in

278
00:12:31,080 --> 00:12:35,420
many ml applications or we we have seen

279
00:12:33,510 --> 00:12:38,160
MPC approaches that leverage this

280
00:12:35,420 --> 00:12:41,040
approximate correctness as well as fhg

281
00:12:38,160 --> 00:12:42,839
constructions that do the same also the

282
00:12:41,040 --> 00:12:44,910
machine learning territory is a

283
00:12:42,840 --> 00:12:46,650
territory where we really can play with

284
00:12:44,910 --> 00:12:48,030
the trade-offs between efficiency and

285
00:12:46,650 --> 00:12:50,040
accuracy

286
00:12:48,030 --> 00:12:51,449
many of these regression based

287
00:12:50,040 --> 00:12:53,490
algorithms that have many iterations

288
00:12:51,450 --> 00:12:57,030
allow you to do this kind of in a nice

289
00:12:53,490 --> 00:13:01,800
seamless way and one final point is that

290
00:12:57,030 --> 00:13:04,110
a lot of them machine learning wants to

291
00:13:01,800 --> 00:13:06,329
distinguish computation on sparse data

292
00:13:04,110 --> 00:13:08,250
there are standards such as the sparse

293
00:13:06,330 --> 00:13:09,870
blast that are focused on how to

294
00:13:08,250 --> 00:13:12,600
optimize algorithms when you are dealing

295
00:13:09,870 --> 00:13:14,250
with sparse data so you could think of

296
00:13:12,600 --> 00:13:17,430
these also something you want to take

297
00:13:14,250 --> 00:13:19,590
advantage in the context of MPC so let

298
00:13:17,430 --> 00:13:23,099
me just give you a few examples

299
00:13:19,590 --> 00:13:24,750
most of what has been done in MPC for

300
00:13:23,100 --> 00:13:26,640
machine learning so one example is

301
00:13:24,750 --> 00:13:28,770
computing distributed linear regression

302
00:13:26,640 --> 00:13:32,100
in this particular work we were looking

303
00:13:28,770 --> 00:13:36,150
at vertically partitioned database and

304
00:13:32,100 --> 00:13:39,720
we were experimenting with model

305
00:13:36,150 --> 00:13:44,069
iterative solutions based on fixed point

306
00:13:39,720 --> 00:13:45,630
gradient descent so kind of one bottom

307
00:13:44,070 --> 00:13:49,040
line is that if you want to solve

308
00:13:45,630 --> 00:13:52,110
systems of linear equations of dimension

309
00:13:49,040 --> 00:13:54,660
500 and you are happy with the

310
00:13:52,110 --> 00:13:56,880
approximate solution where CJD with only

311
00:13:54,660 --> 00:13:58,740
20 iteration already gives you something

312
00:13:56,880 --> 00:14:01,980
very meaningful you could do this in

313
00:13:58,740 --> 00:14:04,080
about a hundred a couple hundred seconds

314
00:14:01,980 --> 00:14:06,660
if you want to talk about solving the

315
00:14:04,080 --> 00:14:09,270
linear regression if we have five

316
00:14:06,660 --> 00:14:12,630
hundred records five hundred thousand

317
00:14:09,270 --> 00:14:15,000
records with attributes with five

318
00:14:12,630 --> 00:14:16,910
hundred attributes you can do this in

319
00:14:15,000 --> 00:14:20,760
about two hours

320
00:14:16,910 --> 00:14:23,939
of course a when we talk about machine

321
00:14:20,760 --> 00:14:26,520
learning neural network computation is

322
00:14:23,940 --> 00:14:28,380
one of the favorite applications so here

323
00:14:26,520 --> 00:14:31,260
we can distinguish between neural net

324
00:14:28,380 --> 00:14:32,730
inference where one party has the model

325
00:14:31,260 --> 00:14:35,220
and the other one has the input and you

326
00:14:32,730 --> 00:14:37,650
want to do the classification in a MPCA

327
00:14:35,220 --> 00:14:39,810
manner and the other one is how to do

328
00:14:37,650 --> 00:14:41,699
the training in secure computation where

329
00:14:39,810 --> 00:14:45,329
the inputs are partitioned between two

330
00:14:41,700 --> 00:14:47,750
parties so here all this some of the

331
00:14:45,330 --> 00:14:50,000
most recent works and what they achieve

332
00:14:47,750 --> 00:14:52,110
this is a work that looks at

333
00:14:50,000 --> 00:14:56,060
convolutional neural networks and this

334
00:14:52,110 --> 00:15:01,440
is the question of private prediction

335
00:14:56,060 --> 00:15:04,199
and this experiments that they presented

336
00:15:01,440 --> 00:15:06,470
so the dissolution combines MPC with fhe

337
00:15:04,200 --> 00:15:09,480
in interesting and non trivial matter

338
00:15:06,470 --> 00:15:11,840
these are some evaluations using the

339
00:15:09,480 --> 00:15:14,970
Emily's data set which is classifying

340
00:15:11,840 --> 00:15:17,040
digits handwritten digits and you can

341
00:15:14,970 --> 00:15:19,500
see that with different topologies for

342
00:15:17,040 --> 00:15:21,920
this convolution neural networks the

343
00:15:19,500 --> 00:15:21,920
runtime

344
00:15:21,980 --> 00:15:29,990
all of 30 seconds to myself and the main

345
00:15:27,230 --> 00:15:34,000
different factor distinguishing actors

346
00:15:29,990 --> 00:15:34,000
really the communication which changes

347
00:15:38,780 --> 00:15:50,829
ask this because it is considered for

348
00:15:47,720 --> 00:15:50,830
you learning

349
00:15:52,660 --> 00:16:03,040
re-evaluation with 13 seconds good news

350
00:16:03,610 --> 00:16:09,980
and communication again goes up once was

351
00:16:07,250 --> 00:16:11,810
the neural net apology starts to be more

352
00:16:09,980 --> 00:16:16,580
complicated communication is the first

353
00:16:11,810 --> 00:16:19,969
thing that seems to go up a more recent

354
00:16:16,580 --> 00:16:22,760
work looks at the same question but they

355
00:16:19,970 --> 00:16:27,350
are trying to look at binaries neuro

356
00:16:22,760 --> 00:16:29,540
networks and there's special types of

357
00:16:27,350 --> 00:16:32,810
neuro networks where use only binary

358
00:16:29,540 --> 00:16:35,120
values and then they show you kind of

359
00:16:32,810 --> 00:16:37,959
new results one thing is that when you

360
00:16:35,120 --> 00:16:40,280
are doing step machine learning

361
00:16:37,960 --> 00:16:42,710
applications you really need to look at

362
00:16:40,280 --> 00:16:44,780
the accuracy getting out of your secure

363
00:16:42,710 --> 00:16:48,950
computation because without accuracy

364
00:16:44,780 --> 00:16:52,430
this is meaningless so in this work they

365
00:16:48,950 --> 00:16:56,089
managed to get very good accuracy for

366
00:16:52,430 --> 00:16:57,859
again the m-miss data set and they are

367
00:16:56,089 --> 00:16:59,890
improving on the run time from the

368
00:16:57,860 --> 00:17:04,189
previous work now the run time is

369
00:16:59,890 --> 00:17:05,150
approximately zero point one to two

370
00:17:04,189 --> 00:17:07,370
seconds

371
00:17:05,150 --> 00:17:09,319
and communication again is the main

372
00:17:07,369 --> 00:17:12,079
distinguishing factor when employing

373
00:17:09,319 --> 00:17:16,670
different topologies that allow you to

374
00:17:12,079 --> 00:17:20,000
go up maybe the most recent work that

375
00:17:16,670 --> 00:17:21,800
I'm aware in this setting is this work

376
00:17:20,000 --> 00:17:25,250
of potion two part in your own network

377
00:17:21,800 --> 00:17:28,250
training and prediction this work looks

378
00:17:25,250 --> 00:17:30,620
at terrorised neural networks and in a

379
00:17:28,250 --> 00:17:32,870
nice way to leverage oblivious transfer

380
00:17:30,620 --> 00:17:36,469
for many of the computations in this in

381
00:17:32,870 --> 00:17:40,479
this context so here they also look at

382
00:17:36,470 --> 00:17:43,640
several real data sets that include

383
00:17:40,480 --> 00:17:47,960
smartphone exilim accelerometer meter

384
00:17:43,640 --> 00:17:53,030
and gyroscope sensor data different data

385
00:17:47,960 --> 00:17:56,150
sets for predicting thyroid and cancer

386
00:17:53,030 --> 00:17:58,879
diseases as well as some information for

387
00:17:56,150 --> 00:18:01,970
German credit card companies that tries

388
00:17:58,880 --> 00:18:04,640
to classify the transactions as bad or

389
00:18:01,970 --> 00:18:07,160
good from different clients so here

390
00:18:04,640 --> 00:18:10,310
their results may be there this is a

391
00:18:07,160 --> 00:18:12,200
little slow small but they show that

392
00:18:10,310 --> 00:18:14,270
essentially if you want to run and

393
00:18:12,200 --> 00:18:17,680
you're on a training you will need

394
00:18:14,270 --> 00:18:20,540
several days if you are happy with

395
00:18:17,680 --> 00:18:22,250
smaller accuracy and if you really want

396
00:18:20,540 --> 00:18:25,280
to go close to the accuracy that's

397
00:18:22,250 --> 00:18:28,370
coming from the floating point this will

398
00:18:25,280 --> 00:18:31,910
take you weeks if not months but one

399
00:18:28,370 --> 00:18:36,040
kind of message here is that sometimes

400
00:18:31,910 --> 00:18:40,270
even in practice in the clear neural net

401
00:18:36,040 --> 00:18:42,800
training takes could take weeks or like

402
00:18:40,270 --> 00:18:45,139
so these numbers are not necessarily

403
00:18:42,800 --> 00:18:49,669
impossible in practice in in view of

404
00:18:45,140 --> 00:18:52,670
what's happening with training in in the

405
00:18:49,670 --> 00:18:54,200
clear the prediction they also show you

406
00:18:52,670 --> 00:19:12,320
how to do prediction that takes

407
00:18:54,200 --> 00:19:14,540
basically a few some of the means I'm

408
00:19:12,320 --> 00:19:16,909
running low on time I will just quickly

409
00:19:14,540 --> 00:19:19,190
say that we have also this work that

410
00:19:16,910 --> 00:19:21,620
looks at the sparsity and what what can

411
00:19:19,190 --> 00:19:23,690
you do with MPC if your sparsity the

412
00:19:21,620 --> 00:19:26,179
level of sparsity of your data is public

413
00:19:23,690 --> 00:19:28,820
and you want to optimize on this and the

414
00:19:26,180 --> 00:19:31,070
main message is that you could get a lot

415
00:19:28,820 --> 00:19:32,149
of improvement many of these datasets

416
00:19:31,070 --> 00:19:35,480
are very sparse

417
00:19:32,150 --> 00:19:39,260
you could have nonzero entries at the

418
00:19:35,480 --> 00:19:41,680
level of one to three percent up to ten

419
00:19:39,260 --> 00:19:46,790
percent and you could significantly

420
00:19:41,680 --> 00:19:49,220
optimize your MPC for this set so now

421
00:19:46,790 --> 00:19:51,020
let's let's go to the other setting for

422
00:19:49,220 --> 00:19:52,430
MPC which was this federated learning

423
00:19:51,020 --> 00:19:56,830
which I mentioned where we have many

424
00:19:52,430 --> 00:19:59,860
parties interacting with one powerful

425
00:19:56,830 --> 00:20:03,620
server so this is a setting that's

426
00:19:59,860 --> 00:20:05,820
really been of interest to many of the

427
00:20:03,620 --> 00:20:08,850
companies so Google

428
00:20:05,820 --> 00:20:11,610
in having this secure gradation protocol

429
00:20:08,850 --> 00:20:15,929
that allows you to accumulate gradient

430
00:20:11,610 --> 00:20:17,459
descents coming from smartphones in a

431
00:20:15,930 --> 00:20:21,600
manner that you review only the

432
00:20:17,460 --> 00:20:24,590
aggregate of these radiant updates so

433
00:20:21,600 --> 00:20:28,620
the numbers that they have demonstrated

434
00:20:24,590 --> 00:20:34,590
show you that if you want to do vectors

435
00:20:28,620 --> 00:20:43,800
of size 100 K and you have about 500

436
00:20:34,590 --> 00:20:46,590
clients and piont

437
00:20:43,800 --> 00:20:49,139
and then on the other hand if you want

438
00:20:46,590 --> 00:20:51,889
to fix the number of clients to about

439
00:20:49,140 --> 00:20:53,820
500 and then you are increasing the

440
00:20:51,890 --> 00:20:55,800
dimension of the vector that you are

441
00:20:53,820 --> 00:21:03,689
aggregating again you have this linear

442
00:20:55,800 --> 00:21:10,200
scaling for the time and you can so this

443
00:21:03,690 --> 00:21:13,650
is this is kind of ballpark run time in

444
00:21:10,200 --> 00:21:17,850
terms of minutes to hours which if you

445
00:21:13,650 --> 00:21:20,820
are running this fairly if you don't

446
00:21:17,850 --> 00:21:23,340
demand real-time response but you're

447
00:21:20,820 --> 00:21:27,210
running this from time to time is you in

448
00:21:23,340 --> 00:21:30,990
the realm of usable efficient another

449
00:21:27,210 --> 00:21:33,150
work that kind of looks at the same the

450
00:21:30,990 --> 00:21:35,250
same question but different architecture

451
00:21:33,150 --> 00:21:38,850
and it assumes that you could split your

452
00:21:35,250 --> 00:21:41,490
server in pour more parties that are non

453
00:21:38,850 --> 00:21:44,669
colluding then you could apply new

454
00:21:41,490 --> 00:21:49,110
different techniques to do this so this

455
00:21:44,670 --> 00:21:52,320
work of which has been also implemented

456
00:21:49,110 --> 00:21:56,340
the deployment of Firefox looks at this

457
00:21:52,320 --> 00:21:58,560
model and it shows you how to do the

458
00:21:56,340 --> 00:22:01,040
dimensional least square regression and

459
00:21:58,560 --> 00:22:05,370
basically this table tells you how much

460
00:22:01,040 --> 00:22:07,740
overhead or slow down the NPC introduces

461
00:22:05,370 --> 00:22:11,699
compared to the computation in plain

462
00:22:07,740 --> 00:22:14,160
text you can see that depending on

463
00:22:11,700 --> 00:22:17,900
Russian they mentioned this overhead

464
00:22:14,160 --> 00:22:19,770
goes from 5 times to about 12 times

465
00:22:17,900 --> 00:22:26,630
compared with

466
00:22:19,770 --> 00:22:29,879
competition so now I want to turn to the

467
00:22:26,630 --> 00:22:32,270
next primitive that I mentioned in the

468
00:22:29,880 --> 00:22:35,760
beginning which is differential privacy

469
00:22:32,270 --> 00:22:38,370
and now differential privacy asks a

470
00:22:35,760 --> 00:22:41,280
different question so far we were

471
00:22:38,370 --> 00:22:43,260
talking about having multiple parties

472
00:22:41,280 --> 00:22:46,040
and computing on their input in a way

473
00:22:43,260 --> 00:22:50,850
that doesn't review anything more than

474
00:22:46,040 --> 00:22:52,500
now equally important problem is asking

475
00:22:50,850 --> 00:22:55,740
the question how much actually the

476
00:22:52,500 --> 00:22:58,320
output of this company and differential

477
00:22:55,740 --> 00:23:00,360
privacy is the technique that looks this

478
00:22:58,320 --> 00:23:03,080
question and tries to give a meaningful

479
00:23:00,360 --> 00:23:05,610
measure of privacy for devaluation

480
00:23:03,080 --> 00:23:08,178
especially when we are in a setting of

481
00:23:05,610 --> 00:23:13,590
computing over the input of many parties

482
00:23:08,179 --> 00:23:16,970
databases people users etc so for

483
00:23:13,590 --> 00:23:19,800
differential privacy we have two main I

484
00:23:16,970 --> 00:23:21,870
thinks one of them is called the central

485
00:23:19,800 --> 00:23:24,750
model which assumes that we have a

486
00:23:21,870 --> 00:23:26,820
trusted aggregator where everybody are

487
00:23:24,750 --> 00:23:29,370
all those clients and their inputs and

488
00:23:26,820 --> 00:23:31,919
distrusted aggregator computes answers

489
00:23:29,370 --> 00:23:37,830
aggregate functionalities and then this

490
00:23:31,920 --> 00:23:39,809
is made public but we are not protecting

491
00:23:37,830 --> 00:23:42,169
against the privacy of the individual

492
00:23:39,809 --> 00:23:45,030
inputs against this trusted aggregator

493
00:23:42,170 --> 00:23:47,700
the other model in differential privacy

494
00:23:45,030 --> 00:23:51,410
which is called the local model wants to

495
00:23:47,700 --> 00:23:57,000
remove the trust in this aggregator and

496
00:23:51,410 --> 00:24:00,120
requires a mechanism that each party or

497
00:23:57,000 --> 00:24:03,150
each participant kind of adds noise to

498
00:24:00,120 --> 00:24:06,719
their data and the untrusted aggregator

499
00:24:03,150 --> 00:24:08,970
can compute this aggregate computation

500
00:24:06,720 --> 00:24:12,420
on based on the based on the inputs that

501
00:24:08,970 --> 00:24:14,550
they receive but without having to put

502
00:24:12,420 --> 00:24:16,710
trust in this aggregator so you want to

503
00:24:14,550 --> 00:24:20,399
guarantee privacy of the individual

504
00:24:16,710 --> 00:24:22,440
input even with respect and the notion

505
00:24:20,400 --> 00:24:23,960
of differential privacy requires that if

506
00:24:22,440 --> 00:24:27,240
you are doing this computation on

507
00:24:23,960 --> 00:24:30,419
databases that differ by a single record

508
00:24:27,240 --> 00:24:33,620
the output of this computation are close

509
00:24:30,419 --> 00:24:36,650
not be distinguished

510
00:24:33,620 --> 00:24:39,139
so there has been a lot of work in the

511
00:24:36,650 --> 00:24:41,510
central model we know to do how we know

512
00:24:39,140 --> 00:24:45,169
how to do many we have general

513
00:24:41,510 --> 00:24:47,120
mechanisms that achieve in the central

514
00:24:45,169 --> 00:24:51,169
model we have also many specialized

515
00:24:47,120 --> 00:24:53,090
methods for different functionalities of

516
00:24:51,169 --> 00:24:55,010
interest such as empirical risk

517
00:24:53,090 --> 00:24:58,639
minimization stochastic gradient descent

518
00:24:55,010 --> 00:25:00,710
Bayesian inference any others one could

519
00:24:58,640 --> 00:25:02,809
argue that really the second model the

520
00:25:00,710 --> 00:25:04,549
local model is much more relevant in

521
00:25:02,809 --> 00:25:08,418
many of the practical applications and

522
00:25:04,549 --> 00:25:11,210
this is what companies like Google and

523
00:25:08,419 --> 00:25:12,919
Apple have been looking at especially

524
00:25:11,210 --> 00:25:15,679
when you are looking at the setting of

525
00:25:12,919 --> 00:25:18,440
collecting private statistics from from

526
00:25:15,679 --> 00:25:20,179
your users of course this second model

527
00:25:18,440 --> 00:25:23,809
is the one that's much more challenging

528
00:25:20,179 --> 00:25:26,870
for coming up with good solutions so one

529
00:25:23,809 --> 00:25:28,309
example that has been studied in the in

530
00:25:26,870 --> 00:25:30,139
the West second model of local

531
00:25:28,309 --> 00:25:34,279
differential privacy is the question of

532
00:25:30,140 --> 00:25:37,340
heavy hitters or detecting if each of

533
00:25:34,279 --> 00:25:40,700
your clients is submitting sample

534
00:25:37,340 --> 00:25:44,120
different words or different values how

535
00:25:40,700 --> 00:25:48,169
do you detect the ones that occur most

536
00:25:44,120 --> 00:25:50,270
often so one of the most recent works in

537
00:25:48,169 --> 00:25:53,419
this area gives you a way that

538
00:25:50,270 --> 00:25:55,240
essentially incurs error where root of n

539
00:25:53,419 --> 00:25:57,440
where n is the number of in of

540
00:25:55,240 --> 00:26:01,899
participants and this is kind of also

541
00:25:57,440 --> 00:26:05,750
the lower bound in this local model and

542
00:26:01,899 --> 00:26:07,820
still this this new construction

543
00:26:05,750 --> 00:26:10,250
guarantees that you could get very very

544
00:26:07,820 --> 00:26:13,850
good fairly good accuracy between the

545
00:26:10,250 --> 00:26:18,470
true counts and the predicted count when

546
00:26:13,850 --> 00:26:21,168
you are looking at both most often

547
00:26:18,470 --> 00:26:24,470
occurring element or the rank 10 or the

548
00:26:21,169 --> 00:26:27,399
rank hundred all so what this work

549
00:26:24,470 --> 00:26:29,929
achieves is that it has a much better

550
00:26:27,399 --> 00:26:32,080
Hill so it gives you much better

551
00:26:29,929 --> 00:26:39,000
accuracy even when you go down to the

552
00:26:32,080 --> 00:26:41,699
most and often counts another

553
00:26:39,000 --> 00:26:43,350
another kind of functionality that you

554
00:26:41,700 --> 00:26:46,110
want to compute in this setting is

555
00:26:43,350 --> 00:26:49,590
frequency estimation and there the work

556
00:26:46,110 --> 00:26:51,418
of rapport from Google heads kind of was

557
00:26:49,590 --> 00:26:52,678
the first solution that was implemented

558
00:26:51,419 --> 00:26:55,470
and people have been looking and

559
00:26:52,679 --> 00:26:58,530
comparing against it so this work from

560
00:26:55,470 --> 00:27:01,650
UNIX in 2017 introduces new hashing

561
00:26:58,530 --> 00:27:04,320
mechanisms that allow you to scale much

562
00:27:01,650 --> 00:27:06,360
more gracefully with the salon which

563
00:27:04,320 --> 00:27:09,000
means essentially that once you start

564
00:27:06,360 --> 00:27:11,100
allowing more leakage in privacy you

565
00:27:09,000 --> 00:27:12,600
suddenly get much more utility it was

566
00:27:11,100 --> 00:27:14,610
one of the biggest challenges in this

567
00:27:12,600 --> 00:27:16,799
setting is to get acceptable utilities

568
00:27:14,610 --> 00:27:19,289
local differential private you usually

569
00:27:16,799 --> 00:27:21,240
gives you a very good privacy but the

570
00:27:19,289 --> 00:27:24,000
challenges are related with the utility

571
00:27:21,240 --> 00:27:28,770
that you're getting in this model so

572
00:27:24,000 --> 00:27:31,700
this work shows you how to do these

573
00:27:28,770 --> 00:27:36,179
trade-offs much more gracefully and also

574
00:27:31,700 --> 00:27:38,309
allows to get much higher level of true

575
00:27:36,179 --> 00:27:39,900
positives so true positives is in a

576
00:27:38,309 --> 00:27:41,460
setting where you want to detect the

577
00:27:39,900 --> 00:27:44,520
counts that are above a certain

578
00:27:41,460 --> 00:27:50,039
threshold correctly and this work shows

579
00:27:44,520 --> 00:27:52,408
you how to get much more positive so and

580
00:27:50,039 --> 00:27:54,470
there has been some direction of very

581
00:27:52,409 --> 00:27:58,140
interesting works that are code

582
00:27:54,470 --> 00:28:00,510
amplification of differential privacy so

583
00:27:58,140 --> 00:28:01,980
there is a general way to compose

584
00:28:00,510 --> 00:28:03,720
differential privacy with secure

585
00:28:01,980 --> 00:28:10,440
computation which essentially says that

586
00:28:03,720 --> 00:28:12,150
you can take a central model of

587
00:28:10,440 --> 00:28:14,730
differential privacy and turn it into a

588
00:28:12,150 --> 00:28:17,880
local model by using secure computation

589
00:28:14,730 --> 00:28:20,400
to implement the aggregator however in

590
00:28:17,880 --> 00:28:23,070
practice this often incurs quite

591
00:28:20,400 --> 00:28:25,620
inefficient solutions so what people

592
00:28:23,070 --> 00:28:27,350
have been looking is this intermediate

593
00:28:25,620 --> 00:28:30,379
model which is called the shuffle model

594
00:28:27,350 --> 00:28:32,969
which assumes that your aggregator is

595
00:28:30,380 --> 00:28:35,130
untrusted but there is an assumption

596
00:28:32,970 --> 00:28:36,690
that there is a shuffle so all the

597
00:28:35,130 --> 00:28:39,150
inputs that are coming from the clients

598
00:28:36,690 --> 00:28:43,740
are first shuffle before tend to the

599
00:28:39,150 --> 00:28:46,320
untrusted aggregate so how you implement

600
00:28:43,740 --> 00:28:48,299
the shuffle is a separate question but

601
00:28:46,320 --> 00:28:50,520
just to point out something is that in

602
00:28:48,299 --> 00:28:52,950
this model now you suddenly start to

603
00:28:50,520 --> 00:28:56,879
rely on other

604
00:28:52,950 --> 00:28:59,700
other users honestly contributing their

605
00:28:56,880 --> 00:29:03,780
inputs because if they don't do that

606
00:28:59,700 --> 00:29:05,850
then your privacy so this model kind of

607
00:29:03,780 --> 00:29:08,520
introduces further assumptions for the

608
00:29:05,850 --> 00:29:10,560
other users but allow you to get much

609
00:29:08,520 --> 00:29:13,410
better trade-offs between privacy and

610
00:29:10,560 --> 00:29:16,320
utility essentially what we know is that

611
00:29:13,410 --> 00:29:18,720
while the local model occurs square root

612
00:29:16,320 --> 00:29:20,760
of n error in terms of the number of

613
00:29:18,720 --> 00:29:22,860
parties now this shuffle model allows

614
00:29:20,760 --> 00:29:27,720
you to get only two logarithmic error

615
00:29:22,860 --> 00:29:30,360
and in particular this paradigm has been

616
00:29:27,720 --> 00:29:33,230
implemented using EDX for the shuffle

617
00:29:30,360 --> 00:29:37,350
and has been demonstrated that you could

618
00:29:33,230 --> 00:29:44,040
do computation up to ten million in

619
00:29:37,350 --> 00:29:45,840
about two hours and also another more

620
00:29:44,040 --> 00:29:47,700
recent work which will be presented here

621
00:29:45,840 --> 00:29:49,770
shows how to formalize further the

622
00:29:47,700 --> 00:29:52,020
shuffle model and get to much better

623
00:29:49,770 --> 00:29:56,460
bounds also extends the result for

624
00:29:52,020 --> 00:29:58,590
larger values of epsilon and kind of

625
00:29:56,460 --> 00:30:05,370
further gives you insight into this new

626
00:29:58,590 --> 00:30:08,250
model also people have been studied a

627
00:30:05,370 --> 00:30:10,709
way to combine this notion of secure a

628
00:30:08,250 --> 00:30:13,440
grenade with the differential privacy in

629
00:30:10,710 --> 00:30:16,860
a way that will allow you to distribute

630
00:30:13,440 --> 00:30:19,620
in a better way that is the the addition

631
00:30:16,860 --> 00:30:22,979
of noise in the setting of differential

632
00:30:19,620 --> 00:30:24,870
privacy and one of the challenges there

633
00:30:22,980 --> 00:30:26,880
is that you would like to minimize the

634
00:30:24,870 --> 00:30:28,830
community communication that's going

635
00:30:26,880 --> 00:30:31,560
from the small devices up to the server

636
00:30:28,830 --> 00:30:33,449
so some of these works have been

637
00:30:31,560 --> 00:30:36,060
studying how to use different type of

638
00:30:33,450 --> 00:30:39,780
noises for differential privacy for

639
00:30:36,060 --> 00:30:42,720
example binomial noise that to skills or

640
00:30:39,780 --> 00:30:46,440
converges to ocean noise when we have

641
00:30:42,720 --> 00:30:48,990
any clients and essentially they have

642
00:30:46,440 --> 00:30:51,210
been studying how for different values

643
00:30:48,990 --> 00:30:57,600
of epsilon which is the level of privacy

644
00:30:51,210 --> 00:31:00,630
you could minimize your community so

645
00:30:57,600 --> 00:31:02,790
that that's this type of solutions also

646
00:31:00,630 --> 00:31:06,270
rely on techniques that are called

647
00:31:02,790 --> 00:31:09,120
quantization which essentially

648
00:31:06,270 --> 00:31:13,290
allow your help you too round when you

649
00:31:09,120 --> 00:31:15,300
are using real numbers and this is the

650
00:31:13,290 --> 00:31:19,440
main point where this quantization

651
00:31:15,300 --> 00:31:21,960
doesn't usually interact very nicely

652
00:31:19,440 --> 00:31:25,860
with Gaussian noise but it interacts and

653
00:31:21,960 --> 00:31:28,890
composes quite nicely with so this work

654
00:31:25,860 --> 00:31:32,250
has been looking at questions and

655
00:31:28,890 --> 00:31:35,790
exploring how far we can reduce the

656
00:31:32,250 --> 00:31:39,840
communication the federated learning

657
00:31:35,790 --> 00:31:42,570
setting so this brings me to the last

658
00:31:39,840 --> 00:31:44,310
type of functionality I wanted to

659
00:31:42,570 --> 00:31:46,590
discuss which is zero knowledge proof

660
00:31:44,310 --> 00:31:50,040
and we already heard a lot about zero

661
00:31:46,590 --> 00:31:51,899
knowledge that we would like to have a

662
00:31:50,040 --> 00:31:53,970
way for example to prove that an

663
00:31:51,900 --> 00:31:57,710
encryption of some value is within some

664
00:31:53,970 --> 00:32:01,200
range and to produce a proof for this

665
00:31:57,710 --> 00:32:04,170
statement of course we can generalize

666
00:32:01,200 --> 00:32:06,510
this to any statement and we've had many

667
00:32:04,170 --> 00:32:09,890
solutions that are achieving this

668
00:32:06,510 --> 00:32:13,320
functionality why do we care about

669
00:32:09,890 --> 00:32:15,330
proofs I think that unnecessary to

670
00:32:13,320 --> 00:32:18,030
justify here of course blockchain

671
00:32:15,330 --> 00:32:20,899
applications have been tagged as one of

672
00:32:18,030 --> 00:32:22,980
the potential uses there are many more

673
00:32:20,900 --> 00:32:25,680
when you talk about machine learning

674
00:32:22,980 --> 00:32:27,780
maybe you care about proving that your

675
00:32:25,680 --> 00:32:29,520
machine learning computation was done

676
00:32:27,780 --> 00:32:33,230
correctly as well so you would like to

677
00:32:29,520 --> 00:32:36,990
produce a zero knowledge proof for your

678
00:32:33,230 --> 00:32:38,280
inference or for your training of course

679
00:32:36,990 --> 00:32:39,630
when we talk about during knowledge

680
00:32:38,280 --> 00:32:41,760
protocols

681
00:32:39,630 --> 00:32:43,770
there are many measures that you need to

682
00:32:41,760 --> 00:32:47,250
consider this is proven efficiency

683
00:32:43,770 --> 00:32:49,590
verify your efficiency think how long is

684
00:32:47,250 --> 00:32:52,730
your proof whether you need an

685
00:32:49,590 --> 00:32:56,520
interaction or non interaction for your

686
00:32:52,730 --> 00:33:00,380
construction and of course whether you

687
00:32:56,520 --> 00:33:03,060
need trusted setup or trusted setup

688
00:33:00,380 --> 00:33:06,750
essentially says that you need compute

689
00:33:03,060 --> 00:33:09,300
some arrests or some common reference

690
00:33:06,750 --> 00:33:10,740
string that depends on great information

691
00:33:09,300 --> 00:33:13,169
and if the security information is

692
00:33:10,740 --> 00:33:19,480
revealed then all soundness guarantees

693
00:33:13,170 --> 00:33:22,180
of your protocol disappear so oftentimes

694
00:33:19,480 --> 00:33:23,980
Ron mentioned in his protocol snarks or

695
00:33:22,180 --> 00:33:27,610
the succinct non-interactive arguments

696
00:33:23,980 --> 00:33:33,670
of knowledge are a primary interest and

697
00:33:27,610 --> 00:33:36,610
then in other other solutions are not so

698
00:33:33,670 --> 00:33:39,070
much but of course the their settings in

699
00:33:36,610 --> 00:33:44,139
which they have applications as well

700
00:33:39,070 --> 00:33:47,710
so for snarks most of our understanding

701
00:33:44,140 --> 00:33:49,150
is that we require trusted setup there

702
00:33:47,710 --> 00:33:51,070
are some works that are looking also at

703
00:33:49,150 --> 00:33:53,470
hybrid solutions that offer a

704
00:33:51,070 --> 00:33:59,230
functionality that would function either

705
00:33:53,470 --> 00:34:01,990
as snark or have a way to back the back

706
00:33:59,230 --> 00:34:06,220
fold to a regular constructions that

707
00:34:01,990 --> 00:34:07,930
don't require your twisted setup so here

708
00:34:06,220 --> 00:34:11,320
I will kind of give you what we know in

709
00:34:07,930 --> 00:34:14,800
terms of efficiency of snarks first so

710
00:34:11,320 --> 00:34:18,610
most of the implementation of the case

711
00:34:14,800 --> 00:34:21,130
narcs rely on this work on quadratic

712
00:34:18,610 --> 00:34:23,610
arithmetic program that are implemented

713
00:34:21,130 --> 00:34:27,520
and instantiated in different ways and

714
00:34:23,610 --> 00:34:31,260
this work from UNIX last year looks at

715
00:34:27,520 --> 00:34:35,230
the way to distribute the prover in a

716
00:34:31,260 --> 00:34:37,510
nark construction so they are trying to

717
00:34:35,230 --> 00:34:41,170
paralyze the work of the approver and

718
00:34:37,510 --> 00:34:44,530
see how they can optimize the prover

719
00:34:41,170 --> 00:34:46,960
work so here what they managed to

720
00:34:44,530 --> 00:34:49,960
achieve is that the overhead for the

721
00:34:46,960 --> 00:34:53,110
prover for each gate and computation is

722
00:34:49,960 --> 00:34:56,020
about 10 microseconds and the verifier

723
00:34:53,110 --> 00:34:58,420
time in this zero knowledge snarks

724
00:34:56,020 --> 00:35:00,100
usually the verifier is very efficient

725
00:34:58,420 --> 00:35:02,620
that's one of the appeal of this

726
00:35:00,100 --> 00:35:07,360
construction so here the verification

727
00:35:02,620 --> 00:35:11,680
time is 2 milliseconds plus 0.5

728
00:35:07,360 --> 00:35:13,810
microseconds per input for the zero

729
00:35:11,680 --> 00:35:15,339
knowledge statement if we want to look

730
00:35:13,810 --> 00:35:16,930
at some of the application that they

731
00:35:15,340 --> 00:35:18,880
consider they can do matrix

732
00:35:16,930 --> 00:35:24,370
multiplication the prover time for

733
00:35:18,880 --> 00:35:25,930
matrix multiplication 24 seconds or if

734
00:35:24,370 --> 00:35:27,549
you want to do linear regression this

735
00:35:25,930 --> 00:35:29,290
was one of the example I was telling you

736
00:35:27,550 --> 00:35:31,650
in the context of machine learning if

737
00:35:29,290 --> 00:35:33,330
you want to do a linear regression on

738
00:35:31,650 --> 00:35:36,760
20,000

739
00:35:33,330 --> 00:35:38,230
in dimension 500 they can generate to

740
00:35:36,760 --> 00:35:43,770
prove that the training was done

741
00:35:38,230 --> 00:35:46,630
correctly in 95 seconds so still in the

742
00:35:43,770 --> 00:35:49,180
setting of zero knowledge with trusted

743
00:35:46,630 --> 00:35:52,540
setup there is another work that really

744
00:35:49,180 --> 00:35:55,480
aims at improving the prover time so

745
00:35:52,540 --> 00:35:58,180
most of the zero knowledge snark

746
00:35:55,480 --> 00:36:02,290
constructions kind of require the prover

747
00:35:58,180 --> 00:36:04,000
to be quasi linear in the size of the

748
00:36:02,290 --> 00:36:06,580
circuit that you are evaluating this

749
00:36:04,000 --> 00:36:08,380
work really brings the prover down to be

750
00:36:06,580 --> 00:36:10,660
linear in the size of the circuit that

751
00:36:08,380 --> 00:36:14,440
you are computing and they are trading

752
00:36:10,660 --> 00:36:16,330
it off with the size of the proof so now

753
00:36:14,440 --> 00:36:18,220
here the proof becomes logarithmic of

754
00:36:16,330 --> 00:36:21,310
the circuit and linear in the depth of

755
00:36:18,220 --> 00:36:25,240
the circuit this work is based on the

756
00:36:21,310 --> 00:36:28,599
gt-r technique and they essentially show

757
00:36:25,240 --> 00:36:31,330
you how you can improve on some of the

758
00:36:28,599 --> 00:36:44,200
snark where the main improvement you

759
00:36:31,330 --> 00:36:45,900
should be looking for it in the and then

760
00:36:44,200 --> 00:36:49,419
we have the comparisons with

761
00:36:45,900 --> 00:36:53,050
verification and proof size we're still

762
00:36:49,420 --> 00:36:55,510
the the snark constructions which were

763
00:36:53,050 --> 00:36:57,040
meant this way their properties achieve

764
00:36:55,510 --> 00:36:59,890
better efficiency in terms of

765
00:36:57,040 --> 00:37:02,279
verification they also looked at some

766
00:36:59,890 --> 00:37:05,200
examples for image scaling where you are

767
00:37:02,280 --> 00:37:07,780
changing from high to low resolution by

768
00:37:05,200 --> 00:37:10,410
scaling your your image and they show

769
00:37:07,780 --> 00:37:26,740
that you can do this for images of size

770
00:37:10,410 --> 00:37:34,720
n to the 6 pixels about 20 seconds and

771
00:37:26,740 --> 00:37:37,740
the proof size is also very small so if

772
00:37:34,720 --> 00:37:40,689
we want to move outside the realm of

773
00:37:37,740 --> 00:37:44,890
trusted setup so this trusted set up is

774
00:37:40,690 --> 00:37:47,140
a requirement that often kind of brings

775
00:37:44,890 --> 00:37:49,089
up issues when you have practical

776
00:37:47,140 --> 00:37:52,180
implementations like in particular who

777
00:37:49,089 --> 00:37:53,410
is going to generate for you so if you

778
00:37:52,180 --> 00:37:55,149
want to look at zero knowledge without

779
00:37:53,410 --> 00:37:59,200
trust it set up there has been a

780
00:37:55,150 --> 00:38:01,029
plethora of work in this setting the

781
00:37:59,200 --> 00:38:04,210
proofs ice starts to vary from

782
00:38:01,029 --> 00:38:07,569
logarithmic to linear in the circuit

783
00:38:04,210 --> 00:38:10,329
size the very firework also varies from

784
00:38:07,569 --> 00:38:15,609
logarithmic to linear the provers work

785
00:38:10,329 --> 00:38:19,029
is quasi linear oftentimes polynomial in

786
00:38:15,609 --> 00:38:20,619
the size of the circuit so we have

787
00:38:19,029 --> 00:38:24,539
different constructions that are based

788
00:38:20,619 --> 00:38:28,299
on discrete log on MPC on this iup model

789
00:38:24,539 --> 00:38:30,279
and we can see oftentimes the canonical

790
00:38:28,299 --> 00:38:32,349
example that people are evaluating with

791
00:38:30,279 --> 00:38:37,359
is computing computation of a Merkle

792
00:38:32,349 --> 00:38:40,420
tree and most of the tempos go to about

793
00:38:37,359 --> 00:38:45,848
228 leaves in the market tree and you

794
00:38:40,420 --> 00:38:46,930
can see that prover time in seconds

795
00:38:45,849 --> 00:38:53,470
depending on the different

796
00:38:46,930 --> 00:38:56,308
implementations verification time again

797
00:38:53,470 --> 00:38:58,720
varies a lot across different

798
00:38:56,309 --> 00:39:02,559
implementation but some of them give you

799
00:38:58,720 --> 00:39:08,019
clothes and some of them really scale

800
00:39:02,559 --> 00:39:10,509
with with the size of the proof so

801
00:39:08,019 --> 00:39:13,118
that's but the message the more the

802
00:39:10,509 --> 00:39:17,799
message work here is that we can handle

803
00:39:13,119 --> 00:39:19,569
this barcode proofs for the sizes of

804
00:39:17,799 --> 00:39:26,079
more countries with times that are

805
00:39:19,569 --> 00:39:28,538
fairly visible and one last work that I

806
00:39:26,079 --> 00:39:32,789
wanted to talk about is this work on GK

807
00:39:28,539 --> 00:39:39,910
Starks that look at the different

808
00:39:32,789 --> 00:39:44,079
paradigm of active or Co proof and they

809
00:39:39,910 --> 00:39:48,489
also claim kind of new trade offs

810
00:39:44,079 --> 00:39:51,749
compared to a stark works as well as

811
00:39:48,489 --> 00:39:59,440
some of the works that don't rely on

812
00:39:51,749 --> 00:40:00,790
trusted but maybe most interesting is to

813
00:39:59,440 --> 00:40:02,890
look at one of their application

814
00:40:00,790 --> 00:40:04,480
which is DNA profile matching

815
00:40:02,890 --> 00:40:08,460
essentially you are trying you have a

816
00:40:04,480 --> 00:40:11,980
database of DNA profiles which goes to

817
00:40:08,460 --> 00:40:13,540
go to there are two experiments go to up

818
00:40:11,980 --> 00:40:16,300
to two to the 32

819
00:40:13,540 --> 00:40:18,700
database size of DNA profiles and then

820
00:40:16,300 --> 00:40:21,820
what you want to compute approve for is

821
00:40:18,700 --> 00:40:25,270
that you have a DNA profile that is not

822
00:40:21,820 --> 00:40:37,690
included in this database so for this

823
00:40:25,270 --> 00:40:41,830
type of computation works in which maybe

824
00:40:37,690 --> 00:40:46,750
is usable in settings where you want to

825
00:40:41,830 --> 00:40:53,680
prove and then verification time is

826
00:40:46,750 --> 00:40:56,740
quite efficient again the trade off goes

827
00:40:53,680 --> 00:41:03,129
into the size of the of the proof which

828
00:40:56,740 --> 00:41:06,040
skills and so with this I think I'm out

829
00:41:03,130 --> 00:41:08,530
of time so I will wrap it up that we

830
00:41:06,040 --> 00:41:10,930
have lots of advanced crypto techniques

831
00:41:08,530 --> 00:41:15,010
that have achieved efficiency that is

832
00:41:10,930 --> 00:41:18,759
very close what we can start using using

833
00:41:15,010 --> 00:41:21,400
in practice using these techniques often

834
00:41:18,760 --> 00:41:23,020
requires to have an expert who knows how

835
00:41:21,400 --> 00:41:25,060
to use this technique so this kind of

836
00:41:23,020 --> 00:41:27,880
brings a challenge when we want to

837
00:41:25,060 --> 00:41:30,460
standardize how to go around is issue

838
00:41:27,880 --> 00:41:32,230
that we need this expert for using these

839
00:41:30,460 --> 00:41:35,620
techniques and what is the best way to

840
00:41:32,230 --> 00:41:37,720
standardize and offer some insights that

841
00:41:35,620 --> 00:41:42,310
maybe people who are not experts could

842
00:41:37,720 --> 00:41:46,509
use as well so there have been discussed

843
00:41:42,310 --> 00:41:50,170
and there are also many sources for for

844
00:41:46,510 --> 00:41:54,310
MPC for $0.50 or previously and then

845
00:41:50,170 --> 00:41:56,670
I'll conclude with cycle elvis's words

846
00:41:54,310 --> 00:41:59,820
that it's time to put these tools to use

847
00:41:56,670 --> 00:41:59,820
thank you

848
00:42:06,910 --> 00:42:12,920
and the question to you so as far as I

849
00:42:10,970 --> 00:42:14,930
know there are no known Turkish your

850
00:42:12,920 --> 00:42:18,110
knowledge what the cause in the plane

851
00:42:14,930 --> 00:42:23,960
model without setup that's gold right

852
00:42:18,110 --> 00:42:26,480
Oren so there are no known Interactive's

853
00:42:23,960 --> 00:42:37,940
your knowledge protocols without trusted

854
00:42:26,480 --> 00:42:40,280
serum it don't exist right so no okay so

855
00:42:37,940 --> 00:42:42,710
what I mean is that you just say that

856
00:42:40,280 --> 00:42:45,440
all these protocols are in the random

857
00:42:42,710 --> 00:42:48,470
worker model there are non-interactive

858
00:42:45,440 --> 00:42:50,660
and they claim to be zero knowledge so

859
00:42:48,470 --> 00:42:52,730
they are have to be some assumptions

860
00:42:50,660 --> 00:42:54,319
when the moral right it's not just you

861
00:42:52,730 --> 00:42:56,270
but I think there are a bunch of papers

862
00:42:54,320 --> 00:42:58,280
here in the titles in this workshop

863
00:42:56,270 --> 00:43:00,920
let's say not interactive zero knowledge

864
00:42:58,280 --> 00:43:08,510
without trust setup which is something

865
00:43:00,920 --> 00:43:10,970
that doesn't exist and doesn't matter

866
00:43:08,510 --> 00:43:12,920
there is a proof that without any

867
00:43:10,970 --> 00:43:14,779
trusted setup no Interactive's your

868
00:43:12,920 --> 00:43:17,770
knowledge does not exist it's involved

869
00:43:14,780 --> 00:43:17,770
you know basic

870
00:43:26,770 --> 00:43:33,140
it's training versus versus the

871
00:43:30,470 --> 00:43:35,180
structured one so maybe the the term

872
00:43:33,140 --> 00:43:36,859
instructor untrusted set up without an

873
00:43:35,180 --> 00:43:44,598
trusted set up is kind of like

874
00:43:36,859 --> 00:43:46,640
misleading no trusted set up has been

875
00:43:44,599 --> 00:43:49,640
used is essentially assuming this

876
00:43:46,640 --> 00:43:54,290
unstructured Griffin Street a reference

877
00:43:49,640 --> 00:43:56,299
string so yeah for those yeah this is

878
00:43:54,290 --> 00:43:58,900
the classification that was used in my

879
00:43:56,300 --> 00:43:58,900
side so yeah

880
00:44:02,520 --> 00:44:06,538
[Applause]

