1
00:00:00,160 --> 00:00:02,640
go ahead

2
00:00:03,199 --> 00:00:06,080
okay so we have an exciting uh

3
00:00:06,080 --> 00:00:07,680
session with five contributor talks the

4
00:00:07,680 --> 00:00:10,000
first one is about secure quantized

5
00:00:10,000 --> 00:00:12,240
training for deep learning

6
00:00:12,240 --> 00:00:15,120
by marcel keller so marcel i guess

7
00:00:15,120 --> 00:00:16,400
you're there could you share your slides

8
00:00:16,400 --> 00:00:18,640
please

9
00:00:18,720 --> 00:00:21,039
yes um

10
00:00:21,039 --> 00:00:22,400
or no

11
00:00:22,400 --> 00:00:24,880
no yes

12
00:00:29,439 --> 00:00:31,039
how is that

13
00:00:31,039 --> 00:00:33,600
it looks fantastic can you try to move

14
00:00:33,600 --> 00:00:36,559
from one side to the other

15
00:00:36,640 --> 00:00:38,559
attack

16
00:00:38,559 --> 00:00:39,440
works

17
00:00:39,440 --> 00:00:41,440
super so yeah

18
00:00:41,440 --> 00:00:45,840
fantastic so 10 minutes uh take it away

19
00:00:45,840 --> 00:00:48,640
all right uh yeah um

20
00:00:48,640 --> 00:00:49,440
uh

21
00:00:49,440 --> 00:00:50,640
thank you and

22
00:00:50,640 --> 00:00:52,800
welcome to my talk my name is marcel

23
00:00:52,800 --> 00:00:55,960
keller i'm a researcher with

24
00:00:55,960 --> 00:00:58,960
crosstalk 61 in australia

25
00:00:58,960 --> 00:01:01,280
and today i'm going to talk about secure

26
00:01:01,280 --> 00:01:04,319
quantized training for deep learning

27
00:01:04,319 --> 00:01:05,600
and

28
00:01:05,600 --> 00:01:08,240
it should come as no surprise that i use

29
00:01:08,240 --> 00:01:10,159
a secure multiparty

30
00:01:10,159 --> 00:01:12,560
computation for this uh you already

31
00:01:12,560 --> 00:01:16,720
heard a good deal about mpc from uh uval

32
00:01:16,720 --> 00:01:19,520
so just a quick roundup we want to uh

33
00:01:19,520 --> 00:01:22,320
compute on secret inputs

34
00:01:22,320 --> 00:01:26,479
that are held by a number of parties

35
00:01:26,479 --> 00:01:28,799
and conceptually we want to replace a

36
00:01:28,799 --> 00:01:29,840
trusted

37
00:01:29,840 --> 00:01:32,400
third party by a protocol between these

38
00:01:32,400 --> 00:01:33,759
parties

39
00:01:33,759 --> 00:01:36,000
and

40
00:01:36,240 --> 00:01:38,960
the model we're going to use here

41
00:01:38,960 --> 00:01:41,119
i would like to call or sometimes called

42
00:01:41,119 --> 00:01:43,280
black box computation which is

43
00:01:43,280 --> 00:01:46,560
essentially all the parties just put

44
00:01:46,560 --> 00:01:49,119
their inputs in into this conceptual

45
00:01:49,119 --> 00:01:50,399
black box

46
00:01:50,399 --> 00:01:53,840
and then they have the black box compute

47
00:01:53,840 --> 00:01:56,320
whatever they want to compute

48
00:01:56,320 --> 00:01:58,320
in this work we're going to have three

49
00:01:58,320 --> 00:02:01,520
parties and one semi-one is corruption

50
00:02:01,520 --> 00:02:04,079
that means that essentially

51
00:02:04,079 --> 00:02:06,079
every party so that can look at their

52
00:02:06,079 --> 00:02:08,720
information and derive nothing from

53
00:02:08,720 --> 00:02:11,120
their information but but all parties

54
00:02:11,120 --> 00:02:14,720
are expected to follow the protocol

55
00:02:14,720 --> 00:02:17,200
so that's the cryptography side now we

56
00:02:17,200 --> 00:02:19,200
go to the machine learning side

57
00:02:19,200 --> 00:02:20,959
a few words about

58
00:02:20,959 --> 00:02:23,680
deep learning it's an established

59
00:02:23,680 --> 00:02:26,000
concept in machine learning

60
00:02:26,000 --> 00:02:28,080
or supervised machine learning which

61
00:02:28,080 --> 00:02:30,080
means that we have

62
00:02:30,080 --> 00:02:32,560
like known inputs outputs basically we

63
00:02:32,560 --> 00:02:35,680
have dot where we know

64
00:02:35,680 --> 00:02:37,920
like like what we want to figure out on

65
00:02:37,920 --> 00:02:41,519
some other input data the computation is

66
00:02:41,519 --> 00:02:44,400
specified as a chain of functions

67
00:02:44,400 --> 00:02:46,000
uh which are called

68
00:02:46,000 --> 00:02:47,040
layers

69
00:02:47,040 --> 00:02:50,400
some of these functions have parameters

70
00:02:50,400 --> 00:02:52,560
and it's these parameters that we want

71
00:02:52,560 --> 00:02:55,840
to try to figure out during a

72
00:02:55,840 --> 00:02:58,080
learning or training process

73
00:02:58,080 --> 00:03:01,280
there is also a function of quantifying

74
00:03:01,280 --> 00:03:02,400
the

75
00:03:02,400 --> 00:03:04,319
the quality of the parameters how good

76
00:03:04,319 --> 00:03:06,800
they work and this quantifica this

77
00:03:06,800 --> 00:03:09,200
function is called

78
00:03:09,200 --> 00:03:11,040
a loss function and the reason to

79
00:03:11,040 --> 00:03:12,879
specify this loss function

80
00:03:12,879 --> 00:03:16,239
uh is that we can

81
00:03:16,400 --> 00:03:17,680
minimize it

82
00:03:17,680 --> 00:03:19,040
by

83
00:03:19,040 --> 00:03:21,920
basically running uh derivation

84
00:03:21,920 --> 00:03:23,760
reverse uh

85
00:03:23,760 --> 00:03:26,400
by running um derivation on the loss

86
00:03:26,400 --> 00:03:28,560
function and with the chain rule

87
00:03:28,560 --> 00:03:31,760
and this gives us a way how to update

88
00:03:31,760 --> 00:03:34,720
and change the parameters so

89
00:03:34,720 --> 00:03:36,080
that we can

90
00:03:36,080 --> 00:03:39,440
sort of have a walk towards minimizing

91
00:03:39,440 --> 00:03:40,879
the loss function

92
00:03:40,879 --> 00:03:43,200
and this process is called backward

93
00:03:43,200 --> 00:03:46,159
propagation

94
00:03:46,159 --> 00:03:49,440
all right now going back to more uh

95
00:03:49,440 --> 00:03:52,159
crypto land what sort of computational

96
00:03:52,159 --> 00:03:54,560
building blocks do we need to uh

97
00:03:54,560 --> 00:03:56,799
implement to achieve this

98
00:03:56,799 --> 00:03:58,239
machine learning

99
00:03:58,239 --> 00:04:00,400
uh we went for

100
00:04:00,400 --> 00:04:03,040
replicated secret sharing because that

101
00:04:03,040 --> 00:04:04,480
allows very

102
00:04:04,480 --> 00:04:07,439
cheap computation as

103
00:04:07,439 --> 00:04:10,319
somewhat mentioned by you all

104
00:04:10,319 --> 00:04:12,000
earlier

105
00:04:12,000 --> 00:04:13,040
for

106
00:04:13,040 --> 00:04:15,680
fractional numbers that is like non

107
00:04:15,680 --> 00:04:17,759
integer numbers if you will we use

108
00:04:17,759 --> 00:04:19,759
quantization or

109
00:04:19,759 --> 00:04:22,560
more specifically the common fixed point

110
00:04:22,560 --> 00:04:25,199
representation where you multiply the

111
00:04:25,199 --> 00:04:27,199
number by a power of 2 and then you just

112
00:04:27,199 --> 00:04:29,360
drawn to the nearest integer

113
00:04:29,360 --> 00:04:32,000
and this implies that the addition is

114
00:04:32,000 --> 00:04:34,560
essentially just integer addition

115
00:04:34,560 --> 00:04:37,040
uh for multiplication you run the

116
00:04:37,040 --> 00:04:39,040
integer multiplication and then kind of

117
00:04:39,040 --> 00:04:41,040
have to do some sort of

118
00:04:41,040 --> 00:04:45,680
right shift or or truncation

119
00:04:45,680 --> 00:04:47,520
we use

120
00:04:47,520 --> 00:04:49,759
uh what is sometimes called the optimize

121
00:04:49,759 --> 00:04:51,919
dot product for linear functions which

122
00:04:51,919 --> 00:04:54,240
mean that with some

123
00:04:54,240 --> 00:04:57,199
secret sharing schemes you can have

124
00:04:57,199 --> 00:04:59,520
communication that is linear in the

125
00:04:59,520 --> 00:05:02,800
output of the linear function or just

126
00:05:02,800 --> 00:05:04,320
basically

127
00:05:04,320 --> 00:05:06,720
the cost of one multiplication for the

128
00:05:06,720 --> 00:05:09,440
whole dot product

129
00:05:09,440 --> 00:05:12,000
we implement comparisons with mixed

130
00:05:12,000 --> 00:05:16,320
circuits uh pretty much a by apy3 style

131
00:05:16,320 --> 00:05:18,880
we implement division using uh

132
00:05:18,880 --> 00:05:20,720
goldschmidt iteration

133
00:05:20,720 --> 00:05:24,800
is also an established algorithm

134
00:05:24,800 --> 00:05:28,960
for exponentiation we split the input to

135
00:05:28,960 --> 00:05:32,800
an integer part and a fractional part

136
00:05:32,800 --> 00:05:37,120
and we use precise computation for the

137
00:05:37,120 --> 00:05:38,479
integer part

138
00:05:38,479 --> 00:05:39,919
and

139
00:05:39,919 --> 00:05:42,639
polynomial approximation for the

140
00:05:42,639 --> 00:05:44,960
for the fractional part

141
00:05:44,960 --> 00:05:48,479
and then uh last but not least uh in

142
00:05:48,479 --> 00:05:51,360
this uh it turned out that we use the

143
00:05:51,360 --> 00:05:53,680
so-called that is best to use and

144
00:05:53,680 --> 00:05:55,840
optimize that's called ams grad and that

145
00:05:55,840 --> 00:05:58,240
relies on inverse square root that is

146
00:05:58,240 --> 00:06:01,759
one over x squared and we

147
00:06:01,759 --> 00:06:03,919
use some approximation that we also use

148
00:06:03,919 --> 00:06:07,199
mixed circuits for

149
00:06:07,199 --> 00:06:09,680
a few words about uh

150
00:06:09,680 --> 00:06:12,400
mnist it's had it has been called the

151
00:06:12,400 --> 00:06:13,919
hello world of

152
00:06:13,919 --> 00:06:16,319
machine learning essentially it's a few

153
00:06:16,319 --> 00:06:17,759
ten thousand

154
00:06:17,759 --> 00:06:21,199
uh 28 by 28 grayscale

155
00:06:21,199 --> 00:06:24,000
images of handwritten

156
00:06:24,000 --> 00:06:26,560
digits so basically what we want to do

157
00:06:26,560 --> 00:06:29,440
we want to devise a machine algorithm

158
00:06:29,440 --> 00:06:31,759
machine learning algorithm that

159
00:06:31,759 --> 00:06:33,360
maps these

160
00:06:33,360 --> 00:06:34,400
is to

161
00:06:34,400 --> 00:06:36,240
zero to nine and

162
00:06:36,240 --> 00:06:38,240
as correct as possible

163
00:06:38,240 --> 00:06:39,919
and this

164
00:06:39,919 --> 00:06:42,639
is widely used in machine learning has

165
00:06:42,639 --> 00:06:45,360
like some historical significance also

166
00:06:45,360 --> 00:06:48,720
because it demonstrated the utility of

167
00:06:48,720 --> 00:06:50,560
convolution

168
00:06:50,560 --> 00:06:52,639
which is some sort of linear

169
00:06:52,639 --> 00:06:56,720
functionality that uses locality so it

170
00:06:56,720 --> 00:06:59,680
looks at sort of small squares of

171
00:06:59,680 --> 00:07:01,759
pixels

172
00:07:01,759 --> 00:07:03,919
if you will

173
00:07:03,919 --> 00:07:06,560
and that already brings me to the result

174
00:07:06,560 --> 00:07:09,840
uh we have implemented uh

175
00:07:09,840 --> 00:07:11,199
this uh

176
00:07:11,199 --> 00:07:14,319
lunette model named after le con

177
00:07:14,319 --> 00:07:16,800
it all this famous machine learning

178
00:07:16,800 --> 00:07:19,759
researchers researcher it uses four

179
00:07:19,759 --> 00:07:21,360
linear layers

180
00:07:21,360 --> 00:07:24,479
as i've mentioned before he used the ams

181
00:07:24,479 --> 00:07:27,440
groud optimizer which is an improvement

182
00:07:27,440 --> 00:07:29,440
on the

183
00:07:29,440 --> 00:07:32,160
better known std or stochastic gradient

184
00:07:32,160 --> 00:07:33,759
descent

185
00:07:33,759 --> 00:07:35,280
we have run

186
00:07:35,280 --> 00:07:36,319
our

187
00:07:36,319 --> 00:07:39,840
computation on three aws

188
00:07:39,840 --> 00:07:43,199
instances and we have found that

189
00:07:43,199 --> 00:07:44,479
it takes about

190
00:07:44,479 --> 00:07:46,080
nine minutes per

191
00:07:46,080 --> 00:07:48,560
epoch which is like one sort of one

192
00:07:48,560 --> 00:07:50,160
iteration of the learning process where

193
00:07:50,160 --> 00:07:52,160
you go over the whole

194
00:07:52,160 --> 00:07:53,840
60 000

195
00:07:53,840 --> 00:07:56,800
example images in this case

196
00:07:56,800 --> 00:07:58,400
and that means

197
00:07:58,400 --> 00:07:59,440
uh

198
00:07:59,440 --> 00:08:01,759
it takes one hour for 99

199
00:08:01,759 --> 00:08:05,280
accuracy so on the left side

200
00:08:05,280 --> 00:08:08,479
you can see how the testing error

201
00:08:08,479 --> 00:08:10,720
goes down so you run on a training set

202
00:08:10,720 --> 00:08:12,400
and then there is 10 000

203
00:08:12,400 --> 00:08:14,639
image test set and you

204
00:08:14,639 --> 00:08:17,280
look at how correct you are on the test

205
00:08:17,280 --> 00:08:18,319
set

206
00:08:18,319 --> 00:08:22,400
and as you can see in the graph

207
00:08:22,400 --> 00:08:24,000
we are

208
00:08:24,000 --> 00:08:25,520
i would say

209
00:08:25,520 --> 00:08:27,680
marginally worse than

210
00:08:27,680 --> 00:08:29,440
running the same

211
00:08:29,440 --> 00:08:31,520
training algorithm in

212
00:08:31,520 --> 00:08:34,320
in clear text and that is

213
00:08:34,320 --> 00:08:36,080
probably because

214
00:08:36,080 --> 00:08:36,958
we

215
00:08:36,958 --> 00:08:38,399
use a lower

216
00:08:38,399 --> 00:08:41,440
precision with the fixed point

217
00:08:41,440 --> 00:08:42,539
quantization

218
00:08:42,539 --> 00:08:44,159
[Music]

219
00:08:44,159 --> 00:08:46,480
and and also for

220
00:08:46,480 --> 00:08:47,279
the

221
00:08:47,279 --> 00:08:49,360
like sort of various mathematical

222
00:08:49,360 --> 00:08:52,240
functions we compute

223
00:08:52,240 --> 00:08:55,920
and that brings me already to the end um

224
00:08:55,920 --> 00:08:58,959
uh there is plenty of material for this

225
00:08:58,959 --> 00:09:01,760
on the internet there is the paper on

226
00:09:01,760 --> 00:09:04,959
archive there is a specific repository

227
00:09:04,959 --> 00:09:06,000
for

228
00:09:06,000 --> 00:09:08,640
scripts and a docker file to

229
00:09:08,640 --> 00:09:11,519
run this um for yourself

230
00:09:11,519 --> 00:09:14,240
uh the whole implementation

231
00:09:14,240 --> 00:09:17,519
is based on empty speeds

232
00:09:17,519 --> 00:09:19,839
for which there is another documentation

233
00:09:19,839 --> 00:09:23,200
site on read readthedocs.io and last but

234
00:09:23,200 --> 00:09:26,240
not least you can follow me on twitter

235
00:09:26,240 --> 00:09:27,360
and

236
00:09:27,360 --> 00:09:31,519
that's it so i guess i'm gonna

237
00:09:31,519 --> 00:09:35,440
try to stop sharing my screen

238
00:09:35,600 --> 00:09:36,720
thanks myself thank you for the

239
00:09:36,720 --> 00:09:38,160
interesting talk

240
00:09:38,160 --> 00:09:39,200
um

241
00:09:39,200 --> 00:09:40,959
i think we have time for one little

242
00:09:40,959 --> 00:09:42,720
question

243
00:09:42,720 --> 00:09:45,600
if there is one

244
00:09:48,320 --> 00:09:50,000
um

245
00:09:50,000 --> 00:09:52,240
maybe just one curiosity that that i

246
00:09:52,240 --> 00:09:54,160
just want to know you said nine minutes

247
00:09:54,160 --> 00:09:56,880
per per epoch for for leonard

248
00:09:56,880 --> 00:09:59,120
uh securely how much time would you

249
00:09:59,120 --> 00:10:01,839
spend on the same machine to train it

250
00:10:01,839 --> 00:10:05,040
that one epoch without any security

251
00:10:05,040 --> 00:10:06,560
just to get a comparison

252
00:10:06,560 --> 00:10:09,200
what the security actually costs

253
00:10:09,200 --> 00:10:10,160
uh

254
00:10:10,160 --> 00:10:13,120
yeah i mean uh

255
00:10:13,600 --> 00:10:15,279
basically when you like when you're on

256
00:10:15,279 --> 00:10:17,279
it it just goes like

257
00:10:17,279 --> 00:10:19,680
i'm gonna i'm gonna say it feels like a

258
00:10:19,680 --> 00:10:21,279
few seconds

259
00:10:21,279 --> 00:10:25,440
okay on on on on the same one gonna say

260
00:10:25,440 --> 00:10:26,320
maybe

261
00:10:26,320 --> 00:10:28,160
maybe 10 seconds

262
00:10:28,160 --> 00:10:29,279
so so

263
00:10:29,279 --> 00:10:31,519
to be honest like i've run it plenty of

264
00:10:31,519 --> 00:10:34,399
times but i never timed it but yeah okay

265
00:10:34,399 --> 00:10:36,560
let's say 10 seconds so security costs

266
00:10:36,560 --> 00:10:39,920
you a factor of 60 so to speak

267
00:10:40,880 --> 00:10:42,720
uh is that the math yeah

268
00:10:42,720 --> 00:10:44,560
i i guess in in

269
00:10:44,560 --> 00:10:47,760
in this in this context yes

270
00:10:47,760 --> 00:10:50,399
unless i'm off and it's actually just

271
00:10:50,399 --> 00:10:52,240
one second

272
00:10:52,240 --> 00:10:53,920
anyways thanks again for the interesting

273
00:10:53,920 --> 00:10:56,240
talk and uh

274
00:10:56,240 --> 00:11:00,000
let's uh

275
00:11:00,000 --> 00:11:02,160
well i think commits has to stop the

276
00:11:02,160 --> 00:11:04,640
recording

277
00:11:05,279 --> 00:11:07,760
no for this session we will go um in the

278
00:11:07,760 --> 00:11:09,040
room okay

279
00:11:09,040 --> 00:11:10,079
okay

280
00:11:10,079 --> 00:11:13,200
good fantastic so the next talk is about

281
00:11:13,200 --> 00:11:15,519
aby 2.0 which

282
00:11:15,519 --> 00:11:17,680
is i suspect the

283
00:11:17,680 --> 00:11:19,120
uh

284
00:11:19,120 --> 00:11:22,880
successor to aby three and this this

285
00:11:22,880 --> 00:11:25,440
talk is called a t 2.0 improved mixed

286
00:11:25,440 --> 00:11:27,040
protocol secure two-party computation

287
00:11:27,040 --> 00:11:28,480
with applications to

288
00:11:28,480 --> 00:11:30,720
privacy preserving machine learning

289
00:11:30,720 --> 00:11:32,800
and the talk will be given by hussein

290
00:11:32,800 --> 00:11:36,240
yalami you already

291
00:11:36,720 --> 00:11:38,560
started screen sharing so saying take it

292
00:11:38,560 --> 00:11:39,839
away

293
00:11:39,839 --> 00:11:41,680
hello do you hear me

294
00:11:41,680 --> 00:11:42,880
yes

295
00:11:42,880 --> 00:11:45,360
okay perfect hi everyone this is hussein

296
00:11:45,360 --> 00:11:47,839
i'm here to present our paper api to

297
00:11:47,839 --> 00:11:49,760
improve mix protocol secure two-part

298
00:11:49,760 --> 00:11:53,200
computation with application to ppml

299
00:11:53,200 --> 00:11:55,120
this is a joint work with arpita patra

300
00:11:55,120 --> 00:11:57,519
and ajit suresh from iisc bangal

301
00:11:57,519 --> 00:12:00,000
university and my advisor to master

302
00:12:00,000 --> 00:12:04,079
schneider for mtu darmstadt

303
00:12:04,800 --> 00:12:07,360
okay in the problem of secure to party

304
00:12:07,360 --> 00:12:09,760
computation i think all of us we know

305
00:12:09,760 --> 00:12:12,320
what is the two pc but as a reminder

306
00:12:12,320 --> 00:12:14,320
that as you know we have two parties

307
00:12:14,320 --> 00:12:16,560
yasmin and aladdin

308
00:12:16,560 --> 00:12:18,800
they have some diamonds

309
00:12:18,800 --> 00:12:21,120
and the goal is to calculate number of

310
00:12:21,120 --> 00:12:23,440
the diamonds with the same color without

311
00:12:23,440 --> 00:12:25,680
revealing any information about the

312
00:12:25,680 --> 00:12:27,200
diamonds

313
00:12:27,200 --> 00:12:29,680
and for that we have one magic

314
00:12:29,680 --> 00:12:32,160
the name of this magic is secure to

315
00:12:32,160 --> 00:12:34,959
party computation with secure to party

316
00:12:34,959 --> 00:12:38,560
computation two parties can calculate

317
00:12:38,560 --> 00:12:41,839
the function on private inputs without

318
00:12:41,839 --> 00:12:44,880
revealing any information

319
00:12:44,880 --> 00:12:48,079
instead of the artwork for two pc is aby

320
00:12:48,079 --> 00:12:51,040
framework in aby they consider three

321
00:12:51,040 --> 00:12:52,480
different protocols

322
00:12:52,480 --> 00:12:54,480
i mean arithmetic sharing boolean

323
00:12:54,480 --> 00:12:56,959
sharing and yellow garbage circuits they

324
00:12:56,959 --> 00:12:59,200
also have conversion between these

325
00:12:59,200 --> 00:13:02,160
protocols and for example in arithmetic

326
00:13:02,160 --> 00:13:04,240
shading they have chip multiplication in

327
00:13:04,240 --> 00:13:07,200
boolean shading they need

328
00:13:07,200 --> 00:13:09,200
interaction for each and gate and gaba

329
00:13:09,200 --> 00:13:10,800
circuits they have some communication

330
00:13:10,800 --> 00:13:13,040
overhead and using

331
00:13:13,040 --> 00:13:15,920
convergence they will try to use the

332
00:13:15,920 --> 00:13:18,639
protocols more efficiently

333
00:13:18,639 --> 00:13:22,079
in high level in a b y we have some high

334
00:13:22,079 --> 00:13:22,959
level

335
00:13:22,959 --> 00:13:24,160
functions

336
00:13:24,160 --> 00:13:26,000
and then each function should be

337
00:13:26,000 --> 00:13:28,240
represented as a boolean circuits or

338
00:13:28,240 --> 00:13:30,079
arithmetic circuits

339
00:13:30,079 --> 00:13:32,800
and then for each circuits we can use

340
00:13:32,800 --> 00:13:34,720
different protocols for example for

341
00:13:34,720 --> 00:13:36,959
arithmetic circuits we can use gmw

342
00:13:36,959 --> 00:13:39,279
arithmetic shading and for boolean

343
00:13:39,279 --> 00:13:40,480
circuits

344
00:13:40,480 --> 00:13:43,120
we can use the yao garbo circuits or gmw

345
00:13:43,120 --> 00:13:45,040
boolean shading

346
00:13:45,040 --> 00:13:47,600
in this form in aby2

347
00:13:47,600 --> 00:13:51,199
we designed new arithmetic shading new

348
00:13:51,199 --> 00:13:53,760
boolean shading and also we provide and

349
00:13:53,760 --> 00:13:54,959
designed

350
00:13:54,959 --> 00:13:57,839
new depths optimized boolean circuits

351
00:13:57,839 --> 00:14:00,480
for all new protocols

352
00:14:00,480 --> 00:14:03,839
talking about our contribution

353
00:14:03,839 --> 00:14:06,959
we have a new passively secure 2pc

354
00:14:06,959 --> 00:14:09,040
protocol with function dependent

355
00:14:09,040 --> 00:14:11,360
preprocessing which is not the case in

356
00:14:11,360 --> 00:14:15,120
aby in aby they use function independent

357
00:14:15,120 --> 00:14:16,639
preprocessing

358
00:14:16,639 --> 00:14:18,720
but the function dependent processing

359
00:14:18,720 --> 00:14:20,800
means we should know about the function

360
00:14:20,800 --> 00:14:23,279
we want to evaluate in advance and is

361
00:14:23,279 --> 00:14:26,000
the case for example for ppm application

362
00:14:26,000 --> 00:14:27,920
and we know the function

363
00:14:27,920 --> 00:14:29,279
in advance

364
00:14:29,279 --> 00:14:31,120
and also we improve the online

365
00:14:31,120 --> 00:14:33,040
communication compared to a state of the

366
00:14:33,040 --> 00:14:36,560
art work at least two times

367
00:14:36,560 --> 00:14:39,760
we support multi-input multiplication at

368
00:14:39,760 --> 00:14:41,360
the same time

369
00:14:41,360 --> 00:14:43,440
for this smart input multiplication we

370
00:14:43,440 --> 00:14:44,959
have online

371
00:14:44,959 --> 00:14:47,680
constant online communication and also

372
00:14:47,680 --> 00:14:49,279
considering this multi-input

373
00:14:49,279 --> 00:14:51,600
multiplication we designed efficient

374
00:14:51,600 --> 00:14:53,680
building blocks

375
00:14:53,680 --> 00:14:56,639
also we have more efficient mixed

376
00:14:56,639 --> 00:14:59,680
world conversions and we designed new

377
00:14:59,680 --> 00:15:01,760
protocols which is more efficient

378
00:15:01,760 --> 00:15:04,079
compared to aby conversions

379
00:15:04,079 --> 00:15:07,680
and especially for ppm we have a new

380
00:15:07,680 --> 00:15:10,560
scalar product with online complexity

381
00:15:10,560 --> 00:15:13,120
independent of the vector dimension

382
00:15:13,120 --> 00:15:15,120
which is the first time in the

383
00:15:15,120 --> 00:15:17,440
literature

384
00:15:17,440 --> 00:15:21,040
the major modification of aby2 is or new

385
00:15:21,040 --> 00:15:23,920
sharing scheme which the general idea is

386
00:15:23,920 --> 00:15:27,040
from tdpc astro

387
00:15:27,040 --> 00:15:30,560
let's assume we have a value v

388
00:15:30,560 --> 00:15:32,480
for this value we have two mass the

389
00:15:32,480 --> 00:15:35,920
first mask is delta v in blue and also

390
00:15:35,920 --> 00:15:39,040
the mass value delta v in redness light

391
00:15:39,040 --> 00:15:42,000
the delta v in red is equal to value

392
00:15:42,000 --> 00:15:43,040
plus

393
00:15:43,040 --> 00:15:45,040
delta in blue

394
00:15:45,040 --> 00:15:48,079
and then each party has these shapes

395
00:15:48,079 --> 00:15:51,519
delta v in red in public and also part

396
00:15:51,519 --> 00:15:52,800
of the

397
00:15:52,800 --> 00:15:54,480
blue value

398
00:15:54,480 --> 00:15:57,519
for each parties

399
00:15:57,600 --> 00:16:02,000
for aby2 addition is free and the

400
00:16:02,000 --> 00:16:04,079
parties can calculate the addition

401
00:16:04,079 --> 00:16:05,199
locally

402
00:16:05,199 --> 00:16:08,480
for that we have x and y and then for

403
00:16:08,480 --> 00:16:10,399
this example it can be written as a

404
00:16:10,399 --> 00:16:11,680
delta

405
00:16:11,680 --> 00:16:14,560
and then each party has access to the

406
00:16:14,560 --> 00:16:17,759
shade of the values of x and y and then

407
00:16:17,759 --> 00:16:20,560
they can simply adopt the shapes and

408
00:16:20,560 --> 00:16:23,680
then they have the shade of the z for

409
00:16:23,680 --> 00:16:26,000
addition

410
00:16:26,000 --> 00:16:28,480
and for multiplication again we have x

411
00:16:28,480 --> 00:16:31,040
and y we have the shade after

412
00:16:31,040 --> 00:16:33,440
and then for

413
00:16:33,440 --> 00:16:35,360
calculating shade of z

414
00:16:35,360 --> 00:16:38,639
the parties has access to the shade of x

415
00:16:38,639 --> 00:16:42,079
and the shade of z i mean delta z1 and

416
00:16:42,079 --> 00:16:43,839
delta z2 here

417
00:16:43,839 --> 00:16:45,040
and then

418
00:16:45,040 --> 00:16:47,759
similar to the traditional approach the

419
00:16:47,759 --> 00:16:50,560
parties can calculate the shade of mass

420
00:16:50,560 --> 00:16:52,959
x and max y

421
00:16:52,959 --> 00:16:54,639
and then the particles simply can

422
00:16:54,639 --> 00:16:57,199
calculate delta z1 and delta z2 and then

423
00:16:57,199 --> 00:16:59,600
they transcend the delta z1 and delta z2

424
00:16:59,600 --> 00:17:01,680
together and then they can calculate

425
00:17:01,680 --> 00:17:02,880
that

426
00:17:02,880 --> 00:17:04,400
as you can see here

427
00:17:04,400 --> 00:17:08,000
for each multiplication we need only two

428
00:17:08,000 --> 00:17:10,640
elements instead of four in traditional

429
00:17:10,640 --> 00:17:13,039
approach

430
00:17:13,359 --> 00:17:16,000
here compared to a b y for each

431
00:17:16,000 --> 00:17:18,959
multiplication or online communication

432
00:17:18,959 --> 00:17:20,400
is two elements

433
00:17:20,400 --> 00:17:24,959
which is four for aby and also as i told

434
00:17:24,959 --> 00:17:27,520
we are function dependent but a b y is

435
00:17:27,520 --> 00:17:28,960
function independent

436
00:17:28,960 --> 00:17:31,280
with the same uh period processing

437
00:17:31,280 --> 00:17:35,200
communication here we can say we are

438
00:17:35,200 --> 00:17:37,600
more efficient like two times uh

439
00:17:37,600 --> 00:17:40,160
considering online communication

440
00:17:40,160 --> 00:17:42,799
and also we extend uh

441
00:17:42,799 --> 00:17:44,960
two input multiplication to three and

442
00:17:44,960 --> 00:17:46,400
four inputs

443
00:17:46,400 --> 00:17:48,799
and we can say or online communication

444
00:17:48,799 --> 00:17:51,039
is independent of the funding of the

445
00:17:51,039 --> 00:17:53,440
gate for example for four input

446
00:17:53,440 --> 00:17:54,880
multiplication

447
00:17:54,880 --> 00:17:57,600
in a b y two we just need two elements

448
00:17:57,600 --> 00:18:02,240
what but it is 12 in the aby

449
00:18:02,240 --> 00:18:04,480
and that's also for the first time in

450
00:18:04,480 --> 00:18:07,120
the literature

451
00:18:07,360 --> 00:18:09,919
and also for each portal course for

452
00:18:09,919 --> 00:18:12,160
arithmetic sharing boolean sharing and

453
00:18:12,160 --> 00:18:15,120
yoga circuit we have the new conversion

454
00:18:15,120 --> 00:18:19,280
which is more efficient than aby

455
00:18:19,280 --> 00:18:23,679
and the key highlight of or aby2 is our

456
00:18:23,679 --> 00:18:26,000
new dot product

457
00:18:26,000 --> 00:18:28,559
for example for the dot product of a

458
00:18:28,559 --> 00:18:32,240
dimension of t the aby needs 4d online

459
00:18:32,240 --> 00:18:33,679
communication

460
00:18:33,679 --> 00:18:38,080
but we need only two elements in online

461
00:18:38,080 --> 00:18:40,960
we can say or dot product is independent

462
00:18:40,960 --> 00:18:44,240
of the vector dimension with the

463
00:18:44,240 --> 00:18:46,320
same pre-processing communication

464
00:18:46,320 --> 00:18:49,280
compared to aby

465
00:18:49,440 --> 00:18:51,600
talking about or benchmarking and

466
00:18:51,600 --> 00:18:54,880
application we implement aby2 using

467
00:18:54,880 --> 00:18:56,799
encrypted library

468
00:18:56,799 --> 00:19:00,160
and also the benchmark over lan and van

469
00:19:00,160 --> 00:19:03,360
with google gelato platform and

470
00:19:03,360 --> 00:19:07,600
here we can see the ppml results

471
00:19:07,600 --> 00:19:11,120
for linear regression inference

472
00:19:11,120 --> 00:19:13,600
or aby2

473
00:19:13,600 --> 00:19:16,240
or runtime in aby2 is like six times

474
00:19:16,240 --> 00:19:19,200
better than secular ml in land setting

475
00:19:19,200 --> 00:19:22,640
and two times in one setting also or two

476
00:19:22,640 --> 00:19:26,160
report is like 35s better than

477
00:19:26,160 --> 00:19:28,640
insecure ml in lan and 11 times better

478
00:19:28,640 --> 00:19:31,440
than in ban

479
00:19:32,160 --> 00:19:34,480
considering neural network inference or

480
00:19:34,480 --> 00:19:36,880
run times better like theory times in

481
00:19:36,880 --> 00:19:41,360
lan and like td times in van and also

482
00:19:41,360 --> 00:19:43,919
arturo put compared to secure ml is like

483
00:19:43,919 --> 00:19:45,679
700

484
00:19:45,679 --> 00:19:48,559
times better than security model and the

485
00:19:48,559 --> 00:19:50,960
main result the main reason of that is

486
00:19:50,960 --> 00:19:54,720
new dot product in aby2

487
00:19:54,720 --> 00:19:57,760
and considering neural network training

488
00:19:57,760 --> 00:19:59,360
we can see

489
00:19:59,360 --> 00:20:02,240
or throughput is like three times better

490
00:20:02,240 --> 00:20:04,720
than secure emerging lan and

491
00:20:04,720 --> 00:20:07,200
van setting and the main reason of that

492
00:20:07,200 --> 00:20:08,320
is for

493
00:20:08,320 --> 00:20:12,799
new softmax in aby2

494
00:20:12,799 --> 00:20:14,960
to conclude this touch we can say we

495
00:20:14,960 --> 00:20:19,120
have new 2pc protocol for passively

496
00:20:19,120 --> 00:20:22,400
securely evaluating the circuits

497
00:20:22,400 --> 00:20:23,440
and

498
00:20:23,440 --> 00:20:26,000
we provide better mixed protocol

499
00:20:26,000 --> 00:20:28,960
conversion regarding run and online

500
00:20:28,960 --> 00:20:32,080
communication compared to avivy

501
00:20:32,080 --> 00:20:34,240
and also we extend to input

502
00:20:34,240 --> 00:20:35,919
multiplication to any input

503
00:20:35,919 --> 00:20:39,760
multiplication but we have only two ring

504
00:20:39,760 --> 00:20:41,760
elements in online

505
00:20:41,760 --> 00:20:44,320
and that's the for the first time in the

506
00:20:44,320 --> 00:20:46,400
literature for any input and gates you

507
00:20:46,400 --> 00:20:49,120
just need two elements

508
00:20:49,120 --> 00:20:51,840
and also we provide efficient building

509
00:20:51,840 --> 00:20:55,120
blocks especially for ppml scalar

510
00:20:55,120 --> 00:20:56,080
product

511
00:20:56,080 --> 00:20:58,159
or scalar product

512
00:20:58,159 --> 00:21:00,080
the online communication is independent

513
00:21:00,080 --> 00:21:04,080
of the vector dimension and also we have

514
00:21:04,080 --> 00:21:05,200
efficient

515
00:21:05,200 --> 00:21:08,159
some new efficient other run efficient

516
00:21:08,159 --> 00:21:11,039
adder by combination of two three four

517
00:21:11,039 --> 00:21:12,640
input and gates

518
00:21:12,640 --> 00:21:15,679
and in a follow-up work of a b y two

519
00:21:15,679 --> 00:21:18,799
we designed another bilimbolux like

520
00:21:18,799 --> 00:21:21,039
multiplication division

521
00:21:21,039 --> 00:21:22,159
relu

522
00:21:22,159 --> 00:21:25,520
operation using hardware synthesis tools

523
00:21:25,520 --> 00:21:27,840
and also we have new matrix

524
00:21:27,840 --> 00:21:30,880
multiplication equality test comparison

525
00:21:30,880 --> 00:21:34,640
and b text selection especially for ppml

526
00:21:34,640 --> 00:21:36,799
application

527
00:21:36,799 --> 00:21:39,360
and thanks a lot for your listening and

528
00:21:39,360 --> 00:21:41,600
i'm here if you have a question

529
00:21:41,600 --> 00:21:43,840
uh thanks for a great talk hussain um

530
00:21:43,840 --> 00:21:47,200
while the next uh speaker divashire is

531
00:21:47,200 --> 00:21:48,320
setting up

532
00:21:48,320 --> 00:21:50,480
uh one quick question from

533
00:21:50,480 --> 00:21:51,520
divya

534
00:21:51,520 --> 00:21:53,039
who asked

535
00:21:53,039 --> 00:21:54,480
does

536
00:21:54,480 --> 00:21:57,440
is this comparison with aby and secureml

537
00:21:57,440 --> 00:21:58,720
for online complexity or total

538
00:21:58,720 --> 00:22:00,080
complexity

539
00:22:00,080 --> 00:22:03,039
it's for online companies

540
00:22:03,039 --> 00:22:06,080
okay so for for total complexity

541
00:22:06,080 --> 00:22:08,000
how would the performance

542
00:22:08,000 --> 00:22:10,880
improvement look there

543
00:22:10,880 --> 00:22:14,640
for example if we consider

544
00:22:14,880 --> 00:22:18,240
uh only gates for example here or for

545
00:22:18,240 --> 00:22:20,559
input multiplication for example or set

546
00:22:20,559 --> 00:22:23,520
up com communication is like three times

547
00:22:23,520 --> 00:22:25,440
more than aby

548
00:22:25,440 --> 00:22:27,360
but considering only online we are very

549
00:22:27,360 --> 00:22:29,440
better than the ap y and especially the

550
00:22:29,440 --> 00:22:31,360
security

551
00:22:31,360 --> 00:22:32,240
okay

552
00:22:32,240 --> 00:22:34,080
um

553
00:22:34,080 --> 00:22:34,960
there's

554
00:22:34,960 --> 00:22:36,799
more questions in the chat but maybe if

555
00:22:36,799 --> 00:22:38,559
you can unshare your screen so the next

556
00:22:38,559 --> 00:22:40,720
speaker can

557
00:22:40,720 --> 00:22:42,159
share it

558
00:22:42,159 --> 00:22:45,360
oh i can answer it

559
00:22:49,840 --> 00:22:51,039
okay

560
00:22:51,039 --> 00:22:55,000
that's what's now i think

561
00:22:56,880 --> 00:22:58,640
so

562
00:22:58,640 --> 00:23:01,600
the australia can you share a screen now

563
00:23:01,600 --> 00:23:03,918
yeah

564
00:23:04,799 --> 00:23:07,200
okay uh can you see my screen

565
00:23:07,200 --> 00:23:08,960
yes

566
00:23:08,960 --> 00:23:11,200
okay

567
00:23:11,440 --> 00:23:15,120
and the next talk will be about si rnn

568
00:23:15,120 --> 00:23:18,080
math library for secure rnn inference

569
00:23:18,080 --> 00:23:19,360
and

570
00:23:19,360 --> 00:23:21,679
by a long list of authors

571
00:23:21,679 --> 00:23:25,120
and uh theo is gonna give the talk

572
00:23:25,120 --> 00:23:27,600
okay hi everyone i'm diveshwar and in

573
00:23:27,600 --> 00:23:29,280
this presentation i'll be talking about

574
00:23:29,280 --> 00:23:31,120
siren which is a math library for

575
00:23:31,120 --> 00:23:33,360
securing an inference saturn was

576
00:23:33,360 --> 00:23:35,520
published at oakland 2021 and it is a

577
00:23:35,520 --> 00:23:36,720
joint work with my colleagues at

578
00:23:36,720 --> 00:23:40,960
microsoft research so let's get started

579
00:23:40,960 --> 00:23:42,960
so first of all what is secure inference

580
00:23:42,960 --> 00:23:44,640
uh so let's say there's a server with a

581
00:23:44,640 --> 00:23:46,159
machine learning model and a client with

582
00:23:46,159 --> 00:23:48,240
some input data and they want to perform

583
00:23:48,240 --> 00:23:50,559
inference that is they want to learn the

584
00:23:50,559 --> 00:23:53,200
model output on the client's data but

585
00:23:53,200 --> 00:23:54,559
this this should be straightforward

586
00:23:54,559 --> 00:23:56,559
however the model is proprietary and the

587
00:23:56,559 --> 00:23:58,720
input data is private so they cannot

588
00:23:58,720 --> 00:24:00,799
freely exchange the information

589
00:24:00,799 --> 00:24:02,320
this brings us to the problem of secure

590
00:24:02,320 --> 00:24:03,840
inference which seeks to answer whether

591
00:24:03,840 --> 00:24:05,919
these parties can form inference without

592
00:24:05,919 --> 00:24:09,039
revealing anything about their inputs

593
00:24:09,039 --> 00:24:10,880
this is actually possible

594
00:24:10,880 --> 00:24:12,960
this is a good scenario for secure to

595
00:24:12,960 --> 00:24:15,679
party computation or to pc which makes

596
00:24:15,679 --> 00:24:17,679
this kind of scenarios possible and

597
00:24:17,679 --> 00:24:19,919
there are already generic solutions for

598
00:24:19,919 --> 00:24:22,480
do pc but those solutions not lead to

599
00:24:22,480 --> 00:24:24,880
efficient solutions for secure inference

600
00:24:24,880 --> 00:24:26,720
which is why many specialized protocols

601
00:24:26,720 --> 00:24:30,159
have been proposed in the past decade

602
00:24:30,159 --> 00:24:32,400
so in this work we focus on secure

603
00:24:32,400 --> 00:24:33,840
intents of deep neural networks which

604
00:24:33,840 --> 00:24:36,480
have uh which basically have two types

605
00:24:36,480 --> 00:24:37,840
convolutional neural networks and

606
00:24:37,840 --> 00:24:39,520
recurrent neural networks

607
00:24:39,520 --> 00:24:41,919
so although uh there are many works

608
00:24:41,919 --> 00:24:44,159
securely evaluating realistic cnns and

609
00:24:44,159 --> 00:24:47,360
uh like quite large cnns have already

610
00:24:47,360 --> 00:24:49,760
been securely evaluated but ions have

611
00:24:49,760 --> 00:24:51,200
been mostly neglected by the secure

612
00:24:51,200 --> 00:24:52,720
inference community

613
00:24:52,720 --> 00:24:54,640
this is not great because rnns are the

614
00:24:54,640 --> 00:24:55,840
state of the art for analyzing

615
00:24:55,840 --> 00:24:57,760
sequential and time series data

616
00:24:57,760 --> 00:24:59,919
and they are they also find applications

617
00:24:59,919 --> 00:25:01,760
in some privacy sensitive domains like

618
00:25:01,760 --> 00:25:03,440
dna sequence analysis and speech

619
00:25:03,440 --> 00:25:05,440
recognition

620
00:25:05,440 --> 00:25:07,679
okay so it's clear that rnn inference is

621
00:25:07,679 --> 00:25:09,440
important secure iron inference is

622
00:25:09,440 --> 00:25:10,880
important but now let's see what are the

623
00:25:10,880 --> 00:25:13,120
challenges in achieving this goal

624
00:25:13,120 --> 00:25:14,720
so first ions use floating point

625
00:25:14,720 --> 00:25:16,720
arithmetic which using current

626
00:25:16,720 --> 00:25:18,000
techniques is not

627
00:25:18,000 --> 00:25:19,360
efficient

628
00:25:19,360 --> 00:25:21,279
luckily we have automated flow to fix

629
00:25:21,279 --> 00:25:23,279
converters to fix this problem which

630
00:25:23,279 --> 00:25:24,880
output a fixed point code which is much

631
00:25:24,880 --> 00:25:26,799
more tractable with two pc

632
00:25:26,799 --> 00:25:28,400
but running this fixed point code has

633
00:25:28,400 --> 00:25:30,480
its own challenges so the first one is

634
00:25:30,480 --> 00:25:32,400
that ions use math functions like

635
00:25:32,400 --> 00:25:34,000
exponentiation sigma tangent and

636
00:25:34,000 --> 00:25:36,320
reciprocal of square root

637
00:25:36,320 --> 00:25:39,360
and the existing support for these

638
00:25:39,360 --> 00:25:41,600
functions is inadequate that is it is

639
00:25:41,600 --> 00:25:44,640
either imprecise or inefficient

640
00:25:44,640 --> 00:25:46,400
the second challenge is that the output

641
00:25:46,400 --> 00:25:48,240
fixed point code uses a mix of different

642
00:25:48,240 --> 00:25:50,320
bit widths and prior works only support

643
00:25:50,320 --> 00:25:52,159
uniform bit width which pays the cost of

644
00:25:52,159 --> 00:25:54,159
largest bit width everywhere

645
00:25:54,159 --> 00:25:56,720
this is not great because uh the

646
00:25:56,720 --> 00:25:58,559
performance of two pc depends critically

647
00:25:58,559 --> 00:26:00,640
on bit width and degrees degrades at

648
00:26:00,640 --> 00:26:03,919
least linearly with increasing liquid

649
00:26:03,919 --> 00:26:05,600
so addressing uh both of these

650
00:26:05,600 --> 00:26:07,520
challenges we have created siren which

651
00:26:07,520 --> 00:26:09,760
is a math library for semi on a secure

652
00:26:09,760 --> 00:26:10,880
inference

653
00:26:10,880 --> 00:26:12,480
siren has support for mixed bit with

654
00:26:12,480 --> 00:26:14,320
arithmetic which is enabled by a new two

655
00:26:14,320 --> 00:26:16,480
pc building blocks

656
00:26:16,480 --> 00:26:19,120
siren also supports math functions uh

657
00:26:19,120 --> 00:26:21,600
through our new math implementations for

658
00:26:21,600 --> 00:26:23,039
sigmoid tanning

659
00:26:23,039 --> 00:26:24,159
reciprocal square root and

660
00:26:24,159 --> 00:26:25,919
exponentiation and our math

661
00:26:25,919 --> 00:26:27,440
implementations are very

662
00:26:27,440 --> 00:26:28,960
proven to be precise

663
00:26:28,960 --> 00:26:30,799
which is why siren suffers no loss in

664
00:26:30,799 --> 00:26:32,720
inference accuracy

665
00:26:32,720 --> 00:26:34,799
and the performance of siren is great it

666
00:26:34,799 --> 00:26:36,320
is two orders of magnitude faster than

667
00:26:36,320 --> 00:26:37,600
prior work

668
00:26:37,600 --> 00:26:39,279
and with siren we are the first ones to

669
00:26:39,279 --> 00:26:41,279
run rnn security and speech data and

670
00:26:41,279 --> 00:26:43,360
perform head detection and images

671
00:26:43,360 --> 00:26:45,520
in the rest of this talk i'll mainly uh

672
00:26:45,520 --> 00:26:48,240
i'll mainly focus on uh the new mixed

673
00:26:48,240 --> 00:26:49,840
bit with building blocks which have wide

674
00:26:49,840 --> 00:26:52,640
applicability then i'll briefly go over

675
00:26:52,640 --> 00:26:54,400
uh the design of our math new math

676
00:26:54,400 --> 00:26:56,159
implementation and finally i'll present

677
00:26:56,159 --> 00:26:58,559
the results on on our largest benchmark

678
00:26:58,559 --> 00:27:01,039
which performs head detection and images

679
00:27:01,039 --> 00:27:02,000
okay

680
00:27:02,000 --> 00:27:03,200
so our first

681
00:27:03,200 --> 00:27:06,080
new building block is extension which uh

682
00:27:06,080 --> 00:27:08,240
basically uh increases the bit width of

683
00:27:08,240 --> 00:27:11,120
the input by prepending it with zeros

684
00:27:11,120 --> 00:27:13,520
the second building block is truncation

685
00:27:13,520 --> 00:27:14,880
which uh

686
00:27:14,880 --> 00:27:17,279
which shifts the bits of the input to

687
00:27:17,279 --> 00:27:19,520
the right by the shift amount and uh we

688
00:27:19,520 --> 00:27:21,039
have two variants of it logical right

689
00:27:21,039 --> 00:27:23,360
shift and truncate reduce uh with the

690
00:27:23,360 --> 00:27:24,480
with the only difference being that

691
00:27:24,480 --> 00:27:26,159
logical right shift maintains the same a

692
00:27:26,159 --> 00:27:27,840
bit width as the input and truncate

693
00:27:27,840 --> 00:27:30,000
reduce reduces the bitrate by the shift

694
00:27:30,000 --> 00:27:31,440
amount

695
00:27:31,440 --> 00:27:32,480
the third building block is

696
00:27:32,480 --> 00:27:34,399
multiplication which can take inputs of

697
00:27:34,399 --> 00:27:37,039
unequal bit width so using this building

698
00:27:37,039 --> 00:27:38,640
block we can actually multiply a three

699
00:27:38,640 --> 00:27:40,159
bit value with a four bit value and

700
00:27:40,159 --> 00:27:43,520
output uh in a six bit value

701
00:27:43,520 --> 00:27:45,679
and the final building block is digit

702
00:27:45,679 --> 00:27:47,760
decomposition which

703
00:27:47,760 --> 00:27:49,360
takes the input and divides it into

704
00:27:49,360 --> 00:27:50,960
smaller bit chunks so using this

705
00:27:50,960 --> 00:27:52,880
building block we can take an eight bit

706
00:27:52,880 --> 00:27:54,559
value and divide it into four chunks of

707
00:27:54,559 --> 00:27:56,320
two bits each

708
00:27:56,320 --> 00:27:58,320
so the examples i have provided here are

709
00:27:58,320 --> 00:28:00,960
for unsigned operations we also have

710
00:28:00,960 --> 00:28:02,960
signed variants of these operations and

711
00:28:02,960 --> 00:28:04,480
in the paper we show how to reduce

712
00:28:04,480 --> 00:28:05,919
signed operations to unsign at no

713
00:28:05,919 --> 00:28:08,080
additional cost

714
00:28:08,080 --> 00:28:10,799
okay now let's see how our protocols for

715
00:28:10,799 --> 00:28:12,640
these building blocks perform against

716
00:28:12,640 --> 00:28:13,840
the

717
00:28:13,840 --> 00:28:15,600
existing baselines

718
00:28:15,600 --> 00:28:17,600
in terms of communication so for

719
00:28:17,600 --> 00:28:19,520
extension and digital composition we are

720
00:28:19,520 --> 00:28:21,279
around six times better

721
00:28:21,279 --> 00:28:23,200
for ride shift and multiplication we are

722
00:28:23,200 --> 00:28:25,440
up to two times better and for truncated

723
00:28:25,440 --> 00:28:27,039
use which is used much more often than

724
00:28:27,039 --> 00:28:28,799
right shift our improvements are much

725
00:28:28,799 --> 00:28:31,679
larger going up to 17.5 times

726
00:28:31,679 --> 00:28:33,120
i'm focusing on communication

727
00:28:33,120 --> 00:28:34,720
improvements here because communication

728
00:28:34,720 --> 00:28:36,480
dictates the performance in our

729
00:28:36,480 --> 00:28:38,399
protocols as well as the baselines as

730
00:28:38,399 --> 00:28:40,480
the computations are wide enough in uh

731
00:28:40,480 --> 00:28:43,120
in the ml models and the cost of rounds

732
00:28:43,120 --> 00:28:45,919
is thus easily amortized

733
00:28:45,919 --> 00:28:47,760
okay so on top of the improvements i

734
00:28:47,760 --> 00:28:49,120
just showed we have an additional

735
00:28:49,120 --> 00:28:51,440
optimization called msp to wrap

736
00:28:51,440 --> 00:28:53,440
which makes the building blocks much

737
00:28:53,440 --> 00:28:54,799
cheaper if the

738
00:28:54,799 --> 00:28:56,240
uh if the most significant bit of the

739
00:28:56,240 --> 00:28:58,720
input is already known and this is

740
00:28:58,720 --> 00:29:00,399
actually possible in the maps in our

741
00:29:00,399 --> 00:29:02,000
math implementation because we know the

742
00:29:02,000 --> 00:29:03,840
msb from domain knowledge

743
00:29:03,840 --> 00:29:06,480
so using this optimization extensions

744
00:29:06,480 --> 00:29:08,320
become essentially free right shifts

745
00:29:08,320 --> 00:29:10,399
become almost as cheap as truncate

746
00:29:10,399 --> 00:29:12,720
reduce and we get up to five times

747
00:29:12,720 --> 00:29:15,039
improvement for multiplication

748
00:29:15,039 --> 00:29:17,360
okay so with these new improved building

749
00:29:17,360 --> 00:29:19,039
blocks we design our new math

750
00:29:19,039 --> 00:29:20,720
implementations which are inspired by

751
00:29:20,720 --> 00:29:22,720
embedded systems in that they use lookup

752
00:29:22,720 --> 00:29:25,039
tables to avoid complex computations and

753
00:29:25,039 --> 00:29:26,480
they use low bitrate fixed point

754
00:29:26,480 --> 00:29:28,240
arithmetic which is great for 2pc

755
00:29:28,240 --> 00:29:30,720
because of its low bit width

756
00:29:30,720 --> 00:29:32,320
and we design new

757
00:29:32,320 --> 00:29:34,399
implementations for exponentiation sigma

758
00:29:34,399 --> 00:29:36,320
tangent reciprocal square root which

759
00:29:36,320 --> 00:29:38,000
have been carefully designed to ensure

760
00:29:38,000 --> 00:29:39,919
minimal bit widths and they also use

761
00:29:39,919 --> 00:29:42,000
mixed bit widths

762
00:29:42,000 --> 00:29:44,399
and as alluded to earlier we use domain

763
00:29:44,399 --> 00:29:46,240
knowledge in all our math implementation

764
00:29:46,240 --> 00:29:47,679
to determine msp of all the

765
00:29:47,679 --> 00:29:49,840
intermediates which is why the msp to

766
00:29:49,840 --> 00:29:51,200
tab optimization from before is

767
00:29:51,200 --> 00:29:53,360
applicable and we can use cheaper mixed

768
00:29:53,360 --> 00:29:55,120
bedford building blocks here in all the

769
00:29:55,120 --> 00:29:56,960
implementations

770
00:29:56,960 --> 00:29:58,480
our implementations are verified for

771
00:29:58,480 --> 00:29:59,600
correctness through exhaustive

772
00:29:59,600 --> 00:30:01,919
enumeration which is possible for small

773
00:30:01,919 --> 00:30:03,600
bedrooms like less than equal to 32

774
00:30:03,600 --> 00:30:05,200
which are used in ml

775
00:30:05,200 --> 00:30:07,600
and our functionalities uh like we

776
00:30:07,600 --> 00:30:09,120
proved that our functionalities have

777
00:30:09,120 --> 00:30:10,880
less than four alpha which means that

778
00:30:10,880 --> 00:30:12,799
their the output is contaminated in at

779
00:30:12,799 --> 00:30:15,039
most two bits

780
00:30:15,039 --> 00:30:16,799
putting all these components together we

781
00:30:16,799 --> 00:30:19,120
implemented a our math library siren

782
00:30:19,120 --> 00:30:21,520
which takes just 16 microseconds per

783
00:30:21,520 --> 00:30:23,279
segment operation

784
00:30:23,279 --> 00:30:25,360
in comparison prior work is either uh

785
00:30:25,360 --> 00:30:27,520
imprecise or inefficient for example

786
00:30:27,520 --> 00:30:29,600
minion used a 12 piece spline in all of

787
00:30:29,600 --> 00:30:31,840
their benchmarks and uh

788
00:30:31,840 --> 00:30:34,240
which has large errors if we were to

789
00:30:34,240 --> 00:30:36,240
take minion strategy and make it make

790
00:30:36,240 --> 00:30:38,000
its precision comparable to siren it

791
00:30:38,000 --> 00:30:40,159
would we would require 48 pieces which

792
00:30:40,159 --> 00:30:42,559
would make it 115 times slower

793
00:30:42,559 --> 00:30:44,559
the other two baselines of deep secure

794
00:30:44,559 --> 00:30:46,480
nmp speeds are also at least 80 times

795
00:30:46,480 --> 00:30:47,840
lower

796
00:30:47,840 --> 00:30:49,919
thus iron is precise as well as 80 to

797
00:30:49,919 --> 00:30:54,000
115 times faster than brad work

798
00:30:54,000 --> 00:30:55,360
okay so the largest benchmark we

799
00:30:55,360 --> 00:30:57,600
evaluate is the hedge benchmark uh which

800
00:30:57,600 --> 00:30:58,960
is this which is a state-of-the-art

801
00:30:58,960 --> 00:31:01,600
network combining cnns and rnns the job

802
00:31:01,600 --> 00:31:03,200
of this

803
00:31:03,200 --> 00:31:05,200
network is to identify human heads in an

804
00:31:05,200 --> 00:31:06,159
image

805
00:31:06,159 --> 00:31:08,559
and this huge network contains 136

806
00:31:08,559 --> 00:31:11,600
sigmoid and tannish layers each us

807
00:31:11,600 --> 00:31:14,320
and uh a total of three million calls of

808
00:31:14,320 --> 00:31:16,000
sigma of

809
00:31:16,000 --> 00:31:17,360
three million calls to both sigmoid and

810
00:31:17,360 --> 00:31:18,559
damage

811
00:31:18,559 --> 00:31:20,399
and like this number is actually three

812
00:31:20,399 --> 00:31:22,480
orders of magnitude larger than uh the

813
00:31:22,480 --> 00:31:25,120
number of sigmoid and tan edge instances

814
00:31:25,120 --> 00:31:27,760
uh considered by any of the prior works

815
00:31:27,760 --> 00:31:29,440
and uh this network also has l2

816
00:31:29,440 --> 00:31:31,519
normalized the l2 normalization layers

817
00:31:31,519 --> 00:31:35,519
which use reciprocal of square root

818
00:31:35,600 --> 00:31:37,919
and uh so in evaluating this network

819
00:31:37,919 --> 00:31:39,919
half of the time is spent in performing

820
00:31:39,919 --> 00:31:42,000
sigmoid and tannage which despite our

821
00:31:42,000 --> 00:31:44,000
large improvement so this shows that how

822
00:31:44,000 --> 00:31:45,919
important it was to make these

823
00:31:45,919 --> 00:31:48,640
operations efficient

824
00:31:48,640 --> 00:31:50,880
and to yeah to finally finally to

825
00:31:50,880 --> 00:31:52,960
evaluate this network using siren

826
00:31:52,960 --> 00:31:55,360
we just take seven minutes and 85.5 gb

827
00:31:55,360 --> 00:31:57,039
of communication and these are total

828
00:31:57,039 --> 00:32:00,000
like total communication in total time

829
00:32:00,000 --> 00:32:01,760
in conclusion we've created a new math

830
00:32:01,760 --> 00:32:03,600
library for secure inference which is

831
00:32:03,600 --> 00:32:04,960
two orders of magnitude faster than

832
00:32:04,960 --> 00:32:06,320
prior work

833
00:32:06,320 --> 00:32:08,640
uh our math library siren has support

834
00:32:08,640 --> 00:32:11,440
for mixed with arithmetic and uh

835
00:32:11,440 --> 00:32:13,360
it also has efficient and proven precise

836
00:32:13,360 --> 00:32:14,960
math implementations

837
00:32:14,960 --> 00:32:17,200
our code and paper are public and i

838
00:32:17,200 --> 00:32:18,880
highly recommend that you check them out

839
00:32:18,880 --> 00:32:20,000
and that's it thank you for your

840
00:32:20,000 --> 00:32:22,399
attention

841
00:32:23,360 --> 00:32:24,799
thank you for the great talk to your

842
00:32:24,799 --> 00:32:26,840
swear

843
00:32:26,840 --> 00:32:30,399
um uh yeah i think maybe if somebody has

844
00:32:30,399 --> 00:32:31,600
a question

845
00:32:31,600 --> 00:32:35,439
please uh go ahead with it

846
00:32:38,240 --> 00:32:40,960
otherwise uh thank you again

847
00:32:40,960 --> 00:32:45,519
as let's move on to the next uh speaker

848
00:32:45,519 --> 00:32:47,279
i just one question what is the round

849
00:32:47,279 --> 00:32:48,880
complexity of the

850
00:32:48,880 --> 00:32:51,679
math functions

851
00:32:51,919 --> 00:32:53,840
the round complexity of the math

852
00:32:53,840 --> 00:32:56,240
functions would be so it's comparable to

853
00:32:56,240 --> 00:32:59,279
uh so it's it's definitely more than uh

854
00:32:59,279 --> 00:33:01,039
let's say like minion which uses a

855
00:33:01,039 --> 00:33:03,279
purely garbage circuit based approach so

856
00:33:03,279 --> 00:33:05,679
that would be a two rounds but uh the

857
00:33:05,679 --> 00:33:07,519
other comparison was it m was the mp

858
00:33:07,519 --> 00:33:09,120
speed so i would say it's comparable to

859
00:33:09,120 --> 00:33:10,960
that so it's comparable to

860
00:33:10,960 --> 00:33:12,720
all the iterative approaches where you

861
00:33:12,720 --> 00:33:14,720
take an initial approximation and then

862
00:33:14,720 --> 00:33:16,000
you iterate over it using

863
00:33:16,000 --> 00:33:17,919
multiplications

864
00:33:17,919 --> 00:33:20,880
so it would be yeah so it's comparable

865
00:33:20,880 --> 00:33:24,360
with all those approaches

866
00:33:24,559 --> 00:33:27,200
okay um i hope that answers your

867
00:33:27,200 --> 00:33:30,320
question amit um

868
00:33:30,799 --> 00:33:34,640
the next speaker would be sherman ciao

869
00:33:34,640 --> 00:33:36,840
sherman if you can share your

870
00:33:36,840 --> 00:33:38,799
screen

871
00:33:38,799 --> 00:33:40,720
and sherman is

872
00:33:40,720 --> 00:33:42,960
hello uh sherman is going to talk about

873
00:33:42,960 --> 00:33:45,200
differential privacy for text

874
00:33:45,200 --> 00:33:47,600
analytics via natural text

875
00:33:47,600 --> 00:33:49,679
sanitization

876
00:33:49,679 --> 00:33:53,200
um so german take it away

877
00:33:53,200 --> 00:33:56,480
thank you um so hi everyone i'm seven

878
00:33:56,480 --> 00:33:58,640
chao from cohk

879
00:33:58,640 --> 00:33:59,440
um

880
00:33:59,440 --> 00:34:02,320
okay so let's just guess why we started

881
00:34:02,320 --> 00:34:03,120
so

882
00:34:03,120 --> 00:34:06,799
we see a recent great progress in

883
00:34:06,799 --> 00:34:09,440
general purpose language model or lm

884
00:34:09,440 --> 00:34:13,839
such as gpt or burst vlt from google

885
00:34:13,839 --> 00:34:15,599
meanwhile as you can see on the screen

886
00:34:15,599 --> 00:34:18,079
already we have found some privacy

887
00:34:18,079 --> 00:34:20,399
concerns so in particular let me

888
00:34:20,399 --> 00:34:22,719
highlight one attack which

889
00:34:22,719 --> 00:34:24,879
basically the same old stuff given the

890
00:34:24,879 --> 00:34:27,520
language model you can extract the uh

891
00:34:27,520 --> 00:34:28,960
sanitize the

892
00:34:28,960 --> 00:34:32,719
data i'm sorry at once to this one

893
00:34:32,719 --> 00:34:36,239
and okay so uh so today's focus will be

894
00:34:36,239 --> 00:34:39,040
we want to ensure test privacy and our

895
00:34:39,040 --> 00:34:41,839
approach will be test sanitization and

896
00:34:41,839 --> 00:34:45,359
we try to say we produce sanitized test

897
00:34:45,359 --> 00:34:48,480
to piece privacy from the wood which we

898
00:34:48,480 --> 00:34:51,440
will make myself clear in

899
00:34:51,440 --> 00:34:52,560
this slide

900
00:34:52,560 --> 00:34:55,119
we consider this as

901
00:34:55,119 --> 00:34:55,839
two

902
00:34:55,839 --> 00:34:57,839
from two aspects and one is about test

903
00:34:57,839 --> 00:35:00,480
representation and for this population i

904
00:35:00,480 --> 00:35:02,000
guess i don't need to motivate too much

905
00:35:02,000 --> 00:35:04,400
about the need of a special approach

906
00:35:04,400 --> 00:35:07,280
because for example the

907
00:35:07,280 --> 00:35:09,280
simple approach of removing pii won't

908
00:35:09,280 --> 00:35:11,839
work due to a sophisticated

909
00:35:11,839 --> 00:35:14,000
identification attack

910
00:35:14,000 --> 00:35:15,040
okay

911
00:35:15,040 --> 00:35:18,880
and now so let me quickly talk about

912
00:35:18,880 --> 00:35:21,599
some prior work and actually as a matter

913
00:35:21,599 --> 00:35:23,520
of fact we don't have too many related

914
00:35:23,520 --> 00:35:25,920
work from the literature uh we try to

915
00:35:25,920 --> 00:35:28,400
broadly classify them into two main

916
00:35:28,400 --> 00:35:31,119
categories so the first one that also

917
00:35:31,119 --> 00:35:32,720
addressed the question i left in last

918
00:35:32,720 --> 00:35:35,280
slide is one existing approach will be

919
00:35:35,280 --> 00:35:36,720
to produce

920
00:35:36,720 --> 00:35:38,560
sanitized tests

921
00:35:38,560 --> 00:35:41,119
while noisy token and batten so what do

922
00:35:41,119 --> 00:35:43,520
i mean by that so let's say we have a

923
00:35:43,520 --> 00:35:46,400
token or vocabulary called like bank

924
00:35:46,400 --> 00:35:47,200
it

925
00:35:47,200 --> 00:35:48,720
the referral first

926
00:35:48,720 --> 00:35:50,400
get is

927
00:35:50,400 --> 00:35:53,040
embedded in writer from some embedded

928
00:35:53,040 --> 00:35:54,640
model such as graph

929
00:35:54,640 --> 00:35:56,320
then we have def

930
00:35:56,320 --> 00:35:59,200
writer we add not passing noise over it

931
00:35:59,200 --> 00:36:01,520
we got a lossy embedding and that is the

932
00:36:01,520 --> 00:36:03,359
last stage which we find a bit

933
00:36:03,359 --> 00:36:04,560
problematic

934
00:36:04,560 --> 00:36:06,960
given the noise and button the approach

935
00:36:06,960 --> 00:36:08,720
will be trying to do a nearest labor

936
00:36:08,720 --> 00:36:11,200
structure over the entire vocabulary to

937
00:36:11,200 --> 00:36:12,480
map it back

938
00:36:12,480 --> 00:36:15,200
to the vocabulary and in this case maybe

939
00:36:15,200 --> 00:36:16,400
embed to

940
00:36:16,400 --> 00:36:19,440
autoteller machine so in this process we

941
00:36:19,440 --> 00:36:21,040
find that there is two large slices

942
00:36:21,040 --> 00:36:23,520
involved which lead to the curse of high

943
00:36:23,520 --> 00:36:26,079
dimensionality and because of the nn

944
00:36:26,079 --> 00:36:28,640
search it's also not that efficient

945
00:36:28,640 --> 00:36:30,320
there are also specialized approaches

946
00:36:30,320 --> 00:36:33,760
for example the one from ir 2018 which

947
00:36:33,760 --> 00:36:35,920
specifically focus on time frequency

948
00:36:35,920 --> 00:36:38,240
writer but yes the problem is it just

949
00:36:38,240 --> 00:36:40,720
takes care of one kind of test analytics

950
00:36:40,720 --> 00:36:42,880
in stuff like general test

951
00:36:42,880 --> 00:36:45,520
so that is one aspect why we say we try

952
00:36:45,520 --> 00:36:48,640
to adjust the problem from the woods

953
00:36:48,640 --> 00:36:51,839
and so here is our problem statement so

954
00:36:51,839 --> 00:36:53,200
as you might expect we are trying to

955
00:36:53,200 --> 00:36:54,720
propose a differentially private

956
00:36:54,720 --> 00:36:56,800
mechanism that is a token level

957
00:36:56,800 --> 00:36:58,000
mechanism

958
00:36:58,000 --> 00:37:00,079
which try to preserve the symmetric

959
00:37:00,079 --> 00:37:01,680
meaning of the token

960
00:37:01,680 --> 00:37:04,320
and we just don't stop there

961
00:37:04,320 --> 00:37:07,040
the next step of our approach is

962
00:37:07,040 --> 00:37:08,960
given we have this kind of dp

963
00:37:08,960 --> 00:37:10,640
standardization mechanism can we do more

964
00:37:10,640 --> 00:37:13,599
about that can we address the

965
00:37:13,599 --> 00:37:15,760
whole pipeline of the test analytics so

966
00:37:15,760 --> 00:37:17,440
that is what i meant by adjust the

967
00:37:17,440 --> 00:37:18,800
problem from the watch from the second

968
00:37:18,800 --> 00:37:20,800
perspective so we try to use our

969
00:37:20,800 --> 00:37:24,880
mechanism to also benefit or adapt the

970
00:37:24,880 --> 00:37:26,800
existing popular pre-training file

971
00:37:26,800 --> 00:37:28,880
training paradigm such as birth on

972
00:37:28,880 --> 00:37:31,680
standardized set tests for downstream

973
00:37:31,680 --> 00:37:33,359
tasks which i will be myself here in a

974
00:37:33,359 --> 00:37:35,680
few slides

975
00:37:35,680 --> 00:37:37,280
so as the

976
00:37:37,280 --> 00:37:39,119
first contributor talk and maybe the

977
00:37:39,119 --> 00:37:41,440
only one which focus on local dp so i

978
00:37:41,440 --> 00:37:43,440
found only to have this slice so to

979
00:37:43,440 --> 00:37:46,079
quickly recap there is a

980
00:37:46,079 --> 00:37:48,880
mechanism that has been deployed in many

981
00:37:48,880 --> 00:37:50,880
many settings

982
00:37:50,880 --> 00:37:53,520
local aspect means that each of the user

983
00:37:53,520 --> 00:37:55,520
or the data contributor assemblies to

984
00:37:55,520 --> 00:37:57,599
their data send their loyalty data to

985
00:37:57,599 --> 00:38:00,079
the analyst and they do some analytics

986
00:38:00,079 --> 00:38:02,480
tasks over the loyalty data set

987
00:38:02,480 --> 00:38:04,480
contributed by a different one so that

988
00:38:04,480 --> 00:38:07,119
is a local dp mechanism in the shell and

989
00:38:07,119 --> 00:38:09,760
the privacy guarantee is uh

990
00:38:09,760 --> 00:38:12,079
captured by this inequality and we have

991
00:38:12,079 --> 00:38:13,040
this

992
00:38:13,040 --> 00:38:15,680
epsilon as the parameter tunable

993
00:38:15,680 --> 00:38:17,520
parameter for the privacy budget so

994
00:38:17,520 --> 00:38:18,720
there is like a trade-off between

995
00:38:18,720 --> 00:38:20,560
privacy and utility

996
00:38:20,560 --> 00:38:24,240
uh we will use two ldp variants

997
00:38:24,240 --> 00:38:26,480
i'm sorry about it

998
00:38:26,480 --> 00:38:28,480
so one assistant variant which was

999
00:38:28,480 --> 00:38:32,560
proposed in csf is matches ldp so as you

1000
00:38:32,560 --> 00:38:34,240
can see on the screen instead of

1001
00:38:34,240 --> 00:38:37,119
considering a uniform privacy budget it

1002
00:38:37,119 --> 00:38:38,720
sort of adapts to the

1003
00:38:38,720 --> 00:38:41,599
distance according to certain batches of

1004
00:38:41,599 --> 00:38:42,480
two

1005
00:38:42,480 --> 00:38:45,359
items to be

1006
00:38:45,359 --> 00:38:48,160
to be centered to be sanitized so uh if

1007
00:38:48,160 --> 00:38:49,839
the two items are filed and that

1008
00:38:49,839 --> 00:38:51,280
naturally uh

1009
00:38:51,280 --> 00:38:53,520
there can not be too many privacy that's

1010
00:38:53,520 --> 00:38:55,520
the intuition that's the first variant

1011
00:38:55,520 --> 00:38:57,599
we consider and the second one is

1012
00:38:57,599 --> 00:39:00,320
so-called utility automatic ldp which is

1013
00:39:00,320 --> 00:39:01,760
recently proposed in

1014
00:39:01,760 --> 00:39:04,079
user security two years ago

1015
00:39:04,079 --> 00:39:05,680
in a nutshell

1016
00:39:05,680 --> 00:39:08,720
we try to allocate our budget wisely so

1017
00:39:08,720 --> 00:39:10,800
there can be some token which is

1018
00:39:10,800 --> 00:39:12,160
non-sensitive

1019
00:39:12,160 --> 00:39:14,560
and for those long sensitive tokens

1020
00:39:14,560 --> 00:39:15,760
uh we

1021
00:39:15,760 --> 00:39:18,560
may apply the ldp mechanism and we may

1022
00:39:18,560 --> 00:39:20,880
not and obviously if

1023
00:39:20,880 --> 00:39:23,920
we try a lot to apply ldp for off

1024
00:39:23,920 --> 00:39:25,359
non-sensitive token there will be a

1025
00:39:25,359 --> 00:39:27,119
problem so that's also why in this case

1026
00:39:27,119 --> 00:39:29,599
reflect a con so in this case

1027
00:39:29,599 --> 00:39:31,920
yes expansion the privacy budget can be

1028
00:39:31,920 --> 00:39:33,680
spent wisely to protect the really

1029
00:39:33,680 --> 00:39:36,400
privacy sensitive data

1030
00:39:36,400 --> 00:39:39,520
so in this work

1031
00:39:39,839 --> 00:39:41,520
sorry my keyboard must have some problem

1032
00:39:41,520 --> 00:39:42,480
um

1033
00:39:42,480 --> 00:39:44,720
so in this work we

1034
00:39:44,720 --> 00:39:47,680
propose two mechanism the first baseline

1035
00:39:47,680 --> 00:39:50,240
send test just use matches ldp

1036
00:39:50,240 --> 00:39:52,880
and the mechanism is we try to sample a

1037
00:39:52,880 --> 00:39:54,880
token y

1038
00:39:54,880 --> 00:39:56,720
with probability inversely proportional

1039
00:39:56,720 --> 00:40:00,320
to the distance over the embedded uh and

1040
00:40:00,320 --> 00:40:02,560
better encoding of the vocabulary so in

1041
00:40:02,560 --> 00:40:04,800
this case we capture also the distant

1042
00:40:04,800 --> 00:40:08,160
matrix so there is a toy sample so bank

1043
00:40:08,160 --> 00:40:10,880
may be sort of similar to atm but it may

1044
00:40:10,880 --> 00:40:12,800
be too different from other vocabulary

1045
00:40:12,800 --> 00:40:15,599
say chair so that is how we sample

1046
00:40:15,599 --> 00:40:16,800
the

1047
00:40:16,800 --> 00:40:19,040
token and we sort of sample it directly

1048
00:40:19,040 --> 00:40:20,400
so it circumvent the curse of

1049
00:40:20,400 --> 00:40:23,040
dimensionality in state of the art

1050
00:40:23,040 --> 00:40:24,960
and the last one is sentence plus our

1051
00:40:24,960 --> 00:40:27,359
main mechanism which also incorporate

1052
00:40:27,359 --> 00:40:31,040
the utterly atomized ldp lotion

1053
00:40:31,040 --> 00:40:33,599
so we divide the vocabulary space into

1054
00:40:33,599 --> 00:40:36,240
sensitive and non-sensitive part and in

1055
00:40:36,240 --> 00:40:38,319
particular for our experiment we just

1056
00:40:38,319 --> 00:40:41,200
use a simple approach of taking popular

1057
00:40:41,200 --> 00:40:42,319
work

1058
00:40:42,319 --> 00:40:44,560
in a certain vocabulary that we consider

1059
00:40:44,560 --> 00:40:47,599
it to be a long sensitive such as like a

1060
00:40:47,599 --> 00:40:49,359
ender or other

1061
00:40:49,359 --> 00:40:51,680
very common vocabulary so of course we

1062
00:40:51,680 --> 00:40:53,599
can use more sophisticated mechanism but

1063
00:40:53,599 --> 00:40:55,680
in our experiment this suffice so again

1064
00:40:55,680 --> 00:40:57,440
as a toy example maybe we are

1065
00:40:57,440 --> 00:41:00,079
considering financial statements so hand

1066
00:41:00,079 --> 00:41:02,720
the lamb and our minds will be sensitive

1067
00:41:02,720 --> 00:41:05,599
but withdrawal deposit is like a regular

1068
00:41:05,599 --> 00:41:07,839
operation so we don't consider to be

1069
00:41:07,839 --> 00:41:10,480
sensitive and then we

1070
00:41:10,480 --> 00:41:12,640
add the lights accordingly

1071
00:41:12,640 --> 00:41:15,280
okay so i let's quickly overview our

1072
00:41:15,280 --> 00:41:17,200
mechanism another second part of the

1073
00:41:17,200 --> 00:41:18,000
work

1074
00:41:18,000 --> 00:41:22,640
so now we have a mechanism so we try to

1075
00:41:22,640 --> 00:41:23,760
make the

1076
00:41:23,760 --> 00:41:25,760
uh whole process of the whole pipeline

1077
00:41:25,760 --> 00:41:28,000
more sanitization aware so what do i

1078
00:41:28,000 --> 00:41:30,400
mean by that so firstly uh that is from

1079
00:41:30,400 --> 00:41:32,079
the language model we have p training

1080
00:41:32,079 --> 00:41:35,440
and phyton and we also our pi centers

1081
00:41:35,440 --> 00:41:38,240
plus over the wall corpus to get a

1082
00:41:38,240 --> 00:41:40,160
standardized public purpose and

1083
00:41:40,160 --> 00:41:42,079
obviously the reason for here is not for

1084
00:41:42,079 --> 00:41:44,000
privacy because the corpus is out there

1085
00:41:44,000 --> 00:41:47,040
anyways the reason is we sort of want to

1086
00:41:47,040 --> 00:41:49,760
get a language model which is like get

1087
00:41:49,760 --> 00:41:52,160
used to our sanitizers test

1088
00:41:52,160 --> 00:41:53,359
so

1089
00:41:53,359 --> 00:41:55,359
in a little bit more detail in the

1090
00:41:55,359 --> 00:41:58,160
language model they work by masking a

1091
00:41:58,160 --> 00:41:59,440
certain

1092
00:41:59,440 --> 00:42:01,920
token and try to use the rest of the

1093
00:42:01,920 --> 00:42:04,000
document we should obviously give you

1094
00:42:04,000 --> 00:42:06,480
the schematic context to predict uh the

1095
00:42:06,480 --> 00:42:09,040
audio token so that is a rough overview

1096
00:42:09,040 --> 00:42:11,359
and for five four five for the last

1097
00:42:11,359 --> 00:42:13,119
stage of five twenty we have two

1098
00:42:13,119 --> 00:42:15,599
approach so you can either just fit in

1099
00:42:15,599 --> 00:42:17,520
the synthetized test to the regular

1100
00:42:17,520 --> 00:42:19,760
model or our standardization aware

1101
00:42:19,760 --> 00:42:21,520
language model

1102
00:42:21,520 --> 00:42:23,920
okay and for the interest of time i just

1103
00:42:23,920 --> 00:42:26,240
highlight two experimental results

1104
00:42:26,240 --> 00:42:29,040
so we try to use the stand

1105
00:42:29,040 --> 00:42:31,839
some standard uh data benchmark

1106
00:42:31,839 --> 00:42:34,720
benchmarking data from the nlp community

1107
00:42:34,720 --> 00:42:35,520
and

1108
00:42:35,520 --> 00:42:37,040
then you can basically see the

1109
00:42:37,040 --> 00:42:40,640
conclusion here is if we try to use

1110
00:42:40,640 --> 00:42:42,720
accuracy as the utility

1111
00:42:42,720 --> 00:42:45,119
then yes we just have a

1112
00:42:45,119 --> 00:42:48,720
reasonably good enough uh accuracy and a

1113
00:42:48,720 --> 00:42:51,680
great improvement over the past art

1114
00:42:51,680 --> 00:42:54,480
so that's one aspect and the last one is

1115
00:42:54,480 --> 00:42:56,079
about

1116
00:42:56,079 --> 00:42:59,520
our major concern privacy so we try to

1117
00:42:59,520 --> 00:43:01,280
use the language model petition

1118
00:43:01,280 --> 00:43:03,520
mechanism to sort of attack our own

1119
00:43:03,520 --> 00:43:04,480
model

1120
00:43:04,480 --> 00:43:06,800
um that is a very rough description of

1121
00:43:06,800 --> 00:43:09,280
the attack and as you might expect so

1122
00:43:09,280 --> 00:43:12,480
the idea or the resource here is that

1123
00:43:12,480 --> 00:43:15,520
somehow we can improve the defense rate

1124
00:43:15,520 --> 00:43:17,040
according to the definition you can see

1125
00:43:17,040 --> 00:43:20,000
on the screen with only a small accuracy

1126
00:43:20,000 --> 00:43:20,880
loss

1127
00:43:20,880 --> 00:43:22,560
so to me that is like the interesting

1128
00:43:22,560 --> 00:43:25,359
part because it's sort of sp some crypto

1129
00:43:25,359 --> 00:43:26,880
behavior

1130
00:43:26,880 --> 00:43:28,160
it is

1131
00:43:28,160 --> 00:43:30,560
preserving the simulated uh meaning for

1132
00:43:30,560 --> 00:43:32,480
the good guys but it does not really

1133
00:43:32,480 --> 00:43:35,440
boost up the attacker's uh successful

1134
00:43:35,440 --> 00:43:36,240
weight

1135
00:43:36,240 --> 00:43:38,640
so uh we also did more experiment in the

1136
00:43:38,640 --> 00:43:41,119
paper so feel free to trail and

1137
00:43:41,119 --> 00:43:42,240
then

1138
00:43:42,240 --> 00:43:44,560
for here i spent 10 seconds to quickly

1139
00:43:44,560 --> 00:43:47,200
advertise some of the other work from

1140
00:43:47,200 --> 00:43:48,240
our group

1141
00:43:48,240 --> 00:43:52,800
okay so to summarize so uh that is our

1142
00:43:52,800 --> 00:43:55,520
dp sanitization approach and thanks to

1143
00:43:55,520 --> 00:43:58,079
the first to offer and they did some

1144
00:43:58,079 --> 00:43:59,760
great work information the problem and

1145
00:43:59,760 --> 00:44:02,880
also with the arable so so here i also

1146
00:44:02,880 --> 00:44:05,920
quickly share or highlight two

1147
00:44:05,920 --> 00:44:08,000
questions so with that uh thank you for

1148
00:44:08,000 --> 00:44:10,240
your attention i'm ready to take your

1149
00:44:10,240 --> 00:44:12,319
questions or comment

1150
00:44:12,319 --> 00:44:14,000
uh thank you for the

1151
00:44:14,000 --> 00:44:15,200
talk sherman

1152
00:44:15,200 --> 00:44:16,079
um

1153
00:44:16,079 --> 00:44:17,760
i think in the interest of time if if

1154
00:44:17,760 --> 00:44:20,319
there are questions could you please uh

1155
00:44:20,319 --> 00:44:22,640
you know answer them in the chat that

1156
00:44:22,640 --> 00:44:25,520
would be helpful or awesome

1157
00:44:25,520 --> 00:44:26,560
um

1158
00:44:26,560 --> 00:44:29,280
for the last speaker of the session

1159
00:44:29,280 --> 00:44:32,960
um that is uh

1160
00:44:32,960 --> 00:44:34,319
could you

1161
00:44:34,319 --> 00:44:36,560
please share your

1162
00:44:36,560 --> 00:44:39,040
slides

1163
00:44:40,960 --> 00:44:44,359
just a second

1164
00:45:16,720 --> 00:45:19,720
teaching

1165
00:45:36,800 --> 00:45:39,359
i'll try again

1166
00:45:39,359 --> 00:45:42,359
yes

1167
00:45:56,160 --> 00:45:57,440
okay

1168
00:45:57,440 --> 00:46:00,760
just a second

1169
00:46:12,560 --> 00:46:15,200
can you it now or i need to

1170
00:46:15,200 --> 00:46:17,280
close and open again still don't see

1171
00:46:17,280 --> 00:46:20,280
anything

1172
00:46:29,599 --> 00:46:30,960
okay

1173
00:46:30,960 --> 00:46:32,560
now the line's going to work you see it

1174
00:46:32,560 --> 00:46:33,599
now

1175
00:46:33,599 --> 00:46:37,040
uh okay now we have it so i think sorry

1176
00:46:37,040 --> 00:46:38,400
but moran is going to talk about

1177
00:46:38,400 --> 00:46:42,240
fighting kovit 19 in the dark

1178
00:46:42,240 --> 00:46:43,359
so

1179
00:46:43,359 --> 00:46:44,240
yeah

1180
00:46:44,240 --> 00:46:45,359
please

1181
00:46:45,359 --> 00:46:48,960
go ahead with the last talk so hello

1182
00:46:48,960 --> 00:46:50,480
my name is

1183
00:46:50,480 --> 00:46:52,400
from berlin university and from iber

1184
00:46:52,400 --> 00:46:54,400
research lab this is a joint work with

1185
00:46:54,400 --> 00:46:56,240
lavender and guy mushkovitz and today

1186
00:46:56,240 --> 00:46:58,240
i'm going to talk about finding covenant

1187
00:46:58,240 --> 00:47:00,319
in the dark methodology for improving

1188
00:47:00,319 --> 00:47:02,000
the inference of a morphiclean encrypted

1189
00:47:02,000 --> 00:47:04,880
people networks

1190
00:47:05,920 --> 00:47:08,079
so our motivation is to run technical

1191
00:47:08,079 --> 00:47:12,079
networks in accuracy cloud environments

1192
00:47:12,720 --> 00:47:14,880
in a non-interactive manner it's crucial

1193
00:47:14,880 --> 00:47:16,720
for many domains and

1194
00:47:16,720 --> 00:47:18,800
we will specifically talk about

1195
00:47:18,800 --> 00:47:20,559
covenanting datasets

1196
00:47:20,559 --> 00:47:23,680
so here we have two datasets for

1197
00:47:23,680 --> 00:47:26,000
one for chest city and the second for

1198
00:47:26,000 --> 00:47:28,079
just x-ray and we

1199
00:47:28,079 --> 00:47:30,319
our goal is to

1200
00:47:30,319 --> 00:47:32,559
classify these images to one of three

1201
00:47:32,559 --> 00:47:36,880
cases normal pneumonia or kobe 19.

1202
00:47:36,880 --> 00:47:40,640
um so one way uh to

1203
00:47:40,640 --> 00:47:42,079
uh

1204
00:47:42,079 --> 00:47:44,079
to use it is with uh philomorphic

1205
00:47:44,079 --> 00:47:46,000
encryption which is a form of encryption

1206
00:47:46,000 --> 00:47:47,920
that enables computation of arithmetic

1207
00:47:47,920 --> 00:47:49,760
operations over encrypted data without

1208
00:47:49,760 --> 00:47:51,599
decrypting

1209
00:47:51,599 --> 00:47:54,160
however each arithmetic operation adds

1210
00:47:54,160 --> 00:47:56,079
an accumulated noise

1211
00:47:56,079 --> 00:47:59,280
and after a maximal value of

1212
00:47:59,280 --> 00:48:01,280
allowed multiplication the ciphertext

1213
00:48:01,280 --> 00:48:03,280
cannot be decrypted anymore

1214
00:48:03,280 --> 00:48:04,720
so there is solution called the

1215
00:48:04,720 --> 00:48:06,079
bootstrapping that removes the

1216
00:48:06,079 --> 00:48:08,400
accumulated noise but it costs in slow

1217
00:48:08,400 --> 00:48:10,000
run time

1218
00:48:10,000 --> 00:48:12,400
and

1219
00:48:12,559 --> 00:48:14,400
we focus on one

1220
00:48:14,400 --> 00:48:18,079
fh scheme which is called ckks which we

1221
00:48:18,079 --> 00:48:19,760
chose it because it's the fastest scheme

1222
00:48:19,760 --> 00:48:21,920
that allows homomorphic addition and

1223
00:48:21,920 --> 00:48:24,640
multiplication over real numbers

1224
00:48:24,640 --> 00:48:26,400
which are mostly needed for different

1225
00:48:26,400 --> 00:48:28,319
learning inference

1226
00:48:28,319 --> 00:48:30,880
and

1227
00:48:32,079 --> 00:48:34,160
so our goal is to define a set of

1228
00:48:34,160 --> 00:48:36,559
methods for training an efficient zika

1229
00:48:36,559 --> 00:48:39,599
compatible model for secure inference

1230
00:48:39,599 --> 00:48:42,079
however it's not an easy task since uh

1231
00:48:42,079 --> 00:48:42,960
common

1232
00:48:42,960 --> 00:48:44,400
learning architectures use

1233
00:48:44,400 --> 00:48:47,359
non-polynomial operations for example uh

1234
00:48:47,359 --> 00:48:49,599
in the right you can see the alex

1235
00:48:49,599 --> 00:48:52,000
architecture which is a network

1236
00:48:52,000 --> 00:48:52,960
for

1237
00:48:52,960 --> 00:48:55,119
image classification it's a

1238
00:48:55,119 --> 00:48:56,880
convolutional neural network with

1239
00:48:56,880 --> 00:48:59,920
convolutional layers can you see in my

1240
00:48:59,920 --> 00:49:02,920
mouse

1241
00:49:02,960 --> 00:49:04,000
yes

1242
00:49:04,000 --> 00:49:06,160
okay so there's a convolutional

1243
00:49:06,160 --> 00:49:09,680
layer pulling layers and fully connected

1244
00:49:09,680 --> 00:49:11,920
also on the output we use

1245
00:49:11,920 --> 00:49:14,160
the softmax and

1246
00:49:14,160 --> 00:49:16,640
we have a irelia activations

1247
00:49:16,640 --> 00:49:17,760
so

1248
00:49:17,760 --> 00:49:20,240
this uh there are some the max flowing

1249
00:49:20,240 --> 00:49:22,000
soft max and non-polynomial activations

1250
00:49:22,000 --> 00:49:24,880
are not supported by the cks scheme

1251
00:49:24,880 --> 00:49:27,040
so

1252
00:49:27,119 --> 00:49:28,559
previous work

1253
00:49:28,559 --> 00:49:29,680
have

1254
00:49:29,680 --> 00:49:31,200
quite good solutions for the maximum

1255
00:49:31,200 --> 00:49:34,160
softmax um since the max

1256
00:49:34,160 --> 00:49:36,319
operation is not allowed it's like an if

1257
00:49:36,319 --> 00:49:37,440
condition

1258
00:49:37,440 --> 00:49:40,400
uh we use average pudding instead and it

1259
00:49:40,400 --> 00:49:43,040
works quite well um and also for the

1260
00:49:43,040 --> 00:49:44,400
softmax

1261
00:49:44,400 --> 00:49:46,640
which works on the output layer

1262
00:49:46,640 --> 00:49:49,440
uh it uses exponentials and we cannot

1263
00:49:49,440 --> 00:49:52,400
use it for ckks so uh since it works

1264
00:49:52,400 --> 00:49:55,920
only on the output we send to the client

1265
00:49:55,920 --> 00:49:57,200
the logits

1266
00:49:57,200 --> 00:49:59,839
and the the client in turn decrypt it

1267
00:49:59,839 --> 00:50:00,480
and

1268
00:50:00,480 --> 00:50:03,839
performs the softmax by themselves

1269
00:50:03,839 --> 00:50:05,359
so

1270
00:50:05,359 --> 00:50:07,119
but we still have the activation

1271
00:50:07,119 --> 00:50:08,720
function issue

1272
00:50:08,720 --> 00:50:10,640
um

1273
00:50:10,640 --> 00:50:13,280
the standard activation function uh that

1274
00:50:13,280 --> 00:50:15,760
we saw in alexander for example is

1275
00:50:15,760 --> 00:50:17,839
called rally which is defined as

1276
00:50:17,839 --> 00:50:20,559
the maximum value between x and zero

1277
00:50:20,559 --> 00:50:23,040
uh but as we said before we cannot have

1278
00:50:23,040 --> 00:50:25,920
max and secage scheme

1279
00:50:25,920 --> 00:50:30,240
so we stick for a polynomial alternative

1280
00:50:30,240 --> 00:50:33,040
and the most common approaches that were

1281
00:50:33,040 --> 00:50:34,720
used in the previous works are the

1282
00:50:34,720 --> 00:50:36,800
square activation and the value

1283
00:50:36,800 --> 00:50:39,520
polynomial approximation

1284
00:50:39,520 --> 00:50:42,160
but as you can see in the bottom left uh

1285
00:50:42,160 --> 00:50:44,960
in the range between minus one to one

1286
00:50:44,960 --> 00:50:47,119
uh the orange line is the rayleigh

1287
00:50:47,119 --> 00:50:48,319
activation

1288
00:50:48,319 --> 00:50:50,720
uh the green is the square activation

1289
00:50:50,720 --> 00:50:54,240
which only on the positive side

1290
00:50:54,240 --> 00:50:56,160
approximating

1291
00:50:56,160 --> 00:50:58,559
the relative activation and the blue

1292
00:50:58,559 --> 00:51:00,640
line is the

1293
00:51:00,640 --> 00:51:02,640
rayleigh approximation

1294
00:51:02,640 --> 00:51:04,480
which is look like

1295
00:51:04,480 --> 00:51:08,720
which is looks like a linear line so if

1296
00:51:08,720 --> 00:51:11,280
we zoom out we see that the square

1297
00:51:11,280 --> 00:51:13,440
activation totally explodes and the

1298
00:51:13,440 --> 00:51:14,800
approximated

1299
00:51:14,800 --> 00:51:17,520
value looks quite good but if we zoom

1300
00:51:17,520 --> 00:51:19,200
out more to

1301
00:51:19,200 --> 00:51:21,119
a larger range

1302
00:51:21,119 --> 00:51:23,920
both activations

1303
00:51:23,920 --> 00:51:25,760
explode

1304
00:51:25,760 --> 00:51:27,839
so

1305
00:51:27,839 --> 00:51:28,640
we

1306
00:51:28,640 --> 00:51:30,800
we would like to show techniques to

1307
00:51:30,800 --> 00:51:32,400
improve the performance of the models

1308
00:51:32,400 --> 00:51:34,960
with the new activations over real data

1309
00:51:34,960 --> 00:51:38,000
sets uh to improve convergence and avoid

1310
00:51:38,000 --> 00:51:41,000
degradation

1311
00:51:41,200 --> 00:51:43,280
uh the first method i'm going to show is

1312
00:51:43,280 --> 00:51:45,760
called the trainable activation function

1313
00:51:45,760 --> 00:51:48,880
so instead of using fixed coefficients

1314
00:51:48,880 --> 00:51:50,480
that will not support different ranges

1315
00:51:50,480 --> 00:51:52,559
of values we would like to let the

1316
00:51:52,559 --> 00:51:55,280
network choose uh the right activation i

1317
00:51:55,280 --> 00:51:57,359
forgot to mention that as we said before

1318
00:51:57,359 --> 00:51:58,960
that we

1319
00:51:58,960 --> 00:52:01,280
we are limited in the number of

1320
00:52:01,280 --> 00:52:03,680
multiplications we would like to use

1321
00:52:03,680 --> 00:52:04,640
only

1322
00:52:04,640 --> 00:52:05,359
a

1323
00:52:05,359 --> 00:52:07,920
polynomial of a second degrees to avoid

1324
00:52:07,920 --> 00:52:09,680
the bootstrapping

1325
00:52:09,680 --> 00:52:11,200
so

1326
00:52:11,200 --> 00:52:13,760
here we have an activation polynomial

1327
00:52:13,760 --> 00:52:14,720
sorry

1328
00:52:14,720 --> 00:52:16,640
activation of second degree

1329
00:52:16,640 --> 00:52:17,680
and

1330
00:52:17,680 --> 00:52:20,640
we would like to set uh

1331
00:52:20,640 --> 00:52:23,200
the uh to let the network choose the

1332
00:52:23,200 --> 00:52:26,400
right a and b coefficients for each uh

1333
00:52:26,400 --> 00:52:29,520
activation layer separately so

1334
00:52:29,520 --> 00:52:30,800
the network will

1335
00:52:30,800 --> 00:52:33,920
know to choose which scaling works the

1336
00:52:33,920 --> 00:52:36,880
best for each layer and for each dataset

1337
00:52:36,880 --> 00:52:39,280
um and all the training during the

1338
00:52:39,280 --> 00:52:41,760
training of the network with the entire

1339
00:52:41,760 --> 00:52:43,680
weights it will also train the

1340
00:52:43,680 --> 00:52:46,319
activation function

1341
00:52:46,319 --> 00:52:48,240
so here we can see the previous works

1342
00:52:48,240 --> 00:52:50,319
with the on the chest city and chest

1343
00:52:50,319 --> 00:52:52,400
x-ray data set with the square

1344
00:52:52,400 --> 00:52:54,559
activation and approximate value

1345
00:52:54,559 --> 00:52:56,559
the bottom line is the reference network

1346
00:52:56,559 --> 00:52:58,720
with value and max putting

1347
00:52:58,720 --> 00:53:00,960
and we see that our the trainable

1348
00:53:00,960 --> 00:53:04,480
polynomial performs the best uh

1349
00:53:04,480 --> 00:53:07,359
but still there is a gap to the value

1350
00:53:07,359 --> 00:53:08,720
activation

1351
00:53:08,720 --> 00:53:10,720
um

1352
00:53:10,720 --> 00:53:13,599
so now i'm going to talk about how to

1353
00:53:13,599 --> 00:53:15,440
change the activation

1354
00:53:15,440 --> 00:53:16,960
uh which we call it the smooth

1355
00:53:16,960 --> 00:53:18,240
transition

1356
00:53:18,240 --> 00:53:20,559
um

1357
00:53:20,559 --> 00:53:22,480
if you remember we used

1358
00:53:22,480 --> 00:53:24,800
the the original model needs the

1359
00:53:24,800 --> 00:53:26,800
rayleigh activation

1360
00:53:26,800 --> 00:53:28,880
but when we move to a polynomial of a

1361
00:53:28,880 --> 00:53:31,359
second degree uh we change all the

1362
00:53:31,359 --> 00:53:33,200
activation to a polynomial of a second

1363
00:53:33,200 --> 00:53:34,960
degree it can shock the network and

1364
00:53:34,960 --> 00:53:37,280
degrade the performance so we would like

1365
00:53:37,280 --> 00:53:38,800
to avoid that

1366
00:53:38,800 --> 00:53:42,240
by gradually replacing each activation

1367
00:53:42,240 --> 00:53:44,079
for example here we have the weighted

1368
00:53:44,079 --> 00:53:47,280
activation uh we we chose a round of 10

1369
00:53:47,280 --> 00:53:49,119
epochs we would like to change uh

1370
00:53:49,119 --> 00:53:51,440
gradually during for 10 epochs for

1371
00:53:51,440 --> 00:53:54,079
example we start for with a rally

1372
00:53:54,079 --> 00:53:57,280
activation and and gradually um shift to

1373
00:53:57,280 --> 00:53:59,359
the trainable polynomial until we have

1374
00:53:59,359 --> 00:54:02,000
only a trainable polynomial activation

1375
00:54:02,000 --> 00:54:05,119
and we can see here uh before the smooth

1376
00:54:05,119 --> 00:54:06,720
transition and after the smooth

1377
00:54:06,720 --> 00:54:07,920
transition

1378
00:54:07,920 --> 00:54:08,800
that

1379
00:54:08,800 --> 00:54:11,800
we

1380
00:54:14,720 --> 00:54:17,839
perform better now

1381
00:54:18,000 --> 00:54:18,880
so

1382
00:54:18,880 --> 00:54:21,760
we will move to our last technique

1383
00:54:21,760 --> 00:54:23,040
which is called

1384
00:54:23,040 --> 00:54:25,200
knowledge distillation

1385
00:54:25,200 --> 00:54:27,520
we adopted an uh

1386
00:54:27,520 --> 00:54:29,440
a technique a noun technique where

1387
00:54:29,440 --> 00:54:32,480
knowledge distillation is used in a

1388
00:54:32,480 --> 00:54:34,240
different network

1389
00:54:34,240 --> 00:54:36,960
to train a small model

1390
00:54:36,960 --> 00:54:38,400
out of a

1391
00:54:38,400 --> 00:54:40,640
big large model which is pre-trained so

1392
00:54:40,640 --> 00:54:42,240
the large model is called the feature

1393
00:54:42,240 --> 00:54:45,040
model and the small model is called the

1394
00:54:45,040 --> 00:54:46,640
student model

1395
00:54:46,640 --> 00:54:48,480
this is pre-trained model and for

1396
00:54:48,480 --> 00:54:51,280
example if we have images

1397
00:54:51,280 --> 00:54:53,760
we when we want to train the student uh

1398
00:54:53,760 --> 00:54:56,480
we give we we feed both of the models

1399
00:54:56,480 --> 00:54:59,119
with uh the training data

1400
00:54:59,119 --> 00:55:01,760
and we take the soft labels the output

1401
00:55:01,760 --> 00:55:04,000
layer the the output of the both

1402
00:55:04,000 --> 00:55:06,559
networks and we would like to minimize

1403
00:55:06,559 --> 00:55:08,480
the distance between

1404
00:55:08,480 --> 00:55:10,160
both

1405
00:55:10,160 --> 00:55:11,680
outputs

1406
00:55:11,680 --> 00:55:12,880
um

1407
00:55:12,880 --> 00:55:16,319
in our case in for he we don't want to

1408
00:55:16,319 --> 00:55:18,480
reduce the number of layers but we won't

1409
00:55:18,480 --> 00:55:21,440
like to to use as a teacher the model

1410
00:55:21,440 --> 00:55:22,240
with

1411
00:55:22,240 --> 00:55:23,280
the

1412
00:55:23,280 --> 00:55:24,480
prohibited

1413
00:55:24,480 --> 00:55:25,920
activations

1414
00:55:25,920 --> 00:55:27,839
uh so the teacher model will be a

1415
00:55:27,839 --> 00:55:30,000
network the original network with max

1416
00:55:30,000 --> 00:55:32,400
pooling and with relu and the student

1417
00:55:32,400 --> 00:55:34,319
model will be the one with the trainable

1418
00:55:34,319 --> 00:55:36,720
activation and average pulling and the

1419
00:55:36,720 --> 00:55:38,799
teacher model will let the student model

1420
00:55:38,799 --> 00:55:40,880
to learn how to

1421
00:55:40,880 --> 00:55:42,640
choose the right coefficients and the

1422
00:55:42,640 --> 00:55:44,000
other weights

1423
00:55:44,000 --> 00:55:45,200
as well

1424
00:55:45,200 --> 00:55:48,640
and we see here that now

1425
00:55:48,640 --> 00:55:50,880
with the polynomial with knowledge

1426
00:55:50,880 --> 00:55:53,599
distillation we are only between

1427
00:55:53,599 --> 00:55:56,880
one to three percent away from the

1428
00:55:56,880 --> 00:56:00,400
reference with value

1429
00:56:00,960 --> 00:56:04,319
so to summarize um

1430
00:56:04,319 --> 00:56:07,760
in we show here the full table and we

1431
00:56:07,760 --> 00:56:09,680
see here that with our methodology we

1432
00:56:09,680 --> 00:56:12,160
improved the performance of the model

1433
00:56:12,160 --> 00:56:14,880
and closed the gap significantly

1434
00:56:14,880 --> 00:56:18,079
compared to previous models um

1435
00:56:18,079 --> 00:56:20,480
question

1436
00:56:21,280 --> 00:56:24,880
thank you for a nice talk move on um

1437
00:56:24,880 --> 00:56:26,880
so for questions i think they should

1438
00:56:26,880 --> 00:56:30,079
probably be either in the zoom chat or

1439
00:56:30,079 --> 00:56:32,559
there's also a zulub chat for this

1440
00:56:32,559 --> 00:56:34,400
um

1441
00:56:34,400 --> 00:56:37,280
for this workshop on the icr website

1442
00:56:37,280 --> 00:56:38,400
available

1443
00:56:38,400 --> 00:56:39,839
um

1444
00:56:39,839 --> 00:56:40,960
i think

1445
00:56:40,960 --> 00:56:43,119
uh yeah that'd be the end of the session

1446
00:56:43,119 --> 00:56:46,160
thanks again to all of the speakers

1447
00:56:46,160 --> 00:56:49,599
um and if i see that correctly then we

1448
00:56:49,599 --> 00:56:52,400
will

