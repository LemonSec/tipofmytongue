1
00:00:02,720 --> 00:00:03,919
okay perfect

2
00:00:03,919 --> 00:00:05,600
so now we are going to have our last

3
00:00:05,600 --> 00:00:07,600
session which is the contributor talks

4
00:00:07,600 --> 00:00:09,519
we are going to have six interesting

5
00:00:09,519 --> 00:00:10,800
talks here

6
00:00:10,800 --> 00:00:13,360
and we are going to start the first one

7
00:00:13,360 --> 00:00:15,280
uh but just to let you know you can ask

8
00:00:15,280 --> 00:00:16,960
all your questions at the end of each

9
00:00:16,960 --> 00:00:19,279
talk uh either by raising hands and

10
00:00:19,279 --> 00:00:20,640
opening up your mic

11
00:00:20,640 --> 00:00:22,240
or you can just write it in the chat so

12
00:00:22,240 --> 00:00:24,160
we can ask you a question from the

13
00:00:24,160 --> 00:00:27,359
speakers so the first talk is uh called

14
00:00:27,359 --> 00:00:30,640
nora crypt is not private uh which the

15
00:00:30,640 --> 00:00:32,320
speaker is going to tell us how they

16
00:00:32,320 --> 00:00:34,719
could attack this nora crypt library

17
00:00:34,719 --> 00:00:36,960
uh said mahuji fire is going to speak

18
00:00:36,960 --> 00:00:39,840
for us and the floor is yours say start

19
00:00:39,840 --> 00:00:42,160
please

20
00:00:42,840 --> 00:00:46,399
thanks do you see my screen

21
00:00:46,399 --> 00:00:47,760
yes perfect

22
00:00:47,760 --> 00:00:49,600
great thank you very much uh onsite

23
00:00:49,600 --> 00:00:51,039
knowledge fire we'll talk about our

24
00:00:51,039 --> 00:00:53,199
world no crypt is not private this is a

25
00:00:53,199 --> 00:00:55,360
joint work with nicholas sanja somash

26
00:00:55,360 --> 00:00:56,840
mohammed and

27
00:00:56,840 --> 00:01:00,160
floyd right let's start

28
00:01:00,160 --> 00:01:02,879
let's start with the problem setup so we

29
00:01:02,879 --> 00:01:05,040
have two parties alice and bob alice has

30
00:01:05,040 --> 00:01:07,520
access to a data set bob has access to a

31
00:01:07,520 --> 00:01:09,600
learning algorithm and the computational

32
00:01:09,600 --> 00:01:10,799
power to

33
00:01:10,799 --> 00:01:12,960
run that learning algorithm alice wants

34
00:01:12,960 --> 00:01:14,720
to use this learning algorithm to get a

35
00:01:14,720 --> 00:01:16,799
model but she doesn't want to send the

36
00:01:16,799 --> 00:01:19,439
data in plane so she first encodes her

37
00:01:19,439 --> 00:01:21,360
data using some

38
00:01:21,360 --> 00:01:23,360
encoding scheme that potentially takes

39
00:01:23,360 --> 00:01:26,720
keys and sends this data set to bar

40
00:01:26,720 --> 00:01:28,560
bob trains the model on this encoded

41
00:01:28,560 --> 00:01:31,439
data and sends it back to send the model

42
00:01:31,439 --> 00:01:34,079
back to alice and now alice can use this

43
00:01:34,079 --> 00:01:36,320
model in a very specific way that is

44
00:01:36,320 --> 00:01:38,159
before feeding the instances to the

45
00:01:38,159 --> 00:01:41,040
model she has to encode it with the same

46
00:01:41,040 --> 00:01:43,600
key and only then she can expect to see

47
00:01:43,600 --> 00:01:45,600
the right answer

48
00:01:45,600 --> 00:01:47,040
they call this uh

49
00:01:47,040 --> 00:01:48,880
primitive uh private learning with

50
00:01:48,880 --> 00:01:50,399
instance encoding

51
00:01:50,399 --> 00:01:53,680
and uh neural crypt is a recent uh

52
00:01:53,680 --> 00:01:55,439
proposal that tries to achieve this

53
00:01:55,439 --> 00:01:57,680
primitive uh through

54
00:01:57,680 --> 00:02:01,759
encodings by random neural networks

55
00:02:02,000 --> 00:02:05,119
so let's see how it works um

56
00:02:05,119 --> 00:02:07,119
it basically only introduces a new

57
00:02:07,119 --> 00:02:08,720
encoding algorithm that uses neural

58
00:02:08,720 --> 00:02:10,239
networks and it doesn't change anything

59
00:02:10,239 --> 00:02:12,080
about the learning algorithm learning

60
00:02:12,080 --> 00:02:13,360
algorithms is a typical learning

61
00:02:13,360 --> 00:02:14,640
algorithm that you apply to neural

62
00:02:14,640 --> 00:02:16,319
networks but before doing that you use

63
00:02:16,319 --> 00:02:18,879
their encoding scheme which has two

64
00:02:18,879 --> 00:02:20,560
procedures the key generation and

65
00:02:20,560 --> 00:02:22,800
encoding the key generation just samples

66
00:02:22,800 --> 00:02:25,760
a neural network with random weights

67
00:02:25,760 --> 00:02:27,520
and that will be the key and when you

68
00:02:27,520 --> 00:02:29,520
want to encode you just run your input

69
00:02:29,520 --> 00:02:30,800
on this

70
00:02:30,800 --> 00:02:33,120
random run network so this bottom

71
00:02:33,120 --> 00:02:35,360
picture has the exact architecture of

72
00:02:35,360 --> 00:02:36,800
the rockery

73
00:02:36,800 --> 00:02:38,400
it has some other details like the

74
00:02:38,400 --> 00:02:39,680
random

75
00:02:39,680 --> 00:02:41,599
permutation at the last layer but you

76
00:02:41,599 --> 00:02:42,959
don't have to worry about that you can

77
00:02:42,959 --> 00:02:45,680
just think of this as a

78
00:02:45,680 --> 00:02:48,840
new neural network with random

79
00:02:48,840 --> 00:02:51,599
widths so what is the privacy definition

80
00:02:51,599 --> 00:02:53,760
here so they define the notion of

81
00:02:53,760 --> 00:02:57,360
perfect privacy that is uh saying for

82
00:02:57,360 --> 00:02:58,560
any two

83
00:02:58,560 --> 00:03:01,360
instances or images with the same label

84
00:03:01,360 --> 00:03:03,120
you expect the distribution of the

85
00:03:03,120 --> 00:03:05,360
encoding to be exactly equal

86
00:03:05,360 --> 00:03:07,519
and the distribution is taken over the

87
00:03:07,519 --> 00:03:09,519
the different keys so that this when you

88
00:03:09,519 --> 00:03:12,400
change the key you expect for any two

89
00:03:12,400 --> 00:03:14,080
distributions of the encodings to be

90
00:03:14,080 --> 00:03:15,680
exact

91
00:03:15,680 --> 00:03:18,159
this is the privacy definition how do

92
00:03:18,159 --> 00:03:21,120
they analyze this

93
00:03:21,200 --> 00:03:23,519
their encoding so they first show a

94
00:03:23,519 --> 00:03:25,760
proof of concept and coding called ideal

95
00:03:25,760 --> 00:03:28,159
encoding that does the following so it

96
00:03:28,159 --> 00:03:31,440
takes an instance and maps it to another

97
00:03:31,440 --> 00:03:33,840
instance with the same label for example

98
00:03:33,840 --> 00:03:36,000
it takes a cat image and maps it to

99
00:03:36,000 --> 00:03:37,680
another cat image

100
00:03:37,680 --> 00:03:39,920
in in the space of all possible cat

101
00:03:39,920 --> 00:03:40,959
images

102
00:03:40,959 --> 00:03:42,159
so

103
00:03:42,159 --> 00:03:44,000
they they show that this skin can

104
00:03:44,000 --> 00:03:46,640
actually achieve perfect privacy and it

105
00:03:46,640 --> 00:03:49,120
could also be used for learning because

106
00:03:49,120 --> 00:03:51,760
um you are preserving a lot of good

107
00:03:51,760 --> 00:03:53,680
information you are just mapping the

108
00:03:53,680 --> 00:03:55,280
chat to another child so it should be

109
00:03:55,280 --> 00:03:58,480
still learnable and

110
00:03:58,480 --> 00:04:01,599
at the end they they claim that murakrid

111
00:04:01,599 --> 00:04:03,920
approximates the idea that

112
00:04:03,920 --> 00:04:05,599
so this is the intuition behind the

113
00:04:05,599 --> 00:04:07,760
privacy of this

114
00:04:07,760 --> 00:04:08,560
so

115
00:04:08,560 --> 00:04:09,519
uh

116
00:04:09,519 --> 00:04:11,920
to to back their claim they provide two

117
00:04:11,920 --> 00:04:14,400
challenge datasets the first challenge

118
00:04:14,400 --> 00:04:15,439
is a

119
00:04:15,439 --> 00:04:18,079
combination of 10 000 images with their

120
00:04:18,079 --> 00:04:21,358
encodings and they ask the community to

121
00:04:21,358 --> 00:04:24,160
match these encodings to the plain

122
00:04:24,160 --> 00:04:25,440
images

123
00:04:25,440 --> 00:04:27,680
they also have another challenge that i

124
00:04:27,680 --> 00:04:29,120
don't want to talk about it here but

125
00:04:29,120 --> 00:04:33,280
i'll get back to it a bit later

126
00:04:33,280 --> 00:04:35,759
so in our paper our contribution was to

127
00:04:35,759 --> 00:04:38,400
first of all break challenge one

128
00:04:38,400 --> 00:04:40,160
so we show that with hundred percent

129
00:04:40,160 --> 00:04:43,199
accuracy you can match the uh

130
00:04:43,199 --> 00:04:44,800
images with their encoding in the

131
00:04:44,800 --> 00:04:47,040
challenge data set we also instantiate

132
00:04:47,040 --> 00:04:48,800
this challenge with our own data set and

133
00:04:48,800 --> 00:04:49,520
we

134
00:04:49,520 --> 00:04:51,840
get the same result so this shows that

135
00:04:51,840 --> 00:04:54,320
neural crypt is not

136
00:04:54,320 --> 00:04:57,040
does not have the perfect privacy and is

137
00:04:57,040 --> 00:04:59,600
not an approximation of the idea of

138
00:04:59,600 --> 00:05:01,919
coding

139
00:05:02,400 --> 00:05:04,560
in a high level the the idea behind

140
00:05:04,560 --> 00:05:06,720
their attack the the high level

141
00:05:06,720 --> 00:05:09,360
intuition is that we use the statistical

142
00:05:09,360 --> 00:05:11,039
invariance in different layers of a

143
00:05:11,039 --> 00:05:13,039
random neural network

144
00:05:13,039 --> 00:05:15,759
for example if you look at the mean

145
00:05:15,759 --> 00:05:18,400
of the pixels in the image you expect a

146
00:05:18,400 --> 00:05:20,400
particular behavior as you go through

147
00:05:20,400 --> 00:05:22,720
the different layers of the network

148
00:05:22,720 --> 00:05:24,320
specifically when your rates are

149
00:05:24,320 --> 00:05:26,320
selected and run

150
00:05:26,320 --> 00:05:28,080
we use this intuition and design our

151
00:05:28,080 --> 00:05:30,960
attack to match the final encodings and

152
00:05:30,960 --> 00:05:33,840
the inputs and we do it specifically in

153
00:05:33,840 --> 00:05:35,120
three steps

154
00:05:35,120 --> 00:05:37,919
so the first step of our attack uh we

155
00:05:37,919 --> 00:05:39,520
train and embedding

156
00:05:39,520 --> 00:05:40,560
uh

157
00:05:40,560 --> 00:05:43,680
that its goal is to map images and their

158
00:05:43,680 --> 00:05:46,080
encodings to almost the same point in

159
00:05:46,080 --> 00:05:49,120
the embedding space so we want the

160
00:05:49,120 --> 00:05:51,120
embedding of the image and its encoding

161
00:05:51,120 --> 00:05:52,800
to be close to each other

162
00:05:52,800 --> 00:05:54,720
but for any other image we want the

163
00:05:54,720 --> 00:05:56,880
encode the embedding of it to be far

164
00:05:56,880 --> 00:05:59,600
from the embedding of the originals so

165
00:05:59,600 --> 00:06:01,199
this is the property that we want in

166
00:06:01,199 --> 00:06:03,840
this embedding we do this by training a

167
00:06:03,840 --> 00:06:05,600
neural network using

168
00:06:05,600 --> 00:06:07,680
standard loss functions for the school

169
00:06:07,680 --> 00:06:09,680
but before that we

170
00:06:09,680 --> 00:06:11,680
calculate the moment vectors of these

171
00:06:11,680 --> 00:06:13,600
images and

172
00:06:13,600 --> 00:06:15,520
give us the give the neural network some

173
00:06:15,520 --> 00:06:17,759
hints of where to look at so we just

174
00:06:17,759 --> 00:06:19,759
asked the neural networks to just look

175
00:06:19,759 --> 00:06:22,080
at the this moment they towards the mean

176
00:06:22,080 --> 00:06:24,880
the variance and up to eighth moment uh

177
00:06:24,880 --> 00:06:26,880
and calculate the embedding just based

178
00:06:26,880 --> 00:06:28,800
on this and this is

179
00:06:28,800 --> 00:06:31,600
why i call it a statistical invariance

180
00:06:31,600 --> 00:06:33,120
because what the embedding is doing is

181
00:06:33,120 --> 00:06:35,840
basically finding some

182
00:06:35,840 --> 00:06:38,400
invariance in the statistics

183
00:06:38,400 --> 00:06:41,199
uh so in the second step what we do is

184
00:06:41,199 --> 00:06:43,360
we use this uh embedding that we trained

185
00:06:43,360 --> 00:06:46,000
on our own data set to find uh some

186
00:06:46,000 --> 00:06:48,880
similarity metric between all the pairs

187
00:06:48,880 --> 00:06:51,599
of encodings and images and after we do

188
00:06:51,599 --> 00:06:54,479
this we just solve the maximum matching

189
00:06:54,479 --> 00:06:56,479
problem on the quadratic graph and that

190
00:06:56,479 --> 00:06:58,479
will be our final match

191
00:06:58,479 --> 00:07:02,000
and as we show this this tag obtains 100

192
00:07:02,000 --> 00:07:05,039
percent accuracy

193
00:07:06,240 --> 00:07:09,039
all right so what about challenge 2 so

194
00:07:09,039 --> 00:07:10,800
uh i didn't tell you what challenge 2

195
00:07:10,800 --> 00:07:13,039
was challenge 2 gives us the encoded

196
00:07:13,039 --> 00:07:16,400
dataset and asked us to find an

197
00:07:16,400 --> 00:07:18,479
approximation of the neural network that

198
00:07:18,479 --> 00:07:22,000
was used to create this encoded dataset

199
00:07:22,000 --> 00:07:23,919
and approximation is done in a very

200
00:07:23,919 --> 00:07:25,759
specific way that is based on the

201
00:07:25,759 --> 00:07:28,479
nearest neighbor match in the encoding

202
00:07:28,479 --> 00:07:31,479
space

203
00:07:32,160 --> 00:07:34,880
so this we don't find challenge 2 to be

204
00:07:34,880 --> 00:07:36,479
an interesting challenge first of all

205
00:07:36,479 --> 00:07:38,240
because it's a cipher text only

206
00:07:38,240 --> 00:07:40,000
challenge and it just tricks the

207
00:07:40,000 --> 00:07:42,720
adversary too much

208
00:07:42,720 --> 00:07:45,280
we believe this is not a good definition

209
00:07:45,280 --> 00:07:47,919
for privacy and uh

210
00:07:47,919 --> 00:07:49,840
it's also it's not clear how this

211
00:07:49,840 --> 00:07:52,160
definition and how this challenge is

212
00:07:52,160 --> 00:07:54,720
related to privacy uh at best this

213
00:07:54,720 --> 00:07:57,039
challenge is analogous to a key recovery

214
00:07:57,039 --> 00:08:00,080
challenge but but we don't see why it's

215
00:08:00,080 --> 00:08:02,240
necessary to break this uh to show

216
00:08:02,240 --> 00:08:04,319
neural crypt is not private in

217
00:08:04,319 --> 00:08:07,440
particular we actually make this formal

218
00:08:07,440 --> 00:08:10,800
and we show uh encoding a scheme

219
00:08:10,800 --> 00:08:14,960
that is provably secure in challenge two

220
00:08:14,960 --> 00:08:18,479
yet it reveals the whole input in plan

221
00:08:18,479 --> 00:08:21,360
so we could try for years to break the

222
00:08:21,360 --> 00:08:22,080
uh

223
00:08:22,080 --> 00:08:24,319
challenge too for this encoding and be

224
00:08:24,319 --> 00:08:26,319
unsuccessful while it's clear that is

225
00:08:26,319 --> 00:08:27,520
broken

226
00:08:27,520 --> 00:08:30,160
so this is another reason that we

227
00:08:30,160 --> 00:08:32,880
don't study this challenge tool because

228
00:08:32,880 --> 00:08:34,559
it's not uh

229
00:08:34,559 --> 00:08:37,440
necessary to show uh neural crime is not

230
00:08:37,440 --> 00:08:39,919
private and we already have a strong

231
00:08:39,919 --> 00:08:43,120
attack which was a sniper

232
00:08:43,120 --> 00:08:44,720
all right so now invite us what about

233
00:08:44,720 --> 00:08:46,800
the ideal encoding they had this ideal

234
00:08:46,800 --> 00:08:48,480
encoding that they analyzed and they

235
00:08:48,480 --> 00:08:49,839
showed it's uh

236
00:08:49,839 --> 00:08:52,480
uh has the perfect privacy

237
00:08:52,480 --> 00:08:54,560
uh can something else approximately

238
00:08:54,560 --> 00:08:56,640
neural crypt doesn't what can something

239
00:08:56,640 --> 00:08:59,200
else does uh approximate this ideal

240
00:08:59,200 --> 00:09:00,959
including to answer this question we

241
00:09:00,959 --> 00:09:03,680
have a formal impossibility result here

242
00:09:03,680 --> 00:09:05,920
saying that if you can approximate the

243
00:09:05,920 --> 00:09:07,760
ideal including for some console

244
00:09:07,760 --> 00:09:09,200
function c

245
00:09:09,200 --> 00:09:12,640
then there is an oracle ppt algorithm

246
00:09:12,640 --> 00:09:15,600
that can extract this concept function

247
00:09:15,600 --> 00:09:18,640
from this encoder by just querying the

248
00:09:18,640 --> 00:09:19,839
encoder

249
00:09:19,839 --> 00:09:21,839
so what does this theorem mean it means

250
00:09:21,839 --> 00:09:24,720
that the the encoder must already know

251
00:09:24,720 --> 00:09:28,000
the concept function which makes it uh

252
00:09:28,000 --> 00:09:30,160
like there is no point in learning c

253
00:09:30,160 --> 00:09:32,000
from data you could just learn it from

254
00:09:32,000 --> 00:09:34,640
your encode

255
00:09:35,279 --> 00:09:38,399
so either the encoding is impossible or

256
00:09:38,399 --> 00:09:40,560
the or learning the concept function

257
00:09:40,560 --> 00:09:42,959
itself is kind of true

258
00:09:42,959 --> 00:09:44,720
all right so

259
00:09:44,720 --> 00:09:48,800
the proof idea here has three steps so

260
00:09:48,800 --> 00:09:51,279
the first step we use the encoder to

261
00:09:51,279 --> 00:09:53,760
sample a data set of points we take one

262
00:09:53,760 --> 00:09:56,240
child we take one dot we use different

263
00:09:56,240 --> 00:09:58,800
keys to generate our own data set and

264
00:09:58,800 --> 00:10:00,959
then in the second step we use this data

265
00:10:00,959 --> 00:10:03,360
set to train a classifier that gets some

266
00:10:03,360 --> 00:10:06,320
non-trivial accuracy on the state on

267
00:10:06,320 --> 00:10:08,560
this distribution

268
00:10:08,560 --> 00:10:10,079
and in the third is that we use this

269
00:10:10,079 --> 00:10:12,480
classifier and use this boosting idea to

270
00:10:12,480 --> 00:10:14,480
make the accuracy of this class for

271
00:10:14,480 --> 00:10:17,040
almost 100 percent the reason we can do

272
00:10:17,040 --> 00:10:18,720
this is again because we have access to

273
00:10:18,720 --> 00:10:20,800
the encoder that can distribute the

274
00:10:20,800 --> 00:10:23,360
error across all points so given a point

275
00:10:23,360 --> 00:10:26,880
x i just sample a bunch of keys uh and

276
00:10:26,880 --> 00:10:29,760
uh encode the x based on those keys feed

277
00:10:29,760 --> 00:10:31,680
it to the classifier and take the

278
00:10:31,680 --> 00:10:34,000
majority vote and you can easily show

279
00:10:34,000 --> 00:10:35,680
that this gets

280
00:10:35,680 --> 00:10:39,360
very close to 100 percent actors

281
00:10:39,360 --> 00:10:41,360
okay to summarize we showed that newer

282
00:10:41,360 --> 00:10:43,360
crypt is not uh

283
00:10:43,360 --> 00:10:44,800
private it doesn't have the perfect

284
00:10:44,800 --> 00:10:46,959
privacy that it claims we show that

285
00:10:46,959 --> 00:10:49,200
challenge two is not uh

286
00:10:49,200 --> 00:10:51,839
much relevant to privacy and we are

287
00:10:51,839 --> 00:10:53,600
at the end show that approximation of

288
00:10:53,600 --> 00:10:55,519
the ideal encoding is basically

289
00:10:55,519 --> 00:10:57,760
impossible that was it thank you very

290
00:10:57,760 --> 00:11:00,320
much

291
00:11:00,320 --> 00:11:03,760
uh thank you very much said um

292
00:11:03,760 --> 00:11:06,640
please if anyone has any question

293
00:11:06,640 --> 00:11:08,480
and raise your hand or

294
00:11:08,480 --> 00:11:11,880
unmute yourself

295
00:11:16,480 --> 00:11:18,800
okay thank you saeed

296
00:11:18,800 --> 00:11:21,440
so we can move on to the second talk uh

297
00:11:21,440 --> 00:11:23,600
which the topic is secure past and

298
00:11:23,600 --> 00:11:25,040
regression

299
00:11:25,040 --> 00:11:27,920
and mahimna kelkar is going to show us

300
00:11:27,920 --> 00:11:30,320
how we can compute poisson regression

301
00:11:30,320 --> 00:11:32,880
into two-party secure computation

302
00:11:32,880 --> 00:11:36,079
mahima take it away

303
00:11:36,079 --> 00:11:37,680
oh thank you i'm gonna

304
00:11:37,680 --> 00:11:41,160
share my screen

305
00:11:42,880 --> 00:11:44,880
yes perfect

306
00:11:44,880 --> 00:11:47,040
okay awesome so i'm not and today i'll

307
00:11:47,040 --> 00:11:48,959
be giving a talk on our recent work on

308
00:11:48,959 --> 00:11:51,200
secure poisson regression and this is

309
00:11:51,200 --> 00:11:56,360
joint work with fee mariana and karn

310
00:11:56,880 --> 00:11:58,560
so privacy preserving machine learning

311
00:11:58,560 --> 00:12:00,480
has been kind of a very active research

312
00:12:00,480 --> 00:12:03,279
area as evident by workshops like these

313
00:12:03,279 --> 00:12:04,880
and a lot of the previous work has

314
00:12:04,880 --> 00:12:06,720
focused on efficiently learning

315
00:12:06,720 --> 00:12:08,880
different types of useful functions in a

316
00:12:08,880 --> 00:12:11,120
secure setting so some common functions

317
00:12:11,120 --> 00:12:12,880
that we've been usually thinking about

318
00:12:12,880 --> 00:12:14,480
have been like

319
00:12:14,480 --> 00:12:16,720
linear regression where the goal is to

320
00:12:16,720 --> 00:12:18,800
jointly learn a linear function or a

321
00:12:18,800 --> 00:12:20,480
logistic regression where you want to

322
00:12:20,480 --> 00:12:22,000
learn the probability of a binary

323
00:12:22,000 --> 00:12:23,600
response variable

324
00:12:23,600 --> 00:12:25,360
there's also been significant progress

325
00:12:25,360 --> 00:12:27,279
on learning more complex functions like

326
00:12:27,279 --> 00:12:29,120
neural networks

327
00:12:29,120 --> 00:12:31,279
so in this work we want to focus on

328
00:12:31,279 --> 00:12:33,360
another common model used in practice

329
00:12:33,360 --> 00:12:34,720
that has not been explored in the

330
00:12:34,720 --> 00:12:36,240
privacy preserving computation

331
00:12:36,240 --> 00:12:38,320
literature so far so we'll look at

332
00:12:38,320 --> 00:12:40,560
poisson regression for which uh the

333
00:12:40,560 --> 00:12:42,399
ghost for the parties to jointly learn a

334
00:12:42,399 --> 00:12:44,480
poisson model without revealing their

335
00:12:44,480 --> 00:12:46,639
inputs and our emphasis will be on

336
00:12:46,639 --> 00:12:49,680
efficiency of this secure computation

337
00:12:49,680 --> 00:12:52,000
so what is a poisson model poisson model

338
00:12:52,000 --> 00:12:54,240
is used to model count or rate based

339
00:12:54,240 --> 00:12:55,120
data

340
00:12:55,120 --> 00:12:57,279
so this is usually modeled by the

341
00:12:57,279 --> 00:13:00,240
function expectation of y given x equals

342
00:13:00,240 --> 00:13:02,480
e to the power dot product between theta

343
00:13:02,480 --> 00:13:04,639
and x where y is the response variable

344
00:13:04,639 --> 00:13:06,800
and x are the predictor variables and

345
00:13:06,800 --> 00:13:09,440
theta is the weights vector

346
00:13:09,440 --> 00:13:10,880
there's also times where you have

347
00:13:10,880 --> 00:13:13,040
another variable called t which models

348
00:13:13,040 --> 00:13:16,720
the exposure for different inputs

349
00:13:16,720 --> 00:13:18,320
which is what's used to model the raid

350
00:13:18,320 --> 00:13:21,839
base instead of the count based data

351
00:13:21,839 --> 00:13:23,360
so poisson models have actually been

352
00:13:23,360 --> 00:13:25,839
used extensively in the past to measure

353
00:13:25,839 --> 00:13:28,079
as i said count or rate based data so

354
00:13:28,079 --> 00:13:30,000
for example in medicine they've been

355
00:13:30,000 --> 00:13:31,519
used to predict mortality rate or

356
00:13:31,519 --> 00:13:33,920
susceptibility of diseases based on age

357
00:13:33,920 --> 00:13:36,240
or gender pre-existing conditions or

358
00:13:36,240 --> 00:13:38,800
habits like smoking or alcohol use

359
00:13:38,800 --> 00:13:40,240
they've been used in politics to

360
00:13:40,240 --> 00:13:42,959
understand voter turnout and engagement

361
00:13:42,959 --> 00:13:45,760
based on demographics or news exposure

362
00:13:45,760 --> 00:13:47,600
they've been used in finance and lending

363
00:13:47,600 --> 00:13:49,839
to predict credit default rates based on

364
00:13:49,839 --> 00:13:51,120
income

365
00:13:51,120 --> 00:13:53,600
or more recently in advertising to model

366
00:13:53,600 --> 00:13:56,240
click rates for online ads

367
00:13:56,240 --> 00:13:58,000
but in all these use cases there's a lot

368
00:13:58,000 --> 00:14:00,000
of private or sensitive information

369
00:14:00,000 --> 00:14:01,199
that's used in the training of the

370
00:14:01,199 --> 00:14:03,120
poisson model so for instance we could

371
00:14:03,120 --> 00:14:05,120
have sensitive medical or demographic

372
00:14:05,120 --> 00:14:07,360
information in this training set which

373
00:14:07,360 --> 00:14:09,040
we don't really want to reveal

374
00:14:09,040 --> 00:14:10,800
so the goal for us is to try to learn

375
00:14:10,800 --> 00:14:12,880
this poisson model without revealing the

376
00:14:12,880 --> 00:14:15,519
sensitive information

377
00:14:15,519 --> 00:14:17,680
so we focus on the semi-honest

378
00:14:17,680 --> 00:14:20,560
two-server model we have two servers a

379
00:14:20,560 --> 00:14:22,880
and b that have uh shares of the

380
00:14:22,880 --> 00:14:25,279
training data uh so the data id is split

381
00:14:25,279 --> 00:14:27,839
between two servers uh as the shares da

382
00:14:27,839 --> 00:14:30,000
and db and these are given to the two

383
00:14:30,000 --> 00:14:32,480
servers and now the goal is to jointly

384
00:14:32,480 --> 00:14:35,120
train a post on model

385
00:14:35,120 --> 00:14:35,839
so

386
00:14:35,839 --> 00:14:38,639
the data here will be given in uh in a

387
00:14:38,639 --> 00:14:40,160
fixed point ring parametrized by three

388
00:14:40,160 --> 00:14:42,240
parameters l f and h

389
00:14:42,240 --> 00:14:44,240
so all the values are given in a fixed

390
00:14:44,240 --> 00:14:46,399
point ring z2 to the l and the fixed

391
00:14:46,399 --> 00:14:48,399
point numbers are represented with f

392
00:14:48,399 --> 00:14:50,560
fractional bits and the absolute value

393
00:14:50,560 --> 00:14:52,639
of all the numbers that we work with is

394
00:14:52,639 --> 00:14:54,639
less than 2 to the l minus h and this is

395
00:14:54,639 --> 00:14:56,160
done to

396
00:14:56,160 --> 00:14:58,320
have a gap between the representation of

397
00:14:58,320 --> 00:14:59,839
the fixed point numbers and the overall

398
00:14:59,839 --> 00:15:04,000
ring to reduce the probability of errors

399
00:15:04,000 --> 00:15:05,440
so more formally for our poisson

400
00:15:05,440 --> 00:15:08,160
regression problem we have a training

401
00:15:08,160 --> 00:15:10,320
data where we have n data points and the

402
00:15:10,320 --> 00:15:13,680
i data point is given with m

403
00:15:13,680 --> 00:15:16,959
variables and t i is the exposure and y

404
00:15:16,959 --> 00:15:19,040
i is the response variable so we can

405
00:15:19,040 --> 00:15:20,880
also represent them using matrices and

406
00:15:20,880 --> 00:15:23,760
now we want to learn a weight vector

407
00:15:23,760 --> 00:15:26,240
theta such that y each y is

408
00:15:26,240 --> 00:15:31,199
approximately t i times e to the theta x

409
00:15:31,199 --> 00:15:33,440
and unfortunately exponentiation is

410
00:15:33,440 --> 00:15:36,399
quite difficult in npc and usually any

411
00:15:36,399 --> 00:15:38,079
kind of nonlinear function is the most

412
00:15:38,079 --> 00:15:40,160
challenging and probably uh the

413
00:15:40,160 --> 00:15:42,639
bottleneck so here we'll need to compute

414
00:15:42,639 --> 00:15:44,480
fixed point exponentiation which is

415
00:15:44,480 --> 00:15:45,759
usually hard and so one of our

416
00:15:45,759 --> 00:15:47,920
contributions is a very efficient

417
00:15:47,920 --> 00:15:50,399
protocol for secure exponentiation

418
00:15:50,399 --> 00:15:52,639
so this surprisingly makes it so that

419
00:15:52,639 --> 00:15:54,399
it's no longer the bottleneck of the

420
00:15:54,399 --> 00:15:57,759
poisson regression function

421
00:15:57,759 --> 00:15:59,680
so in practice to do this poisson

422
00:15:59,680 --> 00:16:00,880
regression

423
00:16:00,880 --> 00:16:02,720
the natural choice is to use gradient

424
00:16:02,720 --> 00:16:04,480
descent to learn the weights

425
00:16:04,480 --> 00:16:06,720
so we start with this loss function l of

426
00:16:06,720 --> 00:16:10,000
theta given the data um where for the

427
00:16:10,000 --> 00:16:11,519
minimization we've computed the gradient

428
00:16:11,519 --> 00:16:13,440
of the loss function as the gradient of

429
00:16:13,440 --> 00:16:17,759
l of uh given theta is like the sum of

430
00:16:17,759 --> 00:16:20,560
um the

431
00:16:20,560 --> 00:16:22,399
sum of how much

432
00:16:22,399 --> 00:16:24,399
the error is between the learned weights

433
00:16:24,399 --> 00:16:27,920
and the actual um function

434
00:16:27,920 --> 00:16:29,920
and of course we can represent this

435
00:16:29,920 --> 00:16:31,920
using matrices again and now the

436
00:16:31,920 --> 00:16:33,920
gradient update function is as follows

437
00:16:33,920 --> 00:16:36,800
where the k k plus 1 iteration

438
00:16:36,800 --> 00:16:38,720
will learn the weight is theta equals 1

439
00:16:38,720 --> 00:16:41,440
minus beta times the previous data

440
00:16:41,440 --> 00:16:44,639
plus alpha times this gradient of the

441
00:16:44,639 --> 00:16:47,120
loss function and here alpha is the

442
00:16:47,120 --> 00:16:48,959
learning rate which is helpful

443
00:16:48,959 --> 00:16:51,040
to tune for better conversions

444
00:16:51,040 --> 00:16:53,759
and beta is a regular regularization

445
00:16:53,759 --> 00:16:55,279
parameter which is used to prevent

446
00:16:55,279 --> 00:16:56,800
overfitting

447
00:16:56,800 --> 00:16:58,639
so this is pretty standard from taking

448
00:16:58,639 --> 00:17:00,880
from uh just post on regression

449
00:17:00,880 --> 00:17:02,639
and trying to understand what kind of

450
00:17:02,639 --> 00:17:04,400
gradient updates we need to do or

451
00:17:04,400 --> 00:17:06,880
replicate in the secure settings

452
00:17:06,880 --> 00:17:08,480
so in our work we'll make the following

453
00:17:08,480 --> 00:17:10,799
contributions first as mentioned before

454
00:17:10,799 --> 00:17:13,280
we'll give a very efficient protocol for

455
00:17:13,280 --> 00:17:15,599
secure fixed point exponentiation

456
00:17:15,599 --> 00:17:18,799
in this semi honest two-party setting

457
00:17:18,799 --> 00:17:21,280
second we'll provide an optimized secure

458
00:17:21,280 --> 00:17:22,959
matrix multiplication that specifically

459
00:17:22,959 --> 00:17:25,199
benefits our setting by amortizing the

460
00:17:25,199 --> 00:17:27,280
cost of multiplications over many of the

461
00:17:27,280 --> 00:17:29,440
iterations of the gradient descent

462
00:17:29,440 --> 00:17:30,960
and finally

463
00:17:30,960 --> 00:17:32,400
this results in

464
00:17:32,400 --> 00:17:34,640
a highly efficient secure post on

465
00:17:34,640 --> 00:17:36,960
regression protocol

466
00:17:36,960 --> 00:17:39,360
so i'll very briefly describe our secure

467
00:17:39,360 --> 00:17:41,600
fixed point exponentiation technique

468
00:17:41,600 --> 00:17:42,320
so

469
00:17:42,320 --> 00:17:45,200
there it's split into um

470
00:17:45,200 --> 00:17:46,480
so here's the high level technique the

471
00:17:46,480 --> 00:17:48,400
first thing that you do is you split the

472
00:17:48,400 --> 00:17:50,400
exponent into fractional and integer

473
00:17:50,400 --> 00:17:52,160
parts and the second thing that you do

474
00:17:52,160 --> 00:17:54,160
is you exponentiate both parts locally

475
00:17:54,160 --> 00:17:55,520
so you exponentiate the integer parts

476
00:17:55,520 --> 00:17:56,640
separately and the fractional parts

477
00:17:56,640 --> 00:17:58,559
separately and then you combine to get

478
00:17:58,559 --> 00:18:00,640
approximately multiplicative shares of

479
00:18:00,640 --> 00:18:02,400
the final computation

480
00:18:02,400 --> 00:18:04,480
so here uh the integer exponentiation

481
00:18:04,480 --> 00:18:05,760
can be done using firmware's little

482
00:18:05,760 --> 00:18:07,440
theorem so it'll give you shares in a

483
00:18:07,440 --> 00:18:09,039
field f2 rather than the fixed point

484
00:18:09,039 --> 00:18:09,840
ring

485
00:18:09,840 --> 00:18:11,600
and for the fractional exponentiation we

486
00:18:11,600 --> 00:18:13,679
have a new technique where rather than

487
00:18:13,679 --> 00:18:16,640
using standard polynomial approximation

488
00:18:16,640 --> 00:18:18,240
we can enable the parties to compute the

489
00:18:18,240 --> 00:18:20,960
fractional exponentiation locally

490
00:18:20,960 --> 00:18:22,480
by exponentiating in real numbers or

491
00:18:22,480 --> 00:18:24,320
floating point and then combine it with

492
00:18:24,320 --> 00:18:26,240
the integer exponentiation also

493
00:18:26,240 --> 00:18:27,679
completely locally

494
00:18:27,679 --> 00:18:30,400
and since the computation is local

495
00:18:30,400 --> 00:18:32,080
and you you're using floating point or

496
00:18:32,080 --> 00:18:33,520
real numbers this actually makes the

497
00:18:33,520 --> 00:18:35,600
error much smaller than polynomial

498
00:18:35,600 --> 00:18:37,039
approximation

499
00:18:37,039 --> 00:18:38,400
and finally now we can take these

500
00:18:38,400 --> 00:18:40,320
multiplicative shares and convert them

501
00:18:40,320 --> 00:18:42,799
to additive shares using only one round

502
00:18:42,799 --> 00:18:44,400
of communication

503
00:18:44,400 --> 00:18:46,160
and this is the only communication step

504
00:18:46,160 --> 00:18:47,679
that would be used in the protocol and

505
00:18:47,679 --> 00:18:50,240
only requires sending a single field

506
00:18:50,240 --> 00:18:51,200
element

507
00:18:51,200 --> 00:18:54,640
to be communicated per party

508
00:18:54,640 --> 00:18:56,880
so just to compare our exponentiation

509
00:18:56,880 --> 00:18:59,039
protocol um a little with other

510
00:18:59,039 --> 00:19:01,919
techniques as i mentioned the error is

511
00:19:01,919 --> 00:19:03,280
much smaller than polynomial

512
00:19:03,280 --> 00:19:05,600
approximation it's usually at least

513
00:19:05,600 --> 00:19:07,520
a couple orders of magnitude smaller and

514
00:19:07,520 --> 00:19:09,760
the gap widens the more fractional bits

515
00:19:09,760 --> 00:19:11,840
that you use for the protocol

516
00:19:11,840 --> 00:19:14,320
we also have a very efficient online

517
00:19:14,320 --> 00:19:16,880
phase where the communication is just

518
00:19:16,880 --> 00:19:19,360
one field element per party and here fq

519
00:19:19,360 --> 00:19:21,520
is a l bit

520
00:19:21,520 --> 00:19:22,799
prime

521
00:19:22,799 --> 00:19:24,320
q is an l bit prime so the field is

522
00:19:24,320 --> 00:19:26,880
roughly the size of the uh

523
00:19:26,880 --> 00:19:28,400
of the fixed point ring

524
00:19:28,400 --> 00:19:30,160
so for the online phase we have one

525
00:19:30,160 --> 00:19:32,400
micro second when we're using a 63 bit

526
00:19:32,400 --> 00:19:35,520
ring and 55 microseconds for when we're

527
00:19:35,520 --> 00:19:37,840
using a 127 bit ring for

528
00:19:37,840 --> 00:19:40,160
the exponentiation computation

529
00:19:40,160 --> 00:19:42,640
we also have a huge advantage when it

530
00:19:42,640 --> 00:19:44,960
comes to pre-processing so our

531
00:19:44,960 --> 00:19:46,480
pre-processing is only for the

532
00:19:46,480 --> 00:19:47,919
multiplicative to additive shares

533
00:19:47,919 --> 00:19:50,559
conversion which only takes two elements

534
00:19:50,559 --> 00:19:52,799
in fq compared to

535
00:19:52,799 --> 00:19:54,559
several megabytes using other techniques

536
00:19:54,559 --> 00:19:56,480
that rely on bit decomposition or

537
00:19:56,480 --> 00:19:58,320
polynomial approximation

538
00:19:58,320 --> 00:20:00,559
so this means that even in like an

539
00:20:00,559 --> 00:20:02,640
online only setting the performance will

540
00:20:02,640 --> 00:20:03,600
be

541
00:20:03,600 --> 00:20:05,039
very good where you can't do

542
00:20:05,039 --> 00:20:07,840
pre-computation earlier

543
00:20:07,840 --> 00:20:09,600
the second technique we'll use is what

544
00:20:09,600 --> 00:20:12,000
we call correlated beaver triples so

545
00:20:12,000 --> 00:20:13,760
briefly recall that multiplication or

546
00:20:13,760 --> 00:20:14,960
beaver triples are used as

547
00:20:14,960 --> 00:20:17,360
pre-processing to compute multiplication

548
00:20:17,360 --> 00:20:19,440
of two shared inputs and these can be

549
00:20:19,440 --> 00:20:20,880
easily generalized to matrix

550
00:20:20,880 --> 00:20:23,440
multiplication so now if you look at the

551
00:20:23,440 --> 00:20:25,200
loss optimization function again you'll

552
00:20:25,200 --> 00:20:27,760
notice that there's in each iteration

553
00:20:27,760 --> 00:20:29,600
you have one multiplication with x

554
00:20:29,600 --> 00:20:32,240
transpose and one multiplication with x

555
00:20:32,240 --> 00:20:34,159
and these are kind of used for each

556
00:20:34,159 --> 00:20:37,440
iteration and the uh x's are the same

557
00:20:37,440 --> 00:20:39,679
and just the theta's change so basically

558
00:20:39,679 --> 00:20:41,360
for the ice iteration we have a

559
00:20:41,360 --> 00:20:43,840
multiplication of the form x times the i

560
00:20:43,840 --> 00:20:45,520
where the zi's change but the x remains

561
00:20:45,520 --> 00:20:47,200
constant

562
00:20:47,200 --> 00:20:48,960
so now with this observation we can do

563
00:20:48,960 --> 00:20:50,799
something clever so instead of using

564
00:20:50,799 --> 00:20:52,640
standard matrix triples as

565
00:20:52,640 --> 00:20:53,840
pre-processing we can use these

566
00:20:53,840 --> 00:20:55,360
correlated triples since one of the

567
00:20:55,360 --> 00:20:57,360
matrix is constant it does not need to

568
00:20:57,360 --> 00:20:58,799
be shared again

569
00:20:58,799 --> 00:21:00,159
and now this means that the

570
00:21:00,159 --> 00:21:01,600
reconstruction phase for the

571
00:21:01,600 --> 00:21:03,200
multiplication needs to be only done

572
00:21:03,200 --> 00:21:06,000
once for the uh first matrix and later

573
00:21:06,000 --> 00:21:08,480
iterations can reuse this

574
00:21:08,480 --> 00:21:10,320
so now the computation cos sorry the

575
00:21:10,320 --> 00:21:12,880
communication cost can be

576
00:21:12,880 --> 00:21:15,440
amortized over many iterations because

577
00:21:15,440 --> 00:21:17,600
the cost of doing many many iterations

578
00:21:17,600 --> 00:21:19,840
is only slightly larger than that of one

579
00:21:19,840 --> 00:21:21,679
iteration

580
00:21:21,679 --> 00:21:23,520
and typically since we're doing hundreds

581
00:21:23,520 --> 00:21:25,200
of iterations of grid of the gradient

582
00:21:25,200 --> 00:21:27,120
descent the efficiency gains are quite

583
00:21:27,120 --> 00:21:30,120
substantial

584
00:21:30,240 --> 00:21:33,760
um as a conclusion our poison regression

585
00:21:33,760 --> 00:21:36,559
only takes four rounds for the entire

586
00:21:36,559 --> 00:21:38,640
protocol and if our data set does not

587
00:21:38,640 --> 00:21:40,880
contain an exposure variable t

588
00:21:40,880 --> 00:21:43,039
then uh we can shave off an additional

589
00:21:43,039 --> 00:21:45,120
round so it only takes three rounds and

590
00:21:45,120 --> 00:21:48,320
we can also use the same protocol for

591
00:21:48,320 --> 00:21:51,360
secure inference

592
00:21:51,520 --> 00:21:53,120
so as a comparison of our secure

593
00:21:53,120 --> 00:21:55,840
protocol to plaintext regression we plot

594
00:21:55,840 --> 00:21:57,280
how the weights change for different

595
00:21:57,280 --> 00:21:59,440
data sets compared to secure versus

596
00:21:59,440 --> 00:22:01,919
plain text and the lines for secure and

597
00:22:01,919 --> 00:22:03,600
plain text regression and these

598
00:22:03,600 --> 00:22:05,840
graphs are essentially identical

599
00:22:05,840 --> 00:22:07,600
so i'll say like the root mean square

600
00:22:07,600 --> 00:22:09,039
error that we computed between the

601
00:22:09,039 --> 00:22:10,880
plaintext weights and

602
00:22:10,880 --> 00:22:14,240
the secure weights is less than uh 10 to

603
00:22:14,240 --> 00:22:16,240
the negative 3 or 10 to the negative 4

604
00:22:16,240 --> 00:22:18,159
depending upon what learning rate we use

605
00:22:18,159 --> 00:22:21,200
and the data set we use so this is um

606
00:22:21,200 --> 00:22:22,880
saying that our secure weights that we

607
00:22:22,880 --> 00:22:25,120
learn are very very close to the weights

608
00:22:25,120 --> 00:22:26,480
that are learned by just the plain text

609
00:22:26,480 --> 00:22:28,960
regression

610
00:22:28,960 --> 00:22:32,000
in terms of just a brief uh

611
00:22:32,000 --> 00:22:34,159
brief data point for the protocol cost

612
00:22:34,159 --> 00:22:36,000
for a data set with 10 000 training

613
00:22:36,000 --> 00:22:38,320
samples and 100 binary predictors the

614
00:22:38,320 --> 00:22:40,480
amortized cost over 100 iterations is

615
00:22:40,480 --> 00:22:43,440
1.35 seconds per iteration online cost

616
00:22:43,440 --> 00:22:46,559
is 2.88 seconds and 1.3 megabytes of

617
00:22:46,559 --> 00:22:48,799
communication per iteration so the total

618
00:22:48,799 --> 00:22:51,200
cost over something like 100 iterations

619
00:22:51,200 --> 00:22:52,880
is a little over two minutes for offline

620
00:22:52,880 --> 00:22:54,799
phase five minutes for online phase and

621
00:22:54,799 --> 00:22:58,159
about 136 megabytes of communication

622
00:22:58,159 --> 00:22:59,919
and i'll sort of end with that here's a

623
00:22:59,919 --> 00:23:01,760
link to the paper in case you're

624
00:23:01,760 --> 00:23:02,799
interested

625
00:23:02,799 --> 00:23:05,120
thank you

626
00:23:07,600 --> 00:23:10,000
thank you very much

627
00:23:10,000 --> 00:23:12,480
it was a great talk

628
00:23:12,480 --> 00:23:15,120
i don't see any questions in the

629
00:23:15,120 --> 00:23:17,200
comment part but if you have any

630
00:23:17,200 --> 00:23:21,000
questions please ask

631
00:23:26,000 --> 00:23:27,760
i think there's a question someone's

632
00:23:27,760 --> 00:23:30,240
raised their hand

633
00:23:30,240 --> 00:23:33,039
yeah it's just it's a quick question

634
00:23:33,039 --> 00:23:35,440
thanks a lot i just would like to ask is

635
00:23:35,440 --> 00:23:37,360
the method that you design for

636
00:23:37,360 --> 00:23:39,280
exponentiation is very specific to two

637
00:23:39,280 --> 00:23:40,960
parties and passive security or can't

638
00:23:40,960 --> 00:23:42,960
extend it like basically so it's

639
00:23:42,960 --> 00:23:44,880
relatively straightforward to extend it

640
00:23:44,880 --> 00:23:47,200
to n parties uh because the only thing

641
00:23:47,200 --> 00:23:48,000
that needs to change is the

642
00:23:48,000 --> 00:23:49,279
multiplicative to additive shares

643
00:23:49,279 --> 00:23:51,279
component which can be generalized it is

644
00:23:51,279 --> 00:23:54,320
a little unclear how to do it with um

645
00:23:54,320 --> 00:23:56,640
malicious security because one of the

646
00:23:56,640 --> 00:23:58,480
advantages of our approach was we can

647
00:23:58,480 --> 00:24:00,480
exponentiate in like real numbers

648
00:24:00,480 --> 00:24:03,120
locally and then combine so it's hard to

649
00:24:03,120 --> 00:24:05,200
prove a correctness of that using

650
00:24:05,200 --> 00:24:07,039
generic techniques so it's unclear but

651
00:24:07,039 --> 00:24:08,720
it's an open question for malicious

652
00:24:08,720 --> 00:24:11,200
security

653
00:24:15,760 --> 00:24:18,159
okay great so thank you so much again

654
00:24:18,159 --> 00:24:20,960
mahinda we are going to

655
00:24:20,960 --> 00:24:22,720
start with the next talk

656
00:24:22,720 --> 00:24:24,880
next talk is called muse secure

657
00:24:24,880 --> 00:24:28,000
inference resilient to malicious clients

658
00:24:28,000 --> 00:24:30,159
you're interested to hear about that uh

659
00:24:30,159 --> 00:24:32,559
pratish mishra is going to give a talk

660
00:24:32,559 --> 00:24:35,120
so please start

661
00:24:35,120 --> 00:24:37,120
everyone let me just

662
00:24:37,120 --> 00:24:39,918
share my screen

663
00:24:43,200 --> 00:24:46,200
okay

664
00:24:48,559 --> 00:24:51,039
get started so hi everyone i'm pratyush

665
00:24:51,039 --> 00:24:52,720
and i'll be presenting muse which is a

666
00:24:52,720 --> 00:24:55,200
protocol for secure inference of neural

667
00:24:55,200 --> 00:24:56,960
networks that's resilient to malicious

668
00:24:56,960 --> 00:24:59,039
clients and this is a joint work with my

669
00:24:59,039 --> 00:25:01,120
excellent co-authors ryan akshay and

670
00:25:01,120 --> 00:25:03,600
reluca

671
00:25:04,320 --> 00:25:05,039
so

672
00:25:05,039 --> 00:25:06,480
okay so what's secure influence the

673
00:25:06,480 --> 00:25:08,320
problem secure inference is developing a

674
00:25:08,320 --> 00:25:10,960
protocol for a client and server to

675
00:25:10,960 --> 00:25:12,480
interact to obtain a neural network

676
00:25:12,480 --> 00:25:14,559
prediction the client has some input x

677
00:25:14,559 --> 00:25:16,240
and the server has some neural network

678
00:25:16,240 --> 00:25:17,760
model m

679
00:25:17,760 --> 00:25:19,039
and the goal is that the client and

680
00:25:19,039 --> 00:25:20,559
server should learn only the prediction

681
00:25:20,559 --> 00:25:22,080
m of x

682
00:25:22,080 --> 00:25:23,360
so that the client does not learn

683
00:25:23,360 --> 00:25:25,120
anything about the server's private

684
00:25:25,120 --> 00:25:27,120
model rates and the server doesn't learn

685
00:25:27,120 --> 00:25:28,559
anything about the client's private

686
00:25:28,559 --> 00:25:30,559
input next

687
00:25:30,559 --> 00:25:32,559
so this is many applications for you

688
00:25:32,559 --> 00:25:35,039
know a lot of cases where uh both x and

689
00:25:35,039 --> 00:25:37,039
m can be sensitive such as you know home

690
00:25:37,039 --> 00:25:39,919
monitoring or baby monitoring or

691
00:25:39,919 --> 00:25:40,840
other

692
00:25:40,840 --> 00:25:43,520
applications so as a result there's been

693
00:25:43,520 --> 00:25:46,080
a lot of work over the past couple of uh

694
00:25:46,080 --> 00:25:48,000
past few years on secure two-party

695
00:25:48,000 --> 00:25:49,679
inference

696
00:25:49,679 --> 00:25:52,159
and these have sorry

697
00:25:52,159 --> 00:25:54,000
a variety of efficiency and security

698
00:25:54,000 --> 00:25:55,679
properties

699
00:25:55,679 --> 00:25:57,360
however like all of these protocols we

700
00:25:57,360 --> 00:25:58,880
can basically classify them into two

701
00:25:58,880 --> 00:26:00,880
quadrants either you have protocols

702
00:26:00,880 --> 00:26:02,720
which achieve a very strong malicious

703
00:26:02,720 --> 00:26:04,320
security

704
00:26:04,320 --> 00:26:06,400
but have relatively slow performance due

705
00:26:06,400 --> 00:26:09,120
to the use of generic sub protocols

706
00:26:09,120 --> 00:26:11,120
all you have protocols which achieve

707
00:26:11,120 --> 00:26:14,320
very good performance on

708
00:26:14,320 --> 00:26:15,919
state-of-the-art neural networks but

709
00:26:15,919 --> 00:26:18,240
which only achieves semi-honors security

710
00:26:18,240 --> 00:26:20,240
so ideally what we want to do is we want

711
00:26:20,240 --> 00:26:22,159
to develop protocols which lie in this

712
00:26:22,159 --> 00:26:24,880
bottom right most quadrant

713
00:26:24,880 --> 00:26:26,400
that is they achieve both malicious

714
00:26:26,400 --> 00:26:28,960
security as well as good performance

715
00:26:28,960 --> 00:26:31,200
but you know militia security is uh

716
00:26:31,200 --> 00:26:33,200
usually very expensive so let's take a

717
00:26:33,200 --> 00:26:35,279
step back and see if we actually need

718
00:26:35,279 --> 00:26:36,840
full malicious

719
00:26:36,840 --> 00:26:39,360
security so okay so in the threat model

720
00:26:39,360 --> 00:26:40,960
of secure inference what you have is

721
00:26:40,960 --> 00:26:42,640
that there's usually only a single

722
00:26:42,640 --> 00:26:44,640
server which has the model but there are

723
00:26:44,640 --> 00:26:46,720
many possibly many clients which

724
00:26:46,720 --> 00:26:48,159
interact with the single server right

725
00:26:48,159 --> 00:26:49,440
and these clients have different kinds

726
00:26:49,440 --> 00:26:51,679
of setups incentives could be operated

727
00:26:51,679 --> 00:26:54,400
by different people right um and for the

728
00:26:54,400 --> 00:26:56,640
more the clients can oftentimes easily

729
00:26:56,640 --> 00:26:58,240
remain anonymous like i can just sign up

730
00:26:58,240 --> 00:26:59,760
with a new account

731
00:26:59,760 --> 00:27:02,000
for an application for example

732
00:27:02,000 --> 00:27:04,720
right um so this setting it maybe makes

733
00:27:04,720 --> 00:27:06,559
sense to consider a slightly weaker

734
00:27:06,559 --> 00:27:08,400
threat model where we only try to

735
00:27:08,400 --> 00:27:10,640
prevent malicious clients we allow the

736
00:27:10,640 --> 00:27:12,720
server to be semi-honest but we allow

737
00:27:12,720 --> 00:27:15,279
the client to be uh malicious right so

738
00:27:15,279 --> 00:27:16,960
we protect against fully malicious

739
00:27:16,960 --> 00:27:18,320
clients

740
00:27:18,320 --> 00:27:20,640
and this makes sense because um in this

741
00:27:20,640 --> 00:27:22,240
threat model at least right it's much

742
00:27:22,240 --> 00:27:24,720
easier for their client to cheat you can

743
00:27:24,720 --> 00:27:28,799
just spin up a new um you know client uh

744
00:27:28,799 --> 00:27:30,159
very cheaply

745
00:27:30,159 --> 00:27:31,520
but if the server gets caught cheating

746
00:27:31,520 --> 00:27:32,880
then then there can be heavy

747
00:27:32,880 --> 00:27:34,640
repercussions for example you know

748
00:27:34,640 --> 00:27:35,919
people might just stop using your

749
00:27:35,919 --> 00:27:39,200
service if you get caught cheating right

750
00:27:39,200 --> 00:27:40,640
okay so in this

751
00:27:40,640 --> 00:27:42,720
so this client malicious security model

752
00:27:42,720 --> 00:27:44,640
seems to make sense and indeed it allows

753
00:27:44,640 --> 00:27:46,480
us to construct

754
00:27:46,480 --> 00:27:48,080
much more efficient protocols and would

755
00:27:48,080 --> 00:27:49,919
be possible in the fully malicious case

756
00:27:49,919 --> 00:27:51,360
as we'll see

757
00:27:51,360 --> 00:27:53,919
so in particular in this paper muse

758
00:27:53,919 --> 00:27:56,080
we provide two contributions first we

759
00:27:56,080 --> 00:27:58,240
show that existing semi honest secure

760
00:27:58,240 --> 00:27:59,679
inference protocols including the

761
00:27:59,679 --> 00:28:01,360
state-of-the-art ones

762
00:28:01,360 --> 00:28:03,360
are vulnerable to a very powerful model

763
00:28:03,360 --> 00:28:05,600
extraction attack uh that is you know

764
00:28:05,600 --> 00:28:08,720
much more efficient than prior attacks

765
00:28:08,720 --> 00:28:10,320
and this is enabled by the fact that

766
00:28:10,320 --> 00:28:12,159
they're using the secure inference

767
00:28:12,159 --> 00:28:13,279
protocol

768
00:28:13,279 --> 00:28:15,279
and then we develop a new

769
00:28:15,279 --> 00:28:17,039
secure inference protocol that is secure

770
00:28:17,039 --> 00:28:19,120
against malicious clients and achieves

771
00:28:19,120 --> 00:28:21,360
good efficiency in particular it's not

772
00:28:21,360 --> 00:28:23,360
vulnerable to our model extraction

773
00:28:23,360 --> 00:28:24,960
attack

774
00:28:24,960 --> 00:28:26,640
okay so let's quickly go through each of

775
00:28:26,640 --> 00:28:28,559
these contributions

776
00:28:28,559 --> 00:28:30,000
beginning with a quick recap of neural

777
00:28:30,000 --> 00:28:32,640
networks so in neural networks you have

778
00:28:32,640 --> 00:28:35,039
alternating in particular convolutional

779
00:28:35,039 --> 00:28:37,200
neural networks you have alternating

780
00:28:37,200 --> 00:28:39,440
linear and non-linear layers the linear

781
00:28:39,440 --> 00:28:40,320
layers

782
00:28:40,320 --> 00:28:42,399
consist of

783
00:28:42,399 --> 00:28:44,960
for example fully of convolutional uh

784
00:28:44,960 --> 00:28:46,720
layers fully connected layers average

785
00:28:46,720 --> 00:28:48,559
pruning layers and so on

786
00:28:48,559 --> 00:28:51,120
meanwhile non-linear layers um the most

787
00:28:51,120 --> 00:28:52,799
popular one and the one we'll consider

788
00:28:52,799 --> 00:28:56,000
in the stock is the value so relu is

789
00:28:56,000 --> 00:28:58,720
zero when the input is negative and the

790
00:28:58,720 --> 00:29:00,720
identity function when the input is

791
00:29:00,720 --> 00:29:03,440
positive right

792
00:29:03,440 --> 00:29:04,399
and looking

793
00:29:04,399 --> 00:29:06,000
slightly ahead

794
00:29:06,000 --> 00:29:08,080
what makes model extraction attacks

795
00:29:08,080 --> 00:29:10,240
often difficult is that is the existence

796
00:29:10,240 --> 00:29:11,919
of these non-linear layers right without

797
00:29:11,919 --> 00:29:13,679
them the network would simply be a

798
00:29:13,679 --> 00:29:15,840
linear system which is fairly easy to

799
00:29:15,840 --> 00:29:18,080
recover just by querying it

800
00:29:18,080 --> 00:29:20,480
a number of times

801
00:29:20,480 --> 00:29:23,120
right okay so quickly a quick recap of

802
00:29:23,120 --> 00:29:24,799
model extraction attacks so model

803
00:29:24,799 --> 00:29:27,120
extraction attacks what we want to do is

804
00:29:27,120 --> 00:29:28,960
you have a malicious client which makes

805
00:29:28,960 --> 00:29:30,880
specially crafted malicious queries to

806
00:29:30,880 --> 00:29:33,039
the server and then the client uses the

807
00:29:33,039 --> 00:29:35,760
responses from these special queries to

808
00:29:35,760 --> 00:29:37,520
learn information about the model so

809
00:29:37,520 --> 00:29:39,039
that eventually after making enough

810
00:29:39,039 --> 00:29:40,480
queries it can

811
00:29:40,480 --> 00:29:42,159
construct a

812
00:29:42,159 --> 00:29:44,559
rough approximation of the clients of

813
00:29:44,559 --> 00:29:46,240
the server's model

814
00:29:46,240 --> 00:29:47,760
right

815
00:29:47,760 --> 00:29:50,000
so this is problematic for our secure

816
00:29:50,000 --> 00:29:51,279
reference case because we don't want to

817
00:29:51,279 --> 00:29:53,279
reveal information about the service

818
00:29:53,279 --> 00:29:55,679
model right and indeed like so far like

819
00:29:55,679 --> 00:29:57,760
all of the attacks that have

820
00:29:57,760 --> 00:29:59,440
been explored generally have been black

821
00:29:59,440 --> 00:30:01,039
box attacks where the client simply

822
00:30:01,039 --> 00:30:04,000
makes input output queries uh so yeah it

823
00:30:04,000 --> 00:30:05,919
makes it only gets input output access

824
00:30:05,919 --> 00:30:08,799
to the model right um and the question

825
00:30:08,799 --> 00:30:11,840
we want to ask is whether the fact that

826
00:30:11,840 --> 00:30:13,039
a client

827
00:30:13,039 --> 00:30:14,960
or whether you know the secure interest

828
00:30:14,960 --> 00:30:16,080
protocol

829
00:30:16,080 --> 00:30:17,440
um

830
00:30:17,440 --> 00:30:19,520
is being used can that fact just enable

831
00:30:19,520 --> 00:30:21,600
new model extraction attacks and i won't

832
00:30:21,600 --> 00:30:23,200
go into the details here but basically

833
00:30:23,200 --> 00:30:26,559
what we show is that by

834
00:30:26,559 --> 00:30:29,120
leveraging leveraging properties of a

835
00:30:29,120 --> 00:30:31,039
particular class of uh

836
00:30:31,039 --> 00:30:33,760
protocols which rely on additive secret

837
00:30:33,760 --> 00:30:34,640
sharing

838
00:30:34,640 --> 00:30:37,360
um we can actually con construct

839
00:30:37,360 --> 00:30:40,000
very uh new very powerful attra attacks

840
00:30:40,000 --> 00:30:42,080
that um

841
00:30:42,080 --> 00:30:43,919
basically eliminate the non-linear layer

842
00:30:43,919 --> 00:30:46,240
entirely from the

843
00:30:46,240 --> 00:30:47,520
from the network

844
00:30:47,520 --> 00:30:48,480
and

845
00:30:48,480 --> 00:30:49,919
in summary what this allows us to do

846
00:30:49,919 --> 00:30:52,720
with our new attack is um

847
00:30:52,720 --> 00:30:55,120
get a much lower query complexity so up

848
00:30:55,120 --> 00:30:57,919
to 300 times fewer queries um we are

849
00:30:57,919 --> 00:30:59,440
able to extract the model weights

850
00:30:59,440 --> 00:31:00,880
perfectly you know rather than

851
00:31:00,880 --> 00:31:02,320
approximating them this is up to

852
00:31:02,320 --> 00:31:04,720
floating point error

853
00:31:04,720 --> 00:31:05,840
and we scale with the number of

854
00:31:05,840 --> 00:31:08,399
parameters in the in the model not the

855
00:31:08,399 --> 00:31:10,720
depth of the network

856
00:31:10,720 --> 00:31:12,799
and to get together put together this

857
00:31:12,799 --> 00:31:15,360
allows us to evaluate our attack on

858
00:31:15,360 --> 00:31:17,679
networks which are 100 times deeper than

859
00:31:17,679 --> 00:31:19,919
prior work and which have

860
00:31:19,919 --> 00:31:21,360
60 times

861
00:31:21,360 --> 00:31:23,600
more parameters right so maybe we're

862
00:31:23,600 --> 00:31:24,640
getting up

863
00:31:24,640 --> 00:31:26,000
close to

864
00:31:26,000 --> 00:31:28,480
deployed models right

865
00:31:28,480 --> 00:31:30,240
and this is just enabled by the fact

866
00:31:30,240 --> 00:31:33,279
that this class of secure inference

867
00:31:33,279 --> 00:31:36,399
protocols they have a semi honest uh

868
00:31:36,399 --> 00:31:38,720
structure which allows

869
00:31:38,720 --> 00:31:40,480
basically the malicious client to

870
00:31:40,480 --> 00:31:43,360
malleat its messages

871
00:31:43,360 --> 00:31:44,799
all right so

872
00:31:44,799 --> 00:31:46,559
let's see if you can solve this so what

873
00:31:46,559 --> 00:31:48,480
we do for that is we construct muse

874
00:31:48,480 --> 00:31:51,039
which is a system for secure inference

875
00:31:51,039 --> 00:31:53,440
on convolutional neural networks which

876
00:31:53,440 --> 00:31:55,760
achieves uh strong security against

877
00:31:55,760 --> 00:31:59,120
client against malicious clients

878
00:31:59,120 --> 00:32:01,360
it supports arbitrary uh

879
00:32:01,360 --> 00:32:04,000
relu based convolutional neural networks

880
00:32:04,000 --> 00:32:05,919
and finally um

881
00:32:05,919 --> 00:32:07,519
achieves much better performance

882
00:32:07,519 --> 00:32:09,279
compared to existing alternatives which

883
00:32:09,279 --> 00:32:12,080
rely on uh maliciously secure protocols

884
00:32:12,080 --> 00:32:14,880
or sub protocols and whose online phase

885
00:32:14,880 --> 00:32:16,960
is very similar to existing semi honest

886
00:32:16,960 --> 00:32:19,600
protocols so you pay most of the cost of

887
00:32:19,600 --> 00:32:21,519
this client maliciousness actually only

888
00:32:21,519 --> 00:32:25,440
in the pre-processing or offline phase

889
00:32:25,440 --> 00:32:26,880
all right so the starting point for our

890
00:32:26,880 --> 00:32:28,480
protocol is

891
00:32:28,480 --> 00:32:30,880
a prior secure inference protocol which

892
00:32:30,880 --> 00:32:33,760
is semi honest called delphi

893
00:32:33,760 --> 00:32:35,120
so delphi there are two phrases a

894
00:32:35,120 --> 00:32:38,000
pre-processing phase and an online phase

895
00:32:38,000 --> 00:32:39,519
and the pre-processing phase itself

896
00:32:39,519 --> 00:32:42,159
divided into two components

897
00:32:42,159 --> 00:32:44,480
first the client and sub interact to

898
00:32:44,480 --> 00:32:46,480
pre-process the linear layers right and

899
00:32:46,480 --> 00:32:48,480
this produces some

900
00:32:48,480 --> 00:32:49,679
material

901
00:32:49,679 --> 00:32:51,679
technically underneath it you use some

902
00:32:51,679 --> 00:32:53,279
homomorphic encryption to compute

903
00:32:53,279 --> 00:32:55,039
correlated randomness

904
00:32:55,039 --> 00:32:57,360
right so once they get the linear

905
00:32:57,360 --> 00:32:59,039
pre-processing done

906
00:32:59,039 --> 00:33:00,080
they can

907
00:33:00,080 --> 00:33:02,159
go ahead to compute the

908
00:33:02,159 --> 00:33:04,559
non-linear pre-processing

909
00:33:04,559 --> 00:33:06,159
and this basically involves the server

910
00:33:06,159 --> 00:33:08,480
gobbling some circuits and the client

911
00:33:08,480 --> 00:33:11,200
obtaining labels for those circuits

912
00:33:11,200 --> 00:33:12,880
and these labels correspond to the

913
00:33:12,880 --> 00:33:15,039
output of the linear preprocessing

914
00:33:15,039 --> 00:33:16,640
right

915
00:33:16,640 --> 00:33:18,799
so basically at the end of the step

916
00:33:18,799 --> 00:33:20,640
the client and the server they have

917
00:33:20,640 --> 00:33:22,000
pre-processing material for both the

918
00:33:22,000 --> 00:33:23,600
linear and the non-linear layers in the

919
00:33:23,600 --> 00:33:24,720
network

920
00:33:24,720 --> 00:33:26,159
then during the online phase when the

921
00:33:26,159 --> 00:33:27,919
client has its

922
00:33:27,919 --> 00:33:29,279
input x

923
00:33:29,279 --> 00:33:31,840
the client and server they can use

924
00:33:31,840 --> 00:33:34,320
their preprocessing material to go ahead

925
00:33:34,320 --> 00:33:36,320
and complete the inference and obtain

926
00:33:36,320 --> 00:33:39,120
the prediction

927
00:33:39,120 --> 00:33:41,039
okay so that's laughing right

928
00:33:41,039 --> 00:33:43,279
um let's see what a malicious client can

929
00:33:43,279 --> 00:33:44,720
do in delphi

930
00:33:44,720 --> 00:33:47,039
first during the pre-processing phase

931
00:33:47,039 --> 00:33:49,919
it can act maliciously to generate some

932
00:33:49,919 --> 00:33:51,039
incorrect

933
00:33:51,039 --> 00:33:52,880
pre-processing material so this is

934
00:33:52,880 --> 00:33:54,559
illustrated here by the

935
00:33:54,559 --> 00:33:56,000
uh red

936
00:33:56,000 --> 00:33:58,480
uh right so you can compute the cl prime

937
00:33:58,480 --> 00:34:00,960
and cn prime

938
00:34:00,960 --> 00:34:03,200
and then during the online phase uh you

939
00:34:03,200 --> 00:34:04,960
can use this again you can act

940
00:34:04,960 --> 00:34:07,519
the client can act maliciously to

941
00:34:07,519 --> 00:34:10,159
deviate from the protocol and uh maybe

942
00:34:10,159 --> 00:34:13,599
compute some malleated output right

943
00:34:13,599 --> 00:34:15,679
so what we need to do actually is two

944
00:34:15,679 --> 00:34:17,280
steps first we need to ensure that

945
00:34:17,280 --> 00:34:19,040
during the offline or preprocessing

946
00:34:19,040 --> 00:34:21,520
phase the client follows a protocol and

947
00:34:21,520 --> 00:34:23,679
then during then we need to make sure

948
00:34:23,679 --> 00:34:25,679
that the output of that preprocessing is

949
00:34:25,679 --> 00:34:27,918
what is fed into the online phase right

950
00:34:27,918 --> 00:34:29,599
so we need to commit the client to the

951
00:34:29,599 --> 00:34:31,359
pre-processed state and make sure that

952
00:34:31,359 --> 00:34:32,879
these are consistent between the offline

953
00:34:32,879 --> 00:34:35,440
and online phases

954
00:34:35,440 --> 00:34:37,199
so let's see how we can do that

955
00:34:37,199 --> 00:34:39,199
so first for the linear layers it's a

956
00:34:39,199 --> 00:34:41,440
pretty straightforward idea we attach an

957
00:34:41,440 --> 00:34:43,199
information theoretic mac only to the

958
00:34:43,199 --> 00:34:44,719
client's linear state because remember

959
00:34:44,719 --> 00:34:46,079
the server is semi honest so we don't

960
00:34:46,079 --> 00:34:48,800
need to authenticate the server state

961
00:34:48,800 --> 00:34:50,879
so the

962
00:34:50,879 --> 00:34:52,960
server generates a mac key and then this

963
00:34:52,960 --> 00:34:56,159
is used to authenticate the client state

964
00:34:56,159 --> 00:34:58,560
and then you proceed as before

965
00:34:58,560 --> 00:35:00,560
computing the non-linear pre-processing

966
00:35:00,560 --> 00:35:02,480
and you feed these uh

967
00:35:02,480 --> 00:35:04,400
these two components into the online

968
00:35:04,400 --> 00:35:05,200
phase

969
00:35:05,200 --> 00:35:07,599
and the idea here is now because it is

970
00:35:07,599 --> 00:35:09,440
that because the

971
00:35:09,440 --> 00:35:11,200
the linear state is authenticated the

972
00:35:11,200 --> 00:35:13,599
server can use the mac to check the

973
00:35:13,599 --> 00:35:16,640
that this the consistency of the output

974
00:35:16,640 --> 00:35:18,480
of the offline phase with the input the

975
00:35:18,480 --> 00:35:20,560
online phase right

976
00:35:20,560 --> 00:35:21,520
um

977
00:35:21,520 --> 00:35:23,119
and furthermore for the non-linear

978
00:35:23,119 --> 00:35:25,599
layers uh basically kabul circuits they

979
00:35:25,599 --> 00:35:27,200
already provide

980
00:35:27,200 --> 00:35:29,440
malicious security against malicious

981
00:35:29,440 --> 00:35:31,760
evaluators right

982
00:35:31,760 --> 00:35:34,000
so if you make the client uh become the

983
00:35:34,000 --> 00:35:35,680
evaluator then we already obtain

984
00:35:35,680 --> 00:35:37,200
security against

985
00:35:37,200 --> 00:35:39,920
uh in the in the non-linear layers for

986
00:35:39,920 --> 00:35:42,640
free in some sense

987
00:35:42,640 --> 00:35:44,720
but actually that's not quite sufficient

988
00:35:44,720 --> 00:35:46,800
because if you remember in delphi we

989
00:35:46,800 --> 00:35:49,040
actually feed the output of the linear

990
00:35:49,040 --> 00:35:50,560
preprocessing into the non-linear

991
00:35:50,560 --> 00:35:52,960
preprocessing so what we need to do is

992
00:35:52,960 --> 00:35:54,800
actually make sure that these are

993
00:35:54,800 --> 00:35:56,400
consistent as well and we're not doing

994
00:35:56,400 --> 00:35:58,720
that right now because

995
00:35:58,720 --> 00:36:00,400
oblivious transfer can't actually

996
00:36:00,400 --> 00:36:02,160
authenticate this it can't check that

997
00:36:02,160 --> 00:36:04,079
the input is consistent

998
00:36:04,079 --> 00:36:06,320
so to overcome this we design a protocol

999
00:36:06,320 --> 00:36:08,640
for conditional disclosure of secrets

1000
00:36:08,640 --> 00:36:11,440
which first checks if the input is valid

1001
00:36:11,440 --> 00:36:13,599
that the mac is correct and if so it

1002
00:36:13,599 --> 00:36:15,040
outputs the global circuit labels that

1003
00:36:15,040 --> 00:36:17,200
correspond to the client's input

1004
00:36:17,200 --> 00:36:18,960
and then by doing this basically we were

1005
00:36:18,960 --> 00:36:21,599
able to authenticate both the

1006
00:36:21,599 --> 00:36:23,520
linear and non-linear layers

1007
00:36:23,520 --> 00:36:24,960
and

1008
00:36:24,960 --> 00:36:26,400
obtain security against malicious

1009
00:36:26,400 --> 00:36:28,800
clients so that was muse in a you know

1010
00:36:28,800 --> 00:36:30,160
very rapid

1011
00:36:30,160 --> 00:36:31,200
uh

1012
00:36:31,200 --> 00:36:33,520
yeah yeah i covered it very rapidly

1013
00:36:33,520 --> 00:36:35,040
um

1014
00:36:35,040 --> 00:36:36,400
the cool thing is that the online phase

1015
00:36:36,400 --> 00:36:38,320
is basically equivalent to semi honest

1016
00:36:38,320 --> 00:36:40,160
uh delphi there's only a very small

1017
00:36:40,160 --> 00:36:41,119
overhead

1018
00:36:41,119 --> 00:36:43,599
um we implemented our protocol um it's

1019
00:36:43,599 --> 00:36:46,160
available online here

1020
00:36:46,160 --> 00:36:48,480
um yeah and in terms of evaluation we

1021
00:36:48,480 --> 00:36:50,100
compared it against

1022
00:36:50,100 --> 00:36:51,599
[Music]

1023
00:36:51,599 --> 00:36:53,920
a couple of baselines both semi honest

1024
00:36:53,920 --> 00:36:55,359
and fully malicious

1025
00:36:55,359 --> 00:36:57,280
and what we found was that compared to

1026
00:36:57,280 --> 00:36:59,280
the fully malicious we have very low

1027
00:36:59,280 --> 00:37:01,599
overhead 21x better and we're only

1028
00:37:01,599 --> 00:37:03,680
slightly worse than

1029
00:37:03,680 --> 00:37:06,240
the semi honest baseline in delphi we

1030
00:37:06,240 --> 00:37:07,839
pay in communication but i think in the

1031
00:37:07,839 --> 00:37:10,079
future work we can try to reduce this

1032
00:37:10,079 --> 00:37:11,839
and this is also true for the online

1033
00:37:11,839 --> 00:37:12,720
phase

1034
00:37:12,720 --> 00:37:14,160
again we have uh only a little bit

1035
00:37:14,160 --> 00:37:17,280
overhead over the semi honest protocols

1036
00:37:17,280 --> 00:37:19,200
yeah so in summary in this in the stock

1037
00:37:19,200 --> 00:37:20,480
i

1038
00:37:20,480 --> 00:37:21,920
yeah in news we developed a new model

1039
00:37:21,920 --> 00:37:23,599
extraction attack that is much more

1040
00:37:23,599 --> 00:37:25,599
efficient than existing ones

1041
00:37:25,599 --> 00:37:28,079
um and then we developed a secure

1042
00:37:28,079 --> 00:37:29,520
reference protocol that is not

1043
00:37:29,520 --> 00:37:31,520
vulnerable to our attack and particular

1044
00:37:31,520 --> 00:37:34,000
provides client malicious security

1045
00:37:34,000 --> 00:37:34,839
thank

1046
00:37:34,839 --> 00:37:36,400
you

1047
00:37:36,400 --> 00:37:38,960
okay thank you very much pratyush uh we

1048
00:37:38,960 --> 00:37:41,680
have two questions in the chat uh amit's

1049
00:37:41,680 --> 00:37:44,000
first question asks how is the honest

1050
00:37:44,000 --> 00:37:46,720
client supposed to query

1051
00:37:46,720 --> 00:37:48,960
um the honest client supposed to query

1052
00:37:48,960 --> 00:37:51,839
in which case and uh

1053
00:37:51,839 --> 00:37:53,760
and the second question you ask is isn't

1054
00:37:53,760 --> 00:37:57,520
uh cbs just ot plus garbage circuit

1055
00:37:57,520 --> 00:37:59,280
um so cts actually we designed a

1056
00:37:59,280 --> 00:38:01,359
protocol not based on global circuits we

1057
00:38:01,359 --> 00:38:03,200
designed a protocol based on arithmetic

1058
00:38:03,200 --> 00:38:05,839
and circuit mpc so in particular

1059
00:38:05,839 --> 00:38:08,800
variant of speeds that is

1060
00:38:08,800 --> 00:38:11,280
we we modify it to only

1061
00:38:11,280 --> 00:38:12,720
have client malicious security which

1062
00:38:12,720 --> 00:38:14,240
that is which is sufficient for our

1063
00:38:14,240 --> 00:38:16,320
application um we could implement it

1064
00:38:16,320 --> 00:38:17,760
using ot plus global circuits but that

1065
00:38:17,760 --> 00:38:20,240
would be more we can't actually be more

1066
00:38:20,240 --> 00:38:21,680
inefficient

1067
00:38:21,680 --> 00:38:23,599
um i'm not sure like what the in what

1068
00:38:23,599 --> 00:38:26,640
context the first question was about but

1069
00:38:26,640 --> 00:38:28,160
if it's about the model extraction

1070
00:38:28,160 --> 00:38:30,079
attack um

1071
00:38:30,079 --> 00:38:31,680
in the model extraction like instead of

1072
00:38:31,680 --> 00:38:33,200
black box attacks you're just querying

1073
00:38:33,200 --> 00:38:35,119
inputs and there's no constraint on what

1074
00:38:35,119 --> 00:38:37,520
your input can be like i as a client can

1075
00:38:37,520 --> 00:38:39,440
make any malicious input

1076
00:38:39,440 --> 00:38:41,200
um

1077
00:38:41,200 --> 00:38:43,599
and so like there's no constraint there

1078
00:38:43,599 --> 00:38:45,920
in the in our particular attack um

1079
00:38:45,920 --> 00:38:47,520
[Music]

1080
00:38:47,520 --> 00:38:48,960
the client basically does some

1081
00:38:48,960 --> 00:38:50,880
undetectable tampering of the of the

1082
00:38:50,880 --> 00:38:52,960
shares um

1083
00:38:52,960 --> 00:38:54,800
that's how it works i can explain more

1084
00:38:54,800 --> 00:38:57,839
offline if there's any questions

1085
00:38:57,839 --> 00:38:59,280
okay

1086
00:38:59,280 --> 00:39:02,320
okay uh thank you very much uh we can

1087
00:39:02,320 --> 00:39:04,480
start uh you can go ahead with the

1088
00:39:04,480 --> 00:39:07,280
second with the next talk

1089
00:39:07,280 --> 00:39:10,320
uh the topic is cerebral a platform for

1090
00:39:10,320 --> 00:39:12,839
multi-party cryptography collaborative

1091
00:39:12,839 --> 00:39:17,520
learning and waking trend is going to

1092
00:39:17,520 --> 00:39:19,200
give us this talk

1093
00:39:19,200 --> 00:39:20,079
so

1094
00:39:20,079 --> 00:39:21,359
take it away

1095
00:39:21,359 --> 00:39:22,720
hi

1096
00:39:22,720 --> 00:39:25,440
uh so hi everyone um my name is whitten

1097
00:39:25,440 --> 00:39:27,520
chen and i'm presenting our world

1098
00:39:27,520 --> 00:39:28,400
cerebral

1099
00:39:28,400 --> 00:39:30,720
which is a platform for cryptographic

1100
00:39:30,720 --> 00:39:33,839
like collaborate collaborative learning

1101
00:39:33,839 --> 00:39:36,079
and this is a joint world where i

1102
00:39:36,079 --> 00:39:38,960
several of my colleagues

1103
00:39:38,960 --> 00:39:41,359
okay let me go to the next slide so the

1104
00:39:41,359 --> 00:39:43,760
motivation of our work is anti-money

1105
00:39:43,760 --> 00:39:46,079
laundering so we have been that are

1106
00:39:46,079 --> 00:39:49,200
responsible to predict and prevent money

1107
00:39:49,200 --> 00:39:51,359
laundering and they want to use machine

1108
00:39:51,359 --> 00:39:54,000
learning model to detect them but this

1109
00:39:54,000 --> 00:39:56,480
is actually quite challenging because

1110
00:39:56,480 --> 00:39:58,720
actually each of the like money transfer

1111
00:39:58,720 --> 00:40:01,520
involved usually involves two banks

1112
00:40:01,520 --> 00:40:03,680
and one single bank doesn't have all the

1113
00:40:03,680 --> 00:40:06,160
information that's necessary to do this

1114
00:40:06,160 --> 00:40:07,920
prediction

1115
00:40:07,920 --> 00:40:10,480
so one good idea is that maybe we can

1116
00:40:10,480 --> 00:40:13,280
have many bands to train a model using

1117
00:40:13,280 --> 00:40:15,760
their joint data set but in the real

1118
00:40:15,760 --> 00:40:17,520
world this is quite challenging just

1119
00:40:17,520 --> 00:40:20,400
because banks have many reason that they

1120
00:40:20,400 --> 00:40:22,800
cannot share the data due to privacy

1121
00:40:22,800 --> 00:40:25,119
concerns regulation and probably

1122
00:40:25,119 --> 00:40:26,960
competition

1123
00:40:26,960 --> 00:40:29,599
and so the question is simple is that

1124
00:40:29,599 --> 00:40:32,319
how to enable this organization to share

1125
00:40:32,319 --> 00:40:34,880
data for learning a model without

1126
00:40:34,880 --> 00:40:37,839
showing the data and people here already

1127
00:40:37,839 --> 00:40:39,760
think that's the answer is to use secure

1128
00:40:39,760 --> 00:40:41,520
multi-party computation

1129
00:40:41,520 --> 00:40:44,079
and always liberal is trying to have

1130
00:40:44,079 --> 00:40:46,319
like developer who are not expert in

1131
00:40:46,319 --> 00:40:48,960
cryptography to use secure multi-party

1132
00:40:48,960 --> 00:40:51,280
communication

1133
00:40:51,280 --> 00:40:53,200
okay so uh

1134
00:40:53,200 --> 00:40:55,119
how can we have the developer what are

1135
00:40:55,119 --> 00:40:57,520
the challenges that developer are facing

1136
00:40:57,520 --> 00:40:59,839
today so we find that the first thing

1137
00:40:59,839 --> 00:41:02,400
that developer might find challenging is

1138
00:41:02,400 --> 00:41:05,119
how to choose the right protocol and

1139
00:41:05,119 --> 00:41:07,920
implement that algorithm efficiently and

1140
00:41:07,920 --> 00:41:09,920
now let me let's assume that we have a

1141
00:41:09,920 --> 00:41:12,480
developer who has a very cool learning

1142
00:41:12,480 --> 00:41:13,599
algorithm

1143
00:41:13,599 --> 00:41:15,680
the first question that the developer

1144
00:41:15,680 --> 00:41:17,680
need to answer is

1145
00:41:17,680 --> 00:41:20,560
well should i implement the algorithm

1146
00:41:20,560 --> 00:41:23,200
using boolean npc protocol or arithmetic

1147
00:41:23,200 --> 00:41:25,119
mpc protocol

1148
00:41:25,119 --> 00:41:27,920
like we know a little bit more about mpc

1149
00:41:27,920 --> 00:41:29,680
we know that like well they have some

1150
00:41:29,680 --> 00:41:30,640
trailer

1151
00:41:30,640 --> 00:41:33,119
but unfortunately the developer doesn't

1152
00:41:33,119 --> 00:41:35,920
know this trail and if we explain this

1153
00:41:35,920 --> 00:41:37,839
trail it's also very difficult for the

1154
00:41:37,839 --> 00:41:39,920
developer to figure out the differences

1155
00:41:39,920 --> 00:41:41,359
between them

1156
00:41:41,359 --> 00:41:44,400
and aries and the like debate between

1157
00:41:44,400 --> 00:41:46,319
like boolean and arithmetic is just the

1158
00:41:46,319 --> 00:41:48,079
beginning of the story because there are

1159
00:41:48,079 --> 00:41:50,480
so many protocols for boolean and

1160
00:41:50,480 --> 00:41:52,720
arithmetic and we know that they perform

1161
00:41:52,720 --> 00:41:55,839
very differently for example we we have

1162
00:41:55,839 --> 00:41:57,839
some heuristic like for example we know

1163
00:41:57,839 --> 00:42:00,160
silent ot is good

1164
00:42:00,160 --> 00:42:02,160
when you have a slow network and low

1165
00:42:02,160 --> 00:42:04,240
gear is very good when you have lower

1166
00:42:04,240 --> 00:42:05,680
number of the party

1167
00:42:05,680 --> 00:42:08,000
but once again our developer doesn't

1168
00:42:08,000 --> 00:42:11,280
know this and in the real world what may

1169
00:42:11,280 --> 00:42:13,280
happen actually usually is that the

1170
00:42:13,280 --> 00:42:16,000
developer who is an expert in data

1171
00:42:16,000 --> 00:42:18,480
science but probably non-expert in

1172
00:42:18,480 --> 00:42:20,000
cryptography

1173
00:42:20,000 --> 00:42:22,000
implement the same protocol in different

1174
00:42:22,000 --> 00:42:24,560
platforms and to measure how slow they

1175
00:42:24,560 --> 00:42:26,800
are and then they will learn the same

1176
00:42:26,800 --> 00:42:29,040
lesson that we show here but in a very

1177
00:42:29,040 --> 00:42:30,800
painful way

1178
00:42:30,800 --> 00:42:33,119
and moreover in the real world actually

1179
00:42:33,119 --> 00:42:35,440
in a lot of the learning algorithm some

1180
00:42:35,440 --> 00:42:37,920
of the computation can be done local

1181
00:42:37,920 --> 00:42:39,680
that is it doesn't need to be inside the

1182
00:42:39,680 --> 00:42:40,800
mpc

1183
00:42:40,800 --> 00:42:42,880
and if you are training the model in

1184
00:42:42,880 --> 00:42:44,800
your semi-only setting

1185
00:42:44,800 --> 00:42:46,960
this is a very important opportunity

1186
00:42:46,960 --> 00:42:49,040
that you don't want to miss but once

1187
00:42:49,040 --> 00:42:51,680
again the developer might not be able to

1188
00:42:51,680 --> 00:42:53,839
figure out like what is the right local

1189
00:42:53,839 --> 00:42:56,000
computation to do

1190
00:42:56,000 --> 00:42:59,040
and so uh cerebral provides a compiler

1191
00:42:59,040 --> 00:43:01,760
that tried to help the developer and

1192
00:43:01,760 --> 00:43:04,000
there is and uh actually two ways that

1193
00:43:04,000 --> 00:43:06,319
we have that firstly is that like well

1194
00:43:06,319 --> 00:43:08,400
our compiler can firstly using

1195
00:43:08,400 --> 00:43:11,359
information flow to see which variable

1196
00:43:11,359 --> 00:43:13,599
in the learning algorithm is like

1197
00:43:13,599 --> 00:43:16,160
visible to some party and by doing that

1198
00:43:16,160 --> 00:43:18,240
we can split the program for the

1199
00:43:18,240 --> 00:43:20,800
developer into a local one so it doesn't

1200
00:43:20,800 --> 00:43:23,119
need to go to the npc

1201
00:43:23,119 --> 00:43:26,160
second is that we were estimated the

1202
00:43:26,160 --> 00:43:29,440
cost of different options to run the mpc

1203
00:43:29,440 --> 00:43:30,560
protocol

1204
00:43:30,560 --> 00:43:31,440
and

1205
00:43:31,440 --> 00:43:33,200
then we will choose the best one for the

1206
00:43:33,200 --> 00:43:35,440
developer and this is not a simple

1207
00:43:35,440 --> 00:43:38,000
decision because whether protocol is

1208
00:43:38,000 --> 00:43:40,079
faster or not actually depends on many

1209
00:43:40,079 --> 00:43:42,480
factors including whether you have good

1210
00:43:42,480 --> 00:43:44,000
network whether you have a lot of

1211
00:43:44,000 --> 00:43:45,680
computational power

1212
00:43:45,680 --> 00:43:48,640
one example is that if you are thinking

1213
00:43:48,640 --> 00:43:51,359
about silent ot but you also have very

1214
00:43:51,359 --> 00:43:53,359
good network maybe you will just do

1215
00:43:53,359 --> 00:43:56,400
non-silent and classical classical ot

1216
00:43:56,400 --> 00:43:58,000
instead

1217
00:43:58,000 --> 00:44:00,000
and so and actually we have some

1218
00:44:00,000 --> 00:44:02,400
observation sometimes this compiler can

1219
00:44:02,400 --> 00:44:04,960
tell you something that you don't know

1220
00:44:04,960 --> 00:44:05,839
like

1221
00:44:05,839 --> 00:44:08,800
so let's look at this figure the first

1222
00:44:08,800 --> 00:44:10,400
observation that we can have is that

1223
00:44:10,400 --> 00:44:12,319
while it's really important to choose

1224
00:44:12,319 --> 00:44:14,720
the right protocol because there is a

1225
00:44:14,720 --> 00:44:16,640
performance gap that's actually not

1226
00:44:16,640 --> 00:44:17,599
small

1227
00:44:17,599 --> 00:44:18,720
and second

1228
00:44:18,720 --> 00:44:21,200
so our heuristic on which protocol is

1229
00:44:21,200 --> 00:44:24,079
better might not be always true like in

1230
00:44:24,079 --> 00:44:26,960
this figure which will actually evaluate

1231
00:44:26,960 --> 00:44:29,280
a 10 layer decision tree in the seminar

1232
00:44:29,280 --> 00:44:31,680
on the setting you can see that like

1233
00:44:31,680 --> 00:44:33,920
when you when you have more party

1234
00:44:33,920 --> 00:44:36,079
subsequently arithmetic protocol becomes

1235
00:44:36,079 --> 00:44:39,760
faster than the boolean one

1236
00:44:39,760 --> 00:44:43,119
and we and we and then we solve another

1237
00:44:43,119 --> 00:44:45,040
challenge that like some of the like

1238
00:44:45,040 --> 00:44:47,520
developer might also face in developing

1239
00:44:47,520 --> 00:44:50,319
like efficient protocol in real world

1240
00:44:50,319 --> 00:44:52,640
when we are doing training we may

1241
00:44:52,640 --> 00:44:55,520
actually have like as like basically

1242
00:44:55,520 --> 00:44:57,359
party might not be the same

1243
00:44:57,359 --> 00:44:59,280
and especially they might not have the

1244
00:44:59,280 --> 00:45:01,920
same network condition now let me give

1245
00:45:01,920 --> 00:45:04,640
you a specific example let's consider

1246
00:45:04,640 --> 00:45:07,920
that like we are doing a global npc

1247
00:45:07,920 --> 00:45:10,960
we have two parties who are in the us

1248
00:45:10,960 --> 00:45:13,119
and they have very good network between

1249
00:45:13,119 --> 00:45:15,520
them locate latency and very high

1250
00:45:15,520 --> 00:45:17,200
language

1251
00:45:17,200 --> 00:45:19,680
unfortunately we have the third party

1252
00:45:19,680 --> 00:45:21,760
who is far away from them

1253
00:45:21,760 --> 00:45:23,520
and the network between them is very

1254
00:45:23,520 --> 00:45:25,200
slow

1255
00:45:25,200 --> 00:45:27,520
which means that the latency is very

1256
00:45:27,520 --> 00:45:29,760
high and also the bandwidth is quite

1257
00:45:29,760 --> 00:45:30,960
slow

1258
00:45:30,960 --> 00:45:33,440
and this is a very challenging situation

1259
00:45:33,440 --> 00:45:35,920
today for our mpc and let me just give

1260
00:45:35,920 --> 00:45:38,400
you a specific example if we are doing

1261
00:45:38,400 --> 00:45:40,160
the semi on this version of the speech

1262
00:45:40,160 --> 00:45:41,359
protocol

1263
00:45:41,359 --> 00:45:43,760
a significant part of the offline phase

1264
00:45:43,760 --> 00:45:46,160
is actually to broadcast a lot of the

1265
00:45:46,160 --> 00:45:48,560
ciphertext and then the party will sum

1266
00:45:48,560 --> 00:45:50,720
this subtitles up

1267
00:45:50,720 --> 00:45:53,119
and as you can see because party 3 have

1268
00:45:53,119 --> 00:45:55,920
slower connection with everyone else

1269
00:45:55,920 --> 00:45:58,560
it means that like well actually party 1

1270
00:45:58,560 --> 00:46:00,560
and party 2 will be waiting for the

1271
00:46:00,560 --> 00:46:02,480
party 3 to finish

1272
00:46:02,480 --> 00:46:04,800
and this light will just slow down the

1273
00:46:04,800 --> 00:46:06,800
entire computation by the slow

1274
00:46:06,800 --> 00:46:08,720
disconnection

1275
00:46:08,720 --> 00:46:11,040
and in other words reboot file that

1276
00:46:11,040 --> 00:46:13,760
there are ways to have the party 3. and

1277
00:46:13,760 --> 00:46:16,480
our idea is that we try to send and

1278
00:46:16,480 --> 00:46:18,800
receive last data to and from the party

1279
00:46:18,800 --> 00:46:21,440
threes the first thing we can do is that

1280
00:46:21,440 --> 00:46:24,079
let's send less data to party threes

1281
00:46:24,079 --> 00:46:26,480
if we are like broadcasting cyber class

1282
00:46:26,480 --> 00:46:28,880
but they will eventually be summing up

1283
00:46:28,880 --> 00:46:29,920
together

1284
00:46:29,920 --> 00:46:33,040
maybe party one and two can sum them up

1285
00:46:33,040 --> 00:46:35,119
first and then just send one server

1286
00:46:35,119 --> 00:46:36,800
class to the positive three instead of

1287
00:46:36,800 --> 00:46:39,119
sending them individually

1288
00:46:39,119 --> 00:46:42,400
second we can lab the position last data

1289
00:46:42,400 --> 00:46:44,560
to the party one and two by doing some

1290
00:46:44,560 --> 00:46:46,960
sort of forwarding and the idea is that

1291
00:46:46,960 --> 00:46:48,720
well if but we need to send the

1292
00:46:48,720 --> 00:46:50,880
subtitles to party one and two let's

1293
00:46:50,880 --> 00:46:52,480
just have the party do it saying to

1294
00:46:52,480 --> 00:46:54,960
party one where party one forwarded to

1295
00:46:54,960 --> 00:46:57,200
party 2.

1296
00:46:57,200 --> 00:46:59,359
and we did an experiment which actually

1297
00:46:59,359 --> 00:47:00,640
showed that this will give you an

1298
00:47:00,640 --> 00:47:03,040
improvement we did an experiment with 12

1299
00:47:03,040 --> 00:47:05,760
parties located in two different regions

1300
00:47:05,760 --> 00:47:07,599
inside the same region they have very

1301
00:47:07,599 --> 00:47:09,760
good network but between these two

1302
00:47:09,760 --> 00:47:13,040
regions there is a very slow connection

1303
00:47:13,040 --> 00:47:15,280
and we found that like by just using

1304
00:47:15,280 --> 00:47:17,839
this optimization that you try to not

1305
00:47:17,839 --> 00:47:20,160
send too much data in the slow this

1306
00:47:20,160 --> 00:47:22,720
connection you can actually get a very

1307
00:47:22,720 --> 00:47:24,559
significant improvement in the like

1308
00:47:24,559 --> 00:47:26,960
computation

1309
00:47:26,960 --> 00:47:29,440
and we and scribble also have other

1310
00:47:29,440 --> 00:47:31,040
components that i don't have time to

1311
00:47:31,040 --> 00:47:32,079
describe

1312
00:47:32,079 --> 00:47:34,559
you can see our paper for like our

1313
00:47:34,559 --> 00:47:36,640
domain specific language

1314
00:47:36,640 --> 00:47:39,680
we have some release policies that have

1315
00:47:39,680 --> 00:47:41,680
ensured parties incentive and we also

1316
00:47:41,680 --> 00:47:44,079
have an auditing framework to try to

1317
00:47:44,079 --> 00:47:46,640
have cash party who provide like bad

1318
00:47:46,640 --> 00:47:49,119
data to the training

1319
00:47:49,119 --> 00:47:52,079
and we also have released a lot of our

1320
00:47:52,079 --> 00:47:55,200
code and if you are working on this area

1321
00:47:55,200 --> 00:47:56,720
we actually implement a lot of the

1322
00:47:56,720 --> 00:47:58,480
example program for machine learning and

1323
00:47:58,480 --> 00:48:00,800
which might be quite helpful

1324
00:48:00,800 --> 00:48:03,119
and in the end i want to tell you a

1325
00:48:03,119 --> 00:48:05,119
little bit of some of the open problems

1326
00:48:05,119 --> 00:48:06,640
that come to our mind when we are

1327
00:48:06,640 --> 00:48:09,680
developing like cerebral and as you can

1328
00:48:09,680 --> 00:48:13,200
see from like the previous figure our

1329
00:48:13,200 --> 00:48:16,559
a very good life opportunity is that is

1330
00:48:16,559 --> 00:48:18,240
we miss the different types of the

1331
00:48:18,240 --> 00:48:20,400
protocol together we have so many

1332
00:48:20,400 --> 00:48:22,720
wonderful words in the recent year about

1333
00:48:22,720 --> 00:48:25,040
misprotocol computation

1334
00:48:25,040 --> 00:48:26,880
cerebral doesn't support such

1335
00:48:26,880 --> 00:48:29,520
computation it have to choose one of the

1336
00:48:29,520 --> 00:48:32,000
protocol either arithmetic or boolean

1337
00:48:32,000 --> 00:48:34,640
and this we actually we are losing a lot

1338
00:48:34,640 --> 00:48:36,400
of the opportunity to get better

1339
00:48:36,400 --> 00:48:38,960
performance so uh open problem our

1340
00:48:38,960 --> 00:48:41,440
future well that's people might actually

1341
00:48:41,440 --> 00:48:43,440
would quite be very useful is that

1342
00:48:43,440 --> 00:48:46,000
whether we can have a compiler that

1343
00:48:46,000 --> 00:48:48,880
automatically split the program into

1344
00:48:48,880 --> 00:48:50,480
different circuits

1345
00:48:50,480 --> 00:48:52,720
and you also have an optimizer that

1346
00:48:52,720 --> 00:48:54,880
actually try to lower the cost this is

1347
00:48:54,880 --> 00:48:56,960
not trivial because you now involve the

1348
00:48:56,960 --> 00:48:58,800
conversion and conversion have an

1349
00:48:58,800 --> 00:49:01,839
overhang it's not just to decide whether

1350
00:49:01,839 --> 00:49:03,599
or not it's also

1351
00:49:03,599 --> 00:49:05,040
you need to decide when to do the

1352
00:49:05,040 --> 00:49:06,960
conversion

1353
00:49:06,960 --> 00:49:09,040
and another open problem that some of

1354
00:49:09,040 --> 00:49:10,800
the audio might already know that is

1355
00:49:10,800 --> 00:49:12,559
that our protocol for the network

1356
00:49:12,559 --> 00:49:15,040
planning which try to have the party 3

1357
00:49:15,040 --> 00:49:17,520
who have a slower connection only work

1358
00:49:17,520 --> 00:49:19,760
in the semi-only setting and we don't

1359
00:49:19,760 --> 00:49:21,760
know how to do that in the malicious

1360
00:49:21,760 --> 00:49:24,240
setting we know that it's challenging

1361
00:49:24,240 --> 00:49:26,400
because that in the malicious setting

1362
00:49:26,400 --> 00:49:28,160
party often need to send out some sort

1363
00:49:28,160 --> 00:49:30,480
of proof of plain knowledge to ensure

1364
00:49:30,480 --> 00:49:32,160
that the plaintiffs are included

1365
00:49:32,160 --> 00:49:33,280
correctly

1366
00:49:33,280 --> 00:49:35,040
and also you need to prove that the

1367
00:49:35,040 --> 00:49:37,599
party actually aggregate the subtitles

1368
00:49:37,599 --> 00:49:40,240
correctly and we don't know how to do uh

1369
00:49:40,240 --> 00:49:42,720
idea that seems to work is more fit like

1370
00:49:42,720 --> 00:49:45,119
commitment but it must be efficient in

1371
00:49:45,119 --> 00:49:46,720
now otherwise it will become the

1372
00:49:46,720 --> 00:49:49,200
performance bottleneck

1373
00:49:49,200 --> 00:49:51,520
and okay and that's all i have and

1374
00:49:51,520 --> 00:49:54,000
thanks everybody for listening and i

1375
00:49:54,000 --> 00:49:55,920
actually i i think there is a question

1376
00:49:55,920 --> 00:49:59,839
in the chat i will take a look

1377
00:50:00,880 --> 00:50:02,880
um number of training sample institute

1378
00:50:02,880 --> 00:50:04,880
learning we so the compiler actually

1379
00:50:04,880 --> 00:50:06,319
needs to learn some of the information

1380
00:50:06,319 --> 00:50:08,319
about the data set because that can also

1381
00:50:08,319 --> 00:50:09,920
like influence how the decisions are

1382
00:50:09,920 --> 00:50:10,960
being made

1383
00:50:10,960 --> 00:50:12,640
yeah

1384
00:50:12,640 --> 00:50:14,960
okay

1385
00:50:14,960 --> 00:50:17,040
any other questions

1386
00:50:17,040 --> 00:50:19,119
yes i guess daniel

1387
00:50:19,119 --> 00:50:20,640
yeah yeah it's gonna be super quick

1388
00:50:20,640 --> 00:50:22,000
thank you so

1389
00:50:22,000 --> 00:50:24,480
just to be sure i i'm not sure i got it

1390
00:50:24,480 --> 00:50:26,079
like the full implementation you have

1391
00:50:26,079 --> 00:50:28,559
after it compiles the code that you

1392
00:50:28,559 --> 00:50:30,960
provide as input does it give like

1393
00:50:30,960 --> 00:50:32,800
recommendations on what to execute or

1394
00:50:32,800 --> 00:50:34,319
does it actually provide the full

1395
00:50:34,319 --> 00:50:36,720
execution environment to run the the

1396
00:50:36,720 --> 00:50:37,760
proper

1397
00:50:37,760 --> 00:50:39,200
mpc protocol

1398
00:50:39,200 --> 00:50:41,599
uh actually it's very close to that

1399
00:50:41,599 --> 00:50:43,280
because that the

1400
00:50:43,280 --> 00:50:45,359
so one very good thing about this

1401
00:50:45,359 --> 00:50:47,200
library is that the domain specific

1402
00:50:47,200 --> 00:50:49,440
language allow you to write in one

1403
00:50:49,440 --> 00:50:51,520
python file and then you can compile it

1404
00:50:51,520 --> 00:50:52,960
to both the boolean one and the

1405
00:50:52,960 --> 00:50:54,480
arithmetic one so you don't need to

1406
00:50:54,480 --> 00:50:56,559
write a program quite twice

1407
00:50:56,559 --> 00:50:58,559
but when the like when the clause lie

1408
00:50:58,559 --> 00:51:00,559
accidentally tell you which one seems to

1409
00:51:00,559 --> 00:51:02,880
have a lower cost you can then go ahead

1410
00:51:02,880 --> 00:51:04,960
and compile that into that format and

1411
00:51:04,960 --> 00:51:07,920
you don't need to write rewrite the code

1412
00:51:07,920 --> 00:51:09,760
so it's very close to that

1413
00:51:09,760 --> 00:51:12,640
that's easy thank you

1414
00:51:15,599 --> 00:51:18,160
okay so if there's any no any other

1415
00:51:18,160 --> 00:51:20,559
questions so we can go ahead with the

1416
00:51:20,559 --> 00:51:23,280
next talk uh the next talk will be

1417
00:51:23,280 --> 00:51:24,160
called

1418
00:51:24,160 --> 00:51:26,079
privacy preserving machine learning for

1419
00:51:26,079 --> 00:51:28,079
support vector machines

1420
00:51:28,079 --> 00:51:30,800
and hernan vanegas is going to give us

1421
00:51:30,800 --> 00:51:32,480
the uh talk

1422
00:51:32,480 --> 00:51:35,119
uh i would like to actually welcome anon

1423
00:51:35,119 --> 00:51:37,440
to the research community and especially

1424
00:51:37,440 --> 00:51:39,119
in pc i know that you are finishing up

1425
00:51:39,119 --> 00:51:42,319
your bachelor degree so kudos to you

1426
00:51:42,319 --> 00:51:45,040
and the floor is yours take it away

1427
00:51:45,040 --> 00:51:50,200
thank you and i will share my screen in

1428
00:51:51,359 --> 00:51:54,558
do you see my presentation

1429
00:51:55,839 --> 00:51:59,359
yes i see that oh okay

1430
00:51:59,540 --> 00:52:01,760
[Music]

1431
00:52:01,760 --> 00:52:02,880
okay

1432
00:52:02,880 --> 00:52:05,440
hey hi my name is hernandez baneras i

1433
00:52:05,440 --> 00:52:07,680
will be talking about privacy preserving

1434
00:52:07,680 --> 00:52:09,200
machine learning for support vector

1435
00:52:09,200 --> 00:52:11,440
machines in particular i will be talking

1436
00:52:11,440 --> 00:52:13,280
about how to use a multi-party

1437
00:52:13,280 --> 00:52:16,000
computation to train support vector

1438
00:52:16,000 --> 00:52:18,400
machines this work was

1439
00:52:18,400 --> 00:52:21,119
done jointly with daniel cavacas and

1440
00:52:21,119 --> 00:52:23,119
daniel escuero at this moment we are

1441
00:52:23,119 --> 00:52:24,880
working a full paper that will contain

1442
00:52:24,880 --> 00:52:28,000
all the details of this work

1443
00:52:28,000 --> 00:52:31,200
our contribution is divided in six parts

1444
00:52:31,200 --> 00:52:33,040
the first contribution is that we

1445
00:52:33,040 --> 00:52:35,119
implement uh three stm training

1446
00:52:35,119 --> 00:52:39,520
algorithms securely uh with the mpsp

1447
00:52:39,520 --> 00:52:42,319
framework a second

1448
00:52:42,319 --> 00:52:45,040
it is important to mention here that it

1449
00:52:45,040 --> 00:52:47,520
is a full-fledged implementation a

1450
00:52:47,520 --> 00:52:49,760
second we discuss the precision

1451
00:52:49,760 --> 00:52:51,359
and the scalability of the secure

1452
00:52:51,359 --> 00:52:54,000
implementation using the fantastic four

1453
00:52:54,000 --> 00:52:55,680
protocol

1454
00:52:55,680 --> 00:52:56,480
we

1455
00:52:56,480 --> 00:52:58,160
place a special tension in the impact of

1456
00:52:58,160 --> 00:52:59,839
the fix pointing the quality of the

1457
00:52:59,839 --> 00:53:02,960
model for example in the accuracy and

1458
00:53:02,960 --> 00:53:05,760
also the the training time and the data

1459
00:53:05,760 --> 00:53:07,359
sent and

1460
00:53:07,359 --> 00:53:09,200
we compute some upper bounds for the

1461
00:53:09,200 --> 00:53:11,599
least squares approach that are critical

1462
00:53:11,599 --> 00:53:14,480
to avoid the overflow when we use fixed

1463
00:53:14,480 --> 00:53:15,520
point

1464
00:53:15,520 --> 00:53:17,200
representation

1465
00:53:17,200 --> 00:53:20,079
we train an sdm with real world data to

1466
00:53:20,079 --> 00:53:23,200
analyze a develop the viability and the

1467
00:53:23,200 --> 00:53:23,920
the

1468
00:53:23,920 --> 00:53:27,599
scaling of our solution and finally we

1469
00:53:27,599 --> 00:53:30,240
obtain an accuracy comparable to clear

1470
00:53:30,240 --> 00:53:32,800
implementation

1471
00:53:32,800 --> 00:53:36,880
using a floating point that presentation

1472
00:53:36,880 --> 00:53:38,240
in the related

1473
00:53:38,240 --> 00:53:41,920
work we found only few works that uses

1474
00:53:41,920 --> 00:53:43,599
npc

1475
00:53:43,599 --> 00:53:46,800
to train a super vector machines most of

1476
00:53:46,800 --> 00:53:49,119
the works using we use homomorphic

1477
00:53:49,119 --> 00:53:51,359
encryption or do not carry the world

1478
00:53:51,359 --> 00:53:55,040
training process privately

1479
00:53:55,280 --> 00:53:58,000
now what is the picture of this work uh

1480
00:53:58,000 --> 00:54:00,480
in the first part we have a methods

1481
00:54:00,480 --> 00:54:02,960
method selection and we

1482
00:54:02,960 --> 00:54:05,280
train an svm using the gradient descent

1483
00:54:05,280 --> 00:54:07,280
algorithm the sequential minimal

1484
00:54:07,280 --> 00:54:09,760
optimization algorithm and they list the

1485
00:54:09,760 --> 00:54:12,480
squares all of them are implemented and

1486
00:54:12,480 --> 00:54:16,079
in an npc in in mp speeds framework

1487
00:54:16,079 --> 00:54:18,240
and we train an svm with these

1488
00:54:18,240 --> 00:54:20,319
algorithms and we select the best

1489
00:54:20,319 --> 00:54:22,079
sweetheart method according to some

1490
00:54:22,079 --> 00:54:23,200
metrics

1491
00:54:23,200 --> 00:54:25,920
in this case the least squares algorithm

1492
00:54:25,920 --> 00:54:28,319
performs better and we compute some

1493
00:54:28,319 --> 00:54:29,920
upper bounds

1494
00:54:29,920 --> 00:54:30,800
to

1495
00:54:30,800 --> 00:54:34,640
for for the least squares algorithm

1496
00:54:34,640 --> 00:54:38,799
the importance here is that a this this

1497
00:54:38,799 --> 00:54:44,079
upper bounds will help us to uh compute

1498
00:54:44,079 --> 00:54:45,119
the

1499
00:54:45,119 --> 00:54:46,799
number of beats needed in the fixed

1500
00:54:46,799 --> 00:54:49,680
point arithmetic

1501
00:54:49,760 --> 00:54:50,799
finally

1502
00:54:50,799 --> 00:54:53,520
we have an experimental evaluation over

1503
00:54:53,520 --> 00:54:57,119
synthetic data and real-world data

1504
00:54:57,119 --> 00:55:00,960
and we unveil some practical limitations

1505
00:55:00,960 --> 00:55:03,200
and we conclude about them practical

1506
00:55:03,200 --> 00:55:06,400
limitations like a training time a bit

1507
00:55:06,400 --> 00:55:08,079
sizing the in the fixed point

1508
00:55:08,079 --> 00:55:11,359
implementation and so on

1509
00:55:11,359 --> 00:55:12,480
now

1510
00:55:12,480 --> 00:55:15,200
in the training method a selection we

1511
00:55:15,200 --> 00:55:17,040
have the gradient descent algorithm the

1512
00:55:17,040 --> 00:55:19,520
sequential minimal optimization a

1513
00:55:19,520 --> 00:55:21,680
algorithm that is a very well-known

1514
00:55:21,680 --> 00:55:23,520
method for training super vector

1515
00:55:23,520 --> 00:55:25,599
machines and at least the squares

1516
00:55:25,599 --> 00:55:27,119
approach

1517
00:55:27,119 --> 00:55:30,640
it's important here to mention that

1518
00:55:30,640 --> 00:55:31,760
in the

1519
00:55:31,760 --> 00:55:34,240
in all the experiments of this work we

1520
00:55:34,240 --> 00:55:37,920
are training all of our super vector

1521
00:55:37,920 --> 00:55:39,040
machines

1522
00:55:39,040 --> 00:55:42,400
locally it is in a laptop that simulates

1523
00:55:42,400 --> 00:55:44,000
all the communication between the

1524
00:55:44,000 --> 00:55:46,960
parties but in the local host

1525
00:55:46,960 --> 00:55:49,440
so we generate an artificial data set

1526
00:55:49,440 --> 00:55:51,520
with 50 rows and two columns and we

1527
00:55:51,520 --> 00:55:53,599
found that the gradient descent

1528
00:55:53,599 --> 00:55:55,920
algorithm is the fastest

1529
00:55:55,920 --> 00:55:58,480
but it has some limitations because the

1530
00:55:58,480 --> 00:56:00,559
gradient decent algorithm does not

1531
00:56:00,559 --> 00:56:02,960
allows us to use the kernel trick and

1532
00:56:02,960 --> 00:56:06,319
the kernel trick is important uh to find

1533
00:56:06,319 --> 00:56:08,000
a non-linear boundaries in the

1534
00:56:08,000 --> 00:56:09,200
classification

1535
00:56:09,200 --> 00:56:12,000
uh algorithm and the sequential minimal

1536
00:56:12,000 --> 00:56:14,319
optimization is just this is load due to

1537
00:56:14,319 --> 00:56:16,400
some heuristic the the

1538
00:56:16,400 --> 00:56:19,280
heuristics uh causes some branching in

1539
00:56:19,280 --> 00:56:22,240
the source code and they and and these

1540
00:56:22,240 --> 00:56:24,240
branchings

1541
00:56:24,240 --> 00:56:28,319
slows down the npx then ps5 execution

1542
00:56:28,319 --> 00:56:31,280
so we select select the least squares

1543
00:56:31,280 --> 00:56:33,839
approach because it performs better than

1544
00:56:33,839 --> 00:56:36,079
the than this sequential minimal

1545
00:56:36,079 --> 00:56:38,559
optimization algorithm and has a

1546
00:56:38,559 --> 00:56:41,599
comparable accuracy

1547
00:56:41,599 --> 00:56:42,559
now

1548
00:56:42,559 --> 00:56:45,520
a the least squares approach in

1549
00:56:45,520 --> 00:56:47,599
the core of this method is to solve a

1550
00:56:47,599 --> 00:56:49,680
linear system of equations presented in

1551
00:56:49,680 --> 00:56:51,040
the equation one

1552
00:56:51,040 --> 00:56:53,200
to solve this a this linear system of

1553
00:56:53,200 --> 00:56:55,440
equations we select the the steepest

1554
00:56:55,440 --> 00:56:57,839
design algorithm that is very similar

1555
00:56:57,839 --> 00:57:00,640
and to the gradient descent algorithms

1556
00:57:00,640 --> 00:57:03,680
with some variation in the steep size uh

1557
00:57:03,680 --> 00:57:06,720
in the line 7 precisely we

1558
00:57:06,720 --> 00:57:09,359
compute the steep size the step sizing

1559
00:57:09,359 --> 00:57:11,760
the in the optimization step

1560
00:57:11,760 --> 00:57:13,119
so let's

1561
00:57:13,119 --> 00:57:15,760
take a look at this a quantity because

1562
00:57:15,760 --> 00:57:17,920
it is very important

1563
00:57:17,920 --> 00:57:19,040
a

1564
00:57:19,040 --> 00:57:21,200
the numerator and the denominator of

1565
00:57:21,200 --> 00:57:23,280
this quantity depends heavily on the

1566
00:57:23,280 --> 00:57:25,599
number of rows and the columns of the

1567
00:57:25,599 --> 00:57:26,960
data set

1568
00:57:26,960 --> 00:57:27,680
so

1569
00:57:27,680 --> 00:57:30,319
a it is important to to

1570
00:57:30,319 --> 00:57:32,720
compute an upper bound to avoid overflow

1571
00:57:32,720 --> 00:57:35,680
due during the training execution so

1572
00:57:35,680 --> 00:57:37,760
if we compute an upper bound for the

1573
00:57:37,760 --> 00:57:40,960
denominator we found that is it is of

1574
00:57:40,960 --> 00:57:44,160
the n to the seven c to the sixth so we

1575
00:57:44,160 --> 00:57:47,280
will use this this upper bound and the

1576
00:57:47,280 --> 00:57:49,760
logarithm of this upper bound

1577
00:57:49,760 --> 00:57:53,040
precisely to find the number of

1578
00:57:53,040 --> 00:57:56,960
bits that avoid the overflow

1579
00:57:56,960 --> 00:57:59,520
so if we look at this table we compute

1580
00:57:59,520 --> 00:58:00,400
uh

1581
00:58:00,400 --> 00:58:03,520
some bits needed the ring size of the

1582
00:58:03,520 --> 00:58:06,000
fantastic four protocol for some number

1583
00:58:06,000 --> 00:58:08,240
of rows and columns and we found that

1584
00:58:08,240 --> 00:58:10,880
for relatively small data sets uh there

1585
00:58:10,880 --> 00:58:12,160
is a

1586
00:58:12,160 --> 00:58:14,640
high a relatively high number of

1587
00:58:14,640 --> 00:58:16,079
fixed point bits

1588
00:58:16,079 --> 00:58:19,119
needed to avoid the overflow

1589
00:58:19,119 --> 00:58:19,920
now

1590
00:58:19,920 --> 00:58:23,440
let's a evaluate a

1591
00:58:23,440 --> 00:58:25,680
let's evaluate what happens

1592
00:58:25,680 --> 00:58:28,880
with the number of rows a and the number

1593
00:58:28,880 --> 00:58:30,400
of columns

1594
00:58:30,400 --> 00:58:32,319
of this data set

1595
00:58:32,319 --> 00:58:33,359
in

1596
00:58:33,359 --> 00:58:34,640
in this case

1597
00:58:34,640 --> 00:58:37,839
if we leave the number of columns

1598
00:58:37,839 --> 00:58:40,799
and we generate a multiple data sets

1599
00:58:40,799 --> 00:58:44,799
with a a variety of number of rows

1600
00:58:44,799 --> 00:58:47,200
we found that the training time and the

1601
00:58:47,200 --> 00:58:50,640
data send increases polynomially

1602
00:58:50,640 --> 00:58:53,839
with the number of rows of the dataset

1603
00:58:53,839 --> 00:58:55,520
and there is a linear relationship

1604
00:58:55,520 --> 00:58:57,440
between the number of products and the

1605
00:58:57,440 --> 00:59:00,079
data set and training time so it tells

1606
00:59:00,079 --> 00:59:02,640
us that most of the time

1607
00:59:02,640 --> 00:59:05,440
the the training time is spent

1608
00:59:05,440 --> 00:59:09,040
uh sending data and the local uh

1609
00:59:09,040 --> 00:59:12,559
computer computation is is

1610
00:59:12,559 --> 00:59:16,480
spent a a low number of

1611
00:59:16,480 --> 00:59:20,000
a long time so hey the most important

1612
00:59:20,000 --> 00:59:22,079
the most important results here is that

1613
00:59:22,079 --> 00:59:25,839
a there is a similar accuracy between a

1614
00:59:25,839 --> 00:59:28,799
training an svm securely

1615
00:59:28,799 --> 00:59:32,160
and training a nice vm in a clear

1616
00:59:32,160 --> 00:59:35,520
implementation using floating point a

1617
00:59:35,520 --> 00:59:37,599
arithmetic

1618
00:59:37,599 --> 00:59:40,799
and and this fact is not modified it

1619
00:59:40,799 --> 00:59:44,319
will increase the number of rows

1620
00:59:44,319 --> 00:59:46,559
and finally

1621
00:59:46,559 --> 00:59:48,960
what happens if we change the number of

1622
00:59:48,960 --> 00:59:50,000
columns

1623
00:59:50,000 --> 00:59:53,119
and we leave the number of rows fixed

1624
00:59:53,119 --> 00:59:56,240
and we generate artificial data sets to

1625
00:59:56,240 --> 00:59:58,799
do this and we found that the training

1626
00:59:58,799 --> 01:00:01,680
time timing increases almost linear

1627
01:00:01,680 --> 01:00:03,200
and uh

1628
01:00:03,200 --> 01:00:05,920
and the the gap in the between the fifth

1629
01:00:05,920 --> 01:00:08,640
and sixth sixth columns is

1630
01:00:08,640 --> 01:00:11,520
a technicality in the mpsp framework

1631
01:00:11,520 --> 01:00:13,359
with the local computation and the word

1632
01:00:13,359 --> 01:00:15,680
size for the local computation of the

1633
01:00:15,680 --> 01:00:18,160
protocol the accuracy here is not

1634
01:00:18,160 --> 01:00:20,079
affected as well as in the rose

1635
01:00:20,079 --> 01:00:21,440
experiment

1636
01:00:21,440 --> 01:00:22,799
finally and

1637
01:00:22,799 --> 01:00:24,799
maybe the one of the most important

1638
01:00:24,799 --> 01:00:28,000
results here is that what happens if we

1639
01:00:28,000 --> 01:00:30,319
leave the number of rows fixed the

1640
01:00:30,319 --> 01:00:32,799
number of columns fixed also

1641
01:00:32,799 --> 01:00:33,599
and

1642
01:00:33,599 --> 01:00:36,000
we change the separation between the two

1643
01:00:36,000 --> 01:00:38,160
classes that we want to separate in the

1644
01:00:38,160 --> 01:00:39,359
data set

1645
01:00:39,359 --> 01:00:42,240
we generate multiple artificial data

1646
01:00:42,240 --> 01:00:45,200
datasets for this and we found that the

1647
01:00:45,200 --> 01:00:47,040
training accuracy

1648
01:00:47,040 --> 01:00:48,960
in the secure implementation and the

1649
01:00:48,960 --> 01:00:52,480
clear implementation is very similar

1650
01:00:52,480 --> 01:00:54,640
the accuracy is not affected by the fact

1651
01:00:54,640 --> 01:00:57,280
that the training is done securely

1652
01:00:57,280 --> 01:01:00,400
so it is a good point in favor though to

1653
01:01:00,400 --> 01:01:03,040
to the use of multi-party computation to

1654
01:01:03,040 --> 01:01:05,359
train support vector machines with the

1655
01:01:05,359 --> 01:01:08,640
least squares approach

1656
01:01:08,880 --> 01:01:11,599
finally we train an svm with a real

1657
01:01:11,599 --> 01:01:14,640
world dataset a heart attack dataset

1658
01:01:14,640 --> 01:01:17,920
taken from cable with 303 samples and 13

1659
01:01:17,920 --> 01:01:18,880
features

1660
01:01:18,880 --> 01:01:22,400
we train the super vector machine with a

1661
01:01:22,400 --> 01:01:24,720
70 percent of the samples and 14

1662
01:01:24,720 --> 01:01:25,839
features

1663
01:01:25,839 --> 01:01:29,599
after some pre-processing

1664
01:01:30,079 --> 01:01:33,200
and we use a linear kernel here

1665
01:01:33,200 --> 01:01:36,319
so we found that the training accuracy

1666
01:01:36,319 --> 01:01:38,880
and the test accuracy are very similar

1667
01:01:38,880 --> 01:01:41,680
the compact the secure implementation is

1668
01:01:41,680 --> 01:01:43,920
very competitive with respect to the

1669
01:01:43,920 --> 01:01:46,720
clear implementation that uses uh

1670
01:01:46,720 --> 01:01:49,760
floating point arithmetic the data sent

1671
01:01:49,760 --> 01:01:53,599
is high it took some hundreds of

1672
01:01:53,599 --> 01:01:56,079
bits so we will talk about this in the

1673
01:01:56,079 --> 01:01:58,160
conclusions uh the the secure

1674
01:01:58,160 --> 01:02:00,240
implementation took more time that the

1675
01:02:00,240 --> 01:02:01,839
clear implementation bodies are very

1676
01:02:01,839 --> 01:02:04,160
well known result it's a trade-off

1677
01:02:04,160 --> 01:02:08,879
between security and and time

1678
01:02:09,039 --> 01:02:11,920
so a in the conclusion we have that that

1679
01:02:11,920 --> 01:02:14,480
yours is not affected by the fact that

1680
01:02:14,480 --> 01:02:17,200
that the training is done securely this

1681
01:02:17,200 --> 01:02:18,160
is an

1682
01:02:18,160 --> 01:02:21,280
important result i and we

1683
01:02:21,280 --> 01:02:24,880
we see this with artificial data set and

1684
01:02:24,880 --> 01:02:27,920
real world data set as well uh most of

1685
01:02:27,920 --> 01:02:30,160
the training time is spending sending

1686
01:02:30,160 --> 01:02:32,799
data it's spent sending data as we see

1687
01:02:32,799 --> 01:02:34,480
in the rows experiment

1688
01:02:34,480 --> 01:02:36,240
and finally the number of bits and the

1689
01:02:36,240 --> 01:02:38,480
ring size of the fantastic four protocol

1690
01:02:38,480 --> 01:02:40,880
depends heavily on the number of rows

1691
01:02:40,880 --> 01:02:43,680
and columns and we must take care here

1692
01:02:43,680 --> 01:02:46,400
because then the if we use another

1693
01:02:46,400 --> 01:02:48,559
polling another kernel

1694
01:02:48,559 --> 01:02:49,359
a

1695
01:02:49,359 --> 01:02:51,839
rather rather than linear kernel for

1696
01:02:51,839 --> 01:02:54,960
example a polynomial kernel of degree 3

1697
01:02:54,960 --> 01:02:57,039
we have that the the upper bound is

1698
01:02:57,039 --> 01:02:58,480
increased

1699
01:02:58,480 --> 01:03:00,720
in a considerably in a considerable

1700
01:03:00,720 --> 01:03:04,240
quantity see this is a problem because

1701
01:03:04,240 --> 01:03:06,079
it will increase the training time and

1702
01:03:06,079 --> 01:03:07,520
the data center in the protocol

1703
01:03:07,520 --> 01:03:10,000
execution in the least dispersed

1704
01:03:10,000 --> 01:03:11,280
approach

1705
01:03:11,280 --> 01:03:15,280
as a future work what happens if we use

1706
01:03:15,280 --> 01:03:18,160
a small fixed number of bits and we use

1707
01:03:18,160 --> 01:03:18,960
it

1708
01:03:18,960 --> 01:03:20,160
wisely

1709
01:03:20,160 --> 01:03:23,039
in in order to to compute

1710
01:03:23,039 --> 01:03:23,839
uh

1711
01:03:23,839 --> 01:03:26,480
the operations that cause an out of

1712
01:03:26,480 --> 01:03:29,680
range operation of another flow in the

1713
01:03:29,680 --> 01:03:32,000
in the least squares algorithm

1714
01:03:32,000 --> 01:03:34,799
and and this will help us

1715
01:03:34,799 --> 01:03:36,000
to to

1716
01:03:36,000 --> 01:03:39,280
to attack the problem of the of the

1717
01:03:39,280 --> 01:03:41,760
of the in creation of the increase of

1718
01:03:41,760 --> 01:03:43,599
beats

1719
01:03:43,599 --> 01:03:44,319
uh

1720
01:03:44,319 --> 01:03:46,799
all the experiments here were executed

1721
01:03:46,799 --> 01:03:48,400
locally what happens with a more

1722
01:03:48,400 --> 01:03:50,160
realistic environment for example a

1723
01:03:50,160 --> 01:03:52,720
distributed network when we consider a

1724
01:03:52,720 --> 01:03:55,839
bandwidth and other things that the data

1725
01:03:55,839 --> 01:03:58,720
the data send that took some hundreds of

1726
01:03:58,720 --> 01:04:01,440
gigabytes and the training time will be

1727
01:04:01,440 --> 01:04:03,119
affected as well thank you for

1728
01:04:03,119 --> 01:04:05,039
everything

1729
01:04:05,039 --> 01:04:07,280
okay thank you very much hernan uh since

1730
01:04:07,280 --> 01:04:09,839
we are having a lack of time

1731
01:04:09,839 --> 01:04:12,000
uh let's uh next move to the next

1732
01:04:12,000 --> 01:04:14,720
speaker uh the talk would be about

1733
01:04:14,720 --> 01:04:16,480
improved multi-party fixed point

1734
01:04:16,480 --> 01:04:17,920
multiplication

1735
01:04:17,920 --> 01:04:20,319
and hernan or other

1736
01:04:20,319 --> 01:04:21,760
authors of this

1737
01:04:21,760 --> 01:04:23,760
support vector machine work if you can

1738
01:04:23,760 --> 01:04:25,760
reply to the questions in the chat that

1739
01:04:25,760 --> 01:04:27,039
would be great

1740
01:04:27,039 --> 01:04:28,319
so

1741
01:04:28,319 --> 01:04:31,920
uh sai krishna badly nayaran

1742
01:04:31,920 --> 01:04:34,480
if i pronounce your name quite correctly

1743
01:04:34,480 --> 01:04:35,920
so please

1744
01:04:35,920 --> 01:04:38,480
floor is yours

1745
01:04:38,480 --> 01:04:39,920
let me just share my screen can you see

1746
01:04:39,920 --> 01:04:42,319
my screen

1747
01:04:42,400 --> 01:04:43,680
yes

1748
01:04:43,680 --> 01:04:45,039
um

1749
01:04:45,039 --> 01:04:47,039
so thanks for the introduction um my

1750
01:04:47,039 --> 01:04:49,200
name is saikrishna welcome to my talk

1751
01:04:49,200 --> 01:04:50,640
i'm going to be talking about improved

1752
01:04:50,640 --> 01:04:53,359
multiparty fixed point multiplication

1753
01:04:53,359 --> 01:04:55,359
and this is joint work with isa payhan

1754
01:04:55,359 --> 01:04:57,599
and peter

1755
01:04:57,599 --> 01:04:58,880
so first let's

1756
01:04:58,880 --> 01:05:00,559
look at our machine learning training as

1757
01:05:00,559 --> 01:05:02,079
or if you are aware through all the

1758
01:05:02,079 --> 01:05:04,160
talks today

1759
01:05:04,160 --> 01:05:06,079
typically in training we have data that

1760
01:05:06,079 --> 01:05:07,920
is stored across multiple different

1761
01:05:07,920 --> 01:05:09,039
sources

1762
01:05:09,039 --> 01:05:11,680
and each party could have very similar

1763
01:05:11,680 --> 01:05:15,280
or even different types of data and

1764
01:05:15,280 --> 01:05:16,960
ideally we'd like more data because more

1765
01:05:16,960 --> 01:05:19,520
data applies we can train a better model

1766
01:05:19,520 --> 01:05:21,760
but unfortunately due to

1767
01:05:21,760 --> 01:05:23,920
competitive or regulatory reasons

1768
01:05:23,920 --> 01:05:25,440
parties are not

1769
01:05:25,440 --> 01:05:27,760
authorized to share the data with each

1770
01:05:27,760 --> 01:05:29,839
other and um so

1771
01:05:29,839 --> 01:05:31,280
it's not uh

1772
01:05:31,280 --> 01:05:32,559
so we would still need some sort of

1773
01:05:32,559 --> 01:05:34,319
privacy preserving machine learning

1774
01:05:34,319 --> 01:05:37,440
training um and the similar similar

1775
01:05:37,440 --> 01:05:38,720
problem also arises in the case of

1776
01:05:38,720 --> 01:05:40,559
inference if we have a pre-trained model

1777
01:05:40,559 --> 01:05:42,480
that is shared across multiple parties

1778
01:05:42,480 --> 01:05:43,359
right

1779
01:05:43,359 --> 01:05:45,200
and as we roll away there's a long list

1780
01:05:45,200 --> 01:05:46,960
of works on using mpc to solve this

1781
01:05:46,960 --> 01:05:48,880
basically the different parties come

1782
01:05:48,880 --> 01:05:50,720
together and run an mpc protocol to

1783
01:05:50,720 --> 01:05:54,079
generate the trained model or to

1784
01:05:54,079 --> 01:05:55,680
to perform inference on the pre-trained

1785
01:05:55,680 --> 01:05:57,200
model

1786
01:05:57,200 --> 01:05:59,200
right

1787
01:05:59,200 --> 01:06:01,359
so much of the prior work in this space

1788
01:06:01,359 --> 01:06:02,720
of using mpc

1789
01:06:02,720 --> 01:06:04,559
has focused on the setting of two three

1790
01:06:04,559 --> 01:06:06,640
or maybe four parties with

1791
01:06:06,640 --> 01:06:08,240
at most one corruption

1792
01:06:08,240 --> 01:06:09,760
and the goal in our work is to extend

1793
01:06:09,760 --> 01:06:11,440
this to the end party settings so can we

1794
01:06:11,440 --> 01:06:13,680
design mpc protocols for secure training

1795
01:06:13,680 --> 01:06:14,880
or influence

1796
01:06:14,880 --> 01:06:18,160
with a larger number of parties

1797
01:06:18,160 --> 01:06:18,960
and

1798
01:06:18,960 --> 01:06:20,480
a natural question one might ask is

1799
01:06:20,480 --> 01:06:22,079
there has been a tremendous improvement

1800
01:06:22,079 --> 01:06:23,839
recently in

1801
01:06:23,839 --> 01:06:25,760
designing very efficient general purpose

1802
01:06:25,760 --> 01:06:27,920
npc protocols so can we just use those

1803
01:06:27,920 --> 01:06:30,799
protocols directly

1804
01:06:30,799 --> 01:06:33,359
um unfortunately it's not as simple as

1805
01:06:33,359 --> 01:06:35,440
that as um as we've seen in previous

1806
01:06:35,440 --> 01:06:37,039
talks machine learning typically

1807
01:06:37,039 --> 01:06:39,280
requires uh dealing with fractional or

1808
01:06:39,280 --> 01:06:42,160
decimal values while mpc

1809
01:06:42,160 --> 01:06:44,839
works well with modular arithmetic

1810
01:06:44,839 --> 01:06:47,680
um and so these two don't necessarily

1811
01:06:47,680 --> 01:06:48,880
fit together

1812
01:06:48,880 --> 01:06:51,520
uh natural naive ideas can we just scale

1813
01:06:51,520 --> 01:06:53,680
the machine learning computation to work

1814
01:06:53,680 --> 01:06:55,440
with a large enough modulus so that we

1815
01:06:55,440 --> 01:06:57,680
can do everything

1816
01:06:57,680 --> 01:06:59,359
over this modulus

1817
01:06:59,359 --> 01:07:00,960
and as you might expect this would fail

1818
01:07:00,960 --> 01:07:01,839
for

1819
01:07:01,839 --> 01:07:03,280
if you wanted to perform a large number

1820
01:07:03,280 --> 01:07:05,119
of multiplications it would just not

1821
01:07:05,119 --> 01:07:06,000
scale

1822
01:07:06,000 --> 01:07:07,839
well enough

1823
01:07:07,839 --> 01:07:10,000
and another idea could be that we just

1824
01:07:10,000 --> 01:07:11,520
perform the fixed point multiplication

1825
01:07:11,520 --> 01:07:12,960
and start using boolean circuits and

1826
01:07:12,960 --> 01:07:16,319
then run our favorite mpc algorithm

1827
01:07:16,319 --> 01:07:17,760
once again this would also be very

1828
01:07:17,760 --> 01:07:20,000
expensive either in terms of number of

1829
01:07:20,000 --> 01:07:21,760
rounds or communication depending on

1830
01:07:21,760 --> 01:07:25,440
which flavor of mpc that we use

1831
01:07:25,760 --> 01:07:28,720
and indeed a lot of the focus in prior

1832
01:07:28,720 --> 01:07:30,319
work has been

1833
01:07:30,319 --> 01:07:32,079
on building efficient fixed point

1834
01:07:32,079 --> 01:07:34,240
multiplication protocols where we want

1835
01:07:34,240 --> 01:07:35,920
the inputs and the output to be secret

1836
01:07:35,920 --> 01:07:37,920
shared amongst the parties

1837
01:07:37,920 --> 01:07:39,760
and in our book

1838
01:07:39,760 --> 01:07:41,280
the question we'd like to ask is can we

1839
01:07:41,280 --> 01:07:43,520
design efficient uh multi-party fixed

1840
01:07:43,520 --> 01:07:45,920
point multiplication protocols where the

1841
01:07:45,920 --> 01:07:47,599
inputs and outputs are secret shared and

1842
01:07:47,599 --> 01:07:50,000
we want it to work for larger values of

1843
01:07:50,000 --> 01:07:52,160
x

1844
01:07:52,799 --> 01:07:54,160
and

1845
01:07:54,160 --> 01:07:56,480
before i go into uh the results that we

1846
01:07:56,480 --> 01:07:57,440
obtained i'm just going to briefly

1847
01:07:57,440 --> 01:07:59,200
recall the notion of replicated secret

1848
01:07:59,200 --> 01:08:01,359
sharing um

1849
01:08:01,359 --> 01:08:02,880
maybe most of you already heard it but

1850
01:08:02,880 --> 01:08:05,039
just for the sake of completeness let's

1851
01:08:05,039 --> 01:08:06,480
uh let's say there are three parties

1852
01:08:06,480 --> 01:08:08,319
here and we want at most one party to be

1853
01:08:08,319 --> 01:08:10,240
corrupt the secret shadow value x we

1854
01:08:10,240 --> 01:08:11,760
just additively split it into three

1855
01:08:11,760 --> 01:08:14,480
components x1 x2 and x3 and we give each

1856
01:08:14,480 --> 01:08:16,238
party two of three two of these three

1857
01:08:16,238 --> 01:08:18,640
components right so p1 gets x1 and x2

1858
01:08:18,640 --> 01:08:19,920
and so on

1859
01:08:19,920 --> 01:08:21,679
um addition would be pretty easy to

1860
01:08:21,679 --> 01:08:23,359
perform you could just locally add your

1861
01:08:23,359 --> 01:08:25,040
respective shares if you have shares of

1862
01:08:25,040 --> 01:08:28,080
x and y to get the sharing of

1863
01:08:28,080 --> 01:08:29,359
the sum

1864
01:08:29,359 --> 01:08:32,080
unfortunately multiplication is not so

1865
01:08:32,080 --> 01:08:33,920
simple you cannot just directly multiply

1866
01:08:33,920 --> 01:08:35,839
your shares as you might expect

1867
01:08:35,839 --> 01:08:37,679
but if you just spend a bit of time

1868
01:08:37,679 --> 01:08:38,880
thinking about it you'll notice that

1869
01:08:38,880 --> 01:08:41,120
multiplication is also not too difficult

1870
01:08:41,120 --> 01:08:42,479
uh if you just wanted to do regular

1871
01:08:42,479 --> 01:08:44,880
multiplication each party can compute

1872
01:08:44,880 --> 01:08:47,439
one of these three terms ci locally

1873
01:08:47,439 --> 01:08:49,920
um so pi computes zi locally sends it to

1874
01:08:49,920 --> 01:08:52,799
pi plus 1 and this way parties get

1875
01:08:52,799 --> 01:08:53,920
dashes

1876
01:08:53,920 --> 01:08:55,120
you would need to add some additional

1877
01:08:55,120 --> 01:08:58,319
zero shares uh just for say privacy

1878
01:08:58,319 --> 01:09:00,238
okay so replicated secret sharing

1879
01:09:00,238 --> 01:09:02,158
provides you a nice way to

1880
01:09:02,158 --> 01:09:03,359
do

1881
01:09:03,359 --> 01:09:05,040
arithmetic computation

1882
01:09:05,040 --> 01:09:06,319
and this can be generalized to any

1883
01:09:06,319 --> 01:09:08,560
arbitrary and parties and it's a useful

1884
01:09:08,560 --> 01:09:10,158
way of secret sharing data if you care

1885
01:09:10,158 --> 01:09:14,479
about uh relatively small values of n

1886
01:09:14,479 --> 01:09:16,080
for larger n uh the number of shares

1887
01:09:16,080 --> 01:09:17,520
would grow exponentially so it might not

1888
01:09:17,520 --> 01:09:19,520
be suited

1889
01:09:19,520 --> 01:09:21,759
with that in mind let me briefly recap

1890
01:09:21,759 --> 01:09:24,158
our results um we design a new technique

1891
01:09:24,158 --> 01:09:26,640
for uh shared truncation what i mean is

1892
01:09:26,640 --> 01:09:28,399
if you have a sharing of x and now you

1893
01:09:28,399 --> 01:09:30,640
wanted to truncate it um

1894
01:09:30,640 --> 01:09:32,799
or if you think of x as a decimal value

1895
01:09:32,799 --> 01:09:35,520
and you want to divide this value x by 2

1896
01:09:35,520 --> 01:09:37,920
to the d for some public value d

1897
01:09:37,920 --> 01:09:40,479
so we design a new technique to perform

1898
01:09:40,479 --> 01:09:43,198
this task and we use this technique to

1899
01:09:43,198 --> 01:09:45,040
design efficient protocols for fixed

1900
01:09:45,040 --> 01:09:47,120
point multiplication

1901
01:09:47,120 --> 01:09:49,040
where the data's secret tried using

1902
01:09:49,040 --> 01:09:50,640
several different schemes

1903
01:09:50,640 --> 01:09:53,279
so um sorry the first scheme is the

1904
01:09:53,279 --> 01:09:54,640
first protocol works for replicated

1905
01:09:54,640 --> 01:09:57,199
secret sharing with semi honest security

1906
01:09:57,199 --> 01:09:58,880
uh the second protocol builds on the

1907
01:09:58,880 --> 01:10:00,239
first one and achieves malicious

1908
01:10:00,239 --> 01:10:02,080
security and both of these protocols

1909
01:10:02,080 --> 01:10:03,840
work in the honest majority setting the

1910
01:10:03,840 --> 01:10:05,600
first protocol has two flavors it can be

1911
01:10:05,600 --> 01:10:08,080
a single round one or a two round one

1912
01:10:08,080 --> 01:10:09,600
the third protocol is once again in the

1913
01:10:09,600 --> 01:10:11,440
semi honest security model with honest

1914
01:10:11,440 --> 01:10:14,239
maturity but works for me secret sharing

1915
01:10:14,239 --> 01:10:15,760
and finally the fourth protocol works

1916
01:10:15,760 --> 01:10:17,600
for additive secret sharing with semi

1917
01:10:17,600 --> 01:10:18,880
honest security

1918
01:10:18,880 --> 01:10:20,640
but we do not tolerate also a dishonest

1919
01:10:20,640 --> 01:10:23,600
majority of corruption

1920
01:10:24,159 --> 01:10:25,600
and then we plug in these protocols to

1921
01:10:25,600 --> 01:10:27,920
design general mpc protocols where the

1922
01:10:27,920 --> 01:10:29,440
arithmetic circuits can have addition

1923
01:10:29,440 --> 01:10:30,800
gates as well as fixed point

1924
01:10:30,800 --> 01:10:33,760
multiplication gates

1925
01:10:34,480 --> 01:10:36,718
um

1926
01:10:37,199 --> 01:10:39,280
so very briefly let me show you uh some

1927
01:10:39,280 --> 01:10:41,760
comparison of our protocol the semi

1928
01:10:41,760 --> 01:10:44,719
honest protocols with some prior work

1929
01:10:44,719 --> 01:10:46,239
let me use n to denote the number of

1930
01:10:46,239 --> 01:10:48,800
parties t to denote the corrupt parties

1931
01:10:48,800 --> 01:10:50,239
and let's focus on the replicated

1932
01:10:50,239 --> 01:10:52,400
setting where

1933
01:10:52,400 --> 01:10:54,960
let me call k to be the modulus

1934
01:10:54,960 --> 01:10:56,000
and so

1935
01:10:56,000 --> 01:10:58,960
our protocol performs better than um the

1936
01:10:58,960 --> 01:11:00,880
prior work of um

1937
01:11:00,880 --> 01:11:03,360
marcel and rendell uh where we unlike

1938
01:11:03,360 --> 01:11:05,120
their work we don't have any offline

1939
01:11:05,120 --> 01:11:07,199
communication or uh interaction between

1940
01:11:07,199 --> 01:11:09,040
the parties the online communication is

1941
01:11:09,040 --> 01:11:10,880
roughly identical except we have a minor

1942
01:11:10,880 --> 01:11:12,800
optimization that allows us to reduce

1943
01:11:12,800 --> 01:11:14,880
the communication uh to trade off for

1944
01:11:14,880 --> 01:11:16,640
more rounds

1945
01:11:16,640 --> 01:11:17,920
and then in the context of shaming

1946
01:11:17,920 --> 01:11:19,360
secret sharing once again we have a

1947
01:11:19,360 --> 01:11:21,280
protocol that works identically in the

1948
01:11:21,280 --> 01:11:23,120
online setting in terms of communication

1949
01:11:23,120 --> 01:11:24,239
where we improve on the offline

1950
01:11:24,239 --> 01:11:26,480
communication cost once again here k

1951
01:11:26,480 --> 01:11:28,960
denotes the log logarithm of the prime

1952
01:11:28,960 --> 01:11:30,159
modulus

1953
01:11:30,159 --> 01:11:31,520
so the takeaway message is that our

1954
01:11:31,520 --> 01:11:32,719
protocols

1955
01:11:32,719 --> 01:11:34,560
improve on prior work in terms of better

1956
01:11:34,560 --> 01:11:36,719
offline communication costs

1957
01:11:36,719 --> 01:11:38,800
and

1958
01:11:38,800 --> 01:11:40,080
the remaining few minutes i'm going to

1959
01:11:40,080 --> 01:11:41,840
use to briefly uh describe the

1960
01:11:41,840 --> 01:11:44,400
techniques in our replicated sharing

1961
01:11:44,400 --> 01:11:46,080
based protocol i just focused on the

1962
01:11:46,080 --> 01:11:50,000
semi honest setting for now um

1963
01:11:50,480 --> 01:11:52,320
so let me recall the approach taken by

1964
01:11:52,320 --> 01:11:54,800
marcel and rendell in the network title

1965
01:11:54,800 --> 01:11:56,640
aby three

1966
01:11:56,640 --> 01:11:58,400
so the idea is let's say you have three

1967
01:11:58,400 --> 01:12:00,640
parties uh and you have inputs x and y

1968
01:12:00,640 --> 01:12:02,800
that are replicated uh the shared using

1969
01:12:02,800 --> 01:12:04,719
replicated secret sharing and you want

1970
01:12:04,719 --> 01:12:07,280
the output z uh which is approximated as

1971
01:12:07,280 --> 01:12:08,880
x times y by

1972
01:12:08,880 --> 01:12:10,560
divided by this public

1973
01:12:10,560 --> 01:12:12,719
value d

1974
01:12:12,719 --> 01:12:14,560
so now in the pre-processing phase uh

1975
01:12:14,560 --> 01:12:16,960
parties interact to generate shares of r

1976
01:12:16,960 --> 01:12:17,840
and s

1977
01:12:17,840 --> 01:12:21,440
where r is random and s is computed as r

1978
01:12:21,440 --> 01:12:23,120
divided by two to the d

1979
01:12:23,120 --> 01:12:26,080
okay so r s is a truncation of r if you

1980
01:12:26,080 --> 01:12:28,880
can think of it that way

1981
01:12:28,880 --> 01:12:31,440
and so what happens in the online phase

1982
01:12:31,440 --> 01:12:33,679
first uh parties run the regular

1983
01:12:33,679 --> 01:12:35,520
multiplication protocol

1984
01:12:35,520 --> 01:12:38,320
that we saw briefly earlier to compute a

1985
01:12:38,320 --> 01:12:40,159
sharing of x times y

1986
01:12:40,159 --> 01:12:42,080
now notice that this sharing

1987
01:12:42,080 --> 01:12:44,400
needs to be truncated or divided by

1988
01:12:44,400 --> 01:12:45,840
d

1989
01:12:45,840 --> 01:12:48,800
um sorry divided by 2 to the d there's a

1990
01:12:48,800 --> 01:12:51,280
type of here

1991
01:12:52,159 --> 01:12:54,480
and the idea is that

1992
01:12:54,480 --> 01:12:55,920
instead of doing the division locally

1993
01:12:55,920 --> 01:12:57,920
anymore what parties do is they compute

1994
01:12:57,920 --> 01:12:59,040
arithmetic

1995
01:12:59,040 --> 01:13:01,440
additive secret shares of

1996
01:13:01,440 --> 01:13:04,080
this product subtracted by this value r

1997
01:13:04,080 --> 01:13:05,520
so they compute the rate of shares of x

1998
01:13:05,520 --> 01:13:07,840
y minus i and reveal all these shares to

1999
01:13:07,840 --> 01:13:11,120
the first two parties p1 and p2

2000
01:13:11,120 --> 01:13:13,760
so now p1 and p2 compute their share of

2001
01:13:13,760 --> 01:13:17,360
the product as z z prime divided by 2 to

2002
01:13:17,360 --> 01:13:19,760
the t so their share would be x y minus

2003
01:13:19,760 --> 01:13:22,480
r by 2 to the d and then the second and

2004
01:13:22,480 --> 01:13:24,400
the third shares could just be

2005
01:13:24,400 --> 01:13:26,320
the truncated shares of s

2006
01:13:26,320 --> 01:13:27,679
directly

2007
01:13:27,679 --> 01:13:29,199
and the idea is that now these three

2008
01:13:29,199 --> 01:13:30,640
shares would now

2009
01:13:30,640 --> 01:13:32,800
give you shares of x y by d and this

2010
01:13:32,800 --> 01:13:34,320
will be very close to the resulting

2011
01:13:34,320 --> 01:13:36,000
output that you would get for except

2012
01:13:36,000 --> 01:13:38,480
with some negligible error okay

2013
01:13:38,480 --> 01:13:40,239
and as you can imagine this can be

2014
01:13:40,239 --> 01:13:41,920
easily generalized to end parties as

2015
01:13:41,920 --> 01:13:43,440
well uh if you just expand on this

2016
01:13:43,440 --> 01:13:44,640
protocol and make it work for the

2017
01:13:44,640 --> 01:13:47,520
n-party setting um unfortunately in the

2018
01:13:47,520 --> 01:13:49,280
n-party setting this pre-processing is

2019
01:13:49,280 --> 01:13:51,120
pretty expensive because parties need to

2020
01:13:51,120 --> 01:13:52,960
interact together to compute these

2021
01:13:52,960 --> 01:13:54,320
random shares

2022
01:13:54,320 --> 01:13:57,440
and the goal in our work is um

2023
01:13:57,440 --> 01:13:58,960
can we generate these truncation pairs

2024
01:13:58,960 --> 01:14:00,480
locally without needing parties to

2025
01:14:00,480 --> 01:14:02,800
interact

2026
01:14:02,800 --> 01:14:04,239
and here is

2027
01:14:04,239 --> 01:14:05,440
quickly the brief idea for

2028
01:14:05,440 --> 01:14:07,920
non-interactive sampling um let's say we

2029
01:14:07,920 --> 01:14:10,159
have a value x that is in

2030
01:14:10,159 --> 01:14:11,920
the range c2 to the k

2031
01:14:11,920 --> 01:14:14,239
now instead of generating shares of r in

2032
01:14:14,239 --> 01:14:16,080
the same space what if the shares of r

2033
01:14:16,080 --> 01:14:17,920
were from a larger space so the idea is

2034
01:14:17,920 --> 01:14:19,679
if we generate each share of r to be

2035
01:14:19,679 --> 01:14:22,480
lambda bits more than the space in which

2036
01:14:22,480 --> 01:14:24,000
x is from

2037
01:14:24,000 --> 01:14:26,640
okay now we have a few observations

2038
01:14:26,640 --> 01:14:29,040
first suppose we're working over um

2039
01:14:29,040 --> 01:14:30,560
an even larger space so let's say we're

2040
01:14:30,560 --> 01:14:32,800
working over z 2 to the k prime where k

2041
01:14:32,800 --> 01:14:34,800
prime is larger than both k and k plus

2042
01:14:34,800 --> 01:14:36,159
lambda

2043
01:14:36,159 --> 01:14:38,560
so now recall that if you uh or observe

2044
01:14:38,560 --> 01:14:40,719
that if you reconstruct r using these

2045
01:14:40,719 --> 01:14:42,880
various shares ri

2046
01:14:42,880 --> 01:14:44,480
the reconstruction would not wrap around

2047
01:14:44,480 --> 01:14:47,440
c to the k prime

2048
01:14:47,440 --> 01:14:49,760
and so the distribution of r and r plus

2049
01:14:49,760 --> 01:14:52,400
x would be statistically close and

2050
01:14:52,400 --> 01:14:54,640
since r is lambda plus 2 which is more

2051
01:14:54,640 --> 01:14:55,760
than our

2052
01:14:55,760 --> 01:14:57,280
x

2053
01:14:57,280 --> 01:14:58,960
and finally

2054
01:14:58,960 --> 01:15:00,560
these shares of r can be generated

2055
01:15:00,560 --> 01:15:03,280
locally just by uh sharing prf keys

2056
01:15:03,280 --> 01:15:05,440
right

2057
01:15:06,480 --> 01:15:07,280
and

2058
01:15:07,280 --> 01:15:08,560
once you have these shares so far you

2059
01:15:08,560 --> 01:15:10,239
can just locally truncate each way to

2060
01:15:10,239 --> 01:15:12,080
get shares of r by two to the d uh

2061
01:15:12,080 --> 01:15:13,600
replicated shares and you don't need to

2062
01:15:13,600 --> 01:15:15,679
interact in the pre-processing at all

2063
01:15:15,679 --> 01:15:17,920
um

2064
01:15:17,920 --> 01:15:19,760
so with this idea let me show how to

2065
01:15:19,760 --> 01:15:21,920
plug in this idea uh into

2066
01:15:21,920 --> 01:15:23,760
a replicated sharing a fixed point

2067
01:15:23,760 --> 01:15:26,000
multiplication protocol that the

2068
01:15:26,000 --> 01:15:27,520
approach is very similar to what we saw

2069
01:15:27,520 --> 01:15:28,400
earlier

2070
01:15:28,400 --> 01:15:29,840
um

2071
01:15:29,840 --> 01:15:31,840
x y in the product z would be over the

2072
01:15:31,840 --> 01:15:33,760
space c two to the k but we'll be

2073
01:15:33,760 --> 01:15:35,679
working over z to the k prime where k

2074
01:15:35,679 --> 01:15:37,520
prime is larger than k

2075
01:15:37,520 --> 01:15:39,199
it need not be significantly larger so

2076
01:15:39,199 --> 01:15:41,600
you just need k k prime to be

2077
01:15:41,600 --> 01:15:44,640
k plus 2 lambda plus logarithmic of the

2078
01:15:44,640 --> 01:15:46,239
number of parties

2079
01:15:46,239 --> 01:15:47,600
um

2080
01:15:47,600 --> 01:15:49,120
m denotes the number of shares so if you

2081
01:15:49,120 --> 01:15:51,040
have n parties you would have n choose n

2082
01:15:51,040 --> 01:15:52,560
minus t shares

2083
01:15:52,560 --> 01:15:55,440
where it is the number of corruptions

2084
01:15:55,440 --> 01:15:57,280
and so the pre-processing parties just

2085
01:15:57,280 --> 01:16:00,080
jointly sample some prf keys which is

2086
01:16:00,080 --> 01:16:01,760
not expensive

2087
01:16:01,760 --> 01:16:03,600
um

2088
01:16:03,600 --> 01:16:04,400
and

2089
01:16:04,400 --> 01:16:06,000
it could be done locally whereas in the

2090
01:16:06,000 --> 01:16:07,679
online phase now just to generate shares

2091
01:16:07,679 --> 01:16:09,840
of these uh random strings are parties

2092
01:16:09,840 --> 01:16:11,760
locally compute uh their individual

2093
01:16:11,760 --> 01:16:14,320
shares all right and each share is uh at

2094
01:16:14,320 --> 01:16:17,040
most uh k plus lambda bits long

2095
01:16:17,040 --> 01:16:18,159
and then the rest of the protocol

2096
01:16:18,159 --> 01:16:20,960
follows quite similarly

2097
01:16:20,960 --> 01:16:22,880
but is compute additive shares of x y

2098
01:16:22,880 --> 01:16:23,920
minus r

2099
01:16:23,920 --> 01:16:25,360
the first

2100
01:16:25,360 --> 01:16:27,920
n minus t parties

2101
01:16:27,920 --> 01:16:31,199
get get all of these shares um they they

2102
01:16:31,199 --> 01:16:33,760
can then reconstruct to compute c prime

2103
01:16:33,760 --> 01:16:35,040
and they set the first share of the

2104
01:16:35,040 --> 01:16:37,120
product to just be z prime by 2 to the d

2105
01:16:37,120 --> 01:16:39,280
plus r 1 by 2 to the d where r 1 was the

2106
01:16:39,280 --> 01:16:41,600
first share of the randomness i and then

2107
01:16:41,600 --> 01:16:42,880
the rest of the shares of the product

2108
01:16:42,880 --> 01:16:45,199
are just truncations of the randomness

2109
01:16:45,199 --> 01:16:46,800
respectively

2110
01:16:46,800 --> 01:16:49,360
um i won't have time to go into the

2111
01:16:49,360 --> 01:16:52,400
details of the malicious protocol

2112
01:16:52,400 --> 01:16:53,840
but the protocol sort of builds on this

2113
01:16:53,840 --> 01:16:55,679
to enforce honest behavior across these

2114
01:16:55,679 --> 01:16:57,920
steps and one more thing to note here is

2115
01:16:57,920 --> 01:17:00,719
we also want to add some zero sharing to

2116
01:17:00,719 --> 01:17:02,960
ensure privacy of these additive shares

2117
01:17:02,960 --> 01:17:04,320
and once again you can add these zero

2118
01:17:04,320 --> 01:17:06,159
sharing by using prf keys and locally

2119
01:17:06,159 --> 01:17:07,840
generate them

2120
01:17:07,840 --> 01:17:09,840
and uh how do you modify this to a two

2121
01:17:09,840 --> 01:17:11,840
round protocol to reduce communication

2122
01:17:11,840 --> 01:17:14,480
um you just don't reveal every share of

2123
01:17:14,480 --> 01:17:16,400
these additive shares to all the parties

2124
01:17:16,400 --> 01:17:18,000
but just reveal them to one party and

2125
01:17:18,000 --> 01:17:19,679
have that party uh

2126
01:17:19,679 --> 01:17:22,320
compute z1 and send it to the others

2127
01:17:22,320 --> 01:17:24,000
um

2128
01:17:24,000 --> 01:17:25,679
yeah i'd like to stop here

2129
01:17:25,679 --> 01:17:29,080
thank you for listening

2130
01:17:30,719 --> 01:17:34,000
okay thank you very much um

2131
01:17:34,000 --> 01:17:36,080
i guess we are done with all the talks

2132
01:17:36,080 --> 01:17:36,960
for

2133
01:17:36,960 --> 01:17:39,440
the workshop i would like to pass the

2134
01:17:39,440 --> 01:17:40,800
mic to

2135
01:17:40,800 --> 01:17:43,360
antiguani if she is available

2136
01:17:43,360 --> 01:17:44,800
so i would like to thank all the

2137
01:17:44,800 --> 01:17:47,520
speakers uh in the whole workshop and

2138
01:17:47,520 --> 01:17:48,719
also

2139
01:17:48,719 --> 01:17:51,520
this country attack session

2140
01:17:51,520 --> 01:17:52,880
thanks everyone

2141
01:17:52,880 --> 01:17:55,600
and antiguani the mic is yours your

2142
01:17:55,600 --> 01:17:58,320
music

