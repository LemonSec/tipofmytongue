1
00:00:07,680 --> 00:00:10,320
so we're very excited to have nicolas

2
00:00:10,320 --> 00:00:11,599
paper not

3
00:00:11,599 --> 00:00:13,519
uh he comes from the massilloni

4
00:00:13,519 --> 00:00:15,440
community i'm not an expert in muscle

5
00:00:15,440 --> 00:00:17,039
learning but i started learning from his

6
00:00:17,039 --> 00:00:18,080
papers

7
00:00:18,080 --> 00:00:20,000
so we're super excited that he's here

8
00:00:20,000 --> 00:00:22,480
with us today let me introduce him

9
00:00:22,480 --> 00:00:24,800
briefly so he is currently an assistant

10
00:00:24,800 --> 00:00:27,039
professor at university of toronto he's

11
00:00:27,039 --> 00:00:28,960
also a faculty member at the vector

12
00:00:28,960 --> 00:00:32,880
institutes where he has a canada c4ai

13
00:00:32,880 --> 00:00:34,160
chair

14
00:00:34,160 --> 00:00:36,079
and also he's a faculty affiliated at

15
00:00:36,079 --> 00:00:37,120
schroeder's

16
00:00:37,120 --> 00:00:39,280
raceman institutes

17
00:00:39,280 --> 00:00:41,440
and he works on security and privacy of

18
00:00:41,440 --> 00:00:43,360
machine learning

19
00:00:43,360 --> 00:00:45,920
also he is a conor researcher

20
00:00:45,920 --> 00:00:48,800
and he was also a google pc fellow

21
00:00:48,800 --> 00:00:51,760
and he also his work also received the

22
00:00:51,760 --> 00:00:53,920
best paper awards

23
00:00:53,920 --> 00:00:55,520
and he also ended his periods in

24
00:00:55,520 --> 00:00:57,760
pennsylvania state

25
00:00:57,760 --> 00:00:59,039
and he

26
00:00:59,039 --> 00:01:01,520
also like um he was a research science

27
00:01:01,520 --> 00:01:03,760
google brain and we're very excited to

28
00:01:03,760 --> 00:01:06,640
have him here i will repeat so

29
00:01:06,640 --> 00:01:08,960
nicolas we can start

30
00:01:08,960 --> 00:01:11,200
thank you antigone i should start my

31
00:01:11,200 --> 00:01:14,000
talk with three disclaimers the first

32
00:01:14,000 --> 00:01:15,520
one is i'm giving it from a poor

33
00:01:15,520 --> 00:01:17,200
internet connection so feel free to

34
00:01:17,200 --> 00:01:19,360
interrupt me if uh the connection gets

35
00:01:19,360 --> 00:01:20,560
too bad

36
00:01:20,560 --> 00:01:22,799
uh the second one is as you said that

37
00:01:22,799 --> 00:01:24,159
i'm coming from the machine learning

38
00:01:24,159 --> 00:01:25,520
community

39
00:01:25,520 --> 00:01:27,840
so i do not pretend uh to be a crypto

40
00:01:27,840 --> 00:01:30,000
expert and i'll i'll try instead in this

41
00:01:30,000 --> 00:01:33,040
talk to give you uh flavors of problems

42
00:01:33,040 --> 00:01:34,880
that i think would benefit from more

43
00:01:34,880 --> 00:01:38,079
input from the cryptography community

44
00:01:38,079 --> 00:01:39,840
and then the last one is that this is

45
00:01:39,840 --> 00:01:42,400
work that i

46
00:01:42,799 --> 00:01:44,479
on a collab

47
00:01:44,479 --> 00:01:46,399
include instead of the use of toronto uh

48
00:01:46,399 --> 00:01:48,720
so i indicate the title of the paper at

49
00:01:48,720 --> 00:01:50,399
the bottom of some of my slides so you

50
00:01:50,399 --> 00:01:52,240
can reverse engineer who contributed to

51
00:01:52,240 --> 00:01:53,520
the work

52
00:01:53,520 --> 00:01:56,079
and credit them

53
00:01:56,079 --> 00:01:59,200
so i wanted to start the talk by giving

54
00:01:59,200 --> 00:02:00,960
an overview of what i mean by

55
00:02:00,960 --> 00:02:02,240
trustworthy machine learning because

56
00:02:02,240 --> 00:02:03,920
this is a term that is sort of used in

57
00:02:03,920 --> 00:02:07,040
many contexts and so here i i wanted to

58
00:02:07,040 --> 00:02:09,360
start with security itself which

59
00:02:09,360 --> 00:02:11,599
typically comes in three flavors

60
00:02:11,599 --> 00:02:13,360
confidentiality integrity and

61
00:02:13,360 --> 00:02:16,560
availability and by now we've seen uh we

62
00:02:16,560 --> 00:02:18,879
have enough insight to

63
00:02:18,879 --> 00:02:20,400
to note that

64
00:02:20,400 --> 00:02:22,160
just like any other computer systems

65
00:02:22,160 --> 00:02:23,760
machine learning systems

66
00:02:23,760 --> 00:02:25,680
are exposed to attacks under each of

67
00:02:25,680 --> 00:02:28,599
these uh aspects so if we look at

68
00:02:28,599 --> 00:02:31,280
confidentiality the model itself could

69
00:02:31,280 --> 00:02:33,680
be a valuable asset especially when

70
00:02:33,680 --> 00:02:34,959
you're

71
00:02:34,959 --> 00:02:37,120
training it with a lot of data and

72
00:02:37,120 --> 00:02:39,760
compute resources so i'll spend

73
00:02:39,760 --> 00:02:42,160
the second half of my talk discussing

74
00:02:42,160 --> 00:02:43,840
model stealing attacks and how we can

75
00:02:43,840 --> 00:02:46,000
defend against those and we can think of

76
00:02:46,000 --> 00:02:47,360
these as sort of

77
00:02:47,360 --> 00:02:48,800
looking at the confidentiality of the

78
00:02:48,800 --> 00:02:51,760
model parameters themselves

79
00:02:51,760 --> 00:02:53,599
the second

80
00:02:53,599 --> 00:02:55,120
aspect of security is typically

81
00:02:55,120 --> 00:02:56,560
integrity

82
00:02:56,560 --> 00:02:59,360
and here we've seen a lot of work on

83
00:02:59,360 --> 00:03:01,440
both training time and test time attacks

84
00:03:01,440 --> 00:03:03,680
against machine learning so at training

85
00:03:03,680 --> 00:03:05,519
time we have attacks

86
00:03:05,519 --> 00:03:06,879
that fall under the spectrum of

87
00:03:06,879 --> 00:03:08,400
poisoning attacks

88
00:03:08,400 --> 00:03:11,040
where um one interesting result that i

89
00:03:11,040 --> 00:03:13,519
wanted to mention in my introduction

90
00:03:13,519 --> 00:03:16,000
is recent work by one of my visiting

91
00:03:16,000 --> 00:03:17,360
students

92
00:03:17,360 --> 00:03:18,480
my love

93
00:03:18,480 --> 00:03:20,400
who demonstrated that it is not only

94
00:03:20,400 --> 00:03:22,640
possible to poison

95
00:03:22,640 --> 00:03:25,280
a machining algorithm's

96
00:03:25,280 --> 00:03:27,680
training procedure by inserting

97
00:03:27,680 --> 00:03:29,840
malicious points in the training set but

98
00:03:29,840 --> 00:03:31,599
you can also achieve the same effect

99
00:03:31,599 --> 00:03:33,760
without inserting

100
00:03:33,760 --> 00:03:35,680
points at all in the training set and so

101
00:03:35,680 --> 00:03:38,159
this is perhaps a little bit surprising

102
00:03:38,159 --> 00:03:39,680
but the way that you can achieve the

103
00:03:39,680 --> 00:03:43,120
same poisoned behavior is by reordering

104
00:03:43,120 --> 00:03:45,840
the points which the algorithm is

105
00:03:45,840 --> 00:03:48,799
analyzing uh throughout training

106
00:03:48,799 --> 00:03:50,239
and so i thought this was an interesting

107
00:03:50,239 --> 00:03:51,879
result because it shows that

108
00:03:51,879 --> 00:03:53,599
stochasticity

109
00:03:53,599 --> 00:03:56,400
in in sgd which which is i guess the

110
00:03:56,400 --> 00:03:58,319
default optimizer to train machine

111
00:03:58,319 --> 00:04:00,959
learning systems is

112
00:04:00,959 --> 00:04:03,280
uh very important if we want to have

113
00:04:03,280 --> 00:04:06,319
guarantees of generalization and so

114
00:04:06,319 --> 00:04:09,280
this is a result that i encourage you to

115
00:04:09,280 --> 00:04:10,959
check out if you're interested in this

116
00:04:10,959 --> 00:04:12,720
line of work of course at test time

117
00:04:12,720 --> 00:04:14,480
we've also seen attacks

118
00:04:14,480 --> 00:04:16,720
with things like adversarial examples

119
00:04:16,720 --> 00:04:18,639
where the inputs are manipulated to

120
00:04:18,639 --> 00:04:20,079
force the model to make wrong

121
00:04:20,079 --> 00:04:22,479
predictions i won't discuss those in the

122
00:04:22,479 --> 00:04:24,639
talk but i'll use them as the motivation

123
00:04:24,639 --> 00:04:26,960
for one of my

124
00:04:26,960 --> 00:04:28,800
points being made

125
00:04:28,800 --> 00:04:30,720
the third component of security is

126
00:04:30,720 --> 00:04:33,919
typically availability and here for for

127
00:04:33,919 --> 00:04:35,680
a few years i thought that there were

128
00:04:35,680 --> 00:04:38,080
not very concrete attacks against the

129
00:04:38,080 --> 00:04:41,120
availability of the machining system

130
00:04:41,120 --> 00:04:44,240
but it turns out that uh here again we

131
00:04:44,240 --> 00:04:46,400
one of my visiting students inaudible

132
00:04:46,400 --> 00:04:48,800
found that it is possible to craft

133
00:04:48,800 --> 00:04:51,360
inputs that maximize the latency

134
00:04:51,360 --> 00:04:53,440
that the underlying hardware that

135
00:04:53,440 --> 00:04:56,639
accelerates the machine learning system

136
00:04:56,639 --> 00:04:58,560
will have

137
00:04:58,560 --> 00:05:00,800
when it processes that input even at

138
00:05:00,800 --> 00:05:02,000
test time

139
00:05:02,000 --> 00:05:02,960
and so

140
00:05:02,960 --> 00:05:04,720
in one of his

141
00:05:04,720 --> 00:05:06,240
experiments what he found is for

142
00:05:06,240 --> 00:05:07,759
instance that you can

143
00:05:07,759 --> 00:05:11,199
take a model served by azure

144
00:05:11,199 --> 00:05:13,759
for translation and increased the

145
00:05:13,759 --> 00:05:16,080
latency of its predictions by a factor

146
00:05:16,080 --> 00:05:16,880
of

147
00:05:16,880 --> 00:05:20,080
up to thousands and so this is of course

148
00:05:20,080 --> 00:05:23,280
if if you tie this back to the energy

149
00:05:23,280 --> 00:05:24,400
consumption

150
00:05:24,400 --> 00:05:26,479
that is increased by this latency you

151
00:05:26,479 --> 00:05:28,720
can see the impact sort of on the carbon

152
00:05:28,720 --> 00:05:31,199
footprint but also on the battery if if

153
00:05:31,199 --> 00:05:32,800
the prediction is made on a mobile

154
00:05:32,800 --> 00:05:34,320
device

155
00:05:34,320 --> 00:05:36,560
so this is the first component of trust

156
00:05:36,560 --> 00:05:37,680
for the machining that i'll be

157
00:05:37,680 --> 00:05:40,639
discussing today and then the second one

158
00:05:40,639 --> 00:05:42,000
is privacy

159
00:05:42,000 --> 00:05:44,720
where instead we're looking at data sets

160
00:05:44,720 --> 00:05:46,400
that are sensitive for instance

161
00:05:46,400 --> 00:05:48,639
healthcare data sets and we're training

162
00:05:48,639 --> 00:05:50,560
machine learning models based on these

163
00:05:50,560 --> 00:05:52,639
data sets and so here we're concerned

164
00:05:52,639 --> 00:05:54,479
about potential leakage of private

165
00:05:54,479 --> 00:05:57,520
information contained uh in in the data

166
00:05:57,520 --> 00:06:00,639
set so i should preface uh the rest of

167
00:06:00,639 --> 00:06:02,720
the talk by saying that

168
00:06:02,720 --> 00:06:03,840
um

169
00:06:03,840 --> 00:06:06,639
i will not be using private in the sense

170
00:06:06,639 --> 00:06:09,280
of private in the crypto community but

171
00:06:09,280 --> 00:06:11,840
in the sense of uh differential privacy

172
00:06:11,840 --> 00:06:13,440
which i'll introduce in just a few

173
00:06:13,440 --> 00:06:16,319
slides and instead i would refer to

174
00:06:16,319 --> 00:06:18,080
private in the sense of crypto by

175
00:06:18,080 --> 00:06:20,720
confidentiality and so here

176
00:06:20,720 --> 00:06:23,759
in uh privacy attacks what we observe is

177
00:06:23,759 --> 00:06:25,919
for instance that if we're able to

178
00:06:25,919 --> 00:06:28,160
observe as an adversary the predictions

179
00:06:28,160 --> 00:06:30,240
that a model makes just the labels that

180
00:06:30,240 --> 00:06:32,319
the model predicts then we're able to

181
00:06:32,319 --> 00:06:34,080
tell whether a specific point was

182
00:06:34,080 --> 00:06:36,000
included or not in the training set and

183
00:06:36,000 --> 00:06:38,880
so you can imagine that if

184
00:06:38,880 --> 00:06:40,720
a model is trained for instance to

185
00:06:40,720 --> 00:06:43,199
predict

186
00:06:43,280 --> 00:06:45,520
when

187
00:06:45,600 --> 00:06:48,400
being part of the training set itself

188
00:06:48,400 --> 00:06:51,360
is potentially very sensitive and so

189
00:06:51,360 --> 00:06:53,520
this is a result that my student

190
00:06:53,520 --> 00:06:55,280
christopher

191
00:06:55,280 --> 00:06:57,840
published recently

192
00:06:57,840 --> 00:07:00,400
so why is it hard to achieve trustworthy

193
00:07:00,400 --> 00:07:02,000
machine learning i'm sure you've seen

194
00:07:02,000 --> 00:07:02,800
many

195
00:07:02,800 --> 00:07:04,319
uh of the papers that have been

196
00:07:04,319 --> 00:07:06,479
published on the topic in in the last

197
00:07:06,479 --> 00:07:07,599
decades

198
00:07:07,599 --> 00:07:10,240
um that i think the the core problem is

199
00:07:10,240 --> 00:07:12,319
that we're we're missing definitions and

200
00:07:12,319 --> 00:07:14,960
so i think this is one first

201
00:07:14,960 --> 00:07:16,560
example of where

202
00:07:16,560 --> 00:07:18,240
uh we can learn from the crypto

203
00:07:18,240 --> 00:07:20,400
community um so here i'm going to

204
00:07:20,400 --> 00:07:22,560
illustrate what i mean with adversarial

205
00:07:22,560 --> 00:07:23,759
examples

206
00:07:23,759 --> 00:07:26,479
which are inputs that are crafted to

207
00:07:26,479 --> 00:07:29,120
force a model to misclassify

208
00:07:29,120 --> 00:07:31,199
the input and so what you can see here

209
00:07:31,199 --> 00:07:34,560
why do adversarial examples exist well

210
00:07:34,560 --> 00:07:36,240
we're trying to learn

211
00:07:36,240 --> 00:07:37,199
this

212
00:07:37,199 --> 00:07:40,240
decision boundary here for the oracle

213
00:07:40,240 --> 00:07:42,000
but of course we're going to learn an

214
00:07:42,000 --> 00:07:43,919
imperfect decision boundary with our

215
00:07:43,919 --> 00:07:45,840
machine learning system and so we're

216
00:07:45,840 --> 00:07:47,840
going to get this red dotted line here

217
00:07:47,840 --> 00:07:50,720
which is not exactly uh the decision

218
00:07:50,720 --> 00:07:52,639
boundary that we wanted to learn

219
00:07:52,639 --> 00:07:55,039
what this means is that if we take this

220
00:07:55,039 --> 00:07:58,720
image of a 3 here it is located

221
00:07:58,720 --> 00:08:01,199
near the decision boundary and what this

222
00:08:01,199 --> 00:08:03,840
means is that the model is too sensitive

223
00:08:03,840 --> 00:08:06,000
to small perturbations

224
00:08:06,000 --> 00:08:08,400
of its inputs

225
00:08:08,400 --> 00:08:09,440
and so

226
00:08:09,440 --> 00:08:11,360
what the community has done the machine

227
00:08:11,360 --> 00:08:12,639
learning community

228
00:08:12,639 --> 00:08:15,680
and the security community has done is

229
00:08:15,680 --> 00:08:17,280
try to produce

230
00:08:17,280 --> 00:08:20,560
training procedures that can provably

231
00:08:20,560 --> 00:08:23,199
demonstrate that the classifier is

232
00:08:23,199 --> 00:08:24,560
constant

233
00:08:24,560 --> 00:08:27,280
around its training inputs and this is

234
00:08:27,280 --> 00:08:29,680
proposed as a way uh to achieve

235
00:08:29,680 --> 00:08:32,080
robustness to these adversarial examples

236
00:08:32,080 --> 00:08:33,919
and so you can see here on the right

237
00:08:33,919 --> 00:08:35,599
this is what we've achieved and the

238
00:08:35,599 --> 00:08:37,120
community has published a number of

239
00:08:37,120 --> 00:08:39,360
approaches to do this where we now have

240
00:08:39,360 --> 00:08:41,200
this black dotted line that is the

241
00:08:41,200 --> 00:08:43,200
machining model that we've trained

242
00:08:43,200 --> 00:08:45,120
and you can see that it has shifted

243
00:08:45,120 --> 00:08:47,200
compared to our red dotted line which

244
00:08:47,200 --> 00:08:49,519
means that now when we look around the

245
00:08:49,519 --> 00:08:51,920
training image of the three all of the

246
00:08:51,920 --> 00:08:54,320
points that are within this ball here

247
00:08:54,320 --> 00:08:56,480
are classified in the same

248
00:08:56,480 --> 00:08:57,839
class

249
00:08:57,839 --> 00:08:59,839
and so this looks like we've made

250
00:08:59,839 --> 00:09:02,640
progress in terms of robustness because

251
00:09:02,640 --> 00:09:05,279
we now have this classifier that is less

252
00:09:05,279 --> 00:09:06,800
sensitive locally

253
00:09:06,800 --> 00:09:08,399
the problem here

254
00:09:08,399 --> 00:09:10,480
that you may have noticed is that

255
00:09:10,480 --> 00:09:12,000
we haven't

256
00:09:12,000 --> 00:09:14,240
said much about how we define this ball

257
00:09:14,240 --> 00:09:16,560
and so in practice the way that we do it

258
00:09:16,560 --> 00:09:17,920
is by using

259
00:09:17,920 --> 00:09:19,200
a p nor

260
00:09:19,200 --> 00:09:21,360
and so the reason we're using p numbers

261
00:09:21,360 --> 00:09:23,279
is we don't have

262
00:09:23,279 --> 00:09:26,320
a computationally uh tractable way of

263
00:09:26,320 --> 00:09:27,440
defining

264
00:09:27,440 --> 00:09:29,360
what is the neighborhood of each of

265
00:09:29,360 --> 00:09:31,120
these training images and so we're going

266
00:09:31,120 --> 00:09:33,279
to approximate this neighborhood

267
00:09:33,279 --> 00:09:36,080
using a p norm the problem is that what

268
00:09:36,080 --> 00:09:38,880
this means is that the the norm does not

269
00:09:38,880 --> 00:09:40,800
reflect the underlying semantics of the

270
00:09:40,800 --> 00:09:42,000
task

271
00:09:42,000 --> 00:09:44,399
and so if we look here what what this

272
00:09:44,399 --> 00:09:46,560
implies in practice is that we can take

273
00:09:46,560 --> 00:09:48,560
this image of a 3 here

274
00:09:48,560 --> 00:09:52,399
and find images within the ball that has

275
00:09:52,399 --> 00:09:53,120
been

276
00:09:53,120 --> 00:09:54,959
certified to be robust for the

277
00:09:54,959 --> 00:09:57,200
classifier that are from different

278
00:09:57,200 --> 00:09:58,480
classes

279
00:09:58,480 --> 00:10:01,279
and the classifier will predict the

280
00:10:01,279 --> 00:10:03,920
label 3 for this image here for instance

281
00:10:03,920 --> 00:10:06,480
of the five because it is close

282
00:10:06,480 --> 00:10:08,640
in under this p norm

283
00:10:08,640 --> 00:10:11,040
of course you as a human know that these

284
00:10:11,040 --> 00:10:13,839
are semantically very different inputs

285
00:10:13,839 --> 00:10:16,000
but here this shows a discrepancy

286
00:10:16,000 --> 00:10:18,480
between what the classifier

287
00:10:18,480 --> 00:10:20,399
has learned and the underlying semantics

288
00:10:20,399 --> 00:10:21,519
of the task

289
00:10:21,519 --> 00:10:23,440
and the problem here is that we're using

290
00:10:23,440 --> 00:10:25,839
an approximation the p norm to define

291
00:10:25,839 --> 00:10:27,920
what we mean by trustworthy

292
00:10:27,920 --> 00:10:30,800
predictions in this case and this

293
00:10:30,800 --> 00:10:33,360
because this definition is incorrect

294
00:10:33,360 --> 00:10:35,440
we're introducing a necessary trade-off

295
00:10:35,440 --> 00:10:37,920
between the accuracy of the model and

296
00:10:37,920 --> 00:10:39,440
the robustness

297
00:10:39,440 --> 00:10:42,560
and this this means that we're going to

298
00:10:42,560 --> 00:10:44,399
always have an arms race

299
00:10:44,399 --> 00:10:45,519
where

300
00:10:45,519 --> 00:10:47,600
here we had an initial vulnerability

301
00:10:47,600 --> 00:10:49,120
that led us to introduce this

302
00:10:49,120 --> 00:10:51,839
pnorm-based robustness which we've now

303
00:10:51,839 --> 00:10:54,000
been able to address

304
00:10:54,000 --> 00:10:56,160
in this case on the right but because of

305
00:10:56,160 --> 00:10:58,399
that we've now introduced new forms of

306
00:10:58,399 --> 00:11:00,480
attacks against this

307
00:11:00,480 --> 00:11:03,600
classifier which are now exploding this

308
00:11:03,600 --> 00:11:05,680
excessive invariance so this is a result

309
00:11:05,680 --> 00:11:08,480
we discussed in this paper which shows

310
00:11:08,480 --> 00:11:11,680
why we're never going to be

311
00:11:11,680 --> 00:11:14,480
achieving a robust classifier using

312
00:11:14,480 --> 00:11:17,120
uh p norm approximations for for

313
00:11:17,120 --> 00:11:18,800
robustness

314
00:11:18,800 --> 00:11:21,600
and so this is sort of uh

315
00:11:21,600 --> 00:11:22,399
an

316
00:11:22,399 --> 00:11:25,360
example that brings up this question

317
00:11:25,360 --> 00:11:26,320
which is

318
00:11:26,320 --> 00:11:28,399
whether machinery systems

319
00:11:28,399 --> 00:11:30,079
allow us to approach

320
00:11:30,079 --> 00:11:32,800
security and in general trust

321
00:11:32,800 --> 00:11:35,279
in a way that is different from real

322
00:11:35,279 --> 00:11:38,000
world computer security so if we look at

323
00:11:38,000 --> 00:11:40,320
computer security and particular system

324
00:11:40,320 --> 00:11:43,320
security

325
00:11:43,440 --> 00:11:44,720
this is

326
00:11:44,720 --> 00:11:45,920
offers

327
00:11:45,920 --> 00:11:46,959
balance

328
00:11:46,959 --> 00:11:48,640
the cost of protection of the risk of

329
00:11:48,640 --> 00:11:50,320
loss and so this this means that we

330
00:11:50,320 --> 00:11:52,399
don't have provable guarantees and so we

331
00:11:52,399 --> 00:11:54,399
could ask ourselves are machine learning

332
00:11:54,399 --> 00:11:57,600
systems in some sense quote-unquote

333
00:11:57,600 --> 00:12:00,000
doomed to have this this similar arms

334
00:12:00,000 --> 00:12:00,880
race

335
00:12:00,880 --> 00:12:03,440
and then this talk i'd like to give a

336
00:12:03,440 --> 00:12:05,360
few examples of why i think this is not

337
00:12:05,360 --> 00:12:07,600
necessarily the case

338
00:12:07,600 --> 00:12:08,480
and

339
00:12:08,480 --> 00:12:11,120
i usually say that a good example

340
00:12:11,120 --> 00:12:13,920
is is that machining systems are

341
00:12:13,920 --> 00:12:16,240
uh very amenable to crypto like

342
00:12:16,240 --> 00:12:18,079
treatment so i think this will resonate

343
00:12:18,079 --> 00:12:20,880
very well uh with this community and i

344
00:12:20,880 --> 00:12:23,600
hope that the the presentation will

345
00:12:23,600 --> 00:12:26,560
give you ideas for future work to

346
00:12:26,560 --> 00:12:28,320
work on in this direction

347
00:12:28,320 --> 00:12:30,800
so the first part of my talk i'm going

348
00:12:30,800 --> 00:12:33,680
to discuss data privacy as an example

349
00:12:33,680 --> 00:12:35,200
where the

350
00:12:35,200 --> 00:12:37,360
definition of what we mean of by

351
00:12:37,360 --> 00:12:39,839
trustworthy machining is very clear and

352
00:12:39,839 --> 00:12:41,360
has allowed us to

353
00:12:41,360 --> 00:12:43,600
achieve principled and systematic

354
00:12:43,600 --> 00:12:45,920
progress towards improving the the

355
00:12:45,920 --> 00:12:48,000
privacy of machine learning

356
00:12:48,000 --> 00:12:50,000
and the definition is

357
00:12:50,000 --> 00:12:52,560
most likely very familiar to many of you

358
00:12:52,560 --> 00:12:55,360
because it comes from cryptographers

359
00:12:55,360 --> 00:12:57,600
and the definition of privacy that is

360
00:12:57,600 --> 00:12:59,040
the consensus now in the machine

361
00:12:59,040 --> 00:13:00,800
learning community

362
00:13:00,800 --> 00:13:02,639
for private data

363
00:13:02,639 --> 00:13:04,399
is differential privacy where the idea

364
00:13:04,399 --> 00:13:06,639
is that we have an adversary

365
00:13:06,639 --> 00:13:09,519
that is observing the output of our

366
00:13:09,519 --> 00:13:12,240
randomized algorithm and the adversary

367
00:13:12,240 --> 00:13:13,920
should not be able to tell

368
00:13:13,920 --> 00:13:16,399
whether the algorithm was operating on

369
00:13:16,399 --> 00:13:18,880
either of these two data sets where the

370
00:13:18,880 --> 00:13:21,680
data set at the top included the record

371
00:13:21,680 --> 00:13:24,399
for one particular individual whereas

372
00:13:24,399 --> 00:13:26,720
the data set at the bottom is identical

373
00:13:26,720 --> 00:13:28,399
to the exception of that record which

374
00:13:28,399 --> 00:13:30,480
has been removed or altered

375
00:13:30,480 --> 00:13:32,079
and so what this means is that we're

376
00:13:32,079 --> 00:13:33,440
able to prove

377
00:13:33,440 --> 00:13:36,000
that the behavior of the algorithm is

378
00:13:36,000 --> 00:13:38,240
statistically indistinguishable on these

379
00:13:38,240 --> 00:13:39,920
two data sets so the adversary is not

380
00:13:39,920 --> 00:13:41,519
even able to

381
00:13:41,519 --> 00:13:44,079
infer whether the person contributed the

382
00:13:44,079 --> 00:13:45,920
record or not and so whether they

383
00:13:45,920 --> 00:13:48,560
existed so this provides them with

384
00:13:48,560 --> 00:13:51,519
privacy and so the way that we

385
00:13:51,519 --> 00:13:52,880
in practice

386
00:13:52,880 --> 00:13:55,360
measure how private an algorithm is

387
00:13:55,360 --> 00:13:58,320
is by analyzing it in order to prove

388
00:13:58,320 --> 00:14:01,120
a bound on on the indistinguishability

389
00:14:01,120 --> 00:14:03,519
which is defined as follows where we're

390
00:14:03,519 --> 00:14:04,959
looking at the probability of the

391
00:14:04,959 --> 00:14:06,240
algorithm

392
00:14:06,240 --> 00:14:08,560
operating on this first data set d

393
00:14:08,560 --> 00:14:11,120
making a certain output being close to

394
00:14:11,120 --> 00:14:12,959
the probability of the same algorithm

395
00:14:12,959 --> 00:14:15,199
operating on the second data set d prime

396
00:14:15,199 --> 00:14:17,360
making that same output and so this has

397
00:14:17,360 --> 00:14:19,279
to be proof for all possible outputs and

398
00:14:19,279 --> 00:14:21,040
all pairs of data sets d and d prime

399
00:14:21,040 --> 00:14:24,240
that only differ by one training record

400
00:14:24,240 --> 00:14:26,639
so once we have this basically

401
00:14:26,639 --> 00:14:28,560
our goal as

402
00:14:28,560 --> 00:14:30,480
machine learning researchers and privacy

403
00:14:30,480 --> 00:14:33,519
researchers is to decrease the bound um

404
00:14:33,519 --> 00:14:35,279
so that we have a tighter

405
00:14:35,279 --> 00:14:36,519
uh upper bound on the

406
00:14:36,519 --> 00:14:38,800
indistinguishability that the that i

407
00:14:38,800 --> 00:14:40,880
just described

408
00:14:40,880 --> 00:14:43,519
and so there are many ways that we have

409
00:14:43,519 --> 00:14:45,360
come up with to train machine learning

410
00:14:45,360 --> 00:14:48,079
models with differential privacy

411
00:14:48,079 --> 00:14:49,600
one of them

412
00:14:49,600 --> 00:14:50,880
is called differentially private

413
00:14:50,880 --> 00:14:52,959
stochastic grain and descent here i'm

414
00:14:52,959 --> 00:14:55,279
going to introduce another one called

415
00:14:55,279 --> 00:14:56,880
pate

416
00:14:56,880 --> 00:14:59,120
because it allows me to illustrate why

417
00:14:59,120 --> 00:15:01,440
differential privacy is such a good fit

418
00:15:01,440 --> 00:15:03,600
for machine learning and in particular

419
00:15:03,600 --> 00:15:06,560
how well it aligns with generalization

420
00:15:06,560 --> 00:15:08,800
so if we take a look at this diagram

421
00:15:08,800 --> 00:15:10,399
here what i'm showing is that in order

422
00:15:10,399 --> 00:15:12,720
to achieve privacy we're going to take

423
00:15:12,720 --> 00:15:14,320
our sensitive data

424
00:15:14,320 --> 00:15:16,800
and partition it and end different

425
00:15:16,800 --> 00:15:18,560
uh subsets so this this is a

426
00:15:18,560 --> 00:15:20,160
mathematical partition so there is no

427
00:15:20,160 --> 00:15:21,440
overlap

428
00:15:21,440 --> 00:15:23,600
and then from each of these partitions

429
00:15:23,600 --> 00:15:25,040
we're going to train a machine learning

430
00:15:25,040 --> 00:15:27,680
model and so there are no requirements

431
00:15:27,680 --> 00:15:29,040
on how we train this machine learning

432
00:15:29,040 --> 00:15:30,399
model the only

433
00:15:30,399 --> 00:15:31,920
constraint is that it has to be trained

434
00:15:31,920 --> 00:15:33,199
independently

435
00:15:33,199 --> 00:15:34,720
and what this means is we now have an

436
00:15:34,720 --> 00:15:36,639
ensemble of and different models that

437
00:15:36,639 --> 00:15:38,320
are all trained from different subsets

438
00:15:38,320 --> 00:15:40,240
of data but that all perform the same

439
00:15:40,240 --> 00:15:41,360
task

440
00:15:41,360 --> 00:15:42,959
and so how do we

441
00:15:42,959 --> 00:15:44,240
have these

442
00:15:44,240 --> 00:15:46,480
models predict with privacy well the

443
00:15:46,480 --> 00:15:48,639
naive thing to do would be to say well

444
00:15:48,639 --> 00:15:51,120
i'm going to take the arg max

445
00:15:51,120 --> 00:15:53,759
and ask each of these different models

446
00:15:53,759 --> 00:15:56,959
to predict a specific label and then i'm

447
00:15:56,959 --> 00:15:58,639
going to take the label that was the

448
00:15:58,639 --> 00:16:00,959
most commonly predicted

449
00:16:00,959 --> 00:16:02,720
this provides an intuitive notion of

450
00:16:02,720 --> 00:16:05,360
privacy because if multiple teachers

451
00:16:05,360 --> 00:16:07,360
make the same prediction

452
00:16:07,360 --> 00:16:09,600
then you can understand why this

453
00:16:09,600 --> 00:16:11,440
prediction is not specific to any of

454
00:16:11,440 --> 00:16:13,839
these uh individual subsets of data and

455
00:16:13,839 --> 00:16:16,720
so if my training record was in this

456
00:16:16,720 --> 00:16:18,480
initial data set it's only going to be

457
00:16:18,480 --> 00:16:20,079
in one of these subsets of data so it

458
00:16:20,079 --> 00:16:22,639
only influenced one of the teachers

459
00:16:22,639 --> 00:16:24,399
the issue with this

460
00:16:24,399 --> 00:16:25,920
and why it does not provide differential

461
00:16:25,920 --> 00:16:27,839
privacy is that for instance if you have

462
00:16:27,839 --> 00:16:29,040
two

463
00:16:29,040 --> 00:16:31,360
labels that receive the same or

464
00:16:31,360 --> 00:16:33,680
almost the same number of votes then one

465
00:16:33,680 --> 00:16:35,759
teacher changing their prediction from

466
00:16:35,759 --> 00:16:38,160
one to the other label can

467
00:16:38,160 --> 00:16:40,800
change the overall outcome of our

468
00:16:40,800 --> 00:16:43,519
aggregation

469
00:16:44,079 --> 00:16:46,560
so this private purpose

470
00:16:46,560 --> 00:16:48,560
differential privacy is by

471
00:16:48,560 --> 00:16:51,040
randomly perturbing the histogram uh

472
00:16:51,040 --> 00:16:54,160
before we output the the rmax

473
00:16:54,160 --> 00:16:56,720
so this provides us with a noisy

474
00:16:56,720 --> 00:16:57,839
aggregation mechanism that is

475
00:16:57,839 --> 00:16:59,600
differentially private uh we can think

476
00:16:59,600 --> 00:17:01,199
of this as a differentially private

477
00:17:01,199 --> 00:17:02,959
machining api

478
00:17:02,959 --> 00:17:04,959
the one catch with this is that every

479
00:17:04,959 --> 00:17:07,199
time that we make a prediction the

480
00:17:07,199 --> 00:17:09,359
privacy budget that ex that is expanded

481
00:17:09,359 --> 00:17:10,880
is going to increase

482
00:17:10,880 --> 00:17:14,880
because we're bounding but uh not making

483
00:17:14,880 --> 00:17:17,119
null the privacy leakage that is

484
00:17:17,119 --> 00:17:19,119
happening for each answer that we're

485
00:17:19,119 --> 00:17:21,760
reviewing and so we have to essentially

486
00:17:21,760 --> 00:17:23,520
take these

487
00:17:23,520 --> 00:17:25,359
this these differentially private

488
00:17:25,359 --> 00:17:27,839
predictions and use them to train an

489
00:17:27,839 --> 00:17:29,440
additional model here which is the

490
00:17:29,440 --> 00:17:30,960
student model

491
00:17:30,960 --> 00:17:32,880
which will then

492
00:17:32,880 --> 00:17:35,440
be used to make predictions uh in the

493
00:17:35,440 --> 00:17:37,120
end and so what this means is that we

494
00:17:37,120 --> 00:17:38,480
transfer the knowledge from this

495
00:17:38,480 --> 00:17:40,880
ensemble into the student model

496
00:17:40,880 --> 00:17:43,200
with privacy guarantees and once we've

497
00:17:43,200 --> 00:17:45,760
trained the student we can delete all of

498
00:17:45,760 --> 00:17:47,919
the the teachers and the even their

499
00:17:47,919 --> 00:17:50,080
training data and the privacy budget is

500
00:17:50,080 --> 00:17:53,039
fixed at that point

501
00:17:53,039 --> 00:17:55,120
so why did i uh

502
00:17:55,120 --> 00:17:56,960
introduce

503
00:17:56,960 --> 00:17:59,039
pate the first the first reason is if we

504
00:17:59,039 --> 00:18:00,640
come back to this is that you can see

505
00:18:00,640 --> 00:18:02,240
there is a strong alignment between

506
00:18:02,240 --> 00:18:04,559
privacy and generalization set up

507
00:18:04,559 --> 00:18:08,080
because the more teachers agree on the

508
00:18:08,080 --> 00:18:10,400
prediction that is being made the more

509
00:18:10,400 --> 00:18:12,320
this prediction is likely to be correct

510
00:18:12,320 --> 00:18:14,000
because it's been made independently

511
00:18:14,000 --> 00:18:15,440
many times

512
00:18:15,440 --> 00:18:17,760
so this is good on the machine

513
00:18:17,760 --> 00:18:18,960
perspective

514
00:18:18,960 --> 00:18:21,200
in terms of privacy the more consensus

515
00:18:21,200 --> 00:18:22,400
we have

516
00:18:22,400 --> 00:18:24,240
the more private the prediction can

517
00:18:24,240 --> 00:18:25,679
potentially be

518
00:18:25,679 --> 00:18:27,520
because we can introduce more random

519
00:18:27,520 --> 00:18:28,400
noise

520
00:18:28,400 --> 00:18:30,799
before we take the arg max without

521
00:18:30,799 --> 00:18:34,240
perturbing the correctness of the arcmax

522
00:18:34,240 --> 00:18:35,520
mechanism

523
00:18:35,520 --> 00:18:37,600
and so what this means is that

524
00:18:37,600 --> 00:18:40,160
in cases where consensus is strong we

525
00:18:40,160 --> 00:18:42,000
can get improvements in both the

526
00:18:42,000 --> 00:18:44,559
accuracy and the privacy which is very

527
00:18:44,559 --> 00:18:47,360
unusual in computer security where you

528
00:18:47,360 --> 00:18:49,840
have an alignment in the security

529
00:18:49,840 --> 00:18:52,799
constraint and the performance

530
00:18:52,799 --> 00:18:54,960
objective and so this is i think one of

531
00:18:54,960 --> 00:18:56,880
the reasons why differential privacy is

532
00:18:56,880 --> 00:18:59,760
such an exciting uh area of research in

533
00:18:59,760 --> 00:19:02,240
the context of michigan

534
00:19:02,240 --> 00:19:04,240
another practical

535
00:19:04,240 --> 00:19:05,919
reason is that

536
00:19:05,919 --> 00:19:07,360
with differential privacy we get

537
00:19:07,360 --> 00:19:09,280
provable guarantees

538
00:19:09,280 --> 00:19:11,360
which are robust to knowledge that the

539
00:19:11,360 --> 00:19:13,760
adversary may acquire in the future or

540
00:19:13,760 --> 00:19:15,679
new capabilities that the adversary may

541
00:19:15,679 --> 00:19:16,799
gain

542
00:19:16,799 --> 00:19:18,960
so if you recall the example that i gave

543
00:19:18,960 --> 00:19:21,919
with adversarial examples research one

544
00:19:21,919 --> 00:19:23,120
problem with

545
00:19:23,120 --> 00:19:24,880
many defenses proposed against

546
00:19:24,880 --> 00:19:27,520
adversarial examples is that they do not

547
00:19:27,520 --> 00:19:30,880
model an adaptive adversary that will

548
00:19:30,880 --> 00:19:32,880
actively seek to leave the defense so

549
00:19:32,880 --> 00:19:34,160
for instance

550
00:19:34,160 --> 00:19:36,320
because adversarial examples are found

551
00:19:36,320 --> 00:19:38,960
typically by performing a grin based

552
00:19:38,960 --> 00:19:40,320
search

553
00:19:40,320 --> 00:19:42,880
many defenses perform an instance of the

554
00:19:42,880 --> 00:19:45,120
following here which is to basically

555
00:19:45,120 --> 00:19:48,160
mask the gradients around inputs that

556
00:19:48,160 --> 00:19:50,000
the model will predict for so that

557
00:19:50,000 --> 00:19:52,880
someone trying to find this step

558
00:19:52,880 --> 00:19:54,720
function here is not going to be able to

559
00:19:54,720 --> 00:19:56,799
because the gradients are not indicating

560
00:19:56,799 --> 00:19:59,039
where the step function is

561
00:19:59,039 --> 00:20:01,520
and so when you have such a defense you

562
00:20:01,520 --> 00:20:03,679
can perform for instance a black box

563
00:20:03,679 --> 00:20:06,080
attack by taking another model and then

564
00:20:06,080 --> 00:20:07,840
finding adversarial examples for this

565
00:20:07,840 --> 00:20:09,760
model and then transferring them

566
00:20:09,760 --> 00:20:12,559
uh to the defended model and so this

567
00:20:12,559 --> 00:20:14,880
this is why um sort of this class of

568
00:20:14,880 --> 00:20:18,000
defenses is not effectively bringing you

569
00:20:18,000 --> 00:20:21,360
any robustness in contrast

570
00:20:21,360 --> 00:20:24,880
in privacy there has been

571
00:20:24,880 --> 00:20:26,159
similar

572
00:20:26,159 --> 00:20:29,520
defenses proposed with

573
00:20:29,520 --> 00:20:30,960
something called membership inference

574
00:20:30,960 --> 00:20:32,880
attacks which is something that i

575
00:20:32,880 --> 00:20:34,559
introduced at the very beginning of my

576
00:20:34,559 --> 00:20:35,600
talk

577
00:20:35,600 --> 00:20:37,360
so if you remember

578
00:20:37,360 --> 00:20:39,280
what i said is that membership inference

579
00:20:39,280 --> 00:20:42,799
attacks is possible uh only by accessing

580
00:20:42,799 --> 00:20:45,360
the label that the model is predicting

581
00:20:45,360 --> 00:20:47,440
and so what this means is that

582
00:20:47,440 --> 00:20:50,080
defenses that previously

583
00:20:50,080 --> 00:20:52,080
tried to make membership inference

584
00:20:52,080 --> 00:20:53,360
harder by

585
00:20:53,360 --> 00:20:55,679
masking the confidence of the model

586
00:20:55,679 --> 00:20:57,600
which makes it easier to tell whether

587
00:20:57,600 --> 00:20:59,600
the point was included in the training

588
00:20:59,600 --> 00:21:02,559
set or not are now easily defeated

589
00:21:02,559 --> 00:21:04,799
by a label-only membership inference

590
00:21:04,799 --> 00:21:07,360
attack and so what we see here on the

591
00:21:07,360 --> 00:21:10,240
right is um

592
00:21:10,240 --> 00:21:12,240
a result by by my student christopher

593
00:21:12,240 --> 00:21:14,400
where what he found is that previous

594
00:21:14,400 --> 00:21:18,400
attacks that required confidence access

595
00:21:18,400 --> 00:21:21,600
performed just as well as his label only

596
00:21:21,600 --> 00:21:22,480
attack

597
00:21:22,480 --> 00:21:24,799
so what this means is that all defenses

598
00:21:24,799 --> 00:21:27,120
that are relying on the fact that

599
00:21:27,120 --> 00:21:27,919
they're

600
00:21:27,919 --> 00:21:30,480
hiding the confidence of the model or

601
00:21:30,480 --> 00:21:33,039
randomizing the confidence of the model

602
00:21:33,039 --> 00:21:35,919
are basically trivially defeated by this

603
00:21:35,919 --> 00:21:37,600
attack which does not rely on the

604
00:21:37,600 --> 00:21:40,080
confidence and performs performs equally

605
00:21:40,080 --> 00:21:40,960
well

606
00:21:40,960 --> 00:21:45,559
whereas instead if you train

607
00:21:46,799 --> 00:21:50,000
what we found is that this is the only

608
00:21:50,000 --> 00:21:54,159
uh approach to uh defeating at least

609
00:21:54,159 --> 00:21:56,720
partially this label only

610
00:21:56,720 --> 00:21:58,640
membership inference attack

611
00:21:58,640 --> 00:22:00,960
and so you do get improvement in in the

612
00:22:00,960 --> 00:22:04,159
privacy uh that is robust to to the

613
00:22:04,159 --> 00:22:06,159
to the developments from the attacker

614
00:22:06,159 --> 00:22:06,960
side

615
00:22:06,960 --> 00:22:08,880
so i think this is this is very very

616
00:22:08,880 --> 00:22:11,520
strong uh advantage again of training

617
00:22:11,520 --> 00:22:14,240
with differential privacy

618
00:22:14,240 --> 00:22:15,600
another reason

619
00:22:15,600 --> 00:22:17,600
oh absolutely yes

620
00:22:17,600 --> 00:22:19,440
does it depend on how many queries you

621
00:22:19,440 --> 00:22:21,600
do when you train with the financial

622
00:22:21,600 --> 00:22:23,679
privacy

623
00:22:23,679 --> 00:22:27,039
so it definitely depends

624
00:22:31,840 --> 00:22:33,280
yeah

625
00:22:33,280 --> 00:22:35,200
yeah so so basically what happens with

626
00:22:35,200 --> 00:22:38,000
differential privacy is that the smaller

627
00:22:38,000 --> 00:22:40,880
the epsilon you're able to achieve

628
00:22:40,880 --> 00:22:42,480
during training

629
00:22:42,480 --> 00:22:43,280
the

630
00:22:43,280 --> 00:22:45,600
lower the advantage of the membership

631
00:22:45,600 --> 00:22:48,400
inference attacker will be so if you

632
00:22:48,400 --> 00:22:50,640
don't have differential privacy you'll

633
00:22:50,640 --> 00:22:51,760
have

634
00:22:51,760 --> 00:22:54,080
better than random chance ability to

635
00:22:54,080 --> 00:22:56,320
guess whether a point is

636
00:22:56,320 --> 00:22:59,200
in the training set or not and as you

637
00:22:59,200 --> 00:23:00,159
improve

638
00:23:00,159 --> 00:23:02,080
the differential privacy guarantee that

639
00:23:02,080 --> 00:23:05,039
you're able to achieve you're going to

640
00:23:05,039 --> 00:23:08,000
bring that attacker closer to random

641
00:23:08,000 --> 00:23:10,240
guess basically a 50 50 for instance if

642
00:23:10,240 --> 00:23:12,000
you have

643
00:23:12,000 --> 00:23:14,720
a baseline of 50 50 being in or out of

644
00:23:14,720 --> 00:23:16,559
the training set does does that answer

645
00:23:16,559 --> 00:23:18,159
your question

646
00:23:18,159 --> 00:23:19,840
so you're saying that it all depends on

647
00:23:19,840 --> 00:23:21,520
the parameter epsilon when you train

648
00:23:21,520 --> 00:23:23,600
with that's right that's right that's

649
00:23:23,600 --> 00:23:26,159
right and that parameter typically

650
00:23:26,159 --> 00:23:27,280
trades off

651
00:23:27,280 --> 00:23:28,240
uh

652
00:23:28,240 --> 00:23:29,919
with performance

653
00:23:29,919 --> 00:23:32,559
event eventually if you prove

654
00:23:32,559 --> 00:23:35,600
uh an epsilon that is very small you're

655
00:23:35,600 --> 00:23:37,919
going to have to pay a price in terms of

656
00:23:37,919 --> 00:23:41,120
the utility of the model

657
00:23:41,120 --> 00:23:42,720
based on the other attacks that you were

658
00:23:42,720 --> 00:23:44,880
comparing in the graph

659
00:23:44,880 --> 00:23:47,120
so the epsilon that you were using was

660
00:23:47,120 --> 00:23:50,400
giving good accuracy or was it

661
00:23:50,400 --> 00:23:52,000
so basically what we're showing in this

662
00:23:52,000 --> 00:23:54,960
graph is that the the the approach where

663
00:23:54,960 --> 00:23:57,279
you train with differential privacy is

664
00:23:57,279 --> 00:23:59,760
able to achieve the the best trade-offs

665
00:23:59,760 --> 00:24:00,720
between

666
00:24:00,720 --> 00:24:02,960
uh improving the robustness to the

667
00:24:02,960 --> 00:24:05,600
attacks and maintaining

668
00:24:05,600 --> 00:24:08,720
uh a good accuracy for for the model

669
00:24:08,720 --> 00:24:10,400
that you're training so that it can

670
00:24:10,400 --> 00:24:13,600
predict uh in a useful manner

671
00:24:13,600 --> 00:24:15,200
so this is what you're showing what

672
00:24:15,200 --> 00:24:16,840
we're showing here in this

673
00:24:16,840 --> 00:24:19,600
box okay

674
00:24:19,600 --> 00:24:20,480
thank you

675
00:24:20,480 --> 00:24:22,480
yeah thank you for the question

676
00:24:22,480 --> 00:24:23,440
so

677
00:24:23,440 --> 00:24:25,840
one other uh reason that i wanted to

678
00:24:25,840 --> 00:24:29,360
introduce pate is that we've recently

679
00:24:29,360 --> 00:24:32,080
extended our work on pate

680
00:24:32,080 --> 00:24:34,159
to include

681
00:24:34,159 --> 00:24:35,039
a

682
00:24:35,039 --> 00:24:36,320
possibility

683
00:24:36,320 --> 00:24:37,039
of

684
00:24:37,039 --> 00:24:39,360
performing pate in distributed setting

685
00:24:39,360 --> 00:24:40,880
and this is i think interesting to this

686
00:24:40,880 --> 00:24:43,440
community because it relies on crypto

687
00:24:43,440 --> 00:24:45,279
cryptographic primitives

688
00:24:45,279 --> 00:24:48,000
and so the the observation here is this

689
00:24:48,000 --> 00:24:49,440
the following is that if you look at

690
00:24:49,440 --> 00:24:51,200
this setup here

691
00:24:51,200 --> 00:24:53,840
we've manually partitioned the data into

692
00:24:53,840 --> 00:24:55,520
multiple partitions

693
00:24:55,520 --> 00:24:57,679
whereas you could you could imagine

694
00:24:57,679 --> 00:24:59,840
setups where this data would already be

695
00:24:59,840 --> 00:25:00,960
partitioned

696
00:25:00,960 --> 00:25:02,640
sort of organically so imagine for

697
00:25:02,640 --> 00:25:04,480
instance if you have a network of

698
00:25:04,480 --> 00:25:05,679
hospitals

699
00:25:05,679 --> 00:25:07,840
they might each have locally a training

700
00:25:07,840 --> 00:25:09,039
set and they

701
00:25:09,039 --> 00:25:11,279
might have already trained their own

702
00:25:11,279 --> 00:25:14,159
model locally but they could benefit

703
00:25:14,159 --> 00:25:18,080
from inferring jointly at test time

704
00:25:18,080 --> 00:25:21,120
because for instance a specific hospital

705
00:25:21,120 --> 00:25:22,799
is lacking

706
00:25:22,799 --> 00:25:25,360
training data for for a specific test

707
00:25:25,360 --> 00:25:26,320
point

708
00:25:26,320 --> 00:25:28,640
and so this is what we've done here in

709
00:25:28,640 --> 00:25:33,360
in capsi uh which is to combine pate to

710
00:25:33,360 --> 00:25:35,919
get a differentially private prediction

711
00:25:35,919 --> 00:25:38,480
with cryptographic primitives to be able

712
00:25:38,480 --> 00:25:41,039
to perform this prediction in a

713
00:25:41,039 --> 00:25:43,039
distributed setting without revealing

714
00:25:43,039 --> 00:25:45,760
information about the models or the test

715
00:25:45,760 --> 00:25:47,760
points that are being predicted up and

716
00:25:47,760 --> 00:25:49,200
so the way that this works is that you

717
00:25:49,200 --> 00:25:51,039
have one of the hospitals in the example

718
00:25:51,039 --> 00:25:53,039
that i just gave that is the querying

719
00:25:53,039 --> 00:25:55,360
party and has a test input that they

720
00:25:55,360 --> 00:25:58,240
would like other hospitals to predict on

721
00:25:58,240 --> 00:26:00,240
so what they're going to do is encrypt

722
00:26:00,240 --> 00:26:01,440
this point

723
00:26:01,440 --> 00:26:04,240
and send it to the other hospitals which

724
00:26:04,240 --> 00:26:05,520
are going to

725
00:26:05,520 --> 00:26:07,760
infer on the point using homomorphic

726
00:26:07,760 --> 00:26:09,600
machine learning so that they can

727
00:26:09,600 --> 00:26:10,480
produce

728
00:26:10,480 --> 00:26:12,400
a logic vector

729
00:26:12,400 --> 00:26:13,760
without

730
00:26:13,760 --> 00:26:16,960
seeing being able to read the test input

731
00:26:16,960 --> 00:26:19,279
and without having to reveal their own

732
00:26:19,279 --> 00:26:21,760
machine learning model to the hospital

733
00:26:21,760 --> 00:26:24,480
that submitted the query once they've

734
00:26:24,480 --> 00:26:25,760
done that

735
00:26:25,760 --> 00:26:28,720
we use secret sharing between

736
00:26:28,720 --> 00:26:32,000
each of the answering parties and

737
00:26:32,000 --> 00:26:33,440
the querying party so we have a

738
00:26:33,440 --> 00:26:36,880
two-party protocol that allows us

739
00:26:36,880 --> 00:26:39,279
to basically turn these logits into one

740
00:26:39,279 --> 00:26:40,720
hot vectors

741
00:26:40,720 --> 00:26:44,440
and then to reconstruct the

742
00:26:44,440 --> 00:26:47,360
histogram that it's cracked where the

743
00:26:47,360 --> 00:26:49,760
answering parties is submitting a vote

744
00:26:49,760 --> 00:26:51,679
for which class they think the test

745
00:26:51,679 --> 00:26:52,720
input

746
00:26:52,720 --> 00:26:54,080
belongs to

747
00:26:54,080 --> 00:26:56,720
and so once we've done this

748
00:26:56,720 --> 00:26:59,760
we're going to submit each of the

749
00:26:59,760 --> 00:27:02,480
answering parties shares to a central

750
00:27:02,480 --> 00:27:05,440
party which is going to aggregate the

751
00:27:05,440 --> 00:27:08,080
votes from the different individual

752
00:27:08,080 --> 00:27:11,200
hospitals and then perform the summation

753
00:27:11,200 --> 00:27:12,480
operation that is required to

754
00:27:12,480 --> 00:27:15,760
reconstruct the histogram and also add

755
00:27:15,760 --> 00:27:17,600
the random noise that is required to

756
00:27:17,600 --> 00:27:19,679
obtain differential privacy

757
00:27:19,679 --> 00:27:21,919
and so what this means here is that once

758
00:27:21,919 --> 00:27:24,399
the the central party has

759
00:27:24,399 --> 00:27:26,799
performed this summation and

760
00:27:26,799 --> 00:27:29,039
noising of the votes

761
00:27:29,039 --> 00:27:31,360
it can perform one last two-party

762
00:27:31,360 --> 00:27:34,480
protocol with the querying party to

763
00:27:34,480 --> 00:27:36,880
reveal to that party the aggregated

764
00:27:36,880 --> 00:27:39,360
label that the teachers have predicted

765
00:27:39,360 --> 00:27:41,600
the different hospitals have predicted

766
00:27:41,600 --> 00:27:43,760
and so what this means is that here in

767
00:27:43,760 --> 00:27:47,039
this setup the querying party

768
00:27:47,039 --> 00:27:48,480
does not

769
00:27:48,480 --> 00:27:51,520
learn anything private that is contained

770
00:27:51,520 --> 00:27:54,000
in the answering party's training sets

771
00:27:54,000 --> 00:27:55,520
because we've produced this prediction

772
00:27:55,520 --> 00:27:57,760
with differential privacy the querying

773
00:27:57,760 --> 00:27:59,679
party also does not need to reveal their

774
00:27:59,679 --> 00:28:01,679
test input to the different answering

775
00:28:01,679 --> 00:28:03,360
parties because we've used morphic

776
00:28:03,360 --> 00:28:04,720
machine learning

777
00:28:04,720 --> 00:28:07,360
and the answering parties do not have to

778
00:28:07,360 --> 00:28:09,200
reveal their models to the to the

779
00:28:09,200 --> 00:28:11,520
querying party and finally the privacy

780
00:28:11,520 --> 00:28:14,399
guardian does not learn anything about

781
00:28:14,399 --> 00:28:17,039
the test input that was submitted but

782
00:28:17,039 --> 00:28:19,039
also about the individual

783
00:28:19,039 --> 00:28:21,039
um

784
00:28:21,039 --> 00:28:23,440
predictions that the the models are

785
00:28:23,440 --> 00:28:25,600
making the answering parties and so we

786
00:28:25,600 --> 00:28:28,159
think this is an interesting setup in

787
00:28:28,159 --> 00:28:29,679
setups where

788
00:28:29,679 --> 00:28:33,120
we have a heterogeneous set of

789
00:28:33,120 --> 00:28:35,279
participants that each have different

790
00:28:35,279 --> 00:28:36,559
models

791
00:28:36,559 --> 00:28:39,360
all performing the same task and these

792
00:28:39,360 --> 00:28:42,159
models already exist and all we want to

793
00:28:42,159 --> 00:28:43,600
do is to

794
00:28:43,600 --> 00:28:44,480
predict

795
00:28:44,480 --> 00:28:48,159
jointly as an ensemble and so this is a

796
00:28:48,159 --> 00:28:50,000
very different setup from federated

797
00:28:50,000 --> 00:28:51,440
learning where instead you have to

798
00:28:51,440 --> 00:28:54,159
assume that everybody jointly

799
00:28:54,159 --> 00:28:57,440
trusts each other to be able to train

800
00:28:57,440 --> 00:29:00,320
a single machinery model centrally here

801
00:29:00,320 --> 00:29:01,600
all of the models remain fully

802
00:29:01,600 --> 00:29:03,200
distributed and

803
00:29:03,200 --> 00:29:04,960
owned independently by the different

804
00:29:04,960 --> 00:29:06,240
participants

805
00:29:06,240 --> 00:29:08,960
and of course each participant can

806
00:29:08,960 --> 00:29:10,159
alternate

807
00:29:10,159 --> 00:29:11,440
roles

808
00:29:11,440 --> 00:29:13,679
and be acquiring party in one of the

809
00:29:13,679 --> 00:29:15,840
following realms of the protocol

810
00:29:15,840 --> 00:29:17,360
and so what this means is that as a

811
00:29:17,360 --> 00:29:19,600
participant you can

812
00:29:19,600 --> 00:29:21,679
perform something like active learning

813
00:29:21,679 --> 00:29:25,200
to improve your own local models

814
00:29:25,200 --> 00:29:27,200
predictions by

815
00:29:27,200 --> 00:29:30,559
integrating the pr the

816
00:29:30,880 --> 00:29:32,880
predictions that capsi is providing you

817
00:29:32,880 --> 00:29:36,080
our protocol is called capsi to uh

818
00:29:36,080 --> 00:29:38,159
improve your training set locally and

819
00:29:38,159 --> 00:29:40,559
then to fine-tune your model and so in

820
00:29:40,559 --> 00:29:42,320
particular what we found is that if one

821
00:29:42,320 --> 00:29:45,039
of the participant is lacking

822
00:29:45,039 --> 00:29:47,440
data for a specific sub-population in

823
00:29:47,440 --> 00:29:50,399
the training set then this can allow

824
00:29:50,399 --> 00:29:51,520
them

825
00:29:51,520 --> 00:29:53,600
to improve the fairness of their

826
00:29:53,600 --> 00:29:56,480
classifier by participating in in this

827
00:29:56,480 --> 00:29:58,640
protocol and so if you're interested in

828
00:29:58,640 --> 00:30:00,559
this i i recommend checking out the

829
00:30:00,559 --> 00:30:03,760
paper which is called capsi learning um

830
00:30:03,760 --> 00:30:04,559
and

831
00:30:04,559 --> 00:30:06,080
essentially you'll see that there's a

832
00:30:06,080 --> 00:30:06,960
lot of

833
00:30:06,960 --> 00:30:09,520
opportunities for

834
00:30:09,520 --> 00:30:11,200
cryptographic improvements of the

835
00:30:11,200 --> 00:30:13,840
protocol in particular here as you've

836
00:30:13,840 --> 00:30:15,760
noticed i've assumed the existence of a

837
00:30:15,760 --> 00:30:17,919
central party so it would be interesting

838
00:30:17,919 --> 00:30:19,919
to have an alternative to this protocol

839
00:30:19,919 --> 00:30:21,440
that does not assume

840
00:30:21,440 --> 00:30:24,000
trust in this third party

841
00:30:24,000 --> 00:30:25,760
and of course

842
00:30:25,760 --> 00:30:27,919
the homomorphic machining

843
00:30:27,919 --> 00:30:29,679
component of the protocol is a

844
00:30:29,679 --> 00:30:31,919
computational bottleneck

845
00:30:31,919 --> 00:30:33,760
in terms of the performance of the

846
00:30:33,760 --> 00:30:35,520
end-to-end protocol so that's always

847
00:30:35,520 --> 00:30:38,240
something that we could benefit from

848
00:30:38,240 --> 00:30:40,720
progress

849
00:30:40,720 --> 00:30:42,080
okay

850
00:30:42,080 --> 00:30:42,960
so

851
00:30:42,960 --> 00:30:45,760
again i just want to

852
00:30:45,760 --> 00:30:48,320
pause here to to remind you of sort of

853
00:30:48,320 --> 00:30:49,520
the different reasons that i've

854
00:30:49,520 --> 00:30:51,600
mentioned differential privacy being

855
00:30:51,600 --> 00:30:53,679
advantages in machine learning

856
00:30:53,679 --> 00:30:56,000
and the first one is that there is not a

857
00:30:56,000 --> 00:30:58,000
necessary trade-off between the privacy

858
00:30:58,000 --> 00:31:00,240
and the machining objective

859
00:31:00,240 --> 00:31:02,320
while it is true that there is often a

860
00:31:02,320 --> 00:31:04,720
trade-off between privacy and

861
00:31:04,720 --> 00:31:07,039
the accuracy of the model it's not

862
00:31:07,039 --> 00:31:08,559
fundamental

863
00:31:08,559 --> 00:31:10,559
we should be able to achieve both

864
00:31:10,559 --> 00:31:12,559
simultaneously

865
00:31:12,559 --> 00:31:15,760
and what we see in practice is that we

866
00:31:15,760 --> 00:31:17,919
have a degradation that is smooth when

867
00:31:17,919 --> 00:31:20,240
it is not possible to learn something

868
00:31:20,240 --> 00:31:21,440
privately

869
00:31:21,440 --> 00:31:22,240
and

870
00:31:22,240 --> 00:31:24,480
in particular i'm now going to give you

871
00:31:24,480 --> 00:31:26,080
two examples

872
00:31:26,080 --> 00:31:28,240
of when differential privacy is not a

873
00:31:28,240 --> 00:31:30,000
silver bullet

874
00:31:30,000 --> 00:31:32,640
the first one is

875
00:31:32,640 --> 00:31:35,600
machine unlearning where instead

876
00:31:35,600 --> 00:31:37,360
we're trying to

877
00:31:37,360 --> 00:31:39,440
delete a training point from from the

878
00:31:39,440 --> 00:31:42,080
machining model and so this is a very

879
00:31:42,080 --> 00:31:45,439
difficult problem

880
00:31:45,519 --> 00:31:47,919
that comes up for the first presenting

881
00:31:47,919 --> 00:31:50,960
attacks the second one is uh laws like

882
00:31:50,960 --> 00:31:53,600
the gdpr regulations like the gdpr which

883
00:31:53,600 --> 00:31:54,960
encourage

884
00:31:54,960 --> 00:31:58,480
users to basically request

885
00:31:58,480 --> 00:32:00,320
the deletion of data that they've

886
00:32:00,320 --> 00:32:03,519
previously allowed company to use

887
00:32:03,519 --> 00:32:05,840
and so here differential privacy is not

888
00:32:05,840 --> 00:32:08,080
a silver bullet because in differential

889
00:32:08,080 --> 00:32:10,000
privacy you're bounding the contribution

890
00:32:10,000 --> 00:32:11,840
of each training point

891
00:32:11,840 --> 00:32:14,080
whereas in machine unlearning you would

892
00:32:14,080 --> 00:32:15,840
like to make the contribution of a

893
00:32:15,840 --> 00:32:19,120
specific point be zero so not only you

894
00:32:19,120 --> 00:32:20,960
want a bound that is zero but you want

895
00:32:20,960 --> 00:32:23,039
that bound to only hold for a specific

896
00:32:23,039 --> 00:32:24,640
point whereas in differential privacy

897
00:32:24,640 --> 00:32:26,080
you want that bound to hold for any

898
00:32:26,080 --> 00:32:28,320
possible

899
00:32:28,320 --> 00:32:30,399
and machine unlearning is very difficult

900
00:32:30,399 --> 00:32:32,960
in particular in

901
00:32:32,960 --> 00:32:35,519
deep learning because it is very

902
00:32:35,519 --> 00:32:37,519
difficult to estimate the influence of

903
00:32:37,519 --> 00:32:39,919
each training example in

904
00:32:39,919 --> 00:32:41,360
in the final model

905
00:32:41,360 --> 00:32:43,440
and so this is for many reasons the

906
00:32:43,440 --> 00:32:45,279
first one is the stochasticity of the

907
00:32:45,279 --> 00:32:47,200
training procedure

908
00:32:47,200 --> 00:32:50,080
in the way that data is sampled but also

909
00:32:50,080 --> 00:32:52,000
the objective itself having multiple

910
00:32:52,000 --> 00:32:54,720
minima but the second one is the nature

911
00:32:54,720 --> 00:32:56,960
of the algorithm being iterative during

912
00:32:56,960 --> 00:32:59,519
training which means that the

913
00:32:59,519 --> 00:33:01,519
the recursion means that we have this

914
00:33:01,519 --> 00:33:03,679
incremental learning procedure that is

915
00:33:03,679 --> 00:33:05,519
difficult

916
00:33:05,519 --> 00:33:06,240
to

917
00:33:06,240 --> 00:33:09,039
analyze the influence of each training

918
00:33:09,039 --> 00:33:12,000
point in the final model parameters that

919
00:33:12,000 --> 00:33:13,120
are learned

920
00:33:13,120 --> 00:33:16,080
through the the incremental procedure

921
00:33:16,080 --> 00:33:17,840
and so

922
00:33:17,840 --> 00:33:20,080
this led us to basically observe that we

923
00:33:20,080 --> 00:33:21,840
won't be able to achieve machine

924
00:33:21,840 --> 00:33:24,640
unlearning through differential privacy

925
00:33:24,640 --> 00:33:26,559
instead we have to look at a slightly

926
00:33:26,559 --> 00:33:28,880
different definition of what it means to

927
00:33:28,880 --> 00:33:29,919
unlearn

928
00:33:29,919 --> 00:33:31,679
which is that we want the distribution

929
00:33:31,679 --> 00:33:34,240
of models that we can learn from a data

930
00:33:34,240 --> 00:33:35,519
set d

931
00:33:35,519 --> 00:33:37,120
to be the same

932
00:33:37,120 --> 00:33:39,600
then the distribution of models that we

933
00:33:39,600 --> 00:33:42,159
would achieve by first learning on this

934
00:33:42,159 --> 00:33:44,480
data set combined with a particular

935
00:33:44,480 --> 00:33:45,760
point

936
00:33:45,760 --> 00:33:48,880
and then unlearning from this point

937
00:33:48,880 --> 00:33:49,919
to

938
00:33:49,919 --> 00:33:52,000
basically remove it from the training

939
00:33:52,000 --> 00:33:54,080
set so what this means is that by

940
00:33:54,080 --> 00:33:56,080
learning and on learning

941
00:33:56,080 --> 00:33:58,240
you get the same distribution of models

942
00:33:58,240 --> 00:34:00,080
than if you had learned in the first

943
00:34:00,080 --> 00:34:03,279
place without the specific point to be

944
00:34:03,279 --> 00:34:05,200
unlearned

945
00:34:05,200 --> 00:34:07,120
and so this is a very strong guarantee

946
00:34:07,120 --> 00:34:08,879
because what this means is that you

947
00:34:08,879 --> 00:34:11,839
could have obtained the model here that

948
00:34:11,839 --> 00:34:14,239
you have on the right

949
00:34:14,239 --> 00:34:15,679
which has been obtained by first

950
00:34:15,679 --> 00:34:17,520
learning and then on learning you could

951
00:34:17,520 --> 00:34:19,520
have obtained that model

952
00:34:19,520 --> 00:34:21,839
directly by

953
00:34:21,839 --> 00:34:24,639
performing another round of training

954
00:34:24,639 --> 00:34:27,440
from scratch from the original data set

955
00:34:27,440 --> 00:34:29,119
that did not contain the point to be

956
00:34:29,119 --> 00:34:30,480
unlearned

957
00:34:30,480 --> 00:34:33,040
and so this is very difficult to achieve

958
00:34:33,040 --> 00:34:34,719
again with things like deep neural

959
00:34:34,719 --> 00:34:36,159
networks

960
00:34:36,159 --> 00:34:38,159
and so the way that we

961
00:34:38,159 --> 00:34:40,079
propose to achieve this definition of

962
00:34:40,079 --> 00:34:43,280
learning is to introduce two knobs in

963
00:34:43,280 --> 00:34:45,040
the way that machine learning models are

964
00:34:45,040 --> 00:34:46,320
trained

965
00:34:46,320 --> 00:34:47,760
and i'm going to discuss this very

966
00:34:47,760 --> 00:34:50,000
briefly because i want to leave time uh

967
00:34:50,000 --> 00:34:51,599
for another topic

968
00:34:51,599 --> 00:34:53,359
but the idea is to say

969
00:34:53,359 --> 00:34:56,320
there are two issues uh with stochastic

970
00:34:56,320 --> 00:34:59,280
green in this set the first one is that

971
00:34:59,280 --> 00:35:02,480
we are analyzing the entire training set

972
00:35:02,480 --> 00:35:04,960
at each epoch which makes it

973
00:35:04,960 --> 00:35:06,640
very difficult

974
00:35:06,640 --> 00:35:08,960
to tie back the contribution of a

975
00:35:08,960 --> 00:35:11,839
specific point into the model parameters

976
00:35:11,839 --> 00:35:13,119
that we're learning so what we're going

977
00:35:13,119 --> 00:35:16,240
to do is reorder the way that training

978
00:35:16,240 --> 00:35:17,920
points are analyzed

979
00:35:17,920 --> 00:35:21,359
to first learn from a subset of the data

980
00:35:21,359 --> 00:35:24,000
let's say we learn from 10 of the data

981
00:35:24,000 --> 00:35:27,839
and we can save the state of the model

982
00:35:27,839 --> 00:35:31,119
and then introduce an additional 10 of

983
00:35:31,119 --> 00:35:34,000
the data and then save again

984
00:35:34,000 --> 00:35:35,680
another state of the model and then

985
00:35:35,680 --> 00:35:38,720
continue introducing data until we have

986
00:35:38,720 --> 00:35:40,400
analyzed the entire training set what

987
00:35:40,400 --> 00:35:42,079
this means is that once we want to

988
00:35:42,079 --> 00:35:43,520
unlearn

989
00:35:43,520 --> 00:35:45,920
we can now trace back

990
00:35:45,920 --> 00:35:46,960
the

991
00:35:46,960 --> 00:35:49,200
point in time where we introduce the

992
00:35:49,200 --> 00:35:51,680
specific point to be unlearned then we

993
00:35:51,680 --> 00:35:53,920
can recover the checkpoint that did not

994
00:35:53,920 --> 00:35:55,119
contain

995
00:35:55,119 --> 00:35:57,680
the training point in particular and we

996
00:35:57,680 --> 00:35:59,839
can resume training from that that

997
00:35:59,839 --> 00:36:01,680
checkpoint and so in expectation we're

998
00:36:01,680 --> 00:36:03,839
going to get an improvement in how much

999
00:36:03,839 --> 00:36:06,240
work we have to do to retrain

1000
00:36:06,240 --> 00:36:07,520
the model

1001
00:36:07,520 --> 00:36:09,440
another problem was to cast a grid and

1002
00:36:09,440 --> 00:36:12,000
decide is that a specific training point

1003
00:36:12,000 --> 00:36:14,079
influences the entire model parameter

1004
00:36:14,079 --> 00:36:17,599
vector so a point may contribute to an

1005
00:36:17,599 --> 00:36:19,280
update in all of the different

1006
00:36:19,280 --> 00:36:21,280
parameters

1007
00:36:21,280 --> 00:36:22,160
and so

1008
00:36:22,160 --> 00:36:24,480
what we propose here is to again use the

1009
00:36:24,480 --> 00:36:27,040
same intuition behind pate and to

1010
00:36:27,040 --> 00:36:29,760
instead of training one large model from

1011
00:36:29,760 --> 00:36:31,599
the entire data set

1012
00:36:31,599 --> 00:36:34,000
we propose to train an ensemble of

1013
00:36:34,000 --> 00:36:36,240
models and what this means is that we're

1014
00:36:36,240 --> 00:36:38,160
training smaller models

1015
00:36:38,160 --> 00:36:41,200
from disjoint partitions and so if we

1016
00:36:41,200 --> 00:36:45,040
want to unlearn a specific

1017
00:36:46,000 --> 00:36:47,079
we need to

1018
00:36:47,079 --> 00:36:50,320
re-then train one single large model

1019
00:36:50,320 --> 00:36:51,680
from scratch

1020
00:36:51,680 --> 00:36:53,839
and so if we combine these two knobs

1021
00:36:53,839 --> 00:36:56,000
what we find is that by reordering the

1022
00:36:56,000 --> 00:36:58,160
data and also partitioning the data

1023
00:36:58,160 --> 00:37:00,880
we're able to effectively retrain the

1024
00:37:00,880 --> 00:37:01,920
model

1025
00:37:01,920 --> 00:37:04,880
as if it had been retrained from scratch

1026
00:37:04,880 --> 00:37:07,359
and to get this equality of distribution

1027
00:37:07,359 --> 00:37:09,920
of models that i just described

1028
00:37:09,920 --> 00:37:12,720
at a much smaller price in terms of

1029
00:37:12,720 --> 00:37:15,040
computation than if we had to retrain

1030
00:37:15,040 --> 00:37:18,240
the entire model from scratch

1031
00:37:18,240 --> 00:37:19,920
and so here i think this is an example

1032
00:37:19,920 --> 00:37:20,880
of where

1033
00:37:20,880 --> 00:37:23,520
the definition of differential privacy

1034
00:37:23,520 --> 00:37:26,480
is not an ideal fit for

1035
00:37:26,480 --> 00:37:29,359
this flavor of privacy but i think here

1036
00:37:29,359 --> 00:37:31,359
there is work to be done in sort of

1037
00:37:31,359 --> 00:37:33,119
improving this definition here that i

1038
00:37:33,119 --> 00:37:35,680
presented which is very crude and very

1039
00:37:35,680 --> 00:37:37,920
difficult to achieve so that we can

1040
00:37:37,920 --> 00:37:41,520
propose mechanisms that more

1041
00:37:41,520 --> 00:37:43,520
effectively unlearn even more

1042
00:37:43,520 --> 00:37:47,280
effectively than this mechanism here

1043
00:37:47,280 --> 00:37:48,800
the second reason why differential

1044
00:37:48,800 --> 00:37:51,119
privacy is not a civil bullying is that

1045
00:37:51,119 --> 00:37:53,760
it introduces a trade-off with fairness

1046
00:37:53,760 --> 00:37:55,839
where in data sets where you have a

1047
00:37:55,839 --> 00:37:58,800
heavy tail such as medical data sets

1048
00:37:58,800 --> 00:38:00,720
the privacy will prevent you from

1049
00:38:00,720 --> 00:38:03,760
learning from tales of your distribution

1050
00:38:03,760 --> 00:38:06,720
for which you have very few examples of

1051
00:38:06,720 --> 00:38:08,480
whereas fairness will

1052
00:38:08,480 --> 00:38:11,520
ask you to have good performance

1053
00:38:11,520 --> 00:38:13,119
on these

1054
00:38:13,119 --> 00:38:14,960
tales of distribution

1055
00:38:14,960 --> 00:38:16,480
and so what this means is that as you

1056
00:38:16,480 --> 00:38:18,560
increase the strength of the privacy

1057
00:38:18,560 --> 00:38:21,200
guarantee you're degrading

1058
00:38:21,200 --> 00:38:23,440
your fairness criteria

1059
00:38:23,440 --> 00:38:26,240
and so this is something uh fairly

1060
00:38:26,240 --> 00:38:28,160
fundamental to certain data sets and in

1061
00:38:28,160 --> 00:38:29,760
particular here in this research we

1062
00:38:29,760 --> 00:38:31,200
found this is

1063
00:38:31,200 --> 00:38:33,839
consistent across several medical data

1064
00:38:33,839 --> 00:38:36,800
sets and so i think here we need to have

1065
00:38:36,800 --> 00:38:40,240
more work to understand how we can adapt

1066
00:38:40,240 --> 00:38:41,920
the definition of what we mean to be

1067
00:38:41,920 --> 00:38:44,240
private to this

1068
00:38:44,240 --> 00:38:47,280
simultaneous fairness criteria and in

1069
00:38:47,280 --> 00:38:50,079
particular we have to visit ideas

1070
00:38:50,079 --> 00:38:52,800
uh where we are going to provide

1071
00:38:52,800 --> 00:38:54,480
guarantees that are specific to

1072
00:38:54,480 --> 00:38:56,480
different points

1073
00:38:56,480 --> 00:38:58,240
in terms of the privacy

1074
00:38:58,240 --> 00:39:01,359
which is of course uh a difficult

1075
00:39:01,359 --> 00:39:03,920
uh question to to navigate in particular

1076
00:39:03,920 --> 00:39:07,440
because it raises ethical uh concerns

1077
00:39:07,440 --> 00:39:08,480
so

1078
00:39:08,480 --> 00:39:11,359
i think what one of the conclusions here

1079
00:39:11,359 --> 00:39:13,839
is that we need to do a lot more work

1080
00:39:13,839 --> 00:39:15,280
aligning machine learning with human

1081
00:39:15,280 --> 00:39:16,960
norms and i've discussed privacy and

1082
00:39:16,960 --> 00:39:18,480
fairness in this first part of the

1083
00:39:18,480 --> 00:39:20,960
presentation but i think in general

1084
00:39:20,960 --> 00:39:22,480
there is a need of

1085
00:39:22,480 --> 00:39:24,320
better definitions like differential

1086
00:39:24,320 --> 00:39:26,720
privacy that align with the goal of

1087
00:39:26,720 --> 00:39:28,320
machine the machine learning community

1088
00:39:28,320 --> 00:39:30,480
which is to produce a classifier that

1089
00:39:30,480 --> 00:39:32,960
generalizes well and so this is work

1090
00:39:32,960 --> 00:39:34,560
where i see a lot of

1091
00:39:34,560 --> 00:39:37,119
opportunities in particular because the

1092
00:39:37,119 --> 00:39:38,240
community

1093
00:39:38,240 --> 00:39:40,240
uh in at the intersection of machine

1094
00:39:40,240 --> 00:39:42,160
learning and security is treating each

1095
00:39:42,160 --> 00:39:44,320
of these problems independently so some

1096
00:39:44,320 --> 00:39:45,680
people are working on privacy some

1097
00:39:45,680 --> 00:39:47,200
people are working on fairness but we're

1098
00:39:47,200 --> 00:39:49,680
not capturing the overall

1099
00:39:49,680 --> 00:39:52,320
trustworthiness of the classifier and

1100
00:39:52,320 --> 00:39:54,160
there are tensions that will be very

1101
00:39:54,160 --> 00:39:55,599
difficult to resolve if we don't

1102
00:39:55,599 --> 00:39:57,839
incorporate this thinking

1103
00:39:57,839 --> 00:39:59,680
ahead of times

1104
00:39:59,680 --> 00:40:01,359
the the last part of the presentation

1105
00:40:01,359 --> 00:40:03,040
i'll try to

1106
00:40:03,040 --> 00:40:06,480
finish in in the next 10 minutes or so

1107
00:40:06,480 --> 00:40:08,000
is

1108
00:40:08,000 --> 00:40:10,800
fairly different and is going to discuss

1109
00:40:10,800 --> 00:40:13,440
uh what i think is another problem space

1110
00:40:13,440 --> 00:40:14,400
where

1111
00:40:14,400 --> 00:40:16,560
cryptographers can help us

1112
00:40:16,560 --> 00:40:19,040
in the machine learning community

1113
00:40:19,040 --> 00:40:21,520
which is the question of model ownership

1114
00:40:21,520 --> 00:40:23,280
and so if you recall in the very

1115
00:40:23,280 --> 00:40:24,640
beginning of the presentation i

1116
00:40:24,640 --> 00:40:27,599
discussed model stealing attacks where

1117
00:40:27,599 --> 00:40:29,920
an adversary that has access to the

1118
00:40:29,920 --> 00:40:31,680
predictions of your model

1119
00:40:31,680 --> 00:40:35,040
is able to extract a copy of that model

1120
00:40:35,040 --> 00:40:37,680
if they have unbounded query ability

1121
00:40:37,680 --> 00:40:39,920
right this is pretty intuitive if you

1122
00:40:39,920 --> 00:40:41,119
have a function

1123
00:40:41,119 --> 00:40:43,839
if you can ask what the value of that

1124
00:40:43,839 --> 00:40:46,079
function is for any point of your choice

1125
00:40:46,079 --> 00:40:47,359
eventually you can recover an

1126
00:40:47,359 --> 00:40:49,599
approximation of that function

1127
00:40:49,599 --> 00:40:52,079
and so here this is a pretty fundamental

1128
00:40:52,079 --> 00:40:54,079
problem in machine learning

1129
00:40:54,079 --> 00:40:55,839
and in particular

1130
00:40:55,839 --> 00:40:58,240
progress in machining

1131
00:40:58,240 --> 00:41:00,079
in learning from

1132
00:41:00,079 --> 00:41:02,000
little supervision has made model

1133
00:41:02,000 --> 00:41:04,160
stealing a lot more efficient what this

1134
00:41:04,160 --> 00:41:06,480
means is that it has made the query

1135
00:41:06,480 --> 00:41:09,920
complexity of model stealing a lot lower

1136
00:41:09,920 --> 00:41:11,760
and in particular there's an area of

1137
00:41:11,760 --> 00:41:13,119
machine learning research called

1138
00:41:13,119 --> 00:41:15,440
semi-supervised learning where the goal

1139
00:41:15,440 --> 00:41:18,319
is to learn from partially labeled data

1140
00:41:18,319 --> 00:41:20,800
rather than fully labeled data

1141
00:41:20,800 --> 00:41:23,280
and so this facilitates model stealing

1142
00:41:23,280 --> 00:41:24,480
attacks

1143
00:41:24,480 --> 00:41:26,319
and this is something that one of my

1144
00:41:26,319 --> 00:41:28,480
interns matthew

1145
00:41:28,480 --> 00:41:31,280
showed because once you have an

1146
00:41:31,280 --> 00:41:32,960
unlabeled set of

1147
00:41:32,960 --> 00:41:36,240
data points you only need to have the

1148
00:41:36,240 --> 00:41:38,400
model that you would like to steal

1149
00:41:38,400 --> 00:41:41,839
label a subset of these points

1150
00:41:41,839 --> 00:41:43,599
and so what this means is that

1151
00:41:43,599 --> 00:41:45,839
you can

1152
00:41:45,839 --> 00:41:46,880
achieve

1153
00:41:46,880 --> 00:41:48,480
completely total

1154
00:41:48,480 --> 00:41:50,160
while making

1155
00:41:50,160 --> 00:41:52,880
much fewer queries uh to the model that

1156
00:41:52,880 --> 00:41:55,200
you're trying to steal and so this is uh

1157
00:41:55,200 --> 00:41:57,200
this is just making it model stealing

1158
00:41:57,200 --> 00:41:59,680
attacks easier another reason is that

1159
00:41:59,680 --> 00:42:03,119
machining researchers are more and more

1160
00:42:03,119 --> 00:42:04,240
relying

1161
00:42:04,240 --> 00:42:08,240
on large pre-trained models to produce

1162
00:42:08,240 --> 00:42:11,200
models for specific tasks so you've

1163
00:42:11,200 --> 00:42:13,200
most likely seen developments around

1164
00:42:13,200 --> 00:42:17,119
gpt3 for instance as a language

1165
00:42:17,119 --> 00:42:19,760
model and so a lot of models are being

1166
00:42:19,760 --> 00:42:21,040
fine-tuned

1167
00:42:21,040 --> 00:42:22,079
from

1168
00:42:22,079 --> 00:42:24,160
from these large pre-trained models what

1169
00:42:24,160 --> 00:42:25,680
this means is the adversary has

1170
00:42:25,680 --> 00:42:26,800
knowledge

1171
00:42:26,800 --> 00:42:28,240
about all

1172
00:42:28,240 --> 00:42:29,839
language models that are produced

1173
00:42:29,839 --> 00:42:32,480
because the adversary knows that if they

1174
00:42:32,480 --> 00:42:35,599
start from this initial checkpoint

1175
00:42:35,599 --> 00:42:37,599
they're more likely to achieve a model

1176
00:42:37,599 --> 00:42:39,680
that is similar to the model that

1177
00:42:39,680 --> 00:42:41,040
they're trying to steal and in

1178
00:42:41,040 --> 00:42:42,400
particular what this means is that you

1179
00:42:42,400 --> 00:42:44,960
can steal machinery models with purely

1180
00:42:44,960 --> 00:42:47,520
random queries that are not from the

1181
00:42:47,520 --> 00:42:49,440
data distribution that the model has

1182
00:42:49,440 --> 00:42:51,200
learned from which is

1183
00:42:51,200 --> 00:42:53,280
pretty surprising and this holds for

1184
00:42:53,280 --> 00:42:55,599
language because of this pre-training

1185
00:42:55,599 --> 00:42:58,560
aspect but it does not hold for vision

1186
00:42:58,560 --> 00:43:01,680
which is something that i was surprised

1187
00:43:01,680 --> 00:43:02,880
and so

1188
00:43:02,880 --> 00:43:05,119
in practice what i want to get across

1189
00:43:05,119 --> 00:43:08,000
here is that it's impossible to prevent

1190
00:43:08,000 --> 00:43:10,720
model extraction from happening because

1191
00:43:10,720 --> 00:43:13,040
if you want to reveal the prediction

1192
00:43:13,040 --> 00:43:14,400
that the model is making to your

1193
00:43:14,400 --> 00:43:17,119
legitimate users there's no way that you

1194
00:43:17,119 --> 00:43:18,960
can prevent the adversary from using

1195
00:43:18,960 --> 00:43:21,680
that information to recreate the model

1196
00:43:21,680 --> 00:43:23,359
locally

1197
00:43:23,359 --> 00:43:25,440
and so what we've been working on in my

1198
00:43:25,440 --> 00:43:28,560
group is post hoc approaches to

1199
00:43:28,560 --> 00:43:29,760
preventing

1200
00:43:29,760 --> 00:43:31,839
model stealing the first one is to

1201
00:43:31,839 --> 00:43:35,040
detect whether a model is stolen or not

1202
00:43:35,040 --> 00:43:36,640
and so this is something we call data

1203
00:43:36,640 --> 00:43:38,960
set inference where we flipped around

1204
00:43:38,960 --> 00:43:40,800
the idea behind membership inference

1205
00:43:40,800 --> 00:43:44,400
attacks to the advantage of

1206
00:43:44,400 --> 00:43:47,280
the the model owner and so the idea here

1207
00:43:47,280 --> 00:43:49,280
is that someone trying to steal your

1208
00:43:49,280 --> 00:43:50,880
machine learning model

1209
00:43:50,880 --> 00:43:53,280
is trying to derive knowledge that was

1210
00:43:53,280 --> 00:43:55,280
contained in your training set no matter

1211
00:43:55,280 --> 00:43:57,680
how they steal the machining model

1212
00:43:57,680 --> 00:43:59,200
they could use a very simple attack they

1213
00:43:59,200 --> 00:44:00,800
could use some of the more sophisticated

1214
00:44:00,800 --> 00:44:02,720
attacks that i just described

1215
00:44:02,720 --> 00:44:04,400
what they're trying to get at is that

1216
00:44:04,400 --> 00:44:06,160
you have training data that they do not

1217
00:44:06,160 --> 00:44:08,240
have access to and they want to learn

1218
00:44:08,240 --> 00:44:09,680
from that training data

1219
00:44:09,680 --> 00:44:11,920
and so what this means is that a useful

1220
00:44:11,920 --> 00:44:14,400
stolen model will contain

1221
00:44:14,400 --> 00:44:15,680
training

1222
00:44:15,680 --> 00:44:17,520
will contain secret information that was

1223
00:44:17,520 --> 00:44:19,359
contained in your training set

1224
00:44:19,359 --> 00:44:22,160
and so as a model owner if you suspect

1225
00:44:22,160 --> 00:44:24,319
someone has stolen your model

1226
00:44:24,319 --> 00:44:26,480
what you can do is

1227
00:44:26,480 --> 00:44:28,079
take your training set

1228
00:44:28,079 --> 00:44:31,280
and query the stolen model with these

1229
00:44:31,280 --> 00:44:33,359
training points

1230
00:44:33,359 --> 00:44:34,319
and

1231
00:44:34,319 --> 00:44:37,359
what you will notice is that when this

1232
00:44:37,359 --> 00:44:39,760
the model was indeed stolen

1233
00:44:39,760 --> 00:44:41,920
it will produce

1234
00:44:41,920 --> 00:44:44,400
predictions that reveal that your

1235
00:44:44,400 --> 00:44:48,160
trained data is a member of that model's

1236
00:44:48,160 --> 00:44:49,839
training

1237
00:44:49,839 --> 00:44:52,480
and so what this means is that you can

1238
00:44:52,480 --> 00:44:54,000
take

1239
00:44:54,000 --> 00:44:56,079
a few of your training points

1240
00:44:56,079 --> 00:44:57,839
for instance use the label only

1241
00:44:57,839 --> 00:45:00,000
membership inference attack

1242
00:45:00,000 --> 00:45:04,160
which consists in querying the the model

1243
00:45:04,160 --> 00:45:04,880
and

1244
00:45:04,880 --> 00:45:06,880
you will essentially

1245
00:45:06,880 --> 00:45:08,160
get

1246
00:45:08,160 --> 00:45:11,599
an um undeniable proof that this model

1247
00:45:11,599 --> 00:45:14,000
contains the same information than your

1248
00:45:14,000 --> 00:45:15,280
training set

1249
00:45:15,280 --> 00:45:18,160
and what's interesting there is that in

1250
00:45:18,160 --> 00:45:20,319
the context of privacy

1251
00:45:20,319 --> 00:45:21,520
you can

1252
00:45:21,520 --> 00:45:24,480
it's difficult to infer membership

1253
00:45:24,480 --> 00:45:27,280
reliably because you're trying to infer

1254
00:45:27,280 --> 00:45:30,000
the membership on a of an individual

1255
00:45:30,000 --> 00:45:31,680
training point

1256
00:45:31,680 --> 00:45:33,839
whereas here in this context you have

1257
00:45:33,839 --> 00:45:36,160
access to the entire training set

1258
00:45:36,160 --> 00:45:39,359
so you can take this the leakage the

1259
00:45:39,359 --> 00:45:42,000
privacy leakage from individual training

1260
00:45:42,000 --> 00:45:42,960
points

1261
00:45:42,960 --> 00:45:45,359
and aggregate this leakage

1262
00:45:45,359 --> 00:45:48,000
over multiple training points until you

1263
00:45:48,000 --> 00:45:50,400
have enough confidence that the model

1264
00:45:50,400 --> 00:45:52,880
was indeed stolen and trained

1265
00:45:52,880 --> 00:45:54,000
from

1266
00:45:54,000 --> 00:45:55,760
from your training data

1267
00:45:55,760 --> 00:45:57,200
and so this is

1268
00:45:57,200 --> 00:45:59,280
something that we've presented earlier

1269
00:45:59,280 --> 00:46:01,760
this year and

1270
00:46:01,760 --> 00:46:02,800
basically

1271
00:46:02,800 --> 00:46:04,400
i think this is very interesting because

1272
00:46:04,400 --> 00:46:06,640
it's a rare example where

1273
00:46:06,640 --> 00:46:09,359
we're providing an asymmetric advantage

1274
00:46:09,359 --> 00:46:11,440
to the defender because the defender has

1275
00:46:11,440 --> 00:46:13,680
access to the training set

1276
00:46:13,680 --> 00:46:15,119
they have more information than the

1277
00:46:15,119 --> 00:46:17,359
adversary here and in particular they

1278
00:46:17,359 --> 00:46:20,079
can detect the attack fairly easily

1279
00:46:20,079 --> 00:46:22,640
without revealing much of their uh their

1280
00:46:22,640 --> 00:46:25,520
training set we found in particular on a

1281
00:46:25,520 --> 00:46:28,880
couple of data sets and models that only

1282
00:46:28,880 --> 00:46:31,440
uh revealing 50 of your training points

1283
00:46:31,440 --> 00:46:33,119
was enough to get

1284
00:46:33,119 --> 00:46:35,280
below a one percent false positive rate

1285
00:46:35,280 --> 00:46:38,400
and detecting that the model was stolen

1286
00:46:38,400 --> 00:46:40,240
and so i think this is an interesting

1287
00:46:40,240 --> 00:46:42,160
example where we flipped

1288
00:46:42,160 --> 00:46:45,839
a privacy attack and

1289
00:46:46,000 --> 00:46:47,359
used

1290
00:46:47,359 --> 00:46:49,920
something that comes up a lot in our

1291
00:46:49,920 --> 00:46:52,240
mission animal research

1292
00:46:52,240 --> 00:46:54,000
the other approach that i want to

1293
00:46:54,000 --> 00:46:56,160
discuss before i conclude is called

1294
00:46:56,160 --> 00:46:57,760
proof of learning and so here the idea

1295
00:46:57,760 --> 00:46:59,920
is motivated by model stealing but it's

1296
00:46:59,920 --> 00:47:01,520
a little bit broader and i think there

1297
00:47:01,520 --> 00:47:03,839
are a lot of open problems that would be

1298
00:47:03,839 --> 00:47:05,440
very interesting to solve

1299
00:47:05,440 --> 00:47:07,280
the idea here is to say

1300
00:47:07,280 --> 00:47:08,839
we'd like to

1301
00:47:08,839 --> 00:47:12,400
obtain a proof that we've trained a

1302
00:47:12,400 --> 00:47:14,800
particular machine learning model

1303
00:47:14,800 --> 00:47:16,560
and so this is a proof that we could use

1304
00:47:16,560 --> 00:47:19,359
to claim ownership for that model but

1305
00:47:19,359 --> 00:47:21,920
also just to prove that the training

1306
00:47:21,920 --> 00:47:23,680
procedure was

1307
00:47:23,680 --> 00:47:24,720
wrong

1308
00:47:24,720 --> 00:47:26,640
with

1309
00:47:26,640 --> 00:47:28,559
correctly so basically to prove the

1310
00:47:28,559 --> 00:47:31,200
integrity of the training procedure

1311
00:47:31,200 --> 00:47:34,000
and so the idea here is to observe that

1312
00:47:34,000 --> 00:47:36,160
training deep neural networks

1313
00:47:36,160 --> 00:47:38,800
accumulates secret information

1314
00:47:38,800 --> 00:47:40,480
organically

1315
00:47:40,480 --> 00:47:42,720
because of the stochasticity of gradient

1316
00:47:42,720 --> 00:47:44,240
descent

1317
00:47:44,240 --> 00:47:46,559
and so it's very hard

1318
00:47:46,559 --> 00:47:50,079
for instance to reproduce twice

1319
00:47:50,079 --> 00:47:52,880
the same machine learning model

1320
00:47:52,880 --> 00:47:56,079
even by simply repeating the exact

1321
00:47:56,079 --> 00:47:58,079
training procedure on the same training

1322
00:47:58,079 --> 00:48:00,319
set and so what this means in practice

1323
00:48:00,319 --> 00:48:02,960
is that each training run

1324
00:48:02,960 --> 00:48:04,400
accumulates

1325
00:48:04,400 --> 00:48:05,920
some secret information from the

1326
00:48:05,920 --> 00:48:07,599
stochasticity

1327
00:48:07,599 --> 00:48:10,079
and so what we proposed here is to say

1328
00:48:10,079 --> 00:48:10,800
well

1329
00:48:10,800 --> 00:48:13,520
if we would like as a model owner

1330
00:48:13,520 --> 00:48:16,319
to provide a proof to a verifier which

1331
00:48:16,319 --> 00:48:20,240
could be and for instance an auditing a

1332
00:48:20,240 --> 00:48:22,400
regulator for instance that is auditing

1333
00:48:22,400 --> 00:48:24,400
us and trying to assess whether we

1334
00:48:24,400 --> 00:48:26,559
indeed train that model or not

1335
00:48:26,559 --> 00:48:28,559
then what we can do is we can reveal

1336
00:48:28,559 --> 00:48:31,440
information from our training run secret

1337
00:48:31,440 --> 00:48:33,440
information from our training run

1338
00:48:33,440 --> 00:48:35,520
to prove that we have indeed performed

1339
00:48:35,520 --> 00:48:38,079
that training run correctly

1340
00:48:38,079 --> 00:48:41,280
and in particular what we found is that

1341
00:48:41,280 --> 00:48:42,480
we can

1342
00:48:42,480 --> 00:48:43,599
reveal

1343
00:48:43,599 --> 00:48:46,000
information about the data that was

1344
00:48:46,000 --> 00:48:49,440
sampled at each of the training steps so

1345
00:48:49,440 --> 00:48:52,160
each of the steps of grain descent

1346
00:48:52,160 --> 00:48:52,960
and

1347
00:48:52,960 --> 00:48:56,000
as well auxiliary information

1348
00:48:56,000 --> 00:48:59,520
to seed some of the randomness

1349
00:48:59,520 --> 00:49:00,559
that

1350
00:49:00,559 --> 00:49:02,240
will be introduced by stochastic grain

1351
00:49:02,240 --> 00:49:03,280
and descent

1352
00:49:03,280 --> 00:49:06,480
so that the verifier can take

1353
00:49:06,480 --> 00:49:08,400
the checkpoints that we produce during

1354
00:49:08,400 --> 00:49:09,520
training

1355
00:49:09,520 --> 00:49:11,119
and verify

1356
00:49:11,119 --> 00:49:13,040
some of the steps of the gradient

1357
00:49:13,040 --> 00:49:14,160
descent

1358
00:49:14,160 --> 00:49:15,760
to assess

1359
00:49:15,760 --> 00:49:17,599
whether we've trained

1360
00:49:17,599 --> 00:49:20,400
the model indeed correctly

1361
00:49:20,400 --> 00:49:23,040
with some uh

1362
00:49:23,040 --> 00:49:24,880
degree of certitude

1363
00:49:24,880 --> 00:49:25,839
and so

1364
00:49:25,839 --> 00:49:27,839
the information that we log in this

1365
00:49:27,839 --> 00:49:30,960
proof is uh the following

1366
00:49:30,960 --> 00:49:33,200
we have the model weights which is

1367
00:49:33,200 --> 00:49:36,559
organically saved as the checkpointing

1368
00:49:36,559 --> 00:49:40,079
procedure for all machining frameworks

1369
00:49:40,079 --> 00:49:42,240
to this we add information about the

1370
00:49:42,240 --> 00:49:44,640
data that is sampled at each of the

1371
00:49:44,640 --> 00:49:46,559
training steps so the indices of the

1372
00:49:46,559 --> 00:49:47,920
training points

1373
00:49:47,920 --> 00:49:49,119
we also

1374
00:49:49,119 --> 00:49:51,839
include the signature of these

1375
00:49:51,839 --> 00:49:53,119
training points

1376
00:49:53,119 --> 00:49:56,319
because we want to be able to commit to

1377
00:49:56,319 --> 00:49:58,400
a specific training set and then

1378
00:49:58,400 --> 00:50:00,720
we want to be we want the verifier to be

1379
00:50:00,720 --> 00:50:03,440
able to assess that we indeed use these

1380
00:50:03,440 --> 00:50:05,359
specific training points and then we

1381
00:50:05,359 --> 00:50:07,119
include auxiliary information which is

1382
00:50:07,119 --> 00:50:09,040
essentially hyper parameters of the

1383
00:50:09,040 --> 00:50:11,359
training procedure which are important

1384
00:50:11,359 --> 00:50:14,000
to be able to again

1385
00:50:14,000 --> 00:50:16,079
make the training procedure as

1386
00:50:16,079 --> 00:50:18,720
deterministic as possible once given

1387
00:50:18,720 --> 00:50:20,880
access to that information

1388
00:50:20,880 --> 00:50:23,040
what we do then as a verifier is the

1389
00:50:23,040 --> 00:50:25,200
following the first part is to verify

1390
00:50:25,200 --> 00:50:26,400
the initialization

1391
00:50:26,400 --> 00:50:27,599
of the model

1392
00:50:27,599 --> 00:50:31,680
because a training run is produced from

1393
00:50:31,680 --> 00:50:33,920
two possible initialization either a

1394
00:50:33,920 --> 00:50:36,000
random initialization which we can

1395
00:50:36,000 --> 00:50:38,559
assess whether a particular model

1396
00:50:38,559 --> 00:50:41,760
parameter vector is random or not

1397
00:50:41,760 --> 00:50:42,880
or

1398
00:50:42,880 --> 00:50:45,040
it's the training procedure is going to

1399
00:50:45,040 --> 00:50:48,640
be initialized from a pre-trained model

1400
00:50:48,640 --> 00:50:52,160
in this case we can simply ask the model

1401
00:50:52,160 --> 00:50:53,040
owner

1402
00:50:53,040 --> 00:50:55,760
to produce a proof of learning

1403
00:50:55,760 --> 00:50:58,480
for that specific pre-trained model or

1404
00:50:58,480 --> 00:50:59,359
to

1405
00:50:59,359 --> 00:51:01,280
explain how they got access to that

1406
00:51:01,280 --> 00:51:02,400
model

1407
00:51:02,400 --> 00:51:03,760
that pre-trained model so you can see

1408
00:51:03,760 --> 00:51:06,400
there is a recursive aspect to this

1409
00:51:06,400 --> 00:51:08,960
which is very interesting

1410
00:51:08,960 --> 00:51:11,839
then what the verifier will do is verify

1411
00:51:11,839 --> 00:51:15,119
the entire training sequence of course

1412
00:51:15,119 --> 00:51:16,160
you cannot

1413
00:51:16,160 --> 00:51:18,480
verify the entire training sequence in a

1414
00:51:18,480 --> 00:51:21,280
single shot because as i just explained

1415
00:51:21,280 --> 00:51:23,359
the stochasticity will lead you to a

1416
00:51:23,359 --> 00:51:25,280
very different model parameter state at

1417
00:51:25,280 --> 00:51:27,440
the very end of the training procedure

1418
00:51:27,440 --> 00:51:29,599
so what we observed in this paper is

1419
00:51:29,599 --> 00:51:32,000
that we need to verify

1420
00:51:32,000 --> 00:51:35,760
individual steps of the grid and descent

1421
00:51:35,760 --> 00:51:37,200
so that the

1422
00:51:37,200 --> 00:51:39,599
we're able to reproduce these individual

1423
00:51:39,599 --> 00:51:40,640
steps

1424
00:51:40,640 --> 00:51:42,480
accurately enough

1425
00:51:42,480 --> 00:51:46,160
that we are able to re

1426
00:51:46,160 --> 00:51:47,680
cover

1427
00:51:47,680 --> 00:51:49,680
the parameter of the one produced by the

1428
00:51:49,680 --> 00:51:52,400
model owner so once we do this

1429
00:51:52,400 --> 00:51:54,480
we're able to take

1430
00:51:54,480 --> 00:51:57,599
some of the gradient descent steps

1431
00:51:57,599 --> 00:52:00,400
which we select by their magnitude and

1432
00:52:00,400 --> 00:52:02,160
as a verifier

1433
00:52:02,160 --> 00:52:03,760
we're going to select these gradient

1434
00:52:03,760 --> 00:52:06,319
descent steps look at the auxiliary

1435
00:52:06,319 --> 00:52:08,720
information produced in the proof

1436
00:52:08,720 --> 00:52:11,599
reproduce the gradient descent step

1437
00:52:11,599 --> 00:52:14,880
and then assess that it is indeed

1438
00:52:14,880 --> 00:52:17,119
close to the one provided in the proof

1439
00:52:17,119 --> 00:52:19,119
of learning by

1440
00:52:19,119 --> 00:52:21,760
the model owner and if we get a result

1441
00:52:21,760 --> 00:52:24,079
that is close enough then we can

1442
00:52:24,079 --> 00:52:27,040
we can validate the proof of learning

1443
00:52:27,040 --> 00:52:28,720
and so the model

1444
00:52:28,720 --> 00:52:31,839
indeed belongs to the model owner and

1445
00:52:31,839 --> 00:52:34,079
the integrity of the training procedure

1446
00:52:34,079 --> 00:52:36,480
is uh verified

1447
00:52:36,480 --> 00:52:38,800
here there are a couple of interesting

1448
00:52:38,800 --> 00:52:40,640
things from the security perspective the

1449
00:52:40,640 --> 00:52:42,000
first one is you could wonder how an

1450
00:52:42,000 --> 00:52:44,000
adversary could spoof

1451
00:52:44,000 --> 00:52:46,559
this this verification

1452
00:52:46,559 --> 00:52:47,520
um

1453
00:52:47,520 --> 00:52:48,960
there

1454
00:52:48,960 --> 00:52:51,440
there is difficulty in spoofing the

1455
00:52:51,440 --> 00:52:52,720
proof of learning because of the

1456
00:52:52,720 --> 00:52:54,720
computational cost

1457
00:52:54,720 --> 00:52:56,079
and so you could do many things you

1458
00:52:56,079 --> 00:52:57,760
could try to

1459
00:52:57,760 --> 00:53:01,680
produce a cheaper proof of learning

1460
00:53:01,680 --> 00:53:04,880
without actually training the model

1461
00:53:04,880 --> 00:53:07,280
and what we've done is we've looked at a

1462
00:53:07,280 --> 00:53:09,680
series of strategies for doing that and

1463
00:53:09,680 --> 00:53:11,040
show that

1464
00:53:11,040 --> 00:53:13,760
they can fail essentially they're

1465
00:53:13,760 --> 00:53:17,440
because of the inherent cost of uh

1466
00:53:17,440 --> 00:53:19,760
training the the model and producing the

1467
00:53:19,760 --> 00:53:22,720
proof but what is uh

1468
00:53:22,720 --> 00:53:24,640
important to observe here

1469
00:53:24,640 --> 00:53:26,480
is that

1470
00:53:26,480 --> 00:53:28,400
it is difficult to invert

1471
00:53:28,400 --> 00:53:30,720
a training procedure because if you want

1472
00:53:30,720 --> 00:53:32,720
to invert the training procedure you

1473
00:53:32,720 --> 00:53:34,720
will have to do at least one forward and

1474
00:53:34,720 --> 00:53:37,520
backward pass in your training algorithm

1475
00:53:37,520 --> 00:53:40,400
which will be at least as expensive as

1476
00:53:40,400 --> 00:53:42,559
training itself

1477
00:53:42,559 --> 00:53:43,520
and so

1478
00:53:43,520 --> 00:53:45,920
this is what we observe here in practice

1479
00:53:45,920 --> 00:53:48,400
we looked at several strategies uh one

1480
00:53:48,400 --> 00:53:51,040
of them is fine-tuning from the final

1481
00:53:51,040 --> 00:53:53,040
checkpoint that the model owner has

1482
00:53:53,040 --> 00:53:55,359
released and this does not pass the

1483
00:53:55,359 --> 00:53:58,240
initial verification step that i've

1484
00:53:58,240 --> 00:54:01,760
described another one is concatenating

1485
00:54:01,760 --> 00:54:02,800
two

1486
00:54:02,800 --> 00:54:03,839
valid

1487
00:54:03,839 --> 00:54:06,480
proof of learning sequences

1488
00:54:06,480 --> 00:54:08,960
to produce one that leads to the model

1489
00:54:08,960 --> 00:54:10,720
that you'd like to incorrectly claim

1490
00:54:10,720 --> 00:54:13,440
ownership to this does not work because

1491
00:54:13,440 --> 00:54:15,599
of the fact that we verify

1492
00:54:15,599 --> 00:54:17,359
the largest

1493
00:54:17,359 --> 00:54:19,200
updates and

1494
00:54:19,200 --> 00:54:21,280
essentially the concatenation step will

1495
00:54:21,280 --> 00:54:23,760
be flagged through that process

1496
00:54:23,760 --> 00:54:24,880
and so i'm not going to go into the

1497
00:54:24,880 --> 00:54:26,079
details but

1498
00:54:26,079 --> 00:54:28,480
basically this is interesting because

1499
00:54:28,480 --> 00:54:30,960
we're we're observing that

1500
00:54:30,960 --> 00:54:32,480
the inherent

1501
00:54:32,480 --> 00:54:33,440
uh

1502
00:54:33,440 --> 00:54:36,839
stochasticity of the training procedure

1503
00:54:36,839 --> 00:54:40,480
produces a proof that we've trained the

1504
00:54:40,480 --> 00:54:44,079
model correctly and that we are the the

1505
00:54:44,079 --> 00:54:46,559
correct owner of this model there are

1506
00:54:46,559 --> 00:54:47,920
some

1507
00:54:47,920 --> 00:54:50,079
aspects that i think we would benefit

1508
00:54:50,079 --> 00:54:50,880
from

1509
00:54:50,880 --> 00:54:52,480
from help from the cryptography

1510
00:54:52,480 --> 00:54:54,720
community the first one is

1511
00:54:54,720 --> 00:54:56,319
decreasing the storage cost and the

1512
00:54:56,319 --> 00:54:58,319
computational cost

1513
00:54:58,319 --> 00:54:59,119
here

1514
00:54:59,119 --> 00:55:01,040
the procedure that i've described

1515
00:55:01,040 --> 00:55:02,359
depends on

1516
00:55:02,359 --> 00:55:05,119
a number of things the storage and the

1517
00:55:05,119 --> 00:55:07,760
computational cost but primarily the

1518
00:55:07,760 --> 00:55:09,920
problem is that it's very difficult to

1519
00:55:09,920 --> 00:55:11,680
store efficiently

1520
00:55:11,680 --> 00:55:14,559
a differential checkpoint of the model

1521
00:55:14,559 --> 00:55:16,079
throughout training

1522
00:55:16,079 --> 00:55:18,400
the second issue is that the

1523
00:55:18,400 --> 00:55:21,599
computational cost includes reproducing

1524
00:55:21,599 --> 00:55:24,240
some of the grid and descent steps and

1525
00:55:24,240 --> 00:55:26,960
so this is inherently expensive and so

1526
00:55:26,960 --> 00:55:29,280
we would benefit from uh

1527
00:55:29,280 --> 00:55:31,200
input from for instance techniques from

1528
00:55:31,200 --> 00:55:33,520
zero knowledge proofs to allow us to

1529
00:55:33,520 --> 00:55:36,480
verify these steps without rerunning

1530
00:55:36,480 --> 00:55:37,760
them

1531
00:55:37,760 --> 00:55:38,960
in practice

1532
00:55:38,960 --> 00:55:41,200
and then the last part which i think

1533
00:55:41,200 --> 00:55:43,280
cryptography can help

1534
00:55:43,280 --> 00:55:45,200
is understanding the sources of

1535
00:55:45,200 --> 00:55:47,520
randomness in training machine learning

1536
00:55:47,520 --> 00:55:49,839
models so that we can better control

1537
00:55:49,839 --> 00:55:52,559
them when we're verifying these proofs

1538
00:55:52,559 --> 00:55:54,319
of learning

1539
00:55:54,319 --> 00:55:57,280
okay so this this concludes my talk and

1540
00:55:57,280 --> 00:56:00,559
i i hope that despite the very diverse

1541
00:56:00,559 --> 00:56:04,079
array of topics i've identified a number

1542
00:56:04,079 --> 00:56:06,079
of opportunities for

1543
00:56:06,079 --> 00:56:08,799
input from cryptography in particular i

1544
00:56:08,799 --> 00:56:11,280
just want to summarize that i think

1545
00:56:11,280 --> 00:56:14,000
there are opportunities in data privacy

1546
00:56:14,000 --> 00:56:15,920
going beyond definitions like

1547
00:56:15,920 --> 00:56:17,839
differential privacy

1548
00:56:17,839 --> 00:56:20,000
and model ownership and

1549
00:56:20,000 --> 00:56:21,920
just as i was talking about training

1550
00:56:21,920 --> 00:56:22,799
time

1551
00:56:22,799 --> 00:56:24,559
integrity verification i think there's

1552
00:56:24,559 --> 00:56:26,160
been a lot of work in the cryptography

1553
00:56:26,160 --> 00:56:27,359
community

1554
00:56:27,359 --> 00:56:29,040
in verifying

1555
00:56:29,040 --> 00:56:31,040
machine learning computations at test

1556
00:56:31,040 --> 00:56:32,720
time when making predictions but very

1557
00:56:32,720 --> 00:56:35,680
few a training time when producing

1558
00:56:35,680 --> 00:56:37,440
model parameters

1559
00:56:37,440 --> 00:56:40,720
and i'll conclude by saying that i think

1560
00:56:40,720 --> 00:56:43,040
trust for the ml research

1561
00:56:43,040 --> 00:56:46,480
is very exciting because it

1562
00:56:46,640 --> 00:56:49,520
allows us to better understand

1563
00:56:49,520 --> 00:56:51,680
michelle's to generalize and so i think

1564
00:56:51,680 --> 00:56:53,760
there are many opportunities to simply

1565
00:56:53,760 --> 00:56:56,480
improve how we train machinery models by

1566
00:56:56,480 --> 00:57:00,160
looking at their security um and so this

1567
00:57:00,160 --> 00:57:02,160
this is one of the reasons why i'm very

1568
00:57:02,160 --> 00:57:05,040
excited uh to look at the the area and

1569
00:57:05,040 --> 00:57:08,000
what results we're going to be producing

1570
00:57:08,000 --> 00:57:09,839
thank you for your attention i'll be

1571
00:57:09,839 --> 00:57:11,520
taking questions if we still have a few

1572
00:57:11,520 --> 00:57:13,520
minutes

1573
00:57:13,520 --> 00:57:15,599
thanks a lot nicholas these are very

1574
00:57:15,599 --> 00:57:16,480
like

1575
00:57:16,480 --> 00:57:18,480
super interesting problems

1576
00:57:18,480 --> 00:57:19,839
and we don't have a lot of time for

1577
00:57:19,839 --> 00:57:23,520
questions maybe for some quick questions

1578
00:57:23,520 --> 00:57:25,520
if somebody has any question please

1579
00:57:25,520 --> 00:57:26,799
either like

1580
00:57:26,799 --> 00:57:29,839
yeah um speak out

1581
00:57:29,839 --> 00:57:30,799
daniel

1582
00:57:30,799 --> 00:57:32,960
yes yes thank you nicholas i just wanted

1583
00:57:32,960 --> 00:57:34,079
to ask you a quick question what's a

1584
00:57:34,079 --> 00:57:36,160
great talk by the way just say um

1585
00:57:36,160 --> 00:57:37,760
when you talk about you describe the

1586
00:57:37,760 --> 00:57:40,160
solution for avoiding a model stealing

1587
00:57:40,160 --> 00:57:42,160
right which is if you steal the model

1588
00:57:42,160 --> 00:57:44,799
then i can try to

1589
00:57:44,799 --> 00:57:47,119
check if someone's still there right and

1590
00:57:47,119 --> 00:57:48,880
the solution is is interesting because

1591
00:57:48,880 --> 00:57:50,480
it basically it

1592
00:57:50,480 --> 00:57:52,400
turns an attack which is a membership

1593
00:57:52,400 --> 00:57:54,640
inference attack into a defense kind of

1594
00:57:54,640 --> 00:57:55,920
right because what you do is that you

1595
00:57:55,920 --> 00:57:57,599
run a yeah

1596
00:57:57,599 --> 00:57:59,520
yeah but then i wonder if if there are

1597
00:57:59,520 --> 00:58:01,680
defenses against membership inference

1598
00:58:01,680 --> 00:58:03,200
attack then the attacker can use his

1599
00:58:03,200 --> 00:58:05,280
defenses to defend against the defense

1600
00:58:05,280 --> 00:58:06,720
right right

1601
00:58:06,720 --> 00:58:08,480
yeah so this is the interesting part

1602
00:58:08,480 --> 00:58:11,119
about it is the asymmetry between the

1603
00:58:11,119 --> 00:58:13,119
attacker and the defender here that i

1604
00:58:13,119 --> 00:58:14,960
was surfacing

1605
00:58:14,960 --> 00:58:17,200
because if you have a defense against

1606
00:58:17,200 --> 00:58:18,799
membership inference

1607
00:58:18,799 --> 00:58:21,359
you will not be able to decrease the

1608
00:58:21,359 --> 00:58:23,680
privacy leakage to zero so you're going

1609
00:58:23,680 --> 00:58:24,960
to for instance if you train with

1610
00:58:24,960 --> 00:58:27,200
differential privacy which is i guess

1611
00:58:27,200 --> 00:58:29,520
the defense that i presented as being

1612
00:58:29,520 --> 00:58:31,359
the most efficient against membership

1613
00:58:31,359 --> 00:58:32,880
inference

1614
00:58:32,880 --> 00:58:34,480
is essentially what you're going to do

1615
00:58:34,480 --> 00:58:36,319
is you're going to bound the privacy

1616
00:58:36,319 --> 00:58:38,799
leakage but you're not going to make it

1617
00:58:38,799 --> 00:58:41,440
zero and so what this means is that

1618
00:58:41,440 --> 00:58:43,920
membership inference will be harder for

1619
00:58:43,920 --> 00:58:45,839
individual points so it will be harder

1620
00:58:45,839 --> 00:58:48,400
for an adversary to guess whether the

1621
00:58:48,400 --> 00:58:50,559
point is part of the training set or not

1622
00:58:50,559 --> 00:58:53,280
but it's not going to be completely

1623
00:58:53,280 --> 00:58:54,559
uh

1624
00:58:54,559 --> 00:58:55,760
basically it's not going to take the

1625
00:58:55,760 --> 00:58:57,599
advantage of that guess

1626
00:58:57,599 --> 00:58:59,520
to a random guess

1627
00:58:59,520 --> 00:59:01,760
does that make sense and so if you take

1628
00:59:01,760 --> 00:59:04,240
multiple training points and you repeat

1629
00:59:04,240 --> 00:59:06,559
the membership inference procedure

1630
00:59:06,559 --> 00:59:07,920
even if you're

1631
00:59:07,920 --> 00:59:10,559
slightly better than random chance

1632
00:59:10,559 --> 00:59:13,040
you're going to accumulate this signal

1633
00:59:13,040 --> 00:59:15,599
across multiple points and so if you

1634
00:59:15,599 --> 00:59:18,319
reveal enough of your training points

1635
00:59:18,319 --> 00:59:21,200
then you will be able to uh get to a

1636
00:59:21,200 --> 00:59:22,880
point where the dataset inference is

1637
00:59:22,880 --> 00:59:26,079
sufficiently confident

1638
00:59:26,079 --> 00:59:27,920
does that answer your question yeah it

1639
00:59:27,920 --> 00:59:30,640
does so the the the owner of the data

1640
00:59:30,640 --> 00:59:32,400
has more power basically to carry out

1641
00:59:32,400 --> 00:59:35,280
like a motor yeah exactly because in in

1642
00:59:35,280 --> 00:59:37,520
a privacy attack you're you care about a

1643
00:59:37,520 --> 00:59:40,960
specific point a single point whereas as

1644
00:59:40,960 --> 00:59:43,920
a data set inference uh

1645
00:59:43,920 --> 00:59:45,599
when in data set inference the model

1646
00:59:45,599 --> 00:59:47,440
owner has multiple points and they can

1647
00:59:47,440 --> 00:59:48,400
reveal

1648
00:59:48,400 --> 00:59:49,680
they don't want to reveal their whole

1649
00:59:49,680 --> 00:59:51,280
training set obviously because that

1650
00:59:51,280 --> 00:59:53,920
would defeat the purpose but even if

1651
00:59:53,920 --> 00:59:55,920
they reveal just a handful of training

1652
00:59:55,920 --> 00:59:58,319
points then they'll get enough

1653
00:59:58,319 --> 01:00:00,799
confidence by aggregating the membership

1654
01:00:00,799 --> 01:00:03,760
signal across these multiple points

1655
01:00:03,760 --> 01:00:05,920
well that's what i think isn't it yeah

1656
01:00:05,920 --> 01:00:08,960
thanks for the great question

1657
01:00:10,880 --> 01:00:13,839
unfortunately we're running on

1658
01:00:13,839 --> 01:00:17,200
somebody has connections please email uh

1659
01:00:17,200 --> 01:00:18,960
nicholas i will also email you have some

1660
01:00:18,960 --> 01:00:21,440
questions and thank you so much

1661
01:00:21,440 --> 01:00:23,119
excellent thank you again for inviting

1662
01:00:23,119 --> 01:00:24,880
me and i look forward to questions over

1663
01:00:24,880 --> 01:00:27,880
email

1664
01:00:28,079 --> 01:00:31,319
take care

