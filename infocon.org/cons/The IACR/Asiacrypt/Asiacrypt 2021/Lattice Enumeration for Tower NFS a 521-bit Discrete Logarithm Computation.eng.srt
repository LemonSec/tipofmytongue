1
00:00:02,480 --> 00:00:05,279
hello i'm happy to be presenting uh some

2
00:00:05,279 --> 00:00:07,120
joint work with kyrie godri and cecil

3
00:00:07,120 --> 00:00:09,440
pierrot on lattice enumeration for the

4
00:00:09,440 --> 00:00:11,719
tower number fieldset algorithm a

5
00:00:11,719 --> 00:00:14,920
521-bit discrete logarithmic

6
00:00:14,920 --> 00:00:16,880
computation so as we all know

7
00:00:16,880 --> 00:00:18,880
cryptography uh gives us means to

8
00:00:18,880 --> 00:00:20,880
communicate in a secure way between two

9
00:00:20,880 --> 00:00:22,640
entities of course even in the presence

10
00:00:22,640 --> 00:00:24,240
of a needs dropper

11
00:00:24,240 --> 00:00:26,720
and in order to do so uh cryptographic

12
00:00:26,720 --> 00:00:28,320
protocols are going to rely on some

13
00:00:28,320 --> 00:00:29,920
building blocks which are cryptographic

14
00:00:29,920 --> 00:00:31,039
primitives

15
00:00:31,039 --> 00:00:33,280
that rely on hard intractable

16
00:00:33,280 --> 00:00:35,360
mathematical problems

17
00:00:35,360 --> 00:00:37,680
these problems can be factoring integers

18
00:00:37,680 --> 00:00:40,000
computing discrete logarithms or even

19
00:00:40,000 --> 00:00:42,079
problems related to lattices for example

20
00:00:42,079 --> 00:00:43,760
or esogenes

21
00:00:43,760 --> 00:00:45,360
in this talk i will focus on one of

22
00:00:45,360 --> 00:00:47,440
these mathematical problems which is the

23
00:00:47,440 --> 00:00:51,199
computation of discrete logarithms

24
00:00:51,440 --> 00:00:52,800
so first of all what is a discrete

25
00:00:52,800 --> 00:00:55,199
logarithm the setup is quite simple we

26
00:00:55,199 --> 00:00:57,199
are given a finite cyclic group g of

27
00:00:57,199 --> 00:00:58,399
some order n

28
00:00:58,399 --> 00:01:00,480
a generator g of that group and some

29
00:01:00,480 --> 00:01:02,480
element h in the group

30
00:01:02,480 --> 00:01:04,799
and so the target the discrete logarithm

31
00:01:04,799 --> 00:01:07,840
of the target element h in base g is

32
00:01:07,840 --> 00:01:10,640
going to be the element x the exponent

33
00:01:10,640 --> 00:01:14,560
such that g to the x is equal to h

34
00:01:14,560 --> 00:01:16,240
now this definition brings forth the

35
00:01:16,240 --> 00:01:17,759
following problem which is known as the

36
00:01:17,759 --> 00:01:20,000
discrete logarithm problem where we are

37
00:01:20,000 --> 00:01:22,320
given a target element h in the group

38
00:01:22,320 --> 00:01:25,200
and we are asked to find the element x

39
00:01:25,200 --> 00:01:27,759
the exponent such that g to the x is

40
00:01:27,759 --> 00:01:29,439
equal to h

41
00:01:29,439 --> 00:01:31,520
now this uh problem the discrete

42
00:01:31,520 --> 00:01:33,840
logarithm problem is known to be hard

43
00:01:33,840 --> 00:01:36,320
but of course it depends on the group g

44
00:01:36,320 --> 00:01:38,079
that is being considered

45
00:01:38,079 --> 00:01:39,759
and for cryptographic reasons we want to

46
00:01:39,759 --> 00:01:41,520
choose a group g for which the discrete

47
00:01:41,520 --> 00:01:44,320
logarithm problem is as hard as possible

48
00:01:44,320 --> 00:01:46,240
and so groups that are often chosen in

49
00:01:46,240 --> 00:01:48,000
cryptography are going to be prime

50
00:01:48,000 --> 00:01:50,479
finite fields finite fields of the form

51
00:01:50,479 --> 00:01:52,960
fpn elliptic curves over finite fields

52
00:01:52,960 --> 00:01:55,680
or genius two hyper elliptic curves

53
00:01:55,680 --> 00:01:57,759
again in this talk we will focus on a

54
00:01:57,759 --> 00:02:00,079
particular group g which is going to be

55
00:02:00,079 --> 00:02:02,159
the multiplicative group of finite

56
00:02:02,159 --> 00:02:05,439
fields of form fpn

57
00:02:05,759 --> 00:02:07,040
so why do we care about discrete

58
00:02:07,040 --> 00:02:09,038
logarithms well many cryptographic

59
00:02:09,038 --> 00:02:10,800
protocols are going to use modular

60
00:02:10,800 --> 00:02:13,360
exponentiation in their algorithm where

61
00:02:13,360 --> 00:02:14,959
the exponent is going to be a secret so

62
00:02:14,959 --> 00:02:16,800
a modular exponentiation is going to be

63
00:02:16,800 --> 00:02:18,640
computing a quantity for example g to

64
00:02:18,640 --> 00:02:21,440
the x and a group g

65
00:02:21,440 --> 00:02:22,800
this is the case for example in the

66
00:02:22,800 --> 00:02:24,879
diffie-hellman key exchange protocol

67
00:02:24,879 --> 00:02:27,200
whereas public data we have elements of

68
00:02:27,200 --> 00:02:29,040
a group g g

69
00:02:29,040 --> 00:02:32,080
generator g due to the a g sub the b and

70
00:02:32,080 --> 00:02:35,120
we have a shared secret key g to the a b

71
00:02:35,120 --> 00:02:36,879
and that one can see that if it is easy

72
00:02:36,879 --> 00:02:39,280
to recover either a or b from the public

73
00:02:39,280 --> 00:02:41,360
data so solving a discrete logarithmic

74
00:02:41,360 --> 00:02:44,080
problem basically then one can compute

75
00:02:44,080 --> 00:02:46,000
uh g to the a b

76
00:02:46,000 --> 00:02:47,440
uh the diffie-hellman key exchange

77
00:02:47,440 --> 00:02:49,440
protocol is not the only example in

78
00:02:49,440 --> 00:02:51,920
paragraphy's protocols we have identity

79
00:02:51,920 --> 00:02:53,920
based encryption or signature schemes

80
00:02:53,920 --> 00:02:56,319
short signature schemes where the

81
00:02:56,319 --> 00:02:58,720
security is going to be based on some

82
00:02:58,720 --> 00:03:01,519
assumptions that would become false if

83
00:03:01,519 --> 00:03:05,760
the discrete logarithm problem is broken

84
00:03:05,760 --> 00:03:08,319
now evaluating the hardness of dlp over

85
00:03:08,319 --> 00:03:09,920
a finite field fpn is not something

86
00:03:09,920 --> 00:03:12,319
which is easy first of all because many

87
00:03:12,319 --> 00:03:14,400
different algorithms exist to solve dlp

88
00:03:14,400 --> 00:03:15,599
and fpm

89
00:03:15,599 --> 00:03:17,760
not only that but also the complexities

90
00:03:17,760 --> 00:03:19,760
of these algorithm depend on the

91
00:03:19,760 --> 00:03:20,959
relation there is between the

92
00:03:20,959 --> 00:03:22,720
characteristic p and the extension

93
00:03:22,720 --> 00:03:24,799
degree n of the finite field

94
00:03:24,799 --> 00:03:26,080
and in order to talk about these

95
00:03:26,080 --> 00:03:28,000
complexities we need to introduce the

96
00:03:28,000 --> 00:03:29,599
following notation which is known as the

97
00:03:29,599 --> 00:03:32,319
l notation expressed uh by the formula

98
00:03:32,319 --> 00:03:34,000
given on the slide

99
00:03:34,000 --> 00:03:36,400
this expression this formula uses two

100
00:03:36,400 --> 00:03:37,840
parameters

101
00:03:37,840 --> 00:03:39,920
alpha which varies between zero and one

102
00:03:39,920 --> 00:03:43,040
and a constant c positive constant c

103
00:03:43,040 --> 00:03:45,280
now one can see that if alpha goes uh to

104
00:03:45,280 --> 00:03:48,080
zero then this expression gives some

105
00:03:48,080 --> 00:03:50,480
polynomial time complexity on the other

106
00:03:50,480 --> 00:03:52,879
side if alpha goes to one we have an

107
00:03:52,879 --> 00:03:55,120
exponential time complexity

108
00:03:55,120 --> 00:03:56,879
now whenever alpha is in between zero

109
00:03:56,879 --> 00:03:59,120
and one we talk about subexponential

110
00:03:59,120 --> 00:04:00,799
time complexities which is the

111
00:04:00,799 --> 00:04:02,879
complexity of all the algorithms we'll

112
00:04:02,879 --> 00:04:06,720
be talking about in this talk

113
00:04:07,120 --> 00:04:09,360
now the l notation is not only used to

114
00:04:09,360 --> 00:04:10,799
talk about the complexities of these

115
00:04:10,799 --> 00:04:12,959
algorithms but it is also used to

116
00:04:12,959 --> 00:04:15,439
characterize families of finite fields

117
00:04:15,439 --> 00:04:17,600
in particular to separate finite fields

118
00:04:17,600 --> 00:04:19,600
into three different families

119
00:04:19,600 --> 00:04:21,680
so if the characteristic p of the finite

120
00:04:21,680 --> 00:04:24,479
field is expressed using l of alpha c

121
00:04:24,479 --> 00:04:26,479
as soon as alpha is smaller than one

122
00:04:26,479 --> 00:04:27,600
third we talk about small

123
00:04:27,600 --> 00:04:29,120
characteristics

124
00:04:29,120 --> 00:04:31,680
in alpha if alpha is in between a third

125
00:04:31,680 --> 00:04:33,040
and two-thirds we talk about medium

126
00:04:33,040 --> 00:04:35,840
charis characteristic finite fields

127
00:04:35,840 --> 00:04:37,840
and if alpha is greater than two-thirds

128
00:04:37,840 --> 00:04:39,680
then we talk about large characteristic

129
00:04:39,680 --> 00:04:41,120
finite fields

130
00:04:41,120 --> 00:04:42,720
now different algorithms are going to be

131
00:04:42,720 --> 00:04:44,800
used in different in each of these areas

132
00:04:44,800 --> 00:04:46,240
and also algorithms are not going to

133
00:04:46,240 --> 00:04:48,560
have the same complexity depending on

134
00:04:48,560 --> 00:04:51,600
which area we're looking at

135
00:04:51,600 --> 00:04:53,040
now we have a lot of algorithms that

136
00:04:53,040 --> 00:04:55,280
self dlp for finite fields in small

137
00:04:55,280 --> 00:04:56,479
characteristics

138
00:04:56,479 --> 00:04:58,160
the most efficient algorithms come from

139
00:04:58,160 --> 00:05:00,000
a family known as quality polynomial

140
00:05:00,000 --> 00:05:01,440
time algorithms

141
00:05:01,440 --> 00:05:03,360
and in medium and large characteristics

142
00:05:03,360 --> 00:05:05,360
we are going to have as most efficient

143
00:05:05,360 --> 00:05:07,520
algorithms the number field sieve and

144
00:05:07,520 --> 00:05:09,440
its many variants

145
00:05:09,440 --> 00:05:11,039
now we focus on medium and large

146
00:05:11,039 --> 00:05:12,400
characteristics

147
00:05:12,400 --> 00:05:13,840
mainly because these are the finite

148
00:05:13,840 --> 00:05:17,359
fields that are being used in practice

149
00:05:18,720 --> 00:05:20,720
so in this work we are going to present

150
00:05:20,720 --> 00:05:23,840
a record computation done with a variant

151
00:05:23,840 --> 00:05:25,280
of the number field sieve which is the

152
00:05:25,280 --> 00:05:27,520
tower number felsive algorithm and so

153
00:05:27,520 --> 00:05:30,240
why do we care about record computations

154
00:05:30,240 --> 00:05:32,400
in general well for a protocol for a

155
00:05:32,400 --> 00:05:34,160
cryptographic protocol it is important

156
00:05:34,160 --> 00:05:36,320
to choose the right key size

157
00:05:36,320 --> 00:05:37,680
of course if we're choosing something

158
00:05:37,680 --> 00:05:39,039
too large a key size which is

159
00:05:39,039 --> 00:05:41,360
ridiculously large we are we are

160
00:05:41,360 --> 00:05:42,800
performing needlessly expensive

161
00:05:42,800 --> 00:05:44,479
computations and on the other hand if

162
00:05:44,479 --> 00:05:46,400
the key size is too small of course our

163
00:05:46,400 --> 00:05:48,720
protocol is not secure

164
00:05:48,720 --> 00:05:50,080
now the running time of discrete

165
00:05:50,080 --> 00:05:51,440
logarithmic algorithms is something

166
00:05:51,440 --> 00:05:53,120
which is hard to predict

167
00:05:53,120 --> 00:05:55,199
and record computations are thus going

168
00:05:55,199 --> 00:05:57,440
to give us useful information

169
00:05:57,440 --> 00:05:58,800
for

170
00:05:58,800 --> 00:06:02,319
a key lifetime for example

171
00:06:02,400 --> 00:06:04,479
so this motivated us to perform a record

172
00:06:04,479 --> 00:06:06,639
computation with a new variant which had

173
00:06:06,639 --> 00:06:08,400
never been implemented before which is

174
00:06:08,400 --> 00:06:10,240
the extended tower number felsif

175
00:06:10,240 --> 00:06:11,360
algorithm

176
00:06:11,360 --> 00:06:14,160
and so why did we choose xtnfs

177
00:06:14,160 --> 00:06:16,000
well this can be seen with this table

178
00:06:16,000 --> 00:06:18,160
which i will now explain the first thing

179
00:06:18,160 --> 00:06:20,880
to see to notice is that extended x in

180
00:06:20,880 --> 00:06:23,919
xtnfs comes from the fact that n is

181
00:06:23,919 --> 00:06:25,039
composite so we are going to be

182
00:06:25,039 --> 00:06:26,479
considering finite fields where the

183
00:06:26,479 --> 00:06:28,880
extension degree can be written as eta

184
00:06:28,880 --> 00:06:30,880
times kappa

185
00:06:30,880 --> 00:06:33,120
now the table on the slides gives the

186
00:06:33,120 --> 00:06:35,600
asymptotic complexities of the variance

187
00:06:35,600 --> 00:06:37,199
of nfs in medium and large

188
00:06:37,199 --> 00:06:38,720
characteristics

189
00:06:38,720 --> 00:06:40,720
now all the complexities of these

190
00:06:40,720 --> 00:06:42,560
algorithms are expressed using the l

191
00:06:42,560 --> 00:06:45,120
notation and the constant alpha is equal

192
00:06:45,120 --> 00:06:46,479
to one-third

193
00:06:46,479 --> 00:06:48,880
this is the same for all the algorithms

194
00:06:48,880 --> 00:06:52,000
what we'll change is uh the second

195
00:06:52,000 --> 00:06:54,160
constant c where we can see that in

196
00:06:54,160 --> 00:06:56,240
meridian characteristics for example the

197
00:06:56,240 --> 00:07:00,080
c is equal to 48 for xtnfs whereas for

198
00:07:00,080 --> 00:07:01,599
the number field save algorithm the

199
00:07:01,599 --> 00:07:04,880
classical nfs we have a much larger

200
00:07:04,880 --> 00:07:07,280
c value 96

201
00:07:07,280 --> 00:07:09,919
so xdnfs seems very promising

202
00:07:09,919 --> 00:07:11,520
for medium characteristics finance

203
00:07:11,520 --> 00:07:12,800
fields

204
00:07:12,800 --> 00:07:14,960
this can also be explained or seen

205
00:07:14,960 --> 00:07:18,000
intuitively if we look at how we can

206
00:07:18,000 --> 00:07:20,800
see or interpret the finite field fpn

207
00:07:20,800 --> 00:07:23,840
we can and because n is composite fpn is

208
00:07:23,840 --> 00:07:27,120
equal to fp the eta times kappa and this

209
00:07:27,120 --> 00:07:30,160
finite field can be seen similarly as a

210
00:07:30,160 --> 00:07:32,960
finite field f capital p kappa where

211
00:07:32,960 --> 00:07:36,960
capital p is of this bit size of um p to

212
00:07:36,960 --> 00:07:38,160
the eta

213
00:07:38,160 --> 00:07:40,000
so what has happened here is that we

214
00:07:40,000 --> 00:07:42,000
have increased the characteristic of the

215
00:07:42,000 --> 00:07:45,120
finite field from p to p to the capital

216
00:07:45,120 --> 00:07:47,360
p which is p to the eta and we have

217
00:07:47,360 --> 00:07:50,319
decreased the extension degree from n to

218
00:07:50,319 --> 00:07:51,520
kappa

219
00:07:51,520 --> 00:07:53,520
and this means that the finite field can

220
00:07:53,520 --> 00:07:54,639
be seen

221
00:07:54,639 --> 00:07:56,879
similarly as a finite field where we

222
00:07:56,879 --> 00:07:59,280
would use an nfs computation at the

223
00:07:59,280 --> 00:08:02,000
second boundary case where the constant

224
00:08:02,000 --> 00:08:05,360
is also equal to 48.

225
00:08:05,360 --> 00:08:07,360
now in order to have a record

226
00:08:07,360 --> 00:08:10,400
computation with xtnfs we had to focus

227
00:08:10,400 --> 00:08:12,720
on the most expensive step of the

228
00:08:12,720 --> 00:08:15,280
algorithm which is the first step of any

229
00:08:15,280 --> 00:08:17,360
index calculus algorithm which is

230
00:08:17,360 --> 00:08:20,000
collecting algebraic relations

231
00:08:20,000 --> 00:08:22,639
and the difficulty of xtnfs is that this

232
00:08:22,639 --> 00:08:24,400
relation collection is going to happen a

233
00:08:24,400 --> 00:08:27,199
dimension greater than 2 whereas in nfs

234
00:08:27,199 --> 00:08:29,440
for example this relation collection

235
00:08:29,440 --> 00:08:32,639
happens in dimension 2.

236
00:08:33,200 --> 00:08:34,958
so let me focus a little more on

237
00:08:34,958 --> 00:08:37,839
relation collections in tnfs so if you

238
00:08:37,839 --> 00:08:39,279
know the number filter algorithm you

239
00:08:39,279 --> 00:08:41,440
have probably already seen a diagram

240
00:08:41,440 --> 00:08:43,519
such as the one on the slide

241
00:08:43,519 --> 00:08:45,440
and so here we start at the top of the

242
00:08:45,440 --> 00:08:48,160
diagram with a ring rx

243
00:08:48,160 --> 00:08:50,800
r in the specific case of tnfs is

244
00:08:50,800 --> 00:08:53,680
z-eotac originated by a polynomial h in

245
00:08:53,680 --> 00:08:55,680
our computation h is a polynomial of

246
00:08:55,680 --> 00:08:58,000
degree 3.

247
00:08:58,000 --> 00:09:00,480
elements of rx are then mapped to two

248
00:09:00,480 --> 00:09:02,160
intermediate number fields where we just

249
00:09:02,160 --> 00:09:04,720
evaluate x into uh

250
00:09:04,720 --> 00:09:07,040
the roots of the two polynomials x four

251
00:09:07,040 --> 00:09:10,320
plus one a x two plus b x plus c

252
00:09:10,320 --> 00:09:11,680
and then elements of these two

253
00:09:11,680 --> 00:09:13,760
intermediate fields can be mapped to the

254
00:09:13,760 --> 00:09:17,200
bottom target finite field f b six

255
00:09:17,200 --> 00:09:19,600
and so this diagram in particular the

256
00:09:19,600 --> 00:09:22,160
polynomials x four plus 1 and a x 2 plus

257
00:09:22,160 --> 00:09:25,600
b x plus 6 plus c sorry are chosen such

258
00:09:25,600 --> 00:09:27,519
that the diagram commutes meaning that

259
00:09:27,519 --> 00:09:29,920
we have two different paths that brings

260
00:09:29,920 --> 00:09:33,120
elements from the top ring rx down to

261
00:09:33,120 --> 00:09:36,959
the target finite field fb6

262
00:09:36,959 --> 00:09:39,200
more precisely the elements on the top

263
00:09:39,200 --> 00:09:41,040
ring are going to be polynomials in yota

264
00:09:41,040 --> 00:09:45,200
and x of the form a utah minus b yota x

265
00:09:45,200 --> 00:09:47,279
these elements are going to be mapped

266
00:09:47,279 --> 00:09:50,240
into k1 and k2 so for example if we look

267
00:09:50,240 --> 00:09:53,360
at the right side k1 we have an element

268
00:09:53,360 --> 00:09:57,600
phi of yota alpha 1.

269
00:09:57,600 --> 00:09:58,959
these elements are then going to be

270
00:09:58,959 --> 00:10:00,720
tested for something which is known as b

271
00:10:00,720 --> 00:10:03,040
smoothness meaning that if we look at

272
00:10:03,040 --> 00:10:07,040
the norm of the element phi yota alpha 1

273
00:10:07,040 --> 00:10:10,720
seen as an ideal we can see we can test

274
00:10:10,720 --> 00:10:13,519
whether all whether it decomposes into

275
00:10:13,519 --> 00:10:16,560
prime factors of smaller than b so we

276
00:10:16,560 --> 00:10:18,959
test whether all the factors

277
00:10:18,959 --> 00:10:21,839
of the norm of 5 are going to be smaller

278
00:10:21,839 --> 00:10:24,480
than a smoothness down b which is chosen

279
00:10:24,480 --> 00:10:27,200
while when we run the algorithm

280
00:10:27,200 --> 00:10:29,120
if this is the case in the finite field

281
00:10:29,120 --> 00:10:32,399
k1 and also in the finite field k2 then

282
00:10:32,399 --> 00:10:34,399
these elements are mapped down to the

283
00:10:34,399 --> 00:10:37,680
bottom finite field fb6 and the equality

284
00:10:37,680 --> 00:10:38,800
that we get

285
00:10:38,800 --> 00:10:40,800
in the bottom finite field

286
00:10:40,800 --> 00:10:42,560
actually corresponds to what is known as

287
00:10:42,560 --> 00:10:44,240
a relation

288
00:10:44,240 --> 00:10:46,800
these relations are products of prime

289
00:10:46,800 --> 00:10:49,200
ideals

290
00:10:49,200 --> 00:10:51,519
and so the idea of relation collection

291
00:10:51,519 --> 00:10:53,120
meaning finding algebraic relations

292
00:10:53,120 --> 00:10:54,800
between elements of what is known as a

293
00:10:54,800 --> 00:10:56,160
factor basis

294
00:10:56,160 --> 00:10:57,200
is given

295
00:10:57,200 --> 00:11:00,880
by this diagram which gives us some sort

296
00:11:00,880 --> 00:11:02,720
of equality between products of prime

297
00:11:02,720 --> 00:11:04,560
ideals and the factor basis that is

298
00:11:04,560 --> 00:11:07,600
being considered are a set of prime

299
00:11:07,600 --> 00:11:10,800
ideals of small norm

300
00:11:12,320 --> 00:11:14,480
so we can look a little bit more uh in

301
00:11:14,480 --> 00:11:16,560
details into relation collection in the

302
00:11:16,560 --> 00:11:19,279
specific context of tnfs so recall at

303
00:11:19,279 --> 00:11:21,120
the very top of my diagram i had

304
00:11:21,120 --> 00:11:22,800
polynomials

305
00:11:22,800 --> 00:11:25,360
in yota x of the form a yota minus the

306
00:11:25,360 --> 00:11:26,800
yota x

307
00:11:26,800 --> 00:11:28,640
so relation collection actually narrows

308
00:11:28,640 --> 00:11:31,360
down to finding a lin a set of linear

309
00:11:31,360 --> 00:11:33,360
polynomials of this form with two

310
00:11:33,360 --> 00:11:35,600
conditions we want bounded coefficients

311
00:11:35,600 --> 00:11:38,399
and we want the norms of a utah min and

312
00:11:38,399 --> 00:11:39,040
b

313
00:11:39,040 --> 00:11:41,600
yota alpha i so in both number fields

314
00:11:41,600 --> 00:11:45,279
for i 1 and 2 to be b smooth

315
00:11:45,279 --> 00:11:47,360
concretely a and b are going to be

316
00:11:47,360 --> 00:11:49,760
polynomials of degree 2.

317
00:11:49,760 --> 00:11:51,360
and so we are going to be looking at

318
00:11:51,360 --> 00:11:52,800
vectors

319
00:11:52,800 --> 00:11:55,600
where we have 6 coefficients three

320
00:11:55,600 --> 00:11:57,200
corresponding to the coefficients of the

321
00:11:57,200 --> 00:11:59,920
polynomial a three corresponding to the

322
00:11:59,920 --> 00:12:02,480
coefficients of the polynomial b so

323
00:12:02,480 --> 00:12:04,480
vectors in z six

324
00:12:04,480 --> 00:12:06,880
such that we have bounded coefficients

325
00:12:06,880 --> 00:12:09,279
this will mean that our vectors c are

326
00:12:09,279 --> 00:12:11,120
going to belong to a region which is

327
00:12:11,120 --> 00:12:13,600
usually known as a sieving region

328
00:12:13,600 --> 00:12:17,040
and we want the norms to be b smooth

329
00:12:17,040 --> 00:12:19,440
meaning that the vector c are going to

330
00:12:19,440 --> 00:12:21,360
belong to the intersection of some

331
00:12:21,360 --> 00:12:23,760
suitably constructed lattices l which i

332
00:12:23,760 --> 00:12:26,720
will now explain

333
00:12:26,959 --> 00:12:29,120
so first let me focus on the sieving

334
00:12:29,120 --> 00:12:31,600
region so we've narrowed down relation

335
00:12:31,600 --> 00:12:34,079
collection to finding vectors c in s

336
00:12:34,079 --> 00:12:37,200
intersect l so what is s in our case s

337
00:12:37,200 --> 00:12:39,920
is a six-dimensional sieving space

338
00:12:39,920 --> 00:12:42,240
this differs a lot from previous work

339
00:12:42,240 --> 00:12:45,279
where in the classical nfs setup we

340
00:12:45,279 --> 00:12:47,040
would be looking at a two-dimensional

341
00:12:47,040 --> 00:12:50,160
space because in nfs we are looking for

342
00:12:50,160 --> 00:12:52,079
pairs a b which belonged to

343
00:12:52,079 --> 00:12:53,120
z2

344
00:12:53,120 --> 00:12:54,639
and in order to efficiently sit in

345
00:12:54,639 --> 00:12:56,560
dimension two franco klein young

346
00:12:56,560 --> 00:12:58,399
provided a very efficient algorithm that

347
00:12:58,399 --> 00:13:00,959
is used in nfs computations

348
00:13:00,959 --> 00:13:03,279
nfs can also sieve in higher dimensions

349
00:13:03,279 --> 00:13:05,279
there has been work in that direction

350
00:13:05,279 --> 00:13:07,760
from crimea lujan remy and also from

351
00:13:07,760 --> 00:13:10,320
mcgregor and robinson more recently

352
00:13:10,320 --> 00:13:11,760
and the common point between all of

353
00:13:11,760 --> 00:13:13,680
these algorithms is that the sitting

354
00:13:13,680 --> 00:13:15,680
space that is being considered is a d

355
00:13:15,680 --> 00:13:18,959
rectangle whether it is two or three

356
00:13:18,959 --> 00:13:21,200
they sieve in a deer rectangle

357
00:13:21,200 --> 00:13:23,839
our work differs from that choice of

358
00:13:23,839 --> 00:13:25,040
sieving space

359
00:13:25,040 --> 00:13:26,880
in the sense that we consider a

360
00:13:26,880 --> 00:13:30,160
six-dimensional sphere

361
00:13:31,040 --> 00:13:32,000
now

362
00:13:32,000 --> 00:13:34,560
we have looked at our sieving space s

363
00:13:34,560 --> 00:13:36,639
now what is the lattice l that is being

364
00:13:36,639 --> 00:13:39,440
considered well l is a lattice that is

365
00:13:39,440 --> 00:13:41,040
constructed in such a way that it

366
00:13:41,040 --> 00:13:44,320
describes the divisibility of the ideals

367
00:13:44,320 --> 00:13:45,120
by

368
00:13:45,120 --> 00:13:47,040
first of all an ideal cue known as a

369
00:13:47,040 --> 00:13:50,000
special q ideal and a prime ideal p in

370
00:13:50,000 --> 00:13:52,480
the intermediate number fields

371
00:13:52,480 --> 00:13:55,199
so the idea of these lattices is that

372
00:13:55,199 --> 00:13:57,360
elements belonging to the lattices are

373
00:13:57,360 --> 00:14:00,720
going to correspond to ideals divisible

374
00:14:00,720 --> 00:14:02,800
by q and by p

375
00:14:02,800 --> 00:14:05,600
so the output of the enumeration are

376
00:14:05,600 --> 00:14:08,000
going to be vectors corresponding to our

377
00:14:08,000 --> 00:14:11,199
polynomial a and b a yota biota

378
00:14:11,199 --> 00:14:13,120
such that the norms

379
00:14:13,120 --> 00:14:15,920
a yota minus b to alpha i are going to

380
00:14:15,920 --> 00:14:18,800
be divisible by the norm of q of our

381
00:14:18,800 --> 00:14:21,920
special q ideal and the norm of p

382
00:14:21,920 --> 00:14:24,399
and i talked about intersection of many

383
00:14:24,399 --> 00:14:27,040
suitably constructed lattices because we

384
00:14:27,040 --> 00:14:28,720
want

385
00:14:28,720 --> 00:14:31,199
we want the divisibility by many prime

386
00:14:31,199 --> 00:14:34,079
ideals p why because we want a high

387
00:14:34,079 --> 00:14:36,079
probability of our elements to be b

388
00:14:36,079 --> 00:14:38,399
smooth

389
00:14:38,959 --> 00:14:40,560
so now that i've narrowed down

390
00:14:40,560 --> 00:14:42,800
enumeration to actually looking for

391
00:14:42,800 --> 00:14:45,040
vectors at the intersection of a sphere

392
00:14:45,040 --> 00:14:46,399
and a lattice

393
00:14:46,399 --> 00:14:49,040
we can look at enumeration algorithms in

394
00:14:49,040 --> 00:14:51,760
particular ursher's numeration algorithm

395
00:14:51,760 --> 00:14:55,199
to efficiently look for these vectors

396
00:14:55,199 --> 00:14:56,959
such nourishers our enumeration

397
00:14:56,959 --> 00:14:58,720
algorithm takes us an input to let us

398
00:14:58,720 --> 00:15:01,360
basis and outputs the shortest non-zero

399
00:15:01,360 --> 00:15:03,680
lattice vector in order to do so they

400
00:15:03,680 --> 00:15:05,040
are going to construct an

401
00:15:05,040 --> 00:15:07,440
enumeration tree consider projections of

402
00:15:07,440 --> 00:15:09,680
their lattice and at each level of the

403
00:15:09,680 --> 00:15:12,000
tree enumerate in an interval and in the

404
00:15:12,000 --> 00:15:13,680
end a depth first search is done in the

405
00:15:13,680 --> 00:15:17,040
tree in order to output the shortest non

406
00:15:17,040 --> 00:15:19,040
zero lattice vector

407
00:15:19,040 --> 00:15:20,880
our setup is slightly different in the

408
00:15:20,880 --> 00:15:23,279
sense that um first of all our lot of

409
00:15:23,279 --> 00:15:25,600
spaces are we are considering six

410
00:15:25,600 --> 00:15:27,600
dimensional lattices so we're looking at

411
00:15:27,600 --> 00:15:30,639
very small lattices and also we are not

412
00:15:30,639 --> 00:15:32,480
interested in the shortest vector of the

413
00:15:32,480 --> 00:15:35,440
lattice but we are looking at look uh we

414
00:15:35,440 --> 00:15:37,920
are interested in all the vectors c such

415
00:15:37,920 --> 00:15:40,320
that the norm of c is smaller than some

416
00:15:40,320 --> 00:15:42,160
parameter r which can be seen as the

417
00:15:42,160 --> 00:15:44,880
radius of our six-dimensional sphere

418
00:15:44,880 --> 00:15:47,040
but the idea of uh the enumeration

419
00:15:47,040 --> 00:15:48,880
algorithm remains the same we heavily

420
00:15:48,880 --> 00:15:51,279
rely on schnorr irsner's construction in

421
00:15:51,279 --> 00:15:52,639
the sense that we do construct a

422
00:15:52,639 --> 00:15:54,959
numeration tree consider projections of

423
00:15:54,959 --> 00:15:56,880
our lattice and then exhaustively search

424
00:15:56,880 --> 00:16:00,079
for all the coefficients v i such that

425
00:16:00,079 --> 00:16:01,279
um our

426
00:16:01,279 --> 00:16:02,639
our vector c

427
00:16:02,639 --> 00:16:06,480
uh satisfy norm of c smaller than r

428
00:16:06,480 --> 00:16:08,800
so we actually adapted storage algorithm

429
00:16:08,800 --> 00:16:11,440
to the tnfs setup by slightly modifying

430
00:16:11,440 --> 00:16:13,920
their algorithm and we also optimized

431
00:16:13,920 --> 00:16:15,920
the computation of the vector c by

432
00:16:15,920 --> 00:16:18,079
reducing the number of computations we

433
00:16:18,079 --> 00:16:19,759
reduce the number of operations that is

434
00:16:19,759 --> 00:16:21,759
being done in our algorithm and this

435
00:16:21,759 --> 00:16:24,800
seems like a very uh small optimization

436
00:16:24,800 --> 00:16:27,279
but because enumeration in tnfs is done

437
00:16:27,279 --> 00:16:29,199
many many many times

438
00:16:29,199 --> 00:16:31,839
then this actually provided us with a 10

439
00:16:31,839 --> 00:16:33,839
improvement on our total saving time

440
00:16:33,839 --> 00:16:37,040
which is non-negligible

441
00:16:37,040 --> 00:16:38,079
so

442
00:16:38,079 --> 00:16:40,160
now that we have enumerated uh our

443
00:16:40,160 --> 00:16:42,480
vector c we can look at the relation

444
00:16:42,480 --> 00:16:44,480
collection altogether and relation

445
00:16:44,480 --> 00:16:46,959
collection can all can often be seen as

446
00:16:46,959 --> 00:16:49,360
a sequence of filters we're sieving the

447
00:16:49,360 --> 00:16:51,600
first part in our case enumeration is

448
00:16:51,600 --> 00:16:53,839
just the very beginning of uh the whole

449
00:16:53,839 --> 00:16:55,120
process

450
00:16:55,120 --> 00:16:57,279
so how does a relation collection happen

451
00:16:57,279 --> 00:16:59,440
altogether well we start with a sieving

452
00:16:59,440 --> 00:17:01,680
step in our case enumeration where we

453
00:17:01,680 --> 00:17:05,119
did find a b pairs in our

454
00:17:05,119 --> 00:17:06,959
sitting region s so in our case in our

455
00:17:06,959 --> 00:17:09,439
six dimensional sphere that intersected

456
00:17:09,439 --> 00:17:11,280
many lattices lp

457
00:17:11,280 --> 00:17:13,839
and so these a b pairs correspond to

458
00:17:13,839 --> 00:17:16,799
potential relations meaning pairs which

459
00:17:16,799 --> 00:17:19,359
have a high probability of being b

460
00:17:19,359 --> 00:17:21,280
smooth

461
00:17:21,280 --> 00:17:23,359
the second part of our relation

462
00:17:23,359 --> 00:17:25,439
collection would be to actually test for

463
00:17:25,439 --> 00:17:27,039
b smoothness for these potential

464
00:17:27,039 --> 00:17:29,200
candidates and this can be done using

465
00:17:29,200 --> 00:17:31,039
different algorithms for example a batch

466
00:17:31,039 --> 00:17:34,240
algorithm or ecm where the idea is to

467
00:17:34,240 --> 00:17:37,039
run these expensive algorithms on a

468
00:17:37,039 --> 00:17:40,240
smaller set so our potential relations

469
00:17:40,240 --> 00:17:42,960
and the output of these algorithms are

470
00:17:42,960 --> 00:17:45,679
going to be wb smooth elements which

471
00:17:45,679 --> 00:17:48,880
correspond to our relations

472
00:17:48,880 --> 00:17:50,720
so by going through these different

473
00:17:50,720 --> 00:17:53,280
filters we actually obtain a set of

474
00:17:53,280 --> 00:17:55,919
relations

475
00:17:55,919 --> 00:17:57,760
now there are some important details

476
00:17:57,760 --> 00:17:59,919
that i have left out of this talk one

477
00:17:59,919 --> 00:18:02,720
important contribution of our work is uh

478
00:18:02,720 --> 00:18:04,960
the removal of duplicate relations in

479
00:18:04,960 --> 00:18:07,440
the context of tnfs so duplicate

480
00:18:07,440 --> 00:18:10,000
relations meaning relations that appear

481
00:18:10,000 --> 00:18:12,320
twice so a b pairs that give the exact

482
00:18:12,320 --> 00:18:14,400
same information is not something which

483
00:18:14,400 --> 00:18:16,240
is new it also exists in the number

484
00:18:16,240 --> 00:18:18,160
field a civ algorithm

485
00:18:18,160 --> 00:18:21,520
however in tnfs duplicates of a

486
00:18:21,520 --> 00:18:23,679
different nature appear and removing

487
00:18:23,679 --> 00:18:26,160
them is a big and important step of the

488
00:18:26,160 --> 00:18:29,160
algorithm

489
00:18:29,280 --> 00:18:31,280
so

490
00:18:31,280 --> 00:18:34,080
towards the end what do we actually need

491
00:18:34,080 --> 00:18:36,240
for a record computation well first we

492
00:18:36,240 --> 00:18:38,240
focus on obtaining a fast saving

493
00:18:38,240 --> 00:18:39,919
algorithm that happens in dimension

494
00:18:39,919 --> 00:18:42,480
higher than two so in our case in

495
00:18:42,480 --> 00:18:44,240
dimension six

496
00:18:44,240 --> 00:18:47,039
then we focused um a lot of time on

497
00:18:47,039 --> 00:18:48,720
actually identifying and removing

498
00:18:48,720 --> 00:18:51,200
duplicate relations specifically in the

499
00:18:51,200 --> 00:18:53,280
context of tnfs

500
00:18:53,280 --> 00:18:55,840
we actually also had to adapt other

501
00:18:55,840 --> 00:18:58,720
notions from nfs to the context of tnfs

502
00:18:58,720 --> 00:18:59,840
this is the case for example of

503
00:18:59,840 --> 00:19:02,799
shirocara maps virtual logarithms

504
00:19:02,799 --> 00:19:04,960
and we also had to branch ourselves to

505
00:19:04,960 --> 00:19:07,600
cado nfs so some pre-existing code for

506
00:19:07,600 --> 00:19:10,880
nfs computations so we did not write

507
00:19:10,880 --> 00:19:13,600
rewrite the entire code for tnfs

508
00:19:13,600 --> 00:19:15,919
linear algebra or the the sun step for

509
00:19:15,919 --> 00:19:18,240
example we simply branched ourselves

510
00:19:18,240 --> 00:19:20,559
into a cado nfs

511
00:19:20,559 --> 00:19:22,240
we chose a nice target

512
00:19:22,240 --> 00:19:24,720
fp6 as mentioned at the very beginning

513
00:19:24,720 --> 00:19:26,559
motivated by the fact that the finite

514
00:19:26,559 --> 00:19:29,760
fields of this form are used in practice

515
00:19:29,760 --> 00:19:32,160
and this allowed us uh to compute

516
00:19:32,160 --> 00:19:35,440
this 521-bit record computation

517
00:19:35,440 --> 00:19:37,280
so our finite field is more or less

518
00:19:37,280 --> 00:19:39,120
between medium and large characteristics

519
00:19:39,120 --> 00:19:40,559
if we use

520
00:19:40,559 --> 00:19:42,799
this separation into three families of

521
00:19:42,799 --> 00:19:46,080
finite fields with the l notation

522
00:19:46,080 --> 00:19:47,840
we can see that the total computation

523
00:19:47,840 --> 00:19:49,440
time of our

524
00:19:49,440 --> 00:19:51,679
of our record is about a little less

525
00:19:51,679 --> 00:19:54,640
than 25 k core hours

526
00:19:54,640 --> 00:19:56,559
the relation collection step obviously

527
00:19:56,559 --> 00:19:58,240
taking the most time so we have a

528
00:19:58,240 --> 00:20:00,160
relation collection step of a little

529
00:20:00,160 --> 00:20:04,080
over 23 south a thousand core hours

530
00:20:04,080 --> 00:20:06,159
and we also uh were able to compare

531
00:20:06,159 --> 00:20:08,640
ourselves with uh previous algorithms

532
00:20:08,640 --> 00:20:10,960
that also sit in higher dimension in

533
00:20:10,960 --> 00:20:13,440
both these cases they saved in dimension

534
00:20:13,440 --> 00:20:15,440
three so this is

535
00:20:15,440 --> 00:20:16,320
um

536
00:20:16,320 --> 00:20:18,640
work that uses the algorithm of florence

537
00:20:18,640 --> 00:20:22,080
and and robinson where they do use nfs

538
00:20:22,080 --> 00:20:24,320
so they don't consider the tower variant

539
00:20:24,320 --> 00:20:27,520
they consider finite fields of size much

540
00:20:27,520 --> 00:20:32,080
smaller than ours around 422 423 whereas

541
00:20:32,080 --> 00:20:36,720
we consider a finite field of size 521

542
00:20:36,720 --> 00:20:38,480
as i just said their sieving dimension

543
00:20:38,480 --> 00:20:40,480
is three whereas we use six

544
00:20:40,480 --> 00:20:42,240
but we can still see that our sieving

545
00:20:42,240 --> 00:20:45,039
time is significantly faster than theirs

546
00:20:45,039 --> 00:20:48,400
so they go up to 270

547
00:20:48,400 --> 00:20:51,600
k core hours whereas we are down to 23

548
00:20:51,600 --> 00:20:52,720
um

549
00:20:52,720 --> 00:20:55,679
thousand core hours so we were um

550
00:20:55,679 --> 00:20:58,320
pleasantly surprised uh by how fast our

551
00:20:58,320 --> 00:21:00,880
sitting algorithm works

552
00:21:00,880 --> 00:21:03,120
and in the end we ran our algorithm and

553
00:21:03,120 --> 00:21:05,280
this uh allowed us to output a discrete

554
00:21:05,280 --> 00:21:08,320
logarithm so again in a finite field fb6

555
00:21:08,320 --> 00:21:11,440
where p is a 87-bit prime we chose a

556
00:21:11,440 --> 00:21:13,520
target where you can recognize uh the

557
00:21:13,520 --> 00:21:14,880
digits of pi

558
00:21:14,880 --> 00:21:18,000
and uh this is our beautiful um

559
00:21:18,000 --> 00:21:19,919
logarithm the discrete logarithm of our

560
00:21:19,919 --> 00:21:23,200
target element in the finite field

561
00:21:23,200 --> 00:21:24,640
so thank you very much for your

562
00:21:24,640 --> 00:21:27,280
attention and i will be happy to answer

563
00:21:27,280 --> 00:21:28,840
any questions

564
00:21:28,840 --> 00:21:33,600
um during asia crypt or even remotely

565
00:21:33,600 --> 00:21:36,840
thank you

