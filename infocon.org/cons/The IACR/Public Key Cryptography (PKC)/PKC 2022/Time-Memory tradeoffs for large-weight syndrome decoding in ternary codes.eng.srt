1
00:00:02,399 --> 00:00:04,640
hi everyone this is a presentation for

2
00:00:04,640 --> 00:00:06,720
pkc 2022

3
00:00:06,720 --> 00:00:08,559
i'm charlotte lefeber and it's this work

4
00:00:08,559 --> 00:00:10,480
in collaboration with pierre carchmann

5
00:00:10,480 --> 00:00:12,559
and the paper is named time memory

6
00:00:12,559 --> 00:00:14,160
trade-offs for large white syndrome

7
00:00:14,160 --> 00:00:17,119
decoding in ternary codes so we'll first

8
00:00:17,119 --> 00:00:18,800
type with a few definitions on the

9
00:00:18,800 --> 00:00:20,880
motivation of our work

10
00:00:20,880 --> 00:00:22,720
in this work we will be always in the

11
00:00:22,720 --> 00:00:24,960
ternary setting and consider linear

12
00:00:24,960 --> 00:00:27,199
chords where n denotes the code length

13
00:00:27,199 --> 00:00:29,119
and k the dimension

14
00:00:29,119 --> 00:00:31,199
we will study a particular instance of

15
00:00:31,199 --> 00:00:32,719
the fixed weight syndrome decoding

16
00:00:32,719 --> 00:00:34,399
problem the fixed weight syndrome

17
00:00:34,399 --> 00:00:36,480
decoding problem takes as input a

18
00:00:36,480 --> 00:00:38,800
rectangular matrix full rank also called

19
00:00:38,800 --> 00:00:41,520
the parity check matrix then a vector

20
00:00:41,520 --> 00:00:43,360
called the syndrome and an integral

21
00:00:43,360 --> 00:00:44,399
called the weight

22
00:00:44,399 --> 00:00:46,719
the aim is to solve the linear system h

23
00:00:46,719 --> 00:00:49,360
e equals s where there is a constraint

24
00:00:49,360 --> 00:00:51,520
on the hamming weight of the solution

25
00:00:51,520 --> 00:00:53,520
we will restrict ourselves in a case

26
00:00:53,520 --> 00:00:55,280
where the parity-check matrix and the

27
00:00:55,280 --> 00:00:57,600
syndrome are assumed to be both sampled

28
00:00:57,600 --> 00:01:00,000
uniformly at random and the code rates

29
00:01:00,000 --> 00:01:02,480
and weight rate are both fixed

30
00:01:02,480 --> 00:01:04,400
in particular we will be in a setting

31
00:01:04,400 --> 00:01:06,159
where there are many solutions to this

32
00:01:06,159 --> 00:01:08,880
problem exponential in the code length

33
00:01:08,880 --> 00:01:11,119
yet the search space is comparatively

34
00:01:11,119 --> 00:01:14,799
huge making thus this problem hard

35
00:01:14,799 --> 00:01:16,720
the main motivation of our work is the

36
00:01:16,720 --> 00:01:18,479
wave signature scheme introduced by

37
00:01:18,479 --> 00:01:22,000
debris a lazarette al in 2018

38
00:01:22,000 --> 00:01:24,159
its high level view is quite classical

39
00:01:24,159 --> 00:01:26,880
in code-based crypto the private key is

40
00:01:26,880 --> 00:01:29,119
a structural parity check matrix under

41
00:01:29,119 --> 00:01:31,360
public key is a random obfuscation of

42
00:01:31,360 --> 00:01:34,240
the latter making it indistinguishable

43
00:01:34,240 --> 00:01:37,360
from a random parity check matrix

44
00:01:37,360 --> 00:01:39,200
next the signature of the message

45
00:01:39,200 --> 00:01:41,360
consists of solving a syndrome decoding

46
00:01:41,360 --> 00:01:43,680
instance where the parity check matrix

47
00:01:43,680 --> 00:01:45,680
is the public key the syndrome is the

48
00:01:45,680 --> 00:01:47,680
hash of the message and the relative

49
00:01:47,680 --> 00:01:50,000
weight is fixed by the scheme in

50
00:01:50,000 --> 00:01:52,479
particular we are in a high weight

51
00:01:52,479 --> 00:01:54,079
regime

52
00:01:54,079 --> 00:01:55,600
the knowledge of the private key

53
00:01:55,600 --> 00:01:57,439
provides a trapdoor that makes the

54
00:01:57,439 --> 00:02:00,240
syndrome decoding problem much easier

55
00:02:00,240 --> 00:02:02,240
on the other side we must ensure that

56
00:02:02,240 --> 00:02:04,159
anyone with only access to the public

57
00:02:04,159 --> 00:02:07,200
key cannot produce valid signatures

58
00:02:07,200 --> 00:02:09,440
therefore one needs to ensure that the

59
00:02:09,440 --> 00:02:11,440
underlying syndrome decoding problem

60
00:02:11,440 --> 00:02:14,879
must be computationally intractable

61
00:02:14,879 --> 00:02:16,879
so to give a global picture of the state

62
00:02:16,879 --> 00:02:18,560
of the art with respect to the syndrome

63
00:02:18,560 --> 00:02:21,280
decoding problem this problem is easy

64
00:02:21,280 --> 00:02:23,360
for a certain weight range in other

65
00:02:23,360 --> 00:02:25,360
words a polynomial algorithm in the

66
00:02:25,360 --> 00:02:28,000
codelens was formed by french

67
00:02:28,000 --> 00:02:30,560
outside of this interval no polynomial

68
00:02:30,560 --> 00:02:32,879
algorithm is known so the problem is

69
00:02:32,879 --> 00:02:34,560
believed to be hard

70
00:02:34,560 --> 00:02:36,800
in particular the weight used in wave

71
00:02:36,800 --> 00:02:39,040
sneaker scheme is in the horizon but

72
00:02:39,040 --> 00:02:40,720
this is far away from the gilbert

73
00:02:40,720 --> 00:02:42,720
version of bone

74
00:02:42,720 --> 00:02:44,800
note that this is said to ensure that

75
00:02:44,800 --> 00:02:46,720
the syndrome decoding problem in wave

76
00:02:46,720 --> 00:02:48,879
signature scheme is untractable for an

77
00:02:48,879 --> 00:02:51,360
adversary one needs to know about the

78
00:02:51,360 --> 00:02:53,599
best possible attacks at this particular

79
00:02:53,599 --> 00:02:54,560
weight

80
00:02:54,560 --> 00:02:56,640
and this has been studied by bricot at

81
00:02:56,640 --> 00:02:58,560
all in 2019

82
00:02:58,560 --> 00:03:00,720
they use the pg plus ss framework

83
00:03:00,720 --> 00:03:02,720
instantiated with a combination of the

84
00:03:02,720 --> 00:03:05,200
k3 under representation technique and

85
00:03:05,200 --> 00:03:08,239
the cost that they derived is given here

86
00:03:08,239 --> 00:03:10,879
however this cost is in time and in

87
00:03:10,879 --> 00:03:13,840
memory making it making it unattractive

88
00:03:13,840 --> 00:03:15,680
in some applications

89
00:03:15,680 --> 00:03:17,680
that's why we propose in this work to

90
00:03:17,680 --> 00:03:19,440
study the time memory trade-offs for

91
00:03:19,440 --> 00:03:21,200
this problem

92
00:03:21,200 --> 00:03:23,120
so to do this we just started the same

93
00:03:23,120 --> 00:03:25,760
way as bricot at all and used the pg

94
00:03:25,760 --> 00:03:27,599
processes framework which stands for

95
00:03:27,599 --> 00:03:30,159
partial gaussian elimination plus subset

96
00:03:30,159 --> 00:03:31,120
sum

97
00:03:31,120 --> 00:03:32,959
this framework has been formalized by

98
00:03:32,959 --> 00:03:35,360
them and this is a modular description

99
00:03:35,360 --> 00:03:39,120
of what most isd-based algorithms do

100
00:03:39,120 --> 00:03:41,120
here we won't present the framework in

101
00:03:41,120 --> 00:03:43,120
all of its generality since we fixed

102
00:03:43,120 --> 00:03:45,040
some parameters

103
00:03:45,040 --> 00:03:46,959
the first step in the framework consists

104
00:03:46,959 --> 00:03:49,040
of doing a partial gaussian elimination

105
00:03:49,040 --> 00:03:51,680
on some rows of the parity-check matrix

106
00:03:51,680 --> 00:03:54,080
and the number of rows is parameterized

107
00:03:54,080 --> 00:03:56,720
by a quantity called l this is an input

108
00:03:56,720 --> 00:03:58,959
to the to this framework

109
00:03:58,959 --> 00:04:00,720
then we split the syndrome and the

110
00:04:00,720 --> 00:04:02,720
candidate solution according to this

111
00:04:02,720 --> 00:04:04,480
division

112
00:04:04,480 --> 00:04:06,319
the next step is to solve a smaller

113
00:04:06,319 --> 00:04:08,239
problem which is a sort of syndrome

114
00:04:08,239 --> 00:04:10,640
decoding problem and the parity-check

115
00:04:10,640 --> 00:04:13,920
matrix is given by h2 the syndrome s2 in

116
00:04:13,920 --> 00:04:16,639
particular it has l-coordinates and the

117
00:04:16,639 --> 00:04:18,880
solution e2 will have k plus

118
00:04:18,880 --> 00:04:20,639
l-coordinates

119
00:04:20,639 --> 00:04:23,120
we require that it is full weight and

120
00:04:23,120 --> 00:04:24,639
one important difference with the

121
00:04:24,639 --> 00:04:26,720
syndrome decoding problem is that here

122
00:04:26,720 --> 00:04:29,759
we ask for many solutions

123
00:04:29,759 --> 00:04:31,919
and the idea is that for every e2

124
00:04:31,919 --> 00:04:34,080
solution of the previous problem there

125
00:04:34,080 --> 00:04:36,400
exists a unique vector e1 such that the

126
00:04:36,400 --> 00:04:38,800
concatenation of both vectors gives a

127
00:04:38,800 --> 00:04:41,199
solution to the linear system

128
00:04:41,199 --> 00:04:43,040
in particular we solve the syndrome

129
00:04:43,040 --> 00:04:45,120
decoding problem if and only if the

130
00:04:45,120 --> 00:04:47,680
hamming weight of the concatenation is w

131
00:04:47,680 --> 00:04:50,000
and here it translates on the on the

132
00:04:50,000 --> 00:04:53,520
condition on the hamming weight of e1

133
00:04:53,520 --> 00:04:55,360
the desired number of solution that the

134
00:04:55,360 --> 00:04:57,440
step number two is in particular the

135
00:04:57,440 --> 00:05:00,160
inverse of the probability that e1 gets

136
00:05:00,160 --> 00:05:02,880
the right hamming weight

137
00:05:02,880 --> 00:05:04,479
so this framework allowed us to

138
00:05:04,479 --> 00:05:06,560
transform the problem into a slightly

139
00:05:06,560 --> 00:05:08,720
different one so in other words find

140
00:05:08,720 --> 00:05:11,120
many solutions to a smaller sub syndrome

141
00:05:11,120 --> 00:05:12,479
decoding problem

142
00:05:12,479 --> 00:05:14,880
as pointed by brico that al this problem

143
00:05:14,880 --> 00:05:17,360
reduces in reality to the subset sum

144
00:05:17,360 --> 00:05:19,039
problem

145
00:05:19,039 --> 00:05:21,280
moreover asking for many solutions

146
00:05:21,280 --> 00:05:23,600
opened the way to several optimization

147
00:05:23,600 --> 00:05:25,759
and in particular we can require that

148
00:05:25,759 --> 00:05:27,759
solutions are returned in constant

149
00:05:27,759 --> 00:05:30,240
amortized time

150
00:05:30,240 --> 00:05:32,880
to go back on this l parameter this is a

151
00:05:32,880 --> 00:05:35,039
quite important parameter

152
00:05:35,039 --> 00:05:37,840
so from now we denote by sl the desired

153
00:05:37,840 --> 00:05:39,360
number of solutions

154
00:05:39,360 --> 00:05:41,600
to the small sub problem

155
00:05:41,600 --> 00:05:44,320
what is interesting here is that when l

156
00:05:44,320 --> 00:05:46,960
increases the size of s2 is going to

157
00:05:46,960 --> 00:05:49,919
increase thus the problem becomes harder

158
00:05:49,919 --> 00:05:52,080
but on the other side the requirement on

159
00:05:52,080 --> 00:05:54,400
the hamming weight of e1 is going to be

160
00:05:54,400 --> 00:05:56,720
much easier to satisfy

161
00:05:56,720 --> 00:05:59,199
so concretely it means that the desired

162
00:05:59,199 --> 00:06:02,880
number of solution is going to decrease

163
00:06:02,880 --> 00:06:04,880
so this is what we can see

164
00:06:04,880 --> 00:06:08,000
on the first stage of this curve that

165
00:06:08,000 --> 00:06:11,919
plots the logarithm of sl according to l

166
00:06:11,919 --> 00:06:14,560
so this l parameter will be central in a

167
00:06:14,560 --> 00:06:17,600
trade-offs since one l parameter plus a

168
00:06:17,600 --> 00:06:19,600
fixed algorithm to solve the smaller

169
00:06:19,600 --> 00:06:22,639
problem can provide us one trade-off and

170
00:06:22,639 --> 00:06:24,720
usually the memory consumption is going

171
00:06:24,720 --> 00:06:28,080
to increase along with l parameter

172
00:06:28,080 --> 00:06:30,080
so this particular plot was done by

173
00:06:30,080 --> 00:06:31,680
fixing the code length in the wave

174
00:06:31,680 --> 00:06:32,880
regime

175
00:06:32,880 --> 00:06:35,440
note that l equal to zero corresponds to

176
00:06:35,440 --> 00:06:38,400
french algorithm which is memoryless and

177
00:06:38,400 --> 00:06:41,039
recorded uh shows the following l

178
00:06:41,039 --> 00:06:43,360
parameters and it and it could provide

179
00:06:43,360 --> 00:06:45,440
them solution it was able to provide

180
00:06:45,440 --> 00:06:47,520
them solutions in constant amortized

181
00:06:47,520 --> 00:06:49,840
time and in our case we are going to

182
00:06:49,840 --> 00:06:52,479
explore uh the l parameter between these

183
00:06:52,479 --> 00:06:54,720
two quantities

184
00:06:54,720 --> 00:06:56,720
we are now going to see how can we

185
00:06:56,720 --> 00:06:59,440
transform the sub problem into only sum

186
00:06:59,440 --> 00:07:01,039
problem

187
00:07:01,039 --> 00:07:03,520
from now i'm going to drop the two index

188
00:07:03,520 --> 00:07:05,840
notation so that now we want to solve h

189
00:07:05,840 --> 00:07:08,800
e equal to s with e full weight

190
00:07:08,800 --> 00:07:10,800
we are going to use algorithm solving

191
00:07:10,800 --> 00:07:12,960
the only sum problem also called the

192
00:07:12,960 --> 00:07:15,280
generalized birthday problem

193
00:07:15,280 --> 00:07:17,360
so this generalized birthday problem

194
00:07:17,360 --> 00:07:19,840
takes as input our list each list

195
00:07:19,840 --> 00:07:22,720
containing vector and the aim is to find

196
00:07:22,720 --> 00:07:24,720
one element in each list such that the

197
00:07:24,720 --> 00:07:27,280
sum of all of these elements give the

198
00:07:27,280 --> 00:07:29,280
null vector

199
00:07:29,280 --> 00:07:31,280
the syndrome decoding problem can be can

200
00:07:31,280 --> 00:07:33,520
be transformed into all this problem by

201
00:07:33,520 --> 00:07:36,240
doing the following so first we split

202
00:07:36,240 --> 00:07:38,240
the evenly the parity check matrix

203
00:07:38,240 --> 00:07:41,199
columns into our slices we do the same

204
00:07:41,199 --> 00:07:43,039
for the candidate solution

205
00:07:43,039 --> 00:07:45,840
and now the linear system h e equals s

206
00:07:45,840 --> 00:07:48,000
can be rewritten as follow

207
00:07:48,000 --> 00:07:50,639
so now the list l i is going to contain

208
00:07:50,639 --> 00:07:52,479
full weight linear combinations of

209
00:07:52,479 --> 00:07:55,360
columns of h i unless the last list

210
00:07:55,360 --> 00:07:56,319
which

211
00:07:56,319 --> 00:07:58,479
will also have uh the syndrome

212
00:07:58,479 --> 00:08:00,240
subtracted

213
00:08:00,240 --> 00:08:02,240
so this is quite a classical approach

214
00:08:02,240 --> 00:08:04,400
and was for instance used by brico that

215
00:08:04,400 --> 00:08:06,639
are in their work

216
00:08:06,639 --> 00:08:08,800
so here even despite the partial

217
00:08:08,800 --> 00:08:11,199
gaussian elimination done within the pg

218
00:08:11,199 --> 00:08:13,759
processes framework we will be still in

219
00:08:13,759 --> 00:08:15,840
a setting where the desired number of

220
00:08:15,840 --> 00:08:18,479
solution is smaller than the total

221
00:08:18,479 --> 00:08:20,560
expected number of solutions to the sub

222
00:08:20,560 --> 00:08:22,800
syndrome decoding problem here

223
00:08:22,800 --> 00:08:24,639
therefore in particular we are not

224
00:08:24,639 --> 00:08:27,120
obliged to consider all possible full

225
00:08:27,120 --> 00:08:29,199
weight linear combinations in english

226
00:08:29,199 --> 00:08:30,639
each list

227
00:08:30,639 --> 00:08:32,719
thus we have some freedom on the size of

228
00:08:32,719 --> 00:08:34,240
the list

229
00:08:34,240 --> 00:08:37,039
however on the other side we cannot have

230
00:08:37,039 --> 00:08:39,200
lists as large as we want because in

231
00:08:39,200 --> 00:08:41,200
particular each list contains full

232
00:08:41,200 --> 00:08:44,640
weight linear combination of k plus l

233
00:08:44,640 --> 00:08:47,040
over r columns of a matrix

234
00:08:47,040 --> 00:08:49,440
so it cannot be as large as we want and

235
00:08:49,440 --> 00:08:51,920
this is exactly what this constraint

236
00:08:51,920 --> 00:08:54,080
translates

237
00:08:54,080 --> 00:08:56,800
to so the underlying only sum problem we

238
00:08:56,800 --> 00:08:59,040
are going to use algorithms and all of

239
00:08:59,040 --> 00:09:01,519
them enumerate smartly a fraction of the

240
00:09:01,519 --> 00:09:02,720
solutions

241
00:09:02,720 --> 00:09:04,560
we have some freedom on the number of

242
00:09:04,560 --> 00:09:07,120
lists the size of the syndrome and to a

243
00:09:07,120 --> 00:09:09,519
certain extent the size of the list

244
00:09:09,519 --> 00:09:11,200
so here are the algorithm that we

245
00:09:11,200 --> 00:09:12,160
consider

246
00:09:12,160 --> 00:09:14,560
the dissection and the k3 algorithm can

247
00:09:14,560 --> 00:09:16,080
be seen as improvements of the

248
00:09:16,080 --> 00:09:18,399
meet-in-the-middle algorithm and this is

249
00:09:18,399 --> 00:09:20,800
also possible to hybrid them in what we

250
00:09:20,800 --> 00:09:23,040
will call here dissection entry

251
00:09:23,040 --> 00:09:25,120
and this is the algorithm that will

252
00:09:25,120 --> 00:09:28,880
provide us our best instances

253
00:09:29,040 --> 00:09:31,519
the algorithm presented here are able to

254
00:09:31,519 --> 00:09:33,279
return solutions to the early sum

255
00:09:33,279 --> 00:09:36,160
problem in constant amortized time so

256
00:09:36,160 --> 00:09:38,240
with respect to time cost we could first

257
00:09:38,240 --> 00:09:40,560
think that this is optimal

258
00:09:40,560 --> 00:09:43,120
however sometimes the algorithm returned

259
00:09:43,120 --> 00:09:45,120
too much solution

260
00:09:45,120 --> 00:09:46,800
compared to the number of solutions that

261
00:09:46,800 --> 00:09:49,279
we want so that the amortized cost

262
00:09:49,279 --> 00:09:51,920
regime works only when we require at

263
00:09:51,920 --> 00:09:54,640
least a certain number of solutions

264
00:09:54,640 --> 00:09:57,279
and in and if this is not satisfied this

265
00:09:57,279 --> 00:09:59,200
yields a loss

266
00:09:59,200 --> 00:10:00,880
that's why we introduce the notion of

267
00:10:00,880 --> 00:10:03,279
granularity and it's going to help us to

268
00:10:03,279 --> 00:10:05,839
assess if an algorithm is adapted with

269
00:10:05,839 --> 00:10:07,600
respect to this quantity

270
00:10:07,600 --> 00:10:09,920
the granularity is simply the smallest

271
00:10:09,920 --> 00:10:11,760
number of solutions that an algorithm

272
00:10:11,760 --> 00:10:14,000
can return while not changing its

273
00:10:14,000 --> 00:10:16,399
amortized cost

274
00:10:16,399 --> 00:10:18,320
looking ahead the dissection is an

275
00:10:18,320 --> 00:10:20,720
algorithm which is very memory friendly

276
00:10:20,720 --> 00:10:22,240
which has on the other side a

277
00:10:22,240 --> 00:10:24,880
granularity not adapted for many l

278
00:10:24,880 --> 00:10:27,600
parameters and concretely it will only

279
00:10:27,600 --> 00:10:30,160
give interesting instances for small l

280
00:10:30,160 --> 00:10:32,640
parameter that corresponds to quite

281
00:10:32,640 --> 00:10:34,399
extreme tradeoffs

282
00:10:34,399 --> 00:10:36,640
in this work we spent a significant time

283
00:10:36,640 --> 00:10:38,880
trying to lower the granularity of some

284
00:10:38,880 --> 00:10:42,880
of this algorithm when it is too coarse

285
00:10:42,880 --> 00:10:44,640
now i'm going to present the different

286
00:10:44,640 --> 00:10:47,600
algorithms and we start with the k3

287
00:10:47,600 --> 00:10:51,440
the k3 was introduced by wagner in 2002

288
00:10:51,440 --> 00:10:53,760
so for simplicity we required that the

289
00:10:53,760 --> 00:10:56,320
number of lists is a power of 2 so 2 to

290
00:10:56,320 --> 00:10:58,720
the a where a denotes the number of

291
00:10:58,720 --> 00:10:59,760
levels

292
00:10:59,760 --> 00:11:02,240
so here is the example of a k3 with r

293
00:11:02,240 --> 00:11:04,800
equals 8 so that the number of levels is

294
00:11:04,800 --> 00:11:06,480
free

295
00:11:06,480 --> 00:11:09,120
the target is split evenly into three

296
00:11:09,120 --> 00:11:11,680
slices and at each level a meet in the

297
00:11:11,680 --> 00:11:14,320
middle algorithm is applied with one

298
00:11:14,320 --> 00:11:16,399
slice of the target

299
00:11:16,399 --> 00:11:18,640
in our case the tree is balanced so that

300
00:11:18,640 --> 00:11:21,279
the number of written solution after

301
00:11:21,279 --> 00:11:23,519
each each merging is expected to be

302
00:11:23,519 --> 00:11:26,959
equal to the initial list size

303
00:11:26,959 --> 00:11:29,040
we can see that one solution written by

304
00:11:29,040 --> 00:11:31,839
this tree is highly structured since you

305
00:11:31,839 --> 00:11:34,000
require for instance that the first two

306
00:11:34,000 --> 00:11:38,240
elements in l1 and l2 sum on zero on l

307
00:11:38,240 --> 00:11:40,880
of a three coordinates by construction

308
00:11:40,880 --> 00:11:43,760
therefore the k3 decimates the solutions

309
00:11:43,760 --> 00:11:45,519
but this is not a problem in our case

310
00:11:45,519 --> 00:11:47,760
since once again we do not want all of

311
00:11:47,760 --> 00:11:50,000
the solutions so it's even an advantage

312
00:11:50,000 --> 00:11:51,760
and even better than that because

313
00:11:51,760 --> 00:11:54,000
decimating the solution will give an

314
00:11:54,000 --> 00:11:56,800
algorithm with a fine granularity

315
00:11:56,800 --> 00:11:59,200
and if we want more solution we can just

316
00:11:59,200 --> 00:12:01,360
iterate this tree using different

317
00:12:01,360 --> 00:12:03,839
intermediate targets so for example here

318
00:12:03,839 --> 00:12:06,240
we can take an arbitrary vector t

319
00:12:06,240 --> 00:12:08,800
instead of the zero target and here

320
00:12:08,800 --> 00:12:12,560
replace the zero by minus t

321
00:12:12,639 --> 00:12:14,480
now given the constraint that we want

322
00:12:14,480 --> 00:12:17,120
solution in constant amortized time for

323
00:12:17,120 --> 00:12:20,240
fixed l and a parameters there exists an

324
00:12:20,240 --> 00:12:22,639
optimal choice for the list size in the

325
00:12:22,639 --> 00:12:24,720
sense that it would minimize both the

326
00:12:24,720 --> 00:12:28,000
granularity and the memory consumption

327
00:12:28,000 --> 00:12:30,240
and both quantities are asymptotically

328
00:12:30,240 --> 00:12:33,760
equal to three to the l of a

329
00:12:33,760 --> 00:12:36,639
therefore given one fixed l one wants to

330
00:12:36,639 --> 00:12:38,880
have a number of levels as large as

331
00:12:38,880 --> 00:12:39,920
possible

332
00:12:39,920 --> 00:12:42,480
but this quantity is limited because of

333
00:12:42,480 --> 00:12:44,160
the constraint on the list stack that we

334
00:12:44,160 --> 00:12:46,720
mentioned earlier so it emits it limits

335
00:12:46,720 --> 00:12:49,040
the maximal number of levels that we can

336
00:12:49,040 --> 00:12:53,760
use given one fixed l parameter

337
00:12:53,760 --> 00:12:55,440
here are the trade-offs offered by the

338
00:12:55,440 --> 00:12:56,959
k3 algorithm

339
00:12:56,959 --> 00:12:59,360
on y-axis this is the logarithm of the

340
00:12:59,360 --> 00:13:01,839
time cost divided by the codelet and on

341
00:13:01,839 --> 00:13:04,639
x-axis the same for the memory cost

342
00:13:04,639 --> 00:13:06,480
this diamond points is the cost that

343
00:13:06,480 --> 00:13:09,120
bricot i'll found in particular they use

344
00:13:09,120 --> 00:13:10,880
the smooth scale tree algorithm that

345
00:13:10,880 --> 00:13:13,040
i'll mention in a slide combined with

346
00:13:13,040 --> 00:13:15,200
the representation technique that i

347
00:13:15,200 --> 00:13:17,200
won't present here

348
00:13:17,200 --> 00:13:20,079
so going back to the k3 graph here one

349
00:13:20,079 --> 00:13:21,920
point represents the application of the

350
00:13:21,920 --> 00:13:25,040
k3 with one l parameters and the values

351
00:13:25,040 --> 00:13:28,320
of l are increasing from left to right

352
00:13:28,320 --> 00:13:30,800
not worthy is that this graph is made

353
00:13:30,800 --> 00:13:33,760
out of several lines in fact one line

354
00:13:33,760 --> 00:13:36,320
represents the k3 with one fixed number

355
00:13:36,320 --> 00:13:37,600
of levels

356
00:13:37,600 --> 00:13:39,440
for each line take for example the

357
00:13:39,440 --> 00:13:40,720
orange line

358
00:13:40,720 --> 00:13:43,279
it corresponds to six levels

359
00:13:43,279 --> 00:13:45,760
and the right test point is the largest

360
00:13:45,760 --> 00:13:49,040
l parameter such that we can use the k3

361
00:13:49,040 --> 00:13:50,560
with six levels

362
00:13:50,560 --> 00:13:52,880
so for any larger l parameter this

363
00:13:52,880 --> 00:13:54,959
constraint is no more satisfied

364
00:13:54,959 --> 00:13:57,199
therefore the number of levels needs to

365
00:13:57,199 --> 00:13:59,839
be decreased

366
00:14:00,000 --> 00:14:01,920
since in nokia is decreasing the number

367
00:14:01,920 --> 00:14:04,160
of levels leads to a larger memory

368
00:14:04,160 --> 00:14:06,480
consumption this explains why we have

369
00:14:06,480 --> 00:14:09,519
some gaps um in the memory consumption

370
00:14:09,519 --> 00:14:12,160
between when we change uh the number of

371
00:14:12,160 --> 00:14:14,240
levels

372
00:14:14,240 --> 00:14:16,240
in fact there exists a way to obtain a

373
00:14:16,240 --> 00:14:18,160
smooth graph with slightly better

374
00:14:18,160 --> 00:14:20,720
trade-offs this is given by the extended

375
00:14:20,720 --> 00:14:23,120
k3 algorithm presented by minda and

376
00:14:23,120 --> 00:14:25,680
sinclair and adapted in the ternary

377
00:14:25,680 --> 00:14:28,480
setting by bricot at all i won't go into

378
00:14:28,480 --> 00:14:30,720
the details but the raw idea is that the

379
00:14:30,720 --> 00:14:33,519
merging at the first level is unbalanced

380
00:14:33,519 --> 00:14:35,920
and partially dedicated to increase the

381
00:14:35,920 --> 00:14:37,040
list size

382
00:14:37,040 --> 00:14:39,279
and concretely it allows to add one more

383
00:14:39,279 --> 00:14:41,120
level than what the constraint on the

384
00:14:41,120 --> 00:14:44,959
list list size dictated

385
00:14:44,959 --> 00:14:47,199
as we can see on this graph it provides

386
00:14:47,199 --> 00:14:49,519
us a smooth curve on better results than

387
00:14:49,519 --> 00:14:51,680
the k3

388
00:14:51,680 --> 00:14:53,519
so now i will mention the dissection

389
00:14:53,519 --> 00:14:55,360
framework that we explored in this work

390
00:14:55,360 --> 00:14:56,880
too

391
00:14:56,880 --> 00:14:59,120
it was introduced by dinner at all in

392
00:14:59,120 --> 00:15:02,079
2012 and is another generalization of

393
00:15:02,079 --> 00:15:04,240
the meet in the middle algorithm

394
00:15:04,240 --> 00:15:06,079
i won't present the inner working of the

395
00:15:06,079 --> 00:15:08,880
dissection but rather the key ids

396
00:15:08,880 --> 00:15:10,639
so it's a memory friendly family of

397
00:15:10,639 --> 00:15:13,120
algorithm that was designed initially to

398
00:15:13,120 --> 00:15:15,680
be exhaustive so return all of the

399
00:15:15,680 --> 00:15:17,920
possible solutions but it is still

400
00:15:17,920 --> 00:15:20,320
possible to not return them all and

401
00:15:20,320 --> 00:15:22,160
have um

402
00:15:22,160 --> 00:15:25,199
and adapt slightly the granularity

403
00:15:25,199 --> 00:15:28,079
but the the overall granularity is going

404
00:15:28,079 --> 00:15:30,720
to remain somehow coarse

405
00:15:30,720 --> 00:15:32,480
some trade-offs are possible with the

406
00:15:32,480 --> 00:15:34,720
choice of the initial list size but

407
00:15:34,720 --> 00:15:36,399
there are only a few choices that

408
00:15:36,399 --> 00:15:38,560
provide solution in constant amortized

409
00:15:38,560 --> 00:15:39,759
time

410
00:15:39,759 --> 00:15:41,680
and contrarily to the category where the

411
00:15:41,680 --> 00:15:44,079
problem is split in a symmetric way the

412
00:15:44,079 --> 00:15:46,240
dissection decomposes the only sum

413
00:15:46,240 --> 00:15:49,120
problem into a small one and a large one

414
00:15:49,120 --> 00:15:50,880
the smaller problem has its solution

415
00:15:50,880 --> 00:15:53,199
stored in memory while the large one has

416
00:15:53,199 --> 00:15:55,519
a solution returned on the fly and this

417
00:15:55,519 --> 00:15:58,000
decomposition can be done recursively

418
00:15:58,000 --> 00:16:00,720
within the dissection framework

419
00:16:00,720 --> 00:16:02,399
now i will present the possible

420
00:16:02,399 --> 00:16:03,519
tradeoffs

421
00:16:03,519 --> 00:16:05,519
on this plot the rate points represent

422
00:16:05,519 --> 00:16:07,920
the trade-off offered by our dissection

423
00:16:07,920 --> 00:16:10,240
with r smaller than 400

424
00:16:10,240 --> 00:16:12,880
we did not increase r any further and it

425
00:16:12,880 --> 00:16:15,040
would only add more points in this zone

426
00:16:15,040 --> 00:16:16,880
of the graph that corresponds to quite

427
00:16:16,880 --> 00:16:18,720
extreme tradeoffs

428
00:16:18,720 --> 00:16:21,839
this graph was grown using one single l

429
00:16:21,839 --> 00:16:24,519
parameter fixed to

430
00:16:24,519 --> 00:16:27,040
0.0.04 times m

431
00:16:27,040 --> 00:16:29,279
this gray horizontal line represents the

432
00:16:29,279 --> 00:16:31,120
desired number of solutions for this

433
00:16:31,120 --> 00:16:32,720
particular l

434
00:16:32,720 --> 00:16:34,959
and in the dice and the dissection curve

435
00:16:34,959 --> 00:16:37,279
does not reach it so it means that it

436
00:16:37,279 --> 00:16:39,360
cannot return the desired number of

437
00:16:39,360 --> 00:16:42,480
solution in constant amortized time

438
00:16:42,480 --> 00:16:43,920
and the reason for that is that the

439
00:16:43,920 --> 00:16:46,000
dissection has a granularity which is

440
00:16:46,000 --> 00:16:48,880
not adapted here and this is true for a

441
00:16:48,880 --> 00:16:51,519
quite large range of l parameters in

442
00:16:51,519 --> 00:16:53,360
general the dissection matches the

443
00:16:53,360 --> 00:16:55,519
desired number of solutions only for

444
00:16:55,519 --> 00:16:57,920
very small l values

445
00:16:57,920 --> 00:17:00,000
so we tried to lower the granularity of

446
00:17:00,000 --> 00:17:02,240
the dissection and it gives us the

447
00:17:02,240 --> 00:17:05,599
following blood dots but it gives only a

448
00:17:05,599 --> 00:17:08,000
very small improvement

449
00:17:08,000 --> 00:17:10,160
one last remark on this graph is that

450
00:17:10,160 --> 00:17:12,959
after a certain point uh it there is a

451
00:17:12,959 --> 00:17:15,439
little interest in increasing further

452
00:17:15,439 --> 00:17:17,359
the memory because increasing further

453
00:17:17,359 --> 00:17:19,439
the memory would only

454
00:17:19,439 --> 00:17:23,760
give a small gain in the time cost

455
00:17:24,000 --> 00:17:25,839
now that this is said we would like to

456
00:17:25,839 --> 00:17:28,240
extract the best tradeoffs by varying l

457
00:17:28,240 --> 00:17:30,400
parameters and it gives the following

458
00:17:30,400 --> 00:17:32,960
plot where each color represents the

459
00:17:32,960 --> 00:17:34,799
dissection plot

460
00:17:34,799 --> 00:17:37,600
for one l parameter

461
00:17:37,600 --> 00:17:39,840
to obtain only the interesting points we

462
00:17:39,840 --> 00:17:42,799
can extract the outline of the curve

463
00:17:42,799 --> 00:17:45,440
and obtain then this figure

464
00:17:45,440 --> 00:17:47,280
and we can now compare the dissection

465
00:17:47,280 --> 00:17:49,679
tradeoffs to the ones of the k3 so the

466
00:17:49,679 --> 00:17:52,160
dissection outperforms the k3 algorithm

467
00:17:52,160 --> 00:17:54,559
for small memories it is meaningful

468
00:17:54,559 --> 00:17:56,080
since in this zone the dissection

469
00:17:56,080 --> 00:17:58,799
granularity usually matches the desired

470
00:17:58,799 --> 00:18:01,120
number of solutions and we can thus

471
00:18:01,120 --> 00:18:03,520
benefit from the memory friendliness of

472
00:18:03,520 --> 00:18:05,200
the dissection

473
00:18:05,200 --> 00:18:07,520
on the contrary the k3 is less memory

474
00:18:07,520 --> 00:18:10,400
efficient yet its granularity is not

475
00:18:10,400 --> 00:18:13,600
limiting up to large earth values

476
00:18:13,600 --> 00:18:15,039
so so far we have seen two

477
00:18:15,039 --> 00:18:16,480
generalizations of the meat in the

478
00:18:16,480 --> 00:18:18,400
middle which are by instance very

479
00:18:18,400 --> 00:18:19,600
different

480
00:18:19,600 --> 00:18:21,840
then one possibility is to consider a

481
00:18:21,840 --> 00:18:24,240
hybrid version that somehow combines the

482
00:18:24,240 --> 00:18:26,960
id of both algorithm that we'll call

483
00:18:26,960 --> 00:18:29,679
here dissection entry

484
00:18:29,679 --> 00:18:32,000
it's an algorithm presented by dinner in

485
00:18:32,000 --> 00:18:33,600
2019

486
00:18:33,600 --> 00:18:35,840
and the idea here is that the lists are

487
00:18:35,840 --> 00:18:38,240
merged in a tree structure just like

488
00:18:38,240 --> 00:18:40,799
with the k3 but instead of applying the

489
00:18:40,799 --> 00:18:43,440
meat in the middle on couples of list

490
00:18:43,440 --> 00:18:45,919
now a x dissection is applied on list

491
00:18:45,919 --> 00:18:47,280
doubles

492
00:18:47,280 --> 00:18:49,520
so here is the example of one of the

493
00:18:49,520 --> 00:18:52,320
digestion dissection tree that gave us

494
00:18:52,320 --> 00:18:55,039
our best results it is a tree with three

495
00:18:55,039 --> 00:18:57,600
levels and each dissection done is the

496
00:18:57,600 --> 00:19:00,240
four dissection

497
00:19:00,240 --> 00:19:02,080
at the moment we impose that every

498
00:19:02,080 --> 00:19:04,640
dissection done is exhaustive and still

499
00:19:04,640 --> 00:19:06,880
required that solutions are written in

500
00:19:06,880 --> 00:19:08,799
constant amortized time

501
00:19:08,799 --> 00:19:11,200
and we shall not forget the constraint

502
00:19:11,200 --> 00:19:13,440
on the initial list size

503
00:19:13,440 --> 00:19:15,600
in fact i didn't mention this before but

504
00:19:15,600 --> 00:19:17,840
the four dissection is the base case in

505
00:19:17,840 --> 00:19:20,160
the dissection framework and this is the

506
00:19:20,160 --> 00:19:22,559
only one which is symmetric in

507
00:19:22,559 --> 00:19:24,400
particular this is just an exhaustive

508
00:19:24,400 --> 00:19:26,160
variant of the k3

509
00:19:26,160 --> 00:19:28,720
therefore if we unfold in this tree what

510
00:19:28,720 --> 00:19:31,120
happens within the four dissection we

511
00:19:31,120 --> 00:19:32,720
obtain a tree

512
00:19:32,720 --> 00:19:34,880
that has exactly the same structure as

513
00:19:34,880 --> 00:19:37,679
the one of a k3 with six levels so we

514
00:19:37,679 --> 00:19:39,600
might wonder what's the difference

515
00:19:39,600 --> 00:19:42,480
between these two algorithms

516
00:19:42,480 --> 00:19:44,320
one of the main difference between them

517
00:19:44,320 --> 00:19:46,799
is in the merging strategy the k3

518
00:19:46,799 --> 00:19:48,880
decimates solution in the somehow steady

519
00:19:48,880 --> 00:19:51,200
way while the dissection entry is

520
00:19:51,200 --> 00:19:53,200
locally exhaustive

521
00:19:53,200 --> 00:19:55,600
so on this plot the run point represents

522
00:19:55,600 --> 00:19:57,840
the trade-offs with the dissection entry

523
00:19:57,840 --> 00:20:00,320
and the square ones with the k3

524
00:20:00,320 --> 00:20:02,720
additionally there is here a color scale

525
00:20:02,720 --> 00:20:04,400
that represents the value of the

526
00:20:04,400 --> 00:20:06,960
underlying l parameters used for every

527
00:20:06,960 --> 00:20:08,159
point

528
00:20:08,159 --> 00:20:10,080
unless for this particular part of the

529
00:20:10,080 --> 00:20:11,919
curve that i will

530
00:20:11,919 --> 00:20:13,600
mention in a moment

531
00:20:13,600 --> 00:20:16,320
one horizontal line gives two points

532
00:20:16,320 --> 00:20:18,880
with the same l values in particular the

533
00:20:18,880 --> 00:20:22,080
same target size and the same time cost

534
00:20:22,080 --> 00:20:24,000
so now this is clear that the dissection

535
00:20:24,000 --> 00:20:27,039
entry outperforms the k3 since it has a

536
00:20:27,039 --> 00:20:29,280
smaller memory consumption

537
00:20:29,280 --> 00:20:31,280
one possible explanation is that the

538
00:20:31,280 --> 00:20:33,760
layer dissection is going to return much

539
00:20:33,760 --> 00:20:35,679
less structured solution

540
00:20:35,679 --> 00:20:37,679
thus it can explore more candidate

541
00:20:37,679 --> 00:20:39,840
solution within the list

542
00:20:39,840 --> 00:20:41,919
but on the other side for amortized

543
00:20:41,919 --> 00:20:44,159
costs reason the dissection in tree is

544
00:20:44,159 --> 00:20:46,240
going to return also much more solutions

545
00:20:46,240 --> 00:20:49,280
than the k3 yet in this case the memory

546
00:20:49,280 --> 00:20:51,520
consumption is still better

547
00:20:51,520 --> 00:20:53,200
and because the memory consumption is

548
00:20:53,200 --> 00:20:56,000
lower there are more l values such that

549
00:20:56,000 --> 00:20:58,320
we can apply this dissection entry

550
00:20:58,320 --> 00:21:00,640
because the constraints on the on the

551
00:21:00,640 --> 00:21:03,440
list size limits the memory up to this

552
00:21:03,440 --> 00:21:06,320
particular value

553
00:21:06,799 --> 00:21:09,039
one downside with the layer dissection

554
00:21:09,039 --> 00:21:12,159
is that it has a closer granularity

555
00:21:12,159 --> 00:21:14,640
with the k3 the granularity was given by

556
00:21:14,640 --> 00:21:16,960
the initial list size while for the

557
00:21:16,960 --> 00:21:19,360
dissection in three the granularity is

558
00:21:19,360 --> 00:21:22,240
the square of the list size

559
00:21:22,240 --> 00:21:24,240
we can see that granularity problems

560
00:21:24,240 --> 00:21:26,480
appear exactly at this trade-off t

561
00:21:26,480 --> 00:21:29,120
equals m square and from this part of

562
00:21:29,120 --> 00:21:31,600
this point one iteration of the layer

563
00:21:31,600 --> 00:21:34,080
dissect of the dissection tree is going

564
00:21:34,080 --> 00:21:36,400
to return too much solutions so that's

565
00:21:36,400 --> 00:21:39,919
why in particular the curve rebounds

566
00:21:39,919 --> 00:21:41,840
the bottleneck in the granularity was

567
00:21:41,840 --> 00:21:44,400
because of the dissection granularity so

568
00:21:44,400 --> 00:21:46,480
that's why to solve this granularity

569
00:21:46,480 --> 00:21:48,720
problems we consider non-exhaustive

570
00:21:48,720 --> 00:21:51,200
dissection so that its granularity

571
00:21:51,200 --> 00:21:53,440
matches exactly the number of desired

572
00:21:53,440 --> 00:21:54,799
solution

573
00:21:54,799 --> 00:21:57,039
in particular by doing that we decimate

574
00:21:57,039 --> 00:21:59,280
the solution quite rapidly so that the

575
00:21:59,280 --> 00:22:01,679
memory efficiency decreases

576
00:22:01,679 --> 00:22:04,480
it gave us the extra points in brown and

577
00:22:04,480 --> 00:22:07,440
we see the loss in the slope of the

578
00:22:07,440 --> 00:22:08,960
curve

579
00:22:08,960 --> 00:22:10,880
in the case of the four dissection with

580
00:22:10,880 --> 00:22:13,280
three levels by doing that we go back

581
00:22:13,280 --> 00:22:16,640
gradually to what does a classical k3 do

582
00:22:16,640 --> 00:22:19,600
when we increase the memory consumption

583
00:22:19,600 --> 00:22:22,240
but in this case what stops the curve is

584
00:22:22,240 --> 00:22:25,600
the constraint on the list size

585
00:22:25,600 --> 00:22:27,840
one natural idea to extend the plot is

586
00:22:27,840 --> 00:22:29,679
then to apply the smoothing technique on

587
00:22:29,679 --> 00:22:30,799
top of that

588
00:22:30,799 --> 00:22:33,520
and it gave us the following green pots

589
00:22:33,520 --> 00:22:35,600
unsurprisingly the rightmost point

590
00:22:35,600 --> 00:22:37,679
corresponds in fact to the small scale

591
00:22:37,679 --> 00:22:39,760
tree with six levels so at this

592
00:22:39,760 --> 00:22:42,559
particular point the four dissection is

593
00:22:42,559 --> 00:22:45,200
just a four tree without any repetition

594
00:22:45,200 --> 00:22:49,039
of the tree with intermediate targets

595
00:22:50,159 --> 00:22:53,200
now let's see a summary of our results

596
00:22:53,200 --> 00:22:55,280
this is a table with the asymptotic cost

597
00:22:55,280 --> 00:22:57,919
given several trade-offs the first line

598
00:22:57,919 --> 00:22:59,440
corresponds to the cost formed by

599
00:22:59,440 --> 00:23:02,240
recruited al with time equals memory

600
00:23:02,240 --> 00:23:03,840
we can see that when the memory

601
00:23:03,840 --> 00:23:05,440
consumption relative to the time

602
00:23:05,440 --> 00:23:07,679
decreases it is more interesting to

603
00:23:07,679 --> 00:23:10,159
consider larger dissections

604
00:23:10,159 --> 00:23:12,559
not worthy is that the product memory

605
00:23:12,559 --> 00:23:14,559
time which can be helpful as a tool to

606
00:23:14,559 --> 00:23:16,640
compare different trade-offs

607
00:23:16,640 --> 00:23:19,280
is smaller with all trade-offs

608
00:23:19,280 --> 00:23:20,799
and we can see all of the results

609
00:23:20,799 --> 00:23:22,559
plotted on this graph

610
00:23:22,559 --> 00:23:24,960
here this is clear that the dissection

611
00:23:24,960 --> 00:23:28,400
in grey and the k3 in black are all

612
00:23:28,400 --> 00:23:30,799
performed by layer dissection

613
00:23:30,799 --> 00:23:32,960
for small memories globally what works

614
00:23:32,960 --> 00:23:35,440
best is the dissection with two levels

615
00:23:35,440 --> 00:23:37,520
while for larger memory we should rather

616
00:23:37,520 --> 00:23:39,919
use a four dissection with three or four

617
00:23:39,919 --> 00:23:41,360
levels

618
00:23:41,360 --> 00:23:43,200
in this plot the cost computed is

619
00:23:43,200 --> 00:23:45,600
asymptotic and many quantities were not

620
00:23:45,600 --> 00:23:47,760
taken into account for example the

621
00:23:47,760 --> 00:23:50,000
factors when doing operation in f3 to

622
00:23:50,000 --> 00:23:52,400
the m and the polynomial factors in the

623
00:23:52,400 --> 00:23:55,039
cost estimates were omitted

624
00:23:55,039 --> 00:23:56,880
so we refined the estimate of the cost

625
00:23:56,880 --> 00:23:59,279
by taking these quantities into account

626
00:23:59,279 --> 00:24:01,120
the idea here is to have a sort of

627
00:24:01,120 --> 00:24:03,440
intermediate cost between the asymptotic

628
00:24:03,440 --> 00:24:05,600
ones and the full and accurate modeling

629
00:24:05,600 --> 00:24:06,799
of the attack

630
00:24:06,799 --> 00:24:08,880
here is the plot that we derived using

631
00:24:08,880 --> 00:24:11,679
the wave signature scheme parameters

632
00:24:11,679 --> 00:24:14,000
in particular lots of craft disappeared

633
00:24:14,000 --> 00:24:16,880
here because these factors made them

634
00:24:16,880 --> 00:24:19,039
uninteresting to use

635
00:24:19,039 --> 00:24:20,720
for example the four dissection

636
00:24:20,720 --> 00:24:22,960
reforlovers becomes always less

637
00:24:22,960 --> 00:24:25,039
interesting than the full dissection

638
00:24:25,039 --> 00:24:26,960
with three levels

639
00:24:26,960 --> 00:24:28,799
now to conclude in this work we

640
00:24:28,799 --> 00:24:31,039
investigated time memory trade-offs for

641
00:24:31,039 --> 00:24:32,799
a particular instance of the syndrome

642
00:24:32,799 --> 00:24:34,880
decoding problem underlying wealth

643
00:24:34,880 --> 00:24:36,880
signature scheme security

644
00:24:36,880 --> 00:24:39,279
to do that we used the pga processes

645
00:24:39,279 --> 00:24:41,279
framework and instantiated it with the

646
00:24:41,279 --> 00:24:44,640
k3 dissection and dissection entry we

647
00:24:44,640 --> 00:24:46,640
spent a significant time trying to

648
00:24:46,640 --> 00:24:48,640
decrease the granularity of the building

649
00:24:48,640 --> 00:24:51,279
blocks so that it matched it it matched

650
00:24:51,279 --> 00:24:53,200
the desired number of solutions in our

651
00:24:53,200 --> 00:24:54,880
case

652
00:24:54,880 --> 00:24:56,960
so this is all for this presentation i

653
00:24:56,960 --> 00:24:59,279
hope that you enjoy it and i wish you a

654
00:24:59,279 --> 00:25:01,840
nice day

