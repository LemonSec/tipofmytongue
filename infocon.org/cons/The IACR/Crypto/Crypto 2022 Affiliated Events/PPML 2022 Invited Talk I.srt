1
00:00:00,240 --> 00:00:01,839
so we're very glad to have our pradeep

2
00:00:01,839 --> 00:00:03,919
here with us he's at the google at

3
00:00:03,919 --> 00:00:06,000
mountain view at the brain team and he

4
00:00:06,000 --> 00:00:07,680
has a great experience both in the

5
00:00:07,680 --> 00:00:10,400
academy and in the industry he did

6
00:00:10,400 --> 00:00:13,440
his speech the university

7
00:00:13,440 --> 00:00:16,160
penn state penn state university and

8
00:00:16,160 --> 00:00:18,480
with adam smith and then he did his post

9
00:00:18,480 --> 00:00:19,760
talk

10
00:00:19,760 --> 00:00:21,600
at microsoft research in silicon valley

11
00:00:21,600 --> 00:00:23,840
and stanford university

12
00:00:23,840 --> 00:00:25,760
then he also joined

13
00:00:25,760 --> 00:00:27,760
yahoo research

14
00:00:27,760 --> 00:00:29,599
and also apple as i said he has great

15
00:00:29,599 --> 00:00:32,159
experience in the industry and also at

16
00:00:32,159 --> 00:00:34,559
apple they did the first system where

17
00:00:34,559 --> 00:00:37,040
they applied differential privacy

18
00:00:37,040 --> 00:00:38,559
and he was in charge of that and i

19
00:00:38,559 --> 00:00:40,320
believe it's the first system in the

20
00:00:40,320 --> 00:00:42,480
industry that used differential privacy

21
00:00:42,480 --> 00:00:44,640
and now he's at google so we are very

22
00:00:44,640 --> 00:00:47,120
glad to have him here and his work is

23
00:00:47,120 --> 00:00:49,360
the intersection between

24
00:00:49,360 --> 00:00:53,600
machine learning and privacy so please

25
00:00:57,039 --> 00:00:59,120
am i audible

26
00:00:59,120 --> 00:01:01,358
okay

27
00:01:02,559 --> 00:01:05,360
yeah so this talk is going to be about

28
00:01:05,360 --> 00:01:06,240
a

29
00:01:06,240 --> 00:01:08,080
differentially private system which we

30
00:01:08,080 --> 00:01:10,799
deployed recently at google

31
00:01:10,799 --> 00:01:13,439
and what i will do is i will take you

32
00:01:13,439 --> 00:01:15,280
through some of the research challenges

33
00:01:15,280 --> 00:01:16,799
and

34
00:01:16,799 --> 00:01:18,240
the system design challenges which

35
00:01:18,240 --> 00:01:20,320
resulted in the algorithmic design we

36
00:01:20,320 --> 00:01:21,360
had to do

37
00:01:21,360 --> 00:01:24,320
during the course of the work

38
00:01:24,320 --> 00:01:26,720
this is not just my work it's like

39
00:01:26,720 --> 00:01:28,080
at least three or four teams are

40
00:01:28,080 --> 00:01:29,280
involved in it

41
00:01:29,280 --> 00:01:30,960
so i'm basically representing the work

42
00:01:30,960 --> 00:01:32,799
of many people here

43
00:01:32,799 --> 00:01:35,040
okay

44
00:01:35,119 --> 00:01:38,600
my list is

45
00:01:40,640 --> 00:01:42,720
okay i'll touch

46
00:01:42,720 --> 00:01:46,000
okay so uh to start with

47
00:01:46,000 --> 00:01:48,079
uh and to set the stage

48
00:01:48,079 --> 00:01:49,920
what are you trying to do here

49
00:01:49,920 --> 00:01:51,600
we want to learn

50
00:01:51,600 --> 00:01:52,880
models

51
00:01:52,880 --> 00:01:55,680
and uh most cartoon view of the world is

52
00:01:55,680 --> 00:01:58,240
that you have a bunch of data samples

53
00:01:58,240 --> 00:01:59,119
or

54
00:01:59,119 --> 00:02:01,680
you can think like user data each di

55
00:02:01,680 --> 00:02:03,840
is a particular user's data whatever

56
00:02:03,840 --> 00:02:05,360
form it may be and there is a trusted

57
00:02:05,360 --> 00:02:06,799
learning algorithm

58
00:02:06,799 --> 00:02:07,600
which

59
00:02:07,600 --> 00:02:10,239
an analyst or a user or an adversary

60
00:02:10,239 --> 00:02:12,800
wants to learn

61
00:02:12,800 --> 00:02:14,560
and through the talk we'll assume that

62
00:02:14,560 --> 00:02:16,640
the trust boundary is

63
00:02:16,640 --> 00:02:18,640
around the data and the algorithm the

64
00:02:18,640 --> 00:02:22,000
adversity only sees the model

65
00:02:23,599 --> 00:02:24,959
furthermore the adversity can make

66
00:02:24,959 --> 00:02:26,800
adaptive communication with it with the

67
00:02:26,800 --> 00:02:29,280
model it can change the queries and i

68
00:02:29,280 --> 00:02:30,480
will tell what i mean by that like

69
00:02:30,480 --> 00:02:32,080
changing the queries and

70
00:02:32,080 --> 00:02:34,840
it will get response

71
00:02:34,840 --> 00:02:37,440
okay so the very high level question is

72
00:02:37,440 --> 00:02:39,200
what practical learning algorithms can

73
00:02:39,200 --> 00:02:41,519
we afford while preserving what is

74
00:02:41,519 --> 00:02:44,640
called differential privacy

75
00:02:45,040 --> 00:02:47,040
and in this talk i will give you a very

76
00:02:47,040 --> 00:02:48,400
specific instance of a particular

77
00:02:48,400 --> 00:02:49,280
problem

78
00:02:49,280 --> 00:02:50,959
and take you through the algorithmic

79
00:02:50,959 --> 00:02:53,280
designs and those are necessary

80
00:02:53,280 --> 00:02:55,200
let's not say anything oh okay

81
00:02:55,200 --> 00:02:57,440
okay

82
00:03:03,360 --> 00:03:06,640
nice working okay good

83
00:03:06,640 --> 00:03:08,239
so yeah

84
00:03:08,239 --> 00:03:09,599
so now the question is like what are the

85
00:03:09,599 --> 00:03:10,959
practical organic algorithms we can

86
00:03:10,959 --> 00:03:12,080
afford while preserving different

87
00:03:12,080 --> 00:03:13,519
privacy

88
00:03:13,519 --> 00:03:16,159
to make things a bit more common

89
00:03:16,159 --> 00:03:18,239
throughout the talk i'll be focusing on

90
00:03:18,239 --> 00:03:21,200
non-context learning problems

91
00:03:21,200 --> 00:03:23,040
and i would look at what is called

92
00:03:23,040 --> 00:03:24,640
minimizing the population risk or the

93
00:03:24,640 --> 00:03:26,480
test error which basically if you are

94
00:03:26,480 --> 00:03:28,560
given a model uh you want to minimize

95
00:03:28,560 --> 00:03:30,400
the

96
00:03:30,400 --> 00:03:31,519
the loss

97
00:03:31,519 --> 00:03:32,879
on a data sample drawn from some

98
00:03:32,879 --> 00:03:35,120
distribution in expectation

99
00:03:35,120 --> 00:03:37,760
okay and this picture is pretty obvious

100
00:03:37,760 --> 00:03:39,120
one deal with non-conference models so

101
00:03:39,120 --> 00:03:42,400
it can have arbitrary behavior

102
00:03:42,640 --> 00:03:44,239
some of the examples are like continents

103
00:03:44,239 --> 00:03:46,560
resonant recurrent neural nets

104
00:03:46,560 --> 00:03:47,840
other things like alternating least

105
00:03:47,840 --> 00:03:49,280
squares these are just like typical

106
00:03:49,280 --> 00:03:50,480
examples of

107
00:03:50,480 --> 00:03:51,760
the setup of the

108
00:03:51,760 --> 00:03:53,120
problem

109
00:03:53,120 --> 00:03:55,439
okay

110
00:03:55,599 --> 00:03:56,959
this slide

111
00:03:56,959 --> 00:03:59,200
may be something completely boring to a

112
00:03:59,200 --> 00:04:01,439
lot of books but still it's worth kind

113
00:04:01,439 --> 00:04:03,680
of spending some time on the slide

114
00:04:03,680 --> 00:04:05,439
so the notion of privacy i will be

115
00:04:05,439 --> 00:04:06,720
focusing on in this talk is what is

116
00:04:06,720 --> 00:04:08,879
called differential privacy this was uh

117
00:04:08,879 --> 00:04:10,799
introduced by george machinisman smith

118
00:04:10,799 --> 00:04:13,439
in 2006 and a follow-up paper by dork at

119
00:04:13,439 --> 00:04:15,200
all in 2006

120
00:04:15,200 --> 00:04:17,358
the high-level version of the definition

121
00:04:17,358 --> 00:04:19,600
says that an adversary learns almost the

122
00:04:19,600 --> 00:04:21,600
same thing about an individual user

123
00:04:21,600 --> 00:04:23,280
independent of their presence or absence

124
00:04:23,280 --> 00:04:25,440
in the data set

125
00:04:25,440 --> 00:04:26,800
so the important thing to remember in

126
00:04:26,800 --> 00:04:28,880
this particular phrase is that

127
00:04:28,880 --> 00:04:30,320
an individual user we are looking at

128
00:04:30,320 --> 00:04:33,120
what is called user level privacy so

129
00:04:33,120 --> 00:04:34,880
the privacy guarantee you would give is

130
00:04:34,880 --> 00:04:36,479
with respect to the contribution all the

131
00:04:36,479 --> 00:04:38,400
contributions of a single user

132
00:04:38,400 --> 00:04:39,919
this is not just a training example if

133
00:04:39,919 --> 00:04:41,520
the user has contributed 10 training

134
00:04:41,520 --> 00:04:42,800
examples the privacy guarantee will

135
00:04:42,800 --> 00:04:44,800
respect all the train training examples

136
00:04:44,800 --> 00:04:47,280
together

137
00:04:47,440 --> 00:04:48,800
formally for the definition says that

138
00:04:48,800 --> 00:04:50,800
the probability of an algorithm

139
00:04:50,800 --> 00:04:53,199
that is a calligraphy k of d

140
00:04:53,199 --> 00:04:55,120
producing something in a

141
00:04:55,120 --> 00:04:57,360
set is close to the probability of the

142
00:04:57,360 --> 00:05:00,160
algorithm producing

143
00:05:00,160 --> 00:05:02,800
uh in the something in the same set as

144
00:05:02,800 --> 00:05:05,280
on a data set d prime where one user's

145
00:05:05,280 --> 00:05:07,280
data has been removed

146
00:05:07,280 --> 00:05:09,280
and these two probabilities are close by

147
00:05:09,280 --> 00:05:10,720
and the closeness is measured by these

148
00:05:10,720 --> 00:05:13,360
two parameters epsilon and delta for the

149
00:05:13,360 --> 00:05:15,039
rest of the target is not exactly

150
00:05:15,039 --> 00:05:17,120
important to understand the semantics of

151
00:05:17,120 --> 00:05:19,600
epsilon and delta just pretend that

152
00:05:19,600 --> 00:05:21,520
epsilon is a small constant

153
00:05:21,520 --> 00:05:24,240
whatever small means here and delta is a

154
00:05:24,240 --> 00:05:26,479
number which is really really small like

155
00:05:26,479 --> 00:05:28,080
one over super polynomial in the number

156
00:05:28,080 --> 00:05:30,639
of data sets samples

157
00:05:30,639 --> 00:05:31,840
okay

158
00:05:31,840 --> 00:05:33,440
and

159
00:05:33,440 --> 00:05:34,720
d and d prime are as i told you

160
00:05:34,720 --> 00:05:36,639
neighboring data sets where in this case

161
00:05:36,639 --> 00:05:37,680
the red

162
00:05:37,680 --> 00:05:39,440
user has been removed into the d prime

163
00:05:39,440 --> 00:05:41,280
that's all you need to know for the

164
00:05:41,280 --> 00:05:43,600
rest of the talk okay and this is a well

165
00:05:43,600 --> 00:05:45,280
accepted notion in the industry over the

166
00:05:45,280 --> 00:05:47,120
last five seven years there'll be a lot

167
00:05:47,120 --> 00:05:48,080
of push

168
00:05:48,080 --> 00:05:49,759
both in academy and industry to kind of

169
00:05:49,759 --> 00:05:51,680
productionize these things

170
00:05:51,680 --> 00:05:52,560
and

171
00:05:52,560 --> 00:05:53,919
what i'm going to say is

172
00:05:53,919 --> 00:05:57,280
one specific example of that

173
00:05:58,160 --> 00:05:59,600
but if you have any questions please

174
00:05:59,600 --> 00:06:01,580
raise your hand

175
00:06:01,580 --> 00:06:04,479
[Music]

176
00:06:04,479 --> 00:06:07,919
so now the uh more systemic setup is the

177
00:06:07,919 --> 00:06:09,919
federated learning setup so whatever i

178
00:06:09,919 --> 00:06:12,560
will be talking to you it will conform

179
00:06:12,560 --> 00:06:13,919
to the structure of cross device

180
00:06:13,919 --> 00:06:15,280
federated learning

181
00:06:15,280 --> 00:06:17,600
it can be also used in centralized model

182
00:06:17,600 --> 00:06:19,840
training but primarily i will focus on

183
00:06:19,840 --> 00:06:21,199
predicted learning so what i mean by

184
00:06:21,199 --> 00:06:24,160
cross device federated learning

185
00:06:24,160 --> 00:06:27,039
you have a bunch of bunch of devices

186
00:06:27,039 --> 00:06:29,280
each device

187
00:06:29,280 --> 00:06:31,759
basically makes local model updates in

188
00:06:31,759 --> 00:06:33,120
the process of gradient descent i think

189
00:06:33,120 --> 00:06:34,639
most people will understand in decent

190
00:06:34,639 --> 00:06:36,400
days so you'll make some local updates

191
00:06:36,400 --> 00:06:38,240
on your gradients

192
00:06:38,240 --> 00:06:39,280
and

193
00:06:39,280 --> 00:06:41,039
after that is done

194
00:06:41,039 --> 00:06:42,319
the gradient of this assigned to the

195
00:06:42,319 --> 00:06:43,600
server

196
00:06:43,600 --> 00:06:45,919
the server would give you a new model

197
00:06:45,919 --> 00:06:47,440
update and the local updates again

198
00:06:47,440 --> 00:06:48,800
proceed

199
00:06:48,800 --> 00:06:50,560
okay

200
00:06:50,560 --> 00:06:52,080
and finally the server will produce a

201
00:06:52,080 --> 00:06:53,599
model

202
00:06:53,599 --> 00:06:55,120
and this model is deployed onto the

203
00:06:55,120 --> 00:06:57,120
devices

204
00:06:57,120 --> 00:06:59,120
so this is the general workflow of cross

205
00:06:59,120 --> 00:07:02,400
device related learning

206
00:07:02,400 --> 00:07:04,080
again for the purpose of this talk it is

207
00:07:04,080 --> 00:07:05,919
important to remember that the trust

208
00:07:05,919 --> 00:07:08,639
boundary is around the cloud

209
00:07:08,639 --> 00:07:10,240
and the devices during the whole

210
00:07:10,240 --> 00:07:12,080
training process what the advertising is

211
00:07:12,080 --> 00:07:13,759
the only the models final models that

212
00:07:13,759 --> 00:07:15,759
get deployed on the devices you can

213
00:07:15,759 --> 00:07:17,919
operate over stronger trust or weaker

214
00:07:17,919 --> 00:07:20,160
trust models but in this talk i'll just

215
00:07:20,160 --> 00:07:23,520
focus on this particular trust model

216
00:07:26,080 --> 00:07:27,039
okay

217
00:07:27,039 --> 00:07:28,240
so here is the main points line of the

218
00:07:28,240 --> 00:07:29,520
talk

219
00:07:29,520 --> 00:07:31,120
so we deployed the next word prediction

220
00:07:31,120 --> 00:07:33,840
model for spanish language gboard so if

221
00:07:33,840 --> 00:07:35,680
anyone uses android keyboard so on the

222
00:07:35,680 --> 00:07:37,280
gboard for network prediction for

223
00:07:37,280 --> 00:07:39,440
spanish language

224
00:07:39,440 --> 00:07:42,319
this is for the european spanish

225
00:07:42,319 --> 00:07:45,039
with epsilon of 8.9 and delta 10 to the

226
00:07:45,039 --> 00:07:46,400
power of minus 10 these are the privacy

227
00:07:46,400 --> 00:07:47,759
parameters

228
00:07:47,759 --> 00:07:50,240
with user level differential privacy

229
00:07:50,240 --> 00:07:53,840
the last part may not be so obvious

230
00:07:53,840 --> 00:07:55,440
so what happens in differential privacy

231
00:07:55,440 --> 00:07:57,120
in the last few years we have

232
00:07:57,120 --> 00:07:59,199
formulated a variety of versions of the

233
00:07:59,199 --> 00:08:02,240
definition for for reasons good or bad

234
00:08:02,240 --> 00:08:03,680
so this what particular version is

235
00:08:03,680 --> 00:08:05,440
called zero concentrated differential

236
00:08:05,440 --> 00:08:07,680
privacy it gives you a much tighter

237
00:08:07,680 --> 00:08:10,319
control over the privacy random variable

238
00:08:10,319 --> 00:08:13,120
and we say that like this epsilon of 8.9

239
00:08:13,120 --> 00:08:14,560
and delta 10 to the power minus 10

240
00:08:14,560 --> 00:08:17,120
corresponds to gcdb guarantee of less

241
00:08:17,120 --> 00:08:20,240
than 0.1 which is 0.81

242
00:08:20,240 --> 00:08:21,599
the meaning of gcdp is not that

243
00:08:21,599 --> 00:08:23,840
important i think that less than 1 is

244
00:08:23,840 --> 00:08:24,960
good

245
00:08:24,960 --> 00:08:27,520
okay two things to import two important

246
00:08:27,520 --> 00:08:28,800
things to notice here and i will harp on

247
00:08:28,800 --> 00:08:31,199
this like as the talk progresses

248
00:08:31,199 --> 00:08:32,640
to our knowledge this is the first

249
00:08:32,640 --> 00:08:33,839
production

250
00:08:33,839 --> 00:08:35,919
ml model

251
00:08:35,919 --> 00:08:36,640
with

252
00:08:36,640 --> 00:08:38,958
a rigorous and the guarantee which is

253
00:08:38,958 --> 00:08:41,200
publicly stated so it's a

254
00:08:41,200 --> 00:08:42,719
we are stating that we are giving this

255
00:08:42,719 --> 00:08:44,159
parameter and we'll tell you like what

256
00:08:44,159 --> 00:08:45,440
are the things that when necessary to

257
00:08:45,440 --> 00:08:47,680
actually make this formal statement so

258
00:08:47,680 --> 00:08:49,519
in some sense the word formal is quite

259
00:08:49,519 --> 00:08:53,000
overloaded here

260
00:08:56,080 --> 00:08:57,040
okay

261
00:08:57,040 --> 00:08:59,120
so the structure of the talk

262
00:08:59,120 --> 00:09:00,880
so first i will give you some background

263
00:09:00,880 --> 00:09:02,320
on differentially private machine

264
00:09:02,320 --> 00:09:04,080
learning in particular this algorithm

265
00:09:04,080 --> 00:09:05,680
quality and dps stochastic gradient

266
00:09:05,680 --> 00:09:07,120
descent

267
00:09:07,120 --> 00:09:08,959
then i will move on to

268
00:09:08,959 --> 00:09:10,240
this algorithm called differentially

269
00:09:10,240 --> 00:09:11,839
private follow the regularized leader

270
00:09:11,839 --> 00:09:13,519
dpftrl

271
00:09:13,519 --> 00:09:15,600
so this algorithm was the one which

272
00:09:15,600 --> 00:09:17,600
actually helped us deploy the model

273
00:09:17,600 --> 00:09:19,279
which you wanted and then i will show

274
00:09:19,279 --> 00:09:21,519
you some empirical evaluation

275
00:09:21,519 --> 00:09:23,680
or both on like public data sets and

276
00:09:23,680 --> 00:09:26,000
also give you some idea on how on the

277
00:09:26,000 --> 00:09:28,160
production system it worked

278
00:09:28,160 --> 00:09:30,160
and then finally in the

279
00:09:30,160 --> 00:09:31,600
fourth section i will give you some

280
00:09:31,600 --> 00:09:33,279
extensions of this work which you have

281
00:09:33,279 --> 00:09:36,839
been working on recently

282
00:09:40,399 --> 00:09:42,640
okay

283
00:09:49,279 --> 00:09:51,120
okay so

284
00:09:51,120 --> 00:09:52,640
dphd

285
00:09:52,640 --> 00:09:55,760
so this algorithm in its variety of

286
00:09:55,760 --> 00:09:57,279
forms has appeared in like at least

287
00:09:57,279 --> 00:09:59,279
three papers including one of mine from

288
00:09:59,279 --> 00:10:00,959
2014

289
00:10:00,959 --> 00:10:02,880
so the idea is very simple you will

290
00:10:02,880 --> 00:10:04,240
essentially do stochastic gradient

291
00:10:04,240 --> 00:10:06,000
descent as if you do not privately

292
00:10:06,000 --> 00:10:09,200
accepting you will add some noise

293
00:10:09,680 --> 00:10:12,959
so you pick a mini batch of some size

294
00:10:12,959 --> 00:10:15,360
from the data set

295
00:10:15,360 --> 00:10:19,040
which is sampled uniformly at random

296
00:10:19,040 --> 00:10:19,920
then

297
00:10:19,920 --> 00:10:22,000
on that minibatch you compute

298
00:10:22,000 --> 00:10:24,240
the gradients of the current model state

299
00:10:24,240 --> 00:10:26,160
on each of the data points

300
00:10:26,160 --> 00:10:27,519
and then is what you do this operation

301
00:10:27,519 --> 00:10:29,279
called clipping you essentially

302
00:10:29,279 --> 00:10:30,880
scale down the gradients

303
00:10:30,880 --> 00:10:33,760
individual gradient if necessary

304
00:10:33,760 --> 00:10:35,440
when it crosses some threshold so you'll

305
00:10:35,440 --> 00:10:36,880
keep a threshold let's say a clipping

306
00:10:36,880 --> 00:10:38,160
norm and you'll say like if the norm of

307
00:10:38,160 --> 00:10:40,079
the gradient l to normal the gradient

308
00:10:40,079 --> 00:10:41,519
goes over the threshold i'll just slip

309
00:10:41,519 --> 00:10:42,959
it down to that

310
00:10:42,959 --> 00:10:44,839
that's called keeping then i take the

311
00:10:44,839 --> 00:10:47,839
average i get the average gradient

312
00:10:47,839 --> 00:10:50,079
then what i do is instead of

313
00:10:50,079 --> 00:10:52,640
feeding the average gradient to the

314
00:10:52,640 --> 00:10:54,079
model update what i will do is i will

315
00:10:54,079 --> 00:10:55,839
add some gaussian noise

316
00:10:55,839 --> 00:10:56,880
i will tell the parameters of the

317
00:10:56,880 --> 00:10:58,640
question noise in a bit

318
00:10:58,640 --> 00:11:01,760
and then essentially what you do do the

319
00:11:01,760 --> 00:11:03,440
standard

320
00:11:03,440 --> 00:11:05,839
hdd update

321
00:11:05,839 --> 00:11:09,120
and the process continues

322
00:11:09,200 --> 00:11:11,519
two thing to remember is that

323
00:11:11,519 --> 00:11:13,600
the first thing is this parameter l it's

324
00:11:13,600 --> 00:11:15,279
the clipping norm

325
00:11:15,279 --> 00:11:17,279
is the scale at which you clip the

326
00:11:17,279 --> 00:11:19,440
gradients each of the gradients

327
00:11:19,440 --> 00:11:21,200
so that's l

328
00:11:21,200 --> 00:11:23,519
and in the denominator you have case k

329
00:11:23,519 --> 00:11:25,200
which is the mini batch size you have

330
00:11:25,200 --> 00:11:26,800
square term because i am writing the

331
00:11:26,800 --> 00:11:28,800
variances so if you think like your

332
00:11:28,800 --> 00:11:29,920
standard deviation then the standard

333
00:11:29,920 --> 00:11:31,120
deviation of the noise is essentially

334
00:11:31,120 --> 00:11:32,640
scaled as

335
00:11:32,640 --> 00:11:34,880
the contribution a single gradient can

336
00:11:34,880 --> 00:11:38,240
have on the average so l over k is the

337
00:11:38,240 --> 00:11:40,320
contribution of the single gradient to

338
00:11:40,320 --> 00:11:42,959
gt and i'm adding noise at that scale

339
00:11:42,959 --> 00:11:44,320
accepting i have this parameter sigma

340
00:11:44,320 --> 00:11:46,720
squared

341
00:11:47,040 --> 00:11:48,720
which is called the noise multiplier so

342
00:11:48,720 --> 00:11:53,200
it is the scaling of the sensitivity

343
00:11:55,200 --> 00:11:57,839
okay

344
00:11:57,920 --> 00:11:59,279
so two things to remember from this

345
00:11:59,279 --> 00:12:01,200
particular slide is that

346
00:12:01,200 --> 00:12:02,959
you have this parameter one over k so

347
00:12:02,959 --> 00:12:04,399
the amount of noise you add is in the

348
00:12:04,399 --> 00:12:06,079
scale of one over k sigma is the

349
00:12:06,079 --> 00:12:07,440
parameter which only depends on the

350
00:12:07,440 --> 00:12:09,040
privacy parameters like epsilon and

351
00:12:09,040 --> 00:12:12,000
delta but k is the batch size so you are

352
00:12:12,000 --> 00:12:13,360
adding noise essentially in the order of

353
00:12:13,360 --> 00:12:15,040
1 over k here that's the only thing to

354
00:12:15,040 --> 00:12:17,680
remember this

355
00:12:19,040 --> 00:12:20,320
okay

356
00:12:20,320 --> 00:12:22,079
so

357
00:12:22,079 --> 00:12:25,040
why does dpsgd work well there is this

358
00:12:25,040 --> 00:12:26,639
tool called privacy amplification by

359
00:12:26,639 --> 00:12:29,120
sampling and there are variants of it

360
00:12:29,120 --> 00:12:30,639
which has been developed in the last few

361
00:12:30,639 --> 00:12:31,440
years

362
00:12:31,440 --> 00:12:33,519
so it says roughly that for the same

363
00:12:33,519 --> 00:12:36,399
level of privacy one can add noise as if

364
00:12:36,399 --> 00:12:37,680
the gradients are computed on the

365
00:12:37,680 --> 00:12:40,000
complete data set as opposed to a mini

366
00:12:40,000 --> 00:12:42,560
batch what i mean by that

367
00:12:42,560 --> 00:12:44,480
so if i'm computing the gradient on the

368
00:12:44,480 --> 00:12:46,880
mini batch of size k i was adding noise

369
00:12:46,880 --> 00:12:48,720
in the order of 1 over k because one

370
00:12:48,720 --> 00:12:50,959
single gradient can change my response

371
00:12:50,959 --> 00:12:53,440
by 1 over k

372
00:12:53,440 --> 00:12:55,519
but now what i am saying is that you can

373
00:12:55,519 --> 00:12:57,360
pretend that you are operating actually

374
00:12:57,360 --> 00:12:59,440
on the complete data set and it is ok to

375
00:12:59,440 --> 00:13:02,480
add noise in the order of 1 over n

376
00:13:02,480 --> 00:13:04,320
so clearly i am adding much lower noise

377
00:13:04,320 --> 00:13:06,079
if i include privacy amplification by

378
00:13:06,079 --> 00:13:07,440
sampling

379
00:13:07,440 --> 00:13:09,279
and that requires

380
00:13:09,279 --> 00:13:11,440
that you sample the mini batch uniformly

381
00:13:11,440 --> 00:13:15,959
at random from the complete data set

382
00:13:25,200 --> 00:13:26,480
so now

383
00:13:26,480 --> 00:13:27,839
if you

384
00:13:27,839 --> 00:13:30,000
if you compare the performance of your

385
00:13:30,000 --> 00:13:31,360
algorithm

386
00:13:31,360 --> 00:13:33,519
on a version where you are accounting

387
00:13:33,519 --> 00:13:35,760
for privacy amplification by sampling

388
00:13:35,760 --> 00:13:37,760
and whether you are not there is a huge

389
00:13:37,760 --> 00:13:38,959
gap

390
00:13:38,959 --> 00:13:41,440
so this is the example from our paper on

391
00:13:41,440 --> 00:13:42,800
cfr 10

392
00:13:42,800 --> 00:13:44,959
you will see that if i don't account for

393
00:13:44,959 --> 00:13:47,120
the amplification by sampling then the

394
00:13:47,120 --> 00:13:48,240
for the corresponding epsilon the

395
00:13:48,240 --> 00:13:50,160
accuracy is what i am getting are much

396
00:13:50,160 --> 00:13:52,800
much lower

397
00:13:52,800 --> 00:13:54,480
so in some sense privacy amplification

398
00:13:54,480 --> 00:13:57,839
by sampling is important in the training

399
00:13:57,839 --> 00:13:58,639
of

400
00:13:58,639 --> 00:14:00,320
dp models

401
00:14:00,320 --> 00:14:03,440
if you are operating on mini batches

402
00:14:03,440 --> 00:14:05,360
which typically you do because full

403
00:14:05,360 --> 00:14:07,120
value gradient descents are extremely

404
00:14:07,120 --> 00:14:09,040
expensive in practice and sometimes

405
00:14:09,040 --> 00:14:12,040
infeasible

406
00:14:16,480 --> 00:14:17,760
so

407
00:14:17,760 --> 00:14:19,199
as i told you in the first i mean

408
00:14:19,199 --> 00:14:21,360
beginning of the section dpsgt and its

409
00:14:21,360 --> 00:14:24,399
close cousin this is the close cousin

410
00:14:24,399 --> 00:14:26,320
it's called dp federated averaging this

411
00:14:26,320 --> 00:14:28,320
is a federated version of the same

412
00:14:28,320 --> 00:14:29,360
algorithm

413
00:14:29,360 --> 00:14:31,279
the idea is roughly the same

414
00:14:31,279 --> 00:14:32,959
what you do is

415
00:14:32,959 --> 00:14:34,880
instead of sampling mini by samples you

416
00:14:34,880 --> 00:14:36,480
don't have a data set

417
00:14:36,480 --> 00:14:38,399
right so you're in the federated world

418
00:14:38,399 --> 00:14:39,760
you sample

419
00:14:39,760 --> 00:14:42,079
a bunch of devices

420
00:14:42,079 --> 00:14:43,680
then you compute the gradients gradient

421
00:14:43,680 --> 00:14:44,880
take the average

422
00:14:44,880 --> 00:14:46,639
at the same scale of noise

423
00:14:46,639 --> 00:14:48,160
the size of the

424
00:14:48,160 --> 00:14:49,760
number of

425
00:14:49,760 --> 00:14:51,839
devices you are sampling

426
00:14:51,839 --> 00:14:53,600
and you make the state update

427
00:14:53,600 --> 00:14:55,360
good

428
00:14:55,360 --> 00:14:57,440
why am i talking about this

429
00:14:57,440 --> 00:14:59,839
well

430
00:15:00,079 --> 00:15:01,360
uh

431
00:15:01,360 --> 00:15:04,160
so i'll tell about this in a minute

432
00:15:04,160 --> 00:15:06,319
so

433
00:15:06,639 --> 00:15:08,160
so this algorithm will have a similar

434
00:15:08,160 --> 00:15:10,639
behavior as dphd if the devices were

435
00:15:10,639 --> 00:15:13,839
sampled uniformly at random

436
00:15:13,920 --> 00:15:15,120
accepting the fact that when you are

437
00:15:15,120 --> 00:15:16,880
doing federated learning and you are

438
00:15:16,880 --> 00:15:18,000
dealing with real world production

439
00:15:18,000 --> 00:15:19,199
systems

440
00:15:19,199 --> 00:15:20,880
it is almost impossible to actually

441
00:15:20,880 --> 00:15:22,480
sample these things at random because

442
00:15:22,480 --> 00:15:24,639
devices come and go

443
00:15:24,639 --> 00:15:28,079
and you don't have any control over it

444
00:15:29,920 --> 00:15:31,680
but we saw in the previous slide that

445
00:15:31,680 --> 00:15:33,759
sampling was necessary for getting the

446
00:15:33,759 --> 00:15:35,839
privacy utility trade-offs which are

447
00:15:35,839 --> 00:15:38,079
reasonable for the phd and so that would

448
00:15:38,079 --> 00:15:41,759
be also 240p federated averaging

449
00:15:41,759 --> 00:15:43,600
i also wanted to make this small comment

450
00:15:43,600 --> 00:15:45,040
that whatever i am talking about

451
00:15:45,040 --> 00:15:47,120
uniformly sampling of the devices one

452
00:15:47,120 --> 00:15:48,480
can flip this around and say that the

453
00:15:48,480 --> 00:15:49,680
devices can decide whether to

454
00:15:49,680 --> 00:15:52,000
participate or not by flipping a point

455
00:15:52,000 --> 00:15:53,759
on this on their end this is called the

456
00:15:53,759 --> 00:15:55,519
poisson sampling and anything i will

457
00:15:55,519 --> 00:15:56,800
talk about privacy amplification by

458
00:15:56,800 --> 00:15:58,560
sampling all its issues would be

459
00:15:58,560 --> 00:16:00,320
relevant in the same context of personal

460
00:16:00,320 --> 00:16:02,800
sampling

461
00:16:05,920 --> 00:16:06,800
okay

462
00:16:06,800 --> 00:16:09,839
so why did i say it is

463
00:16:10,320 --> 00:16:12,639
hard to near impossible of for doing

464
00:16:12,639 --> 00:16:14,800
this

465
00:16:15,600 --> 00:16:16,800
privacy amplification with something in

466
00:16:16,800 --> 00:16:18,399
the production system

467
00:16:18,399 --> 00:16:20,399
so this is a pattern of the client

468
00:16:20,399 --> 00:16:24,160
availability in production

469
00:16:24,240 --> 00:16:24,959
so

470
00:16:24,959 --> 00:16:26,720
you will see that these neutral patterns

471
00:16:26,720 --> 00:16:28,720
are there so during the day it goes up

472
00:16:28,720 --> 00:16:30,399
during night it goes on or the other way

473
00:16:30,399 --> 00:16:31,759
down whatever so but you will have a

474
00:16:31,759 --> 00:16:33,440
sinusoidal pattern of this form and

475
00:16:33,440 --> 00:16:37,240
these things are not even stable

476
00:16:37,519 --> 00:16:38,880
so how does a client communication

477
00:16:38,880 --> 00:16:40,399
typically work you have a bunch of the

478
00:16:40,399 --> 00:16:43,759
devices first it has to be charging

479
00:16:43,759 --> 00:16:47,279
then it has to be on wi-fi

480
00:16:48,160 --> 00:16:51,040
and it is willing to submit gradients

481
00:16:51,040 --> 00:16:52,480
so these are the four side conditions

482
00:16:52,480 --> 00:16:53,759
that you have to satisfy actually to

483
00:16:53,759 --> 00:16:56,800
give a gradient to the server

484
00:16:59,360 --> 00:17:00,880
so

485
00:17:00,880 --> 00:17:02,399
it is very difficult to know the size of

486
00:17:02,399 --> 00:17:03,759
active population typical population

487
00:17:03,759 --> 00:17:05,199
sizes range from 10 to the power six to

488
00:17:05,199 --> 00:17:06,959
the power of ten typical number of

489
00:17:06,959 --> 00:17:09,439
devices participating can vary in the

490
00:17:09,439 --> 00:17:12,079
order of hundreds so now

491
00:17:12,079 --> 00:17:14,000
it is very hard from this kind of

492
00:17:14,000 --> 00:17:15,679
variable situation to actually sample

493
00:17:15,679 --> 00:17:17,520
uniformly sample devices

494
00:17:17,520 --> 00:17:19,119
because you don't have the control on

495
00:17:19,119 --> 00:17:20,240
the devices which is going to give you

496
00:17:20,240 --> 00:17:21,679
the data

497
00:17:21,679 --> 00:17:22,720
okay

498
00:17:22,720 --> 00:17:23,760
part then want to make the things

499
00:17:23,760 --> 00:17:24,720
challenging

500
00:17:24,720 --> 00:17:26,559
so these are like real systems right so

501
00:17:26,559 --> 00:17:28,960
you will have load balancing methods

502
00:17:28,960 --> 00:17:30,559
which have been developed over the few

503
00:17:30,559 --> 00:17:33,360
years and just for an algorithm to train

504
00:17:33,360 --> 00:17:34,480
with differential privacy you cannot

505
00:17:34,480 --> 00:17:35,600
just throw everything out of the window

506
00:17:35,600 --> 00:17:36,400
and

507
00:17:36,400 --> 00:17:38,480
do something completely different so it

508
00:17:38,480 --> 00:17:40,559
is also incompatible with

509
00:17:40,559 --> 00:17:41,919
uh some of the custom load balancing

510
00:17:41,919 --> 00:17:43,120
techniques which are there called base

511
00:17:43,120 --> 00:17:44,799
steering

512
00:17:44,799 --> 00:17:47,120
so this makes amplification fairly hard

513
00:17:47,120 --> 00:17:49,918
to use in practice

514
00:17:50,080 --> 00:17:52,480
we did develop a feasible protocol in

515
00:17:52,480 --> 00:17:54,559
one of our papers in 2020 but it

516
00:17:54,559 --> 00:17:56,559
required fairly complex changes to the

517
00:17:56,559 --> 00:17:58,840
production infrastructure

518
00:17:58,840 --> 00:18:03,120
and these challenges these changes are

519
00:18:03,120 --> 00:18:05,200
engineering was pretty expensive and the

520
00:18:05,200 --> 00:18:07,360
second thing is that

521
00:18:07,360 --> 00:18:09,039
we also saw that

522
00:18:09,039 --> 00:18:10,480
i mean in simulation we also saw that

523
00:18:10,480 --> 00:18:12,960
the training time would increase like

524
00:18:12,960 --> 00:18:15,200
to prohibitive levels

525
00:18:15,200 --> 00:18:17,200
if we had to rely on amplification

526
00:18:17,200 --> 00:18:18,880
because it would require the

527
00:18:18,880 --> 00:18:23,160
availability of large number of devices

528
00:18:33,679 --> 00:18:35,600
so the question i mean at this point i

529
00:18:35,600 --> 00:18:37,600
am still harping on this point that

530
00:18:37,600 --> 00:18:38,960
why privacy amplification is somewhat

531
00:18:38,960 --> 00:18:41,760
necessary so why not using dp federated

532
00:18:41,760 --> 00:18:43,760
averaging without privacy amplification

533
00:18:43,760 --> 00:18:44,720
if i don't try to do privacy

534
00:18:44,720 --> 00:18:46,640
amplification then what happens

535
00:18:46,640 --> 00:18:48,640
you will see a same plot what we saw for

536
00:18:48,640 --> 00:18:51,280
c part 10 this is on stack overflow why

537
00:18:51,280 --> 00:18:52,799
am i talking about stack overflow this

538
00:18:52,799 --> 00:18:54,400
is the kind of a task that will be

539
00:18:54,400 --> 00:18:56,400
actually the real production one

540
00:18:56,400 --> 00:18:58,000
so i'll be talking about stack overflow

541
00:18:58,000 --> 00:19:01,679
quite more in the rest of the talk

542
00:19:01,679 --> 00:19:04,480
so we see still the gap

543
00:19:04,480 --> 00:19:06,000
and again the question is like can we

544
00:19:06,000 --> 00:19:07,039
avoid

545
00:19:07,039 --> 00:19:08,480
sampling and achieve similar price

546
00:19:08,480 --> 00:19:11,840
privacy utility trade-offs

547
00:19:14,480 --> 00:19:16,400
so this was the background on the phd

548
00:19:16,400 --> 00:19:18,640
and db federated averaging

549
00:19:18,640 --> 00:19:20,480
now let's go to the

550
00:19:20,480 --> 00:19:22,480
real algorithm which we want to

551
00:19:22,480 --> 00:19:24,320
implement this algorithm is called

552
00:19:24,320 --> 00:19:25,520
differentially private follow the

553
00:19:25,520 --> 00:19:28,320
regularized leader

554
00:19:28,559 --> 00:19:30,799
tpftrl

555
00:19:30,799 --> 00:19:31,919
so

556
00:19:31,919 --> 00:19:34,799
so to do that let me go back to the phd

557
00:19:34,799 --> 00:19:36,400
or hdd rather

558
00:19:36,400 --> 00:19:38,720
and open up the recursion of hdd

559
00:19:38,720 --> 00:19:39,919
and let's say we are not doing any

560
00:19:39,919 --> 00:19:41,679
projection so this is like the vanilla

561
00:19:41,679 --> 00:19:43,919
gcd if you open up the recursion of http

562
00:19:43,919 --> 00:19:45,600
you will see that

563
00:19:45,600 --> 00:19:48,160
your update at time t plus 1 really

564
00:19:48,160 --> 00:19:49,679
depends on the sum of the gradients what

565
00:19:49,679 --> 00:19:52,240
you have seen so far

566
00:19:52,240 --> 00:19:53,120
right

567
00:19:53,120 --> 00:19:54,080
and

568
00:19:54,080 --> 00:19:56,480
i mean this is not like

569
00:19:56,480 --> 00:19:58,080
any deep insight but some of this

570
00:19:58,080 --> 00:20:00,240
particular observation actually will

571
00:20:00,240 --> 00:20:01,679
help us design the algorithm that we

572
00:20:01,679 --> 00:20:02,960
want

573
00:20:02,960 --> 00:20:04,320
so

574
00:20:04,320 --> 00:20:06,720
so remember that uh from the slide the

575
00:20:06,720 --> 00:20:08,720
update at time t plus 1 is essentially a

576
00:20:08,720 --> 00:20:10,000
scaled version of the gradients what you

577
00:20:10,000 --> 00:20:12,640
have seen so far

578
00:20:14,320 --> 00:20:15,600
so

579
00:20:15,600 --> 00:20:17,280
now let's try to

580
00:20:17,280 --> 00:20:19,440
build the algorithm slowly

581
00:20:19,440 --> 00:20:23,039
so you have these gradients g 0 to g n

582
00:20:23,039 --> 00:20:24,720
and this gradients are data dependent

583
00:20:24,720 --> 00:20:26,240
this is important because your next

584
00:20:26,240 --> 00:20:27,280
gradient what you're going to query

585
00:20:27,280 --> 00:20:29,120
depends on the model what is the

586
00:20:29,120 --> 00:20:31,760
previous statement okay and what dpht

587
00:20:31,760 --> 00:20:33,600
roughly does is it adds independent

588
00:20:33,600 --> 00:20:35,440
noise to each update that's essentially

589
00:20:35,440 --> 00:20:36,799
what is happening

590
00:20:36,799 --> 00:20:38,960
right

591
00:20:41,200 --> 00:20:43,840
okay so how does it work

592
00:20:43,840 --> 00:20:45,760
in the phd this is a pretty much a

593
00:20:45,760 --> 00:20:48,080
cartoon you have g0 you add some noise

594
00:20:48,080 --> 00:20:51,280
you get and you add it up to g one

595
00:20:51,280 --> 00:20:53,200
plus some noise you add it up to g two

596
00:20:53,200 --> 00:20:55,280
plus some minus and g three plus some so

597
00:20:55,280 --> 00:20:56,640
essentially you are getting this is how

598
00:20:56,640 --> 00:20:58,559
the state update process works in phd if

599
00:20:58,559 --> 00:21:01,879
you look at victoria

600
00:21:03,440 --> 00:21:04,559
accepting

601
00:21:04,559 --> 00:21:07,360
now if i look at the total noise added

602
00:21:07,360 --> 00:21:08,720
to the sum of the gradients like let's

603
00:21:08,720 --> 00:21:09,679
say i'm looking at the total noise that

604
00:21:09,679 --> 00:21:11,440
is the last gradient

605
00:21:11,440 --> 00:21:13,520
in the last model then dphd would add

606
00:21:13,520 --> 00:21:15,200
noise in the order of square root of n

607
00:21:15,200 --> 00:21:16,720
over epsilon

608
00:21:16,720 --> 00:21:20,159
if you don't have amplification

609
00:21:20,320 --> 00:21:21,440
where n is the number of gradient

610
00:21:21,440 --> 00:21:22,880
updates i'm doing

611
00:21:22,880 --> 00:21:25,600
if i use amplification i will get a get

612
00:21:25,600 --> 00:21:27,679
to the error in the order of 1 over

613
00:21:27,679 --> 00:21:29,679
epsilon so i get a boost of square root

614
00:21:29,679 --> 00:21:31,280
of n

615
00:21:31,280 --> 00:21:33,120
and in the previous pictures i was like

616
00:21:33,120 --> 00:21:34,880
handwave is saying that like deep

617
00:21:34,880 --> 00:21:36,240
amplification helps you get a better

618
00:21:36,240 --> 00:21:37,840
privacy it will trade off this is the

619
00:21:37,840 --> 00:21:39,200
quantification version of it like you

620
00:21:39,200 --> 00:21:40,880
get a square root of an improvement if

621
00:21:40,880 --> 00:21:44,760
they make end state updates

622
00:21:48,159 --> 00:21:49,120
now

623
00:21:49,120 --> 00:21:50,400
the

624
00:21:50,400 --> 00:21:52,400
the trick we would use is what is called

625
00:21:52,400 --> 00:21:55,039
tree aggregation for prefix sum

626
00:21:55,039 --> 00:21:58,000
this idea has been there from 2010 by a

627
00:21:58,000 --> 00:22:00,400
couple of papers by dwork

628
00:22:00,400 --> 00:22:04,799
now petasi dot plum from 2010 and

629
00:22:05,200 --> 00:22:06,159
chen

630
00:22:06,159 --> 00:22:07,280
song

631
00:22:07,280 --> 00:22:09,600
and she from 2010

632
00:22:09,600 --> 00:22:11,440
so these two papers simultaneously came

633
00:22:11,440 --> 00:22:13,200
up with the idea and that is fairly

634
00:22:13,200 --> 00:22:16,080
elegant what you do is

635
00:22:16,080 --> 00:22:18,159
instead of blindly adding noise

636
00:22:18,159 --> 00:22:21,360
to each of the each of the new state of

637
00:22:21,360 --> 00:22:23,520
new gradients what you do is ultimately

638
00:22:23,520 --> 00:22:25,440
you have to create the prefix sum right

639
00:22:25,440 --> 00:22:26,559
that's what you want to do you want to

640
00:22:26,559 --> 00:22:28,400
find out the at every time step you want

641
00:22:28,400 --> 00:22:30,400
to find out all the some of the

642
00:22:30,400 --> 00:22:32,960
gradients you have seen so far so this

643
00:22:32,960 --> 00:22:34,480
is what i am calling as prefix term

644
00:22:34,480 --> 00:22:36,960
someone calls it cumulative sum also

645
00:22:36,960 --> 00:22:38,080
so that is the

646
00:22:38,080 --> 00:22:40,000
idea is very fairly simple essentially

647
00:22:40,000 --> 00:22:42,640
you take g0 and g1

648
00:22:42,640 --> 00:22:44,720
and you start building a tree on top of

649
00:22:44,720 --> 00:22:45,760
it

650
00:22:45,760 --> 00:22:47,360
so what you do

651
00:22:47,360 --> 00:22:48,559
so when you get

652
00:22:48,559 --> 00:22:50,159
g0

653
00:22:50,159 --> 00:22:53,039
you add noise to g0 so in the root in

654
00:22:53,039 --> 00:22:55,120
the leaf level of the tree

655
00:22:55,120 --> 00:22:58,000
you will add independent noise

656
00:22:58,000 --> 00:23:00,640
then when you get g1 you will also

657
00:23:00,640 --> 00:23:03,280
compute g0 plus g1 and add some noise to

658
00:23:03,280 --> 00:23:05,440
it

659
00:23:05,679 --> 00:23:10,400
when you get uh g3 and g4 you will add

660
00:23:10,400 --> 00:23:11,760
some noise to

661
00:23:11,760 --> 00:23:14,080
g3 and in the second level for adding up

662
00:23:14,080 --> 00:23:15,440
the red and the green one you will add

663
00:23:15,440 --> 00:23:17,760
some noise and at the top level you will

664
00:23:17,760 --> 00:23:19,039
sum up all of these four gradients and

665
00:23:19,039 --> 00:23:21,200
you'll have some knives why did i do

666
00:23:21,200 --> 00:23:25,200
this notice that if i change any one of

667
00:23:25,200 --> 00:23:27,679
the leaf nodes any of the gradients

668
00:23:27,679 --> 00:23:29,840
only the path from the leaf to the root

669
00:23:29,840 --> 00:23:32,639
only gets changed

670
00:23:34,400 --> 00:23:37,039
whereas when i was doing uh

671
00:23:37,039 --> 00:23:39,039
the dpg style analysis you notice that

672
00:23:39,039 --> 00:23:41,039
if i change the first gradient all the

673
00:23:41,039 --> 00:23:43,039
cumulative sum changes

674
00:23:43,039 --> 00:23:45,200
together here only the path from the

675
00:23:45,200 --> 00:23:47,279
leaf to the root changes and

676
00:23:47,279 --> 00:23:49,200
in differential privacy essentially all

677
00:23:49,200 --> 00:23:51,760
we need to do is to bound

678
00:23:51,760 --> 00:23:52,559
the

679
00:23:52,559 --> 00:23:54,720
uh the the what you call the sensitivity

680
00:23:54,720 --> 00:23:56,559
which essentially boils down to how much

681
00:23:56,559 --> 00:23:58,400
by changing one user's data or one

682
00:23:58,400 --> 00:24:01,279
gradient affects the computation and in

683
00:24:01,279 --> 00:24:03,120
this case it only affects the log depth

684
00:24:03,120 --> 00:24:05,360
of the height of the tree

685
00:24:05,360 --> 00:24:06,480
right

686
00:24:06,480 --> 00:24:08,320
good

687
00:24:08,320 --> 00:24:09,520
so

688
00:24:09,520 --> 00:24:12,080
now what i can do is i can add noise

689
00:24:12,080 --> 00:24:14,159
according to this tree

690
00:24:14,159 --> 00:24:15,039
and

691
00:24:15,039 --> 00:24:16,159
my noise

692
00:24:16,159 --> 00:24:19,520
would scale as log n over epsilon

693
00:24:19,520 --> 00:24:20,559
because

694
00:24:20,559 --> 00:24:22,480
now any leaf node which is the

695
00:24:22,480 --> 00:24:24,000
individual grid it only affects the path

696
00:24:24,000 --> 00:24:27,039
from the leaf to the loop

697
00:24:28,400 --> 00:24:31,200
it is close to dp hdd but not

698
00:24:31,200 --> 00:24:33,760
uh but the notice that

699
00:24:33,760 --> 00:24:35,760
uh we are off by a factor of log n with

700
00:24:35,760 --> 00:24:38,159
respect to this dphd bound but remember

701
00:24:38,159 --> 00:24:40,000
dphd required amplification whereas this

702
00:24:40,000 --> 00:24:41,600
algorithm will not require any form of

703
00:24:41,600 --> 00:24:43,840
amplification it would act work on

704
00:24:43,840 --> 00:24:46,080
streaming data essentially so you don't

705
00:24:46,080 --> 00:24:49,918
need any kind of randomness to amplify

706
00:24:50,720 --> 00:24:52,080
you can take

707
00:24:52,080 --> 00:24:54,000
variance of this tree for weighted

708
00:24:54,000 --> 00:24:56,159
averages to get some better

709
00:24:56,159 --> 00:24:58,880
better estimates of the prefix sum but

710
00:24:58,880 --> 00:25:00,720
that i am leaving it for the

711
00:25:00,720 --> 00:25:02,880
top

712
00:25:02,880 --> 00:25:04,799
if you are noticing this picture for the

713
00:25:04,799 --> 00:25:08,720
first time it is also worth noting that

714
00:25:08,720 --> 00:25:11,039
you can write in the form of

715
00:25:11,039 --> 00:25:13,039
what are called hard wavelets i mean is

716
00:25:13,039 --> 00:25:15,039
a fancy name given to this tree but

717
00:25:15,039 --> 00:25:16,400
essentially

718
00:25:16,400 --> 00:25:20,000
i am taking vaselia wavelet transform

719
00:25:25,600 --> 00:25:27,440
so

720
00:25:27,440 --> 00:25:30,640
dpftl is what is the crux of the talk

721
00:25:30,640 --> 00:25:32,960
but ftrl is not fdl has been done from

722
00:25:32,960 --> 00:25:35,039
2010 it is a fairly well studied

723
00:25:35,039 --> 00:25:37,360
algorithm in the

724
00:25:37,360 --> 00:25:38,960
in the learning community

725
00:25:38,960 --> 00:25:40,960
from 2010 i mean there are at least

726
00:25:40,960 --> 00:25:42,720
three papers which can be attributed to

727
00:25:42,720 --> 00:25:46,320
this idea mcmahon in 2010 duchy singer

728
00:25:46,320 --> 00:25:47,840
i'm forging the

729
00:25:47,840 --> 00:25:49,120
h part

730
00:25:49,120 --> 00:25:50,799
and

731
00:25:50,799 --> 00:25:53,279
lingaro from 2010. so the idea is i mean

732
00:25:53,279 --> 00:25:55,679
again looking at hdd update

733
00:25:55,679 --> 00:25:56,960
you have theta t plus 1 and theta t

734
00:25:56,960 --> 00:25:59,279
minus 1 over lambda gt

735
00:25:59,279 --> 00:26:02,320
and ftrl essentially does

736
00:26:02,320 --> 00:26:04,879
the following

737
00:26:08,000 --> 00:26:09,360
which is

738
00:26:09,360 --> 00:26:11,440
it minimizes

739
00:26:11,440 --> 00:26:12,240
the

740
00:26:12,240 --> 00:26:14,080
inner product of the cumulative sum with

741
00:26:14,080 --> 00:26:16,000
the model

742
00:26:16,000 --> 00:26:19,760
and with the l2 regularizer

743
00:26:19,760 --> 00:26:22,720
i will wait for 30 seconds

744
00:26:22,720 --> 00:26:24,400
but is worth noting that these two

745
00:26:24,400 --> 00:26:27,760
updates are exactly the same

746
00:26:28,000 --> 00:26:29,760
if you write the minimizer

747
00:26:29,760 --> 00:26:31,600
and just take the gradient the update

748
00:26:31,600 --> 00:26:34,480
should be identical

749
00:26:36,640 --> 00:26:40,400
so the idea of representing gradients

750
00:26:40,400 --> 00:26:42,480
in the form of cumulative i mean the

751
00:26:42,480 --> 00:26:44,080
state updates in the form of cumulative

752
00:26:44,080 --> 00:26:46,559
sum or prefix sums is not new to our

753
00:26:46,559 --> 00:26:48,080
work it has been there from 2010 these

754
00:26:48,080 --> 00:26:52,199
are also called dual averaging methods

755
00:27:00,720 --> 00:27:01,760
so the observation is that it is

756
00:27:01,760 --> 00:27:03,120
equivalent to hdd under appropriate

757
00:27:03,120 --> 00:27:04,720
choice of the initial model so i mean

758
00:27:04,720 --> 00:27:06,400
the way i wrote it theta zero has to be

759
00:27:06,400 --> 00:27:08,080
zero basically to make sure that they

760
00:27:08,080 --> 00:27:10,720
are equivalent

761
00:27:12,159 --> 00:27:14,559
so the crucial aspect

762
00:27:14,559 --> 00:27:15,760
is

763
00:27:15,760 --> 00:27:19,039
to get a private version of all the

764
00:27:19,039 --> 00:27:22,559
prefix sums during the training process

765
00:27:22,559 --> 00:27:24,640
and what it says me is that it does it

766
00:27:24,640 --> 00:27:28,159
does not require amplification

767
00:27:32,240 --> 00:27:33,279
okay

768
00:27:33,279 --> 00:27:35,360
so now the idea is pretty simple uh what

769
00:27:35,360 --> 00:27:36,480
i will do is

770
00:27:36,480 --> 00:27:38,559
instead of

771
00:27:38,559 --> 00:27:40,159
feeding in the true

772
00:27:40,159 --> 00:27:42,720
prefix sum of the gradients i will feed

773
00:27:42,720 --> 00:27:44,960
it through this binary tree what we just

774
00:27:44,960 --> 00:27:46,159
built

775
00:27:46,159 --> 00:27:48,640
so again the binary what you do you have

776
00:27:48,640 --> 00:27:50,799
the as the gradients come in

777
00:27:50,799 --> 00:27:52,799
you continue building the tree and each

778
00:27:52,799 --> 00:27:54,320
node in the tree

779
00:27:54,320 --> 00:27:56,640
stores the sum of

780
00:27:56,640 --> 00:27:59,919
all the gradients in its uh lymph nodes

781
00:27:59,919 --> 00:28:02,000
of the subtree and you will add noise

782
00:28:02,000 --> 00:28:04,080
independently to each of them

783
00:28:04,080 --> 00:28:05,840
each of the nodes in the tree

784
00:28:05,840 --> 00:28:08,960
and the point is that any single node in

785
00:28:08,960 --> 00:28:11,520
the leaf can affect only the path from

786
00:28:11,520 --> 00:28:15,080
the leaf to the root

787
00:28:18,880 --> 00:28:19,919
okay

788
00:28:19,919 --> 00:28:22,720
again going back to the flow diagram

789
00:28:22,720 --> 00:28:24,240
here the important part is that there is

790
00:28:24,240 --> 00:28:25,600
no sampling requirement so you will just

791
00:28:25,600 --> 00:28:27,120
take the next batch of devices whichever

792
00:28:27,120 --> 00:28:28,480
comes to you

793
00:28:28,480 --> 00:28:30,240
compute the for example gradient clip

794
00:28:30,240 --> 00:28:31,919
the gradients take the average

795
00:28:31,919 --> 00:28:34,399
do the dp tree aggregation

796
00:28:34,399 --> 00:28:36,960
and you feed in this prefix sum to the

797
00:28:36,960 --> 00:28:39,520
model except in the model update is not

798
00:28:39,520 --> 00:28:40,320
now

799
00:28:40,320 --> 00:28:42,960
on theta t to the t plus one you will

800
00:28:42,960 --> 00:28:44,320
basically feed in the cumulative

801
00:28:44,320 --> 00:28:46,159
gradients and you will open up the deep

802
00:28:46,159 --> 00:28:47,919
the hdd recursion

803
00:28:47,919 --> 00:28:50,960
and you will update that way

804
00:28:50,960 --> 00:28:52,799
in implementation we don't do that in

805
00:28:52,799 --> 00:28:54,480
implementation you essentially will take

806
00:28:54,480 --> 00:28:55,760
the difference of the noises to make

807
00:28:55,760 --> 00:28:56,799
sure that

808
00:28:56,799 --> 00:28:58,720
the things are equivalent

809
00:28:58,720 --> 00:29:00,159
basically what i'm saying is that you

810
00:29:00,159 --> 00:29:02,480
can feed in this update equation

811
00:29:02,480 --> 00:29:04,559
into an hdd optimizer in a particular

812
00:29:04,559 --> 00:29:05,919
way and they will be equivalent so you

813
00:29:05,919 --> 00:29:08,399
don't need to write any optimizer per se

814
00:29:08,399 --> 00:29:10,080
all you need to do is to take care of

815
00:29:10,080 --> 00:29:13,158
the noise

816
00:29:14,799 --> 00:29:15,840
okay

817
00:29:15,840 --> 00:29:17,360
and yeah so this version of the

818
00:29:17,360 --> 00:29:18,960
algorithm is from our recent paper in

819
00:29:18,960 --> 00:29:20,399
2021

820
00:29:20,399 --> 00:29:22,480
which essentially the deployment got

821
00:29:22,480 --> 00:29:25,480
built

822
00:29:26,080 --> 00:29:27,510
here is some

823
00:29:27,510 --> 00:29:30,240
[Music]

824
00:29:30,240 --> 00:29:32,880
equations to convince yourself that this

825
00:29:32,880 --> 00:29:34,320
is not just a practical algorithm it has

826
00:29:34,320 --> 00:29:35,520
some theoretical nice theoretical

827
00:29:35,520 --> 00:29:38,000
properties

828
00:29:38,000 --> 00:29:39,600
i will not go into the details of what

829
00:29:39,600 --> 00:29:43,440
is meant by online learning but it is a

830
00:29:43,440 --> 00:29:45,679
yeah

831
00:29:45,840 --> 00:29:47,679
it is a framework where you basically

832
00:29:47,679 --> 00:29:48,799
study

833
00:29:48,799 --> 00:29:51,520
as data points arrive

834
00:29:51,520 --> 00:29:53,440
you give

835
00:29:53,440 --> 00:29:55,600
you basically make some prediction and

836
00:29:55,600 --> 00:29:57,200
you pay

837
00:29:57,200 --> 00:29:59,360
the price in the hindsight as if if the

838
00:29:59,360 --> 00:30:01,039
data were known to you in advance this

839
00:30:01,039 --> 00:30:03,120
is the classic idea of online learning

840
00:30:03,120 --> 00:30:04,799
and what this particular algorithm shows

841
00:30:04,799 --> 00:30:06,960
is that

842
00:30:06,960 --> 00:30:08,399
this algorithm has the best regret

843
00:30:08,399 --> 00:30:10,320
guarantees what you know of

844
00:30:10,320 --> 00:30:12,000
today for any dp online learning

845
00:30:12,000 --> 00:30:13,440
algorithm

846
00:30:13,440 --> 00:30:15,919
in particular for linear losses or least

847
00:30:15,919 --> 00:30:17,279
square losses like online linear

848
00:30:17,279 --> 00:30:19,120
regression

849
00:30:19,120 --> 00:30:21,039
you will have

850
00:30:21,039 --> 00:30:22,880
both the adversarial regret or the

851
00:30:22,880 --> 00:30:24,320
stochastic regret i haven't defined

852
00:30:24,320 --> 00:30:26,399
these terms adversarial regret is

853
00:30:26,399 --> 00:30:27,919
essentially where

854
00:30:27,919 --> 00:30:29,840
the adversary chooses the data point

855
00:30:29,840 --> 00:30:31,919
after c is the after it sees the output

856
00:30:31,919 --> 00:30:33,440
stochastic regret is the setting where

857
00:30:33,440 --> 00:30:35,039
the data points are drawn id from some

858
00:30:35,039 --> 00:30:37,840
distribution and we get for linear and

859
00:30:37,840 --> 00:30:39,600
least square losses

860
00:30:39,600 --> 00:30:40,960
both expected and high probability

861
00:30:40,960 --> 00:30:42,880
regret to be the optimal one which is of

862
00:30:42,880 --> 00:30:44,880
the form 1 over square root of n and the

863
00:30:44,880 --> 00:30:47,120
number of data points you are seeing

864
00:30:47,120 --> 00:30:48,799
plus square root of p p is the

865
00:30:48,799 --> 00:30:50,399
dimensionality of the model

866
00:30:50,399 --> 00:30:53,039
over epsilon n the important part is

867
00:30:53,039 --> 00:30:54,799
the dp term the term that depends on the

868
00:30:54,799 --> 00:30:56,720
differential privacy on epsilon that is

869
00:30:56,720 --> 00:30:58,880
the lower order term in n

870
00:30:58,880 --> 00:31:01,679
so this is in that sense the optimal one

871
00:31:01,679 --> 00:31:04,240
for general constant convex losses but

872
00:31:04,240 --> 00:31:06,799
we can show that it is i mean you get p

873
00:31:06,799 --> 00:31:08,159
to the power of one fourth over square

874
00:31:08,159 --> 00:31:10,640
root of epsilon n

875
00:31:10,640 --> 00:31:12,960
as far as we know

876
00:31:12,960 --> 00:31:16,640
this i mean this boundary is not tight

877
00:31:16,640 --> 00:31:18,320
or at least for population risk we don't

878
00:31:18,320 --> 00:31:19,600
know for regret guarantees whether this

879
00:31:19,600 --> 00:31:22,158
is right or not

880
00:31:23,360 --> 00:31:24,799
and

881
00:31:24,799 --> 00:31:26,320
that's what and these are the

882
00:31:26,320 --> 00:31:28,159
conclusions so these are best ignored

883
00:31:28,159 --> 00:31:30,880
guarantees uh it improves over a paper

884
00:31:30,880 --> 00:31:33,440
of mine in 2013 to get better regular

885
00:31:33,440 --> 00:31:35,360
guarantees

886
00:31:35,360 --> 00:31:36,720
why am i talking about regret guarantees

887
00:31:36,720 --> 00:31:39,039
regret guarantee will immediately imply

888
00:31:39,039 --> 00:31:40,799
population risk or

889
00:31:40,799 --> 00:31:42,799
test letter guarantee

890
00:31:42,799 --> 00:31:44,000
and

891
00:31:44,000 --> 00:31:45,440
the last statement says that this is the

892
00:31:45,440 --> 00:31:46,960
best access population risk guarantee

893
00:31:46,960 --> 00:31:48,640
for any single pass algorithm will take

894
00:31:48,640 --> 00:31:50,159
a single pass algorithm

895
00:31:50,159 --> 00:31:51,919
which does not rely on convexity for

896
00:31:51,919 --> 00:31:53,279
privacy so there are single pass

897
00:31:53,279 --> 00:31:54,799
algorithms which should give you better

898
00:31:54,799 --> 00:31:56,720
population risk guarantees

899
00:31:56,720 --> 00:31:57,679
than

900
00:31:57,679 --> 00:31:59,760
the algorithm what i am talking about

901
00:31:59,760 --> 00:32:01,120
but

902
00:32:01,120 --> 00:32:03,440
they may require

903
00:32:03,440 --> 00:32:06,000
convexity for privacy guarantees

904
00:32:06,000 --> 00:32:07,760
so the privacy guarantee the proof of

905
00:32:07,760 --> 00:32:09,279
privacy would require convexity in the

906
00:32:09,279 --> 00:32:11,039
model and why that is not good for us

907
00:32:11,039 --> 00:32:12,399
because the first slide in the second

908
00:32:12,399 --> 00:32:14,159
slide i told you that i'll be operating

909
00:32:14,159 --> 00:32:15,840
on non-convex model so they become

910
00:32:15,840 --> 00:32:19,120
inapplicable in our city

911
00:32:19,679 --> 00:32:21,679
it's a good open question to think that

912
00:32:21,679 --> 00:32:23,679
whether we can extend the style of

913
00:32:23,679 --> 00:32:26,320
algorithms to get

914
00:32:26,320 --> 00:32:28,720
optimal excess population risk and that

915
00:32:28,720 --> 00:32:32,720
would not depend on the city of literacy

916
00:32:33,440 --> 00:32:35,039
all right so that's the theory part of

917
00:32:35,039 --> 00:32:35,760
the

918
00:32:35,760 --> 00:32:37,760
algorithm

919
00:32:37,760 --> 00:32:40,640
now let me give you some ideas on

920
00:32:40,640 --> 00:32:43,440
how this algorithm appears in practice

921
00:32:43,440 --> 00:32:46,720
going to empirical evaluation

922
00:32:52,880 --> 00:32:54,799
so we start with a

923
00:32:54,799 --> 00:32:56,720
public data set

924
00:32:56,720 --> 00:32:59,200
stack overflow

925
00:32:59,200 --> 00:33:01,519
where we show that

926
00:33:01,519 --> 00:33:04,880
dpftrl outperforms dps

927
00:33:04,880 --> 00:33:05,919
federated averaging without

928
00:33:05,919 --> 00:33:08,320
amplification and this is in the

929
00:33:08,320 --> 00:33:10,000
you can think like the setup is

930
00:33:10,000 --> 00:33:11,840
essentially a simulation of federated

931
00:33:11,840 --> 00:33:13,919
learning

932
00:33:13,919 --> 00:33:15,919
it is not important how the whole design

933
00:33:15,919 --> 00:33:17,440
is there you can read in the paper but

934
00:33:17,440 --> 00:33:18,799
the point is that

935
00:33:18,799 --> 00:33:20,480
if you just take the pad rated averaging

936
00:33:20,480 --> 00:33:21,840
with that amplification it is no way

937
00:33:21,840 --> 00:33:23,120
close

938
00:33:23,120 --> 00:33:24,720
2 dpftrm

939
00:33:24,720 --> 00:33:27,760
in terms of accuracy

940
00:33:29,039 --> 00:33:31,360
dpftrl is completed to dp federated

941
00:33:31,360 --> 00:33:33,600
averaging

942
00:33:33,600 --> 00:33:35,039
when epsilon is less than or equal to 3.

943
00:33:35,039 --> 00:33:36,799
so you will see that like when for small

944
00:33:36,799 --> 00:33:38,720
epsilon and when s dp federated

945
00:33:38,720 --> 00:33:39,919
everything that's the point when i'm and

946
00:33:39,919 --> 00:33:41,279
since this is the simulation experiment

947
00:33:41,279 --> 00:33:42,880
i can pretend that there is

948
00:33:42,880 --> 00:33:44,320
amplification there because i control

949
00:33:44,320 --> 00:33:46,159
everything in the simulation right

950
00:33:46,159 --> 00:33:47,679
so if i do that i will see that at

951
00:33:47,679 --> 00:33:50,559
epsilon less than equal to 3 dp

952
00:33:50,559 --> 00:33:52,799
federated averaging may perform better

953
00:33:52,799 --> 00:33:54,320
but it is still competitive ftl is

954
00:33:54,320 --> 00:33:56,080
complete competitive but as i go

955
00:33:56,080 --> 00:33:57,679
increase the epsilon

956
00:33:57,679 --> 00:34:02,679
ftl scores over related averaging

957
00:34:06,399 --> 00:34:08,159
more generally dpp federated averaging

958
00:34:08,159 --> 00:34:10,000
load amplification will increase epsilon

959
00:34:10,000 --> 00:34:11,440
so i mean i haven't told how

960
00:34:11,440 --> 00:34:12,879
amplification works but essentially if

961
00:34:12,879 --> 00:34:14,399
you have smaller epsilon in the final

962
00:34:14,399 --> 00:34:16,000
guarantee this is actually better for an

963
00:34:16,000 --> 00:34:17,599
amplification so dpf federated

964
00:34:17,599 --> 00:34:21,240
everything does better

965
00:34:21,918 --> 00:34:24,000
then

966
00:34:24,000 --> 00:34:26,079
dpf tail as i told you does not require

967
00:34:26,079 --> 00:34:27,599
amplification

968
00:34:27,599 --> 00:34:29,679
it use it's just the application of what

969
00:34:29,679 --> 00:34:31,280
is called the gaussian mechanism one of

970
00:34:31,280 --> 00:34:32,639
the earliest

971
00:34:32,639 --> 00:34:33,760
noise addition mechanism for

972
00:34:33,760 --> 00:34:36,000
differential privacy

973
00:34:36,000 --> 00:34:37,679
and as a result we can get what is

974
00:34:37,679 --> 00:34:38,399
called

975
00:34:38,399 --> 00:34:40,000
zero concentrator differential privacy

976
00:34:40,000 --> 00:34:42,079
guarantee it is a slightly stronger

977
00:34:42,079 --> 00:34:43,599
version of the definition what i showed

978
00:34:43,599 --> 00:34:45,199
to you earlier in the first slide over

979
00:34:45,199 --> 00:34:47,119
differential privacy

980
00:34:47,119 --> 00:34:49,839
and for what it is worth gctp is the

981
00:34:49,839 --> 00:34:52,159
definition that has been used for

982
00:34:52,159 --> 00:34:54,399
uh reporting by u.s census as far as we

983
00:34:54,399 --> 00:34:57,440
know from public sources

984
00:34:59,839 --> 00:35:04,240
so this is on academic data set

985
00:35:04,240 --> 00:35:06,079
now let's go to the actual production

986
00:35:06,079 --> 00:35:08,400
model

987
00:35:10,800 --> 00:35:12,160
for obvious reasons i cannot give you

988
00:35:12,160 --> 00:35:14,079
the exact parameters

989
00:35:14,079 --> 00:35:16,560
or the accuracy numbers

990
00:35:16,560 --> 00:35:19,200
but i'll give you as far as we know how

991
00:35:19,200 --> 00:35:20,480
to say publicly

992
00:35:20,480 --> 00:35:23,040
so we trained a 1.3 million parameter

993
00:35:23,040 --> 00:35:25,599
recurrent unit network language model

994
00:35:25,599 --> 00:35:26,960
this was used for next word prediction

995
00:35:26,960 --> 00:35:28,800
for spanish keyboard users

996
00:35:28,800 --> 00:35:30,720
so basically when you open keyboard the

997
00:35:30,720 --> 00:35:32,240
next word you are typing it makes a

998
00:35:32,240 --> 00:35:35,599
prediction using some rnn model

999
00:35:35,599 --> 00:35:37,280
the training the actual final training

1000
00:35:37,280 --> 00:35:39,599
actually ran for 2000 rounds over six

1001
00:35:39,599 --> 00:35:40,960
days

1002
00:35:40,960 --> 00:35:42,320
and

1003
00:35:42,320 --> 00:35:43,760
six and a half thousand devices

1004
00:35:43,760 --> 00:35:46,400
participating per round

1005
00:35:46,400 --> 00:35:48,320
so

1006
00:35:48,320 --> 00:35:50,720
when i told you that

1007
00:35:50,720 --> 00:35:52,640
there are ways of incorporating privacy

1008
00:35:52,640 --> 00:35:55,119
amplification in the system design

1009
00:35:55,119 --> 00:35:57,200
this number from 60s would shoot up like

1010
00:35:57,200 --> 00:35:59,520
significantly higher so that makes it

1011
00:35:59,520 --> 00:36:03,280
harder for them to kind of train

1012
00:36:05,760 --> 00:36:07,359
we were looking at user level privacy

1013
00:36:07,359 --> 00:36:09,200
guarantee so it was necessary that any

1014
00:36:09,200 --> 00:36:10,880
user does not contribute arbitrary

1015
00:36:10,880 --> 00:36:12,640
number of times we configured each

1016
00:36:12,640 --> 00:36:14,240
device to participate in training at

1017
00:36:14,240 --> 00:36:17,759
most once every 24 hours

1018
00:36:19,119 --> 00:36:21,040
the privacy accounting

1019
00:36:21,040 --> 00:36:24,079
i mean to be honest was a mess

1020
00:36:24,079 --> 00:36:25,119
because

1021
00:36:25,119 --> 00:36:26,960
users are participating every 24 hours

1022
00:36:26,960 --> 00:36:28,240
you don't know when exactly the users

1023
00:36:28,240 --> 00:36:30,560
are participating so you have to kind of

1024
00:36:30,560 --> 00:36:32,240
account for all of that and we have to

1025
00:36:32,240 --> 00:36:33,280
design

1026
00:36:33,280 --> 00:36:34,720
ways to account for the privacy which

1027
00:36:34,720 --> 00:36:36,640
can be provable stated

1028
00:36:36,640 --> 00:36:39,200
so we have a multi-part dpf title

1029
00:36:39,200 --> 00:36:41,359
accounting privacy accounting

1030
00:36:41,359 --> 00:36:42,560
based on what is called minimum

1031
00:36:42,560 --> 00:36:43,680
separation

1032
00:36:43,680 --> 00:36:47,279
and this is our archive paper

1033
00:36:49,359 --> 00:36:50,960
we can get uh

1034
00:36:50,960 --> 00:36:52,000
i mean the

1035
00:36:52,000 --> 00:36:54,720
the accounting essentially depends on a

1036
00:36:54,720 --> 00:36:57,520
not so nice looking dynamic program

1037
00:36:57,520 --> 00:36:58,560
and

1038
00:36:58,560 --> 00:37:00,560
the part that is important here is that

1039
00:37:00,560 --> 00:37:02,079
all our implementation of the privacy

1040
00:37:02,079 --> 00:37:04,320
component is public

1041
00:37:04,320 --> 00:37:05,520
it is

1042
00:37:05,520 --> 00:37:07,359
the collab what you use for privacy and

1043
00:37:07,359 --> 00:37:09,520
the privacy accounting

1044
00:37:09,520 --> 00:37:11,359
is public and also it is implemented in

1045
00:37:11,359 --> 00:37:13,920
tensorflow privacy which is one of the

1046
00:37:13,920 --> 00:37:16,000
dp libraries from google

1047
00:37:16,000 --> 00:37:17,839
okay

1048
00:37:17,839 --> 00:37:20,400
so this is how we train the model

1049
00:37:20,400 --> 00:37:23,280
and

1050
00:37:23,280 --> 00:37:25,200
the two things are and i will have in

1051
00:37:25,200 --> 00:37:26,480
the next slide one thing important is

1052
00:37:26,480 --> 00:37:30,000
that although i did not give you that

1053
00:37:30,000 --> 00:37:31,520
although it did not give you the gap

1054
00:37:31,520 --> 00:37:32,960
from the non-private model to the

1055
00:37:32,960 --> 00:37:34,480
private model

1056
00:37:34,480 --> 00:37:36,240
this is obvious because here we have to

1057
00:37:36,240 --> 00:37:37,280
train with privacy we don't have the

1058
00:37:37,280 --> 00:37:39,760
nonprivate model in per se

1059
00:37:39,760 --> 00:37:41,839
and the i also did not give you the

1060
00:37:41,839 --> 00:37:43,920
accuracy numbers but what is important

1061
00:37:43,920 --> 00:37:45,839
that this is a production model and the

1062
00:37:45,839 --> 00:37:47,680
fact that is a production model you

1063
00:37:47,680 --> 00:37:50,240
cannot ship a model unless it reaches a

1064
00:37:50,240 --> 00:37:52,079
particular accuracy bar which has got

1065
00:37:52,079 --> 00:37:54,240
nothing to do with privacy

1066
00:37:54,240 --> 00:37:55,520
so the team would not actually allow us

1067
00:37:55,520 --> 00:37:57,680
to deploy it so it had to actually have

1068
00:37:57,680 --> 00:37:58,800
a pretty good

1069
00:37:58,800 --> 00:38:00,880
accuracy

1070
00:38:00,880 --> 00:38:01,760
okay

1071
00:38:01,760 --> 00:38:03,280
so here's the board slim

1072
00:38:03,280 --> 00:38:04,560
to our knowledge is the first production

1073
00:38:04,560 --> 00:38:06,079
needle network trained directly on user

1074
00:38:06,079 --> 00:38:07,920
data announced with a formal dp

1075
00:38:07,920 --> 00:38:10,320
guarantee

1076
00:38:10,320 --> 00:38:11,839
and as in the beginning of the talk i

1077
00:38:11,839 --> 00:38:13,359
told you that the word formula is kind

1078
00:38:13,359 --> 00:38:16,320
of heavy here and the reason is that we

1079
00:38:16,320 --> 00:38:17,359
have to say something which is

1080
00:38:17,359 --> 00:38:18,640
defendable

1081
00:38:18,640 --> 00:38:20,400
and in this case

1082
00:38:20,400 --> 00:38:22,400
we have to do like all of this like

1083
00:38:22,400 --> 00:38:25,119
design a new algorithm design

1084
00:38:25,119 --> 00:38:26,400
change some of the protocols of the

1085
00:38:26,400 --> 00:38:28,800
federated learning where you do user's

1086
00:38:28,800 --> 00:38:30,720
contribution bounding the user does not

1087
00:38:30,720 --> 00:38:32,320
contribute multiple times user controls

1088
00:38:32,320 --> 00:38:34,079
once in every 24 hours and all those

1089
00:38:34,079 --> 00:38:35,680
things

1090
00:38:35,680 --> 00:38:36,640
the

1091
00:38:36,640 --> 00:38:38,960
full details of how we do it and what

1092
00:38:38,960 --> 00:38:40,000
are the things

1093
00:38:40,000 --> 00:38:42,560
is in a google a blog post from

1094
00:38:42,560 --> 00:38:45,560
february

1095
00:38:48,640 --> 00:38:50,640
this training satisfies user level 0.1

1096
00:38:50,640 --> 00:38:53,680
gcdp for well-behaved clients why it is

1097
00:38:53,680 --> 00:38:54,880
well-behaved

1098
00:38:54,880 --> 00:38:56,640
something which a device which

1099
00:38:56,640 --> 00:38:57,920
faithfully follows the algorithm

1100
00:38:57,920 --> 00:39:01,839
including participation limits

1101
00:39:02,079 --> 00:39:04,160
and this is all we can in you see right

1102
00:39:04,160 --> 00:39:06,240
because i as a device i can do arbitrary

1103
00:39:06,240 --> 00:39:08,480
things

1104
00:39:08,480 --> 00:39:09,520
but for wealth we have times which

1105
00:39:09,520 --> 00:39:10,720
faithfully follow the algorithm this is

1106
00:39:10,720 --> 00:39:12,320
the property the point is another point

1107
00:39:12,320 --> 00:39:13,920
to mention is that the mid-fair clients

1108
00:39:13,920 --> 00:39:15,920
do not aggressively affect the

1109
00:39:15,920 --> 00:39:17,119
privacy guarantee given to the

1110
00:39:17,119 --> 00:39:18,800
well-being clients so the well-behaved

1111
00:39:18,800 --> 00:39:20,079
clients will still have the same privacy

1112
00:39:20,079 --> 00:39:23,760
guarantee the friends may not have that

1113
00:39:24,560 --> 00:39:26,160
and the point is that we tried our best

1114
00:39:26,160 --> 00:39:28,160
to minimize the number of misbehave

1115
00:39:28,160 --> 00:39:30,480
lines

1116
00:39:31,200 --> 00:39:32,400
which is not

1117
00:39:32,400 --> 00:39:34,000
like which is not something significant

1118
00:39:34,000 --> 00:39:36,480
that number

1119
00:39:36,720 --> 00:39:38,880
and the model quality improved over the

1120
00:39:38,880 --> 00:39:40,880
previous dp federated average train

1121
00:39:40,880 --> 00:39:43,440
model which is a published work so we

1122
00:39:43,440 --> 00:39:46,160
can look at that

1123
00:39:49,920 --> 00:39:51,839
all the incompatible just to said that

1124
00:39:51,839 --> 00:39:53,040
in context

1125
00:39:53,040 --> 00:39:55,359
your sensors use the

1126
00:39:55,359 --> 00:39:56,800
u.s sensors are solving a different

1127
00:39:56,800 --> 00:39:58,400
problem

1128
00:39:58,400 --> 00:40:00,160
so these are absolutely not compatible

1129
00:40:00,160 --> 00:40:02,079
numbers but to set the thing in context

1130
00:40:02,079 --> 00:40:04,560
your sensors was using a gcdp of 2.56

1131
00:40:04,560 --> 00:40:07,119
and we get a gctp of 0.81

1132
00:40:07,119 --> 00:40:09,440
because if i don't give the scale it is

1133
00:40:09,440 --> 00:40:11,040
unclear how do i even interpret the

1134
00:40:11,040 --> 00:40:13,359
number so you get a cdp of less than one

1135
00:40:13,359 --> 00:40:17,200
and your senses uses 2.56

1136
00:40:17,440 --> 00:40:18,240
okay

1137
00:40:18,240 --> 00:40:20,400
so this is all i have to say of this

1138
00:40:20,400 --> 00:40:23,800
production model

1139
00:40:24,480 --> 00:40:26,640
in the last part i would say something

1140
00:40:26,640 --> 00:40:29,839
more speculative less

1141
00:40:30,079 --> 00:40:31,040
less

1142
00:40:31,040 --> 00:40:32,240
i guess

1143
00:40:32,240 --> 00:40:33,839
less thought out

1144
00:40:33,839 --> 00:40:36,000
but we have been working on improving

1145
00:40:36,000 --> 00:40:37,359
the

1146
00:40:37,359 --> 00:40:39,359
improving the algorithm in the last few

1147
00:40:39,359 --> 00:40:41,760
months and we have some new results so

1148
00:40:41,760 --> 00:40:45,119
this is dpf title plus plus

1149
00:40:45,839 --> 00:40:47,760
again fdl update you are computing a

1150
00:40:47,760 --> 00:40:49,599
sequence of prefix sums and you are

1151
00:40:49,599 --> 00:40:51,359
computing using this tree

1152
00:40:51,359 --> 00:40:53,280
right so let me take you to a different

1153
00:40:53,280 --> 00:40:55,280
view of the same algorithm

1154
00:40:55,280 --> 00:40:56,240
is

1155
00:40:56,240 --> 00:40:58,160
you have a matrix

1156
00:40:58,160 --> 00:40:59,760
and the cumulative sum essentially is a

1157
00:40:59,760 --> 00:41:01,119
matrix vector multiplication where the

1158
00:41:01,119 --> 00:41:03,280
gradients are showing up

1159
00:41:03,280 --> 00:41:05,839
and you are multiplying with the

1160
00:41:05,839 --> 00:41:09,440
lower triangular matrix is all once

1161
00:41:10,000 --> 00:41:11,760
it might not be obvious immediately but

1162
00:41:11,760 --> 00:41:13,839
this is a form of convolution basically

1163
00:41:13,839 --> 00:41:16,640
computing convolution

1164
00:41:16,640 --> 00:41:20,118
now part of the consolidation

1165
00:41:20,160 --> 00:41:22,319
so now let's call this matrix to be the

1166
00:41:22,319 --> 00:41:24,079
query matrix a

1167
00:41:24,079 --> 00:41:26,000
and this gradients are appearing and you

1168
00:41:26,000 --> 00:41:27,359
are you are trying to do a matrix vector

1169
00:41:27,359 --> 00:41:29,200
multiplication for simplicity just

1170
00:41:29,200 --> 00:41:30,560
written in one dimension trust me it

1171
00:41:30,560 --> 00:41:31,520
will less than two higher dimensions

1172
00:41:31,520 --> 00:41:32,640
fairly easily if you write the linear

1173
00:41:32,640 --> 00:41:34,720
algebra

1174
00:41:34,720 --> 00:41:37,599
so you have to contribute a times c

1175
00:41:37,599 --> 00:41:40,000
now the idea is that i will factorize a

1176
00:41:40,000 --> 00:41:42,319
into b c

1177
00:41:42,319 --> 00:41:44,319
so in the context of

1178
00:41:44,319 --> 00:41:45,920
tree aggregation

1179
00:41:45,920 --> 00:41:47,440
this is a

1180
00:41:47,440 --> 00:41:49,119
the lower triangular matrix that's the

1181
00:41:49,119 --> 00:41:51,280
prefix that we wanted to compute

1182
00:41:51,280 --> 00:41:54,319
c is the encoding of the matrix as

1183
00:41:54,319 --> 00:41:56,880
encoding of the tree

1184
00:41:56,880 --> 00:41:58,160
look at it for 10 seconds it should be

1185
00:41:58,160 --> 00:42:00,079
obvious why is an encoding of the tree

1186
00:42:00,079 --> 00:42:02,960
so the leaf nodes are the top four rows

1187
00:42:02,960 --> 00:42:05,280
and then the second level is the

1188
00:42:05,280 --> 00:42:06,319
fifth row

1189
00:42:06,319 --> 00:42:08,000
and the sixth row and the final level is

1190
00:42:08,000 --> 00:42:09,040
the

1191
00:42:09,040 --> 00:42:10,720
seventh row right

1192
00:42:10,720 --> 00:42:12,400
and the reconstruction how do you

1193
00:42:12,400 --> 00:42:14,240
actually finally get the prediction you

1194
00:42:14,240 --> 00:42:17,040
can write the matrix v

1195
00:42:17,040 --> 00:42:19,839
the point is that

1196
00:42:20,400 --> 00:42:21,599
trigger irrigation is a special

1197
00:42:21,599 --> 00:42:24,960
instantiation of matrix factorization

1198
00:42:24,960 --> 00:42:27,119
why do i write it this way

1199
00:42:27,119 --> 00:42:29,119
well if i want to do dp estimate of a

1200
00:42:29,119 --> 00:42:32,319
times c i can write at a time g is equal

1201
00:42:32,319 --> 00:42:34,160
to b times c times g g is the gradient

1202
00:42:34,160 --> 00:42:36,960
vector plus some noise

1203
00:42:36,960 --> 00:42:38,720
which if i factorize out essentially i

1204
00:42:38,720 --> 00:42:40,560
get b times noise

1205
00:42:40,560 --> 00:42:41,839
now the noise should be sufficient to

1206
00:42:41,839 --> 00:42:43,359
ensure dp

1207
00:42:43,359 --> 00:42:45,440
that's what is necessary

1208
00:42:45,440 --> 00:42:47,040
but for the error

1209
00:42:47,040 --> 00:42:48,640
essentially what it happens like if you

1210
00:42:48,640 --> 00:42:50,800
write the error of dpftl it is

1211
00:42:50,800 --> 00:42:52,720
essentially would be dependent on the

1212
00:42:52,720 --> 00:42:54,160
squared norm of the

1213
00:42:54,160 --> 00:42:55,599
b times the noise

1214
00:42:55,599 --> 00:42:57,680
so essentially you want to minimize b

1215
00:42:57,680 --> 00:42:59,280
times the noise squared

1216
00:42:59,280 --> 00:43:00,960
or the expectation of that

1217
00:43:00,960 --> 00:43:02,240
subject to the condition that the noise

1218
00:43:02,240 --> 00:43:05,439
is sufficient for giving you dv

1219
00:43:05,920 --> 00:43:07,359
okay

1220
00:43:07,359 --> 00:43:08,960
in our work we design fast and noise

1221
00:43:08,960 --> 00:43:10,839
optimal methods to factorize the matrix

1222
00:43:10,839 --> 00:43:14,560
a so we'll feed in this constraint to

1223
00:43:14,560 --> 00:43:16,640
minimize the squared norm

1224
00:43:16,640 --> 00:43:18,640
and then we will try to factorize the

1225
00:43:18,640 --> 00:43:21,200
matrix a

1226
00:43:22,319 --> 00:43:24,640
one privacy challenge

1227
00:43:24,640 --> 00:43:28,240
for people who follow this area is that

1228
00:43:28,240 --> 00:43:29,599
it is

1229
00:43:29,599 --> 00:43:33,440
so we needed to get a new privacy proof

1230
00:43:33,520 --> 00:43:37,040
if the matrices c are non non lower

1231
00:43:37,040 --> 00:43:38,480
triangular

1232
00:43:38,480 --> 00:43:41,119
meaning that when you take c energy g is

1233
00:43:41,119 --> 00:43:43,440
the gradient vector right

1234
00:43:43,440 --> 00:43:45,520
so the noise added at the next step can

1235
00:43:45,520 --> 00:43:47,520
depend on the noise in the previous step

1236
00:43:47,520 --> 00:43:49,839
or the or the noise at the later stages

1237
00:43:49,839 --> 00:43:52,480
so if c is not lower triangular then we

1238
00:43:52,480 --> 00:43:54,079
needed a new privacy proof to prove

1239
00:43:54,079 --> 00:43:56,000
adaptive privacy because

1240
00:43:56,000 --> 00:43:58,079
the gradients are adaptive in nature so

1241
00:43:58,079 --> 00:43:59,440
the next entry compute depends on the

1242
00:43:59,440 --> 00:44:01,200
previous answer and this kind of creates

1243
00:44:01,200 --> 00:44:02,880
weird correlations amongst the noises so

1244
00:44:02,880 --> 00:44:06,000
you need a new privacy tool

1245
00:44:06,880 --> 00:44:09,440
this is a publicity of our recent paper

1246
00:44:09,440 --> 00:44:11,920
which is on archive

1247
00:44:11,920 --> 00:44:15,280
where we essentially do all these things

1248
00:44:15,280 --> 00:44:17,359
and what we show is that for stack

1249
00:44:17,359 --> 00:44:18,480
overflow

1250
00:44:18,480 --> 00:44:20,160
so what is so sorry i have to say this

1251
00:44:20,160 --> 00:44:22,480
over this like yeah so this is our paper

1252
00:44:22,480 --> 00:44:24,400
and we again do the experiment on stack

1253
00:44:24,400 --> 00:44:26,640
of workflow and here you will see bunch

1254
00:44:26,640 --> 00:44:28,800
of algorithm names optimal factorization

1255
00:44:28,800 --> 00:44:31,119
optimal prefix sum on the calculator

1256
00:44:31,119 --> 00:44:34,960
online none of them have uh

1257
00:44:34,960 --> 00:44:35,839
i've

1258
00:44:35,839 --> 00:44:37,440
told you what they are optimal

1259
00:44:37,440 --> 00:44:39,520
factorization is the one where i

1260
00:44:39,520 --> 00:44:42,800
actually factorize a into b times c

1261
00:44:42,800 --> 00:44:45,520
where i try to optimally minimize the l

1262
00:44:45,520 --> 00:44:47,760
two squared error between b and the

1263
00:44:47,760 --> 00:44:49,520
noise

1264
00:44:49,520 --> 00:44:51,760
optimal prefix sum is the algorithm

1265
00:44:51,760 --> 00:44:52,640
where

1266
00:44:52,640 --> 00:44:54,319
i fix

1267
00:44:54,319 --> 00:44:55,119
the

1268
00:44:55,119 --> 00:44:57,440
uh the c matrix that is which is

1269
00:44:57,440 --> 00:44:58,640
interacting with the data to be the

1270
00:44:58,640 --> 00:45:00,800
prefix sub matrix

1271
00:45:00,800 --> 00:45:02,960
to be the tree aggregation matrix i only

1272
00:45:02,960 --> 00:45:05,839
optimize for b

1273
00:45:06,319 --> 00:45:08,560
honey control is a version of trigger

1274
00:45:08,560 --> 00:45:10,800
aggregation where i do some variance

1275
00:45:10,800 --> 00:45:13,680
reduction and so is on a colon line

1276
00:45:13,680 --> 00:45:16,319
the full the point is that

1277
00:45:16,319 --> 00:45:18,400
the blue line surpasses over

1278
00:45:18,400 --> 00:45:19,599
all the three

1279
00:45:19,599 --> 00:45:20,400
and

1280
00:45:20,400 --> 00:45:21,760
even if you on this test accuracy

1281
00:45:21,760 --> 00:45:25,599
numbers the gaps look reasonably small

1282
00:45:25,599 --> 00:45:28,800
it is a significant gap

1283
00:45:30,880 --> 00:45:32,560
further furthermore

1284
00:45:32,560 --> 00:45:34,400
to prove the privacy

1285
00:45:34,400 --> 00:45:36,880
for the first three algorithms we needed

1286
00:45:36,880 --> 00:45:39,440
this adaptive privacy guarantee what

1287
00:45:39,440 --> 00:45:41,440
what i mentioned in the previous slide

1288
00:45:41,440 --> 00:45:43,200
so i encourage you if you are interested

1289
00:45:43,200 --> 00:45:44,960
in this line of work please go and check

1290
00:45:44,960 --> 00:45:47,839
out our paper

1291
00:45:51,359 --> 00:45:55,359
now i'm almost done with my talk

1292
00:45:56,480 --> 00:45:58,720
i'll tell some of the future directions

1293
00:45:58,720 --> 00:46:01,520
so the question is

1294
00:46:01,520 --> 00:46:03,440
we looked at dpftl as an algorithm and

1295
00:46:03,440 --> 00:46:06,319
this is a purely theory question is

1296
00:46:06,319 --> 00:46:08,560
can we get tight regret guarantees for

1297
00:46:08,560 --> 00:46:11,440
dp online convince optimization dpo cos

1298
00:46:11,440 --> 00:46:13,200
this algorithm gives the tight

1299
00:46:13,200 --> 00:46:15,359
guarantees for linear and least weight

1300
00:46:15,359 --> 00:46:18,000
losses is unclear you get it for general

1301
00:46:18,000 --> 00:46:20,480
osu's

1302
00:46:21,119 --> 00:46:24,560
next is more of a philosophical question

1303
00:46:24,560 --> 00:46:25,359
in

1304
00:46:25,359 --> 00:46:27,200
lot of the implementations of dp

1305
00:46:27,200 --> 00:46:28,640
learning

1306
00:46:28,640 --> 00:46:30,240
variants of privacy amplification is

1307
00:46:30,240 --> 00:46:32,480
embedded into it

1308
00:46:32,480 --> 00:46:35,200
this is one case where i showed that

1309
00:46:35,200 --> 00:46:37,280
it was you can design algorithms it does

1310
00:46:37,280 --> 00:46:38,960
not rely on amplification which can

1311
00:46:38,960 --> 00:46:41,359
operate on over streaming data

1312
00:46:41,359 --> 00:46:43,920
the philosophical question is is privacy

1313
00:46:43,920 --> 00:46:46,079
amplification necessary in any form

1314
00:46:46,079 --> 00:46:49,040
when you are dealing with practice

1315
00:46:49,040 --> 00:46:52,079
the third question is

1316
00:46:53,119 --> 00:46:55,119
in what other real-world applications we

1317
00:46:55,119 --> 00:46:56,640
can provide meaningful user level dp

1318
00:46:56,640 --> 00:47:00,078
guarantees which can be defended

1319
00:47:00,240 --> 00:47:02,079
so in many situations

1320
00:47:02,079 --> 00:47:03,920
it is said that okay our algorithm

1321
00:47:03,920 --> 00:47:05,200
satisfies difference differential

1322
00:47:05,200 --> 00:47:06,560
privacy that's not a well quantified

1323
00:47:06,560 --> 00:47:07,680
statement it has to be quantified with

1324
00:47:07,680 --> 00:47:10,640
the parameter and second and thirdly

1325
00:47:10,640 --> 00:47:14,800
that parameter has to be defendable

1326
00:47:14,800 --> 00:47:16,400
if you're if you're staying in the terms

1327
00:47:16,400 --> 00:47:17,680
of differential privacy of course there

1328
00:47:17,680 --> 00:47:20,079
are other ways of minimizing privacy

1329
00:47:20,079 --> 00:47:22,800
risks which are equally important but if

1330
00:47:22,800 --> 00:47:23,599
you are staying the realm of

1331
00:47:23,599 --> 00:47:25,200
differential privacy then

1332
00:47:25,200 --> 00:47:26,319
it is

1333
00:47:26,319 --> 00:47:28,720
and it is

1334
00:47:28,720 --> 00:47:31,200
it's probably important to state the

1335
00:47:31,200 --> 00:47:33,440
privacy parameter

1336
00:47:33,440 --> 00:47:36,800
and the final question is like

1337
00:47:36,800 --> 00:47:38,880
how can we

1338
00:47:38,880 --> 00:47:41,680
so okay when we do dp learning it is

1339
00:47:41,680 --> 00:47:43,839
known that the privacy actually in this

1340
00:47:43,839 --> 00:47:46,160
case it was not that but in many cases

1341
00:47:46,160 --> 00:47:48,240
privacy actually works as a significant

1342
00:47:48,240 --> 00:47:50,400
impediment in terms of the accuracy the

1343
00:47:50,400 --> 00:47:52,800
question is can we leverage

1344
00:47:52,800 --> 00:47:54,880
other forms of data like public data or

1345
00:47:54,880 --> 00:47:56,559
different data sources and combine them

1346
00:47:56,559 --> 00:47:58,400
effectively to get

1347
00:47:58,400 --> 00:48:01,839
accuracy boost in dp learning

1348
00:48:04,240 --> 00:48:07,599
and here is a acknowledgement to all the

1349
00:48:07,599 --> 00:48:09,040
folks who were involved in the whole

1350
00:48:09,040 --> 00:48:11,440
project this was not just my project a

1351
00:48:11,440 --> 00:48:13,040
lot of people were involved across

1352
00:48:13,040 --> 00:48:14,559
multiple teams and it was a multi-year

1353
00:48:14,559 --> 00:48:16,559
project

1354
00:48:16,559 --> 00:48:17,599
and

1355
00:48:17,599 --> 00:48:19,040
that is

1356
00:48:19,040 --> 00:48:21,839
all i have

1357
00:48:29,599 --> 00:48:32,400
so much so let's get some questions so

1358
00:48:32,400 --> 00:48:33,920
whoever has a question you can either

1359
00:48:33,920 --> 00:48:36,720
come in the microphone or i can give you

1360
00:48:36,720 --> 00:48:38,000
the microphone

1361
00:48:38,000 --> 00:48:40,800
so they can find

1362
00:48:47,119 --> 00:48:49,839
i believe so

1363
00:48:51,200 --> 00:48:52,319
hello

1364
00:48:52,319 --> 00:48:54,800
can you hear me hi

1365
00:48:54,800 --> 00:48:56,079
um

1366
00:48:56,079 --> 00:48:57,280
so it seems like you're trying to move

1367
00:48:57,280 --> 00:48:58,480
away from

1368
00:48:58,480 --> 00:49:00,720
using privacy implication

1369
00:49:00,720 --> 00:49:02,960
amplification but i'm wondering could

1370
00:49:02,960 --> 00:49:05,200
you apply privacy amplification on top

1371
00:49:05,200 --> 00:49:06,960
of the follow the regular last leader

1372
00:49:06,960 --> 00:49:09,119
thing no why is that

1373
00:49:09,119 --> 00:49:10,640
because we

1374
00:49:10,640 --> 00:49:13,040
uh the algorithm how you do it first of

1375
00:49:13,040 --> 00:49:14,079
all you don't know how to use privacy

1376
00:49:14,079 --> 00:49:15,040
amplification first of all in the

1377
00:49:15,040 --> 00:49:16,960
setting we are operating on yeah because

1378
00:49:16,960 --> 00:49:18,400
i told you that in this case i don't

1379
00:49:18,400 --> 00:49:20,480
have control over the data i cannot

1380
00:49:20,480 --> 00:49:22,800
and we are looking at

1381
00:49:22,800 --> 00:49:24,720
even if we are in a kit you would but

1382
00:49:24,720 --> 00:49:25,839
also the

1383
00:49:25,839 --> 00:49:27,119
the style of the algorithm if you look

1384
00:49:27,119 --> 00:49:29,280
at it it is unclear how to use privacy

1385
00:49:29,280 --> 00:49:30,880
amplification in this particular case in

1386
00:49:30,880 --> 00:49:32,319
this particular algorithm in hdd you can

1387
00:49:32,319 --> 00:49:34,319
use pretty easily but here i'm just

1388
00:49:34,319 --> 00:49:37,040
adding also the cumulative sums so it's

1389
00:49:37,040 --> 00:49:39,440
kind of unclear how to do or what extra

1390
00:49:39,440 --> 00:49:40,720
benefit you would get from privacy

1391
00:49:40,720 --> 00:49:43,040
amplification okay cool um and then my

1392
00:49:43,040 --> 00:49:44,480
other question is just

1393
00:49:44,480 --> 00:49:46,640
uh for each client

1394
00:49:46,640 --> 00:49:48,640
are they providing multiple

1395
00:49:48,640 --> 00:49:50,960
data points every time they're

1396
00:49:50,960 --> 00:49:52,319
sending data

1397
00:49:52,319 --> 00:49:54,160
no every time they stand data they do

1398
00:49:54,160 --> 00:49:55,599
local updates and they send one

1399
00:49:55,599 --> 00:49:56,720
gradients

1400
00:49:56,720 --> 00:49:58,319
but throughout the training process the

1401
00:49:58,319 --> 00:50:00,240
client would participate multiple times

1402
00:50:00,240 --> 00:50:01,599
but we minimize the number of times that

1403
00:50:01,599 --> 00:50:03,280
line participates and the number of

1404
00:50:03,280 --> 00:50:05,440
times the client part i mean a client

1405
00:50:05,440 --> 00:50:07,760
would participate once in every 24 hours

1406
00:50:07,760 --> 00:50:10,400
right and is that so that they're not

1407
00:50:10,400 --> 00:50:11,760
like over

1408
00:50:11,760 --> 00:50:13,599
like so that if you take one client out

1409
00:50:13,599 --> 00:50:14,880
they're not like you're not taking too

1410
00:50:14,880 --> 00:50:16,960
much data out right everything okay

1411
00:50:16,960 --> 00:50:19,280
thank you

1412
00:50:19,280 --> 00:50:22,400
no hi uh very nice talking oh it's very

1413
00:50:22,400 --> 00:50:24,559
impressive to see uh differentiated

1414
00:50:24,559 --> 00:50:26,800
private federation actually before any

1415
00:50:26,800 --> 00:50:27,920
practice

1416
00:50:27,920 --> 00:50:30,160
but um i have a couple of kind of

1417
00:50:30,160 --> 00:50:32,160
non-technical questions

1418
00:50:32,160 --> 00:50:33,040
um

1419
00:50:33,040 --> 00:50:34,400
first

1420
00:50:34,400 --> 00:50:35,359
um

1421
00:50:35,359 --> 00:50:37,839
a lot of times when we try to deploy you

1422
00:50:37,839 --> 00:50:38,640
know

1423
00:50:38,640 --> 00:50:40,400
differentiate private or

1424
00:50:40,400 --> 00:50:43,280
um anything that involves privacy

1425
00:50:43,280 --> 00:50:47,280
um into practice there tend to be

1426
00:50:47,280 --> 00:50:49,760
uh a trade-off you know between privacy

1427
00:50:49,760 --> 00:50:52,559
with performance efficiency and so on um

1428
00:50:52,559 --> 00:50:56,079
i was wondering um in the uh process of

1429
00:50:56,079 --> 00:50:59,359
deploying them into practice

1430
00:50:59,359 --> 00:51:02,000
did you face challenges that are

1431
00:51:02,000 --> 00:51:04,559
non-technical like in for example

1432
00:51:04,559 --> 00:51:08,480
convincing the leadership to deploy

1433
00:51:08,480 --> 00:51:10,960
something that involves privacy

1434
00:51:10,960 --> 00:51:11,680
or

1435
00:51:11,680 --> 00:51:12,960
like

1436
00:51:12,960 --> 00:51:15,839
did you have any challenges in that or

1437
00:51:15,839 --> 00:51:17,839
it's just very supportive that

1438
00:51:17,839 --> 00:51:19,839
the big tech companies know that this is

1439
00:51:19,839 --> 00:51:21,599
the right thing to do and we should do

1440
00:51:21,599 --> 00:51:22,400
it

1441
00:51:22,400 --> 00:51:25,040
uh what's the main motivation for them

1442
00:51:25,040 --> 00:51:26,319
to adopt

1443
00:51:26,319 --> 00:51:30,480
uh privacy techniques um

1444
00:51:30,480 --> 00:51:32,319
that's a fairly deep question so let me

1445
00:51:32,319 --> 00:51:33,200
try to

1446
00:51:33,200 --> 00:51:35,920
say it like on my small car whatever

1447
00:51:35,920 --> 00:51:37,760
whatever my experience has been over the

1448
00:51:37,760 --> 00:51:39,599
years

1449
00:51:39,599 --> 00:51:40,800
so

1450
00:51:40,800 --> 00:51:44,400
privacy yes it creates trouble when you

1451
00:51:44,400 --> 00:51:46,640
try to train models but in this

1452
00:51:46,640 --> 00:51:48,880
particular case we did not for

1453
00:51:48,880 --> 00:51:52,319
for the setup of the problem maybe

1454
00:51:53,599 --> 00:51:56,079
and as as i tried to highlight in the

1455
00:51:56,079 --> 00:51:58,480
talk that the problem itself required an

1456
00:51:58,480 --> 00:52:00,880
new algorithm design

1457
00:52:00,880 --> 00:52:02,800
and required some time so

1458
00:52:02,800 --> 00:52:04,640
non-technical i mean a non-technical

1459
00:52:04,640 --> 00:52:06,079
answer is that like it requires time to

1460
00:52:06,079 --> 00:52:08,000
develop these things so i mean if you're

1461
00:52:08,000 --> 00:52:10,319
working at a shorter time window

1462
00:52:10,319 --> 00:52:12,640
there can be pushbacks

1463
00:52:12,640 --> 00:52:15,200
but my personal experience has been uh

1464
00:52:15,200 --> 00:52:18,079
that i have not seen pushback from the

1465
00:52:18,079 --> 00:52:19,599
leadership on this kind of records it's

1466
00:52:19,599 --> 00:52:21,040
actually very supportive

1467
00:52:21,040 --> 00:52:23,119
so but this is only for the organization

1468
00:52:23,119 --> 00:52:24,720
very well

1469
00:52:24,720 --> 00:52:26,880
i don't know

1470
00:52:26,880 --> 00:52:28,800
that's good to hear

1471
00:52:28,800 --> 00:52:30,079
the other question

1472
00:52:30,079 --> 00:52:31,440
differential privacy has been very

1473
00:52:31,440 --> 00:52:33,359
successful and you know getting deployed

1474
00:52:33,359 --> 00:52:36,000
in many companies many scenarios and

1475
00:52:36,000 --> 00:52:36,880
also

1476
00:52:36,880 --> 00:52:37,839
um

1477
00:52:37,839 --> 00:52:40,400
there's more and more awareness in the

1478
00:52:40,400 --> 00:52:41,839
privacy issues

1479
00:52:41,839 --> 00:52:44,559
um even to the general public so i was

1480
00:52:44,559 --> 00:52:46,319
wondering uh if you think if there is

1481
00:52:46,319 --> 00:52:47,440
any

1482
00:52:47,440 --> 00:52:50,559
experience that we can imagine um

1483
00:52:50,559 --> 00:52:52,319
in the future maybe we try to deploy

1484
00:52:52,319 --> 00:52:54,800
other uh techniques from from the

1485
00:52:54,800 --> 00:52:57,760
crystal community

1486
00:52:59,440 --> 00:53:03,119
i guess the only thing is i mean to when

1487
00:53:03,119 --> 00:53:05,760
we say general public is it like

1488
00:53:05,760 --> 00:53:07,359
i mean it's clearly not the people who

1489
00:53:07,359 --> 00:53:09,599
come to this conference

1490
00:53:09,599 --> 00:53:12,000
not yes definitely not

1491
00:53:12,000 --> 00:53:13,440
here but also

1492
00:53:13,440 --> 00:53:15,280
when you talk to people about privacy

1493
00:53:15,280 --> 00:53:16,559
they would

1494
00:53:16,559 --> 00:53:18,319
know the notion at least to be aware of

1495
00:53:18,319 --> 00:53:20,800
the notion of differential privacy and

1496
00:53:20,800 --> 00:53:22,559
the companies are doing the right thing

1497
00:53:22,559 --> 00:53:25,520
for them protecting their privacy uh and

1498
00:53:25,520 --> 00:53:26,880
so on

1499
00:53:26,880 --> 00:53:28,880
so i think one thing to do is that i

1500
00:53:28,880 --> 00:53:31,040
mean

1501
00:53:32,559 --> 00:53:34,720
be as open as you can

1502
00:53:34,720 --> 00:53:36,319
about the work

1503
00:53:36,319 --> 00:53:38,319
try to make it heavily scrutinized by

1504
00:53:38,319 --> 00:53:40,000
the external world and if you're making

1505
00:53:40,000 --> 00:53:41,599
a statement make a claim like okay this

1506
00:53:41,599 --> 00:53:43,359
is what i'm saying

1507
00:53:43,359 --> 00:53:44,640
and

1508
00:53:44,640 --> 00:53:46,319
and make a refutable claim which can be

1509
00:53:46,319 --> 00:53:48,800
reputed and give enough evidence to the

1510
00:53:48,800 --> 00:53:50,720
other side to actually make it possible

1511
00:53:50,720 --> 00:53:53,200
to repute it now the question is

1512
00:53:53,200 --> 00:53:54,720
yeah with that would probably build some

1513
00:53:54,720 --> 00:53:56,480
confidence because if you talk to a

1514
00:53:56,480 --> 00:53:58,319
general public it is very hard to kind

1515
00:53:58,319 --> 00:53:59,200
of

1516
00:53:59,200 --> 00:54:00,880
kind of explain what differential

1517
00:54:00,880 --> 00:54:02,559
privacy is like it's a very technical

1518
00:54:02,559 --> 00:54:03,760
notion and

1519
00:54:03,760 --> 00:54:04,800
that would be

1520
00:54:04,800 --> 00:54:06,640
one way to kind of give

1521
00:54:06,640 --> 00:54:08,400
faith the community that this is

1522
00:54:08,400 --> 00:54:11,040
meaningful

1523
00:54:11,040 --> 00:54:13,520
thank you

1524
00:54:15,200 --> 00:54:19,040
hi um i really like your talk um i was

1525
00:54:19,040 --> 00:54:20,559
wondering so at the beginning of the

1526
00:54:20,559 --> 00:54:22,240
talk we situated us

1527
00:54:22,240 --> 00:54:27,200
in a in a um setting where uh trust

1528
00:54:27,200 --> 00:54:29,200
boundaries is drawn

1529
00:54:29,200 --> 00:54:31,200
uh around

1530
00:54:31,200 --> 00:54:32,160
um

1531
00:54:32,160 --> 00:54:34,720
the data holders and the

1532
00:54:34,720 --> 00:54:35,680
uh

1533
00:54:35,680 --> 00:54:39,520
person training the model right um

1534
00:54:39,520 --> 00:54:40,880
how do you

1535
00:54:40,880 --> 00:54:43,760
imagine this work translating to

1536
00:54:43,760 --> 00:54:45,440
a

1537
00:54:45,440 --> 00:54:47,280
tighter security model where there's

1538
00:54:47,280 --> 00:54:49,440
less trust between the data holders and

1539
00:54:49,440 --> 00:54:51,920
the model trainer yes in this case you

1540
00:54:51,920 --> 00:54:54,079
will try to use some kind of sql

1541
00:54:54,079 --> 00:54:55,920
multiparty computation or some versions

1542
00:54:55,920 --> 00:54:57,440
of that where

1543
00:54:57,440 --> 00:54:58,799
essentially

1544
00:54:58,799 --> 00:55:00,960
the devices are trusted the protocol i

1545
00:55:00,960 --> 00:55:02,319
mean you'll run some crypto protocol to

1546
00:55:02,319 --> 00:55:04,400
run on it

1547
00:55:04,400 --> 00:55:08,480
these are these are questions which

1548
00:55:08,480 --> 00:55:10,559
people are looking at i'm unfortunately

1549
00:55:10,559 --> 00:55:11,920
not the right person to kind of tell

1550
00:55:11,920 --> 00:55:13,599
something about it this was involved

1551
00:55:13,599 --> 00:55:17,359
crypto but my

1552
00:55:17,359 --> 00:55:18,880
very big understanding of these things

1553
00:55:18,880 --> 00:55:20,000
are like the scale at which you're

1554
00:55:20,000 --> 00:55:21,760
operating the technology is not yet

1555
00:55:21,760 --> 00:55:23,760
there to actually kind of operate at the

1556
00:55:23,760 --> 00:55:24,720
scale like

1557
00:55:24,720 --> 00:55:26,079
smc is at like i don't know how many

1558
00:55:26,079 --> 00:55:27,599
billions of devices

1559
00:55:27,599 --> 00:55:30,400
but yeah so theoretically it is feasible

1560
00:55:30,400 --> 00:55:31,760
the question and

1561
00:55:31,760 --> 00:55:33,280
it is a very important question to push

1562
00:55:33,280 --> 00:55:36,000
the trust boundary out of this part

1563
00:55:36,000 --> 00:55:36,880
but

1564
00:55:36,880 --> 00:55:38,880
in the talk i explicitly said the trust

1565
00:55:38,880 --> 00:55:40,240
boundary because i had to i said the

1566
00:55:40,240 --> 00:55:41,599
word formal i will give a formal

1567
00:55:41,599 --> 00:55:43,200
guidance it means like what responded in

1568
00:55:43,200 --> 00:55:45,920
operating and this is boundary

1569
00:55:45,920 --> 00:55:49,079
thank you

1570
00:55:51,680 --> 00:55:54,000
prince

1571
00:55:55,839 --> 00:55:57,839
so if some questions comment at the

1572
00:55:57,839 --> 00:56:00,799
front i have some questions

1573
00:56:00,799 --> 00:56:02,720
so i want to ask first about the mpc

1574
00:56:02,720 --> 00:56:05,280
question uh is there any restriction on

1575
00:56:05,280 --> 00:56:07,119
the running time because if you use mpc

1576
00:56:07,119 --> 00:56:08,400
it will also increase a lot of the

1577
00:56:08,400 --> 00:56:09,680
running time so

1578
00:56:09,680 --> 00:56:11,680
do you have any like restriction on how

1579
00:56:11,680 --> 00:56:13,680
fast you want

1580
00:56:13,680 --> 00:56:16,720
this computation to happen reasonable

1581
00:56:16,720 --> 00:56:18,880
like here it was like six days

1582
00:56:18,880 --> 00:56:21,040
i mean six days to i mean this also

1583
00:56:21,040 --> 00:56:23,200
depends on like wheat i mean in the in

1584
00:56:23,200 --> 00:56:24,960
the organization setup we don't own the

1585
00:56:24,960 --> 00:56:26,720
product it's on the product side we

1586
00:56:26,720 --> 00:56:28,319
would actually own it and

1587
00:56:28,319 --> 00:56:29,680
if i go and tell them it would require

1588
00:56:29,680 --> 00:56:32,079
two months to train

1589
00:56:32,079 --> 00:56:33,480
it's uh yeah so

1590
00:56:33,480 --> 00:56:34,880
[Music]

1591
00:56:34,880 --> 00:56:36,160
yeah there is some version of the

1592
00:56:36,160 --> 00:56:38,640
restriction there and in in any of these

1593
00:56:38,640 --> 00:56:40,079
deployments i guess like

1594
00:56:40,079 --> 00:56:41,359
not just this one like any other

1595
00:56:41,359 --> 00:56:42,640
deployments like

1596
00:56:42,640 --> 00:56:44,079
if the running time goes prohibitively

1597
00:56:44,079 --> 00:56:46,400
large

1598
00:56:46,960 --> 00:56:48,160
that makes sense

1599
00:56:48,160 --> 00:56:49,920
and also when asked about the misbehaved

1600
00:56:49,920 --> 00:56:52,640
clients yeah so two questions there so

1601
00:56:52,640 --> 00:56:53,920
in practice when you put it in

1602
00:56:53,920 --> 00:56:56,079
production like how many clients must be

1603
00:56:56,079 --> 00:56:58,319
hit meaning they can they can submit

1604
00:56:58,319 --> 00:56:59,440
like

1605
00:56:59,440 --> 00:57:01,200
uh distributions of gradients that

1606
00:57:01,200 --> 00:57:03,280
they're like too wrong or too far so how

1607
00:57:03,280 --> 00:57:05,040
does this affect the accuracy or it

1608
00:57:05,040 --> 00:57:07,119
didn't mean practice it did not impact

1609
00:57:07,119 --> 00:57:08,480
the accuracy

1610
00:57:08,480 --> 00:57:10,400
uh i mean the main reason it did not

1611
00:57:10,400 --> 00:57:11,839
have impact accuracy we use this

1612
00:57:11,839 --> 00:57:13,760
clipping the ingredients are clipping so

1613
00:57:13,760 --> 00:57:15,680
even if i some something something i'll

1614
00:57:15,680 --> 00:57:17,520
clip it

1615
00:57:17,520 --> 00:57:19,280
right so yeah and

1616
00:57:19,280 --> 00:57:20,559
talking about the means we have plans

1617
00:57:20,559 --> 00:57:21,760
first of all the numbers are like

1618
00:57:21,760 --> 00:57:23,760
minuscule

1619
00:57:23,760 --> 00:57:24,880
and

1620
00:57:24,880 --> 00:57:27,680
the say at least whatever we felt or the

1621
00:57:27,680 --> 00:57:29,040
scale it was

1622
00:57:29,040 --> 00:57:32,640
and the second thing is that

1623
00:57:32,880 --> 00:57:35,599
at one level you you cannot control them

1624
00:57:35,599 --> 00:57:36,799
to be honest

1625
00:57:36,799 --> 00:57:38,640
because if i like flush my os and

1626
00:57:38,640 --> 00:57:40,559
install like a malicious os and do

1627
00:57:40,559 --> 00:57:42,319
something so

1628
00:57:42,319 --> 00:57:44,000
but the another important point is that

1629
00:57:44,000 --> 00:57:45,680
the behavior of the miss behave clients

1630
00:57:45,680 --> 00:57:47,200
does not impact the privacy of the will

1631
00:57:47,200 --> 00:57:49,839
be advanced

1632
00:57:50,720 --> 00:57:52,079
just like curious but you're saying that

1633
00:57:52,079 --> 00:57:54,880
not so many ones behaving

1634
00:57:54,880 --> 00:57:57,040
and also if the client participates very

1635
00:57:57,040 --> 00:57:58,480
often i mean you mentioned something

1636
00:57:58,480 --> 00:58:00,799
about a 24 hour lag

1637
00:58:00,799 --> 00:58:01,680
um

1638
00:58:01,680 --> 00:58:03,599
so if they keep using the same data so

1639
00:58:03,599 --> 00:58:05,040
do you need to have any notion of

1640
00:58:05,040 --> 00:58:06,720
explanation privacy under continual

1641
00:58:06,720 --> 00:58:08,400
observation this is this is completely

1642
00:58:08,400 --> 00:58:11,200
continuous observation

1643
00:58:11,200 --> 00:58:14,319
yeah see the complete user level

1644
00:58:15,200 --> 00:58:18,078
are there more questions

1645
00:58:18,720 --> 00:58:20,400
and the last question out of curiosity

1646
00:58:20,400 --> 00:58:23,359
why is spanish language what why spanish

1647
00:58:23,359 --> 00:58:25,680
language what is why is european spanish

1648
00:58:25,680 --> 00:58:28,160
yeah what about the other good question

1649
00:58:28,160 --> 00:58:30,240
actually i

1650
00:58:30,240 --> 00:58:33,280
i was not involved in that part but i

1651
00:58:33,280 --> 00:58:35,359
guess that was the group which was

1652
00:58:35,359 --> 00:58:37,599
interested in doing this at this point

1653
00:58:37,599 --> 00:58:40,319
starting with like

1654
00:58:42,079 --> 00:58:46,920
okay thank you so much it was great

