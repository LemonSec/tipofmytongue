1
00:00:43,840 --> 00:00:45,920
you

2
00:01:06,479 --> 00:01:07,840
okay everyone so let's continue with the

3
00:01:07,840 --> 00:01:09,920
session this is part of the contributor

4
00:01:09,920 --> 00:01:11,280
talk session the first one was in the

5
00:01:11,280 --> 00:01:13,280
morning and this is the second part

6
00:01:13,280 --> 00:01:15,680
and this is ari from boston university

7
00:01:15,680 --> 00:01:17,360
who is going to

8
00:01:17,360 --> 00:01:19,600
talk to us about the limits of probable

9
00:01:19,600 --> 00:01:22,320
security against model extraction thank

10
00:01:22,320 --> 00:01:23,280
you

11
00:01:23,280 --> 00:01:25,520
yeah thanks daniel and thanks to the

12
00:01:25,520 --> 00:01:28,240
rest of the organizers

13
00:01:28,240 --> 00:01:30,479
so this is uh based on a couple of

14
00:01:30,479 --> 00:01:31,600
papers

15
00:01:31,600 --> 00:01:34,479
the first is joint with ron canetti

16
00:01:34,479 --> 00:01:38,799
and both you can find on eprint

17
00:01:38,799 --> 00:01:41,920
so uh in a model extraction attack

18
00:01:41,920 --> 00:01:44,240
an adversary interacts with some

19
00:01:44,240 --> 00:01:46,720
interface to a machine learning model

20
00:01:46,720 --> 00:01:49,119
uh in an attempt to extract the model

21
00:01:49,119 --> 00:01:50,640
itself

22
00:01:50,640 --> 00:01:53,600
so in this case what i mean by that is

23
00:01:53,600 --> 00:01:55,360
the adversary will make some queries

24
00:01:55,360 --> 00:01:57,600
obtain the labels and

25
00:01:57,600 --> 00:02:00,079
then try to output some classifier that

26
00:02:00,079 --> 00:02:02,640
agrees on most of the points with the

27
00:02:02,640 --> 00:02:06,240
underlying classifier in the cloud

28
00:02:06,399 --> 00:02:10,080
so why steel models um well first of all

29
00:02:10,080 --> 00:02:11,440
uh

30
00:02:11,440 --> 00:02:13,440
confidential and proprietary models are

31
00:02:13,440 --> 00:02:14,400
often

32
00:02:14,400 --> 00:02:16,239
offered as machine learning as a service

33
00:02:16,239 --> 00:02:19,120
where the owner charges on a per query

34
00:02:19,120 --> 00:02:20,800
basis so

35
00:02:20,800 --> 00:02:24,160
anytime the client can infer some points

36
00:02:24,160 --> 00:02:26,319
or and for infer some labels at points

37
00:02:26,319 --> 00:02:28,480
it has not yet queried then it's kind of

38
00:02:28,480 --> 00:02:30,480
extracting some value away from the

39
00:02:30,480 --> 00:02:32,160
owner

40
00:02:32,160 --> 00:02:34,840
and then second i think maybe more

41
00:02:34,840 --> 00:02:37,360
interestingly uh model extraction can be

42
00:02:37,360 --> 00:02:40,640
seen to decrease security and privacy

43
00:02:40,640 --> 00:02:42,800
uh especially with respect to model

44
00:02:42,800 --> 00:02:46,560
inversion or adversarial example attacks

45
00:02:46,560 --> 00:02:49,360
uh so for example biggio at al

46
00:02:49,360 --> 00:02:51,680
show how to find adversarial examples in

47
00:02:51,680 --> 00:02:53,599
malware

48
00:02:53,599 --> 00:02:55,519
using model parameters

49
00:02:55,519 --> 00:02:57,760
and fredrickson i'll show how to do

50
00:02:57,760 --> 00:03:00,000
image reconstruction from a facial

51
00:03:00,000 --> 00:03:04,959
recognition model in this white box 7.

52
00:03:04,959 --> 00:03:07,440
so how do we defend against these model

53
00:03:07,440 --> 00:03:09,440
extraction attacks

54
00:03:09,440 --> 00:03:11,040
so one thing you could do is inject

55
00:03:11,040 --> 00:03:13,280
noise you could do this either

56
00:03:13,280 --> 00:03:15,760
independently for query or you could

57
00:03:15,760 --> 00:03:18,720
even modify the underlying model

58
00:03:18,720 --> 00:03:20,319
deliberately

59
00:03:20,319 --> 00:03:22,560
and the point of this type of defense is

60
00:03:22,560 --> 00:03:24,080
to limit the amount of information

61
00:03:24,080 --> 00:03:26,480
that's revealed per query

62
00:03:26,480 --> 00:03:28,239
and it indeed does make it harder to

63
00:03:28,239 --> 00:03:31,599
extract but it's unsatisfactory for some

64
00:03:31,599 --> 00:03:33,360
very important applications like for

65
00:03:33,360 --> 00:03:35,200
example

66
00:03:35,200 --> 00:03:37,440
when accuracy is is critical so

67
00:03:37,440 --> 00:03:41,920
autonomous driving or medical diagnosis

68
00:03:42,480 --> 00:03:44,159
another thing you could do is use

69
00:03:44,159 --> 00:03:47,760
cryptography and there's a bunch of

70
00:03:47,760 --> 00:03:49,680
proposals based off of secure function

71
00:03:49,680 --> 00:03:50,959
evaluation

72
00:03:50,959 --> 00:03:53,360
so in secure function evaluation alice

73
00:03:53,360 --> 00:03:55,920
the model owner and bob some client

74
00:03:55,920 --> 00:03:57,120
would

75
00:03:57,120 --> 00:03:59,040
be able to jointly compute the output of

76
00:03:59,040 --> 00:04:01,680
the model on bob's input in such a way

77
00:04:01,680 --> 00:04:02,560
where

78
00:04:02,560 --> 00:04:04,080
alice would learn nothing about bob's

79
00:04:04,080 --> 00:04:06,400
input and bob would learn nothing about

80
00:04:06,400 --> 00:04:09,280
model but crucially nothing beyond what

81
00:04:09,280 --> 00:04:10,959
is revealed by

82
00:04:10,959 --> 00:04:13,519
the output of this joint computation

83
00:04:13,519 --> 00:04:15,519
so this is great for secure function

84
00:04:15,519 --> 00:04:17,040
evaluation but

85
00:04:17,040 --> 00:04:19,759
it's not sufficient for model extraction

86
00:04:19,759 --> 00:04:21,918
because we don't get any guarantees for

87
00:04:21,918 --> 00:04:23,280
the case that

88
00:04:23,280 --> 00:04:25,759
bob is allowed to repeatedly interact

89
00:04:25,759 --> 00:04:27,360
with alice

90
00:04:27,360 --> 00:04:29,680
with the model owner and build a large

91
00:04:29,680 --> 00:04:33,360
data set to then infer from

92
00:04:33,440 --> 00:04:35,440
and similarly program obfuscation

93
00:04:35,440 --> 00:04:37,280
doesn't fully solve the problem because

94
00:04:37,280 --> 00:04:40,960
it also preserves functionality

95
00:04:41,680 --> 00:04:44,479
and the point here is that

96
00:04:44,479 --> 00:04:47,680
basically the ideal world in these uh

97
00:04:47,680 --> 00:04:49,759
cryptographic solutions is is insecure

98
00:04:49,759 --> 00:04:51,520
against model extraction

99
00:04:51,520 --> 00:04:53,840
and um this was point out pointed out by

100
00:04:53,840 --> 00:04:55,040
the node

101
00:04:55,040 --> 00:04:57,120
by clinton athan and his talk at this

102
00:04:57,120 --> 00:05:00,080
workshop last year

103
00:05:00,320 --> 00:05:02,639
so a third type of defense which is

104
00:05:02,639 --> 00:05:05,120
going to be the focus of this talk

105
00:05:05,120 --> 00:05:07,039
is what i like to call an observational

106
00:05:07,039 --> 00:05:09,199
model extraction defense or omed for

107
00:05:09,199 --> 00:05:10,560
short

108
00:05:10,560 --> 00:05:12,160
so in an omed

109
00:05:12,160 --> 00:05:14,240
the server who holds the model or the

110
00:05:14,240 --> 00:05:16,080
model owner would take the queries which

111
00:05:16,080 --> 00:05:18,560
are requested by the client

112
00:05:18,560 --> 00:05:19,680
and

113
00:05:19,680 --> 00:05:21,440
forward them to some kind of analysis

114
00:05:21,440 --> 00:05:25,120
algorithm the omed which would then uh

115
00:05:25,120 --> 00:05:27,280
either accept or reject

116
00:05:27,280 --> 00:05:30,080
and in the case of acceptance

117
00:05:30,080 --> 00:05:32,639
then the omed would advise the owner to

118
00:05:32,639 --> 00:05:34,960
forward the labels back to the client

119
00:05:34,960 --> 00:05:38,880
and on rejection would say server ignore

120
00:05:38,880 --> 00:05:41,360
the client

121
00:05:43,199 --> 00:05:43,919
so

122
00:05:43,919 --> 00:05:46,000
the point of this

123
00:05:46,000 --> 00:05:46,960
omen

124
00:05:46,960 --> 00:05:50,160
which many practical instances exist in

125
00:05:50,160 --> 00:05:52,400
the literature is and it's often cited

126
00:05:52,400 --> 00:05:55,120
that uh the point is to determine if a

127
00:05:55,120 --> 00:05:58,400
client is acting adversarially or benign

128
00:05:58,400 --> 00:05:59,680
um

129
00:05:59,680 --> 00:06:02,639
and i don't i'm not going to talk about

130
00:06:02,639 --> 00:06:05,039
all the plenty of different

131
00:06:05,039 --> 00:06:06,560
instantiations that exist in the

132
00:06:06,560 --> 00:06:08,400
literature um

133
00:06:08,400 --> 00:06:11,600
they work well in practice

134
00:06:11,600 --> 00:06:13,520
and they use very different techniques

135
00:06:13,520 --> 00:06:15,600
but what i will say is that

136
00:06:15,600 --> 00:06:17,120
they're all kind of boiled down to

137
00:06:17,120 --> 00:06:19,360
efficient statistical tests

138
00:06:19,360 --> 00:06:21,840
uh on client query distributions

139
00:06:21,840 --> 00:06:23,840
and despite working well in practice and

140
00:06:23,840 --> 00:06:25,600
being very lightweight

141
00:06:25,600 --> 00:06:27,360
they um they don't have any known

142
00:06:27,360 --> 00:06:29,280
security guarantees at least not the

143
00:06:29,280 --> 00:06:31,680
type not the type of provable security

144
00:06:31,680 --> 00:06:33,039
guarantees you'd expect from a

145
00:06:33,039 --> 00:06:35,840
cryptographic solution

146
00:06:37,039 --> 00:06:39,840
so that being said uh it's not really

147
00:06:39,840 --> 00:06:42,160
clear how to even define security for

148
00:06:42,160 --> 00:06:42,880
this

149
00:06:42,880 --> 00:06:44,800
this kind of problem so you could try

150
00:06:44,800 --> 00:06:48,000
something that's zero knowledge style

151
00:06:48,000 --> 00:06:50,479
but really this is too harsh because at

152
00:06:50,479 --> 00:06:52,720
the end of the day the client lead needs

153
00:06:52,720 --> 00:06:54,560
to be able to learn something about the

154
00:06:54,560 --> 00:06:56,800
model

155
00:06:56,800 --> 00:06:57,840
you know

156
00:06:57,840 --> 00:06:59,680
in the example of machine learning as a

157
00:06:59,680 --> 00:07:01,280
service otherwise

158
00:07:01,280 --> 00:07:03,360
the client may just

159
00:07:03,360 --> 00:07:05,759
not use the model if it knows it's not

160
00:07:05,759 --> 00:07:08,880
going to learn anything going into it

161
00:07:08,880 --> 00:07:11,039
so something more realistic which is a

162
00:07:11,039 --> 00:07:13,360
little weaker is is uh

163
00:07:13,360 --> 00:07:14,800
maybe the guarantee that the client

164
00:07:14,800 --> 00:07:16,800
learns nothing beyond what can be

165
00:07:16,800 --> 00:07:18,800
efficiently deduced from some random

166
00:07:18,800 --> 00:07:20,880
examples about the

167
00:07:20,880 --> 00:07:22,400
about the uh

168
00:07:22,400 --> 00:07:24,080
the model

169
00:07:24,080 --> 00:07:24,800
so

170
00:07:24,800 --> 00:07:27,120
this is a bit more interesting

171
00:07:27,120 --> 00:07:28,319
and

172
00:07:28,319 --> 00:07:29,280
i think

173
00:07:29,280 --> 00:07:32,000
my point is that uh thinking back to the

174
00:07:32,000 --> 00:07:33,280
omeds

175
00:07:33,280 --> 00:07:35,680
what they're really trying to do in this

176
00:07:35,680 --> 00:07:38,479
paradigm of acceptance first rejecting

177
00:07:38,479 --> 00:07:40,639
is to confine the

178
00:07:40,639 --> 00:07:44,000
client to some specific distribution

179
00:07:44,000 --> 00:07:46,000
and so really one can see that

180
00:07:46,000 --> 00:07:48,000
implicitly the elements are already

181
00:07:48,000 --> 00:07:50,479
using this type of security model and

182
00:07:50,479 --> 00:07:52,479
the omits themselves are used to kind of

183
00:07:52,479 --> 00:07:54,879
enforce it enforce this type of

184
00:07:54,879 --> 00:07:58,599
uh paradigm

185
00:07:59,039 --> 00:08:00,639
so um

186
00:08:00,639 --> 00:08:02,800
how can we actually get something

187
00:08:02,800 --> 00:08:05,759
related to making extracting models hard

188
00:08:05,759 --> 00:08:07,280
out of this

189
00:08:07,280 --> 00:08:10,160
i'll think of a client as

190
00:08:10,160 --> 00:08:12,000
one who just picks some distribution

191
00:08:12,000 --> 00:08:13,759
over queries

192
00:08:13,759 --> 00:08:16,080
and in this case the omed is

193
00:08:16,080 --> 00:08:18,080
basically a distribution tester for some

194
00:08:18,080 --> 00:08:20,400
kind of honesty property

195
00:08:20,400 --> 00:08:22,319
so kind of inspired by interactive

196
00:08:22,319 --> 00:08:24,840
proofs something we might want is

197
00:08:24,840 --> 00:08:27,199
completeness which is when honestly

198
00:08:27,199 --> 00:08:29,599
distributed queries would be

199
00:08:29,599 --> 00:08:32,000
accepted and soundness

200
00:08:32,000 --> 00:08:33,839
when adversarially distributed

201
00:08:33,839 --> 00:08:37,200
distributed queries would be rejected

202
00:08:37,200 --> 00:08:39,200
and here so i'm still thinking of this

203
00:08:39,200 --> 00:08:41,120
abstract honesty property as some kind

204
00:08:41,120 --> 00:08:43,919
of or some subset of

205
00:08:43,919 --> 00:08:48,000
all distributions over the input space

206
00:08:50,800 --> 00:08:53,440
so once we have if we were to have and

207
00:08:53,440 --> 00:08:55,279
sound omeds then the question that

208
00:08:55,279 --> 00:08:57,920
remains is well how do you choose p

209
00:08:57,920 --> 00:09:00,080
um

210
00:09:00,080 --> 00:09:01,519
so we want something that makes the

211
00:09:01,519 --> 00:09:03,279
distribution or we want some kind of

212
00:09:03,279 --> 00:09:05,200
distribution that makes extraction hard

213
00:09:05,200 --> 00:09:06,399
and

214
00:09:06,399 --> 00:09:08,959
i argue that the natural thing to do is

215
00:09:08,959 --> 00:09:11,200
to use hardness assumptions about pac

216
00:09:11,200 --> 00:09:15,760
learning uh to pick this distribution

217
00:09:15,760 --> 00:09:17,760
um

218
00:09:17,760 --> 00:09:20,560
so what we can obtain in our work is a

219
00:09:20,560 --> 00:09:24,399
type of provable security lemma where

220
00:09:24,399 --> 00:09:26,480
if we have a complete and sound omid and

221
00:09:26,480 --> 00:09:28,080
we're willing to make some hardness

222
00:09:28,080 --> 00:09:30,080
assumption about pac learning

223
00:09:30,080 --> 00:09:31,040
then

224
00:09:31,040 --> 00:09:32,800
you can show that

225
00:09:32,800 --> 00:09:35,040
when the models come from this

226
00:09:35,040 --> 00:09:37,360
the underlying model comes from this

227
00:09:37,360 --> 00:09:40,240
uh class then it's basically

228
00:09:40,240 --> 00:09:42,959
cryptographically hard to extract it

229
00:09:42,959 --> 00:09:45,200
and um the intuition here is that when

230
00:09:45,200 --> 00:09:46,959
the client queries honestly then the

231
00:09:46,959 --> 00:09:48,800
model is protected by the hardness

232
00:09:48,800 --> 00:09:50,640
assumption

233
00:09:50,640 --> 00:09:52,880
and

234
00:09:53,680 --> 00:09:54,800
on the other hand when the client

235
00:09:54,800 --> 00:09:57,600
queries adversarially then the model is

236
00:09:57,600 --> 00:09:59,519
protected by the soundness of the omit

237
00:09:59,519 --> 00:10:02,560
and the fact that in that case

238
00:10:02,560 --> 00:10:04,480
the the adversary never would have seen

239
00:10:04,480 --> 00:10:07,040
the labels

240
00:10:08,399 --> 00:10:09,440
uh so

241
00:10:09,440 --> 00:10:12,000
can we actually build these omets um

242
00:10:12,000 --> 00:10:14,560
after all we're competing with uh some

243
00:10:14,560 --> 00:10:16,240
practical lightweight instantiations

244
00:10:16,240 --> 00:10:18,720
that are very efficient

245
00:10:18,720 --> 00:10:21,040
and so the main result in this work is

246
00:10:21,040 --> 00:10:22,000
that

247
00:10:22,000 --> 00:10:24,880
the answer is no

248
00:10:25,360 --> 00:10:27,680
no efficient omits when the model is a

249
00:10:27,680 --> 00:10:29,519
decision tree and that's assuming that

250
00:10:29,519 --> 00:10:31,920
learning parity with noise assumption

251
00:10:31,920 --> 00:10:34,319
holds

252
00:10:34,399 --> 00:10:36,560
so the way we prove this is by making a

253
00:10:36,560 --> 00:10:39,360
connection to the covert learning

254
00:10:39,360 --> 00:10:42,000
model from the first paper

255
00:10:42,000 --> 00:10:45,120
so in the covert learning model the

256
00:10:45,120 --> 00:10:46,240
kind of situation that we're

257
00:10:46,240 --> 00:10:49,040
entertaining is when

258
00:10:49,040 --> 00:10:51,279
a learner is

259
00:10:51,279 --> 00:10:52,800
working in the learning with membership

260
00:10:52,800 --> 00:10:55,040
queries uh model

261
00:10:55,040 --> 00:10:56,320
and

262
00:10:56,320 --> 00:10:58,240
with a twist so the learner is allowed

263
00:10:58,240 --> 00:10:59,440
to query

264
00:10:59,440 --> 00:11:01,839
some function and to try to obtain an

265
00:11:01,839 --> 00:11:03,760
approximation but at the same time

266
00:11:03,760 --> 00:11:06,240
there's an adversary who is viewing all

267
00:11:06,240 --> 00:11:08,160
these queries and responses

268
00:11:08,160 --> 00:11:12,079
and the game for the learner is to

269
00:11:12,079 --> 00:11:14,399
have a pac learning guarantee as usual

270
00:11:14,399 --> 00:11:17,279
but also prevent adversary from

271
00:11:17,279 --> 00:11:18,480
learning

272
00:11:18,480 --> 00:11:20,079
at the same time

273
00:11:20,079 --> 00:11:21,839
and um

274
00:11:21,839 --> 00:11:24,000
so the way this is formalized is is

275
00:11:24,000 --> 00:11:24,959
using

276
00:11:24,959 --> 00:11:27,279
uh the simulation paradigm and basically

277
00:11:27,279 --> 00:11:29,200
the guarantee that the learner wants to

278
00:11:29,200 --> 00:11:32,480
achieve is that the transcript over the

279
00:11:32,480 --> 00:11:35,600
queries and labels is efficiently

280
00:11:35,600 --> 00:11:38,320
simulatable using only random examples

281
00:11:38,320 --> 00:11:40,160
to the function

282
00:11:40,160 --> 00:11:42,560
so this is uh

283
00:11:42,560 --> 00:11:44,240
often seen kind of in a qualitative

284
00:11:44,240 --> 00:11:46,320
sense to be much less information than

285
00:11:46,320 --> 00:11:49,440
full oracle access

286
00:11:49,839 --> 00:11:50,959
and so

287
00:11:50,959 --> 00:11:51,839
the

288
00:11:51,839 --> 00:11:53,600
idea here is that the covert learning

289
00:11:53,600 --> 00:11:56,959
algorithms basically fool the omnis

290
00:11:56,959 --> 00:11:58,240
um

291
00:11:58,240 --> 00:11:59,680
and

292
00:11:59,680 --> 00:12:01,920
this is seen concretely in the case that

293
00:12:01,920 --> 00:12:04,160
you have some kind of or some complete

294
00:12:04,160 --> 00:12:06,720
omed then it must have very bad

295
00:12:06,720 --> 00:12:08,720
soundness parameters

296
00:12:08,720 --> 00:12:10,560
and that's because

297
00:12:10,560 --> 00:12:12,160
the covert learning algorithm can

298
00:12:12,160 --> 00:12:15,360
essentially learn extract the model from

299
00:12:15,360 --> 00:12:17,360
using a query distribution which has the

300
00:12:17,360 --> 00:12:19,120
adversarial property

301
00:12:19,120 --> 00:12:21,040
but is computationally distinguishable

302
00:12:21,040 --> 00:12:22,959
from

303
00:12:22,959 --> 00:12:24,720
distributions that have the honesty

304
00:12:24,720 --> 00:12:25,760
property

305
00:12:25,760 --> 00:12:28,560
so in that case what happens is that um

306
00:12:28,560 --> 00:12:32,639
despite the decline acting adversarially

307
00:12:32,639 --> 00:12:34,639
uh from from the perspective of any

308
00:12:34,639 --> 00:12:38,160
efficient omad um it will not be able to

309
00:12:38,160 --> 00:12:40,079
basically classify it as adversarial or

310
00:12:40,079 --> 00:12:42,639
benign so assuming that

311
00:12:42,639 --> 00:12:44,160
the omed

312
00:12:44,160 --> 00:12:47,040
accepts a large percentage of

313
00:12:47,040 --> 00:12:50,320
of honestly distributed uh queer uh

314
00:12:50,320 --> 00:12:53,839
query sets then it must also

315
00:12:53,839 --> 00:12:55,680
accept with high probability this

316
00:12:55,680 --> 00:12:57,040
adversarial

317
00:12:57,040 --> 00:13:00,000
uh distribution

318
00:13:01,040 --> 00:13:02,079
um so

319
00:13:02,079 --> 00:13:04,880
the point is here that uh our provable

320
00:13:04,880 --> 00:13:06,959
security dilemma required

321
00:13:06,959 --> 00:13:09,680
uh complete and sound omens so if you

322
00:13:09,680 --> 00:13:10,880
believe in the learning parody with

323
00:13:10,880 --> 00:13:12,480
noise assumption then you can

324
00:13:12,480 --> 00:13:15,279
instantiate that lemma

325
00:13:15,279 --> 00:13:17,760
and yeah i also want to mention that

326
00:13:17,760 --> 00:13:19,200
um

327
00:13:19,200 --> 00:13:21,200
in his talk last year at this workshop

328
00:13:21,200 --> 00:13:23,839
vinod um

329
00:13:23,839 --> 00:13:25,200
kind of

330
00:13:25,200 --> 00:13:27,200
dreamt up this the scenario and he

331
00:13:27,200 --> 00:13:30,160
actually came up with an attack

332
00:13:30,160 --> 00:13:32,320
that uh i think at the time wasn't known

333
00:13:32,320 --> 00:13:35,839
as a covert learning attack but it was

334
00:13:36,160 --> 00:13:38,240
where the advantage there was that

335
00:13:38,240 --> 00:13:39,920
actually

336
00:13:39,920 --> 00:13:41,920
uh the attack worked against unbounded

337
00:13:41,920 --> 00:13:44,160
omits because the query distribution was

338
00:13:44,160 --> 00:13:46,240
statistically uh

339
00:13:46,240 --> 00:13:48,639
indistinguishable from random

340
00:13:48,639 --> 00:13:50,480
but on the other hand it was working

341
00:13:50,480 --> 00:13:54,160
against a much more contrived

342
00:13:54,160 --> 00:13:56,399
set of machine learning models being

343
00:13:56,399 --> 00:13:59,279
noisy linear equations mod q

344
00:13:59,279 --> 00:14:02,000
so hours is the first that's uh working

345
00:14:02,000 --> 00:14:03,680
against a much larger

346
00:14:03,680 --> 00:14:06,320
kind of practically inspired class

347
00:14:06,320 --> 00:14:10,079
being polynomial-sized decision trees

348
00:14:10,240 --> 00:14:13,360
so i think um in the interest of time uh

349
00:14:13,360 --> 00:14:14,880
i'll stop there

350
00:14:14,880 --> 00:14:18,000
but i want to encourage people to

351
00:14:18,000 --> 00:14:19,279
approach me

352
00:14:19,279 --> 00:14:21,839
to talk about any of the many different

353
00:14:21,839 --> 00:14:23,440
interesting directions that arise from

354
00:14:23,440 --> 00:14:24,800
here

355
00:14:24,800 --> 00:14:27,359
thank you

356
00:14:29,440 --> 00:14:31,120
other questions ferrari we definitely

357
00:14:31,120 --> 00:14:33,920
have time for questions

358
00:14:37,920 --> 00:14:40,000
so uh i would like to ask you a question

359
00:14:40,000 --> 00:14:41,440
you talk about all these different

360
00:14:41,440 --> 00:14:44,639
defenses for model extraction and i

361
00:14:44,639 --> 00:14:46,079
understand that we don't have probable

362
00:14:46,079 --> 00:14:47,519
security guarantees for them but let's

363
00:14:47,519 --> 00:14:49,440
say that we are confident that certain

364
00:14:49,440 --> 00:14:51,199
solution is secure enough

365
00:14:51,199 --> 00:14:52,160
how

366
00:14:52,160 --> 00:14:53,920
let's say for instance in the context of

367
00:14:53,920 --> 00:14:56,240
secure multi-party computation how npc

368
00:14:56,240 --> 00:14:58,000
friendly they are because at the end of

369
00:14:58,000 --> 00:14:59,440
the day you need to implement it in that

370
00:14:59,440 --> 00:15:02,000
box that does secure things so if they

371
00:15:02,000 --> 00:15:03,360
are super complex it's not going to be

372
00:15:03,360 --> 00:15:05,440
super efficient those

373
00:15:05,440 --> 00:15:08,160
yeah that's a great question um i'm not

374
00:15:08,160 --> 00:15:09,920
sure i really know the answer

375
00:15:09,920 --> 00:15:11,839
so i think when it comes to the

376
00:15:11,839 --> 00:15:14,800
practical omeds that i kind of briefly

377
00:15:14,800 --> 00:15:16,720
cited on the on the screen

378
00:15:16,720 --> 00:15:18,399
um

379
00:15:18,399 --> 00:15:19,519
they are

380
00:15:19,519 --> 00:15:20,720
from what i can tell they're pretty

381
00:15:20,720 --> 00:15:22,639
lightweight and they don't

382
00:15:22,639 --> 00:15:25,360
they they kind of work just on top of

383
00:15:25,360 --> 00:15:27,279
the if you think of like a machine

384
00:15:27,279 --> 00:15:29,680
learning as a service scenario they work

385
00:15:29,680 --> 00:15:31,680
just directly on top of

386
00:15:31,680 --> 00:15:34,240
the the model um so it's almost like a

387
00:15:34,240 --> 00:15:36,160
you can think of it as like a

388
00:15:36,160 --> 00:15:38,959
third-party algorithm uh it doesn't need

389
00:15:38,959 --> 00:15:41,600
to interact at all um so

390
00:15:41,600 --> 00:15:43,440
yeah i i don't really know the answer

391
00:15:43,440 --> 00:15:44,800
but but you were talking for example

392
00:15:44,800 --> 00:15:47,920
about checking the humming waves um yeah

393
00:15:47,920 --> 00:15:50,160
inputs or stuff like that like this is a

394
00:15:50,160 --> 00:15:52,320
non-linear operation that right in the

395
00:15:52,320 --> 00:15:54,240
light of the whole model may not be that

396
00:15:54,240 --> 00:15:56,720
bad because the model already is quite

397
00:15:56,720 --> 00:15:58,320
expensive to compute but

398
00:15:58,320 --> 00:16:00,000
if it is a super uh

399
00:16:00,000 --> 00:16:01,839
it involves exponentiations and so on

400
00:16:01,839 --> 00:16:03,839
like it may not be very good anyway i

401
00:16:03,839 --> 00:16:05,199
mean it does not yeah like i'm just

402
00:16:05,199 --> 00:16:07,279
saying so if i remember correctly so the

403
00:16:07,279 --> 00:16:09,360
hamming way is referring to

404
00:16:09,360 --> 00:16:10,240
um

405
00:16:10,240 --> 00:16:12,800
testing the the hamming distances

406
00:16:12,800 --> 00:16:15,440
between queries from the clients

407
00:16:15,440 --> 00:16:18,079
so that can happen prior to

408
00:16:18,079 --> 00:16:19,920
any evaluation

409
00:16:19,920 --> 00:16:21,360
on the model

410
00:16:21,360 --> 00:16:22,480
i understand your

411
00:16:22,480 --> 00:16:25,040
question correctly so no but like if if

412
00:16:25,040 --> 00:16:26,959
you have for example uh machine learning

413
00:16:26,959 --> 00:16:28,320
as a service

414
00:16:28,320 --> 00:16:29,440
system

415
00:16:29,440 --> 00:16:31,440
and you have three two non-colluding

416
00:16:31,440 --> 00:16:32,800
servers who are gonna maintain the

417
00:16:32,800 --> 00:16:34,880
system they have to check the validity

418
00:16:34,880 --> 00:16:37,040
or they have to check this p property

419
00:16:37,040 --> 00:16:39,199
from the input line and they they

420
00:16:39,199 --> 00:16:41,759
the input is going to be secret right

421
00:16:41,759 --> 00:16:43,120
so you cannot see what the input is and

422
00:16:43,120 --> 00:16:45,199
you need to do this check securely

423
00:16:45,199 --> 00:16:48,079
yeah okay so this uh misunderstanding

424
00:16:48,079 --> 00:16:48,800
yeah

425
00:16:48,800 --> 00:16:50,720
in this case so i mentioned secure

426
00:16:50,720 --> 00:16:52,079
function evaluation which has the

427
00:16:52,079 --> 00:16:53,440
guarantee that

428
00:16:53,440 --> 00:16:55,839
bob's input is not revealed

429
00:16:55,839 --> 00:16:57,440
i think normally when it comes to model

430
00:16:57,440 --> 00:17:00,160
extraction uh

431
00:17:00,160 --> 00:17:01,839
it's we're not really caring about

432
00:17:01,839 --> 00:17:03,759
whether the input is revealed to the

433
00:17:03,759 --> 00:17:05,119
server

434
00:17:05,119 --> 00:17:07,599
so but if your question is do those

435
00:17:07,599 --> 00:17:09,439
things work together

436
00:17:09,439 --> 00:17:10,319
uh

437
00:17:10,319 --> 00:17:12,480
that's a good question i i don't and i

438
00:17:12,480 --> 00:17:13,599
don't know that that would be a good

439
00:17:13,599 --> 00:17:16,559
thing to to look at yeah

440
00:17:16,559 --> 00:17:19,520
any question for you

441
00:17:20,079 --> 00:17:23,599
please repeat the question

442
00:17:31,200 --> 00:17:33,919
so um it works

443
00:17:33,919 --> 00:17:36,160
the decision tree the model itself

444
00:17:36,160 --> 00:17:38,000
doesn't

445
00:17:38,000 --> 00:17:40,240
need to be um

446
00:17:40,240 --> 00:17:41,760
coming from any distribution so the

447
00:17:41,760 --> 00:17:44,080
model can be any decision tree

448
00:17:44,080 --> 00:17:45,440
you might want to make a hardness

449
00:17:45,440 --> 00:17:47,120
assumption which is average case so that

450
00:17:47,120 --> 00:17:49,200
you can get a guarantee that

451
00:17:49,200 --> 00:17:52,480
most models are hard to extract

452
00:17:52,480 --> 00:17:53,679
but um

453
00:17:53,679 --> 00:17:55,679
the actual machine learning model that

454
00:17:55,679 --> 00:17:57,039
needs to be extracted can be

455
00:17:57,039 --> 00:18:00,679
adversarially chosen

456
00:18:11,840 --> 00:18:15,120
on model extraction uh i won't recommend

457
00:18:15,120 --> 00:18:17,919
a specific survey but i think that all

458
00:18:17,919 --> 00:18:20,000
of the talks

459
00:18:20,000 --> 00:18:22,720
uh on specific especially the defenses

460
00:18:22,720 --> 00:18:25,520
so there's a ton of um attacks papers on

461
00:18:25,520 --> 00:18:26,559
attacks

462
00:18:26,559 --> 00:18:29,200
and they were all any talk or any paper

463
00:18:29,200 --> 00:18:31,280
the introduction i think would would do

464
00:18:31,280 --> 00:18:34,320
great um yeah

465
00:18:34,320 --> 00:18:38,439
okay so listening gary again

466
00:18:43,200 --> 00:18:46,240
um okay and the next speaker can

467
00:18:46,240 --> 00:18:48,720
come kindly thank you i also want to

468
00:18:48,720 --> 00:18:50,720
quickly remind the audience this morning

469
00:18:50,720 --> 00:18:52,240
we had to

470
00:18:52,240 --> 00:18:54,480
omit the first stock

471
00:18:54,480 --> 00:18:58,559
and the speaker is actually available so

472
00:18:58,559 --> 00:19:00,000
we're going to

473
00:19:00,000 --> 00:19:01,840
make a slightly longer session today at

474
00:19:01,840 --> 00:19:03,679
the end to accommodate for the first

475
00:19:03,679 --> 00:19:06,679
stock

