1
00:00:01,120 --> 00:00:05,520
hi so i'm nicholas and i'm going to

2
00:00:03,600 --> 00:00:08,000
present our paper on crypt analytic

3
00:00:05,520 --> 00:00:09,678
extraction of neural network models

4
00:00:08,000 --> 00:00:11,440
this is joint work with my co-authors

5
00:00:09,679 --> 00:00:13,280
matthew and elia who worked on this with

6
00:00:11,440 --> 00:00:15,518
me when they were at google

7
00:00:13,280 --> 00:00:17,198
so the basic question in our paper is we

8
00:00:15,519 --> 00:00:20,640
have some machine learning model

9
00:00:17,199 --> 00:00:21,439
which exists in some black box and we're

10
00:00:20,640 --> 00:00:23,600
allowed to

11
00:00:21,439 --> 00:00:25,198
make queries of this model with

12
00:00:23,600 --> 00:00:27,680
arbitrary images

13
00:00:25,199 --> 00:00:29,119
and observe the arbitrary outputs and

14
00:00:27,680 --> 00:00:30,480
the parameters of this model have

15
00:00:29,119 --> 00:00:32,880
already been loaded into it

16
00:00:30,480 --> 00:00:35,440
but we don't have direct access to those

17
00:00:32,880 --> 00:00:38,079
and so the question is as the adversary

18
00:00:35,440 --> 00:00:39,599
if we're given access to make arbitrary

19
00:00:38,079 --> 00:00:42,000
image requests

20
00:00:39,600 --> 00:00:43,360
and are aware of the type of model

21
00:00:42,000 --> 00:00:45,200
that's being deployed

22
00:00:43,360 --> 00:00:46,719
and are able to see the outputs of the

23
00:00:45,200 --> 00:00:49,600
model but don't

24
00:00:46,719 --> 00:00:50,719
know what the actual parameters are can

25
00:00:49,600 --> 00:00:52,480
we

26
00:00:50,719 --> 00:00:54,960
solve for these parameters given these

27
00:00:52,480 --> 00:00:57,120
other variables

28
00:00:54,960 --> 00:00:58,320
now the reason this is important is that

29
00:00:57,120 --> 00:01:00,959
machine learning models

30
00:00:58,320 --> 00:01:04,159
are very very expensive to train so the

31
00:01:00,960 --> 00:01:06,720
recently released openai's gpt-3 model

32
00:01:04,159 --> 00:01:08,400
took probably roughly 10 million dollars

33
00:01:06,720 --> 00:01:09,600
to obtain these parameters that are

34
00:01:08,400 --> 00:01:12,080
loaded into it

35
00:01:09,600 --> 00:01:13,919
but everything else like the

36
00:01:12,080 --> 00:01:16,000
architecture was actually fairly easy to

37
00:01:13,920 --> 00:01:18,159
be specified ahead of time

38
00:01:16,000 --> 00:01:19,600
and so it's very valuable for companies

39
00:01:18,159 --> 00:01:20,400
to be able to protect the intellectual

40
00:01:19,600 --> 00:01:23,039
property

41
00:01:20,400 --> 00:01:24,479
of the their parameters and so the

42
00:01:23,040 --> 00:01:25,200
question is can from the adversary's

43
00:01:24,479 --> 00:01:28,560
perspective

44
00:01:25,200 --> 00:01:29,520
can we steal them so

45
00:01:28,560 --> 00:01:31,520
we're going to try and answer this

46
00:01:29,520 --> 00:01:33,280
question given query access to our

47
00:01:31,520 --> 00:01:36,320
neural network can we extract these

48
00:01:33,280 --> 00:01:38,079
parameters from the model

49
00:01:36,320 --> 00:01:39,679
now there are two ways you can view this

50
00:01:38,079 --> 00:01:41,600
problem the first way

51
00:01:39,680 --> 00:01:42,799
is what most prior work has done and

52
00:01:41,600 --> 00:01:44,158
studied this as a machine learning

53
00:01:42,799 --> 00:01:45,920
problem

54
00:01:44,159 --> 00:01:47,200
neural networks are types of functions

55
00:01:45,920 --> 00:01:48,399
machine learning is good at function

56
00:01:47,200 --> 00:01:50,159
approximation

57
00:01:48,399 --> 00:01:52,720
so let's treat the neural network that

58
00:01:50,159 --> 00:01:54,960
we have access to as some supervisor

59
00:01:52,720 --> 00:01:56,320
and use this to supervise the collection

60
00:01:54,960 --> 00:01:58,240
of some new data set we're going to

61
00:01:56,320 --> 00:02:00,000
train a new model in this data set

62
00:01:58,240 --> 00:02:01,280
and use this in order to solve our

63
00:02:00,000 --> 00:02:04,079
problem

64
00:02:01,280 --> 00:02:05,360
and that works reasonably well what

65
00:02:04,079 --> 00:02:07,199
we're going to try and do is we're going

66
00:02:05,360 --> 00:02:09,520
to try and do a different approach

67
00:02:07,200 --> 00:02:11,599
and solve this as a direct mathematical

68
00:02:09,520 --> 00:02:13,280
problem treat the neural network as a

69
00:02:11,599 --> 00:02:14,640
sequence of functions that we can

70
00:02:13,280 --> 00:02:16,560
analyze directly

71
00:02:14,640 --> 00:02:18,480
and try and actually recover the weights

72
00:02:16,560 --> 00:02:20,000
that way and the reason why we're going

73
00:02:18,480 --> 00:02:20,480
to do this is that this is going to let

74
00:02:20,000 --> 00:02:22,959
us

75
00:02:20,480 --> 00:02:24,879
actually recover nearly identical models

76
00:02:22,959 --> 00:02:26,879
instead of just a similar model

77
00:02:24,879 --> 00:02:29,040
that solves a similar task reasonably

78
00:02:26,879 --> 00:02:31,840
well

79
00:02:29,040 --> 00:02:34,319
so this is our question given query

80
00:02:31,840 --> 00:02:36,800
access can we extract a neural network

81
00:02:34,319 --> 00:02:37,440
and basically the answer to our paper is

82
00:02:36,800 --> 00:02:39,680
says

83
00:02:37,440 --> 00:02:41,680
yes we are able to do this with various

84
00:02:39,680 --> 00:02:44,480
caveats

85
00:02:41,680 --> 00:02:45,519
okay so in order to understand our work

86
00:02:44,480 --> 00:02:47,599
i need to give a little bit of

87
00:02:45,519 --> 00:02:49,440
background on how neural networks work

88
00:02:47,599 --> 00:02:50,720
for that fortunately just about a minute

89
00:02:49,440 --> 00:02:53,280
is sufficient

90
00:02:50,720 --> 00:02:54,560
so neural networks are a sequence of

91
00:02:53,280 --> 00:02:57,120
linear layers

92
00:02:54,560 --> 00:02:59,360
with nonlinear activation functions

93
00:02:57,120 --> 00:03:02,080
where each layer has a bunch of neurons

94
00:02:59,360 --> 00:03:03,760
so the way this works is i have some

95
00:03:02,080 --> 00:03:05,440
values that go into the input this is

96
00:03:03,760 --> 00:03:06,720
maybe the bits of an image

97
00:03:05,440 --> 00:03:08,800
and then the neural network will

98
00:03:06,720 --> 00:03:11,359
propagate these values through

99
00:03:08,800 --> 00:03:12,640
process them at the neurons apply some

100
00:03:11,360 --> 00:03:15,200
nonlinearity

101
00:03:12,640 --> 00:03:16,399
repeat this process layer after layer

102
00:03:15,200 --> 00:03:18,879
until finally

103
00:03:16,400 --> 00:03:21,200
we end up with some set of values which

104
00:03:18,879 --> 00:03:22,959
give us the final output of the model

105
00:03:21,200 --> 00:03:24,958
and this final output of the model is

106
00:03:22,959 --> 00:03:26,400
then emitted to the outside world

107
00:03:24,959 --> 00:03:28,959
as the result of the neural network

108
00:03:26,400 --> 00:03:30,720
evaluation now if i zoom in

109
00:03:28,959 --> 00:03:32,080
on one of these neurons to show you

110
00:03:30,720 --> 00:03:34,480
what's going on here

111
00:03:32,080 --> 00:03:36,080
the input neurons have some value and

112
00:03:34,480 --> 00:03:37,280
the way that i compute the output

113
00:03:36,080 --> 00:03:39,280
neuron's value

114
00:03:37,280 --> 00:03:41,519
is i basically just take the dot product

115
00:03:39,280 --> 00:03:44,000
of these with the weights

116
00:03:41,519 --> 00:03:45,680
now the weights here are a1 and a2 these

117
00:03:44,000 --> 00:03:46,239
are the parameters which we're trying to

118
00:03:45,680 --> 00:03:48,159
learn

119
00:03:46,239 --> 00:03:51,519
which we don't have access to but what

120
00:03:48,159 --> 00:03:53,679
we can do is provide arbitrary inputs

121
00:03:51,519 --> 00:03:54,879
so all we're doing here is just a dot

122
00:03:53,680 --> 00:03:56,640
product and taking the sum

123
00:03:54,879 --> 00:03:58,879
or if you view the entire layer as one

124
00:03:56,640 --> 00:04:00,480
operation a matrix multiply

125
00:03:58,879 --> 00:04:02,480
now if neural networks were completely

126
00:04:00,480 --> 00:04:03,679
linear this would be very boring because

127
00:04:02,480 --> 00:04:04,238
they would all collapse down to one

128
00:04:03,680 --> 00:04:06,080
layer

129
00:04:04,239 --> 00:04:07,360
so we have to introduce one single

130
00:04:06,080 --> 00:04:10,000
non-linearity

131
00:04:07,360 --> 00:04:11,360
and most often is the value activation

132
00:04:10,000 --> 00:04:12,560
function so that's what we study in our

133
00:04:11,360 --> 00:04:14,959
paper

134
00:04:12,560 --> 00:04:16,798
and the way that this works is literally

135
00:04:14,959 --> 00:04:19,040
all you do it's a rectified linear

136
00:04:16,798 --> 00:04:20,638
we're just going to take the max of the

137
00:04:19,040 --> 00:04:21,919
input and zero

138
00:04:20,639 --> 00:04:23,759
and just compute this as the only

139
00:04:21,918 --> 00:04:25,359
non-linearity in the entire neural

140
00:04:23,759 --> 00:04:27,520
network

141
00:04:25,360 --> 00:04:28,560
so sort of zooming back out we're going

142
00:04:27,520 --> 00:04:30,159
to do a dot product

143
00:04:28,560 --> 00:04:32,400
and then take the non-linearity with

144
00:04:30,160 --> 00:04:34,400
this value and we're going to do this

145
00:04:32,400 --> 00:04:36,799
for every single neuron in the neural

146
00:04:34,400 --> 00:04:38,159
network

147
00:04:36,800 --> 00:04:40,240
okay and that's all that machine

148
00:04:38,160 --> 00:04:41,680
learning is for the purpose of this talk

149
00:04:40,240 --> 00:04:43,199
our results are independent of anything

150
00:04:41,680 --> 00:04:45,360
else about how this model itself was

151
00:04:43,199 --> 00:04:48,720
trained

152
00:04:45,360 --> 00:04:50,160
so let's talk about extraction then so

153
00:04:48,720 --> 00:04:52,240
the way that we're going to phrase our

154
00:04:50,160 --> 00:04:53,199
question is given oracle query access to

155
00:04:52,240 --> 00:04:55,440
a neural network

156
00:04:53,199 --> 00:04:56,639
can we extract the exact model and it

157
00:04:55,440 --> 00:04:58,000
turns out that there's a proof of

158
00:04:56,639 --> 00:04:59,360
impossibility here that says we can't

159
00:04:58,000 --> 00:05:00,880
hope to achieve this

160
00:04:59,360 --> 00:05:02,560
and there are multiple neural networks

161
00:05:00,880 --> 00:05:04,639
that compute the same function

162
00:05:02,560 --> 00:05:06,240
but have different bit representations

163
00:05:04,639 --> 00:05:06,880
so the best we can possibly hope to

164
00:05:06,240 --> 00:05:08,479
achieve

165
00:05:06,880 --> 00:05:10,719
is what we call functional equivalent

166
00:05:08,479 --> 00:05:11,758
extraction which means that now what

167
00:05:10,720 --> 00:05:13,840
we're going to try and ask

168
00:05:11,759 --> 00:05:14,880
is just ask for any one model which

169
00:05:13,840 --> 00:05:16,880
behaves the same

170
00:05:14,880 --> 00:05:18,479
input output behavior as the model we're

171
00:05:16,880 --> 00:05:20,320
trying to steal

172
00:05:18,479 --> 00:05:21,680
it turns out again there's another proof

173
00:05:20,320 --> 00:05:23,120
of impossibility here

174
00:05:21,680 --> 00:05:25,280
that says we can't hope to achieve this

175
00:05:23,120 --> 00:05:27,440
for some pathological neural networks

176
00:05:25,280 --> 00:05:29,198
and so what we're going to ask instead

177
00:05:27,440 --> 00:05:30,479
is in the typical case

178
00:05:29,199 --> 00:05:32,479
of a neural network learned with

179
00:05:30,479 --> 00:05:34,159
stochastic gradient descent

180
00:05:32,479 --> 00:05:36,000
are we able to achieve functional

181
00:05:34,160 --> 00:05:38,240
equivalent extraction

182
00:05:36,000 --> 00:05:39,199
and then the main result of our paper is

183
00:05:38,240 --> 00:05:42,800
to say that yes

184
00:05:39,199 --> 00:05:42,800
empirically we are able to do this

185
00:05:42,880 --> 00:05:47,120
okay so now let me give you some

186
00:05:45,280 --> 00:05:49,919
background on how previous attacks have

187
00:05:47,120 --> 00:05:51,120
tried to approach this direct extraction

188
00:05:49,919 --> 00:05:51,840
there are two papers here that have done

189
00:05:51,120 --> 00:05:53,840
this and they

190
00:05:51,840 --> 00:05:56,479
both work for the case of one hidden

191
00:05:53,840 --> 00:05:58,159
layer neural networks

192
00:05:56,479 --> 00:06:00,479
the visual intuition for these attacks

193
00:05:58,160 --> 00:06:01,919
will go something like this

194
00:06:00,479 --> 00:06:03,599
neural networks that have one hidden

195
00:06:01,919 --> 00:06:05,359
layer look like this

196
00:06:03,600 --> 00:06:06,720
the number of neurons in each layer is

197
00:06:05,360 --> 00:06:08,800
arbitrary i've put

198
00:06:06,720 --> 00:06:10,720
two input neurons here because that lets

199
00:06:08,800 --> 00:06:12,319
us draw them nicely on slides

200
00:06:10,720 --> 00:06:15,680
three hidden neurons for simplicity and

201
00:06:12,319 --> 00:06:18,400
one output without loss of generality

202
00:06:15,680 --> 00:06:19,120
so the neuron exists like this and if we

203
00:06:18,400 --> 00:06:20,479
draw the

204
00:06:19,120 --> 00:06:23,199
what the neural network actually looks

205
00:06:20,479 --> 00:06:26,318
like on the plane it looks like this

206
00:06:23,199 --> 00:06:28,479
what i'm showing here is each point here

207
00:06:26,319 --> 00:06:30,080
corresponds to a potential input to the

208
00:06:28,479 --> 00:06:32,479
neural network

209
00:06:30,080 --> 00:06:33,440
where along the x-axis we're varying the

210
00:06:32,479 --> 00:06:35,359
first neuron

211
00:06:33,440 --> 00:06:37,039
and along the y-axis we're varying the

212
00:06:35,360 --> 00:06:39,120
second neuron

213
00:06:37,039 --> 00:06:41,360
now each of the colored regions here

214
00:06:39,120 --> 00:06:42,400
corresponds to a linear region within

215
00:06:41,360 --> 00:06:44,319
this function

216
00:06:42,400 --> 00:06:46,080
recall that neural networks are

217
00:06:44,319 --> 00:06:48,160
piecewise linear functions because of

218
00:06:46,080 --> 00:06:49,599
how they're constructed

219
00:06:48,160 --> 00:06:51,520
and what you should notice here is not

220
00:06:49,599 --> 00:06:52,880
the fact that there are these seven

221
00:06:51,520 --> 00:06:54,639
different regions

222
00:06:52,880 --> 00:06:57,039
but instead the fact that there are

223
00:06:54,639 --> 00:06:59,680
really these three lines that divide

224
00:06:57,039 --> 00:07:01,680
this region into spaces

225
00:06:59,680 --> 00:07:03,440
now each of these polytopes has the

226
00:07:01,680 --> 00:07:05,759
property that

227
00:07:03,440 --> 00:07:08,319
we are in some particular linear region

228
00:07:05,759 --> 00:07:10,720
with respect to the neural network

229
00:07:08,319 --> 00:07:11,599
so maybe this this first line here might

230
00:07:10,720 --> 00:07:13,520
correspond

231
00:07:11,599 --> 00:07:15,120
to what we call the critical hyperplane

232
00:07:13,520 --> 00:07:16,960
for this middle neuron

233
00:07:15,120 --> 00:07:18,319
where if you're above here then you're

234
00:07:16,960 --> 00:07:19,599
in the positive region

235
00:07:18,319 --> 00:07:21,919
and if you're below then you're in the

236
00:07:19,599 --> 00:07:22,800
negative region of this particular

237
00:07:21,919 --> 00:07:24,960
neuron

238
00:07:22,800 --> 00:07:27,919
similarly for the other neurons we have

239
00:07:24,960 --> 00:07:29,599
this positive and negative side

240
00:07:27,919 --> 00:07:31,359
now what this means is that i can what i

241
00:07:29,599 --> 00:07:32,400
can do is i can label each of these

242
00:07:31,360 --> 00:07:34,240
polytopes

243
00:07:32,400 --> 00:07:35,679
um with the neuron with the positive or

244
00:07:34,240 --> 00:07:38,479
negative sign assignment

245
00:07:35,680 --> 00:07:40,000
of each of these three neurons and if i

246
00:07:38,479 --> 00:07:42,400
zoom in on just one of these

247
00:07:40,000 --> 00:07:43,759
what this means in particular is that

248
00:07:42,400 --> 00:07:45,359
the neural network we're actually

249
00:07:43,759 --> 00:07:46,960
considering if we're within this one

250
00:07:45,360 --> 00:07:48,960
linear region

251
00:07:46,960 --> 00:07:50,318
only has the neurons that are active

252
00:07:48,960 --> 00:07:51,599
actually being applied and the other

253
00:07:50,319 --> 00:07:54,160
ones are inactive

254
00:07:51,599 --> 00:07:56,159
which essentially means they don't exist

255
00:07:54,160 --> 00:07:59,919
and we could collapse this again down

256
00:07:56,160 --> 00:08:01,680
to just one single linear layer

257
00:07:59,919 --> 00:08:03,280
okay so that's what's going on in these

258
00:08:01,680 --> 00:08:05,599
diagrams and they'll be important later

259
00:08:03,280 --> 00:08:07,520
as well

260
00:08:05,599 --> 00:08:09,440
so the basic observation that prior work

261
00:08:07,520 --> 00:08:10,479
makes is that the location of these

262
00:08:09,440 --> 00:08:12,160
hyperplanes

263
00:08:10,479 --> 00:08:13,599
almost completely determines what the

264
00:08:12,160 --> 00:08:15,280
function is that the neural network is

265
00:08:13,599 --> 00:08:16,560
evaluating

266
00:08:15,280 --> 00:08:18,479
and the reason why that's the case is

267
00:08:16,560 --> 00:08:20,479
that it turns out that given where these

268
00:08:18,479 --> 00:08:22,240
hyperplanes are we can directly learn

269
00:08:20,479 --> 00:08:23,599
values of the weights of the neural

270
00:08:22,240 --> 00:08:25,840
network

271
00:08:23,599 --> 00:08:28,000
so why is that the case let's suppose

272
00:08:25,840 --> 00:08:31,198
that we had some particular input

273
00:08:28,000 --> 00:08:33,440
that was on this critical hyperplane

274
00:08:31,199 --> 00:08:36,080
now if we took a small direction in the

275
00:08:33,440 --> 00:08:38,080
red axis i'll call this the x-axis

276
00:08:36,080 --> 00:08:39,839
and then asked how far we have to travel

277
00:08:38,080 --> 00:08:40,880
in along the y-axis to get back on the

278
00:08:39,839 --> 00:08:42,479
hyperplane

279
00:08:40,880 --> 00:08:44,480
that allows us to compute essentially

280
00:08:42,479 --> 00:08:45,920
the normal vector to this hyperplane

281
00:08:44,480 --> 00:08:47,920
and it turns out we can use this to

282
00:08:45,920 --> 00:08:49,920
directly learn pieces of information

283
00:08:47,920 --> 00:08:51,680
about the weights

284
00:08:49,920 --> 00:08:53,279
so how do we do that well let's take a

285
00:08:51,680 --> 00:08:54,560
look again at the neural network

286
00:08:53,279 --> 00:08:56,800
and let's suppose that we have this

287
00:08:54,560 --> 00:08:59,040
value x and y that is at the critical

288
00:08:56,800 --> 00:09:02,560
hyperplane which in particular means

289
00:08:59,040 --> 00:09:04,000
that this middle neuron is now zero

290
00:09:02,560 --> 00:09:06,399
so we can do is we can say well what if

291
00:09:04,000 --> 00:09:07,839
we go to x plus epsilon

292
00:09:06,399 --> 00:09:09,600
of course this is going to change all of

293
00:09:07,839 --> 00:09:10,320
the values in the neural network into

294
00:09:09,600 --> 00:09:12,399
something we don't

295
00:09:10,320 --> 00:09:13,600
necessarily know but what we do know is

296
00:09:12,399 --> 00:09:16,240
if we now change y

297
00:09:13,600 --> 00:09:16,800
to y plus delta then we again have the

298
00:09:16,240 --> 00:09:19,680
property

299
00:09:16,800 --> 00:09:21,040
that this middle neuron is zero and in

300
00:09:19,680 --> 00:09:21,680
particular if we then look at the

301
00:09:21,040 --> 00:09:23,599
weights

302
00:09:21,680 --> 00:09:25,839
going into this neuron that zero let's

303
00:09:23,600 --> 00:09:27,279
call them a1 and a2

304
00:09:25,839 --> 00:09:29,120
then we can learn the piece of

305
00:09:27,279 --> 00:09:31,120
information that

306
00:09:29,120 --> 00:09:32,800
negative epsilon over delta equals a2

307
00:09:31,120 --> 00:09:34,480
over a1

308
00:09:32,800 --> 00:09:36,079
because the only way that we could have

309
00:09:34,480 --> 00:09:38,640
both x and y

310
00:09:36,080 --> 00:09:40,399
leading to zero and x plus epsilon and y

311
00:09:38,640 --> 00:09:43,600
plus delta leading to zero

312
00:09:40,399 --> 00:09:46,800
is if this assignment held true

313
00:09:43,600 --> 00:09:48,240
so this is good but there's one thing

314
00:09:46,800 --> 00:09:49,920
that we lose

315
00:09:48,240 --> 00:09:51,839
and the problem is that while it's true

316
00:09:49,920 --> 00:09:54,800
we learn the ratio of these two weights

317
00:09:51,839 --> 00:09:55,519
epsilon and delta here what we don't

318
00:09:54,800 --> 00:09:58,479
know

319
00:09:55,519 --> 00:10:00,240
is the magnitude of this vector we don't

320
00:09:58,480 --> 00:10:02,320
know how big it is

321
00:10:00,240 --> 00:10:04,240
now fortunately we can push any positive

322
00:10:02,320 --> 00:10:06,480
constants through to the next layer and

323
00:10:04,240 --> 00:10:08,800
things will just work themselves out

324
00:10:06,480 --> 00:10:09,600
the problem though is that we lose

325
00:10:08,800 --> 00:10:11,120
information

326
00:10:09,600 --> 00:10:13,040
as we lose whether or not this is

327
00:10:11,120 --> 00:10:14,480
pointing up or whether or not this is

328
00:10:13,040 --> 00:10:16,319
pointing down

329
00:10:14,480 --> 00:10:17,760
and this sign information is actually a

330
00:10:16,320 --> 00:10:18,000
critical piece of information that we

331
00:10:17,760 --> 00:10:20,959
just

332
00:10:18,000 --> 00:10:21,519
lose and can't recover and it turns out

333
00:10:20,959 --> 00:10:23,839
that

334
00:10:21,519 --> 00:10:26,720
local information is just insufficient

335
00:10:23,839 --> 00:10:28,880
to recover this neuron sign

336
00:10:26,720 --> 00:10:30,000
so we need some way to recover it and

337
00:10:28,880 --> 00:10:32,959
the way we're going to do this

338
00:10:30,000 --> 00:10:33,839
is basically brute force so all we're

339
00:10:32,959 --> 00:10:35,040
going to do

340
00:10:33,839 --> 00:10:37,600
is we're going to query the neural

341
00:10:35,040 --> 00:10:38,959
network on a couple of different random

342
00:10:37,600 --> 00:10:40,959
points

343
00:10:38,959 --> 00:10:42,479
and then say we know the weights of all

344
00:10:40,959 --> 00:10:43,839
of these now because we know the normal

345
00:10:42,480 --> 00:10:44,800
direction we just need to recover the

346
00:10:43,839 --> 00:10:46,640
sign

347
00:10:44,800 --> 00:10:48,160
so this means that there are eight

348
00:10:46,640 --> 00:10:49,839
possible assignments to the sign

349
00:10:48,160 --> 00:10:51,040
because we have three neurons in this

350
00:10:49,839 --> 00:10:52,399
hidden layer in general it's an

351
00:10:51,040 --> 00:10:54,399
exponential number of

352
00:10:52,399 --> 00:10:55,839
neurons we need to recover the sign for

353
00:10:54,399 --> 00:10:58,079
and we're just going to ask for each of

354
00:10:55,839 --> 00:11:00,240
these possible sign assignments

355
00:10:58,079 --> 00:11:02,719
could the function have the values that

356
00:11:00,240 --> 00:11:04,560
we queried at these points

357
00:11:02,720 --> 00:11:06,079
if the answer is yes then we've

358
00:11:04,560 --> 00:11:07,119
extracted the signs correctly and if the

359
00:11:06,079 --> 00:11:08,319
answer is no

360
00:11:07,120 --> 00:11:10,240
then we just try the next sign

361
00:11:08,320 --> 00:11:12,320
assignment so we might maybe guess

362
00:11:10,240 --> 00:11:14,000
initially that all of the signs are

363
00:11:12,320 --> 00:11:15,680
positive positive positive

364
00:11:14,000 --> 00:11:17,680
and then we check the sign assignment we

365
00:11:15,680 --> 00:11:19,359
check that this works and if it doesn't

366
00:11:17,680 --> 00:11:21,279
then we try positive positive negative

367
00:11:19,360 --> 00:11:23,200
and repeat until we find something that

368
00:11:21,279 --> 00:11:24,880
happens to work

369
00:11:23,200 --> 00:11:26,880
now once we have the first layer with

370
00:11:24,880 --> 00:11:28,560
the signs extracted correctly

371
00:11:26,880 --> 00:11:30,480
now we can extract the second layer

372
00:11:28,560 --> 00:11:31,279
trivially because it's just a linear

373
00:11:30,480 --> 00:11:33,279
function

374
00:11:31,279 --> 00:11:36,560
we just directly compute with least

375
00:11:33,279 --> 00:11:40,640
squares and solve for the second layer

376
00:11:36,560 --> 00:11:42,560
okay so that's how prior work did this

377
00:11:40,640 --> 00:11:43,680
up until the point that we need to find

378
00:11:42,560 --> 00:11:45,279
these witnesses

379
00:11:43,680 --> 00:11:46,719
to these critical points on these

380
00:11:45,279 --> 00:11:48,240
hyperplanes

381
00:11:46,720 --> 00:11:50,079
so that actually is again a fairly

382
00:11:48,240 --> 00:11:51,680
simple procedure and the way that this

383
00:11:50,079 --> 00:11:53,120
works is we're going to draw a random

384
00:11:51,680 --> 00:11:55,439
line through the input space

385
00:11:53,120 --> 00:11:57,200
from maybe u to v and we're just going

386
00:11:55,440 --> 00:11:59,680
to sweep across this line and look for

387
00:11:57,200 --> 00:12:01,040
discontinuities in the gradient

388
00:11:59,680 --> 00:12:02,880
and the reason we can do this is if i

389
00:12:01,040 --> 00:12:03,519
plot instead of from like a top down

390
00:12:02,880 --> 00:12:05,519
view

391
00:12:03,519 --> 00:12:08,000
of what's happening if i plot the output

392
00:12:05,519 --> 00:12:09,600
of the function as we travel from u to v

393
00:12:08,000 --> 00:12:11,839
we get a plot that looks something like

394
00:12:09,600 --> 00:12:14,000
this and we'll see that

395
00:12:11,839 --> 00:12:15,920
now the output of the function has

396
00:12:14,000 --> 00:12:17,279
really three different

397
00:12:15,920 --> 00:12:19,439
lines at which the gradient is

398
00:12:17,279 --> 00:12:21,439
discontinuous corresponding to these

399
00:12:19,440 --> 00:12:22,800
four linear regions

400
00:12:21,440 --> 00:12:24,720
and the points at which the gradient is

401
00:12:22,800 --> 00:12:27,279
this continues continuous

402
00:12:24,720 --> 00:12:28,399
directly correspond to these critical

403
00:12:27,279 --> 00:12:30,079
hyperplanes so that

404
00:12:28,399 --> 00:12:32,480
lets us very efficiently recover these

405
00:12:30,079 --> 00:12:35,839
points

406
00:12:32,480 --> 00:12:37,680
okay so the main contribution of

407
00:12:35,839 --> 00:12:39,279
our paper is to do three different

408
00:12:37,680 --> 00:12:40,880
things

409
00:12:39,279 --> 00:12:42,880
the first thing we do is we show how to

410
00:12:40,880 --> 00:12:44,320
extract deep neural networks

411
00:12:42,880 --> 00:12:46,320
these prior papers i showed you were

412
00:12:44,320 --> 00:12:48,000
able to do this in the case of neural

413
00:12:46,320 --> 00:12:49,120
networks that had one hidden layer

414
00:12:48,000 --> 00:12:51,120
and were able to extend this to

415
00:12:49,120 --> 00:12:52,399
arbitrary depth

416
00:12:51,120 --> 00:12:54,399
the second thing that we do is we show

417
00:12:52,399 --> 00:12:55,920
how to do this efficiently one other

418
00:12:54,399 --> 00:12:56,560
paper that came out around the same time

419
00:12:55,920 --> 00:12:58,479
as us

420
00:12:56,560 --> 00:13:00,079
was able to show how to extract deep

421
00:12:58,480 --> 00:13:01,680
neural networks and we're roughly a

422
00:13:00,079 --> 00:13:03,120
thousand times more query efficient than

423
00:13:01,680 --> 00:13:04,880
that paper is

424
00:13:03,120 --> 00:13:06,240
and the third thing that we do is we do

425
00:13:04,880 --> 00:13:08,480
what's called high fidelity

426
00:13:06,240 --> 00:13:09,920
extraction and we can extract neural

427
00:13:08,480 --> 00:13:12,000
networks that are

428
00:13:09,920 --> 00:13:15,439
basically up to floating point precision

429
00:13:12,000 --> 00:13:18,160
identical to the original model

430
00:13:15,440 --> 00:13:19,680
okay because of the limited time on this

431
00:13:18,160 --> 00:13:21,439
talk i can only cover one of these so

432
00:13:19,680 --> 00:13:23,359
i'm going to cover just the first

433
00:13:21,440 --> 00:13:24,639
and again because of limited time i'm

434
00:13:23,360 --> 00:13:25,440
only going to show you what happens in

435
00:13:24,639 --> 00:13:27,040
the case

436
00:13:25,440 --> 00:13:28,959
of two deep neural networks these are

437
00:13:27,040 --> 00:13:32,399
neural networks with just two

438
00:13:28,959 --> 00:13:33,518
hidden neuron layers so putting aside

439
00:13:32,399 --> 00:13:34,079
the stuff that i'm not going to talk

440
00:13:33,519 --> 00:13:36,160
about

441
00:13:34,079 --> 00:13:37,760
there are two pieces to our attack then

442
00:13:36,160 --> 00:13:38,800
as before we're first going to recover

443
00:13:37,760 --> 00:13:40,079
the weights

444
00:13:38,800 --> 00:13:42,880
and then we're going to recover the

445
00:13:40,079 --> 00:13:45,120
signs of the neurons

446
00:13:42,880 --> 00:13:45,920
so getting started a two deep neural

447
00:13:45,120 --> 00:13:47,839
network

448
00:13:45,920 --> 00:13:49,519
looks something like this where now

449
00:13:47,839 --> 00:13:51,440
instead of just adding one hidden layer

450
00:13:49,519 --> 00:13:53,199
we have two

451
00:13:51,440 --> 00:13:54,560
so again i can show what the same

452
00:13:53,199 --> 00:13:56,079
diagram will look like

453
00:13:54,560 --> 00:13:58,079
and again we'll notice the space is

454
00:13:56,079 --> 00:14:00,079
partitioned into these polytopes

455
00:13:58,079 --> 00:14:01,439
of different regions that i've colored

456
00:14:00,079 --> 00:14:04,160
again where

457
00:14:01,440 --> 00:14:04,720
each color again has one complete linear

458
00:14:04,160 --> 00:14:06,560
region

459
00:14:04,720 --> 00:14:08,079
and now the sine assignments are not

460
00:14:06,560 --> 00:14:09,680
just for one set of neurons

461
00:14:08,079 --> 00:14:11,439
but two sets of neurons neurons on the

462
00:14:09,680 --> 00:14:13,519
first layer and neurons on the second

463
00:14:11,440 --> 00:14:16,079
layer

464
00:14:13,519 --> 00:14:17,440
now you'll notice that there are these

465
00:14:16,079 --> 00:14:19,920
same properties

466
00:14:17,440 --> 00:14:21,600
that there are these neurons that exist

467
00:14:19,920 --> 00:14:23,199
on the first layer

468
00:14:21,600 --> 00:14:25,600
whereas straight lines to the input

469
00:14:23,199 --> 00:14:26,880
space but now we also have neurons in

470
00:14:25,600 --> 00:14:29,440
the second layer

471
00:14:26,880 --> 00:14:30,880
and the way those work is these ones are

472
00:14:29,440 --> 00:14:33,600
bent a little bit

473
00:14:30,880 --> 00:14:35,439
by the first layer hyperplanes if we

474
00:14:33,600 --> 00:14:37,839
were able to visualize what was going on

475
00:14:35,440 --> 00:14:38,720
with respect to the inputs to this

476
00:14:37,839 --> 00:14:40,000
second layer

477
00:14:38,720 --> 00:14:41,839
this would look like a straight linear

478
00:14:40,000 --> 00:14:43,360
hyperplane but

479
00:14:41,839 --> 00:14:45,920
because we're only viewing this with

480
00:14:43,360 --> 00:14:47,519
respect to the input space

481
00:14:45,920 --> 00:14:49,360
which is then distorted by the first

482
00:14:47,519 --> 00:14:52,160
linear layer what we see

483
00:14:49,360 --> 00:14:55,199
here is a bent hyperplane only because

484
00:14:52,160 --> 00:14:58,719
the first layer bends it

485
00:14:55,199 --> 00:14:59,040
okay so the first thing we're going to

486
00:14:58,720 --> 00:15:00,560
do

487
00:14:59,040 --> 00:15:02,079
in our attack is we're going to recover

488
00:15:00,560 --> 00:15:04,719
the first layer up to sine

489
00:15:02,079 --> 00:15:05,279
as before and the way we're going to do

490
00:15:04,720 --> 00:15:07,360
that

491
00:15:05,279 --> 00:15:10,160
is we're going to start off by drawing

492
00:15:07,360 --> 00:15:13,519
three or in general our arbitrary number

493
00:15:10,160 --> 00:15:15,360
of random lines through the input space

494
00:15:13,519 --> 00:15:17,120
and sweeping to find witnesses to these

495
00:15:15,360 --> 00:15:19,279
critical points using the same algorithm

496
00:15:17,120 --> 00:15:20,320
as identified before

497
00:15:19,279 --> 00:15:22,320
and now we're going to do is we're going

498
00:15:20,320 --> 00:15:23,920
to try and use this information to not

499
00:15:22,320 --> 00:15:26,639
only figure out

500
00:15:23,920 --> 00:15:28,399
what what the weights are but like which

501
00:15:26,639 --> 00:15:30,320
neuron they correspond to on either the

502
00:15:28,399 --> 00:15:31,519
first or the second layer

503
00:15:30,320 --> 00:15:32,800
and we do this through a matching

504
00:15:31,519 --> 00:15:33,680
algorithm we're going to start off by

505
00:15:32,800 --> 00:15:35,120
pairing

506
00:15:33,680 --> 00:15:37,359
all neurons with all of the neurons and

507
00:15:35,120 --> 00:15:40,399
we're going to say for each pair

508
00:15:37,360 --> 00:15:43,040
could these neurons be witnesses to

509
00:15:40,399 --> 00:15:43,600
the same critical point critical neuron

510
00:15:43,040 --> 00:15:45,519
being as

511
00:15:43,600 --> 00:15:46,720
um the same neuron being at its critical

512
00:15:45,519 --> 00:15:48,320
point

513
00:15:46,720 --> 00:15:50,320
so for these two basically we're asking

514
00:15:48,320 --> 00:15:52,399
is do their neuro

515
00:15:50,320 --> 00:15:53,600
do their normal vectors align in such a

516
00:15:52,399 --> 00:15:55,199
way that we could have drawn a line

517
00:15:53,600 --> 00:15:55,680
through these two and the answer here is

518
00:15:55,199 --> 00:15:57,758
no

519
00:15:55,680 --> 00:16:00,000
because their normals don't align we can

520
00:15:57,759 --> 00:16:01,600
ask this for the next neuron

521
00:16:00,000 --> 00:16:03,920
for the next critical point for the next

522
00:16:01,600 --> 00:16:08,000
critical point we can keep on going

523
00:16:03,920 --> 00:16:09,839
until finally we end up with now

524
00:16:08,000 --> 00:16:12,720
two points that are witnesses to the

525
00:16:09,839 --> 00:16:14,320
same neuron at the same critical point

526
00:16:12,720 --> 00:16:16,320
and this means in particular that this

527
00:16:14,320 --> 00:16:18,000
is actually a constant linear region

528
00:16:16,320 --> 00:16:19,920
and therefore this is probably a point

529
00:16:18,000 --> 00:16:22,079
on the first layer

530
00:16:19,920 --> 00:16:23,680
and we can repeat this for each of the

531
00:16:22,079 --> 00:16:25,199
other layers

532
00:16:23,680 --> 00:16:27,439
sorry for each of the other neurons in

533
00:16:25,199 --> 00:16:30,240
order to identify all of the neurons

534
00:16:27,440 --> 00:16:31,759
that are on the first layer because only

535
00:16:30,240 --> 00:16:33,519
neurons on the first layer will match up

536
00:16:31,759 --> 00:16:35,440
in this way

537
00:16:33,519 --> 00:16:37,199
the problem about this is that it's

538
00:16:35,440 --> 00:16:40,399
possible that we could

539
00:16:37,199 --> 00:16:42,880
spuriously have a situation where

540
00:16:40,399 --> 00:16:43,600
two neurons um where neuron the second

541
00:16:42,880 --> 00:16:46,240
layer

542
00:16:43,600 --> 00:16:47,759
is accidentally found because we

543
00:16:46,240 --> 00:16:49,440
happened to query it sort of

544
00:16:47,759 --> 00:16:51,120
very close to each other and we found

545
00:16:49,440 --> 00:16:52,800
something in the second layer

546
00:16:51,120 --> 00:16:54,160
but in practice this occurs with fairly

547
00:16:52,800 --> 00:16:56,639
low probability

548
00:16:54,160 --> 00:16:58,319
and if we do this enough times these

549
00:16:56,639 --> 00:17:00,399
sort of things will just sort of

550
00:16:58,320 --> 00:17:01,680
be filtered out in the noise and the

551
00:17:00,399 --> 00:17:04,160
correct things will sort of bubble to

552
00:17:01,680 --> 00:17:07,280
the top

553
00:17:04,160 --> 00:17:08,000
okay so this lets us recover again the

554
00:17:07,280 --> 00:17:11,359
three

555
00:17:08,000 --> 00:17:14,400
lines that correspond to these to these

556
00:17:11,359 --> 00:17:15,839
to the weights of the first layer

557
00:17:14,400 --> 00:17:17,760
now we need to do is we need to recover

558
00:17:15,839 --> 00:17:19,760
the sign for this neuron is pointing up

559
00:17:17,760 --> 00:17:23,439
or pointing down

560
00:17:19,760 --> 00:17:25,520
for all of these three neurons

561
00:17:23,439 --> 00:17:27,520
now previously what we were able to do

562
00:17:25,520 --> 00:17:29,840
is we could just through brute force

563
00:17:27,520 --> 00:17:32,080
uh enumerate a couple different values

564
00:17:29,840 --> 00:17:33,600
and just trial and error check

565
00:17:32,080 --> 00:17:34,960
the problem is we can't do that again

566
00:17:33,600 --> 00:17:36,320
here because if i were to make some

567
00:17:34,960 --> 00:17:38,880
random queries

568
00:17:36,320 --> 00:17:40,159
i would have to then completely extract

569
00:17:38,880 --> 00:17:41,679
the second layer

570
00:17:40,160 --> 00:17:43,280
in order to check if my solution was

571
00:17:41,679 --> 00:17:44,880
valid

572
00:17:43,280 --> 00:17:46,639
and that's problematic because

573
00:17:44,880 --> 00:17:48,000
extracting the second layer actually

574
00:17:46,640 --> 00:17:50,160
requires queries

575
00:17:48,000 --> 00:17:52,000
so therefore my attack would not only be

576
00:17:50,160 --> 00:17:53,760
exponential in time but also be

577
00:17:52,000 --> 00:17:56,160
exponential in queries

578
00:17:53,760 --> 00:17:58,320
and that's not something we're okay with

579
00:17:56,160 --> 00:18:01,760
so we develop a more efficient procedure

580
00:17:58,320 --> 00:18:03,439
to do efficient sign recovery

581
00:18:01,760 --> 00:18:05,520
by efficient in queries but not

582
00:18:03,440 --> 00:18:07,200
necessarily in time

583
00:18:05,520 --> 00:18:08,720
so the way that we're going to do this

584
00:18:07,200 --> 00:18:09,679
is we're going to start off with a

585
00:18:08,720 --> 00:18:11,679
single point

586
00:18:09,679 --> 00:18:12,720
that's on the hyperplane from the second

587
00:18:11,679 --> 00:18:14,080
layer neuron

588
00:18:12,720 --> 00:18:15,520
and we know it's a second layer neuron

589
00:18:14,080 --> 00:18:17,760
because it's not a first layer neuron

590
00:18:15,520 --> 00:18:19,918
because we've found all of those

591
00:18:17,760 --> 00:18:21,520
and then we're going to trace its path

592
00:18:19,919 --> 00:18:23,840
through the input space

593
00:18:21,520 --> 00:18:25,520
and follow everywhere that it goes to in

594
00:18:23,840 --> 00:18:29,120
order to identify the line

595
00:18:25,520 --> 00:18:30,639
of the path that it takes and once we've

596
00:18:29,120 --> 00:18:32,799
done this now

597
00:18:30,640 --> 00:18:34,480
we can again do our same style of

598
00:18:32,799 --> 00:18:36,480
argument as before

599
00:18:34,480 --> 00:18:37,919
we know that this is the path that it

600
00:18:36,480 --> 00:18:39,919
empirically takes

601
00:18:37,919 --> 00:18:41,039
so let's try all possible sign

602
00:18:39,919 --> 00:18:43,919
assignments

603
00:18:41,039 --> 00:18:44,640
of the first layer and ask which of

604
00:18:43,919 --> 00:18:46,960
these

605
00:18:44,640 --> 00:18:48,640
could possibly permit the fact that this

606
00:18:46,960 --> 00:18:49,919
nine exists in exactly this

607
00:18:48,640 --> 00:18:51,360
configuration

608
00:18:49,919 --> 00:18:53,200
because there are only really three

609
00:18:51,360 --> 00:18:54,080
variables that determine where this line

610
00:18:53,200 --> 00:18:55,280
actually is

611
00:18:54,080 --> 00:18:57,039
which are the weights from these three

612
00:18:55,280 --> 00:18:58,000
neurons into the neuron that has now

613
00:18:57,039 --> 00:18:59,919
been bent

614
00:18:58,000 --> 00:19:01,520
and so by enumerating all of the signs

615
00:18:59,919 --> 00:19:03,919
in the previous layer

616
00:19:01,520 --> 00:19:05,600
we can check for a valid solution with

617
00:19:03,919 --> 00:19:09,200
an efficient number of queries even if

618
00:19:05,600 --> 00:19:09,199
we have to do exponential compute

619
00:19:09,280 --> 00:19:13,280
okay so the question now is then how do

620
00:19:11,600 --> 00:19:14,240
we actually do this hyperplane following

621
00:19:13,280 --> 00:19:16,559
algorithm

622
00:19:14,240 --> 00:19:17,760
and that again is fairly simple from an

623
00:19:16,559 --> 00:19:19,200
ideas perspective

624
00:19:17,760 --> 00:19:20,960
but in practice has lots of

625
00:19:19,200 --> 00:19:23,360
implementation problems

626
00:19:20,960 --> 00:19:24,080
so the way we do this is at some point

627
00:19:23,360 --> 00:19:25,520
in time

628
00:19:24,080 --> 00:19:28,000
we're going to have a point on the

629
00:19:25,520 --> 00:19:29,200
hyperplane and what we need to do is we

630
00:19:28,000 --> 00:19:31,679
need to figure out

631
00:19:29,200 --> 00:19:33,440
how to follow this point to make sure we

632
00:19:31,679 --> 00:19:36,000
stay on the same hyperplane and don't

633
00:19:33,440 --> 00:19:38,160
accidentally go somewhere else

634
00:19:36,000 --> 00:19:39,360
so let's suppose that we had this point

635
00:19:38,160 --> 00:19:41,760
here

636
00:19:39,360 --> 00:19:43,360
and we follow it along and following

637
00:19:41,760 --> 00:19:43,840
along is fairly easy we just make sure

638
00:19:43,360 --> 00:19:46,879
we

639
00:19:43,840 --> 00:19:49,840
we move not sort of

640
00:19:46,880 --> 00:19:50,880
dot product 0 with the normal now we're

641
00:19:49,840 --> 00:19:52,240
at this point that's a multiple

642
00:19:50,880 --> 00:19:53,280
intersection point we need to figure out

643
00:19:52,240 --> 00:19:54,880
what to do

644
00:19:53,280 --> 00:19:57,918
and ideally we want to make sure that we

645
00:19:54,880 --> 00:19:59,840
move in sort of the correct direction

646
00:19:57,919 --> 00:20:01,200
what we want to make sure we don't do is

647
00:19:59,840 --> 00:20:03,199
when we get from here

648
00:20:01,200 --> 00:20:04,559
to this multiple intersection point we

649
00:20:03,200 --> 00:20:07,760
want to make sure that we don't

650
00:20:04,559 --> 00:20:09,520
end up and accidentally go along a first

651
00:20:07,760 --> 00:20:10,879
layer hyperplane or back where we came

652
00:20:09,520 --> 00:20:12,480
from

653
00:20:10,880 --> 00:20:14,640
fortunately both of those are relatively

654
00:20:12,480 --> 00:20:15,679
easy to prevent because we know where

655
00:20:14,640 --> 00:20:17,200
all of the first lane

656
00:20:15,679 --> 00:20:19,120
layer hyperplanes are and so we can just

657
00:20:17,200 --> 00:20:20,320
not travel on those and we know which

658
00:20:19,120 --> 00:20:22,158
direction we just came from

659
00:20:20,320 --> 00:20:25,439
so by process of elimination we can make

660
00:20:22,159 --> 00:20:28,320
sure we travel in the correct direction

661
00:20:25,440 --> 00:20:30,159
then we have all of the weights and the

662
00:20:28,320 --> 00:20:31,918
signs recovered from the first layer

663
00:20:30,159 --> 00:20:34,480
and so we can just peel off the first

664
00:20:31,919 --> 00:20:36,400
layer and completely re-run our attack

665
00:20:34,480 --> 00:20:38,960
exactly as before starting from the

666
00:20:36,400 --> 00:20:40,720
second layer working on

667
00:20:38,960 --> 00:20:42,000
now there are actually a couple more

668
00:20:40,720 --> 00:20:42,960
details of course

669
00:20:42,000 --> 00:20:44,960
that i'm not going to be able to get

670
00:20:42,960 --> 00:20:46,159
into in particular the two most

671
00:20:44,960 --> 00:20:48,320
important of these

672
00:20:46,159 --> 00:20:49,760
is that in practice we have bounded

673
00:20:48,320 --> 00:20:51,678
floating point precision

674
00:20:49,760 --> 00:20:53,120
the gpus like to do all sorts of things

675
00:20:51,679 --> 00:20:54,080
that mess with us and so all of our

676
00:20:53,120 --> 00:20:56,719
algorithms

677
00:20:54,080 --> 00:20:57,840
have to be numerically stable and in

678
00:20:56,720 --> 00:21:00,159
general

679
00:20:57,840 --> 00:21:01,360
when extracting deep layers not all of

680
00:21:00,159 --> 00:21:03,840
the hidden states

681
00:21:01,360 --> 00:21:05,678
are completely accessible for example i

682
00:21:03,840 --> 00:21:07,760
can't feed a negative number

683
00:21:05,679 --> 00:21:09,200
into a neuron on the second layer

684
00:21:07,760 --> 00:21:10,400
because all neurons on the first layer

685
00:21:09,200 --> 00:21:12,240
have values

686
00:21:10,400 --> 00:21:13,919
and so this is just not possible and

687
00:21:12,240 --> 00:21:16,000
this complicates our attack somewhat

688
00:21:13,919 --> 00:21:17,280
for inner layers to extract the deep um

689
00:21:16,000 --> 00:21:19,520
to extract deep inner layers of the

690
00:21:17,280 --> 00:21:22,240
neural network

691
00:21:19,520 --> 00:21:22,799
okay so to briefly summarize our results

692
00:21:22,240 --> 00:21:24,559
then

693
00:21:22,799 --> 00:21:26,480
um here's sort of the main table from

694
00:21:24,559 --> 00:21:27,600
our paper where on the left we have the

695
00:21:26,480 --> 00:21:30,240
architecture this is

696
00:21:27,600 --> 00:21:31,678
the number of neurons in each layer this

697
00:21:30,240 --> 00:21:33,919
is the number of parameters the total

698
00:21:31,679 --> 00:21:35,440
number of weights in the neural network

699
00:21:33,919 --> 00:21:36,880
and then what we have here are the

700
00:21:35,440 --> 00:21:37,600
number of queries we need to make of the

701
00:21:36,880 --> 00:21:39,520
model

702
00:21:37,600 --> 00:21:41,039
and then various ways of measuring how

703
00:21:39,520 --> 00:21:44,400
well we extracted it

704
00:21:41,039 --> 00:21:46,799
where lower numbers are better

705
00:21:44,400 --> 00:21:47,520
so if you compare to prior work that was

706
00:21:46,799 --> 00:21:50,400
able to do

707
00:21:47,520 --> 00:21:52,799
extraction for one layer neural networks

708
00:21:50,400 --> 00:21:53,600
our paper requires roughly twice as many

709
00:21:52,799 --> 00:21:56,080
queries

710
00:21:53,600 --> 00:21:57,280
but has the benefit that it's maybe two

711
00:21:56,080 --> 00:21:59,918
to the thirtieth times

712
00:21:57,280 --> 00:22:00,559
more precise which in particular means

713
00:21:59,919 --> 00:22:03,600
that

714
00:22:00,559 --> 00:22:06,158
the worst case error between our

715
00:22:03,600 --> 00:22:08,639
local copy and the remote model is at

716
00:22:06,159 --> 00:22:12,080
most 2 to the minus 30th

717
00:22:08,640 --> 00:22:13,760
in most settings in the case when we

718
00:22:12,080 --> 00:22:15,840
compare to prior work who is able to do

719
00:22:13,760 --> 00:22:19,360
deep extraction of neural networks

720
00:22:15,840 --> 00:22:19,918
with two or more layers not only are we

721
00:22:19,360 --> 00:22:22,559
now

722
00:22:19,919 --> 00:22:23,840
much more query efficient again we can

723
00:22:22,559 --> 00:22:26,559
extract models

724
00:22:23,840 --> 00:22:29,280
that are much more precise in how we

725
00:22:26,559 --> 00:22:29,280
extract them

726
00:22:29,520 --> 00:22:32,799
to briefly conclude there are a couple

727
00:22:30,960 --> 00:22:35,840
conclusions i think are important

728
00:22:32,799 --> 00:22:37,760
the first of these is to say that this

729
00:22:35,840 --> 00:22:39,199
direct analysis of neural networks is a

730
00:22:37,760 --> 00:22:40,960
really

731
00:22:39,200 --> 00:22:43,280
useful way of thinking about machine

732
00:22:40,960 --> 00:22:45,360
learning we don't need to care

733
00:22:43,280 --> 00:22:46,799
about the atom optimizer or if we're

734
00:22:45,360 --> 00:22:49,039
using rms prop

735
00:22:46,799 --> 00:22:50,080
or exactly why batch normalization does

736
00:22:49,039 --> 00:22:51,200
or does not work

737
00:22:50,080 --> 00:22:52,639
we just need to know that they're

738
00:22:51,200 --> 00:22:55,280
mathematical functions and we can

739
00:22:52,640 --> 00:22:57,120
analyze them directly

740
00:22:55,280 --> 00:22:59,120
the second consequence of our paper is

741
00:22:57,120 --> 00:23:01,520
that the field of secure inference

742
00:22:59,120 --> 00:23:02,799
maybe isn't so secure so a secure

743
00:23:01,520 --> 00:23:04,400
inference is a field that

744
00:23:02,799 --> 00:23:06,080
takes together secure multi-party

745
00:23:04,400 --> 00:23:07,440
computation and neural network

746
00:23:06,080 --> 00:23:10,639
evaluation

747
00:23:07,440 --> 00:23:11,440
in order to evaluate f x when f is held

748
00:23:10,640 --> 00:23:13,039
by one party

749
00:23:11,440 --> 00:23:14,720
x is held by the other and they don't

750
00:23:13,039 --> 00:23:15,760
want to reveal their inputs to each

751
00:23:14,720 --> 00:23:18,640
other

752
00:23:15,760 --> 00:23:20,879
as a result of our attack it means that

753
00:23:18,640 --> 00:23:23,520
revealing the value of f of x

754
00:23:20,880 --> 00:23:24,080
is as good as revealing the function of

755
00:23:23,520 --> 00:23:26,400
f

756
00:23:24,080 --> 00:23:27,918
if given enough queries and so the field

757
00:23:26,400 --> 00:23:29,679
of secure inference is going to have to

758
00:23:27,919 --> 00:23:31,679
take into our tax into account

759
00:23:29,679 --> 00:23:33,120
in order to design mechanisms to prevent

760
00:23:31,679 --> 00:23:35,520
these kinds of attacks

761
00:23:33,120 --> 00:23:37,199
so that people can't just query a model

762
00:23:35,520 --> 00:23:40,240
even in this mpc setting

763
00:23:37,200 --> 00:23:43,039
and still learn the parameters

764
00:23:40,240 --> 00:23:44,159
more broadly there's a talk by matthew

765
00:23:43,039 --> 00:23:45,679
who is the intern who

766
00:23:44,159 --> 00:23:47,760
who was with us at google when we did

767
00:23:45,679 --> 00:23:49,120
this doing this work called don't put

768
00:23:47,760 --> 00:23:50,480
neural networks in your ideal

769
00:23:49,120 --> 00:23:52,959
functionalities

770
00:23:50,480 --> 00:23:54,240
and the basic idea behind this is to say

771
00:23:52,960 --> 00:23:57,520
that

772
00:23:54,240 --> 00:23:59,840
we in crypto like to think of maybe

773
00:23:57,520 --> 00:24:01,120
aes as if it was some perfect block

774
00:23:59,840 --> 00:24:03,840
cipher

775
00:24:01,120 --> 00:24:04,639
and broadly speaking it is and we can do

776
00:24:03,840 --> 00:24:07,279
that

777
00:24:04,640 --> 00:24:09,679
but neural networks don't really fit

778
00:24:07,279 --> 00:24:12,320
well into any ideal environment

779
00:24:09,679 --> 00:24:12,960
and are incredibly leaky abstractions

780
00:24:12,320 --> 00:24:15,200
and so

781
00:24:12,960 --> 00:24:16,000
for the time being it really is not

782
00:24:15,200 --> 00:24:18,400
advisable

783
00:24:16,000 --> 00:24:20,799
to try and idealize neural networks in

784
00:24:18,400 --> 00:24:23,840
any reasonable way

785
00:24:20,799 --> 00:24:25,760
so with that um on friday we're going to

786
00:24:23,840 --> 00:24:28,959
have a live q a

787
00:24:25,760 --> 00:24:31,600
at 8 a.m pacific um if

788
00:24:28,960 --> 00:24:33,520
you're watching this after the fact in a

789
00:24:31,600 --> 00:24:35,760
non-pandemic world

790
00:24:33,520 --> 00:24:36,799
i'd be happy to take any questions over

791
00:24:35,760 --> 00:24:38,720
email

792
00:24:36,799 --> 00:24:40,080
and the code to reproduce um our

793
00:24:38,720 --> 00:24:42,320
experiments on our paper

794
00:24:40,080 --> 00:24:43,600
is available online thank you very much

795
00:24:42,320 --> 00:24:45,039
and i'd be happy to

796
00:24:43,600 --> 00:24:47,840
take questions in one of these two

797
00:24:45,039 --> 00:24:47,840
formats

