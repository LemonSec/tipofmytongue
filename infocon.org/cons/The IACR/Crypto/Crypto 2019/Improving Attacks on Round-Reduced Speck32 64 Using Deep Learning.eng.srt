1
00:00:04,880 --> 00:00:11,990
so yes I'm quite happy to be here and my

2
00:00:08,630 --> 00:00:14,959
the topic of my talk today is improving

3
00:00:11,990 --> 00:00:19,330
attacks on ground produced specs 3264

4
00:00:14,959 --> 00:00:23,660
using deep learning and I won't have

5
00:00:19,330 --> 00:00:26,139
there's quite a few results in this

6
00:00:23,660 --> 00:00:28,700
paper which I won't have much time to

7
00:00:26,140 --> 00:00:30,619
tell you about so I will start with

8
00:00:28,700 --> 00:00:34,039
giving you sort of a bird's eye view of

9
00:00:30,619 --> 00:00:39,190
the whole paper and so the first thing

10
00:00:34,039 --> 00:00:41,780
that I'm doing is I we are building

11
00:00:39,190 --> 00:00:44,659
differential distinguishes using machine

12
00:00:41,780 --> 00:00:48,409
learning for specs 32 64 which are

13
00:00:44,659 --> 00:00:50,440
superior to classical differential

14
00:00:48,409 --> 00:00:52,909
distinguishes even using the full

15
00:00:50,440 --> 00:00:55,068
difference distribution table which we

16
00:00:52,909 --> 00:00:57,170
also calculate for up to eight rounds of

17
00:00:55,069 --> 00:01:02,210
spec and one choose an input difference

18
00:00:57,170 --> 00:01:05,170
and these neural network based

19
00:01:02,210 --> 00:01:08,120
distinguishes achieve a higher

20
00:01:05,170 --> 00:01:09,799
classification accuracies and the

21
00:01:08,120 --> 00:01:11,600
difference and the differential

22
00:01:09,799 --> 00:01:13,789
distinguish are based on the different

23
00:01:11,600 --> 00:01:16,100
distribution table and the reason it can

24
00:01:13,789 --> 00:01:19,509
do this is that the neural network

25
00:01:16,100 --> 00:01:22,548
distinct based distinguisher exploits

26
00:01:19,509 --> 00:01:25,219
the ciphertext pair difference instead

27
00:01:22,549 --> 00:01:28,759
of the instead of the distribution of

28
00:01:25,219 --> 00:01:31,280
differences then having these

29
00:01:28,759 --> 00:01:33,549
distinguishes be used by easy an

30
00:01:31,280 --> 00:01:36,590
optimization that is basically that's a

31
00:01:33,549 --> 00:01:39,200
generic optimization procedure that is

32
00:01:36,590 --> 00:01:42,770
used for instance in machine learning to

33
00:01:39,200 --> 00:01:45,619
optimize hyper parameter sets basically

34
00:01:42,770 --> 00:01:47,780
it's a method that takes a function that

35
00:01:45,619 --> 00:01:49,640
is difficult to optimize but we are

36
00:01:47,780 --> 00:01:51,700
something about the shape of the

37
00:01:49,640 --> 00:01:54,200
function is known and where you want to

38
00:01:51,700 --> 00:01:59,270
find maybe the maximum of the function

39
00:01:54,200 --> 00:02:01,069
and tries to gain from some few

40
00:01:59,270 --> 00:02:03,380
evaluations of that expensive to

41
00:02:01,069 --> 00:02:05,359
evaluate function some knowledge about

42
00:02:03,380 --> 00:02:10,008
where the maximum is and he applies us

43
00:02:05,359 --> 00:02:13,010
two to the problem of searching for the

44
00:02:10,008 --> 00:02:16,910
last round key in respect and Crabtree

45
00:02:13,010 --> 00:02:17,959
using one of our euro distinguishes and

46
00:02:16,910 --> 00:02:21,290
this leads to a net

47
00:02:17,959 --> 00:02:24,709
that uses very few trial descriptions

48
00:02:21,290 --> 00:02:26,870
namely for 11 rounds for our 11th round

49
00:02:24,709 --> 00:02:30,049
attack we have in total a few million

50
00:02:26,870 --> 00:02:32,450
trial descriptions that we do and that's

51
00:02:30,049 --> 00:02:35,719
roughly that's a few million times less

52
00:02:32,450 --> 00:02:38,358
than the best attacks on 11 rounds that

53
00:02:35,719 --> 00:02:40,790
were previously in the literature so the

54
00:02:38,359 --> 00:02:43,870
best previous attacks take about two to

55
00:02:40,790 --> 00:02:46,760
the forty five trial descriptions and

56
00:02:43,870 --> 00:02:50,450
our attack takes a few million trial

57
00:02:46,760 --> 00:02:53,569
decryptions however evaluating a neural

58
00:02:50,450 --> 00:02:56,298
network is expensive so this advantage

59
00:02:53,569 --> 00:02:59,268
doesn't fully translate to a two to a

60
00:02:56,299 --> 00:03:01,220
speed-up of the attack what we get in

61
00:02:59,269 --> 00:03:03,049
the end for 11 rounds of SPECT is an

62
00:03:01,220 --> 00:03:07,459
attack that is roughly 200 times faster

63
00:03:03,049 --> 00:03:10,099
than the state of the art and we use

64
00:03:07,459 --> 00:03:12,469
also manual crypt analysis we use manual

65
00:03:10,099 --> 00:03:14,388
crypt analysis to define learning tasks

66
00:03:12,469 --> 00:03:17,299
to design the overall attack and to

67
00:03:14,389 --> 00:03:21,139
extend trained distinguishes two more

68
00:03:17,299 --> 00:03:23,359
rounds and you can absolutely try that

69
00:03:21,139 --> 00:03:28,430
at home because there is code available

70
00:03:23,359 --> 00:03:30,859
on github so one other things that is

71
00:03:28,430 --> 00:03:32,629
different about this attack compared to

72
00:03:30,859 --> 00:03:35,599
previous attacks is that this attack

73
00:03:32,629 --> 00:03:38,268
does not use the key schedulers the best

74
00:03:35,599 --> 00:03:40,668
previous attacks they generate a large

75
00:03:38,269 --> 00:03:44,930
number of candidates for the last four

76
00:03:40,669 --> 00:03:48,109
round keys and then test every every one

77
00:03:44,930 --> 00:03:49,849
of those keys against a known plaintext

78
00:03:48,109 --> 00:03:54,019
ciphertext pair and this is only

79
00:03:49,849 --> 00:03:56,298
possible because see because the key

80
00:03:54,019 --> 00:04:01,250
schedule is efficiently invertible and

81
00:03:56,299 --> 00:04:02,750
just attack that that this your network

82
00:04:01,250 --> 00:04:05,659
based attack doesn't need that because

83
00:04:02,750 --> 00:04:09,199
we can solve for the round keys one by

84
00:04:05,659 --> 00:04:10,870
one so we would be able to break 11

85
00:04:09,199 --> 00:04:13,759
rounds back this essentially the same

86
00:04:10,870 --> 00:04:16,459
complexity also if it used the free cat

87
00:04:13,759 --> 00:04:19,310
freaky schedule or if the key schedule

88
00:04:16,459 --> 00:04:22,310
was changed to not be efficiently

89
00:04:19,310 --> 00:04:25,430
invertible anymore so it's a bit more a

90
00:04:22,310 --> 00:04:28,849
bit more resilient against changes in

91
00:04:25,430 --> 00:04:31,110
key schedule the knurl distinguishes

92
00:04:28,849 --> 00:04:32,370
they can be

93
00:04:31,110 --> 00:04:37,230
to show that they can efficiently

94
00:04:32,370 --> 00:04:39,960
recognize sort of randomized output

95
00:04:37,230 --> 00:04:44,300
ciphertext pairs so if you take a pair

96
00:04:39,960 --> 00:04:46,979
of 32-bit values and this pair has a

97
00:04:44,300 --> 00:04:49,260
corresponds to a high likelihood output

98
00:04:46,980 --> 00:04:52,110
difference and it's not necessarily a

99
00:04:49,260 --> 00:04:54,120
pair that would appear with high

100
00:04:52,110 --> 00:04:57,180
likelihood in the output distribution

101
00:04:54,120 --> 00:05:00,810
and the neuro distinguishes can

102
00:04:57,180 --> 00:05:05,490
recognize that unlike unlike traditional

103
00:05:00,810 --> 00:05:07,710
differential distinguishes and we in the

104
00:05:05,490 --> 00:05:09,480
paper we show some actual examples of

105
00:05:07,710 --> 00:05:12,539
this so we show for instance one example

106
00:05:09,480 --> 00:05:15,060
of a ciphertext pair that gets an almost

107
00:05:12,540 --> 00:05:17,250
zero neural distinguisher score and

108
00:05:15,060 --> 00:05:21,570
which but which belongs to a 2 to the

109
00:05:17,250 --> 00:05:24,960
minus 26 likelihood output difference

110
00:05:21,570 --> 00:05:29,250
and it has in fact we show that it has a

111
00:05:24,960 --> 00:05:31,260
zero chance of being an actual that it

112
00:05:29,250 --> 00:05:35,490
cannot be that it's an impossible output

113
00:05:31,260 --> 00:05:37,469
pair actually and one other things that

114
00:05:35,490 --> 00:05:39,630
we do in the paper is we use few short

115
00:05:37,470 --> 00:05:42,780
learning on cryptographic problems huge

116
00:05:39,630 --> 00:05:48,690
art learning is something that is a well

117
00:05:42,780 --> 00:05:51,419
I mean common a common common idea about

118
00:05:48,690 --> 00:05:54,330
machine learning is that when you do you

119
00:05:51,420 --> 00:05:55,980
might be able to do interesting things

120
00:05:54,330 --> 00:05:58,740
with machine learning that you need lots

121
00:05:55,980 --> 00:06:01,200
of data whereas we humans we can

122
00:05:58,740 --> 00:06:05,070
sometimes learn from one example of you

123
00:06:01,200 --> 00:06:08,340
examples of some problem and do useful

124
00:06:05,070 --> 00:06:11,490
in fear ins about that problem later on

125
00:06:08,340 --> 00:06:14,039
and in machine learning this is also

126
00:06:11,490 --> 00:06:15,960
possible there are a range of techniques

127
00:06:14,040 --> 00:06:18,180
that go under the heading of huge sharp

128
00:06:15,960 --> 00:06:20,580
learning so you show a machine learning

129
00:06:18,180 --> 00:06:22,560
algorithm just a few examples of some

130
00:06:20,580 --> 00:06:24,719
output distributions and it might be

131
00:06:22,560 --> 00:06:27,720
able to recognize it output distribution

132
00:06:24,720 --> 00:06:29,250
and we also show that this is to some

133
00:06:27,720 --> 00:06:31,320
extent with some prior knowledge that

134
00:06:29,250 --> 00:06:33,210
this is possible to do inspect so we can

135
00:06:31,320 --> 00:06:35,610
observe for instance three rounds back

136
00:06:33,210 --> 00:06:37,500
for a few million examples with our

137
00:06:35,610 --> 00:06:40,770
random input difference and then give it

138
00:06:37,500 --> 00:06:42,540
six rounds back to classify and after

139
00:06:40,770 --> 00:06:43,318
seeing a few examples of the new

140
00:06:42,540 --> 00:06:45,839
distribution

141
00:06:43,319 --> 00:06:49,229
we'll get it we'll get our distinguishes

142
00:06:45,839 --> 00:06:52,099
that has some statistically some some

143
00:06:49,229 --> 00:06:56,219
advantage that is very statistically

144
00:06:52,099 --> 00:06:57,748
significant over random guessing and we

145
00:06:56,219 --> 00:07:00,058
can also use the few short learning

146
00:06:57,749 --> 00:07:02,759
capability to find good info differences

147
00:07:00,059 --> 00:07:06,119
if you didn't know them without using

148
00:07:02,759 --> 00:07:07,949
Prai or crypt analysis finally in the

149
00:07:06,119 --> 00:07:09,300
paper we also deliver some partial

150
00:07:07,949 --> 00:07:11,039
insight into the source of the

151
00:07:09,300 --> 00:07:13,979
additional signal basically showing that

152
00:07:11,039 --> 00:07:16,139
the neural networks have an internal

153
00:07:13,979 --> 00:07:18,808
data representation that groups the

154
00:07:16,139 --> 00:07:20,969
output cipher taxpayers into some more

155
00:07:18,809 --> 00:07:23,550
fine-grained system of equivalence

156
00:07:20,969 --> 00:07:26,789
classes than the different equivalence

157
00:07:23,550 --> 00:07:28,139
classes and as an additional benchmark

158
00:07:26,789 --> 00:07:30,748
we also developed differential

159
00:07:28,139 --> 00:07:33,270
distinguish us for five rounds back that

160
00:07:30,749 --> 00:07:35,279
exploit the ciphertext pair distribution

161
00:07:33,270 --> 00:07:36,869
perfectly and these are of course done a

162
00:07:35,279 --> 00:07:38,789
little bit better than the neural

163
00:07:36,869 --> 00:07:40,169
networks so say yeah

164
00:07:38,789 --> 00:07:44,009
they are a little bit better but also

165
00:07:40,169 --> 00:07:48,620
much slower so spec is a family of block

166
00:07:44,009 --> 00:07:51,689
ciphers design Plaza and si in 2013 and

167
00:07:48,620 --> 00:07:54,839
what member of this family that we are

168
00:07:51,689 --> 00:07:57,089
looking at today aspect 32 64 and this

169
00:07:54,839 --> 00:07:59,849
is the smallest member of this family

170
00:07:57,089 --> 00:08:03,449
it's a lightweight error X construction

171
00:07:59,849 --> 00:08:05,219
has 22 rounds and it has all nonlinear

172
00:08:03,449 --> 00:08:08,729
key schedules that basically reuses the

173
00:08:05,219 --> 00:08:11,370
round function in prior work the best

174
00:08:08,729 --> 00:08:14,930
attacks on 11 round specs used about 2

175
00:08:11,370 --> 00:08:18,959
to the 46 reduced spec encryptions and

176
00:08:14,930 --> 00:08:22,649
used 2014 chosen plaintext on average

177
00:08:18,959 --> 00:08:26,129
there are also attacks on 12 to 15

178
00:08:22,649 --> 00:08:29,219
rounds and but they have higher

179
00:08:26,129 --> 00:08:31,050
complexity and these attacks they would

180
00:08:29,219 --> 00:08:32,399
all depend on the key schedule being

181
00:08:31,050 --> 00:08:35,549
efficiently invertible

182
00:08:32,399 --> 00:08:37,559
whereas our attack breaks 11 rounds in

183
00:08:35,549 --> 00:08:40,859
roughly 15 minutes on average on one

184
00:08:37,559 --> 00:08:43,859
thread of a normal single on a normal

185
00:08:40,860 --> 00:08:45,870
desktop computer and this is roughly

186
00:08:43,860 --> 00:08:48,899
equivalent to about two to the 38

187
00:08:45,870 --> 00:08:50,610
reduced spec encryption and one thing

188
00:08:48,899 --> 00:08:53,639
that I should probably emphasize is that

189
00:08:50,610 --> 00:08:55,800
this represents one time data trade-off

190
00:08:53,639 --> 00:08:56,329
among many so if you used more data we

191
00:08:55,800 --> 00:08:58,628
could build

192
00:08:56,329 --> 00:09:02,388
faster text for instance so it could be

193
00:08:58,629 --> 00:09:07,999
roughly about a factor of 10 or maybe 20

194
00:09:02,389 --> 00:09:11,059
faster if we used a lot more data but

195
00:09:07,999 --> 00:09:13,579
this in this with these settings the

196
00:09:11,059 --> 00:09:16,160
tech is comparable in data complexity to

197
00:09:13,579 --> 00:09:18,349
the best attacks that are known and so

198
00:09:16,160 --> 00:09:22,639
it makes a attack nicely comparable to

199
00:09:18,350 --> 00:09:25,879
the literature now to calculate the

200
00:09:22,639 --> 00:09:28,610
difference distribution of of round

201
00:09:25,879 --> 00:09:31,639
produced spec what to to get a baseline

202
00:09:28,610 --> 00:09:35,420
for our distinguishes we do basically

203
00:09:31,639 --> 00:09:39,579
just just standard things so we treat

204
00:09:35,420 --> 00:09:41,899
the bit street spec we treat the

205
00:09:39,579 --> 00:09:44,149
differential transitions from one round

206
00:09:41,899 --> 00:09:46,989
to the other as a Markov chain this 2

207
00:09:44,149 --> 00:09:49,939
2013 2 minus 1 possible states and we

208
00:09:46,989 --> 00:09:52,399
calculate the distribution the predicted

209
00:09:49,939 --> 00:09:55,488
distribution of that mark of that Markov

210
00:09:52,399 --> 00:09:58,549
process given one known input difference

211
00:09:55,489 --> 00:10:00,319
and this gives us one row of the

212
00:09:58,549 --> 00:10:04,189
difference distribution table for specs

213
00:10:00,319 --> 00:10:07,389
32 64 and we choose this input

214
00:10:04,189 --> 00:10:10,040
difference here which is which oops you

215
00:10:07,389 --> 00:10:12,589
choose this input difference here which

216
00:10:10,040 --> 00:10:14,089
is known from the literature we

217
00:10:12,589 --> 00:10:16,309
calculate the induced difference

218
00:10:14,089 --> 00:10:18,379
distribution then there are various

219
00:10:16,309 --> 00:10:20,209
sources of model arab for instance we

220
00:10:18,379 --> 00:10:23,089
use double precision arithmetic and not

221
00:10:20,209 --> 00:10:25,878
excel exact arithmetic then there could

222
00:10:23,089 --> 00:10:27,889
be dependencies between transitions but

223
00:10:25,879 --> 00:10:29,869
empirically this model works very well I

224
00:10:27,889 --> 00:10:31,610
mean we have checked some of we have

225
00:10:29,869 --> 00:10:33,290
checked for instance the highest

226
00:10:31,610 --> 00:10:35,360
likelihood transitions and they are

227
00:10:33,290 --> 00:10:38,480
predicted extremely varied by this model

228
00:10:35,360 --> 00:10:40,730
and the whole process is straightforward

229
00:10:38,480 --> 00:10:42,889
but it's somewhat expensive it takes a

230
00:10:40,730 --> 00:10:45,470
few hundred CPU days of computing time

231
00:10:42,889 --> 00:10:48,350
and about 35 gigabytes of memory for the

232
00:10:45,470 --> 00:10:53,239
final computed difference distribution

233
00:10:48,350 --> 00:10:55,399
table machine learning is an umbrella

234
00:10:53,239 --> 00:10:57,980
term for various techniques that aim to

235
00:10:55,399 --> 00:10:58,790
make agents or machines learn from

236
00:10:57,980 --> 00:11:01,009
experience

237
00:10:58,790 --> 00:11:02,868
it's very useful for some problems that

238
00:11:01,009 --> 00:11:05,929
are for also for some high profile

239
00:11:02,869 --> 00:11:08,440
problems but it's also not difficult to

240
00:11:05,929 --> 00:11:10,839
find simple problems where am I

241
00:11:08,440 --> 00:11:12,670
and Ella proach would fail or struggle

242
00:11:10,839 --> 00:11:16,329
very badly to find a solution for

243
00:11:12,670 --> 00:11:18,128
instance just if you dumped 64-bit

244
00:11:16,329 --> 00:11:21,758
numbers and the parity on a neural

245
00:11:18,129 --> 00:11:24,310
network without doing anything else and

246
00:11:21,759 --> 00:11:26,050
I think it that would be a fairly hard

247
00:11:24,310 --> 00:11:28,569
problem for the neural network to learn

248
00:11:26,050 --> 00:11:31,240
to calculate the parity you can do it

249
00:11:28,569 --> 00:11:35,378
but you have to you have to give it some

250
00:11:31,240 --> 00:11:37,750
other some other training data and there

251
00:11:35,379 --> 00:11:39,639
have been spectacular successes in

252
00:11:37,750 --> 00:11:42,819
recent time for instance the game of Go

253
00:11:39,639 --> 00:11:44,170
or poker or machine translation and I

254
00:11:42,819 --> 00:11:45,910
think it's also important to realize

255
00:11:44,170 --> 00:11:48,519
that these mostly use machine learning

256
00:11:45,910 --> 00:11:50,680
just as one crucial part of a larger

257
00:11:48,519 --> 00:11:54,490
problem solving system and that's not

258
00:11:50,680 --> 00:11:58,120
different here so on your network is

259
00:11:54,490 --> 00:12:00,189
basically differentiable family of

260
00:11:58,120 --> 00:12:02,230
functions that is parameterize by some

261
00:12:00,189 --> 00:12:06,399
date so you have some inputs going in

262
00:12:02,230 --> 00:12:09,069
here at the you have some inputs going

263
00:12:06,399 --> 00:12:10,990
in at the input layer and these inputs

264
00:12:09,069 --> 00:12:12,969
are then multiplied these are real value

265
00:12:10,990 --> 00:12:15,130
values yeah as a multiplied by some

266
00:12:12,970 --> 00:12:16,839
real-valued weights and you apply some

267
00:12:15,130 --> 00:12:20,079
activation functions and pass to the

268
00:12:16,839 --> 00:12:23,470
next layer and you can approximate a

269
00:12:20,079 --> 00:12:25,239
wide range of functions by these neural

270
00:12:23,470 --> 00:12:27,250
networks and to find a good neural

271
00:12:25,240 --> 00:12:29,860
network for a particular problem you

272
00:12:27,250 --> 00:12:32,560
define a loss function you let it loose

273
00:12:29,860 --> 00:12:34,870
on training data and you tries and to

274
00:12:32,560 --> 00:12:37,630
find good weights by stochastic gradient

275
00:12:34,870 --> 00:12:39,279
descent and then you test your sister

276
00:12:37,630 --> 00:12:41,350
whether your system actually has learned

277
00:12:39,279 --> 00:12:43,930
anything you test you find out by

278
00:12:41,350 --> 00:12:46,360
testing it on some other data on data

279
00:12:43,930 --> 00:12:48,069
that was not the training data so on

280
00:12:46,360 --> 00:12:50,350
cryptology machine learning hasn't been

281
00:12:48,069 --> 00:12:52,540
used very much and i think the outlook

282
00:12:50,350 --> 00:12:55,420
of the community has been fairly bleak

283
00:12:52,540 --> 00:12:57,430
in terms of machine learning being

284
00:12:55,420 --> 00:12:59,769
useful for crypt analysis for instance

285
00:12:57,430 --> 00:13:01,239
or I have found here two votes one by

286
00:12:59,769 --> 00:13:03,990
Bruce Schneier

287
00:13:01,240 --> 00:13:06,430
from his textbook applied cryptography

288
00:13:03,990 --> 00:13:08,920
neural nets work well in structured

289
00:13:06,430 --> 00:13:10,359
environments but not in the high entropy

290
00:13:08,920 --> 00:13:12,699
seemingly random world of cryptography

291
00:13:10,360 --> 00:13:15,610
and then another folk from tumor-free

292
00:13:12,699 --> 00:13:18,109
learning researchers who wrote a fairly

293
00:13:15,610 --> 00:13:21,320
high-profile paper on

294
00:13:18,110 --> 00:13:24,620
- neural networks that learn to protect

295
00:13:21,320 --> 00:13:26,960
their communications from certain

296
00:13:24,620 --> 00:13:29,480
network if that was trying to break

297
00:13:26,960 --> 00:13:31,040
their communications and they also wrote

298
00:13:29,480 --> 00:13:32,660
neural networks are generally not meant

299
00:13:31,040 --> 00:13:35,150
to be great at cryptography so I think

300
00:13:32,660 --> 00:13:37,579
this sums up the community view the

301
00:13:35,150 --> 00:13:39,260
prevailing law fairly well but they have

302
00:13:37,580 --> 00:13:41,690
been a lot of works on side channel

303
00:13:39,260 --> 00:13:44,330
analysis so there have been some other

304
00:13:41,690 --> 00:13:47,450
works on machine learning and cryptology

305
00:13:44,330 --> 00:13:49,280
is some work by cream of McKagan and

306
00:13:47,450 --> 00:13:51,410
shamea who used euro networks to break a

307
00:13:49,280 --> 00:13:53,449
public key encryption scheme that was

308
00:13:51,410 --> 00:13:55,550
itself based on neural networks and one

309
00:13:53,450 --> 00:13:58,070
worked the same grade almost building a

310
00:13:55,550 --> 00:14:01,760
model of the Enigma using a recurrent

311
00:13:58,070 --> 00:14:05,450
neural network and also this work a

312
00:14:01,760 --> 00:14:08,060
recent work from AI CLR 2018 that showed

313
00:14:05,450 --> 00:14:10,490
that guns can break simple short period

314
00:14:08,060 --> 00:14:12,609
Visionnaire ciphers in an unsupervised

315
00:14:10,490 --> 00:14:15,940
setting that was a work aimed at

316
00:14:12,610 --> 00:14:19,430
improving machine trance machine

317
00:14:15,940 --> 00:14:21,680
translation techniques so to train a

318
00:14:19,430 --> 00:14:23,630
distinguisher for reduced spec we just

319
00:14:21,680 --> 00:14:26,839
generate a few million real and random

320
00:14:23,630 --> 00:14:28,580
examples a ciphertext pairs with our

321
00:14:26,840 --> 00:14:30,530
choose an input difference for the real

322
00:14:28,580 --> 00:14:32,390
pairs and this is very quick this takes

323
00:14:30,530 --> 00:14:34,400
a few seconds then we train a deep

324
00:14:32,390 --> 00:14:36,199
resilient newer networks that's a type

325
00:14:34,400 --> 00:14:39,980
of network that has been quite

326
00:14:36,200 --> 00:14:43,270
successful for a variety of applications

327
00:14:39,980 --> 00:14:47,840
from image recognition to board games

328
00:14:43,270 --> 00:14:50,510
and then we and for five to seven rounds

329
00:14:47,840 --> 00:14:53,600
of spec encryption if you have a GTX 180

330
00:14:50,510 --> 00:14:55,010
TI graphics card that gives you a better

331
00:14:53,600 --> 00:14:56,660
classifiers and the difference

332
00:14:55,010 --> 00:14:59,750
distribution table after a few minutes

333
00:14:56,660 --> 00:15:02,030
of calculation so this is very quick and

334
00:14:59,750 --> 00:15:04,490
for several rounds you can also use us

335
00:15:02,030 --> 00:15:07,250
like a more expensive training scheme to

336
00:15:04,490 --> 00:15:09,740
get some better network and for eight

337
00:15:07,250 --> 00:15:11,870
rounds you just fail but you can use

338
00:15:09,740 --> 00:15:16,250
curricular training ie designing a

339
00:15:11,870 --> 00:15:19,520
sequence of a sequence of intermediate

340
00:15:16,250 --> 00:15:22,130
tasks to learn on and then you get also

341
00:15:19,520 --> 00:15:23,870
a distinguished are these distinguished

342
00:15:22,130 --> 00:15:26,180
as they are quite small and they have

343
00:15:23,870 --> 00:15:28,400
about 44,000 weights and - if you

344
00:15:26,180 --> 00:15:30,260
truncate the earth weights of the seven

345
00:15:28,400 --> 00:15:31,910
round distinguish are two 16-bit floats

346
00:15:30,260 --> 00:15:33,470
then this doesn't see

347
00:15:31,910 --> 00:15:35,600
to hurt the distinguisher so you can

348
00:15:33,470 --> 00:15:39,050
pack the distinguisher in about 90 KB

349
00:15:35,600 --> 00:15:42,590
storage instead of 35 gigabytes for the

350
00:15:39,050 --> 00:15:44,780
difference distribution table and the

351
00:15:42,590 --> 00:15:47,680
results are quite good you get we get

352
00:15:44,780 --> 00:15:49,880
better better

353
00:15:47,680 --> 00:15:52,219
accuracies and the difference

354
00:15:49,880 --> 00:15:53,960
distribution table across the board in

355
00:15:52,220 --> 00:15:58,070
all of these tasks for five to eight

356
00:15:53,960 --> 00:16:01,430
rounds and advantage the advantages are

357
00:15:58,070 --> 00:16:03,620
quite small but if you have more than

358
00:16:01,430 --> 00:16:06,500
one ciphertext players and of course it

359
00:16:03,620 --> 00:16:10,940
will this advantage will be slit will be

360
00:16:06,500 --> 00:16:12,380
larger so to build a nine round attack

361
00:16:10,940 --> 00:16:14,360
from this we take the seven round

362
00:16:12,380 --> 00:16:16,310
distinguish us we add one round at the

363
00:16:14,360 --> 00:16:19,310
bottom by manipulating the inputs which

364
00:16:16,310 --> 00:16:21,229
we can do because the first edition of

365
00:16:19,310 --> 00:16:24,170
around key happens after the only

366
00:16:21,230 --> 00:16:27,200
nonlinear operation which is he which is

367
00:16:24,170 --> 00:16:29,240
a modular addition and we add one round

368
00:16:27,200 --> 00:16:32,990
at the top by brute force trial

369
00:16:29,240 --> 00:16:35,300
decryption and then we use a number of

370
00:16:32,990 --> 00:16:39,590
chosen plaintext pairs so we use in this

371
00:16:35,300 --> 00:16:41,660
case for instance 64 and classify ISA

372
00:16:39,590 --> 00:16:44,540
buys a difference distribution table or

373
00:16:41,660 --> 00:16:47,420
buys the neural network and when we do

374
00:16:44,540 --> 00:16:49,880
this we see that the neural network

375
00:16:47,420 --> 00:16:51,829
graph gives much better mean and median

376
00:16:49,880 --> 00:16:54,020
key ranks than the difference

377
00:16:51,830 --> 00:16:55,760
distribution table so for instance a

378
00:16:54,020 --> 00:16:58,490
median key rank of the neural network as

379
00:16:55,760 --> 00:17:00,950
one and difference distribution tables

380
00:16:58,490 --> 00:17:04,849
nine swords imbalance the mean is

381
00:17:00,950 --> 00:17:09,050
roughly five times lower so then to

382
00:17:04,849 --> 00:17:11,659
build to build an effective attack and

383
00:17:09,050 --> 00:17:13,639
if an effective key recovery policy we

384
00:17:11,660 --> 00:17:16,010
look at the wrong key randomization had

385
00:17:13,640 --> 00:17:18,140
processes which would if the one P of

386
00:17:16,010 --> 00:17:20,420
randomization hypothesis were to holds

387
00:17:18,140 --> 00:17:22,339
and romkey decryptions will tell you

388
00:17:20,420 --> 00:17:25,550
nothing so you would maybe see one of

389
00:17:22,339 --> 00:17:27,770
your team maybe one one ciphertext pair

390
00:17:25,550 --> 00:17:29,389
being much higher rated than all the

391
00:17:27,770 --> 00:17:30,920
others but you wouldn't but from the

392
00:17:29,390 --> 00:17:34,010
wrong key descriptions you wouldn't get

393
00:17:30,920 --> 00:17:36,350
any information about the about the

394
00:17:34,010 --> 00:17:40,990
value of the real sub key that you're

395
00:17:36,350 --> 00:17:42,919
targeting but if you look at the at the

396
00:17:40,990 --> 00:17:44,150
distinguisher response of the newer

397
00:17:42,920 --> 00:17:46,730
distinguish are

398
00:17:44,150 --> 00:17:48,860
as a function of the difference of the

399
00:17:46,730 --> 00:17:50,990
bitwise difference of the ROM key to the

400
00:17:48,860 --> 00:17:53,090
real key then you actually see that

401
00:17:50,990 --> 00:17:55,309
there's a lot of information in there in

402
00:17:53,090 --> 00:17:58,909
the in the ROM key descriptions and we

403
00:17:55,309 --> 00:18:01,040
use that by we use that by trying to

404
00:17:58,910 --> 00:18:03,590
decrypt a small set of cipher texts

405
00:18:01,040 --> 00:18:05,629
under a few random keys then we observe

406
00:18:03,590 --> 00:18:07,429
the average distinguish our response and

407
00:18:05,630 --> 00:18:09,140
then we derive as new set of key

408
00:18:07,430 --> 00:18:12,050
hypothesis that maximizes the likelihood

409
00:18:09,140 --> 00:18:14,390
of the distinguish our responses and we

410
00:18:12,050 --> 00:18:16,520
repeat this a few times like 3 or 5

411
00:18:14,390 --> 00:18:18,860
times was maybe something like 32 keys

412
00:18:16,520 --> 00:18:25,540
and then one of those keys that we find

413
00:18:18,860 --> 00:18:28,790
very usual ibiza the right key and and

414
00:18:25,540 --> 00:18:30,889
to build an 11th round attack v as an

415
00:18:28,790 --> 00:18:32,480
extent this nine round attack to 11

416
00:18:30,890 --> 00:18:35,720
rounds by adding a two round initial

417
00:18:32,480 --> 00:18:37,460
different differential trail we recreate

418
00:18:35,720 --> 00:18:40,790
the conditions of the nine round attack

419
00:18:37,460 --> 00:18:43,160
ie that we have a couple of plaintext

420
00:18:40,790 --> 00:18:45,620
pairs with this desired input difference

421
00:18:43,160 --> 00:18:48,290
by using some neutral bits for the

422
00:18:45,620 --> 00:18:50,570
initial two round trail and he applies a

423
00:18:48,290 --> 00:18:52,730
key search policy to derive a short list

424
00:18:50,570 --> 00:18:54,800
of key candidates and we detect success

425
00:18:52,730 --> 00:18:56,750
by looking at distinguish our scores

426
00:18:54,800 --> 00:18:58,669
returned and if the scores look good and

427
00:18:56,750 --> 00:19:01,100
we derive a short list of key candidates

428
00:18:58,670 --> 00:19:03,559
for one more round and if those scores

429
00:19:01,100 --> 00:19:06,559
and look good which are then judged by

430
00:19:03,559 --> 00:19:09,559
another neural distinguisher for one

431
00:19:06,559 --> 00:19:11,899
less round then if they are sufficiently

432
00:19:09,559 --> 00:19:14,149
high V output a key guess for the last

433
00:19:11,900 --> 00:19:16,550
two round keys and otherwise we ask for

434
00:19:14,150 --> 00:19:19,640
more data or we look again a data

435
00:19:16,550 --> 00:19:21,830
already acquired so in conclusion I

436
00:19:19,640 --> 00:19:23,900
think one can say machine learning

437
00:19:21,830 --> 00:19:26,510
worked really well in this instance see

438
00:19:23,900 --> 00:19:29,960
neural network efficiently exploits the

439
00:19:26,510 --> 00:19:31,790
ciphertext pair distribution it is

440
00:19:29,960 --> 00:19:34,309
crucial to choose the right learning

441
00:19:31,790 --> 00:19:37,850
task and choosing a good model structure

442
00:19:34,309 --> 00:19:39,920
to in order to be successful we also

443
00:19:37,850 --> 00:19:41,719
used some manual crypt analysis to

444
00:19:39,920 --> 00:19:44,809
derive a competitive attack from the

445
00:19:41,720 --> 00:19:47,210
distinguish us we automatically more or

446
00:19:44,809 --> 00:19:49,370
less derived in efficient key search

447
00:19:47,210 --> 00:19:52,820
policy that is very helpful to for

448
00:19:49,370 --> 00:19:55,310
making a fast attack and one thing that

449
00:19:52,820 --> 00:19:56,820
I would like to also add that in the

450
00:19:55,310 --> 00:19:58,950
neural networks that are not

451
00:19:56,820 --> 00:20:01,350
they have a reputation for being black

452
00:19:58,950 --> 00:20:03,779
boxes but in this context I think it's

453
00:20:01,350 --> 00:20:05,519
more it's more appropriate to think of

454
00:20:03,779 --> 00:20:07,200
them as grey boxes because for instance

455
00:20:05,519 --> 00:20:09,029
with just access to a black box

456
00:20:07,200 --> 00:20:10,590
distinguish ah you wouldn't be able to

457
00:20:09,029 --> 00:20:12,990
do the few short learning but with

458
00:20:10,590 --> 00:20:15,360
access to a neural network that has

459
00:20:12,990 --> 00:20:17,220
learned to recognize three rounds back

460
00:20:15,360 --> 00:20:19,740
you can very quickly Zen you learn to

461
00:20:17,220 --> 00:20:22,350
recognize sixth-round spectrum using

462
00:20:19,740 --> 00:20:26,190
input distribution and again the code is

463
00:20:22,350 --> 00:20:35,490
all and github and that's is that I'm

464
00:20:26,190 --> 00:20:37,139
done thank you very much I think an

465
00:20:35,490 --> 00:20:40,259
immediate question would be whether this

466
00:20:37,139 --> 00:20:42,379
applies also to Simon the techniques

467
00:20:40,259 --> 00:20:44,610
apply also to other block ciphers I

468
00:20:42,379 --> 00:20:47,850
expect they will apply to other block

469
00:20:44,610 --> 00:20:49,620
ciphers yeah I would expect so I mean

470
00:20:47,850 --> 00:20:51,899
you may not be able I think for Markov

471
00:20:49,620 --> 00:20:55,229
ciphers you shouldn't obviously be able

472
00:20:51,899 --> 00:20:56,699
to get something this thing will forever

473
00:20:55,230 --> 00:20:59,580
that was better than the difference

474
00:20:56,700 --> 00:21:02,399
distribution table but I would say it's

475
00:20:59,580 --> 00:21:04,799
nothing I think that respect specific

476
00:21:02,399 --> 00:21:06,570
here and I didn't try it on a million

477
00:21:04,799 --> 00:21:09,418
ciphers I tried it on once I first

478
00:21:06,570 --> 00:21:11,039
inspect and divert respect so I would

479
00:21:09,419 --> 00:21:14,159
definitely expect that this will work

480
00:21:11,039 --> 00:21:23,070
for other ciphers yeah okay is there any

481
00:21:14,159 --> 00:21:26,309
other question question about an

482
00:21:23,070 --> 00:21:28,950
alternative a technique which is used by

483
00:21:26,309 --> 00:21:32,009
the machine learning community and this

484
00:21:28,950 --> 00:21:35,490
is the auto encoder approach when you

485
00:21:32,009 --> 00:21:39,360
have time to use auto encoder you try to

486
00:21:35,490 --> 00:21:42,120
teach there the machine learning to find

487
00:21:39,360 --> 00:21:44,389
a kind of condensed representation of

488
00:21:42,120 --> 00:21:44,389
the day

489
00:21:47,179 --> 00:21:52,909
auto-encoders and whether they were

490
00:21:49,159 --> 00:21:55,299
successful in analytic approaches maybe

491
00:21:52,909 --> 00:21:58,789
you're giving a bunch of plain text

492
00:21:55,299 --> 00:22:06,109
there's a bunch of cipher text your time

493
00:21:58,789 --> 00:22:09,169
to find a concise representation that I

494
00:22:06,109 --> 00:22:13,178
am not aware of anybody who has tried

495
00:22:09,169 --> 00:22:16,609
this I've tried some I've tried some

496
00:22:13,179 --> 00:22:19,279
some unsupervised techniques to find

497
00:22:16,609 --> 00:22:22,399
some connections between between

498
00:22:19,279 --> 00:22:23,629
plaintext and ciphertext I I haven't

499
00:22:22,399 --> 00:22:25,728
tried the auto-encoder

500
00:22:23,629 --> 00:22:29,019
but I've tried some other techniques

501
00:22:25,729 --> 00:22:30,919
that was the results weren't

502
00:22:29,019 --> 00:22:35,479
uninteresting that there was nothing

503
00:22:30,919 --> 00:22:38,239
sort of said that that was clearly

504
00:22:35,479 --> 00:22:46,869
better than the known techniques that I

505
00:22:38,239 --> 00:22:49,890
found in that area any further question

506
00:22:46,869 --> 00:22:55,069
then let's thank you

507
00:22:49,890 --> 00:22:55,069
[Applause]

