1
00:00:04,800 --> 00:00:09,600
thank you very much so yeah this is John

2
00:00:07,439 --> 00:00:11,280
work with my former and recently

3
00:00:09,600 --> 00:00:12,389
graduated student Vinny Chen well

4
00:00:11,280 --> 00:00:16,890
unfortunately couldn't make it here to

5
00:00:12,389 --> 00:00:19,529
give the talk so broadly this work is

6
00:00:16,890 --> 00:00:21,930
about designing functions that are

7
00:00:19,529 --> 00:00:23,790
moderately hard to compute and in

8
00:00:21,930 --> 00:00:25,290
addition to being hard to invert so

9
00:00:23,790 --> 00:00:27,150
these are of course useful in particular

10
00:00:25,290 --> 00:00:29,669
in the context of hashing passwords but

11
00:00:27,150 --> 00:00:33,000
also be used say for proofs of work in

12
00:00:29,670 --> 00:00:35,489
cryptocurrencies and modern harness here

13
00:00:33,000 --> 00:00:37,019
means that on the one hand the function

14
00:00:35,489 --> 00:00:38,730
should be sufficiently expensive to

15
00:00:37,020 --> 00:00:40,860
compute for example to slow down brute

16
00:00:38,730 --> 00:00:41,910
force password cracking attacks but on

17
00:00:40,860 --> 00:00:44,820
the other hand you should not be

18
00:00:41,910 --> 00:00:46,319
excessively expensive to slow down or to

19
00:00:44,820 --> 00:00:48,960
increase to affect the performance of a

20
00:00:46,320 --> 00:00:52,320
system using it right and hence the

21
00:00:48,960 --> 00:00:54,320
balancing act so also these moderate

22
00:00:52,320 --> 00:00:57,570
harness should be somewhat egalitarian

23
00:00:54,320 --> 00:00:59,969
so we will wish that a dedicated

24
00:00:57,570 --> 00:01:01,920
hardware should not give significant

25
00:00:59,969 --> 00:01:03,600
benefit in evaluating the function in

26
00:01:01,920 --> 00:01:05,610
order this property is really not

27
00:01:03,600 --> 00:01:08,159
obvious to enforce for example legacy

28
00:01:05,610 --> 00:01:10,920
password hashing algorithms do not

29
00:01:08,159 --> 00:01:13,920
satisfy it and so in an effort to

30
00:01:10,920 --> 00:01:16,079
satisfy this property the notion of

31
00:01:13,920 --> 00:01:18,060
memory functions was proposed and a

32
00:01:16,079 --> 00:01:21,149
memory her function or mhf for short is

33
00:01:18,060 --> 00:01:23,340
a moderately hard function that a

34
00:01:21,149 --> 00:01:25,920
subject to a time memory trade-off in

35
00:01:23,340 --> 00:01:28,979
its evaluation so what I mean by that is

36
00:01:25,920 --> 00:01:32,130
that moderately fast evaluation requires

37
00:01:28,979 --> 00:01:34,590
large memory whereas with small memory

38
00:01:32,130 --> 00:01:38,189
the evaluation should be slow right and

39
00:01:34,590 --> 00:01:39,659
this should imply a high area time cost

40
00:01:38,189 --> 00:01:41,098
for example in hardware which in turn

41
00:01:39,659 --> 00:01:44,700
implies a high dollar cost for

42
00:01:41,099 --> 00:01:48,599
fabrication of the hardware and so this

43
00:01:44,700 --> 00:01:50,340
as its so the goal of memory harness was

44
00:01:48,599 --> 00:01:52,169
present in a number of practical designs

45
00:01:50,340 --> 00:01:54,719
for password hashing including a script

46
00:01:52,169 --> 00:01:56,848
and organ to the winner of the password

47
00:01:54,719 --> 00:01:58,889
hashing competition and also it was a

48
00:01:56,849 --> 00:02:01,709
substantial effort and the theoretical

49
00:01:58,889 --> 00:02:03,539
end to validate the memory harness of

50
00:02:01,709 --> 00:02:05,099
this design which has led to a large

51
00:02:03,539 --> 00:02:07,139
number of papers over the last few years

52
00:02:05,099 --> 00:02:08,280
and if your paper is not up here you're

53
00:02:07,139 --> 00:02:09,750
really just because I ran out of space

54
00:02:08,280 --> 00:02:12,930
there are really many papers in this

55
00:02:09,750 --> 00:02:14,220
corner but a common denominator of all

56
00:02:12,930 --> 00:02:16,080
of these works which is the starting

57
00:02:14,220 --> 00:02:18,180
point of this work is that they all

58
00:02:16,080 --> 00:02:20,640
treat memory horror functions

59
00:02:18,180 --> 00:02:23,129
as modes of operation of some underlying

60
00:02:20,640 --> 00:02:24,988
mon elliptical hash function as we will

61
00:02:23,129 --> 00:02:28,260
see this is not really an accurate model

62
00:02:24,989 --> 00:02:29,489
for reality well before I get there

63
00:02:28,260 --> 00:02:31,739
just to make me a little bit more

64
00:02:29,489 --> 00:02:33,750
concrete let me first introduce the

65
00:02:31,739 --> 00:02:35,010
large class of memory hire functions

66
00:02:33,750 --> 00:02:37,829
that we will deal with in this work

67
00:02:35,010 --> 00:02:39,780
which is that of data independent memory

68
00:02:37,829 --> 00:02:44,000
heart functions or IMH FS which include

69
00:02:39,780 --> 00:02:47,900
most designs to date so an IM h f is

70
00:02:44,000 --> 00:02:50,459
defined by a direct acyclic graph or dag

71
00:02:47,900 --> 00:02:52,530
g which is a certain number of vertices

72
00:02:50,459 --> 00:02:54,810
and one of more sources and sinks so in

73
00:02:52,530 --> 00:02:57,090
this example it's one each and the

74
00:02:54,810 --> 00:02:59,849
evaluation of the function proceeds by

75
00:02:57,090 --> 00:03:01,889
assigning labels to vertices and in

76
00:02:59,849 --> 00:03:03,209
particular this is implicit here and in

77
00:03:01,889 --> 00:03:05,129
the rest of the talk but the source

78
00:03:03,209 --> 00:03:07,019
labels are going to be some value

79
00:03:05,129 --> 00:03:09,899
depending on dependent on the actual

80
00:03:07,019 --> 00:03:12,389
input to the function and every order

81
00:03:09,900 --> 00:03:14,700
label of a vertex is the output of a so

82
00:03:12,389 --> 00:03:17,909
called labeling function apply to the

83
00:03:14,700 --> 00:03:20,250
labels of the predecessors in finally

84
00:03:17,909 --> 00:03:21,978
the output of the function is the label

85
00:03:20,250 --> 00:03:24,810
of the sink

86
00:03:21,979 --> 00:03:28,620
now in all of these analysis I mentioned

87
00:03:24,810 --> 00:03:30,030
before the labeling function is a

88
00:03:28,620 --> 00:03:32,699
monolithic hash function which in

89
00:03:30,030 --> 00:03:36,090
security proofs is modeled as a random

90
00:03:32,699 --> 00:03:37,260
Oracle and so what method fear in here

91
00:03:36,090 --> 00:03:39,569
will tell you to leave the graph has

92
00:03:37,260 --> 00:03:41,489
some good comunitaria property then the

93
00:03:39,569 --> 00:03:43,888
function is memory hard in the random

94
00:03:41,489 --> 00:03:46,079
Oracle model so here we want to have a

95
00:03:43,889 --> 00:03:48,689
closer look at this assumption that the

96
00:03:46,079 --> 00:03:51,209
labeling function is a random Oracle and

97
00:03:48,689 --> 00:03:52,888
in particular if you look at concrete

98
00:03:51,209 --> 00:03:55,079
associations of memory her function

99
00:03:52,889 --> 00:03:57,000
actual designs what we see is that the

100
00:03:55,079 --> 00:03:59,280
labels are usually very large so we're

101
00:03:57,000 --> 00:04:00,840
talking about thousands of bits and so

102
00:03:59,280 --> 00:04:03,120
it's already very hard to use an

103
00:04:00,840 --> 00:04:04,829
off-the-shelf hash function and plus you

104
00:04:03,120 --> 00:04:07,889
might want some more properties that

105
00:04:04,829 --> 00:04:09,930
we'll see later and so what what what

106
00:04:07,889 --> 00:04:12,720
designer is doing sted is they come up

107
00:04:09,930 --> 00:04:14,099
with a dark constructions I mean often

108
00:04:12,720 --> 00:04:16,560
based on usually based on some

109
00:04:14,099 --> 00:04:19,409
underlying primitive here it's a it's a

110
00:04:16,560 --> 00:04:21,690
permutation and these designs are

111
00:04:19,409 --> 00:04:23,630
usually not random Oracle's at all even

112
00:04:21,690 --> 00:04:26,370
if you assume that the underlying

113
00:04:23,630 --> 00:04:27,779
primitive behaves ideally so this is an

114
00:04:26,370 --> 00:04:29,639
example of a design which is inspired

115
00:04:27,779 --> 00:04:30,940
from what SPF does but I could have put

116
00:04:29,639 --> 00:04:32,620
here the design of ardent

117
00:04:30,940 --> 00:04:35,230
Jeremiah is going to talk about in the

118
00:04:32,620 --> 00:04:36,610
next doc and it's a similar story right

119
00:04:35,230 --> 00:04:39,370
so we would like to understand what's

120
00:04:36,610 --> 00:04:40,810
happening under the hood here and this

121
00:04:39,370 --> 00:04:42,640
was really not done and it's not clear

122
00:04:40,810 --> 00:04:45,340
whether existing analysis apply to

123
00:04:42,640 --> 00:04:47,950
actual constructions and so I should

124
00:04:45,340 --> 00:04:49,150
mention here that Jeremiah in the next

125
00:04:47,950 --> 00:04:51,370
talk is going to talk about some

126
00:04:49,150 --> 00:04:53,380
concurrent work that also started

127
00:04:51,370 --> 00:04:55,920
looking under the hood and see what

128
00:04:53,380 --> 00:04:59,710
happens within actual constructions of

129
00:04:55,920 --> 00:05:01,630
labeling functions okay so the main goal

130
00:04:59,710 --> 00:05:03,370
of this work was really to initiate the

131
00:05:01,630 --> 00:05:05,920
study and it's really mostly at a

132
00:05:03,370 --> 00:05:07,630
theoretical level as you will see of the

133
00:05:05,920 --> 00:05:09,670
security of constructions of labeling

134
00:05:07,630 --> 00:05:12,040
functions and the memory heart functions

135
00:05:09,670 --> 00:05:13,960
they're used in and when they are built

136
00:05:12,040 --> 00:05:15,550
out of simple primitives and we are

137
00:05:13,960 --> 00:05:17,859
going to look at permutations we also

138
00:05:15,550 --> 00:05:20,470
look at constructions from block ciphers

139
00:05:17,860 --> 00:05:22,000
and compression functions now there are

140
00:05:20,470 --> 00:05:24,520
really two challenges two main

141
00:05:22,000 --> 00:05:26,680
challenges when when when doing this the

142
00:05:24,520 --> 00:05:28,240
first one is dealing with the the

143
00:05:26,680 --> 00:05:30,490
primitives themselves so even if we

144
00:05:28,240 --> 00:05:32,350
model them as ideal so as a random

145
00:05:30,490 --> 00:05:34,690
permutation or ideal ciphers as we do

146
00:05:32,350 --> 00:05:37,600
improves the techniques that we have so

147
00:05:34,690 --> 00:05:39,969
far are really inherently tied to using

148
00:05:37,600 --> 00:05:43,000
monolithic random Oracle's so we need

149
00:05:39,970 --> 00:05:45,010
new proofs and also understanding what

150
00:05:43,000 --> 00:05:46,840
is a good construction of a labeling

151
00:05:45,010 --> 00:05:48,909
function is really a hard task to start

152
00:05:46,840 --> 00:05:50,619
with in particular we don't have any

153
00:05:48,910 --> 00:05:52,720
good notions you might think for example

154
00:05:50,620 --> 00:05:54,850
that using the notion of in

155
00:05:52,720 --> 00:05:56,860
differentiability that tells us when a

156
00:05:54,850 --> 00:05:58,980
construction is a good random Oracle in

157
00:05:56,860 --> 00:06:02,260
an ideal model will work but actually

158
00:05:58,980 --> 00:06:04,060
memory harness deals with memory bounded

159
00:06:02,260 --> 00:06:05,890
adversaries and the resulting security

160
00:06:04,060 --> 00:06:07,720
games or multistage games and we know

161
00:06:05,890 --> 00:06:09,550
that in differentiability doesn't apply

162
00:06:07,720 --> 00:06:11,400
to them so we don't really even know

163
00:06:09,550 --> 00:06:14,830
where to start

164
00:06:11,400 --> 00:06:17,590
so in further talk I will specifically

165
00:06:14,830 --> 00:06:19,659
focus just on the case of permutations I

166
00:06:17,590 --> 00:06:21,310
think is the most important one so these

167
00:06:19,660 --> 00:06:22,920
are efficiently computable and

168
00:06:21,310 --> 00:06:25,630
efficiently invertible keyless

169
00:06:22,920 --> 00:06:27,190
permutations so this is the case in as

170
00:06:25,630 --> 00:06:29,980
far as I know in most practical designs

171
00:06:27,190 --> 00:06:31,780
and also permutations are attractive for

172
00:06:29,980 --> 00:06:34,900
example because you can't might think to

173
00:06:31,780 --> 00:06:36,849
accentuate them from fixed key AES and

174
00:06:34,900 --> 00:06:38,799
build a memory heart function out of

175
00:06:36,850 --> 00:06:42,240
that and one attractive feature of that

176
00:06:38,800 --> 00:06:44,530
is that you were CPU mostly comes with

177
00:06:42,240 --> 00:06:47,140
Hardware base efficient implementation

178
00:06:44,530 --> 00:06:49,299
yes and this will help you already

179
00:06:47,140 --> 00:06:50,890
already the primitive level reduce the

180
00:06:49,300 --> 00:06:55,750
gap between a software and a hardware

181
00:06:50,890 --> 00:06:58,150
implementation okay so we saw that the

182
00:06:55,750 --> 00:07:01,090
general blueprint here is that we start

183
00:06:58,150 --> 00:07:02,710
with from some dag and now we want to

184
00:07:01,090 --> 00:07:04,750
build a memory heart function out of it

185
00:07:02,710 --> 00:07:07,950
by instantiating the labeling function

186
00:07:04,750 --> 00:07:09,880
from some underlying permutation and

187
00:07:07,950 --> 00:07:12,700
technically we really have to look at

188
00:07:09,880 --> 00:07:14,680
two distinct cases here the first one is

189
00:07:12,700 --> 00:07:16,750
what I refer to as the small block case

190
00:07:14,680 --> 00:07:19,150
this is the case where the labels

191
00:07:16,750 --> 00:07:20,800
assigned to the vertices have length

192
00:07:19,150 --> 00:07:23,190
which is equal to the input output

193
00:07:20,800 --> 00:07:25,720
length of the underlying permutation and

194
00:07:23,190 --> 00:07:27,940
then another case which is what most

195
00:07:25,720 --> 00:07:30,700
commonly happen in practice instead of

196
00:07:27,940 --> 00:07:32,230
is the white block case where the labels

197
00:07:30,700 --> 00:07:35,050
are actually much larger by a

198
00:07:32,230 --> 00:07:36,280
multiplicative factor K and as we have

199
00:07:35,050 --> 00:07:38,070
to build something which will make

200
00:07:36,280 --> 00:07:40,090
multiple calls to the underlying

201
00:07:38,070 --> 00:07:45,250
permutation and we'll start with the

202
00:07:40,090 --> 00:07:47,320
with the simpler small block case now a

203
00:07:45,250 --> 00:07:49,479
little bit more concretely the way we

204
00:07:47,320 --> 00:07:51,700
will formalize memory harness in this

205
00:07:49,479 --> 00:07:54,070
talk is by looking at the cumulative

206
00:07:51,700 --> 00:07:55,840
memory complexity which was introduced

207
00:07:54,070 --> 00:07:57,729
by oven and serving anko which looks at

208
00:07:55,840 --> 00:07:59,590
the memory usage at every point in time

209
00:07:57,729 --> 00:08:02,409
during the evaluation of the function

210
00:07:59,590 --> 00:08:05,169
and then sums this memory usages up okay

211
00:08:02,410 --> 00:08:07,600
and a little bit more formally we will

212
00:08:05,169 --> 00:08:09,930
consider an adversary that will need to

213
00:08:07,600 --> 00:08:12,940
evaluate the function on an input M and

214
00:08:09,930 --> 00:08:15,729
its execution will proceed in rounds and

215
00:08:12,940 --> 00:08:18,969
at every round the adversary will make

216
00:08:15,729 --> 00:08:20,919
queries to the permutation which we

217
00:08:18,970 --> 00:08:22,570
model as a random permutation and in

218
00:08:20,919 --> 00:08:24,010
particular I will make a vector of

219
00:08:22,570 --> 00:08:26,620
parallel queries which might be both

220
00:08:24,010 --> 00:08:30,669
forward and backward queries and then

221
00:08:26,620 --> 00:08:32,260
produce a state for the next step and in

222
00:08:30,669 --> 00:08:33,909
the next step the adversary will get the

223
00:08:32,260 --> 00:08:36,099
state and the answers to these queries

224
00:08:33,909 --> 00:08:37,870
and then these over and over until in

225
00:08:36,099 --> 00:08:40,300
the final step when ready the adversary

226
00:08:37,870 --> 00:08:43,690
will output the output of the function

227
00:08:40,299 --> 00:08:45,640
on input M now near the the cumulative

228
00:08:43,690 --> 00:08:49,260
memory complexity of an execution is

229
00:08:45,640 --> 00:08:53,530
just the sum of the sizes of the state

230
00:08:49,260 --> 00:08:56,020
now a related quantity is that of the

231
00:08:53,530 --> 00:08:57,670
cumulative pebble in complexity of a

232
00:08:56,020 --> 00:09:00,400
graph so here

233
00:08:57,670 --> 00:09:02,560
Lucca the Dagda might define an mhf and

234
00:09:00,400 --> 00:09:04,240
we consider a combinatorial game where

235
00:09:02,560 --> 00:09:07,359
you adversary at any point in time can

236
00:09:04,240 --> 00:09:09,010
place a pebble on vertex if all of the

237
00:09:07,360 --> 00:09:11,050
predecessor vertices have a pebble on

238
00:09:09,010 --> 00:09:13,270
them and also in the same step can

239
00:09:11,050 --> 00:09:15,219
remove any of the pebbles and the goal

240
00:09:13,270 --> 00:09:17,920
is to place a pebble on the sink of the

241
00:09:15,220 --> 00:09:19,690
graph so we might look at if you look at

242
00:09:17,920 --> 00:09:21,520
the strategy we want to characterize its

243
00:09:19,690 --> 00:09:23,380
complexity and what we're going to do

244
00:09:21,520 --> 00:09:25,090
we're going to look at the size of the

245
00:09:23,380 --> 00:09:26,590
different babbling configuration so for

246
00:09:25,090 --> 00:09:28,630
example we might have a strategy or the

247
00:09:26,590 --> 00:09:30,040
places an initial pebble on the source

248
00:09:28,630 --> 00:09:31,570
this can be done at any time he doesn't

249
00:09:30,040 --> 00:09:33,339
know predecessors and then might

250
00:09:31,570 --> 00:09:35,260
continue by placing another pebble so

251
00:09:33,340 --> 00:09:37,420
now we have two pebbles on the graph

252
00:09:35,260 --> 00:09:38,740
then we can place one on three but

253
00:09:37,420 --> 00:09:40,990
remove one on one we're going to have

254
00:09:38,740 --> 00:09:42,790
two pebbles and so on now we have three

255
00:09:40,990 --> 00:09:44,800
pebbles now we have three pebbles now we

256
00:09:42,790 --> 00:09:47,319
only have two and finally we place a

257
00:09:44,800 --> 00:09:49,839
pebble on the sink and if we now sum up

258
00:09:47,320 --> 00:09:51,580
these numbers what we get is the

259
00:09:49,840 --> 00:09:54,340
cumulative babbling complexity of this

260
00:09:51,580 --> 00:09:56,320
strategy and now you can naturally have

261
00:09:54,340 --> 00:09:58,270
a quantity associated with a dog which

262
00:09:56,320 --> 00:10:00,580
is which will be 14 here we can have a

263
00:09:58,270 --> 00:10:02,740
quantity which is the pebbly complexity

264
00:10:00,580 --> 00:10:04,600
of the graph which is just the pebble in

265
00:10:02,740 --> 00:10:07,840
complexity of the best strategy the

266
00:10:04,600 --> 00:10:10,480
lowest one and we know by prior works

267
00:10:07,840 --> 00:10:12,820
that we can for example give graphs or

268
00:10:10,480 --> 00:10:15,760
family of graphs with constant degree or

269
00:10:12,820 --> 00:10:17,260
even degree to which have optimal CPC

270
00:10:15,760 --> 00:10:21,130
cumulative pebble in complexity of n

271
00:10:17,260 --> 00:10:23,140
square over log n so now intuitively if

272
00:10:21,130 --> 00:10:25,330
you look at the case where the labeling

273
00:10:23,140 --> 00:10:28,270
function is a random Oracle the pebble

274
00:10:25,330 --> 00:10:30,790
in complexity gives you already measure

275
00:10:28,270 --> 00:10:33,130
is correlated directly with the

276
00:10:30,790 --> 00:10:35,290
cumulative memory complexity for those

277
00:10:33,130 --> 00:10:37,900
strategies that only store exactly

278
00:10:35,290 --> 00:10:39,370
labels in memory however a general

279
00:10:37,900 --> 00:10:41,470
strategy might try to do something

280
00:10:39,370 --> 00:10:44,490
clever and compress information on the

281
00:10:41,470 --> 00:10:47,260
way stores xers of labels or whatnot but

282
00:10:44,490 --> 00:10:49,450
result by evidence Urban Ink Oh shows

283
00:10:47,260 --> 00:10:51,010
that essentially that's the best that it

284
00:10:49,450 --> 00:10:52,960
can be done up to some terms I'm

285
00:10:51,010 --> 00:10:54,970
actually hiding here for simplicity nemi

286
00:10:52,960 --> 00:10:57,370
with high probability over the choice of

287
00:10:54,970 --> 00:10:59,590
a random Oracle the cumulative memory

288
00:10:57,370 --> 00:11:01,360
complexity of an adversary is lower

289
00:10:59,590 --> 00:11:03,310
bounded by the pebbly complexity of the

290
00:11:01,360 --> 00:11:04,600
graph times the length of the labels

291
00:11:03,310 --> 00:11:07,930
which here is the random Oracle output

292
00:11:04,600 --> 00:11:09,850
length and so our first result for the

293
00:11:07,930 --> 00:11:11,170
small block case will show something

294
00:11:09,850 --> 00:11:13,839
analogous

295
00:11:11,170 --> 00:11:15,790
for the case of random permutation where

296
00:11:13,839 --> 00:11:17,769
we instantiate the small labeling

297
00:11:15,790 --> 00:11:20,019
function with a function that simply

298
00:11:17,769 --> 00:11:21,550
takes the XOR of two labels so this is

299
00:11:20,019 --> 00:11:24,339
specific for the case of in degree two

300
00:11:21,550 --> 00:11:26,050
and then applies a permutation which is

301
00:11:24,339 --> 00:11:28,630
invertible and so to eliminate is

302
00:11:26,050 --> 00:11:32,260
invertibility with an XOR the XOR of the

303
00:11:28,630 --> 00:11:33,550
labels to be output again and the proof

304
00:11:32,260 --> 00:11:36,160
of this result follows a similar

305
00:11:33,550 --> 00:11:37,689
blueprint as prior proofs in this domain

306
00:11:36,160 --> 00:11:41,380
using something called an ex post facto

307
00:11:37,690 --> 00:11:43,029
babbling argument but the real challenge

308
00:11:41,380 --> 00:11:45,730
which I'm not going to go into due to

309
00:11:43,029 --> 00:11:48,160
time is actually to deal with inversion

310
00:11:45,730 --> 00:11:50,350
queries in the proof so the adversary

311
00:11:48,160 --> 00:11:52,120
can query the permutation both forward

312
00:11:50,350 --> 00:11:56,260
and backward and this is what makes the

313
00:11:52,120 --> 00:11:58,209
proof much harder now but let me go back

314
00:11:56,260 --> 00:12:01,269
for the second part of the talk to the

315
00:11:58,209 --> 00:12:03,910
white block case so remember now our

316
00:12:01,269 --> 00:12:05,500
labels are going to have size K times L

317
00:12:03,910 --> 00:12:07,630
where L is the input output length of

318
00:12:05,500 --> 00:12:09,970
the permutation and we want to build a

319
00:12:07,630 --> 00:12:11,769
more complex labeling function out of

320
00:12:09,970 --> 00:12:13,720
the permutation and the first question

321
00:12:11,769 --> 00:12:15,790
you should actually ask is why should we

322
00:12:13,720 --> 00:12:17,589
actually even care

323
00:12:15,790 --> 00:12:19,180
so actually one answer is the disease

324
00:12:17,589 --> 00:12:21,940
done in practice but now we actually

325
00:12:19,180 --> 00:12:25,329
have a result that shows us a lower

326
00:12:21,940 --> 00:12:27,850
bound on the CMC based on the babbling

327
00:12:25,329 --> 00:12:29,949
complexity so if you want to enforce a

328
00:12:27,850 --> 00:12:32,050
certain lower bound here just use a

329
00:12:29,949 --> 00:12:34,209
sufficiently good graph and large enough

330
00:12:32,050 --> 00:12:36,790
to have a large family complexity until

331
00:12:34,209 --> 00:12:39,250
you meet what what you want but but the

332
00:12:36,790 --> 00:12:41,079
key point here on one key point is I

333
00:12:39,250 --> 00:12:43,779
think also motivated practitioners but

334
00:12:41,079 --> 00:12:46,599
I'm not entirely sure is that if we use

335
00:12:43,779 --> 00:12:47,860
a permutation our length L the input

336
00:12:46,600 --> 00:12:50,470
output length of the permutation might

337
00:12:47,860 --> 00:12:52,839
actually be small so say up to down 228

338
00:12:50,470 --> 00:12:56,019
bits for AES and so if we want to

339
00:12:52,839 --> 00:12:58,480
achieve a certain large CMC then we need

340
00:12:56,019 --> 00:13:00,279
a large graph and in many cases this

341
00:12:58,480 --> 00:13:01,720
also implies a large description of this

342
00:13:00,279 --> 00:13:03,730
graph some of these graphs are picked by

343
00:13:01,720 --> 00:13:05,769
choosing random edges so that will take

344
00:13:03,730 --> 00:13:07,569
space so if we instead we can prove a

345
00:13:05,769 --> 00:13:09,880
lower bound that additional depends on K

346
00:13:07,569 --> 00:13:12,430
and in fact we can up to even have

347
00:13:09,880 --> 00:13:13,930
something super linear in K then we

348
00:13:12,430 --> 00:13:15,219
might be able to achieve the same lower

349
00:13:13,930 --> 00:13:16,300
bound but with a smaller graph

350
00:13:15,220 --> 00:13:19,420
description so it's really about

351
00:13:16,300 --> 00:13:21,670
minimizing the description here ok so

352
00:13:19,420 --> 00:13:23,140
what we're going to do here is we're

353
00:13:21,670 --> 00:13:25,089
going to look at wide block labeling

354
00:13:23,140 --> 00:13:27,430
functions that are also built

355
00:13:25,089 --> 00:13:30,309
out of DAGs so in particular they say

356
00:13:27,430 --> 00:13:32,019
that the label size is K times L and we

357
00:13:30,309 --> 00:13:33,699
look at graphs with some in degree Delta

358
00:13:32,019 --> 00:13:36,759
and now we are going to build our

359
00:13:33,699 --> 00:13:41,128
labeling function from a gadget dag gage

360
00:13:36,759 --> 00:13:44,499
which is going to have K times Delta

361
00:13:41,129 --> 00:13:46,059
sources and K designated nodes that are

362
00:13:44,499 --> 00:13:47,259
Xing nodes and will correspond to the

363
00:13:46,059 --> 00:13:49,959
outputs and then we'll actually have

364
00:13:47,259 --> 00:13:52,360
constant degree in degree two and that

365
00:13:49,959 --> 00:13:54,069
we will compile this into an actual

366
00:13:52,360 --> 00:13:55,870
labeling function by using the small

367
00:13:54,069 --> 00:13:58,059
block labeling function we define before

368
00:13:55,870 --> 00:13:59,290
and the advantage of doing this is if

369
00:13:58,059 --> 00:14:01,629
you know we look at the memory card

370
00:13:59,290 --> 00:14:03,579
function defined by a base graph G with

371
00:14:01,629 --> 00:14:06,069
this wide block labeling function this

372
00:14:03,579 --> 00:14:08,709
is equivalent to a function defined by a

373
00:14:06,069 --> 00:14:10,660
compose graph the with the basic graph

374
00:14:08,709 --> 00:14:12,128
and a gadget graph for the small block

375
00:14:10,660 --> 00:14:14,079
labeling function which we can study

376
00:14:12,129 --> 00:14:15,879
with the theorem we had before and then

377
00:14:14,079 --> 00:14:18,309
the goal of course is to find such a

378
00:14:15,879 --> 00:14:20,559
gadget graph that will maximize the

379
00:14:18,309 --> 00:14:22,300
pebble in complexity and what this

380
00:14:20,559 --> 00:14:25,719
composition here it's quite natural but

381
00:14:22,300 --> 00:14:27,430
just to be precise so we say we look at

382
00:14:25,720 --> 00:14:30,129
our based gravity there's some in degree

383
00:14:27,430 --> 00:14:33,790
so here three and what we do is that

384
00:14:30,129 --> 00:14:35,980
first we blow up every vertex by K so

385
00:14:33,790 --> 00:14:38,589
every vertex is mapped to K vertices so

386
00:14:35,980 --> 00:14:40,660
here tree and then for every sub graph

387
00:14:38,589 --> 00:14:43,389
which is made by an inner vertex and its

388
00:14:40,660 --> 00:14:45,550
predecessors we are going to map it to

389
00:14:43,389 --> 00:14:48,490
an instance of the gadget graph in the

390
00:14:45,550 --> 00:14:50,740
compose graph and and then we do this of

391
00:14:48,490 --> 00:14:52,179
course all over the graph and now we get

392
00:14:50,740 --> 00:14:54,610
a new graph which is the compose graph

393
00:14:52,179 --> 00:14:57,910
which now has actually also constant in

394
00:14:54,610 --> 00:14:59,920
degree 2 and now we would like to prove

395
00:14:57,910 --> 00:15:01,870
something about it and give a concrete

396
00:14:59,920 --> 00:15:03,660
Association of the gadget graph so in

397
00:15:01,870 --> 00:15:05,769
the paper we give a family of

398
00:15:03,660 --> 00:15:07,120
associations for gadget graph that to

399
00:15:05,769 --> 00:15:09,370
initiate this study and this is an

400
00:15:07,120 --> 00:15:12,129
example of such a family so it has Delta

401
00:15:09,370 --> 00:15:13,839
x k sources at the bottom k xing notes

402
00:15:12,129 --> 00:15:16,449
at the right hand and the number of

403
00:15:13,839 --> 00:15:18,399
vertices here is quadratic in K so we

404
00:15:16,449 --> 00:15:20,378
will expect now to prove something of

405
00:15:18,399 --> 00:15:22,149
this form where we lower bound the

406
00:15:20,379 --> 00:15:25,059
pebbly complexity of the compose graph

407
00:15:22,149 --> 00:15:27,009
as by the public complexity of the base

408
00:15:25,059 --> 00:15:29,199
graph times something which depends on K

409
00:15:27,009 --> 00:15:30,879
and Delta and ideally should be as large

410
00:15:29,199 --> 00:15:33,399
as possible like quadratic in the size

411
00:15:30,879 --> 00:15:36,160
of the graph so unfortunately we don't

412
00:15:33,399 --> 00:15:38,080
quite know how to do it what we do is we

413
00:15:36,160 --> 00:15:40,780
do the second best thing we can do

414
00:15:38,080 --> 00:15:42,880
and we observed that in almost all cases

415
00:15:40,780 --> 00:15:45,130
lower bounds on the public complexity

416
00:15:42,880 --> 00:15:48,010
for good graphs are proved via the

417
00:15:45,130 --> 00:15:52,570
notion of dab robustness and in

418
00:15:48,010 --> 00:15:55,000
particular we say that a graph is de Deb

419
00:15:52,570 --> 00:15:58,630
robust if whenever you remove up to

420
00:15:55,000 --> 00:16:00,730
everted C's from this deck the graph

421
00:15:58,630 --> 00:16:04,900
still has a long path of length at least

422
00:16:00,730 --> 00:16:06,760
D in the rest of the graph and the DEP

423
00:16:04,900 --> 00:16:08,650
robustus implies in particular that the

424
00:16:06,760 --> 00:16:12,700
graph has Pavlin complexity at least D

425
00:16:08,650 --> 00:16:15,760
times B and so what we show is that our

426
00:16:12,700 --> 00:16:18,460
compose graph has the property that if

427
00:16:15,760 --> 00:16:21,010
the base graph is de tab robust then the

428
00:16:18,460 --> 00:16:22,870
compose graph is also some good NAB

429
00:16:21,010 --> 00:16:24,540
robustness which is going to give us a

430
00:16:22,870 --> 00:16:27,610
lower bound and from the cumulative

431
00:16:24,540 --> 00:16:29,410
pebbly complexity in the idea of the

432
00:16:27,610 --> 00:16:31,660
proof here I mean I won't read all

433
00:16:29,410 --> 00:16:34,390
through all of this but essentially what

434
00:16:31,660 --> 00:16:36,189
we want to do is we want to map a set of

435
00:16:34,390 --> 00:16:37,990
vertices in the compose graph back to a

436
00:16:36,190 --> 00:16:41,380
set of vertices in the base graph and

437
00:16:37,990 --> 00:16:43,450
use which is not larger than E and then

438
00:16:41,380 --> 00:16:45,430
use the existence of a long path in the

439
00:16:43,450 --> 00:16:49,720
rest to build a longer path back in the

440
00:16:45,430 --> 00:16:51,579
compose graph and to do this what we

441
00:16:49,720 --> 00:16:54,820
need to crucially use as the following

442
00:16:51,580 --> 00:16:58,180
property of our gadget graph which is

443
00:16:54,820 --> 00:17:00,640
that whenever you now remove from this

444
00:16:58,180 --> 00:17:02,530
graph a subset of its vertices which is

445
00:17:00,640 --> 00:17:04,810
not too large they are most K over 4

446
00:17:02,530 --> 00:17:07,329
this is way larger than K over 4 but I

447
00:17:04,810 --> 00:17:11,649
just did it for dramatization purposes

448
00:17:07,329 --> 00:17:14,560
here then what we have is that for every

449
00:17:11,650 --> 00:17:17,680
possible source of the graph there is

450
00:17:14,560 --> 00:17:20,530
always going to be an exit node on the

451
00:17:17,680 --> 00:17:25,630
right-hand side such that there exists a

452
00:17:20,530 --> 00:17:28,870
long path of length Omega of Delta times

453
00:17:25,630 --> 00:17:30,790
K square between the source and the

454
00:17:28,870 --> 00:17:33,790
Exynos or something like this which I

455
00:17:30,790 --> 00:17:35,379
have I lighted here so now what do we

456
00:17:33,790 --> 00:17:37,420
get out of this construction so it's

457
00:17:35,380 --> 00:17:39,640
important now to go slowly to the to the

458
00:17:37,420 --> 00:17:42,610
end result so what we can hope to get

459
00:17:39,640 --> 00:17:44,560
here if we get a graph with optimal

460
00:17:42,610 --> 00:17:48,040
babbling complexity say n square over

461
00:17:44,560 --> 00:17:49,990
log n and say constant in degree because

462
00:17:48,040 --> 00:17:51,909
our final graph we have will have

463
00:17:49,990 --> 00:17:53,920
roughly n times K square

464
00:17:51,910 --> 00:17:55,900
so we can hope to have is something of

465
00:17:53,920 --> 00:17:58,090
the type a CFC which is at least n

466
00:17:55,900 --> 00:18:01,600
square times K to the fourth over log n

467
00:17:58,090 --> 00:18:04,179
times L what we got here is something

468
00:18:01,600 --> 00:18:07,030
slightly worse namely we get N squared

469
00:18:04,180 --> 00:18:09,370
times K to the third over log N and now

470
00:18:07,030 --> 00:18:12,070
a question we may ask is this ne is this

471
00:18:09,370 --> 00:18:13,629
any good right it's not clear what he

472
00:18:12,070 --> 00:18:15,129
means here to be good so this is really

473
00:18:13,630 --> 00:18:16,330
a question we haven't asked before we

474
00:18:15,130 --> 00:18:18,160
were just assuming that these random

475
00:18:16,330 --> 00:18:21,070
articles were given we're costing you

476
00:18:18,160 --> 00:18:23,080
one to call them no extra memory nothing

477
00:18:21,070 --> 00:18:26,379
suddenly we look into it and it's really

478
00:18:23,080 --> 00:18:28,330
not clear what we want okay so a first

479
00:18:26,380 --> 00:18:31,150
thing we might want to do which has been

480
00:18:28,330 --> 00:18:34,720
done in several prior works is just to

481
00:18:31,150 --> 00:18:36,820
consider the ratio between the

482
00:18:34,720 --> 00:18:38,800
efficiency of the sequential strategy

483
00:18:36,820 --> 00:18:40,389
that harnessed user my use to evaluate

484
00:18:38,800 --> 00:18:42,970
the efficiency of the sequential

485
00:18:40,390 --> 00:18:46,060
strategy just honest users my use to

486
00:18:42,970 --> 00:18:49,570
evaluate the function and the the CMC

487
00:18:46,060 --> 00:18:51,610
louver bound and if we do that and we

488
00:18:49,570 --> 00:18:53,560
look what is the best existing strategy

489
00:18:51,610 --> 00:18:55,689
that you will actually implement you get

490
00:18:53,560 --> 00:18:57,070
a ratio which is generically log N and

491
00:18:55,690 --> 00:18:59,020
for some graphs can even be made

492
00:18:57,070 --> 00:19:01,360
constant and this is analogue to the

493
00:18:59,020 --> 00:19:02,830
best result we had for monolithic random

494
00:19:01,360 --> 00:19:06,040
Oracle's that's already a good baseline

495
00:19:02,830 --> 00:19:09,460
to satisfy but actually this is not

496
00:19:06,040 --> 00:19:12,490
really a hard to meet goal you could do

497
00:19:09,460 --> 00:19:14,850
it with much simpler constructions so a

498
00:19:12,490 --> 00:19:18,130
better thing we might want to look at is

499
00:19:14,850 --> 00:19:21,490
you know how high the CMC is as a

500
00:19:18,130 --> 00:19:23,410
function of the running time of the

501
00:19:21,490 --> 00:19:25,150
sequential strategy so you're given a

502
00:19:23,410 --> 00:19:26,890
certain budget of time to hash passwords

503
00:19:25,150 --> 00:19:29,470
and you want to get CMC which is as high

504
00:19:26,890 --> 00:19:31,390
as possible and and here what we see is

505
00:19:29,470 --> 00:19:33,580
that if we've right now the running time

506
00:19:31,390 --> 00:19:35,110
is and the number of vertices times the

507
00:19:33,580 --> 00:19:36,939
time to evaluate the labeling function

508
00:19:35,110 --> 00:19:39,490
we just build which is roughly K Square

509
00:19:36,940 --> 00:19:42,910
and then we compare it with our CMC

510
00:19:39,490 --> 00:19:45,400
lower bound then the ratio is roughly is

511
00:19:42,910 --> 00:19:49,090
of the order n times square root of T

512
00:19:45,400 --> 00:19:51,880
lambda times L and again ideally we will

513
00:19:49,090 --> 00:19:54,760
hope to achieve n times T lambda times L

514
00:19:51,880 --> 00:19:56,440
but they're really not sure whether this

515
00:19:54,760 --> 00:19:57,790
is feasible with a simple construction

516
00:19:56,440 --> 00:19:59,830
so it's a great open question but it

517
00:19:57,790 --> 00:20:01,870
seems it's at least as hard as again

518
00:19:59,830 --> 00:20:03,220
looking inside you are labeling function

519
00:20:01,870 --> 00:20:05,739
and making it again a memory hard

520
00:20:03,220 --> 00:20:07,809
function plus more so this might be

521
00:20:05,740 --> 00:20:09,190
be exactly what you're looking for and

522
00:20:07,809 --> 00:20:11,530
another interesting question here is

523
00:20:09,190 --> 00:20:14,350
that we have this quadratic blow-up from

524
00:20:11,530 --> 00:20:15,850
K to K square complexity maybe we can

525
00:20:14,350 --> 00:20:17,260
build something which is linearly about

526
00:20:15,850 --> 00:20:20,080
the running times and what does it even

527
00:20:17,260 --> 00:20:21,640
mean okay but it's important here that

528
00:20:20,080 --> 00:20:23,559
we're doing something non-trivial if for

529
00:20:21,640 --> 00:20:25,600
example you use something like based on

530
00:20:23,559 --> 00:20:27,520
merkle-damgard to Easton she ate your

531
00:20:25,600 --> 00:20:29,110
labeling function and then you try to

532
00:20:27,520 --> 00:20:30,970
prove something about it what you would

533
00:20:29,110 --> 00:20:32,500
have gotten there was a ratio of n times

534
00:20:30,970 --> 00:20:34,780
L that will not depend on the actual

535
00:20:32,500 --> 00:20:37,360
time to evaluate your merkle-damgard so

536
00:20:34,780 --> 00:20:38,980
we are achieving something far from 3d

537
00:20:37,360 --> 00:20:42,520
by having this extra term in the ratio

538
00:20:38,980 --> 00:20:45,370
so the ratio goes up okay so as you see

539
00:20:42,520 --> 00:20:47,440
I think in this work we open more

540
00:20:45,370 --> 00:20:49,000
question that we actually solved so this

541
00:20:47,440 --> 00:20:51,280
is really mostly the goal was to prove

542
00:20:49,000 --> 00:20:52,450
some basic theorems and to attract the

543
00:20:51,280 --> 00:20:54,280
attention to the problem and start

544
00:20:52,450 --> 00:20:56,050
giving you some theoretical designs I

545
00:20:54,280 --> 00:20:57,370
think is a very important question it's

546
00:20:56,050 --> 00:20:59,649
something that has been overlooked just

547
00:20:57,370 --> 00:21:00,850
because it's not easy in Prior works on

548
00:20:59,650 --> 00:21:03,370
memory hurt functions but there are a

549
00:21:00,850 --> 00:21:04,809
lot of problems I mean one open problem

550
00:21:03,370 --> 00:21:07,479
so one of them is of course that I

551
00:21:04,809 --> 00:21:09,399
really consider only CMC there is a lot

552
00:21:07,480 --> 00:21:11,679
of work on considering other metric and

553
00:21:09,400 --> 00:21:14,590
other aspect like bandwidth harness like

554
00:21:11,679 --> 00:21:16,240
space sustained space complexity and

555
00:21:14,590 --> 00:21:19,510
much more so we would like to extend it

556
00:21:16,240 --> 00:21:21,790
to that also it will be like one will I

557
00:21:19,510 --> 00:21:23,110
course like to look at practical design

558
00:21:21,790 --> 00:21:24,490
and prove something about and whether

559
00:21:23,110 --> 00:21:26,020
they are secure or not and we actually

560
00:21:24,490 --> 00:21:27,520
don't really have a good sense many of

561
00:21:26,020 --> 00:21:29,139
them do not fit in the framework I use

562
00:21:27,520 --> 00:21:31,210
there in a graph theoretic framework

563
00:21:29,140 --> 00:21:32,230
also exactly for that reason we would

564
00:21:31,210 --> 00:21:33,850
like to have some more generic

565
00:21:32,230 --> 00:21:36,250
high-level properties of these labeling

566
00:21:33,850 --> 00:21:37,689
functions that are sufficient and also

567
00:21:36,250 --> 00:21:38,980
at the end we will really want to

568
00:21:37,690 --> 00:21:40,900
understand all of this parameter

569
00:21:38,980 --> 00:21:42,700
trade-offs and what I really mean so if

570
00:21:40,900 --> 00:21:44,920
I make this labeling function even

571
00:21:42,700 --> 00:21:46,900
slower and so on what what do I really

572
00:21:44,920 --> 00:21:49,210
want and how easy it is to represent

573
00:21:46,900 --> 00:21:52,300
these graphs and and much more all right

574
00:21:49,210 --> 00:21:53,679
so thank you so this ends my talk so we

575
00:21:52,300 --> 00:21:54,909
are kind of working on an extended full

576
00:21:53,679 --> 00:21:56,050
version of the paper that hopefully is

577
00:21:54,910 --> 00:21:57,460
going to appear soon where we want to

578
00:21:56,050 --> 00:21:59,530
highlight a bit better these open

579
00:21:57,460 --> 00:22:03,540
questions and parameter issues and so on

580
00:21:59,530 --> 00:22:03,540
so I'm happy to answer any questions

581
00:22:06,379 --> 00:22:10,350
Thank You Stefano if you don't have the

582
00:22:08,940 --> 00:22:18,899
question please come to the microphones

583
00:22:10,350 --> 00:22:20,639
at the front walk long path let's have a

584
00:22:18,899 --> 00:22:22,199
quick I'm just curious why the

585
00:22:20,639 --> 00:22:23,668
cumulative memory complexity is the

586
00:22:22,200 --> 00:22:26,669
right measure rather than the maximum

587
00:22:23,669 --> 00:22:30,179
for example oh I see so yeah so the

588
00:22:26,669 --> 00:22:32,279
point is that you want to differentiate

589
00:22:30,179 --> 00:22:34,409
between strategies that for example

590
00:22:32,279 --> 00:22:35,729
require a lot of memory just because

591
00:22:34,409 --> 00:22:37,470
there is a peak where you're using

592
00:22:35,729 --> 00:22:38,940
memory just at one point and then you

593
00:22:37,470 --> 00:22:40,590
don't need it so what it becomes

594
00:22:38,940 --> 00:22:42,239
important is for example when you do

595
00:22:40,590 --> 00:22:44,279
mass evaluation right and you want to

596
00:22:42,239 --> 00:22:45,960
evaluate multiple instances right then

597
00:22:44,279 --> 00:22:50,920
you know you could use that free memory

598
00:22:45,960 --> 00:22:55,119
for let's thank the panel again

599
00:22:50,920 --> 00:22:55,119
[Applause]

