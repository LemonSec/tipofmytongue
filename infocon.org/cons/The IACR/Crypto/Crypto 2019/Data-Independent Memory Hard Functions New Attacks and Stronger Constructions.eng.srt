1
00:00:04,870 --> 00:00:10,660
alright thanks for the introduction so

2
00:00:08,109 --> 00:00:13,140
I'm excited to tell you about more about

3
00:00:10,660 --> 00:00:16,240
data independent memory card functions

4
00:00:13,140 --> 00:00:18,550
so let me start off with a motivation

5
00:00:16,239 --> 00:00:21,220
which is really why I started working on

6
00:00:18,550 --> 00:00:23,050
memory card functions right so password

7
00:00:21,220 --> 00:00:25,390
storage we have an attacker that breaks

8
00:00:23,050 --> 00:00:27,369
into an authentication server steals the

9
00:00:25,390 --> 00:00:29,560
hash of a user's password and now the

10
00:00:27,369 --> 00:00:31,119
attacker can try as many guesses as they

11
00:00:29,560 --> 00:00:33,220
want offline to crack the user's

12
00:00:31,119 --> 00:00:34,989
password they're really only limited by

13
00:00:33,220 --> 00:00:37,780
the resources that they invest in this

14
00:00:34,989 --> 00:00:39,910
in this attack unfortunately these

15
00:00:37,780 --> 00:00:45,010
attacks are an increasingly common

16
00:00:39,910 --> 00:00:46,540
problem at this point you know breaches

17
00:00:45,010 --> 00:00:49,718
happen so fast that I'm not even able to

18
00:00:46,540 --> 00:00:52,000
keep this slide up to date and we've had

19
00:00:49,719 --> 00:00:56,140
billions of user accounts that that have

20
00:00:52,000 --> 00:00:57,940
been affected so the goal of a memory

21
00:00:56,140 --> 00:01:00,039
heart function intuitively we want a

22
00:00:57,940 --> 00:01:02,739
martyr moderately expensive hash

23
00:01:00,039 --> 00:01:05,530
function which would be fast to evaluate

24
00:01:02,739 --> 00:01:08,080
on a personal computer and expensive to

25
00:01:05,530 --> 00:01:10,179
evaluate even if the attacker is using

26
00:01:08,080 --> 00:01:13,390
some application specific integrated

27
00:01:10,179 --> 00:01:17,590
circuit so maybe some supercomputer as

28
00:01:13,390 --> 00:01:20,229
shown on the left here so intuitively

29
00:01:17,590 --> 00:01:22,450
very hard function is a function where

30
00:01:20,229 --> 00:01:25,000
computation costs should be dominated by

31
00:01:22,450 --> 00:01:26,860
memory cost so our goal here is we want

32
00:01:25,000 --> 00:01:28,539
to force the attacker to lock up large

33
00:01:26,860 --> 00:01:31,619
amounts of memory for the entire

34
00:01:28,539 --> 00:01:34,509
duration of computation and this

35
00:01:31,619 --> 00:01:36,910
intuitively implies that the computation

36
00:01:34,509 --> 00:01:41,080
will be expensive even if the adversary

37
00:01:36,910 --> 00:01:43,240
is using customized hardware so we have

38
00:01:41,080 --> 00:01:46,619
a couple examples of memory hard

39
00:01:43,240 --> 00:01:49,300
functions S crypt is a prime example

40
00:01:46,619 --> 00:01:51,729
here s crypt is an example of a data

41
00:01:49,300 --> 00:01:54,039
dependent memory hard function so the

42
00:01:51,729 --> 00:01:56,319
memory access pattern here depends on

43
00:01:54,039 --> 00:01:58,420
the users input and so we have to worry

44
00:01:56,319 --> 00:02:00,130
about side channel leakage right so a

45
00:01:58,420 --> 00:02:03,849
side channel might leak information

46
00:02:00,130 --> 00:02:07,330
about the users input which in this case

47
00:02:03,849 --> 00:02:09,039
might be a sensitive password so in this

48
00:02:07,330 --> 00:02:10,959
talk we're going to focus entirely on

49
00:02:09,038 --> 00:02:13,208
something called data independent memory

50
00:02:10,959 --> 00:02:15,640
heart functions these are just memory

51
00:02:13,209 --> 00:02:17,230
hard functions whose memory access

52
00:02:15,640 --> 00:02:20,488
pattern does not leak any information

53
00:02:17,230 --> 00:02:20,488
about the users input

54
00:02:20,910 --> 00:02:26,799
okay so to specify a data independent

55
00:02:24,520 --> 00:02:29,290
memory card function we need to do two

56
00:02:26,800 --> 00:02:32,200
things we need to specify a directed

57
00:02:29,290 --> 00:02:36,070
acyclic graph G and a labeling function

58
00:02:32,200 --> 00:02:38,589
H now typically H we think of as being a

59
00:02:36,070 --> 00:02:41,829
random Oracle as we saw in the last talk

60
00:02:38,590 --> 00:02:45,520
it's OK for H to be you know ideal

61
00:02:41,830 --> 00:02:47,290
permutation or an ideal cypher so

62
00:02:45,520 --> 00:02:49,690
there's other ways to build H here but

63
00:02:47,290 --> 00:02:52,959
in this talk will focus on on random

64
00:02:49,690 --> 00:02:55,450
Oracle's intuitively the dag encodes

65
00:02:52,960 --> 00:02:57,610
data dependencies right so the label of

66
00:02:55,450 --> 00:02:59,920
an internal node is the hash of the

67
00:02:57,610 --> 00:03:01,720
labels of its parents and the output of

68
00:02:59,920 --> 00:03:05,320
this function is just a label of the

69
00:03:01,720 --> 00:03:07,480
last last node in this graph so we'll

70
00:03:05,320 --> 00:03:10,109
use big n to denote the total number of

71
00:03:07,480 --> 00:03:12,190
graphs and typically we'll assume that

72
00:03:10,110 --> 00:03:14,470
at least in this talk we'll assume that

73
00:03:12,190 --> 00:03:18,940
N is a power of two so we'll use little

74
00:03:14,470 --> 00:03:21,280
end for the that power okay so when we

75
00:03:18,940 --> 00:03:25,030
talk about evaluating a memory hard

76
00:03:21,280 --> 00:03:26,950
function a common way to to describe

77
00:03:25,030 --> 00:03:29,580
evaluation strategies is in terms of

78
00:03:26,950 --> 00:03:31,750
graph tab link so here we've got a graph

79
00:03:29,580 --> 00:03:33,400
we start off with no pebbles in the

80
00:03:31,750 --> 00:03:36,130
graph then we can place a pebble on the

81
00:03:33,400 --> 00:03:37,890
source now that we have a pebble on node

82
00:03:36,130 --> 00:03:40,210
one we can place a pebble in node two

83
00:03:37,890 --> 00:03:43,359
right now we can place a pebble on node

84
00:03:40,210 --> 00:03:45,940
three now we have the data to you know

85
00:03:43,360 --> 00:03:51,390
pebble node for a couple node five etc

86
00:03:45,940 --> 00:03:54,220
so the rules of a of a pebble egg is

87
00:03:51,390 --> 00:03:55,809
basically you have to wait until you

88
00:03:54,220 --> 00:03:58,120
have to wait to pebble a node until all

89
00:03:55,810 --> 00:04:00,580
the dependent data labels are already in

90
00:03:58,120 --> 00:04:03,070
met memory all right so when we talk

91
00:04:00,580 --> 00:04:07,540
about evaluation we're typically going

92
00:04:03,070 --> 00:04:10,120
to use the language of graph pebble all

93
00:04:07,540 --> 00:04:12,100
right so now the important question is

94
00:04:10,120 --> 00:04:13,360
how do we measure the cost of the pebble

95
00:04:12,100 --> 00:04:18,579
egg and there's a couple different

96
00:04:13,360 --> 00:04:22,690
approaches one nice initial attempt is

97
00:04:18,579 --> 00:04:24,460
space-time complexity so here you just

98
00:04:22,690 --> 00:04:25,870
look at the maximum number of pebbles on

99
00:04:24,460 --> 00:04:27,900
the graph at any point in time in the

100
00:04:25,870 --> 00:04:30,340
pebble a multiplied by the total time

101
00:04:27,900 --> 00:04:33,090
right so this is maximum space used

102
00:04:30,340 --> 00:04:35,940
multiplied by

103
00:04:33,090 --> 00:04:39,419
so what's the problem with this well the

104
00:04:35,940 --> 00:04:42,030
problem is that space-time complexity

105
00:04:39,420 --> 00:04:44,130
doesn't amortize nicely so as we can see

106
00:04:42,030 --> 00:04:45,840
on this slide let's suppose that we have

107
00:04:44,130 --> 00:04:48,120
a computation that requires a lot of

108
00:04:45,840 --> 00:04:51,630
space initially and then runs for a lot

109
00:04:48,120 --> 00:04:53,130
of time without much space at all in

110
00:04:51,630 --> 00:04:54,810
this case if the attacker wants to

111
00:04:53,130 --> 00:04:56,670
compute multiple instances of this

112
00:04:54,810 --> 00:04:59,790
function they can compute all of those

113
00:04:56,670 --> 00:05:02,550
instances at one time and or leave them

114
00:04:59,790 --> 00:05:04,710
and here the space time complexity of

115
00:05:02,550 --> 00:05:06,570
you know computing multiple instances is

116
00:05:04,710 --> 00:05:08,370
roughly the same space time complexity

117
00:05:06,570 --> 00:05:10,469
of computing just one instance of the

118
00:05:08,370 --> 00:05:13,620
function right so if this is a password

119
00:05:10,470 --> 00:05:17,430
attacker we don't want we don't want to

120
00:05:13,620 --> 00:05:19,980
give the attacker this advantage so all

121
00:05:17,430 --> 00:05:22,290
one answer Mineko proposed a different

122
00:05:19,980 --> 00:05:24,990
metric called cumulative memory

123
00:05:22,290 --> 00:05:27,900
complexity or cumulative coupling

124
00:05:24,990 --> 00:05:30,960
complexity for graphs here we're just

125
00:05:27,900 --> 00:05:32,880
going to sum over all pebble in rounds

126
00:05:30,960 --> 00:05:35,130
total number of pebbles on the graph at

127
00:05:32,880 --> 00:05:37,800
that point in time right so instead of

128
00:05:35,130 --> 00:05:40,080
the area around this curve we're going

129
00:05:37,800 --> 00:05:41,790
to look at the area under the curve the

130
00:05:40,080 --> 00:05:44,159
nice thing about this is that it

131
00:05:41,790 --> 00:05:47,460
actually approximates amortized area

132
00:05:44,160 --> 00:05:48,750
time complexity quite nicely right the

133
00:05:47,460 --> 00:05:50,700
cumulative memory complexity of

134
00:05:48,750 --> 00:05:53,040
computing two instances of this function

135
00:05:50,700 --> 00:05:58,140
is 2 times the complexity of computing

136
00:05:53,040 --> 00:06:00,540
one instance of the function ok so we're

137
00:05:58,140 --> 00:06:04,169
going to use this notion of attack

138
00:06:00,540 --> 00:06:08,160
quality when we evaluate memory hard

139
00:06:04,169 --> 00:06:10,469
functions so the naive sequential peb

140
00:06:08,160 --> 00:06:12,540
Ling is just going to cost N squared

141
00:06:10,470 --> 00:06:15,060
over two roughly right pebble node one

142
00:06:12,540 --> 00:06:16,620
pebble node to keep all the pebbles on

143
00:06:15,060 --> 00:06:19,770
the graph at the end of the day your

144
00:06:16,620 --> 00:06:22,500
cost is roughly N squared over two so

145
00:06:19,770 --> 00:06:24,180
intuitively we want to ensure that the

146
00:06:22,500 --> 00:06:27,000
optimal peddling that the attacker might

147
00:06:24,180 --> 00:06:30,030
try is nearly as expensive as this naive

148
00:06:27,000 --> 00:06:33,060
pebble in strategy right so the attack

149
00:06:30,030 --> 00:06:35,190
quality really measures the attackers

150
00:06:33,060 --> 00:06:37,020
success in reducing his cost right so we

151
00:06:35,190 --> 00:06:39,150
can talk about the quality of a pebble

152
00:06:37,020 --> 00:06:42,469
attack as the ratio N squared over 2

153
00:06:39,150 --> 00:06:44,960
divided by the cumulative cost of the

154
00:06:42,470 --> 00:06:46,970
so for example if we have a pebble

155
00:06:44,960 --> 00:06:49,280
inning with attack quality 10 this means

156
00:06:46,970 --> 00:06:54,740
that the attacker reduced his cost by

157
00:06:49,280 --> 00:06:59,150
one order of magnitude all right one

158
00:06:54,740 --> 00:07:01,699
more metric to measure the quality of

159
00:06:59,150 --> 00:07:05,448
the memory card function sustained space

160
00:07:01,699 --> 00:07:08,060
complexity so here this is actually a

161
00:07:05,449 --> 00:07:10,729
more more stringent requirement than

162
00:07:08,060 --> 00:07:12,830
cumulative memory complexity so we'll

163
00:07:10,729 --> 00:07:15,650
define s sustained space complexity as a

164
00:07:12,830 --> 00:07:16,969
time or a number of pebble arounds in

165
00:07:15,650 --> 00:07:19,989
which the attacker has at least s

166
00:07:16,969 --> 00:07:23,240
pebbles on the graph right so

167
00:07:19,990 --> 00:07:25,610
intuitively right if we have s sustained

168
00:07:23,240 --> 00:07:27,650
space complexity T then the cumulative

169
00:07:25,610 --> 00:07:31,009
memory complexity has to be at least s

170
00:07:27,650 --> 00:07:37,128
times T right so this is a a stronger

171
00:07:31,009 --> 00:07:39,979
requirement and intuitively it it if we

172
00:07:37,129 --> 00:07:42,590
if we have high st sustained space

173
00:07:39,979 --> 00:07:45,020
complexity then we can rule out to tax

174
00:07:42,590 --> 00:07:47,840
in which the attacker has low memory low

175
00:07:45,020 --> 00:07:51,770
memory usage for any for any point in

176
00:07:47,840 --> 00:07:54,109
time what I'll say about this is that

177
00:07:51,770 --> 00:07:58,389
building a graph with I sustained space

178
00:07:54,110 --> 00:08:02,449
complexity is a very challenging goal so

179
00:07:58,389 --> 00:08:05,360
we gave a theoretical construction at

180
00:08:02,449 --> 00:08:07,580
euro crypt last year but there are no

181
00:08:05,360 --> 00:08:09,529
known practical constructions with

182
00:08:07,580 --> 00:08:16,969
strong sustain space complexity

183
00:08:09,529 --> 00:08:20,500
guarantees ok good so let me give you a

184
00:08:16,969 --> 00:08:22,729
brief overview of what we know about

185
00:08:20,500 --> 00:08:25,339
constructing and attacking depth robust

186
00:08:22,729 --> 00:08:28,370
graphs in particular there's a property

187
00:08:25,339 --> 00:08:31,460
called depth robustness which is really

188
00:08:28,370 --> 00:08:34,070
the key to analyzing data independent

189
00:08:31,460 --> 00:08:37,130
memory heart functions so depth

190
00:08:34,070 --> 00:08:40,219
robustness given a dag G we say that

191
00:08:37,130 --> 00:08:41,500
it's edy depth robust if for all sets of

192
00:08:40,219 --> 00:08:43,550
Eno --tz--

193
00:08:41,500 --> 00:08:46,269
deleting those nodes from the graph

194
00:08:43,549 --> 00:08:49,550
still leaves a path of length at least D

195
00:08:46,269 --> 00:08:51,140
right so we can say that a graph has ET

196
00:08:49,550 --> 00:08:53,089
depth robust if it satisfies this

197
00:08:51,140 --> 00:08:55,130
property if it doesn't satisfy this

198
00:08:53,089 --> 00:08:56,139
property we can say that the graph is ET

199
00:08:55,130 --> 00:08:59,300
reducible

200
00:08:56,139 --> 00:09:01,250
so here's an example of an eID

201
00:08:59,300 --> 00:09:03,199
irreducible graph right if we delete

202
00:09:01,250 --> 00:09:06,910
these two nodes then the longest

203
00:09:03,199 --> 00:09:14,060
remaining path in the graph is is 2

204
00:09:06,910 --> 00:09:16,629
ok so crypto 2016 we gave an attack

205
00:09:14,060 --> 00:09:21,199
which shows that if your graph is not

206
00:09:16,629 --> 00:09:23,959
depth robust then we can get right

207
00:09:21,199 --> 00:09:26,060
parallel pebble an attack that has

208
00:09:23,959 --> 00:09:28,399
cumulative memory complexity little o of

209
00:09:26,060 --> 00:09:34,029
N squared so in other words the attack

210
00:09:28,399 --> 00:09:38,930
quality is going to to be Omega of 1 a

211
00:09:34,029 --> 00:09:41,680
corollary here is actually write any any

212
00:09:38,930 --> 00:09:44,569
directed acyclic graph with in degree 2

213
00:09:41,680 --> 00:09:46,969
has cumulative memory complexity at most

214
00:09:44,569 --> 00:09:49,310
N squared log log n over log n right so

215
00:09:46,970 --> 00:09:51,259
there's this general upper bound and

216
00:09:49,310 --> 00:09:52,550
this actually separates data independent

217
00:09:51,259 --> 00:09:54,259
memory heart functions from

218
00:09:52,550 --> 00:09:55,790
data dependent memory heart functions

219
00:09:54,259 --> 00:09:57,709
right so data dependent memory heart

220
00:09:55,790 --> 00:10:00,889
functions we can get all the way up to N

221
00:09:57,709 --> 00:10:04,189
squared but that's not possible for data

222
00:10:00,889 --> 00:10:07,759
independent memory card functions and in

223
00:10:04,189 --> 00:10:09,759
fact for practical instantiation of

224
00:10:07,759 --> 00:10:14,410
memory heart functions like argon to Y

225
00:10:09,759 --> 00:10:17,959
we can get even higher quality attacks

226
00:10:14,410 --> 00:10:21,589
okay so what about building a secure

227
00:10:17,959 --> 00:10:23,300
memory heart function well there's this

228
00:10:21,589 --> 00:10:25,189
generic lower bound that says that if

229
00:10:23,300 --> 00:10:27,109
your graph is et depth or bust then your

230
00:10:25,189 --> 00:10:33,649
cumulative peddling cost is at least e

231
00:10:27,110 --> 00:10:36,079
times D and at CCS in 2017 we gave a

232
00:10:33,649 --> 00:10:38,839
practical construction of a graph that's

233
00:10:36,079 --> 00:10:43,910
edie depth or bust where e is n over log

234
00:10:38,839 --> 00:10:47,389
n D is Omega n here this means that C C

235
00:10:43,910 --> 00:10:49,610
is at least n squared over over log n ok

236
00:10:47,389 --> 00:10:54,350
so this is nice theoretically this is

237
00:10:49,610 --> 00:10:57,889
nearly optimal and so if we compare to

238
00:10:54,350 --> 00:11:01,220
argon 2 I asymptotically D our sample is

239
00:10:57,889 --> 00:11:03,740
superior so then of course we can ask

240
00:11:01,220 --> 00:11:05,480
the important question well you know

241
00:11:03,740 --> 00:11:09,050
sometimes theory is different different

242
00:11:05,480 --> 00:11:09,440
from practice so what about the constant

243
00:11:09,050 --> 00:11:15,709
factor

244
00:11:09,440 --> 00:11:18,500
here and in the CCS paper our empirical

245
00:11:15,709 --> 00:11:22,369
analysis suggested that theory matched

246
00:11:18,500 --> 00:11:24,470
practice right so here we can see the

247
00:11:22,370 --> 00:11:26,329
best peddling attack we found against T

248
00:11:24,470 --> 00:11:30,079
our sample and it has low quality and

249
00:11:26,329 --> 00:11:32,029
then we can look at the best attack we

250
00:11:30,079 --> 00:11:36,319
found against argon 2i and it has higher

251
00:11:32,029 --> 00:11:38,000
quality right so empirically the you

252
00:11:36,319 --> 00:11:43,639
know theory seemed to match match

253
00:11:38,000 --> 00:11:46,430
practice so now I can tell you about our

254
00:11:43,639 --> 00:11:49,100
contributions so we present a new

255
00:11:46,430 --> 00:11:51,439
analysis of the greedy pebble attack and

256
00:11:49,100 --> 00:11:54,019
we show that it's surprisingly effective

257
00:11:51,439 --> 00:11:56,509
against D our sample and in fact it

258
00:11:54,019 --> 00:11:59,240
reverses prior conclusions so actually

259
00:11:56,509 --> 00:12:02,050
argon 2 I now provides stronger

260
00:11:59,240 --> 00:12:04,519
resistance to known pebble attacks than

261
00:12:02,050 --> 00:12:08,750
D our sample at least for practical

262
00:12:04,519 --> 00:12:10,819
parameter regimes we also give a new

263
00:12:08,750 --> 00:12:13,880
heuristic algorithm for constructing

264
00:12:10,819 --> 00:12:15,560
small depth reducing sets

265
00:12:13,880 --> 00:12:17,360
unfortunately for lack of time I'm not

266
00:12:15,560 --> 00:12:19,430
going to be able to describe that but

267
00:12:17,360 --> 00:12:22,250
you can see the paper for for details

268
00:12:19,430 --> 00:12:25,550
I'll just remark that it significantly

269
00:12:22,250 --> 00:12:28,430
improves on prior algorithms in all of

270
00:12:25,550 --> 00:12:30,680
our empirical analysis and it

271
00:12:28,430 --> 00:12:32,599
potentially has implications for many

272
00:12:30,680 --> 00:12:35,479
other cryptographic objects where depth

273
00:12:32,600 --> 00:12:38,149
robust graphs are used such as proofs of

274
00:12:35,480 --> 00:12:42,709
replication proofs of space obviously

275
00:12:38,149 --> 00:12:46,699
memory hard functions etc ok another

276
00:12:42,709 --> 00:12:49,099
contribution here and this is a trivial

277
00:12:46,699 --> 00:12:50,990
observation but nevertheless one I

278
00:12:49,100 --> 00:12:53,990
include because I think it's important

279
00:12:50,990 --> 00:12:56,300
so there's an easy way to parallelize

280
00:12:53,990 --> 00:13:00,470
computation of the argon two i round

281
00:12:56,300 --> 00:13:03,170
function and if you implement this it

282
00:13:00,470 --> 00:13:05,089
actually increases or it reduces the

283
00:13:03,170 --> 00:13:10,009
attackers cost by nearly an order of

284
00:13:05,089 --> 00:13:13,970
magnitude so what about on the positive

285
00:13:10,009 --> 00:13:15,079
side well we give a new construction of

286
00:13:13,970 --> 00:13:18,050
the data independent memory hard

287
00:13:15,079 --> 00:13:20,750
function by combining D our sample with

288
00:13:18,050 --> 00:13:22,709
a graph called a bit reversal graph and

289
00:13:20,750 --> 00:13:24,839
we show that this graph

290
00:13:22,710 --> 00:13:29,520
has optimal resistance to all known

291
00:13:24,839 --> 00:13:32,580
public attacks and also it's the first

292
00:13:29,520 --> 00:13:34,560
practical construction with strong

293
00:13:32,580 --> 00:13:37,850
sustained space complexity guarantees

294
00:13:34,560 --> 00:13:41,040
I'll say more about that in a bit

295
00:13:37,850 --> 00:13:45,779
so we also give a new peddling reduction

296
00:13:41,040 --> 00:13:47,819
for the xor labeling rule I'll say more

297
00:13:45,779 --> 00:13:49,800
about that in a bit but this is the

298
00:13:47,820 --> 00:13:53,399
labeling rule that is is used in

299
00:13:49,800 --> 00:13:56,339
practice so I should contrast the public

300
00:13:53,399 --> 00:13:58,950
reduction with Finney and Stefano's

301
00:13:56,339 --> 00:14:01,920
reduction so we're still assuming that H

302
00:13:58,950 --> 00:14:04,320
is a random Oracle here but the way that

303
00:14:01,920 --> 00:14:09,990
the random Oracle is used in practice

304
00:14:04,320 --> 00:14:12,630
uses this xor labeling rule the way well

305
00:14:09,990 --> 00:14:13,860
prior public reductions assumes that the

306
00:14:12,630 --> 00:14:16,529
random Oracle was used in a much

307
00:14:13,860 --> 00:14:21,740
different way and that can introduce

308
00:14:16,529 --> 00:14:23,850
some some challenges ok we also give a

309
00:14:21,740 --> 00:14:26,720
construction of an inherently sequential

310
00:14:23,850 --> 00:14:32,330
round function so we can reclaim this

311
00:14:26,720 --> 00:14:36,690
this order of magnitude cost reduction

312
00:14:32,330 --> 00:14:39,300
ok so let me start off by telling you

313
00:14:36,690 --> 00:14:42,200
about the greedy pebble attack and this

314
00:14:39,300 --> 00:14:45,479
is actually quite a simple simple attack

315
00:14:42,200 --> 00:14:47,370
so here we're going to pebble nodes in

316
00:14:45,480 --> 00:14:51,420
topological order and we're just going

317
00:14:47,370 --> 00:14:54,150
to discard a note on pebble V right

318
00:14:51,420 --> 00:14:56,430
after that pebble is no longer needed

319
00:14:54,150 --> 00:14:59,100
right so as soon as we've pebbled the

320
00:14:56,430 --> 00:15:01,620
greatest child of node V we can remove

321
00:14:59,100 --> 00:15:06,920
our pebble on unknown V right so it's a

322
00:15:01,620 --> 00:15:10,350
simple sequential pebble in strategy and

323
00:15:06,920 --> 00:15:12,959
bunet at all showed that the strategy

324
00:15:10,350 --> 00:15:19,320
yields attack quality roughly you know

325
00:15:12,959 --> 00:15:23,310
five against argon to I so know the

326
00:15:19,320 --> 00:15:23,490
parallel public attacks against argon to

327
00:15:23,310 --> 00:15:25,859
I

328
00:15:23,490 --> 00:15:29,640
they actually yield a higher quality

329
00:15:25,860 --> 00:15:32,190
attack right so we get omega n to the

330
00:15:29,640 --> 00:15:34,500
zero point two two so it's an asymptotic

331
00:15:32,190 --> 00:15:36,750
improvement not a constant constant

332
00:15:34,500 --> 00:15:40,470
factor improvement

333
00:15:36,750 --> 00:15:42,660
and for this reason in our CCS paper we

334
00:15:40,470 --> 00:15:45,449
actually didn't look at the performance

335
00:15:42,660 --> 00:15:48,680
of the greedy peddling algorithm and in

336
00:15:45,449 --> 00:15:52,109
hindsight that was that was a mistake

337
00:15:48,680 --> 00:15:55,079
right so what we show in this paper is

338
00:15:52,110 --> 00:15:57,060
that the greedy pebbly algorithm is

339
00:15:55,079 --> 00:16:01,170
effective against the our sample namely

340
00:15:57,060 --> 00:16:03,959
it achieves attack quality login and

341
00:16:01,170 --> 00:16:08,519
this actually matches the asymptotic

342
00:16:03,959 --> 00:16:10,500
lower bound right we prove that CC is at

343
00:16:08,519 --> 00:16:13,410
least n squared over log n so this is a

344
00:16:10,500 --> 00:16:16,680
this is tight what's important here is

345
00:16:13,410 --> 00:16:19,319
that we have good constant factors so

346
00:16:16,680 --> 00:16:22,829
now if we redo the plot including the

347
00:16:19,319 --> 00:16:23,519
greedy publishing strategy here's our

348
00:16:22,829 --> 00:16:26,550
gone to eye

349
00:16:23,519 --> 00:16:29,399
and here's attack quality versus D our

350
00:16:26,550 --> 00:16:31,949
sample so we have high quality attacks

351
00:16:29,399 --> 00:16:34,350
against both but we have even higher

352
00:16:31,949 --> 00:16:38,040
quality attacks against against T our

353
00:16:34,350 --> 00:16:41,459
sample ok and this is even if we go all

354
00:16:38,040 --> 00:16:43,050
the way up to n is equal to 2 to the 24

355
00:16:41,459 --> 00:16:46,859
which I think is on the higher end of

356
00:16:43,050 --> 00:16:51,378
what what one might consider for for

357
00:16:46,860 --> 00:16:53,910
parameter settings ok now I'll just

358
00:16:51,379 --> 00:16:56,519
advertise here our new construction is

359
00:16:53,910 --> 00:17:03,779
down at the bottom of this plot so we do

360
00:16:56,519 --> 00:17:05,609
find a way to fix the problem ok so now

361
00:17:03,779 --> 00:17:08,250
we have an attack which is surprisingly

362
00:17:05,609 --> 00:17:09,899
effective against the our sample we'd

363
00:17:08,250 --> 00:17:11,789
like to construct a new dag which

364
00:17:09,900 --> 00:17:16,530
provides strong resistance to both

365
00:17:11,789 --> 00:17:20,459
attacks so right the parallel depth

366
00:17:16,530 --> 00:17:22,289
reducing attacks from crypto 2016 were

367
00:17:20,459 --> 00:17:25,470
especially effective against argon 2 I

368
00:17:22,289 --> 00:17:27,959
and many other I M HF candidates D our

369
00:17:25,470 --> 00:17:31,350
sample was specifically designed to

370
00:17:27,959 --> 00:17:33,390
resist these types of attacks but as we

371
00:17:31,350 --> 00:17:36,689
just showed the greedy pebble attack is

372
00:17:33,390 --> 00:17:38,580
effective against the our sample so to

373
00:17:36,690 --> 00:17:41,220
construct a dag that resists both

374
00:17:38,580 --> 00:17:45,510
attacks we start off with this bit

375
00:17:41,220 --> 00:17:49,110
reversal graph here right we can

376
00:17:45,510 --> 00:17:50,730
represent each node by by a bit sequence

377
00:17:49,110 --> 00:17:53,280
and we have two layers of nodes

378
00:17:50,730 --> 00:17:55,350
and we'll have an edge from let's look

379
00:17:53,280 --> 00:17:57,930
at this node 0 1 1 so if we reverse

380
00:17:55,350 --> 00:18:01,830
those bits that's 1 1 0 so we'll add an

381
00:17:57,930 --> 00:18:05,460
edge from here 2 1 1 0 now this is an

382
00:18:01,830 --> 00:18:08,850
old graph it was analyzed by linguae

383
00:18:05,460 --> 00:18:11,160
orient arjun back in 1982 and they

384
00:18:08,850 --> 00:18:14,310
showed that any sequential pedaling has

385
00:18:11,160 --> 00:18:16,950
space time complexity N squared in fact

386
00:18:14,310 --> 00:18:21,840
for this reason this was the basis of

387
00:18:16,950 --> 00:18:23,970
the Catena I mhf proposal so a natural

388
00:18:21,840 --> 00:18:25,230
question to ask is what about the

389
00:18:23,970 --> 00:18:29,300
cumulative memory complexity of

390
00:18:25,230 --> 00:18:32,670
sequential couplings and this is a

391
00:18:29,300 --> 00:18:34,080
harder result but if you use the right

392
00:18:32,670 --> 00:18:36,870
potential function you can actually

393
00:18:34,080 --> 00:18:39,870
prove that the cumulative memory

394
00:18:36,870 --> 00:18:42,479
complexity is still N squared right so

395
00:18:39,870 --> 00:18:44,129
not just the space-time complexity but

396
00:18:42,480 --> 00:18:46,170
the cumulative memory complexity is also

397
00:18:44,130 --> 00:18:50,490
N squared but this is only for

398
00:18:46,170 --> 00:18:53,310
sequential peb lines so unfortunately

399
00:18:50,490 --> 00:18:57,300
the bit reversal graph is not a good

400
00:18:53,310 --> 00:19:00,300
candidate in when we use it alone the

401
00:18:57,300 --> 00:19:03,389
reason is that all one and Serban echo

402
00:19:00,300 --> 00:19:06,000
back in 2015 they gave a parallel

403
00:19:03,390 --> 00:19:07,920
peddling with space time complexity n to

404
00:19:06,000 --> 00:19:09,690
the one point five right so if you allow

405
00:19:07,920 --> 00:19:11,910
for parallel peb wings actually the

406
00:19:09,690 --> 00:19:18,780
space-time complexity is not not very

407
00:19:11,910 --> 00:19:21,300
high okay so to patch this problem we

408
00:19:18,780 --> 00:19:23,610
overlay the bit reversal graph with a

409
00:19:21,300 --> 00:19:26,280
depth robust graph in particular D our

410
00:19:23,610 --> 00:19:27,959
sample so if we start with D our sample

411
00:19:26,280 --> 00:19:30,300
our depth robust graph on the bottom and

412
00:19:27,960 --> 00:19:33,000
our bit reversal graph on the top all

413
00:19:30,300 --> 00:19:35,399
we're going to do is we're going to you

414
00:19:33,000 --> 00:19:37,830
know move D our sample on top of the the

415
00:19:35,400 --> 00:19:41,940
bit reversal graph and this gives us our

416
00:19:37,830 --> 00:19:45,149
overlayed graph the first layer now

417
00:19:41,940 --> 00:19:46,890
represents the depth robust graph and we

418
00:19:45,150 --> 00:19:51,600
have all the same edges connecting the

419
00:19:46,890 --> 00:19:54,120
first and second layer so empirical

420
00:19:51,600 --> 00:19:56,550
analysis demonstrates that this graph is

421
00:19:54,120 --> 00:19:59,520
resistant to all known public attacks

422
00:19:56,550 --> 00:20:01,980
through it inherence it inherits

423
00:19:59,520 --> 00:20:04,429
resistance to depth reducing attacks

424
00:20:01,980 --> 00:20:08,269
from D our sample it in hair

425
00:20:04,429 --> 00:20:10,369
resistance to sequential peb wings from

426
00:20:08,269 --> 00:20:12,860
the bit reversal graph and this includes

427
00:20:10,369 --> 00:20:14,899
the greedy pebble attack in any variant

428
00:20:12,860 --> 00:20:16,399
of the you know greedy pebble attack

429
00:20:14,899 --> 00:20:21,139
because we can show that it resists any

430
00:20:16,399 --> 00:20:22,100
sequential peddling strategy but more

431
00:20:21,139 --> 00:20:24,139
interesting

432
00:20:22,100 --> 00:20:26,869
this combined graph actually yields

433
00:20:24,139 --> 00:20:29,240
stronger new properties so it's the

434
00:20:26,869 --> 00:20:32,779
first graph with strong sustain space

435
00:20:29,240 --> 00:20:35,059
complexity guarantees so in particular

436
00:20:32,779 --> 00:20:37,970
we can prove that any parallel peddling

437
00:20:35,059 --> 00:20:39,889
of this graph either has maximal

438
00:20:37,970 --> 00:20:42,799
sustained space complexity and over

439
00:20:39,889 --> 00:20:44,389
login pebbles for n routes or the

440
00:20:42,799 --> 00:20:48,080
cumulative cost of this pebble egg is

441
00:20:44,389 --> 00:20:50,029
even larger than N squared right so this

442
00:20:48,080 --> 00:20:52,100
graph actually achieves high sustained

443
00:20:50,029 --> 00:20:54,619
space complexity and fact if you play

444
00:20:52,100 --> 00:20:59,658
with these parameters you can guarantee

445
00:20:54,619 --> 00:21:02,570
that you know you need s pebbles for 40

446
00:20:59,659 --> 00:21:07,190
rounds and you get a steeper penalty if

447
00:21:02,570 --> 00:21:08,990
you decrease s here we can also prove

448
00:21:07,190 --> 00:21:10,940
that under plausible conjectures which

449
00:21:08,990 --> 00:21:14,090
you know see the paper for those

450
00:21:10,940 --> 00:21:16,669
conjectures any parallel peb Ling has

451
00:21:14,090 --> 00:21:18,949
cumulative cost at least n squared log

452
00:21:16,669 --> 00:21:21,860
log n over log n right so we pick up

453
00:21:18,950 --> 00:21:24,110
this extra log n factor in this this

454
00:21:21,860 --> 00:21:33,049
would be an asymptotically tight tight

455
00:21:24,110 --> 00:21:35,178
result okay good so we can also now turn

456
00:21:33,049 --> 00:21:38,539
our attention back to two pebble

457
00:21:35,179 --> 00:21:40,879
reductions so almond and sermon anko

458
00:21:38,539 --> 00:21:43,990
previously showed that you know any dag

459
00:21:40,879 --> 00:21:46,039
in the parallel random Oracle sorry any

460
00:21:43,990 --> 00:21:47,600
algorithm evaluating our memory hard

461
00:21:46,039 --> 00:21:49,519
function the parallel random Oracle

462
00:21:47,600 --> 00:21:52,699
model can be described in terms of an

463
00:21:49,519 --> 00:21:55,129
equivalent cost pebble angst rata G so

464
00:21:52,700 --> 00:21:57,110
the implication now is that to analyze

465
00:21:55,129 --> 00:22:00,918
the graph it's sufficient to understand

466
00:21:57,110 --> 00:22:02,719
the pebble and cost of of G but now

467
00:22:00,919 --> 00:22:04,249
we're gonna open the hood a little bit

468
00:22:02,720 --> 00:22:07,450
and ask what's the labeling function

469
00:22:04,249 --> 00:22:09,950
that's used so in almond and Serban anko

470
00:22:07,450 --> 00:22:13,609
they assume that the labeling function

471
00:22:09,950 --> 00:22:15,320
used concatenation of labels right so

472
00:22:13,609 --> 00:22:17,340
before we apply the hash function we can

473
00:22:15,320 --> 00:22:19,740
catenate all the parent values and

474
00:22:17,340 --> 00:22:22,919
- them together now in practice

475
00:22:19,740 --> 00:22:24,900
something different is done namely we

476
00:22:22,920 --> 00:22:27,750
take all the parent labels we XOR them

477
00:22:24,900 --> 00:22:30,600
together and then we hash so we'll call

478
00:22:27,750 --> 00:22:33,330
this the XOR label rule and this tends

479
00:22:30,600 --> 00:22:37,379
to be what's used in in practice namely

480
00:22:33,330 --> 00:22:39,510
because it's much more efficient so this

481
00:22:37,380 --> 00:22:41,190
prior intuition that we can focus on

482
00:22:39,510 --> 00:22:44,100
pebble and costs one might wonder is

483
00:22:41,190 --> 00:22:48,380
this actually still true for IMH F's

484
00:22:44,100 --> 00:22:52,740
that use the xor labeling role and

485
00:22:48,380 --> 00:22:55,890
actually the answer is not always so

486
00:22:52,740 --> 00:22:59,130
here's here's a graph where if you use

487
00:22:55,890 --> 00:23:00,630
the xor labeling rule the function that

488
00:22:59,130 --> 00:23:04,490
you define is actually a constant

489
00:23:00,630 --> 00:23:09,120
function right and here the problem is

490
00:23:04,490 --> 00:23:10,650
that W and X they have the same sets of

491
00:23:09,120 --> 00:23:14,040
parents which means that they're going

492
00:23:10,650 --> 00:23:17,730
to have the same labels and now why is

493
00:23:14,040 --> 00:23:20,940
the hash of label W X or label X which

494
00:23:17,730 --> 00:23:23,160
is just a hash of label 0 right so in

495
00:23:20,940 --> 00:23:25,410
fact it's pretty easy to compute this

496
00:23:23,160 --> 00:23:26,970
function and the cost is not at all

497
00:23:25,410 --> 00:23:30,930
related to the pebble and cost of this

498
00:23:26,970 --> 00:23:33,450
of this graph so what we can show is

499
00:23:30,930 --> 00:23:35,790
that if your graph satisfies this

500
00:23:33,450 --> 00:23:39,420
property which we call the unique parent

501
00:23:35,790 --> 00:23:41,730
property that now the same peddling

502
00:23:39,420 --> 00:23:44,700
reduction can go through so namely the

503
00:23:41,730 --> 00:23:46,380
cost of peddling the graph is equal to

504
00:23:44,700 --> 00:23:49,680
the cumulative memory complexity of the

505
00:23:46,380 --> 00:23:52,050
the function here we lose a factor of

506
00:23:49,680 --> 00:23:54,750
Delta where Delta is the maximum in

507
00:23:52,050 --> 00:23:58,050
degree of the graph and we might ask

508
00:23:54,750 --> 00:24:01,410
right is this reduction is this loss of

509
00:23:58,050 --> 00:24:03,389
the Delta factor necessary it turns out

510
00:24:01,410 --> 00:24:05,970
it is so if you take the complete graph

511
00:24:03,390 --> 00:24:08,640
deltas n the cumulative memory

512
00:24:05,970 --> 00:24:11,400
complexity is just n but the cumulative

513
00:24:08,640 --> 00:24:14,130
pebble in complexity is N squared so in

514
00:24:11,400 --> 00:24:16,230
this case loss of lots of Delta is

515
00:24:14,130 --> 00:24:18,720
actually necessary in the theorem but

516
00:24:16,230 --> 00:24:21,750
I'll remark that in practice Delta's you

517
00:24:18,720 --> 00:24:24,090
know a small constant and the unique

518
00:24:21,750 --> 00:24:26,100
parent property does hold in practice

519
00:24:24,090 --> 00:24:28,590
right so this this pebble in reduction

520
00:24:26,100 --> 00:24:31,709
covers all of the interesting examples

521
00:24:28,590 --> 00:24:34,168
that we would we would see in practice

522
00:24:31,710 --> 00:24:37,530
okay so we also find a way to improve

523
00:24:34,169 --> 00:24:41,130
the round function I will remark that we

524
00:24:37,530 --> 00:24:44,700
we implemented our construction and you

525
00:24:41,130 --> 00:24:46,650
can see here whether we use the old edge

526
00:24:44,700 --> 00:24:48,900
structure or our new edge structure

527
00:24:46,650 --> 00:24:50,549
whether we use the old round front old

528
00:24:48,900 --> 00:24:52,650
round function or our new inherently

529
00:24:50,549 --> 00:24:56,639
sequential round function the

530
00:24:52,650 --> 00:24:58,500
performance is the same right so the

531
00:24:56,640 --> 00:25:00,330
optimal thing to do would be are to use

532
00:24:58,500 --> 00:25:02,309
this D our sample plus bit reversal

533
00:25:00,330 --> 00:25:04,439
graph edge structure use the inherently

534
00:25:02,309 --> 00:25:07,379
sequential round function this is going

535
00:25:04,440 --> 00:25:10,230
to maximize the the attackers cost all

536
00:25:07,380 --> 00:25:12,990
right so I'm running out of time I'll

537
00:25:10,230 --> 00:25:14,720
just throw up a few open questions I

538
00:25:12,990 --> 00:25:17,039
think one of the biggest open questions

539
00:25:14,720 --> 00:25:21,030
is really tightening some of these

540
00:25:17,039 --> 00:25:23,640
bounds these concrete bounds on C C of G

541
00:25:21,030 --> 00:25:24,928
right so we have asymptotically where

542
00:25:23,640 --> 00:25:28,260
we're almost there

543
00:25:24,929 --> 00:25:30,390
but there's large gaps between the lower

544
00:25:28,260 --> 00:25:33,960
bounds and upper bounds in terms of

545
00:25:30,390 --> 00:25:36,890
constant factors all right so I will

546
00:25:33,960 --> 00:25:36,890
close there

547
00:25:41,180 --> 00:25:47,650
Thank You Jeremiah we have time for one

548
00:25:44,210 --> 00:25:47,650
quick question if someone has something

549
00:25:51,670 --> 00:25:57,140
you had inputs to some of your functions

550
00:25:54,410 --> 00:25:59,390
where the inputs were x-ored and so the

551
00:25:57,140 --> 00:26:02,830
question arises does it make sense to

552
00:25:59,390 --> 00:26:05,450
compute partial X or x' a head of time

553
00:26:02,830 --> 00:26:07,340
okay yeah so the question is when you're

554
00:26:05,450 --> 00:26:09,230
doing X or s can you compute partial X

555
00:26:07,340 --> 00:26:11,899
or x' ahead of time and the answer is

556
00:26:09,230 --> 00:26:14,140
yes you can you can compute partial X or

557
00:26:11,900 --> 00:26:18,080
x' ahead of time and this is actually

558
00:26:14,140 --> 00:26:19,880
this is actually the reason why we lose

559
00:26:18,080 --> 00:26:22,280
this factor of Delta in our reduction

560
00:26:19,880 --> 00:26:26,720
right so this is this is exactly the

561
00:26:22,280 --> 00:26:29,329
challenge okay thank you Jeremiah

562
00:26:26,720 --> 00:26:29,329
[Applause]

