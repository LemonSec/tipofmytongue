1
00:00:00,359 --> 00:00:04,920
so that was my group and although so we

2
00:00:04,920 --> 00:00:07,099
have the morning session of last year

3
00:00:07,099 --> 00:00:11,780
and uh okay I think we said

4
00:00:12,000 --> 00:00:14,519
so the first half of the session is on

5
00:00:14,519 --> 00:00:18,440
public encryption from Google genius

6
00:00:18,560 --> 00:00:21,840
by Andrew Miguel Charlotte and Alan and

7
00:00:21,840 --> 00:00:23,820
Charlotte is going to give the talk

8
00:00:23,820 --> 00:00:28,519
okay thanks so let's get started

9
00:00:28,740 --> 00:00:30,779
um public building for public encryption

10
00:00:30,779 --> 00:00:32,340
is one of the major goals of

11
00:00:32,340 --> 00:00:34,260
cryptography however the public

12
00:00:34,260 --> 00:00:36,300
encryption schemes we have today are

13
00:00:36,300 --> 00:00:38,040
based on relatively few computational

14
00:00:38,040 --> 00:00:40,020
assumptions

15
00:00:40,020 --> 00:00:42,120
and recently a new assumption has been

16
00:00:42,120 --> 00:00:44,219
introduced the so-called homogeneous

17
00:00:44,219 --> 00:00:45,420
continuous learning with errors

18
00:00:45,420 --> 00:00:48,840
assumption or short hclwe

19
00:00:48,840 --> 00:00:52,260
introduced by Bruno Bruno

20
00:00:52,260 --> 00:00:55,379
and they also prove hardness of their

21
00:00:55,379 --> 00:00:57,239
assumption from standard lattice

22
00:00:57,239 --> 00:00:59,660
problems

23
00:00:59,699 --> 00:01:03,019
then shortly afterwards

24
00:01:03,019 --> 00:01:07,619
gave a reduction from awe to hclwe

25
00:01:07,619 --> 00:01:10,439
so that means that hcw is certainly no

26
00:01:10,439 --> 00:01:12,540
easier than awe

27
00:01:12,540 --> 00:01:14,400
well we don't have a reduction in the

28
00:01:14,400 --> 00:01:17,040
other direction so hcawa might actually

29
00:01:17,040 --> 00:01:20,119
be a weaker assumption

30
00:01:21,240 --> 00:01:23,700
so the original motivation of brunette

31
00:01:23,700 --> 00:01:26,040
to introduce this assumption was to

32
00:01:26,040 --> 00:01:28,200
prove hardness of learning calcium

33
00:01:28,200 --> 00:01:31,320
mixtures so my motivation was not to be

34
00:01:31,320 --> 00:01:34,080
photography and then we asked the

35
00:01:34,080 --> 00:01:35,700
question if this is possible if we can

36
00:01:35,700 --> 00:01:37,979
also build crypto from the hclw

37
00:01:37,979 --> 00:01:39,479
assumption

38
00:01:39,479 --> 00:01:42,659
and the answer is yes so in this work

39
00:01:42,659 --> 00:01:44,700
really construct four public encryption

40
00:01:44,700 --> 00:01:48,240
skills based on hcae hcwe but have

41
00:01:48,240 --> 00:01:50,280
varying tradeoffs

42
00:01:50,280 --> 00:01:52,619
and I'm using the schemes we can also

43
00:01:52,619 --> 00:01:55,320
prove that hcawa is contained in the

44
00:01:55,320 --> 00:01:58,680
complexity class for CK which stands for

45
00:01:58,680 --> 00:02:01,880
statistical zero knowledge

46
00:02:01,920 --> 00:02:03,780
um and this is the complexity picture so

47
00:02:03,780 --> 00:02:06,360
SDK sits in the intersection of NP and

48
00:02:06,360 --> 00:02:08,520
co-np

49
00:02:08,520 --> 00:02:10,860
and we don't know how to prove that hclw

50
00:02:10,860 --> 00:02:13,500
is in code MP without going through our

51
00:02:13,500 --> 00:02:15,120
schemes first so without using our

52
00:02:15,120 --> 00:02:17,160
schemes

53
00:02:17,160 --> 00:02:19,500
so let's take a look at the hcw

54
00:02:19,500 --> 00:02:21,420
distribution before we will see the

55
00:02:21,420 --> 00:02:23,819
encryption scheme

56
00:02:23,819 --> 00:02:25,200
um so in this distribution we have a

57
00:02:25,200 --> 00:02:28,379
secret Vector W which is a unit vector

58
00:02:28,379 --> 00:02:30,840
and the samples are of this form they

59
00:02:30,840 --> 00:02:33,060
are normally distributed but in every

60
00:02:33,060 --> 00:02:35,940
direction orthogonal to W and then in

61
00:02:35,940 --> 00:02:37,860
Direction w it's a noisy discrete

62
00:02:37,860 --> 00:02:40,220
calcium

63
00:02:40,260 --> 00:02:42,060
so this is what the distribution looks

64
00:02:42,060 --> 00:02:44,879
like in Direction w we're at this value

65
00:02:44,879 --> 00:02:47,340
location bills that our distance one

66
00:02:47,340 --> 00:02:50,720
over gamma from one another

67
00:02:51,140 --> 00:02:53,160
situation sometimes pancreat

68
00:02:53,160 --> 00:02:54,780
distribution because it looks like

69
00:02:54,780 --> 00:02:58,700
pancake stepped on top of each other

70
00:03:02,840 --> 00:03:06,000
will look like standard normal samples

71
00:03:06,000 --> 00:03:08,700
but if we look in at the same bricks and

72
00:03:08,700 --> 00:03:11,220
direction we will again see this pancake

73
00:03:11,220 --> 00:03:13,440
structure

74
00:03:13,440 --> 00:03:16,440
and the hcw Assumption states that given

75
00:03:16,440 --> 00:03:19,379
a polynomial number of hclwa samples it

76
00:03:19,379 --> 00:03:20,819
is hard to distinguish them from normal

77
00:03:20,819 --> 00:03:23,420
samples

78
00:03:23,760 --> 00:03:26,519
so um right let's see a summary of the

79
00:03:26,519 --> 00:03:28,500
hardness results so here we're down here

80
00:03:28,500 --> 00:03:30,959
we have standard latest problems and

81
00:03:30,959 --> 00:03:32,280
then brunette

82
00:03:32,280 --> 00:03:34,200
improve that there is a reduction from

83
00:03:34,200 --> 00:03:36,659
these problems to an assumption courts

84
00:03:36,659 --> 00:03:39,000
and then from CMW they give a reduction

85
00:03:39,000 --> 00:03:42,300
to hclwe

86
00:03:42,659 --> 00:03:44,420
um

87
00:03:44,420 --> 00:03:48,420
based on the same lattice assumptions

88
00:03:48,420 --> 00:03:50,519
then I'll show that there's a reduction

89
00:03:50,519 --> 00:03:54,180
from LW to cawe

90
00:03:54,180 --> 00:03:56,159
um if you restrict the space of the

91
00:03:56,159 --> 00:03:59,220
secret of the secret directions that you

92
00:03:59,220 --> 00:04:01,319
can also obtain a reduction from CLW to

93
00:04:01,319 --> 00:04:04,080
lwe

94
00:04:04,080 --> 00:04:06,120
now in our work we build encryption

95
00:04:06,120 --> 00:04:09,120
schemes uh from the h2w Assumption the

96
00:04:09,120 --> 00:04:10,739
first one is called pancake encryption

97
00:04:10,739 --> 00:04:13,379
scheme and as I said before we also show

98
00:04:13,379 --> 00:04:16,798
that hcwe is contained in SDK

99
00:04:16,798 --> 00:04:20,899
and this is also true for awe

100
00:04:22,079 --> 00:04:24,360
so let's take a look at the encryption

101
00:04:24,360 --> 00:04:27,360
scheme so as our secret key we take a

102
00:04:27,360 --> 00:04:30,479
random unit Vector w

103
00:04:30,479 --> 00:04:32,280
and then the public key is the white

104
00:04:32,280 --> 00:04:35,759
Matrix a that consists of hcawe samples

105
00:04:35,759 --> 00:04:39,180
with secret Direction w

106
00:04:39,180 --> 00:04:41,100
so recall this is what the distribution

107
00:04:41,100 --> 00:04:43,500
looks like in Direction w

108
00:04:43,500 --> 00:04:47,100
we have these pancakes that are at

109
00:04:47,100 --> 00:04:48,600
distance one over camera from each other

110
00:04:48,600 --> 00:04:52,080
regardless just some problem parameter

111
00:04:52,080 --> 00:04:53,940
so that means that if I take my secret

112
00:04:53,940 --> 00:04:56,820
Direction w multiplied with the Matrix a

113
00:04:56,820 --> 00:04:59,880
and then multiply all of the spell gamma

114
00:04:59,880 --> 00:05:01,919
and we get something that's close to an

115
00:05:01,919 --> 00:05:04,380
integer so in particular it should be

116
00:05:04,380 --> 00:05:07,699
close to zero Modular One

117
00:05:08,340 --> 00:05:10,199
now in this encryption scheme encrypts

118
00:05:10,199 --> 00:05:12,720
single bits so to encrypt it with zero

119
00:05:12,720 --> 00:05:14,820
we sample a uniformly random binary

120
00:05:14,820 --> 00:05:17,580
Vector T we multiply the public key a

121
00:05:17,580 --> 00:05:19,800
with t and then we put the out rounding

122
00:05:19,800 --> 00:05:22,979
output the rounding of this vector

123
00:05:22,979 --> 00:05:25,259
to encrypted with one we just sample a

124
00:05:25,259 --> 00:05:27,479
normal vector and then we put output the

125
00:05:27,479 --> 00:05:30,380
rounding of this vector

126
00:05:30,419 --> 00:05:32,759
to decrypt we take the inner product of

127
00:05:32,759 --> 00:05:34,620
the secret Direction and the cipher text

128
00:05:34,620 --> 00:05:37,860
uh we multiply everything with gamma and

129
00:05:37,860 --> 00:05:39,419
then we check if this is close to zero

130
00:05:39,419 --> 00:05:40,500
Modular One

131
00:05:40,500 --> 00:05:43,380
if yes we output 0 and if not we output

132
00:05:43,380 --> 00:05:45,300
the bit one

133
00:05:45,300 --> 00:05:47,100
now where is this correct so let's

134
00:05:47,100 --> 00:05:48,780
assume we are encrypting it with zero

135
00:05:48,780 --> 00:05:51,120
then in secret Direction w it will

136
00:05:51,120 --> 00:05:53,759
varies from these pancakes and because

137
00:05:53,759 --> 00:05:56,940
the vector T is binary and multiplying L

138
00:05:56,940 --> 00:05:59,580
times two corresponds to just adding and

139
00:05:59,580 --> 00:06:03,320
subtracting hcw samples

140
00:06:03,320 --> 00:06:05,699
if I look into the secret direction for

141
00:06:05,699 --> 00:06:07,979
example and I take a value from one page

142
00:06:07,979 --> 00:06:10,500
10 code and add it to a value from a

143
00:06:10,500 --> 00:06:12,900
different time code then the result will

144
00:06:12,900 --> 00:06:15,300
also be close to another pancake so the

145
00:06:15,300 --> 00:06:16,560
decryption will output something close

146
00:06:16,560 --> 00:06:18,660
to zero Modular One

147
00:06:18,660 --> 00:06:20,880
and if I'm encrypting a bit one I'm just

148
00:06:20,880 --> 00:06:22,860
taking a stand up on the vector so there

149
00:06:22,860 --> 00:06:24,419
is a good probability that this one

150
00:06:24,419 --> 00:06:26,759
actually lands within the pen in between

151
00:06:26,759 --> 00:06:29,340
the pancakes

152
00:06:29,340 --> 00:06:31,560
okay

153
00:06:31,560 --> 00:06:33,720
so to prove security of our encryption

154
00:06:33,720 --> 00:06:36,060
we use a standard hybrid argument so we

155
00:06:36,060 --> 00:06:38,400
want to prove that giving us a public

156
00:06:38,400 --> 00:06:40,620
key a the encryption of the bit one is

157
00:06:40,620 --> 00:06:42,479
indistinguishable from an encryption of

158
00:06:42,479 --> 00:06:44,360
the bit 0.

159
00:06:44,360 --> 00:06:46,680
so the encryption episode looks like

160
00:06:46,680 --> 00:06:48,300
this as we have just synced so we take

161
00:06:48,300 --> 00:06:50,340
the public key multiplied with a random

162
00:06:50,340 --> 00:06:54,060
binary vector and output the rounding

163
00:06:54,060 --> 00:06:56,880
then by the hclw Assumption we can

164
00:06:56,880 --> 00:06:59,039
replace the public key with the standard

165
00:06:59,039 --> 00:07:02,719
normal Matrix n

166
00:07:03,720 --> 00:07:05,220
we shall in our paper that this

167
00:07:05,220 --> 00:07:07,259
distribution is statistically close to

168
00:07:07,259 --> 00:07:09,000
the distribution end together with the

169
00:07:09,000 --> 00:07:11,639
rounding of a standard normal vector uh

170
00:07:11,639 --> 00:07:12,960
the statistic resistance is a small

171
00:07:12,960 --> 00:07:15,180
constant

172
00:07:15,180 --> 00:07:18,120
and then we can use the hcw Assumption

173
00:07:18,120 --> 00:07:21,720
to replace n with our public key and we

174
00:07:21,720 --> 00:07:24,660
get the encryption of one

175
00:07:24,660 --> 00:07:26,819
now proving this orange came was one of

176
00:07:26,819 --> 00:07:29,479
the main difficulties of our in our work

177
00:07:29,479 --> 00:07:32,520
and the strategy to prove it is that

178
00:07:32,520 --> 00:07:34,020
first we need to define a suitable

179
00:07:34,020 --> 00:07:36,300
running function

180
00:07:36,300 --> 00:07:39,000
and then when we take your pre-image of

181
00:07:39,000 --> 00:07:40,680
the running function

182
00:07:40,680 --> 00:07:42,300
and look at the set then we show that

183
00:07:42,300 --> 00:07:44,580
the probability of n times T Landing in

184
00:07:44,580 --> 00:07:47,039
the set is approximately equal to its

185
00:07:47,039 --> 00:07:49,560
gaussian measure and we proved this

186
00:07:49,560 --> 00:07:53,780
using gaussian hyperconductivity results

187
00:07:53,819 --> 00:07:56,539
foreign

188
00:07:57,919 --> 00:08:00,780
which is based on the hcaw assumption

189
00:08:00,780 --> 00:08:03,120
but some of you might have already

190
00:08:03,120 --> 00:08:05,639
noticed the errors are not optimal so

191
00:08:05,639 --> 00:08:07,319
the decryption error is polynomial

192
00:08:07,319 --> 00:08:10,620
because we have a polynomial Mouse and

193
00:08:10,620 --> 00:08:13,020
the security error is constant

194
00:08:13,020 --> 00:08:14,759
now there are standard techniques to

195
00:08:14,759 --> 00:08:16,319
amplify these errors and bring them down

196
00:08:16,319 --> 00:08:17,639
to negligible

197
00:08:17,639 --> 00:08:19,379
but this comes at a cost because it

198
00:08:19,379 --> 00:08:22,139
increases the complexity of the scheme

199
00:08:22,139 --> 00:08:24,240
so in our work we present three

200
00:08:24,240 --> 00:08:26,220
different ideas that bring down either

201
00:08:26,220 --> 00:08:27,900
the decryption error or the security

202
00:08:27,900 --> 00:08:29,520
error and they're all built on this

203
00:08:29,520 --> 00:08:32,700
pancake scheme which is the basic scheme

204
00:08:32,700 --> 00:08:35,940
um yeah there are sometimes based on a

205
00:08:35,940 --> 00:08:37,200
little bit different assumptions but

206
00:08:37,200 --> 00:08:40,500
these assumptions are lie between crwe

207
00:08:40,500 --> 00:08:43,620
and hclwe

208
00:08:43,620 --> 00:08:45,600
and the good thing about these ideas is

209
00:08:45,600 --> 00:08:48,300
that you can also mix and match them so

210
00:08:48,300 --> 00:08:50,519
each of these idea brings them either

211
00:08:50,519 --> 00:08:51,899
the description error or the security

212
00:08:51,899 --> 00:08:54,180
error but then if you combine two of

213
00:08:54,180 --> 00:08:56,040
them for example the brand model scheme

214
00:08:56,040 --> 00:08:58,080
at the Disco test scheme then we get a

215
00:08:58,080 --> 00:08:59,700
scheme with perfect decryption and

216
00:08:59,700 --> 00:09:03,140
negligible security error

217
00:09:03,240 --> 00:09:06,060
another result as I already said is that

218
00:09:06,060 --> 00:09:09,360
hcrw e is contained in the class SDK and

219
00:09:09,360 --> 00:09:12,000
therefore also in co-np and this is

220
00:09:12,000 --> 00:09:13,980
fairly very surprising result because

221
00:09:13,980 --> 00:09:16,080
this means that even if I have samples

222
00:09:16,080 --> 00:09:17,880
that are just stand up normal I can

223
00:09:17,880 --> 00:09:19,860
prove to you that there's no direction

224
00:09:19,860 --> 00:09:21,959
in which we have this pancake directions

225
00:09:21,959 --> 00:09:24,120
to stand up normal

226
00:09:24,120 --> 00:09:25,440
foreign

227
00:09:25,440 --> 00:09:28,440
programs

228
00:09:28,620 --> 00:09:30,779
first we would like to make the public

229
00:09:30,779 --> 00:09:32,940
encryption scheme more practical uh

230
00:09:32,940 --> 00:09:34,260
because all of the screams above the

231
00:09:34,260 --> 00:09:37,339
encrypt single bits

232
00:09:37,500 --> 00:09:39,899
uh it would also be interesting to

233
00:09:39,899 --> 00:09:42,720
and if SDK membership also holds but

234
00:09:42,720 --> 00:09:44,640
comparing the mixtures of calcium for

235
00:09:44,640 --> 00:09:46,680
example because all of the public key

236
00:09:46,680 --> 00:09:49,200
encryption schemes we have today are

237
00:09:49,200 --> 00:09:50,940
based on assumptions in SDK and

238
00:09:50,940 --> 00:09:52,920
therefore and Co and P it would be very

239
00:09:52,920 --> 00:09:55,260
nice to find an assumption outside of it

240
00:09:55,260 --> 00:09:57,540
error class and try to break public

241
00:09:57,540 --> 00:10:00,120
encryption scheme from it

242
00:10:00,120 --> 00:10:02,040
and then finally we also investigate

243
00:10:02,040 --> 00:10:03,540
further if there might be a reduction

244
00:10:03,540 --> 00:10:07,980
from hclwe to awe

245
00:10:07,980 --> 00:10:09,899
and that's it I'm happy to take

246
00:10:09,899 --> 00:10:12,080
questions

247
00:10:17,339 --> 00:10:19,260
hello thanks for the group you know we

248
00:10:19,260 --> 00:10:22,080
have a lot of times a question so I mean

249
00:10:22,080 --> 00:10:26,060
yeah any questions from governments

250
00:10:26,459 --> 00:10:29,779
so we will get started

251
00:10:30,959 --> 00:10:32,959
um

252
00:10:34,040 --> 00:10:37,740
yeah sure we can go back to this

253
00:10:37,740 --> 00:10:42,420
pancakes kick show once again right so

254
00:10:42,420 --> 00:10:45,300
um our running function is basically uh

255
00:10:45,300 --> 00:10:46,740
where you turn it like this you look at

256
00:10:46,740 --> 00:10:49,500
just the question is there's a standard

257
00:10:49,500 --> 00:10:51,899
number distribution and then we take the

258
00:10:51,899 --> 00:10:54,180
real line and we'll divide it into

259
00:10:54,180 --> 00:10:55,860
interviews that have equal gaussian

260
00:10:55,860 --> 00:10:58,560
measures so just the intervals will be

261
00:10:58,560 --> 00:11:00,480
very small and then make it larger and

262
00:11:00,480 --> 00:11:02,519
larger

263
00:11:02,519 --> 00:11:04,740
um right so that's running function and

264
00:11:04,740 --> 00:11:06,660
then in multiple Dimension you just do

265
00:11:06,660 --> 00:11:08,880
this for every entry

266
00:11:08,880 --> 00:11:11,880
because

267
00:11:19,380 --> 00:11:24,000
so and so I think in our paper we show

268
00:11:24,000 --> 00:11:26,220
that the Precision of like roughly look

269
00:11:26,220 --> 00:11:28,620
in is enough to

270
00:11:28,620 --> 00:11:30,720
um yeah for correctness of the schemes

271
00:11:30,720 --> 00:11:33,779
so it doesn't correct means The

272
00:11:33,779 --> 00:11:36,620
Possession episodes

273
00:11:39,770 --> 00:11:42,789
[Music]

274
00:11:43,680 --> 00:11:46,380
um application yeah because I mean

275
00:11:46,380 --> 00:11:47,700
there's another service production

276
00:11:47,700 --> 00:11:49,500
pancakes they have variants which has to

277
00:11:49,500 --> 00:11:51,060
be an inverse polynomial it cannot be

278
00:11:51,060 --> 00:11:52,860
smaller okay but then it has preliminary

279
00:11:52,860 --> 00:11:54,779
so if you just take something from them

280
00:11:54,779 --> 00:11:56,940
there's always this polynomial chance

281
00:11:56,940 --> 00:12:00,440
that you end up independent

282
00:12:06,779 --> 00:12:08,279
and yeah

283
00:12:08,279 --> 00:12:11,480
there's a question sorry sorry

284
00:12:11,480 --> 00:12:14,700
I just mentioned like uh how do you

285
00:12:14,700 --> 00:12:17,810
combine this is discretized

286
00:12:17,810 --> 00:12:18,120
[Music]

287
00:12:18,120 --> 00:12:19,620
um

288
00:12:19,620 --> 00:12:21,600
um yeah so we haven't seen the scheme so

289
00:12:21,600 --> 00:12:24,120
it's yeah I guess it's not obvious so

290
00:12:24,120 --> 00:12:27,120
but as you can see we both we have um

291
00:12:27,120 --> 00:12:28,860
so the Bermuda scheme brings the

292
00:12:28,860 --> 00:12:30,360
decryption error down to zero and the

293
00:12:30,360 --> 00:12:32,700
discrete scheme makes the security error

294
00:12:32,700 --> 00:12:34,980
negative a and the district case scheme

295
00:12:34,980 --> 00:12:37,260
uh which display oh I forgot to say it's

296
00:12:37,260 --> 00:12:39,180
influenced that time worked by scheme of

297
00:12:39,180 --> 00:12:41,459
97 that's important to say

298
00:12:41,459 --> 00:12:43,620
um this is a way to discretize the

299
00:12:43,620 --> 00:12:46,160
public key

300
00:12:46,200 --> 00:12:48,600
um and then so you can first do that and

301
00:12:48,600 --> 00:12:50,579
then you can use the bimodal idea on top

302
00:12:50,579 --> 00:12:53,040
which means services that you don't only

303
00:12:53,040 --> 00:12:55,320
use a samples that are close to zero

304
00:12:55,320 --> 00:12:57,720
what do you mean but you publish two

305
00:12:57,720 --> 00:12:59,639
matrices one with samples so that close

306
00:12:59,639 --> 00:13:02,100
to zero one of the samples that close to

307
00:13:02,100 --> 00:13:04,980
one half Modular One and so then if you

308
00:13:04,980 --> 00:13:07,019
encrypted zero you take the one public

309
00:13:07,019 --> 00:13:09,240
key Matrix if you encrypt one would take

310
00:13:09,240 --> 00:13:11,399
the other one and so this brings down

311
00:13:11,399 --> 00:13:13,260
the decryption error and you can also on

312
00:13:13,260 --> 00:13:14,880
top you can also discretize the samples

313
00:13:14,880 --> 00:13:19,160
and then yeah use both ideas together

314
00:13:19,279 --> 00:13:22,820
any further questions

315
00:13:22,920 --> 00:13:25,760
sticker again

316
00:14:00,660 --> 00:14:03,380
sorry

317
00:14:35,820 --> 00:14:38,100
okay so on to the next talk which is

318
00:14:38,100 --> 00:14:41,399
about uh Peep and is as hard as LW and

319
00:14:41,399 --> 00:14:42,899
iterated squaring and the paper is by

320
00:14:42,899 --> 00:14:46,800
near ARCA Justin Alex Omer Ron and Jaden

321
00:14:46,800 --> 00:14:49,459
is going to give the talk

322
00:14:56,279 --> 00:15:00,180
now yeah good

323
00:15:00,180 --> 00:15:03,180
uh thanks Russia uh so

324
00:15:03,180 --> 00:15:05,760
so let's speak about uh it stands for

325
00:15:05,760 --> 00:15:07,860
polynomial priority argument on directed

326
00:15:07,860 --> 00:15:10,380
graphs it's a class of search problems

327
00:15:10,380 --> 00:15:12,839
and and it contains uh search problems

328
00:15:12,839 --> 00:15:14,519
which which which come from various

329
00:15:14,519 --> 00:15:17,459
Fields like combinatorics topology green

330
00:15:17,459 --> 00:15:19,560
Theory perhaps the most famous one is

331
00:15:19,560 --> 00:15:21,420
the is the problem of computing a Nash

332
00:15:21,420 --> 00:15:22,620
equilibrium

333
00:15:22,620 --> 00:15:24,959
and people have been studying uh looking

334
00:15:24,959 --> 00:15:27,240
for efficient algorithms for all these

335
00:15:27,240 --> 00:15:29,220
uh problems and and they have not

336
00:15:29,220 --> 00:15:31,380
succeeded therefore there is some

337
00:15:31,380 --> 00:15:33,779
inherent uh evidence that these problems

338
00:15:33,779 --> 00:15:35,820
are all hard uh so what we are

339
00:15:35,820 --> 00:15:38,040
interested in this talk is uh is whether

340
00:15:38,040 --> 00:15:40,139
we can give explicit uh evidence for

341
00:15:40,139 --> 00:15:42,959
hardness so we want to show that uh you

342
00:15:42,959 --> 00:15:47,300
can use crypto to show 100 people

343
00:15:47,300 --> 00:15:50,760
so why do we believe this should hold so

344
00:15:50,760 --> 00:15:53,399
pivot is a is a subset of a larger class

345
00:15:53,399 --> 00:15:57,720
called tfnp and and tfnp is home to many

346
00:15:57,720 --> 00:16:00,180
of our favorite uh hardness assumptions

347
00:16:00,180 --> 00:16:03,360
like factoring or or finding collisions

348
00:16:03,360 --> 00:16:05,579
in hash functions but we don't know

349
00:16:05,579 --> 00:16:07,440
whether any of these uh assumptions

350
00:16:07,440 --> 00:16:10,320
implied hardness in pipad

351
00:16:10,320 --> 00:16:11,940
and that's what you would like to

352
00:16:11,940 --> 00:16:14,519
investigate

353
00:16:14,519 --> 00:16:18,060
so uh we want to show uh that crypto

354
00:16:18,060 --> 00:16:19,980
implies hardness in in this class P bad

355
00:16:19,980 --> 00:16:22,139
and and we want to show uh it from

356
00:16:22,139 --> 00:16:23,940
standard crypto assumptions so we want

357
00:16:23,940 --> 00:16:26,699
to rely on well studied and and poly

358
00:16:26,699 --> 00:16:29,279
assumptions where by poly I mean uh that

359
00:16:29,279 --> 00:16:31,800
the Assumption it should be just with

360
00:16:31,800 --> 00:16:34,560
respect to polynomial size adversaries

361
00:16:34,560 --> 00:16:36,360
and and there have been two broad

362
00:16:36,360 --> 00:16:38,339
approaches uh which have been used to

363
00:16:38,339 --> 00:16:40,259
show prepared harness the first is

364
00:16:40,259 --> 00:16:42,120
called the apostopia approach and and

365
00:16:42,120 --> 00:16:43,980
the second system we will call the proof

366
00:16:43,980 --> 00:16:46,800
system approach uh in the postopia

367
00:16:46,800 --> 00:16:48,779
approach as the name suggests we rely on

368
00:16:48,779 --> 00:16:50,880
apostopia Primitives to show the bad

369
00:16:50,880 --> 00:16:54,180
hardness and and prior works showed that

370
00:16:54,180 --> 00:16:56,459
if you have functional encryption you

371
00:16:56,459 --> 00:16:59,160
can get prepared harness and when you

372
00:16:59,160 --> 00:17:01,259
couple this with the recent advances in

373
00:17:01,259 --> 00:17:03,480
in functional encryption you get pipette

374
00:17:03,480 --> 00:17:05,760
hardness from the same three assumptions

375
00:17:05,760 --> 00:17:10,500
which give you a functional encryption

376
00:17:10,500 --> 00:17:13,980
and on the other side you have the the

377
00:17:13,980 --> 00:17:16,079
proof system approach and here uh what

378
00:17:16,079 --> 00:17:18,599
you do is you you build uh something

379
00:17:18,599 --> 00:17:20,339
called mergeable and unique

380
00:17:20,339 --> 00:17:22,859
non-interrupt arguments and and this is

381
00:17:22,859 --> 00:17:24,540
known to suffice to give you people

382
00:17:24,540 --> 00:17:25,559
hardness

383
00:17:25,559 --> 00:17:28,020
but we didn't know how to construct this

384
00:17:28,020 --> 00:17:30,780
uh this this unique mergeable

385
00:17:30,780 --> 00:17:32,400
non-interactive arguments which I will

386
00:17:32,400 --> 00:17:34,200
just refer to as special non-interactive

387
00:17:34,200 --> 00:17:36,720
arguments uh we didn't know how to

388
00:17:36,720 --> 00:17:39,059
construct this from standard assumptions

389
00:17:39,059 --> 00:17:40,700
and and that is what we do in this paper

390
00:17:40,700 --> 00:17:43,919
so we we will construct uh this special

391
00:17:43,919 --> 00:17:45,840
non-integrative Arguments for iterative

392
00:17:45,840 --> 00:17:48,539
squaring assuming learning with errors

393
00:17:48,539 --> 00:17:50,520
uh so for those of you who don't know

394
00:17:50,520 --> 00:17:52,320
what iterated squaring is so you're

395
00:17:52,320 --> 00:17:54,419
working in a group of unknown order and

396
00:17:54,419 --> 00:17:57,120
you're given a group element G small G

397
00:17:57,120 --> 00:17:59,640
and the time parameter T your goal is to

398
00:17:59,640 --> 00:18:01,980
compute G to the 2 to the T So exponent

399
00:18:01,980 --> 00:18:04,679
J basically many times and it's assumed

400
00:18:04,679 --> 00:18:06,539
that this is hard

401
00:18:06,539 --> 00:18:08,520
uh so I'd like to point out that like

402
00:18:08,520 --> 00:18:09,900
yeah well these problems have been

403
00:18:09,900 --> 00:18:13,940
around for a few decades now

404
00:18:13,980 --> 00:18:16,799
okay so so we have reduced the task of

405
00:18:16,799 --> 00:18:18,980
uh showing people harness to to to

406
00:18:18,980 --> 00:18:20,940
constructing these special uh

407
00:18:20,940 --> 00:18:23,220
non-interactive arguments and the way

408
00:18:23,220 --> 00:18:25,799
that our work and also prior Works do

409
00:18:25,799 --> 00:18:28,559
this is by first constructing special uh

410
00:18:28,559 --> 00:18:30,900
interactive protocols for some hard

411
00:18:30,900 --> 00:18:32,460
language

412
00:18:32,460 --> 00:18:36,140
and then applying the pH semi transform

413
00:18:36,140 --> 00:18:39,059
so so we can like like plot all the

414
00:18:39,059 --> 00:18:41,760
prior Works in like in a nice graph and

415
00:18:41,760 --> 00:18:44,220
so on the x-axis you have the the

416
00:18:44,220 --> 00:18:45,360
Assumption required for the other

417
00:18:45,360 --> 00:18:48,419
language and on the on the y-axis we we

418
00:18:48,419 --> 00:18:52,080
plot uh the the Assumption required to

419
00:18:52,080 --> 00:18:54,179
to uh

420
00:18:54,179 --> 00:18:57,360
required for the psmware hash function

421
00:18:57,360 --> 00:19:00,059
so so the assumptions become stronger as

422
00:19:00,059 --> 00:19:01,559
you go away from the origin so you need

423
00:19:01,559 --> 00:19:04,500
something ideally closer to the origin

424
00:19:04,500 --> 00:19:06,600
so the first line of Works they were in

425
00:19:06,600 --> 00:19:08,400
the random Oracle model what they showed

426
00:19:08,400 --> 00:19:10,380
is that you can construct these current

427
00:19:10,380 --> 00:19:12,660
interactive arguments from either the

428
00:19:12,660 --> 00:19:14,820
subject protocol or the subject like

429
00:19:14,820 --> 00:19:16,860
protocol of pH stack

430
00:19:16,860 --> 00:19:20,640
uh yeah and and then the next lineup

431
00:19:20,640 --> 00:19:22,559
Works uh basically got rid of these

432
00:19:22,559 --> 00:19:24,960
random articles and then replace them

433
00:19:24,960 --> 00:19:27,419
with new hash functions uh from sub

434
00:19:27,419 --> 00:19:30,360
exponential learning with errors

435
00:19:30,360 --> 00:19:34,799
and to get our result uh what we do is

436
00:19:34,799 --> 00:19:37,740
we we rely on a new interactive protocol

437
00:19:37,740 --> 00:19:40,200
uh by block at all which appeared at the

438
00:19:40,200 --> 00:19:41,220
last crypto

439
00:19:41,220 --> 00:19:43,320
you can think of this as like a very

440
00:19:43,320 --> 00:19:45,480
complicated uh parallel limited version

441
00:19:45,480 --> 00:19:47,340
of PHX protocol

442
00:19:47,340 --> 00:19:49,919
and then we use the the new hash

443
00:19:49,919 --> 00:19:52,500
function of hlr

444
00:19:52,500 --> 00:19:54,600
for parallel limited IPS so when you

445
00:19:54,600 --> 00:19:57,600
combine these two we can show uh we can

446
00:19:57,600 --> 00:19:59,100
construct this special on interactive

447
00:19:59,100 --> 00:20:01,380
organs

448
00:20:01,380 --> 00:20:05,220
okay so uh I'm not yet told you what

449
00:20:05,220 --> 00:20:07,380
these two uh properties are

450
00:20:07,380 --> 00:20:09,179
what makes the Donald director argument

451
00:20:09,179 --> 00:20:10,260
special

452
00:20:10,260 --> 00:20:12,720
so I will not uh give like a general

453
00:20:12,720 --> 00:20:14,520
definition since the for the rest of the

454
00:20:14,520 --> 00:20:16,679
talk I I I I will focus on iterative

455
00:20:16,679 --> 00:20:18,900
squaring I will just Define this

456
00:20:18,900 --> 00:20:21,419
properties for the language of iterative

457
00:20:21,419 --> 00:20:24,299
squaring so to recall uh in enter this

458
00:20:24,299 --> 00:20:25,860
querying you are given a group of a

459
00:20:25,860 --> 00:20:27,679
non-order and a group element small G

460
00:20:27,679 --> 00:20:30,059
time parameter T and your goal is to

461
00:20:30,059 --> 00:20:32,039
compute digital to the T so you can

462
00:20:32,039 --> 00:20:34,320
think of it as there being like a a long

463
00:20:34,320 --> 00:20:37,100
line of computation

464
00:20:37,100 --> 00:20:41,700
and uh so this naturally defines a a

465
00:20:41,700 --> 00:20:45,600
language uh for for this for for this uh

466
00:20:45,600 --> 00:20:48,600
problem and it can consists of of tuples

467
00:20:48,600 --> 00:20:50,520
of the form GTH

468
00:20:50,520 --> 00:20:53,760
such that uh DTS uh respects the

469
00:20:53,760 --> 00:20:56,220
exponentiation

470
00:20:56,220 --> 00:20:58,860
so the first properties is called merge

471
00:20:58,860 --> 00:21:00,960
ability and what we require from this

472
00:21:00,960 --> 00:21:02,340
property is that it should be possible

473
00:21:02,340 --> 00:21:05,520
to take two small proofs of related

474
00:21:05,520 --> 00:21:08,100
statements and then merge them into a

475
00:21:08,100 --> 00:21:10,140
proof for a larger related statement and

476
00:21:10,140 --> 00:21:12,780
and and the key property we need is this

477
00:21:12,780 --> 00:21:14,820
should be faster than Computing the

478
00:21:14,820 --> 00:21:18,559
proof the larger proof from scratch

479
00:21:18,660 --> 00:21:20,700
and the second so let's look at an

480
00:21:20,700 --> 00:21:21,780
example

481
00:21:21,780 --> 00:21:25,740
so here you have a

482
00:21:25,740 --> 00:21:28,620
uh approved by one for the left half of

483
00:21:28,620 --> 00:21:30,419
the computation and a pipe two proof by

484
00:21:30,419 --> 00:21:31,860
two for the the second half of the

485
00:21:31,860 --> 00:21:33,059
computation and you should be able to

486
00:21:33,059 --> 00:21:34,620
produce a proof price for the whole

487
00:21:34,620 --> 00:21:36,780
computation faster than Computing it

488
00:21:36,780 --> 00:21:39,260
from scratch

489
00:21:39,720 --> 00:21:41,659
and the second property we need is is

490
00:21:41,659 --> 00:21:45,120
uniqueness and as the name suggests this

491
00:21:45,120 --> 00:21:46,679
this says that it should be hard to

492
00:21:46,679 --> 00:21:49,260
compute two proofs for the same uh true

493
00:21:49,260 --> 00:21:51,740
statement

494
00:21:51,780 --> 00:21:53,760
uh for example it should be hard to

495
00:21:53,760 --> 00:21:56,760
produce a proof by two tilde for for the

496
00:21:56,760 --> 00:21:59,039
second of the computation

497
00:21:59,039 --> 00:22:00,720
so for the rest of the truck we will

498
00:22:00,720 --> 00:22:04,200
focus on obtaining on achieving a

499
00:22:04,200 --> 00:22:07,200
uniqueness because we know kind of how

500
00:22:07,200 --> 00:22:09,440
to obtain merge merge ability

501
00:22:09,440 --> 00:22:12,240
relying on previous works so we'll focus

502
00:22:12,240 --> 00:22:14,880
on uniqueness

503
00:22:14,880 --> 00:22:17,700
okay so our goal is to construct this

504
00:22:17,700 --> 00:22:20,039
special non-interactive arguments and

505
00:22:20,039 --> 00:22:22,260
our first attempt would be to just look

506
00:22:22,260 --> 00:22:24,000
at psgx protocol

507
00:22:24,000 --> 00:22:27,059
for iterated squaring so what is PSX

508
00:22:27,059 --> 00:22:29,520
protocol uh so it's an interactive

509
00:22:29,520 --> 00:22:31,620
protocol the prover and the very first

510
00:22:31,620 --> 00:22:34,140
start with the statement of lens t

511
00:22:34,140 --> 00:22:36,299
uh GTH

512
00:22:36,299 --> 00:22:38,940
and the protocols recursive

513
00:22:38,940 --> 00:22:42,419
and the Brewer in the first round sends

514
00:22:42,419 --> 00:22:45,179
a midpoint mu this splits the the

515
00:22:45,179 --> 00:22:46,860
statement the original statement into

516
00:22:46,860 --> 00:22:49,799
two statements of of length half

517
00:22:49,799 --> 00:22:52,140
then the verifier sends a long random

518
00:22:52,140 --> 00:22:54,539
string which which the program merger uh

519
00:22:54,539 --> 00:22:56,940
program verifier used to merge the the

520
00:22:56,940 --> 00:22:59,100
two intermediate statements into into a

521
00:22:59,100 --> 00:23:03,659
new statement of length uh a t over 2.

522
00:23:03,659 --> 00:23:06,299
uh and the number is using a random

523
00:23:06,299 --> 00:23:08,220
linear combination

524
00:23:08,220 --> 00:23:11,220
and they recurse they keep on doing this

525
00:23:11,220 --> 00:23:14,280
until they end up with a proof with the

526
00:23:14,280 --> 00:23:16,380
statement of lens two at which point the

527
00:23:16,380 --> 00:23:18,120
verifier can check itself whether the

528
00:23:18,120 --> 00:23:21,059
statement is true or not

529
00:23:21,059 --> 00:23:23,700
so so why is this protocol sound it can

530
00:23:23,700 --> 00:23:26,580
be shown that there exists a a unique

531
00:23:26,580 --> 00:23:30,240
bad challenge R where I call a bad

532
00:23:30,240 --> 00:23:32,700
challenge R which takes a true statement

533
00:23:32,700 --> 00:23:36,059
and then at the end of the merging you

534
00:23:36,059 --> 00:23:38,159
end up with a true statement this is

535
00:23:38,159 --> 00:23:40,740
what I will call bad challenge R and

536
00:23:40,740 --> 00:23:43,919
this will imply statistical soundness

537
00:23:43,919 --> 00:23:46,320
but the problem is like we don't know

538
00:23:46,320 --> 00:23:48,900
how to instantiate the pH summer uh uh

539
00:23:48,900 --> 00:23:53,100
for uh for this uh protocol and the

540
00:23:53,100 --> 00:23:55,280
problem is tied to this uh bad challenge

541
00:23:55,280 --> 00:23:59,340
so very uh briefly that the complexity

542
00:23:59,340 --> 00:24:01,860
uh required for instantiating the hash

543
00:24:01,860 --> 00:24:04,740
function for the first summer it is it

544
00:24:04,740 --> 00:24:07,020
is it it relies on the the hardness of

545
00:24:07,020 --> 00:24:10,679
computing this bad R and this will be uh

546
00:24:10,679 --> 00:24:12,900
this will end up being like a hard DLP

547
00:24:12,900 --> 00:24:15,900
and so we don't know how to uh use prior

548
00:24:15,900 --> 00:24:19,860
Works to instantiate the hash function

549
00:24:19,860 --> 00:24:22,080
so this doesn't work

550
00:24:22,080 --> 00:24:24,539
uh so so what's the next logical step

551
00:24:24,539 --> 00:24:27,360
the hurdle in the previous talk was this

552
00:24:27,360 --> 00:24:29,700
this long uh random strings right so

553
00:24:29,700 --> 00:24:31,559
what you could do is like you could work

554
00:24:31,559 --> 00:24:33,600
with smaller strings smaller challenges

555
00:24:33,600 --> 00:24:36,360
but then parallel repeat and that's what

556
00:24:36,360 --> 00:24:37,799
is shown here

557
00:24:37,799 --> 00:24:39,960
the details are not important

558
00:24:39,960 --> 00:24:42,720
uh and this protocol it actually turns

559
00:24:42,720 --> 00:24:45,000
out uh to to be song because like

560
00:24:45,000 --> 00:24:47,159
parallel repetition it it amplifies

561
00:24:47,159 --> 00:24:48,780
soundness so we can show this to be

562
00:24:48,780 --> 00:24:50,640
sound

563
00:24:50,640 --> 00:24:53,340
um more importantly it can we can use

564
00:24:53,340 --> 00:24:56,100
the the hash function from hlr21 to

565
00:24:56,100 --> 00:24:57,720
instantiate the hash function for this

566
00:24:57,720 --> 00:24:59,520
interactive protocol

567
00:24:59,520 --> 00:25:02,159
and so it's great so we have like solve

568
00:25:02,159 --> 00:25:03,539
like the first issue like one of the

569
00:25:03,539 --> 00:25:04,559
problems

570
00:25:04,559 --> 00:25:07,140
however it turns out that this uh

571
00:25:07,140 --> 00:25:09,360
protocol uh it does it's not it does not

572
00:25:09,360 --> 00:25:10,919
have unique proofs

573
00:25:10,919 --> 00:25:13,860
uh so let's see what is the craze

574
00:25:13,860 --> 00:25:16,380
uh so so recall that the goal is to

575
00:25:16,380 --> 00:25:17,640
compute two proofs for the same

576
00:25:17,640 --> 00:25:20,039
statement so so the what the what the

577
00:25:20,039 --> 00:25:22,740
cheating program can do is just deviate

578
00:25:22,740 --> 00:25:26,820
uh like at like one in one round uh in

579
00:25:26,820 --> 00:25:29,220
at some point and what this what happens

580
00:25:29,220 --> 00:25:31,559
is that since the the random coins are

581
00:25:31,559 --> 00:25:33,539
small for for this for this repetition

582
00:25:33,539 --> 00:25:35,760
there's a good chance that the the the

583
00:25:35,760 --> 00:25:38,039
the the the the protocol recovers in

584
00:25:38,039 --> 00:25:40,679
some sense and and this the proof you

585
00:25:40,679 --> 00:25:42,120
that that you get at the end of

586
00:25:42,120 --> 00:25:45,000
everything still verifies

587
00:25:45,000 --> 00:25:48,659
so so to to reiterate uh like the the

588
00:25:48,659 --> 00:25:50,460
parallel repeated protocol it has like

589
00:25:50,460 --> 00:25:54,720
single point of failure

590
00:25:54,720 --> 00:25:57,360
so this also doesn't work and as our

591
00:25:57,360 --> 00:25:59,460
solution is to not do like a vanilla

592
00:25:59,460 --> 00:26:01,559
parallel repetition but to do parallel

593
00:26:01,559 --> 00:26:04,080
repetition with mixing as was done in a

594
00:26:04,080 --> 00:26:07,140
recent work of block at all so uh

595
00:26:07,140 --> 00:26:09,419
instead of just taking the the

596
00:26:09,419 --> 00:26:11,120
intermediate statements and and and

597
00:26:11,120 --> 00:26:13,679
mixing them separately what what we do

598
00:26:13,679 --> 00:26:15,179
is like we take all the intermediate

599
00:26:15,179 --> 00:26:18,059
statements and then mix them into like a

600
00:26:18,059 --> 00:26:20,100
single statement

601
00:26:20,100 --> 00:26:23,340
and and uh this condition to be sound

602
00:26:23,340 --> 00:26:26,159
and more importantly this this preserves

603
00:26:26,159 --> 00:26:27,980
uniqueness and to see what is the case

604
00:26:27,980 --> 00:26:30,720
if the Brewer tries to deviate like

605
00:26:30,720 --> 00:26:33,059
before what happens is that we can show

606
00:26:33,059 --> 00:26:35,340
that there exists like a a false

607
00:26:35,340 --> 00:26:37,740
statement and there is some false

608
00:26:37,740 --> 00:26:40,080
statement in some repetition

609
00:26:40,080 --> 00:26:42,840
and and this suffices uh for the

610
00:26:42,840 --> 00:26:44,820
verifier to reject the proof so it's

611
00:26:44,820 --> 00:26:46,559
it's so to it's hard to compute two

612
00:26:46,559 --> 00:26:50,779
proofs uh for the same statement

613
00:26:51,000 --> 00:26:54,720
and uh we can use the the hash from hlr

614
00:26:54,720 --> 00:26:58,320
still or inserting instantiating the the

615
00:26:58,320 --> 00:27:00,419
the the pH summer and it it will

616
00:27:00,419 --> 00:27:03,679
preserve uniqueness as well

617
00:27:04,340 --> 00:27:07,980
so uh to to wrap up uh so we can take

618
00:27:07,980 --> 00:27:10,020
the the non-injective argument the

619
00:27:10,020 --> 00:27:11,460
special non-interactive argument we got

620
00:27:11,460 --> 00:27:13,799
in the the previous uh slide it will

621
00:27:13,799 --> 00:27:16,980
give us prepared hardness from poly uh

622
00:27:16,980 --> 00:27:18,840
hardness of both uh iterative squaring

623
00:27:18,840 --> 00:27:21,299
and learning with errors

624
00:27:21,299 --> 00:27:24,000
and we can uh generalize in various ways

625
00:27:24,000 --> 00:27:25,740
but for the interest rate of time I'll

626
00:27:25,740 --> 00:27:27,900
skip it but I will uh mention that it

627
00:27:27,900 --> 00:27:30,179
remains an interesting open question to

628
00:27:30,179 --> 00:27:32,960
to weaken the assumptions further from

629
00:27:32,960 --> 00:27:36,539
uh from uh further so for example can we

630
00:27:36,539 --> 00:27:38,400
do it from factoring or just learning

631
00:27:38,400 --> 00:27:40,620
with errors

632
00:27:40,620 --> 00:27:45,240
uh as a bonus we get uh vdfs unique vdfs

633
00:27:45,240 --> 00:27:47,400
so these are videos with unique pros

634
00:27:47,400 --> 00:27:49,980
from uh iterated squaring and learning

635
00:27:49,980 --> 00:27:52,559
if there's all quality assumptions

636
00:27:52,559 --> 00:27:55,799
and queryville in the next talk uh talk

637
00:27:55,799 --> 00:27:57,960
about how you can further weaken these

638
00:27:57,960 --> 00:27:59,940
assumptions to to any sequentially hard

639
00:27:59,940 --> 00:28:02,700
function and LW

640
00:28:02,700 --> 00:28:05,840
so thank you

641
00:28:11,220 --> 00:28:13,440
thanks children so questions from

642
00:28:13,440 --> 00:28:15,299
billions

643
00:28:15,299 --> 00:28:18,000
meanwhile okay so uh is there any hope

644
00:28:18,000 --> 00:28:20,340
to reduce it down to just I treated

645
00:28:20,340 --> 00:28:22,620
squaring just I didn't know no other

646
00:28:22,620 --> 00:28:25,320
assumption it's not

647
00:28:25,320 --> 00:28:27,539
um there's no uh refutation to that but

648
00:28:27,539 --> 00:28:29,400
yeah but the hard thing is we don't know

649
00:28:29,400 --> 00:28:31,559
anything uh food production techniques

650
00:28:31,559 --> 00:28:33,659
from my credit squaring that's the no

651
00:28:33,659 --> 00:28:35,520
the issue is we don't know anything for

652
00:28:35,520 --> 00:28:38,580
uh Collision intractability from my

653
00:28:38,580 --> 00:28:40,200
assumptions yeah I mean it's like any

654
00:28:40,200 --> 00:28:41,400
type of hash function that you can do

655
00:28:41,400 --> 00:28:43,980
use for long reduction yes so all the

656
00:28:43,980 --> 00:28:46,860
the the constructions we know is from

657
00:28:46,860 --> 00:28:50,159
like LPN or lwe or CDH type assumptions

658
00:28:50,159 --> 00:28:53,039
nothing from factoring prep assumption

659
00:28:53,039 --> 00:28:55,820
more questions

660
00:28:56,400 --> 00:28:59,000
please

661
00:28:59,880 --> 00:29:02,600
our hands this time

662
00:29:02,659 --> 00:29:07,700
yeah no no there's a mic it's coming too

663
00:29:08,880 --> 00:29:10,980
all right I must have I I think I missed

664
00:29:10,980 --> 00:29:12,960
something on the last slide so people

665
00:29:12,960 --> 00:29:15,600
had hardness does not imply you you eopl

666
00:29:15,600 --> 00:29:17,940
no that's right no but you do show you

667
00:29:17,940 --> 00:29:21,120
what what does the eobl under okay so I

668
00:29:21,120 --> 00:29:22,980
didn't talk about it but this is a class

669
00:29:22,980 --> 00:29:25,620
which lies lower than p-pad right and

670
00:29:25,620 --> 00:29:28,020
and we can extend the results we should

671
00:29:28,020 --> 00:29:30,240
probably pad to Europe I didn't talk

672
00:29:30,240 --> 00:29:32,279
about that but you can do it and you

673
00:29:32,279 --> 00:29:36,919
show that in the paper yes thank you

674
00:29:37,220 --> 00:29:39,659
so any other questions

675
00:29:39,659 --> 00:29:41,640
I've got an email the next speaker sets

676
00:29:41,640 --> 00:29:43,200
up so just a quick question that how

677
00:29:43,200 --> 00:29:44,820
many repetitions do you need what's the

678
00:29:44,820 --> 00:29:47,340
is there like it's some statistical

679
00:29:47,340 --> 00:29:49,799
parameter Lambda okay can it be sort of

680
00:29:49,799 --> 00:29:51,299
just smaller because that might make the

681
00:29:51,299 --> 00:29:53,580
uh the hash function simpler

682
00:29:53,580 --> 00:29:57,120
absolutely questions so we know some

683
00:29:57,120 --> 00:30:00,899
tricks how to uh to to make like to

684
00:30:00,899 --> 00:30:02,399
reduce the number of competitions but

685
00:30:02,399 --> 00:30:04,080
I'm not sure whether it applies to to

686
00:30:04,080 --> 00:30:06,120
this particular protocol okay that's an

687
00:30:06,120 --> 00:30:09,678
interesting question okay thanks

688
00:30:18,500 --> 00:30:21,559
thank you

689
00:30:40,919 --> 00:30:43,559
all right so uh Jaden's talk was a

690
00:30:43,559 --> 00:30:45,120
perfect segue to coding stocks so code

691
00:30:45,120 --> 00:30:46,380
is going to talk about parallel

692
00:30:46,380 --> 00:30:48,600
delegation from lwa and this will work

693
00:30:48,600 --> 00:30:51,779
with Raphael and now and uh Naomi

694
00:30:51,779 --> 00:30:52,980
thanks

695
00:30:52,980 --> 00:30:55,620
sorry yeah thank you very much uh yeah

696
00:30:55,620 --> 00:30:57,480
so hopefully got a good introduction

697
00:30:57,480 --> 00:30:59,460
from the previous talk um and we'll show

698
00:30:59,460 --> 00:31:01,679
a different uh way we show some what

699
00:31:01,679 --> 00:31:04,200
similar uh final result but yeah this is

700
00:31:04,200 --> 00:31:06,899
on paralyzable delegation from lwe with

701
00:31:06,899 --> 00:31:09,600
Rafael Naomi

702
00:31:09,600 --> 00:31:11,520
so the main task we're considering today

703
00:31:11,520 --> 00:31:13,559
is that of delegating gram computation

704
00:31:13,559 --> 00:31:15,600
this is the situation where you have

705
00:31:15,600 --> 00:31:17,880
some weak client verifier V think of

706
00:31:17,880 --> 00:31:20,039
like a smartphone who wants to compute

707
00:31:20,039 --> 00:31:22,559
some uh program M I think a ram program

708
00:31:22,559 --> 00:31:25,140
on some input X but it's not able to

709
00:31:25,140 --> 00:31:26,640
compute it locally so instead it

710
00:31:26,640 --> 00:31:29,580
delegates this task to a large server uh

711
00:31:29,580 --> 00:31:32,220
who is the prover in this interaction

712
00:31:32,220 --> 00:31:34,020
the prover is going to compute the

713
00:31:34,020 --> 00:31:37,500
output y on and on of M of X send it

714
00:31:37,500 --> 00:31:39,480
over to the verifier as well as a proof

715
00:31:39,480 --> 00:31:41,520
uh attesting to the veracity of the

716
00:31:41,520 --> 00:31:43,020
statement

717
00:31:43,020 --> 00:31:44,820
so we require the standard Notions of

718
00:31:44,820 --> 00:31:47,220
completeness which says the the approver

719
00:31:47,220 --> 00:31:48,720
should be able to convince the verifier

720
00:31:48,720 --> 00:31:51,480
on a statements soundness which says no

721
00:31:51,480 --> 00:31:53,880
computationally bounded prover can

722
00:31:53,880 --> 00:31:56,220
convince the pro the verifier on false

723
00:31:56,220 --> 00:31:58,620
statements and in order for this

724
00:31:58,620 --> 00:32:00,000
interaction to be meaningful we need

725
00:32:00,000 --> 00:32:01,919
some notion of succinctness which says

726
00:32:01,919 --> 00:32:03,240
that the size of the proof and the

727
00:32:03,240 --> 00:32:04,919
running time of the verifier are

728
00:32:04,919 --> 00:32:08,100
sublinear in the time bound to to run

729
00:32:08,100 --> 00:32:09,840
the machine M on input X which we'll

730
00:32:09,840 --> 00:32:11,880
call T and in fact in this work we'll

731
00:32:11,880 --> 00:32:13,799
look at the the case where these are

732
00:32:13,799 --> 00:32:15,960
actually poly logarithmic in the time

733
00:32:15,960 --> 00:32:18,059
bound t

734
00:32:18,059 --> 00:32:19,740
um and throughout the talk I'll ignore

735
00:32:19,740 --> 00:32:21,539
polynomial terms in the security

736
00:32:21,539 --> 00:32:23,340
parameter but these are kind of hidden

737
00:32:23,340 --> 00:32:26,159
everywhere I say poly log T also there

738
00:32:26,159 --> 00:32:28,679
will be a CRS going around uh for all

739
00:32:28,679 --> 00:32:30,659
these non-interactive arguments

740
00:32:30,659 --> 00:32:32,399
um and as far as efficiency goal will

741
00:32:32,399 --> 00:32:34,080
also require the size of the crs's at

742
00:32:34,080 --> 00:32:36,419
most poly log T but I'll kind of ignore

743
00:32:36,419 --> 00:32:39,620
that first take a presentation

744
00:32:39,899 --> 00:32:41,580
so what does the proverb actually do in

745
00:32:41,580 --> 00:32:44,700
a typical delegation execution

746
00:32:44,700 --> 00:32:47,460
uh I'll represents this proverb strategy

747
00:32:47,460 --> 00:32:49,380
in terms of kind of wall clock time on

748
00:32:49,380 --> 00:32:51,960
the x-axis so first thing it does is

749
00:32:51,960 --> 00:32:53,640
generally it'll compute the machine M on

750
00:32:53,640 --> 00:32:56,039
input X which takes some time T by

751
00:32:56,039 --> 00:32:57,059
assumption

752
00:32:57,059 --> 00:32:59,820
and then it can take however long it

753
00:32:59,820 --> 00:33:02,220
needs to to generate a proof Pi for the

754
00:33:02,220 --> 00:33:04,740
computation and the best protocols we

755
00:33:04,740 --> 00:33:07,260
know have kind of this nice quasi-linear

756
00:33:07,260 --> 00:33:08,880
running time where the approver runs in

757
00:33:08,880 --> 00:33:11,399
total time T times poly log T and it

758
00:33:11,399 --> 00:33:13,500
gives some concrete numbers to to what's

759
00:33:13,500 --> 00:33:15,659
happening here because we're thinking of

760
00:33:15,659 --> 00:33:18,419
this uh this delegation problem for

761
00:33:18,419 --> 00:33:19,679
something the verifier can't compute

762
00:33:19,679 --> 00:33:22,200
itself maybe this T corresponds to like

763
00:33:22,200 --> 00:33:24,299
an hour long computation

764
00:33:24,299 --> 00:33:26,760
um and then T times poly log T even very

765
00:33:26,760 --> 00:33:29,460
optimistic uh maybe is like 100 hours

766
00:33:29,460 --> 00:33:31,740
and kind of what we're we're thinking

767
00:33:31,740 --> 00:33:33,120
about in this work is maybe there are

768
00:33:33,120 --> 00:33:35,760
situations where uh getting the results

769
00:33:35,760 --> 00:33:37,559
and having a proof is very time

770
00:33:37,559 --> 00:33:40,080
sensitive so paying this extra amount of

771
00:33:40,080 --> 00:33:41,460
time just doesn't make the delegation

772
00:33:41,460 --> 00:33:43,559
task worth it

773
00:33:43,559 --> 00:33:46,080
so in terms of what we know and how to

774
00:33:46,080 --> 00:33:48,899
build these delegation protocols from uh

775
00:33:48,899 --> 00:33:50,760
with this optimal efficiency where the

776
00:33:50,760 --> 00:33:52,740
approver runs in time T times polylog T

777
00:33:52,740 --> 00:33:54,960
first we know it from sort of these

778
00:33:54,960 --> 00:33:56,640
strong assumptions like in the random

779
00:33:56,640 --> 00:33:58,019
Oracle model or from extractability

780
00:33:58,019 --> 00:34:00,179
assumptions like snarks

781
00:34:00,179 --> 00:34:01,500
um but then there was this beautiful

782
00:34:01,500 --> 00:34:03,539
recent work by chowder Regina and Jin in

783
00:34:03,539 --> 00:34:06,539
2021 that show how to base this just on

784
00:34:06,539 --> 00:34:10,139
the polynomial hardness of lwb

785
00:34:10,139 --> 00:34:12,780
so in this setting we are considering

786
00:34:12,780 --> 00:34:16,080
the task of paralyzable Delegation or

787
00:34:16,080 --> 00:34:18,000
for short a succinct paralyzable

788
00:34:18,000 --> 00:34:20,879
argument or a sparg

789
00:34:20,879 --> 00:34:22,859
and the idea here is that we want to

790
00:34:22,859 --> 00:34:24,480
somehow be able to compute the proof in

791
00:34:24,480 --> 00:34:25,980
parallel to the computation so we have

792
00:34:25,980 --> 00:34:27,899
an hour long that it takes to do this

793
00:34:27,899 --> 00:34:30,179
computation can we somehow fit that

794
00:34:30,179 --> 00:34:32,520
large proof computation from before into

795
00:34:32,520 --> 00:34:35,040
this kind of hour time frame at the same

796
00:34:35,040 --> 00:34:37,260
time as the computation and will allow

797
00:34:37,260 --> 00:34:40,139
maybe some additive small overheads but

798
00:34:40,139 --> 00:34:41,699
nothing multiplicative in the time bound

799
00:34:41,699 --> 00:34:42,540
t

800
00:34:42,540 --> 00:34:44,399
and furthermore we want to do this while

801
00:34:44,399 --> 00:34:47,040
maintaining some reasonable parallel

802
00:34:47,040 --> 00:34:48,480
efficiency for the provers so we'll give

803
00:34:48,480 --> 00:34:50,280
it say a poly logarithmic number of

804
00:34:50,280 --> 00:34:51,839
processors

805
00:34:51,839 --> 00:34:54,060
and in fact we even extend this further

806
00:34:54,060 --> 00:34:56,099
to not just Ram programs but also to

807
00:34:56,099 --> 00:34:57,599
parallel programs where the underlying

808
00:34:57,599 --> 00:35:00,119
machine maybe uses P processors itself

809
00:35:00,119 --> 00:35:02,280
and we want to only have a blow up of

810
00:35:02,280 --> 00:35:04,320
polylog multiplicative in the number of

811
00:35:04,320 --> 00:35:05,760
processors that we allow the proof to

812
00:35:05,760 --> 00:35:07,140
use

813
00:35:07,140 --> 00:35:10,320
so this was originally considered by our

814
00:35:10,320 --> 00:35:12,960
previous Works uh looking at Sparks or

815
00:35:12,960 --> 00:35:14,280
assisting paralyzable arguments of

816
00:35:14,280 --> 00:35:16,740
knowledge and for this kind of stronger

817
00:35:16,740 --> 00:35:18,660
object

818
00:35:18,660 --> 00:35:20,099
um we were able to show this

819
00:35:20,099 --> 00:35:22,920
non-interactive delegation setting only

820
00:35:22,920 --> 00:35:24,960
from again this strong assumption on

821
00:35:24,960 --> 00:35:27,119
snarks

822
00:35:27,119 --> 00:35:29,220
and so our main result in this work is

823
00:35:29,220 --> 00:35:31,320
showing that when we restrict just to

824
00:35:31,320 --> 00:35:34,079
deterministic and just for uh arguments

825
00:35:34,079 --> 00:35:35,880
we show assuming only the polynomial

826
00:35:35,880 --> 00:35:37,859
hardness of lwe there exists these

827
00:35:37,859 --> 00:35:40,020
paralyzable delegation schemes for any

828
00:35:40,020 --> 00:35:43,140
pram computation

829
00:35:43,140 --> 00:35:45,180
and one of the main applications we see

830
00:35:45,180 --> 00:35:47,099
for this is to that of verifiable delay

831
00:35:47,099 --> 00:35:49,320
functions a notion proposed by bonadol

832
00:35:49,320 --> 00:35:50,820
in 2018

833
00:35:50,820 --> 00:35:52,980
and this is a very publicly verifiable

834
00:35:52,980 --> 00:35:55,079
function that cannot be sped up with

835
00:35:55,079 --> 00:35:57,300
many processors so it takes an hour to

836
00:35:57,300 --> 00:35:58,560
compute this function but with 100

837
00:35:58,560 --> 00:36:00,300
processors you can't compute it in say

838
00:36:00,300 --> 00:36:02,339
30 minutes or anything less

839
00:36:02,339 --> 00:36:04,980
and this has been useful in these kind

840
00:36:04,980 --> 00:36:06,359
of distributed blockchain applications

841
00:36:06,359 --> 00:36:08,520
to generate Randomness

842
00:36:08,520 --> 00:36:11,640
um but as far as uh TCC goes we're

843
00:36:11,640 --> 00:36:13,079
interested in kind of what are the core

844
00:36:13,079 --> 00:36:14,760
assumptions needed to build these

845
00:36:14,760 --> 00:36:15,839
objects

846
00:36:15,839 --> 00:36:16,920
um and for that reason I'm going to be

847
00:36:16,920 --> 00:36:18,540
focusing uh on more theoretical

848
00:36:18,540 --> 00:36:21,119
instructions in the plane model

849
00:36:21,119 --> 00:36:23,940
so first this original work uh observed

850
00:36:23,940 --> 00:36:26,520
that given a specific sequential

851
00:36:26,520 --> 00:36:29,579
function that's uh iterated in low space

852
00:36:29,579 --> 00:36:32,420
um you have any any function like that

853
00:36:32,420 --> 00:36:35,220
additionally uh efficient enough snogs

854
00:36:35,220 --> 00:36:37,619
for p you can use that to generically

855
00:36:37,619 --> 00:36:40,260
get a verifiable delay function

856
00:36:40,260 --> 00:36:43,140
uh our previous work on Sparks showed

857
00:36:43,140 --> 00:36:45,240
that from any sequential function so no

858
00:36:45,240 --> 00:36:47,579
longer a structured iterated one

859
00:36:47,579 --> 00:36:49,859
um thus again assuming the strong snark

860
00:36:49,859 --> 00:36:52,079
assumption for NP we showed how to build

861
00:36:52,079 --> 00:36:54,839
verifiability functions and then in the

862
00:36:54,839 --> 00:36:57,359
previous talk uh it didn't go into great

863
00:36:57,359 --> 00:37:00,000
detail on the vdf application but it

864
00:37:00,000 --> 00:37:02,040
follows from their work that just with a

865
00:37:02,040 --> 00:37:03,599
specific function of repeated squaring

866
00:37:03,599 --> 00:37:06,660
uh and uh the polynomial hardness of lwe

867
00:37:06,660 --> 00:37:08,220
they can construct verifiable delay

868
00:37:08,220 --> 00:37:10,560
functions

869
00:37:10,560 --> 00:37:12,900
and just to think of okay well what do

870
00:37:12,900 --> 00:37:15,060
spargs give us I want to kind of get in

871
00:37:15,060 --> 00:37:16,920
your heads a general Paradigm that says

872
00:37:16,920 --> 00:37:18,839
a spar gives you a way to take any

873
00:37:18,839 --> 00:37:21,119
function add a spark to it and it makes

874
00:37:21,119 --> 00:37:23,460
it a verifiable function and furthermore

875
00:37:23,460 --> 00:37:25,020
it does so in a way that preserves the

876
00:37:25,020 --> 00:37:26,940
parallel running time of this

877
00:37:26,940 --> 00:37:28,740
computation

878
00:37:28,740 --> 00:37:31,500
so in particular that means now assuming

879
00:37:31,500 --> 00:37:34,020
just the polynomial hardness of lwe and

880
00:37:34,020 --> 00:37:35,760
any sequential function we get a

881
00:37:35,760 --> 00:37:37,680
verifiable a function and I want to

882
00:37:37,680 --> 00:37:40,980
point out that this seems like uh lwe

883
00:37:40,980 --> 00:37:43,200
plus a very minimal assumption

884
00:37:43,200 --> 00:37:46,320
um because vdfs themselves imply a

885
00:37:46,320 --> 00:37:48,000
notion sequential functions whereas they

886
00:37:48,000 --> 00:37:49,320
don't imply something like an iterated

887
00:37:49,320 --> 00:37:50,760
sequential function or repeated squaring

888
00:37:50,760 --> 00:37:52,740
so in this sense this is some minimal

889
00:37:52,740 --> 00:37:55,140
sequential hardness assumption plus only

890
00:37:55,140 --> 00:37:57,060
uh learning with errors we know how to

891
00:37:57,060 --> 00:38:00,078
construct a vdf now

892
00:38:00,240 --> 00:38:02,040
um and additionally because this

893
00:38:02,040 --> 00:38:03,480
Paradigm is very generic in the

894
00:38:03,480 --> 00:38:05,339
underlying function if our sequential

895
00:38:05,339 --> 00:38:06,900
function satisfies something like memory

896
00:38:06,900 --> 00:38:08,220
hardness

897
00:38:08,220 --> 00:38:09,960
um then in this transformation we will

898
00:38:09,960 --> 00:38:11,820
also get a memory hard vdf at the end of

899
00:38:11,820 --> 00:38:13,140
the day

900
00:38:13,140 --> 00:38:15,119
um and the only previous construction we

901
00:38:15,119 --> 00:38:17,880
knew for this was based on uh the

902
00:38:17,880 --> 00:38:19,380
snark's work in our previous work from

903
00:38:19,380 --> 00:38:21,060
2020 and so this is the first

904
00:38:21,060 --> 00:38:22,440
construction we have without these sort

905
00:38:22,440 --> 00:38:25,520
of knowledge type assumptions

906
00:38:26,160 --> 00:38:28,320
so very quickly I want to give a detour

907
00:38:28,320 --> 00:38:30,599
and talk about an additional aspect of

908
00:38:30,599 --> 00:38:32,460
what we call a Time independent spark

909
00:38:32,460 --> 00:38:34,500
and the observation here is for normal

910
00:38:34,500 --> 00:38:35,940
arguments you can just run the

911
00:38:35,940 --> 00:38:37,500
computation you can see how long it

912
00:38:37,500 --> 00:38:40,440
takes and somehow use the time bounty to

913
00:38:40,440 --> 00:38:42,300
compute the proof

914
00:38:42,300 --> 00:38:45,119
but for spargs this isn't true uh maybe

915
00:38:45,119 --> 00:38:46,740
you're running the computation you don't

916
00:38:46,740 --> 00:38:48,180
know how long it's going to take in

917
00:38:48,180 --> 00:38:51,300
advance so it seems maybe complicated to

918
00:38:51,300 --> 00:38:53,940
how to know how do you kind of fit the

919
00:38:53,940 --> 00:38:55,619
computation of the proof into this time

920
00:38:55,619 --> 00:38:56,880
bound T that you don't even know in

921
00:38:56,880 --> 00:38:58,500
advance

922
00:38:58,500 --> 00:39:00,300
and in particular the previous work we

923
00:39:00,300 --> 00:39:02,839
had on Sparks showed that you actually

924
00:39:02,839 --> 00:39:05,820
uh we actually did rely on knowing T in

925
00:39:05,820 --> 00:39:08,700
advance and so in this work we ask if

926
00:39:08,700 --> 00:39:10,320
this is necessary and fortunately we

927
00:39:10,320 --> 00:39:12,359
show that the answer is no

928
00:39:12,359 --> 00:39:15,180
so in particular we show that given any

929
00:39:15,180 --> 00:39:17,820
sort of generic spark that depends on

930
00:39:17,820 --> 00:39:19,740
the time down T you can use this to

931
00:39:19,740 --> 00:39:21,720
construct construct a Time independent

932
00:39:21,720 --> 00:39:23,579
notion of a spark

933
00:39:23,579 --> 00:39:25,579
and the key idea is to kind of

934
00:39:25,579 --> 00:39:27,720
non-deterministically guess the binary

935
00:39:27,720 --> 00:39:30,240
representation of T and come up with

936
00:39:30,240 --> 00:39:32,460
different proofs for each uh kind of one

937
00:39:32,460 --> 00:39:34,440
in the representation and you piece them

938
00:39:34,440 --> 00:39:36,300
together and the efficiency guarantee of

939
00:39:36,300 --> 00:39:39,420
the spark kind of carryovers carries

940
00:39:39,420 --> 00:39:42,720
over to the time independent Spark

941
00:39:42,720 --> 00:39:45,000
but I'm going to move back and focus

942
00:39:45,000 --> 00:39:47,040
kind of on our main results uh building

943
00:39:47,040 --> 00:39:49,140
out the sparg

944
00:39:49,140 --> 00:39:51,480
um and just to start I want to give an

945
00:39:51,480 --> 00:39:53,820
outline of the approach of efk P20 that

946
00:39:53,820 --> 00:39:56,400
we had for Sparks

947
00:39:56,400 --> 00:39:59,099
so here they show that given any snark

948
00:39:59,099 --> 00:40:01,920
for NP uh via recursive composition you

949
00:40:01,920 --> 00:40:04,619
can uh bootstrap this into a

950
00:40:04,619 --> 00:40:06,599
quasi-linear snark this is a snark where

951
00:40:06,599 --> 00:40:09,000
the prover has uh T times polylog T

952
00:40:09,000 --> 00:40:10,619
overhead

953
00:40:10,619 --> 00:40:12,180
and then this work gives a generic

954
00:40:12,180 --> 00:40:13,920
transformation that compiles this into

955
00:40:13,920 --> 00:40:16,500
smart Sparks for NP with this argument

956
00:40:16,500 --> 00:40:18,240
of knowledge property

957
00:40:18,240 --> 00:40:20,640
and our work we start instead with a

958
00:40:20,640 --> 00:40:23,700
specific snark for P from lwe the the

959
00:40:23,700 --> 00:40:26,099
one of the work of chaturi Jin and Jin

960
00:40:26,099 --> 00:40:29,460
and we use this to build a new primitive

961
00:40:29,460 --> 00:40:30,960
that we call a quasi-linear updatable

962
00:40:30,960 --> 00:40:33,119
ram delegation scheme

963
00:40:33,119 --> 00:40:35,599
then if we plug this into this compiler

964
00:40:35,599 --> 00:40:38,280
essentially that that suffices to get

965
00:40:38,280 --> 00:40:39,960
just Sparks for p

966
00:40:39,960 --> 00:40:41,820
so now I'll go into what this updatable

967
00:40:41,820 --> 00:40:43,619
Ram delegation looks like

968
00:40:43,619 --> 00:40:45,540
so in this case we have some Ram

969
00:40:45,540 --> 00:40:49,200
configuration CF where uh trans we're

970
00:40:49,200 --> 00:40:51,480
moving T steps forward getting some

971
00:40:51,480 --> 00:40:53,579
final configuration CF Prime

972
00:40:53,579 --> 00:40:56,579
and the updatable aspect of it uh we

973
00:40:56,579 --> 00:40:58,380
have some additional auxiliary input on

974
00:40:58,380 --> 00:40:59,640
the side that we're going to update

975
00:40:59,640 --> 00:41:01,740
along the way

976
00:41:01,740 --> 00:41:03,480
and the point is that this final

977
00:41:03,480 --> 00:41:06,060
auxiliary information gives us a way to

978
00:41:06,060 --> 00:41:08,520
generate kind of a witness that can help

979
00:41:08,520 --> 00:41:11,700
approver prove this Ram transformation

980
00:41:11,700 --> 00:41:14,579
this Ram transition and this is a

981
00:41:14,579 --> 00:41:16,380
deterministic computation so really this

982
00:41:16,380 --> 00:41:17,940
notion of a witness here is just for

983
00:41:17,940 --> 00:41:20,820
efficiency but helps understand what's

984
00:41:20,820 --> 00:41:21,720
going on

985
00:41:21,720 --> 00:41:24,420
but the point is that we can kind of

986
00:41:24,420 --> 00:41:26,280
delegate this on the side while

987
00:41:26,280 --> 00:41:27,839
continuing the computation and updating

988
00:41:27,839 --> 00:41:29,700
this auxiliary information

989
00:41:29,700 --> 00:41:31,800
and the efficiency Notions we need is

990
00:41:31,800 --> 00:41:33,599
that essentially you can compute this

991
00:41:33,599 --> 00:41:36,480
auxiliary information uh on the side uh

992
00:41:36,480 --> 00:41:39,240
with very a little additive overhead

993
00:41:39,240 --> 00:41:41,520
and furthermore given this witness we

994
00:41:41,520 --> 00:41:44,760
can compute a proof Pi in time that only

995
00:41:44,760 --> 00:41:46,380
depends on the number of steps is

996
00:41:46,380 --> 00:41:47,760
quasi-linear in the number of steps of

997
00:41:47,760 --> 00:41:50,599
the computation in particular naively

998
00:41:50,599 --> 00:41:52,680
approver trying to prove this would

999
00:41:52,680 --> 00:41:54,599
depend on the size of the configuration

1000
00:41:54,599 --> 00:41:56,160
which grows with the space of the

1001
00:41:56,160 --> 00:41:58,079
computation

1002
00:41:58,079 --> 00:42:01,380
so very briefly I want to show how uh

1003
00:42:01,380 --> 00:42:03,900
the just key ideas of how we instantiate

1004
00:42:03,900 --> 00:42:07,020
this primitive from lwe first relying on

1005
00:42:07,020 --> 00:42:08,579
only Collision resistant hash functions

1006
00:42:08,579 --> 00:42:09,900
we use this concurrently updatable

1007
00:42:09,900 --> 00:42:12,839
Merkle tree from our work from 2020

1008
00:42:12,839 --> 00:42:15,540
this witness corresponds to update paths

1009
00:42:15,540 --> 00:42:16,980
for each of these steps of the

1010
00:42:16,980 --> 00:42:18,660
computation

1011
00:42:18,660 --> 00:42:21,960
and then this uh proof consists of just

1012
00:42:21,960 --> 00:42:23,760
a summer extractable commitment and the

1013
00:42:23,760 --> 00:42:25,980
specific batch arguments from the paper

1014
00:42:25,980 --> 00:42:27,480
of cjj

1015
00:42:27,480 --> 00:42:29,280
and all of these are known from either

1016
00:42:29,280 --> 00:42:31,140
Collision resistance or lwe so we can

1017
00:42:31,140 --> 00:42:33,300
base this on the polynomial hardness of

1018
00:42:33,300 --> 00:42:35,339
lwe

1019
00:42:35,339 --> 00:42:37,859
very briefly I want to just show what

1020
00:42:37,859 --> 00:42:40,619
this overall transformation looks like

1021
00:42:40,619 --> 00:42:43,980
um so again x-axis we have time we're

1022
00:42:43,980 --> 00:42:45,359
going to first start Computing some

1023
00:42:45,359 --> 00:42:47,760
amount of the computation we have our

1024
00:42:47,760 --> 00:42:50,040
auxiliary information on the side and

1025
00:42:50,040 --> 00:42:52,260
just to note at the very beginning this

1026
00:42:52,260 --> 00:42:54,240
just depends on depends on kind of the

1027
00:42:54,240 --> 00:42:55,740
initial configuration which is

1028
00:42:55,740 --> 00:42:58,020
relatively small so we can compute this

1029
00:42:58,020 --> 00:42:59,220
efficiently

1030
00:42:59,220 --> 00:43:01,380
but then from this augs one we can

1031
00:43:01,380 --> 00:43:03,540
compute this witness W1 that allows us

1032
00:43:03,540 --> 00:43:05,339
to prove this first part of the

1033
00:43:05,339 --> 00:43:07,619
computation

1034
00:43:07,619 --> 00:43:09,020
um and then we're going to keep

1035
00:43:09,020 --> 00:43:11,280
Computing more Computing this auxiliary

1036
00:43:11,280 --> 00:43:13,859
input and delegating these proofs for

1037
00:43:13,859 --> 00:43:16,980
every kind of piece of this computation

1038
00:43:16,980 --> 00:43:19,380
and the the point is that we set these

1039
00:43:19,380 --> 00:43:21,000
kind of pieces of the computation into

1040
00:43:21,000 --> 00:43:24,359
different sizes K1 K2 up to some km that

1041
00:43:24,359 --> 00:43:26,400
are of geometrically decreasing size in

1042
00:43:26,400 --> 00:43:28,859
a way such that the proof of the I

1043
00:43:28,859 --> 00:43:31,260
statement will finish in some time bound

1044
00:43:31,260 --> 00:43:33,420
T so this is why we need to know this

1045
00:43:33,420 --> 00:43:35,339
time bounty in advance for this initial

1046
00:43:35,339 --> 00:43:37,520
Construction

1047
00:43:37,619 --> 00:43:39,960
and then the point is that because we

1048
00:43:39,960 --> 00:43:41,640
have this quasi-linear efficiency for

1049
00:43:41,640 --> 00:43:43,859
this updatable Ram delegation uh the

1050
00:43:43,859 --> 00:43:46,200
number of proofs we need in in this kind

1051
00:43:46,200 --> 00:43:48,720
of geometrically decreasing fashion will

1052
00:43:48,720 --> 00:43:50,819
just be a poly logarithmic number based

1053
00:43:50,819 --> 00:43:53,099
on the time bound t

1054
00:43:53,099 --> 00:43:55,560
uh and so yeah that's a very high level

1055
00:43:55,560 --> 00:43:57,480
idea of how we use this generic

1056
00:43:57,480 --> 00:43:58,920
transformation

1057
00:43:58,920 --> 00:44:00,359
um and yeah with that I'll take any

1058
00:44:00,359 --> 00:44:02,819
questions and just uh here's a recap of

1059
00:44:02,819 --> 00:44:06,140
the main results we showed in this work

1060
00:44:11,400 --> 00:44:15,319
so my personal models thank you

1061
00:44:18,420 --> 00:44:19,500
thanks

1062
00:44:19,500 --> 00:44:21,180
um so

1063
00:44:21,180 --> 00:44:23,940
is there any chance that any of that

1064
00:44:23,940 --> 00:44:26,400
plus polylog T work in there could be

1065
00:44:26,400 --> 00:44:29,160
made independent of like the T steps

1066
00:44:29,160 --> 00:44:30,780
themselves like could you do it sort of

1067
00:44:30,780 --> 00:44:33,900
start before the computation maybe maybe

1068
00:44:33,900 --> 00:44:36,060
if you proved something over some like

1069
00:44:36,060 --> 00:44:39,180
you know Universal circuits like uh

1070
00:44:39,180 --> 00:44:40,619
layers and then fill in their

1071
00:44:40,619 --> 00:44:42,180
information afterwards like could you

1072
00:44:42,180 --> 00:44:43,740
sort of shift that Blue Block so that

1073
00:44:43,740 --> 00:44:46,740
it's like you know like time negative

1074
00:44:46,740 --> 00:44:50,000
poly log T to zero and then up to T

1075
00:44:50,000 --> 00:44:52,800
yeah so

1076
00:44:52,800 --> 00:44:53,400
um

1077
00:44:53,400 --> 00:44:56,940
it might be possible like it gets a

1078
00:44:56,940 --> 00:44:58,020
little messy because we're working with

1079
00:44:58,020 --> 00:44:59,819
like these Merkle trees and maybe you'll

1080
00:44:59,819 --> 00:45:02,220
have some large configuration uh because

1081
00:45:02,220 --> 00:45:04,260
naively you could maybe like cut this

1082
00:45:04,260 --> 00:45:05,339
off and just give the rest of the

1083
00:45:05,339 --> 00:45:07,440
verifier to check but because the

1084
00:45:07,440 --> 00:45:10,680
configuration grows large uh it turns

1085
00:45:10,680 --> 00:45:12,180
out to be a little challenging but it's

1086
00:45:12,180 --> 00:45:15,319
not completely rolled out or anything

1087
00:45:22,200 --> 00:45:24,500
foreign

1088
00:45:25,880 --> 00:45:28,440
if you could say anything about the

1089
00:45:28,440 --> 00:45:30,660
space efficiency of the strategy I was

1090
00:45:30,660 --> 00:45:32,160
particularly interested in the memory

1091
00:45:32,160 --> 00:45:35,760
hard uh vdf application so

1092
00:45:35,760 --> 00:45:37,260
um right when you're Computing the proof

1093
00:45:37,260 --> 00:45:39,359
in parallel how much uh extra space is

1094
00:45:39,359 --> 00:45:40,800
required

1095
00:45:40,800 --> 00:45:45,180
yeah so naively or

1096
00:45:45,180 --> 00:45:47,460
if you start with something uh this

1097
00:45:47,460 --> 00:45:49,740
quasi-linear updatable Ram delegation

1098
00:45:49,740 --> 00:45:52,619
thing we have if that somehow preserves

1099
00:45:52,619 --> 00:45:54,780
the space to the computation then the

1100
00:45:54,780 --> 00:45:55,920
resulting thing will be complexity

1101
00:45:55,920 --> 00:45:58,740
preserving but for example the one we're

1102
00:45:58,740 --> 00:46:01,680
using uh is not space preserving so we

1103
00:46:01,680 --> 00:46:04,140
will have space that depends on the time

1104
00:46:04,140 --> 00:46:08,160
Bounty uh but if you kind of improve one

1105
00:46:08,160 --> 00:46:10,319
of the underlying building blocks that

1106
00:46:10,319 --> 00:46:12,359
we'll carry over here

1107
00:46:12,359 --> 00:46:14,940
okay great so let's move on to the next

1108
00:46:14,940 --> 00:46:17,599
talk and thanks

1109
00:46:43,800 --> 00:46:45,359
all right so the last book of the

1110
00:46:45,359 --> 00:46:47,339
section is on sampling discrete

1111
00:46:47,339 --> 00:46:50,040
gaussians and more from a random article

1112
00:46:50,040 --> 00:46:53,099
and the workers by George and Brent and

1113
00:46:53,099 --> 00:46:55,740
George is going to give the talk

1114
00:46:55,740 --> 00:46:58,220
foreign

1115
00:46:58,220 --> 00:47:01,440
Okay cool so

1116
00:47:01,440 --> 00:47:03,780
yeah so I this talk is going to be about

1117
00:47:03,780 --> 00:47:05,760
how to sample a discrete gaussian more

1118
00:47:05,760 --> 00:47:07,560
from a random Oracle

1119
00:47:07,560 --> 00:47:09,540
so

1120
00:47:09,540 --> 00:47:11,819
red you all know the random Oracle model

1121
00:47:11,819 --> 00:47:14,760
it's been very uh helpful for I guess

1122
00:47:14,760 --> 00:47:16,920
both realizing like feasibility and

1123
00:47:16,920 --> 00:47:19,040
improving efficiency for a lot of like

1124
00:47:19,040 --> 00:47:22,020
cryptographic Primitives uh but a lot of

1125
00:47:22,020 --> 00:47:23,400
times when we think about the random

1126
00:47:23,400 --> 00:47:24,780
Oracle model

1127
00:47:24,780 --> 00:47:27,540
both in I guess in theory and sort of

1128
00:47:27,540 --> 00:47:28,859
when we sort of heuristically

1129
00:47:28,859 --> 00:47:31,079
instantiate it with hack functions we

1130
00:47:31,079 --> 00:47:32,579
usually think about it as outputting a

1131
00:47:32,579 --> 00:47:34,740
uniformly random string

1132
00:47:34,740 --> 00:47:37,319
but a lot of the times to construct

1133
00:47:37,319 --> 00:47:39,300
these Primitives it's much more natural

1134
00:47:39,300 --> 00:47:42,180
to think of a random Oracle which gives

1135
00:47:42,180 --> 00:47:44,099
an output onto some structured

1136
00:47:44,099 --> 00:47:45,720
distribution

1137
00:47:45,720 --> 00:47:48,000
and one way you think how you'd actually

1138
00:47:48,000 --> 00:47:49,260
realize that is that you have your

1139
00:47:49,260 --> 00:47:51,300
random Oracle let's say in some

1140
00:47:51,300 --> 00:47:53,220
practical world this might be like sha

1141
00:47:53,220 --> 00:47:55,800
which gives you random bits and then you

1142
00:47:55,800 --> 00:47:57,060
just run it through some sampling

1143
00:47:57,060 --> 00:48:00,119
algorithm which gives you your structure

1144
00:48:00,119 --> 00:48:02,280
distribution

1145
00:48:02,280 --> 00:48:05,460
um but for random oracles the reason

1146
00:48:05,460 --> 00:48:08,099
it's uh a lot of the times to prove

1147
00:48:08,099 --> 00:48:10,200
security with them you want the property

1148
00:48:10,200 --> 00:48:12,780
of programmability which means that in

1149
00:48:12,780 --> 00:48:14,160
your reduction you have some challenge

1150
00:48:14,160 --> 00:48:16,280
information

1151
00:48:16,280 --> 00:48:19,500
which you use to tell your article the

1152
00:48:19,500 --> 00:48:21,420
value on some particular input

1153
00:48:21,420 --> 00:48:24,240
but if you if you break down what we

1154
00:48:24,240 --> 00:48:27,000
just set are like instantiation of a

1155
00:48:27,000 --> 00:48:28,319
random Oracle from a structured

1156
00:48:28,319 --> 00:48:30,060
distribution there's some disconnect

1157
00:48:30,060 --> 00:48:31,380
where

1158
00:48:31,380 --> 00:48:34,440
your random Oracle is programmed on some

1159
00:48:34,440 --> 00:48:37,200
uniformly random string but your

1160
00:48:37,200 --> 00:48:38,760
challenge information you still want to

1161
00:48:38,760 --> 00:48:42,140
embed into your structured distribution

1162
00:48:42,140 --> 00:48:43,920
so

1163
00:48:43,920 --> 00:48:46,500
there's been a lot of Prior work in this

1164
00:48:46,500 --> 00:48:49,319
area in particular for random oracles to

1165
00:48:49,319 --> 00:48:52,440
group uh to Hash the groups there's been

1166
00:48:52,440 --> 00:48:55,140
uh works on like RSA for domain hashing

1167
00:48:55,140 --> 00:48:57,240
and a hash to point functions on the

1168
00:48:57,240 --> 00:48:59,640
left of Curves which tell you when this

1169
00:48:59,640 --> 00:49:00,359
is

1170
00:49:00,359 --> 00:49:03,060
I guess like doable and then more

1171
00:49:03,060 --> 00:49:04,760
generally there's been work on

1172
00:49:04,760 --> 00:49:06,900
indifferentiability and Universal

1173
00:49:06,900 --> 00:49:08,819
Samplers which kind of give a more

1174
00:49:08,819 --> 00:49:10,040
General

1175
00:49:10,040 --> 00:49:13,579
framework for when you can replace these

1176
00:49:13,579 --> 00:49:16,980
Oracle these functionalities

1177
00:49:16,980 --> 00:49:18,900
uh where

1178
00:49:18,900 --> 00:49:21,300
we were in uh I guess specifically

1179
00:49:21,300 --> 00:49:23,060
motivated in the context of lattices

1180
00:49:23,060 --> 00:49:26,160
where the I guess more specifically like

1181
00:49:26,160 --> 00:49:28,440
Lata space Abe where there's

1182
00:49:28,440 --> 00:49:30,480
several schemes where it'd be very

1183
00:49:30,480 --> 00:49:33,540
natural to formulate them uh using a

1184
00:49:33,540 --> 00:49:35,460
random Oracle to a discrete gaussian

1185
00:49:35,460 --> 00:49:36,960
distribution

1186
00:49:36,960 --> 00:49:39,540
um but because this isn't something that

1187
00:49:39,540 --> 00:49:40,740
sort of

1188
00:49:40,740 --> 00:49:43,380
um immediately exists a lot of these

1189
00:49:43,380 --> 00:49:46,800
Works go through some workarounds uh for

1190
00:49:46,800 --> 00:49:47,760
example

1191
00:49:47,760 --> 00:49:50,819
it could uh some this multi-authority a

1192
00:49:50,819 --> 00:49:53,819
b Paper uh just increases lwe parameters

1193
00:49:53,819 --> 00:49:55,920
and uses some like sub-exponential

1194
00:49:55,920 --> 00:49:57,900
smudging

1195
00:49:57,900 --> 00:50:00,960
and I guess the goal of this is to have

1196
00:50:00,960 --> 00:50:02,300
a more

1197
00:50:02,300 --> 00:50:06,300
systematic approach to this problem so

1198
00:50:06,300 --> 00:50:08,460
we defined this uh

1199
00:50:08,460 --> 00:50:11,579
redefine explainability which is um

1200
00:50:11,579 --> 00:50:13,020
when you have a sample you have a

1201
00:50:13,020 --> 00:50:14,700
sampling algorithm we also require

1202
00:50:14,700 --> 00:50:17,700
there's a explain algorithm which goes

1203
00:50:17,700 --> 00:50:19,619
from your structured distribution back

1204
00:50:19,619 --> 00:50:21,839
to uniform Randomness and we'll tie

1205
00:50:21,839 --> 00:50:24,000
these together is that if you explain

1206
00:50:24,000 --> 00:50:26,220
some element on your structure

1207
00:50:26,220 --> 00:50:27,900
distribution it should produce

1208
00:50:27,900 --> 00:50:29,640
Randomness that would have when run

1209
00:50:29,640 --> 00:50:31,740
through sample give you back the same

1210
00:50:31,740 --> 00:50:32,819
element

1211
00:50:32,819 --> 00:50:35,819
so how this would go into proof is that

1212
00:50:35,819 --> 00:50:39,359
in your real world your article would

1213
00:50:39,359 --> 00:50:41,460
output uniform Randomness and you would

1214
00:50:41,460 --> 00:50:43,859
run your sampler to get your structured

1215
00:50:43,859 --> 00:50:44,640
element

1216
00:50:44,640 --> 00:50:46,319
but then

1217
00:50:46,319 --> 00:50:48,420
in the proof you could move to an ideal

1218
00:50:48,420 --> 00:50:51,440
scheme where your Oracle actually

1219
00:50:51,440 --> 00:50:53,819
directly goes to this

1220
00:50:53,819 --> 00:50:56,400
structure distribution and then you use

1221
00:50:56,400 --> 00:50:58,200
the explain algorithm to get back the

1222
00:50:58,200 --> 00:51:00,240
you the uniform Randomness that would

1223
00:51:00,240 --> 00:51:02,280
have been produced by the original

1224
00:51:02,280 --> 00:51:05,400
random Oracle and then after you after

1225
00:51:05,400 --> 00:51:07,140
you do this you can produce proceed with

1226
00:51:07,140 --> 00:51:09,720
the rest of your proof as if you had a

1227
00:51:09,720 --> 00:51:11,339
random Oracle to the structure

1228
00:51:11,339 --> 00:51:13,260
distribution

1229
00:51:13,260 --> 00:51:15,960
so more concretely we have we're just

1230
00:51:15,960 --> 00:51:17,940
considering two distributions one of

1231
00:51:17,940 --> 00:51:19,800
which is just you sample your uniform

1232
00:51:19,800 --> 00:51:21,000
Randomness and run it through your

1233
00:51:21,000 --> 00:51:23,640
sampler and then the other you run your

1234
00:51:23,640 --> 00:51:25,500
sampler on some Randomness and then you

1235
00:51:25,500 --> 00:51:28,079
produce an alternative Randomness using

1236
00:51:28,079 --> 00:51:29,760
this explain algorithm

1237
00:51:29,760 --> 00:51:31,920
and the distribution of R and X here

1238
00:51:31,920 --> 00:51:34,740
should have some small statistical

1239
00:51:34,740 --> 00:51:37,020
distance uh

1240
00:51:37,020 --> 00:51:38,520
one thing I'd like to point out is that

1241
00:51:38,520 --> 00:51:40,680
we have this Precision parameter Capital

1242
00:51:40,680 --> 00:51:44,400
here and uh unlike some of the previous

1243
00:51:44,400 --> 00:51:46,980
Works which if you are aware of require

1244
00:51:46,980 --> 00:51:49,079
like computational or like statistically

1245
00:51:49,079 --> 00:51:52,380
negligible distance we only require

1246
00:51:52,380 --> 00:51:54,660
statistical distance which is polynomial

1247
00:51:54,660 --> 00:51:57,180
and that's because this explain

1248
00:51:57,180 --> 00:51:59,760
algorithm only exists in the proof of

1249
00:51:59,760 --> 00:52:01,619
your reduction so if you had some

1250
00:52:01,619 --> 00:52:04,140
adversary which broke uh broke some

1251
00:52:04,140 --> 00:52:05,940
assumption with probability let's say

1252
00:52:05,940 --> 00:52:09,480
like 1 over 1 over Lambda squared

1253
00:52:09,480 --> 00:52:11,640
then you could just set your Capital to

1254
00:52:11,640 --> 00:52:13,800
be something like 1 over 2 Lambda

1255
00:52:13,800 --> 00:52:16,680
squared and once you proceed

1256
00:52:16,680 --> 00:52:19,140
once you go through this step of moving

1257
00:52:19,140 --> 00:52:21,480
from the real to the ideal World

1258
00:52:21,480 --> 00:52:23,880
um your address I can still

1259
00:52:23,880 --> 00:52:25,500
break whatever assumption with

1260
00:52:25,500 --> 00:52:29,099
non-negligible probability afterwards

1261
00:52:29,099 --> 00:52:32,700
so the first result we get is uh showing

1262
00:52:32,700 --> 00:52:35,280
that the potential Walter 2017 discrete

1263
00:52:35,280 --> 00:52:38,040
gaussian sampler is explainable

1264
00:52:38,040 --> 00:52:40,440
so those

1265
00:52:40,440 --> 00:52:43,079
so actually in one of the prior works of

1266
00:52:43,079 --> 00:52:45,780
aw the broadcast encryption work they

1267
00:52:45,780 --> 00:52:47,940
show that the gpv sampler

1268
00:52:47,940 --> 00:52:49,920
they sort of sketch out the gpv Samplers

1269
00:52:49,920 --> 00:52:52,440
reverse sample ability

1270
00:52:52,440 --> 00:52:52,980
um

1271
00:52:52,980 --> 00:52:55,980
but I guess I'd like to say that

1272
00:52:55,980 --> 00:52:58,619
students uh following GPD there's been a

1273
00:52:58,619 --> 00:53:00,480
lot of works on just like how to sample

1274
00:53:00,480 --> 00:53:02,520
efficiently from discrete gaussians and

1275
00:53:02,520 --> 00:53:04,319
if we wanted to take advantage of those

1276
00:53:04,319 --> 00:53:07,980
in these sort of lattice based uh random

1277
00:53:07,980 --> 00:53:09,660
Oracle schemes we would need to show

1278
00:53:09,660 --> 00:53:12,059
that those Samplers too would be uh

1279
00:53:12,059 --> 00:53:13,800
samplable

1280
00:53:13,800 --> 00:53:16,859
uh so at a very high level with the

1281
00:53:16,859 --> 00:53:20,099
machancho Walter sampler does is it it's

1282
00:53:20,099 --> 00:53:22,260
split into two phases the first of which

1283
00:53:22,260 --> 00:53:24,480
you generate a bunch of discrete

1284
00:53:24,480 --> 00:53:27,180
gaussian samples with small standard

1285
00:53:27,180 --> 00:53:30,140
deviation and then you combine them

1286
00:53:30,140 --> 00:53:32,339
and then you combine them to get your

1287
00:53:32,339 --> 00:53:34,800
discrete calcium sample on some set of

1288
00:53:34,800 --> 00:53:38,339
parameters that you later find out for

1289
00:53:38,339 --> 00:53:40,440
the sake of this talk we'll focus most

1290
00:53:40,440 --> 00:53:42,420
we'll we'll just cover the first part of

1291
00:53:42,420 --> 00:53:44,339
this and in fact

1292
00:53:44,339 --> 00:53:45,720
I'll tackle a slightly more General

1293
00:53:45,720 --> 00:53:48,839
problem which is just explaining

1294
00:53:48,839 --> 00:53:51,359
distributions with polynomial range so

1295
00:53:51,359 --> 00:53:52,619
it turns out this is actually pretty

1296
00:53:52,619 --> 00:53:54,660
easy because if a distribution is

1297
00:53:54,660 --> 00:53:57,420
polynomial range you can actually just

1298
00:53:57,420 --> 00:53:59,400
try a bunch of Randomness in your sample

1299
00:53:59,400 --> 00:54:01,800
and eventually and just eventually hope

1300
00:54:01,800 --> 00:54:03,960
that you'll hit the target element

1301
00:54:03,960 --> 00:54:05,520
that should be pretty clear that since

1302
00:54:05,520 --> 00:54:07,559
you're sampling this randomness

1303
00:54:07,559 --> 00:54:08,819
uniformly

1304
00:54:08,819 --> 00:54:10,859
it's going to produce the same

1305
00:54:10,859 --> 00:54:14,160
distribution of R if you do find it

1306
00:54:14,160 --> 00:54:17,640
so the the only complication here is how

1307
00:54:17,640 --> 00:54:19,200
how many times you actually try to do

1308
00:54:19,200 --> 00:54:21,839
this and we can show

1309
00:54:21,839 --> 00:54:24,540
I can show that if you just samples your

1310
00:54:24,540 --> 00:54:26,400
Precision parameter times the size of

1311
00:54:26,400 --> 00:54:29,400
the range many trials then this will

1312
00:54:29,400 --> 00:54:31,380
only fail with one over Kappa

1313
00:54:31,380 --> 00:54:34,500
probability which is good enough for our

1314
00:54:34,500 --> 00:54:37,280
explained definition

1315
00:54:37,559 --> 00:54:39,540
and then

1316
00:54:39,540 --> 00:54:42,420
our second result is a more General

1317
00:54:42,420 --> 00:54:44,280
framework which we can use to explain

1318
00:54:44,280 --> 00:54:47,520
not only uh discrete gaussians

1319
00:54:47,520 --> 00:54:51,059
of any standard uh deviation but also

1320
00:54:51,059 --> 00:54:53,760
just a bunch of other distributions

1321
00:54:53,760 --> 00:54:56,819
and our approach here is we're going to

1322
00:54:56,819 --> 00:54:59,760
build a explainable sampler from a

1323
00:54:59,760 --> 00:55:01,680
non-explainable one with a couple other

1324
00:55:01,680 --> 00:55:03,599
ingredients

1325
00:55:03,599 --> 00:55:06,839
so first of all we need a sampling

1326
00:55:06,839 --> 00:55:09,300
algorithm which doesn't necessarily have

1327
00:55:09,300 --> 00:55:12,059
to be explainable and just for the sake

1328
00:55:12,059 --> 00:55:14,520
of a concrete example I'll show what

1329
00:55:14,520 --> 00:55:16,619
this would be in in the case of discrete

1330
00:55:16,619 --> 00:55:19,079
gaussians we also need to be able to

1331
00:55:19,079 --> 00:55:21,540
compute the probability density

1332
00:55:21,540 --> 00:55:23,819
which if you're aware for a discrete or

1333
00:55:23,819 --> 00:55:26,400
continuous gaussians this is

1334
00:55:26,400 --> 00:55:27,839
this is a

1335
00:55:27,839 --> 00:55:30,119
this is a probability density usually

1336
00:55:30,119 --> 00:55:32,579
there's a normalizing uh term in front

1337
00:55:32,579 --> 00:55:35,160
of this but we don't need it because

1338
00:55:35,160 --> 00:55:37,619
um we just required this function to

1339
00:55:37,619 --> 00:55:38,819
compute something proportional to the

1340
00:55:38,819 --> 00:55:40,440
probability density

1341
00:55:40,440 --> 00:55:42,960
and finally we need this thing we call a

1342
00:55:42,960 --> 00:55:45,960
heavy element sampler which is which

1343
00:55:45,960 --> 00:55:47,520
and sort of based on the probability

1344
00:55:47,520 --> 00:55:49,859
density function except it takes in a

1345
00:55:49,859 --> 00:55:51,720
threshold of probability density and

1346
00:55:51,720 --> 00:55:53,880
samples uniformly on any element above

1347
00:55:53,880 --> 00:55:56,579
that probability density so in the case

1348
00:55:56,579 --> 00:55:58,680
of discrete gaussians you know you just

1349
00:55:58,680 --> 00:56:01,200
take this uh you take this algebra

1350
00:56:01,200 --> 00:56:02,760
function and you essentially solve set

1351
00:56:02,760 --> 00:56:05,520
it equal to p and solve for it so for a

1352
00:56:05,520 --> 00:56:07,140
lot of I guess mathematical

1353
00:56:07,140 --> 00:56:08,520
distributions this is actually pretty

1354
00:56:08,520 --> 00:56:12,000
straightforward to compute

1355
00:56:12,000 --> 00:56:14,579
and so what our sampler is going to do

1356
00:56:14,579 --> 00:56:17,040
is actually it's going to take a sample

1357
00:56:17,040 --> 00:56:18,780
from the

1358
00:56:18,780 --> 00:56:21,300
using the non-explainable sampler

1359
00:56:21,300 --> 00:56:24,300
compute the PDF of the function

1360
00:56:24,300 --> 00:56:25,740
and then

1361
00:56:25,740 --> 00:56:27,480
it's going to sample a random point

1362
00:56:27,480 --> 00:56:31,319
uniformly from zero to the PDF

1363
00:56:31,319 --> 00:56:35,099
and use this as a threshold to compute

1364
00:56:35,099 --> 00:56:37,380
to sample another element in the domain

1365
00:56:37,380 --> 00:56:40,260
using this heavy element sampler

1366
00:56:40,260 --> 00:56:42,660
and this is the element that our sampler

1367
00:56:42,660 --> 00:56:44,520
is going to Output

1368
00:56:44,520 --> 00:56:47,700
so to explain this we kind of just work

1369
00:56:47,700 --> 00:56:49,319
our way backwards you know we start out

1370
00:56:49,319 --> 00:56:52,200
with this output element

1371
00:56:52,200 --> 00:56:56,040
uh we again we compute the PDF and we

1372
00:56:56,040 --> 00:56:58,500
sample a point uniformly on this as the

1373
00:56:58,500 --> 00:57:01,140
threshold

1374
00:57:01,140 --> 00:57:04,500
and remember that our uh that our heavy

1375
00:57:04,500 --> 00:57:06,180
element sampler is in fact explainable

1376
00:57:06,180 --> 00:57:08,940
so we can we can

1377
00:57:08,940 --> 00:57:11,300
output the randomness from that directly

1378
00:57:11,300 --> 00:57:15,300
using the uniform explain algorithm and

1379
00:57:15,300 --> 00:57:16,020
then

1380
00:57:16,020 --> 00:57:19,200
finally you know in order to get back to

1381
00:57:19,200 --> 00:57:21,900
the beginning we'd want a we'd want this

1382
00:57:21,900 --> 00:57:23,760
like red line which was originally

1383
00:57:23,760 --> 00:57:26,460
produced by the old sampler right

1384
00:57:26,460 --> 00:57:28,920
um unfortunately this isn't exactly

1385
00:57:28,920 --> 00:57:30,780
doable right if you don't know too much

1386
00:57:30,780 --> 00:57:32,579
about your distribution and if your

1387
00:57:32,579 --> 00:57:34,500
distribution has more than polynomial

1388
00:57:34,500 --> 00:57:37,020
range you can't exactly hope to Brute

1389
00:57:37,020 --> 00:57:39,540
Force something like this so

1390
00:57:39,540 --> 00:57:42,420
instead instead of aiming for exactly

1391
00:57:42,420 --> 00:57:45,119
this element we

1392
00:57:45,119 --> 00:57:47,700
we sort of bucket DL we divide all the

1393
00:57:47,700 --> 00:57:49,740
elements into buckets where the size of

1394
00:57:49,740 --> 00:57:51,300
these buckets are determined by the

1395
00:57:51,300 --> 00:57:54,540
Precision parameter and instead we're

1396
00:57:54,540 --> 00:57:56,720
just going to aim for NL any element

1397
00:57:56,720 --> 00:57:59,579
within that bucket

1398
00:57:59,579 --> 00:58:02,760
so after that this proceeds pretty

1399
00:58:02,760 --> 00:58:04,859
similarly to the

1400
00:58:04,859 --> 00:58:06,900
to the polynomial range sampling where

1401
00:58:06,900 --> 00:58:08,460
you just Brute Force

1402
00:58:08,460 --> 00:58:11,040
until you find something within that

1403
00:58:11,040 --> 00:58:13,319
bucket and then you output that as the

1404
00:58:13,319 --> 00:58:15,720
randomness of your explain algorithm

1405
00:58:15,720 --> 00:58:18,000
and yeah that's it

1406
00:58:18,000 --> 00:58:20,480
thank you

1407
00:58:23,339 --> 00:58:24,900
thanks George

1408
00:58:24,900 --> 00:58:27,619
so questions

1409
00:58:34,380 --> 00:58:37,079
yeah some of the results one is that you

1410
00:58:37,079 --> 00:58:38,880
you combine these small gaussians to

1411
00:58:38,880 --> 00:58:41,819
produce to the bigger one right this is

1412
00:58:41,819 --> 00:58:44,579
combinations how is it made like by

1413
00:58:44,579 --> 00:58:46,859
mixtures of calciums or

1414
00:58:46,859 --> 00:58:50,180
oh sorry for the first result yeah

1415
00:58:50,180 --> 00:58:52,380
uh I mean it's pretty much another

1416
00:58:52,380 --> 00:58:54,240
something similar to where you like work

1417
00:58:54,240 --> 00:58:56,400
backwards uh you have your like large

1418
00:58:56,400 --> 00:58:58,740
discrete gaussian sample right and then

1419
00:58:58,740 --> 00:59:00,599
if you have like a linear combination

1420
00:59:00,599 --> 00:59:02,040
you just need to reason about the

1421
00:59:02,040 --> 00:59:04,559
distribution of like I guess there's

1422
00:59:04,559 --> 00:59:05,520
some

1423
00:59:05,520 --> 00:59:07,500
uh and then it turns out that the the

1424
00:59:07,500 --> 00:59:09,359
like marginal Distribution on each of

1425
00:59:09,359 --> 00:59:10,859
these is still a dispute gaussian so you

1426
00:59:10,859 --> 00:59:12,480
just need to compute the

1427
00:59:12,480 --> 00:59:15,420
the appropriate standard deviation and

1428
00:59:15,420 --> 00:59:18,020
okay

1429
00:59:18,900 --> 00:59:21,000
sorry they're independent right yeah

1430
00:59:21,000 --> 00:59:22,619
when you sample them oh yeah when you

1431
00:59:22,619 --> 00:59:23,579
sample them initially they're

1432
00:59:23,579 --> 00:59:25,799
independent okay thank you

1433
00:59:25,799 --> 00:59:28,940
more questions

1434
00:59:28,980 --> 00:59:30,720
so maybe a quick question so it's

1435
00:59:30,720 --> 00:59:32,760
interesting so we explain algorithm the

1436
00:59:32,760 --> 00:59:34,500
parameter copper that's just basically

1437
00:59:34,500 --> 00:59:37,920
that you can specify uh on the fly it

1438
00:59:37,920 --> 00:59:39,540
doesn't have to be selected during maybe

1439
00:59:39,540 --> 00:59:41,520
even sort of just a sample the

1440
00:59:41,520 --> 00:59:42,960
parameters for the discrete origin

1441
00:59:42,960 --> 00:59:45,119
sample itself that's basically unbounded

1442
00:59:45,119 --> 00:59:47,099
technically that can be it doesn't have

1443
00:59:47,099 --> 00:59:49,559
to relate to the security parameter at

1444
00:59:49,559 --> 00:59:51,720
all of the scheme right so it's it's

1445
00:59:51,720 --> 00:59:53,880
pretty much used just in the proof

1446
00:59:53,880 --> 00:59:55,380
um where like when you actually run

1447
00:59:55,380 --> 00:59:57,299
these when you actually have a scheme it

1448
00:59:57,299 --> 00:59:59,700
would only use a sampler but in like you

1449
00:59:59,700 --> 01:00:01,200
said like if you assume you have an

1450
01:00:01,200 --> 01:00:03,420
adversary which breaks some

1451
01:00:03,420 --> 01:00:04,980
break with some like noticeable

1452
01:00:04,980 --> 01:00:06,540
probability you set your Kappa

1453
01:00:06,540 --> 01:00:09,540
proportion uh based on what that success

1454
01:00:09,540 --> 01:00:10,920
probability is external proves whether

1455
01:00:10,920 --> 01:00:11,940
you're going to use it it's going to be

1456
01:00:11,940 --> 01:00:13,200
a non-uniform step where you're going to

1457
01:00:13,200 --> 01:00:15,240
assume that this is the advantage of the

1458
01:00:15,240 --> 01:00:16,319
attacker and based on that you're going

1459
01:00:16,319 --> 01:00:18,599
to sort of just use that to choose the

1460
01:00:18,599 --> 01:00:20,940
copper value for that typically okay

1461
01:00:20,940 --> 01:00:22,980
great more questions

1462
01:00:22,980 --> 01:00:24,299
all right

1463
01:00:24,299 --> 01:00:26,760
so I guess we just along the time for on

1464
01:00:26,760 --> 01:00:29,160
time actually for the coffee break so

1465
01:00:29,160 --> 01:00:32,660
yeah yeah thank you all and thanks

