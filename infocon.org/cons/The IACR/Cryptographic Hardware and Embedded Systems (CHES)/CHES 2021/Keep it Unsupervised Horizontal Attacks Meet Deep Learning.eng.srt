1
00:00:00,560 --> 00:00:02,159
welcome everybody

2
00:00:02,159 --> 00:00:04,080
here i will present the foundations and

3
00:00:04,080 --> 00:00:05,759
results from our paper keep it

4
00:00:05,759 --> 00:00:07,919
unsupervised horizontal attacks meet

5
00:00:07,919 --> 00:00:10,400
deep learning the paper was published on

6
00:00:10,400 --> 00:00:13,679
chess 2021 my name is guilherme petting

7
00:00:13,679 --> 00:00:15,679
and this research paper was elaborated

8
00:00:15,679 --> 00:00:17,440
together with my colleague bukash and

9
00:00:17,440 --> 00:00:19,680
professors stephan pichek and leila

10
00:00:19,680 --> 00:00:22,080
batsina this is a work from teal delft

11
00:00:22,080 --> 00:00:25,799
and rod about university

12
00:00:30,080 --> 00:00:32,238
we propose an even supervised attack

13
00:00:32,238 --> 00:00:34,960
against public key implementations

14
00:00:34,960 --> 00:00:36,880
as we know horizontal attacks are

15
00:00:36,880 --> 00:00:39,120
applicable to real-world protected ecc

16
00:00:39,120 --> 00:00:41,520
and rsa implementations

17
00:00:41,520 --> 00:00:43,840
however due to practical limitations

18
00:00:43,840 --> 00:00:45,760
horizontal attacks tend to deliver a

19
00:00:45,760 --> 00:00:47,920
high amount of wrong beats resulting in

20
00:00:47,920 --> 00:00:48,719
a low

21
00:00:48,719 --> 00:00:51,280
success rate correcting these wrong bits

22
00:00:51,280 --> 00:00:53,600
is not very trivial mainly because we

23
00:00:53,600 --> 00:00:55,680
cannot know where these wrong bits are

24
00:00:55,680 --> 00:00:57,440
actually located

25
00:00:57,440 --> 00:00:59,680
to solve this problem we propose a deep

26
00:00:59,680 --> 00:01:01,199
learning based framework that

27
00:01:01,199 --> 00:01:03,520
iteratively correct wrong beats from key

28
00:01:03,520 --> 00:01:05,920
candidates obtained from horizontal

29
00:01:05,920 --> 00:01:08,159
attacks when a simple horizontal attack

30
00:01:08,159 --> 00:01:10,640
delivers only 52 percent of correct key

31
00:01:10,640 --> 00:01:13,520
bits from a private key our framework is

32
00:01:13,520 --> 00:01:15,040
able to correct these wrong bits and

33
00:01:15,040 --> 00:01:16,320
deliver 100

34
00:01:16,320 --> 00:01:19,039
of accuracy

35
00:01:21,280 --> 00:01:23,280
first let's start with some background

36
00:01:23,280 --> 00:01:27,320
information from our research

37
00:01:28,640 --> 00:01:30,240
when attacking protected ecc

38
00:01:30,240 --> 00:01:32,560
implementations an attacker can for

39
00:01:32,560 --> 00:01:35,280
instance query ecdsa commands and

40
00:01:35,280 --> 00:01:37,360
execute multiple signature generations

41
00:01:37,360 --> 00:01:39,439
in the target device

42
00:01:39,439 --> 00:01:41,280
the adversary can measure side channel

43
00:01:41,280 --> 00:01:43,200
traces for instance power consumption

44
00:01:43,200 --> 00:01:46,159
from ecdsa executions

45
00:01:46,159 --> 00:01:48,720
a main operation in an ecdsa execution

46
00:01:48,720 --> 00:01:50,960
is the scholar multiplication that uses

47
00:01:50,960 --> 00:01:52,640
the private key

48
00:01:52,640 --> 00:01:54,079
normally a protected scholar

49
00:01:54,079 --> 00:01:55,920
multiplication would implement scalar

50
00:01:55,920 --> 00:01:58,000
blight encounter measure making each

51
00:01:58,000 --> 00:02:00,000
scalar multiplication to be computed

52
00:02:00,000 --> 00:02:02,000
over a randomized key

53
00:02:02,000 --> 00:02:03,520
additional counter measures include

54
00:02:03,520 --> 00:02:05,920
pointer randomization for example scalar

55
00:02:05,920 --> 00:02:09,280
blinding factor are usually 32 or 64-bit

56
00:02:09,280 --> 00:02:12,000
random numbers

57
00:02:14,319 --> 00:02:16,000
let's consider a protected scalar

58
00:02:16,000 --> 00:02:18,160
multiplication besides this color blind

59
00:02:18,160 --> 00:02:20,239
encounter measure vulnerabilities can

60
00:02:20,239 --> 00:02:22,400
still be exploited if double enough is

61
00:02:22,400 --> 00:02:25,760
implemented with its naive algorithm

62
00:02:25,760 --> 00:02:27,440
in this situation we can easily

63
00:02:27,440 --> 00:02:29,360
distinguish the patterns of double and

64
00:02:29,360 --> 00:02:31,040
add operations inside a scalar

65
00:02:31,040 --> 00:02:32,959
multiplication interval

66
00:02:32,959 --> 00:02:34,800
from these patterns an adversary would

67
00:02:34,800 --> 00:02:36,879
be able to recover the private key bits

68
00:02:36,879 --> 00:02:39,200
from single trace by passing all other

69
00:02:39,200 --> 00:02:41,760
protections

70
00:02:44,720 --> 00:02:46,879
the solution is therefore to consider

71
00:02:46,879 --> 00:02:49,760
the regular double-end solutions such as

72
00:02:49,760 --> 00:02:51,599
 marie ladder and double adat

73
00:02:51,599 --> 00:02:53,280
always

74
00:02:53,280 --> 00:02:55,360
in this case as we can see in this trace

75
00:02:55,360 --> 00:02:57,440
illustration double and add operations

76
00:02:57,440 --> 00:02:59,200
are still distinguishable from each

77
00:02:59,200 --> 00:03:01,680
other however these two operations are

78
00:03:01,680 --> 00:03:03,920
always executed for every private key

79
00:03:03,920 --> 00:03:06,239
bit making the power consumption profile

80
00:03:06,239 --> 00:03:10,080
protected against sba attacks

81
00:03:13,280 --> 00:03:15,120
as a result a protected scholar

82
00:03:15,120 --> 00:03:17,360
multiplication should implement at least

83
00:03:17,360 --> 00:03:19,360
uniform double and add operations

84
00:03:19,360 --> 00:03:21,599
message or coordinate randomization and

85
00:03:21,599 --> 00:03:24,080
scholar gliding

86
00:03:24,080 --> 00:03:26,239
together this counter measures mitigate

87
00:03:26,239 --> 00:03:28,000
non-profiled attacks and profiling

88
00:03:28,000 --> 00:03:29,840
attacks that are based on multiple side

89
00:03:29,840 --> 00:03:32,640
channel traces an obvious solution is to

90
00:03:32,640 --> 00:03:34,879
attack single traces through horizontal

91
00:03:34,879 --> 00:03:37,280
attacks

92
00:03:42,400 --> 00:03:44,560
in practice a first step to implement a

93
00:03:44,560 --> 00:03:46,799
horizontal attack is to identify trace

94
00:03:46,799 --> 00:03:48,799
intervals that represent the processing

95
00:03:48,799 --> 00:03:51,040
of a single private key bit

96
00:03:51,040 --> 00:03:53,840
then using battery extraction mechanisms

97
00:03:53,840 --> 00:03:55,599
the trace is split into several

98
00:03:55,599 --> 00:03:57,920
sub-traces the amount of subtracts is

99
00:03:57,920 --> 00:03:59,760
equivalent to the amount of private key

100
00:03:59,760 --> 00:04:01,760
or secret scholar bits

101
00:04:01,760 --> 00:04:03,439
exceptions happen when a sub trace

102
00:04:03,439 --> 00:04:05,040
interval represents more than one

103
00:04:05,040 --> 00:04:06,959
scholar bit which happens in windows

104
00:04:06,959 --> 00:04:08,799
color multiplication that also include

105
00:04:08,799 --> 00:04:11,519
precomputations in our work each

106
00:04:11,519 --> 00:04:13,519
subtractor represents a single scalar

107
00:04:13,519 --> 00:04:15,840
bit

108
00:04:18,399 --> 00:04:20,639
main types of horizontal attacks include

109
00:04:20,639 --> 00:04:22,560
horizontal correlation analysis

110
00:04:22,560 --> 00:04:24,800
horizontal collision correlation or

111
00:04:24,800 --> 00:04:26,800
cross correlation attacks whole line

112
00:04:26,800 --> 00:04:29,520
template attacks and cluster attacks

113
00:04:29,520 --> 00:04:31,360
in this work we considered clustering

114
00:04:31,360 --> 00:04:33,120
attacks as this type of horizontal

115
00:04:33,120 --> 00:04:37,120
attack has less practical limitations

116
00:04:39,600 --> 00:04:41,520
to implement a cluster invasive process

117
00:04:41,520 --> 00:04:43,280
against a single scholar multiplication

118
00:04:43,280 --> 00:04:46,000
trace a k-means algorithm for example is

119
00:04:46,000 --> 00:04:48,080
applied to each sample column from the

120
00:04:48,080 --> 00:04:50,880
sub-trace interval when an adversary can

121
00:04:50,880 --> 00:04:53,040
identify points of interest inside the

122
00:04:53,040 --> 00:04:55,360
sub-trace interval it is possible to

123
00:04:55,360 --> 00:04:57,199
obtain a key candidate from each

124
00:04:57,199 --> 00:04:59,680
clustered point of interest

125
00:04:59,680 --> 00:05:01,919
at the end the adversary would combine

126
00:05:01,919 --> 00:05:03,680
all key candidates into a final key

127
00:05:03,680 --> 00:05:05,600
candidate this can be done through

128
00:05:05,600 --> 00:05:07,600
different statistical methods in our

129
00:05:07,600 --> 00:05:09,600
paper we follow the most simple approach

130
00:05:09,600 --> 00:05:12,000
by selecting 20 points of interest then

131
00:05:12,000 --> 00:05:13,919
we apply k-means to these points of

132
00:05:13,919 --> 00:05:16,080
interest and at the end we combine the

133
00:05:16,080 --> 00:05:20,320
key candidates with the majority rule

134
00:05:22,000 --> 00:05:24,080
these types of attacks encounter several

135
00:05:24,080 --> 00:05:26,639
limitations usually tracy splitting into

136
00:05:26,639 --> 00:05:28,560
subtracer requires the knowledge of the

137
00:05:28,560 --> 00:05:30,160
implemented scholar multiplication

138
00:05:30,160 --> 00:05:32,400
algorithm it is expected that an

139
00:05:32,400 --> 00:05:34,080
anniversary would consider target

140
00:05:34,080 --> 00:05:35,840
documentation which is good which could

141
00:05:35,840 --> 00:05:37,919
be available in the internet or even

142
00:05:37,919 --> 00:05:40,080
reverse engineering

143
00:05:40,080 --> 00:05:42,000
also to correct the wrong beats from a

144
00:05:42,000 --> 00:05:44,479
recovered key is not an easy task in

145
00:05:44,479 --> 00:05:46,240
principle as the attack is fully

146
00:05:46,240 --> 00:05:48,560
unsupervised the locations of the wrong

147
00:05:48,560 --> 00:05:50,560
beats are unknown to the adversary and

148
00:05:50,560 --> 00:05:52,479
the brute force alternative becomes

149
00:05:52,479 --> 00:05:55,120
infeasible

150
00:05:58,240 --> 00:05:59,520
as i mentioned before in this

151
00:05:59,520 --> 00:06:01,280
presentation this work proposed a

152
00:06:01,280 --> 00:06:03,520
solution to correct error or wrong bits

153
00:06:03,520 --> 00:06:05,039
recovered from the previous attack

154
00:06:05,039 --> 00:06:07,520
method such as unsupervised clustering

155
00:06:07,520 --> 00:06:09,919
attacks

156
00:06:11,120 --> 00:06:12,800
let me first start explaining the

157
00:06:12,800 --> 00:06:14,639
principle behind our proposed dp

158
00:06:14,639 --> 00:06:16,080
learning approach

159
00:06:16,080 --> 00:06:18,639
recent research papers demonstrated that

160
00:06:18,639 --> 00:06:20,479
deep neural networks can actually be

161
00:06:20,479 --> 00:06:22,240
robust against noisy labels from

162
00:06:22,240 --> 00:06:23,919
training sets

163
00:06:23,919 --> 00:06:26,160
in our case noisy labels would be wrong

164
00:06:26,160 --> 00:06:29,360
bits from the recovered scalar bit

165
00:06:29,360 --> 00:06:31,120
for the two classes classification

166
00:06:31,120 --> 00:06:32,960
problem if the amount of wrong bits is

167
00:06:32,960 --> 00:06:35,360
lower than 50 percent a regularized

168
00:06:35,360 --> 00:06:37,120
neural network should still be able to

169
00:06:37,120 --> 00:06:38,800
learn the main classes from the training

170
00:06:38,800 --> 00:06:39,919
set

171
00:06:39,919 --> 00:06:42,000
as a consequence predicting a separate

172
00:06:42,000 --> 00:06:44,000
validation set that should also contain

173
00:06:44,000 --> 00:06:46,400
noisy labels could result in a correct

174
00:06:46,400 --> 00:06:48,639
prediction the network in this case is

175
00:06:48,639 --> 00:06:51,120
not getting confused by the noisy labels

176
00:06:51,120 --> 00:06:52,720
and still making correct labels

177
00:06:52,720 --> 00:06:55,360
association

178
00:06:58,000 --> 00:07:00,160
now how you explain the execution flow

179
00:07:00,160 --> 00:07:03,039
of our proposed iterative framework from

180
00:07:03,039 --> 00:07:05,199
a set of sub-traces we first divide the

181
00:07:05,199 --> 00:07:07,680
set into two separate subsets that are

182
00:07:07,680 --> 00:07:09,680
initially labeled from an unsupervised

183
00:07:09,680 --> 00:07:11,520
clustering attack

184
00:07:11,520 --> 00:07:13,919
next we train a neural network with each

185
00:07:13,919 --> 00:07:16,080
of the subsets the third step

186
00:07:16,080 --> 00:07:18,240
corresponds to the relabeling of these

187
00:07:18,240 --> 00:07:20,720
two subsets in this case we swap the

188
00:07:20,720 --> 00:07:22,720
subsets and predict the neural network

189
00:07:22,720 --> 00:07:24,800
that was trained with one subset with

190
00:07:24,800 --> 00:07:26,479
another subset

191
00:07:26,479 --> 00:07:28,639
the output prediction should now contain

192
00:07:28,639 --> 00:07:30,720
a lower error rate in comparison to

193
00:07:30,720 --> 00:07:33,840
initial labels as a last step the two

194
00:07:33,840 --> 00:07:36,479
subsets are combined and shuffled after

195
00:07:36,479 --> 00:07:38,960
the process starts all over again

196
00:07:38,960 --> 00:07:40,960
in our experiment we always run the

197
00:07:40,960 --> 00:07:45,159
framework for 50 iterations

198
00:07:46,639 --> 00:07:48,479
in a first moment the framework is

199
00:07:48,479 --> 00:07:50,560
executed with neural networks having

200
00:07:50,560 --> 00:07:52,800
fixed hyper parameters across multiple

201
00:07:52,800 --> 00:07:55,039
framework iterations

202
00:07:55,039 --> 00:07:57,199
to deal with noisy labels we considered

203
00:07:57,199 --> 00:07:58,879
in compare the different types of

204
00:07:58,879 --> 00:08:01,520
regularization methods for comparison we

205
00:08:01,520 --> 00:08:03,280
start by running the framework without

206
00:08:03,280 --> 00:08:06,080
any explicit regularization

207
00:08:06,080 --> 00:08:08,720
then we consider dropout dropout plus

208
00:08:08,720 --> 00:08:10,879
data augmentation and data augmentation

209
00:08:10,879 --> 00:08:13,199
only date documentation is based on the

210
00:08:13,199 --> 00:08:17,280
random shifts applied to the subtraces

211
00:08:18,720 --> 00:08:20,639
in a second moment we randomize the

212
00:08:20,639 --> 00:08:22,560
hyper parameters and define a different

213
00:08:22,560 --> 00:08:24,479
neural network at each framework

214
00:08:24,479 --> 00:08:27,039
iteration in this case we also combined

215
00:08:27,039 --> 00:08:28,479
random hyper parameters with

216
00:08:28,479 --> 00:08:31,758
regularization methods

217
00:08:33,599 --> 00:08:35,279
the motivation for random hyper

218
00:08:35,279 --> 00:08:37,360
parameters is simple training the same

219
00:08:37,360 --> 00:08:39,200
neural network multiple times with

220
00:08:39,200 --> 00:08:41,279
similar labels will overfit the model

221
00:08:41,279 --> 00:08:43,279
very soon during the training process

222
00:08:43,279 --> 00:08:45,279
therefore different neural networks will

223
00:08:45,279 --> 00:08:48,839
reduce this overfitting

224
00:08:54,080 --> 00:08:56,160
for this work we consider two data sets

225
00:08:56,160 --> 00:08:58,399
that are software implementations of ecc

226
00:08:58,399 --> 00:09:01,040
on arm cortex m the implementation is

227
00:09:01,040 --> 00:09:03,839
based on curve 25 519 and uses

228
00:09:03,839 --> 00:09:05,600
montgomery ladder for this color

229
00:09:05,600 --> 00:09:06,880
multiplication

230
00:09:06,880 --> 00:09:08,959
as an extra protection the montgomery

231
00:09:08,959 --> 00:09:10,959
ladder implements conditional swap for

232
00:09:10,959 --> 00:09:13,200
constant time operations in the first

233
00:09:13,200 --> 00:09:15,040
data set the c-swap is based on

234
00:09:15,040 --> 00:09:17,200
arithmetic means and the second data set

235
00:09:17,200 --> 00:09:19,839
uses pointer and swapping for the rest

236
00:09:19,839 --> 00:09:21,519
of this presentation i will refer to

237
00:09:21,519 --> 00:09:23,839
these data sets as c swap are it and c

238
00:09:23,839 --> 00:09:25,440
swap pointer

239
00:09:25,440 --> 00:09:28,640
in total we measure 300 power traces for

240
00:09:28,640 --> 00:09:30,800
each of the implementations as each

241
00:09:30,800 --> 00:09:33,120
scalar multiplication contains 255

242
00:09:33,120 --> 00:09:35,360
operations with a random scalar we will

243
00:09:35,360 --> 00:09:38,080
have a total of 300 times 255 that is

244
00:09:38,080 --> 00:09:42,080
equal to 76 and 500 subtrees

245
00:09:42,080 --> 00:09:43,760
for each color multiplication

246
00:09:43,760 --> 00:09:47,760
measurements this color is randomized

247
00:09:49,760 --> 00:09:51,519
this is an illustration of how we

248
00:09:51,519 --> 00:09:53,839
pre-process the data sets in order to

249
00:09:53,839 --> 00:09:55,600
split the full scalar multiplication

250
00:09:55,600 --> 00:09:58,959
trace into a set of sub-traces

251
00:09:58,959 --> 00:10:00,800
the first step is to identify each of

252
00:10:00,800 --> 00:10:02,800
the ladder steps and cut the full trace

253
00:10:02,800 --> 00:10:04,800
accordingly in our case for each

254
00:10:04,800 --> 00:10:06,399
sub-trace we select the interval

255
00:10:06,399 --> 00:10:08,160
corresponding to address and control

256
00:10:08,160 --> 00:10:10,399
executions mainly because we explore the

257
00:10:10,399 --> 00:10:13,040
leakage present after field or ecc

258
00:10:13,040 --> 00:10:16,240
operations for from each ladder step

259
00:10:16,240 --> 00:10:18,240
the c-swap are arith's data set will

260
00:10:18,240 --> 00:10:21,200
contain 8 000 samples for each sub-trace

261
00:10:21,200 --> 00:10:23,519
and the c-swap pointer data set contains

262
00:10:23,519 --> 00:10:27,360
1 000 sample per sub-trace

263
00:10:31,200 --> 00:10:33,279
we also performed a leakage assessment

264
00:10:33,279 --> 00:10:35,279
on the data sets in order to understand

265
00:10:35,279 --> 00:10:37,519
the leakage present in our measurements

266
00:10:37,519 --> 00:10:39,600
for that we use a t-test and signal to

267
00:10:39,600 --> 00:10:40,880
noise radio

268
00:10:40,880 --> 00:10:42,240
using the true labels and

269
00:10:42,240 --> 00:10:44,480
signal-to-noise radio we can clearly

270
00:10:44,480 --> 00:10:46,880
identify the leakage locations inside

271
00:10:46,880 --> 00:10:49,040
the sub-trace intervals for both data

272
00:10:49,040 --> 00:10:50,160
sets

273
00:10:50,160 --> 00:10:51,920
when we consider the labels obtained

274
00:10:51,920 --> 00:10:54,320
from the unsupervised clustering attack

275
00:10:54,320 --> 00:10:57,040
which has an accuracy of 52 percent we

276
00:10:57,040 --> 00:10:59,040
can see that detected leakage is not

277
00:10:59,040 --> 00:11:01,440
very significant anymore

278
00:11:01,440 --> 00:11:04,240
however the main snr and t test peaks

279
00:11:04,240 --> 00:11:06,000
are still located around the same

280
00:11:06,000 --> 00:11:08,720
regions indicating that this 52 will be

281
00:11:08,720 --> 00:11:12,680
enough as a label initialization

282
00:11:15,040 --> 00:11:16,959
let's see the results that we obtain it

283
00:11:16,959 --> 00:11:18,959
by applying the proposed deep learning

284
00:11:18,959 --> 00:11:21,518
framework

285
00:11:22,399 --> 00:11:24,320
in terms of training configurations for

286
00:11:24,320 --> 00:11:26,240
each neural network we consider 10

287
00:11:26,240 --> 00:11:28,560
epochs as the overfitting happens very

288
00:11:28,560 --> 00:11:30,640
soon as a consequence of the framework

289
00:11:30,640 --> 00:11:34,640
iterations the batch size is set to 64.

290
00:11:34,640 --> 00:11:36,560
250 scalar multiplications are

291
00:11:36,560 --> 00:11:38,399
considered for training which are

292
00:11:38,399 --> 00:11:40,560
divided into two groups each group will

293
00:11:40,560 --> 00:11:42,760
then contain 31

294
00:11:42,760 --> 00:11:45,600
875 sub traces

295
00:11:45,600 --> 00:11:47,279
a separate set of 50 scholar

296
00:11:47,279 --> 00:11:50,800
multiplications making a total of 12 750

297
00:11:50,800 --> 00:11:53,839
subtraces are used as test set in order

298
00:11:53,839 --> 00:11:56,240
to verify the accuracy at the end of

299
00:11:56,240 --> 00:11:59,040
each epoch

300
00:12:02,160 --> 00:12:04,079
now i will shortly describe the neural

301
00:12:04,079 --> 00:12:05,839
networks that we use for the case of

302
00:12:05,839 --> 00:12:08,240
fixed hyper parameters

303
00:12:08,240 --> 00:12:10,399
for both data sets we consider similar

304
00:12:10,399 --> 00:12:12,160
convolutional neural networks where the

305
00:12:12,160 --> 00:12:14,079
main difference is the carrier size and

306
00:12:14,079 --> 00:12:16,160
stride in convolutional layers the

307
00:12:16,160 --> 00:12:18,240
network contains a first average

308
00:12:18,240 --> 00:12:19,440
building layer to reduce the

309
00:12:19,440 --> 00:12:22,079
dimensionality of traces followed by a

310
00:12:22,079 --> 00:12:23,760
batch normalization layer and three

311
00:12:23,760 --> 00:12:25,440
convolution layers for the

312
00:12:25,440 --> 00:12:27,279
classification we consider two density

313
00:12:27,279 --> 00:12:29,920
layers with 100 euros each a soft max

314
00:12:29,920 --> 00:12:34,160
displace it at as the output layer

315
00:12:36,480 --> 00:12:38,160
when dropout is considered as a

316
00:12:38,160 --> 00:12:40,160
regularization method we place two

317
00:12:40,160 --> 00:12:42,079
dropout layers in between the dense

318
00:12:42,079 --> 00:12:42,959
layers

319
00:12:42,959 --> 00:12:47,518
the dropout rate is set to 0.5

320
00:12:49,760 --> 00:12:51,680
first i will present framework results

321
00:12:51,680 --> 00:12:54,000
for the case of fixed hyper parameters

322
00:12:54,000 --> 00:12:56,399
as we can see without regularization we

323
00:12:56,399 --> 00:12:58,320
cannot achieve a successful attack for

324
00:12:58,320 --> 00:13:01,839
both data sets after 50 iterations

325
00:13:01,839 --> 00:13:04,160
the y-axis indicates the maximum key

326
00:13:04,160 --> 00:13:06,639
recovery rate or accuracy for a single

327
00:13:06,639 --> 00:13:10,079
scalar multiplication trace

328
00:13:12,079 --> 00:13:14,320
when data augmentation is considered we

329
00:13:14,320 --> 00:13:16,720
can obtain better results for c swap

330
00:13:16,720 --> 00:13:18,800
arith data set the improvement was not

331
00:13:18,800 --> 00:13:21,360
very significant however for the c swap

332
00:13:21,360 --> 00:13:23,279
pointer data set the addition of data

333
00:13:23,279 --> 00:13:24,959
augmentation was enough to make the

334
00:13:24,959 --> 00:13:27,920
attack successful

335
00:13:30,240 --> 00:13:32,399
a significant improvement for syswob

336
00:13:32,399 --> 00:13:34,959
arith dataset is observed when dropout

337
00:13:34,959 --> 00:13:37,519
is considered as a regularization method

338
00:13:37,519 --> 00:13:39,360
in this case we are able to reach a

339
00:13:39,360 --> 00:13:41,600
final and maximum single trace accuracy

340
00:13:41,600 --> 00:13:43,440
of 93 percent

341
00:13:43,440 --> 00:13:45,440
for the other data set dropout also

342
00:13:45,440 --> 00:13:49,800
provided 100 percent of accuracy

343
00:13:51,360 --> 00:13:53,680
now the combination of dropout and data

344
00:13:53,680 --> 00:13:55,519
augmentation delivers the best results

345
00:13:55,519 --> 00:13:58,639
so far for the c swap arith data set we

346
00:13:58,639 --> 00:14:00,560
reach a successful attack that was not

347
00:14:00,560 --> 00:14:02,720
possible before with only dropout or

348
00:14:02,720 --> 00:14:05,199
only data augmentation for cs5 pointer

349
00:14:05,199 --> 00:14:06,959
data set the combination of both

350
00:14:06,959 --> 00:14:10,399
regularization allows us to achieve 100

351
00:14:10,399 --> 00:14:15,000
after only three framework iterations

352
00:14:17,360 --> 00:14:19,680
to define random neural networks we vary

353
00:14:19,680 --> 00:14:21,760
the number of filters kernel size and

354
00:14:21,760 --> 00:14:24,320
strides in convolution layers the number

355
00:14:24,320 --> 00:14:25,920
of density layers and the number of

356
00:14:25,920 --> 00:14:28,480
neurons in density layers we also vary

357
00:14:28,480 --> 00:14:30,480
the activation function that is equal

358
00:14:30,480 --> 00:14:33,600
for all hidden layers

359
00:14:35,760 --> 00:14:37,440
now let's move to the results with

360
00:14:37,440 --> 00:14:39,760
random either parameters without any

361
00:14:39,760 --> 00:14:41,839
regularization we already obtained very

362
00:14:41,839 --> 00:14:43,920
high final single trace accuracy for

363
00:14:43,920 --> 00:14:46,880
both data sets

364
00:14:48,399 --> 00:14:50,480
adding data augmentation is enough to

365
00:14:50,480 --> 00:14:52,560
achieve 100 percent of single trace

366
00:14:52,560 --> 00:14:56,239
accuracy for both data sets

367
00:14:57,519 --> 00:14:59,600
in this case dropout regularization

368
00:14:59,600 --> 00:15:01,839
combination with random hyper parameters

369
00:15:01,839 --> 00:15:04,160
also deliver good results a higher

370
00:15:04,160 --> 00:15:05,920
improvement can be seen for c swap

371
00:15:05,920 --> 00:15:08,959
pointer dataset

372
00:15:10,800 --> 00:15:12,880
finally the combination of dropout and

373
00:15:12,880 --> 00:15:14,720
data augmentation again delivers to

374
00:15:14,720 --> 00:15:16,959
sexual results in this case the

375
00:15:16,959 --> 00:15:18,639
combination of two regularization

376
00:15:18,639 --> 00:15:20,560
methods did not improve results in

377
00:15:20,560 --> 00:15:24,920
comparison to dropout only

378
00:15:26,639 --> 00:15:28,800
here i want to show a final comparison

379
00:15:28,800 --> 00:15:30,880
between the framework executions for

380
00:15:30,880 --> 00:15:32,880
fixed and random hyper parameters

381
00:15:32,880 --> 00:15:34,880
clearly random hyper parameters

382
00:15:34,880 --> 00:15:37,440
increases the chance to succeed

383
00:15:37,440 --> 00:15:39,920
for each regularization scenario we run

384
00:15:39,920 --> 00:15:42,000
10 framework executions

385
00:15:42,000 --> 00:15:44,480
for fixed hyper parameters only one case

386
00:15:44,480 --> 00:15:47,360
return 100 of key recovery for the

387
00:15:47,360 --> 00:15:49,279
random case we achieve 100 percent of

388
00:15:49,279 --> 00:15:54,000
key recovery for 12 out of 40 executions

389
00:15:54,000 --> 00:15:55,839
note also how the case without

390
00:15:55,839 --> 00:15:58,000
regularization provides better results

391
00:15:58,000 --> 00:15:59,600
when random hybrid parameters are

392
00:15:59,600 --> 00:16:02,079
considered in this case we obtain a

393
00:16:02,079 --> 00:16:04,480
final single trace accuracy of 86

394
00:16:04,480 --> 00:16:06,800
percent and for fixed hyper parameters

395
00:16:06,800 --> 00:16:11,120
we were able to reach only 76 percent

396
00:16:13,279 --> 00:16:15,519
for the c-swap pointer data set the

397
00:16:15,519 --> 00:16:17,600
overall results show some differences

398
00:16:17,600 --> 00:16:19,199
between fixed and random hyper

399
00:16:19,199 --> 00:16:21,519
parameters indeed fixed hyper-parameter

400
00:16:21,519 --> 00:16:23,920
delivered better results

401
00:16:23,920 --> 00:16:26,639
in total we were able to succeed in 25

402
00:16:26,639 --> 00:16:28,560
percent of cases for fixed hyper

403
00:16:28,560 --> 00:16:31,279
parameters when random parameters are

404
00:16:31,279 --> 00:16:34,320
considered during framework executions

405
00:16:34,320 --> 00:16:37,600
we recover the full key in 17 out of 40

406
00:16:37,600 --> 00:16:40,160
cases

407
00:16:42,639 --> 00:16:44,720
we also investigated the possibility of

408
00:16:44,720 --> 00:16:46,399
improving the results by trimming the

409
00:16:46,399 --> 00:16:49,440
attacked trace interval we we do that by

410
00:16:49,440 --> 00:16:51,519
obtaining input gradient visualization

411
00:16:51,519 --> 00:16:54,000
which indicate what what samples are

412
00:16:54,000 --> 00:16:55,759
more important for the neural network

413
00:16:55,759 --> 00:16:57,920
decisions it is important to mention

414
00:16:57,920 --> 00:16:59,920
that calculation of input gradients is

415
00:16:59,920 --> 00:17:02,639
totally independent of labels

416
00:17:02,639 --> 00:17:04,480
as you can see the larger input

417
00:17:04,480 --> 00:17:06,000
gradients coincide with many

418
00:17:06,000 --> 00:17:08,319
signal-to-noise radio peaks obtained

419
00:17:08,319 --> 00:17:10,240
with two labels indicating that our

420
00:17:10,240 --> 00:17:12,559
trimmed interval based on gradients is

421
00:17:12,559 --> 00:17:16,599
aligned with the leakage location

422
00:17:18,319 --> 00:17:20,000
these are the best results obtaining

423
00:17:20,000 --> 00:17:22,319
with fixed ip parameters for both data

424
00:17:22,319 --> 00:17:24,720
sets after using gradient visualization

425
00:17:24,720 --> 00:17:26,799
for c swap arit

426
00:17:26,799 --> 00:17:28,960
a data set all regularization cases

427
00:17:28,960 --> 00:17:31,600
provided successful results for c swap

428
00:17:31,600 --> 00:17:34,080
pointer data set dropout only case did

429
00:17:34,080 --> 00:17:36,240
not provided hundred percent of accuracy

430
00:17:36,240 --> 00:17:39,440
after 50 iteration

431
00:17:40,400 --> 00:17:42,320
gradient visualization usage also

432
00:17:42,320 --> 00:17:44,160
affects the results for random hybrid

433
00:17:44,160 --> 00:17:47,120
parameters for c swap arith data set all

434
00:17:47,120 --> 00:17:49,120
scenarios including no regularization

435
00:17:49,120 --> 00:17:52,320
succeed for cs4 pointer data set we can

436
00:17:52,320 --> 00:17:54,559
see a very early convergence with the

437
00:17:54,559 --> 00:17:57,840
regularization cases

438
00:17:59,760 --> 00:18:01,840
here we can see how selecting a trimmed

439
00:18:01,840 --> 00:18:04,080
interval improves the results for c swap

440
00:18:04,080 --> 00:18:06,640
arid data set in the particular case of

441
00:18:06,640 --> 00:18:08,799
fixed hyper parameters we were able to

442
00:18:08,799 --> 00:18:11,679
succeed only one time before after

443
00:18:11,679 --> 00:18:14,240
gradient visualization we obtain 19 out

444
00:18:14,240 --> 00:18:17,120
of 40 successful attacks a significant

445
00:18:17,120 --> 00:18:18,880
improvement is also verified for the

446
00:18:18,880 --> 00:18:20,960
random case

447
00:18:20,960 --> 00:18:22,799
on the other hand for cs5 pointer

448
00:18:22,799 --> 00:18:25,039
dataset trimming the attack interval

449
00:18:25,039 --> 00:18:26,720
does not improve the results this is

450
00:18:26,720 --> 00:18:28,559
mainly because the attacked interval for

451
00:18:28,559 --> 00:18:32,799
1000 samples was already short enough

452
00:18:35,200 --> 00:18:37,440
as conclusions we demonstrated that db

453
00:18:37,440 --> 00:18:39,120
learning approaches can improve attack

454
00:18:39,120 --> 00:18:41,679
success rate due to its capacity of

455
00:18:41,679 --> 00:18:44,240
dealing with noise or wrong labels

456
00:18:44,240 --> 00:18:46,160
for that we proposed an iterative

457
00:18:46,160 --> 00:18:48,160
framework with regularized models that

458
00:18:48,160 --> 00:18:51,039
can achieve successful attack results

459
00:18:51,039 --> 00:18:53,039
the framework should be applicable to

460
00:18:53,039 --> 00:18:54,960
other targets including rsa

461
00:18:54,960 --> 00:18:56,559
implementations

462
00:18:56,559 --> 00:18:58,400
as a future work we would like to

463
00:18:58,400 --> 00:19:00,400
investigate how the framework behaves in

464
00:19:00,400 --> 00:19:02,559
the presence of trace misalignment and

465
00:19:02,559 --> 00:19:05,039
other label initialization methods

466
00:19:05,039 --> 00:19:06,480
thank you very much for watching this

467
00:19:06,480 --> 00:19:09,480
presentation

468
00:19:12,720 --> 00:19:14,799
you

