1
00:00:00,880 --> 00:00:02,720
hi i'm wong gyeong zhong from seoul

2
00:00:02,720 --> 00:00:05,120
national university and this is talk

3
00:00:05,120 --> 00:00:07,200
about over 100 times faster

4
00:00:07,200 --> 00:00:09,360
bootstrapping and fully homomorphic

5
00:00:09,360 --> 00:00:11,120
encryption through memory centric

6
00:00:11,120 --> 00:00:13,200
optimization with gpus

7
00:00:13,200 --> 00:00:14,920
so let's

8
00:00:14,920 --> 00:00:18,880
begin so what is homoph encryption or h

9
00:00:18,880 --> 00:00:19,680
e

10
00:00:19,680 --> 00:00:22,560
h is a cryptographic scheme that enables

11
00:00:22,560 --> 00:00:25,920
computations on encrypted messages here

12
00:00:25,920 --> 00:00:28,840
a user encrypts message one and two into

13
00:00:28,840 --> 00:00:31,840
ciphertext then she sends the ciphertext

14
00:00:31,840 --> 00:00:33,040
into

15
00:00:33,040 --> 00:00:35,200
a service provider who can compute on

16
00:00:35,200 --> 00:00:37,760
the ciphertext without ever decryption

17
00:00:37,760 --> 00:00:39,920
after that the provider sends back the

18
00:00:39,920 --> 00:00:42,079
ciphertext 3 which is the output of the

19
00:00:42,079 --> 00:00:43,600
homomorphic multiplication between

20
00:00:43,600 --> 00:00:45,360
ciphertext 1 and 2.

21
00:00:45,360 --> 00:00:47,520
after decryption user can get the

22
00:00:47,520 --> 00:00:49,680
message 3 which is the same as the

23
00:00:49,680 --> 00:00:51,600
multiplication output between the two

24
00:00:51,600 --> 00:00:55,199
messages message 1 and 2.

25
00:00:55,199 --> 00:00:57,600
there are popular 80 schemes and each of

26
00:00:57,600 --> 00:00:59,840
them supports different data types and

27
00:00:59,840 --> 00:01:01,920
different hd operations

28
00:01:01,920 --> 00:01:04,559
among them we choose ckks as our target

29
00:01:04,559 --> 00:01:07,040
in this paper it supports fixed point

30
00:01:07,040 --> 00:01:09,840
numbers and real number computations so

31
00:01:09,840 --> 00:01:11,760
it is widely used in many real

32
00:01:11,760 --> 00:01:13,680
applications such as machine learning

33
00:01:13,680 --> 00:01:16,080
tests

34
00:01:16,080 --> 00:01:18,000
the main problem of age is they are

35
00:01:18,000 --> 00:01:20,720
extremely high cost

36
00:01:20,720 --> 00:01:23,040
first the ciphertexts are extremely

37
00:01:23,040 --> 00:01:26,320
large the size of a single text in ckks

38
00:01:26,320 --> 00:01:28,640
which is even hundreds of megabytes

39
00:01:28,640 --> 00:01:30,720
because each ciphertext consists of

40
00:01:30,720 --> 00:01:33,920
polynomials having many coefficients and

41
00:01:33,920 --> 00:01:37,200
also high degree

42
00:01:38,000 --> 00:01:41,040
for that the hd operations are heavy

43
00:01:41,040 --> 00:01:43,280
compared to the negative integer or

44
00:01:43,280 --> 00:01:44,960
floating point operations they are

45
00:01:44,960 --> 00:01:46,320
slower by

46
00:01:46,320 --> 00:01:49,200
200 times in addition or by even up to

47
00:01:49,200 --> 00:01:53,960
10 thousands of times in multiplication

48
00:01:54,640 --> 00:01:57,439
moreover the bootstrapping operation is

49
00:01:57,439 --> 00:02:00,960
the heaviest one in hd the bootstrapping

50
00:02:00,960 --> 00:02:03,439
which is a concept introduced by gentry

51
00:02:03,439 --> 00:02:06,479
in 2009 is an operation consisting of

52
00:02:06,479 --> 00:02:08,479
other hd operations

53
00:02:08,479 --> 00:02:11,200
after each 80 operation on a ciphertext

54
00:02:11,200 --> 00:02:14,160
a noise is accumulated to a ciphertext

55
00:02:14,160 --> 00:02:16,239
and this limits the number of operations

56
00:02:16,239 --> 00:02:19,120
on the ciphertext by applying however

57
00:02:19,120 --> 00:02:20,959
the bootstrapping before the noise

58
00:02:20,959 --> 00:02:23,760
explodes we can apply any number of hd

59
00:02:23,760 --> 00:02:25,599
operations and this is called fully

60
00:02:25,599 --> 00:02:27,520
homology encryption

61
00:02:27,520 --> 00:02:28,480
however

62
00:02:28,480 --> 00:02:30,879
the cost is extremely high and

63
00:02:30,879 --> 00:02:33,200
it takes even dozens of seconds in a

64
00:02:33,200 --> 00:02:37,399
recent single threaded environment

65
00:02:37,760 --> 00:02:40,080
in this case hardware accelerators can

66
00:02:40,080 --> 00:02:41,440
do their job

67
00:02:41,440 --> 00:02:43,680
first there are massive parallelisms in

68
00:02:43,680 --> 00:02:45,280
he

69
00:02:45,280 --> 00:02:47,760
ac uses number theoretic transformation

70
00:02:47,760 --> 00:02:49,680
which is called entity in their

71
00:02:49,680 --> 00:02:52,080
polynomial multiplications it is an

72
00:02:52,080 --> 00:02:53,680
integer version of discrete fourier

73
00:02:53,680 --> 00:02:56,879
transformation and as in dft it converts

74
00:02:56,879 --> 00:02:59,599
the n square complexity of polynomial

75
00:02:59,599 --> 00:03:01,200
multiplication

76
00:03:01,200 --> 00:03:03,680
which is a convolution into analog and

77
00:03:03,680 --> 00:03:06,319
complexity

78
00:03:06,319 --> 00:03:08,000
the second thing is residual number

79
00:03:08,000 --> 00:03:09,040
system

80
00:03:09,040 --> 00:03:11,200
it uses ancient chinese remainder

81
00:03:11,200 --> 00:03:12,159
theorem

82
00:03:12,159 --> 00:03:14,560
representing a big number into a set of

83
00:03:14,560 --> 00:03:18,400
residues are 0 to rl by applying modular

84
00:03:18,400 --> 00:03:22,319
operations with a given prime set q0 to

85
00:03:22,319 --> 00:03:23,680
ql

86
00:03:23,680 --> 00:03:26,239
then it transformed the

87
00:03:26,239 --> 00:03:29,200
multiplication between two big integers

88
00:03:29,200 --> 00:03:31,360
into an element-wise modular

89
00:03:31,360 --> 00:03:33,040
multiplication

90
00:03:33,040 --> 00:03:35,040
these two algorithms give large

91
00:03:35,040 --> 00:03:38,000
parallelism to he and as you know cpus

92
00:03:38,000 --> 00:03:40,720
can enjoy it

93
00:03:42,080 --> 00:03:45,200
so why we choose diffuse over cpus they

94
00:03:45,200 --> 00:03:48,400
both have dozens of cores but gpus have

95
00:03:48,400 --> 00:03:50,319
much more higher integer operation

96
00:03:50,319 --> 00:03:52,720
throughput per core per cycle

97
00:03:52,720 --> 00:03:55,120
compared to that of cpus

98
00:03:55,120 --> 00:03:57,439
gpus also outperform

99
00:03:57,439 --> 00:04:00,000
cpus around 10 times in both total

100
00:04:00,000 --> 00:04:02,640
integer throughput and also in main

101
00:04:02,640 --> 00:04:05,439
memory bandwidth so for this reason

102
00:04:05,439 --> 00:04:08,239
many hd research papers appearing now

103
00:04:08,239 --> 00:04:12,239
are exploiting gpus more and more

104
00:04:13,120 --> 00:04:14,959
so in this work

105
00:04:14,959 --> 00:04:17,040
we represent the first gpu

106
00:04:17,040 --> 00:04:19,040
implementation of bootstrapping in a

107
00:04:19,040 --> 00:04:21,839
recent ckks scheme

108
00:04:21,839 --> 00:04:24,400
and we also analyzed this severe memory

109
00:04:24,400 --> 00:04:26,880
bandwidth bottleneck in the baseline gpu

110
00:04:26,880 --> 00:04:29,600
implementation

111
00:04:29,600 --> 00:04:31,759
so we applied software-based memory

112
00:04:31,759 --> 00:04:34,240
centering optimizations to amortize the

113
00:04:34,240 --> 00:04:36,479
memory bandwidth bottleneck and this

114
00:04:36,479 --> 00:04:39,120
results in a speed up of around 200

115
00:04:39,120 --> 00:04:42,400
times faster bootstrapping compared to

116
00:04:42,400 --> 00:04:46,159
a single threaded cpu case

117
00:04:46,720 --> 00:04:48,560
so let me first describe a recent full

118
00:04:48,560 --> 00:04:52,880
rns kks scheme which appeared in hk20

119
00:04:52,880 --> 00:04:55,360
so these are some parameters

120
00:04:55,360 --> 00:04:57,199
the multiplicative level of this

121
00:04:57,199 --> 00:05:00,000
parameter set is represented as l

122
00:05:00,000 --> 00:05:03,199
where primes are represented as q0 to ql

123
00:05:03,199 --> 00:05:05,680
and also p1 to pk

124
00:05:05,680 --> 00:05:08,160
also the modulus used in this parameter

125
00:05:08,160 --> 00:05:08,960
set

126
00:05:08,960 --> 00:05:11,840
is called large q and last p

127
00:05:11,840 --> 00:05:15,280
there is a secret polynomial s x and

128
00:05:15,280 --> 00:05:17,199
an error polynomial e x whose

129
00:05:17,199 --> 00:05:20,560
coefficients are small enough

130
00:05:21,199 --> 00:05:24,400
we represent each plaintext as mx or m

131
00:05:24,400 --> 00:05:27,039
the plaintext in rns form is represented

132
00:05:27,039 --> 00:05:30,240
as mx which is in a polynomial ring

133
00:05:30,240 --> 00:05:32,960
called rq whose coefficients are modular

134
00:05:32,960 --> 00:05:33,840
q

135
00:05:33,840 --> 00:05:36,160
and its degrees up to n

136
00:05:36,160 --> 00:05:38,240
in entity form we represent the

137
00:05:38,240 --> 00:05:40,479
polynomial mxsm

138
00:05:40,479 --> 00:05:43,280
our plaintext is actually an encoded

139
00:05:43,280 --> 00:05:45,520
vector of up to n over two complex

140
00:05:45,520 --> 00:05:48,000
numbers

141
00:05:48,320 --> 00:05:51,600
each ciphertext is represented as a pair

142
00:05:51,600 --> 00:05:55,440
of polynomials bx and ax in rns form

143
00:05:55,440 --> 00:05:58,160
or b and a in entity form

144
00:05:58,160 --> 00:06:00,880
so from a ciphertext

145
00:06:00,880 --> 00:06:04,400
we get the plaintext m by decryption

146
00:06:04,400 --> 00:06:06,479
which is the dot product between the

147
00:06:06,479 --> 00:06:10,000
ciphertext city and one comma s

148
00:06:10,000 --> 00:06:12,160
which is the secret key

149
00:06:12,160 --> 00:06:15,360
we get the plain text m with a small

150
00:06:15,360 --> 00:06:17,360
error e

151
00:06:17,360 --> 00:06:19,360
i will explain some key operations in

152
00:06:19,360 --> 00:06:22,080
skks there is a plaintext multiplication

153
00:06:22,080 --> 00:06:24,000
called simult which is just an

154
00:06:24,000 --> 00:06:25,759
element-wise multiplication between

155
00:06:25,759 --> 00:06:29,520
plain text m and ciphertext ct

156
00:06:29,520 --> 00:06:31,280
on the other hand the ciphertext

157
00:06:31,280 --> 00:06:33,280
multiplication is much more complicated

158
00:06:33,280 --> 00:06:35,360
it first computes a tensor product

159
00:06:35,360 --> 00:06:38,000
between the input cipher text 1 and 2

160
00:06:38,000 --> 00:06:41,039
and for the output 3 polynomials from d0

161
00:06:41,039 --> 00:06:44,960
to d2 we perform a key switching to d2

162
00:06:44,960 --> 00:06:47,680
with a key called multiplication key

163
00:06:47,680 --> 00:06:49,759
and then for the output of the key

164
00:06:49,759 --> 00:06:54,800
switching we add that with d0 and d1

165
00:06:54,800 --> 00:06:56,319
there is another operation called

166
00:06:56,319 --> 00:06:58,639
ciphertext rotation which the circular

167
00:06:58,639 --> 00:07:01,440
shift on a ciphertext by a rotation

168
00:07:01,440 --> 00:07:02,880
index r

169
00:07:02,880 --> 00:07:05,120
it performs on automorphism which

170
00:07:05,120 --> 00:07:08,000
computes x to the 5 to the r on both ax

171
00:07:08,000 --> 00:07:10,560
and bx then as in multiplication it

172
00:07:10,560 --> 00:07:12,720
returns a key switching output but with

173
00:07:12,720 --> 00:07:14,840
its own rotation key by

174
00:07:14,840 --> 00:07:18,240
r the difference between prior works and

175
00:07:18,240 --> 00:07:20,880
hk 20 is that it introduces a new key

176
00:07:20,880 --> 00:07:23,360
switching method called generalized key

177
00:07:23,360 --> 00:07:27,440
switching or hybrid key switching

178
00:07:27,440 --> 00:07:29,120
so let's understand the key switching in

179
00:07:29,120 --> 00:07:31,840
sk20 there is a polynomial a with

180
00:07:31,840 --> 00:07:34,160
modulus q we do key switching with a

181
00:07:34,160 --> 00:07:37,120
switching key swk

182
00:07:37,120 --> 00:07:39,599
first it splits the residue set of a

183
00:07:39,599 --> 00:07:42,960
polynomial a into denom parts from q0 to

184
00:07:42,960 --> 00:07:46,400
git qdnom for a given denom parameter

185
00:07:46,400 --> 00:07:48,319
where dnom is shorthanded for

186
00:07:48,319 --> 00:07:50,240
decomposition number

187
00:07:50,240 --> 00:07:52,560
its residue set after decomposition has

188
00:07:52,560 --> 00:07:55,280
alpha modally

189
00:07:55,280 --> 00:07:58,319
then modab raises the modulus of each

190
00:07:58,319 --> 00:08:01,919
residue set from qi to pq

191
00:08:01,919 --> 00:08:04,000
here the parameter p is said to be

192
00:08:04,000 --> 00:08:07,520
bigger than any other qi

193
00:08:07,520 --> 00:08:10,720
the third step is inner product the key

194
00:08:10,720 --> 00:08:13,599
swk is a ciphertext vector with the size

195
00:08:13,599 --> 00:08:15,919
of genome and modulus pq

196
00:08:15,919 --> 00:08:17,919
we multiply each part of input with the

197
00:08:17,919 --> 00:08:20,240
corresponding element in the key and do

198
00:08:20,240 --> 00:08:22,720
summation

199
00:08:22,960 --> 00:08:25,360
and finally we reduce the

200
00:08:25,360 --> 00:08:26,800
output

201
00:08:26,800 --> 00:08:28,879
the modulus of the output to q

202
00:08:28,879 --> 00:08:31,199
and multiply by one over p

203
00:08:31,199 --> 00:08:33,760
this step is called mod down the point

204
00:08:33,760 --> 00:08:35,760
here is that it will decompose the

205
00:08:35,760 --> 00:08:37,519
polynomial into multiple small

206
00:08:37,519 --> 00:08:40,479
polynomials

207
00:08:40,479 --> 00:08:42,640
then how do we set the parameters

208
00:08:42,640 --> 00:08:45,120
one thing to know is that

209
00:08:45,120 --> 00:08:48,240
large load pq value lowers security

210
00:08:48,240 --> 00:08:50,560
and also the dna parameter affects the

211
00:08:50,560 --> 00:08:53,040
security as well as computational

212
00:08:53,040 --> 00:08:56,000
complexity key size

213
00:08:56,000 --> 00:08:58,800
so for a fixed queue when dinom is 1

214
00:08:58,800 --> 00:09:00,399
which is the minimum

215
00:09:00,399 --> 00:09:02,800
the value p should almost be q

216
00:09:02,800 --> 00:09:04,560
increasing pq

217
00:09:04,560 --> 00:09:06,560
therefore it decreases the security

218
00:09:06,560 --> 00:09:09,040
however it reduces the size of each key

219
00:09:09,040 --> 00:09:12,640
switching key which is the only one

220
00:09:12,640 --> 00:09:14,160
on the other hand we can increase the

221
00:09:14,160 --> 00:09:17,600
dna value up to l which is the maximum

222
00:09:17,600 --> 00:09:19,680
in this case the value p

223
00:09:19,680 --> 00:09:22,959
would only be q to the one over l which

224
00:09:22,959 --> 00:09:24,880
increases the security

225
00:09:24,880 --> 00:09:28,560
but we need l sized keys

226
00:09:28,560 --> 00:09:30,560
besides the computational complexity

227
00:09:30,560 --> 00:09:32,800
becomes minimal somewhere between t now

228
00:09:32,800 --> 00:09:36,640
equals to zero one two l so in the

229
00:09:36,640 --> 00:09:38,720
previous work they choose the value that

230
00:09:38,720 --> 00:09:40,320
minimizes the number of modular

231
00:09:40,320 --> 00:09:43,200
multiplications

232
00:09:43,839 --> 00:09:46,640
however our observation is that on gpus

233
00:09:46,640 --> 00:09:49,680
we are more memory bottlenecked

234
00:09:49,680 --> 00:09:51,920
these are the last level cache size of

235
00:09:51,920 --> 00:09:55,680
modern cpus and gpus compared to cpu

236
00:09:55,680 --> 00:09:58,000
cpus have only several megabytes of

237
00:09:58,000 --> 00:09:59,120
caches

238
00:09:59,120 --> 00:10:00,720
which are hardly accommodating

239
00:10:00,720 --> 00:10:02,839
ciphertexts whose sizes are dozens of

240
00:10:02,839 --> 00:10:05,440
megabytes this makes hd operations

241
00:10:05,440 --> 00:10:07,040
running on the gpu more memory

242
00:10:07,040 --> 00:10:09,599
bottleneck

243
00:10:09,839 --> 00:10:11,920
so this is the latency of ckks

244
00:10:11,920 --> 00:10:13,920
multiplication over different genomes

245
00:10:13,920 --> 00:10:19,200
both in a cpu with 8 threads and a cpu

246
00:10:19,200 --> 00:10:21,040
as you can see the cpu implementation

247
00:10:21,040 --> 00:10:22,720
performs the best with a genome that

248
00:10:22,720 --> 00:10:24,320
minimizes the number of modular

249
00:10:24,320 --> 00:10:27,279
multiplications

250
00:10:27,279 --> 00:10:29,360
on the other hand gpa performs the best

251
00:10:29,360 --> 00:10:30,240
with

252
00:10:30,240 --> 00:10:32,720
other dinner value is minimizing the

253
00:10:32,720 --> 00:10:34,720
number of total memory accesses

254
00:10:34,720 --> 00:10:37,519
especially main memory accesses

255
00:10:37,519 --> 00:10:39,600
so we made the gpu load frame for the

256
00:10:39,600 --> 00:10:41,200
functions comprising on hd

257
00:10:41,200 --> 00:10:44,640
multiplication if a point is close to

258
00:10:44,640 --> 00:10:46,800
the sloping roof it means that it has

259
00:10:46,800 --> 00:10:49,279
low arithmetic intensity implying the

260
00:10:49,279 --> 00:10:51,839
function is bottlenecked by main memory

261
00:10:51,839 --> 00:10:54,320
bandwidth rather than arithmetic units

262
00:10:54,320 --> 00:10:56,160
on the other hand if a point is on the

263
00:10:56,160 --> 00:10:58,839
flat side then it means it is compute

264
00:10:58,839 --> 00:11:01,519
bound as you can see from the figure it

265
00:11:01,519 --> 00:11:03,360
seems that most of the functions are

266
00:11:03,360 --> 00:11:08,240
bounded by main memory bandits of a cpu

267
00:11:08,480 --> 00:11:10,880
this motivated us to focus on memory

268
00:11:10,880 --> 00:11:13,040
central optimizations on the gpu

269
00:11:13,040 --> 00:11:15,279
implementation

270
00:11:15,279 --> 00:11:17,200
first let me give you a brief

271
00:11:17,200 --> 00:11:19,040
introduction to a contemporary gpu

272
00:11:19,040 --> 00:11:20,800
programming model

273
00:11:20,800 --> 00:11:22,560
a gpu has many streaming

274
00:11:22,560 --> 00:11:24,959
multi-processors called sm and each

275
00:11:24,959 --> 00:11:26,800
asset manages threads

276
00:11:26,800 --> 00:11:30,480
where they are grouped in parallel

277
00:11:30,480 --> 00:11:32,399
a function executed on the gpu is called

278
00:11:32,399 --> 00:11:34,880
kernel and gpu threads run the same

279
00:11:34,880 --> 00:11:36,800
corner and instructions in parallel on

280
00:11:36,800 --> 00:11:38,640
the sms this is called sim t

281
00:11:38,640 --> 00:11:43,279
architecture by nvidia corporation

282
00:11:43,279 --> 00:11:44,959
so each kernel is configured with the

283
00:11:44,959 --> 00:11:47,360
number of threads it used and

284
00:11:47,360 --> 00:11:49,360
the amount of memory shared memory it

285
00:11:49,360 --> 00:11:51,279
used the shared memory is a user

286
00:11:51,279 --> 00:11:53,920
configurable scratch pad memory which is

287
00:11:53,920 --> 00:11:57,600
extremely fast but also small

288
00:11:57,600 --> 00:12:00,079
so we implemented the baseline cpu

289
00:12:00,079 --> 00:12:02,880
implementation based on the prior works

290
00:12:02,880 --> 00:12:05,600
that implement hd schemes first for

291
00:12:05,600 --> 00:12:06,560
example

292
00:12:06,560 --> 00:12:09,680
a cache friendly data layout or how do

293
00:12:09,680 --> 00:12:12,240
we launch threads in a kernel

294
00:12:12,240 --> 00:12:16,560
or fast entity implementations and so on

295
00:12:17,120 --> 00:12:18,720
our key contribution is that we applied

296
00:12:18,720 --> 00:12:20,800
carnal fusion techniques on the baseline

297
00:12:20,800 --> 00:12:23,839
implementation of gpu carnal fusion or

298
00:12:23,839 --> 00:12:25,680
operation fusion is a common technique

299
00:12:25,680 --> 00:12:28,000
that fuses multiple gpu kernels into a

300
00:12:28,000 --> 00:12:29,440
single kernel

301
00:12:29,440 --> 00:12:32,000
here we have kernel a and b each kernel

302
00:12:32,000 --> 00:12:34,560
reads data from gpu's main memory which

303
00:12:34,560 --> 00:12:36,800
is also called global memory shown as

304
00:12:36,800 --> 00:12:39,680
gmail here then the kernel computes on

305
00:12:39,680 --> 00:12:40,880
the data

306
00:12:40,880 --> 00:12:43,120
and writes back to the global memory

307
00:12:43,120 --> 00:12:45,040
after computing

308
00:12:45,040 --> 00:12:47,680
after kernel fusion the two corners are

309
00:12:47,680 --> 00:12:50,800
fused into a single kernel

310
00:12:50,800 --> 00:12:52,560
so there are two advantages on the

311
00:12:52,560 --> 00:12:55,519
kernel fusion technique

312
00:12:55,519 --> 00:12:57,680
first it saves some amount of global

313
00:12:57,680 --> 00:13:00,160
memory accesses here as you fuse the two

314
00:13:00,160 --> 00:13:02,560
kernels global memory reads and writes

315
00:13:02,560 --> 00:13:04,399
between the two kernels are converted

316
00:13:04,399 --> 00:13:06,720
into registries and writes

317
00:13:06,720 --> 00:13:08,639
it's because we can reuse the data in

318
00:13:08,639 --> 00:13:11,040
registers or shared memory which are

319
00:13:11,040 --> 00:13:12,959
much faster than dram

320
00:13:12,959 --> 00:13:14,560
this is especially good for low

321
00:13:14,560 --> 00:13:17,120
operation per byte kernels because they

322
00:13:17,120 --> 00:13:19,040
are most boundless by main memory

323
00:13:19,040 --> 00:13:21,680
bandwidth

324
00:13:21,680 --> 00:13:23,680
second we can reduce the kernel launch

325
00:13:23,680 --> 00:13:25,920
overhead if the kernels are small

326
00:13:25,920 --> 00:13:27,600
each corner has its own kernel on the

327
00:13:27,600 --> 00:13:29,519
overhead which is not negligible if the

328
00:13:29,519 --> 00:13:31,519
kernel is extremely small

329
00:13:31,519 --> 00:13:34,240
or small enough we can feel such small

330
00:13:34,240 --> 00:13:36,079
corners to avoid the kernel launch

331
00:13:36,079 --> 00:13:39,199
overhead between the kernels

332
00:13:39,199 --> 00:13:41,279
in this work we found many corner vision

333
00:13:41,279 --> 00:13:44,000
opportunities in intra and inter-agey

334
00:13:44,000 --> 00:13:46,880
operation manner

335
00:13:47,680 --> 00:13:49,839
we first introduced intra-hd operation

336
00:13:49,839 --> 00:13:50,880
fusion

337
00:13:50,880 --> 00:13:54,240
this is called model fusion

338
00:13:56,560 --> 00:13:58,720
actually the model consists of multiple

339
00:13:58,720 --> 00:14:03,360
functions so let's see it's inside

340
00:14:03,360 --> 00:14:05,040
so this is the computational graph of

341
00:14:05,040 --> 00:14:06,160
both of

342
00:14:06,160 --> 00:14:08,639
including intt space conversions and

343
00:14:08,639 --> 00:14:12,079
entities we show each single kernel as a

344
00:14:12,079 --> 00:14:14,000
gray rounded square

345
00:14:14,000 --> 00:14:16,480
first because the decomposed inputs are

346
00:14:16,480 --> 00:14:18,959
all in entity domain differ from intt

347
00:14:18,959 --> 00:14:19,839
first

348
00:14:19,839 --> 00:14:22,079
and then we perform base conversions

349
00:14:22,079 --> 00:14:24,480
which can raise or reduce the modulus

350
00:14:24,480 --> 00:14:26,480
but in this case we raise the modulus of

351
00:14:26,480 --> 00:14:27,920
each input

352
00:14:27,920 --> 00:14:30,240
after that we apply entity to support

353
00:14:30,240 --> 00:14:32,720
multiplication between a key switching

354
00:14:32,720 --> 00:14:33,519
key

355
00:14:33,519 --> 00:14:35,680
here the base conversion itself also

356
00:14:35,680 --> 00:14:38,560
consists of two functions

357
00:14:38,560 --> 00:14:41,279
scaling and matrix matrix multiplication

358
00:14:41,279 --> 00:14:44,079
for more details please see the paper

359
00:14:44,079 --> 00:14:46,560
what we do here is that we fuse multiple

360
00:14:46,560 --> 00:14:51,040
small int kernels into a large one here

361
00:14:51,040 --> 00:14:53,040
because the modulus of each decomposed

362
00:14:53,040 --> 00:14:56,160
part is small as q sub zero to qsub

363
00:14:56,160 --> 00:14:59,120
genome their kernels are small

364
00:14:59,120 --> 00:15:01,199
by fusing the small int kernels we

365
00:15:01,199 --> 00:15:02,720
reduce the chronicle overheads and

366
00:15:02,720 --> 00:15:07,040
increase the sm utilization in gpu

367
00:15:07,279 --> 00:15:09,360
second we also fuse the scaling kernels

368
00:15:09,360 --> 00:15:11,680
which are just element-wise operations

369
00:15:11,680 --> 00:15:14,720
before saving the intt outputs to main

370
00:15:14,720 --> 00:15:15,680
memory

371
00:15:15,680 --> 00:15:18,079
we perform scaling operations saving

372
00:15:18,079 --> 00:15:20,240
memory and rights of the memory bound

373
00:15:20,240 --> 00:15:23,120
scaling kernel

374
00:15:23,519 --> 00:15:26,959
the second one is inert product fusion

375
00:15:26,959 --> 00:15:28,959
you can see that in this figure

376
00:15:28,959 --> 00:15:30,639
in the baseline implementation there are

377
00:15:30,639 --> 00:15:32,800
multiplication kernels and addition

378
00:15:32,800 --> 00:15:35,279
kernels for multiplication with a key

379
00:15:35,279 --> 00:15:37,440
switching key

380
00:15:37,440 --> 00:15:39,120
in this region we perform all the

381
00:15:39,120 --> 00:15:41,600
corners i mentioned in this

382
00:15:41,600 --> 00:15:45,040
uh single b kernel

383
00:15:45,440 --> 00:15:48,320
the last one is multivision we take a

384
00:15:48,320 --> 00:15:51,600
look at the bottom first

385
00:15:53,120 --> 00:15:55,279
in what down we first split the residue

386
00:15:55,279 --> 00:15:57,360
set of the input polynomial into two

387
00:15:57,360 --> 00:15:59,839
sets q part and p part

388
00:15:59,839 --> 00:16:01,680
so this is the computational graph after

389
00:16:01,680 --> 00:16:04,079
the split for the p part

390
00:16:04,079 --> 00:16:07,759
we apply int phase conversion entity and

391
00:16:07,759 --> 00:16:10,399
subtract that from the q part

392
00:16:10,399 --> 00:16:13,360
then we scale the output by one over p

393
00:16:13,360 --> 00:16:15,440
in this smalltown fusion we fuse the

394
00:16:15,440 --> 00:16:17,360
three last kernels

395
00:16:17,360 --> 00:16:20,160
like this we fuse the subtraction kernel

396
00:16:20,160 --> 00:16:22,399
and the scaling one with its preceding

397
00:16:22,399 --> 00:16:23,759
entity kernel

398
00:16:23,759 --> 00:16:25,759
because the subtraction and the scaling

399
00:16:25,759 --> 00:16:28,399
are both element wise operation

400
00:16:28,399 --> 00:16:30,639
and they are all memory bound they can

401
00:16:30,639 --> 00:16:36,199
benefit from the kernel fusion

402
00:16:36,480 --> 00:16:39,680
evaluated the intra hd operation fusions

403
00:16:39,680 --> 00:16:41,199
on

404
00:16:41,199 --> 00:16:44,399
a single gpu environment so this shows

405
00:16:44,399 --> 00:16:46,320
the latency breakdown of homomorphic

406
00:16:46,320 --> 00:16:48,959
multiplication and rotation after

407
00:16:48,959 --> 00:16:52,079
applying our techniques

408
00:16:52,079 --> 00:16:55,040
after applying all fusions down to mdf

409
00:16:55,040 --> 00:16:57,839
we get almost 2 times the speed up with

410
00:16:57,839 --> 00:16:59,680
a maximum genome

411
00:16:59,680 --> 00:17:02,560
most of the speedups come from iaf inner

412
00:17:02,560 --> 00:17:05,119
product fusion this is because the key

413
00:17:05,119 --> 00:17:07,199
sizes are extremely large with such

414
00:17:07,199 --> 00:17:09,679
large genome dominating the overall

415
00:17:09,679 --> 00:17:13,280
multiplication and rotation times

416
00:17:13,280 --> 00:17:16,480
and what about small genomes

417
00:17:16,480 --> 00:17:18,880
the confusions become less effective

418
00:17:18,880 --> 00:17:20,799
because the version of the inner product

419
00:17:20,799 --> 00:17:23,280
is significantly decreased although it

420
00:17:23,280 --> 00:17:27,280
gives around 1.5 times of speedups

421
00:17:27,280 --> 00:17:29,440
we can compare our performance results

422
00:17:29,440 --> 00:17:32,080
with that of a prior gpu implementation

423
00:17:32,080 --> 00:17:34,559
which uses maximum denum

424
00:17:34,559 --> 00:17:36,880
after applying all the fusions and by

425
00:17:36,880 --> 00:17:39,039
using smaller genomes we get seven times

426
00:17:39,039 --> 00:17:41,120
the speed of where the prior work

427
00:17:41,120 --> 00:17:43,280
reported around 50 milliseconds of

428
00:17:43,280 --> 00:17:45,679
multiplication time

429
00:17:45,679 --> 00:17:48,480
so these were the results of intra-ig

430
00:17:48,480 --> 00:17:51,039
operation fusions but then what about

431
00:17:51,039 --> 00:17:54,799
internal h operation fusions

432
00:17:55,120 --> 00:17:57,039
we applied inter-achieve operation

433
00:17:57,039 --> 00:17:59,440
fusion in foot strapping before we get

434
00:17:59,440 --> 00:18:01,919
into it we first explain bootstrapping

435
00:18:01,919 --> 00:18:03,919
in the recent ckks

436
00:18:03,919 --> 00:18:06,720
bootstrapping itself is an ac circuit

437
00:18:06,720 --> 00:18:08,880
made up of many hd operations for

438
00:18:08,880 --> 00:18:12,240
detailed algorithm please see the paper

439
00:18:12,240 --> 00:18:14,480
we first show the breakdown of a single

440
00:18:14,480 --> 00:18:18,080
bootstrapping latency on a gpu it's just

441
00:18:18,080 --> 00:18:22,080
that most most of the time is consumed

442
00:18:22,080 --> 00:18:24,160
on the functions called slot coefficient

443
00:18:24,160 --> 00:18:26,080
and coefficient to slot

444
00:18:26,080 --> 00:18:27,600
which are homomorphic linear

445
00:18:27,600 --> 00:18:30,720
transformations taking off of around 60

446
00:18:30,720 --> 00:18:33,760
percent of the time

447
00:18:34,559 --> 00:18:36,559
then how do we compute the horrific

448
00:18:36,559 --> 00:18:38,799
linear transformation in bootstrapping

449
00:18:38,799 --> 00:18:40,400
the linear transformation in

450
00:18:40,400 --> 00:18:42,960
bootstrapping represented as matrix

451
00:18:42,960 --> 00:18:45,280
vector multiplications the vector of

452
00:18:45,280 --> 00:18:48,160
friend is a ciphertext whose message is

453
00:18:48,160 --> 00:18:50,080
a vector of complex numbers

454
00:18:50,080 --> 00:18:52,240
and each matrix operand is a sparse

455
00:18:52,240 --> 00:18:55,280
diagonal matrix where each diagonal is a

456
00:18:55,280 --> 00:18:56,640
plain text

457
00:18:56,640 --> 00:18:58,320
then how do we compute this matrix

458
00:18:58,320 --> 00:19:01,520
vector multiplication

459
00:19:04,080 --> 00:19:06,240
an algorithm called baby step giant step

460
00:19:06,240 --> 00:19:08,720
vsts is used here

461
00:19:08,720 --> 00:19:10,960
so the multiplication between the sparse

462
00:19:10,960 --> 00:19:12,640
diagonal matrix m

463
00:19:12,640 --> 00:19:14,480
and the vector v is shown in this

464
00:19:14,480 --> 00:19:16,000
equation

465
00:19:16,000 --> 00:19:18,559
here the ice diagonal element which is

466
00:19:18,559 --> 00:19:20,080
predetermined

467
00:19:20,080 --> 00:19:21,200
is shown

468
00:19:21,200 --> 00:19:22,080
and

469
00:19:22,080 --> 00:19:24,799
it computes a dot product between

470
00:19:24,799 --> 00:19:27,520
i mean with the input vector rotated by

471
00:19:27,520 --> 00:19:30,160
i which is a ciphertext the baby steps

472
00:19:30,160 --> 00:19:33,520
and step algorithm turns this equation

473
00:19:33,520 --> 00:19:36,000
into another equation having two loops

474
00:19:36,000 --> 00:19:38,720
with loop variables l and k

475
00:19:38,720 --> 00:19:41,679
here the rotated vector is a ciphertext

476
00:19:41,679 --> 00:19:43,919
and the corresponding multiplicand is a

477
00:19:43,919 --> 00:19:47,440
precomputed plain text

478
00:19:47,520 --> 00:19:51,520
so in vsgs by setting both l and k to

479
00:19:51,520 --> 00:19:52,400
around

480
00:19:52,400 --> 00:19:55,280
a root n the number of expensive h

481
00:19:55,280 --> 00:19:57,760
rotations become

482
00:19:57,760 --> 00:20:01,440
o n to all root n

483
00:20:01,760 --> 00:20:04,640
so this is the inter h e of region

484
00:20:04,640 --> 00:20:08,240
fusion also what we call molten ed

485
00:20:08,240 --> 00:20:10,000
batching

486
00:20:10,000 --> 00:20:12,480
so knight multiplication and add

487
00:20:12,480 --> 00:20:15,520
requires multiple memory accesses on the

488
00:20:15,520 --> 00:20:18,640
temporal ciphertext city as shown in the

489
00:20:18,640 --> 00:20:21,840
left blue box

490
00:20:22,640 --> 00:20:25,679
however we can fuse all the kernels in

491
00:20:25,679 --> 00:20:28,400
the blue box into a single kernel on the

492
00:20:28,400 --> 00:20:29,840
bottom side

493
00:20:29,840 --> 00:20:32,799
saving most of the global memory reads

494
00:20:32,799 --> 00:20:38,158
and writes on the temporal ciphertext ct

495
00:20:39,280 --> 00:20:41,760
we also evaluated the optimization in a

496
00:20:41,760 --> 00:20:44,159
firework called hoisting in baby's

497
00:20:44,159 --> 00:20:46,720
design step we rotate a single

498
00:20:46,720 --> 00:20:49,120
ciphertext by multiple rotation indices

499
00:20:49,120 --> 00:20:50,799
as shown in the blue box on the right

500
00:20:50,799 --> 00:20:52,960
side that is we apply different

501
00:20:52,960 --> 00:20:55,600
automotives and then we do multiple key

502
00:20:55,600 --> 00:20:58,080
switches

503
00:20:59,039 --> 00:21:00,960
the hoisting technique changes the

504
00:21:00,960 --> 00:21:03,520
operation order to do model first on the

505
00:21:03,520 --> 00:21:05,919
ciphertext after then

506
00:21:05,919 --> 00:21:07,919
it performs different automations on

507
00:21:07,919 --> 00:21:10,640
that this optimization saves the number

508
00:21:10,640 --> 00:21:14,320
of modups multiple times

509
00:21:15,120 --> 00:21:17,440
so this is the bootstrapping latency on

510
00:21:17,440 --> 00:21:19,919
the cpu after incrementally applying

511
00:21:19,919 --> 00:21:22,080
several optimizations

512
00:21:22,080 --> 00:21:24,720
so before we apply any optimization we

513
00:21:24,720 --> 00:21:27,440
already got a bootstrapping latency

514
00:21:27,440 --> 00:21:29,120
under a second

515
00:21:29,120 --> 00:21:31,760
after applying all the optimizations we

516
00:21:31,760 --> 00:21:34,960
get over 200 times the speed up compared

517
00:21:34,960 --> 00:21:39,039
to a single threaded cpu case and also

518
00:21:39,039 --> 00:21:41,360
we evaluated the effectiveness of our

519
00:21:41,360 --> 00:21:44,400
implementation in the real application

520
00:21:44,400 --> 00:21:46,720
in training a binary classification

521
00:21:46,720 --> 00:21:47,600
model

522
00:21:47,600 --> 00:21:51,039
we get 40 times the speed up compared to

523
00:21:51,039 --> 00:21:55,039
a threaded cpu implementation

524
00:21:55,120 --> 00:21:57,280
finally we propose a metric called

525
00:21:57,280 --> 00:21:59,919
amortized multiplication time

526
00:21:59,919 --> 00:22:02,320
this is the multiplication time also

527
00:22:02,320 --> 00:22:05,120
considering the bootstrapping cost

528
00:22:05,120 --> 00:22:07,919
the metric is the bootstrapping time

529
00:22:07,919 --> 00:22:11,039
plus multiplication times divided by the

530
00:22:11,039 --> 00:22:13,280
number of multiplications available

531
00:22:13,280 --> 00:22:16,399
after bootstrapping

532
00:22:16,559 --> 00:22:18,400
this is the latency of amortized

533
00:22:18,400 --> 00:22:21,280
multiplication time and its boost

534
00:22:21,280 --> 00:22:23,520
stripping overhead both in optimized

535
00:22:23,520 --> 00:22:26,480
version and our baseline implementation

536
00:22:26,480 --> 00:22:29,600
our implementation largely amortized the

537
00:22:29,600 --> 00:22:32,159
memory battle leg especially in large

538
00:22:32,159 --> 00:22:34,559
denum values

539
00:22:34,559 --> 00:22:36,799
however you can still see that most of

540
00:22:36,799 --> 00:22:38,840
the times are spent on

541
00:22:38,840 --> 00:22:41,039
bootstrapping our future work will be

542
00:22:41,039 --> 00:22:43,919
designing a custom hardware that reduces

543
00:22:43,919 --> 00:22:47,919
the bootstrapping cost

544
00:22:48,080 --> 00:22:49,919
so this is the conclusion we

545
00:22:49,919 --> 00:22:52,960
demonstrated a gpu implementation of a

546
00:22:52,960 --> 00:22:55,039
recent ckk scheme

547
00:22:55,039 --> 00:22:57,600
and the first implementation of its

548
00:22:57,600 --> 00:22:59,280
bootstrapping

549
00:22:59,280 --> 00:23:01,039
we found that the memory bottleneck is

550
00:23:01,039 --> 00:23:02,559
the key obstacle

551
00:23:02,559 --> 00:23:06,240
in the gpu implementation and so we

552
00:23:06,240 --> 00:23:08,640
applied a memory sampling optimizations

553
00:23:08,640 --> 00:23:11,200
leading to a large speed up compared to

554
00:23:11,200 --> 00:23:14,320
cp implementations

555
00:23:14,640 --> 00:23:16,640
so this is the reference used in these

556
00:23:16,640 --> 00:23:17,760
slides

557
00:23:17,760 --> 00:23:20,799
please refer to the paper

558
00:23:20,799 --> 00:23:23,840
thank you

