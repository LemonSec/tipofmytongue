1
00:00:01,520 --> 00:00:04,480
hello everyone my name is li chao in

2
00:00:04,480 --> 00:00:06,080
this presentation we will show our

3
00:00:06,080 --> 00:00:08,400
recent research reinforcement learning

4
00:00:08,400 --> 00:00:10,320
for hyper parameter tuning in deep

5
00:00:10,320 --> 00:00:12,480
learning-based site channel analysis

6
00:00:12,480 --> 00:00:14,559
this is joint work with

7
00:00:14,559 --> 00:00:16,239
uri restack

8
00:00:16,239 --> 00:00:19,520
gilham purine and stefan pisac

9
00:00:19,520 --> 00:00:21,359
first let's give a brief introduction

10
00:00:21,359 --> 00:00:23,680
about the sidechain analysis assuming

11
00:00:23,680 --> 00:00:25,840
that we have a hardware running the

12
00:00:25,840 --> 00:00:28,000
encryption algorithm we input plain text

13
00:00:28,000 --> 00:00:29,599
and we can get the ciphertext at the

14
00:00:29,599 --> 00:00:30,720
output

15
00:00:30,720 --> 00:00:32,558
however during this in

16
00:00:32,558 --> 00:00:33,920
executions

17
00:00:33,920 --> 00:00:35,360
we will have the leakage we call it

18
00:00:35,360 --> 00:00:37,680
sidechain leakage it can come from the

19
00:00:37,680 --> 00:00:38,719
power

20
00:00:38,719 --> 00:00:42,000
domain or electromagnetic domain well by

21
00:00:42,000 --> 00:00:44,640
analyzing this leakage

22
00:00:44,640 --> 00:00:47,039
doing some mathematic stuff

23
00:00:47,039 --> 00:00:49,360
we can actually retrieve the secret

24
00:00:49,360 --> 00:00:51,680
information and we call it side channel

25
00:00:51,680 --> 00:00:54,000
analysis

26
00:00:54,000 --> 00:00:56,239
there are two type of

27
00:00:56,239 --> 00:00:58,399
side channel attack um

28
00:00:58,399 --> 00:01:00,160
non-profile sighted attack and flight

29
00:01:00,160 --> 00:01:02,320
cycle side channel attack for the

30
00:01:02,320 --> 00:01:04,479
providing side channel attack

31
00:01:04,479 --> 00:01:06,240
we assume that attackers have the full

32
00:01:06,240 --> 00:01:08,400
control of the device which means that

33
00:01:08,400 --> 00:01:10,640
they can collect both the traces and

34
00:01:10,640 --> 00:01:11,600
labels

35
00:01:11,600 --> 00:01:13,920
during the executions and we call this

36
00:01:13,920 --> 00:01:16,479
providing traces and providing labels

37
00:01:16,479 --> 00:01:20,640
and by collecting these two

38
00:01:20,720 --> 00:01:23,520
data new type of data they can build the

39
00:01:23,520 --> 00:01:25,680
profiling models to map the relationship

40
00:01:25,680 --> 00:01:28,320
between the traces and labels well i

41
00:01:28,320 --> 00:01:29,920
want to know that here are the traces we

42
00:01:29,920 --> 00:01:31,759
mean the leakage traces

43
00:01:31,759 --> 00:01:33,280
so uh

44
00:01:33,280 --> 00:01:34,960
by mapping these two

45
00:01:34,960 --> 00:01:36,799
they can just simply do the attack by

46
00:01:36,799 --> 00:01:39,040
fading the attack traces

47
00:01:39,040 --> 00:01:41,040
obtained from the attack device to the

48
00:01:41,040 --> 00:01:43,040
providing models and ask the providing

49
00:01:43,040 --> 00:01:45,600
models to predict do the predictions

50
00:01:45,600 --> 00:01:47,040
and

51
00:01:47,040 --> 00:01:48,880
when knowing the attack labels they can

52
00:01:48,880 --> 00:01:51,040
do the simple calculations

53
00:01:51,040 --> 00:01:53,280
and the security information that

54
00:01:53,280 --> 00:01:55,680
carried by the leakage can be retrieved

55
00:01:55,680 --> 00:01:58,000
easily

56
00:01:58,000 --> 00:01:59,840
there are two

57
00:01:59,840 --> 00:02:02,159
popular profiling side channel attack

58
00:02:02,159 --> 00:02:04,399
template attack and deep learning attack

59
00:02:04,399 --> 00:02:07,119
but the timely attack is based on

60
00:02:07,119 --> 00:02:09,360
building the probability this is a

61
00:02:09,360 --> 00:02:10,878
function between

62
00:02:10,878 --> 00:02:12,640
different clusters well for the deep

63
00:02:12,640 --> 00:02:14,400
learning attacks it just map the

64
00:02:14,400 --> 00:02:16,720
relationship between their traces and

65
00:02:16,720 --> 00:02:21,120
labels by training a deep learning model

66
00:02:21,120 --> 00:02:23,520
however training essential model is not

67
00:02:23,520 --> 00:02:24,319
that

68
00:02:24,319 --> 00:02:26,560
easy especially designing such a model

69
00:02:26,560 --> 00:02:28,959
is really a difficult task

70
00:02:28,959 --> 00:02:31,040
consider that we have different type of

71
00:02:31,040 --> 00:02:33,120
data set and we want to cut we need to

72
00:02:33,120 --> 00:02:35,200
customize the deployment model for each

73
00:02:35,200 --> 00:02:37,519
data set it will be a really difficult

74
00:02:37,519 --> 00:02:38,879
difficult task

75
00:02:38,879 --> 00:02:41,360
and also we know that there are a lot of

76
00:02:41,360 --> 00:02:44,319
papers that represent the present that

77
00:02:44,319 --> 00:02:47,040
using different networks can

78
00:02:47,040 --> 00:02:49,360
give us better performance than the

79
00:02:49,360 --> 00:02:50,400
other

80
00:02:50,400 --> 00:02:51,599
and

81
00:02:51,599 --> 00:02:53,840
it's really difficult to make the

82
00:02:53,840 --> 00:02:56,080
decision which

83
00:02:56,080 --> 00:02:57,840
type of parameters or which network is

84
00:02:57,840 --> 00:02:58,720
better

85
00:02:58,720 --> 00:03:01,760
so here we list three

86
00:03:01,760 --> 00:03:03,040
type of

87
00:03:03,040 --> 00:03:04,959
sorry category of the hyper parameters

88
00:03:04,959 --> 00:03:07,280
the first one is the network types we

89
00:03:07,280 --> 00:03:08,959
have the multi-layer perceptrons

90
00:03:08,959 --> 00:03:11,760
convolutional neural network and resnet

91
00:03:11,760 --> 00:03:13,680
the layer configurations we have the

92
00:03:13,680 --> 00:03:15,360
convolutional layer pulling layer patch

93
00:03:15,360 --> 00:03:18,480
normalization layer and this layer and

94
00:03:18,480 --> 00:03:20,480
for the training configurations which is

95
00:03:20,480 --> 00:03:22,959
also really important

96
00:03:22,959 --> 00:03:24,879
we have learning rate batch size and

97
00:03:24,879 --> 00:03:26,319
training ebooks

98
00:03:26,319 --> 00:03:27,920
and

99
00:03:27,920 --> 00:03:30,319
as an attacker evaluator it's really

100
00:03:30,319 --> 00:03:32,879
difficult to fund optimal company hyper

101
00:03:32,879 --> 00:03:36,159
parameter combinations of these options

102
00:03:36,159 --> 00:03:38,319
and performs a good lag

103
00:03:38,319 --> 00:03:40,319
and the goal of this paper is to

104
00:03:40,319 --> 00:03:42,560
automate this process to simplify this

105
00:03:42,560 --> 00:03:44,400
process and

106
00:03:44,400 --> 00:03:47,440
the attacker and evaluator can use our

107
00:03:47,440 --> 00:03:50,720
methodologies to apply our attack in a

108
00:03:50,720 --> 00:03:53,519
different data set

109
00:03:53,519 --> 00:03:54,959
and

110
00:03:54,959 --> 00:03:56,879
we use the reinforcement learning more

111
00:03:56,879 --> 00:03:59,360
specifically more specifically the q

112
00:03:59,360 --> 00:04:00,400
learning

113
00:04:00,400 --> 00:04:02,640
note that there are three uh

114
00:04:02,640 --> 00:04:05,599
important notations the state which is s

115
00:04:05,599 --> 00:04:07,840
the actions which is a

116
00:04:07,840 --> 00:04:10,159
and the reward which is r

117
00:04:10,159 --> 00:04:11,920
let's start from the beginning

118
00:04:11,920 --> 00:04:13,519
so the

119
00:04:13,519 --> 00:04:15,439
we fit the state

120
00:04:15,439 --> 00:04:18,160
current state to the action to the agent

121
00:04:18,160 --> 00:04:20,399
and based on the current state the agent

122
00:04:20,399 --> 00:04:23,600
will take an action the action will

123
00:04:23,600 --> 00:04:25,280
influence the environment though the

124
00:04:25,280 --> 00:04:27,840
environment will update to a new state

125
00:04:27,840 --> 00:04:29,680
and also based on the current actions

126
00:04:29,680 --> 00:04:32,240
the environment will give us a reward

127
00:04:32,240 --> 00:04:35,520
and we fit the state the new state and

128
00:04:35,520 --> 00:04:38,240
the new reward to the agent again and we

129
00:04:38,240 --> 00:04:40,479
start this loop by

130
00:04:40,479 --> 00:04:43,199
iterator by this interactive process

131
00:04:43,199 --> 00:04:46,479
between the agent and the environment

132
00:04:46,479 --> 00:04:48,560
the agent can knowing better and better

133
00:04:48,560 --> 00:04:51,600
about the environment and finally at a

134
00:04:51,600 --> 00:04:54,240
certain step the agent can give us the

135
00:04:54,240 --> 00:04:57,520
optimal solutions about

136
00:04:57,520 --> 00:05:01,360
the objective we set it to the agent

137
00:05:01,360 --> 00:05:03,039
here is a simple example of how

138
00:05:03,039 --> 00:05:04,479
q-learning works

139
00:05:04,479 --> 00:05:06,240
so

140
00:05:06,240 --> 00:05:07,840
assuming that the agent is standing in

141
00:05:07,840 --> 00:05:10,080
the middle and the objective is moving

142
00:05:10,080 --> 00:05:12,560
to the right up corner which is a green

143
00:05:12,560 --> 00:05:15,199
block while getting the maximum reward

144
00:05:15,199 --> 00:05:17,280
know that for each block we have the

145
00:05:17,280 --> 00:05:18,880
reward

146
00:05:18,880 --> 00:05:20,240
and

147
00:05:20,240 --> 00:05:22,560
because the agent have no idea what the

148
00:05:22,560 --> 00:05:24,880
environment look like and no idea about

149
00:05:24,880 --> 00:05:26,240
the block

150
00:05:26,240 --> 00:05:28,639
so uh the agent need to explore this

151
00:05:28,639 --> 00:05:30,080
environment

152
00:05:30,080 --> 00:05:32,720
so the initial state

153
00:05:32,720 --> 00:05:34,800
of the agent is the current positions

154
00:05:34,800 --> 00:05:37,199
and the agent take the actions actions

155
00:05:37,199 --> 00:05:38,479
in this

156
00:05:38,479 --> 00:05:40,240
situations could be

157
00:05:40,240 --> 00:05:42,880
moving up moving down moving right or

158
00:05:42,880 --> 00:05:44,720
right or left

159
00:05:44,720 --> 00:05:47,280
and by taking such actions the

160
00:05:47,280 --> 00:05:49,280
environment has changed and

161
00:05:49,280 --> 00:05:51,600
the agent has been have been moved to

162
00:05:51,600 --> 00:05:55,039
the new space a new new uh state

163
00:05:55,039 --> 00:05:57,120
so for example we are moving left and

164
00:05:57,120 --> 00:05:59,680
then the state will be the

165
00:05:59,680 --> 00:06:02,319
blocks on the left and also by moving

166
00:06:02,319 --> 00:06:04,080
left we are

167
00:06:04,080 --> 00:06:06,960
collecting a reward and the agent will

168
00:06:06,960 --> 00:06:08,880
going forward to a different way the

169
00:06:08,880 --> 00:06:11,440
different actions based on the current

170
00:06:11,440 --> 00:06:14,479
state and the reward and then

171
00:06:14,479 --> 00:06:16,000
finish this

172
00:06:16,000 --> 00:06:19,280
iterations and start the next iterations

173
00:06:19,280 --> 00:06:22,639
know that for this um

174
00:06:22,639 --> 00:06:25,440
simple examples we calculate the q value

175
00:06:25,440 --> 00:06:27,680
which then for the qualities why we want

176
00:06:27,680 --> 00:06:29,440
to kill values because we want to

177
00:06:29,440 --> 00:06:30,560
evaluate

178
00:06:30,560 --> 00:06:32,080
the

179
00:06:32,080 --> 00:06:35,199
quality of such state with this uh state

180
00:06:35,199 --> 00:06:37,919
action pair for example if we are moving

181
00:06:37,919 --> 00:06:40,639
right we want to know if the quality of

182
00:06:40,639 --> 00:06:43,039
such movement is good or not

183
00:06:43,039 --> 00:06:45,440
if it's a good moment it's indicate or

184
00:06:45,440 --> 00:06:47,919
it's it's the q value is high is

185
00:06:47,919 --> 00:06:50,639
indicated that

186
00:06:50,639 --> 00:06:53,039
this movement is pretty nice and it may

187
00:06:53,039 --> 00:06:55,840
indicate that we can achieve the our

188
00:06:55,840 --> 00:06:59,039
destination which is a green block and

189
00:06:59,039 --> 00:07:01,680
get a higher reward and if the q value

190
00:07:01,680 --> 00:07:02,720
is low

191
00:07:02,720 --> 00:07:04,400
this means that probably this action is

192
00:07:04,400 --> 00:07:05,919
not that good enough and probably we

193
00:07:05,919 --> 00:07:07,919
want to take different actions

194
00:07:07,919 --> 00:07:10,240
so how to calculate this q value or how

195
00:07:10,240 --> 00:07:12,479
to update this q value because we are

196
00:07:12,479 --> 00:07:14,560
doing this interactive process

197
00:07:14,560 --> 00:07:17,199
here is the equation it seems that it

198
00:07:17,199 --> 00:07:19,840
seems this equation is complex but it's

199
00:07:19,840 --> 00:07:22,080
really simple is this equation is based

200
00:07:22,080 --> 00:07:24,880
on the bill my equations and we have

201
00:07:24,880 --> 00:07:26,639
two

202
00:07:26,639 --> 00:07:29,520
blocks the old q value and learned q

203
00:07:29,520 --> 00:07:30,400
value

204
00:07:30,400 --> 00:07:33,680
the old q values stand for the q value

205
00:07:33,680 --> 00:07:36,319
we learned from the previous

206
00:07:36,319 --> 00:07:37,759
iterations

207
00:07:37,759 --> 00:07:41,120
and the learned q value is a value

208
00:07:41,120 --> 00:07:43,199
we learned from the current step so for

209
00:07:43,199 --> 00:07:45,120
the learning value we have two steps as

210
00:07:45,120 --> 00:07:47,280
we mentioned before the reward

211
00:07:47,280 --> 00:07:50,479
um from the new state and also the

212
00:07:50,479 --> 00:07:52,000
maximum

213
00:07:52,000 --> 00:07:54,000
q value that we can take actions in the

214
00:07:54,000 --> 00:07:56,479
current uh in the new state

215
00:07:56,479 --> 00:07:59,280
so um by uh

216
00:07:59,280 --> 00:08:01,759
uh by balancing this old value and

217
00:08:01,759 --> 00:08:04,080
learned value with the learning rate q

218
00:08:04,080 --> 00:08:07,039
learning rate alpha we can update this q

219
00:08:07,039 --> 00:08:09,919
value and also know that we have this

220
00:08:09,919 --> 00:08:12,639
discount factors which determines the

221
00:08:12,639 --> 00:08:15,120
weight given to the short-term rewards

222
00:08:15,120 --> 00:08:18,479
over the future rewards

223
00:08:18,560 --> 00:08:23,280
please check our papers for more details

224
00:08:23,280 --> 00:08:25,120
so

225
00:08:25,120 --> 00:08:26,639
we uh

226
00:08:26,639 --> 00:08:28,560
demonstrate how the q learning works and

227
00:08:28,560 --> 00:08:31,120
we know how to update the q values we

228
00:08:31,120 --> 00:08:33,120
can simply uh

229
00:08:33,120 --> 00:08:36,958
observe that the um reward here is very

230
00:08:36,958 --> 00:08:38,799
important no matter we are calculating

231
00:08:38,799 --> 00:08:41,679
the new reward new q value or updated q

232
00:08:41,679 --> 00:08:43,440
value this reward plays a really

233
00:08:43,440 --> 00:08:45,040
important role here

234
00:08:45,040 --> 00:08:45,839
and

235
00:08:45,839 --> 00:08:47,680
this remote actually determines the

236
00:08:47,680 --> 00:08:50,320
objective so what we really want to

237
00:08:50,320 --> 00:08:52,560
optimize this reward for the side

238
00:08:52,560 --> 00:08:56,000
channel and here is the reward we are

239
00:08:56,000 --> 00:08:57,839
using

240
00:08:57,839 --> 00:09:00,240
so as you can see here we use

241
00:09:00,240 --> 00:09:03,040
four matrix

242
00:09:03,040 --> 00:09:05,360
and the detailed calculation is shown

243
00:09:05,360 --> 00:09:08,000
here so the t stands for the percentage

244
00:09:08,000 --> 00:09:10,480
of the traces required to get the gas

245
00:09:10,480 --> 00:09:13,360
entropy to zero out of the fixed maximum

246
00:09:13,360 --> 00:09:14,720
attack size

247
00:09:14,720 --> 00:09:17,839
the ge10 stands for the gas entropy

248
00:09:17,839 --> 00:09:19,680
value use 10

249
00:09:19,680 --> 00:09:22,959
of the attack traces the g50 stands for

250
00:09:22,959 --> 00:09:26,240
the gas entropy value using 50 of the

251
00:09:26,240 --> 00:09:27,760
attack traces

252
00:09:27,760 --> 00:09:30,240
and finally we use the accuracy the

253
00:09:30,240 --> 00:09:32,640
accuracy stands for the

254
00:09:32,640 --> 00:09:34,399
accuracy of the network on the

255
00:09:34,399 --> 00:09:36,800
validation set

256
00:09:36,800 --> 00:09:39,360
also we designed a different or

257
00:09:39,360 --> 00:09:41,440
alternative reward function which we

258
00:09:41,440 --> 00:09:43,839
called it reward small the reason we

259
00:09:43,839 --> 00:09:45,920
designed this is because in the recent

260
00:09:45,920 --> 00:09:48,160
paper there are

261
00:09:48,160 --> 00:09:51,040
research showing that the models can be

262
00:09:51,040 --> 00:09:53,040
uh really small while performing is

263
00:09:53,040 --> 00:09:56,880
really good so uh we also want our uh q

264
00:09:56,880 --> 00:09:59,200
learning process to found this uh small

265
00:09:59,200 --> 00:10:02,480
but good models so uh by doing so by

266
00:10:02,480 --> 00:10:04,399
doing this

267
00:10:04,399 --> 00:10:06,800
we are design

268
00:10:06,800 --> 00:10:08,720
additional metric

269
00:10:08,720 --> 00:10:11,920
which is the p value which stands for

270
00:10:11,920 --> 00:10:13,920
the trainable parameters we set the

271
00:10:13,920 --> 00:10:16,000
strength threshold of the trainable

272
00:10:16,000 --> 00:10:17,519
parameters based on the state of art

273
00:10:17,519 --> 00:10:18,399
models

274
00:10:18,399 --> 00:10:20,880
and then we subtract the state of our

275
00:10:20,880 --> 00:10:23,120
the state of our training parameters

276
00:10:23,120 --> 00:10:24,640
with our

277
00:10:24,640 --> 00:10:27,120
selected network topologies

278
00:10:27,120 --> 00:10:29,360
higher p-value means that our model is

279
00:10:29,360 --> 00:10:32,560
smaller gives us the give gives gives

280
00:10:32,560 --> 00:10:34,959
add value added value to the reward and

281
00:10:34,959 --> 00:10:37,600
the reward if the remote smart is higher

282
00:10:37,600 --> 00:10:40,000
means that we can found

283
00:10:40,000 --> 00:10:44,839
good performed and small models

284
00:10:46,079 --> 00:10:48,720
then we are by by knowing all of this

285
00:10:48,720 --> 00:10:50,880
background we are showing the

286
00:10:50,880 --> 00:10:53,519
network searching method we start by

287
00:10:53,519 --> 00:10:56,320
sample the network topology and we train

288
00:10:56,320 --> 00:10:58,800
the network and finally we evaluate

289
00:10:58,800 --> 00:11:01,440
the model and update the q value

290
00:11:01,440 --> 00:11:04,079
we are doing this process again and

291
00:11:04,079 --> 00:11:06,240
again base with the epsilon graded

292
00:11:06,240 --> 00:11:09,279
schedule the epsilon grade schedule here

293
00:11:09,279 --> 00:11:12,800
balance the exploration and exploitation

294
00:11:12,800 --> 00:11:13,600
we

295
00:11:13,600 --> 00:11:15,519
in the beginning of the search with that

296
00:11:15,519 --> 00:11:18,480
epsilon to a higher value so the sampled

297
00:11:18,480 --> 00:11:22,000
network topology will be random and the

298
00:11:22,000 --> 00:11:25,040
agent can better explore the searching

299
00:11:25,040 --> 00:11:25,920
space

300
00:11:25,920 --> 00:11:27,040
and then

301
00:11:27,040 --> 00:11:28,959
when the agent knowing the evaluator

302
00:11:28,959 --> 00:11:29,920
better

303
00:11:29,920 --> 00:11:31,120
knowing the

304
00:11:31,120 --> 00:11:33,920
environment better

305
00:11:33,920 --> 00:11:37,760
we gradually decrease this epsilon value

306
00:11:37,760 --> 00:11:39,200
and then the

307
00:11:39,200 --> 00:11:41,360
agent is moving from exploration to

308
00:11:41,360 --> 00:11:44,560
exploitation and the agent will tend to

309
00:11:44,560 --> 00:11:46,000
select

310
00:11:46,000 --> 00:11:48,800
the topologies that gives higher q value

311
00:11:48,800 --> 00:11:51,279
or higher reward

312
00:11:51,279 --> 00:11:54,240
also for sampling the network topologies

313
00:11:54,240 --> 00:11:57,360
we customize

314
00:11:57,360 --> 00:11:58,720
the the

315
00:11:58,720 --> 00:12:01,440
or we set the restrictions on the design

316
00:12:01,440 --> 00:12:04,079
the network first we set the searching

317
00:12:04,079 --> 00:12:07,600
range and also we limit the

318
00:12:07,600 --> 00:12:10,560
number limit the the selected the

319
00:12:10,560 --> 00:12:12,720
selectable uh

320
00:12:12,720 --> 00:12:14,399
layers for example here we have the

321
00:12:14,399 --> 00:12:16,480
convolutional layer pulling layer with

322
00:12:16,480 --> 00:12:18,399
the connect layer software

323
00:12:18,399 --> 00:12:19,680
and also

324
00:12:19,680 --> 00:12:21,760
we have this patch normalization layer

325
00:12:21,760 --> 00:12:24,079
and gap layer note that these two layers

326
00:12:24,079 --> 00:12:26,800
are shown in a later late recent

327
00:12:26,800 --> 00:12:29,040
research and it performs really good

328
00:12:29,040 --> 00:12:32,560
and so we are still add this to to the

329
00:12:32,560 --> 00:12:34,399
our research

330
00:12:34,399 --> 00:12:37,279
or to our searching space and

331
00:12:37,279 --> 00:12:39,680
probably it will give us a good

332
00:12:39,680 --> 00:12:41,519
uh models

333
00:12:41,519 --> 00:12:44,480
so um we also said as i mentioned before

334
00:12:44,480 --> 00:12:46,480
we set the restrictions to the models

335
00:12:46,480 --> 00:12:47,680
for example

336
00:12:47,680 --> 00:12:50,240
for the fully connected layer we set

337
00:12:50,240 --> 00:12:52,720
we reset the threshold so the number of

338
00:12:52,720 --> 00:12:55,040
the fully connect layer cannot exceed

339
00:12:55,040 --> 00:12:56,639
our threshold

340
00:12:56,639 --> 00:12:59,279
for more details please check our papers

341
00:12:59,279 --> 00:13:02,560
so compare with the normal

342
00:13:02,560 --> 00:13:04,800
network design methodologies our

343
00:13:04,800 --> 00:13:06,880
searching methodologies give small

344
00:13:06,880 --> 00:13:09,600
possibilities on different

345
00:13:09,600 --> 00:13:12,320
type of layers type of configurations

346
00:13:12,320 --> 00:13:14,320
and probably it will give us a better

347
00:13:14,320 --> 00:13:16,160
result we will show that in an

348
00:13:16,160 --> 00:13:19,360
experimental result part

349
00:13:19,760 --> 00:13:23,120
so here um we are experienced our where

350
00:13:23,120 --> 00:13:25,440
we test our

351
00:13:25,440 --> 00:13:28,240
framework or our searching method uh

352
00:13:28,240 --> 00:13:30,240
with three public available data set

353
00:13:30,240 --> 00:13:32,720
ascot fixed key data set as random key

354
00:13:32,720 --> 00:13:36,399
data set and chess ctf for each dataset

355
00:13:36,399 --> 00:13:39,120
we are showing the certain overview the

356
00:13:39,120 --> 00:13:41,279
benchmark with the state of art models

357
00:13:41,279 --> 00:13:44,320
reward and gets into b

358
00:13:44,320 --> 00:13:46,560
this is the result for the ascot fixed

359
00:13:46,560 --> 00:13:48,079
key set

360
00:13:48,079 --> 00:13:49,839
on the right on the left it's showing

361
00:13:49,839 --> 00:13:52,240
the certain overviews

362
00:13:52,240 --> 00:13:54,800
first we notice that we are experienced

363
00:13:54,800 --> 00:13:56,560
different leakage models hanging with

364
00:13:56,560 --> 00:13:59,120
leakage model and identity leakage model

365
00:13:59,120 --> 00:14:01,360
and also we test different reward

366
00:14:01,360 --> 00:14:03,279
functions

367
00:14:03,279 --> 00:14:06,880
to understand this graph the x-axis

368
00:14:06,880 --> 00:14:09,040
stands for the q learning reward the

369
00:14:09,040 --> 00:14:12,079
higher the better and the y-axis stands

370
00:14:12,079 --> 00:14:13,360
for the

371
00:14:13,360 --> 00:14:15,760
trainable parameters of each model the

372
00:14:15,760 --> 00:14:17,120
smaller the better

373
00:14:17,120 --> 00:14:18,160
and

374
00:14:18,160 --> 00:14:20,800
the dot the dot in the

375
00:14:20,800 --> 00:14:23,600
graph represents the topologies of the

376
00:14:23,600 --> 00:14:25,360
models and you can say there are a lot

377
00:14:25,360 --> 00:14:26,160
of

378
00:14:26,160 --> 00:14:28,000
that means that we have tests a lot of

379
00:14:28,000 --> 00:14:29,279
different

380
00:14:29,279 --> 00:14:30,880
topologies

381
00:14:30,880 --> 00:14:31,760
and

382
00:14:31,760 --> 00:14:34,959
we know that in a in a in the dot have

383
00:14:34,959 --> 00:14:37,040
different color the color stands for the

384
00:14:37,040 --> 00:14:39,519
different different abstract values that

385
00:14:39,519 --> 00:14:40,560
we are

386
00:14:40,560 --> 00:14:42,959
in a different searching stage

387
00:14:42,959 --> 00:14:44,959
like for example exploration

388
00:14:44,959 --> 00:14:46,800
exploitation

389
00:14:46,800 --> 00:14:48,880
and the red cross here stands for the

390
00:14:48,880 --> 00:14:51,440
state of our models and we are directly

391
00:14:51,440 --> 00:14:53,680
benchmark the state of our models with

392
00:14:53,680 --> 00:14:56,240
other models within our reinforcement

393
00:14:56,240 --> 00:14:59,680
learning framework so from the result we

394
00:14:59,680 --> 00:15:01,519
can see that

395
00:15:01,519 --> 00:15:04,240
the model on the right down corner

396
00:15:04,240 --> 00:15:06,560
performs better than the state bar model

397
00:15:06,560 --> 00:15:08,560
and although it's also it's smaller than

398
00:15:08,560 --> 00:15:11,360
state-of-the-art models so uh both for

399
00:15:11,360 --> 00:15:13,680
both reward and reward small reward

400
00:15:13,680 --> 00:15:16,079
functions it we can get central models

401
00:15:16,079 --> 00:15:17,760
and especially for the remote small

402
00:15:17,760 --> 00:15:21,040
functions we can get plenty of such

403
00:15:21,040 --> 00:15:22,000
of this

404
00:15:22,000 --> 00:15:24,079
best performance models

405
00:15:24,079 --> 00:15:27,040
some to do benchmark our uh

406
00:15:27,040 --> 00:15:28,959
best models with the state-of-the-art

407
00:15:28,959 --> 00:15:30,720
research result

408
00:15:30,720 --> 00:15:34,240
we have the tables on the right so um

409
00:15:34,240 --> 00:15:36,720
here we highlight our result here

410
00:15:36,720 --> 00:15:39,199
our best model performance of our best

411
00:15:39,199 --> 00:15:42,320
model here so clearly our

412
00:15:42,320 --> 00:15:44,320
model is way smaller than the state of

413
00:15:44,320 --> 00:15:47,199
art and performs better than it

414
00:15:47,199 --> 00:15:48,800
so it shows

415
00:15:48,800 --> 00:15:52,800
the efficiency of our framework

416
00:15:52,800 --> 00:15:55,600
also uh to evaluate our searching

417
00:15:55,600 --> 00:15:58,800
process we also calculate the reward and

418
00:15:58,800 --> 00:16:01,360
the gas entropy of the best models for

419
00:16:01,360 --> 00:16:05,040
the reward we consider two terms the

420
00:16:05,040 --> 00:16:08,959
reload resulting average reward and the

421
00:16:08,959 --> 00:16:12,160
average reward per epsilon so for the

422
00:16:12,160 --> 00:16:13,839
rolling average reward

423
00:16:13,839 --> 00:16:14,560
we

424
00:16:14,560 --> 00:16:16,959
calculate the average reward over

425
00:16:16,959 --> 00:16:19,360
from the previous 50 iterations and for

426
00:16:19,360 --> 00:16:21,040
the average ipson

427
00:16:21,040 --> 00:16:22,639
and for the absolute

428
00:16:22,639 --> 00:16:25,440
average reward per installment simply

429
00:16:25,440 --> 00:16:27,199
calculate the average

430
00:16:27,199 --> 00:16:30,320
reward for all of the models with

431
00:16:30,320 --> 00:16:33,199
specific epsilon value

432
00:16:33,199 --> 00:16:36,240
so here from the

433
00:16:36,240 --> 00:16:38,480
plot on the left we

434
00:16:38,480 --> 00:16:39,839
see that

435
00:16:39,839 --> 00:16:42,240
for both cases with different reward

436
00:16:42,240 --> 00:16:44,560
functions the

437
00:16:44,560 --> 00:16:47,600
reward is getting higher and higher when

438
00:16:47,600 --> 00:16:48,880
the

439
00:16:48,880 --> 00:16:50,959
epsilon is getting lower

440
00:16:50,959 --> 00:16:53,040
which means that our agent is really

441
00:16:53,040 --> 00:16:55,040
learning from the environment and when

442
00:16:55,040 --> 00:16:57,920
moving from exploration to exploitations

443
00:16:57,920 --> 00:17:00,160
it can really generate the models that

444
00:17:00,160 --> 00:17:01,680
performs really good

445
00:17:01,680 --> 00:17:04,079
performs well on the data set we are

446
00:17:04,079 --> 00:17:05,119
testing

447
00:17:05,119 --> 00:17:07,520
and also on the right of the plot we are

448
00:17:07,520 --> 00:17:10,079
showing the

449
00:17:10,240 --> 00:17:12,959
guessing entropy of the best models from

450
00:17:12,959 --> 00:17:15,439
the search and the result is pretty

451
00:17:15,439 --> 00:17:17,520
consistent to what we observed from the

452
00:17:17,520 --> 00:17:21,119
searching plan in the previous slide

453
00:17:21,119 --> 00:17:23,439
next we are showing the ascot random key

454
00:17:23,439 --> 00:17:25,599
data set note that here we do not have

455
00:17:25,599 --> 00:17:28,160
the red cross with stanford state of art

456
00:17:28,160 --> 00:17:31,120
however we still benchmark um

457
00:17:31,120 --> 00:17:33,440
the reported result from the other

458
00:17:33,440 --> 00:17:35,520
papers with our result

459
00:17:35,520 --> 00:17:37,760
um the searching process is kind of

460
00:17:37,760 --> 00:17:40,320
similar but the results still showing

461
00:17:40,320 --> 00:17:42,640
that we can found a good models

462
00:17:42,640 --> 00:17:44,640
really

463
00:17:44,640 --> 00:17:47,280
well performed model but the model is

464
00:17:47,280 --> 00:17:50,960
still with limited size so pretty

465
00:17:50,960 --> 00:17:53,760
amazing result

466
00:17:53,760 --> 00:17:56,160
and also one we're showing the reward

467
00:17:56,160 --> 00:17:58,559
which is a reward similar to the

468
00:17:58,559 --> 00:17:59,679
previous

469
00:17:59,679 --> 00:18:00,880
data set

470
00:18:00,880 --> 00:18:02,160
so here

471
00:18:02,160 --> 00:18:04,799
we observe that for the

472
00:18:04,799 --> 00:18:07,039
template leakage model the reward is

473
00:18:07,039 --> 00:18:08,880
going higher when the epsilon is

474
00:18:08,880 --> 00:18:12,240
decreased while with id model it's

475
00:18:12,240 --> 00:18:14,640
it's it's it's going from diff to the

476
00:18:14,640 --> 00:18:16,000
different directions

477
00:18:16,000 --> 00:18:18,880
so meaning that when the

478
00:18:18,880 --> 00:18:22,559
uh reward is getting uh getting smaller

479
00:18:22,559 --> 00:18:25,360
and the epsilon is getting larger

480
00:18:25,360 --> 00:18:27,919
so uh we assume that this is because

481
00:18:27,919 --> 00:18:30,480
that model our searching for framework

482
00:18:30,480 --> 00:18:33,200
is stuck into the local optima and by

483
00:18:33,200 --> 00:18:34,880
more searching the reward is getting

484
00:18:34,880 --> 00:18:36,240
smaller

485
00:18:36,240 --> 00:18:38,799
however our solution would be that

486
00:18:38,799 --> 00:18:40,320
fitting the

487
00:18:40,320 --> 00:18:42,880
fitting more providing traces when doing

488
00:18:42,880 --> 00:18:45,200
the searching all the urinations

489
00:18:45,200 --> 00:18:46,799
and the property it will solve the

490
00:18:46,799 --> 00:18:48,640
problems

491
00:18:48,640 --> 00:18:50,400
and also we're showing the gas entropy

492
00:18:50,400 --> 00:18:52,080
here again that performs really

493
00:18:52,080 --> 00:18:53,840
consistent

494
00:18:53,840 --> 00:18:55,760
compared what we uh are

495
00:18:55,760 --> 00:18:57,760
compared to what we

496
00:18:57,760 --> 00:19:00,559
observed before

497
00:19:00,559 --> 00:19:02,880
finally we show the chest ctf data set

498
00:19:02,880 --> 00:19:04,559
here we only show the hem with leakage

499
00:19:04,559 --> 00:19:07,039
model because this data set do not leak

500
00:19:07,039 --> 00:19:09,360
leak much id

501
00:19:09,360 --> 00:19:11,919
leakage so uh

502
00:19:11,919 --> 00:19:14,880
from the result uh the uh

503
00:19:14,880 --> 00:19:17,360
for both certain result and the reward

504
00:19:17,360 --> 00:19:18,080
is

505
00:19:18,080 --> 00:19:19,840
pretty uh

506
00:19:19,840 --> 00:19:22,320
normal well as expected because his

507
00:19:22,320 --> 00:19:24,080
reward is getting higher and when the

508
00:19:24,080 --> 00:19:26,080
epsilon is getting smaller we are

509
00:19:26,080 --> 00:19:27,919
finding better models

510
00:19:27,919 --> 00:19:30,320
and uh for the result

511
00:19:30,320 --> 00:19:34,000
our uh best thing and best and for the

512
00:19:34,000 --> 00:19:36,080
with the reverse small models

513
00:19:36,080 --> 00:19:38,400
our model is the smallest well performs

514
00:19:38,400 --> 00:19:39,679
the best

515
00:19:39,679 --> 00:19:42,880
and the gas entropy confirms our

516
00:19:42,880 --> 00:19:45,520
our statement

517
00:19:45,520 --> 00:19:46,880
okay

518
00:19:46,880 --> 00:19:48,320
let's move to the conclusion and the

519
00:19:48,320 --> 00:19:50,160
future works

520
00:19:50,160 --> 00:19:51,760
in this paper we propose the

521
00:19:51,760 --> 00:19:53,050
reinforcement learning

522
00:19:53,050 --> 00:19:54,080
[Music]

523
00:19:54,080 --> 00:19:56,320
framework that enables automate

524
00:19:56,320 --> 00:19:58,400
automated and powerful search for

525
00:19:58,400 --> 00:19:59,600
students

526
00:19:59,600 --> 00:20:01,520
for the providing side channel

527
00:20:01,520 --> 00:20:02,799
analysis

528
00:20:02,799 --> 00:20:06,080
we motivate and develop customer reward

529
00:20:06,080 --> 00:20:07,919
functions for the hyperparameter tuning

530
00:20:07,919 --> 00:20:10,480
in side channel and may demonstrate the

531
00:20:10,480 --> 00:20:12,480
effectiveness of our method with

532
00:20:12,480 --> 00:20:14,400
different data sets

533
00:20:14,400 --> 00:20:16,799
for future work it would be interesting

534
00:20:16,799 --> 00:20:20,000
to investigate the q learning paradigm

535
00:20:20,000 --> 00:20:21,120
performance

536
00:20:21,120 --> 00:20:23,200
and we would like to know how

537
00:20:23,200 --> 00:20:24,960
the best model obtained through the

538
00:20:24,960 --> 00:20:27,760
reinforcement learning behaves in the

539
00:20:27,760 --> 00:20:31,679
examples of models and also we want to

540
00:20:31,679 --> 00:20:33,520
learn better about our searching

541
00:20:33,520 --> 00:20:35,600
framework

542
00:20:35,600 --> 00:20:37,919
in when searching for other type of

543
00:20:37,919 --> 00:20:39,679
network for example the multi-layer

544
00:20:39,679 --> 00:20:41,360
perceptron

545
00:20:41,360 --> 00:20:44,918
thanks for your attention

