1
00:00:00,080 --> 00:00:02,960
bonjour i'm gabrielle i'm a phd student

2
00:00:02,960 --> 00:00:04,640
from the revolution laboratory in the

3
00:00:04,640 --> 00:00:07,440
teres itself and today i will propose

4
00:00:07,440 --> 00:00:10,000
you a new loss function that maximizes

5
00:00:10,000 --> 00:00:12,080
the success rate of a neural network

6
00:00:12,080 --> 00:00:14,320
trained to perform sidechain attacks

7
00:00:14,320 --> 00:00:15,839
this work is a joint work with lady

8
00:00:15,839 --> 00:00:17,920
auburn america from the revoking

9
00:00:17,920 --> 00:00:20,000
laboratory and alexandre nelly and

10
00:00:20,000 --> 00:00:21,840
francoise sons from many specimen

11
00:00:21,840 --> 00:00:24,320
conductors but first what's the point

12
00:00:24,320 --> 00:00:25,920
with psychiatric attacks

13
00:00:25,920 --> 00:00:28,080
so to understand this concept let's take

14
00:00:28,080 --> 00:00:30,320
a look at an example

15
00:00:30,320 --> 00:00:32,159
so let's assume that an adversary has

16
00:00:32,159 --> 00:00:34,480
access to a physical device in which a

17
00:00:34,480 --> 00:00:37,040
cryptographic algorithm like for example

18
00:00:37,040 --> 00:00:40,000
ies is implemented and such that she can

19
00:00:40,000 --> 00:00:41,840
configure the secret key and the plain

20
00:00:41,840 --> 00:00:44,079
text to perform an encryption such that

21
00:00:44,079 --> 00:00:46,480
obviously the goal of the advertory is

22
00:00:46,480 --> 00:00:48,719
to recover the sensitive information

23
00:00:48,719 --> 00:00:51,280
manipulated during its operation

24
00:00:51,280 --> 00:00:53,920
so to perform a sidechaining attack she

25
00:00:53,920 --> 00:00:56,879
needs one probe here an om probe and one

26
00:00:56,879 --> 00:00:59,440
oscilloscope at least such that

27
00:00:59,440 --> 00:01:02,320
during the encryption she can capture a

28
00:01:02,320 --> 00:01:04,640
physical trust that directly depends on

29
00:01:04,640 --> 00:01:06,799
the sensitive information manipulated by

30
00:01:06,799 --> 00:01:08,640
the cryptographic module

31
00:01:08,640 --> 00:01:11,439
and because she can perform as many

32
00:01:11,439 --> 00:01:14,320
encryption she wants the other authority

33
00:01:14,320 --> 00:01:16,640
can generate them plenty of physical

34
00:01:16,640 --> 00:01:18,880
trust and thus find some equalities

35
00:01:18,880 --> 00:01:20,960
related to the secret she wants to

36
00:01:20,960 --> 00:01:22,720
retrieve

37
00:01:22,720 --> 00:01:24,000
in the deep learning based side

38
00:01:24,000 --> 00:01:25,920
challenge attacks the adversary

39
00:01:25,920 --> 00:01:27,840
generates a nerve network to

40
00:01:27,840 --> 00:01:30,240
automatically match a physical trap with

41
00:01:30,240 --> 00:01:32,560
the correct target sensitive value such

42
00:01:32,560 --> 00:01:35,360
that this attack can be decomposed into

43
00:01:35,360 --> 00:01:37,520
two parts a profiling phase and an

44
00:01:37,520 --> 00:01:39,119
attack phase

45
00:01:39,119 --> 00:01:41,759
so first in the profiling phase the

46
00:01:41,759 --> 00:01:44,479
adversary trains a neural network to

47
00:01:44,479 --> 00:01:45,840
predict the correct sensitive

48
00:01:45,840 --> 00:01:47,759
information she knows based on the

49
00:01:47,759 --> 00:01:50,399
physical traces generated in the

50
00:01:50,399 --> 00:01:53,799
previous slide

51
00:01:54,720 --> 00:01:56,719
so when this phase is performed the

52
00:01:56,719 --> 00:01:59,520
adversary can predict the intermediate

53
00:01:59,520 --> 00:02:01,759
variable on a target device containing a

54
00:02:01,759 --> 00:02:04,399
secret she wants to retrieve so during

55
00:02:04,399 --> 00:02:06,479
the attack phase by using a new set of

56
00:02:06,479 --> 00:02:09,038
physical trusses she can compute the

57
00:02:09,038 --> 00:02:11,760
probability of observing each key

58
00:02:11,760 --> 00:02:14,879
protesis but unfortunately one physical

59
00:02:14,879 --> 00:02:17,200
truss is not enough to retrieve the

60
00:02:17,200 --> 00:02:18,959
correct targeted value

61
00:02:18,959 --> 00:02:21,040
hence the adversary has to predict the

62
00:02:21,040 --> 00:02:22,480
score related

63
00:02:22,480 --> 00:02:25,760
to each key partici multiple times

64
00:02:25,760 --> 00:02:29,280
to finally aggregate all the predictions

65
00:02:29,280 --> 00:02:32,080
and recover the secret key value

66
00:02:32,080 --> 00:02:34,480
so one way to evaluate the efficiency of

67
00:02:34,480 --> 00:02:36,319
a neural network is to assess the

68
00:02:36,319 --> 00:02:39,040
benefits of the applied loss function to

69
00:02:39,040 --> 00:02:41,840
optimize the attack performance

70
00:02:41,840 --> 00:02:42,640
but

71
00:02:42,640 --> 00:02:44,480
what is the classical learning metric

72
00:02:44,480 --> 00:02:46,319
used in deep learning-based side-channel

73
00:02:46,319 --> 00:02:47,680
attacks

74
00:02:47,680 --> 00:02:50,000
so the most widely used loss function is

75
00:02:50,000 --> 00:02:52,160
called cross-entropy and its function is

76
00:02:52,160 --> 00:02:53,760
reminded here

77
00:02:53,760 --> 00:02:55,920
so that to get a clear overview of its

78
00:02:55,920 --> 00:02:58,239
impact during the training process let's

79
00:02:58,239 --> 00:03:00,720
back to our previous example

80
00:03:00,720 --> 00:03:03,200
so here the adversary generates a set of

81
00:03:03,200 --> 00:03:05,680
physical trusses such that she knows the

82
00:03:05,680 --> 00:03:08,080
target its value manipulated by the

83
00:03:08,080 --> 00:03:09,599
device under test

84
00:03:09,599 --> 00:03:11,760
so during the profiling phase the neural

85
00:03:11,760 --> 00:03:14,879
network output a set of score related to

86
00:03:14,879 --> 00:03:17,680
each hypothetical value such that the i

87
00:03:17,680 --> 00:03:20,000
test its core the item the probability

88
00:03:20,000 --> 00:03:22,480
of observing the related value but

89
00:03:22,480 --> 00:03:24,000
however to fit with the maximum

90
00:03:24,000 --> 00:03:26,799
likelihood distinguisher the scores has

91
00:03:26,799 --> 00:03:28,239
to be converted to a probability

92
00:03:28,239 --> 00:03:30,080
distribution

93
00:03:30,080 --> 00:03:32,400
thus an additional function called

94
00:03:32,400 --> 00:03:34,959
softmax function is applied in order to

95
00:03:34,959 --> 00:03:36,959
convert the scores to a normalized

96
00:03:36,959 --> 00:03:40,159
probability distribution

97
00:03:40,319 --> 00:03:41,760
so through the minimization of the

98
00:03:41,760 --> 00:03:43,519
cross-entropy loss function the

99
00:03:43,519 --> 00:03:46,000
adversary asks the neural network to

100
00:03:46,000 --> 00:03:48,400
maximize the probability of observing

101
00:03:48,400 --> 00:03:50,959
the correct output with respect to each

102
00:03:50,959 --> 00:03:54,239
hypothetical values

103
00:03:57,519 --> 00:03:59,680
thus minimizing the ranking loss

104
00:03:59,680 --> 00:04:01,840
functions seems treated to conduct

105
00:04:01,840 --> 00:04:04,640
sidechain attacks but concretely

106
00:04:04,640 --> 00:04:06,400
how should the loss function be

107
00:04:06,400 --> 00:04:08,319
minimized so to perform such

108
00:04:08,319 --> 00:04:10,799
minimization the adversary employs some

109
00:04:10,799 --> 00:04:12,640
optimization algorithm based on the

110
00:04:12,640 --> 00:04:14,640
gradient descent principle

111
00:04:14,640 --> 00:04:17,680
this technique consists in iteratively

112
00:04:17,680 --> 00:04:20,160
updating the trainable parameters theta

113
00:04:20,160 --> 00:04:22,000
of the neural network based on the

114
00:04:22,000 --> 00:04:24,000
gradient of the loss function

115
00:04:24,000 --> 00:04:26,000
so concretely given a neural network

116
00:04:26,000 --> 00:04:29,280
with two parameters theta 0 and theta 1

117
00:04:29,280 --> 00:04:31,360
we can plot the evolution of the cross

118
00:04:31,360 --> 00:04:33,280
entropy as function depending on the

119
00:04:33,280 --> 00:04:36,000
value of both stringable parameters so

120
00:04:36,000 --> 00:04:37,840
through this plot we can see that

121
00:04:37,840 --> 00:04:40,400
depending on the initialization of zero

122
00:04:40,400 --> 00:04:43,199
and theta one the optimization algorithm

123
00:04:43,199 --> 00:04:44,240
can reach

124
00:04:44,240 --> 00:04:47,280
local minimum

125
00:04:48,560 --> 00:04:50,720
or in the best case scenario it can

126
00:04:50,720 --> 00:04:54,000
reach a global minimum

127
00:04:54,880 --> 00:04:56,240
so to obtain the most efficient

128
00:04:56,240 --> 00:04:58,960
parametric model f theta the adversary

129
00:04:58,960 --> 00:05:00,960
expect the gradient descent algorithm to

130
00:05:00,960 --> 00:05:02,400
converse throughout the global minimum

131
00:05:02,400 --> 00:05:04,720
but unfortunately this solution cannot

132
00:05:04,720 --> 00:05:06,880
beat ensure and in this talk the

133
00:05:06,880 --> 00:05:09,440
resulting error is called optimization

134
00:05:09,440 --> 00:05:10,560
error

135
00:05:10,560 --> 00:05:12,560
more formally it characterizes all the

136
00:05:12,560 --> 00:05:14,960
errors induced by the learning process

137
00:05:14,960 --> 00:05:16,639
from the selection of definite model

138
00:05:16,639 --> 00:05:18,560
space to the error induced by the

139
00:05:18,560 --> 00:05:20,479
optimization algorithm

140
00:05:20,479 --> 00:05:23,199
so to obtain the most efficient model

141
00:05:23,199 --> 00:05:25,360
the goal of the adversary is to find the

142
00:05:25,360 --> 00:05:27,600
solution that maximizes the success rate

143
00:05:27,600 --> 00:05:29,280
metric for a minimum number of attack

144
00:05:29,280 --> 00:05:31,199
traces thus the gradient descent

145
00:05:31,199 --> 00:05:32,880
algorithm should update the trainable

146
00:05:32,880 --> 00:05:35,120
parameter theta in order to penalize the

147
00:05:35,120 --> 00:05:38,160
errors provided on the success rate so

148
00:05:38,160 --> 00:05:39,919
let's see if the penalization term

149
00:05:39,919 --> 00:05:41,440
induced by the cross entropy less

150
00:05:41,440 --> 00:05:44,000
function perfectly fits with this

151
00:05:44,000 --> 00:05:45,840
maximization problem

152
00:05:45,840 --> 00:05:48,560
so here two scenarios can be observed

153
00:05:48,560 --> 00:05:50,880
first if the derivative is completed

154
00:05:50,880 --> 00:05:53,280
with respect to the targeted class then

155
00:05:53,280 --> 00:05:56,000
the penalization term should reflect the

156
00:05:56,000 --> 00:05:58,720
impact of each irrelevant score on the

157
00:05:58,720 --> 00:06:01,039
correct output and this is exactly what

158
00:06:01,039 --> 00:06:03,039
the cross-entropy does at it penalize

159
00:06:03,039 --> 00:06:04,800
the related output with respect to the

160
00:06:04,800 --> 00:06:07,759
probability r then if the derivative is

161
00:06:07,759 --> 00:06:10,000
computed with respect to the untargeted

162
00:06:10,000 --> 00:06:12,720
class the penalization should be only

163
00:06:12,720 --> 00:06:15,360
dependent on the score's distance

164
00:06:15,360 --> 00:06:19,280
between j and k stars such that indeed

165
00:06:19,280 --> 00:06:21,440
following the success rate matrix the

166
00:06:21,440 --> 00:06:24,479
adversary aims at maximizing the score

167
00:06:24,479 --> 00:06:27,360
of k-stars with respect to each wrong

168
00:06:27,360 --> 00:06:30,960
hypothetical values but unfortunately as

169
00:06:30,960 --> 00:06:33,600
illustrated in this slide the derivative

170
00:06:33,600 --> 00:06:35,600
considered by the cross-entropy loss

171
00:06:35,600 --> 00:06:38,000
function does not perfectly reflect the

172
00:06:38,000 --> 00:06:40,560
relative order of the untargeted class

173
00:06:40,560 --> 00:06:42,639
with respect to the targeted one

174
00:06:42,639 --> 00:06:44,319
indeed the cross entropy combined with

175
00:06:44,319 --> 00:06:46,639
the softmax function carries more about

176
00:06:46,639 --> 00:06:48,639
the absolute value of a class with

177
00:06:48,639 --> 00:06:51,120
respect to each other which can induce

178
00:06:51,120 --> 00:06:52,960
blurred results regarding the

179
00:06:52,960 --> 00:06:55,440
optimization of the success rate and

180
00:06:55,440 --> 00:06:57,680
because the wrong penalization of the

181
00:06:57,680 --> 00:07:00,000
class j impacts the probability of the

182
00:07:00,000 --> 00:07:02,479
targeted output we suggest that the

183
00:07:02,479 --> 00:07:04,400
cross entropy does not totally reflect

184
00:07:04,400 --> 00:07:05,759
the probability distribution the

185
00:07:05,759 --> 00:07:08,240
adversary wants to optimize during the

186
00:07:08,240 --> 00:07:11,120
training process in the following this

187
00:07:11,120 --> 00:07:12,800
error they are induced by the

188
00:07:12,800 --> 00:07:14,560
probability distribution is called

189
00:07:14,560 --> 00:07:16,880
approximation errors and it highlights

190
00:07:16,880 --> 00:07:18,560
the deviation of the predicted

191
00:07:18,560 --> 00:07:21,120
distribution from the real unknown one

192
00:07:21,120 --> 00:07:22,800
but however the cross entropy loss

193
00:07:22,800 --> 00:07:25,520
function states beneficial as majority

194
00:07:25,520 --> 00:07:29,599
suggests at just 2020. indeed material

195
00:07:29,599 --> 00:07:30,960
demonstrates the benefits of the

196
00:07:30,960 --> 00:07:32,880
cross-entropy loss function as it

197
00:07:32,880 --> 00:07:35,039
maximizes the perceived information

198
00:07:35,039 --> 00:07:37,560
introduced by ronno and al at error clip

199
00:07:37,560 --> 00:07:40,639
2011. so more concretely three sources

200
00:07:40,639 --> 00:07:42,720
of errors can be alighted

201
00:07:42,720 --> 00:07:45,360
so first the optimization error already

202
00:07:45,360 --> 00:07:47,840
defined characterizes the error induced

203
00:07:47,840 --> 00:07:49,680
by the learning process as well as the

204
00:07:49,680 --> 00:07:51,759
selection of a finite model space in

205
00:07:51,759 --> 00:07:53,759
order to obtain the optimal parametric

206
00:07:53,759 --> 00:07:54,720
model

207
00:07:54,720 --> 00:07:55,520
then

208
00:07:55,520 --> 00:07:57,199
how the adversary maximizes an

209
00:07:57,199 --> 00:07:59,840
estimation of the perfect information

210
00:07:59,840 --> 00:08:02,639
instead of the true personal information

211
00:08:02,639 --> 00:08:04,639
she needs an infinite number of traffic

212
00:08:04,639 --> 00:08:06,080
in order to converge throughout this

213
00:08:06,080 --> 00:08:08,479
solution and add in practice she can

214
00:08:08,479 --> 00:08:10,800
only deal with infinite number of traces

215
00:08:10,800 --> 00:08:12,800
the estimation error characterize the

216
00:08:12,800 --> 00:08:14,960
gap between the empirical and the true

217
00:08:14,960 --> 00:08:17,919
pro-third information and finally the

218
00:08:17,919 --> 00:08:20,160
approximation error defines the

219
00:08:20,160 --> 00:08:21,759
deviation between the birth of

220
00:08:21,759 --> 00:08:23,440
information estimated with the

221
00:08:23,440 --> 00:08:25,280
cross-entropy function and the mutual

222
00:08:25,280 --> 00:08:28,400
information this error is negligible if

223
00:08:28,400 --> 00:08:30,960
and only if the probability distribution

224
00:08:30,960 --> 00:08:32,719
associated with the optimal model

225
00:08:32,719 --> 00:08:35,120
minimizing the cross-entropy function is

226
00:08:35,120 --> 00:08:37,279
similar to the true unknown probability

227
00:08:37,279 --> 00:08:40,320
distribution so in our work we propose a

228
00:08:40,320 --> 00:08:42,719
new loss function which derived from the

229
00:08:42,719 --> 00:08:45,040
success rate metric and prevent the

230
00:08:45,040 --> 00:08:48,240
approximational effect

231
00:08:48,240 --> 00:08:50,160
so when an adversary performs a

232
00:08:50,160 --> 00:08:52,560
sidechain attack she aims at optimizing

233
00:08:52,560 --> 00:08:54,160
the success rate matrix for a given

234
00:08:54,160 --> 00:08:56,560
number of trusses such that this metric

235
00:08:56,560 --> 00:08:58,399
is defined as follows

236
00:08:58,399 --> 00:09:00,640
so in other words the adversary wants to

237
00:09:00,640 --> 00:09:03,200
optimize this probability such that the

238
00:09:03,200 --> 00:09:05,360
score related to the secret key is

239
00:09:05,360 --> 00:09:07,680
always higher than any other key

240
00:09:07,680 --> 00:09:10,000
potential so what the advertory wants in

241
00:09:10,000 --> 00:09:12,160
this scenario is to approximate the

242
00:09:12,160 --> 00:09:14,160
indicator function to make it easy to

243
00:09:14,160 --> 00:09:15,920
understand and following the work

244
00:09:15,920 --> 00:09:18,000
provided by king lew and lee we know

245
00:09:18,000 --> 00:09:20,000
that a natural fashion consists in

246
00:09:20,000 --> 00:09:22,000
taking the sigmoid function in order to

247
00:09:22,000 --> 00:09:24,240
approximate the indicator function such

248
00:09:24,240 --> 00:09:26,959
that on the right side on this slide we

249
00:09:26,959 --> 00:09:28,959
can see that depending on the value of

250
00:09:28,959 --> 00:09:31,040
alpha we can monitor the approximation

251
00:09:31,040 --> 00:09:33,600
of the indicator function so to optimize

252
00:09:33,600 --> 00:09:35,360
the turnable parameters that maximize

253
00:09:35,360 --> 00:09:37,760
the success rate the adversary has to

254
00:09:37,760 --> 00:09:40,000
apply the binary crash entropy in order

255
00:09:40,000 --> 00:09:41,839
to penalize the deviation of the model

256
00:09:41,839 --> 00:09:43,680
probabilities from the desired

257
00:09:43,680 --> 00:09:44,880
predictions

258
00:09:44,880 --> 00:09:47,200
so in other words we want to penalize

259
00:09:47,200 --> 00:09:49,680
the loss function when this expected

260
00:09:49,680 --> 00:09:51,760
relation is not observed

261
00:09:51,760 --> 00:09:53,760
thus when the binary cross entropy is

262
00:09:53,760 --> 00:09:55,920
applied the adversary obtains the

263
00:09:55,920 --> 00:09:58,800
following partial ranking loss function

264
00:09:58,800 --> 00:10:01,440
so this result gives us an insight into

265
00:10:01,440 --> 00:10:03,200
how the cut function penalized the

266
00:10:03,200 --> 00:10:04,399
training process

267
00:10:04,399 --> 00:10:07,680
when this relation is not respected for

268
00:10:07,680 --> 00:10:10,000
therefore maximizing the success rate

269
00:10:10,000 --> 00:10:12,240
tends to minimize the ranking error

270
00:10:12,240 --> 00:10:14,720
between the secret key k stars and any

271
00:10:14,720 --> 00:10:16,399
key potential k

272
00:10:16,399 --> 00:10:18,560
but in order to efficiently find the

273
00:10:18,560 --> 00:10:20,560
model that maximizes the ranking r we

274
00:10:20,560 --> 00:10:23,040
have to apply this partial cost function

275
00:10:23,040 --> 00:10:24,720
on each key protesis in order to

276
00:10:24,720 --> 00:10:27,360
maximize the rank of the secret key

277
00:10:27,360 --> 00:10:28,959
one interesting property with the

278
00:10:28,959 --> 00:10:30,880
ranking function that we demonstrate in

279
00:10:30,880 --> 00:10:33,760
our paper is that this new loss function

280
00:10:33,760 --> 00:10:36,079
can be considered as an upper bound of

281
00:10:36,079 --> 00:10:39,040
the attack success rate thus minimizing

282
00:10:39,040 --> 00:10:41,600
the ranking loss directly optimize the

283
00:10:41,600 --> 00:10:43,440
performance metric the adversary wants

284
00:10:43,440 --> 00:10:44,720
to maximize

285
00:10:44,720 --> 00:10:47,440
but however what are the benefits of

286
00:10:47,440 --> 00:10:49,519
considering the ranking loss instead of

287
00:10:49,519 --> 00:10:52,079
the cross-entropy loss function so first

288
00:10:52,079 --> 00:10:54,320
let's back to the penalization process

289
00:10:54,320 --> 00:10:56,640
so as a reminder the adversary's goal is

290
00:10:56,640 --> 00:10:58,720
to find a model that maximize the attack

291
00:10:58,720 --> 00:11:00,560
success rate such that during the

292
00:11:00,560 --> 00:11:02,800
profiling phase the turnable parameters

293
00:11:02,800 --> 00:11:05,120
are updated in order to maximize this

294
00:11:05,120 --> 00:11:07,600
performance metric so if the derivative

295
00:11:07,600 --> 00:11:10,399
is computed over k stars the ranking

296
00:11:10,399 --> 00:11:12,720
loss pushes the score of the secret key

297
00:11:12,720 --> 00:11:14,640
up via gradient ascent

298
00:11:14,640 --> 00:11:16,959
on the other hand if the derivative is

299
00:11:16,959 --> 00:11:19,600
computed over an irrelevant output the

300
00:11:19,600 --> 00:11:21,760
ranking loss pushes the score of each

301
00:11:21,760 --> 00:11:24,399
key process down via gradient descent so

302
00:11:24,399 --> 00:11:26,560
that dispensation only depends on the

303
00:11:26,560 --> 00:11:30,079
distance between k stars and j so death

304
00:11:30,079 --> 00:11:31,760
in opposition to the cross entropy last

305
00:11:31,760 --> 00:11:33,760
function the score related to each of

306
00:11:33,760 --> 00:11:35,360
the key potentials do not blur the

307
00:11:35,360 --> 00:11:37,920
polarization term indeed for each player

308
00:11:37,920 --> 00:11:40,480
k star k there are two forces at the

309
00:11:40,480 --> 00:11:42,720
play such that the four that each player

310
00:11:42,720 --> 00:11:44,320
exerts is proportionate to the

311
00:11:44,320 --> 00:11:47,040
difference of the scores so def the

312
00:11:47,040 --> 00:11:49,200
penalization process induces by the

313
00:11:49,200 --> 00:11:51,920
ranking loss maximizes the success rate

314
00:11:51,920 --> 00:11:53,440
such that the approximation error

315
00:11:53,440 --> 00:11:54,959
induced by the cross-entropy loss

316
00:11:54,959 --> 00:11:57,200
function is prevented but from an

317
00:11:57,200 --> 00:11:59,680
optimal model perspective what does this

318
00:11:59,680 --> 00:12:02,320
result mean so if we did not the other

319
00:12:02,320 --> 00:12:04,399
rule allowing the extraction of the

320
00:12:04,399 --> 00:12:06,320
targeted sensitive variable from a

321
00:12:06,320 --> 00:12:08,959
parametric model we know that maximizing

322
00:12:08,959 --> 00:12:11,040
the success rate is equivalent to find

323
00:12:11,040 --> 00:12:13,120
an estimation of the optimal model such

324
00:12:13,120 --> 00:12:15,519
that finding the parameters theta that

325
00:12:15,519 --> 00:12:18,320
minimize this term is equivalent to the

326
00:12:18,320 --> 00:12:20,800
optimal distinguisher and this process

327
00:12:20,800 --> 00:12:22,959
describes exactly what the ranking was

328
00:12:22,959 --> 00:12:25,120
done during the training process

329
00:12:25,120 --> 00:12:27,279
so while the ranking loss aims at

330
00:12:27,279 --> 00:12:29,680
finding the parameters theta that

331
00:12:29,680 --> 00:12:32,720
maximizes these functions we can define

332
00:12:32,720 --> 00:12:34,639
a parametric model considering the

333
00:12:34,639 --> 00:12:36,800
ranking loss as a lower bound of the

334
00:12:36,800 --> 00:12:39,040
mutual information such that if the

335
00:12:39,040 --> 00:12:41,040
adversary has an infinite number of

336
00:12:41,040 --> 00:12:43,440
profiling traces and optimal parameter

337
00:12:43,440 --> 00:12:46,000
theta she retrieves the related mutual

338
00:12:46,000 --> 00:12:48,240
information which is highly beneficial

339
00:12:48,240 --> 00:12:50,160
from an attack perspective as it

340
00:12:50,160 --> 00:12:52,240
reflects the minimum number of attack

341
00:12:52,240 --> 00:12:54,480
traces needed to retrieve the frequency

342
00:12:54,480 --> 00:12:57,120
so from an error analysis perspective

343
00:12:57,120 --> 00:12:59,920
we observe that the minimization error

344
00:12:59,920 --> 00:13:02,560
as well as the estimation error still

345
00:13:02,560 --> 00:13:05,040
affect a model train with a ranking loss

346
00:13:05,040 --> 00:13:07,519
but the latter solution is beneficial to

347
00:13:07,519 --> 00:13:10,320
prevent the approximation error so to

348
00:13:10,320 --> 00:13:12,079
verify all these theoretical

349
00:13:12,079 --> 00:13:14,079
observations through different scenarios

350
00:13:14,079 --> 00:13:16,320
we decide to perform it on the most

351
00:13:16,320 --> 00:13:18,240
common database used in deep learning

352
00:13:18,240 --> 00:13:21,040
based section analysis

353
00:13:21,040 --> 00:13:23,200
for the ranking loss what performed on

354
00:13:23,200 --> 00:13:25,440
two public data set and other data sets

355
00:13:25,440 --> 00:13:27,279
are studied in our paper

356
00:13:27,279 --> 00:13:29,600
so first the cheap resper data set is an

357
00:13:29,600 --> 00:13:33,440
unprotected emulation of aes 128

358
00:13:33,440 --> 00:13:35,279
implemented in software on a cheap

359
00:13:35,279 --> 00:13:37,440
whisperer board which is composed by an

360
00:13:37,440 --> 00:13:40,079
8-bit microcontroller

361
00:13:40,079 --> 00:13:42,160
so due to the lack of control r0s the

362
00:13:42,160 --> 00:13:44,000
adversary can recover the secret key

363
00:13:44,000 --> 00:13:46,000
directly and in this experiment we

364
00:13:46,000 --> 00:13:48,560
attack the first xbox operation then the

365
00:13:48,560 --> 00:13:50,880
ascot dataset is the first open database

366
00:13:50,880 --> 00:13:52,800
that has been specified to serve as

367
00:13:52,800 --> 00:13:54,800
common basis for further work in deep

368
00:13:54,800 --> 00:13:56,800
learning based saturn attacks

369
00:13:56,800 --> 00:13:59,360
so the targeted platform is also an

370
00:13:59,360 --> 00:14:02,320
8-bit microcontroller where the first

371
00:14:02,320 --> 00:14:04,000
order masking is implemented with

372
00:14:04,000 --> 00:14:06,240
different level of the synchronization

373
00:14:06,240 --> 00:14:08,160
so to accurately define the performance

374
00:14:08,160 --> 00:14:09,600
of a neural network we introduce

375
00:14:09,600 --> 00:14:12,560
symmetrical ntge that defines the number

376
00:14:12,560 --> 00:14:14,639
of trusses that are needed for reaching

377
00:14:14,639 --> 00:14:17,680
a constant guessing or trophy of one

378
00:14:17,680 --> 00:14:19,839
for being confident in our experiment we

379
00:14:19,839 --> 00:14:23,120
perform 100 attacks and define ntge bar

380
00:14:23,120 --> 00:14:25,680
at our future metric from a practical

381
00:14:25,680 --> 00:14:28,639
perspective the generation of suitable

382
00:14:28,639 --> 00:14:30,560
architectures is known as a difficult

383
00:14:30,560 --> 00:14:31,519
task

384
00:14:31,519 --> 00:14:33,680
hence two kinds of models can be

385
00:14:33,680 --> 00:14:34,880
considered

386
00:14:34,880 --> 00:14:37,920
so first let's take a look at an example

387
00:14:37,920 --> 00:14:40,480
where the parametric model f theta

388
00:14:40,480 --> 00:14:43,120
exploits a partial set of points of

389
00:14:43,120 --> 00:14:46,160
interest on the gps per data set so once

390
00:14:46,160 --> 00:14:48,480
we capture the physical trusses we

391
00:14:48,480 --> 00:14:50,720
design a neural network such that first

392
00:14:50,720 --> 00:14:52,639
we consider the cross-entropy loss

393
00:14:52,639 --> 00:14:56,160
function as learning matrix

394
00:14:56,240 --> 00:14:59,519
what the profiling model was trained

395
00:14:59,519 --> 00:15:02,639
we used some visualization tools in

396
00:15:02,639 --> 00:15:04,959
order to assess the ability of the model

397
00:15:04,959 --> 00:15:06,639
to correctly retrieve the point of

398
00:15:06,639 --> 00:15:07,839
interest

399
00:15:07,839 --> 00:15:10,079
so here we use a weight visualization

400
00:15:10,079 --> 00:15:14,240
introduced by zayden al at just 2020.

401
00:15:14,240 --> 00:15:16,160
once this fade was performed with the

402
00:15:16,160 --> 00:15:18,480
quest entropy loss function we applied

403
00:15:18,480 --> 00:15:20,399
exactly the same process with the

404
00:15:20,399 --> 00:15:22,800
ranking loss in order to compare the

405
00:15:22,800 --> 00:15:25,839
features retrieved by each model with

406
00:15:25,839 --> 00:15:28,160
the snr computation

407
00:15:28,160 --> 00:15:30,240
so our first observation suggests that

408
00:15:30,240 --> 00:15:33,040
both models successfully retrieve the

409
00:15:33,040 --> 00:15:35,279
point of interest located in the first

410
00:15:35,279 --> 00:15:37,600
200 time samples

411
00:15:37,600 --> 00:15:40,079
indeed this result considered with the

412
00:15:40,079 --> 00:15:43,360
peaks returned by the snr computation

413
00:15:43,360 --> 00:15:45,600
but unfortunately if we look more

414
00:15:45,600 --> 00:15:47,360
carefully at the model train with the

415
00:15:47,360 --> 00:15:49,759
crosshand plus function an unexpected

416
00:15:49,759 --> 00:15:51,839
peak appears and this peak is not

417
00:15:51,839 --> 00:15:54,959
detected by the snr computation as well

418
00:15:54,959 --> 00:15:57,519
as the model train with the ranking loss

419
00:15:57,519 --> 00:15:59,519
so while the model train with the quater

420
00:15:59,519 --> 00:16:01,279
entropy loss function considers this

421
00:16:01,279 --> 00:16:04,000
peak as relevant for its making decision

422
00:16:04,000 --> 00:16:06,959
due to approximation error its impact

423
00:16:06,959 --> 00:16:08,560
can be non-negligible during the

424
00:16:08,560 --> 00:16:10,880
expectation phase

425
00:16:10,880 --> 00:16:13,199
indeed on this slide we affect the

426
00:16:13,199 --> 00:16:15,519
ability of each model to retrieve the

427
00:16:15,519 --> 00:16:17,519
secret key when additional gaussian

428
00:16:17,519 --> 00:16:19,600
noise is applied so through this slide

429
00:16:19,600 --> 00:16:21,440
we can notice that the performance gap

430
00:16:21,440 --> 00:16:23,600
between both model inquiries with the

431
00:16:23,600 --> 00:16:25,920
level of the noise thus

432
00:16:25,920 --> 00:16:28,480
if the optimization or inducing each

433
00:16:28,480 --> 00:16:30,880
model is similar considering the ranking

434
00:16:30,880 --> 00:16:32,959
loss is beneficial as it directly

435
00:16:32,959 --> 00:16:35,920
optimizes the expected probability what

436
00:16:35,920 --> 00:16:38,000
the cross-entropy last function doesn't

437
00:16:38,000 --> 00:16:40,000
the second scenario consider in this

438
00:16:40,000 --> 00:16:42,320
talk is the following so if the

439
00:16:42,320 --> 00:16:44,880
adversary constructs models that exploit

440
00:16:44,880 --> 00:16:46,480
all the relevant information in the

441
00:16:46,480 --> 00:16:50,079
leakage classes then she can ask which

442
00:16:50,079 --> 00:16:52,480
function is beneficial to convert faster

443
00:16:52,480 --> 00:16:55,120
towards the best attack performance so

444
00:16:55,120 --> 00:16:57,120
have the models perfectly retrieve the

445
00:16:57,120 --> 00:16:59,680
point of interest it can be assumed that

446
00:16:59,680 --> 00:17:02,160
the approximation error is negligible

447
00:17:02,160 --> 00:17:04,319
thus in such context only the

448
00:17:04,319 --> 00:17:06,400
optimization error and the estimation

449
00:17:06,400 --> 00:17:09,199
error hold so by performing the attack

450
00:17:09,199 --> 00:17:11,039
on the scat data set we plot the

451
00:17:11,039 --> 00:17:12,880
following graph such that you can

452
00:17:12,880 --> 00:17:14,799
visualize the performance evolution of

453
00:17:14,799 --> 00:17:17,039
both model depending on the number of

454
00:17:17,039 --> 00:17:18,720
profiling traces used during the

455
00:17:18,720 --> 00:17:20,400
training process

456
00:17:20,400 --> 00:17:22,559
thus in the y axis you can notice the

457
00:17:22,559 --> 00:17:25,439
ntge bar value previously introduced so

458
00:17:25,439 --> 00:17:27,679
when the ascad data set is considered we

459
00:17:27,679 --> 00:17:29,679
observed that the model train with the

460
00:17:29,679 --> 00:17:31,919
ranking loss always converged faster

461
00:17:31,919 --> 00:17:33,360
toward the best attack performance

462
00:17:33,360 --> 00:17:35,919
solution whatever the desynchronization

463
00:17:35,919 --> 00:17:39,280
level applied thus given a fixed ntg bar

464
00:17:39,280 --> 00:17:41,919
value a model train with a ranking loss

465
00:17:41,919 --> 00:17:44,640
always needs less profiling trusses than

466
00:17:44,640 --> 00:17:45,760
a model train with the quest

467
00:17:45,760 --> 00:17:47,520
entrepreneur's function such that in

468
00:17:47,520 --> 00:17:49,360
this example using the ranking glass is

469
00:17:49,360 --> 00:17:50,960
beneficial to reduce the estimation

470
00:17:50,960 --> 00:17:51,840
error

471
00:17:51,840 --> 00:17:54,559
to conclude about our work we first

472
00:17:54,559 --> 00:17:56,559
linked the learning touring paradigm

473
00:17:56,559 --> 00:17:58,640
with the site channel approach so that

474
00:17:58,640 --> 00:18:00,640
it helps us to develop a new loss

475
00:18:00,640 --> 00:18:02,559
function that maximize the attack

476
00:18:02,559 --> 00:18:04,160
success rate through this new

477
00:18:04,160 --> 00:18:06,559
proposition we are more concerned with

478
00:18:06,559 --> 00:18:08,480
the relative order between the secret

479
00:18:08,480 --> 00:18:10,480
key and each key protege such that it is

480
00:18:10,480 --> 00:18:12,880
beneficial to prevent the impact induced

481
00:18:12,880 --> 00:18:14,640
by the thought max function which we

482
00:18:14,640 --> 00:18:16,799
called the approximation error and

483
00:18:16,799 --> 00:18:18,559
obviously this work is the starting

484
00:18:18,559 --> 00:18:20,640
point of assessing the benefits of the

485
00:18:20,640 --> 00:18:22,160
learning touring approach inside china

486
00:18:22,160 --> 00:18:23,919
attacks and further investigations

487
00:18:23,919 --> 00:18:25,600
should be made on all the learning to

488
00:18:25,600 --> 00:18:27,760
rank approaches as this right approach

489
00:18:27,760 --> 00:18:30,320
for example finally all of our results

490
00:18:30,320 --> 00:18:32,000
can be reproduced through a github

491
00:18:32,000 --> 00:18:34,799
repository and if you have any question

492
00:18:34,799 --> 00:18:37,200
do not hesitate to contact me over email

493
00:18:37,200 --> 00:18:40,240
thank you very much

