1
00:00:01,120 --> 00:00:03,520
hello everybody welcome to this

2
00:00:03,520 --> 00:00:06,080
presentation today i will be giving a

3
00:00:06,080 --> 00:00:09,120
talk about our recent chess paper pay

4
00:00:09,120 --> 00:00:11,040
attention to raw treatises a deep

5
00:00:11,040 --> 00:00:13,120
learning architecture for end-to-end

6
00:00:13,120 --> 00:00:16,679
profiling attacks

7
00:00:17,359 --> 00:00:19,760
let's start with the basic motivation of

8
00:00:19,760 --> 00:00:22,640
this work deep learning has been widely

9
00:00:22,640 --> 00:00:25,359
used in the session analysis in recent

10
00:00:25,359 --> 00:00:26,320
years

11
00:00:26,320 --> 00:00:28,000
many works displease the quantum

12
00:00:28,000 --> 00:00:30,599
measures like masking and address the

13
00:00:30,599 --> 00:00:33,040
desylchronization issue simultaneously

14
00:00:33,040 --> 00:00:34,640
by deep learning

15
00:00:34,640 --> 00:00:37,120
the neural network has shown its ability

16
00:00:37,120 --> 00:00:39,600
in the set channel context

17
00:00:39,600 --> 00:00:42,079
in almost all of these papers the

18
00:00:42,079 --> 00:00:45,120
selected choice intervals instead of the

19
00:00:45,120 --> 00:00:47,200
draw choices are used

20
00:00:47,200 --> 00:00:49,039
even when the implementation is

21
00:00:49,039 --> 00:00:52,079
protected by marking countermeasure

22
00:00:52,079 --> 00:00:54,239
that means there is a manual feature

23
00:00:54,239 --> 00:00:56,960
expression before the profiling

24
00:00:56,960 --> 00:00:59,440
in our opinion these previous works do

25
00:00:59,440 --> 00:01:01,920
the research with an implicit assumption

26
00:01:01,920 --> 00:01:04,400
that the number of time samples in raw

27
00:01:04,400 --> 00:01:06,479
choices can be reduced before the

28
00:01:06,479 --> 00:01:08,720
profaning easily

29
00:01:08,720 --> 00:01:11,600
however if we consider a practical black

30
00:01:11,600 --> 00:01:15,040
box analysis on a masked implementation

31
00:01:15,040 --> 00:01:17,680
locating the leakages is arguably the

32
00:01:17,680 --> 00:01:19,520
most challenging part of the whole

33
00:01:19,520 --> 00:01:21,040
analysis

34
00:01:21,040 --> 00:01:23,840
the assumption may be too strong or even

35
00:01:23,840 --> 00:01:26,000
invalid in a practical analysis

36
00:01:26,000 --> 00:01:29,119
targeting a marked implementation

37
00:01:29,119 --> 00:01:31,600
therefore we argue that to fully utilize

38
00:01:31,600 --> 00:01:34,159
the potential of deep learning and get

39
00:01:34,159 --> 00:01:37,360
rid of any manual intervention we need

40
00:01:37,360 --> 00:01:40,079
the end-to-end providing that directly

41
00:01:40,079 --> 00:01:42,640
maps draw choices to target intermediate

42
00:01:42,640 --> 00:01:44,079
values

43
00:01:44,079 --> 00:01:46,399
so in our people we propose an

44
00:01:46,399 --> 00:01:49,040
end-to-end architecture composed of

45
00:01:49,040 --> 00:01:51,520
encoders attention mechanisms and a

46
00:01:51,520 --> 00:01:54,000
classifier to conduct the end-to-end

47
00:01:54,000 --> 00:01:56,640
proceeding

48
00:01:56,880 --> 00:01:59,360
and here we have four main contributions

49
00:01:59,360 --> 00:02:00,640
in this work

50
00:02:00,640 --> 00:02:02,799
the first is an architecture for

51
00:02:02,799 --> 00:02:05,200
practical end-to-end profiling so we

52
00:02:05,200 --> 00:02:08,399
could directly profile raw switches the

53
00:02:08,399 --> 00:02:10,560
second is that to build the end-to-end

54
00:02:10,560 --> 00:02:12,800
architecture we introduce some new

55
00:02:12,800 --> 00:02:15,200
structures into the saa

56
00:02:15,200 --> 00:02:17,680
third we get satisfying results on

57
00:02:17,680 --> 00:02:19,520
public data science with our

58
00:02:19,520 --> 00:02:20,800
architecture

59
00:02:20,800 --> 00:02:23,360
and fourth we explore the tension

60
00:02:23,360 --> 00:02:26,080
mechanism in the ic context which

61
00:02:26,080 --> 00:02:28,319
further explains why our end-to-end

62
00:02:28,319 --> 00:02:31,519
architecture works

63
00:02:31,599 --> 00:02:34,080
here is our new architecture

64
00:02:34,080 --> 00:02:36,480
we propose this architecture to conduct

65
00:02:36,480 --> 00:02:38,959
the end-to-end profiling no matter the

66
00:02:38,959 --> 00:02:41,360
raw choices are de-synchronized or

67
00:02:41,360 --> 00:02:44,480
protected by the masking cut mirrors

68
00:02:44,480 --> 00:02:46,319
and we could see how these three

69
00:02:46,319 --> 00:02:49,280
companies work from the viewpoint of sce

70
00:02:49,280 --> 00:02:51,120
in this picture

71
00:02:51,120 --> 00:02:53,440
roughly speaking the includers will

72
00:02:53,440 --> 00:02:55,760
first include raw cheeses to the

73
00:02:55,760 --> 00:02:57,360
abstract features

74
00:02:57,360 --> 00:02:59,599
then the attention mechanism will give

75
00:02:59,599 --> 00:03:02,080
each feature a score and we teach sum

76
00:03:02,080 --> 00:03:03,360
them up

77
00:03:03,360 --> 00:03:05,599
finally the classifier generates the

78
00:03:05,599 --> 00:03:07,840
probabilities from the final feature

79
00:03:07,840 --> 00:03:09,280
vector

80
00:03:09,280 --> 00:03:11,840
i will give a more detailed introduction

81
00:03:11,840 --> 00:03:14,239
of our end-to-end architecture in the

82
00:03:14,239 --> 00:03:16,239
next

83
00:03:16,239 --> 00:03:18,640
first is the encoder component the

84
00:03:18,640 --> 00:03:21,200
encoder content includes two parts the

85
00:03:21,200 --> 00:03:24,080
junior encoder and the senior encoder

86
00:03:24,080 --> 00:03:26,239
here we start from the junior encoder

87
00:03:26,239 --> 00:03:27,519
first

88
00:03:27,519 --> 00:03:30,319
for the junior encoder we also have two

89
00:03:30,319 --> 00:03:32,239
kind of structure to handle the

90
00:03:32,239 --> 00:03:34,319
synchronized and desynchronized

91
00:03:34,319 --> 00:03:36,000
scenarios

92
00:03:36,000 --> 00:03:39,120
for the synchronized scenario we use the

93
00:03:39,120 --> 00:03:41,440
locally connected layer

94
00:03:41,440 --> 00:03:43,680
this layer is quite similar to the

95
00:03:43,680 --> 00:03:46,239
convolutional layer except it does not

96
00:03:46,239 --> 00:03:48,720
share the filter width among the steps

97
00:03:48,720 --> 00:03:51,599
when it slides along the traces

98
00:03:51,599 --> 00:03:53,840
we could see how it works in the red

99
00:03:53,840 --> 00:03:55,599
hand figure

100
00:03:55,599 --> 00:03:57,920
we choose this layer because the locally

101
00:03:57,920 --> 00:04:00,720
connected layer decodes its neural from

102
00:04:00,720 --> 00:04:03,439
the whole high dimensional input

103
00:04:03,439 --> 00:04:06,319
which is crucial to generate fine grid

104
00:04:06,319 --> 00:04:08,879
features with high quality as we

105
00:04:08,879 --> 00:04:10,799
observed

106
00:04:10,799 --> 00:04:13,519
for the desynchronized scenario we still

107
00:04:13,519 --> 00:04:16,238
use the stacked convolutional layers and

108
00:04:16,238 --> 00:04:17,600
pulling layers

109
00:04:17,600 --> 00:04:20,478
we use them to instruct shift invariant

110
00:04:20,478 --> 00:04:22,079
features to address the

111
00:04:22,079 --> 00:04:23,680
desynchronization

112
00:04:23,680 --> 00:04:26,560
but we avoid the folic connections and

113
00:04:26,560 --> 00:04:28,960
very deep convolutional structure by

114
00:04:28,960 --> 00:04:31,360
other other companies like senior

115
00:04:31,360 --> 00:04:34,720
encoder and attention

116
00:04:34,960 --> 00:04:37,199
then is the senior encoder for the

117
00:04:37,199 --> 00:04:40,000
senior encoder we use the longshore term

118
00:04:40,000 --> 00:04:41,120
memory

119
00:04:41,120 --> 00:04:44,479
so why do we choose astm as a senior

120
00:04:44,479 --> 00:04:46,160
encoder

121
00:04:46,160 --> 00:04:48,639
first lcm could learn to control the

122
00:04:48,639 --> 00:04:51,040
data flow automatically when it goes

123
00:04:51,040 --> 00:04:53,199
through the sequential data

124
00:04:53,199 --> 00:04:55,919
there are three keys in rstm and this

125
00:04:55,919 --> 00:04:58,240
disc will control what information is

126
00:04:58,240 --> 00:05:01,280
collected for god and yielding

127
00:05:01,280 --> 00:05:04,160
second there is a built-in structure in

128
00:05:04,160 --> 00:05:07,280
rstm called memory field for storing the

129
00:05:07,280 --> 00:05:08,960
combined features

130
00:05:08,960 --> 00:05:11,520
compared to some other guitar recurrent

131
00:05:11,520 --> 00:05:14,160
layers without the built-in memory

132
00:05:14,160 --> 00:05:16,800
like ciu we find that the built-in

133
00:05:16,800 --> 00:05:18,000
memory

134
00:05:18,000 --> 00:05:19,759
makes the feature combination more

135
00:05:19,759 --> 00:05:21,120
stable

136
00:05:21,120 --> 00:05:22,080
third

137
00:05:22,080 --> 00:05:24,800
rstm could theoretically handle infinite

138
00:05:24,800 --> 00:05:26,800
long sequences

139
00:05:26,800 --> 00:05:30,240
the rstm in our architecture works under

140
00:05:30,240 --> 00:05:32,639
the second two second mode

141
00:05:32,639 --> 00:05:35,199
the reason of using this mode is that

142
00:05:35,199 --> 00:05:37,600
including a two line sequence into a

143
00:05:37,600 --> 00:05:41,199
single feature vector is still too hard

144
00:05:41,199 --> 00:05:43,680
the lstm stereo phase is a greater than

145
00:05:43,680 --> 00:05:46,080
usual in practice

146
00:05:46,080 --> 00:05:48,960
as a result we need some mechanism to

147
00:05:48,960 --> 00:05:51,360
reduce the hardness of training

148
00:05:51,360 --> 00:05:54,160
ideally if we can choose some critical

149
00:05:54,160 --> 00:05:57,199
time steps or at least shorten the time

150
00:05:57,199 --> 00:05:59,600
service or automatically

151
00:05:59,600 --> 00:06:02,319
the training will be much easier

152
00:06:02,319 --> 00:06:04,560
therefore we select the sequence to

153
00:06:04,560 --> 00:06:07,120
thank the mode which exposes the hiding

154
00:06:07,120 --> 00:06:10,160
state at each time step to satisfy the

155
00:06:10,160 --> 00:06:13,680
necessary pre-condition

156
00:06:13,919 --> 00:06:17,039
our senior encoder consists of two rstm

157
00:06:17,039 --> 00:06:19,120
layers with different directions one

158
00:06:19,120 --> 00:06:21,039
forward and one backward

159
00:06:21,039 --> 00:06:23,360
since one of the errors change with the

160
00:06:23,360 --> 00:06:25,840
input backward the order of accessing

161
00:06:25,840 --> 00:06:28,479
the sensitive ledges is reversed from

162
00:06:28,479 --> 00:06:29,919
the forward one

163
00:06:29,919 --> 00:06:32,240
these two rscms may learn different

164
00:06:32,240 --> 00:06:35,120
kinds of combinations of features

165
00:06:35,120 --> 00:06:37,600
and there are also two ways to utilize

166
00:06:37,600 --> 00:06:40,319
the output segments of these two rstm

167
00:06:40,319 --> 00:06:41,520
layers

168
00:06:41,520 --> 00:06:43,680
we could concatenate them along the

169
00:06:43,680 --> 00:06:46,800
feature axis like the figure on the left

170
00:06:46,800 --> 00:06:49,599
side or keep them independent like the

171
00:06:49,599 --> 00:06:52,000
figure on the right side

172
00:06:52,000 --> 00:06:55,199
if we don't want the output of rstms to

173
00:06:55,199 --> 00:06:57,280
interact with each other through the

174
00:06:57,280 --> 00:06:59,840
merging operation nor limit the

175
00:06:59,840 --> 00:07:02,160
representing flexibility of higher

176
00:07:02,160 --> 00:07:04,319
layers we could just keep them

177
00:07:04,319 --> 00:07:05,840
independent

178
00:07:05,840 --> 00:07:08,479
and the independent ertms give better

179
00:07:08,479 --> 00:07:10,639
explanation of how our architecture

180
00:07:10,639 --> 00:07:13,680
locates the informative intervals

181
00:07:13,680 --> 00:07:15,599
we will give more evidence when we

182
00:07:15,599 --> 00:07:19,599
visualize the attention mechanisms later

183
00:07:19,599 --> 00:07:21,759
next is the attention mechanism the

184
00:07:21,759 --> 00:07:24,240
attention mechanism evaluates its visual

185
00:07:24,240 --> 00:07:26,960
validator it's from the thinner encoder

186
00:07:26,960 --> 00:07:29,280
and give each of them a score

187
00:07:29,280 --> 00:07:31,520
the primary motivation of using

188
00:07:31,520 --> 00:07:34,160
attention is to reduce the valid length

189
00:07:34,160 --> 00:07:37,280
of a segment so it is unnecessary for

190
00:07:37,280 --> 00:07:39,840
rstm to include the whole feature

191
00:07:39,840 --> 00:07:42,479
sequence to a single feature vector

192
00:07:42,479 --> 00:07:44,560
the attention mechanism selects some

193
00:07:44,560 --> 00:07:47,520
important time steps and it essentially

194
00:07:47,520 --> 00:07:49,120
departs the sequence by the

195
00:07:49,120 --> 00:07:51,280
probabilities where the value is large

196
00:07:51,280 --> 00:07:52,479
enough

197
00:07:52,479 --> 00:07:55,440
either results in ideal conditions the

198
00:07:55,440 --> 00:07:58,319
rstm could only remember the information

199
00:07:58,319 --> 00:08:01,199
in a certain interval that is remember

200
00:08:01,199 --> 00:08:03,280
the information before a distinct

201
00:08:03,280 --> 00:08:04,960
attention peak

202
00:08:04,960 --> 00:08:07,440
this initially solves the gradient issue

203
00:08:07,440 --> 00:08:09,039
in practice

204
00:08:09,039 --> 00:08:11,280
for the detailed implementation we

205
00:08:11,280 --> 00:08:14,080
modify the attention mechanism property

206
00:08:14,080 --> 00:08:17,120
proposed by burner by inserting a base

207
00:08:17,120 --> 00:08:19,520
normal division layer after calculating

208
00:08:19,520 --> 00:08:20,960
the row of growth

209
00:08:20,960 --> 00:08:23,120
then the roof goes are still normalized

210
00:08:23,120 --> 00:08:25,840
by the photomax function

211
00:08:25,840 --> 00:08:28,479
finally we conduct a weighted thumb

212
00:08:28,479 --> 00:08:30,960
according to the normalized growth also

213
00:08:30,960 --> 00:08:34,080
known as the attention probability

214
00:08:34,080 --> 00:08:36,719
we just give the introduction of our

215
00:08:36,719 --> 00:08:39,839
classifier content component as it is

216
00:08:39,839 --> 00:08:42,049
just a simple fully connected layer

217
00:08:42,049 --> 00:08:44,640
[Music]

218
00:08:44,640 --> 00:08:47,040
if we jump out of the implementation

219
00:08:47,040 --> 00:08:49,440
details there are two variants of our

220
00:08:49,440 --> 00:08:50,720
architecture

221
00:08:50,720 --> 00:08:53,200
these two variants are related to how

222
00:08:53,200 --> 00:08:56,480
our two areas teams are concatenated

223
00:08:56,480 --> 00:08:59,040
for the key that the irs teams are

224
00:08:59,040 --> 00:09:02,160
connected along the feature axis we use

225
00:09:02,160 --> 00:09:04,480
only one attention instance as the

226
00:09:04,480 --> 00:09:06,320
figure will write one

227
00:09:06,320 --> 00:09:09,360
for the key that the two areas teams are

228
00:09:09,360 --> 00:09:10,720
independent

229
00:09:10,720 --> 00:09:12,880
we obtained an attention instinct on

230
00:09:12,880 --> 00:09:14,399
each lstm

231
00:09:14,399 --> 00:09:16,800
like the figure variant 2.

232
00:09:16,800 --> 00:09:19,680
in veron 2 the attention components are

233
00:09:19,680 --> 00:09:21,839
also directional

234
00:09:21,839 --> 00:09:24,000
found that the varane 2 could handle

235
00:09:24,000 --> 00:09:26,720
more complicated leaking scenarios

236
00:09:26,720 --> 00:09:29,440
for example the sensitive leakages are

237
00:09:29,440 --> 00:09:32,240
far from each other or spread in

238
00:09:32,240 --> 00:09:34,240
multiple clock cycles

239
00:09:34,240 --> 00:09:36,880
but the rental one is more efficient in

240
00:09:36,880 --> 00:09:38,959
training when the leaking scenario is

241
00:09:38,959 --> 00:09:41,359
simple

242
00:09:41,920 --> 00:09:44,160
so what do we get through our new

243
00:09:44,160 --> 00:09:46,720
architecture let me start from some

244
00:09:46,720 --> 00:09:48,839
basic settings of our

245
00:09:48,839 --> 00:09:51,360
experiments first we consider the

246
00:09:51,360 --> 00:09:54,360
identity label that means we have

247
00:09:54,360 --> 00:09:57,839
256 labels and typically we use the

248
00:09:57,839 --> 00:10:00,720
output of ees box

249
00:10:00,720 --> 00:10:03,360
and we also consider both synchronized

250
00:10:03,360 --> 00:10:06,240
and desynchronized cases

251
00:10:06,240 --> 00:10:08,880
below are the data sides we used in our

252
00:10:08,880 --> 00:10:10,399
experiments

253
00:10:10,399 --> 00:10:13,279
four of them are public two from dpa

254
00:10:13,279 --> 00:10:16,560
contest and the last two from sk

255
00:10:16,560 --> 00:10:18,880
these four datasets are protected by

256
00:10:18,880 --> 00:10:20,959
boolean mask

257
00:10:20,959 --> 00:10:24,519
we also acquire et128 dataside from an

258
00:10:24,519 --> 00:10:26,959
atmega128 mcu

259
00:10:26,959 --> 00:10:29,279
on which we simulate a masking cut

260
00:10:29,279 --> 00:10:31,120
mirror

261
00:10:31,120 --> 00:10:34,480
the e t 1 to 8 dash f means the legacy

262
00:10:34,480 --> 00:10:37,120
of mask and marked value are far from

263
00:10:37,120 --> 00:10:38,320
each other

264
00:10:38,320 --> 00:10:41,279
so it is a harder trick that compared to

265
00:10:41,279 --> 00:10:43,839
the 8 1 to 8 test n

266
00:10:43,839 --> 00:10:46,240
when we consider when we consider the

267
00:10:46,240 --> 00:10:49,040
end-to-end profiling

268
00:10:49,040 --> 00:10:51,440
you can see from the third column that

269
00:10:51,440 --> 00:10:54,480
we could use over 400 000 time samples

270
00:10:54,480 --> 00:10:57,200
directly in profiling

271
00:10:57,200 --> 00:11:00,079
we also want to clarify that we use fcat

272
00:11:00,079 --> 00:11:03,279
v2 to refer to the data set collected on

273
00:11:03,279 --> 00:11:07,279
each mega 851 file with variable key

274
00:11:07,279 --> 00:11:09,600
the holders released a new chisel

275
00:11:09,600 --> 00:11:12,640
collected on arm stm32 after the

276
00:11:12,640 --> 00:11:14,720
submission of our paper and also

277
00:11:14,720 --> 00:11:17,680
numerate it at cad v2

278
00:11:17,680 --> 00:11:21,920
so don't confuse these two data size

279
00:11:24,000 --> 00:11:26,079
first we apply our network to

280
00:11:26,079 --> 00:11:29,040
synchronize choices here are the results

281
00:11:29,040 --> 00:11:32,640
of the dataset dp contest v4.1

282
00:11:32,640 --> 00:11:35,200
the first second and the third column

283
00:11:35,200 --> 00:11:37,600
are the variation accuracy validation

284
00:11:37,600 --> 00:11:39,440
loss and the guessing entropy

285
00:11:39,440 --> 00:11:40,959
respectively

286
00:11:40,959 --> 00:11:43,120
in the first row we do not use the

287
00:11:43,120 --> 00:11:44,800
knowledge of mask

288
00:11:44,800 --> 00:11:47,279
we reach the getting entropy zero in

289
00:11:47,279 --> 00:11:49,200
four or five pieces

290
00:11:49,200 --> 00:11:51,120
and in the second row we suppose the

291
00:11:51,120 --> 00:11:54,079
value of mask is known we need only one

292
00:11:54,079 --> 00:11:57,360
chip to recover the key

293
00:11:57,360 --> 00:11:59,920
then we test the dataset dp contest v

294
00:11:59,920 --> 00:12:03,040
photo 2. in this dataset there are 13

295
00:12:03,040 --> 00:12:04,959
subsets each corresponding to a

296
00:12:04,959 --> 00:12:07,600
different key we use the choices from

297
00:12:07,600 --> 00:12:09,920
all of the 16 subsets to train and

298
00:12:09,920 --> 00:12:11,279
attack

299
00:12:11,279 --> 00:12:13,440
in the getting injury figure we could

300
00:12:13,440 --> 00:12:15,760
see that most subset could be

301
00:12:15,760 --> 00:12:18,880
successfully attacked in 10 traces

302
00:12:18,880 --> 00:12:21,920
the sunset 12 is harder to attack and it

303
00:12:21,920 --> 00:12:25,600
needs about 120 trees

304
00:12:25,600 --> 00:12:28,639
next we consider the scale data side

305
00:12:28,639 --> 00:12:31,680
from the figures of atkid v1 we find

306
00:12:31,680 --> 00:12:34,160
that our network could reduce the gas

307
00:12:34,160 --> 00:12:36,880
entropy to zero very efficiently it

308
00:12:36,880 --> 00:12:40,800
costs about 6 choices to recover the key

309
00:12:40,800 --> 00:12:43,360
to our best knowledge this result is

310
00:12:43,360 --> 00:12:45,760
state of the art even compared to the

311
00:12:45,760 --> 00:12:48,880
results based on the selected price

312
00:12:48,880 --> 00:12:52,160
from the figures of etiquette with 2 we

313
00:12:52,160 --> 00:12:54,480
see that the attack is also pretty

314
00:12:54,480 --> 00:12:55,760
efficient

315
00:12:55,760 --> 00:12:58,160
we use about 10 pieces to recover the

316
00:12:58,160 --> 00:12:59,760
correct key

317
00:12:59,760 --> 00:13:02,079
to our best knowledge this is also a

318
00:13:02,079 --> 00:13:04,880
state of the art taking results on this

319
00:13:04,880 --> 00:13:07,440
data set even even we use the draw

320
00:13:07,440 --> 00:13:08,639
choices

321
00:13:08,639 --> 00:13:11,360
we give detailed explanations about the

322
00:13:11,360 --> 00:13:14,240
loss and accuracy curve in our paper so

323
00:13:14,240 --> 00:13:16,560
i will not dive into them in this

324
00:13:16,560 --> 00:13:18,880
presentation

325
00:13:18,880 --> 00:13:22,639
next we come to the 8128 data lines

326
00:13:22,639 --> 00:13:25,200
we use both of the variants on these two

327
00:13:25,200 --> 00:13:27,760
data sides to explore how to use

328
00:13:27,760 --> 00:13:30,160
architecture according to the different

329
00:13:30,160 --> 00:13:32,000
leaking scenarios

330
00:13:32,000 --> 00:13:33,760
in

331
00:13:33,760 --> 00:13:36,240
test end the leakages are limited in a

332
00:13:36,240 --> 00:13:38,639
narrower interval on the road with it

333
00:13:38,639 --> 00:13:41,839
where in 81 to 8 days f the leakages are

334
00:13:41,839 --> 00:13:43,600
far from each other

335
00:13:43,600 --> 00:13:46,240
besides the efficient attack the getting

336
00:13:46,240 --> 00:13:49,199
entropy indicates that the network based

337
00:13:49,199 --> 00:13:51,600
on variant 2 handles both leaking

338
00:13:51,600 --> 00:13:54,320
conditions better in terms of profiling

339
00:13:54,320 --> 00:13:55,680
quality

340
00:13:55,680 --> 00:13:58,320
the advantage of variant 1 is that it

341
00:13:58,320 --> 00:14:00,720
could converge with a larger batch size

342
00:14:00,720 --> 00:14:03,120
when the number of training choices is

343
00:14:03,120 --> 00:14:05,920
the same and thus get a good network for

344
00:14:05,920 --> 00:14:07,760
attack faster

345
00:14:07,760 --> 00:14:10,160
we also give a more detailed discussion

346
00:14:10,160 --> 00:14:12,399
in our paper about how to choose these

347
00:14:12,399 --> 00:14:14,560
two variants

348
00:14:14,560 --> 00:14:16,160
so what we are happening when we

349
00:14:16,160 --> 00:14:18,560
consider the desynchronized treaty we

350
00:14:18,560 --> 00:14:21,839
are in d synchronized cases we use that

351
00:14:21,839 --> 00:14:24,240
convolutional layer to replace the

352
00:14:24,240 --> 00:14:26,639
locally connected layer the delay

353
00:14:26,639 --> 00:14:29,120
interval in figures means the range of

354
00:14:29,120 --> 00:14:31,920
random shift we added to the treated to

355
00:14:31,920 --> 00:14:34,560
simulate the de-synchronization

356
00:14:34,560 --> 00:14:37,040
we also find that the number of choices

357
00:14:37,040 --> 00:14:39,839
for both versions of the ad kit is not

358
00:14:39,839 --> 00:14:41,760
quite enough for our end-to-end

359
00:14:41,760 --> 00:14:44,399
programming so we also conduct data

360
00:14:44,399 --> 00:14:46,000
augmentation

361
00:14:46,000 --> 00:14:48,240
finally at the figure 2

362
00:14:48,240 --> 00:14:50,880
we could reduce the gas entropy to 0

363
00:14:50,880 --> 00:14:53,600
with very few of choices

364
00:14:53,600 --> 00:14:56,800
next we test the desynchronized 81-8

365
00:14:56,800 --> 00:14:59,839
data size for efficient attack we also

366
00:14:59,839 --> 00:15:02,720
conduct a data augmentation

367
00:15:02,720 --> 00:15:05,360
and here are the getting entropy results

368
00:15:05,360 --> 00:15:07,600
again you can see that we need only

369
00:15:07,600 --> 00:15:12,000
several choices to recover the key

370
00:15:12,000 --> 00:15:13,839
here are some comparisons to the

371
00:15:13,839 --> 00:15:16,160
previous works on the data side dp

372
00:15:16,160 --> 00:15:19,920
contest v 4.1 and squared v1

373
00:15:19,920 --> 00:15:22,079
in the first figure the blue curve is

374
00:15:22,079 --> 00:15:24,800
our result using the low traces and

375
00:15:24,800 --> 00:15:26,959
without knowing the mask

376
00:15:26,959 --> 00:15:29,600
well the previous was using the certain

377
00:15:29,600 --> 00:15:31,920
trees with no mask

378
00:15:31,920 --> 00:15:34,079
we can see that although our profiling

379
00:15:34,079 --> 00:15:36,800
is conducted on those pieces the attack

380
00:15:36,800 --> 00:15:38,800
is more efficient

381
00:15:38,800 --> 00:15:40,800
in the second figure the comparison

382
00:15:40,800 --> 00:15:43,839
result is similar our network trained on

383
00:15:43,839 --> 00:15:46,079
load choices performs much better than

384
00:15:46,079 --> 00:15:48,560
the previous work uh trained on the

385
00:15:48,560 --> 00:15:51,360
reduced traces and the curve of our

386
00:15:51,360 --> 00:15:55,040
getting entropy is in the corner

387
00:15:55,040 --> 00:15:58,079
here is a summary of our text the first

388
00:15:58,079 --> 00:16:00,079
column is the data set

389
00:16:00,079 --> 00:16:02,560
the second column is the random three we

390
00:16:02,560 --> 00:16:05,759
used to simulate the d synchronization

391
00:16:05,759 --> 00:16:07,680
and the third column is the number of

392
00:16:07,680 --> 00:16:09,920
choices to recover the key

393
00:16:09,920 --> 00:16:12,639
we can see that for most of the cases we

394
00:16:12,639 --> 00:16:14,880
could reduce the guess entropy to zero

395
00:16:14,880 --> 00:16:16,639
in several choices

396
00:16:16,639 --> 00:16:18,880
for some more challenging scenarios we

397
00:16:18,880 --> 00:16:23,759
need 10 or even more than hundred trees

398
00:16:23,759 --> 00:16:26,320
now we show some variations of our

399
00:16:26,320 --> 00:16:29,040
network to explain how one of our key

400
00:16:29,040 --> 00:16:32,399
structure attention mechanism works

401
00:16:32,399 --> 00:16:35,360
these are the results of our network our

402
00:16:35,360 --> 00:16:39,519
81 to 8 test n based on the variant one

403
00:16:39,519 --> 00:16:42,079
from the ethernet and gradient we could

404
00:16:42,079 --> 00:16:44,639
conclude that our network had found out

405
00:16:44,639 --> 00:16:46,000
the liquidity

406
00:16:46,000 --> 00:16:48,800
and the very vision of the attention so

407
00:16:48,800 --> 00:16:51,440
that the tension mechanism decides the

408
00:16:51,440 --> 00:16:54,639
time step after the backward rstm go

409
00:16:54,639 --> 00:16:57,680
through the legs is very important

410
00:16:57,680 --> 00:16:59,759
so we can see a

411
00:16:59,759 --> 00:17:02,639
tension peak in the subfigure c and

412
00:17:02,639 --> 00:17:05,039
subtrigger d

413
00:17:05,039 --> 00:17:07,520
the network between our f k based on

414
00:17:07,520 --> 00:17:09,679
variant 2 could give a more clear

415
00:17:09,679 --> 00:17:12,079
iteration of how our attention mechanism

416
00:17:12,079 --> 00:17:14,799
focus on the informative interval

417
00:17:14,799 --> 00:17:16,959
in south figure f we plot the

418
00:17:16,959 --> 00:17:19,520
probabilities of forward and backward

419
00:17:19,520 --> 00:17:22,160
attention we could observe that both

420
00:17:22,160 --> 00:17:24,319
attention instances pay special

421
00:17:24,319 --> 00:17:27,520
attention just after rstms go through

422
00:17:27,520 --> 00:17:31,679
the 10 steps between about 800 and 900

423
00:17:31,679 --> 00:17:34,000
and this time steps correspond to an

424
00:17:34,000 --> 00:17:35,120
interval

425
00:17:35,120 --> 00:17:39,440
around index 45 000 on the low species

426
00:17:39,440 --> 00:17:41,280
in other words this interval is

427
00:17:41,280 --> 00:17:44,080
subjected by attention mechanism to be

428
00:17:44,080 --> 00:17:46,559
the most informative interval on the raw

429
00:17:46,559 --> 00:17:47,919
traces

430
00:17:47,919 --> 00:17:50,559
not surprisingly this interval includes

431
00:17:50,559 --> 00:17:53,600
the 700 time samples that are manually

432
00:17:53,600 --> 00:17:57,918
selected by the authors of s-cad

433
00:17:58,160 --> 00:18:01,039
finally we delve deeper into the rstn we

434
00:18:01,039 --> 00:18:04,080
plot the git activation state and try to

435
00:18:04,080 --> 00:18:06,480
find out whether the rstm indeed

436
00:18:06,480 --> 00:18:08,640
collects the leakages before the

437
00:18:08,640 --> 00:18:11,760
attention asks it to output

438
00:18:11,760 --> 00:18:14,400
the git state is presented by blue

439
00:18:14,400 --> 00:18:17,919
curves we can see the gate is activated

440
00:18:17,919 --> 00:18:20,400
just as a position where the peaks of

441
00:18:20,400 --> 00:18:23,679
snrs are located therefore the input

442
00:18:23,679 --> 00:18:26,880
keys of rstm is definitely controlling

443
00:18:26,880 --> 00:18:29,440
the rtm to collect the informative

444
00:18:29,440 --> 00:18:30,880
disease

445
00:18:30,880 --> 00:18:32,880
the releases among the tension

446
00:18:32,880 --> 00:18:36,880
probabilities snr and git activism

447
00:18:36,880 --> 00:18:39,600
indicate that the rftm and attension

448
00:18:39,600 --> 00:18:43,760
mechanism interact as we design

449
00:18:43,760 --> 00:18:46,559
finally we move to the conclusion

450
00:18:46,559 --> 00:18:48,799
in this paper we introduce a neural

451
00:18:48,799 --> 00:18:51,200
network architecture for end-to-end

452
00:18:51,200 --> 00:18:52,320
profiling

453
00:18:52,320 --> 00:18:54,400
that means rather than leveraging the

454
00:18:54,400 --> 00:18:56,559
knowledge of implementation and the

455
00:18:56,559 --> 00:18:59,360
value of masks to locate the informative

456
00:18:59,360 --> 00:19:01,600
intervals we could profile the

457
00:19:01,600 --> 00:19:04,000
associated directory now

458
00:19:04,000 --> 00:19:06,480
to the best of our knowledge this is the

459
00:19:06,480 --> 00:19:08,720
first architecture that achieves

460
00:19:08,720 --> 00:19:10,799
end-to-end profiling

461
00:19:10,799 --> 00:19:13,360
we also introduced some new structures

462
00:19:13,360 --> 00:19:15,679
into the ic field to build the

463
00:19:15,679 --> 00:19:18,559
architecture like the locally connected

464
00:19:18,559 --> 00:19:21,280
layers and the changing mechanism

465
00:19:21,280 --> 00:19:24,240
beside the effectiveness we find our

466
00:19:24,240 --> 00:19:26,320
approach working under the end-to-end

467
00:19:26,320 --> 00:19:28,720
context performs even better than the

468
00:19:28,720 --> 00:19:32,000
networks trained on reduced traces

469
00:19:32,000 --> 00:19:34,080
we also explore how the attention

470
00:19:34,080 --> 00:19:36,400
mechanism works in the third channel

471
00:19:36,400 --> 00:19:39,440
context to verify the architecture works

472
00:19:39,440 --> 00:19:42,240
as we design

473
00:19:43,120 --> 00:19:45,120
for the future work it will be

474
00:19:45,120 --> 00:19:48,559
interesting to replace the rstm to self

475
00:19:48,559 --> 00:19:50,640
attention mechanism to fasten the

476
00:19:50,640 --> 00:19:54,240
training process since the rxtm cannot

477
00:19:54,240 --> 00:19:56,480
be paralyzed the training is time

478
00:19:56,480 --> 00:19:58,240
consuming now

479
00:19:58,240 --> 00:20:00,880
and we will also explore other neural

480
00:20:00,880 --> 00:20:03,600
networks or training strategies to

481
00:20:03,600 --> 00:20:05,919
improve the performance of deep learning

482
00:20:05,919 --> 00:20:08,000
in the fca

483
00:20:08,000 --> 00:20:09,840
so that's it thank you for your

484
00:20:09,840 --> 00:20:10,960
attention

485
00:20:10,960 --> 00:20:13,440
and you are welcome to read our paper to

486
00:20:13,440 --> 00:20:15,360
get all the details

487
00:20:15,360 --> 00:20:17,679
if you have any question feel free to

488
00:20:17,679 --> 00:20:21,840
contact us thank you

