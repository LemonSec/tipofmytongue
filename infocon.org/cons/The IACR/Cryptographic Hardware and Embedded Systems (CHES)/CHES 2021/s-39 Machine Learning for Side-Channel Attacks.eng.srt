1
00:00:00,960 --> 00:00:03,679
ready to go have a good session

2
00:00:03,679 --> 00:00:04,880
thank you

3
00:00:04,880 --> 00:00:05,920
thank you

4
00:00:05,920 --> 00:00:07,919
hello and welcome to session machine

5
00:00:07,919 --> 00:00:10,400
learning for site channel attacks

6
00:00:10,400 --> 00:00:12,960
where six papers will be presented after

7
00:00:12,960 --> 00:00:16,160
each presentation the speaker can take

8
00:00:16,160 --> 00:00:18,080
some of your questions so please post

9
00:00:18,080 --> 00:00:19,920
those in the

10
00:00:19,920 --> 00:00:24,240
zurip or here in the chat panel of

11
00:00:24,240 --> 00:00:27,439
zoom and

12
00:00:27,519 --> 00:00:29,599
yeah if

13
00:00:29,599 --> 00:00:32,640
time permits we can take more questions

14
00:00:32,640 --> 00:00:33,760
after

15
00:00:33,760 --> 00:00:36,239
um we are done with all six

16
00:00:36,239 --> 00:00:37,600
uh

17
00:00:37,600 --> 00:00:38,719
papers

18
00:00:38,719 --> 00:00:45,200
the first talk is uh given by

19
00:00:45,200 --> 00:00:46,960
just a moment

20
00:00:46,960 --> 00:00:50,160
it's going uh to be given by

21
00:00:50,160 --> 00:00:51,840
gabriel's date

22
00:00:51,840 --> 00:00:53,680
and the paper entitled

23
00:00:53,680 --> 00:00:56,800
ranking loss maximizing the success rate

24
00:00:56,800 --> 00:00:58,719
in deep learning side channel

25
00:00:58,719 --> 00:01:01,120
analysis the paper was written by

26
00:01:01,120 --> 00:01:02,879
gabriel lillian

27
00:01:02,879 --> 00:01:04,799
francois

28
00:01:04,799 --> 00:01:07,280
amore alexander

29
00:01:07,280 --> 00:01:09,840
and gabriel the floor is yours

30
00:01:09,840 --> 00:01:10,840
thank you very

31
00:01:10,840 --> 00:01:12,320
much uh

32
00:01:12,320 --> 00:01:14,320
just before starting can you clearly

33
00:01:14,320 --> 00:01:17,119
hear me and can you clearly see my slide

34
00:01:17,119 --> 00:01:18,560
yes we can

35
00:01:18,560 --> 00:01:21,200
okay thank you very much

36
00:01:21,200 --> 00:01:23,040
so thank you very much for this

37
00:01:23,040 --> 00:01:24,960
introduction so

38
00:01:24,960 --> 00:01:26,960
as mentioned i'm gabriel i'm a phd

39
00:01:26,960 --> 00:01:29,040
student from the berkeley laboratory and

40
00:01:29,040 --> 00:01:30,640
the text itself

41
00:01:30,640 --> 00:01:32,320
and the work i'll present to you today

42
00:01:32,320 --> 00:01:34,240
is a joint work with lilian bosuele and

43
00:01:34,240 --> 00:01:37,360
amarya from the berkeley laboratory and

44
00:01:37,360 --> 00:01:39,360
francoisesons and alexandre nelly from

45
00:01:39,360 --> 00:01:41,520
nxp semiconductors

46
00:01:41,520 --> 00:01:44,159
so the goal of our work consisting

47
00:01:44,159 --> 00:01:46,000
reducing the gap between machine

48
00:01:46,000 --> 00:01:48,880
learning matrix and the one used in

49
00:01:48,880 --> 00:01:50,479
sectional context

50
00:01:50,479 --> 00:01:52,880
such that we develop a new ranking loss

51
00:01:52,880 --> 00:01:53,680
that

52
00:01:53,680 --> 00:01:55,119
aims at

53
00:01:55,119 --> 00:01:57,759
penalizing the learning process in order

54
00:01:57,759 --> 00:02:00,640
to maximize the attack success rate of

55
00:02:00,640 --> 00:02:02,960
a neural network

56
00:02:02,960 --> 00:02:04,159
so

57
00:02:04,159 --> 00:02:07,520
when an adversary wants to conduct a

58
00:02:07,520 --> 00:02:09,919
deep learning-based channel attacks

59
00:02:09,919 --> 00:02:12,400
she first aimed at developing a

60
00:02:12,400 --> 00:02:14,640
parametric function f theta

61
00:02:14,640 --> 00:02:16,560
so that during the training process the

62
00:02:16,560 --> 00:02:18,720
adversary wants to optimize the tita

63
00:02:18,720 --> 00:02:20,800
parameters in order to maximize the

64
00:02:20,800 --> 00:02:22,480
attack success rate

65
00:02:22,480 --> 00:02:24,560
for a given number

66
00:02:24,560 --> 00:02:26,959
of trusses so that the attack success

67
00:02:26,959 --> 00:02:28,879
such that the success rate metric can be

68
00:02:28,879 --> 00:02:31,920
expressed as as follows

69
00:02:31,920 --> 00:02:33,360
so here

70
00:02:33,360 --> 00:02:35,280
the success rate defines the probability

71
00:02:35,280 --> 00:02:37,519
so that the score related to the secret

72
00:02:37,519 --> 00:02:40,319
key k stars is higher than the score

73
00:02:40,319 --> 00:02:43,200
related to any other key vertices so

74
00:02:43,200 --> 00:02:46,080
that in that case the score is provided

75
00:02:46,080 --> 00:02:49,280
by the parametric function f theta

76
00:02:49,280 --> 00:02:51,599
so during the the training process we

77
00:02:51,599 --> 00:02:54,080
want to penalize the neural network

78
00:02:54,080 --> 00:02:57,599
when this relation is not respected

79
00:02:57,599 --> 00:02:59,519
but unfortunately from

80
00:02:59,519 --> 00:03:02,159
an optimization perspective optimizing

81
00:03:02,159 --> 00:03:04,959
the indicator function can be known as a

82
00:03:04,959 --> 00:03:07,920
difficult task so first we have to

83
00:03:07,920 --> 00:03:10,080
approximate it in order to penalize the

84
00:03:10,080 --> 00:03:12,239
neural network when

85
00:03:12,239 --> 00:03:15,760
this relation is not respected

86
00:03:15,840 --> 00:03:18,159
so thanks to the work provided by king

87
00:03:18,159 --> 00:03:20,800
lewin in 2010 we know that a good

88
00:03:20,800 --> 00:03:22,800
approximation of the indicator function

89
00:03:22,800 --> 00:03:25,040
can be made thanks to the sigmoid

90
00:03:25,040 --> 00:03:26,080
function

91
00:03:26,080 --> 00:03:28,720
so that depending on an eaper parameter

92
00:03:28,720 --> 00:03:31,120
alpha the adversary can monitor

93
00:03:31,120 --> 00:03:33,440
its approximation of the indicator

94
00:03:33,440 --> 00:03:35,599
function

95
00:03:35,599 --> 00:03:38,080
so from this sigma from this sigmoid

96
00:03:38,080 --> 00:03:40,959
function we develop

97
00:03:40,959 --> 00:03:42,959
the ranking loss that penalize the

98
00:03:42,959 --> 00:03:44,480
learning process

99
00:03:44,480 --> 00:03:47,920
when this relation is not respected

100
00:03:47,920 --> 00:03:49,840
so in other words we want to penalize

101
00:03:49,840 --> 00:03:54,400
the learning process when uh the

102
00:03:54,480 --> 00:03:57,120
the model probabilities provided by the

103
00:03:57,120 --> 00:04:00,400
model deviates from the true predictions

104
00:04:00,400 --> 00:04:03,599
so such that so the the ranking loss can

105
00:04:03,599 --> 00:04:05,920
be expressed as failed

106
00:04:05,920 --> 00:04:08,799
and one interesting point of the ranking

107
00:04:08,799 --> 00:04:11,040
loss that we demonstrate in our paper is

108
00:04:11,040 --> 00:04:12,799
that minimizing the ranking loss is

109
00:04:12,799 --> 00:04:14,400
beneficial to

110
00:04:14,400 --> 00:04:16,238
minimize the error provided on the

111
00:04:16,238 --> 00:04:18,238
success rate or in other words

112
00:04:18,238 --> 00:04:20,160
minimizing the the ranking loss is

113
00:04:20,160 --> 00:04:21,759
beneficial to

114
00:04:21,759 --> 00:04:24,160
increase the success rate

115
00:04:24,160 --> 00:04:27,040
for a given parametric function f theta

116
00:04:27,040 --> 00:04:29,360
and a number of traces

117
00:04:29,360 --> 00:04:30,639
so okay

118
00:04:30,639 --> 00:04:32,320
once we developed the ranking loss

119
00:04:32,320 --> 00:04:34,639
function we decide to evaluate its

120
00:04:34,639 --> 00:04:38,080
benefits from a practical perspective

121
00:04:38,080 --> 00:04:39,919
and first

122
00:04:39,919 --> 00:04:41,759
in order to evaluate these benefits we

123
00:04:41,759 --> 00:04:44,639
have to introduce the following

124
00:04:44,639 --> 00:04:46,080
um

125
00:04:46,080 --> 00:04:48,560
performance metric that we call ntg

126
00:04:48,560 --> 00:04:50,639
and that defines the number of attack

127
00:04:50,639 --> 00:04:52,560
traces that are needed in order to

128
00:04:52,560 --> 00:04:54,479
retrieve the secret key byte

129
00:04:54,479 --> 00:04:57,440
so and we denote ntge bar

130
00:04:57,440 --> 00:05:00,639
the the average value of ntg over 100

131
00:05:00,639 --> 00:05:02,080
tests

132
00:05:02,080 --> 00:05:03,440
okay

133
00:05:03,440 --> 00:05:05,360
and in order to assess the benefits of

134
00:05:05,360 --> 00:05:08,160
the ranking loss we consider the escape

135
00:05:08,160 --> 00:05:10,479
data set that that has been introduced

136
00:05:10,479 --> 00:05:13,039
to serve as a common database in a deep

137
00:05:13,039 --> 00:05:14,960
learning based hr attacks

138
00:05:14,960 --> 00:05:16,800
and the ascot data set considers two

139
00:05:16,800 --> 00:05:18,880
country modules namely the random data

140
00:05:18,880 --> 00:05:20,720
effect with different level of the

141
00:05:20,720 --> 00:05:22,960
synchronization and the first order

142
00:05:22,960 --> 00:05:25,280
boolean masking

143
00:05:25,280 --> 00:05:26,960
so to assess the benefit of the ranking

144
00:05:26,960 --> 00:05:27,759
lost

145
00:05:27,759 --> 00:05:30,720
first we developed a model that we train

146
00:05:30,720 --> 00:05:32,880
with the ranking lab function and we

147
00:05:32,880 --> 00:05:35,120
decide to assess

148
00:05:35,120 --> 00:05:36,720
its benefits from a performance

149
00:05:36,720 --> 00:05:38,880
perspective with

150
00:05:38,880 --> 00:05:42,160
a model train

151
00:05:42,160 --> 00:05:44,160
with another loss function that that is

152
00:05:44,160 --> 00:05:45,840
called the cross-entropy loss function

153
00:05:45,840 --> 00:05:48,320
and this loss function is widely used in

154
00:05:48,320 --> 00:05:50,840
a in deep learning based agent and

155
00:05:50,840 --> 00:05:55,039
attacks so to compare both models we

156
00:05:55,039 --> 00:05:58,319
plot the evolution of the ntge value

157
00:05:58,319 --> 00:06:00,400
following the number of profiling

158
00:06:00,400 --> 00:06:02,319
choices so so that the number of

159
00:06:02,319 --> 00:06:03,840
profiling traces are the number of

160
00:06:03,840 --> 00:06:05,280
choices that are used during the

161
00:06:05,280 --> 00:06:07,120
training process

162
00:06:07,120 --> 00:06:09,759
and here in this plot you can first

163
00:06:09,759 --> 00:06:11,600
uh

164
00:06:11,600 --> 00:06:14,720
observe the evolution of the ntg value

165
00:06:14,720 --> 00:06:16,639
when the

166
00:06:16,639 --> 00:06:18,560
the question tropidos function is used

167
00:06:18,560 --> 00:06:20,800
so here in green and in blue you have

168
00:06:20,800 --> 00:06:22,880
the evolution of the ntg value when the

169
00:06:22,880 --> 00:06:24,880
model is trained with the ranking loss

170
00:06:24,880 --> 00:06:27,039
and once we assess

171
00:06:27,039 --> 00:06:29,280
once we conduct this experiment when the

172
00:06:29,280 --> 00:06:31,120
synchronization occurs

173
00:06:31,120 --> 00:06:33,360
we also evaluate the benefits of the

174
00:06:33,360 --> 00:06:34,800
ranking loss

175
00:06:34,800 --> 00:06:36,400
in comparison with the model train with

176
00:06:36,400 --> 00:06:38,479
the cross-entropy

177
00:06:38,479 --> 00:06:42,560
when some desynchronization occurs

178
00:06:42,560 --> 00:06:45,039
so first through these

179
00:06:45,039 --> 00:06:47,199
figures we can observe that for a given

180
00:06:47,199 --> 00:06:50,160
number of profiling traces a model train

181
00:06:50,160 --> 00:06:51,919
with the ranking lots

182
00:06:51,919 --> 00:06:54,479
performs in the worst case similarly to

183
00:06:54,479 --> 00:06:56,240
a model train with the cross-entropy

184
00:06:56,240 --> 00:06:57,520
loss function

185
00:06:57,520 --> 00:07:00,800
and on the other hand for a given ntg

186
00:07:00,800 --> 00:07:03,199
value we observe that the number of

187
00:07:03,199 --> 00:07:05,199
profiling choices that are needed to

188
00:07:05,199 --> 00:07:08,080
reach this solution is lower when the

189
00:07:08,080 --> 00:07:09,919
rankiness is considered

190
00:07:09,919 --> 00:07:13,120
so from a practical perspective

191
00:07:13,120 --> 00:07:15,039
considering the ranking us instead of

192
00:07:15,039 --> 00:07:17,759
the cross-entrepreneurs function can be

193
00:07:17,759 --> 00:07:20,960
considered as a good alternative inside

194
00:07:20,960 --> 00:07:23,039
channel context

195
00:07:23,039 --> 00:07:25,199
so if we want to conclude about this

196
00:07:25,199 --> 00:07:26,560
brief talk

197
00:07:26,560 --> 00:07:28,800
first

198
00:07:28,800 --> 00:07:29,599
by

199
00:07:29,599 --> 00:07:32,720
introducing the ranking loss function we

200
00:07:32,720 --> 00:07:34,560
aim that reducing the gap between the

201
00:07:34,560 --> 00:07:36,319
machine learning metric and the one

202
00:07:36,319 --> 00:07:38,400
introduced in such a context

203
00:07:38,400 --> 00:07:40,880
and through our

204
00:07:40,880 --> 00:07:43,199
investigation we observe that minimizing

205
00:07:43,199 --> 00:07:45,360
the ranking loss is equivalent to

206
00:07:45,360 --> 00:07:48,160
maximizing the success rate metric

207
00:07:48,160 --> 00:07:49,199
because

208
00:07:49,199 --> 00:07:51,520
as i briefly mentioned using the ranking

209
00:07:51,520 --> 00:07:52,560
loss is

210
00:07:52,560 --> 00:07:55,360
useful to penalize the learning process

211
00:07:55,360 --> 00:07:57,280
when the score related to the secret

212
00:07:57,280 --> 00:07:59,759
information so to the private key

213
00:07:59,759 --> 00:08:02,160
stars is not considered to the highest

214
00:08:02,160 --> 00:08:04,319
value

215
00:08:04,319 --> 00:08:06,400
finally if you want to

216
00:08:06,400 --> 00:08:08,800
get more information about the benefits

217
00:08:08,800 --> 00:08:10,400
of the ranking loss

218
00:08:10,400 --> 00:08:12,240
we encourage you to read our paper

219
00:08:12,240 --> 00:08:13,680
because we provide some additional

220
00:08:13,680 --> 00:08:15,840
contribution in particular we

221
00:08:15,840 --> 00:08:17,360
investigate the impact of the ranking

222
00:08:17,360 --> 00:08:19,120
laws during the training process and we

223
00:08:19,120 --> 00:08:21,599
perform

224
00:08:21,599 --> 00:08:24,319
a concrete theoretical analysis between

225
00:08:24,319 --> 00:08:26,000
the benefits and the limitations of the

226
00:08:26,000 --> 00:08:28,160
ranking us in comparison with the

227
00:08:28,160 --> 00:08:29,520
cross-entrepreneurs function that is

228
00:08:29,520 --> 00:08:31,680
rightly used in such context and finally

229
00:08:31,680 --> 00:08:34,559
we also provide additional experiment

230
00:08:34,559 --> 00:08:36,640
experiments in order to

231
00:08:36,640 --> 00:08:38,958
assess the benefits and to validate the

232
00:08:38,958 --> 00:08:40,399
benefits of

233
00:08:40,399 --> 00:08:41,279
the

234
00:08:41,279 --> 00:08:42,958
ranking loss function

235
00:08:42,958 --> 00:08:47,040
from a practical perspective

236
00:08:47,040 --> 00:08:48,560
so thank you very much for listening if

237
00:08:48,560 --> 00:08:50,399
you have any questions do not hesitate

238
00:08:50,399 --> 00:08:54,399
to ask or to contact me over email

239
00:08:54,399 --> 00:08:56,959
thank you very much

240
00:08:56,959 --> 00:08:58,800
thank you for the talk

241
00:08:58,800 --> 00:09:01,440
uh there are no questions in zuleim yet

242
00:09:01,440 --> 00:09:05,600
but i think sabah has a question to ask

243
00:09:05,600 --> 00:09:08,880
i have one question and and i think

244
00:09:08,880 --> 00:09:10,880
um it was

245
00:09:10,880 --> 00:09:13,360
two slides before the uh that venue

246
00:09:13,360 --> 00:09:15,440
showed the results

247
00:09:15,440 --> 00:09:18,080
uh for the ranking laws

248
00:09:18,080 --> 00:09:19,519
applied to

249
00:09:19,519 --> 00:09:22,000
ascad database

250
00:09:22,000 --> 00:09:25,920
my understanding is that the

251
00:09:26,000 --> 00:09:27,360
framework

252
00:09:27,360 --> 00:09:30,320
made by you is quite generic and you can

253
00:09:30,320 --> 00:09:32,800
use it for other

254
00:09:32,800 --> 00:09:36,399
databases have you tried that on ascat

255
00:09:36,399 --> 00:09:37,440
v2

256
00:09:37,440 --> 00:09:39,279
to see

257
00:09:39,279 --> 00:09:41,360
what could come out

258
00:09:41,360 --> 00:09:43,839
unfortunately not we do we do not

259
00:09:43,839 --> 00:09:45,360
investigate the benefits of the ranking

260
00:09:45,360 --> 00:09:47,519
loss on the scad v2

261
00:09:47,519 --> 00:09:52,000
but we investigated on other data sets

262
00:09:52,320 --> 00:09:53,680
with a

263
00:09:53,680 --> 00:09:55,600
low level of

264
00:09:55,600 --> 00:09:56,720
snr

265
00:09:56,720 --> 00:09:59,680
measures and also on asymmetric

266
00:09:59,680 --> 00:10:01,920
implementations and the wealth we

267
00:10:01,920 --> 00:10:05,440
obtained are always the same and

268
00:10:05,440 --> 00:10:07,440
they are similar to those i introduce

269
00:10:07,440 --> 00:10:10,399
you on the scad v1 dataset

270
00:10:10,399 --> 00:10:13,680
thank you that was answering my question

271
00:10:13,680 --> 00:10:16,240
and there is a question from the chat

272
00:10:16,240 --> 00:10:19,040
ranking loss is considered a new metric

273
00:10:19,040 --> 00:10:21,360
to evaluate loss

274
00:10:21,360 --> 00:10:23,839
uh is ranking laws considered a new

275
00:10:23,839 --> 00:10:26,079
metric to evaluate loss what's the

276
00:10:26,079 --> 00:10:27,920
advantage

277
00:10:27,920 --> 00:10:30,640
is there uh is it because life stresses

278
00:10:30,640 --> 00:10:32,480
are needed

279
00:10:32,480 --> 00:10:35,200
so the ranking loss is not the use

280
00:10:35,200 --> 00:10:37,200
the new metric to evaluate clauses is

281
00:10:37,200 --> 00:10:39,680
more like an alternative to the

282
00:10:39,680 --> 00:10:41,680
classical losses used in such channel

283
00:10:41,680 --> 00:10:42,800
context

284
00:10:42,800 --> 00:10:45,760
because if you want

285
00:10:45,760 --> 00:10:47,920
thanks to a work provided

286
00:10:47,920 --> 00:10:50,959
in at the chest 2019

287
00:10:50,959 --> 00:10:52,560
we know that the

288
00:10:52,560 --> 00:10:56,479
metric classically used in deep learning

289
00:10:56,640 --> 00:10:58,160
approach has not

290
00:10:58,160 --> 00:10:59,760
used

291
00:10:59,760 --> 00:11:01,920
does not totally reflect what an

292
00:11:01,920 --> 00:11:05,680
adversary wants in in such a context so

293
00:11:05,680 --> 00:11:07,519
we have to adapt

294
00:11:07,519 --> 00:11:10,079
the classical learning metric used in

295
00:11:10,079 --> 00:11:12,480
deep learning approach in order to fit

296
00:11:12,480 --> 00:11:15,200
with what an adversary wants in in in

297
00:11:15,200 --> 00:11:16,800
our context

298
00:11:16,800 --> 00:11:17,760
so

299
00:11:17,760 --> 00:11:20,160
what's the advantage uh in comparison

300
00:11:20,160 --> 00:11:22,320
with the cross entropy for example

301
00:11:22,320 --> 00:11:24,480
as we can observe

302
00:11:24,480 --> 00:11:26,560
through this slide the number of

303
00:11:26,560 --> 00:11:28,160
profiling trusses that are needed to

304
00:11:28,160 --> 00:11:30,640
reach a

305
00:11:30,640 --> 00:11:34,240
good solution can be considered as

306
00:11:34,240 --> 00:11:35,680
lesser

307
00:11:35,680 --> 00:11:37,200
so

308
00:11:37,200 --> 00:11:40,000
lower so

309
00:11:40,839 --> 00:11:43,120
uh from

310
00:11:43,120 --> 00:11:45,200
if the adversary has to deal with a

311
00:11:45,200 --> 00:11:48,079
restricted number of trusses

312
00:11:48,079 --> 00:11:49,519
therefore

313
00:11:49,519 --> 00:11:51,360
you can consider the the ranking loss

314
00:11:51,360 --> 00:11:52,320
function

315
00:11:52,320 --> 00:11:53,880
but on the other hand

316
00:11:53,880 --> 00:11:55,760
[Music]

317
00:11:55,760 --> 00:11:57,360
with the different experiments we

318
00:11:57,360 --> 00:11:58,720
provide on

319
00:11:58,720 --> 00:12:00,800
the rankiness function we also observe

320
00:12:00,800 --> 00:12:03,120
that in some cases we have some model

321
00:12:03,120 --> 00:12:05,440
that can retrieve the secret key using

322
00:12:05,440 --> 00:12:07,519
the ranking loss while using other loss

323
00:12:07,519 --> 00:12:08,480
function

324
00:12:08,480 --> 00:12:12,560
other loss function don't don't

325
00:12:14,720 --> 00:12:16,880
okay

326
00:12:16,880 --> 00:12:18,560
sorry

327
00:12:18,560 --> 00:12:20,240
no problem

328
00:12:20,240 --> 00:12:22,240
thanks uh i i think we need to go to the

329
00:12:22,240 --> 00:12:24,399
next talk now if you're more confused

330
00:12:24,399 --> 00:12:27,360
you can post in tulip

331
00:12:27,360 --> 00:12:30,000
so the next talk

332
00:12:30,000 --> 00:12:32,720
is titled the keeping unsupervised

333
00:12:32,720 --> 00:12:35,519
horizontal attacks meet deep learning

334
00:12:35,519 --> 00:12:37,360
and the authors are

335
00:12:37,360 --> 00:12:40,720
galimer perring lukash

336
00:12:40,720 --> 00:12:42,959
jamir lowsky

337
00:12:42,959 --> 00:12:47,440
leila battina and stephan piczek

338
00:12:47,440 --> 00:12:50,480
where mayor the floor is yours

339
00:12:50,480 --> 00:12:52,000
thank you very much are you seeing my

340
00:12:52,000 --> 00:12:54,000
screen can you confirm

341
00:12:54,000 --> 00:12:55,680
okay good thank you

342
00:12:55,680 --> 00:12:57,279
uh so

343
00:12:57,279 --> 00:12:59,519
uh hello everyone thank you for

344
00:12:59,519 --> 00:13:02,720
joining this session uh so i will

345
00:13:02,720 --> 00:13:04,399
shortly present

346
00:13:04,399 --> 00:13:06,000
a summary of our paper keep it

347
00:13:06,000 --> 00:13:07,279
unsupervised

348
00:13:07,279 --> 00:13:08,959
horizontal attacks meet people learning

349
00:13:08,959 --> 00:13:11,440
so what we propose is a framework to

350
00:13:11,440 --> 00:13:13,440
improve horizontal attack results using

351
00:13:13,440 --> 00:13:16,480
deep neural networks

352
00:13:18,959 --> 00:13:21,680
so uh horizontal attacks in practice uh

353
00:13:21,680 --> 00:13:23,760
just very briefly to introduce the

354
00:13:23,760 --> 00:13:27,360
context it's a very limited attack uh in

355
00:13:27,360 --> 00:13:29,760
some cases because it can show very low

356
00:13:29,760 --> 00:13:31,839
occurs so and then

357
00:13:31,839 --> 00:13:34,720
uh in our case uh we use it uh

358
00:13:34,720 --> 00:13:37,120
unsupervised cholesterol attacks using a

359
00:13:37,120 --> 00:13:40,240
k-means technique to retrieve the key

360
00:13:40,240 --> 00:13:42,959
and we did a very simple initial attack

361
00:13:42,959 --> 00:13:46,000
where we recovered 52 percent 52.2

362
00:13:46,000 --> 00:13:48,399
percent of the key and what our target

363
00:13:48,399 --> 00:13:49,440
devices

364
00:13:49,440 --> 00:13:51,519
as i will show later is a ecc

365
00:13:51,519 --> 00:13:53,920
implementation software

366
00:13:53,920 --> 00:13:57,360
so uh the the basics behind our our idea

367
00:13:57,360 --> 00:13:59,920
is that dp learning can be efficient in

368
00:13:59,920 --> 00:14:02,399
some cases where you have noisy or wrong

369
00:14:02,399 --> 00:14:04,079
labels and then

370
00:14:04,079 --> 00:14:05,040
for that

371
00:14:05,040 --> 00:14:06,800
as you can see in this illustration

372
00:14:06,800 --> 00:14:09,120
let's say the dash labels here are the

373
00:14:09,120 --> 00:14:11,440
ones that will be wrong and then in our

374
00:14:11,440 --> 00:14:13,440
case we have two classes the key beats

375
00:14:13,440 --> 00:14:15,279
one and zero

376
00:14:15,279 --> 00:14:16,880
and when you're training when you train

377
00:14:16,880 --> 00:14:19,440
a regularized network network

378
00:14:19,440 --> 00:14:21,839
uh using these noisy labels

379
00:14:21,839 --> 00:14:24,000
and you predict uh with a separate

380
00:14:24,000 --> 00:14:26,480
validation set that also contains noisy

381
00:14:26,480 --> 00:14:29,680
labels uh the network can

382
00:14:29,680 --> 00:14:32,079
some if the regularization is working is

383
00:14:32,079 --> 00:14:34,399
working well it can predict uh the

384
00:14:34,399 --> 00:14:38,160
labels correctly or with lo with less

385
00:14:38,160 --> 00:14:40,959
wrong bits wrong labels at least

386
00:14:40,959 --> 00:14:43,920
and so we propose the framework here

387
00:14:43,920 --> 00:14:46,320
sorry we propose the framework that can

388
00:14:46,320 --> 00:14:49,279
uh iteratively recover uh

389
00:14:49,279 --> 00:14:51,519
the let's say correct the wrong bits

390
00:14:51,519 --> 00:14:53,040
during the process

391
00:14:53,040 --> 00:14:55,519
and the process is the following so we

392
00:14:55,519 --> 00:14:58,240
firstly we take the full data the data

393
00:14:58,240 --> 00:15:00,160
set that contains all the sub traces

394
00:15:00,160 --> 00:15:02,560
that are cutted pieces of from the full

395
00:15:02,560 --> 00:15:05,360
traces in a horizontal attack way

396
00:15:05,360 --> 00:15:07,199
and today those are the traces are

397
00:15:07,199 --> 00:15:08,760
initially labeled with

398
00:15:08,760 --> 00:15:12,000
a horizontal attack and then we train

399
00:15:12,000 --> 00:15:14,000
two separate in the second step we train

400
00:15:14,000 --> 00:15:16,800
two separate models uh which it's a

401
00:15:16,800 --> 00:15:18,560
subset that are

402
00:15:18,560 --> 00:15:21,360
part of the first the big subset the big

403
00:15:21,360 --> 00:15:23,920
training set and then in the next phase

404
00:15:23,920 --> 00:15:26,639
what we do is to swap data sets

405
00:15:26,639 --> 00:15:30,079
uh and then we predict each the network

406
00:15:30,079 --> 00:15:31,759
that was trained with one subset with

407
00:15:31,759 --> 00:15:34,160
another subset and then the uh the

408
00:15:34,160 --> 00:15:36,639
prediction now uh because the network is

409
00:15:36,639 --> 00:15:38,959
regularized and able to do

410
00:15:38,959 --> 00:15:40,720
to correct

411
00:15:40,720 --> 00:15:43,440
the part of this wrong labels it will

412
00:15:43,440 --> 00:15:46,240
predict uh with a lower error rate in

413
00:15:46,240 --> 00:15:49,920
comparison to the first case

414
00:15:49,920 --> 00:15:52,720
so and then at the end the define

415
00:15:52,720 --> 00:15:54,720
we shuffled it we combine the traces

416
00:15:54,720 --> 00:15:57,519
into one group again and then we shuffle

417
00:15:57,519 --> 00:15:59,519
them and then restart the process again

418
00:15:59,519 --> 00:16:01,839
and then in in our work we

419
00:16:01,839 --> 00:16:03,920
we did this process by iterating

420
00:16:03,920 --> 00:16:08,199
iteratively by 50 times

421
00:16:08,240 --> 00:16:10,720
so we test the different scenarios also

422
00:16:10,720 --> 00:16:12,000
uh

423
00:16:12,000 --> 00:16:14,399
with where the networks during the

424
00:16:14,399 --> 00:16:16,720
iterations are uh

425
00:16:16,720 --> 00:16:19,360
can be fixed i mean the params can be

426
00:16:19,360 --> 00:16:21,680
fixed or can be randomized during the

427
00:16:21,680 --> 00:16:24,639
process so for each uh iteration of the

428
00:16:24,639 --> 00:16:26,959
framework we can also select a random uh

429
00:16:26,959 --> 00:16:29,120
group of hyper parameters

430
00:16:29,120 --> 00:16:30,399
and then we test the different

431
00:16:30,399 --> 00:16:32,800
regularization techniques that are

432
00:16:32,800 --> 00:16:34,160
very common for

433
00:16:34,160 --> 00:16:36,000
uh deep learning domains especially

434
00:16:36,000 --> 00:16:37,920
inside chat analysis that is dropout and

435
00:16:37,920 --> 00:16:40,720
data augmentation

436
00:16:40,720 --> 00:16:43,920
um so we consider two different data

437
00:16:43,920 --> 00:16:46,639
sets uh for our case which is uh the

438
00:16:46,639 --> 00:16:49,360
first data set is uh so

439
00:16:49,360 --> 00:16:51,040
the data set is comes from the same

440
00:16:51,040 --> 00:16:52,000
device

441
00:16:52,000 --> 00:16:53,920
which is a software implementation on

442
00:16:53,920 --> 00:16:56,720
arm cortex and of ecc of scalar

443
00:16:56,720 --> 00:16:58,560
multiplication of

444
00:16:58,560 --> 00:17:02,800
q25 519 which is a montgomery ladder

445
00:17:02,800 --> 00:17:04,640
implementation of scalar multiplication

446
00:17:04,640 --> 00:17:06,160
and then the main difference between the

447
00:17:06,160 --> 00:17:07,839
two data sets is that

448
00:17:07,839 --> 00:17:09,839
is the in the conditional swap operation

449
00:17:09,839 --> 00:17:12,959
for constant time and one is is computed

450
00:17:12,959 --> 00:17:14,799
using arithmetic means and the other is

451
00:17:14,799 --> 00:17:17,760
using pointer swapping

452
00:17:17,760 --> 00:17:21,599
uh we measure 300 uh traces 300 scalar

453
00:17:21,599 --> 00:17:22,959
multiplication every scholar

454
00:17:22,959 --> 00:17:25,439
multiplication has a random

455
00:17:25,439 --> 00:17:26,400
scalar

456
00:17:26,400 --> 00:17:27,599
random uh

457
00:17:27,599 --> 00:17:29,760
yes it has a random private key which is

458
00:17:29,760 --> 00:17:31,120
a random scope

459
00:17:31,120 --> 00:17:33,280
and then we split all these traces into

460
00:17:33,280 --> 00:17:36,960
subtraces which will give us sorry 76

461
00:17:36,960 --> 00:17:39,200
and 500 subtrees

462
00:17:39,200 --> 00:17:42,080
and we use it 250 traces for training

463
00:17:42,080 --> 00:17:46,720
and 50 traces for uh testing

464
00:17:46,720 --> 00:17:48,559
so here just to summarize some of the

465
00:17:48,559 --> 00:17:51,200
results uh you can see that

466
00:17:51,200 --> 00:17:52,720
during the iterations of the the

467
00:17:52,720 --> 00:17:56,080
framework iterations we can

468
00:17:56,320 --> 00:17:58,320
when we start with a very low accuracy

469
00:17:58,320 --> 00:18:01,200
around 52 percent uh during the

470
00:18:01,200 --> 00:18:03,440
framework iterations we gradually

471
00:18:03,440 --> 00:18:06,000
increase this accuracy of a single trace

472
00:18:06,000 --> 00:18:08,080
let's say this single trace accuracy

473
00:18:08,080 --> 00:18:10,400
means that the best accuracy out of 50

474
00:18:10,400 --> 00:18:12,480
scalar multiplication traces

475
00:18:12,480 --> 00:18:15,840
and uh sorry i don't know why my slides

476
00:18:15,840 --> 00:18:18,799
are going to the next uh so uh

477
00:18:18,799 --> 00:18:21,200
then we can see that for situations when

478
00:18:21,200 --> 00:18:24,000
we have uh regularization especially in

479
00:18:24,000 --> 00:18:25,280
the uh

480
00:18:25,280 --> 00:18:28,480
the second data set on the right

481
00:18:28,480 --> 00:18:31,280
it it converts very fast to 100 percent

482
00:18:31,280 --> 00:18:34,000
and for this cis swap are its data set

483
00:18:34,000 --> 00:18:36,080
it's a bit more difficult data set and

484
00:18:36,080 --> 00:18:38,240
then it takes more time but also we can

485
00:18:38,240 --> 00:18:39,520
see the combining multiple

486
00:18:39,520 --> 00:18:41,520
regularization methods which is dropout

487
00:18:41,520 --> 00:18:43,120
and data augmentation

488
00:18:43,120 --> 00:18:45,840
uh it it goes to 100 as well after

489
00:18:45,840 --> 00:18:48,559
something like 54 iterations

490
00:18:48,559 --> 00:18:50,400
but when you use data augmentation only

491
00:18:50,400 --> 00:18:53,120
on this data set or no regularization

492
00:18:53,120 --> 00:18:55,039
uh you can see that the

493
00:18:55,039 --> 00:18:56,640
attack doesn't work

494
00:18:56,640 --> 00:18:58,720
even if in this is what pointer data set

495
00:18:58,720 --> 00:19:02,080
which is easier uh than the first uh

496
00:19:02,080 --> 00:19:02,960
it

497
00:19:02,960 --> 00:19:05,840
doesn't reach 100

498
00:19:06,400 --> 00:19:08,480
when we also uh randomize the ip

499
00:19:08,480 --> 00:19:09,840
parameters during the framework

500
00:19:09,840 --> 00:19:11,600
iterations you can see that for the

501
00:19:11,600 --> 00:19:13,679
first data set the results are slightly

502
00:19:13,679 --> 00:19:15,039
better

503
00:19:15,039 --> 00:19:15,840
uh

504
00:19:15,840 --> 00:19:16,640
but

505
00:19:16,640 --> 00:19:18,240
they said for the second one the result

506
00:19:18,240 --> 00:19:21,600
doesn't improve so uh

507
00:19:21,600 --> 00:19:23,360
testing with different cases you can see

508
00:19:23,360 --> 00:19:25,120
that the results can get a bit better

509
00:19:25,120 --> 00:19:27,520
and use a different combination of

510
00:19:27,520 --> 00:19:30,559
random hyper parameters

511
00:19:30,559 --> 00:19:33,840
we also try uh improve the bt results

512
00:19:33,840 --> 00:19:34,799
for

513
00:19:34,799 --> 00:19:36,640
some cases when we use it gradient

514
00:19:36,640 --> 00:19:39,600
virtualization to trim

515
00:19:39,600 --> 00:19:40,880
the interval

516
00:19:40,880 --> 00:19:43,600
and in this case we can select a sub

517
00:19:43,600 --> 00:19:46,400
area of the trace in order to focus on

518
00:19:46,400 --> 00:19:47,840
the where the leakage is happening and

519
00:19:47,840 --> 00:19:50,799
then we observe that

520
00:19:50,799 --> 00:19:52,720
the the gradients are higher when the

521
00:19:52,720 --> 00:19:54,880
leakage is located on the trace so if

522
00:19:54,880 --> 00:19:56,640
you compare the signal to noise radio to

523
00:19:56,640 --> 00:19:58,400
the gradient you can see that the peaks

524
00:19:58,400 --> 00:20:00,720
are located more or less around the same

525
00:20:00,720 --> 00:20:02,080
places

526
00:20:02,080 --> 00:20:03,840
um

527
00:20:03,840 --> 00:20:06,480
so just a final slide here to summarize

528
00:20:06,480 --> 00:20:08,960
what we got uh we tested many different

529
00:20:08,960 --> 00:20:11,280
scenarios so for each scenario we run

530
00:20:11,280 --> 00:20:14,159
the algorithm the framework 10 times

531
00:20:14,159 --> 00:20:16,320
and you can see that

532
00:20:16,320 --> 00:20:18,320
in some cases we don't succeed at all

533
00:20:18,320 --> 00:20:20,320
like say when we have no regularization

534
00:20:20,320 --> 00:20:22,240
for example but when we add some

535
00:20:22,240 --> 00:20:23,760
regularization

536
00:20:23,760 --> 00:20:25,039
to this

537
00:20:25,039 --> 00:20:26,480
scenarios we can

538
00:20:26,480 --> 00:20:29,760
sometimes succeed sometimes we don't but

539
00:20:29,760 --> 00:20:31,520
what we see is that

540
00:20:31,520 --> 00:20:33,039
the results are

541
00:20:33,039 --> 00:20:36,240
largely improved when you have uh

542
00:20:36,240 --> 00:20:38,000
when you consider gradient visualization

543
00:20:38,000 --> 00:20:41,440
especially for c swap a read data set

544
00:20:41,440 --> 00:20:43,840
and when we in the first case we

545
00:20:43,840 --> 00:20:46,480
uh we succeed one time out of 40 and

546
00:20:46,480 --> 00:20:48,400
when we stream the interval we should

547
00:20:48,400 --> 00:20:51,600
see 19 times out of 40. so using the dp

548
00:20:51,600 --> 00:20:53,039
learning mechanisms to improve the

549
00:20:53,039 --> 00:20:55,280
attack we can drastically

550
00:20:55,280 --> 00:20:57,120
i mean significantly improve the attack

551
00:20:57,120 --> 00:20:58,320
results

552
00:20:58,320 --> 00:20:59,840
for the other data sets the gradient

553
00:20:59,840 --> 00:21:02,240
visualization does improved so much but

554
00:21:02,240 --> 00:21:04,159
using regularization of course it's the

555
00:21:04,159 --> 00:21:07,840
key elements to make successful

556
00:21:08,400 --> 00:21:10,720
so just a summary and conclusions of the

557
00:21:10,720 --> 00:21:11,520
work

558
00:21:11,520 --> 00:21:14,880
so in this paper we show that

559
00:21:14,880 --> 00:21:16,960
we proposed that uh

560
00:21:16,960 --> 00:21:19,360
different a new unsupervised attack

561
00:21:19,360 --> 00:21:21,360
combining horizontal attack and dp

562
00:21:21,360 --> 00:21:22,720
learning approaches

563
00:21:22,720 --> 00:21:25,039
that can uh iteratively improve and

564
00:21:25,039 --> 00:21:27,600
correct uh the wrong bits in the

565
00:21:27,600 --> 00:21:31,440
in the private key uh obtain it from the

566
00:21:31,440 --> 00:21:33,039
horizontal attack

567
00:21:33,039 --> 00:21:35,280
and uh we should

568
00:21:35,280 --> 00:21:37,600
we observed that regular regularizing

569
00:21:37,600 --> 00:21:39,440
neural networks is the

570
00:21:39,440 --> 00:21:42,400
are the key to to do this

571
00:21:42,400 --> 00:21:43,520
and we

572
00:21:43,520 --> 00:21:45,440
we assume that this framework should be

573
00:21:45,440 --> 00:21:47,200
applicable to other public

574
00:21:47,200 --> 00:21:50,000
implementations for example rsa

575
00:21:50,000 --> 00:21:52,080
scholar modular exponentiation for

576
00:21:52,080 --> 00:21:53,120
example

577
00:21:53,120 --> 00:21:54,080
and

578
00:21:54,080 --> 00:21:56,559
we plan to investigate for now some

579
00:21:56,559 --> 00:21:58,240
other

580
00:21:58,240 --> 00:21:59,840
benefits from the framework from the

581
00:21:59,840 --> 00:22:02,000
framework for example against race

582
00:22:02,000 --> 00:22:03,760
misalignment or

583
00:22:03,760 --> 00:22:05,280
uh analyzing how the label

584
00:22:05,280 --> 00:22:06,799
initialization process from the

585
00:22:06,799 --> 00:22:09,120
horizontal attack can interfere in the

586
00:22:09,120 --> 00:22:12,159
in the attack results uh for example

587
00:22:12,159 --> 00:22:14,080
yeah so this is my short presentation

588
00:22:14,080 --> 00:22:16,000
would like to thank you and uh if you

589
00:22:16,000 --> 00:22:18,159
would like to check out our day i'll

590
00:22:18,159 --> 00:22:20,320
call this available disney

591
00:22:20,320 --> 00:22:21,600
thank you

592
00:22:21,600 --> 00:22:23,840
thank you for the interesting talk uh

593
00:22:23,840 --> 00:22:26,000
there is one question

594
00:22:26,000 --> 00:22:28,320
asked in the ulip

595
00:22:28,320 --> 00:22:30,559
that reads in your experiments you have

596
00:22:30,559 --> 00:22:33,039
started with a very low accuracy let's

597
00:22:33,039 --> 00:22:37,679
say 52 percent um maybe hope to correct

598
00:22:37,679 --> 00:22:39,840
errors with your methodology starting

599
00:22:39,840 --> 00:22:40,960
but yeah

600
00:22:40,960 --> 00:22:43,840
lucky totally random clustering i guess

601
00:22:43,840 --> 00:22:47,440
no that was the answer of this um

602
00:22:47,440 --> 00:22:48,480
attendee

603
00:22:48,480 --> 00:22:50,559
thus my question is what kind of

604
00:22:50,559 --> 00:22:52,720
information must be

605
00:22:52,720 --> 00:22:56,960
brought by the initial clustering

606
00:22:57,919 --> 00:22:59,520
um

607
00:22:59,520 --> 00:23:01,360
if i understood correctly the question

608
00:23:01,360 --> 00:23:03,039
uh so the

609
00:23:03,039 --> 00:23:05,520
this is about the how to

610
00:23:05,520 --> 00:23:08,559
how to initialize the labels how how to

611
00:23:08,559 --> 00:23:09,679
make the

612
00:23:09,679 --> 00:23:10,720
uh

613
00:23:10,720 --> 00:23:13,520
the initial labels uh properly for the

614
00:23:13,520 --> 00:23:15,520
framework to work

615
00:23:15,520 --> 00:23:18,880
so we tested already some random labels

616
00:23:18,880 --> 00:23:20,799
initially to see how it worked because

617
00:23:20,799 --> 00:23:23,360
the 52 percent is very close to run if

618
00:23:23,360 --> 00:23:26,320
you see and then uh but it's if you if

619
00:23:26,320 --> 00:23:28,159
you look at the paper some some of the

620
00:23:28,159 --> 00:23:30,159
figures uh which we do a leakage

621
00:23:30,159 --> 00:23:33,360
assessment on this uh 52 and then you

622
00:23:33,360 --> 00:23:34,559
can see that

623
00:23:34,559 --> 00:23:37,360
uh even if with 52 percent of accuracy

624
00:23:37,360 --> 00:23:39,120
you can still

625
00:23:39,120 --> 00:23:40,799
more or less uh

626
00:23:40,799 --> 00:23:42,799
signal to noise ratio peaks around the

627
00:23:42,799 --> 00:23:44,960
same real leakage parts

628
00:23:44,960 --> 00:23:46,799
but of course

629
00:23:46,799 --> 00:23:49,520
in many scenarios as we as i try to show

630
00:23:49,520 --> 00:23:52,320
in this slide here we don't succeed so

631
00:23:52,320 --> 00:23:53,279
uh

632
00:23:53,279 --> 00:23:56,240
the the better the higher the initiator

633
00:23:56,240 --> 00:23:58,400
is of course the better but in many

634
00:23:58,400 --> 00:24:00,720
situations if you if you're very close

635
00:24:00,720 --> 00:24:04,159
to 50 50 you will not succeed but if you

636
00:24:04,159 --> 00:24:06,480
go slightly above there are chances that

637
00:24:06,480 --> 00:24:08,240
the framework will start correcting the

638
00:24:08,240 --> 00:24:10,559
bits along the framework iterations so

639
00:24:10,559 --> 00:24:12,559
we can be i we believe we can be still

640
00:24:12,559 --> 00:24:16,399
lower than 52 the initial level

641
00:24:17,039 --> 00:24:18,400
yeah i don't know if i answered the

642
00:24:18,400 --> 00:24:20,159
question but that's why i

643
00:24:20,159 --> 00:24:22,720
understood it would be the way

644
00:24:22,720 --> 00:24:24,880
thank you i suggest that if there's any

645
00:24:24,880 --> 00:24:27,120
a follow-up question you can

646
00:24:27,120 --> 00:24:28,159
ask

647
00:24:28,159 --> 00:24:30,000
after this

648
00:24:30,000 --> 00:24:32,720
session or just by a zulib

649
00:24:32,720 --> 00:24:33,760
and

650
00:24:33,760 --> 00:24:36,799
if you agree we can go to the next talk

651
00:24:36,799 --> 00:24:39,360
because i don't want to

652
00:24:39,360 --> 00:24:44,080
run late the next arc will be given by

653
00:24:44,960 --> 00:24:46,799
let me first thank you for the nice

654
00:24:46,799 --> 00:24:49,279
presentation i just forget about it

655
00:24:49,279 --> 00:24:50,960
it's late in the

656
00:24:50,960 --> 00:24:53,440
usa sorry about it

657
00:24:53,440 --> 00:24:56,159
uh so the next thought will be given by

658
00:24:56,159 --> 00:24:59,840
xiangjung and the paper is entitled pay

659
00:24:59,840 --> 00:25:03,120
attention pay attention to raw traces a

660
00:25:03,120 --> 00:25:04,720
deep learning architecture for

661
00:25:04,720 --> 00:25:07,279
end-to-end profiling attack and the

662
00:25:07,279 --> 00:25:10,159
paper is written by shangren

663
00:25:10,159 --> 00:25:12,320
she

664
00:25:13,120 --> 00:25:16,400
i think it is pronounced pain and down

665
00:25:16,400 --> 00:25:18,559
and hanging

666
00:25:18,559 --> 00:25:21,360
children the floor is yours

667
00:25:21,360 --> 00:25:24,880
okay thank you uh can you hear me and

668
00:25:24,880 --> 00:25:26,240
see my slides

669
00:25:26,240 --> 00:25:30,799
yes we can okay thank you

670
00:25:33,039 --> 00:25:35,440
okay

671
00:25:38,880 --> 00:25:41,760
okay uh hello everybody welcome to

672
00:25:41,760 --> 00:25:44,559
this presentation and i will be giving a

673
00:25:44,559 --> 00:25:47,039
talk about our written set paper

674
00:25:47,039 --> 00:25:49,200
pay attention to rochi the deep learning

675
00:25:49,200 --> 00:25:51,120
architecture for end-to-end profiling

676
00:25:51,120 --> 00:25:52,640
attack

677
00:25:52,640 --> 00:25:54,799
so let's start with the basic motivation

678
00:25:54,799 --> 00:25:57,279
of the work deep learning has been

679
00:25:57,279 --> 00:25:58,960
widely used in the third channel

680
00:25:58,960 --> 00:26:01,120
analytics in recent years

681
00:26:01,120 --> 00:26:02,799
many worlds have deleted the

682
00:26:02,799 --> 00:26:04,559
contemporaries and addressed the

683
00:26:04,559 --> 00:26:06,960
difficultization issues simultaneously

684
00:26:06,960 --> 00:26:08,640
by deep learning

685
00:26:08,640 --> 00:26:11,520
however in almost all of these papers

686
00:26:11,520 --> 00:26:14,080
the selected narrative intervals instead

687
00:26:14,080 --> 00:26:16,640
of the word cities are used uh even when

688
00:26:16,640 --> 00:26:18,480
the implementation is protected by

689
00:26:18,480 --> 00:26:20,000
masking

690
00:26:20,000 --> 00:26:21,679
that means there is a manual feature

691
00:26:21,679 --> 00:26:24,720
expression before the profaning but if

692
00:26:24,720 --> 00:26:26,880
we consider a practical black box

693
00:26:26,880 --> 00:26:29,440
analysis on a master implementation

694
00:26:29,440 --> 00:26:32,000
locating the requisite is arguably the

695
00:26:32,000 --> 00:26:33,600
most exciting part of the whole

696
00:26:33,600 --> 00:26:34,960
narrative

697
00:26:34,960 --> 00:26:37,120
therefore we argue that to fully utilize

698
00:26:37,120 --> 00:26:38,000
the

699
00:26:38,000 --> 00:26:40,000
potential of deep learning and get rid

700
00:26:40,000 --> 00:26:43,039
of any manual intervention we need

701
00:26:43,039 --> 00:26:45,120
end-to-end profiling that directly

702
00:26:45,120 --> 00:26:46,880
mapped raw choices to target

703
00:26:46,880 --> 00:26:48,960
intermediate values

704
00:26:48,960 --> 00:26:51,360
so in our paper we propose an end-to-end

705
00:26:51,360 --> 00:26:53,440
architecture composed of encoders

706
00:26:53,440 --> 00:26:56,000
attention mechanisms and a classifier to

707
00:26:56,000 --> 00:26:59,679
conduct the end-to-end profiling

708
00:26:59,840 --> 00:27:02,400
and here is our new architecture we

709
00:27:02,400 --> 00:27:04,120
propose this architecture to

710
00:27:04,120 --> 00:27:05,919
professionalism no matter they are

711
00:27:05,919 --> 00:27:08,960
disciplined or protected by masking

712
00:27:08,960 --> 00:27:10,960
as we can see in this figure

713
00:27:10,960 --> 00:27:13,679
the encoder will first include logistics

714
00:27:13,679 --> 00:27:16,159
to the optogenetic features

715
00:27:16,159 --> 00:27:18,080
then the attention mechanism will give

716
00:27:18,080 --> 00:27:18,880
each

717
00:27:18,880 --> 00:27:21,360
feature a score and we need to sum them

718
00:27:21,360 --> 00:27:22,159
up

719
00:27:22,159 --> 00:27:24,559
finally the classifier will generate the

720
00:27:24,559 --> 00:27:26,399
probabilities from the final feature

721
00:27:26,399 --> 00:27:28,080
vector

722
00:27:28,080 --> 00:27:30,240
next i will give a brief introduction of

723
00:27:30,240 --> 00:27:33,679
each component in our architecture

724
00:27:33,679 --> 00:27:35,919
first is the encoder companies the

725
00:27:35,919 --> 00:27:38,080
encoder the encoder company includes two

726
00:27:38,080 --> 00:27:40,080
parts the junior encoder and the senior

727
00:27:40,080 --> 00:27:41,279
encoder

728
00:27:41,279 --> 00:27:43,440
in junior encoder we use a locally

729
00:27:43,440 --> 00:27:45,919
connected layer or stacked convolutional

730
00:27:45,919 --> 00:27:49,039
layers to encode the raw pieces and get

731
00:27:49,039 --> 00:27:52,159
the fan grid features

732
00:27:52,159 --> 00:27:53,919
for the senior encoder we use a long

733
00:27:53,919 --> 00:27:57,200
short-term memory we use rtm because it

734
00:27:57,200 --> 00:27:59,200
could learn to control the data flow

735
00:27:59,200 --> 00:28:01,919
automatically when it goes through the

736
00:28:01,919 --> 00:28:03,360
sequential data

737
00:28:03,360 --> 00:28:06,640
and there are three keys in rtm uh

738
00:28:06,640 --> 00:28:08,799
this case will control what information

739
00:28:08,799 --> 00:28:12,000
is collected for both and eod

740
00:28:12,000 --> 00:28:14,559
and there are also other grid properties

741
00:28:14,559 --> 00:28:16,960
of irstm to handle the long sequence of

742
00:28:16,960 --> 00:28:18,640
data

743
00:28:18,640 --> 00:28:21,840
the lstm in our architecture works

744
00:28:21,840 --> 00:28:24,799
under the sequence two sequence modes

745
00:28:24,799 --> 00:28:26,799
because including a two long segment

746
00:28:26,799 --> 00:28:28,960
into a single feature vector is zero too

747
00:28:28,960 --> 00:28:32,000
hard even we use rstm

748
00:28:32,000 --> 00:28:34,000
so we need uh we need the attention

749
00:28:34,000 --> 00:28:37,360
mechanism to reduce the hardness and

750
00:28:37,360 --> 00:28:40,240
exposing the hiding state of rtm it's

751
00:28:40,240 --> 00:28:45,039
just the precondition to your attention

752
00:28:45,039 --> 00:28:47,919
and the next is the attention component

753
00:28:47,919 --> 00:28:50,399
the attention mechanism evaluates each

754
00:28:50,399 --> 00:28:52,960
feature vector is generated from the

755
00:28:52,960 --> 00:28:55,760
cleaner senior encoder and gives of them

756
00:28:55,760 --> 00:28:57,520
a probability

757
00:28:57,520 --> 00:29:00,480
so it essentially part of the sequence

758
00:29:00,480 --> 00:29:02,559
by the probabilities where the value is

759
00:29:02,559 --> 00:29:04,000
large enough

760
00:29:04,000 --> 00:29:06,880
so the rstm could focus on the parties

761
00:29:06,880 --> 00:29:09,039
subsequences rather than the whole

762
00:29:09,039 --> 00:29:10,480
sequence

763
00:29:10,480 --> 00:29:13,679
and for uh implementation details uh we

764
00:29:13,679 --> 00:29:16,720
modify the bottlenose attention and use

765
00:29:16,720 --> 00:29:18,880
the variant with an additional batch

766
00:29:18,880 --> 00:29:20,399
normalization

767
00:29:20,399 --> 00:29:22,480
for better performance on the raw

768
00:29:22,480 --> 00:29:24,960
treated

769
00:29:25,039 --> 00:29:27,919
uh so so what what do we get through our

770
00:29:27,919 --> 00:29:30,960
new architecture and uh share uh some

771
00:29:30,960 --> 00:29:33,360
basic information about our experiments

772
00:29:33,360 --> 00:29:35,919
uh basically we use the identity label

773
00:29:35,919 --> 00:29:38,640
and uh we consider both different and

774
00:29:38,640 --> 00:29:40,960
difficult scenarios

775
00:29:40,960 --> 00:29:43,279
below are the data side we used

776
00:29:43,279 --> 00:29:44,399
and

777
00:29:44,399 --> 00:29:46,799
we can you can see in the fourth column

778
00:29:46,799 --> 00:29:49,039
that we could use over four hundred

779
00:29:49,039 --> 00:29:52,559
thousand time samples directly

780
00:29:53,279 --> 00:29:55,360
to save time uh

781
00:29:55,360 --> 00:29:57,360
we only saw the attacking result of ad

782
00:29:57,360 --> 00:29:58,799
cap data size

783
00:29:58,799 --> 00:30:01,919
we refer to our paper for more detail

784
00:30:01,919 --> 00:30:04,720
we can see in the third column

785
00:30:04,720 --> 00:30:07,200
that for the synchronized switches we

786
00:30:07,200 --> 00:30:09,520
need only six to eight choices

787
00:30:09,520 --> 00:30:11,919
to recover the keys of both scale v1 and

788
00:30:11,919 --> 00:30:14,320
v2

789
00:30:14,559 --> 00:30:17,440
for the desynchronized scenario we find

790
00:30:17,440 --> 00:30:19,760
that the number of streets is not quite

791
00:30:19,760 --> 00:30:22,159
enough so we conduct a data augmentation

792
00:30:22,159 --> 00:30:23,600
in this scenario

793
00:30:23,600 --> 00:30:26,320
and as a result we could also reduce the

794
00:30:26,320 --> 00:30:30,320
entropy to zero with very few choices

795
00:30:31,440 --> 00:30:34,240
and here is a summary of our tests on

796
00:30:34,240 --> 00:30:36,880
different data sites the second column

797
00:30:36,880 --> 00:30:39,520
is the random delay we used to simulate

798
00:30:39,520 --> 00:30:42,399
to simulate the discrimination and the

799
00:30:42,399 --> 00:30:44,640
third column is the number of choices to

800
00:30:44,640 --> 00:30:46,960
recover the correct key

801
00:30:46,960 --> 00:30:49,279
you can see that from for most of the

802
00:30:49,279 --> 00:30:51,279
cases we could reduce the gate entropy

803
00:30:51,279 --> 00:30:54,159
to zero in several gc therefore our

804
00:30:54,159 --> 00:30:55,279
attacks

805
00:30:55,279 --> 00:30:57,840
are even more efficient than the

806
00:30:57,840 --> 00:31:02,240
networks trained on the shortened trees

807
00:31:02,799 --> 00:31:05,360
and finally uh i will show some very

808
00:31:05,360 --> 00:31:08,320
decent of our network to explain how the

809
00:31:08,320 --> 00:31:10,399
potential mechanism works

810
00:31:10,399 --> 00:31:12,799
uh the where these things give a insight

811
00:31:12,799 --> 00:31:14,880
of how our tension mechanism focus on

812
00:31:14,880 --> 00:31:17,519
the informative interval

813
00:31:17,519 --> 00:31:19,760
as we can see in the subfigure f

814
00:31:19,760 --> 00:31:22,000
we plot the uh

815
00:31:22,000 --> 00:31:24,240
probabilities of forward and backward

816
00:31:24,240 --> 00:31:25,440
attention

817
00:31:25,440 --> 00:31:27,919
we could observe we could observe that

818
00:31:27,919 --> 00:31:29,919
both attention instantly pay special

819
00:31:29,919 --> 00:31:33,360
attention just after the rtm go through

820
00:31:33,360 --> 00:31:37,279
the time steps between about 800 and 900

821
00:31:37,279 --> 00:31:39,840
and this time steps correspond to an

822
00:31:39,840 --> 00:31:43,039
interval around index 45 000 on the raw

823
00:31:43,039 --> 00:31:44,399
traces

824
00:31:44,399 --> 00:31:46,240
in other words this interval is

825
00:31:46,240 --> 00:31:47,679
suggested by

826
00:31:47,679 --> 00:31:50,240
our change mechanism to be the

827
00:31:50,240 --> 00:31:52,480
most informative interval on the world

828
00:31:52,480 --> 00:31:53,840
choices

829
00:31:53,840 --> 00:31:56,960
and note surprisingly this interval

830
00:31:56,960 --> 00:31:59,440
includes the 700 time samples that are

831
00:31:59,440 --> 00:32:01,919
manually selected by the authors authors

832
00:32:01,919 --> 00:32:03,519
of icat

833
00:32:03,519 --> 00:32:05,679
so our attention mechanism could indeed

834
00:32:05,679 --> 00:32:09,279
find out the informative interval

835
00:32:09,679 --> 00:32:12,799
okay finally we move to the conclusion

836
00:32:12,799 --> 00:32:15,039
in this paper we introduced a neural

837
00:32:15,039 --> 00:32:16,720
network architecture for end-to-end

838
00:32:16,720 --> 00:32:19,200
profiling that means rather than

839
00:32:19,200 --> 00:32:20,720
leveraging the knowledge of

840
00:32:20,720 --> 00:32:23,200
implementations to manually locate the

841
00:32:23,200 --> 00:32:25,840
informative intervals we could profile

842
00:32:25,840 --> 00:32:28,559
the rotation directly now

843
00:32:28,559 --> 00:32:29,760
we also

844
00:32:29,760 --> 00:32:32,000
introduced some new structures into the

845
00:32:32,000 --> 00:32:34,559
xa field to build the architecture like

846
00:32:34,559 --> 00:32:37,120
the locally connected layers and

847
00:32:37,120 --> 00:32:39,039
extension mechanisms

848
00:32:39,039 --> 00:32:41,600
besides the effectiveness we find our

849
00:32:41,600 --> 00:32:43,519
approach working under the

850
00:32:43,519 --> 00:32:44,960
end-to-end context

851
00:32:44,960 --> 00:32:47,600
performed even better than the network

852
00:32:47,600 --> 00:32:50,240
trained on the reduced treaty

853
00:32:50,240 --> 00:32:52,320
and we also explore how the tension

854
00:32:52,320 --> 00:32:54,159
mechanism works in the third channel

855
00:32:54,159 --> 00:32:56,960
context to verify that our our

856
00:32:56,960 --> 00:33:00,720
architecture works as we design

857
00:33:01,279 --> 00:33:03,039
so this is the end of my short

858
00:33:03,039 --> 00:33:05,279
presentation thank you for listening and

859
00:33:05,279 --> 00:33:06,880
i'm willing to answer all of your

860
00:33:06,880 --> 00:33:08,799
questions

861
00:33:08,799 --> 00:33:10,559
thanks for your presentation

862
00:33:10,559 --> 00:33:13,200
uh there is a question did you label the

863
00:33:13,200 --> 00:33:16,240
traces with just for example first round

864
00:33:16,240 --> 00:33:19,519
intermediate values but trend using full

865
00:33:19,519 --> 00:33:21,360
round that is

866
00:33:21,360 --> 00:33:24,559
10 round traces

867
00:33:24,559 --> 00:33:27,039
uh yes we just label the treatise uh

868
00:33:27,039 --> 00:33:29,760
with the output of exports in the first

869
00:33:29,760 --> 00:33:31,519
round and uh

870
00:33:31,519 --> 00:33:32,640
uh in

871
00:33:32,640 --> 00:33:34,799
training we could use the

872
00:33:34,799 --> 00:33:39,360
other other uh the four length ratio

873
00:33:40,320 --> 00:33:43,120
thanks uh next question is how long does

874
00:33:43,120 --> 00:33:44,480
the

875
00:33:44,480 --> 00:33:47,519
uh la la does the training phase for

876
00:33:47,519 --> 00:33:50,720
end-to-end profiling model last

877
00:33:50,720 --> 00:33:53,279
uh have you ever encountered

878
00:33:53,279 --> 00:33:54,960
convergence issues

879
00:33:54,960 --> 00:33:57,279
which is often met with the deep

880
00:33:57,279 --> 00:33:59,600
learning based channel analysis

881
00:33:59,600 --> 00:34:02,640
especially against mask implementations

882
00:34:02,640 --> 00:34:06,159
yes we give some static statistics in

883
00:34:06,159 --> 00:34:09,199
our people about uh how long it is

884
00:34:09,199 --> 00:34:12,000
relative to converge uh end-to-end pro

885
00:34:12,000 --> 00:34:13,199
profiling

886
00:34:13,199 --> 00:34:14,639
so uh

887
00:34:14,639 --> 00:34:16,719
generally uh

888
00:34:16,719 --> 00:34:19,520
it will take uh more than one or two

889
00:34:19,520 --> 00:34:22,079
days but most time it will converge in

890
00:34:22,079 --> 00:34:23,599
several days

891
00:34:23,599 --> 00:34:25,919
and

892
00:34:31,199 --> 00:34:34,560
okay that's it

893
00:34:34,560 --> 00:34:35,760
thanks uh

894
00:34:35,760 --> 00:34:38,399
we need to move to the next talk now and

895
00:34:38,399 --> 00:34:39,839
uh

896
00:34:39,839 --> 00:34:41,520
if you have more questions you can post

897
00:34:41,520 --> 00:34:43,520
in tulip

898
00:34:43,520 --> 00:34:47,440
so the next talk is titled the dlla deep

899
00:34:47,440 --> 00:34:50,079
learning leakage assessment among the

900
00:34:50,079 --> 00:34:52,960
modern road

901
00:34:54,960 --> 00:34:57,280
sorry

902
00:34:58,560 --> 00:35:02,000
a modern road map for sa evaluations uh

903
00:35:02,000 --> 00:35:04,000
the authors are so

904
00:35:04,000 --> 00:35:05,760
ben moose

905
00:35:05,760 --> 00:35:07,200
felix

906
00:35:07,200 --> 00:35:09,760
wegener and amir moradi

907
00:35:09,760 --> 00:35:12,480
sermon uh the floor is yours

908
00:35:12,480 --> 00:35:14,400
okay thank you thank you for the nice

909
00:35:14,400 --> 00:35:16,640
introduction and also thanks

910
00:35:16,640 --> 00:35:18,800
for joining to this short presentation

911
00:35:18,800 --> 00:35:21,520
of our paper

912
00:35:21,520 --> 00:35:24,000
this work deals with leakage assessment

913
00:35:24,000 --> 00:35:26,000
and leakage assessment is essentially a

914
00:35:26,000 --> 00:35:28,400
technique we use to determine whether

915
00:35:28,400 --> 00:35:30,480
input dependent information is included

916
00:35:30,480 --> 00:35:32,640
in side channel measurements of a device

917
00:35:32,640 --> 00:35:35,040
under test

918
00:35:35,040 --> 00:35:36,880
the most common approach to do this is

919
00:35:36,880 --> 00:35:38,000
trying to

920
00:35:38,000 --> 00:35:39,920
distinguish leakage distributions for

921
00:35:39,920 --> 00:35:41,520
different input classes using a

922
00:35:41,520 --> 00:35:44,079
statistical hypothesis test and the most

923
00:35:44,079 --> 00:35:45,839
commonly used input classes for that are

924
00:35:45,839 --> 00:35:48,720
fixed was random and fixed was fixed

925
00:35:48,720 --> 00:35:50,320
and the most common statistical

926
00:35:50,320 --> 00:35:52,400
hypothesis tests are the worst t-test

927
00:35:52,400 --> 00:35:55,599
and the pearson's g-square test

928
00:35:55,599 --> 00:35:57,520
these classical tests have been used for

929
00:35:57,520 --> 00:35:59,520
many years in the site channel community

930
00:35:59,520 --> 00:36:01,119
and they are a great tool for many

931
00:36:01,119 --> 00:36:02,480
purposes

932
00:36:02,480 --> 00:36:04,880
however they also have some drawbacks

933
00:36:04,880 --> 00:36:07,440
and the main one is that the tests are

934
00:36:07,440 --> 00:36:09,280
normally applied to each point in the

935
00:36:09,280 --> 00:36:12,079
trace individually and independently

936
00:36:12,079 --> 00:36:14,240
therefore they are typically not able to

937
00:36:14,240 --> 00:36:16,560
detect leakages which are spread over

938
00:36:16,560 --> 00:36:19,359
multiple time samples like multivariate

939
00:36:19,359 --> 00:36:21,440
or horizontal leakages

940
00:36:21,440 --> 00:36:23,599
and also the separation of statistical

941
00:36:23,599 --> 00:36:25,520
orders in the t-test has been shown to

942
00:36:25,520 --> 00:36:26,400
lead to

943
00:36:26,400 --> 00:36:30,480
false negatives in some corner cases

944
00:36:30,480 --> 00:36:33,440
so we thought about different strategies

945
00:36:33,440 --> 00:36:35,440
and have noticed that leakage assessment

946
00:36:35,440 --> 00:36:36,960
is essentially a statistical

947
00:36:36,960 --> 00:36:38,640
classification problem

948
00:36:38,640 --> 00:36:40,640
if we are able to build a software that

949
00:36:40,640 --> 00:36:42,560
determines correctly whether a given

950
00:36:42,560 --> 00:36:44,960
side channel trace belongs to group 0 or

951
00:36:44,960 --> 00:36:47,520
group 1 with a better success rate than

952
00:36:47,520 --> 00:36:50,000
guessing over the whole set of traces

953
00:36:50,000 --> 00:36:51,760
then we can conclude that input

954
00:36:51,760 --> 00:36:53,680
dependent information is included in the

955
00:36:53,680 --> 00:36:54,960
trace set

956
00:36:54,960 --> 00:36:56,880
and to us this sounded like a perfect

957
00:36:56,880 --> 00:37:00,240
application for deep neural networks

958
00:37:00,240 --> 00:37:02,720
so how is this done first we record a

959
00:37:02,720 --> 00:37:04,640
set of side channel measurements in

960
00:37:04,640 --> 00:37:06,720
fixed versus fixed manner

961
00:37:06,720 --> 00:37:08,560
then we split the whole set into a

962
00:37:08,560 --> 00:37:11,119
training set and validation set and

963
00:37:11,119 --> 00:37:12,960
train the neural network on the training

964
00:37:12,960 --> 00:37:14,240
set

965
00:37:14,240 --> 00:37:16,560
afterwards we use a trend network to

966
00:37:16,560 --> 00:37:18,480
classify the traces in the validation

967
00:37:18,480 --> 00:37:20,720
set while hiding the true labels from

968
00:37:20,720 --> 00:37:22,400
the network

969
00:37:22,400 --> 00:37:24,480
then we determine the number of correct

970
00:37:24,480 --> 00:37:26,880
classifications achieved by the

971
00:37:26,880 --> 00:37:29,440
network on the validation set and what

972
00:37:29,440 --> 00:37:31,359
we are able to do with this number is

973
00:37:31,359 --> 00:37:34,000
calculating the probability that this

974
00:37:34,000 --> 00:37:36,000
amount of correct classifications could

975
00:37:36,000 --> 00:37:38,320
have been achieved just by chance by a

976
00:37:38,320 --> 00:37:41,520
classifier that is merely guessing

977
00:37:41,520 --> 00:37:43,599
and if this probability is below a

978
00:37:43,599 --> 00:37:45,839
certain threshold we reject the null

979
00:37:45,839 --> 00:37:48,400
hypothesis because we can conclude that

980
00:37:48,400 --> 00:37:50,240
indeed information is included in the

981
00:37:50,240 --> 00:37:51,839
traces

982
00:37:51,839 --> 00:37:54,160
which allows classification into the two

983
00:37:54,160 --> 00:37:56,480
groups

984
00:37:56,800 --> 00:37:58,640
and in fact the trend classifier does

985
00:37:58,640 --> 00:38:00,560
not even need to be very good at

986
00:38:00,560 --> 00:38:02,000
classifying

987
00:38:02,000 --> 00:38:03,520
in the sense that it achieves a very

988
00:38:03,520 --> 00:38:06,320
high validation accuracy it only needs

989
00:38:06,320 --> 00:38:09,200
to classify slightly more than 50 of the

990
00:38:09,200 --> 00:38:11,839
traces correctly if done over a large

991
00:38:11,839 --> 00:38:15,040
enough number of traces for example of a

992
00:38:15,040 --> 00:38:17,520
total of 182

993
00:38:17,520 --> 00:38:20,640
200 traces the classifier only needs to

994
00:38:20,640 --> 00:38:23,920
get fifty point five percent correct to

995
00:38:23,920 --> 00:38:28,160
confidently reject the null hypothesis

996
00:38:28,480 --> 00:38:30,640
uh here is one case study based on a

997
00:38:30,640 --> 00:38:32,640
present threshold implementation

998
00:38:32,640 --> 00:38:37,440
measured on a spartan 6 fpga

999
00:38:37,520 --> 00:38:39,359
we can clearly see that the third order

1000
00:38:39,359 --> 00:38:41,599
t-test and the g-square test detect

1001
00:38:41,599 --> 00:38:42,560
leakage

1002
00:38:42,560 --> 00:38:44,960
the largest amplitudes reach slightly

1003
00:38:44,960 --> 00:38:48,079
above 20 and slightly above 50 in terms

1004
00:38:48,079 --> 00:38:51,359
of minus log 10 probabilities after 10

1005
00:38:51,359 --> 00:38:54,160
million traces

1006
00:38:54,880 --> 00:38:57,440
when training our deep neural network on

1007
00:38:57,440 --> 00:39:00,400
three million traces over 50 epochs and

1008
00:39:00,400 --> 00:39:03,520
validating on 1.5 million traces we get

1009
00:39:03,520 --> 00:39:06,480
confidence values above 1000 again in

1010
00:39:06,480 --> 00:39:08,880
terms of minus log 10 probabilities

1011
00:39:08,880 --> 00:39:11,440
after 50 aprils

1012
00:39:11,440 --> 00:39:13,280
we can also see that the sensitivity

1013
00:39:13,280 --> 00:39:14,960
analysis or it's a gradient

1014
00:39:14,960 --> 00:39:19,040
visualization on the left side

1015
00:39:19,040 --> 00:39:21,680
shows that that only the most leaky part

1016
00:39:21,680 --> 00:39:23,200
in the middle of the trace is really

1017
00:39:23,200 --> 00:39:25,280
used for the classification the other

1018
00:39:25,280 --> 00:39:27,119
parts of the trace are apparently

1019
00:39:27,119 --> 00:39:28,800
ignored because

1020
00:39:28,800 --> 00:39:30,640
likely they carry only redundant

1021
00:39:30,640 --> 00:39:34,319
information for the classification

1022
00:39:36,240 --> 00:39:38,320
the next case study is also based on a

1023
00:39:38,320 --> 00:39:40,480
presence threshold implementation but

1024
00:39:40,480 --> 00:39:42,480
this time its shares and component

1025
00:39:42,480 --> 00:39:45,200
functions are clocked sequentially so

1026
00:39:45,200 --> 00:39:47,359
that the circuit exhibits no univariate

1027
00:39:47,359 --> 00:39:49,200
but only multivariate higher order

1028
00:39:49,200 --> 00:39:50,320
leakage

1029
00:39:50,320 --> 00:39:51,920
we can see here that the univariate

1030
00:39:51,920 --> 00:39:54,480
tests the third order t-test and the

1031
00:39:54,480 --> 00:39:57,119
g-square test do not detect any leakage

1032
00:39:57,119 --> 00:40:00,720
with up to 50 million traces

1033
00:40:00,720 --> 00:40:03,040
however a multivariate extension of the

1034
00:40:03,040 --> 00:40:04,800
third-order t-test using the correct

1035
00:40:04,800 --> 00:40:07,359
offsets can detect it after roughly 45

1036
00:40:07,359 --> 00:40:09,440
million traces

1037
00:40:09,440 --> 00:40:11,680
for a reward scenario if the underlying

1038
00:40:11,680 --> 00:40:14,000
source code is not known this requires

1039
00:40:14,000 --> 00:40:16,079
intense reverse engineering and expert

1040
00:40:16,079 --> 00:40:17,359
knowledge

1041
00:40:17,359 --> 00:40:19,680
because the exhaustive methods for this

1042
00:40:19,680 --> 00:40:22,079
kind of evaluation are mostly too

1043
00:40:22,079 --> 00:40:24,960
complex to be really carried out

1044
00:40:24,960 --> 00:40:26,960
in my personal experience even when not

1045
00:40:26,960 --> 00:40:28,800
knowing the underlying implement even by

1046
00:40:28,800 --> 00:40:30,480
knowing the underlying implementation

1047
00:40:30,480 --> 00:40:32,480
this is not always super straightforward

1048
00:40:32,480 --> 00:40:36,560
and always requires some manual work

1049
00:40:36,640 --> 00:40:38,319
the deep neural networks on the other

1050
00:40:38,319 --> 00:40:40,400
hand do all that work for you

1051
00:40:40,400 --> 00:40:41,760
automatically

1052
00:40:41,760 --> 00:40:44,400
uh here we have trained it on 20 million

1053
00:40:44,400 --> 00:40:46,880
traces and validate its classification

1054
00:40:46,880 --> 00:40:49,760
ability on further 5 million traces and

1055
00:40:49,760 --> 00:40:52,400
we can see that after 25 epos

1056
00:40:52,400 --> 00:40:54,640
the detection threshold is overcome and

1057
00:40:54,640 --> 00:40:56,960
a pretty high confidence is reached

1058
00:40:56,960 --> 00:41:00,160
after 50 equals

1059
00:41:00,800 --> 00:41:03,920
okay in summary leakage assessment using

1060
00:41:03,920 --> 00:41:06,640
neural networks is indeed feasible and

1061
00:41:06,640 --> 00:41:09,359
also worth the extra run time in more

1062
00:41:09,359 --> 00:41:11,440
complex detection scenarios as for

1063
00:41:11,440 --> 00:41:13,119
example multivariate higher order

1064
00:41:13,119 --> 00:41:14,079
leakage

1065
00:41:14,079 --> 00:41:16,160
since the deep neural networks are able

1066
00:41:16,160 --> 00:41:17,920
to learn many forms of leakage

1067
00:41:17,920 --> 00:41:20,240
automatically without any pre-processing

1068
00:41:20,240 --> 00:41:22,640
or manual effort

1069
00:41:22,640 --> 00:41:25,040
also the analysis often requires fewer

1070
00:41:25,040 --> 00:41:26,880
traces than classical tests because the

1071
00:41:26,880 --> 00:41:28,720
detection is based on the full trace at

1072
00:41:28,720 --> 00:41:31,839
once and not on each point or a

1073
00:41:31,839 --> 00:41:34,800
combination of a few points individually

1074
00:41:34,800 --> 00:41:37,280
the confidence produced is also orders

1075
00:41:37,280 --> 00:41:40,160
of magnitude higher usually and the risk

1076
00:41:40,160 --> 00:41:42,160
of false positives is reduced when the

1077
00:41:42,160 --> 00:41:44,560
same threshold is considered because you

1078
00:41:44,560 --> 00:41:47,119
only give one value instead of one value

1079
00:41:47,119 --> 00:41:50,240
per point the trace

1080
00:41:50,319 --> 00:41:52,640
okay that's already all from my side and

1081
00:41:52,640 --> 00:41:55,440
i'm happy to take any questions

1082
00:41:55,440 --> 00:41:57,280
thank you torben that was really

1083
00:41:57,280 --> 00:41:58,720
interesting

1084
00:41:58,720 --> 00:42:00,720
there is one question

1085
00:42:00,720 --> 00:42:02,800
that reads is there any particular

1086
00:42:02,800 --> 00:42:06,640
reason you opted for fixed versus fixed

1087
00:42:06,640 --> 00:42:08,800
instead of fixed versus random which

1088
00:42:08,800 --> 00:42:11,839
might be more common

1089
00:42:11,839 --> 00:42:14,400
uh yes indeed because even in simple

1090
00:42:14,400 --> 00:42:17,359
cases you have with fixed versus random

1091
00:42:17,359 --> 00:42:19,440
a larger overlap between your

1092
00:42:19,440 --> 00:42:21,599
distributions at each point with fixed

1093
00:42:21,599 --> 00:42:24,000
was fixed you can get like completely

1094
00:42:24,000 --> 00:42:26,160
distinct distributions with fixed versus

1095
00:42:26,160 --> 00:42:28,720
random because the fixed distribution so

1096
00:42:28,720 --> 00:42:31,440
the fixed is also part of the random

1097
00:42:31,440 --> 00:42:33,200
group so there will always be a larger

1098
00:42:33,200 --> 00:42:35,920
overlap so for better distinction

1099
00:42:35,920 --> 00:42:38,240
fixed was fixed is really the better

1100
00:42:38,240 --> 00:42:41,560
choice here

1101
00:42:44,000 --> 00:42:46,000
question in the

1102
00:42:46,000 --> 00:42:47,280
zulip and

1103
00:42:47,280 --> 00:42:50,319
let's call it the last question in this

1104
00:42:50,319 --> 00:42:52,640
talk can you hear me

1105
00:42:52,640 --> 00:42:55,040
because i think my internet is not very

1106
00:42:55,040 --> 00:42:57,440
stable the connection is poor

1107
00:42:57,440 --> 00:42:59,599
the question is can you also use the

1108
00:42:59,599 --> 00:43:00,880
network

1109
00:43:00,880 --> 00:43:03,599
you have used to identify leakages to

1110
00:43:03,599 --> 00:43:07,280
perform cure recovery attack

1111
00:43:07,280 --> 00:43:09,359
yes so we have not done this in this

1112
00:43:09,359 --> 00:43:12,400
paper but essentially you can you can do

1113
00:43:12,400 --> 00:43:14,480
exactly that so first you can of course

1114
00:43:14,480 --> 00:43:16,319
find the location of any leakage like a

1115
00:43:16,319 --> 00:43:18,560
typical leakage assessment but you can

1116
00:43:18,560 --> 00:43:21,440
also identify specific leakages for an

1117
00:43:21,440 --> 00:43:24,480
s-box output bit

1118
00:43:24,480 --> 00:43:26,880
where you also have two classes and you

1119
00:43:26,880 --> 00:43:30,880
can perform completely the same strategy

1120
00:43:30,880 --> 00:43:33,520
thank you uh there are other questions

1121
00:43:33,520 --> 00:43:35,920
in the zoodip i would like to to ask

1122
00:43:35,920 --> 00:43:37,359
turban to

1123
00:43:37,359 --> 00:43:39,280
go through them and

1124
00:43:39,280 --> 00:43:42,160
offline thank you

1125
00:43:48,960 --> 00:43:51,839
so the next talk

1126
00:43:51,839 --> 00:43:54,960
will be given by

1127
00:43:54,960 --> 00:43:58,319
chao and the paper is entitled rings for

1128
00:43:58,319 --> 00:44:00,000
reinforcement learning for hyper

1129
00:44:00,000 --> 00:44:02,640
parameter tuning in deep learning based

1130
00:44:02,640 --> 00:44:07,879
such an analysis the paper is written by

1131
00:44:09,680 --> 00:44:10,880
um

1132
00:44:10,880 --> 00:44:12,960
yet the paper is written by

1133
00:44:12,960 --> 00:44:16,640
uri li chao kyam and estepan

1134
00:44:16,640 --> 00:44:21,879
uh reach out the floor is yours

1135
00:44:22,000 --> 00:44:24,560
cannot hear you you're muted

1136
00:44:24,560 --> 00:44:27,280
can you please unmute yes that's perfect

1137
00:44:27,280 --> 00:44:29,520
so you can hear uh my voice and also see

1138
00:44:29,520 --> 00:44:30,960
my slides

1139
00:44:30,960 --> 00:44:32,480
yes we can

1140
00:44:32,480 --> 00:44:35,040
yes thanks for the nice introductions

1141
00:44:35,040 --> 00:44:37,280
and my name is nichol

1142
00:44:37,280 --> 00:44:39,520
and today i'm going to introduce our

1143
00:44:39,520 --> 00:44:42,000
recent work reinforcement learning for

1144
00:44:42,000 --> 00:44:43,440
type environmental tuning in deep

1145
00:44:43,440 --> 00:44:45,599
learning-based cycle analysis and this

1146
00:44:45,599 --> 00:44:47,359
is a joint work with your iristic

1147
00:44:47,359 --> 00:44:51,480
gilchon curry and stefan pisang

1148
00:44:53,839 --> 00:44:55,920
providing a side channel attack is part

1149
00:44:55,920 --> 00:44:58,079
of the cycle attack and it's it's really

1150
00:44:58,079 --> 00:44:59,760
powerful because it's under really

1151
00:44:59,760 --> 00:45:02,319
strong assumptions some assume that the

1152
00:45:02,319 --> 00:45:04,000
attacker have the full control of the

1153
00:45:04,000 --> 00:45:06,640
chrome device so to perform a providing

1154
00:45:06,640 --> 00:45:08,400
side channel attack there are two phase

1155
00:45:08,400 --> 00:45:10,160
the first phase is doing the profiling

1156
00:45:10,160 --> 00:45:12,319
so building the profiling models with

1157
00:45:12,319 --> 00:45:14,560
providing traces and labels and then

1158
00:45:14,560 --> 00:45:16,720
just use attack traces across from the

1159
00:45:16,720 --> 00:45:18,800
attack device and asks the providing

1160
00:45:18,800 --> 00:45:21,440
model to do the prediction of the attack

1161
00:45:21,440 --> 00:45:23,119
labels

1162
00:45:23,119 --> 00:45:25,280
there are two popular

1163
00:45:25,280 --> 00:45:27,119
providing side channel attack template

1164
00:45:27,119 --> 00:45:29,200
attack and deep learning attack recent

1165
00:45:29,200 --> 00:45:31,040
work shows that the deploying attack is

1166
00:45:31,040 --> 00:45:33,040
really powerful especially in dealing

1167
00:45:33,040 --> 00:45:36,160
with the traces with countermeasures

1168
00:45:36,160 --> 00:45:38,560
however develop such a network is not

1169
00:45:38,560 --> 00:45:41,119
that easy especially there are so many

1170
00:45:41,119 --> 00:45:42,800
different type of parameters such as

1171
00:45:42,800 --> 00:45:45,280
network pipes layer configurations and

1172
00:45:45,280 --> 00:45:47,760
training configurations and it's not

1173
00:45:47,760 --> 00:45:49,440
that straightforward to design such a

1174
00:45:49,440 --> 00:45:51,440
network and that can be easily applied

1175
00:45:51,440 --> 00:45:53,040
to different data sets

1176
00:45:53,040 --> 00:45:56,079
so the goal of this work is to automate

1177
00:45:56,079 --> 00:45:58,319
and to stimulate simplify this process

1178
00:45:58,319 --> 00:46:00,480
and help the evaluator and attacker to

1179
00:46:00,480 --> 00:46:02,720
develop a better network that is

1180
00:46:02,720 --> 00:46:05,359
suitable for the data set

1181
00:46:05,359 --> 00:46:07,599
so we are using uh reinforcement

1182
00:46:07,599 --> 00:46:09,680
learnings more specifically the

1183
00:46:09,680 --> 00:46:12,319
q-learning so this is the plot to

1184
00:46:12,319 --> 00:46:14,640
illustrate how the q learning works so

1185
00:46:14,640 --> 00:46:16,560
basically this is just an interaction

1186
00:46:16,560 --> 00:46:18,800
between the agent and the environment so

1187
00:46:18,800 --> 00:46:20,880
the agent is knowing better about the

1188
00:46:20,880 --> 00:46:22,720
environment based on the objective we

1189
00:46:22,720 --> 00:46:24,640
said to them and for the side channel

1190
00:46:24,640 --> 00:46:27,200
context the objective is we will be

1191
00:46:27,200 --> 00:46:29,760
founding a good network

1192
00:46:29,760 --> 00:46:30,560
and

1193
00:46:30,560 --> 00:46:32,480
the the following equations also

1194
00:46:32,480 --> 00:46:35,520
represent this diagram of this plot

1195
00:46:35,520 --> 00:46:38,319
so it's based on the bill my equations

1196
00:46:38,319 --> 00:46:39,359
and

1197
00:46:39,359 --> 00:46:41,599
besides the details which you can find

1198
00:46:41,599 --> 00:46:42,480
in the

1199
00:46:42,480 --> 00:46:46,160
paper this calculate the killer q value

1200
00:46:46,160 --> 00:46:47,920
which is the quality value of the

1201
00:46:47,920 --> 00:46:50,319
currency state action pair the state is

1202
00:46:50,319 --> 00:46:53,359
the state of the agent and action is the

1203
00:46:53,359 --> 00:46:55,599
agent takes actions based on the current

1204
00:46:55,599 --> 00:46:58,319
state so calculate this q value is

1205
00:46:58,319 --> 00:47:00,480
important because it gives us reward on

1206
00:47:00,480 --> 00:47:02,560
the current state action pair and as you

1207
00:47:02,560 --> 00:47:05,280
can see in this equation besides the

1208
00:47:05,280 --> 00:47:07,520
other factors reward is really important

1209
00:47:07,520 --> 00:47:10,240
because as as i said before the rewards

1210
00:47:10,240 --> 00:47:13,520
that object to the objective the agent

1211
00:47:13,520 --> 00:47:16,000
and i will want to uh let the agent to

1212
00:47:16,000 --> 00:47:18,319
explore the environment which is

1213
00:47:18,319 --> 00:47:21,040
possible network topologies and gives us

1214
00:47:21,040 --> 00:47:23,200
that a really good network that's

1215
00:47:23,200 --> 00:47:25,839
suitable for the specific data set

1216
00:47:25,839 --> 00:47:28,880
so we designed the reward with the

1217
00:47:28,880 --> 00:47:31,359
different evaluation matrix especially

1218
00:47:31,359 --> 00:47:33,200
side channel operation matrix so

1219
00:47:33,200 --> 00:47:35,680
basically there are four

1220
00:47:35,680 --> 00:47:37,440
and there are additional one i will

1221
00:47:37,440 --> 00:47:40,720
cover it so uh they consider the uh

1222
00:47:40,720 --> 00:47:43,040
percentage of the traces that uh

1223
00:47:43,040 --> 00:47:44,720
requires to uh

1224
00:47:44,720 --> 00:47:47,680
uh convert to gas into b0 and also where

1225
00:47:47,680 --> 00:47:50,240
you've got a different gas entropy

1226
00:47:50,240 --> 00:47:51,839
value with different number of the

1227
00:47:51,839 --> 00:47:54,880
traces and also we noticed that there

1228
00:47:54,880 --> 00:47:58,079
are papers uh focus on funding a good

1229
00:47:58,079 --> 00:48:00,319
performance network while the network

1230
00:48:00,319 --> 00:48:02,319
should be small because it's trained

1231
00:48:02,319 --> 00:48:04,400
it's easy to be trained so we also

1232
00:48:04,400 --> 00:48:06,240
consider the trainable parameters which

1233
00:48:06,240 --> 00:48:07,680
is a p-value

1234
00:48:07,680 --> 00:48:08,960
so uh

1235
00:48:08,960 --> 00:48:12,160
from this we developed two uh different

1236
00:48:12,160 --> 00:48:13,920
version of the reward function which is

1237
00:48:13,920 --> 00:48:15,599
reward and reverse small and reverse

1238
00:48:15,599 --> 00:48:17,760
small stands for the

1239
00:48:17,760 --> 00:48:19,520
reward function that we want to fund the

1240
00:48:19,520 --> 00:48:21,119
small but good

1241
00:48:21,119 --> 00:48:23,520
attack models or deep learning models

1242
00:48:23,520 --> 00:48:25,920
so this is our network searching method

1243
00:48:25,920 --> 00:48:28,400
so we start by sample network topologies

1244
00:48:28,400 --> 00:48:30,480
then we train the network and finally

1245
00:48:30,480 --> 00:48:32,880
evaluate and update the queue values

1246
00:48:32,880 --> 00:48:35,280
and they repeat this process again again

1247
00:48:35,280 --> 00:48:37,280
with its own graded schedule so the

1248
00:48:37,280 --> 00:48:39,520
details can be found in paper but

1249
00:48:39,520 --> 00:48:41,920
the goal of this schedule is just to

1250
00:48:41,920 --> 00:48:43,680
balance the exploration

1251
00:48:43,680 --> 00:48:45,760
and exploitation

1252
00:48:45,760 --> 00:48:48,000
and also for some details the sample of

1253
00:48:48,000 --> 00:48:50,720
the network topology is followed by some

1254
00:48:50,720 --> 00:48:53,040
groups and the rule is defined by the

1255
00:48:53,040 --> 00:48:55,359
latest state of art research and for the

1256
00:48:55,359 --> 00:48:59,119
details please found it in our paper

1257
00:48:59,119 --> 00:49:00,480
and by the way there are batch

1258
00:49:00,480 --> 00:49:02,720
normalization and gap layer which is not

1259
00:49:02,720 --> 00:49:03,760
really

1260
00:49:03,760 --> 00:49:06,079
commonly used in recent favors but we

1261
00:49:06,079 --> 00:49:08,559
also consider it because it is possible

1262
00:49:08,559 --> 00:49:11,520
to produce models

1263
00:49:11,520 --> 00:49:13,599
and finally we move to experimental

1264
00:49:13,599 --> 00:49:15,760
results we have experienced a straight

1265
00:49:15,760 --> 00:49:18,480
data set but here we only show one uh

1266
00:49:18,480 --> 00:49:22,000
ascetic c data set which is ascatwon1

1267
00:49:22,000 --> 00:49:24,559
so uh here's a result from the left you

1268
00:49:24,559 --> 00:49:27,599
can see our searching overview so for

1269
00:49:27,599 --> 00:49:30,240
each dot is then it stands for the one

1270
00:49:30,240 --> 00:49:31,839
network topology

1271
00:49:31,839 --> 00:49:34,319
and the access is axis then for the q

1272
00:49:34,319 --> 00:49:36,319
learning reward which is the reward i

1273
00:49:36,319 --> 00:49:38,640
just mentioned before and the y-axis

1274
00:49:38,640 --> 00:49:39,920
stands for the number of trainable

1275
00:49:39,920 --> 00:49:43,280
parameters and the two uh the curse of

1276
00:49:43,280 --> 00:49:45,280
two red lines stand for the state-of-art

1277
00:49:45,280 --> 00:49:47,359
model so we want to benchmark with the

1278
00:49:47,359 --> 00:49:49,760
state of art models so as you can see

1279
00:49:49,760 --> 00:49:52,079
that we have test different leakage

1280
00:49:52,079 --> 00:49:54,319
models and different reward functions

1281
00:49:54,319 --> 00:49:55,119
and

1282
00:49:55,119 --> 00:49:57,599
apparently the models on the right down

1283
00:49:57,599 --> 00:49:59,359
corners performs better than the state

1284
00:49:59,359 --> 00:50:01,440
of art because it reaches higher q

1285
00:50:01,440 --> 00:50:03,040
learning reward with smaller trainable

1286
00:50:03,040 --> 00:50:04,160
parameters

1287
00:50:04,160 --> 00:50:07,119
so in both cases we can form good models

1288
00:50:07,119 --> 00:50:09,440
and here's a benchmark on the right

1289
00:50:09,440 --> 00:50:11,280
table you can see the benchmark those

1290
00:50:11,280 --> 00:50:14,000
for both uh for both reward functions we

1291
00:50:14,000 --> 00:50:17,760
can get really uh small models but the

1292
00:50:17,760 --> 00:50:20,640
the performance is really good or even

1293
00:50:20,640 --> 00:50:23,200
better than the state of art

1294
00:50:23,200 --> 00:50:25,520
and also we show the reward uh the

1295
00:50:25,520 --> 00:50:28,640
reward during the evaluations and as i

1296
00:50:28,640 --> 00:50:30,559
said before we use epsilon graded

1297
00:50:30,559 --> 00:50:32,880
schedules and when we're moving from the

1298
00:50:32,880 --> 00:50:34,960
exploration to exploitation you can see

1299
00:50:34,960 --> 00:50:37,040
that reward is actually going higher

1300
00:50:37,040 --> 00:50:39,200
this means that our agent is learning

1301
00:50:39,200 --> 00:50:40,960
better about the environment and also we

1302
00:50:40,960 --> 00:50:43,040
test our base entropy and it performs

1303
00:50:43,040 --> 00:50:44,720
really well

1304
00:50:44,720 --> 00:50:46,480
so finally we move to the conclusion and

1305
00:50:46,480 --> 00:50:48,720
future work so in this paper we propose

1306
00:50:48,720 --> 00:50:50,640
a reinforcement learning framework that

1307
00:50:50,640 --> 00:50:52,160
enables automate

1308
00:50:52,160 --> 00:50:54,079
and powerful search for convolutional

1309
00:50:54,079 --> 00:50:56,640
neural network for performing cycle

1310
00:50:56,640 --> 00:50:59,599
we uh motivate and customize our reward

1311
00:50:59,599 --> 00:51:01,280
functions and i will demonstrate the

1312
00:51:01,280 --> 00:51:04,480
effective of effectiveness of our method

1313
00:51:04,480 --> 00:51:06,480
and for the future work it will be

1314
00:51:06,480 --> 00:51:08,559
interesting to investigate the deep q

1315
00:51:08,559 --> 00:51:11,280
learning paradigm and also how is the

1316
00:51:11,280 --> 00:51:12,319
best

1317
00:51:12,319 --> 00:51:14,400
models workspace and symbols of the

1318
00:51:14,400 --> 00:51:17,680
models and also we want to apply our

1319
00:51:17,680 --> 00:51:19,760
search methodology methodologies for

1320
00:51:19,760 --> 00:51:21,680
other type of network

1321
00:51:21,680 --> 00:51:23,440
thanks for your attention and if you are

1322
00:51:23,440 --> 00:51:26,079
interested in our code please check this

1323
00:51:26,079 --> 00:51:27,599
link and you can

1324
00:51:27,599 --> 00:51:29,920
implement our schemes really easily

1325
00:51:29,920 --> 00:51:31,440
thank you

1326
00:51:31,440 --> 00:51:34,000
thank you for the presentation

1327
00:51:34,000 --> 00:51:36,880
there is a question

1328
00:51:36,880 --> 00:51:39,280
interestingly funding good hyper

1329
00:51:39,280 --> 00:51:41,599
parameters while penalizing bigger

1330
00:51:41,599 --> 00:51:44,400
architectures has somehow been addressed

1331
00:51:44,400 --> 00:51:47,160
with for example structural risk

1332
00:51:47,160 --> 00:51:50,000
minimization or minimum description

1333
00:51:50,000 --> 00:51:51,040
length

1334
00:51:51,040 --> 00:51:53,680
both approaches theoretically explain

1335
00:51:53,680 --> 00:51:57,119
how to properly set a panel like

1336
00:51:57,119 --> 00:51:59,839
panelizing term for bigger architectures

1337
00:51:59,839 --> 00:52:02,880
in hyper parameter tuning have you

1338
00:52:02,880 --> 00:52:04,880
compared your algorithm with any of

1339
00:52:04,880 --> 00:52:07,599
those approaches and if so which works

1340
00:52:07,599 --> 00:52:09,040
the best

1341
00:52:09,040 --> 00:52:10,480
thanks for the question actually it's a

1342
00:52:10,480 --> 00:52:13,200
really good one um so the

1343
00:52:13,200 --> 00:52:15,920
initial thought of our initial design of

1344
00:52:15,920 --> 00:52:19,520
our report function do not consider the

1345
00:52:19,520 --> 00:52:21,359
trainable parameters because we believe

1346
00:52:21,359 --> 00:52:22,960
that for the side channel network is

1347
00:52:22,960 --> 00:52:24,960
already super small compared to the

1348
00:52:24,960 --> 00:52:26,720
traditional deep learning models but

1349
00:52:26,720 --> 00:52:29,359
still we added but unfortunately in this

1350
00:52:29,359 --> 00:52:32,559
paper we do not compare it with other uh

1351
00:52:32,559 --> 00:52:33,920
other

1352
00:52:33,920 --> 00:52:36,079
searching methodologies that can

1353
00:52:36,079 --> 00:52:38,960
penetrate the bigger network but we

1354
00:52:38,960 --> 00:52:42,319
believe that we can also easily apply in

1355
00:52:42,319 --> 00:52:45,520
our scheme by designing a new remote

1356
00:52:45,520 --> 00:52:47,920
function that emphasized that we need to

1357
00:52:47,920 --> 00:52:49,839
find a smaller but good performance

1358
00:52:49,839 --> 00:52:52,240
models

1359
00:52:52,319 --> 00:52:54,240
thank you

1360
00:52:54,240 --> 00:52:55,920
due to time constraint we will move to

1361
00:52:55,920 --> 00:52:58,640
the next talk now

1362
00:52:58,640 --> 00:53:01,680
the title of the next presentation is

1363
00:53:01,680 --> 00:53:04,480
cross device profiled

1364
00:53:04,480 --> 00:53:07,520
side channel attack with unsupervised

1365
00:53:07,520 --> 00:53:10,640
domain adaptation

1366
00:53:10,640 --> 00:53:15,839
and the authors are uh

1367
00:53:16,160 --> 00:53:17,520
lucian jing

1368
00:53:17,520 --> 00:53:20,079
and gudao

1369
00:53:20,079 --> 00:53:22,720
top is a floyd's yes

1370
00:53:22,720 --> 00:53:24,960
yes can you hear me

1371
00:53:24,960 --> 00:53:26,319
yes yes

1372
00:53:26,319 --> 00:53:28,720
okay thanks for the introduction

1373
00:53:28,720 --> 00:53:31,040
hello and welcome to our people across

1374
00:53:31,040 --> 00:53:33,599
device profiles and channel takeaways

1375
00:53:33,599 --> 00:53:37,280
and supervised domain adaptation

1376
00:53:37,520 --> 00:53:39,760
so in this work we focus on the open

1377
00:53:39,760 --> 00:53:42,319
question of portability which is a major

1378
00:53:42,319 --> 00:53:44,800
limitation of profiled attacks

1379
00:53:44,800 --> 00:53:46,880
the portability issue occurs when there

1380
00:53:46,880 --> 00:53:49,040
is a gap between experimental settings

1381
00:53:49,040 --> 00:53:51,920
and reality for example in its payments

1382
00:53:51,920 --> 00:53:53,839
we usually use a single device for

1383
00:53:53,839 --> 00:53:55,680
profiling and then we use the same

1384
00:53:55,680 --> 00:53:58,319
device for the attack but in reality the

1385
00:53:58,319 --> 00:54:00,480
profiling device and the target device

1386
00:54:00,480 --> 00:54:02,800
are different in most cases even for

1387
00:54:02,800 --> 00:54:05,200
devices of the same tab the leakage of

1388
00:54:05,200 --> 00:54:07,839
the sentence information is inevitably

1389
00:54:07,839 --> 00:54:08,720
different

1390
00:54:08,720 --> 00:54:10,720
unfortunately this device describes

1391
00:54:10,720 --> 00:54:13,040
information is not utilized in this

1392
00:54:13,040 --> 00:54:15,599
classic two-phase profile text

1393
00:54:15,599 --> 00:54:17,920
as a result attacking a different device

1394
00:54:17,920 --> 00:54:20,000
may cause a successful single device

1395
00:54:20,000 --> 00:54:22,240
attack to fail

1396
00:54:22,240 --> 00:54:24,480
today device discrepancy is still about

1397
00:54:24,480 --> 00:54:26,720
like restricting the application of

1398
00:54:26,720 --> 00:54:30,000
profile types in practice

1399
00:54:30,000 --> 00:54:32,160
so in order to address the portability

1400
00:54:32,160 --> 00:54:34,559
issue we propose to extend the

1401
00:54:34,559 --> 00:54:37,040
traditional profile tags and introduce

1402
00:54:37,040 --> 00:54:38,960
an additional functioning phase to

1403
00:54:38,960 --> 00:54:41,280
adjust the printing model

1404
00:54:41,280 --> 00:54:43,440
fan twin is a widely used technique in

1405
00:54:43,440 --> 00:54:44,960
transfer learning for deep neural

1406
00:54:44,960 --> 00:54:47,040
networks where a few approaches of

1407
00:54:47,040 --> 00:54:48,960
training are applied to the parenting

1408
00:54:48,960 --> 00:54:51,200
model's parameters to adapt them to a

1409
00:54:51,200 --> 00:54:52,240
new task

1410
00:54:52,240 --> 00:54:53,760
a straightforward approach for

1411
00:54:53,760 --> 00:54:56,319
functioning is to take a printing model

1412
00:54:56,319 --> 00:54:59,040
and then change parts of its parameters

1413
00:54:59,040 --> 00:55:01,280
using the data from the target domain

1414
00:55:01,280 --> 00:55:04,319
however in realistic profile tags there

1415
00:55:04,319 --> 00:55:06,480
is no labeled trace made from the target

1416
00:55:06,480 --> 00:55:09,520
device so in our strategy the inputs of

1417
00:55:09,520 --> 00:55:11,680
the functioning phase are the original

1418
00:55:11,680 --> 00:55:14,400
profile traces with knowing labels and

1419
00:55:14,400 --> 00:55:16,960
limited number of enabled faces measured

1420
00:55:16,960 --> 00:55:18,799
from the target device

1421
00:55:18,799 --> 00:55:20,400
our network should capture the

1422
00:55:20,400 --> 00:55:22,319
description information of the two

1423
00:55:22,319 --> 00:55:23,920
domains and to learn to make

1424
00:55:23,920 --> 00:55:26,880
environmental features

1425
00:55:26,880 --> 00:55:28,880
so in order to capture the description

1426
00:55:28,880 --> 00:55:30,960
information we must decide how to

1427
00:55:30,960 --> 00:55:32,880
qualify the distance between the

1428
00:55:32,880 --> 00:55:35,119
profiling and attack traces

1429
00:55:35,119 --> 00:55:37,920
so in this work we introduce the maximum

1430
00:55:37,920 --> 00:55:40,720
mean discrepancy which is a standard

1431
00:55:40,720 --> 00:55:43,200
distribution distance metric and has

1432
00:55:43,200 --> 00:55:45,200
been widely used in many other transfer

1433
00:55:45,200 --> 00:55:46,960
learning tasks

1434
00:55:46,960 --> 00:55:48,799
so in order to learn domain environment

1435
00:55:48,799 --> 00:55:50,480
features we must design new loss

1436
00:55:50,480 --> 00:55:52,720
function the loss function is composed

1437
00:55:52,720 --> 00:55:55,440
of two parts the classification loss and

1438
00:55:55,440 --> 00:55:58,319
mmd laws the classification loss makes

1439
00:55:58,319 --> 00:56:00,839
sure that the learn features are

1440
00:56:00,839 --> 00:56:03,440
discriminative in our experiments we use

1441
00:56:03,440 --> 00:56:05,520
cross intervals by the thoughts

1442
00:56:05,520 --> 00:56:07,680
the mmd laws tries to make the two

1443
00:56:07,680 --> 00:56:10,480
domains appear as similar as possible it

1444
00:56:10,480 --> 00:56:13,119
can be regarded as a constant term with

1445
00:56:13,119 --> 00:56:16,319
a penalty parameter lambda

1446
00:56:16,319 --> 00:56:18,160
as for the architecture there are two

1447
00:56:18,160 --> 00:56:20,400
main differences between classics and

1448
00:56:20,400 --> 00:56:22,799
models and our architecture

1449
00:56:22,799 --> 00:56:25,200
first our function network receives two

1450
00:56:25,200 --> 00:56:27,440
batches of quizzes for each training

1451
00:56:27,440 --> 00:56:30,000
process specifically one batch of

1452
00:56:30,000 --> 00:56:32,480
laboratories is created from the profile

1453
00:56:32,480 --> 00:56:35,040
device the other batch of traces is

1454
00:56:35,040 --> 00:56:37,280
enabled created from the target device

1455
00:56:37,280 --> 00:56:38,960
the second difference is we have to

1456
00:56:38,960 --> 00:56:41,839
decide where to calculate the mmt loss

1457
00:56:41,839 --> 00:56:44,480
in our network actually previous works

1458
00:56:44,480 --> 00:56:47,119
have shown that the features must

1459
00:56:47,119 --> 00:56:50,000
transform from generic to task specific

1460
00:56:50,000 --> 00:56:52,079
as one goes up the layers of a deep

1461
00:56:52,079 --> 00:56:53,200
sense

1462
00:56:53,200 --> 00:56:55,839
therefore we decide to minimize the mmd

1463
00:56:55,839 --> 00:56:58,160
loss on the fully connected layers the

1464
00:56:58,160 --> 00:57:00,240
convolutional blocks of the network is

1465
00:57:00,240 --> 00:57:02,720
still trainable during the phantom

1466
00:57:02,720 --> 00:57:04,880
because we expect them to learn shift

1467
00:57:04,880 --> 00:57:07,200
the environment features in case the

1468
00:57:07,200 --> 00:57:10,960
target domain is not well aligned

1469
00:57:10,960 --> 00:57:13,839
the figure on the left shows the it's x

1470
00:57:13,839 --> 00:57:16,319
manga chips we initialize each device

1471
00:57:16,319 --> 00:57:18,720
with a different circular key

1472
00:57:18,720 --> 00:57:21,040
we can see from the signal noise ratio

1473
00:57:21,040 --> 00:57:24,319
that the leakage differs for example the

1474
00:57:24,319 --> 00:57:25,680
device

1475
00:57:25,680 --> 00:57:26,799
for

1476
00:57:26,799 --> 00:57:29,440
is shifted apparently in time

1477
00:57:29,440 --> 00:57:31,280
these are the results on the x magnetic

1478
00:57:31,280 --> 00:57:33,440
sides we can compare the performance of

1479
00:57:33,440 --> 00:57:35,200
the quantity models and the function

1480
00:57:35,200 --> 00:57:36,559
models

1481
00:57:36,559 --> 00:57:39,920
the results of nt donates the number of

1482
00:57:39,920 --> 00:57:42,000
attack traces to successfully recover

1483
00:57:42,000 --> 00:57:42,960
the key

1484
00:57:42,960 --> 00:57:45,200
for printing models we can observe that

1485
00:57:45,200 --> 00:57:47,680
although the value of nt for single

1486
00:57:47,680 --> 00:57:50,000
device attack given our terminal is very

1487
00:57:50,000 --> 00:57:52,319
small it varies widely in the case of

1488
00:57:52,319 --> 00:57:54,720
cross device attacks in fact the bad

1489
00:57:54,720 --> 00:57:57,440
performance of cross-device attacks can

1490
00:57:57,440 --> 00:58:00,079
also be explained by our fitting we

1491
00:58:00,079 --> 00:58:01,839
therefore draw the learning codes when

1492
00:58:01,839 --> 00:58:03,440
we train the network

1493
00:58:03,440 --> 00:58:05,280
the radical is the test loss on the

1494
00:58:05,280 --> 00:58:07,200
target device it's interesting that the

1495
00:58:07,200 --> 00:58:09,440
test loss increases rapidly when we

1496
00:58:09,440 --> 00:58:11,680
train the network which means the model

1497
00:58:11,680 --> 00:58:14,240
is already outfitted in the first field

1498
00:58:14,240 --> 00:58:16,640
approach unfortunately this kind of

1499
00:58:16,640 --> 00:58:19,359
overfitting is hard to identify because

1500
00:58:19,359 --> 00:58:22,079
in realistic attacks the attack traces

1501
00:58:22,079 --> 00:58:24,000
are unlabeled and cannot be used to

1502
00:58:24,000 --> 00:58:25,920
calculate the test loss

1503
00:58:25,920 --> 00:58:27,920
after functioning the performance of

1504
00:58:27,920 --> 00:58:29,520
cross device attack is very much

1505
00:58:29,520 --> 00:58:31,680
improved we also draw the learning

1506
00:58:31,680 --> 00:58:34,480
curves when we function in the network

1507
00:58:34,480 --> 00:58:36,640
the curve in blue is

1508
00:58:36,640 --> 00:58:38,720
the mmd loss that we add to the loss

1509
00:58:38,720 --> 00:58:41,200
function the curving right is the test

1510
00:58:41,200 --> 00:58:43,920
loss on the title device we can see that

1511
00:58:43,920 --> 00:58:46,880
minimizing the mmd loss can effectively

1512
00:58:46,880 --> 00:58:48,799
reduce the test loss on the target

1513
00:58:48,799 --> 00:58:50,799
device

1514
00:58:50,799 --> 00:58:53,200
so what's the cost of this approach

1515
00:58:53,200 --> 00:58:56,240
uh in our experiments we can observe

1516
00:58:56,240 --> 00:58:58,640
that the input time for fine tuning is

1517
00:58:58,640 --> 00:59:01,440
about twice that of training however the

1518
00:59:01,440 --> 00:59:03,760
temp cost is still affordable for

1519
00:59:03,760 --> 00:59:06,240
example if we during the phantom phase

1520
00:59:06,240 --> 00:59:08,880
for 15 approaches this process can be

1521
00:59:08,880 --> 00:59:11,280
completed with within two minutes for

1522
00:59:11,280 --> 00:59:14,079
all these decides

1523
00:59:14,079 --> 00:59:16,720
so finally we move to the conclusions

1524
00:59:16,720 --> 00:59:19,440
this work focuses on the open question

1525
00:59:19,440 --> 00:59:21,680
of portability in profound certain

1526
00:59:21,680 --> 00:59:23,599
attacks using transfer learning

1527
00:59:23,599 --> 00:59:24,799
techniques

1528
00:59:24,799 --> 00:59:27,520
although c is known to be proposed

1529
00:59:27,520 --> 00:59:30,240
against misalignment we show that cnn

1530
00:59:30,240 --> 00:59:32,799
may not generalize well if only clean

1531
00:59:32,799 --> 00:59:34,960
faces are fired however after the

1532
00:59:34,960 --> 00:59:37,440
functioning with mmd loss our network is

1533
00:59:37,440 --> 00:59:38,880
able to learn domain environment

1534
00:59:38,880 --> 00:59:41,359
features besides this approach does not

1535
00:59:41,359 --> 00:59:44,880
require multiple professional device

1536
00:59:44,880 --> 00:59:47,839
packet specific purpose testing and an

1537
00:59:47,839 --> 00:59:50,000
information of the type device so thank

1538
00:59:50,000 --> 00:59:51,440
you for attention and if you have any

1539
00:59:51,440 --> 00:59:55,520
questions feel free to ask thank you

1540
00:59:57,599 --> 01:00:00,240
thank you for the nice presentation i

1541
01:00:00,240 --> 01:00:03,280
cannot see any questions

1542
01:00:03,280 --> 01:00:06,400
asked here in the chat panel or

1543
01:00:06,400 --> 01:00:08,720
where is zooey

1544
01:00:08,720 --> 01:00:09,839
if there is

1545
01:00:09,839 --> 01:00:13,359
no question i can ask one

1546
01:00:13,359 --> 01:00:14,880
when you say that

1547
01:00:14,880 --> 01:00:18,000
cnn may not generalize well if only the

1548
01:00:18,000 --> 01:00:22,160
clean traces are provided

1549
01:00:22,160 --> 01:00:24,480
you know i think i missed that point

1550
01:00:24,480 --> 01:00:27,359
where the uh

1551
01:00:27,359 --> 01:00:29,520
not clean traces

1552
01:00:29,520 --> 01:00:31,280
um they're used

1553
01:00:31,280 --> 01:00:32,559
do you use them

1554
01:00:32,559 --> 01:00:34,319
for the

1555
01:00:34,319 --> 01:00:36,160
um

1556
01:00:36,160 --> 01:00:38,240
training and the

1557
01:00:38,240 --> 01:00:40,240
noise in that traces

1558
01:00:40,240 --> 01:00:41,280
are

1559
01:00:41,280 --> 01:00:44,160
used to do regularization or

1560
01:00:44,160 --> 01:00:45,599
um

1561
01:00:45,599 --> 01:00:48,880
actually i'm not sure how you use

1562
01:00:48,880 --> 01:00:52,400
those not clean traces

1563
01:00:52,799 --> 01:00:56,040
uh yes

1564
01:00:56,079 --> 01:00:59,839
in fact in our people we also test our

1565
01:00:59,839 --> 01:01:01,520
approach on

1566
01:01:01,520 --> 01:01:02,400
the

1567
01:01:02,400 --> 01:01:05,440
distinct chronic data size for example

1568
01:01:05,440 --> 01:01:09,839
the scd we add add a

1569
01:01:09,839 --> 01:01:12,559
mislament to the to the to the lotuses

1570
01:01:12,559 --> 01:01:13,599
rotates

1571
01:01:13,599 --> 01:01:14,319
and

1572
01:01:14,319 --> 01:01:17,520
we use our approach to remove the

1573
01:01:17,520 --> 01:01:18,799
mislament

1574
01:01:18,799 --> 01:01:21,599
the effect of instruments and

1575
01:01:21,599 --> 01:01:22,839
yes it

1576
01:01:22,839 --> 01:01:24,400
works

1577
01:01:24,400 --> 01:01:26,960
i see thank you definitely i will take a

1578
01:01:26,960 --> 01:01:29,359
look at your paper again because that's

1579
01:01:29,359 --> 01:01:30,880
very interesting

1580
01:01:30,880 --> 01:01:33,839
um there is no other questions and

1581
01:01:33,839 --> 01:01:35,520
there was one question remaining the

1582
01:01:35,520 --> 01:01:38,240
chat panel i copied that in this zoolip

1583
01:01:38,240 --> 01:01:39,760
i asked the

1584
01:01:39,760 --> 01:01:42,160
uh attendee um

1585
01:01:42,160 --> 01:01:44,480
i think this just to check again the

1586
01:01:44,480 --> 01:01:48,160
zulip for the answer to that question

1587
01:01:48,160 --> 01:01:50,558
thank you

1588
01:01:51,040 --> 01:01:54,000
okay um

1589
01:01:54,000 --> 01:01:55,760
we would like to thank the speakers and

1590
01:01:55,760 --> 01:01:59,839
attendees of this session and

