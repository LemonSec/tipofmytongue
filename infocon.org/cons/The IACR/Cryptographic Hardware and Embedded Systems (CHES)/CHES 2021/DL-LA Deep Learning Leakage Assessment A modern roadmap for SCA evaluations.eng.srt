1
00:00:01,199 --> 00:00:03,439
hello and welcome to this presentation

2
00:00:03,439 --> 00:00:06,480
of the paper dlla deep learning leakage

3
00:00:06,480 --> 00:00:08,639
assessment a modern roadmap for sca

4
00:00:08,639 --> 00:00:10,080
evaluations

5
00:00:10,080 --> 00:00:12,160
my name is thor moss and this is a joint

6
00:00:12,160 --> 00:00:14,240
work with my colleagues felix wegner and

7
00:00:14,240 --> 00:00:16,320
amir moradi from the royal university

8
00:00:16,320 --> 00:00:18,720
board

9
00:00:19,279 --> 00:00:21,760
this work proposes a new strategy for

10
00:00:21,760 --> 00:00:24,160
black box leakage assessment

11
00:00:24,160 --> 00:00:26,320
in simple words leakage assessment is a

12
00:00:26,320 --> 00:00:28,240
technique to determine whether input

13
00:00:28,240 --> 00:00:30,560
dependent information can be detected in

14
00:00:30,560 --> 00:00:32,238
side channel measurements of a device

15
00:00:32,238 --> 00:00:33,600
under test

16
00:00:33,600 --> 00:00:35,760
typically if i can learn information

17
00:00:35,760 --> 00:00:37,760
about the processed inputs from a side

18
00:00:37,760 --> 00:00:40,160
channel trace then indeed some kind of

19
00:00:40,160 --> 00:00:42,320
side channel leakage is present

20
00:00:42,320 --> 00:00:44,239
this does not necessarily mean that the

21
00:00:44,239 --> 00:00:46,800
device is vulnerable to attacks but it's

22
00:00:46,800 --> 00:00:48,960
usually desirable that no detectable

23
00:00:48,960 --> 00:00:50,960
input dependent information is leaked to

24
00:00:50,960 --> 00:00:53,360
potential adversaries

25
00:00:53,360 --> 00:00:55,039
the most common approach to perform

26
00:00:55,039 --> 00:00:56,640
leakage assessment is to try to

27
00:00:56,640 --> 00:00:59,120
distinguish leakage distributions for

28
00:00:59,120 --> 00:01:01,120
different input classes using a

29
00:01:01,120 --> 00:01:03,440
statistical hypothesis test

30
00:01:03,440 --> 00:01:05,438
the most commonly used input classes are

31
00:01:05,438 --> 00:01:07,760
fixed versus random where one group for

32
00:01:07,760 --> 00:01:09,680
for fixed input is collected and one

33
00:01:09,680 --> 00:01:12,479
group for many different random inputs

34
00:01:12,479 --> 00:01:14,799
and then fixed was fixed where two

35
00:01:14,799 --> 00:01:17,280
groups for two distinct fixed inputs are

36
00:01:17,280 --> 00:01:18,479
collected

37
00:01:18,479 --> 00:01:20,640
then either the welsh t-test or the

38
00:01:20,640 --> 00:01:22,560
pearson's g-square tests are used to

39
00:01:22,560 --> 00:01:24,799
evaluate the null hypothesis which

40
00:01:24,799 --> 00:01:26,960
states that both groups are drawn from

41
00:01:26,960 --> 00:01:28,640
the same population and therefore

42
00:01:28,640 --> 00:01:31,600
indistinguishable

43
00:01:32,560 --> 00:01:34,400
this method has been applied for many

44
00:01:34,400 --> 00:01:36,400
years in the community and different

45
00:01:36,400 --> 00:01:37,920
extensions to the tests and the

46
00:01:37,920 --> 00:01:40,079
methodology exist to tackle specific

47
00:01:40,079 --> 00:01:41,119
problems

48
00:01:41,119 --> 00:01:42,799
but there are still a number of aspects

49
00:01:42,799 --> 00:01:44,399
where the technique works less than

50
00:01:44,399 --> 00:01:45,360
ideal

51
00:01:45,360 --> 00:01:47,360
first of all side channel measurements

52
00:01:47,360 --> 00:01:49,360
usually consist of hundreds or thousands

53
00:01:49,360 --> 00:01:51,439
of sample points or even more

54
00:01:51,439 --> 00:01:52,960
but the classical tests are normally

55
00:01:52,960 --> 00:01:54,479
applied to each sample point

56
00:01:54,479 --> 00:01:56,320
individually instead of on the full

57
00:01:56,320 --> 00:01:57,920
trace at once

58
00:01:57,920 --> 00:01:59,759
therefore side channel leakages that are

59
00:01:59,759 --> 00:02:01,920
spread over multiple points for example

60
00:02:01,920 --> 00:02:04,479
multivariate or horizontal leakages may

61
00:02:04,479 --> 00:02:06,399
go undetected

62
00:02:06,399 --> 00:02:08,479
also it is known that the separation of

63
00:02:08,479 --> 00:02:11,280
statistical orders in the welsh t-test

64
00:02:11,280 --> 00:02:13,040
can lead to false negatives when the

65
00:02:13,040 --> 00:02:14,959
noise level is low and the leakage is of

66
00:02:14,959 --> 00:02:17,120
higher order or when the leakage is

67
00:02:17,120 --> 00:02:19,360
distributed over multiple statistical

68
00:02:19,360 --> 00:02:22,360
moments

69
00:02:22,480 --> 00:02:24,000
we have thought about techniques that

70
00:02:24,000 --> 00:02:25,760
could potentially avoid some of these

71
00:02:25,760 --> 00:02:27,760
drawbacks and we noticed that leakage

72
00:02:27,760 --> 00:02:29,760
assessment is essentially a statistical

73
00:02:29,760 --> 00:02:31,519
classification problem

74
00:02:31,519 --> 00:02:34,160
if we can generate a software an

75
00:02:34,160 --> 00:02:36,800
algorithm or a machine that determines

76
00:02:36,800 --> 00:02:38,959
whether a leakage trace belongs to group

77
00:02:38,959 --> 00:02:41,680
0 or group 1 with a better success rate

78
00:02:41,680 --> 00:02:44,400
than randomly guessing then indeed

79
00:02:44,400 --> 00:02:46,400
input dependent information is included

80
00:02:46,400 --> 00:02:47,840
in the traces

81
00:02:47,840 --> 00:02:49,440
and to us this seemed like a pretty

82
00:02:49,440 --> 00:02:52,400
straightforward task for a newer network

83
00:02:52,400 --> 00:02:53,840
again the potential benefits in

84
00:02:53,840 --> 00:02:55,519
comparison to the classical methods

85
00:02:55,519 --> 00:02:57,519
would be that the network would receive

86
00:02:57,519 --> 00:02:59,440
each trace in full length and therefore

87
00:02:59,440 --> 00:03:01,519
can make a decision with respect to a

88
00:03:01,519 --> 00:03:04,239
full trace instead of one point only and

89
00:03:04,239 --> 00:03:06,319
also that the classifier is not limited

90
00:03:06,319 --> 00:03:08,640
to simple univariate first order leakage

91
00:03:08,640 --> 00:03:12,400
but inherently captures all leakages

92
00:03:12,400 --> 00:03:14,640
that normally occur

93
00:03:14,640 --> 00:03:16,800
based on this idea we have created a new

94
00:03:16,800 --> 00:03:18,720
leakage assessment methodology which we

95
00:03:18,720 --> 00:03:21,040
called deep learning leakage assessment

96
00:03:21,040 --> 00:03:24,679
in short dlla

97
00:03:25,519 --> 00:03:27,840
the procedure looks like this first we

98
00:03:27,840 --> 00:03:30,000
record a set of sidechain measurements

99
00:03:30,000 --> 00:03:31,760
while supplying the device under test

100
00:03:31,760 --> 00:03:34,159
with a randomly interleaved sequence of

101
00:03:34,159 --> 00:03:36,799
two distinct fixed inputs

102
00:03:36,799 --> 00:03:38,799
then we split the whole set into a

103
00:03:38,799 --> 00:03:41,120
training set and a validation set and

104
00:03:41,120 --> 00:03:42,799
train the neural network on the training

105
00:03:42,799 --> 00:03:44,159
set

106
00:03:44,159 --> 00:03:46,159
afterwards we use a trained network to

107
00:03:46,159 --> 00:03:48,319
classify the traces in the validation

108
00:03:48,319 --> 00:03:50,239
set while hiding the true labels from

109
00:03:50,239 --> 00:03:51,519
the network

110
00:03:51,519 --> 00:03:53,280
if this is done we can compare the

111
00:03:53,280 --> 00:03:55,760
classifications with the true labels and

112
00:03:55,760 --> 00:03:57,120
count the number of correct

113
00:03:57,120 --> 00:03:58,879
classifications

114
00:03:58,879 --> 00:04:00,560
when we have that number we can

115
00:04:00,560 --> 00:04:02,640
calculate the probability that the

116
00:04:02,640 --> 00:04:04,879
correct classifications could have been

117
00:04:04,879 --> 00:04:07,280
achieved just by chance

118
00:04:07,280 --> 00:04:09,519
if this probability is smaller than a

119
00:04:09,519 --> 00:04:12,000
chosen threshold for example the typical

120
00:04:12,000 --> 00:04:14,239
10 to the power of -5 threshold in

121
00:04:14,239 --> 00:04:15,680
leakage assessment

122
00:04:15,680 --> 00:04:18,399
then we can reject the null hypothesis

123
00:04:18,399 --> 00:04:20,000
the number of traces for the detection

124
00:04:20,000 --> 00:04:21,918
is then the cardinality of the training

125
00:04:21,918 --> 00:04:24,479
set only because this set was sufficient

126
00:04:24,479 --> 00:04:26,639
to extract and learn a generalizable

127
00:04:26,639 --> 00:04:29,040
feature from the traces

128
00:04:29,040 --> 00:04:31,120
in the validation period the network is

129
00:04:31,120 --> 00:04:33,600
not updated so it cannot absorb any

130
00:04:33,600 --> 00:04:34,880
additional information from the

131
00:04:34,880 --> 00:04:37,520
validation set but of course the number

132
00:04:37,520 --> 00:04:38,880
of traces needed for the whole

133
00:04:38,880 --> 00:04:41,360
evaluation is the combined cardinality

134
00:04:41,360 --> 00:04:45,040
of training and validation set

135
00:04:45,919 --> 00:04:48,000
here is a formula to calculate the

136
00:04:48,000 --> 00:04:50,040
probability that sm correct

137
00:04:50,040 --> 00:04:52,320
classifications are achieved by a

138
00:04:52,320 --> 00:04:54,800
randomly guessing binary classifier

139
00:04:54,800 --> 00:04:56,320
where m is the cardinality of the

140
00:04:56,320 --> 00:04:58,960
validation set and v is the validation

141
00:04:58,960 --> 00:05:01,520
accuracy

142
00:05:02,720 --> 00:05:04,639
to get an intuition for the required

143
00:05:04,639 --> 00:05:06,639
validation accuracy to confidently

144
00:05:06,639 --> 00:05:08,880
reject the null hypothesis for different

145
00:05:08,880 --> 00:05:10,880
sizes of the validation set we have

146
00:05:10,880 --> 00:05:13,280
listed a few examples here

147
00:05:13,280 --> 00:05:16,880
for a set of only 76 traces at least 57

148
00:05:16,880 --> 00:05:18,720
of them need to be classified correctly

149
00:05:18,720 --> 00:05:21,360
which corresponds to a 75 validation

150
00:05:21,360 --> 00:05:25,440
accuracy for a set of 45 600 validation

151
00:05:25,440 --> 00:05:27,440
traces for example the validation

152
00:05:27,440 --> 00:05:30,720
accuracy of 51 is already sufficient to

153
00:05:30,720 --> 00:05:32,639
reject the null hypothesis

154
00:05:32,639 --> 00:05:34,560
for even larger validation sets the

155
00:05:34,560 --> 00:05:36,479
required validation accuracy shrinks

156
00:05:36,479 --> 00:05:38,880
further and further in that regard it

157
00:05:38,880 --> 00:05:41,759
becomes clear that dlla does not require

158
00:05:41,759 --> 00:05:43,840
the trained classifiers to be very good

159
00:05:43,840 --> 00:05:46,479
at the classification of a single trace

160
00:05:46,479 --> 00:05:48,320
it is only important that it classifies

161
00:05:48,320 --> 00:05:50,479
the traces better than a random guess

162
00:05:50,479 --> 00:05:54,320
over a large number of traces

163
00:05:54,720 --> 00:05:56,800
this is the python code that defines a

164
00:05:56,800 --> 00:05:59,680
multi-layer perceptron in kiras which we

165
00:05:59,680 --> 00:06:01,520
have used throughout this work

166
00:06:01,520 --> 00:06:03,759
it is an extremely simple network with

167
00:06:03,759 --> 00:06:06,080
four fully connected layers and 2 output

168
00:06:06,080 --> 00:06:08,479
neurons for the classification

169
00:06:08,479 --> 00:06:10,800
we have kept it very simple on purpose

170
00:06:10,800 --> 00:06:12,639
to make it suitable for a wide range of

171
00:06:12,639 --> 00:06:14,639
side channel traces and different forms

172
00:06:14,639 --> 00:06:17,759
of side channel leakage

173
00:06:18,880 --> 00:06:20,880
the second network we have used for our

174
00:06:20,880 --> 00:06:22,880
case studies is a convolutional neural

175
00:06:22,880 --> 00:06:25,120
network its description in python code

176
00:06:25,120 --> 00:06:26,720
is shown on this slide

177
00:06:26,720 --> 00:06:28,400
here we actually apply a couple of

178
00:06:28,400 --> 00:06:30,080
parameters and will evaluate the

179
00:06:30,080 --> 00:06:32,479
robustness of its detection performance

180
00:06:32,479 --> 00:06:34,560
across eight different hyper parameter

181
00:06:34,560 --> 00:06:37,199
combinations

182
00:06:38,160 --> 00:06:39,759
we have tested our new leakage

183
00:06:39,759 --> 00:06:41,840
assessment strategy in a total of nine

184
00:06:41,840 --> 00:06:44,000
different case studies using side

185
00:06:44,000 --> 00:06:45,360
channel data from different

186
00:06:45,360 --> 00:06:47,680
implementations and platforms

187
00:06:47,680 --> 00:06:50,560
we always compare dlla to the t-test and

188
00:06:50,560 --> 00:06:52,639
the g-square test in order to judge the

189
00:06:52,639 --> 00:06:54,160
quality of the deep learning based

190
00:06:54,160 --> 00:06:55,840
leakage detection

191
00:06:55,840 --> 00:06:57,440
the first seven case studies are all

192
00:06:57,440 --> 00:06:59,039
based on hardware implementations

193
00:06:59,039 --> 00:07:01,120
measured on an fpga

194
00:07:01,120 --> 00:07:03,039
for some of them the traces are aligned

195
00:07:03,039 --> 00:07:05,360
for others they are misaligned some

196
00:07:05,360 --> 00:07:06,960
circuits are protected by a masking

197
00:07:06,960 --> 00:07:09,280
counter measures some others are not

198
00:07:09,280 --> 00:07:10,800
some of the trace sets contain

199
00:07:10,800 --> 00:07:12,560
univariate leakage and some others

200
00:07:12,560 --> 00:07:15,120
include only multivariate leakage

201
00:07:15,120 --> 00:07:16,960
in addition to the fpga-based case

202
00:07:16,960 --> 00:07:18,720
studies there's also one case study

203
00:07:18,720 --> 00:07:21,120
based on measurements from a custom asic

204
00:07:21,120 --> 00:07:22,560
and another one where a software

205
00:07:22,560 --> 00:07:24,639
implementation is executed on an arm

206
00:07:24,639 --> 00:07:28,080
cortex m0 microcontroller

207
00:07:28,080 --> 00:07:30,000
in summary we have tested our method on

208
00:07:30,000 --> 00:07:32,400
a large variety of realistic target

209
00:07:32,400 --> 00:07:34,639
implementations and compared its success

210
00:07:34,639 --> 00:07:37,840
to the classical methods

211
00:07:38,160 --> 00:07:40,160
the one thing that all case studies have

212
00:07:40,160 --> 00:07:41,440
in common is the underlying

213
00:07:41,440 --> 00:07:43,360
cryptographic primitive namely the

214
00:07:43,360 --> 00:07:45,840
present 80 ultra lightweight block

215
00:07:45,840 --> 00:07:46,800
cipher

216
00:07:46,800 --> 00:07:48,560
this slide shows the architecture of the

217
00:07:48,560 --> 00:07:50,800
unprotected serialized present hardware

218
00:07:50,800 --> 00:07:53,039
implementation that has been analyzed in

219
00:07:53,039 --> 00:07:56,479
the two following examples

220
00:07:56,479 --> 00:07:58,639
on the top of this slide we see a sample

221
00:07:58,639 --> 00:08:00,960
trace recorded by measuring the voltage

222
00:08:00,960 --> 00:08:03,759
drop over a 1 ohm shunt resistor in the

223
00:08:03,759 --> 00:08:05,919
vdd path of the power supply of a

224
00:08:05,919 --> 00:08:09,360
spartan 6 fpga using a digital sampling

225
00:08:09,360 --> 00:08:10,960
oscilloscope

226
00:08:10,960 --> 00:08:12,479
on the bottom there are four figures

227
00:08:12,479 --> 00:08:14,479
showing the results of a univariate

228
00:08:14,479 --> 00:08:17,520
first order t-test and a g-square test

229
00:08:17,520 --> 00:08:19,120
they also show the progress of the

230
00:08:19,120 --> 00:08:20,879
highest confidence values over the

231
00:08:20,879 --> 00:08:22,800
number of traces

232
00:08:22,800 --> 00:08:24,560
clearly a significant amount of leakage

233
00:08:24,560 --> 00:08:26,400
is present and can be detected with

234
00:08:26,400 --> 00:08:29,680
confidence after less than 100 traces

235
00:08:29,680 --> 00:08:31,759
the maximum confidence expressed as a

236
00:08:31,759 --> 00:08:36,800
minus log 10 value is slightly above 80.

237
00:08:37,919 --> 00:08:40,320
using deep learning leakage assessment

238
00:08:40,320 --> 00:08:42,080
on the same tray set we achieve the

239
00:08:42,080 --> 00:08:43,760
results shown here

240
00:08:43,760 --> 00:08:46,480
since a dlla procedure does not produce

241
00:08:46,480 --> 00:08:48,399
one confidence value per sample point

242
00:08:48,399 --> 00:08:50,959
but instead only one in total we have to

243
00:08:50,959 --> 00:08:52,640
use a different method to extract

244
00:08:52,640 --> 00:08:54,720
spatial information about the leakage in

245
00:08:54,720 --> 00:08:55,920
the trace

246
00:08:55,920 --> 00:08:57,440
this can be done with a so-called

247
00:08:57,440 --> 00:08:59,839
sensitivity analysis based on input

248
00:08:59,839 --> 00:09:01,760
activation gradients

249
00:09:01,760 --> 00:09:03,680
this method locates the points of

250
00:09:03,680 --> 00:09:05,760
interest by quantifying how much they

251
00:09:05,760 --> 00:09:07,920
contributed to the leakage function

252
00:09:07,920 --> 00:09:10,240
learned by the neural network

253
00:09:10,240 --> 00:09:12,000
more details on that can be found in the

254
00:09:12,000 --> 00:09:13,200
paper

255
00:09:13,200 --> 00:09:15,200
on the right side the confidence values

256
00:09:15,200 --> 00:09:17,040
over the number of training traces are

257
00:09:17,040 --> 00:09:18,160
depicted

258
00:09:18,160 --> 00:09:20,480
obviously the confidence values achieved

259
00:09:20,480 --> 00:09:22,000
are much much higher than for the

260
00:09:22,000 --> 00:09:24,240
classical methods however for the sake

261
00:09:24,240 --> 00:09:25,680
of completeness we have to mention that

262
00:09:25,680 --> 00:09:27,360
the validation set is not counted in

263
00:09:27,360 --> 00:09:29,360
this figure and that the total number of

264
00:09:29,360 --> 00:09:31,839
traces required for the evaluation is

265
00:09:31,839 --> 00:09:33,120
sometimes higher than for the

266
00:09:33,120 --> 00:09:34,959
traditional methods in these simple case

267
00:09:34,959 --> 00:09:38,880
studies with unprotected implementations

268
00:09:39,600 --> 00:09:41,440
the next case study we want to take a

269
00:09:41,440 --> 00:09:43,680
look at is based on the same present at

270
00:09:43,680 --> 00:09:45,680
implementation as before

271
00:09:45,680 --> 00:09:47,600
but this time the clock of the circuit

272
00:09:47,600 --> 00:09:50,240
was randomized by an lfsr circuit

273
00:09:50,240 --> 00:09:52,320
leading to a misalignment of the traces

274
00:09:52,320 --> 00:09:54,640
and a large amount of noise we can see

275
00:09:54,640 --> 00:09:56,640
that the traditional methods require up

276
00:09:56,640 --> 00:10:01,199
to 5000 traces now to detect the leakage

277
00:10:01,760 --> 00:10:04,320
using dlla for the same analysis the

278
00:10:04,320 --> 00:10:06,720
detection succeeds with about 150

279
00:10:06,720 --> 00:10:09,120
training traces however the acquired

280
00:10:09,120 --> 00:10:11,120
confidence is also reduced by a large

281
00:10:11,120 --> 00:10:13,360
margin and the identification of the

282
00:10:13,360 --> 00:10:15,839
points of interest might not be as sharp

283
00:10:15,839 --> 00:10:19,320
as for the t-test

284
00:10:19,440 --> 00:10:21,600
here we see a serialized three-share

285
00:10:21,600 --> 00:10:23,440
threshold implementation of the present

286
00:10:23,440 --> 00:10:25,440
80 cipher which is the underlying

287
00:10:25,440 --> 00:10:27,120
hardware implementation for the next

288
00:10:27,120 --> 00:10:29,680
case study the s-box of present needs to

289
00:10:29,680 --> 00:10:31,920
be decomposed into two quadratic

290
00:10:31,920 --> 00:10:34,399
functions before a ti with three shares

291
00:10:34,399 --> 00:10:37,360
can be realized

292
00:10:37,920 --> 00:10:39,920
on the top of this slide a sample trace

293
00:10:39,920 --> 00:10:41,839
is shown which has been recorded during

294
00:10:41,839 --> 00:10:44,000
the execution of the first one and a

295
00:10:44,000 --> 00:10:45,760
half rounds of the presence threshold

296
00:10:45,760 --> 00:10:47,200
implementation

297
00:10:47,200 --> 00:10:49,360
below there are again t-test

298
00:10:49,360 --> 00:10:51,200
entry-square test results showing the

299
00:10:51,200 --> 00:10:53,440
detection of higher order leakage

300
00:10:53,440 --> 00:10:55,440
for the t-test only the third order is

301
00:10:55,440 --> 00:10:57,440
shown as the statistical confidence is

302
00:10:57,440 --> 00:10:59,200
the highest for this moment but the

303
00:10:59,200 --> 00:11:01,120
results for first and second order can

304
00:11:01,120 --> 00:11:03,600
also be found in the paper

305
00:11:03,600 --> 00:11:05,279
since the leakage of the implementation

306
00:11:05,279 --> 00:11:07,040
is partially found in the second and

307
00:11:07,040 --> 00:11:08,720
partially in the third statistical

308
00:11:08,720 --> 00:11:10,720
moment it is no surprise that the

309
00:11:10,720 --> 00:11:15,800
g-square test outperforms a t-test here

310
00:11:15,920 --> 00:11:19,200
we also applied dlla to the same traces

311
00:11:19,200 --> 00:11:21,200
but unlike before we do not plot the

312
00:11:21,200 --> 00:11:23,440
confidence values over the number of

313
00:11:23,440 --> 00:11:26,079
traces anymore because such an analysis

314
00:11:26,079 --> 00:11:27,519
requires the analyst to train a

315
00:11:27,519 --> 00:11:29,519
classifier many times using different

316
00:11:29,519 --> 00:11:31,680
sizes of the training set

317
00:11:31,680 --> 00:11:33,920
although this allows a nicer comparison

318
00:11:33,920 --> 00:11:35,680
to the classical method

319
00:11:35,680 --> 00:11:38,000
it's computationally expensive and

320
00:11:38,000 --> 00:11:40,000
rather unnecessary

321
00:11:40,000 --> 00:11:41,920
therefore we have first chosen a fixed

322
00:11:41,920 --> 00:11:43,839
training set of 3 million traces and

323
00:11:43,839 --> 00:11:45,680
plots a confidence value

324
00:11:45,680 --> 00:11:48,480
over the number of epos to show that a

325
00:11:48,480 --> 00:11:50,160
much higher confidence can be achieved

326
00:11:50,160 --> 00:11:53,120
by dlla than by the classical methods

327
00:11:53,120 --> 00:11:54,839
with the same number of

328
00:11:54,839 --> 00:11:57,519
traces we can also see an important

329
00:11:57,519 --> 00:11:59,519
conceptual differences between leakage

330
00:11:59,519 --> 00:12:02,240
location using sensitivity analysis on a

331
00:12:02,240 --> 00:12:03,760
trained network

332
00:12:03,760 --> 00:12:06,639
compared to the classical approaches

333
00:12:06,639 --> 00:12:08,560
parts of the trace that contribute to

334
00:12:08,560 --> 00:12:10,720
leakage but correlate heavily with other

335
00:12:10,720 --> 00:12:13,120
parts contributing to the leakage might

336
00:12:13,120 --> 00:12:14,480
not be learned and used for

337
00:12:14,480 --> 00:12:16,880
distinguishing the two groups

338
00:12:16,880 --> 00:12:18,800
because there is no intrinsic incentive

339
00:12:18,800 --> 00:12:20,560
for the neural net to learn redundant

340
00:12:20,560 --> 00:12:21,760
information

341
00:12:21,760 --> 00:12:23,440
thus we can see that the leakage is

342
00:12:23,440 --> 00:12:25,200
pinpointed only at the point of the

343
00:12:25,200 --> 00:12:27,519
strongest leakage here and not in each

344
00:12:27,519 --> 00:12:28,959
point of the trace where there is

345
00:12:28,959 --> 00:12:32,399
actually leakage located

346
00:12:32,880 --> 00:12:34,480
here we have used a significantly

347
00:12:34,480 --> 00:12:36,480
smaller training set to demonstrate that

348
00:12:36,480 --> 00:12:38,800
using dlla it's also possible to

349
00:12:38,800 --> 00:12:41,120
confidently detect leakage using a

350
00:12:41,120 --> 00:12:43,200
smaller amount of traces than the t and

351
00:12:43,200 --> 00:12:44,959
g square test require

352
00:12:44,959 --> 00:12:46,639
the confidence threshold is overcome

353
00:12:46,639 --> 00:12:49,440
while training on only 500 000 traces

354
00:12:49,440 --> 00:12:52,880
although just barely

355
00:12:52,880 --> 00:12:54,720
the final case study we are going

356
00:12:54,720 --> 00:12:56,560
through in this presentation is also

357
00:12:56,560 --> 00:12:57,920
based on a present threshold

358
00:12:57,920 --> 00:12:59,200
implementation

359
00:12:59,200 --> 00:13:01,600
but here we ensure that all six

360
00:13:01,600 --> 00:13:05,680
component functions g1 g2 g3 f1 f2 s3

361
00:13:05,680 --> 00:13:08,399
together with the respective registers

362
00:13:08,399 --> 00:13:10,320
are clocked sequentially and not in

363
00:13:10,320 --> 00:13:12,320
parallel we did this by gating their

364
00:13:12,320 --> 00:13:14,399
respective inputs with and gates which

365
00:13:14,399 --> 00:13:16,959
are controlled by a finite state machine

366
00:13:16,959 --> 00:13:18,959
therefore no univariate leakage should

367
00:13:18,959 --> 00:13:21,279
be detectable but combining multiple

368
00:13:21,279 --> 00:13:23,120
points in the trace should still allow

369
00:13:23,120 --> 00:13:25,040
to distinguish the two fixed groups from

370
00:13:25,040 --> 00:13:26,160
each other

371
00:13:26,160 --> 00:13:28,480
we can see here that indeed neither the

372
00:13:28,480 --> 00:13:30,959
university test nor the univariate g

373
00:13:30,959 --> 00:13:33,920
square test detect any leakage using up

374
00:13:33,920 --> 00:13:37,360
to 50 million traces

375
00:13:37,600 --> 00:13:40,240
however when using a multivariate t-test

376
00:13:40,240 --> 00:13:42,320
by combining the respective points with

377
00:13:42,320 --> 00:13:44,240
the correct offsets we can get a

378
00:13:44,240 --> 00:13:46,639
confident detection after roughly 45

379
00:13:46,639 --> 00:13:49,760
million traces yet this is a process

380
00:13:49,760 --> 00:13:52,079
that cannot easily be automated for any

381
00:13:52,079 --> 00:13:54,480
implementation as it requires exact

382
00:13:54,480 --> 00:13:55,760
knowledge about the underlying

383
00:13:55,760 --> 00:13:57,040
implementation

384
00:13:57,040 --> 00:13:59,199
thus it is not perfectly suited for

385
00:13:59,199 --> 00:14:02,800
black box leakage assessment

386
00:14:02,800 --> 00:14:05,360
using dlla we do not require any

387
00:14:05,360 --> 00:14:06,880
knowledge or information about the

388
00:14:06,880 --> 00:14:09,120
underlying implementation we simply

389
00:14:09,120 --> 00:14:11,279
train a classifier on 20 million raw

390
00:14:11,279 --> 00:14:13,839
traces and the neural network learns the

391
00:14:13,839 --> 00:14:16,720
multivariate leakage automatically

392
00:14:16,720 --> 00:14:18,560
when we evaluate the classifier on

393
00:14:18,560 --> 00:14:20,480
further 5 million traces we obtain a

394
00:14:20,480 --> 00:14:22,160
high confidence to reject the null

395
00:14:22,160 --> 00:14:23,680
hypothesis

396
00:14:23,680 --> 00:14:26,320
these cases of multivariate higher order

397
00:14:26,320 --> 00:14:28,320
leakage are the most relevant and

398
00:14:28,320 --> 00:14:30,480
interesting application scenarios for

399
00:14:30,480 --> 00:14:32,000
dlla

400
00:14:32,000 --> 00:14:33,760
since the classical hypothesis test

401
00:14:33,760 --> 00:14:35,680
cannot easily be extended to capture

402
00:14:35,680 --> 00:14:39,439
such leakages universally

403
00:14:39,839 --> 00:14:42,079
of course the ability of the networks to

404
00:14:42,079 --> 00:14:44,000
automatically find and learn different

405
00:14:44,000 --> 00:14:46,480
leakage patterns does not come for free

406
00:14:46,480 --> 00:14:48,160
but at the price of a significant

407
00:14:48,160 --> 00:14:50,480
runtime overhead just like in most

408
00:14:50,480 --> 00:14:52,639
applications of deep learning

409
00:14:52,639 --> 00:14:54,000
here we have listed some numbers

410
00:14:54,000 --> 00:14:56,560
regarding the runtime on our machines in

411
00:14:56,560 --> 00:14:58,399
short if you want to analyze the same

412
00:14:58,399 --> 00:15:00,320
number of traces and train the

413
00:15:00,320 --> 00:15:03,519
classifier between 30 and 50 epos

414
00:15:03,519 --> 00:15:06,160
the dlla procedure will have a runtime

415
00:15:06,160 --> 00:15:09,120
that is about 100 times larger even when

416
00:15:09,120 --> 00:15:11,199
using the gpu support

417
00:15:11,199 --> 00:15:13,120
yet the absolute run times are still

418
00:15:13,120 --> 00:15:14,880
reasonable for a realistic number of

419
00:15:14,880 --> 00:15:17,040
traces and might spare you a lot of

420
00:15:17,040 --> 00:15:18,560
manual work for the more complex

421
00:15:18,560 --> 00:15:21,199
scenarios

422
00:15:22,480 --> 00:15:25,040
finally we have also tested the cnn

423
00:15:25,040 --> 00:15:27,279
network which was introduced earlier for

424
00:15:27,279 --> 00:15:29,120
its ability to learn the different forms

425
00:15:29,120 --> 00:15:31,279
of leakage in the case studies from the

426
00:15:31,279 --> 00:15:33,120
different platforms

427
00:15:33,120 --> 00:15:35,120
also in this case we have tested whether

428
00:15:35,120 --> 00:15:37,759
small changes to its parameters have

429
00:15:37,759 --> 00:15:39,600
destructive effects on its detection

430
00:15:39,600 --> 00:15:41,040
performance

431
00:15:41,040 --> 00:15:43,199
however we can see that although changes

432
00:15:43,199 --> 00:15:44,959
are noticeable the confidence values

433
00:15:44,959 --> 00:15:47,519
achieved are typically in the same range

434
00:15:47,519 --> 00:15:49,519
the largest variation is seen for case

435
00:15:49,519 --> 00:15:53,160
study 4 here

436
00:15:54,800 --> 00:15:57,040
to conclude this talk we have shown that

437
00:15:57,040 --> 00:15:59,199
leakage assessment using neural networks

438
00:15:59,199 --> 00:16:01,600
is feasible and worth the extra run time

439
00:16:01,600 --> 00:16:04,399
for complex detection scenarios

440
00:16:04,399 --> 00:16:06,399
we demonstrated that very simple

441
00:16:06,399 --> 00:16:08,560
networks can deliver quite universal

442
00:16:08,560 --> 00:16:10,720
detection performance even when faced

443
00:16:10,720 --> 00:16:13,759
with multivariate higher order leakages

444
00:16:13,759 --> 00:16:15,680
also we have seen that the amount of

445
00:16:15,680 --> 00:16:17,519
traces required for detection is

446
00:16:17,519 --> 00:16:20,079
typically lower in dlla compared to

447
00:16:20,079 --> 00:16:22,560
classical detection while also providing

448
00:16:22,560 --> 00:16:24,480
a higher confidence

449
00:16:24,480 --> 00:16:26,800
finally since dla produces one

450
00:16:26,800 --> 00:16:28,880
confidence value per set of traces

451
00:16:28,880 --> 00:16:31,360
instead of one value per sample point

452
00:16:31,360 --> 00:16:33,920
the risk of false negatives is lower if

453
00:16:33,920 --> 00:16:37,759
the same threshold is chosen

454
00:16:37,759 --> 00:16:39,920
with that i would like to end this talk

455
00:16:39,920 --> 00:16:41,680
thank you for your attention if there

456
00:16:41,680 --> 00:16:43,759
are any questions feel free to ask them

457
00:16:43,759 --> 00:16:46,720
during the live session at chess 2021 on

458
00:16:46,720 --> 00:16:50,279
september 16th

