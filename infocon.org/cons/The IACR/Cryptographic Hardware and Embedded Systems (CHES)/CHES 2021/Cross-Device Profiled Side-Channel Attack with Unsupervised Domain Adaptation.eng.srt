1
00:00:00,960 --> 00:00:03,679
hello and welcome to our paper cross

2
00:00:03,679 --> 00:00:06,480
device profiled cell channel techniques

3
00:00:06,480 --> 00:00:09,120
and supervise the domain adaptation

4
00:00:09,120 --> 00:00:11,360
my name is hopi and this work is a

5
00:00:11,360 --> 00:00:16,239
chance to work with zhang jin and kudo

6
00:00:16,239 --> 00:00:18,720
so we have three main contributions in

7
00:00:18,720 --> 00:00:19,760
this work

8
00:00:19,760 --> 00:00:22,720
first is a class device profile set

9
00:00:22,720 --> 00:00:25,199
channel text strategy with transfer

10
00:00:25,199 --> 00:00:26,720
learning techniques

11
00:00:26,720 --> 00:00:29,199
we'll use fan twin to adjust the

12
00:00:29,199 --> 00:00:32,880
printing model with unlabeled techniques

13
00:00:32,880 --> 00:00:36,160
second is no loss function and

14
00:00:36,160 --> 00:00:39,520
say an architecture for robust prophetic

15
00:00:39,520 --> 00:00:40,480
attacks

16
00:00:40,480 --> 00:00:42,800
and third we hope to provide a benchmark

17
00:00:42,800 --> 00:00:45,440
of cross-device set channel analysis

18
00:00:45,440 --> 00:00:49,199
with satisfying results

19
00:00:49,280 --> 00:00:51,600
inside the channel analysis we usually

20
00:00:51,600 --> 00:00:54,480
divide the text into two categories

21
00:00:54,480 --> 00:00:57,680
first is non-profile text where we only

22
00:00:57,680 --> 00:01:00,800
have a typed device available we can't

23
00:01:00,800 --> 00:01:03,359
change the key and usually we attack on

24
00:01:03,359 --> 00:01:06,880
fly with method like dpa and cpa for

25
00:01:06,880 --> 00:01:08,000
example

26
00:01:08,000 --> 00:01:10,240
on the other side we have profiled a

27
00:01:10,240 --> 00:01:13,200
text where we can get access to a cloned

28
00:01:13,200 --> 00:01:16,000
device which is open and similar enough

29
00:01:16,000 --> 00:01:18,560
to the target device we can use the

30
00:01:18,560 --> 00:01:21,520
cloud device to characterize the leakage

31
00:01:21,520 --> 00:01:24,240
of the target device with the method of

32
00:01:24,240 --> 00:01:26,320
machine learning techniques for example

33
00:01:26,320 --> 00:01:29,520
the software machine and random forest

34
00:01:29,520 --> 00:01:32,479
more recently the channel community

35
00:01:32,479 --> 00:01:34,479
highlights the probability of deep

36
00:01:34,479 --> 00:01:37,280
learning because first it can deal with

37
00:01:37,280 --> 00:01:39,439
high dimensional inputs better than

38
00:01:39,439 --> 00:01:40,799
template attack

39
00:01:40,799 --> 00:01:43,439
second neural networks naturally cater

40
00:01:43,439 --> 00:01:46,880
to masking kilometers and third diploma

41
00:01:46,880 --> 00:01:49,040
based attacks are robust to

42
00:01:49,040 --> 00:01:51,600
misalignments by using specific

43
00:01:51,600 --> 00:01:53,840
architecture like convolutional neural

44
00:01:53,840 --> 00:01:56,399
networks

45
00:01:57,360 --> 00:02:00,240
however profound attacks have a major

46
00:02:00,240 --> 00:02:03,360
limitation called portability which is

47
00:02:03,360 --> 00:02:06,479
often neglected in research papers

48
00:02:06,479 --> 00:02:09,199
the portability issue occurs when there

49
00:02:09,199 --> 00:02:11,760
is a gap between experimental settings

50
00:02:11,760 --> 00:02:14,879
and reality for example in experiments

51
00:02:14,879 --> 00:02:17,440
we usually use a single device for

52
00:02:17,440 --> 00:02:20,080
profiling and then use a sim device for

53
00:02:20,080 --> 00:02:24,000
the tag but in reality no two chips are

54
00:02:24,000 --> 00:02:25,840
exactly the same

55
00:02:25,840 --> 00:02:28,480
even for devices of the syntax the

56
00:02:28,480 --> 00:02:31,519
leakage of the subchannel information is

57
00:02:31,519 --> 00:02:35,040
inevitably different which is likely due

58
00:02:35,040 --> 00:02:38,560
to random process variations

59
00:02:38,560 --> 00:02:40,959
introduced during fabrication and

60
00:02:40,959 --> 00:02:42,480
packaging

61
00:02:42,480 --> 00:02:45,599
unfortunately this describes information

62
00:02:45,599 --> 00:02:48,160
is not utilized in the classic

63
00:02:48,160 --> 00:02:50,800
two-phases profile the text

64
00:02:50,800 --> 00:02:53,599
as a result attacking a different device

65
00:02:53,599 --> 00:02:56,000
may cause a successful single device

66
00:02:56,000 --> 00:02:57,440
model attack to

67
00:02:57,440 --> 00:02:59,360
completely fail

68
00:02:59,360 --> 00:03:02,560
today device discrepancy is still a

69
00:03:02,560 --> 00:03:05,040
bottleneck restricting the application

70
00:03:05,040 --> 00:03:09,200
of profit attacks in practice

71
00:03:09,200 --> 00:03:12,640
we note that an implicit hypothesis of

72
00:03:12,640 --> 00:03:15,120
deep learning techniques is that the

73
00:03:15,120 --> 00:03:18,080
training data must be independent and

74
00:03:18,080 --> 00:03:20,560
identically distributed with the test

75
00:03:20,560 --> 00:03:23,920
data however when we adopt deep learning

76
00:03:23,920 --> 00:03:26,239
in the context of preferred sentinel

77
00:03:26,239 --> 00:03:29,280
attack this hypothesis is too strong

78
00:03:29,280 --> 00:03:32,159
because attack traces are often acquired

79
00:03:32,159 --> 00:03:35,599
from a different device without control

80
00:03:35,599 --> 00:03:38,720
in such a context vr settings can easily

81
00:03:38,720 --> 00:03:41,840
break the hypothesis and lead to poor

82
00:03:41,840 --> 00:03:44,400
performance when we try to attack the

83
00:03:44,400 --> 00:03:45,680
device

84
00:03:45,680 --> 00:03:48,640
in fact device discrepancy is not the

85
00:03:48,640 --> 00:03:51,760
only reason for the portability issue

86
00:03:51,760 --> 00:03:54,080
different implementations and different

87
00:03:54,080 --> 00:03:57,200
settings of acquisition can also lead to

88
00:03:57,200 --> 00:03:59,280
pro attack performance

89
00:03:59,280 --> 00:04:01,439
for example turning on hiding

90
00:04:01,439 --> 00:04:03,200
countermeasures and changing the

91
00:04:03,200 --> 00:04:05,760
placement of the probes can also relate

92
00:04:05,760 --> 00:04:09,760
into different distributions of traces

93
00:04:09,760 --> 00:04:13,040
so a profound attack is composed of two

94
00:04:13,040 --> 00:04:15,920
phases a profiling phase and an attack

95
00:04:15,920 --> 00:04:16,880
phase

96
00:04:16,880 --> 00:04:19,040
we note that a limitation of the two

97
00:04:19,040 --> 00:04:22,320
phases attack is that it cannot utilize

98
00:04:22,320 --> 00:04:25,199
the descriptive information which is

99
00:04:25,199 --> 00:04:28,400
directly neglected so in order to

100
00:04:28,400 --> 00:04:30,400
address the possibility issue we

101
00:04:30,400 --> 00:04:32,479
proposed to extend the traditional

102
00:04:32,479 --> 00:04:34,880
profit attack and introduce an

103
00:04:34,880 --> 00:04:37,280
additional functioning phase to adjust

104
00:04:37,280 --> 00:04:39,680
the quenching model

105
00:04:39,680 --> 00:04:42,560
phantom is a widely adopted technique in

106
00:04:42,560 --> 00:04:44,160
transfer learning for

107
00:04:44,160 --> 00:04:47,199
deep neural networks where a few

108
00:04:47,199 --> 00:04:50,000
apologies of training applied to the

109
00:04:50,000 --> 00:04:52,639
project model's parameters to adapt them

110
00:04:52,639 --> 00:04:54,000
to a new task

111
00:04:54,000 --> 00:04:55,680
a straightforward approach for

112
00:04:55,680 --> 00:04:58,160
fine-tuning and to take a prevention

113
00:04:58,160 --> 00:05:00,800
network and then train parts of its

114
00:05:00,800 --> 00:05:03,440
parameters use the data from the target

115
00:05:03,440 --> 00:05:04,400
domain

116
00:05:04,400 --> 00:05:07,440
however in a realistic central tech

117
00:05:07,440 --> 00:05:10,160
scenery there is no labeled twist

118
00:05:10,160 --> 00:05:12,240
measured from the target device

119
00:05:12,240 --> 00:05:15,199
so in our strategy the impulse of the

120
00:05:15,199 --> 00:05:18,160
phantom phase are the original profanity

121
00:05:18,160 --> 00:05:21,280
traces with known labels and a limited

122
00:05:21,280 --> 00:05:24,000
number of unlabeled traces measured from

123
00:05:24,000 --> 00:05:25,520
the target device

124
00:05:25,520 --> 00:05:27,840
our network should capture the

125
00:05:27,840 --> 00:05:30,160
discrepancy information of the two

126
00:05:30,160 --> 00:05:32,720
domains and to learn the main

127
00:05:32,720 --> 00:05:36,080
environment features

128
00:05:36,320 --> 00:05:38,639
so in order to capture the discrepancy

129
00:05:38,639 --> 00:05:41,280
information we must decide how to

130
00:05:41,280 --> 00:05:43,120
quantify the distance between the

131
00:05:43,120 --> 00:05:45,440
profiling and attack traces

132
00:05:45,440 --> 00:05:48,240
in this work we introduce the maximum

133
00:05:48,240 --> 00:05:50,639
mean discrepancy which is a standard

134
00:05:50,639 --> 00:05:53,120
distribution distance matrix and has

135
00:05:53,120 --> 00:05:55,600
been widely used in many other transfer

136
00:05:55,600 --> 00:05:57,759
learning tasks

137
00:05:57,759 --> 00:05:59,280
in order to learn the my environment

138
00:05:59,280 --> 00:06:01,440
features we must design a new loss

139
00:06:01,440 --> 00:06:02,479
function

140
00:06:02,479 --> 00:06:04,960
the loss function is composed of two

141
00:06:04,960 --> 00:06:09,120
parts a classification loss and mfd loss

142
00:06:09,120 --> 00:06:11,520
the classification also makes sure that

143
00:06:11,520 --> 00:06:14,240
the learned features are discriminative

144
00:06:14,240 --> 00:06:16,800
your experience will cross introverts by

145
00:06:16,800 --> 00:06:18,800
default however

146
00:06:18,800 --> 00:06:20,639
attack can also select other loss

147
00:06:20,639 --> 00:06:23,199
functions that are specific to the deep

148
00:06:23,199 --> 00:06:26,000
learning based saa

149
00:06:26,000 --> 00:06:28,479
the materials can be regarded as a

150
00:06:28,479 --> 00:06:30,800
constraint term with a quantity

151
00:06:30,800 --> 00:06:33,520
parameter lambda the lambda here behaves

152
00:06:33,520 --> 00:06:36,160
similarly as the pointing parameter in

153
00:06:36,160 --> 00:06:40,919
l1 and l2 regulations

154
00:06:41,440 --> 00:06:44,240
as for the network used for far twin

155
00:06:44,240 --> 00:06:47,120
there are two main differences between

156
00:06:47,120 --> 00:06:49,440
classic scene models and our

157
00:06:49,440 --> 00:06:52,000
architecture first our front twin

158
00:06:52,000 --> 00:06:55,120
network receives two batches of choices

159
00:06:55,120 --> 00:06:56,960
for each training process

160
00:06:56,960 --> 00:07:00,000
specifically one batch of label traces

161
00:07:00,000 --> 00:07:02,880
is created from the profile device the

162
00:07:02,880 --> 00:07:05,520
other batch of traces is unlabeled

163
00:07:05,520 --> 00:07:08,000
occurred from the target device

164
00:07:08,000 --> 00:07:10,319
the second difference is we have to

165
00:07:10,319 --> 00:07:13,440
decide whether to calculate the mmd loss

166
00:07:13,440 --> 00:07:15,039
in our network

167
00:07:15,039 --> 00:07:17,520
in fact previous works have shown that

168
00:07:17,520 --> 00:07:20,160
the deep features must transform from

169
00:07:20,160 --> 00:07:23,759
generic to task specific as one concept

170
00:07:23,759 --> 00:07:26,639
the layers of a deep network in other

171
00:07:26,639 --> 00:07:29,520
words the transferability of the hidden

172
00:07:29,520 --> 00:07:32,960
replantation tends to significantly drop

173
00:07:32,960 --> 00:07:35,599
in here layers with increasing domain

174
00:07:35,599 --> 00:07:38,560
discrepancy therefore we decided to

175
00:07:38,560 --> 00:07:41,199
minimize the mmd loss on the fully

176
00:07:41,199 --> 00:07:42,960
connected layers

177
00:07:42,960 --> 00:07:45,599
the commercial block of the network is

178
00:07:45,599 --> 00:07:48,479
still trainable during the phantom phase

179
00:07:48,479 --> 00:07:52,000
to further adapt to the target domain we

180
00:07:52,000 --> 00:07:55,039
make the cardboard trainable because we

181
00:07:55,039 --> 00:07:57,680
expect the convolutional block to learn

182
00:07:57,680 --> 00:08:00,319
shift environment features in case the

183
00:08:00,319 --> 00:08:04,160
tech domain is not well led

184
00:08:04,720 --> 00:08:07,840
to evaluate our methodology we consider

185
00:08:07,840 --> 00:08:10,160
four data sites covering the main types

186
00:08:10,160 --> 00:08:12,560
of set channel tech scenarios

187
00:08:12,560 --> 00:08:15,599
the xml cat data site is created from 8

188
00:08:15,599 --> 00:08:17,039
x mega chips

189
00:08:17,039 --> 00:08:19,520
by marrying the power consumption we're

190
00:08:19,520 --> 00:08:23,360
using an unprotected es algorithm

191
00:08:23,360 --> 00:08:25,840
the sakura data set is created from

192
00:08:25,840 --> 00:08:28,800
three security evolution ports which

193
00:08:28,800 --> 00:08:31,840
corresponds to an unprotected hardware

194
00:08:31,840 --> 00:08:35,440
implementation of esr fpga

195
00:08:35,440 --> 00:08:38,080
we also use the well-known asid data

196
00:08:38,080 --> 00:08:41,200
site to show the potential of our method

197
00:08:41,200 --> 00:08:43,919
in removing some noise and quantum

198
00:08:43,919 --> 00:08:44,880
errors

199
00:08:44,880 --> 00:08:48,279
finally the x-mega em data set provides

200
00:08:48,279 --> 00:08:50,880
electromagnetic measurements created

201
00:08:50,880 --> 00:08:53,519
from the hx mega chips

202
00:08:53,519 --> 00:08:55,760
we use the same problem for all chips

203
00:08:55,760 --> 00:08:59,120
but at different positions and distance

204
00:08:59,120 --> 00:09:01,600
we will show how our method helps in

205
00:09:01,600 --> 00:09:04,480
such a scenario

206
00:09:04,800 --> 00:09:07,360
the picture on the left shows the 8x

207
00:09:07,360 --> 00:09:08,640
mega chips

208
00:09:08,640 --> 00:09:10,959
we initialize each device with a

209
00:09:10,959 --> 00:09:14,080
different section key then we calculate

210
00:09:14,080 --> 00:09:16,640
the signal noise ratio of each device

211
00:09:16,640 --> 00:09:19,600
calculating the leakage of the xbox

212
00:09:19,600 --> 00:09:22,560
output we can conclude from the results

213
00:09:22,560 --> 00:09:25,519
that the leakage defaults and each box

214
00:09:25,519 --> 00:09:28,800
has its own liquid characteristics we

215
00:09:28,800 --> 00:09:31,279
notice that device 4 is shifted

216
00:09:31,279 --> 00:09:33,519
apparently in time this could be

217
00:09:33,519 --> 00:09:36,000
explained by the in-process clock

218
00:09:36,000 --> 00:09:40,399
because we expect to have 625

219
00:09:40,399 --> 00:09:43,440
points in 10 clock cycles but we only

220
00:09:43,440 --> 00:09:48,800
observe 616 points for direct4

221
00:09:48,800 --> 00:09:52,080
we first show the results on the x-mega

222
00:09:52,080 --> 00:09:53,360
data set

223
00:09:53,360 --> 00:09:55,440
we can compare the performance of the

224
00:09:55,440 --> 00:09:58,959
printing models and the phantom models

225
00:09:58,959 --> 00:10:01,440
int here donates the number of attack

226
00:10:01,440 --> 00:10:05,120
traces to successfully recover the key

227
00:10:05,120 --> 00:10:08,240
for printing models we can observe that

228
00:10:08,240 --> 00:10:10,560
although the value of nt for single

229
00:10:10,560 --> 00:10:12,560
device attack given under

230
00:10:12,560 --> 00:10:16,240
diagonal is very small it varies widely

231
00:10:16,240 --> 00:10:19,120
in the case of cross device attacks

232
00:10:19,120 --> 00:10:21,600
in fact the better performance of

233
00:10:21,600 --> 00:10:23,680
cross-device attacks can also be

234
00:10:23,680 --> 00:10:25,839
explained by orbiting

235
00:10:25,839 --> 00:10:28,240
in other words the model trained on the

236
00:10:28,240 --> 00:10:31,040
profiling device cannot be safely

237
00:10:31,040 --> 00:10:33,839
generalized to the target device

238
00:10:33,839 --> 00:10:36,160
we therefore draw the learning curve

239
00:10:36,160 --> 00:10:38,800
when we trim the networks the blue and

240
00:10:38,800 --> 00:10:41,279
the green curves are the training loss

241
00:10:41,279 --> 00:10:44,320
and validation loss respectively on the

242
00:10:44,320 --> 00:10:46,560
profane device

243
00:10:46,560 --> 00:10:49,120
the red cross is the tesla loss on the

244
00:10:49,120 --> 00:10:50,640
target device

245
00:10:50,640 --> 00:10:53,600
on the profile device our fitting is not

246
00:10:53,600 --> 00:10:56,320
observed because the training loss and

247
00:10:56,320 --> 00:10:59,440
validation loss are highly consistent

248
00:10:59,440 --> 00:11:00,399
however

249
00:11:00,399 --> 00:11:03,040
on the target device the load the test

250
00:11:03,040 --> 00:11:05,839
loss increase rapidly which means the

251
00:11:05,839 --> 00:11:08,880
model is overfitted in the first few

252
00:11:08,880 --> 00:11:09,839
aprils

253
00:11:09,839 --> 00:11:10,880
this

254
00:11:10,880 --> 00:11:13,680
kind of overfitting is hard to identify

255
00:11:13,680 --> 00:11:16,320
because in realistic attacks

256
00:11:16,320 --> 00:11:19,360
the labeled attack traces are available

257
00:11:19,360 --> 00:11:21,680
and cannot be used to calculate the test

258
00:11:21,680 --> 00:11:23,680
laws

259
00:11:23,680 --> 00:11:26,320
after fine tuning the performance of the

260
00:11:26,320 --> 00:11:30,240
cross divisor tag is very much improved

261
00:11:30,240 --> 00:11:32,880
we also draw the learning codes when we

262
00:11:32,880 --> 00:11:34,959
fine tune the network

263
00:11:34,959 --> 00:11:38,320
the code in blue is the mmd loss that we

264
00:11:38,320 --> 00:11:41,680
want to minimize in the loss function

265
00:11:41,680 --> 00:11:44,240
the coming right is the test loss on the

266
00:11:44,240 --> 00:11:46,959
target device you can conclude from the

267
00:11:46,959 --> 00:11:50,800
results that minimizing the mmd loss can

268
00:11:50,800 --> 00:11:53,839
effectively reduce the test loss and

269
00:11:53,839 --> 00:11:58,720
improve the generalization of the models

270
00:11:58,720 --> 00:12:02,399
apart from power analysis en based cell

271
00:12:02,399 --> 00:12:04,959
channel tag is becoming more and more

272
00:12:04,959 --> 00:12:08,160
popular we note that em measurements are

273
00:12:08,160 --> 00:12:11,760
very sensitive to pro placement however

274
00:12:11,760 --> 00:12:14,240
when we consider the realistic profiled

275
00:12:14,240 --> 00:12:16,639
attack scenario the problem must be

276
00:12:16,639 --> 00:12:18,959
moved from the profile device to the

277
00:12:18,959 --> 00:12:21,920
target device so there is always a

278
00:12:21,920 --> 00:12:24,639
slight difference in the plot placement

279
00:12:24,639 --> 00:12:26,800
caused by human error due to the

280
00:12:26,800 --> 00:12:29,519
position distance and orientation

281
00:12:29,519 --> 00:12:32,800
to investigate the impact of human error

282
00:12:32,800 --> 00:12:35,680
we perform more cross device experiments

283
00:12:35,680 --> 00:12:39,120
on the x-mega em data set we can observe

284
00:12:39,120 --> 00:12:42,240
that the nt matrix is very similar to

285
00:12:42,240 --> 00:12:43,920
the results

286
00:12:43,920 --> 00:12:47,680
of x metadata sites as we expected the

287
00:12:47,680 --> 00:12:50,399
factoring models outperform the protein

288
00:12:50,399 --> 00:12:53,920
models significantly again we see that

289
00:12:53,920 --> 00:12:56,800
the evolution of mmd loss is highly

290
00:12:56,800 --> 00:12:59,440
consistent with the test loss which

291
00:12:59,440 --> 00:13:03,760
confirms our previous results

292
00:13:03,839 --> 00:13:07,040
unlike the about described dataset the

293
00:13:07,040 --> 00:13:10,320
support side provides measurements of an

294
00:13:10,320 --> 00:13:13,600
unprotected hardware implementation of

295
00:13:13,600 --> 00:13:15,680
esr fpga

296
00:13:15,680 --> 00:13:17,760
since the signal noise which of this

297
00:13:17,760 --> 00:13:20,480
data site is relatively small our

298
00:13:20,480 --> 00:13:23,440
protein models require around 1000

299
00:13:23,440 --> 00:13:26,480
stresses to successfully recover the key

300
00:13:26,480 --> 00:13:28,160
of the sim device

301
00:13:28,160 --> 00:13:31,200
when we apply the python models to other

302
00:13:31,200 --> 00:13:34,639
devices the required level of choices is

303
00:13:34,639 --> 00:13:36,800
likely to double

304
00:13:36,800 --> 00:13:39,279
we have though that all the cross device

305
00:13:39,279 --> 00:13:42,160
experiments get improved after applying

306
00:13:42,160 --> 00:13:45,120
our method most effective models achieve

307
00:13:45,120 --> 00:13:47,519
most similar performance as using the

308
00:13:47,519 --> 00:13:49,760
sim device for attacking

309
00:13:49,760 --> 00:13:52,800
therefore our approach is also suitable

310
00:13:52,800 --> 00:13:55,199
and efficient for hardware

311
00:13:55,199 --> 00:13:58,160
implementations

312
00:13:58,399 --> 00:14:01,839
apart from different devices the varies

313
00:14:01,839 --> 00:14:05,440
in implementations can also lead to bad

314
00:14:05,440 --> 00:14:07,360
attacking performance

315
00:14:07,360 --> 00:14:11,360
based on the esad data sites we simulate

316
00:14:11,360 --> 00:14:14,480
different implementations by adding

317
00:14:14,480 --> 00:14:17,199
artificial conveyors or noise to the

318
00:14:17,199 --> 00:14:18,959
original data set

319
00:14:18,959 --> 00:14:21,920
after the simulation we change the same

320
00:14:21,920 --> 00:14:24,880
model of the original data set and

321
00:14:24,880 --> 00:14:28,000
evaluate its performance on the deformed

322
00:14:28,000 --> 00:14:29,519
data sets

323
00:14:29,519 --> 00:14:32,560
this experiment simulates a complex

324
00:14:32,560 --> 00:14:35,199
attack scenario that the target device

325
00:14:35,199 --> 00:14:38,320
is treated as a black box that can turn

326
00:14:38,320 --> 00:14:40,639
on send channel code mirrors

327
00:14:40,639 --> 00:14:43,440
as we can see gaussian noise distorts

328
00:14:43,440 --> 00:14:46,480
the shape of the original traces in the

329
00:14:46,480 --> 00:14:48,480
amplitude domain

330
00:14:48,480 --> 00:14:51,040
well this inclination and the clock

331
00:14:51,040 --> 00:14:54,880
tutors add redness in the time domain

332
00:14:54,880 --> 00:14:57,120
we compare the performance of the

333
00:14:57,120 --> 00:15:00,480
protein models and phantom models using

334
00:15:00,480 --> 00:15:01,920
gas entropy

335
00:15:01,920 --> 00:15:04,720
the dotted line donates the printed

336
00:15:04,720 --> 00:15:08,079
models and the solid lines donate the

337
00:15:08,079 --> 00:15:09,519
phantom models

338
00:15:09,519 --> 00:15:12,720
it is obvious that the phantom models

339
00:15:12,720 --> 00:15:15,760
outperform the protein models we can

340
00:15:15,760 --> 00:15:18,800
infer from the results that cnn may not

341
00:15:18,800 --> 00:15:22,000
generalize well if only clean traces are

342
00:15:22,000 --> 00:15:23,680
fed to the network

343
00:15:23,680 --> 00:15:25,519
however fan

344
00:15:25,519 --> 00:15:27,839
use a small number of unlabeled node

345
00:15:27,839 --> 00:15:30,959
matrices can unleash the power of

346
00:15:30,959 --> 00:15:33,600
science and travel the network to learn

347
00:15:33,600 --> 00:15:37,279
the my environment features

348
00:15:37,680 --> 00:15:41,040
here is the computation cost of training

349
00:15:41,040 --> 00:15:42,399
and phantomy

350
00:15:42,399 --> 00:15:45,120
the training time of each approach is

351
00:15:45,120 --> 00:15:47,199
many determined by the size of the

352
00:15:47,199 --> 00:15:49,759
training data site the batch size and

353
00:15:49,759 --> 00:15:52,079
the length of the raw traces

354
00:15:52,079 --> 00:15:54,639
we can observe that the import temp for

355
00:15:54,639 --> 00:15:57,279
functioning is approximately twice that

356
00:15:57,279 --> 00:16:00,240
of training this is reasonable since

357
00:16:00,240 --> 00:16:02,800
more traces are processed and an

358
00:16:02,800 --> 00:16:06,000
additional mmd loss is calculated in the

359
00:16:06,000 --> 00:16:07,360
fountain phase

360
00:16:07,360 --> 00:16:10,079
in addition the function time is still

361
00:16:10,079 --> 00:16:13,040
affordable for example if we run the

362
00:16:13,040 --> 00:16:16,079
functioning phase for 15 airports this

363
00:16:16,079 --> 00:16:18,560
process can be completed within 2

364
00:16:18,560 --> 00:16:22,800
minutes for all considered data sets

365
00:16:22,800 --> 00:16:24,880
so in order to further understand how

366
00:16:24,880 --> 00:16:27,519
the location of the adaptation layers

367
00:16:27,519 --> 00:16:31,279
affect the output we conduct a series of

368
00:16:31,279 --> 00:16:34,240
experiments on the x metadata set

369
00:16:34,240 --> 00:16:36,800
with different adaptation layers

370
00:16:36,800 --> 00:16:39,920
we use a cnn whose classification part

371
00:16:39,920 --> 00:16:42,959
has three full connected layers we first

372
00:16:42,959 --> 00:16:45,360
fine-tune the network use only a single

373
00:16:45,360 --> 00:16:48,320
year and then compare it with the result

374
00:16:48,320 --> 00:16:50,959
of use using australians

375
00:16:50,959 --> 00:16:54,000
and overviews of the vision is that our

376
00:16:54,000 --> 00:16:56,720
matter still works even a single year is

377
00:16:56,720 --> 00:16:58,720
used for minimizing the

378
00:16:58,720 --> 00:17:02,320
mt loss another observation is that the

379
00:17:02,320 --> 00:17:04,880
deeper the layer the more difficult it

380
00:17:04,880 --> 00:17:08,079
seems to learn environment features

381
00:17:08,079 --> 00:17:10,640
this is renewable since the features

382
00:17:10,640 --> 00:17:13,760
obtained in clear layers must depend

383
00:17:13,760 --> 00:17:16,000
greatly on the specific

384
00:17:16,000 --> 00:17:17,199
datasets

385
00:17:17,199 --> 00:17:19,760
which are not safely transferable to

386
00:17:19,760 --> 00:17:21,679
normal ultimates

387
00:17:21,679 --> 00:17:24,640
still using all three layers of the

388
00:17:24,640 --> 00:17:27,599
classification parts is a good trade-off

389
00:17:27,599 --> 00:17:29,360
which usually brings

390
00:17:29,360 --> 00:17:31,679
better results than using a single

391
00:17:31,679 --> 00:17:33,919
adaptation layer

392
00:17:33,919 --> 00:17:36,160
the hyperparameter lambda in the dot

393
00:17:36,160 --> 00:17:38,480
function determines how strongly we

394
00:17:38,480 --> 00:17:40,880
would like to confuse the source and

395
00:17:40,880 --> 00:17:42,559
detect domains

396
00:17:42,559 --> 00:17:45,840
intuitively setting the lambda too small

397
00:17:45,840 --> 00:17:48,960
can cause the mmd regular result to have

398
00:17:48,960 --> 00:17:52,400
no effect on the learned replantation

399
00:17:52,400 --> 00:17:54,799
but setting the lambda too large will

400
00:17:54,799 --> 00:17:58,320
regularize too heavily enemy resulting a

401
00:17:58,320 --> 00:18:00,880
degenerate replication in which our

402
00:18:00,880 --> 00:18:03,200
features are too close together

403
00:18:03,200 --> 00:18:05,520
although there is usually a wide range

404
00:18:05,520 --> 00:18:07,760
of lambda where the printing model got

405
00:18:07,760 --> 00:18:08,880
improved

406
00:18:08,880 --> 00:18:11,840
a good empirical choice is to start with

407
00:18:11,840 --> 00:18:14,880
a relative small value especially when

408
00:18:14,880 --> 00:18:17,280
the signal noise range of the data side

409
00:18:17,280 --> 00:18:18,400
is small

410
00:18:18,400 --> 00:18:20,799
a smaller value of lambda means that the

411
00:18:20,799 --> 00:18:23,600
optimizer should put more effect into

412
00:18:23,600 --> 00:18:26,080
the tough classification task

413
00:18:26,080 --> 00:18:29,200
if it is observed that the reduction of

414
00:18:29,200 --> 00:18:32,000
the mmd loss is not significant or too

415
00:18:32,000 --> 00:18:35,600
slow we can gradually increase the value

416
00:18:35,600 --> 00:18:37,679
of lambda to speed up the finding

417
00:18:37,679 --> 00:18:40,080
process

418
00:18:40,480 --> 00:18:43,600
although fun trim with mmd loss help us

419
00:18:43,600 --> 00:18:47,360
obtain a robust model we need a set of

420
00:18:47,360 --> 00:18:50,880
attack traces to estimate the mmd

421
00:18:50,880 --> 00:18:53,200
this part is a factor query multiple

422
00:18:53,200 --> 00:18:56,559
enabled traces from the target device is

423
00:18:56,559 --> 00:18:59,440
not a strong assumption it is too

424
00:18:59,440 --> 00:19:02,160
meaningful to figure out how many traces

425
00:19:02,160 --> 00:19:04,720
are appropriate in practice when we

426
00:19:04,720 --> 00:19:06,880
fine-tune the network

427
00:19:06,880 --> 00:19:09,600
therefore we conduct a series of

428
00:19:09,600 --> 00:19:12,400
experiments with number of choices

429
00:19:12,400 --> 00:19:16,240
varying from 100 to 900 it can be

430
00:19:16,240 --> 00:19:19,679
observed that 100 traces are sufficient

431
00:19:19,679 --> 00:19:22,240
for the phantom phrase it is not

432
00:19:22,240 --> 00:19:25,760
surprising because 100 enabled traces

433
00:19:25,760 --> 00:19:28,559
can provide sufficient information that

434
00:19:28,559 --> 00:19:32,160
is distinguishable from the sustainment

435
00:19:32,160 --> 00:19:35,360
we can also conclude that using matrices

436
00:19:35,360 --> 00:19:37,360
could lead to a mouse table on and

437
00:19:37,360 --> 00:19:38,400
robust

438
00:19:38,400 --> 00:19:41,919
phantom model this is reasonable since

439
00:19:41,919 --> 00:19:44,880
more traces are passed to obtain a more

440
00:19:44,880 --> 00:19:49,280
precise estimate of mmd

441
00:19:49,760 --> 00:19:52,880
so they are coming to the conclusion

442
00:19:52,880 --> 00:19:55,679
this work focuses on addressing the open

443
00:19:55,679 --> 00:19:58,720
question of portability in profile set

444
00:19:58,720 --> 00:20:01,039
channel tech using transfer learning

445
00:20:01,039 --> 00:20:02,240
techniques

446
00:20:02,240 --> 00:20:04,960
we introduce a functioning phrase that

447
00:20:04,960 --> 00:20:08,400
utilize the non-descriptive information

448
00:20:08,400 --> 00:20:11,280
to enhance the printing model

449
00:20:11,280 --> 00:20:12,640
although

450
00:20:12,640 --> 00:20:15,200
is known to be robust against

451
00:20:15,200 --> 00:20:18,480
misalignment we show that satan may not

452
00:20:18,480 --> 00:20:22,400
generalize well if only clintus are fed

453
00:20:22,400 --> 00:20:25,520
however after finding with mmd loss our

454
00:20:25,520 --> 00:20:28,080
network is able to learn domain

455
00:20:28,080 --> 00:20:29,840
investment features

456
00:20:29,840 --> 00:20:32,880
besides this support does not require

457
00:20:32,880 --> 00:20:35,280
multiple profile devices targeted

458
00:20:35,280 --> 00:20:38,320
specific for processing and any

459
00:20:38,320 --> 00:20:41,520
information of the device so thank you

460
00:20:41,520 --> 00:20:43,280
for your attention and

461
00:20:43,280 --> 00:20:45,840
i welcome you to read our paper to get

462
00:20:45,840 --> 00:20:47,120
all the details

463
00:20:47,120 --> 00:20:49,600
if you have any questions feel free to

464
00:20:49,600 --> 00:20:53,559
ask thank you

