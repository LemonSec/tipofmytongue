1
00:00:00,480 --> 00:00:03,199
bonjour i'm gabrielle i'm a phd student

2
00:00:03,199 --> 00:00:05,040
from the revolution laboratory and the

3
00:00:05,040 --> 00:00:08,080
trs itself and today i will present you

4
00:00:08,080 --> 00:00:10,080
a new approach consisting in combining

5
00:00:10,080 --> 00:00:12,240
multiple neural networks in order to

6
00:00:12,240 --> 00:00:14,400
enhance the attacks against pkc

7
00:00:14,400 --> 00:00:15,759
algorithms

8
00:00:15,759 --> 00:00:17,359
so this work is a joint work with living

9
00:00:17,359 --> 00:00:19,279
about three and a half from the uber

10
00:00:19,279 --> 00:00:21,840
current laboratory and alexandre nelly

11
00:00:21,840 --> 00:00:24,320
from any space semiconductors

12
00:00:24,320 --> 00:00:26,320
but first what's the point with such an

13
00:00:26,320 --> 00:00:28,240
attacks targeting asymmetric

14
00:00:28,240 --> 00:00:30,320
cryptographic algorithms so to

15
00:00:30,320 --> 00:00:31,840
understand this concept let's take a

16
00:00:31,840 --> 00:00:34,160
look at an example

17
00:00:34,160 --> 00:00:36,079
so let's assume that an adversary has

18
00:00:36,079 --> 00:00:38,079
access to a physical device in which a

19
00:00:38,079 --> 00:00:41,520
pkc algorithm like rsa is implemented

20
00:00:41,520 --> 00:00:43,120
and such that you can configure the

21
00:00:43,120 --> 00:00:44,879
private key and the plaintext to compute

22
00:00:44,879 --> 00:00:47,360
the signature so obviously here the goal

23
00:00:47,360 --> 00:00:49,039
of the advertiser is to recover the

24
00:00:49,039 --> 00:00:51,039
sensitive information manipulated during

25
00:00:51,039 --> 00:00:53,440
this operation so to perform a sidechain

26
00:00:53,440 --> 00:00:56,480
attack she needs one probe here an m

27
00:00:56,480 --> 00:00:58,719
probe and one oscilloscope at least

28
00:00:58,719 --> 00:01:00,719
such that during the signature she

29
00:01:00,719 --> 00:01:02,480
captures a physical truss that directly

30
00:01:02,480 --> 00:01:06,159
depends on the sensitive information

31
00:01:06,159 --> 00:01:08,960
for example if the rsa algorithm is

32
00:01:08,960 --> 00:01:11,360
considered the targeted operation can be

33
00:01:11,360 --> 00:01:13,520
the modular exponentiation and this

34
00:01:13,520 --> 00:01:15,280
operation will be considered in the rest

35
00:01:15,280 --> 00:01:16,400
of this talk

36
00:01:16,400 --> 00:01:18,960
because the adversary case can compute

37
00:01:18,960 --> 00:01:21,040
as many signatures she wants the

38
00:01:21,040 --> 00:01:22,720
adversary can generate plenty of

39
00:01:22,720 --> 00:01:24,880
physical traces and thus find some

40
00:01:24,880 --> 00:01:27,200
patterns related to the secret the

41
00:01:27,200 --> 00:01:29,360
adversary 133

42
00:01:29,360 --> 00:01:32,320
if no consumers are implemented the

43
00:01:32,320 --> 00:01:34,880
adversary look for some patterns that

44
00:01:34,880 --> 00:01:36,799
directly depends on the private key

45
00:01:36,799 --> 00:01:39,439
bytes manipulated during the modularized

46
00:01:39,439 --> 00:01:40,880
pronunciation

47
00:01:40,880 --> 00:01:42,399
so through these examples you can

48
00:01:42,399 --> 00:01:44,320
identify two patterns

49
00:01:44,320 --> 00:01:46,399
that are dependent on the private key

50
00:01:46,399 --> 00:01:47,280
bits

51
00:01:47,280 --> 00:01:49,200
such that if a square and multiply

52
00:01:49,200 --> 00:01:51,040
algorithm is considered the shortest

53
00:01:51,040 --> 00:01:54,159
pattern define a square operation and

54
00:01:54,159 --> 00:01:56,719
characterize a bit equals to zero while

55
00:01:56,719 --> 00:01:58,799
the longest pattern highlight a square

56
00:01:58,799 --> 00:02:00,719
and multiply operation

57
00:02:00,719 --> 00:02:02,719
such that it characterizes a bit equals

58
00:02:02,719 --> 00:02:03,759
to one

59
00:02:03,759 --> 00:02:06,479
so in this example yes the adversary can

60
00:02:06,479 --> 00:02:08,239
identify some patterns that directly

61
00:02:08,239 --> 00:02:10,720
depends on the private key bits and

62
00:02:10,720 --> 00:02:13,120
guesses all the bits related to the

63
00:02:13,120 --> 00:02:15,520
modularization

64
00:02:15,520 --> 00:02:17,760
however if the same operation are

65
00:02:17,760 --> 00:02:20,400
processed whatever the value of the bits

66
00:02:20,400 --> 00:02:22,640
the adversary cannot clearly distinguish

67
00:02:22,640 --> 00:02:25,120
which bit is manipulated

68
00:02:25,120 --> 00:02:27,760
so to circumvent this issue she can

69
00:02:27,760 --> 00:02:29,920
employ some techniques introduced in the

70
00:02:29,920 --> 00:02:31,440
deep learning based site channel field

71
00:02:31,440 --> 00:02:33,599
to correctly retrieve the values of the

72
00:02:33,599 --> 00:02:35,280
processed bits

73
00:02:35,280 --> 00:02:36,720
in the deep learning based section

74
00:02:36,720 --> 00:02:38,879
attacks the adversary generates a neural

75
00:02:38,879 --> 00:02:40,720
network to automatically match a

76
00:02:40,720 --> 00:02:42,480
physical trust with the correct target

77
00:02:42,480 --> 00:02:44,319
sensitive value

78
00:02:44,319 --> 00:02:46,000
in such scenario the attack can be

79
00:02:46,000 --> 00:02:48,160
decomposed into two parts the profiling

80
00:02:48,160 --> 00:02:49,599
and the attack phase

81
00:02:49,599 --> 00:02:51,599
so first during the profiling phase the

82
00:02:51,599 --> 00:02:54,000
adversary decomposes its physical trust

83
00:02:54,000 --> 00:02:56,720
and substances such that each element

84
00:02:56,720 --> 00:02:59,120
represents the process related to each

85
00:02:59,120 --> 00:02:59,920
bit

86
00:02:59,920 --> 00:03:02,159
in the profiling phase the adversary

87
00:03:02,159 --> 00:03:04,480
trains a nerve network to predict the

88
00:03:04,480 --> 00:03:07,760
correct sensitive information she knows

89
00:03:07,760 --> 00:03:10,319
based on the sub-physical trusses

90
00:03:10,319 --> 00:03:11,680
generated

91
00:03:11,680 --> 00:03:14,720
in the previous slide

92
00:03:17,360 --> 00:03:18,959
once this phase is performed the

93
00:03:18,959 --> 00:03:20,800
adversary can predict the intermediate

94
00:03:20,800 --> 00:03:23,360
variable on a target device containing a

95
00:03:23,360 --> 00:03:26,480
secret she wishes to retrieve by using a

96
00:03:26,480 --> 00:03:29,040
new physical address in the attack phase

97
00:03:29,040 --> 00:03:32,400
she can assign a value to each bit from

98
00:03:32,400 --> 00:03:34,799
the probabilities returned by the neural

99
00:03:34,799 --> 00:03:38,080
network then by estimating the value of

100
00:03:38,080 --> 00:03:41,120
edge bit induced in dedicated truss the

101
00:03:41,120 --> 00:03:43,200
adversary can guess the private key

102
00:03:43,200 --> 00:03:46,480
manipulated by the cryptographic module

103
00:03:46,480 --> 00:03:49,440
but unfortunately the adversary rarely

104
00:03:49,440 --> 00:03:51,680
correctly predicts all the bits related

105
00:03:51,680 --> 00:03:53,360
to the private key

106
00:03:53,360 --> 00:03:55,360
indeed if the adversary does not

107
00:03:55,360 --> 00:03:57,200
perfectly retrieve the bits of the

108
00:03:57,200 --> 00:03:59,760
private key then remaining operation

109
00:03:59,760 --> 00:04:01,920
must be performed in order to correct

110
00:04:01,920 --> 00:04:03,599
the wrong predictions

111
00:04:03,599 --> 00:04:05,360
so depending on the knowledge of the

112
00:04:05,360 --> 00:04:07,439
adversary different scenarios can be

113
00:04:07,439 --> 00:04:09,760
considered in order to correct these

114
00:04:09,760 --> 00:04:12,480
wrong predictions in our work we define

115
00:04:12,480 --> 00:04:14,799
the three following scenarios

116
00:04:14,799 --> 00:04:16,959
so first the worst case scenario

117
00:04:16,959 --> 00:04:19,120
suggests that an adversary does not know

118
00:04:19,120 --> 00:04:22,320
the position of uncertain predictions

119
00:04:22,320 --> 00:04:24,479
indeed given a guest private key the

120
00:04:24,479 --> 00:04:27,120
adversary measures the related maximal

121
00:04:27,120 --> 00:04:30,240
percentage of errors in order to compute

122
00:04:30,240 --> 00:04:32,240
how the resulted possible privacy

123
00:04:32,240 --> 00:04:33,440
candidates

124
00:04:33,440 --> 00:04:35,919
assuming that the maximum of n bits is

125
00:04:35,919 --> 00:04:38,160
run used then the other three starts by

126
00:04:38,160 --> 00:04:40,720
correcting the first bit and then

127
00:04:40,720 --> 00:04:43,199
verifies if the corrected private key is

128
00:04:43,199 --> 00:04:44,800
validated or not

129
00:04:44,800 --> 00:04:47,680
if not the advert3 cracks the second bit

130
00:04:47,680 --> 00:04:50,639
and verify the related private key guest

131
00:04:50,639 --> 00:04:53,840
and so on and so forth

132
00:04:53,840 --> 00:04:56,000
then all the combinations must be

133
00:04:56,000 --> 00:04:57,919
computed in order to retrieve the true

134
00:04:57,919 --> 00:05:00,560
private key and this complexity called

135
00:05:00,560 --> 00:05:02,639
naive complexity can be computed as

136
00:05:02,639 --> 00:05:05,360
follows such that epsilon bit

137
00:05:05,360 --> 00:05:07,680
denotes the arbitrate the second

138
00:05:07,680 --> 00:05:09,440
scenario suggested that the adversary

139
00:05:09,440 --> 00:05:11,680
can find the earth's uncertain bits by

140
00:05:11,680 --> 00:05:13,680
manipulating a threshold so that all

141
00:05:13,680 --> 00:05:15,919
bits predicted with a lower probability

142
00:05:15,919 --> 00:05:18,160
are considered as uncertain such that

143
00:05:18,160 --> 00:05:19,039
here

144
00:05:19,039 --> 00:05:21,360
all orange bits are considered as

145
00:05:21,360 --> 00:05:24,240
uncertain as each bit can take value in

146
00:05:24,240 --> 00:05:27,440
range 0 1 the resulted complexity can be

147
00:05:27,440 --> 00:05:29,680
expressed as follows

148
00:05:29,680 --> 00:05:31,840
finally if the adversary must deal with

149
00:05:31,840 --> 00:05:34,160
blinding scalar or exponent she can

150
00:05:34,160 --> 00:05:36,240
consider the alternate attack introduced

151
00:05:36,240 --> 00:05:38,639
by schindler and veiners how the related

152
00:05:38,639 --> 00:05:40,880
attack depends on the targeted public

153
00:05:40,880 --> 00:05:43,360
key cryptographic algorithm we define a

154
00:05:43,360 --> 00:05:45,520
complexity metric when the adversary

155
00:05:45,520 --> 00:05:49,120
targets an rsa algorithm without crt

156
00:05:49,120 --> 00:05:51,600
mode such that it can be defined as

157
00:05:51,600 --> 00:05:53,280
follows through these complexity

158
00:05:53,280 --> 00:05:55,280
measures we can notice that reducing the

159
00:05:55,280 --> 00:05:57,759
error rate epsilon bits is actually

160
00:05:57,759 --> 00:05:59,440
beneficial to reduce the number of

161
00:05:59,440 --> 00:06:01,440
remaining operations whatever the

162
00:06:01,440 --> 00:06:03,680
complexity measure used and more

163
00:06:03,680 --> 00:06:06,160
interestingly those results suggest that

164
00:06:06,160 --> 00:06:08,160
a slight improvement in accuracy can be

165
00:06:08,160 --> 00:06:10,000
highly beneficial from an attack

166
00:06:10,000 --> 00:06:12,400
perspective because a realistic

167
00:06:12,400 --> 00:06:15,120
improvement can be observed for a full

168
00:06:15,120 --> 00:06:17,520
attack scenario in deep learning one

169
00:06:17,520 --> 00:06:19,759
typical solution to increase even

170
00:06:19,759 --> 00:06:22,560
slightly and accuracy consists in using

171
00:06:22,560 --> 00:06:25,280
the ensemble principle but what does the

172
00:06:25,280 --> 00:06:27,919
principle of ensembling mean so given an

173
00:06:27,919 --> 00:06:30,000
adversary generating three neural

174
00:06:30,000 --> 00:06:32,560
networks the ensemble methods combine

175
00:06:32,560 --> 00:06:34,880
individual predictions via a consensus

176
00:06:34,880 --> 00:06:36,479
method such as

177
00:06:36,479 --> 00:06:38,800
the majority vote in order to reduce the

178
00:06:38,800 --> 00:06:41,759
overall error thus an ensemble model

179
00:06:41,759 --> 00:06:44,080
represented as follows induces

180
00:06:44,080 --> 00:06:46,240
interaction with the targeted variables

181
00:06:46,240 --> 00:06:48,800
such as a bid value and interaction with

182
00:06:48,800 --> 00:06:50,960
other committee members in order to

183
00:06:50,960 --> 00:06:53,440
reduce the global error

184
00:06:53,440 --> 00:06:55,759
indeed following the work provided by

185
00:06:55,759 --> 00:06:57,919
tamar and gosh we know that an ensemble

186
00:06:57,919 --> 00:07:00,639
method can reduce the expected at lr by

187
00:07:00,639 --> 00:07:02,560
the number of committee members induced

188
00:07:02,560 --> 00:07:04,960
in the ensemble model such that

189
00:07:04,960 --> 00:07:07,199
depending on their error correlation

190
00:07:07,199 --> 00:07:08,800
denoted by delta

191
00:07:08,800 --> 00:07:11,440
then symbol expected r may not be

192
00:07:11,440 --> 00:07:13,680
reduced if the errors are highly

193
00:07:13,680 --> 00:07:16,400
correlated or in the best case scenario

194
00:07:16,400 --> 00:07:19,440
it can be divided by one over nc such

195
00:07:19,440 --> 00:07:21,199
that nc defines the number of committee

196
00:07:21,199 --> 00:07:23,599
members in the ensemble model if the

197
00:07:23,599 --> 00:07:26,639
individual errors are uncorrelated so in

198
00:07:26,639 --> 00:07:28,400
the rest of this talk we define the

199
00:07:28,400 --> 00:07:30,560
diversity as a quantity measuring the

200
00:07:30,560 --> 00:07:33,199
difference in term of predictions among

201
00:07:33,199 --> 00:07:36,080
the committee members typically liu and

202
00:07:36,080 --> 00:07:39,599
al introduce three sources of diversity

203
00:07:39,599 --> 00:07:41,680
so first the thai point diversity

204
00:07:41,680 --> 00:07:43,919
characterizes the variety of committee

205
00:07:43,919 --> 00:07:46,800
members architectures introduced in an

206
00:07:46,800 --> 00:07:48,319
ensemble model

207
00:07:48,319 --> 00:07:51,440
then the type 2 diversity selects a

208
00:07:51,440 --> 00:07:53,759
subset of members from a pool of noble

209
00:07:53,759 --> 00:07:56,479
networks in order to only keep those

210
00:07:56,479 --> 00:07:59,120
with a minimum amount of correlations

211
00:07:59,120 --> 00:08:02,479
and finally the type 3 diversity induces

212
00:08:02,479 --> 00:08:03,840
interaction between the committee

213
00:08:03,840 --> 00:08:06,080
members during the training process such

214
00:08:06,080 --> 00:08:08,240
that even if the same neural network

215
00:08:08,240 --> 00:08:11,039
architecture is duplicated over the

216
00:08:11,039 --> 00:08:12,400
ensemble model

217
00:08:12,400 --> 00:08:14,639
the training process forces the ensemble

218
00:08:14,639 --> 00:08:16,879
model to decorate their individual

219
00:08:16,879 --> 00:08:19,440
errors in this work we develop a new

220
00:08:19,440 --> 00:08:21,919
loss function that optimizes the type 3

221
00:08:21,919 --> 00:08:23,919
diversity in order to maximize the

222
00:08:23,919 --> 00:08:26,000
mutual information between the ensemble

223
00:08:26,000 --> 00:08:28,960
model and the targeted variable

224
00:08:28,960 --> 00:08:30,720
from information theory point of view

225
00:08:30,720 --> 00:08:32,719
the ensembl apple approach can be

226
00:08:32,719 --> 00:08:34,559
summarized as follows

227
00:08:34,559 --> 00:08:37,279
so let's assume that a message is sent

228
00:08:37,279 --> 00:08:39,360
through communications channel such that

229
00:08:39,360 --> 00:08:41,440
the output is characterized by an

230
00:08:41,440 --> 00:08:44,320
encoding value x thus to retrieve the

231
00:08:44,320 --> 00:08:47,200
message x should be decoded such that

232
00:08:47,200 --> 00:08:49,519
the output provides an approximation of

233
00:08:49,519 --> 00:08:52,160
the message to reduce the r induced by

234
00:08:52,160 --> 00:08:55,600
the decoder a function g must be found

235
00:08:55,600 --> 00:08:57,920
such that the following inequality is

236
00:08:57,920 --> 00:08:58,720
all

237
00:08:58,720 --> 00:09:00,959
thus to reduce the error term the mutual

238
00:09:00,959 --> 00:09:03,519
information between x and the message

239
00:09:03,519 --> 00:09:05,360
should be maximized

240
00:09:05,360 --> 00:09:08,640
similarly brown zoo and lee proposed to

241
00:09:08,640 --> 00:09:10,560
extend this notion to the ensembling

242
00:09:10,560 --> 00:09:13,200
approach such that given a label the

243
00:09:13,200 --> 00:09:15,600
ensemble model characterizes the encoded

244
00:09:15,600 --> 00:09:18,000
representation then to retrieve the

245
00:09:18,000 --> 00:09:20,480
value of the correct label the ensemble

246
00:09:20,480 --> 00:09:22,800
model has to generate interaction

247
00:09:22,800 --> 00:09:24,880
between the committee members such that

248
00:09:24,880 --> 00:09:27,040
the estimated level corresponds to the

249
00:09:27,040 --> 00:09:29,040
targeted one thus the previous

250
00:09:29,040 --> 00:09:31,200
inequality can be written as follows

251
00:09:31,200 --> 00:09:34,160
such that to minimize vr induced by the

252
00:09:34,160 --> 00:09:35,760
ensemble model's prediction the

253
00:09:35,760 --> 00:09:38,480
adversary had to maximize the mutual

254
00:09:38,480 --> 00:09:41,200
information between the targeted level

255
00:09:41,200 --> 00:09:43,440
and the ensemble model so to solve this

256
00:09:43,440 --> 00:09:45,760
problem brown zoo and lee proposed a

257
00:09:45,760 --> 00:09:48,080
mutual information on civil diversity

258
00:09:48,080 --> 00:09:50,000
but unfortunately from a practical

259
00:09:50,000 --> 00:09:51,839
perspective it is quite difficult to

260
00:09:51,839 --> 00:09:54,560
estimate lighter harder interaction

261
00:09:54,560 --> 00:09:57,440
information because indeed currently

262
00:09:57,440 --> 00:09:59,360
there is no effective computational

263
00:09:59,360 --> 00:10:02,160
approach in the iterator that allow the

264
00:10:02,160 --> 00:10:05,120
computation of this equation so to solve

265
00:10:05,120 --> 00:10:07,760
this issue brown proposed to simplify

266
00:10:07,760 --> 00:10:10,320
this equation by considering only

267
00:10:10,320 --> 00:10:12,800
pairwise components typically this

268
00:10:12,800 --> 00:10:14,720
pairwise approximation can be summarized

269
00:10:14,720 --> 00:10:17,200
as follows such that first this mutual

270
00:10:17,200 --> 00:10:19,519
information measures the relevance of

271
00:10:19,519 --> 00:10:21,519
the ensemble model to retrieve the class

272
00:10:21,519 --> 00:10:24,560
levels and burns itself predictions then

273
00:10:24,560 --> 00:10:26,079
this mutual information called

274
00:10:26,079 --> 00:10:28,000
redundancy measures the dependency

275
00:10:28,000 --> 00:10:30,839
between current classifier and existing

276
00:10:30,839 --> 00:10:34,240
classifiers such that it indicates that

277
00:10:34,240 --> 00:10:36,640
a low class conditional correlation is

278
00:10:36,640 --> 00:10:39,040
needed in order to perform an efficient

279
00:10:39,040 --> 00:10:41,519
and simple model and finally the

280
00:10:41,519 --> 00:10:43,920
conditional redundancy measures the

281
00:10:43,920 --> 00:10:45,920
conditional dependency between current

282
00:10:45,920 --> 00:10:48,560
classifier and existing classifiers

283
00:10:48,560 --> 00:10:50,959
given the class level but while the

284
00:10:50,959 --> 00:10:53,120
relevance and the conditional redundancy

285
00:10:53,120 --> 00:10:55,279
should be maximized the redundancy

286
00:10:55,279 --> 00:10:57,279
should be minimized in order to reduce

287
00:10:57,279 --> 00:10:58,800
the correlation between the committee

288
00:10:58,800 --> 00:11:01,200
members induced in the ensemble model

289
00:11:01,200 --> 00:11:03,920
however from this pairwise notation an

290
00:11:03,920 --> 00:11:06,079
adversary can question the feasibility

291
00:11:06,079 --> 00:11:08,000
of designing a loss function projected

292
00:11:08,000 --> 00:11:09,680
the mutual information between an

293
00:11:09,680 --> 00:11:12,079
assembler model and the targeted label

294
00:11:12,079 --> 00:11:13,760
so to solve this problem we develop a

295
00:11:13,760 --> 00:11:16,320
new loss function called ensembling loss

296
00:11:16,320 --> 00:11:18,240
which promotes the interaction between

297
00:11:18,240 --> 00:11:20,000
the committee members in order to

298
00:11:20,000 --> 00:11:21,760
approximate the material information

299
00:11:21,760 --> 00:11:23,920
previously introduced typically to

300
00:11:23,920 --> 00:11:26,000
conduct ensemble methods three learning

301
00:11:26,000 --> 00:11:30,160
approaches can be considered so first

302
00:11:30,160 --> 00:11:32,480
in independent learning strategy each

303
00:11:32,480 --> 00:11:35,200
committee member only interacts with the

304
00:11:35,200 --> 00:11:36,959
targeted level during the training

305
00:11:36,959 --> 00:11:38,959
process though they are independently

306
00:11:38,959 --> 00:11:40,480
obtained such that their posterior

307
00:11:40,480 --> 00:11:42,640
probabilities are combined once the

308
00:11:42,640 --> 00:11:44,959
learning process is fully performed

309
00:11:44,959 --> 00:11:47,600
then the sequential training process

310
00:11:47,600 --> 00:11:49,680
suggests that the committee members of a

311
00:11:49,680 --> 00:11:51,200
constantly trained such that the

312
00:11:51,200 --> 00:11:54,000
posterior probabilities obtained from

313
00:11:54,000 --> 00:11:56,000
the previous model can be used to

314
00:11:56,000 --> 00:11:57,920
penalize the learning process related to

315
00:11:57,920 --> 00:12:00,560
the current model and finally the

316
00:12:00,560 --> 00:12:02,800
semi-tenuous ensemble learning consists

317
00:12:02,800 --> 00:12:05,279
in promoting the interaction between the

318
00:12:05,279 --> 00:12:06,880
committee members during the training

319
00:12:06,880 --> 00:12:09,200
process in order to reduce the variation

320
00:12:09,200 --> 00:12:11,600
errors and in our work a particular

321
00:12:11,600 --> 00:12:14,320
focus was made on the last solution

322
00:12:14,320 --> 00:12:17,600
indeed in our paper we introduced the

323
00:12:17,600 --> 00:12:20,240
ensembling loss such that given a set of

324
00:12:20,240 --> 00:12:22,800
profiling truss an ensemble model and a

325
00:12:22,800 --> 00:12:25,120
number of attack traces this loss

326
00:12:25,120 --> 00:12:27,600
function can be decomposed into three

327
00:12:27,600 --> 00:12:30,880
third losses so first the relevance loss

328
00:12:30,880 --> 00:12:33,040
aims at maximizing an approximation of

329
00:12:33,040 --> 00:12:35,120
the mutual information between a

330
00:12:35,120 --> 00:12:38,160
committee member denoted f m and a

331
00:12:38,160 --> 00:12:40,560
targeted variable z

332
00:12:40,560 --> 00:12:42,480
in other words through the learning

333
00:12:42,480 --> 00:12:45,200
process we want to penalize a model when

334
00:12:45,200 --> 00:12:48,079
the correct label z is not ranked as the

335
00:12:48,079 --> 00:12:50,320
highest hypothetical class

336
00:12:50,320 --> 00:12:52,079
to understand the impact of this sub

337
00:12:52,079 --> 00:12:54,560
loss function let's assume a multi-class

338
00:12:54,560 --> 00:12:57,120
classification problem with three output

339
00:12:57,120 --> 00:12:58,880
such that from a machine learning

340
00:12:58,880 --> 00:13:00,639
perspective the maximization of the

341
00:13:00,639 --> 00:13:03,120
related mutual information tends to

342
00:13:03,120 --> 00:13:05,680
generate three compact clusters

343
00:13:05,680 --> 00:13:08,000
if false positives or false negatives

344
00:13:08,000 --> 00:13:09,519
appeal during the training process the

345
00:13:09,519 --> 00:13:11,440
ensemble model will be overconfident on

346
00:13:11,440 --> 00:13:13,680
its prediction and the resulted errors

347
00:13:13,680 --> 00:13:15,600
could be persistent so to reduce this

348
00:13:15,600 --> 00:13:17,279
effect a solution is to provide

349
00:13:17,279 --> 00:13:19,440
diversity in order to limit the impact

350
00:13:19,440 --> 00:13:21,279
of this false positive and false

351
00:13:21,279 --> 00:13:24,000
negative example so all the fab losses

352
00:13:24,000 --> 00:13:26,720
must be defined thus the redundancy loss

353
00:13:26,720 --> 00:13:28,560
is introduced to minimize the ensemble

354
00:13:28,560 --> 00:13:30,160
information between two committee

355
00:13:30,160 --> 00:13:32,480
members such that in other words

356
00:13:32,480 --> 00:13:34,560
minimizing an approximation of this

357
00:13:34,560 --> 00:13:36,480
mutual information is equivalent to an

358
00:13:36,480 --> 00:13:39,600
approximation of maximizing the entropy

359
00:13:39,600 --> 00:13:42,880
of observing f m given f m

360
00:13:42,880 --> 00:13:45,279
so consequently we want to increase the

361
00:13:45,279 --> 00:13:48,639
uncertainty of f m given f n

362
00:13:48,639 --> 00:13:50,639
thus through the minimization of the

363
00:13:50,639 --> 00:13:53,040
redundancy loss we promote the cluster

364
00:13:53,040 --> 00:13:55,279
scattering and reduce the global

365
00:13:55,279 --> 00:13:57,199
confidence of the committee members on

366
00:13:57,199 --> 00:13:58,959
the false positives and defaults

367
00:13:58,959 --> 00:14:01,360
negative in order to decrease their

368
00:14:01,360 --> 00:14:03,839
consistency but unfortunately this term

369
00:14:03,839 --> 00:14:06,639
can negatively affect the confidence of

370
00:14:06,639 --> 00:14:09,519
the correct output thus an additional

371
00:14:09,519 --> 00:14:11,760
term should be considered

372
00:14:11,760 --> 00:14:14,240
that's why the conditional redundancy

373
00:14:14,240 --> 00:14:16,399
loss has been introduced

374
00:14:16,399 --> 00:14:18,720
in order to maximize an approximation

375
00:14:18,720 --> 00:14:20,639
half the following conditional mutual

376
00:14:20,639 --> 00:14:21,920
information

377
00:14:21,920 --> 00:14:25,199
indeed through this sub losses function

378
00:14:25,199 --> 00:14:27,279
we want to minimize the dissimilarity

379
00:14:27,279 --> 00:14:30,720
between the pairwise model f n and f m

380
00:14:30,720 --> 00:14:33,360
knowing the targeted values

381
00:14:33,360 --> 00:14:35,839
this penalization will have the effect

382
00:14:35,839 --> 00:14:37,760
of increasing the consensus of the

383
00:14:37,760 --> 00:14:39,440
network on the true positive and the

384
00:14:39,440 --> 00:14:42,160
true negative example such that we

385
00:14:42,160 --> 00:14:44,800
consolidate the good prediction with

386
00:14:44,800 --> 00:14:46,560
more persistency

387
00:14:46,560 --> 00:14:48,959
thus combining those three third losses

388
00:14:48,959 --> 00:14:51,199
is beneficial to construct an ensemble

389
00:14:51,199 --> 00:14:53,760
model that promotes interactions between

390
00:14:53,760 --> 00:14:56,000
the committee members in order to enhand

391
00:14:56,000 --> 00:14:57,440
their diversity

392
00:14:57,440 --> 00:14:58,800
but however

393
00:14:58,800 --> 00:15:01,519
how does its loss function consider in

394
00:15:01,519 --> 00:15:04,959
the lsca promote diversity so to deeply

395
00:15:04,959 --> 00:15:06,959
understand the benefits of each loss

396
00:15:06,959 --> 00:15:08,880
function used in deep learning based

397
00:15:08,880 --> 00:15:10,639
such an attacks

398
00:15:10,639 --> 00:15:12,800
we decide to employ the disney

399
00:15:12,800 --> 00:15:16,000
visualization tool in order to reflect

400
00:15:16,000 --> 00:15:18,480
the evolution of the data's prediction

401
00:15:18,480 --> 00:15:21,279
depending on the loss function used and

402
00:15:21,279 --> 00:15:23,839
those visualizations illustrate this

403
00:15:23,839 --> 00:15:26,240
evolution

404
00:15:26,240 --> 00:15:28,079
so first when the cross-entrepreneurs

405
00:15:28,079 --> 00:15:30,320
function considered we observed that the

406
00:15:30,320 --> 00:15:32,480
ensemble model is not trained enough to

407
00:15:32,480 --> 00:15:34,800
efficiently discriminate each class

408
00:15:34,800 --> 00:15:36,240
indeed there are many connections

409
00:15:36,240 --> 00:15:38,160
between each class leading to a loss of

410
00:15:38,160 --> 00:15:40,160
the global performance

411
00:15:40,160 --> 00:15:42,399
hence in this example many false

412
00:15:42,399 --> 00:15:45,279
positives and false negatives can badly

413
00:15:45,279 --> 00:15:47,680
influence the global performance of the

414
00:15:47,680 --> 00:15:50,079
model

415
00:15:50,079 --> 00:15:52,399
on the other hand the ranking loss

416
00:15:52,399 --> 00:15:56,079
generates three separate clusters and as

417
00:15:56,079 --> 00:15:58,399
previously notified the ranking loss can

418
00:15:58,399 --> 00:16:01,040
be formulated as the relevant source

419
00:16:01,040 --> 00:16:01,759
so

420
00:16:01,759 --> 00:16:03,279
through the minimization of this

421
00:16:03,279 --> 00:16:05,600
function we want to minimize the

422
00:16:05,600 --> 00:16:09,680
conditional entropy hv given fm which

423
00:16:09,680 --> 00:16:12,320
promotes the generation of three compact

424
00:16:12,320 --> 00:16:15,120
clusters such that here the disney

425
00:16:15,120 --> 00:16:18,480
visualizations confirm this result has

426
00:16:18,480 --> 00:16:21,199
the ensemble model is overconfident in

427
00:16:21,199 --> 00:16:23,440
the features captured during the

428
00:16:23,440 --> 00:16:24,800
learning process

429
00:16:24,800 --> 00:16:27,680
from a diversity perspective the best

430
00:16:27,680 --> 00:16:29,440
solution should create three separate

431
00:16:29,440 --> 00:16:31,680
clusters when the ensemble model is

432
00:16:31,680 --> 00:16:34,160
confident in its prediction

433
00:16:34,160 --> 00:16:36,639
while the errors or the uncertain

434
00:16:36,639 --> 00:16:38,720
prediction should converge towards the

435
00:16:38,720 --> 00:16:41,600
equidistant point of the centroid of the

436
00:16:41,600 --> 00:16:43,519
clusters hopefully through the disney

437
00:16:43,519 --> 00:16:45,839
visualization we observe that the

438
00:16:45,839 --> 00:16:47,839
assembling loss converged towards this

439
00:16:47,839 --> 00:16:49,600
best solution such that this result

440
00:16:49,600 --> 00:16:52,959
tends to reduce the number of consistent

441
00:16:52,959 --> 00:16:55,680
false positives and false negatives such

442
00:16:55,680 --> 00:16:58,639
that few heroes can be detected on each

443
00:16:58,639 --> 00:17:00,880
clusters in contrast with the cross

444
00:17:00,880 --> 00:17:02,959
entropy loss or the ranking loss

445
00:17:02,959 --> 00:17:04,880
functions so to assess the benefits of

446
00:17:04,880 --> 00:17:06,799
the assembling loss we decide to compare

447
00:17:06,799 --> 00:17:08,799
the performance it provides with the

448
00:17:08,799 --> 00:17:11,439
quest entropy and the ranking loss so in

449
00:17:11,439 --> 00:17:13,679
this talk we consider the following data

450
00:17:13,679 --> 00:17:16,480
the data set is a cqrs a implementation

451
00:17:16,480 --> 00:17:18,559
that has been introduced by carbon and

452
00:17:18,559 --> 00:17:21,760
al at chess 2019. more precisely it

453
00:17:21,760 --> 00:17:23,760
implements three confirmations namely

454
00:17:23,760 --> 00:17:25,280
input randomization modulus

455
00:17:25,280 --> 00:17:27,839
randomization and exponent randomization

456
00:17:27,839 --> 00:17:30,080
such that in this talk we target the

457
00:17:30,080 --> 00:17:32,240
same operation as carbon and aisle

458
00:17:32,240 --> 00:17:34,480
namely exponentiation algorithm which

459
00:17:34,480 --> 00:17:38,320
induces a variable name seg3 that takes

460
00:17:38,320 --> 00:17:40,240
three possible values

461
00:17:40,240 --> 00:17:42,080
depending on the value of take three the

462
00:17:42,080 --> 00:17:43,840
adversary retrieves the blinding

463
00:17:43,840 --> 00:17:46,080
spanning bits such that if two

464
00:17:46,080 --> 00:17:49,280
consecutive sex-free values are similar

465
00:17:49,280 --> 00:17:51,360
the resulted private key guess equals

466
00:17:51,360 --> 00:17:54,880
one and zero hold arise

467
00:17:54,880 --> 00:17:57,280
thus knowing all the sec free values at

468
00:17:57,280 --> 00:17:59,280
filter it without the blinding exponent

469
00:17:59,280 --> 00:18:01,600
bits to evaluate the performance of the

470
00:18:01,600 --> 00:18:03,520
ensemble model to retrieve the private

471
00:18:03,520 --> 00:18:05,840
key bits we can reconstruct the private

472
00:18:05,840 --> 00:18:08,240
key guest from a sequence of the free

473
00:18:08,240 --> 00:18:10,799
value and then we can compute the

474
00:18:10,799 --> 00:18:13,600
accuracy of retrieve each bit of the

475
00:18:13,600 --> 00:18:15,679
private key and because we know the

476
00:18:15,679 --> 00:18:17,440
accuracy we can also retrieve the

477
00:18:17,440 --> 00:18:19,600
arbitrate epsilon bit so from those

478
00:18:19,600 --> 00:18:21,679
metrics the evaluator can finally

479
00:18:21,679 --> 00:18:24,000
compute those complexity measures that

480
00:18:24,000 --> 00:18:25,840
depends on the adversary's knowledge as

481
00:18:25,840 --> 00:18:27,679
introduced in the first part of this

482
00:18:27,679 --> 00:18:30,320
topic so first we evaluate the benefits

483
00:18:30,320 --> 00:18:32,400
of using a single neural network and

484
00:18:32,400 --> 00:18:34,799
compare the performance depending on the

485
00:18:34,799 --> 00:18:37,760
loss used i have only a single neural

486
00:18:37,760 --> 00:18:40,320
network old the only laws considered are

487
00:18:40,320 --> 00:18:42,880
the cross entropy and the ranking loss

488
00:18:42,880 --> 00:18:45,679
and through our comparison we observed

489
00:18:45,679 --> 00:18:47,520
that the ranking loss outperformed the

490
00:18:47,520 --> 00:18:49,280
world network trying to reach the quest

491
00:18:49,280 --> 00:18:51,679
entrepreneur's function then we have set

492
00:18:51,679 --> 00:18:54,080
the benefits of combining five identical

493
00:18:54,080 --> 00:18:56,799
neural matrix when no interactions are

494
00:18:56,799 --> 00:18:59,120
induced during the training phase this

495
00:18:59,120 --> 00:19:01,200
first observation shows that a

496
00:19:01,200 --> 00:19:03,200
non-negligible improvement can be

497
00:19:03,200 --> 00:19:05,679
observed whatever the loss function used

498
00:19:05,679 --> 00:19:07,760
indeed i've previously mentioned the

499
00:19:07,760 --> 00:19:10,240
ensembling approach is beneficial as it

500
00:19:10,240 --> 00:19:12,559
reduced the global errors following

501
00:19:12,559 --> 00:19:15,360
their correlation observed between each

502
00:19:15,360 --> 00:19:17,919
individual committee members hence even

503
00:19:17,919 --> 00:19:19,919
if the same neural network architecture

504
00:19:19,919 --> 00:19:22,160
is considered the initialization of the

505
00:19:22,160 --> 00:19:23,919
trainable parameters induces some

506
00:19:23,919 --> 00:19:25,440
dissimilarities between the trained

507
00:19:25,440 --> 00:19:27,520
committee members and enhance the

508
00:19:27,520 --> 00:19:29,760
ensemble model performance

509
00:19:29,760 --> 00:19:32,160
finally considering the ensembling loss

510
00:19:32,160 --> 00:19:34,400
is beneficial to generate interactions

511
00:19:34,400 --> 00:19:36,000
between the committee members during the

512
00:19:36,000 --> 00:19:38,640
training phase and as these interactions

513
00:19:38,640 --> 00:19:41,440
promote uncorrelated errors it improves

514
00:19:41,440 --> 00:19:43,520
even more the performance of the

515
00:19:43,520 --> 00:19:46,000
ensemble model but looking at the

516
00:19:46,000 --> 00:19:48,320
resulting complexity materials we can

517
00:19:48,320 --> 00:19:50,720
denote that the number of operation

518
00:19:50,720 --> 00:19:53,200
remaining high however following the

519
00:19:53,200 --> 00:19:56,000
european sergei security guidance a

520
00:19:56,000 --> 00:19:58,480
maximum brute force complexity of around

521
00:19:58,480 --> 00:20:01,760
2 power 100 is considered as practical

522
00:20:01,760 --> 00:20:04,000
thus while the state-of-the-art results

523
00:20:04,000 --> 00:20:06,720
mainly consider the first row of the

524
00:20:06,720 --> 00:20:09,360
table it suggests that the rsr

525
00:20:09,360 --> 00:20:11,919
implementation is secure whatever the

526
00:20:11,919 --> 00:20:14,480
adversary's knowledge but using ensemble

527
00:20:14,480 --> 00:20:16,720
models drastically reduce the complexity

528
00:20:16,720 --> 00:20:18,559
matrix such that depending on the

529
00:20:18,559 --> 00:20:20,559
adversary's knowledge the security of

530
00:20:20,559 --> 00:20:22,960
the implementation can be reconsidered

531
00:20:22,960 --> 00:20:25,360
thus in such an area considering

532
00:20:25,360 --> 00:20:28,000
ensemble methods are useful in such

533
00:20:28,000 --> 00:20:30,799
context as a slight gain in accuracy

534
00:20:30,799 --> 00:20:33,600
provides a realistic improvement for a

535
00:20:33,600 --> 00:20:36,240
full attack scenario finally to fully

536
00:20:36,240 --> 00:20:37,919
assess the benefits of the diversity

537
00:20:37,919 --> 00:20:40,960
types we perform an attack combining the

538
00:20:40,960 --> 00:20:44,159
type 1 type 2 and type 3 diversity

539
00:20:44,159 --> 00:20:47,120
so in other words we generate a pool of

540
00:20:47,120 --> 00:20:50,080
committee members and select a subset of

541
00:20:50,080 --> 00:20:52,799
models in order to only keep those with

542
00:20:52,799 --> 00:20:55,280
a minimum amount of our correlation and

543
00:20:55,280 --> 00:20:57,760
finally we promote interactions during

544
00:20:57,760 --> 00:20:59,840
the training process in order to enhance

545
00:20:59,840 --> 00:21:02,080
the error and correlation so depending

546
00:21:02,080 --> 00:21:03,919
on the loss function used for example

547
00:21:03,919 --> 00:21:05,600
the cross entropy of the ranking loss

548
00:21:05,600 --> 00:21:07,600
the type 3 diversity cannot be

549
00:21:07,600 --> 00:21:10,240
considered indeed as the ensembling loss

550
00:21:10,240 --> 00:21:11,760
is the only solution promoting

551
00:21:11,760 --> 00:21:13,679
interaction during the profiling phase

552
00:21:13,679 --> 00:21:15,760
the related ensemble model is the only

553
00:21:15,760 --> 00:21:17,840
solution considering all the diversity

554
00:21:17,840 --> 00:21:20,159
types so once again depending on the

555
00:21:20,159 --> 00:21:22,080
adversary's knowledge the european

556
00:21:22,080 --> 00:21:24,640
sagaya scheme considered the rsa

557
00:21:24,640 --> 00:21:27,200
implementations at run secures so to

558
00:21:27,200 --> 00:21:29,360
conclude the ensemble methods sound

559
00:21:29,360 --> 00:21:31,280
useful to perform such an attacks

560
00:21:31,280 --> 00:21:33,919
against pkc implementations because

561
00:21:33,919 --> 00:21:35,840
slight improvement in accuracy can be

562
00:21:35,840 --> 00:21:38,400
useful to reconsider the security of the

563
00:21:38,400 --> 00:21:39,679
implementation

564
00:21:39,679 --> 00:21:42,960
and even if the training time increases

565
00:21:42,960 --> 00:21:45,919
it stays negligible regarding the gain

566
00:21:45,919 --> 00:21:48,000
in reminding operation characterized by

567
00:21:48,000 --> 00:21:50,080
the complexity matrix through our new

568
00:21:50,080 --> 00:21:52,960
loss function called ensembling loss we

569
00:21:52,960 --> 00:21:54,960
promote interactions between the

570
00:21:54,960 --> 00:21:56,400
committee members during the training

571
00:21:56,400 --> 00:21:59,360
process in order to enhance diversity

572
00:21:59,360 --> 00:22:01,760
and reduce the art correlation as we

573
00:22:01,760 --> 00:22:03,679
assess the benefits of our proposition

574
00:22:03,679 --> 00:22:05,280
from a diversity point of view its

575
00:22:05,280 --> 00:22:07,280
application can be extended to a wide

576
00:22:07,280 --> 00:22:08,960
range of scenario as image

577
00:22:08,960 --> 00:22:11,440
classification throat detection or even

578
00:22:11,440 --> 00:22:13,360
targeted symmetric implementations

579
00:22:13,360 --> 00:22:15,520
however for the latter use case a

580
00:22:15,520 --> 00:22:17,360
trade-off should be found between

581
00:22:17,360 --> 00:22:19,440
computational issues and performance

582
00:22:19,440 --> 00:22:22,480
gains finally in our paper additional

583
00:22:22,480 --> 00:22:24,240
results are provided such that for

584
00:22:24,240 --> 00:22:26,720
example we validate our approach on a

585
00:22:26,720 --> 00:22:29,520
secure ecc implementation configuring

586
00:22:29,520 --> 00:22:31,120
another multi-class classification

587
00:22:31,120 --> 00:22:33,840
problem and we also evaluate the impact

588
00:22:33,840 --> 00:22:35,679
of the number of committee members on

589
00:22:35,679 --> 00:22:37,919
the diversity gain as well as the

590
00:22:37,919 --> 00:22:40,640
benefits of classical assembling metals

591
00:22:40,640 --> 00:22:43,039
all of our results can be obviously

592
00:22:43,039 --> 00:22:45,520
reproduced through a github repository

593
00:22:45,520 --> 00:22:47,600
and if you have any question do not

594
00:22:47,600 --> 00:22:50,159
hesitate to contact me hovering

595
00:22:50,159 --> 00:22:53,039
thank you very much

