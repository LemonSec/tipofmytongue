1
00:00:00,000 --> 00:00:01,979
welcome for the third time to this

2
00:00:01,979 --> 00:00:03,419
session on deep learning for Science and

3
00:00:03,419 --> 00:00:05,580
Analysis it's my pleasure to introduce

4
00:00:05,580 --> 00:00:07,980
our speaker Stephen from radbot

5
00:00:07,980 --> 00:00:10,679
University and this is Joint work with

6
00:00:10,679 --> 00:00:14,480
Guillermo perin and Lee Chao

7
00:00:14,580 --> 00:00:16,560
sure

8
00:00:16,560 --> 00:00:20,520
yeah okay hi everyone welcome to the

9
00:00:20,520 --> 00:00:23,100
start of the session after lunch so I

10
00:00:23,100 --> 00:00:25,500
will talk today about deep learning

11
00:00:25,500 --> 00:00:27,720
assistant template attack and like

12
00:00:27,720 --> 00:00:30,420
Ileana already said it's a it's a joint

13
00:00:30,420 --> 00:00:34,079
work with Lee Chavo and guillerme piran

14
00:00:34,079 --> 00:00:37,860
so what was the big idea of this work we

15
00:00:37,860 --> 00:00:40,980
all know that hopefully we believe that

16
00:00:40,980 --> 00:00:43,620
deep learning works well and it can help

17
00:00:43,620 --> 00:00:46,559
us make attacks more powerful and break

18
00:00:46,559 --> 00:00:49,020
targets more efficiently but still we

19
00:00:49,020 --> 00:00:52,440
had kind of question okay but we do not

20
00:00:52,440 --> 00:00:54,300
necessarily need to use deep learning

21
00:00:54,300 --> 00:00:57,239
only to attack so we could use deep

22
00:00:57,239 --> 00:01:00,239
learning Also let's say in a feature

23
00:01:00,239 --> 00:01:02,940
engineering phase so to make our

24
00:01:02,940 --> 00:01:06,180
selected features more powerful somehow

25
00:01:06,180 --> 00:01:09,600
better and then use such features to

26
00:01:09,600 --> 00:01:12,360
make the attack and then you could plug

27
00:01:12,360 --> 00:01:14,340
in also some kind of simpler technique

28
00:01:14,340 --> 00:01:16,939
and still hope for a good

29
00:01:16,939 --> 00:01:20,400
performance yeah since there was already

30
00:01:20,400 --> 00:01:23,100
a lot of talks I will skip the basics on

31
00:01:23,100 --> 00:01:26,100
site Channel analysis and I will

32
00:01:26,100 --> 00:01:27,900
immediately go into profiling side

33
00:01:27,900 --> 00:01:30,240
Channel analysis and I will just mention

34
00:01:30,240 --> 00:01:32,340
here

35
00:01:32,340 --> 00:01:34,380
the attack consists of two phases

36
00:01:34,380 --> 00:01:36,780
profiling phase and the attack phase in

37
00:01:36,780 --> 00:01:38,880
profiling phase we learn things about

38
00:01:38,880 --> 00:01:41,040
the leakage and we build the model

39
00:01:41,040 --> 00:01:42,780
whether it's template or machine

40
00:01:42,780 --> 00:01:45,720
learning model and we use that model to

41
00:01:45,720 --> 00:01:47,659
break the target

42
00:01:47,659 --> 00:01:51,360
and one option that is very very that's

43
00:01:51,360 --> 00:01:53,460
a traditional how to do this attack and

44
00:01:53,460 --> 00:01:57,360
yeah yesterday there was also award yeah

45
00:01:57,360 --> 00:01:59,820
was it yesterday two days ago two days

46
00:01:59,820 --> 00:02:02,759
ago there was a word for uh test of time

47
00:02:02,759 --> 00:02:05,340
so 20 years template attack

48
00:02:05,340 --> 00:02:08,340
is successfully breaking targets but in

49
00:02:08,340 --> 00:02:10,679
let's say last couple of years people

50
00:02:10,679 --> 00:02:12,840
move their attention from template

51
00:02:12,840 --> 00:02:14,879
attack and went to

52
00:02:14,879 --> 00:02:17,819
more powerful at least more fancier deep

53
00:02:17,819 --> 00:02:20,280
learning things to do the attacks and

54
00:02:20,280 --> 00:02:23,360
this is the Deep learning perspective

55
00:02:23,360 --> 00:02:26,099
and what what is the point what's the

56
00:02:26,099 --> 00:02:29,040
goal of everything when we are doing is

57
00:02:29,040 --> 00:02:30,959
well we have measurements we have traces

58
00:02:30,959 --> 00:02:32,700
and then either we say we take Road

59
00:02:32,700 --> 00:02:35,099
traces or we service take some interval

60
00:02:35,099 --> 00:02:37,500
but in the end

61
00:02:37,500 --> 00:02:39,780
mostly at least with public available

62
00:02:39,780 --> 00:02:42,000
targets there is only a couple of

63
00:02:42,000 --> 00:02:45,840
leaking places and everything else is

64
00:02:45,840 --> 00:02:47,340
somehow meant

65
00:02:47,340 --> 00:02:50,640
to be disregarded and of course with

66
00:02:50,640 --> 00:02:52,739
let's say template attack you want to

67
00:02:52,739 --> 00:02:55,620
select your features so that you have as

68
00:02:55,620 --> 00:02:57,660
good attack as possible with machine

69
00:02:57,660 --> 00:02:59,580
learning so simple machine learning

70
00:02:59,580 --> 00:03:02,160
attacks like seven years ago support

71
00:03:02,160 --> 00:03:04,379
Vector machines random Forest you kind

72
00:03:04,379 --> 00:03:06,780
of also try to do feature selection to

73
00:03:06,780 --> 00:03:08,519
make the attack more powerful and then

74
00:03:08,519 --> 00:03:10,019
we went to deep learning and said yeah

75
00:03:10,019 --> 00:03:12,599
the advantage of deep learning is you do

76
00:03:12,599 --> 00:03:15,480
not need to select features so you leave

77
00:03:15,480 --> 00:03:17,760
everything for deep learning to do and

78
00:03:17,760 --> 00:03:19,680
while I mean apparently this is good

79
00:03:19,680 --> 00:03:22,560
scenario because we constantly report it

80
00:03:22,560 --> 00:03:25,140
works the question is

81
00:03:25,140 --> 00:03:27,540
is it always the best solution because

82
00:03:27,540 --> 00:03:29,819
if we know something about what we are

83
00:03:29,819 --> 00:03:32,400
trying to do it does make sense to

84
00:03:32,400 --> 00:03:34,319
pre-process to do feature engineering to

85
00:03:34,319 --> 00:03:36,300
do whatever to make the attack more

86
00:03:36,300 --> 00:03:38,640
powerful I mean just think of it why

87
00:03:38,640 --> 00:03:40,980
would I make life for my neural network

88
00:03:40,980 --> 00:03:43,860
more difficult if I know something I can

89
00:03:43,860 --> 00:03:46,560
use that knowledge to then plug in into

90
00:03:46,560 --> 00:03:48,720
something and make the attack more

91
00:03:48,720 --> 00:03:51,599
powerful so the question is how to

92
00:03:51,599 --> 00:03:54,120
select relevant features

93
00:03:54,120 --> 00:03:56,940
and of course since with many techniques

94
00:03:56,940 --> 00:03:59,159
we needed to select features there are

95
00:03:59,159 --> 00:04:01,440
many techniques how to do that so I

96
00:04:01,440 --> 00:04:05,220
don't know based on correlation uh sosd

97
00:04:05,220 --> 00:04:08,420
sosd chi-square

98
00:04:08,420 --> 00:04:10,920
autoencoders more recently deep learning

99
00:04:10,920 --> 00:04:13,980
approach uh so there is quite a lot of

100
00:04:13,980 --> 00:04:15,060
options

101
00:04:15,060 --> 00:04:18,660
but there is also one more recent option

102
00:04:18,660 --> 00:04:21,060
that is based on the concept of

103
00:04:21,060 --> 00:04:23,759
similarity learning and

104
00:04:23,759 --> 00:04:25,919
what we call what is called triplet

105
00:04:25,919 --> 00:04:28,320
networks and then triplet loss and so on

106
00:04:28,320 --> 00:04:31,620
so on the concept is kind of simple

107
00:04:31,620 --> 00:04:34,560
yes you can see the pointer okay barely

108
00:04:34,560 --> 00:04:37,680
but hopefully you can see so what do we

109
00:04:37,680 --> 00:04:40,320
want to do so here is the example AI

110
00:04:40,320 --> 00:04:44,639
so that's called anchor then Pi be like

111
00:04:44,639 --> 00:04:47,520
positive for example and end I plus one

112
00:04:47,520 --> 00:04:51,120
or whatever n is negative example so the

113
00:04:51,120 --> 00:04:53,940
point is what we could do is we could

114
00:04:53,940 --> 00:04:58,139
try to make a system that minimizes the

115
00:04:58,139 --> 00:05:00,600
distances intra class so whatever

116
00:05:00,600 --> 00:05:03,000
belongs to let's say to a leakage will

117
00:05:03,000 --> 00:05:05,639
somehow come closer to boost the effect

118
00:05:05,639 --> 00:05:09,000
of leakage whatever does not come is not

119
00:05:09,000 --> 00:05:11,340
part of the leakage will simply move

120
00:05:11,340 --> 00:05:14,580
away so inter-class distance will

121
00:05:14,580 --> 00:05:17,220
somehow increase and by doing that

122
00:05:17,220 --> 00:05:19,380
intuitively you can think okay if

123
00:05:19,380 --> 00:05:21,000
relevant things are grouped together

124
00:05:21,000 --> 00:05:24,539
while irrelevant thing is discarded away

125
00:05:24,539 --> 00:05:27,000
if you plug in the relevant things it

126
00:05:27,000 --> 00:05:28,860
does make sense the attack could be more

127
00:05:28,860 --> 00:05:30,000
powerful

128
00:05:30,000 --> 00:05:33,000
so this is the whole concept how

129
00:05:33,000 --> 00:05:35,280
triplet networks can work and like I

130
00:05:35,280 --> 00:05:37,320
said there are three types of let's call

131
00:05:37,320 --> 00:05:40,160
them labels positive anchor and negative

132
00:05:40,160 --> 00:05:42,539
positive and could have the same label

133
00:05:42,539 --> 00:05:45,060
while negative can be any label that is

134
00:05:45,060 --> 00:05:47,639
not correct and that's why we call it

135
00:05:47,639 --> 00:05:49,680
triplet Network there are three deep

136
00:05:49,680 --> 00:05:52,979
learning networks each of them makes its

137
00:05:52,979 --> 00:05:55,979
own embedding you fit all that into a

138
00:05:55,979 --> 00:05:59,100
triplet loss function and what you're in

139
00:05:59,100 --> 00:06:02,940
the end obtain is somehow embedded

140
00:06:02,940 --> 00:06:05,520
we could call it oscillating space which

141
00:06:05,520 --> 00:06:09,240
is compressed but contains the most

142
00:06:09,240 --> 00:06:12,419
relevant information about the leakage

143
00:06:12,419 --> 00:06:15,900
one could ask okay but what is it that

144
00:06:15,900 --> 00:06:18,960
are those final embedding space what are

145
00:06:18,960 --> 00:06:21,479
those leakages because we could do that

146
00:06:21,479 --> 00:06:23,160
with other techniques

147
00:06:23,160 --> 00:06:25,919
it is non-linear combination of leakages

148
00:06:25,919 --> 00:06:28,560
so your triplet loss will build

149
00:06:28,560 --> 00:06:31,620
something that is non-linear which is a

150
00:06:31,620 --> 00:06:34,020
difference from many simpler techniques

151
00:06:34,020 --> 00:06:36,300
like PCA that will build a linear

152
00:06:36,300 --> 00:06:38,520
combination of features and one could

153
00:06:38,520 --> 00:06:40,680
also say well out encoders will build

154
00:06:40,680 --> 00:06:43,800
something that is non-linear true but

155
00:06:43,800 --> 00:06:47,220
out encoders are non-supervised while

156
00:06:47,220 --> 00:06:49,500
here we are in the supervised model so

157
00:06:49,500 --> 00:06:51,720
we also use the information of labels so

158
00:06:51,720 --> 00:06:54,419
in a way you can think to try to use all

159
00:06:54,419 --> 00:06:56,520
the information you have so you are

160
00:06:56,520 --> 00:06:58,979
building no linear representation where

161
00:06:58,979 --> 00:07:02,220
also label gives you extra information

162
00:07:02,220 --> 00:07:06,060
so yeah uh the example with two apples

163
00:07:06,060 --> 00:07:09,419
and and one uh a negative of banana is

164
00:07:09,419 --> 00:07:11,819
example of positive anchor negative

165
00:07:11,819 --> 00:07:13,620
setup

166
00:07:13,620 --> 00:07:16,020
and now some someone can ask okay but

167
00:07:16,020 --> 00:07:17,880
how do you do the learning what does it

168
00:07:17,880 --> 00:07:21,240
actually mean so basically you define

169
00:07:21,240 --> 00:07:24,360
your anchor and you define the distance

170
00:07:24,360 --> 00:07:26,639
up to the positive example

171
00:07:26,639 --> 00:07:29,759
and then what is in that margin in that

172
00:07:29,759 --> 00:07:32,580
uh euclidean distance space you can say

173
00:07:32,580 --> 00:07:36,360
Well it belongs to intra class what is

174
00:07:36,360 --> 00:07:38,520
outside of it it's

175
00:07:38,520 --> 00:07:41,340
inter class difference so here you can

176
00:07:41,340 --> 00:07:43,440
see where very easy example and this is

177
00:07:43,440 --> 00:07:46,380
called easy positive easy negative why

178
00:07:46,380 --> 00:07:48,539
well because the the distance from

179
00:07:48,539 --> 00:07:50,819
anchor to positive is very small the

180
00:07:50,819 --> 00:07:53,639
distance to negative is relatively large

181
00:07:53,639 --> 00:07:55,800
and then for neural network it's easy to

182
00:07:55,800 --> 00:07:57,599
say yeah this is the same this is

183
00:07:57,599 --> 00:07:58,919
different

184
00:07:58,919 --> 00:08:01,740
you can also have a different example

185
00:08:01,740 --> 00:08:05,759
where negative example is as close or

186
00:08:05,759 --> 00:08:08,400
even closer than positive example and

187
00:08:08,400 --> 00:08:10,979
this is called hard negatives or hard

188
00:08:10,979 --> 00:08:14,699
triplets and this when you have is make

189
00:08:14,699 --> 00:08:17,639
your algorithm uh

190
00:08:17,639 --> 00:08:19,979
struggle a lot to to converge to learn

191
00:08:19,979 --> 00:08:22,259
because the distance to negative example

192
00:08:22,259 --> 00:08:24,720
to what you want to avoid can be smaller

193
00:08:24,720 --> 00:08:26,340
than the distance to the positive

194
00:08:26,340 --> 00:08:28,860
example what you want to take

195
00:08:28,860 --> 00:08:31,620
so there is something in the between and

196
00:08:31,620 --> 00:08:34,080
this is called semi heart which is

197
00:08:34,080 --> 00:08:36,479
basically telling you that

198
00:08:36,479 --> 00:08:37,740
there is

199
00:08:37,740 --> 00:08:40,320
the distance till the till this kind of

200
00:08:40,320 --> 00:08:42,360
example is larger than to the easy

201
00:08:42,360 --> 00:08:46,260
example but it's smaller than to make to

202
00:08:46,260 --> 00:08:49,140
negative examples that are easy and here

203
00:08:49,140 --> 00:08:51,660
you govern how far do you do this

204
00:08:51,660 --> 00:08:54,600
differentiation by adding a parameter

205
00:08:54,600 --> 00:08:57,120
called margin so the bigger the margin

206
00:08:57,120 --> 00:08:59,279
the bigger is your centroid the bigger

207
00:08:59,279 --> 00:09:03,660
centroid your approach becomes easier to

208
00:09:03,660 --> 00:09:06,240
to do classification but it's also more

209
00:09:06,240 --> 00:09:07,800
difficult to recognize the differences

210
00:09:07,800 --> 00:09:09,720
between examples

211
00:09:09,720 --> 00:09:13,440
of course uh you could make

212
00:09:13,440 --> 00:09:16,260
something more because a side channel

213
00:09:16,260 --> 00:09:18,600
will offer you a bit more information

214
00:09:18,600 --> 00:09:20,700
that we maybe don't have in some other

215
00:09:20,700 --> 00:09:22,680
domains well triplet networks are

216
00:09:22,680 --> 00:09:24,260
commonly used like

217
00:09:24,260 --> 00:09:27,600
NLP so natural language processing why

218
00:09:27,600 --> 00:09:30,120
because think of the example

219
00:09:30,120 --> 00:09:32,220
you want to say what name is more

220
00:09:32,220 --> 00:09:34,500
distant from some other name Alice Bob

221
00:09:34,500 --> 00:09:35,519
Eve

222
00:09:35,519 --> 00:09:38,820
it's very hard to say if Ali's name is

223
00:09:38,820 --> 00:09:42,300
closer to Bob or it's closer to Eve

224
00:09:42,300 --> 00:09:44,100
objectively of course we can say

225
00:09:44,100 --> 00:09:45,899
whatever we want what's closer what's

226
00:09:45,899 --> 00:09:48,720
further but inside Channel we have the

227
00:09:48,720 --> 00:09:51,180
the nice side that when we talk about

228
00:09:51,180 --> 00:09:53,279
labels depending on the leakage model

229
00:09:53,279 --> 00:09:55,920
some things are closer than some other

230
00:09:55,920 --> 00:09:57,720
things just consider the Hemingway

231
00:09:57,720 --> 00:10:00,540
leakage model and there the the

232
00:10:00,540 --> 00:10:02,100
consumption is proportional to the

233
00:10:02,100 --> 00:10:04,620
Heming rate so one would say well to

234
00:10:04,620 --> 00:10:07,860
Heming rate one Hemingway two is closer

235
00:10:07,860 --> 00:10:10,500
than Hemingway seven so you you we can

236
00:10:10,500 --> 00:10:12,360
use that extra information about the

237
00:10:12,360 --> 00:10:14,720
labels to build what we actually called

238
00:10:14,720 --> 00:10:16,980
hybrid distance

239
00:10:16,980 --> 00:10:20,399
and this is represented here with uh

240
00:10:20,399 --> 00:10:22,620
with uh with this formula

241
00:10:22,620 --> 00:10:25,920
okay so how does the whole attack work

242
00:10:25,920 --> 00:10:28,019
well you have your measurements your

243
00:10:28,019 --> 00:10:30,300
leakage traces you put them in triplet

244
00:10:30,300 --> 00:10:32,760
Network you obtain different embeddings

245
00:10:32,760 --> 00:10:35,519
from those embeddings you plug those

246
00:10:35,519 --> 00:10:38,399
embeddings as input to your attack

247
00:10:38,399 --> 00:10:41,220
so in a way it's very simple and attack

248
00:10:41,220 --> 00:10:44,160
in general can be whatever you want and

249
00:10:44,160 --> 00:10:47,399
this is let's say pre-processing phase

250
00:10:47,399 --> 00:10:51,480
so uh the experiment wise we did on

251
00:10:51,480 --> 00:10:53,160
yeah different neural networks different

252
00:10:53,160 --> 00:10:56,100
data sets so common suspects ask at

253
00:10:56,100 --> 00:10:59,579
fixed key ask at variable key aeshd data

254
00:10:59,579 --> 00:11:01,980
set Hemingway Heming distance ideal

255
00:11:01,980 --> 00:11:04,920
leakage models for neural networks

256
00:11:04,920 --> 00:11:07,560
we did quite a lot of experiments

257
00:11:07,560 --> 00:11:09,480
very small neural networks work fine

258
00:11:09,480 --> 00:11:11,880
which is always nice easy to tune you

259
00:11:11,880 --> 00:11:14,579
also don't need to tune them a lot and

260
00:11:14,579 --> 00:11:17,579
for the attack itself we used template

261
00:11:17,579 --> 00:11:21,060
attack okay so we wanted to say if if we

262
00:11:21,060 --> 00:11:23,579
do nice pre-processing

263
00:11:23,579 --> 00:11:25,620
and then if you plug in template attack

264
00:11:25,620 --> 00:11:28,200
can that compare with state-of-the-art

265
00:11:28,200 --> 00:11:31,260
deep learning attacks direct ones

266
00:11:31,260 --> 00:11:34,500
so result wise here you can see a bunch

267
00:11:34,500 --> 00:11:38,040
of numbers for different data sets uh

268
00:11:38,040 --> 00:11:41,820
yeah uh mostly we were quite happy with

269
00:11:41,820 --> 00:11:44,100
the results because for for most of the

270
00:11:44,100 --> 00:11:46,680
scenarios you can see that template

271
00:11:46,680 --> 00:11:48,660
attack actually managed to get good

272
00:11:48,660 --> 00:11:51,120
performance of course there are cases

273
00:11:51,120 --> 00:11:53,760
where other works actually don't know

274
00:11:53,760 --> 00:11:56,339
which one is this worked better

275
00:11:56,339 --> 00:11:59,700
but all in all the numbers looked very

276
00:11:59,700 --> 00:12:03,000
nice so template did manage to to use

277
00:12:03,000 --> 00:12:05,519
the information from embedded space to

278
00:12:05,519 --> 00:12:08,100
to break the targets more efficiently

279
00:12:08,100 --> 00:12:12,120
than I don't know methodology paper or

280
00:12:12,120 --> 00:12:13,740
so and so on

281
00:12:13,740 --> 00:12:17,459
and one could also ask yes but what

282
00:12:17,459 --> 00:12:19,620
happens when you add more noise the

283
00:12:19,620 --> 00:12:22,260
synchronization to data set whatever so

284
00:12:22,260 --> 00:12:25,079
we we tested also between that here the

285
00:12:25,079 --> 00:12:27,420
comparison was a bit more difficult

286
00:12:27,420 --> 00:12:30,060
because of course not every prior work

287
00:12:30,060 --> 00:12:32,040
had all the setups that we wanted to

288
00:12:32,040 --> 00:12:36,000
consider but what we did see is that the

289
00:12:36,000 --> 00:12:38,640
same triplet Network used before for

290
00:12:38,640 --> 00:12:41,399
embedding still works very nicely for

291
00:12:41,399 --> 00:12:44,279
new attack when you also add some kind

292
00:12:44,279 --> 00:12:47,459
of desynchronization or something

293
00:12:47,459 --> 00:12:49,800
and of course

294
00:12:49,800 --> 00:12:52,800
one could ask why did you use the loss

295
00:12:52,800 --> 00:12:55,260
function that we used and because there

296
00:12:55,260 --> 00:12:57,180
are many loss functions and these are

297
00:12:57,180 --> 00:12:58,740
not the classical loss functions like

298
00:12:58,740 --> 00:13:00,959
categorical cross entropy or whatever we

299
00:13:00,959 --> 00:13:03,480
know from before there are specific loss

300
00:13:03,480 --> 00:13:05,940
functions just for similarity learning

301
00:13:05,940 --> 00:13:07,320
so

302
00:13:07,320 --> 00:13:09,839
those are also a bunch of those we also

303
00:13:09,839 --> 00:13:11,220
tested

304
00:13:11,220 --> 00:13:13,380
most common examples like contrastive

305
00:13:13,380 --> 00:13:16,680
loss lift structure uh pinball hard semi

306
00:13:16,680 --> 00:13:18,660
hard and so on so on but the one the

307
00:13:18,660 --> 00:13:20,940
hybrid one actually worked very nicely

308
00:13:20,940 --> 00:13:22,980
because it included information about

309
00:13:22,980 --> 00:13:26,399
the labels and there are many hyper

310
00:13:26,399 --> 00:13:29,880
parameters extra ones to consider so if

311
00:13:29,880 --> 00:13:33,420
you remember our loss function from here

312
00:13:33,420 --> 00:13:35,220
we needed to

313
00:13:35,220 --> 00:13:39,060
add new parameter Alpha so what happens

314
00:13:39,060 --> 00:13:42,060
when you add Alpha how how difficult is

315
00:13:42,060 --> 00:13:46,440
to to tweak that hyper parameter and

316
00:13:46,440 --> 00:13:49,019
how how important is the embedding size

317
00:13:49,019 --> 00:13:51,180
how important how long do you need to

318
00:13:51,180 --> 00:13:53,820
train how big training set needs to be

319
00:13:53,820 --> 00:13:56,279
and so on so on so classical questions

320
00:13:56,279 --> 00:13:59,820
so some kind of of study on the

321
00:13:59,820 --> 00:14:01,980
influence of parameters hyper parameters

322
00:14:01,980 --> 00:14:05,040
the most important message so there are

323
00:14:05,040 --> 00:14:07,500
different graphs showing here so loss

324
00:14:07,500 --> 00:14:09,720
function you can so Alpha hyper

325
00:14:09,720 --> 00:14:11,339
parameter you can see

326
00:14:11,339 --> 00:14:13,980
small values are rather stable

327
00:14:13,980 --> 00:14:17,100
embedding size this is in my opinion the

328
00:14:17,100 --> 00:14:19,260
most interesting one you can see that

329
00:14:19,260 --> 00:14:21,720
the small embedded size work very nicely

330
00:14:21,720 --> 00:14:24,180
and why is this interesting well if you

331
00:14:24,180 --> 00:14:26,279
have small embedding size that means

332
00:14:26,279 --> 00:14:28,920
your covariance matrices with template

333
00:14:28,920 --> 00:14:31,980
will also not have strong requirements

334
00:14:31,980 --> 00:14:34,139
which will allow you in many many

335
00:14:34,139 --> 00:14:36,000
situations to build your template

336
00:14:36,000 --> 00:14:37,620
attacks so you will not get singular

337
00:14:37,620 --> 00:14:40,620
singular Matrix and so on so on

338
00:14:40,620 --> 00:14:43,199
triplet margin so how big is the

339
00:14:43,199 --> 00:14:45,839
difference between a positive negative

340
00:14:45,839 --> 00:14:48,660
again we found this to be quite

341
00:14:48,660 --> 00:14:52,079
reasonably easy to tweak so whatever we

342
00:14:52,079 --> 00:14:54,480
tried unless we tried very stupid values

343
00:14:54,480 --> 00:14:56,639
worked good

344
00:14:56,639 --> 00:14:59,579
and also very interesting thing

345
00:14:59,579 --> 00:15:01,920
to to train something like this you need

346
00:15:01,920 --> 00:15:03,959
very little time so

347
00:15:03,959 --> 00:15:07,260
a couple of epochs is enough to train

348
00:15:07,260 --> 00:15:12,060
uh your model to do embedding extraction

349
00:15:12,060 --> 00:15:15,300
and training size again

350
00:15:15,300 --> 00:15:18,899
reasonable training sizes work stable so

351
00:15:18,899 --> 00:15:21,839
unless you really feed extremely small

352
00:15:21,839 --> 00:15:25,139
set that is not enough to generalize you

353
00:15:25,139 --> 00:15:28,260
will get good results if you feed a big

354
00:15:28,260 --> 00:15:30,420
data set of course it works nice but it

355
00:15:30,420 --> 00:15:33,300
takes more time and so on so on and to

356
00:15:33,300 --> 00:15:34,380
conclude

357
00:15:34,380 --> 00:15:36,360
is well

358
00:15:36,360 --> 00:15:39,540
it seems cool because indeed we can be

359
00:15:39,540 --> 00:15:41,160
found that you can

360
00:15:41,160 --> 00:15:43,920
make embedding that works for template

361
00:15:43,920 --> 00:15:47,100
attack which was not clear at the

362
00:15:47,100 --> 00:15:49,260
beginning can you really combine

363
00:15:49,260 --> 00:15:51,420
features in such a non-linear way that

364
00:15:51,420 --> 00:15:53,940
they will still make sense for for

365
00:15:53,940 --> 00:15:55,740
template attack to use

366
00:15:55,740 --> 00:15:58,800
and in a way with minimal computation

367
00:15:58,800 --> 00:16:01,980
efforts we we were quite lucky to see

368
00:16:01,980 --> 00:16:05,760
that we do match state-of-the-art deep

369
00:16:05,760 --> 00:16:07,920
learning attacks so really attack that

370
00:16:07,920 --> 00:16:10,260
attacks on the raw features which is

371
00:16:10,260 --> 00:16:12,300
also telling us that this non-linear

372
00:16:12,300 --> 00:16:15,180
embeddings do contain a lot more

373
00:16:15,180 --> 00:16:18,000
information than just judging by the not

374
00:16:18,000 --> 00:16:20,220
by the size of embedding because for

375
00:16:20,220 --> 00:16:23,100
instance size of embedding was 30 40 50.

376
00:16:23,100 --> 00:16:25,980
so very small when you compare it with

377
00:16:25,980 --> 00:16:29,339
raw features of thousands or to 700 for

378
00:16:29,339 --> 00:16:31,920
us get fixed and so on so on

379
00:16:31,920 --> 00:16:34,920
uh yeah the the evaluation of critical

380
00:16:34,920 --> 00:16:37,139
hyper parameters showed stability which

381
00:16:37,139 --> 00:16:39,600
is always nice because even if you have

382
00:16:39,600 --> 00:16:42,300
good solution but becomes very difficult

383
00:16:42,300 --> 00:16:44,399
to tweak very difficult to reproduce the

384
00:16:44,399 --> 00:16:47,279
results then it's always problematic so

385
00:16:47,279 --> 00:16:49,740
we found at least on the data sets we

386
00:16:49,740 --> 00:16:53,519
tested to be very easy to to tune the

387
00:16:53,519 --> 00:16:57,000
hyper parameters that being said yeah

388
00:16:57,000 --> 00:17:00,060
paper is everywhere uh available on

389
00:17:00,060 --> 00:17:03,779
chess page there is on our git you can

390
00:17:03,779 --> 00:17:05,220
find the code

391
00:17:05,220 --> 00:17:09,179
uh who was at the tutorial Sunday I also

392
00:17:09,179 --> 00:17:11,760
gave the quote for this I believe uh

393
00:17:11,760 --> 00:17:13,380
some people tried it already they told

394
00:17:13,380 --> 00:17:15,720
me it works even works so that's nice

395
00:17:15,720 --> 00:17:19,679
the and that being said yeah uh quick if

396
00:17:19,679 --> 00:17:21,419
you have any questions I will do my best

397
00:17:21,419 --> 00:17:23,839
to answer

398
00:17:29,940 --> 00:17:33,480
I think we have a question there I will

399
00:17:33,480 --> 00:17:36,440
give you my microphone

400
00:17:39,539 --> 00:17:43,620
hello hi thanks for the talk uh I just

401
00:17:43,620 --> 00:17:45,299
wanted to ask if you can comment a bit

402
00:17:45,299 --> 00:17:48,720
more on the difference between LDA and

403
00:17:48,720 --> 00:17:50,280
your method for selecting features

404
00:17:50,280 --> 00:17:53,760
because LDA is also minimizing the intra

405
00:17:53,760 --> 00:17:55,320
class difference and maximizing the

406
00:17:55,320 --> 00:17:56,760
inter-class device yes so that's

407
00:17:56,760 --> 00:17:58,080
precisely what you're algorithm is doing

408
00:17:58,080 --> 00:18:01,860
as far as I understood yes but it's yeah

409
00:18:01,860 --> 00:18:05,220
it's in the way how to to do it so with

410
00:18:05,220 --> 00:18:06,780
uh with LDA

411
00:18:06,780 --> 00:18:10,919
I mean l in in ldm means linear so you

412
00:18:10,919 --> 00:18:13,980
do it in a linear fashion here

413
00:18:13,980 --> 00:18:16,500
you do it in non-linear fashion and we

414
00:18:16,500 --> 00:18:19,020
assume anything that can be approximated

415
00:18:19,020 --> 00:18:20,880
with with a neural network function and

416
00:18:20,880 --> 00:18:22,500
then from the universal approximation

417
00:18:22,500 --> 00:18:24,720
theorem is seven we could approximate in

418
00:18:24,720 --> 00:18:27,140
theory whatever we want so you can build

419
00:18:27,140 --> 00:18:30,360
extremely complicated function that Maps

420
00:18:30,360 --> 00:18:32,940
your original features into embedded

421
00:18:32,940 --> 00:18:35,700
space and even more than that you use

422
00:18:35,700 --> 00:18:38,280
the information about the label because

423
00:18:38,280 --> 00:18:40,740
one could say out encoder also does the

424
00:18:40,740 --> 00:18:42,840
non-linear part but once you add the

425
00:18:42,840 --> 00:18:44,700
information about the label you can

426
00:18:44,700 --> 00:18:46,980
actually really tweak the differences

427
00:18:46,980 --> 00:18:49,080
like I said Heming rate

428
00:18:49,080 --> 00:18:52,620
2 and Hemingway 3 are potentially more

429
00:18:52,620 --> 00:18:54,900
similar than Hemingway zero and

430
00:18:54,900 --> 00:18:57,840
Hemingway eight at least from the from

431
00:18:57,840 --> 00:19:00,179
the Hemingway leakage model from the

432
00:19:00,179 --> 00:19:02,700
consumption proportional leakage so once

433
00:19:02,700 --> 00:19:04,520
you the more information you add

434
00:19:04,520 --> 00:19:06,840
potentially the better attack you can

435
00:19:06,840 --> 00:19:09,120
have and since its profiling attack and

436
00:19:09,120 --> 00:19:10,980
we always say well in profiling phase we

437
00:19:10,980 --> 00:19:13,380
know whatever we want to know there is

438
00:19:13,380 --> 00:19:15,240
no reason why not to use that kind of

439
00:19:15,240 --> 00:19:18,840
information so LDA is always a good

440
00:19:18,840 --> 00:19:21,480
choice it's much simpler to do than than

441
00:19:21,480 --> 00:19:24,120
this thing but the way how you embed the

442
00:19:24,120 --> 00:19:26,820
space it's very much different

443
00:19:26,820 --> 00:19:29,520
so what sort of extra information you

444
00:19:29,520 --> 00:19:31,320
can add for example to your method done

445
00:19:31,320 --> 00:19:33,120
to LD this is the part that is not very

446
00:19:33,120 --> 00:19:34,620
clear to me oh

447
00:19:34,620 --> 00:19:37,500
but I mean uh

448
00:19:37,500 --> 00:19:40,140
LTA will build you a linear

449
00:19:40,140 --> 00:19:43,200
combination of features yeah so I don't

450
00:19:43,200 --> 00:19:45,240
know feature one plus feature two or

451
00:19:45,240 --> 00:19:47,160
whatever here

452
00:19:47,160 --> 00:19:50,400
it can be whatever you want so uh the

453
00:19:50,400 --> 00:19:52,200
non-linear combination of features for

454
00:19:52,200 --> 00:19:55,080
instance in a way can also com be

455
00:19:55,080 --> 00:19:57,299
combination of a mask already with with

456
00:19:57,299 --> 00:20:00,059
the intermediate value so already the

457
00:20:00,059 --> 00:20:03,960
the embedding can have the must part

458
00:20:03,960 --> 00:20:06,419
I'm not saying that that this of course

459
00:20:06,419 --> 00:20:09,240
happens but you give the opportunity for

460
00:20:09,240 --> 00:20:11,460
neural network to allow that so you can

461
00:20:11,460 --> 00:20:14,340
already have the data set unmasked

462
00:20:14,340 --> 00:20:18,179
and let's say aligned even before you

463
00:20:18,179 --> 00:20:21,140
start with the attack part

464
00:20:21,840 --> 00:20:23,460
so you have time for a short question

465
00:20:23,460 --> 00:20:26,760
and a short answer okay I will try uh

466
00:20:26,760 --> 00:20:28,860
then let's keep this question short

467
00:20:28,860 --> 00:20:31,320
um you analyze the influence of

468
00:20:31,320 --> 00:20:33,900
different hyper parameter choices yes

469
00:20:33,900 --> 00:20:35,880
how did you optimize the hyper

470
00:20:35,880 --> 00:20:37,980
parameters was this done manually or did

471
00:20:37,980 --> 00:20:39,660
you use some Optimizer here we did it

472
00:20:39,660 --> 00:20:42,020
manually so because

473
00:20:42,020 --> 00:20:44,580
some of these hyper parameters for more

474
00:20:44,580 --> 00:20:46,799
side Channel Side you know like training

475
00:20:46,799 --> 00:20:48,419
set size this is something that one

476
00:20:48,419 --> 00:20:51,480
commonly plays manually and from the

477
00:20:51,480 --> 00:20:53,700
deep learning side you had the parameter

478
00:20:53,700 --> 00:20:56,820
hyper parameter Alpha which we said well

479
00:20:56,820 --> 00:20:59,280
let's just do a grid search you know

480
00:20:59,280 --> 00:21:01,620
zero one zero two zero three and then

481
00:21:01,620 --> 00:21:03,780
when we are somewhere interesting then

482
00:21:03,780 --> 00:21:06,000
let's try values between zero one and

483
00:21:06,000 --> 00:21:08,280
zero two and things like that and

484
00:21:08,280 --> 00:21:11,700
embedding size we also uh well embedding

485
00:21:11,700 --> 00:21:13,559
size was actually quite easy because for

486
00:21:13,559 --> 00:21:16,500
instance Ascot fix key has 700 features

487
00:21:16,500 --> 00:21:19,260
it does not in the in the interval it

488
00:21:19,260 --> 00:21:21,000
does not make sense to say well I have

489
00:21:21,000 --> 00:21:23,820
my small latent space that is 699

490
00:21:23,820 --> 00:21:26,100
features then I can just plug in the

491
00:21:26,100 --> 00:21:28,380
original one so we said it makes sense

492
00:21:28,380 --> 00:21:31,080
to start for smaller and then see how it

493
00:21:31,080 --> 00:21:33,240
progresses so we started with extremely

494
00:21:33,240 --> 00:21:35,280
small embedding space and at the moment

495
00:21:35,280 --> 00:21:37,919
when the results started to go verse

496
00:21:37,919 --> 00:21:40,679
will set we will stop here of course it

497
00:21:40,679 --> 00:21:43,080
does not mean that

498
00:21:43,080 --> 00:21:45,600
going for some strange numbers somewhere

499
00:21:45,600 --> 00:21:48,419
else would not work but we also want to

500
00:21:48,419 --> 00:21:50,520
help our template attack to to be as

501
00:21:50,520 --> 00:21:52,200
efficient as possible and for that

502
00:21:52,200 --> 00:21:54,720
having less features is good otherwise

503
00:21:54,720 --> 00:21:56,400
we can have problems with with

504
00:21:56,400 --> 00:21:57,780
covariance

505
00:21:57,780 --> 00:22:02,000
yeah did you try going back to uh yeah

506
00:22:02,000 --> 00:22:05,640
did I try what embedding size one did

507
00:22:05,640 --> 00:22:07,500
you try that one

508
00:22:07,500 --> 00:22:10,559
yes yes we tried that I don't think it's

509
00:22:10,559 --> 00:22:12,480
in the paper because that did not work

510
00:22:12,480 --> 00:22:15,480
well as far as I remember but this would

511
00:22:15,480 --> 00:22:17,280
be definitely for offline because I

512
00:22:17,280 --> 00:22:18,960
would need to check

513
00:22:18,960 --> 00:22:22,100
thanks Dearborn again

514
00:22:26,460 --> 00:22:31,400
our next speaker is online

515
00:22:32,240 --> 00:22:37,280
yes I'm principal online

516
00:22:41,039 --> 00:22:43,559
hey uh do you want to give the

517
00:22:43,559 --> 00:22:45,059
presentation through Zoom or should we

518
00:22:45,059 --> 00:22:46,440
just play the video

519
00:22:46,440 --> 00:22:49,500
I think it just plays a video it's a bit

520
00:22:49,500 --> 00:22:53,580
safer in terms of connection quality

521
00:22:53,580 --> 00:22:56,059
okay

522
00:23:12,919 --> 00:23:16,080
so the topic of my talk today is braking

523
00:23:16,080 --> 00:23:18,240
Mast implementations of Clyde by means

524
00:23:18,240 --> 00:23:20,220
of site Channel analysis and this is

525
00:23:20,220 --> 00:23:22,320
Joint work with Friedrich alaus and

526
00:23:22,320 --> 00:23:25,080
Donna Schindler

527
00:23:25,080 --> 00:23:26,820
um to give you a short outline of my

528
00:23:26,820 --> 00:23:28,500
talk I will first give a brief

529
00:23:28,500 --> 00:23:30,299
introduction to side Channel analysis

530
00:23:30,299 --> 00:23:31,620
and I will talk about the chess

531
00:23:31,620 --> 00:23:33,419
challenge 2020 which was a power

532
00:23:33,419 --> 00:23:36,720
analysis task I will describe a

533
00:23:36,720 --> 00:23:39,179
blueprint of a solution

534
00:23:39,179 --> 00:23:42,900
um that outlines these the main steps

535
00:23:42,900 --> 00:23:45,840
that we take to to solve the chess

536
00:23:45,840 --> 00:23:49,559
challenge 2020 software challenges I

537
00:23:49,559 --> 00:23:51,480
will discuss some minor and some major

538
00:23:51,480 --> 00:23:55,080
problems that we had to overcome and the

539
00:23:55,080 --> 00:23:57,000
most significant problems that we had to

540
00:23:57,000 --> 00:23:59,220
overcome was a convergence problem with

541
00:23:59,220 --> 00:24:01,860
the Deep learning model that we use to

542
00:24:01,860 --> 00:24:05,419
break this chess challenge 2020.

543
00:24:05,419 --> 00:24:07,640
and um

544
00:24:07,640 --> 00:24:11,340
this we invented a trick called the

545
00:24:11,340 --> 00:24:14,179
scatter shot encoding that helps us

546
00:24:14,179 --> 00:24:17,700
resolve this convergence problem

547
00:24:17,700 --> 00:24:21,059
and in the end I will first Analyze This

548
00:24:21,059 --> 00:24:24,320
by using the scatter shot encoding on a

549
00:24:24,320 --> 00:24:26,159
synthetic problem

550
00:24:26,159 --> 00:24:28,640
and we will

551
00:24:28,640 --> 00:24:33,360
supply a comparison to this to of our

552
00:24:33,360 --> 00:24:35,460
results to results obtained using the

553
00:24:35,460 --> 00:24:37,380
stochastic approach when used on the

554
00:24:37,380 --> 00:24:41,940
same task of the chess challenge 2020.

555
00:24:41,940 --> 00:24:43,280
so

556
00:24:43,280 --> 00:24:45,659
mathematical crypto analysis tends to be

557
00:24:45,659 --> 00:24:47,880
difficult for unbeaconed versions of

558
00:24:47,880 --> 00:24:50,820
modern ciphers so it's much easier to

559
00:24:50,820 --> 00:24:53,460
break the implementation one way to do

560
00:24:53,460 --> 00:24:55,500
this is by side Channel attacks this

561
00:24:55,500 --> 00:24:57,179
means using physical side effects of

562
00:24:57,179 --> 00:24:58,740
computation to gain additional

563
00:24:58,740 --> 00:25:00,000
information

564
00:25:00,000 --> 00:25:03,480
and about the secret that is being

565
00:25:03,480 --> 00:25:06,000
processed by an implementation and a

566
00:25:06,000 --> 00:25:07,620
site Channel attacks are a very

567
00:25:07,620 --> 00:25:09,600
practical threat when the adversary can

568
00:25:09,600 --> 00:25:11,400
do the required measurements and

569
00:25:11,400 --> 00:25:13,200
physical side effects of computation

570
00:25:13,200 --> 00:25:15,780
might be things like power usage or

571
00:25:15,780 --> 00:25:18,179
electromagnetic emanations or acoustic

572
00:25:18,179 --> 00:25:19,559
imaginations

573
00:25:19,559 --> 00:25:22,020
can we do anything about it yes we can

574
00:25:22,020 --> 00:25:26,159
do things about it we can try to in in

575
00:25:26,159 --> 00:25:28,440
various ways we can try to make it

576
00:25:28,440 --> 00:25:32,120
impossible for the adversary to try to

577
00:25:32,120 --> 00:25:36,600
either perform the measurements uh on

578
00:25:36,600 --> 00:25:38,580
the physical side effects of computation

579
00:25:38,580 --> 00:25:41,700
that they want to exploit or to exploit

580
00:25:41,700 --> 00:25:43,919
the

581
00:25:43,919 --> 00:25:46,919
um to to get useful measurements

582
00:25:46,919 --> 00:25:49,260
and one of surveys we can prevent the

583
00:25:49,260 --> 00:25:52,880
adversary from getting the from from

584
00:25:52,880 --> 00:25:55,740
retrieving information about our secrets

585
00:25:55,740 --> 00:25:57,659
from the measurements that they can make

586
00:25:57,659 --> 00:26:01,200
is to employ masking and

587
00:26:01,200 --> 00:26:04,140
um masking has a fairly good theoretical

588
00:26:04,140 --> 00:26:07,020
justification but tends to also be

589
00:26:07,020 --> 00:26:08,880
relatively expensive therefore there's

590
00:26:08,880 --> 00:26:11,340
interest in creating masking friendly

591
00:26:11,340 --> 00:26:13,559
ciphers and the Clyde Cipher this was

592
00:26:13,559 --> 00:26:16,020
which was the object of the chess 2020

593
00:26:16,020 --> 00:26:18,299
challenge is one of those masking

594
00:26:18,299 --> 00:26:20,279
friendly ciphers

595
00:26:20,279 --> 00:26:22,020
so to give you a basic idea about

596
00:26:22,020 --> 00:26:23,220
masking

597
00:26:23,220 --> 00:26:26,460
um usually a side Channel adversary will

598
00:26:26,460 --> 00:26:29,400
have to use multiple traces multiple

599
00:26:29,400 --> 00:26:32,059
measurements in order to extract

600
00:26:32,059 --> 00:26:35,340
enough information about the secret that

601
00:26:35,340 --> 00:26:37,740
they want to extract to gain a

602
00:26:37,740 --> 00:26:39,740
significant advantage

603
00:26:39,740 --> 00:26:44,159
and the idea here is that the secret

604
00:26:44,159 --> 00:26:46,200
that is being processed by a

605
00:26:46,200 --> 00:26:48,299
cryptographic implementation remains

606
00:26:48,299 --> 00:26:52,020
constant across across executions

607
00:26:52,020 --> 00:26:54,419
and the only thing that changes is the

608
00:26:54,419 --> 00:26:56,520
data that is the other data that is

609
00:26:56,520 --> 00:26:57,960
being processed by the implementation

610
00:26:57,960 --> 00:26:59,700
and the noise that's the implementation

611
00:26:59,700 --> 00:27:03,240
generates and therefore the secret might

612
00:27:03,240 --> 00:27:05,360
generate a

613
00:27:05,360 --> 00:27:08,400
consistent signals at the adversary can

614
00:27:08,400 --> 00:27:11,760
try to exploit to extract that signal

615
00:27:11,760 --> 00:27:13,679
that secret and the basic idea of

616
00:27:13,679 --> 00:27:15,919
masking is to

617
00:27:15,919 --> 00:27:18,240
structure the implementation in such a

618
00:27:18,240 --> 00:27:20,159
way that this assumption that the secret

619
00:27:20,159 --> 00:27:22,919
remains and remains constant across

620
00:27:22,919 --> 00:27:25,400
implementations across

621
00:27:25,400 --> 00:27:28,320
executions is from so we break up the

622
00:27:28,320 --> 00:27:31,559
secret key and the data into shares in a

623
00:27:31,559 --> 00:27:34,020
SQL sharing scheme one runs the

624
00:27:34,020 --> 00:27:35,360
execution

625
00:27:35,360 --> 00:27:39,320
of The Primitives that one is trying to

626
00:27:39,320 --> 00:27:43,200
run on these shares instead of on the

627
00:27:43,200 --> 00:27:46,140
original data so one has to rewrite the

628
00:27:46,140 --> 00:27:49,200
um the um for instance the encryption

629
00:27:49,200 --> 00:27:50,400
operation

630
00:27:50,400 --> 00:27:53,100
if it is if if The Primitives that we

631
00:27:53,100 --> 00:27:55,200
are trying to protect and encrypts

632
00:27:55,200 --> 00:27:58,799
something and we refresh the shares

633
00:27:58,799 --> 00:28:01,260
across executions or even within one

634
00:28:01,260 --> 00:28:05,279
execution of the scheme and in this way

635
00:28:05,279 --> 00:28:08,159
um we prevent the adversary from seeing

636
00:28:08,159 --> 00:28:09,360
the same

637
00:28:09,360 --> 00:28:12,360
um the same sensitive values when they

638
00:28:12,360 --> 00:28:14,279
observe different executions of our

639
00:28:14,279 --> 00:28:16,880
primitive

640
00:28:17,279 --> 00:28:21,600
so in the chess challenge 2020 we had to

641
00:28:21,600 --> 00:28:23,340
break mask implementations of the

642
00:28:23,340 --> 00:28:25,500
masking friendly Clyde Cipher the power

643
00:28:25,500 --> 00:28:26,880
analysis

644
00:28:26,880 --> 00:28:28,980
Clyde is a tweakable lightweight block

645
00:28:28,980 --> 00:28:31,860
Cipher with a 128 bit block key and

646
00:28:31,860 --> 00:28:33,240
tweak size there were seven different

647
00:28:33,240 --> 00:28:35,039
targets for software and three Hardware

648
00:28:35,039 --> 00:28:37,559
targets all of them were masked

649
00:28:37,559 --> 00:28:39,539
implementations the software targets had

650
00:28:39,539 --> 00:28:41,640
three four six and eight-fold masking

651
00:28:41,640 --> 00:28:44,340
only the software targets were solved

652
00:28:44,340 --> 00:28:46,380
and the power traces for the software

653
00:28:46,380 --> 00:28:48,299
targets were captured at fairly high

654
00:28:48,299 --> 00:28:50,720
resolution

655
00:28:50,900 --> 00:28:53,700
on the order of several tens of

656
00:28:53,700 --> 00:28:56,400
thousands of data points per Trace

657
00:28:56,400 --> 00:28:59,159
the power traces included roughly the

658
00:28:59,159 --> 00:29:01,260
first round of the cipher but excluded

659
00:29:01,260 --> 00:29:03,120
the randomness generation and hence we

660
00:29:03,120 --> 00:29:04,799
had to treat the randomness used for

661
00:29:04,799 --> 00:29:06,779
share generation and refreshment as

662
00:29:06,779 --> 00:29:09,419
completely unpredictable

663
00:29:09,419 --> 00:29:11,100
um breaking a challenge meant achieving

664
00:29:11,100 --> 00:29:13,440
a mean key rank smaller than 2 to the 32

665
00:29:13,440 --> 00:29:16,260
using a number of traces chosen by the

666
00:29:16,260 --> 00:29:19,679
team that was claiming a break the goal

667
00:29:19,679 --> 00:29:21,240
of the contest was to break as many

668
00:29:21,240 --> 00:29:22,919
challenges as possible with as few

669
00:29:22,919 --> 00:29:25,799
traces as possible and it turns out that

670
00:29:25,799 --> 00:29:28,620
our team was the only group to submit a

671
00:29:28,620 --> 00:29:31,320
solution to any of the challenges within

672
00:29:31,320 --> 00:29:33,179
the contest

673
00:29:33,179 --> 00:29:34,080
um

674
00:29:34,080 --> 00:29:35,580
it's a

675
00:29:35,580 --> 00:29:37,919
it's a organizers of the contest

676
00:29:37,919 --> 00:29:40,620
provided 200 000 power traces together

677
00:29:40,620 --> 00:29:42,720
with the secret key secret key shares

678
00:29:42,720 --> 00:29:45,240
tweak values and plain text values for

679
00:29:45,240 --> 00:29:49,140
each challenge for training Solutions in

680
00:29:49,140 --> 00:29:50,880
addition they provided a python

681
00:29:50,880 --> 00:29:54,480
implementation of the mass Clyde logic

682
00:29:54,480 --> 00:29:58,740
um they also provided test data that

683
00:29:58,740 --> 00:30:04,380
with fixed keys so that uh so that

684
00:30:04,380 --> 00:30:07,380
um and so that competitors could test

685
00:30:07,380 --> 00:30:10,320
their Solutions against up to 100 000

686
00:30:10,320 --> 00:30:13,820
traces with a fixed key for each Target

687
00:30:13,820 --> 00:30:16,799
and entries were evaluated against a

688
00:30:16,799 --> 00:30:20,039
data set kept secret by the organizer's

689
00:30:20,039 --> 00:30:21,960
key ranking was done by standard

690
00:30:21,960 --> 00:30:24,299
histogram based methods

691
00:30:24,299 --> 00:30:28,200
so Clyde acts on blocks of 128 bits

692
00:30:28,200 --> 00:30:31,260
which are arranged as four times 32-bit

693
00:30:31,260 --> 00:30:34,500
arrays it runs six rounds of encryption

694
00:30:34,500 --> 00:30:36,779
each round being composed of a three key

695
00:30:36,779 --> 00:30:39,179
addition and Xbox layer an lbox layer

696
00:30:39,179 --> 00:30:41,760
and the addition of a constant another

697
00:30:41,760 --> 00:30:44,460
Tweaky addition happens in the at the

698
00:30:44,460 --> 00:30:47,100
end of the cipher the three key schedule

699
00:30:47,100 --> 00:30:49,380
is fairly simple and on the whole the

700
00:30:49,380 --> 00:30:51,240
cipher is implementable with a fairly

701
00:30:51,240 --> 00:30:53,360
small number of gates which

702
00:30:53,360 --> 00:30:57,480
which contributes to its being masking

703
00:30:57,480 --> 00:30:58,679
friendly

704
00:30:58,679 --> 00:31:01,320
so our solution uses deep neural

705
00:31:01,320 --> 00:31:04,559
networks and all of our code and data

706
00:31:04,559 --> 00:31:07,799
can be downloaded from GitHub

707
00:31:07,799 --> 00:31:09,899
so we wanted to achieve the following we

708
00:31:09,899 --> 00:31:12,240
wanted to first of all if possible win

709
00:31:12,240 --> 00:31:13,919
the contest by having the most efficient

710
00:31:13,919 --> 00:31:17,580
attacks we wanted to use a fairly highly

711
00:31:17,580 --> 00:31:20,580
Auto automated attacks so we tried to

712
00:31:20,580 --> 00:31:23,399
avoid too much manual tuning with

713
00:31:23,399 --> 00:31:25,679
regards to hyper parameters point of

714
00:31:25,679 --> 00:31:27,960
Interest selection or leakage Target

715
00:31:27,960 --> 00:31:30,000
selection and we wanted to break all the

716
00:31:30,000 --> 00:31:32,039
software challenges using the same

717
00:31:32,039 --> 00:31:33,720
methods

718
00:31:33,720 --> 00:31:36,419
and basically what we do is we get

719
00:31:36,419 --> 00:31:38,940
predictions for the shared Cipher state

720
00:31:38,940 --> 00:31:41,460
for all of the bits of the shared Cipher

721
00:31:41,460 --> 00:31:44,640
State at some particular point of

722
00:31:44,640 --> 00:31:47,460
execution then we derive I guess for the

723
00:31:47,460 --> 00:31:49,799
unshared state which of course comes

724
00:31:49,799 --> 00:31:52,860
with an increase in uncertainty of our

725
00:31:52,860 --> 00:31:56,100
predictions as we have to predict as we

726
00:31:56,100 --> 00:31:58,620
have to combine predictions for

727
00:31:58,620 --> 00:32:01,260
different shares to get a prediction for

728
00:32:01,260 --> 00:32:03,059
the unfair state

729
00:32:03,059 --> 00:32:06,059
see unshared State then only depends on

730
00:32:06,059 --> 00:32:08,039
the Clyde logic and the inputs and not

731
00:32:08,039 --> 00:32:10,620
on the masking anymore which means that

732
00:32:10,620 --> 00:32:15,059
for each key hypothesis we can calculate

733
00:32:15,059 --> 00:32:17,159
the unshared cipher State given all the

734
00:32:17,159 --> 00:32:19,740
inputs to Clyde and compare compare the

735
00:32:19,740 --> 00:32:22,380
results of that calculation to the

736
00:32:22,380 --> 00:32:24,360
results of the side Channel extraction

737
00:32:24,360 --> 00:32:27,779
and that allows us to Output a ranking

738
00:32:27,779 --> 00:32:31,320
of the keys and in the end it allows us

739
00:32:31,320 --> 00:32:34,020
to Output a ranking of the keys where

740
00:32:34,020 --> 00:32:37,919
the target key is among the first 2 to

741
00:32:37,919 --> 00:32:39,539
the 32

742
00:32:39,539 --> 00:32:40,440
um

743
00:32:40,440 --> 00:32:44,640
keys in in this ranking as desired

744
00:32:44,640 --> 00:32:46,980
so how did we pick the leakage Target we

745
00:32:46,980 --> 00:32:49,980
just Target the state after the first s

746
00:32:49,980 --> 00:32:50,880
box

747
00:32:50,880 --> 00:32:54,000
then how do we avoid a large search cost

748
00:32:54,000 --> 00:32:56,460
when ranking key hypotheses well giving

749
00:32:56,460 --> 00:32:58,440
the leakage Target we can rank key

750
00:32:58,440 --> 00:33:00,659
hypotheses vulnerable at a time so we

751
00:33:00,659 --> 00:33:04,380
have only 16 hypotheses per nibble and

752
00:33:04,380 --> 00:33:06,779
how do we process large traces without

753
00:33:06,779 --> 00:33:09,480
manual point of Interest selections we

754
00:33:09,480 --> 00:33:11,399
reuse a neural network structure

755
00:33:11,399 --> 00:33:14,039
introduced at SEC 2020 by Gore Jakob and

756
00:33:14,039 --> 00:33:16,799
Schindler which was designed to handle

757
00:33:16,799 --> 00:33:19,760
large traces directly and the main ideas

758
00:33:19,760 --> 00:33:22,159
behind the networks

759
00:33:22,159 --> 00:33:25,500
are to deal with large trade sizes by

760
00:33:25,500 --> 00:33:27,600
sub sampling the traces with varying

761
00:33:27,600 --> 00:33:29,220
offsets and then combining the

762
00:33:29,220 --> 00:33:31,679
predictions made for the the sub-sampled

763
00:33:31,679 --> 00:33:34,519
sub traces

764
00:33:34,519 --> 00:33:37,260
and um

765
00:33:37,260 --> 00:33:39,539
yeah we've just reused that Network

766
00:33:39,539 --> 00:33:40,980
structure

767
00:33:40,980 --> 00:33:43,980
when we try to implement this however we

768
00:33:43,980 --> 00:33:46,559
saw that our networks converge well for

769
00:33:46,559 --> 00:33:48,600
many bits of the shared state but failed

770
00:33:48,600 --> 00:33:51,059
completely for others and of course if

771
00:33:51,059 --> 00:33:55,919
we failed completely for to converge for

772
00:33:55,919 --> 00:34:00,120
um one for for some bit of the shared

773
00:34:00,120 --> 00:34:04,019
State we will not see the um we will not

774
00:34:04,019 --> 00:34:05,100
see one

775
00:34:05,100 --> 00:34:08,580
the corresponding bit of the unshared

776
00:34:08,580 --> 00:34:11,940
state and if that happens then

777
00:34:11,940 --> 00:34:14,280
since we don't see any bias for that bit

778
00:34:14,280 --> 00:34:16,619
more data is not going to help us

779
00:34:16,619 --> 00:34:19,020
resolve the problem so we had to we had

780
00:34:19,020 --> 00:34:21,599
to solve this convergence problem in

781
00:34:21,599 --> 00:34:24,080
order to um in order to get predictions

782
00:34:24,080 --> 00:34:29,099
that uh get us to the desired key rank

783
00:34:29,099 --> 00:34:31,379
so the problem

784
00:34:31,379 --> 00:34:33,839
that we saw was that when predicting the

785
00:34:33,839 --> 00:34:36,300
internal Cipher state naively so one bit

786
00:34:36,300 --> 00:34:38,280
at a time then all predicted bits are

787
00:34:38,280 --> 00:34:40,918
close to independent of each other which

788
00:34:40,918 --> 00:34:43,139
means that we can make progress on

789
00:34:43,139 --> 00:34:45,239
learning to predict some bits while not

790
00:34:45,239 --> 00:34:47,520
learning how to predict the others and

791
00:34:47,520 --> 00:34:49,560
in order to solve this we invented a

792
00:34:49,560 --> 00:34:51,300
trick that we call the Scatterfield

793
00:34:51,300 --> 00:34:53,040
encoding which is an alternative

794
00:34:53,040 --> 00:34:54,899
encoding of the data that we want to

795
00:34:54,899 --> 00:34:58,320
predict namely we pick random subsets of

796
00:34:58,320 --> 00:35:00,420
the target bits and predict instead of

797
00:35:00,420 --> 00:35:02,580
the target bits themselves the Hamming

798
00:35:02,580 --> 00:35:04,440
weights of these random subsets of

799
00:35:04,440 --> 00:35:07,380
Target bits and if you have noisy

800
00:35:07,380 --> 00:35:09,060
predictions of these Hamming dates then

801
00:35:09,060 --> 00:35:10,920
we can obtain noisy predictions of all

802
00:35:10,920 --> 00:35:14,060
the target bits by some linear algebra

803
00:35:14,060 --> 00:35:16,260
post-processing that we do

804
00:35:16,260 --> 00:35:18,839
and all of these sub problems that we

805
00:35:18,839 --> 00:35:20,700
that we construct all of these

806
00:35:20,700 --> 00:35:22,980
predictions of these hemming weights are

807
00:35:22,980 --> 00:35:25,560
strongly related to each other because

808
00:35:25,560 --> 00:35:29,520
these uh these subsets of the target

809
00:35:29,520 --> 00:35:31,800
bits are not disjoint with each other so

810
00:35:31,800 --> 00:35:33,720
they they share a lot of bit bit

811
00:35:33,720 --> 00:35:36,380
positions and

812
00:35:36,380 --> 00:35:38,579
therefore one would hope that

813
00:35:38,579 --> 00:35:41,040
convergence across the subtask should be

814
00:35:41,040 --> 00:35:44,220
more uniform than than it is in the case

815
00:35:44,220 --> 00:35:46,859
when we naively try to predict the

816
00:35:46,859 --> 00:35:49,020
target bits themselves and this is

817
00:35:49,020 --> 00:35:50,940
indeed true so here we see the

818
00:35:50,940 --> 00:35:53,760
performance of our solutions to the age

819
00:35:53,760 --> 00:35:55,619
here challenge which is the hardest one

820
00:35:55,619 --> 00:35:57,780
of the software challenges and we see

821
00:35:57,780 --> 00:36:01,619
that with about 35 000 traces we get um

822
00:36:01,619 --> 00:36:05,880
we get below a median key rank below

823
00:36:05,880 --> 00:36:09,740
that uh below the target of 2 to the 32.

824
00:36:09,740 --> 00:36:13,680
so our solution Works quite efficiently

825
00:36:13,680 --> 00:36:17,339
for the three share Challenge and um but

826
00:36:17,339 --> 00:36:19,859
we see that the that the number of of

827
00:36:19,859 --> 00:36:23,220
traces that we need to get to be below

828
00:36:23,220 --> 00:36:26,160
that key rank uh it it does rise fairly

829
00:36:26,160 --> 00:36:29,880
fairly quickly but we managed to solve

830
00:36:29,880 --> 00:36:33,320
all of the software challenges

831
00:36:33,780 --> 00:36:35,160
um after the challenge was closed

832
00:36:35,160 --> 00:36:36,960
another solution was published by

833
00:36:36,960 --> 00:36:39,839
Bronson and stand there and that

834
00:36:39,839 --> 00:36:41,579
solution is based on deep belief

835
00:36:41,579 --> 00:36:44,040
networks and their solution is more

836
00:36:44,040 --> 00:36:46,079
efficient than ours for the six and

837
00:36:46,079 --> 00:36:47,880
eight share challenges but comparable

838
00:36:47,880 --> 00:36:50,520
for the four share Challenge and less

839
00:36:50,520 --> 00:36:52,920
efficient for the three three share

840
00:36:52,920 --> 00:36:55,220
challenge

841
00:36:55,280 --> 00:36:58,560
so to analyze our technique further we

842
00:36:58,560 --> 00:37:00,420
we looked at the scatter shot encoding

843
00:37:00,420 --> 00:37:02,099
in more detail because the scatter shot

844
00:37:02,099 --> 00:37:05,040
encoding is a slightly counter-intuitive

845
00:37:05,040 --> 00:37:06,900
trick because it basically just

846
00:37:06,900 --> 00:37:08,700
multiplies the values we would like to

847
00:37:08,700 --> 00:37:11,640
predict by a fixed random binary Matrix

848
00:37:11,640 --> 00:37:14,579
before running training and one one does

849
00:37:14,579 --> 00:37:17,760
one devices helps and to answer this or

850
00:37:17,760 --> 00:37:19,320
to get more insight into this we

851
00:37:19,320 --> 00:37:21,140
designed a simple synthetic problem

852
00:37:21,140 --> 00:37:24,480
namely learning to predict a particular

853
00:37:24,480 --> 00:37:27,960
F2 linear function that is that has a

854
00:37:27,960 --> 00:37:29,880
very simple structure it basically just

855
00:37:29,880 --> 00:37:33,839
outputs an F2 linear combination of its

856
00:37:33,839 --> 00:37:35,820
input bits so there is no noise no

857
00:37:35,820 --> 00:37:38,099
Randomness and the function to predict

858
00:37:38,099 --> 00:37:42,300
is quite simple but it has this function

859
00:37:42,300 --> 00:37:45,780
has components that are a bit wise

860
00:37:45,780 --> 00:37:47,940
edition of many of the input bits and

861
00:37:47,940 --> 00:37:49,680
those components are quite difficult to

862
00:37:49,680 --> 00:37:52,619
learn for a neural network and looking

863
00:37:52,619 --> 00:37:56,160
at the performance of neural networks

864
00:37:56,160 --> 00:37:59,040
that we tried at learning to predict

865
00:37:59,040 --> 00:38:00,599
this function we see that the naive

866
00:38:00,599 --> 00:38:02,820
approach and the scatter shot approach

867
00:38:02,820 --> 00:38:04,980
show qualitatively similar behavior on

868
00:38:04,980 --> 00:38:07,619
this simple synthetic problem as on our

869
00:38:07,619 --> 00:38:09,859
side Channel Challenge and so to

870
00:38:09,859 --> 00:38:12,359
illustrate this here is the learning

871
00:38:12,359 --> 00:38:16,140
history for the um for the on on the

872
00:38:16,140 --> 00:38:17,640
synthetic problem for the naive

873
00:38:17,640 --> 00:38:20,760
prediction of all of the bits by a

874
00:38:20,760 --> 00:38:22,619
neural network so we see that the neural

875
00:38:22,619 --> 00:38:25,200
network repeatedly gets stuck and in the

876
00:38:25,200 --> 00:38:28,079
end it fails on the on a few of the

877
00:38:28,079 --> 00:38:30,720
hardest bits it fails to see any bias

878
00:38:30,720 --> 00:38:32,820
whereas with the scatter shot encoding

879
00:38:32,820 --> 00:38:34,940
we get a fairly smooth learning history

880
00:38:34,940 --> 00:38:39,599
and also we um we do obtain some degree

881
00:38:39,599 --> 00:38:41,820
of convergence even for the hardest bits

882
00:38:41,820 --> 00:38:43,740
so we see at least some bias even for

883
00:38:43,740 --> 00:38:45,420
the hardest bits here

884
00:38:45,420 --> 00:38:49,320
we also compared our results with

885
00:38:49,320 --> 00:38:51,480
results that one can obtain by applying

886
00:38:51,480 --> 00:38:54,420
the stochastic approach to the uh to the

887
00:38:54,420 --> 00:38:57,240
to the chess challenge 2020 which

888
00:38:57,240 --> 00:38:58,680
required adapting the stochastic

889
00:38:58,680 --> 00:39:00,480
approach to dealing with secret shared

890
00:39:00,480 --> 00:39:03,300
Keys accounting for the high number of

891
00:39:03,300 --> 00:39:06,180
masking bits becomes quickly intractable

892
00:39:06,180 --> 00:39:09,119
even for the three share challenge

893
00:39:09,119 --> 00:39:11,099
both the scatter shot encoding and the

894
00:39:11,099 --> 00:39:14,160
stochastic approach see large biases in

895
00:39:14,160 --> 00:39:18,500
the same OS boxes however so we see um

896
00:39:18,500 --> 00:39:22,619
we we see a Sim leakage of a similar

897
00:39:22,619 --> 00:39:25,800
nature with both approaches and we found

898
00:39:25,800 --> 00:39:27,540
that our neural networks draw in

899
00:39:27,540 --> 00:39:29,280
information from some parts of the

900
00:39:29,280 --> 00:39:31,740
recorded traces that are surprising and

901
00:39:31,740 --> 00:39:33,359
therefore missed by the manual point of

902
00:39:33,359 --> 00:39:35,099
interest and Analysis done for the

903
00:39:35,099 --> 00:39:37,200
stochastic approach and overall the

904
00:39:37,200 --> 00:39:38,540
neural network based solution

905
00:39:38,540 --> 00:39:40,800
significantly outperforms the stochastic

906
00:39:40,800 --> 00:39:43,020
approach here

907
00:39:43,020 --> 00:39:46,140
it's a deep learning approach finds more

908
00:39:46,140 --> 00:39:48,359
unanticipated features and therefore

909
00:39:48,359 --> 00:39:50,460
yields better better results on this

910
00:39:50,460 --> 00:39:53,520
problem however the features exploited

911
00:39:53,520 --> 00:39:55,260
by the stochastic approach are human

912
00:39:55,260 --> 00:39:57,300
comprehensible by Design which is an

913
00:39:57,300 --> 00:40:00,720
advantage so in conclusion we can break

914
00:40:00,720 --> 00:40:02,640
implementations with a significant

915
00:40:02,640 --> 00:40:04,980
degree of protection

916
00:40:04,980 --> 00:40:07,619
using neural networks without a need for

917
00:40:07,619 --> 00:40:10,020
deep analysis of the implementation by

918
00:40:10,020 --> 00:40:12,660
an analyst however masking Works in

919
00:40:12,660 --> 00:40:14,820
principle because we see that the um

920
00:40:14,820 --> 00:40:17,640
that the number of traces required for a

921
00:40:17,640 --> 00:40:19,320
successful attack Rises quickly with

922
00:40:19,320 --> 00:40:21,780
masking order and study of simple

923
00:40:21,780 --> 00:40:23,339
synthetic problems can be quite

924
00:40:23,339 --> 00:40:25,020
insightful for deep learning based

925
00:40:25,020 --> 00:40:26,820
General analysis as we have seen with

926
00:40:26,820 --> 00:40:28,440
our synthetic task

927
00:40:28,440 --> 00:40:31,619
and I think for future work it would be

928
00:40:31,619 --> 00:40:33,300
quite desirable to gain a deeper

929
00:40:33,300 --> 00:40:34,560
understanding of the scatter shot

930
00:40:34,560 --> 00:40:37,079
encoding and

931
00:40:37,079 --> 00:40:39,060
um versus and to see whether there are

932
00:40:39,060 --> 00:40:41,280
other similar tricks and since the

933
00:40:41,280 --> 00:40:42,900
scatter shop encoding could also be seen

934
00:40:42,900 --> 00:40:46,079
as an advanced loss function it would be

935
00:40:46,079 --> 00:40:47,700
interesting to see whether one can learn

936
00:40:47,700 --> 00:40:49,800
Advanced loss functions for side Channel

937
00:40:49,800 --> 00:40:53,040
analysis and this concludes my talk

938
00:40:53,040 --> 00:40:56,480
thank you for your attention

939
00:41:01,800 --> 00:41:05,460
thank you I think Aaron is also online

940
00:41:05,460 --> 00:41:06,960
are there any questions from the

941
00:41:06,960 --> 00:41:08,700
audience yes

942
00:41:08,700 --> 00:41:11,040
uh

943
00:41:11,040 --> 00:41:15,119
uh hi do you hear me okay I can hear you

944
00:41:15,119 --> 00:41:18,900
I uh it's uh Olivia so that you know

945
00:41:18,900 --> 00:41:21,839
whisping uh so thank you very much for

946
00:41:21,839 --> 00:41:23,520
the token for the paper and for

947
00:41:23,520 --> 00:41:26,820
contributing to the the chess CTF

948
00:41:26,820 --> 00:41:28,800
um I think one key ingredient of your

949
00:41:28,800 --> 00:41:30,900
attack is knowing the values of the

950
00:41:30,900 --> 00:41:33,180
shares when running the attack

951
00:41:33,180 --> 00:41:35,940
which is usually not done

952
00:41:35,940 --> 00:41:39,060
um by the the rest I mean a larger

953
00:41:39,060 --> 00:41:40,700
majority of the rest of the community

954
00:41:40,700 --> 00:41:44,700
especially in the Deep learning area so

955
00:41:44,700 --> 00:41:46,560
could you like comment on that and do

956
00:41:46,560 --> 00:41:49,079
you think that you could have succeeded

957
00:41:49,079 --> 00:41:52,380
with such a and impressive success

958
00:41:52,380 --> 00:41:55,740
without knowing the randomness used

959
00:41:55,740 --> 00:41:58,320
I I think I think knowing the randomness

960
00:41:58,320 --> 00:42:01,560
used was essential for for what we did

961
00:42:01,560 --> 00:42:03,900
yes I mean we we

962
00:42:03,900 --> 00:42:09,240
um we use the um the supplied uh flight

963
00:42:09,240 --> 00:42:12,000
logic um so it's a mask light logic and

964
00:42:12,000 --> 00:42:15,359
so and the um the the known values of

965
00:42:15,359 --> 00:42:19,020
the randomness to derive those internal

966
00:42:19,020 --> 00:42:23,040
uh to derive those um internal share

967
00:42:23,040 --> 00:42:26,400
values uh during training and um without

968
00:42:26,400 --> 00:42:30,180
modification of the of the methods uh I

969
00:42:30,180 --> 00:42:32,579
don't think that we would have been able

970
00:42:32,579 --> 00:42:35,579
to get a model that performs that well

971
00:42:35,579 --> 00:42:38,640
just if we had just known if we had just

972
00:42:38,640 --> 00:42:42,720
known the the unshared keys and see

973
00:42:42,720 --> 00:42:45,359
um and and the algorithm itself then I

974
00:42:45,359 --> 00:42:47,520
don't think this is the most methods

975
00:42:47,520 --> 00:42:50,300
that we used that we could have uh

976
00:42:50,300 --> 00:42:53,160
obtained that result yeah and I'm

977
00:42:53,160 --> 00:42:55,079
feeling like as the BSI you are in a

978
00:42:55,079 --> 00:42:57,180
position like say saying is it relevant

979
00:42:57,180 --> 00:42:59,160
or not to know the randomness when you

980
00:42:59,160 --> 00:43:03,540
analyze a cipher so personally I think I

981
00:43:03,540 --> 00:43:08,300
think it makes sense to um to

982
00:43:08,300 --> 00:43:10,079
look at

983
00:43:10,079 --> 00:43:11,480
um

984
00:43:11,480 --> 00:43:16,079
an attack an adversary model that is uh

985
00:43:16,079 --> 00:43:21,359
that is strong enough to well that is um

986
00:43:21,359 --> 00:43:23,160
personally I think it makes sense to

987
00:43:23,160 --> 00:43:25,560
look at powerful adversary models um

988
00:43:25,560 --> 00:43:28,440
because

989
00:43:28,440 --> 00:43:29,220
um

990
00:43:29,220 --> 00:43:32,099
I mean the the algorithm is supposed

991
00:43:32,099 --> 00:43:35,160
certainly supposed to be known by by any

992
00:43:35,160 --> 00:43:37,740
by the adversary by kirchhoff's

993
00:43:37,740 --> 00:43:39,960
principle and

994
00:43:39,960 --> 00:43:41,520
um

995
00:43:41,520 --> 00:43:44,040
I think in terms of

996
00:43:44,040 --> 00:43:47,040
in terms of evaluating

997
00:43:47,040 --> 00:43:48,839
um the

998
00:43:48,839 --> 00:43:52,560
worst case that one can I mean the the

999
00:43:52,560 --> 00:43:55,339
worst case that one has to deal with

1000
00:43:55,339 --> 00:43:58,440
when ensuring that a particular

1001
00:43:58,440 --> 00:44:01,440
implementation is secure it does make

1002
00:44:01,440 --> 00:44:03,660
sense in my opinion to

1003
00:44:03,660 --> 00:44:07,560
looking at an adversary who can be

1004
00:44:07,560 --> 00:44:10,020
assumed to have weakened the

1005
00:44:10,020 --> 00:44:12,060
implementation in such a way that he can

1006
00:44:12,060 --> 00:44:14,099
that for training purposes he can see

1007
00:44:14,099 --> 00:44:16,200
the randomness

1008
00:44:16,200 --> 00:44:18,900
um but I I think it it also makes it it

1009
00:44:18,900 --> 00:44:21,240
also makes perfect sense to look at Vita

1010
00:44:21,240 --> 00:44:24,300
adversary models and and to to see

1011
00:44:24,300 --> 00:44:26,099
um how much one loses

1012
00:44:26,099 --> 00:44:27,359
um if one

1013
00:44:27,359 --> 00:44:30,359
um moves to a weaker adversary model

1014
00:44:30,359 --> 00:44:32,760
we have one more question it's going to

1015
00:44:32,760 --> 00:44:35,700
be short and if possible the answer also

1016
00:44:35,700 --> 00:44:39,420
short yeah sure uh hello sorry so you

1017
00:44:39,420 --> 00:44:42,000
mentioned uh hi this is Marius uh you

1018
00:44:42,000 --> 00:44:44,099
mentioned the use of stochastic models

1019
00:44:44,099 --> 00:44:46,079
with a manual selection of points of

1020
00:44:46,079 --> 00:44:47,880
interest but there is also a method that

1021
00:44:47,880 --> 00:44:50,819
we shown on how to combine stochastic uh

1022
00:44:50,819 --> 00:44:54,240
models with LDA and PCA did you try that

1023
00:44:54,240 --> 00:44:57,240
so automatic selection via projection of

1024
00:44:57,240 --> 00:45:00,599
lempc with stochastic models

1025
00:45:00,599 --> 00:45:02,660
um we

1026
00:45:02,660 --> 00:45:05,700
we did it

1027
00:45:05,700 --> 00:45:07,700
um

1028
00:45:07,859 --> 00:45:11,160
we did a we did a we did have more or

1029
00:45:11,160 --> 00:45:15,599
less a semi automatic approach to the to

1030
00:45:15,599 --> 00:45:18,839
the point of Interest selection

1031
00:45:18,839 --> 00:45:21,119
um but I'm

1032
00:45:21,119 --> 00:45:25,020
I'm not quite sure I would have to talk

1033
00:45:25,020 --> 00:45:27,599
to I would have to talk to my

1034
00:45:27,599 --> 00:45:30,839
collaborators what exactly we did

1035
00:45:30,839 --> 00:45:33,300
um what what exactly

1036
00:45:33,300 --> 00:45:36,000
um which approaches we exactly used to

1037
00:45:36,000 --> 00:45:38,099
do the point of Interest selection for

1038
00:45:38,099 --> 00:45:41,040
the stochastic approach as I mostly

1039
00:45:41,040 --> 00:45:43,140
dealt with the neural network side of

1040
00:45:43,140 --> 00:45:44,520
things

1041
00:45:44,520 --> 00:45:46,920
thank you very much for this answer and

1042
00:45:46,920 --> 00:45:49,920
let's thank the speaker

1043
00:45:49,920 --> 00:45:52,760
foreign

1044
00:46:00,599 --> 00:46:04,800
who came all the way from Japan yeah

1045
00:46:04,800 --> 00:46:06,060
so

1046
00:46:06,060 --> 00:46:07,859
um

1047
00:46:07,859 --> 00:46:10,800
you have the floor yeah

1048
00:46:10,800 --> 00:46:13,020
thank you for the introduction I'm

1049
00:46:13,020 --> 00:46:16,560
living with University Japan today I'm

1050
00:46:16,560 --> 00:46:18,599
talking about Singapore Singletary

1051
00:46:18,599 --> 00:46:21,839
attack on artificiality this is the

1052
00:46:21,839 --> 00:46:24,799
collaborative work with

1053
00:46:35,480 --> 00:46:38,280
tried to distinguish squaring and

1054
00:46:38,280 --> 00:46:39,839
multiplication during the modular

1055
00:46:39,839 --> 00:46:42,000
exponentiation because the secret

1056
00:46:42,000 --> 00:46:44,819
experimented the directory represent the

1057
00:46:44,819 --> 00:46:46,560
secretary of the RSA signing over

1058
00:46:46,560 --> 00:46:48,359
decryption

1059
00:46:48,359 --> 00:46:51,359
uh

1060
00:47:02,599 --> 00:47:07,440
is very important to be because a public

1061
00:47:07,440 --> 00:47:10,079
key cryptography is not necessarily

1062
00:47:10,079 --> 00:47:14,280
perform the other frequently Primitives

1063
00:47:16,640 --> 00:47:19,800
artificiality has been also studied for

1064
00:47:19,800 --> 00:47:22,920
a long time so this attack estimates

1065
00:47:22,920 --> 00:47:24,740
that secret key

1066
00:47:24,740 --> 00:47:26,940
secretary from partial information of

1067
00:47:26,940 --> 00:47:29,540
the secretary

1068
00:47:30,500 --> 00:47:33,740
this is so attack estimates

1069
00:47:33,740 --> 00:47:36,660
security key by discarding the wrong

1070
00:47:36,660 --> 00:47:38,819
secretary contradicting the statistical

1071
00:47:38,819 --> 00:47:41,240
leakage within like this

1072
00:47:41,240 --> 00:47:47,359
and by such a binary three manner

1073
00:47:59,359 --> 00:48:01,920
obtained by the attacker is not always

1074
00:48:01,920 --> 00:48:05,540
corrected or correct

1075
00:48:05,540 --> 00:48:08,160
and actually this talk is about deep

1076
00:48:08,160 --> 00:48:11,220
learning about social and Ducks uh let

1077
00:48:11,220 --> 00:48:13,560
me omit to introduce drsca because

1078
00:48:13,560 --> 00:48:15,900
previous talks likely introduce the

1079
00:48:15,900 --> 00:48:17,099
grsga

1080
00:48:17,099 --> 00:48:20,040
and what I'd like to say here is the

1081
00:48:20,040 --> 00:48:22,440
deer is very strong theory for Ortho for

1082
00:48:22,440 --> 00:48:27,300
the fgf but we we should still consider

1083
00:48:27,300 --> 00:48:29,400
what to run by the year for efficient

1084
00:48:29,400 --> 00:48:32,180
key recovery

1085
00:48:32,180 --> 00:48:35,099
in my opinion for symmetric key

1086
00:48:35,099 --> 00:48:37,520
cryptography it will be a very

1087
00:48:37,520 --> 00:48:40,619
established because many existing beers

1088
00:48:40,619 --> 00:48:42,960
focuses on the conditional probability

1089
00:48:42,960 --> 00:48:46,520
distribution of the Xbox output uh

1090
00:48:46,520 --> 00:48:51,480
and its optimality is well uh either

1091
00:48:51,480 --> 00:48:53,640
Analytical in some studies

1092
00:48:53,640 --> 00:48:57,680
on the other hand foreign

1093
00:49:04,760 --> 00:49:09,060
dependent on the algorithm itself

1094
00:49:09,060 --> 00:49:12,319
so today uh we talk about the

1095
00:49:12,319 --> 00:49:15,540
essential new deep learning basis or

1096
00:49:15,540 --> 00:49:18,240
cycle attack or artificiality so with

1097
00:49:18,240 --> 00:49:21,720
reality is how to estimate the figure at

1098
00:49:21,720 --> 00:49:23,880
this point from digital traits and the

1099
00:49:23,880 --> 00:49:27,260
new partial key exposure attack

1100
00:49:27,960 --> 00:49:30,960
major

1101
00:49:41,000 --> 00:49:43,400
implementation is

1102
00:49:43,400 --> 00:49:46,500
in the many open source libraries so why

1103
00:49:46,500 --> 00:49:49,560
the existing drfcs or rsf were discrete

1104
00:49:49,560 --> 00:49:51,839
longer revenue for targets binary

1105
00:49:51,839 --> 00:49:54,619
exponentiation

1106
00:49:56,060 --> 00:49:58,980
using actual implementation using the

1107
00:49:58,980 --> 00:50:01,619
GMP this is the major March prevention

1108
00:50:01,619 --> 00:50:05,040
library and this is a frequently used in

1109
00:50:05,040 --> 00:50:07,020
the cryptographic operation

1110
00:50:07,020 --> 00:50:09,359
and we also confirmed the availability

1111
00:50:09,359 --> 00:50:11,280
of The Proposal attack to some related

1112
00:50:11,280 --> 00:50:13,920
to cryptographic libraries including

1113
00:50:13,920 --> 00:50:17,339
open SSR bota and revision Crypt

1114
00:50:17,339 --> 00:50:19,740
okay let me start from RS encrypted

1115
00:50:19,740 --> 00:50:22,319
systems so I so I know uh credit

1116
00:50:22,319 --> 00:50:26,040
Graphics has seen this other formula so

1117
00:50:26,040 --> 00:50:29,400
very very frequently and forty thought

1118
00:50:29,400 --> 00:50:31,920
yes so you are interested in how to

1119
00:50:31,920 --> 00:50:34,440
implement these formulas Security on the

1120
00:50:34,440 --> 00:50:36,660
exchange

1121
00:50:36,660 --> 00:50:38,880
so let's focus on open source RC

1122
00:50:38,880 --> 00:50:40,800
implementation because such an

1123
00:50:40,800 --> 00:50:43,260
implementations uh have a very high

1124
00:50:43,260 --> 00:50:45,420
performance and very practical because

1125
00:50:45,420 --> 00:50:48,599
this is implemented by expat of the

1126
00:50:48,599 --> 00:50:51,980
cryptographic implementation

1127
00:50:53,480 --> 00:50:56,640
is usually useful for the RSA

1128
00:50:56,640 --> 00:50:59,760
description of signing because it can

1129
00:50:59,760 --> 00:51:01,619
release the computational cost by factor

1130
00:51:01,619 --> 00:51:04,020
of two to four without almost no

1131
00:51:04,020 --> 00:51:05,819
overhead

1132
00:51:05,819 --> 00:51:09,720
uh in implementing RFA the explanation

1133
00:51:09,720 --> 00:51:13,619
algorithm is very important which mainly

1134
00:51:13,619 --> 00:51:15,420
determines the security and efficiency

1135
00:51:15,420 --> 00:51:18,619
so this table summarizes the

1136
00:51:18,619 --> 00:51:20,640
exponentiation major exponentiation

1137
00:51:20,640 --> 00:51:23,460
algorithms for implementing RSA as a

1138
00:51:23,460 --> 00:51:26,280
decent classified to two types of the

1139
00:51:26,280 --> 00:51:28,319
binary exponentiation and the windowed

1140
00:51:28,319 --> 00:51:31,640
expression so actually many open source

1141
00:51:31,640 --> 00:51:34,579
libraries uses

1142
00:51:34,579 --> 00:51:38,220
we know the explanation thanks to its

1143
00:51:38,220 --> 00:51:40,140
high performance and some levels of

1144
00:51:40,140 --> 00:51:42,359
resistance to the financial simpler

1145
00:51:42,359 --> 00:51:44,900
analysis

1146
00:51:45,300 --> 00:51:47,640
so I introduced a fixed window

1147
00:51:47,640 --> 00:51:49,859
exponentiation so this is the fastest

1148
00:51:49,859 --> 00:51:51,960
constant time exponentiation to the rest

1149
00:51:51,960 --> 00:51:54,359
of my direct so let WD of the window

1150
00:51:54,359 --> 00:51:57,000
side as a parameter so solving the

1151
00:51:57,000 --> 00:51:59,579
explanations of that two stages of

1152
00:51:59,579 --> 00:52:01,859
pre-computation and Main Loop in

1153
00:52:01,859 --> 00:52:04,260
pre-computation we make precomposition

1154
00:52:04,260 --> 00:52:07,020
table which contains two to the power of

1155
00:52:07,020 --> 00:52:10,859
w elements and in the iso address of the

1156
00:52:10,859 --> 00:52:14,579
table it stores uh sheet to the power

1157
00:52:14,579 --> 00:52:17,760
right yes C is the base

1158
00:52:17,760 --> 00:52:20,760
and now for all following the main Loop

1159
00:52:20,760 --> 00:52:22,980
uh

1160
00:52:22,980 --> 00:52:25,800
we perform scouting double times and

1161
00:52:25,800 --> 00:52:27,599
then powerful multiplication with the

1162
00:52:27,599 --> 00:52:29,880
pre-computation template value uh

1163
00:52:29,880 --> 00:52:31,380
according to the temporary window

1164
00:52:31,380 --> 00:52:33,900
barrier and we repeat this procedure so

1165
00:52:33,900 --> 00:52:36,300
this figure shows the example of a

1166
00:52:36,300 --> 00:52:37,980
strong window exponentiation we still

1167
00:52:37,980 --> 00:52:39,859
will be the exponent and the double

1168
00:52:39,859 --> 00:52:44,160
because uh explain the detail of it so

1169
00:52:44,160 --> 00:52:46,319
we have a three four bit temporary

1170
00:52:46,319 --> 00:52:48,000
window body because double equal four

1171
00:52:48,000 --> 00:52:50,760
and after the pre-computation we perform

1172
00:52:50,760 --> 00:52:53,099
scaling the scarring scaling scoring and

1173
00:52:53,099 --> 00:52:55,319
then perform multiplication is a free

1174
00:52:55,319 --> 00:52:57,720
computational table value yeah the

1175
00:52:57,720 --> 00:53:01,020
temporal window value is the 1101 we

1176
00:53:01,020 --> 00:53:02,760
perform the multiplication with this G

1177
00:53:02,760 --> 00:53:05,700
to the power of 1 1 0 1 so starting

1178
00:53:05,700 --> 00:53:07,859
so we repeated this procedure until the

1179
00:53:07,859 --> 00:53:10,740
end of the exponent bit

1180
00:53:10,740 --> 00:53:13,500
now how about the security of a

1181
00:53:13,500 --> 00:53:14,640
statistical security of this

1182
00:53:14,640 --> 00:53:17,819
exponentiation the in fact this is

1183
00:53:17,819 --> 00:53:20,160
secure against the simplified analysis

1184
00:53:20,160 --> 00:53:23,700
because squaring multiplication sequence

1185
00:53:23,700 --> 00:53:26,040
is independent of the secret experience

1186
00:53:26,040 --> 00:53:28,260
however they are the other possibility

1187
00:53:28,260 --> 00:53:31,200
of the leakage on the temporary window

1188
00:53:31,200 --> 00:53:35,099
value in fact the attacker can make

1189
00:53:35,099 --> 00:53:37,400
about the temporary window barrier

1190
00:53:37,400 --> 00:53:40,559
immediately have the secret key because

1191
00:53:40,559 --> 00:53:42,180
temporary in the body directly

1192
00:53:42,180 --> 00:53:44,880
represents the secret experience

1193
00:53:44,880 --> 00:53:47,880
analysis

1194
00:53:52,500 --> 00:53:55,440
so uh because the multiplication operand

1195
00:53:55,440 --> 00:53:57,720
they draw it from the temporal window uh

1196
00:53:57,720 --> 00:53:59,640
sorry uh in order to from a

1197
00:53:59,640 --> 00:54:01,619
pre-computation table so which the

1198
00:54:01,619 --> 00:54:03,240
Consular leakage and the security of the

1199
00:54:03,240 --> 00:54:05,400
operand routing

1200
00:54:05,400 --> 00:54:08,760
in fact uh many open source libraries

1201
00:54:08,760 --> 00:54:10,920
the employee dummy Road or other counter

1202
00:54:10,920 --> 00:54:12,660
measure to hide the temporary window

1203
00:54:12,660 --> 00:54:15,119
body so this figure shows the operand

1204
00:54:15,119 --> 00:54:19,140
loading uh listening GMP uh the main

1205
00:54:19,140 --> 00:54:22,140
idea of this implementation is the old

1206
00:54:22,140 --> 00:54:25,380
operands are accessible in every

1207
00:54:25,380 --> 00:54:27,660
multiplication even if it is not

1208
00:54:27,660 --> 00:54:29,880
actually use the photographation and

1209
00:54:29,880 --> 00:54:31,859
this algorithm shows the mask value to

1210
00:54:31,859 --> 00:54:35,700
determine uh two uh to the road and this

1211
00:54:35,700 --> 00:54:38,040
right figures of equivalent to represent

1212
00:54:38,040 --> 00:54:40,800
such representation of this algorithm so

1213
00:54:40,800 --> 00:54:44,099
these are the facts operates with two to

1214
00:54:44,099 --> 00:54:47,720
the power of w iterations

1215
00:54:48,559 --> 00:54:52,020
this intermediate resistor is stores the

1216
00:54:52,020 --> 00:54:54,059
pre-composition table valued auditory of

1217
00:54:54,059 --> 00:54:55,260
ice address

1218
00:54:55,260 --> 00:55:00,780
so otherwise if the foreign discarded

1219
00:55:06,000 --> 00:55:07,800
such a combination of the window the

1220
00:55:07,800 --> 00:55:10,940
exponentiation and the terminal the

1221
00:55:10,940 --> 00:55:13,740
related to be sufficient to protect

1222
00:55:13,740 --> 00:55:18,000
again against the motor timing attack or

1223
00:55:18,000 --> 00:55:21,240
remote cache data but we are talking

1224
00:55:21,240 --> 00:55:24,720
about the power volume social attacks on

1225
00:55:24,720 --> 00:55:27,900
such an implementation today so in fact

1226
00:55:27,900 --> 00:55:32,339
uh Library developers does not always

1227
00:55:32,339 --> 00:55:35,520
consider such a physical Sagittarius but

1228
00:55:35,520 --> 00:55:39,000
such an open implementation first choice

1229
00:55:39,000 --> 00:55:41,280
if you would like to implement RFA or

1230
00:55:41,280 --> 00:55:43,920
embed devices due to its high

1231
00:55:43,920 --> 00:55:48,440
performance maturity or high prevalence

1232
00:55:58,460 --> 00:56:03,800
so uh very similar to the profile data

1233
00:56:04,819 --> 00:56:07,380
includes the development of the new

1234
00:56:07,380 --> 00:56:11,240
techniques for data sets

1235
00:56:12,020 --> 00:56:13,980
summarizes that proposed the methodology

1236
00:56:13,980 --> 00:56:17,160
so the proponent focusing on the first

1237
00:56:17,160 --> 00:56:20,280
fact that the operand rolling concept of

1238
00:56:20,280 --> 00:56:22,859
the only one three Road and all the

1239
00:56:22,859 --> 00:56:25,619
remaining load are dummy Road

1240
00:56:25,619 --> 00:56:28,559
so this factor is the two important

1241
00:56:28,559 --> 00:56:31,980
points for the social attacker so the

1242
00:56:31,980 --> 00:56:34,440
first point is the value of the list is

1243
00:56:34,440 --> 00:56:37,500
changed only when the Three Rod but this

1244
00:56:37,500 --> 00:56:39,780
makes the difference in the financial

1245
00:56:39,780 --> 00:56:41,880
Territory between the three and dummy

1246
00:56:41,880 --> 00:56:43,980
road which can be exploited by the

1247
00:56:43,980 --> 00:56:45,119
attacker

1248
00:56:45,119 --> 00:56:46,619
then

1249
00:56:46,619 --> 00:56:48,720
the second point is more important so

1250
00:56:48,720 --> 00:56:49,940
although

1251
00:56:49,940 --> 00:56:51,839
fully depends on the temporary window

1252
00:56:51,839 --> 00:56:54,420
body and actually interior and the

1253
00:56:54,420 --> 00:56:56,460
mirror sequence represent the temporal

1254
00:56:56,460 --> 00:57:00,319
window body of the wild code

1255
00:57:16,760 --> 00:57:20,220
dependent on this value so namely the

1256
00:57:20,220 --> 00:57:22,559
the temporary window bar is the 1101 so

1257
00:57:22,559 --> 00:57:26,099
I mean 13. so 13 is loading the battery

1258
00:57:26,099 --> 00:57:28,500
load and the remaining order is the

1259
00:57:28,500 --> 00:57:31,319
tummy road so I'd like to say the

1260
00:57:31,319 --> 00:57:33,900
distinguishing the true and dummy road

1261
00:57:33,900 --> 00:57:37,559
is sufficient for recording a secret

1262
00:57:37,559 --> 00:57:40,619
experience and secret key

1263
00:57:40,619 --> 00:57:44,819
and uh so so let's consider how to ex

1264
00:57:44,819 --> 00:57:46,859
how to distinguish between dummy rules

1265
00:57:46,859 --> 00:57:51,480
so we use uh DNA dsga for this purpose

1266
00:57:51,480 --> 00:57:54,000
so we employ two classification neural

1267
00:57:54,000 --> 00:57:55,440
network to distinguished through and

1268
00:57:55,440 --> 00:57:58,680
dummy road so proposal is the profile

1269
00:57:58,680 --> 00:58:01,380
data containing the training and attack

1270
00:58:01,380 --> 00:58:04,740
queries in training phase we train on

1271
00:58:04,740 --> 00:58:08,359
neural network using uh

1272
00:58:13,319 --> 00:58:16,079
the in and these figures of the overview

1273
00:58:16,079 --> 00:58:19,319
of the attack phase so in turning phase

1274
00:58:19,319 --> 00:58:22,020
we uh trained the neutral neural network

1275
00:58:22,020 --> 00:58:25,020
to imitate uh conditional probability

1276
00:58:25,020 --> 00:58:27,420
distribution of the reload a given 500

1277
00:58:27,420 --> 00:58:31,380
Trace so to determine the three load we

1278
00:58:31,380 --> 00:58:33,660
perform uh two to the power with W2

1279
00:58:33,660 --> 00:58:36,059
classifications to distinguish to

1280
00:58:36,059 --> 00:58:39,180
another mirror in estimating one

1281
00:58:39,180 --> 00:58:42,379
temporary window body

1282
00:58:53,000 --> 00:58:55,740
and we estimate load operation with

1283
00:58:55,740 --> 00:58:58,559
highest probability of the three load uh

1284
00:58:58,559 --> 00:59:01,440
because it is most likely one

1285
00:59:01,440 --> 00:59:04,920
this is simply done by taking org box of

1286
00:59:04,920 --> 00:59:08,460
the origin outputs for these inferences

1287
00:59:08,460 --> 00:59:10,920
what is the important here is that any

1288
00:59:10,920 --> 00:59:13,140
inference is released to classification

1289
00:59:13,140 --> 00:59:16,559
from to the power of w classification

1290
00:59:16,559 --> 00:59:18,900
so such a two classification is the

1291
00:59:18,900 --> 00:59:21,059
first simple task than two

1292
00:59:21,059 --> 00:59:23,400
multi-graphical Transportation so this

1293
00:59:23,400 --> 00:59:24,740
is the

1294
00:59:24,740 --> 00:59:28,319
Improvement the reaction of the learning

1295
00:59:28,319 --> 00:59:33,740
cost and finally uh efficient attack

1296
00:59:33,780 --> 00:59:36,480
so we also propose a new password key

1297
00:59:36,480 --> 00:59:38,640
exposure attack but let me omit the

1298
00:59:38,640 --> 00:59:41,400
detail explanation of this the detail of

1299
00:59:41,400 --> 00:59:42,540
this algorithm due to the time

1300
00:59:42,540 --> 00:59:43,559
constraint

1301
00:59:43,559 --> 00:59:46,500
uh however the proposal key idea of The

1302
00:59:46,500 --> 00:59:49,920
Proposal attack is we perform that this

1303
00:59:49,920 --> 00:59:51,140
uh

1304
00:59:51,140 --> 00:59:55,319
in the WB assuming that the wb1 while

1305
00:59:55,319 --> 00:59:58,520
the existing attack is

1306
00:59:58,520 --> 01:00:01,020
uniformly distributed

1307
01:00:01,020 --> 01:00:04,020
foreign

1308
01:00:16,200 --> 01:00:17,480
hmm

1309
01:00:17,480 --> 01:00:21,599
and uh are we demonstrates a

1310
01:00:21,599 --> 01:00:25,619
experimental attacks on uh 10 24 bit

1311
01:00:25,619 --> 01:00:29,240
artificialt implementation with uh GMP

1312
01:00:29,240 --> 01:00:34,920
uh here uh 10 020 artificiality

1313
01:00:34,920 --> 01:00:38,359
implementation meet so we require

1314
01:00:38,359 --> 01:00:41,099
128 times two temporary window body

1315
01:00:41,099 --> 01:00:43,859
estimation for double equal to four so

1316
01:00:43,859 --> 01:00:46,440
double equal four is the the determined

1317
01:00:46,440 --> 01:00:49,140
by the GMP of the optical parameter in

1318
01:00:49,140 --> 01:00:50,280
this case

1319
01:00:50,280 --> 01:00:54,480
uh we used to almost 60 million in

1320
01:00:54,480 --> 01:00:57,119
electronics for the training and this

1321
01:00:57,119 --> 01:01:00,540
figure should I iterated for today and

1322
01:01:00,540 --> 01:01:02,880
dummy Road all right please note that

1323
01:01:02,880 --> 01:01:05,760
it's quite difficult at least for me to

1324
01:01:05,760 --> 01:01:08,819
distinguish 300 from these types of

1325
01:01:08,819 --> 01:01:12,140
geratoices and we use the convolutionary

1326
01:01:12,140 --> 01:01:15,500
CNN for our experiment

1327
01:01:15,500 --> 01:01:19,440
and this table shows the other dessert

1328
01:01:19,440 --> 01:01:21,960
so we evaluated the testified accuracy

1329
01:01:21,960 --> 01:01:25,920
using 24 different secret keys I mean we

1330
01:01:25,920 --> 01:01:29,460
performed the estimation of 48 exponents

1331
01:01:29,460 --> 01:01:34,260
48 times 700 128 bit temporary window by

1332
01:01:34,260 --> 01:01:36,500
estimation and

1333
01:01:36,500 --> 01:01:41,220
48 times 128 times 63 and damilola

1334
01:01:41,220 --> 01:01:43,819
distinguish

1335
01:01:53,359 --> 01:01:57,540
99 percent and uh

1336
01:01:57,540 --> 01:01:58,640
uh

1337
01:01:58,640 --> 01:02:02,180
you know Network also can achieve the

1338
01:02:02,180 --> 01:02:05,040
exponentary companies 80 percent to

1339
01:02:05,040 --> 01:02:07,559
accuracy because this is the success

1340
01:02:07,559 --> 01:02:10,020
rate of the other attack is the almost

1341
01:02:10,020 --> 01:02:12,119
80 percent even without particular

1342
01:02:12,119 --> 01:02:14,280
attack

1343
01:02:14,280 --> 01:02:16,680
uh really also perform the temperature

1344
01:02:16,680 --> 01:02:18,780
attack and treated the power of w

1345
01:02:18,780 --> 01:02:21,200
classification in here for a comparison

1346
01:02:21,200 --> 01:02:24,180
at the temperature the major existing

1347
01:02:24,180 --> 01:02:26,940
method and the multiples concentration

1348
01:02:26,940 --> 01:02:28,440
is the straightforward extension with

1349
01:02:28,440 --> 01:02:31,680
the existing new dnsta

1350
01:02:31,680 --> 01:02:34,260
after that we cannot achieve very high

1351
01:02:34,260 --> 01:02:37,680
accuracy for using them and they are

1352
01:02:37,680 --> 01:02:39,540
interesting for the key recovery even

1353
01:02:39,540 --> 01:02:43,759
using the partial key exposure attack

1354
01:02:53,480 --> 01:02:57,000
during the exponentiation

1355
01:02:57,000 --> 01:02:59,520
and finally we average accelerator with

1356
01:02:59,520 --> 01:03:02,040
the overall success rate using partial

1357
01:03:02,040 --> 01:03:04,740
key exposure attack so we generated to

1358
01:03:04,740 --> 01:03:08,280
100 random artists she she artificialt

1359
01:03:08,280 --> 01:03:11,400
secret keys with wide errors to simulate

1360
01:03:11,400 --> 01:03:14,579
errors increditing the ourca

1361
01:03:14,579 --> 01:03:16,680
this figure shows the result the

1362
01:03:16,680 --> 01:03:18,299
horizontal axis is the number of the

1363
01:03:18,299 --> 01:03:21,660
errors and particles axis is a

1364
01:03:21,660 --> 01:03:23,339
computational cost

1365
01:03:23,339 --> 01:03:26,059
so we confirmed the proposal attraction

1366
01:03:26,059 --> 01:03:30,740
with 100 substrate within a few

1367
01:03:30,740 --> 01:03:32,940
thousands of seconds it's very

1368
01:03:32,940 --> 01:03:37,380
practically feasible rate

1369
01:03:42,480 --> 01:03:44,579
so we confirmed the feasibility and

1370
01:03:44,579 --> 01:03:47,359
effectiveness of the proposed attack

1371
01:03:47,359 --> 01:03:51,059
so this result does not mean our attack

1372
01:03:51,059 --> 01:03:53,579
is superior to the Head titled attack

1373
01:03:53,579 --> 01:03:55,740
and the actually means our attack is

1374
01:03:55,740 --> 01:03:58,859
very calibrated to our drfca

1375
01:03:58,859 --> 01:04:02,579
okay it's closing uh that's close in my

1376
01:04:02,579 --> 01:04:05,520
talk we developed a new dsga on

1377
01:04:05,520 --> 01:04:08,400
applicability without a crtrc

1378
01:04:08,400 --> 01:04:12,000
implementations in our paper we propose

1379
01:04:12,000 --> 01:04:13,500
the contamination against propose the

1380
01:04:13,500 --> 01:04:16,680
attack please see a paper for detail

1381
01:04:16,680 --> 01:04:19,319
uh finally uh we'd like to say that

1382
01:04:19,319 --> 01:04:21,859
offer stronger attacks

1383
01:04:21,859 --> 01:04:25,799
implementations but if we implementation

1384
01:04:25,799 --> 01:04:27,960
details are available so we can achieve

1385
01:04:27,960 --> 01:04:31,140
a stronger attack a doctor thank you for

1386
01:04:31,140 --> 01:04:33,558
your attention

1387
01:04:47,640 --> 01:04:50,040
uh thank you

1388
01:04:50,040 --> 01:04:52,700
that's good

1389
01:04:53,700 --> 01:04:57,359
uh okay sorry thank you for the talk uh

1390
01:04:57,359 --> 01:04:59,819
you said that the integration algorithm

1391
01:04:59,819 --> 01:05:01,859
was not applicable can you elaborate a

1392
01:05:01,859 --> 01:05:05,819
bit on this uh you said that the inigir

1393
01:05:05,819 --> 01:05:07,740
shasham algorithm was not applicable for

1394
01:05:07,740 --> 01:05:10,319
the Q reconstruction can you elaborate a

1395
01:05:10,319 --> 01:05:12,240
bit on this or

1396
01:05:12,240 --> 01:05:13,940
sorry uh

1397
01:05:13,940 --> 01:05:16,500
speak literally because I found it not

1398
01:05:16,500 --> 01:05:18,780
very clearly I think sorry ah please

1399
01:05:18,780 --> 01:05:20,700
speak slowly because the fund is not

1400
01:05:20,700 --> 01:05:21,960
very clear

1401
01:05:21,960 --> 01:05:24,059
maybe we can take this offline after I'm

1402
01:05:24,059 --> 01:05:27,020
not sure I understand what

1403
01:05:31,859 --> 01:05:34,578
in English

1404
01:05:36,619 --> 01:05:38,660
I'm not sure

1405
01:05:38,660 --> 01:05:41,779
as well

1406
01:05:48,680 --> 01:05:51,960
so the first one yeah you said that the

1407
01:05:51,960 --> 01:05:54,480
it's not applicable for you I know yeah

1408
01:05:54,480 --> 01:05:59,059
so so uh why uh

1409
01:05:59,059 --> 01:06:03,980
the difference of the leakage model so

1410
01:06:03,980 --> 01:06:07,619
we can uh estimate the Secretary of a

1411
01:06:07,619 --> 01:06:09,740
complete secretary

1412
01:06:09,740 --> 01:06:13,920
Erasure but contains the trips so this

1413
01:06:13,920 --> 01:06:15,839
is the this indicate model is not

1414
01:06:15,839 --> 01:06:17,640
covered by heading attack

1415
01:06:17,640 --> 01:06:21,078
okay thank you

1416
01:06:26,760 --> 01:06:29,039
yeah I I'm under the impression that you

1417
01:06:29,039 --> 01:06:30,299
didn't make it very clear how many

1418
01:06:30,299 --> 01:06:32,119
traces you need per key

1419
01:06:32,119 --> 01:06:34,980
sorry about it how many traces you need

1420
01:06:34,980 --> 01:06:36,539
per key

1421
01:06:36,539 --> 01:06:39,780
how many traces you need I'm having

1422
01:06:39,780 --> 01:06:43,280
tries yes

1423
01:06:46,640 --> 01:06:50,039
only one tourist for the attack

1424
01:06:50,039 --> 01:06:53,900
so this is a Singletary attack

1425
01:06:54,660 --> 01:06:57,380
is the data set publicly available

1426
01:06:57,380 --> 01:07:01,400
your data set is it available

1427
01:07:02,119 --> 01:07:05,819
sorry your data set is it available I

1428
01:07:05,819 --> 01:07:10,319
have already publishing uh sorry

1429
01:07:10,920 --> 01:07:12,660
um thank you very much for a very nice

1430
01:07:12,660 --> 01:07:15,839
presentation what was the target uh

1431
01:07:15,839 --> 01:07:18,180
what did you run it on the the

1432
01:07:18,180 --> 01:07:19,920
experiments

1433
01:07:19,920 --> 01:07:22,920
uh

1434
01:07:32,240 --> 01:07:34,859
was it was it running on like bare metal

1435
01:07:34,859 --> 01:07:37,760
I assume okay

1436
01:07:37,760 --> 01:07:40,740
thank you

1437
01:07:40,740 --> 01:07:42,000
okay

1438
01:07:42,000 --> 01:07:42,839
um

1439
01:07:42,839 --> 01:07:45,740
the flight uh

1440
01:07:47,520 --> 01:07:50,000
we talk

1441
01:07:55,380 --> 01:07:57,539
and it's my pleasure to introduce our

1442
01:07:57,539 --> 01:08:00,980
next speaker guillerme perine

1443
01:08:13,980 --> 01:08:16,140
and in the meantime I want to thank you

1444
01:08:16,140 --> 01:08:18,600
as the audience for having a lot of

1445
01:08:18,600 --> 01:08:19,920
questions and having interesting

1446
01:08:19,920 --> 01:08:22,738
discussion makes my life as the session

1447
01:08:22,738 --> 01:08:25,519
chair very easy

1448
01:08:29,600 --> 01:08:32,040
so hi everyone

1449
01:08:32,040 --> 01:08:34,979
good afternoon so um I will talk about

1450
01:08:34,979 --> 01:08:38,279
today some about a very specific uh open

1451
01:08:38,279 --> 01:08:40,259
problem in deep learning basic sidechain

1452
01:08:40,259 --> 01:08:41,880
analysis that is feature selection that

1453
01:08:41,880 --> 01:08:44,219
we believe it's not really well covered

1454
01:08:44,219 --> 01:08:46,500
in the recent research

1455
01:08:46,500 --> 01:08:49,020
so uh the context of this work is

1456
01:08:49,020 --> 01:08:51,839
profile inside Channel analysis like it

1457
01:08:51,839 --> 01:08:53,939
was we saw many presentations before

1458
01:08:53,939 --> 01:08:56,399
already describing this context so I

1459
01:08:56,399 --> 01:08:58,560
will skip this part

1460
01:08:58,560 --> 01:09:00,899
um so we had many assumptions for this

1461
01:09:00,899 --> 01:09:03,660
work and the first one is about the

1462
01:09:03,660 --> 01:09:05,939
different it's about assumptions on the

1463
01:09:05,939 --> 01:09:09,120
attack itself like we have a white on

1464
01:09:09,120 --> 01:09:10,620
the extreme sides we have white box

1465
01:09:10,620 --> 01:09:12,779
profiling analysis we have black box

1466
01:09:12,779 --> 01:09:15,839
profiling attacks and uh although uh

1467
01:09:15,839 --> 01:09:17,460
what was the worst case security

1468
01:09:17,460 --> 01:09:20,640
evaluation is possible with a white box

1469
01:09:20,640 --> 01:09:23,160
profiling attacks on the on the black

1470
01:09:23,160 --> 01:09:25,620
box side this is not really possible and

1471
01:09:25,620 --> 01:09:27,960
then this how you differentiate both

1472
01:09:27,960 --> 01:09:30,000
types of evaluations is on the

1473
01:09:30,000 --> 01:09:31,500
assumptions and the knowledge that the

1474
01:09:31,500 --> 01:09:34,040
attacker is considered at the attacker

1475
01:09:34,040 --> 01:09:36,299
possessed so for example for the white

1476
01:09:36,299 --> 01:09:38,399
box analysis you have knowledge on the

1477
01:09:38,399 --> 01:09:41,100
randomness for example the masks and

1478
01:09:41,100 --> 01:09:42,600
also the source code the of the

1479
01:09:42,600 --> 01:09:45,238
implementation while in the Black Box

1480
01:09:45,238 --> 01:09:47,279
scenario you don't assume any previous

1481
01:09:47,279 --> 01:09:49,319
knowledge about a Target basically what

1482
01:09:49,319 --> 01:09:52,859
is public available only so white box

1483
01:09:52,859 --> 01:09:54,719
profiling attacks are fast and efficient

1484
01:09:54,719 --> 01:09:58,080
in order to uh describe the security of

1485
01:09:58,080 --> 01:10:01,020
the implementation and black box attacks

1486
01:10:01,020 --> 01:10:03,300
are more difficult to implement and

1487
01:10:03,300 --> 01:10:04,980
subject to many mistakes for many

1488
01:10:04,980 --> 01:10:06,900
different reasons and this is also

1489
01:10:06,900 --> 01:10:08,280
related to the profiling model

1490
01:10:08,280 --> 01:10:11,820
reliability so as an example you would

1491
01:10:11,820 --> 01:10:13,739
do it for example you choose deeper

1492
01:10:13,739 --> 01:10:15,300
neural networks to do a profiling attack

1493
01:10:15,300 --> 01:10:17,580
but if you get better results in these

1494
01:10:17,580 --> 01:10:18,960
things that you don't or good results

1495
01:10:18,960 --> 01:10:20,820
depending on the perspective if you

1496
01:10:20,820 --> 01:10:22,860
don't break the target then you need to

1497
01:10:22,860 --> 01:10:24,179
understand if the target is really

1498
01:10:24,179 --> 01:10:26,219
secure or if the profiling attack is

1499
01:10:26,219 --> 01:10:28,080
wrong

1500
01:10:28,080 --> 01:10:30,060
so there is this gap between these two

1501
01:10:30,060 --> 01:10:32,699
uh scenarios and one thing that we ask

1502
01:10:32,699 --> 01:10:34,500
ourselves is uh if we apply deep

1503
01:10:34,500 --> 01:10:36,360
learning attacks which provides in

1504
01:10:36,360 --> 01:10:38,820
practice very good results how the IP

1505
01:10:38,820 --> 01:10:41,100
parameter search effort actually covers

1506
01:10:41,100 --> 01:10:44,880
uh this Gap can not make of course white

1507
01:10:44,880 --> 01:10:47,159
box and black box equal in performance

1508
01:10:47,159 --> 01:10:49,380
but reduces the gap between those two

1509
01:10:49,380 --> 01:10:51,360
approaches

1510
01:10:51,360 --> 01:10:54,120
and obviously on the white box scenario

1511
01:10:54,120 --> 01:10:56,159
we have feature selection and on the

1512
01:10:56,159 --> 01:10:57,900
Black Box scenario we don't have feature

1513
01:10:57,900 --> 01:10:59,820
selection IDs

1514
01:10:59,820 --> 01:11:01,920
so the second motivation is about the

1515
01:11:01,920 --> 01:11:04,140
model size so

1516
01:11:04,140 --> 01:11:06,360
um to define the neural network you want

1517
01:11:06,360 --> 01:11:09,179
to implement so it's becoming more and

1518
01:11:09,179 --> 01:11:11,040
more large the options for for

1519
01:11:11,040 --> 01:11:13,260
describing to implement the attack and

1520
01:11:13,260 --> 01:11:16,020
when we go for for example small models

1521
01:11:16,020 --> 01:11:18,480
and usually this is the this is selected

1522
01:11:18,480 --> 01:11:20,760
based on small data sets as well so

1523
01:11:20,760 --> 01:11:23,400
small models uh they have their own

1524
01:11:23,400 --> 01:11:25,580
benefits for example they are implicitly

1525
01:11:25,580 --> 01:11:27,600
regularized so they are less prone to

1526
01:11:27,600 --> 01:11:29,520
overheating but they are more efficient

1527
01:11:29,520 --> 01:11:33,300
on the small uh data sets and I would

1528
01:11:33,300 --> 01:11:34,860
like to refer to this very nice work

1529
01:11:34,860 --> 01:11:38,340
from vouchers from 2020 which this puts

1530
01:11:38,340 --> 01:11:40,800
a very nice perspective on how what is

1531
01:11:40,800 --> 01:11:42,360
the capacity of small models against

1532
01:11:42,360 --> 01:11:44,640
several data sets

1533
01:11:44,640 --> 01:11:47,040
so also small models together with

1534
01:11:47,040 --> 01:11:49,800
optimization methods deliver also very

1535
01:11:49,800 --> 01:11:52,020
good results so next year's we saw

1536
01:11:52,020 --> 01:11:54,659
papers uh or in the recent years we saw

1537
01:11:54,659 --> 01:11:56,280
papers using hyper parameter search

1538
01:11:56,280 --> 01:11:58,440
together with the small neural networks

1539
01:11:58,440 --> 01:12:01,500
and they were able to get even better

1540
01:12:01,500 --> 01:12:04,679
results also on small data sets not

1541
01:12:04,679 --> 01:12:08,040
really large scale data sets

1542
01:12:08,040 --> 01:12:10,620
and finally we saw Publications for

1543
01:12:10,620 --> 01:12:13,920
example last year a paper appear shown

1544
01:12:13,920 --> 01:12:17,040
uh showing very large data sets wrote

1545
01:12:17,040 --> 01:12:20,600
measurements on some devices ending

1546
01:12:20,600 --> 01:12:23,219
consider a very long very big neural

1547
01:12:23,219 --> 01:12:25,380
networks for that so the attacks were

1548
01:12:25,380 --> 01:12:28,040
crazy efficient very few traces like

1549
01:12:28,040 --> 01:12:30,900
zero one to ten traces for some data

1550
01:12:30,900 --> 01:12:33,080
sets were enough to break the Target on

1551
01:12:33,080 --> 01:12:35,280
applying them on the road trade so it's

1552
01:12:35,280 --> 01:12:37,620
quite impressive to see but the the

1553
01:12:37,620 --> 01:12:40,860
models are very very large so this also

1554
01:12:40,860 --> 01:12:42,840
puts a question like is it necessary to

1555
01:12:42,840 --> 01:12:44,880
improve the model size if we improve

1556
01:12:44,880 --> 01:12:48,300
this the scale of the data set

1557
01:12:48,300 --> 01:12:51,120
so feature selection basically uh

1558
01:12:51,120 --> 01:12:53,159
yeah so we need to select in the main

1559
01:12:53,159 --> 01:12:55,080
points of interest in our attack so if

1560
01:12:55,080 --> 01:12:57,000
we can if we go for the more black box

1561
01:12:57,000 --> 01:12:59,699
evaluations but we also select small

1562
01:12:59,699 --> 01:13:01,980
intervals which is also done mostly in

1563
01:13:01,980 --> 01:13:05,159
the recent Works uh we don't see that

1564
01:13:05,159 --> 01:13:07,800
even a small neural network we struggle

1565
01:13:07,800 --> 01:13:09,900
so much in finding the second order

1566
01:13:09,900 --> 01:13:11,699
leakages and first order musket

1567
01:13:11,699 --> 01:13:13,679
implementation for example because we're

1568
01:13:13,679 --> 01:13:15,900
going to have several points of interest

1569
01:13:15,900 --> 01:13:17,760
leak information and then to

1570
01:13:17,760 --> 01:13:19,380
differentiate between what is noise what

1571
01:13:19,380 --> 01:13:21,060
is leakage doesn't become a very

1572
01:13:21,060 --> 01:13:23,219
difficult task and for people that

1573
01:13:23,219 --> 01:13:25,860
practice on this topic it we see that

1574
01:13:25,860 --> 01:13:27,480
it's not really difficult to find the

1575
01:13:27,480 --> 01:13:29,400
efficient neural network against data

1576
01:13:29,400 --> 01:13:32,219
public available data sets

1577
01:13:32,219 --> 01:13:35,159
on the other hand if we do uh if we

1578
01:13:35,159 --> 01:13:37,860
apply if we take a much longer interval

1579
01:13:37,860 --> 01:13:39,900
where the location of the points of

1580
01:13:39,900 --> 01:13:41,760
interest is related to our intermediate

1581
01:13:41,760 --> 01:13:44,100
variable are more cons are concentrated

1582
01:13:44,100 --> 01:13:45,840
in almost very specific points of

1583
01:13:45,840 --> 01:13:48,480
interest then we see a situation when

1584
01:13:48,480 --> 01:13:51,120
the neural network needs to uh find

1585
01:13:51,120 --> 01:13:52,980
these points of interest and discard the

1586
01:13:52,980 --> 01:13:55,500
rest of the uh of the trace so if the

1587
01:13:55,500 --> 01:13:57,540
leakage is located in some specific

1588
01:13:57,540 --> 01:13:59,520
points then we start to question

1589
01:13:59,520 --> 01:14:02,280
ourselves if small models are enough or

1590
01:14:02,280 --> 01:14:05,820
not for uh for the task and also if we

1591
01:14:05,820 --> 01:14:08,880
ideally here or intuitively we would go

1592
01:14:08,880 --> 01:14:11,100
for more for larger data sets because

1593
01:14:11,100 --> 01:14:14,159
this would provide more capacity but on

1594
01:14:14,159 --> 01:14:16,560
the other hand larger data sets a new

1595
01:14:16,560 --> 01:14:19,860
networks sorry they are they are they

1596
01:14:19,860 --> 01:14:22,080
can easily over fit depending on the

1597
01:14:22,080 --> 01:14:24,239
size of the data set and in terms of

1598
01:14:24,239 --> 01:14:26,400
number of measurements and therefore you

1599
01:14:26,400 --> 01:14:30,000
have another problem a problem to solve

1600
01:14:30,000 --> 01:14:32,340
so one way to to defeat the Regal of

1601
01:14:32,340 --> 01:14:34,020
your fitting obviously is regularizing

1602
01:14:34,020 --> 01:14:36,239
the model in a way that you reduce its

1603
01:14:36,239 --> 01:14:38,400
capacity either by reducing its size or

1604
01:14:38,400 --> 01:14:41,219
by applying some explicit regularization

1605
01:14:41,219 --> 01:14:44,100
techniques like for example Dropout or

1606
01:14:44,100 --> 01:14:47,540
uh l102 early stop in debt augmentation

1607
01:14:47,540 --> 01:14:51,420
and so what we wanted to do the model to

1608
01:14:51,420 --> 01:14:53,820
do using the regularization is to make

1609
01:14:53,820 --> 01:14:56,640
it more insensitive to noise or all the

1610
01:14:56,640 --> 01:14:58,500
relevant features contained in the trace

1611
01:14:58,500 --> 01:15:01,159
and make it more uh

1612
01:15:01,159 --> 01:15:03,659
looking only to the relevant ones that

1613
01:15:03,659 --> 01:15:05,699
appear in the in the measurements

1614
01:15:05,699 --> 01:15:08,100
so in reality feature selection either

1615
01:15:08,100 --> 01:15:10,920
in academic or in Industry are done they

1616
01:15:10,920 --> 01:15:12,780
are done in different ways so

1617
01:15:12,780 --> 01:15:14,640
if we considered all the Publications

1618
01:15:14,640 --> 01:15:16,980
that we have recently uh in the previous

1619
01:15:16,980 --> 01:15:18,060
works

1620
01:15:18,060 --> 01:15:20,400
we see that feature selection is usually

1621
01:15:20,400 --> 01:15:22,560
take into account because we usually

1622
01:15:22,560 --> 01:15:24,920
attack data sets that are already

1623
01:15:24,920 --> 01:15:27,719
optimized for in terms of interval of

1624
01:15:27,719 --> 01:15:30,060
selection so I can give an example of

1625
01:15:30,060 --> 01:15:33,000
ascad data set that is massively used in

1626
01:15:33,000 --> 01:15:35,820
our previous works and we attack a very

1627
01:15:35,820 --> 01:15:37,860
specific interval which already is

1628
01:15:37,860 --> 01:15:39,480
selected based on the knowledge of some

1629
01:15:39,480 --> 01:15:41,640
of the randomness

1630
01:15:41,640 --> 01:15:43,739
uh but there is a reason for that it's

1631
01:15:43,739 --> 01:15:45,360
because we are more interested in the

1632
01:15:45,360 --> 01:15:47,100
method itself of the attack and not

1633
01:15:47,100 --> 01:15:49,080
really on this practicality of the

1634
01:15:49,080 --> 01:15:51,540
attack so we invest more time on the

1635
01:15:51,540 --> 01:15:54,300
optimization algorithms metrics loss

1636
01:15:54,300 --> 01:15:57,120
functions Etc rather than the Practical

1637
01:15:57,120 --> 01:16:00,179
aspects and for industry or real world

1638
01:16:00,179 --> 01:16:02,340
targets uh I can say that this

1639
01:16:02,340 --> 01:16:04,080
pre-processing part of feature selection

1640
01:16:04,080 --> 01:16:06,659
will never be ignored because it's not

1641
01:16:06,659 --> 01:16:09,360
smart just to take a row measurement and

1642
01:16:09,360 --> 01:16:11,219
throw away into a neural network so

1643
01:16:11,219 --> 01:16:13,020
usually we're going to do some

1644
01:16:13,020 --> 01:16:15,120
pre-processing steps because it's it's

1645
01:16:15,120 --> 01:16:17,179
intuitive to do them

1646
01:16:17,179 --> 01:16:19,679
and pre-processing can be difficult

1647
01:16:19,679 --> 01:16:23,340
sometimes and so uh you can trim as much

1648
01:16:23,340 --> 01:16:25,020
as you can you can optimize as much as

1649
01:16:25,020 --> 01:16:26,880
you can but there is a point where you

1650
01:16:26,880 --> 01:16:29,520
cannot do it anymore and then this is

1651
01:16:29,520 --> 01:16:31,140
why deep neural networks also became

1652
01:16:31,140 --> 01:16:33,540
more popular in the domain because they

1653
01:16:33,540 --> 01:16:36,000
are assuming to bypass uh the problems

1654
01:16:36,000 --> 01:16:38,219
of feature selection and but to what

1655
01:16:38,219 --> 01:16:40,620
extent this is really true we don't know

1656
01:16:40,620 --> 01:16:42,420
exactly but then we started to

1657
01:16:42,420 --> 01:16:44,340
investigate in this work a bit more with

1658
01:16:44,340 --> 01:16:46,380
more experiments

1659
01:16:46,380 --> 01:16:49,140
so for that we defined the three uh the

1660
01:16:49,140 --> 01:16:51,719
scenarios the first one is similar to

1661
01:16:51,719 --> 01:16:53,880
white box evaluations where we select

1662
01:16:53,880 --> 01:16:56,040
points of interest based on leakage

1663
01:16:56,040 --> 01:16:58,739
assessment and masks are known for that

1664
01:16:58,739 --> 01:17:01,500
so the second scenario is optimized

1665
01:17:01,500 --> 01:17:03,360
points of interest which which you're

1666
01:17:03,360 --> 01:17:04,500
going to see it's similar to previous

1667
01:17:04,500 --> 01:17:07,620
works so we use a

1668
01:17:07,620 --> 01:17:10,320
yeah so masks are known for the

1669
01:17:10,320 --> 01:17:12,960
selection of the intervals or for the

1670
01:17:12,960 --> 01:17:15,000
feature selection on the second case but

1671
01:17:15,000 --> 01:17:17,100
there is one data set that we use that

1672
01:17:17,100 --> 01:17:18,840
we don't have access to the masks and

1673
01:17:18,840 --> 01:17:20,460
still we were able to define the

1674
01:17:20,460 --> 01:17:22,920
intervals for the the data set

1675
01:17:22,920 --> 01:17:24,719
and finally the most interesting

1676
01:17:24,719 --> 01:17:27,120
scenario which is completely back black

1677
01:17:27,120 --> 01:17:30,060
box where we don't know the masks and we

1678
01:17:30,060 --> 01:17:31,920
attack all available features or

1679
01:17:31,920 --> 01:17:33,840
intervals where the attack should be

1680
01:17:33,840 --> 01:17:35,640
obvious discarding some of the parts

1681
01:17:35,640 --> 01:17:37,380
that are Irrelevant for example we

1682
01:17:37,380 --> 01:17:39,659
attack the first round of the s-box and

1683
01:17:39,659 --> 01:17:41,940
then we simply discard the rest of the

1684
01:17:41,940 --> 01:17:44,900
Xbox rounds

1685
01:17:44,900 --> 01:17:47,699
so feature selection what we usually do

1686
01:17:47,699 --> 01:17:51,060
is when we have

1687
01:17:51,060 --> 01:17:53,400
a different data sets to attack we go

1688
01:17:53,400 --> 01:17:55,460
for different hyper parameter tuning

1689
01:17:55,460 --> 01:17:58,199
strategies and also depending on feature

1690
01:17:58,199 --> 01:18:00,360
selection we also go for different hyper

1691
01:18:00,360 --> 01:18:02,159
parameter tuning strategies but in this

1692
01:18:02,159 --> 01:18:04,640
paper what we do is to see how the same

1693
01:18:04,640 --> 01:18:06,780
parameter tuning strategy the same

1694
01:18:06,780 --> 01:18:08,880
configuration the same range can be

1695
01:18:08,880 --> 01:18:11,880
applied among uh for multiple scenarios

1696
01:18:11,880 --> 01:18:13,980
and multiple scenarios I mean different

1697
01:18:13,980 --> 01:18:16,800
data sets leakage models model types I

1698
01:18:16,800 --> 01:18:18,480
mean convolutional neural networks

1699
01:18:18,480 --> 01:18:21,060
multi-layer perception and feature

1700
01:18:21,060 --> 01:18:23,400
selection scenarios and for each case we

1701
01:18:23,400 --> 01:18:26,940
we search it for up to 500 models and

1702
01:18:26,940 --> 01:18:29,040
except for the first scenario where we

1703
01:18:29,040 --> 01:18:31,460
the the to find good models were not

1704
01:18:31,460 --> 01:18:35,219
very difficult so we searched for 128.

1705
01:18:35,219 --> 01:18:37,440
and the total number of experiments in

1706
01:18:37,440 --> 01:18:40,500
this paper is around 50 000 to cover all

1707
01:18:40,500 --> 01:18:44,420
the possible scenarios that we want

1708
01:18:44,460 --> 01:18:46,500
and we implemented Grid in random search

1709
01:18:46,500 --> 01:18:49,440
grid search for the white box case and

1710
01:18:49,440 --> 01:18:51,000
random search for the other two

1711
01:18:51,000 --> 01:18:54,000
scenarios so but even but to make it

1712
01:18:54,000 --> 01:18:56,340
more realistically we defined very large

1713
01:18:56,340 --> 01:18:58,920
search space you know in a way that what

1714
01:18:58,920 --> 01:19:01,260
we cover with our search is really

1715
01:19:01,260 --> 01:19:03,060
irrelevant amount compared to the other

1716
01:19:03,060 --> 01:19:07,320
possibilities and this we keep kept

1717
01:19:07,320 --> 01:19:09,420
using small neural networks where we

1718
01:19:09,420 --> 01:19:11,400
allow them to go up to eight hidden

1719
01:19:11,400 --> 01:19:13,739
layers only

1720
01:19:13,739 --> 01:19:16,440
and to the valuation metrics to select

1721
01:19:16,440 --> 01:19:18,300
the model from the random search from

1722
01:19:18,300 --> 01:19:20,400
the upper parameter search was guessing

1723
01:19:20,400 --> 01:19:22,020
entropy I mean the number of attack

1724
01:19:22,020 --> 01:19:25,320
traces uh necessary to reach guessing

1725
01:19:25,320 --> 01:19:27,120
entropy equal to zero or to one

1726
01:19:27,120 --> 01:19:29,219
depending how with the scale we're going

1727
01:19:29,219 --> 01:19:31,860
to use and a perceived information for

1728
01:19:31,860 --> 01:19:34,739
the uh white box the first scenario

1729
01:19:34,739 --> 01:19:36,420
which is white box

1730
01:19:36,420 --> 01:19:38,040
and

1731
01:19:38,040 --> 01:19:40,320
we use the perceived information in the

1732
01:19:40,320 --> 01:19:42,900
first case and because it's a new metric

1733
01:19:42,900 --> 01:19:46,080
and also we saw that the results uh the

1734
01:19:46,080 --> 01:19:47,219
that are

1735
01:19:47,219 --> 01:19:49,560
that are computed with equation there on

1736
01:19:49,560 --> 01:19:50,940
the right are quite precise they're

1737
01:19:50,940 --> 01:19:53,580
quite uh

1738
01:19:53,580 --> 01:19:56,760
I would say it yeah accurate for what we

1739
01:19:56,760 --> 01:19:57,900
are measuring if compared to the

1740
01:19:57,900 --> 01:19:59,880
guessing entropy

1741
01:19:59,880 --> 01:20:01,620
and then for the perceived information

1742
01:20:01,620 --> 01:20:03,420
we estimate the amount of traces

1743
01:20:03,420 --> 01:20:07,340
required for asserting success rate

1744
01:20:07,679 --> 01:20:09,960
so we did experiments on four data sets

1745
01:20:09,960 --> 01:20:13,920
ascad version one uh on fixed key and

1746
01:20:13,920 --> 01:20:17,580
random keys and dpiv 4.2 DPA from DPA

1747
01:20:17,580 --> 01:20:19,080
context data set

1748
01:20:19,080 --> 01:20:22,739
and also chess CTF 2018. so the only

1749
01:20:22,739 --> 01:20:24,960
data set that contains the same keys and

1750
01:20:24,960 --> 01:20:26,880
defer in the attack and the profiling

1751
01:20:26,880 --> 01:20:29,159
phase is the Ascot with fixed key there

1752
01:20:29,159 --> 01:20:31,320
but all all the rest of the data sets

1753
01:20:31,320 --> 01:20:32,699
they have different keys on both

1754
01:20:32,699 --> 01:20:36,060
profiling and uh validation or attack

1755
01:20:36,060 --> 01:20:38,280
phases so they are also first starting

1756
01:20:38,280 --> 01:20:41,040
musket software implementations and we

1757
01:20:41,040 --> 01:20:43,320
always so we always search for a model

1758
01:20:43,320 --> 01:20:46,440
for uh for what single key byte and then

1759
01:20:46,440 --> 01:20:48,420
we extend the same model on the rest of

1760
01:20:48,420 --> 01:20:50,780
the key

1761
01:20:51,840 --> 01:20:53,940
so there is I will share some results

1762
01:20:53,940 --> 01:20:56,580
from all the scenarios we tried so first

1763
01:20:56,580 --> 01:20:59,280
for the refined points of interest which

1764
01:20:59,280 --> 01:21:01,860
is uh what in the white box case

1765
01:21:01,860 --> 01:21:05,520
and here we saw that uh we tested it we

1766
01:21:05,520 --> 01:21:07,560
applied the on the all data sets I will

1767
01:21:07,560 --> 01:21:11,340
show for two of them and we we applied a

1768
01:21:11,340 --> 01:21:13,920
gaussian noise on the traces to see how

1769
01:21:13,920 --> 01:21:16,679
uh white box and the hyper parameter

1770
01:21:16,679 --> 01:21:18,960
search can actually work how the

1771
01:21:18,960 --> 01:21:21,420
resilience of the of the of the

1772
01:21:21,420 --> 01:21:22,980
implementation when it's more noisy

1773
01:21:22,980 --> 01:21:25,320
against hyper parameter search we saw of

1774
01:21:25,320 --> 01:21:27,600
course that more noisy the data set

1775
01:21:27,600 --> 01:21:30,420
becomes more tricky we need but what we

1776
01:21:30,420 --> 01:21:31,980
saw is that we keep increasing the

1777
01:21:31,980 --> 01:21:33,659
number of points of interest

1778
01:21:33,659 --> 01:21:36,540
for the attack then the noise really

1779
01:21:36,540 --> 01:21:38,880
doesn't care it doesn't affect so much

1780
01:21:38,880 --> 01:21:41,520
the performance of the attack

1781
01:21:41,520 --> 01:21:44,460
so and on the white axis here what you

1782
01:21:44,460 --> 01:21:46,560
see is that that metric derived from

1783
01:21:46,560 --> 01:21:47,840
perceived information

1784
01:21:47,840 --> 01:21:50,699
and we use this in this scenario because

1785
01:21:50,699 --> 01:21:52,380
when perceived information is positive

1786
01:21:52,380 --> 01:21:54,960
then we saw that the the amount of Trace

1787
01:21:54,960 --> 01:21:56,880
is estimated using this metric is quite

1788
01:21:56,880 --> 01:21:58,860
aligned with the amount of traces

1789
01:21:58,860 --> 01:22:02,340
estimated using guessing entropy

1790
01:22:02,340 --> 01:22:04,800
and so the purple line there is gaussian

1791
01:22:04,800 --> 01:22:08,159
template attacks and with what we saw

1792
01:22:08,159 --> 01:22:09,900
interesting is that gaussian template

1793
01:22:09,900 --> 01:22:11,580
attack sometimes is better than the

1794
01:22:11,580 --> 01:22:13,199
convolutional neural networks and

1795
01:22:13,199 --> 01:22:14,940
multi-layer perception

1796
01:22:14,940 --> 01:22:17,400
when we have a reduced amount of models

1797
01:22:17,400 --> 01:22:19,020
when we go for more and more and more of

1798
01:22:19,020 --> 01:22:21,120
those we always find somebody that has

1799
01:22:21,120 --> 01:22:24,000
more uh capacity in terms of number of

1800
01:22:24,000 --> 01:22:27,120
required traces and we always find some

1801
01:22:27,120 --> 01:22:29,280
results showing that the key is possible

1802
01:22:29,280 --> 01:22:31,020
to be recovered with a single

1803
01:22:31,020 --> 01:22:32,159
measurement

1804
01:22:32,159 --> 01:22:34,380
here on the behavior for the two which

1805
01:22:34,380 --> 01:22:36,780
is a little bit more noisy data set it

1806
01:22:36,780 --> 01:22:39,659
contains less uh points of interest to

1807
01:22:39,659 --> 01:22:42,600
be selected that are very leaky and we

1808
01:22:42,600 --> 01:22:45,540
saw that again deep in your networks uh

1809
01:22:45,540 --> 01:22:47,940
were in in general more efficient than

1810
01:22:47,940 --> 01:22:50,400
gaussian template attacks but the amount

1811
01:22:50,400 --> 01:22:52,080
of traces required there's soil is more

1812
01:22:52,080 --> 01:22:54,300
that we cannot say more efficient but

1813
01:22:54,300 --> 01:22:57,500
because both methods work quite well

1814
01:22:57,500 --> 01:23:00,540
so we also did the optimize points of

1815
01:23:00,540 --> 01:23:03,360
interest mostly for a reference because

1816
01:23:03,360 --> 01:23:06,120
this is a these are data sets largely

1817
01:23:06,120 --> 01:23:08,760
considered in previous work and from the

1818
01:23:08,760 --> 01:23:10,500
amount of from the performance that we

1819
01:23:10,500 --> 01:23:12,420
found with iPad with our hyper parameter

1820
01:23:12,420 --> 01:23:13,620
search process

1821
01:23:13,620 --> 01:23:16,460
we see that we are actually

1822
01:23:16,460 --> 01:23:18,659
aligned with the state of the art it's

1823
01:23:18,659 --> 01:23:21,719
just to set some reference so our IP

1824
01:23:21,719 --> 01:23:23,640
parameter search is not really biased to

1825
01:23:23,640 --> 01:23:25,440
make better or worse results in some of

1826
01:23:25,440 --> 01:23:26,400
the case

1827
01:23:26,400 --> 01:23:29,580
and so we could see for example we sell

1828
01:23:29,580 --> 01:23:32,280
pre-selecting a specific interval for

1829
01:23:32,280 --> 01:23:35,400
attack we cannot reduce the we cannot

1830
01:23:35,400 --> 01:23:38,100
reduce the amount of traces below for

1831
01:23:38,100 --> 01:23:42,540
example 78 for cnns on ascat random keys

1832
01:23:42,540 --> 01:23:44,820
but we were able to get with a single

1833
01:23:44,820 --> 01:23:47,520
measurement on DPA before the two so

1834
01:23:47,520 --> 01:23:49,440
indicating that deep neural networks

1835
01:23:49,440 --> 01:23:53,820
although can actually do quite well even

1836
01:23:53,820 --> 01:23:56,340
when the the interval of the feature

1837
01:23:56,340 --> 01:23:59,040
selection that we are doing includes

1838
01:23:59,040 --> 01:24:01,140
more noisy samples

1839
01:24:01,140 --> 01:24:03,960
and for chess CTF which is um we had

1840
01:24:03,960 --> 01:24:06,060
much less measurement I think 30 000

1841
01:24:06,060 --> 01:24:09,780
measurements for profiling than uh the

1842
01:24:09,780 --> 01:24:11,460
performance the model was not good for

1843
01:24:11,460 --> 01:24:13,860
all scenarios but for Hemingway to

1844
01:24:13,860 --> 01:24:17,040
moderate Works quite well

1845
01:24:17,040 --> 01:24:20,460
so now finally for the last uh scenario

1846
01:24:20,460 --> 01:24:22,739
here on the non-optimized points of

1847
01:24:22,739 --> 01:24:24,900
interest which is more aligned with

1848
01:24:24,900 --> 01:24:28,679
black box evaluations uh we we saw this

1849
01:24:28,679 --> 01:24:30,719
was very surprising because this was the

1850
01:24:30,719 --> 01:24:32,699
starting point of the paper we started

1851
01:24:32,699 --> 01:24:35,820
directly working on the all this uh on

1852
01:24:35,820 --> 01:24:39,239
the road traces and then we saw that uh

1853
01:24:39,239 --> 01:24:42,420
for ascad random Keys specifically we

1854
01:24:42,420 --> 01:24:45,780
were able to get the key in this in the

1855
01:24:45,780 --> 01:24:47,239
attack phase with the single measurement

1856
01:24:47,239 --> 01:24:50,520
using a single uh

1857
01:24:50,520 --> 01:24:52,980
single heating layer MLP containing 500

1858
01:24:52,980 --> 01:24:54,000
euros

1859
01:24:54,000 --> 01:24:57,540
and this was uh so and when we compared

1860
01:24:57,540 --> 01:25:00,420
to the previous work where uh that I

1861
01:25:00,420 --> 01:25:01,380
mentioned at the beginning of the

1862
01:25:01,380 --> 01:25:03,780
presentation that Implement a very large

1863
01:25:03,780 --> 01:25:06,540
neural network containing up to 30 or 40

1864
01:25:06,540 --> 01:25:09,060
layers then this there is quite a big

1865
01:25:09,060 --> 01:25:11,280
difference so with a single hidden layer

1866
01:25:11,280 --> 01:25:12,780
here you can recover the queue with a

1867
01:25:12,780 --> 01:25:14,340
single measurement so

1868
01:25:14,340 --> 01:25:16,440
that was quite interesting

1869
01:25:16,440 --> 01:25:19,260
for Ascot fix key also we got with a

1870
01:25:19,260 --> 01:25:21,420
single measurement but this data set is

1871
01:25:21,420 --> 01:25:24,000
the same key on the both sides so I

1872
01:25:24,000 --> 01:25:26,280
might be interpreted differently if you

1873
01:25:26,280 --> 01:25:28,440
go into more details

1874
01:25:28,440 --> 01:25:31,860
or so with less leaky data sets with

1875
01:25:31,860 --> 01:25:34,620
Ascot is the Nascar is relatively more

1876
01:25:34,620 --> 01:25:38,219
leaky than DPA V4 and chess CTF so for

1877
01:25:38,219 --> 01:25:41,780
these two data sets here uh we saw also

1878
01:25:41,780 --> 01:25:44,760
very good results on chess CTF we could

1879
01:25:44,760 --> 01:25:47,100
get the key with eight measurements on

1880
01:25:47,100 --> 01:25:51,500
the road data set and um

1881
01:25:51,500 --> 01:25:54,900
yeah so but it worked better for uh for

1882
01:25:54,900 --> 01:25:57,060
Hemingway to leakage model so we

1883
01:25:57,060 --> 01:25:59,100
attacked the first part of the Implement

1884
01:25:59,100 --> 01:26:02,760
so this this implementation has somehow

1885
01:26:02,760 --> 01:26:03,480
um

1886
01:26:03,480 --> 01:26:06,600
600 000 points I think so we attack only

1887
01:26:06,600 --> 01:26:09,420
150 000 points which are the first part

1888
01:26:09,420 --> 01:26:12,780
of the measurement but without any this

1889
01:26:12,780 --> 01:26:14,760
can be considered as without feature

1890
01:26:14,760 --> 01:26:16,560
selection because this is obvious and

1891
01:26:16,560 --> 01:26:18,600
eight traces is already a very small

1892
01:26:18,600 --> 01:26:20,699
amount for DPA before the same random

1893
01:26:20,699 --> 01:26:23,219
search strategy we were able to get with

1894
01:26:23,219 --> 01:26:26,040
400 traces it's relatively much more

1895
01:26:26,040 --> 01:26:28,860
than the other data sets but considering

1896
01:26:28,860 --> 01:26:30,600
that we didn't change anything in the

1897
01:26:30,600 --> 01:26:32,040
search strategy this is quite an

1898
01:26:32,040 --> 01:26:33,540
advantage

1899
01:26:33,540 --> 01:26:35,940
and then we apply this the model found

1900
01:26:35,940 --> 01:26:38,280
for one key by to the rest of the uh of

1901
01:26:38,280 --> 01:26:41,699
the key bytes and we can see that we are

1902
01:26:41,699 --> 01:26:44,280
at least for one scenario uh one

1903
01:26:44,280 --> 01:26:46,080
combination of model type and leakage

1904
01:26:46,080 --> 01:26:48,960
model we were able to get the the full

1905
01:26:48,960 --> 01:26:51,480
key and specifically for ask at the

1906
01:26:51,480 --> 01:26:53,400
random Keys the third line on the table

1907
01:26:53,400 --> 01:26:55,380
you can see that we needed three

1908
01:26:55,380 --> 01:26:57,780
measurements for the full key recovery

1909
01:26:57,780 --> 01:26:59,040
on the

1910
01:26:59,040 --> 01:27:02,780
on the raw data sets

1911
01:27:03,300 --> 01:27:05,580
um yeah so we also did analysis on the

1912
01:27:05,580 --> 01:27:06,860
synchronization

1913
01:27:06,860 --> 01:27:09,960
uh and then we didn't change any hyper

1914
01:27:09,960 --> 01:27:12,000
parameter in the search in the search

1915
01:27:12,000 --> 01:27:13,800
space we just apply the same random

1916
01:27:13,800 --> 01:27:16,620
search and we were and we did with and

1917
01:27:16,620 --> 01:27:18,900
without data augmentation applying

1918
01:27:18,900 --> 01:27:21,000
random shifts during the training

1919
01:27:21,000 --> 01:27:24,179
and we saw also very good results for uh

1920
01:27:24,179 --> 01:27:27,420
for all data sets except for dpav4.2

1921
01:27:27,420 --> 01:27:29,880
where we reduce the get the entropy of

1922
01:27:29,880 --> 01:27:32,760
the key but not to want to consider as a

1923
01:27:32,760 --> 01:27:35,280
successful attack but for ascad random

1924
01:27:35,280 --> 01:27:37,500
keys for example we could take with 25

1925
01:27:37,500 --> 01:27:39,739
Trace

1926
01:27:39,739 --> 01:27:42,300
and obviously so this is a summary of a

1927
01:27:42,300 --> 01:27:45,600
result and that showed that with with

1928
01:27:45,600 --> 01:27:47,580
more relaxed feature selection scenario

1929
01:27:47,580 --> 01:27:49,380
more hyper parameter search you need

1930
01:27:49,380 --> 01:27:51,659
more effort you need on this side

1931
01:27:51,659 --> 01:27:54,060
uh when we have the synchronization for

1932
01:27:54,060 --> 01:27:57,300
in some cases only one percent on even

1933
01:27:57,300 --> 01:28:00,120
less of their of the search space gave

1934
01:28:00,120 --> 01:28:03,360
us a successful result and but for white

1935
01:28:03,360 --> 01:28:06,239
box sometimes it's 100 of the mods that

1936
01:28:06,239 --> 01:28:08,460
good positive results

1937
01:28:08,460 --> 01:28:11,880
and yeah so this emphasized that it's

1938
01:28:11,880 --> 01:28:13,920
very extensive random search is some

1939
01:28:13,920 --> 01:28:16,860
somehow necessary to to make very

1940
01:28:16,860 --> 01:28:19,980
efficient attacks on Black Box way

1941
01:28:19,980 --> 01:28:22,440
so in conclusion there so we concluded

1942
01:28:22,440 --> 01:28:24,780
more or less that yes deep learning can

1943
01:28:24,780 --> 01:28:26,880
skip feature selection which is uh

1944
01:28:26,880 --> 01:28:28,739
written in many places that you can skip

1945
01:28:28,739 --> 01:28:32,100
so somehow it can as long as we do long

1946
01:28:32,100 --> 01:28:33,840
hyper parameter search Pros so the

1947
01:28:33,840 --> 01:28:35,219
important the Opera parameter search

1948
01:28:35,219 --> 01:28:38,280
needs to be applied and we apply the

1949
01:28:38,280 --> 01:28:40,260
same search strategy on all the targets

1950
01:28:40,260 --> 01:28:43,800
all the all the situations and we would

1951
01:28:43,800 --> 01:28:46,620
say that black in black box scenario to

1952
01:28:46,620 --> 01:28:49,139
estimate the strength of the attacker

1953
01:28:49,139 --> 01:28:51,480
the learnability capacity is something

1954
01:28:51,480 --> 01:28:53,580
to be taken into account

1955
01:28:53,580 --> 01:28:56,340
and we emphasize also that small modules

1956
01:28:56,340 --> 01:28:59,820
and small CNN and MLP models uh can

1957
01:28:59,820 --> 01:29:02,820
defeat multiple targets uh first order

1958
01:29:02,820 --> 01:29:05,040
musket implementations even on very

1959
01:29:05,040 --> 01:29:07,980
large intervals or raw data but of

1960
01:29:07,980 --> 01:29:10,139
course I think we should invest on more

1961
01:29:10,139 --> 01:29:12,420
complex deep learning approach also

1962
01:29:12,420 --> 01:29:15,120
random search approach when we move to

1963
01:29:15,120 --> 01:29:17,760
more difficult targets for example a

1964
01:29:17,760 --> 01:29:20,159
Target Ascot V2 which is publicly

1965
01:29:20,159 --> 01:29:23,639
available and contains more protections

1966
01:29:23,639 --> 01:29:25,560
okay so thank you very much for

1967
01:29:25,560 --> 01:29:27,300
obtaining this presentation if you have

1968
01:29:27,300 --> 01:29:29,400
any questions I would like to have I

1969
01:29:29,400 --> 01:29:31,620
would be happy to answer

1970
01:29:31,620 --> 01:29:34,400
thank you

1971
01:29:39,600 --> 01:29:42,199
questions

1972
01:29:51,120 --> 01:29:52,620
thank you very much for a nice

1973
01:29:52,620 --> 01:29:54,840
presentation I have a question about the

1974
01:29:54,840 --> 01:29:58,800
the fixed um fixed key data set how did

1975
01:29:58,800 --> 01:30:01,380
you make sure did you choose did you

1976
01:30:01,380 --> 01:30:04,020
select the part of the trace that like

1977
01:30:04,020 --> 01:30:06,620
like yeah I mean the bias due to the key

1978
01:30:06,620 --> 01:30:09,719
did you try that method or

1979
01:30:09,719 --> 01:30:11,639
selecting part of the trace you mean

1980
01:30:11,639 --> 01:30:14,280
like yeah how how did you make sure that

1981
01:30:14,280 --> 01:30:17,040
it's not biased how I make sure that for

1982
01:30:17,040 --> 01:30:18,540
example there is no first order leakage

1983
01:30:18,540 --> 01:30:20,820
for example or that I don't learn only

1984
01:30:20,820 --> 01:30:22,739
the key I mean okay you are using

1985
01:30:22,739 --> 01:30:25,080
identity model yes yes so there is a

1986
01:30:25,080 --> 01:30:27,540
there is a an issue in that so this is

1987
01:30:27,540 --> 01:30:30,780
why I I included most results on the

1988
01:30:30,780 --> 01:30:33,300
desk at the random keys because I think

1989
01:30:33,300 --> 01:30:35,100
there is this is something to be

1990
01:30:35,100 --> 01:30:37,440
investigated but I believe if you learn

1991
01:30:37,440 --> 01:30:39,780
a data set that has a fixed key and then

1992
01:30:39,780 --> 01:30:41,520
apply attack phase on a fixed key your

1993
01:30:41,520 --> 01:30:43,560
own identity leakage mode you can only

1994
01:30:43,560 --> 01:30:45,060
learn that situation it doesn't

1995
01:30:45,060 --> 01:30:47,940
generalize to different keys so that

1996
01:30:47,940 --> 01:30:50,900
could be possibility

1997
01:30:56,790 --> 01:30:58,920
[Music]

1998
01:30:58,920 --> 01:31:01,639
yeah man

1999
01:31:07,620 --> 01:31:09,620
um

2000
01:31:13,340 --> 01:31:16,440
we went over time

2001
01:31:16,440 --> 01:31:19,580
I went

