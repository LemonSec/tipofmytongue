1
00:00:04,310 --> 00:00:08,399
[Music]

2
00:00:10,019 --> 00:00:20,680
Cadell my ass today's a todos good

3
00:00:15,520 --> 00:00:23,590
afternoon everyone welcome both to new

4
00:00:20,680 --> 00:00:28,560
faces who've just joined us now and to

5
00:00:23,590 --> 00:00:28,560
those who are here to the bitter end

6
00:00:29,160 --> 00:00:34,059
this is going to be the last session for

7
00:00:32,259 --> 00:00:37,750
the day today and we're going to be

8
00:00:34,059 --> 00:00:39,970
hearing to Santiago but before I

9
00:00:37,750 --> 00:00:45,900
introduce you to him let me just remind

10
00:00:39,970 --> 00:00:49,540
you that you can participate on Twitter

11
00:00:45,900 --> 00:00:53,760
with our hashtag we encourage you to

12
00:00:49,540 --> 00:00:58,900
share pictures comments contents

13
00:00:53,760 --> 00:01:01,599
whatever so let me give you a little bit

14
00:00:58,900 --> 00:01:04,979
of background air for the next session

15
00:01:01,600 --> 00:01:08,640
we have more and more threats to

16
00:01:04,979 --> 00:01:11,200
information security and this struggle

17
00:01:08,640 --> 00:01:15,220
makes it very clear that we need to

18
00:01:11,200 --> 00:01:15,940
further protect our data as much as

19
00:01:15,220 --> 00:01:19,690
possible

20
00:01:15,940 --> 00:01:23,110
well handling information can be

21
00:01:19,690 --> 00:01:25,390
enhanced through machine learning and

22
00:01:23,110 --> 00:01:29,200
we're going to be hearing from Santiago

23
00:01:25,390 --> 00:01:32,500
and Monday's promise about how to do so

24
00:01:29,200 --> 00:01:37,870
effectively Santiago is senior security

25
00:01:32,500 --> 00:01:43,090
analyst at cyber shock at Deloitte he

26
00:01:37,870 --> 00:01:45,490
has a master's and PhD on information

27
00:01:43,090 --> 00:01:48,040
science at the university of zaragoza

28
00:01:45,490 --> 00:01:51,158
i'm sure that you will make the most of

29
00:01:48,040 --> 00:01:55,540
this workshop decision and welcomes at

30
00:01:51,159 --> 00:01:58,020
you the floor is all yours I hope you

31
00:01:55,540 --> 00:02:02,770
can hear me okay

32
00:01:58,020 --> 00:02:04,360
let me thank you for joining me at this

33
00:02:02,770 --> 00:02:11,560
workshop I want to thank the organizers

34
00:02:04,360 --> 00:02:16,690
for inviting me back once again so let's

35
00:02:11,560 --> 00:02:20,500
start with the title of such which is

36
00:02:16,690 --> 00:02:27,970
machine learning our new ally for

37
00:02:20,500 --> 00:02:29,549
information security is this a true

38
00:02:27,970 --> 00:02:34,959
statement

39
00:02:29,549 --> 00:02:37,450
well not quite I say what is not quite

40
00:02:34,959 --> 00:02:41,080
right well the fact that machine

41
00:02:37,450 --> 00:02:43,839
learning is not quite a new ally it

42
00:02:41,080 --> 00:02:48,220
hasn't come up or popped up in the last

43
00:02:43,840 --> 00:02:49,840
couple of years it was there but because

44
00:02:48,220 --> 00:02:53,349
in the past we couldn't capture

45
00:02:49,840 --> 00:02:56,560
information or process it as fast as we

46
00:02:53,349 --> 00:03:00,518
can now what it hadn't boomed the way it

47
00:02:56,560 --> 00:03:03,459
it has more recently this is the table

48
00:03:00,519 --> 00:03:06,970
of contents for my presentation I hope

49
00:03:03,459 --> 00:03:08,130
you can see it ok from the back we'll do

50
00:03:06,970 --> 00:03:12,069
our best

51
00:03:08,130 --> 00:03:14,910
so I will start telling you about the

52
00:03:12,069 --> 00:03:18,450
current situation on cybersecurity

53
00:03:14,910 --> 00:03:23,079
connected with artificial intelligence

54
00:03:18,450 --> 00:03:29,679
and what is machine learning is a buzz

55
00:03:23,079 --> 00:03:31,690
word or a buzz expression we'll see what

56
00:03:29,680 --> 00:03:33,660
applications are possible right now

57
00:03:31,690 --> 00:03:39,639
we'll see something about linear

58
00:03:33,660 --> 00:03:43,600
regression or linear sorry regression we

59
00:03:39,639 --> 00:03:45,609
will see how algorithms can learn I

60
00:03:43,600 --> 00:03:48,459
don't know if learning is the word I

61
00:03:45,609 --> 00:03:51,389
think that is a buzz word in that case

62
00:03:48,459 --> 00:03:55,000
and then more advanced techniques like

63
00:03:51,389 --> 00:03:56,910
classification clustering anomaly

64
00:03:55,000 --> 00:04:00,570
detection and

65
00:03:56,910 --> 00:04:03,299
very simple techniques that you can

66
00:04:00,570 --> 00:04:07,709
probably use for basic security in any

67
00:04:03,300 --> 00:04:09,750
company I don't think we need this

68
00:04:07,710 --> 00:04:12,990
information because I've already been

69
00:04:09,750 --> 00:04:16,048
introduced to you before starting so

70
00:04:12,990 --> 00:04:19,200
let's see the current situation

71
00:04:16,048 --> 00:04:25,380
what is our starting point for cyber

72
00:04:19,200 --> 00:04:28,380
security here and now very briefly at

73
00:04:25,380 --> 00:04:33,570
least I would like to stress a number of

74
00:04:28,380 --> 00:04:36,659
points for instance even though in the

75
00:04:33,570 --> 00:04:39,570
last few years there there seem to be

76
00:04:36,660 --> 00:04:43,260
many more cyber attacks more companies

77
00:04:39,570 --> 00:04:46,080
impacted and at a larger scale that's

78
00:04:43,260 --> 00:04:48,539
what we hear but in fact threats have

79
00:04:46,080 --> 00:04:51,030
not changed that much in the last few

80
00:04:48,540 --> 00:04:53,400
years so that's good news right for us

81
00:04:51,030 --> 00:04:55,859
to detect them it is true that they're

82
00:04:53,400 --> 00:04:59,609
more large-scale more people and more

83
00:04:55,860 --> 00:05:03,180
companies are impacted but still malware

84
00:04:59,610 --> 00:05:05,460
run somewhere different types are the

85
00:05:03,180 --> 00:05:09,330
same very much the same or very similar

86
00:05:05,460 --> 00:05:12,330
at least I also wanted to tell you that

87
00:05:09,330 --> 00:05:16,710
the fact that we have more cyber attacks

88
00:05:12,330 --> 00:05:20,250
or more larger scale cyber attacks led

89
00:05:16,710 --> 00:05:24,330
by maybe mafias shall we call them

90
00:05:20,250 --> 00:05:29,640
rather than by loners as Charlie Miller

91
00:05:24,330 --> 00:05:32,940
says behind and over and about and above

92
00:05:29,640 --> 00:05:35,909
all of this what is at the end of it

93
00:05:32,940 --> 00:05:41,250
what is it well is the bottom line is

94
00:05:35,910 --> 00:05:43,919
money its economy that matters Miller

95
00:05:41,250 --> 00:05:46,410
said that there is a strong evidence

96
00:05:43,919 --> 00:05:49,520
that the best researchers in cyber

97
00:05:46,410 --> 00:05:53,520
security are now motivated more by

98
00:05:49,520 --> 00:05:56,909
monetary gain than by prestige that is

99
00:05:53,520 --> 00:06:00,960
how much money can I make

100
00:05:56,910 --> 00:06:04,260
out of this particular vulnerability

101
00:06:00,960 --> 00:06:06,400
that's why we have mafias that buy

102
00:06:04,260 --> 00:06:11,039
vulnerabilities they launch them

103
00:06:06,400 --> 00:06:13,140
large-scale and they make good money

104
00:06:11,040 --> 00:06:17,830
from doing so

105
00:06:13,140 --> 00:06:19,630
so before we start talking about machine

106
00:06:17,830 --> 00:06:22,409
learning I would like to tell you a

107
00:06:19,630 --> 00:06:25,719
little bit about artificial intelligence

108
00:06:22,410 --> 00:06:29,200
what is the difference between AI and M

109
00:06:25,720 --> 00:06:33,970
a ml sorry they're connected but they're

110
00:06:29,200 --> 00:06:39,640
not the same this is a report from the

111
00:06:33,970 --> 00:06:43,480
American government we see two quotes

112
00:06:39,640 --> 00:06:47,320
from 2018 from this year how artificial

113
00:06:43,480 --> 00:06:50,370
intelligence is changed the American

114
00:06:47,320 --> 00:06:54,849
lifestyle thanks to that we can now

115
00:06:50,370 --> 00:06:59,380
diagnose pathologies better financed

116
00:06:54,850 --> 00:07:01,600
work better transport and mobility has

117
00:06:59,380 --> 00:07:04,480
been enhanced for instance we have

118
00:07:01,600 --> 00:07:07,090
autonomous vehicles to prove us as the

119
00:07:04,480 --> 00:07:12,250
first quote up on the screen and that

120
00:07:07,090 --> 00:07:15,369
very report also said experts forecast

121
00:07:12,250 --> 00:07:18,550
that rapid process in the field of

122
00:07:15,370 --> 00:07:22,360
specialized artificial intelligence will

123
00:07:18,550 --> 00:07:26,680
continue but we don't think that in the

124
00:07:22,360 --> 00:07:31,830
coming 20 years cognitive capacity for

125
00:07:26,680 --> 00:07:35,020
AI will be the same as human cognitive

126
00:07:31,830 --> 00:07:37,510
capabilities so that means that

127
00:07:35,020 --> 00:07:39,729
artificial intelligence is improving

128
00:07:37,510 --> 00:07:42,760
many processes but we should still have

129
00:07:39,729 --> 00:07:45,789
our feet on the ground and understand

130
00:07:42,760 --> 00:07:50,210
that artificial intelligence is well

131
00:07:45,790 --> 00:07:53,670
below a human cognitive

132
00:07:50,210 --> 00:07:56,930
skills and capabilities not now and it

133
00:07:53,670 --> 00:08:01,710
won't be in the coming 20 years probably

134
00:07:56,930 --> 00:08:06,060
certain tasks will improve and some

135
00:08:01,710 --> 00:08:10,320
might even exceed a human capabilities

136
00:08:06,060 --> 00:08:15,390
but not for the global number of tasks

137
00:08:10,320 --> 00:08:22,170
so artificial intelligence and cyber

138
00:08:15,390 --> 00:08:24,960
security this is nothing new this is not

139
00:08:22,170 --> 00:08:27,300
a new field and no matter how some

140
00:08:24,960 --> 00:08:29,400
people are still reluctant when they

141
00:08:27,300 --> 00:08:32,570
hear about machine learning we have to

142
00:08:29,400 --> 00:08:36,329
acknowledge it is used here and now

143
00:08:32,570 --> 00:08:38,700
although not for global complete

144
00:08:36,330 --> 00:08:42,090
solutions but in traditional

145
00:08:38,700 --> 00:08:44,490
cybersecurity processes we can get a

146
00:08:42,090 --> 00:08:47,880
part of it and replace it by machine

147
00:08:44,490 --> 00:08:52,080
learning with a final global improvement

148
00:08:47,880 --> 00:08:56,040
of the process in American report said

149
00:08:52,080 --> 00:08:59,640
the following today's AI has important

150
00:08:56,040 --> 00:09:03,750
applications in cybersecurity and it is

151
00:08:59,640 --> 00:09:08,550
expected that it will play that it will

152
00:09:03,750 --> 00:09:12,230
play increasingly a row both in

153
00:09:08,550 --> 00:09:15,839
defensive and offensive cyber measures

154
00:09:12,230 --> 00:09:18,210
that means that in the coming years the

155
00:09:15,840 --> 00:09:20,660
offensive role will also be present not

156
00:09:18,210 --> 00:09:23,640
just offensively but offensively

157
00:09:20,660 --> 00:09:27,780
attackers using artificial intelligence

158
00:09:23,640 --> 00:09:31,560
and machine learning to develop new more

159
00:09:27,780 --> 00:09:35,100
harmful attacks I haven't seen a lot of

160
00:09:31,560 --> 00:09:39,719
literature connected with a offensive

161
00:09:35,100 --> 00:09:41,730
cybersecurity paired up with AI and AI

162
00:09:39,720 --> 00:09:47,520
now but that's certainly a field of

163
00:09:41,730 --> 00:09:51,130
research for future papers so now let's

164
00:09:47,520 --> 00:09:55,000
see the connection between AI and AI mal

165
00:09:51,130 --> 00:09:59,150
well more and more in recent years

166
00:09:55,000 --> 00:10:09,440
successful applications of artificial

167
00:09:59,150 --> 00:10:12,079
intelligence most all the time they're

168
00:10:09,440 --> 00:10:15,410
using machine machine learning or some

169
00:10:12,080 --> 00:10:16,730
sub disciplines within marshy machine

170
00:10:15,410 --> 00:10:18,890
learning that means that machine

171
00:10:16,730 --> 00:10:23,840
learning is part of artificial

172
00:10:18,890 --> 00:10:28,130
intelligence so which part of artificial

173
00:10:23,840 --> 00:10:30,950
intelligence does it equate to not all

174
00:10:28,130 --> 00:10:33,020
artificial intelligence issues can be

175
00:10:30,950 --> 00:10:34,940
solved through machine learning why

176
00:10:33,020 --> 00:10:38,990
because for machine learning we need

177
00:10:34,940 --> 00:10:43,820
data that subsets of artificial

178
00:10:38,990 --> 00:10:49,250
intelligence machine learning what it

179
00:10:43,820 --> 00:10:54,050
does is uses statistics and processing

180
00:10:49,250 --> 00:10:56,600
but always on data it needs data what is

181
00:10:54,050 --> 00:10:59,449
machine learning in fact well this is

182
00:10:56,600 --> 00:11:01,280
probably the most common definition for

183
00:10:59,450 --> 00:11:06,250
machine learning we see it up on the

184
00:11:01,280 --> 00:11:10,339
screen machine learning is a subset

185
00:11:06,250 --> 00:11:14,570
within artificial intelligence that help

186
00:11:10,340 --> 00:11:17,180
systems learn and improve automatically

187
00:11:14,570 --> 00:11:19,880
on the basis of experience without

188
00:11:17,180 --> 00:11:22,780
having to be programmed to do so it's

189
00:11:19,880 --> 00:11:26,090
based on different hypothesis underlying

190
00:11:22,780 --> 00:11:30,680
the model and it tries to improve it

191
00:11:26,090 --> 00:11:36,350
adjusting with more data within the

192
00:11:30,680 --> 00:11:39,410
model over time the most important part

193
00:11:36,350 --> 00:11:41,330
of that definition has to do with the

194
00:11:39,410 --> 00:11:44,270
fact that we're working with mathematics

195
00:11:41,330 --> 00:11:46,340
and data but then we can infer certain

196
00:11:44,270 --> 00:11:49,640
features that will help us make

197
00:11:46,340 --> 00:11:52,160
decisions in the future make forecasts

198
00:11:49,640 --> 00:11:56,660
and all of this importantly without

199
00:11:52,160 --> 00:11:57,500
teaching the system or programming is to

200
00:11:56,660 --> 00:12:06,939
make those

201
00:11:57,500 --> 00:12:11,480
predictions and forecasts so we now know

202
00:12:06,940 --> 00:12:14,780
what machine learning is although some

203
00:12:11,480 --> 00:12:17,060
of this is quite complex and adjusting

204
00:12:14,780 --> 00:12:19,910
the model or the parameters of the model

205
00:12:17,060 --> 00:12:22,790
we will see more about that shortly and

206
00:12:19,910 --> 00:12:27,050
what types of machine learning do we

207
00:12:22,790 --> 00:12:33,290
have one of them would be supervised

208
00:12:27,050 --> 00:12:35,630
learning we'll see later how we define

209
00:12:33,290 --> 00:12:38,329
learning but let's just remember that

210
00:12:35,630 --> 00:12:41,260
one of the subsets within machine

211
00:12:38,330 --> 00:12:46,340
learning is supervised learning that is

212
00:12:41,260 --> 00:12:50,090
machine learning where there's a mapping

213
00:12:46,340 --> 00:12:52,310
between input and output based on pairs

214
00:12:50,090 --> 00:12:54,560
that means that we have a set of data

215
00:12:52,310 --> 00:12:57,709
where we have an input value output

216
00:12:54,560 --> 00:13:00,949
value we give that to the algorithm we

217
00:12:57,710 --> 00:13:04,580
then give the algorithm an input value

218
00:13:00,950 --> 00:13:07,880
and it will automatically come up with

219
00:13:04,580 --> 00:13:09,230
the output value for us we'll see more

220
00:13:07,880 --> 00:13:11,930
details about that later

221
00:13:09,230 --> 00:13:14,810
and then supervised learning is divided

222
00:13:11,930 --> 00:13:16,969
into two different types classification

223
00:13:14,810 --> 00:13:19,430
and regression was the difference

224
00:13:16,970 --> 00:13:22,510
between them an algorithm using

225
00:13:19,430 --> 00:13:25,130
supervised learning and using

226
00:13:22,510 --> 00:13:29,030
classification it means it tries to

227
00:13:25,130 --> 00:13:32,900
predict or forecast a discrete values

228
00:13:29,030 --> 00:13:36,860
that means they're clearly limited for

229
00:13:32,900 --> 00:13:39,650
instance is an email spam or not that

230
00:13:36,860 --> 00:13:43,460
type of classification whereas in

231
00:13:39,650 --> 00:13:46,880
regression the prediction is or

232
00:13:43,460 --> 00:13:49,790
attempted prediction is for continuous

233
00:13:46,880 --> 00:13:53,680
values or neighboring values for

234
00:13:49,790 --> 00:13:56,689
instance how much does this house cost

235
00:13:53,680 --> 00:13:59,270
well I tell the algorithm that is a

236
00:13:56,690 --> 00:14:00,540
three-bedroom house and then it can

237
00:13:59,270 --> 00:14:06,030
predict

238
00:14:00,540 --> 00:14:09,060
or forecast a costume let's see now more

239
00:14:06,030 --> 00:14:11,130
about regression within supervised

240
00:14:09,060 --> 00:14:14,969
machine learning let me give you an

241
00:14:11,130 --> 00:14:21,350
example we want to predict the cost in

242
00:14:14,970 --> 00:14:24,630
Euros of handling an insurance claim

243
00:14:21,350 --> 00:14:27,090
depending on the system or the equipment

244
00:14:24,630 --> 00:14:31,280
that's been affected for instance in a

245
00:14:27,090 --> 00:14:35,760
security auditing we find a security

246
00:14:31,280 --> 00:14:39,959
failure that involves 100 different

247
00:14:35,760 --> 00:14:45,090
pieces within a company well how can we

248
00:14:39,960 --> 00:14:48,800
calculate and forecast what cost would

249
00:14:45,090 --> 00:14:51,950
be involved in an attack to that system

250
00:14:48,800 --> 00:14:54,930
we need to start by finding other

251
00:14:51,950 --> 00:14:57,570
security incident incidents that

252
00:14:54,930 --> 00:15:01,469
happened in the past see how much those

253
00:14:57,570 --> 00:15:08,070
cost and a check up against the number

254
00:15:01,470 --> 00:15:11,220
of PCs were involved then on a graph how

255
00:15:08,070 --> 00:15:15,120
would that look something like this this

256
00:15:11,220 --> 00:15:18,960
dot here is an incident where 50 pcs

257
00:15:15,120 --> 00:15:22,640
were impacted and the cost was 100,000

258
00:15:18,960 --> 00:15:27,600
euros this is another case with a cost

259
00:15:22,640 --> 00:15:31,260
this is just a way to portray the data

260
00:15:27,600 --> 00:15:35,930
in a group more graphic manner with that

261
00:15:31,260 --> 00:15:40,400
line so the machine learning algorithm

262
00:15:35,930 --> 00:15:44,550
basically builds a model for prediction

263
00:15:40,400 --> 00:15:47,610
look at this very simple example where

264
00:15:44,550 --> 00:15:53,370
the model seems to fit quite well

265
00:15:47,610 --> 00:15:56,280
like so that line would join the dots on

266
00:15:53,370 --> 00:16:00,660
the incident that we've had in the past

267
00:15:56,280 --> 00:16:02,990
and that would be that line would tell

268
00:16:00,660 --> 00:16:06,449
us what the trend is

269
00:16:02,990 --> 00:16:11,790
so let's predict how much a security

270
00:16:06,450 --> 00:16:15,840
incident would cost where 5075 pieces

271
00:16:11,790 --> 00:16:18,780
were involved I have the input variable

272
00:16:15,840 --> 00:16:20,460
there are 75 pcs and that's the result

273
00:16:18,780 --> 00:16:22,530
one hundred and seventy-five thousand

274
00:16:20,460 --> 00:16:28,980
zeroes that would be the predicted

275
00:16:22,530 --> 00:16:33,420
costume this is linear regression where

276
00:16:28,980 --> 00:16:36,450
that straight line is the function for

277
00:16:33,420 --> 00:16:39,199
that forecast for that prediction what

278
00:16:36,450 --> 00:16:43,830
about supervised learning in this case

279
00:16:39,200 --> 00:16:45,780
focusing on classification as opposed to

280
00:16:43,830 --> 00:16:48,360
regression in this case we are

281
00:16:45,780 --> 00:16:52,709
predicting a discrete value is this

282
00:16:48,360 --> 00:16:55,140
particular email spam or not in this

283
00:16:52,710 --> 00:16:58,020
case how am I going to predict it

284
00:16:55,140 --> 00:16:59,730
depending on the spelling mistakes the

285
00:16:58,020 --> 00:17:02,910
number of spelling mistakes and the

286
00:16:59,730 --> 00:17:05,550
number of the on the exclamation marks

287
00:17:02,910 --> 00:17:08,129
on that particular email

288
00:17:05,550 --> 00:17:13,099
why because spam tends to have those two

289
00:17:08,130 --> 00:17:17,610
right so in a small success of emails I

290
00:17:13,099 --> 00:17:20,790
study and see which ones were actually

291
00:17:17,609 --> 00:17:22,429
spam or not and count the number of

292
00:17:20,790 --> 00:17:26,460
spelling mistakes in the number of

293
00:17:22,430 --> 00:17:29,340
exclamation marks on real emails right

294
00:17:26,460 --> 00:17:32,280
this is just a very tiny example with a

295
00:17:29,340 --> 00:17:36,439
very small subset look at this email for

296
00:17:32,280 --> 00:17:39,600
instance with for spelling mistakes six

297
00:17:36,440 --> 00:17:42,060
exclamation marks and it was spam how do

298
00:17:39,600 --> 00:17:43,949
we know well because we've classified it

299
00:17:42,060 --> 00:17:47,090
we know it's not a prediction

300
00:17:43,950 --> 00:17:52,670
well our function could then predict it

301
00:17:47,090 --> 00:17:52,669
like so this would be the function

302
00:17:54,070 --> 00:18:01,519
we're a particular email has to be

303
00:17:57,399 --> 00:18:03,949
checked for those two elements so if

304
00:18:01,519 --> 00:18:07,070
it's on the left of the function

305
00:18:03,950 --> 00:18:09,380
function it would be alleged email

306
00:18:07,070 --> 00:18:12,549
whereas if it falls on the right-hand

307
00:18:09,380 --> 00:18:17,090
side it would be classified as spam

308
00:18:12,549 --> 00:18:20,120
simple example another important area

309
00:18:17,090 --> 00:18:24,199
for machine learning is non supervised

310
00:18:20,120 --> 00:18:27,199
learning in supervised learning the idea

311
00:18:24,200 --> 00:18:32,120
was that we had a set of pairs for

312
00:18:27,200 --> 00:18:34,820
inputs and outputs that is pcs affected

313
00:18:32,120 --> 00:18:37,489
and costs involved right

314
00:18:34,820 --> 00:18:39,379
what about non supervised learning what

315
00:18:37,490 --> 00:18:42,950
it would be machine learning where a

316
00:18:39,380 --> 00:18:46,480
function would be predicted with data

317
00:18:42,950 --> 00:18:49,850
that have not been tagged that is not

318
00:18:46,480 --> 00:18:52,730
classified or categorized that means

319
00:18:49,850 --> 00:18:56,539
that we don't have the output value

320
00:18:52,730 --> 00:18:59,330
let's go back to the spam example we

321
00:18:56,539 --> 00:19:02,480
have a set of emails we don't know if

322
00:18:59,330 --> 00:19:06,049
they're spam or not we categorize them

323
00:19:02,480 --> 00:19:11,559
for exclamation marks and spelling

324
00:19:06,049 --> 00:19:16,490
mistakes and we had the line or function

325
00:19:11,559 --> 00:19:20,389
so that information was given to the

326
00:19:16,490 --> 00:19:21,950
machine learning algorithm as to what

327
00:19:20,389 --> 00:19:25,668
happened in real life

328
00:19:21,950 --> 00:19:29,480
so that it could then predict it not in

329
00:19:25,669 --> 00:19:32,090
this case it's not supervised so the

330
00:19:29,480 --> 00:19:35,269
algorithm doesn't know which emails are

331
00:19:32,090 --> 00:19:39,830
spam or not which is give it the data

332
00:19:35,269 --> 00:19:42,980
and the set of emails in unsupervised

333
00:19:39,830 --> 00:19:45,309
learning machine learning we would end

334
00:19:42,980 --> 00:19:49,700
up with a structure that makes sense

335
00:19:45,309 --> 00:19:53,059
let's look at this situation illogical

336
00:19:49,700 --> 00:19:55,429
grouping it would be having ending up

337
00:19:53,059 --> 00:19:58,879
with these two clusters can you see how

338
00:19:55,429 --> 00:20:01,600
the points there the dots kind of

339
00:19:58,880 --> 00:20:04,150
cluster up together in one area

340
00:20:01,600 --> 00:20:07,750
and the same for the other subset that

341
00:20:04,150 --> 00:20:11,620
would be non supervised learning machine

342
00:20:07,750 --> 00:20:15,370
learning so let's go more in-depth now

343
00:20:11,620 --> 00:20:18,330
for linear regression for us to

344
00:20:15,370 --> 00:20:24,840
understand how learning takes place

345
00:20:18,330 --> 00:20:24,840
don't panic this is quite a common graph

346
00:20:25,650 --> 00:20:32,410
for instance Coursera uses it for their

347
00:20:29,590 --> 00:20:35,980
machine learning courses this is a new

348
00:20:32,410 --> 00:20:39,520
way to define supervised machine

349
00:20:35,980 --> 00:20:43,660
learning in this case we see that we

350
00:20:39,520 --> 00:20:47,050
have the training data sets and that are

351
00:20:43,660 --> 00:20:49,570
fed to the learning algorithm as a

352
00:20:47,050 --> 00:20:54,250
result of that the algorithm creates a

353
00:20:49,570 --> 00:21:00,129
function that we label as a hypothesis

354
00:20:54,250 --> 00:21:04,140
or hypothetic function based on previous

355
00:21:00,130 --> 00:21:07,120
incidents so after that we can have an

356
00:21:04,140 --> 00:21:09,790
input variable the number of PCs that

357
00:21:07,120 --> 00:21:15,790
were affected and then the function

358
00:21:09,790 --> 00:21:17,649
would decide on the forecast or the

359
00:21:15,790 --> 00:21:21,300
prediction based on the training

360
00:21:17,650 --> 00:21:25,690
examples let's go back to the idea of

361
00:21:21,300 --> 00:21:28,930
costs after a security incident how

362
00:21:25,690 --> 00:21:36,210
would a linear regression algorithm work

363
00:21:28,930 --> 00:21:39,700
with his totally made-up set of data

364
00:21:36,210 --> 00:21:45,760
with a number of PCs that were affected

365
00:21:39,700 --> 00:21:49,720
the cost in thousand euros after a

366
00:21:45,760 --> 00:21:51,680
number of incidents where we had those

367
00:21:49,720 --> 00:21:53,840
problems in

368
00:21:51,680 --> 00:21:57,350
whatever company or in a number of

369
00:21:53,840 --> 00:22:00,050
companies so this table is exactly the

370
00:21:57,350 --> 00:22:03,320
same here it's just that on the right we

371
00:22:00,050 --> 00:22:08,149
had a graphic representation in the idea

372
00:22:03,320 --> 00:22:11,540
what this means this is the function for

373
00:22:08,150 --> 00:22:14,560
a line on a graph

374
00:22:11,540 --> 00:22:14,560
straight line

375
00:22:16,680 --> 00:22:23,070
the machine learning algorithm then is

376
00:22:19,350 --> 00:22:26,639
fed the input data and then on the basis

377
00:22:23,070 --> 00:22:30,120
of that it adjust is adjusts a function

378
00:22:26,640 --> 00:22:31,800
so predictions can be made so let's

379
00:22:30,120 --> 00:22:33,780
start with a very simple function write

380
00:22:31,800 --> 00:22:37,500
the straight line something that we

381
00:22:33,780 --> 00:22:40,260
learned back in primary school let's

382
00:22:37,500 --> 00:22:44,850
start by replacing with random

383
00:22:40,260 --> 00:22:50,010
parameters the function has two

384
00:22:44,850 --> 00:22:57,179
different parameters the slope and the

385
00:22:50,010 --> 00:23:01,440
point where it Criss crosses for that

386
00:22:57,180 --> 00:23:02,580
linear function so this is the graphic

387
00:23:01,440 --> 00:23:06,990
representation

388
00:23:02,580 --> 00:23:12,149
we've been plotting here the same is the

389
00:23:06,990 --> 00:23:15,470
same function so if I showed you this

390
00:23:12,150 --> 00:23:20,340
and the number table and I asked you how

391
00:23:15,470 --> 00:23:23,550
how does this represent the trend is it

392
00:23:20,340 --> 00:23:27,209
a good representation or not well you

393
00:23:23,550 --> 00:23:31,379
would have no idea right whereas if you

394
00:23:27,210 --> 00:23:34,230
you look at it like this you get a much

395
00:23:31,380 --> 00:23:38,480
better idea because they are subsets of

396
00:23:34,230 --> 00:23:41,190
data looks like it had a different trend

397
00:23:38,480 --> 00:23:44,910
while the machine learning algorithm

398
00:23:41,190 --> 00:23:48,930
starts like this it generates a line

399
00:23:44,910 --> 00:23:51,720
that doesn't stick to reality and then

400
00:23:48,930 --> 00:23:55,920
the error is calculated what does that

401
00:23:51,720 --> 00:24:02,040
mean well the algorithm gets information

402
00:23:55,920 --> 00:24:06,480
from previous incidents it then portrays

403
00:24:02,040 --> 00:24:09,450
them on the graph and then where we knew

404
00:24:06,480 --> 00:24:11,910
both the input variable and the output

405
00:24:09,450 --> 00:24:18,110
variable well that helps us calculate

406
00:24:11,910 --> 00:24:18,110
the error through an error function

407
00:24:18,800 --> 00:24:27,240
mathematically that works but you can

408
00:24:22,890 --> 00:24:31,530
also use an optimization function what

409
00:24:27,240 --> 00:24:34,140
does that mean that parameters are moved

410
00:24:31,530 --> 00:24:38,490
and changed so that the error can be

411
00:24:34,140 --> 00:24:40,920
minimized how well for instance if I

412
00:24:38,490 --> 00:24:49,980
move it this way it looks like the error

413
00:24:40,920 --> 00:24:52,860
margin is lower it does so as much as

414
00:24:49,980 --> 00:24:55,920
possible until the error can't be

415
00:24:52,860 --> 00:24:58,560
reduced or limited any further what that

416
00:24:55,920 --> 00:25:02,160
would be a pretty good representation of

417
00:24:58,560 --> 00:25:04,830
our data subsets until the error has

418
00:25:02,160 --> 00:25:08,730
been minimized as possible as much as

419
00:25:04,830 --> 00:25:11,939
possible why because it's a straight

420
00:25:08,730 --> 00:25:20,100
line on the graph but is not quite

421
00:25:11,940 --> 00:25:23,910
straight on reality right so now we can

422
00:25:20,100 --> 00:25:26,760
see how algorithms learn in machine

423
00:25:23,910 --> 00:25:29,580
learning quite simply by getting a

424
00:25:26,760 --> 00:25:32,670
particular function in our case the

425
00:25:29,580 --> 00:25:36,120
linear function then seeing which

426
00:25:32,670 --> 00:25:42,570
parameters are important for that model

427
00:25:36,120 --> 00:25:46,770
of function and once that is ready then

428
00:25:42,570 --> 00:25:50,310
it tries to calculate the error present

429
00:25:46,770 --> 00:25:53,430
in our training data subset and try and

430
00:25:50,310 --> 00:25:56,820
optimize it until error is at a minimum

431
00:25:53,430 --> 00:25:59,340
and that would mean the function would

432
00:25:56,820 --> 00:26:03,120
be good for predicting on that

433
00:25:59,340 --> 00:26:09,139
particular data set it's more simple

434
00:26:03,120 --> 00:26:13,919
than you thought it might be right for

435
00:26:09,140 --> 00:26:16,710
algorithm learning so once the error has

436
00:26:13,920 --> 00:26:19,680
been minimized for the model parameters

437
00:26:16,710 --> 00:26:23,910
then we could get the function

438
00:26:19,680 --> 00:26:26,190
we could forget about the data we had we

439
00:26:23,910 --> 00:26:28,350
would just keep the function that means

440
00:26:26,190 --> 00:26:32,300
we would keep the model has been

441
00:26:28,350 --> 00:26:37,290
generated and then use it to predict new

442
00:26:32,300 --> 00:26:41,250
situations for instance when somebody

443
00:26:37,290 --> 00:26:44,879
asks how much would it cost if there was

444
00:26:41,250 --> 00:26:48,600
a security incident and 100 pcs were

445
00:26:44,880 --> 00:26:52,890
affected well we could then go to our

446
00:26:48,600 --> 00:26:59,760
hypothetical function the X will be the

447
00:26:52,890 --> 00:27:01,830
number of pcs affected or impacted a and

448
00:26:59,760 --> 00:27:05,550
then let me remind you that the model

449
00:27:01,830 --> 00:27:08,340
was used just to get these parameters

450
00:27:05,550 --> 00:27:11,690
here once we've got them then the

451
00:27:08,340 --> 00:27:15,030
training of the algorithm is over the

452
00:27:11,690 --> 00:27:18,600
prediction would be reliable for they

453
00:27:15,030 --> 00:27:24,320
said the success that we used for

454
00:27:18,600 --> 00:27:28,020
algorithm training so that's the most

455
00:27:24,320 --> 00:27:32,570
simple size of machine learning how al

456
00:27:28,020 --> 00:27:38,280
gore-ism x' work how supervised learning

457
00:27:32,570 --> 00:27:41,370
gets the data both inputs and outputs

458
00:27:38,280 --> 00:27:45,270
where as non supervised learning doesn't

459
00:27:41,370 --> 00:27:48,050
get the output output variables now we

460
00:27:45,270 --> 00:27:50,960
are ready to talk about classification

461
00:27:48,050 --> 00:27:56,240
which is the part of supervised learning

462
00:27:50,960 --> 00:28:03,570
where we want to predict discrete

463
00:27:56,240 --> 00:28:05,310
examples if an email is spam or not we

464
00:28:03,570 --> 00:28:07,020
see the linear regression that help

465
00:28:05,310 --> 00:28:10,290
predict continues values such as the

466
00:28:07,020 --> 00:28:12,960
cost of a security incident incidents is

467
00:28:10,290 --> 00:28:17,129
this an example I gave you we could also

468
00:28:12,960 --> 00:28:19,860
use it to forecast how many 15 emails or

469
00:28:17,130 --> 00:28:22,310
span emails will be receiver to given

470
00:28:19,860 --> 00:28:24,320
Organization for a month we can even

471
00:28:22,310 --> 00:28:27,629
forecast

472
00:28:24,320 --> 00:28:29,820
within a company which one would get

473
00:28:27,630 --> 00:28:33,480
most of the spam email so the

474
00:28:29,820 --> 00:28:35,850
classification here if you check it out

475
00:28:33,480 --> 00:28:39,780
and you think it through this would be

476
00:28:35,850 --> 00:28:43,139
our daily activity as professionals in

477
00:28:39,780 --> 00:28:45,600
cybersecurity for example for every

478
00:28:43,140 --> 00:28:48,090
email that we received an analyst can

479
00:28:45,600 --> 00:28:51,570
see whether it is phishing or it is not

480
00:28:48,090 --> 00:28:54,480
for every file on the network whether it

481
00:28:51,570 --> 00:28:57,260
has malware or not also any query for a

482
00:28:54,480 --> 00:28:59,790
command and control and these are all

483
00:28:57,260 --> 00:29:02,220
predictions discrete predictions of

484
00:28:59,790 --> 00:29:07,320
forecasts is it a command control yes or

485
00:29:02,220 --> 00:29:13,610
not is a bank transfer yes true note

486
00:29:07,320 --> 00:29:13,610
these request is a do s attack yes or no

487
00:29:18,170 --> 00:29:23,580
we also need to consider when we talk

488
00:29:20,880 --> 00:29:28,770
about classification is the fact that

489
00:29:23,580 --> 00:29:30,750
many companies nowadays gather data such

490
00:29:28,770 --> 00:29:32,910
as logs and events and processing all

491
00:29:30,750 --> 00:29:36,720
this data and the search for requests

492
00:29:32,910 --> 00:29:38,670
for threats can be done down in a way

493
00:29:36,720 --> 00:29:40,620
that is not machine learning the fact

494
00:29:38,670 --> 00:29:43,320
that we are using data to predict

495
00:29:40,620 --> 00:29:45,989
whether an email spam or not doesn't

496
00:29:43,320 --> 00:29:47,639
mean that were using machine learning if

497
00:29:45,990 --> 00:29:49,650
we use it to see whether a URL is

498
00:29:47,640 --> 00:29:52,130
malware or not does not mean that this

499
00:29:49,650 --> 00:29:52,130
is machine learning

500
00:29:54,450 --> 00:30:01,799
what about gridforce onin attacks on our

501
00:29:58,470 --> 00:30:08,580
side in our organization as analysts we

502
00:30:01,799 --> 00:30:11,970
check the server there's a force brute

503
00:30:08,580 --> 00:30:15,418
attacks and when we see ten requests

504
00:30:11,970 --> 00:30:18,149
from the same ip it's probably that so

505
00:30:15,419 --> 00:30:20,970
we create heuristics and we say that if

506
00:30:18,149 --> 00:30:23,428
there is more than ten requests its

507
00:30:20,970 --> 00:30:26,399
brute forcing and so you blocked out

508
00:30:23,429 --> 00:30:28,710
this would be a way to attack it all

509
00:30:26,399 --> 00:30:30,928
block it based on data from the locks

510
00:30:28,710 --> 00:30:33,899
generated and this is not actually

511
00:30:30,929 --> 00:30:35,759
machine learning so where would we start

512
00:30:33,899 --> 00:30:37,830
thinking that something is machine

513
00:30:35,759 --> 00:30:41,609
learning from the previous example as

514
00:30:37,830 --> 00:30:43,859
you can see we might say okay how come

515
00:30:41,609 --> 00:30:48,299
we've decided that 10 was the optimal

516
00:30:43,859 --> 00:30:50,789
number why a minute of time this is done

517
00:30:48,299 --> 00:30:53,399
based on the analyst experience over 10

518
00:30:50,789 --> 00:30:54,720
requests in one minute has brute force

519
00:30:53,399 --> 00:30:57,629
and from machine learning that

520
00:30:54,720 --> 00:31:00,269
subjectivity does not exist machine

521
00:30:57,629 --> 00:31:02,699
learning is using algorithms process in

522
00:31:00,269 --> 00:31:05,639
the data history as I said before in

523
00:31:02,700 --> 00:31:07,350
fearing inferring optimal classification

524
00:31:05,639 --> 00:31:09,899
standards following mathematical

525
00:31:07,350 --> 00:31:12,899
principles the difference between days

526
00:31:09,899 --> 00:31:15,268
and days here were contributing that

527
00:31:12,899 --> 00:31:17,070
part of subjectiveness by our

528
00:31:15,269 --> 00:31:19,379
subjectivity by the analyst bad with

529
00:31:17,070 --> 00:31:21,299
machine learning what we are doing

530
00:31:19,379 --> 00:31:24,939
actually is that every parameter out

531
00:31:21,299 --> 00:31:26,269
there are inferred by the algorithm

532
00:31:24,940 --> 00:31:28,019
[Music]

533
00:31:26,269 --> 00:31:36,379
through a series of mathematical

534
00:31:28,019 --> 00:31:41,700
principles and here let me show team

535
00:31:36,379 --> 00:31:43,769
this exercise and how we can use this

536
00:31:41,700 --> 00:31:46,019
very simple algorithm were the same

537
00:31:43,769 --> 00:31:48,619
which is a classification algorithm

538
00:31:46,019 --> 00:31:51,510
logistic regression for spam detection

539
00:31:48,619 --> 00:31:55,320
here for detecting spam

540
00:31:51,510 --> 00:31:58,610
and use a set of data which is track 7

541
00:31:55,320 --> 00:31:58,610
you can download it from the internet

542
00:31:59,390 --> 00:32:05,690
which was collected in 2007 as users of

543
00:32:03,570 --> 00:32:09,510
real emails if I'm not mistaken in

544
00:32:05,690 --> 00:32:13,710
75,000 and that are made public so that

545
00:32:09,510 --> 00:32:17,840
the spam detection is improved we gather

546
00:32:13,710 --> 00:32:17,840
all these emails and if you see

547
00:32:24,190 --> 00:32:27,549
wait fruit

548
00:32:34,530 --> 00:32:43,300
this is raw emails you have fun and

549
00:32:39,520 --> 00:32:45,760
return and everything the email has if

550
00:32:43,300 --> 00:32:49,680
we just need instead of how it is

551
00:32:45,760 --> 00:32:49,680
rendered by the email client

552
00:33:04,910 --> 00:33:10,800
what's the problem now problem is that

553
00:33:08,310 --> 00:33:14,850
we have a set of data that is a bunch of

554
00:33:10,800 --> 00:33:20,639
email 75,000 but in their will they are

555
00:33:14,850 --> 00:33:28,110
wrong and we have all the headlines we

556
00:33:20,640 --> 00:33:32,100
have HTML all the numbers and characters

557
00:33:28,110 --> 00:33:35,699
and automated algorithm would get a set

558
00:33:32,100 --> 00:33:38,250
of data that were numbers that will then

559
00:33:35,700 --> 00:33:41,400
process and there would be an equation

560
00:33:38,250 --> 00:33:43,200
so we need some pre-processing so that

561
00:33:41,400 --> 00:33:45,450
we can get all those emails and turn

562
00:33:43,200 --> 00:33:47,790
them into a rendering that is the right

563
00:33:45,450 --> 00:33:51,120
one for our machine learning algorithm

564
00:33:47,790 --> 00:33:52,830
to build the equation so one of the

565
00:33:51,120 --> 00:33:54,689
first things to be done when we use

566
00:33:52,830 --> 00:33:57,449
machine learning is to have the

567
00:33:54,690 --> 00:34:00,660
pre-processing on our set of data if

568
00:33:57,450 --> 00:34:04,290
we're using it on real examples odds are

569
00:34:00,660 --> 00:34:06,180
that our set of data that we got them do

570
00:34:04,290 --> 00:34:08,580
not show like a table this is the

571
00:34:06,180 --> 00:34:12,510
feature and these are the numbers if we

572
00:34:08,580 --> 00:34:14,850
use it to detect Network abnormalities

573
00:34:12,510 --> 00:34:20,220
or anomalies we will have lots of geekus

574
00:34:14,850 --> 00:34:25,230
and if we use it for URLs of to download

575
00:34:20,219 --> 00:34:28,409
malware probably we will have a dataset

576
00:34:25,230 --> 00:34:30,210
with a half million your app URLs and we

577
00:34:28,409 --> 00:34:32,190
say once we have it what do we do with

578
00:34:30,210 --> 00:34:35,340
it how do we feed them into the

579
00:34:32,190 --> 00:34:40,250
algorithm that's pre-processing so we

580
00:34:35,340 --> 00:34:40,250
will get this we will send the emails

581
00:34:44,870 --> 00:34:51,150
and we will take out several things we

582
00:34:48,840 --> 00:34:53,250
will remove the headlines so far we are

583
00:34:51,150 --> 00:34:56,780
not interested we will just keep the

584
00:34:53,250 --> 00:34:58,020
body of the email if there is tax or

585
00:34:56,780 --> 00:35:01,710
html's

586
00:34:58,020 --> 00:35:03,960
we will strip it and we will just keep

587
00:35:01,710 --> 00:35:08,210
what's inside of it whatever is going to

588
00:35:03,960 --> 00:35:08,210
be rendered and once we have it all

589
00:35:08,480 --> 00:35:11,870
we'll be using

590
00:35:27,109 --> 00:35:32,308
will is a given equation of function

591
00:35:30,150 --> 00:35:35,190
getting the whole text strain vector

592
00:35:32,309 --> 00:35:37,589
turn it into a vectors so that all those

593
00:35:35,190 --> 00:35:40,500
words are represented as a number and

594
00:35:37,589 --> 00:35:43,200
once we have the number we will send it

595
00:35:40,500 --> 00:35:47,280
to our machine learning algorithm that

596
00:35:43,200 --> 00:35:50,480
will get the parameters us using the

597
00:35:47,280 --> 00:35:53,849
logistic regression which is a simple

598
00:35:50,480 --> 00:35:56,160
equation same as with linear equation it

599
00:35:53,849 --> 00:35:58,799
will equate the model parameters and

600
00:35:56,160 --> 00:36:01,319
with that we will build the function we

601
00:35:58,799 --> 00:36:05,099
saw before and as we send the same email

602
00:36:01,319 --> 00:36:11,359
passed the same way that is spam or it

603
00:36:05,099 --> 00:36:11,359
is not so if we were to run

604
00:36:18,530 --> 00:36:25,579
this software first thing you'd put cars

605
00:36:21,329 --> 00:36:30,540
emails one by one till it's done at all

606
00:36:25,579 --> 00:36:38,040
on the one side stripping every HTML and

607
00:36:30,540 --> 00:36:38,759
headlines and and once it's done that

608
00:36:38,040 --> 00:36:40,079
would be it

609
00:36:38,760 --> 00:36:42,060
it's not the headlines for the header

610
00:36:40,079 --> 00:36:45,300
sorry so this is what we have once it's

611
00:36:42,060 --> 00:36:47,490
over so this is the algorithm here we

612
00:36:45,300 --> 00:36:50,010
see again linear regression and we see

613
00:36:47,490 --> 00:36:52,439
how the code works but it's trained the

614
00:36:50,010 --> 00:36:54,960
algorithm there's been a focused or

615
00:36:52,440 --> 00:36:57,290
prediction and it said that for a set of

616
00:36:54,960 --> 00:37:00,030
data that had not been classified

617
00:36:57,290 --> 00:37:02,490
previously even though we had classified

618
00:37:00,030 --> 00:37:05,250
them to see whether it was a good

619
00:37:02,490 --> 00:37:08,098
prediction so we had our set of data for

620
00:37:05,250 --> 00:37:10,530
training another separate set we've said

621
00:37:08,099 --> 00:37:12,900
well this is also labeled and I know

622
00:37:10,530 --> 00:37:14,359
whether it is spam or not leave it here

623
00:37:12,900 --> 00:37:16,170
so once I've trained my algorithm

624
00:37:14,359 --> 00:37:17,819
calculated the parameters and they have

625
00:37:16,170 --> 00:37:20,670
the function and the model I will get

626
00:37:17,820 --> 00:37:22,290
that set stand-in there I'll send emails

627
00:37:20,670 --> 00:37:25,619
and then and see whether they are

628
00:37:22,290 --> 00:37:27,930
properly classified so after having

629
00:37:25,619 --> 00:37:30,270
trained our algorithm in a logistic

630
00:37:27,930 --> 00:37:32,520
regression very simple algorithm and

631
00:37:30,270 --> 00:37:35,520
having tried our set of data we have a

632
00:37:32,520 --> 00:37:37,619
99% classification right as part of

633
00:37:35,520 --> 00:37:39,750
those set of data that were several

634
00:37:37,619 --> 00:37:43,950
thousand emails they were properly

635
00:37:39,750 --> 00:37:44,790
classified at a 99% probability so

636
00:37:43,950 --> 00:37:53,279
that's good

637
00:37:44,790 --> 00:37:56,339
I just refer to how this works but it

638
00:37:53,280 --> 00:38:00,240
had not gone deeper into explaining the

639
00:37:56,339 --> 00:38:04,470
code let me explain something to this

640
00:38:00,240 --> 00:38:08,368
way simpler let me show the set of data

641
00:38:04,470 --> 00:38:10,709
first first plan was a real set of data

642
00:38:08,369 --> 00:38:13,349
real emails were probably you saw the

643
00:38:10,710 --> 00:38:16,230
potential and here you have a set of

644
00:38:13,349 --> 00:38:18,990
data that I've built myself I've made it

645
00:38:16,230 --> 00:38:22,319
up trying to represent the example we

646
00:38:18,990 --> 00:38:25,410
saw before for linear regression here we

647
00:38:22,319 --> 00:38:27,790
have a set of data which is CSV a column

648
00:38:25,410 --> 00:38:32,440
with the number of devices

649
00:38:27,790 --> 00:38:37,029
then the next one is cast of the

650
00:38:32,440 --> 00:38:39,640
security incidents how do we program an

651
00:38:37,030 --> 00:38:42,280
algorithm for machine learning to build

652
00:38:39,640 --> 00:38:45,549
it to build a function you see with the

653
00:38:42,280 --> 00:38:49,150
existing libraries with Python this is

654
00:38:45,550 --> 00:38:52,710
simply straightforward and you can do

655
00:38:49,150 --> 00:38:55,150
just like this this is how we build a

656
00:38:52,710 --> 00:38:57,250
linear regression model as the one we

657
00:38:55,150 --> 00:38:59,800
see in the previous example that would

658
00:38:57,250 --> 00:39:02,320
be adjusted first we input two libraries

659
00:38:59,800 --> 00:39:05,980
that are very well-known in the field of

660
00:39:02,320 --> 00:39:09,390
machine learning panda which is a

661
00:39:05,980 --> 00:39:12,760
library to distri managed set of data

662
00:39:09,390 --> 00:39:18,129
this is silly one because we only have a

663
00:39:12,760 --> 00:39:21,400
few examples I would say 20 year 34 the

664
00:39:18,130 --> 00:39:24,130
emails there's this worth 75,000 there

665
00:39:21,400 --> 00:39:26,590
we need two library where we can process

666
00:39:24,130 --> 00:39:29,350
all those eight and grow and do some

667
00:39:26,590 --> 00:39:31,210
fishing means if instead of 75,000 it's

668
00:39:29,350 --> 00:39:33,700
a million what we need is something that

669
00:39:31,210 --> 00:39:37,690
is truly efficient we can not just build

670
00:39:33,700 --> 00:39:39,810
a loop to go through the file one by one

671
00:39:37,690 --> 00:39:43,000
because it would take days

672
00:39:39,810 --> 00:39:45,130
once we've imported both libraries all

673
00:39:43,000 --> 00:39:51,220
the things we do is we reach our set of

674
00:39:45,130 --> 00:39:52,900
data here is data dot c sv and usually

675
00:39:51,220 --> 00:39:55,990
this is a data frame that's the name

676
00:39:52,900 --> 00:39:59,680
it's given once imported the set of data

677
00:39:55,990 --> 00:40:01,870
you see pandas can read the CSV format

678
00:39:59,680 --> 00:40:05,430
right away and it's turned into an

679
00:40:01,870 --> 00:40:10,830
object of its own and so we cut them

680
00:40:05,430 --> 00:40:13,169
our input variables so we stripped it

681
00:40:10,830 --> 00:40:17,090
out of the failure class just the number

682
00:40:13,170 --> 00:40:20,010
of devices and then we also keep the

683
00:40:17,090 --> 00:40:22,800
output variable once we have both sense

684
00:40:20,010 --> 00:40:26,010
we split to the set of data and why is

685
00:40:22,800 --> 00:40:28,470
that why do we split it what we do so in

686
00:40:26,010 --> 00:40:30,960
order to do what we did before we have a

687
00:40:28,470 --> 00:40:33,390
set of data and with that we will build

688
00:40:30,960 --> 00:40:36,000
our model but once built how do we know

689
00:40:33,390 --> 00:40:38,879
that it's properly built and not just in

690
00:40:36,000 --> 00:40:40,680
any way since we have a set of data from

691
00:40:38,880 --> 00:40:43,560
which we know what the right answers are

692
00:40:40,680 --> 00:40:45,990
we're gonna split it and leave part of

693
00:40:43,560 --> 00:40:48,330
the data to verify that mother has been

694
00:40:45,990 --> 00:40:53,839
rightly done with the other half of the

695
00:40:48,330 --> 00:40:57,170
set of data usually it would be 20% 30%

696
00:40:53,840 --> 00:41:00,270
33% for testing and the other

697
00:40:57,170 --> 00:41:04,380
percentages for training of algorithms

698
00:41:00,270 --> 00:41:07,410
here you see this as X techs and eat X

699
00:41:04,380 --> 00:41:09,810
for testing and extraneous any training

700
00:41:07,410 --> 00:41:11,819
what I will use for training how do you

701
00:41:09,810 --> 00:41:14,610
trade an algorithm with one of these

702
00:41:11,820 --> 00:41:17,370
libraries is very simple we just import

703
00:41:14,610 --> 00:41:19,440
the algorithm this is regression linear

704
00:41:17,370 --> 00:41:22,620
regression as you see here and we've

705
00:41:19,440 --> 00:41:25,530
said it so and with the fit method we

706
00:41:22,620 --> 00:41:28,500
train the algorithm what does it need

707
00:41:25,530 --> 00:41:30,990
for to be trained easy-peasy our

708
00:41:28,500 --> 00:41:33,510
training data and the responses as well

709
00:41:30,990 --> 00:41:35,810
to calculate the mistake and adjust the

710
00:41:33,510 --> 00:41:39,270
straight line we feed both things and

711
00:41:35,810 --> 00:41:42,060
once we've done it I can start

712
00:41:39,270 --> 00:41:47,100
predicting what's generated here

713
00:41:42,060 --> 00:41:48,120
basically this is a model right same as

714
00:41:47,100 --> 00:41:51,600
saying here

715
00:41:48,120 --> 00:41:54,450
I need the function this is a straight

716
00:41:51,600 --> 00:41:56,549
the linear function down here the

717
00:41:54,450 --> 00:41:58,529
parameters as we've seen based on what I

718
00:41:56,550 --> 00:42:01,530
feed you for training and once you've

719
00:41:58,530 --> 00:42:03,720
adjusted the parameters I have the

720
00:42:01,530 --> 00:42:05,700
object here with the function and the

721
00:42:03,720 --> 00:42:08,580
parameters already adjusted with its

722
00:42:05,700 --> 00:42:11,040
model I feed what I want to predict and

723
00:42:08,580 --> 00:42:14,790
you see what I'm feeling is just

724
00:42:11,040 --> 00:42:18,810
tist so only the first column so that's

725
00:42:14,790 --> 00:42:21,270
the number of devices impacted and the e

726
00:42:18,810 --> 00:42:23,490
predict is the replied the second column

727
00:42:21,270 --> 00:42:26,520
the cast in which they have for every

728
00:42:23,490 --> 00:42:30,660
device based on the training once we

729
00:42:26,520 --> 00:42:32,730
have that what we do is well here I just

730
00:42:30,660 --> 00:42:35,790
got on the screen the prediction and

731
00:42:32,730 --> 00:42:38,550
what's real so what really was as our

732
00:42:35,790 --> 00:42:41,040
set of data and what was predicted what

733
00:42:38,550 --> 00:42:43,110
do we use here and we've seen that these

734
00:42:41,040 --> 00:42:46,500
are metrics these are metrics that are

735
00:42:43,110 --> 00:42:48,510
be used to assess how our models behave

736
00:42:46,500 --> 00:42:53,220
how has been built there are so many

737
00:42:48,510 --> 00:42:56,580
metrics kirisame and precision every one

738
00:42:53,220 --> 00:42:59,069
score many many of them when we checked

739
00:42:56,580 --> 00:43:01,710
the class rain we will see we have

740
00:42:59,070 --> 00:43:05,100
silhouette proficient and many others

741
00:43:01,710 --> 00:43:07,680
used to assess different what the

742
00:43:05,100 --> 00:43:10,500
results mean one might measure the false

743
00:43:07,680 --> 00:43:16,319
positive to true positive ratio some

744
00:43:10,500 --> 00:43:24,060
others the false positive rate etc so if

745
00:43:16,320 --> 00:43:25,500
we run this algorithm very fast why this

746
00:43:24,060 --> 00:43:27,270
one so first and the other one service

747
00:43:25,500 --> 00:43:30,960
low because I had to parse the emails

748
00:43:27,270 --> 00:43:33,030
and I had to train it using so many

749
00:43:30,960 --> 00:43:35,670
examples training example seventy-five

750
00:43:33,030 --> 00:43:38,160
thousand which are not this many but

751
00:43:35,670 --> 00:43:40,080
here we had a bunch see that the

752
00:43:38,160 --> 00:43:41,430
prediction of the forecast when I split

753
00:43:40,080 --> 00:43:46,110
it into twenty percent

754
00:43:41,430 --> 00:43:48,319
we only got these examples here is just

755
00:43:46,110 --> 00:43:51,710
one two three four

756
00:43:48,320 --> 00:43:55,010
so we had view training exams and if we

757
00:43:51,710 --> 00:43:57,500
compared forecast with reality we might

758
00:43:55,010 --> 00:43:59,980
see that it's not properly adjusted is

759
00:43:57,500 --> 00:44:02,510
not a problem much for reality we had

760
00:43:59,980 --> 00:44:05,480
227 that took us on our first incidence

761
00:44:02,510 --> 00:44:08,210
as we predicted using number of machines

762
00:44:05,480 --> 00:44:10,280
is over a hundred thousand and over ten

763
00:44:08,210 --> 00:44:14,930
thousand the prediction was sixty

764
00:44:10,280 --> 00:44:17,750
thousand 65 55 thousand so it seems that

765
00:44:14,930 --> 00:44:18,680
the ratio is preserved but it's not

766
00:44:17,750 --> 00:44:21,860
properly

767
00:44:18,680 --> 00:44:23,750
well fine-tuned it was not the right one

768
00:44:21,860 --> 00:44:25,850
and why is that well it happened because

769
00:44:23,750 --> 00:44:28,790
we only had a few training examples

770
00:44:25,850 --> 00:44:32,690
actually so fear that we only had team

771
00:44:28,790 --> 00:44:34,550
point placed in a random way and with

772
00:44:32,690 --> 00:44:37,580
the straight line we cannot really see

773
00:44:34,550 --> 00:44:39,770
was the trend of our set of data if we

774
00:44:37,580 --> 00:44:41,630
did have some more spots or points that

775
00:44:39,770 --> 00:44:43,940
would truly represent the relationship

776
00:44:41,630 --> 00:44:47,510
between the number of devices and the

777
00:44:43,940 --> 00:44:57,620
cost we could have a much more accurate

778
00:44:47,510 --> 00:45:02,270
prediction okay then next topic a bit

779
00:44:57,620 --> 00:45:04,940
more advanced clustering clustering

780
00:45:02,270 --> 00:45:07,310
takes us back to you the relationship or

781
00:45:04,940 --> 00:45:09,950
how we understand for any examples in

782
00:45:07,310 --> 00:45:14,230
our daily life a cyber security analysts

783
00:45:09,950 --> 00:45:18,879
and paper why would we use clustering

784
00:45:14,230 --> 00:45:21,530
nor why an analyst with years flustering

785
00:45:18,880 --> 00:45:25,220
malicious activity oftentimes happens

786
00:45:21,530 --> 00:45:27,950
with groups or sets see this example

787
00:45:25,220 --> 00:45:30,439
here if an attacker tries to gain access

788
00:45:27,950 --> 00:45:33,259
to your network they will need to scan

789
00:45:30,440 --> 00:45:36,680
and have traffic with similar features

790
00:45:33,260 --> 00:45:39,800
so try to come in so recognition of the

791
00:45:36,680 --> 00:45:42,980
network and so some network scans that

792
00:45:39,800 --> 00:45:45,320
had produced high traffic why TCP or

793
00:45:42,980 --> 00:45:47,960
whatever but it is pretty similar so

794
00:45:45,320 --> 00:45:50,750
malicious activity happened within a set

795
00:45:47,960 --> 00:45:53,000
of data they're very similar then if an

796
00:45:50,750 --> 00:45:55,040
attacker tries to exploit an SQL

797
00:45:53,000 --> 00:45:57,140
injection they will try to try several

798
00:45:55,040 --> 00:45:58,680
similar sequences before they find the

799
00:45:57,140 --> 00:46:01,890
right one so

800
00:45:58,680 --> 00:46:04,470
the name of the table or the parameters

801
00:46:01,890 --> 00:46:09,710
that are the right ones if an attacker

802
00:46:04,470 --> 00:46:13,100
hat it performs a first force Britain

803
00:46:09,710 --> 00:46:16,100
they will be trying several of them

804
00:46:13,100 --> 00:46:19,319
brute force and that is several of them

805
00:46:16,100 --> 00:46:23,450
so probably they send similars phishing

806
00:46:19,320 --> 00:46:23,450
emails to many employees

807
00:46:23,930 --> 00:46:30,660
this happens with sets and that's

808
00:46:28,140 --> 00:46:32,940
clustering for security clustering is

809
00:46:30,660 --> 00:46:35,190
just another kind of algorithm for

810
00:46:32,940 --> 00:46:38,310
machine learning which is based on

811
00:46:35,190 --> 00:46:44,010
unsupervised learning here our set of

812
00:46:38,310 --> 00:46:45,570
data we will have so many data we don't

813
00:46:44,010 --> 00:46:47,750
know whether they're good or bad because

814
00:46:45,570 --> 00:46:50,730
we're not have not Dib fat type

815
00:46:47,750 --> 00:46:53,430
classified them before what's the goal

816
00:46:50,730 --> 00:46:55,710
here so if we have a large amount of

817
00:46:53,430 --> 00:46:58,169
data so let's say that we have a large

818
00:46:55,710 --> 00:47:00,120
amount of logs and we'll be using the

819
00:46:58,170 --> 00:47:02,640
clustering algorithm to bring together

820
00:47:00,120 --> 00:47:05,130
all those data and locks into groups

821
00:47:02,640 --> 00:47:06,839
that follow similar features and when

822
00:47:05,130 --> 00:47:10,170
looking to them we can start somewhere

823
00:47:06,840 --> 00:47:14,010
and it is easier as we think that

824
00:47:10,170 --> 00:47:16,890
malicious activity happens as part of a

825
00:47:14,010 --> 00:47:19,140
group or in groups probably one of those

826
00:47:16,890 --> 00:47:21,450
small clusters of our millions of locks

827
00:47:19,140 --> 00:47:23,460
we will have some locks that are part of

828
00:47:21,450 --> 00:47:26,399
a malicious activity so if we are

829
00:47:23,460 --> 00:47:28,980
developing a threat hunting we are 300

830
00:47:26,400 --> 00:47:30,780
him for a company and we're trying to

831
00:47:28,980 --> 00:47:32,400
check threats in the company with that

832
00:47:30,780 --> 00:47:35,100
know into what we're looking for and we

833
00:47:32,400 --> 00:47:37,020
have many instead of Lux to look into a

834
00:47:35,100 --> 00:47:40,220
good starting point would be clustering

835
00:47:37,020 --> 00:47:43,380
techniques as we will see in a minute

836
00:47:40,220 --> 00:47:46,109
so here you have it the idea is to have

837
00:47:43,380 --> 00:47:49,470
the grouping of data as I said before

838
00:47:46,110 --> 00:47:51,750
and something important here once we say

839
00:47:49,470 --> 00:47:54,390
these data are grouped we need to see

840
00:47:51,750 --> 00:47:57,330
how to give them these would be through

841
00:47:54,390 --> 00:47:59,339
the proximity concept and I prefer to

842
00:47:57,330 --> 00:48:04,230
proximity here but you might say okay

843
00:47:59,340 --> 00:48:07,260
percerin II but what do you mean how do

844
00:48:04,230 --> 00:48:09,720
I know that these you are L is closer to

845
00:48:07,260 --> 00:48:13,050
this one doesn't make sense as it so

846
00:48:09,720 --> 00:48:15,750
this is measured with metrics metrics

847
00:48:13,050 --> 00:48:19,590
would be a specific way we use to assess

848
00:48:15,750 --> 00:48:21,810
the proximity between points or spots

849
00:48:19,590 --> 00:48:25,100
let's see there's a basic example

850
00:48:21,810 --> 00:48:27,630
something we can see in our dating knife

851
00:48:25,100 --> 00:48:30,089
but it's not considered machine learning

852
00:48:27,630 --> 00:48:32,580
in our definition we would choose one or

853
00:48:30,090 --> 00:48:35,040
several dimensions and define every

854
00:48:32,580 --> 00:48:37,049
facet as a group of elements that share

855
00:48:35,040 --> 00:48:39,180
values in those dimensions when you hear

856
00:48:37,050 --> 00:48:42,660
the matches what we are saying are this

857
00:48:39,180 --> 00:48:45,509
columns in the back before we had on two

858
00:48:42,660 --> 00:48:47,759
dimensions in our set of data so we can

859
00:48:45,510 --> 00:48:51,030
represent it there we had the number of

860
00:48:47,760 --> 00:48:53,430
devices and the cost of the incident it

861
00:48:51,030 --> 00:48:55,950
thats easy to represent and usually our

862
00:48:53,430 --> 00:48:58,950
set of data will not have to you but not

863
00:48:55,950 --> 00:49:01,560
many more 10 2013 this cannot be

864
00:48:58,950 --> 00:49:03,080
represented graphically

865
00:49:01,560 --> 00:49:05,120
[Music]

866
00:49:03,080 --> 00:49:09,529
so we can use a clustering technique

867
00:49:05,120 --> 00:49:12,950
which says two dimensions okay one is

868
00:49:09,530 --> 00:49:16,010
the number of devices and then or

869
00:49:12,950 --> 00:49:19,549
endpoints and then everything else value

870
00:49:16,010 --> 00:49:22,970
shared or a group these would be as a

871
00:49:19,550 --> 00:49:29,000
basic example of what's used by the SQL

872
00:49:22,970 --> 00:49:31,939
example a grip let's move on to more

873
00:49:29,000 --> 00:49:34,870
advanced cases this is the first machine

874
00:49:31,940 --> 00:49:37,840
learning or clustering algorithm

875
00:49:34,870 --> 00:49:41,390
probably one of the most popular popular

876
00:49:37,840 --> 00:49:43,430
algorithms which is k-means k-means have

877
00:49:41,390 --> 00:49:46,609
several features the metrics to measure

878
00:49:43,430 --> 00:49:49,100
distance this is a but measure and

879
00:49:46,610 --> 00:49:50,960
proximity between points and bring them

880
00:49:49,100 --> 00:49:53,900
together so they are quite the closest

881
00:49:50,960 --> 00:49:57,250
so understand it that scenario of

882
00:49:53,900 --> 00:50:00,230
premise that it all happens in groups so

883
00:49:57,250 --> 00:50:04,100
proximity concept is measured with that

884
00:50:00,230 --> 00:50:06,770
instance with the euclidean instance the

885
00:50:04,100 --> 00:50:09,200
great feature here is that is easily

886
00:50:06,770 --> 00:50:12,080
scalable to larger grapes so it's an

887
00:50:09,200 --> 00:50:13,910
algorithm that is used for production

888
00:50:12,080 --> 00:50:16,250
centers because of that because we have

889
00:50:13,910 --> 00:50:18,920
million of date and maybe other advanced

890
00:50:16,250 --> 00:50:21,980
advanced algorithms and also useful for

891
00:50:18,920 --> 00:50:23,600
scalability reasons but then there are

892
00:50:21,980 --> 00:50:25,940
some other considerations we need to

893
00:50:23,600 --> 00:50:28,100
consider and I'll refer to that and how

894
00:50:25,940 --> 00:50:30,680
does came into word game Ian's works as

895
00:50:28,100 --> 00:50:34,700
it follows there's something that we

896
00:50:30,680 --> 00:50:39,740
will call centroids which are points we

897
00:50:34,700 --> 00:50:42,259
will set into our map randomly do

898
00:50:39,740 --> 00:50:44,509
remember that as we said that there is a

899
00:50:42,260 --> 00:50:46,880
table and a graphic representation same

900
00:50:44,510 --> 00:50:50,080
here so these would be our state of day

901
00:50:46,880 --> 00:50:52,850
so we have a table and we will randomly

902
00:50:50,080 --> 00:50:55,870
initialize some of this data we will

903
00:50:52,850 --> 00:50:59,600
decide how many little beam all of this

904
00:50:55,870 --> 00:51:02,180
once initialized and once they're inside

905
00:50:59,600 --> 00:51:06,830
of our set of data we might say that

906
00:51:02,180 --> 00:51:09,799
they will be assigned data that are

907
00:51:06,830 --> 00:51:11,029
closest to them we started there and the

908
00:51:09,800 --> 00:51:14,099
closest our

909
00:51:11,030 --> 00:51:17,250
allocated this is will all on the blue

910
00:51:14,099 --> 00:51:20,760
because they are closed there once all

911
00:51:17,250 --> 00:51:22,410
those points have been assigned to the

912
00:51:20,760 --> 00:51:24,619
centroids that we've looked into you

913
00:51:22,410 --> 00:51:28,890
will calculate the distance of every

914
00:51:24,619 --> 00:51:32,070
spot and take the average and move it as

915
00:51:28,890 --> 00:51:34,618
close as we can and back again to the

916
00:51:32,070 --> 00:51:37,260
beginning so we move our centroid so

917
00:51:34,619 --> 00:51:38,820
first it's all on one then we calculate

918
00:51:37,260 --> 00:51:41,700
the average and then back to the average

919
00:51:38,820 --> 00:51:44,730
for everything then some here and then

920
00:51:41,700 --> 00:51:47,970
to the middle or the average again and

921
00:51:44,730 --> 00:51:49,560
then we stop moving that around when we

922
00:51:47,970 --> 00:51:51,839
see that they are not moving anymore

923
00:51:49,560 --> 00:51:53,880
when it comes that point where they

924
00:51:51,839 --> 00:51:56,359
don't move anymore we think that every

925
00:51:53,880 --> 00:51:59,010
of those has to reach a state where

926
00:51:56,359 --> 00:52:01,470
every point that have been assigned to

927
00:51:59,010 --> 00:52:05,490
them are part of a cluster and you see

928
00:52:01,470 --> 00:52:06,899
that they're all of them combined Eric

929
00:52:05,490 --> 00:52:10,950
classroom because we've moved the

930
00:52:06,900 --> 00:52:13,140
average around the centroids so what is

931
00:52:10,950 --> 00:52:14,250
to be considered when we're using this

932
00:52:13,140 --> 00:52:17,279
algorithm

933
00:52:14,250 --> 00:52:20,579
I meant to share this with Eve but I

934
00:52:17,280 --> 00:52:22,319
wanted to make it clear so the animal

935
00:52:20,579 --> 00:52:24,329
isn't it to know we helped beforehand

936
00:52:22,319 --> 00:52:27,000
the number of clusters since it is asked

937
00:52:24,329 --> 00:52:28,950
to indicate the number of clusters need

938
00:52:27,000 --> 00:52:31,319
to be initialized and we'll remove to

939
00:52:28,950 --> 00:52:33,060
rent beforehand we need to state the

940
00:52:31,319 --> 00:52:34,950
number of clusters what's the problem

941
00:52:33,060 --> 00:52:37,500
here well if we're looking into a set of

942
00:52:34,950 --> 00:52:39,810
data such as locks in our search for

943
00:52:37,500 --> 00:52:42,359
security threats and we truly don't know

944
00:52:39,810 --> 00:52:43,980
how many clusters are in there it is

945
00:52:42,359 --> 00:52:45,960
difficult to say well I'll initialize

946
00:52:43,980 --> 00:52:50,099
how many three four I don't know how

947
00:52:45,960 --> 00:52:53,790
many are there and if we had a set of

948
00:52:50,099 --> 00:52:55,800
tagged data or we knew what we're

949
00:52:53,790 --> 00:52:59,430
looking for on the results we're looking

950
00:52:55,800 --> 00:53:01,290
for we can choose one two three times

951
00:52:59,430 --> 00:53:04,710
the number of tags we have if we have a

952
00:53:01,290 --> 00:53:08,490
set of data where we say well inside of

953
00:53:04,710 --> 00:53:10,560
this I know there is 1 million items but

954
00:53:08,490 --> 00:53:15,180
there are three threats in it we would

955
00:53:10,560 --> 00:53:17,609
get those tags they're all those and say

956
00:53:15,180 --> 00:53:20,368
initialize one to three clusters right

957
00:53:17,609 --> 00:53:21,540
three times the number of threats here

958
00:53:20,369 --> 00:53:26,040
it would be three

959
00:53:21,540 --> 00:53:27,630
- six or nine mine 9 as well and we use

960
00:53:26,040 --> 00:53:31,620
them seeing how they grouped together

961
00:53:27,630 --> 00:53:35,150
and then we check the clusters then we

962
00:53:31,620 --> 00:53:37,080
also need to use streamlining or

963
00:53:35,150 --> 00:53:40,490
standardization this is a bit more

964
00:53:37,080 --> 00:53:42,420
advanced but is we referred to

965
00:53:40,490 --> 00:53:44,520
standardizations a lability

966
00:53:42,420 --> 00:53:47,640
has to be used if we have a set of data

967
00:53:44,520 --> 00:53:50,520
that are out of balance so we get

968
00:53:47,640 --> 00:53:52,680
several dimensions as part of our set of

969
00:53:50,520 --> 00:53:55,290
data there's a first line where the

970
00:53:52,680 --> 00:53:58,770
valleys oscillate from 1 to 5 and the

971
00:53:55,290 --> 00:54:00,600
second one has valleys and they are fed

972
00:53:58,770 --> 00:54:03,509
values from wanted to million maybe

973
00:54:00,600 --> 00:54:05,940
because one here it would be the number

974
00:54:03,510 --> 00:54:08,760
of rooms that's one of the features in a

975
00:54:05,940 --> 00:54:12,180
how they're the one the price of the

976
00:54:08,760 --> 00:54:14,880
home or anything of that kind so there

977
00:54:12,180 --> 00:54:16,740
are features inside those set of data

978
00:54:14,880 --> 00:54:19,350
that are out of balance this is AG

979
00:54:16,740 --> 00:54:22,649
Lydian distance as I said and if they

980
00:54:19,350 --> 00:54:24,150
are so wide apart it doesn't work so we

981
00:54:22,650 --> 00:54:27,600
need to follow standardization

982
00:54:24,150 --> 00:54:30,570
mechanisms which keep the data trend but

983
00:54:27,600 --> 00:54:34,950
somehow equalizes every features so that

984
00:54:30,570 --> 00:54:37,500
they have similar valleys what else we

985
00:54:34,950 --> 00:54:42,799
cannot use k-means with a categorical

986
00:54:37,500 --> 00:54:47,160
data or one hold encoding either

987
00:54:42,800 --> 00:54:51,680
if we have one of our features which is

988
00:54:47,160 --> 00:54:55,470
no number but categorical numbers so a

989
00:54:51,680 --> 00:54:56,850
category if something is good or bad we

990
00:54:55,470 --> 00:54:59,189
cannot feed it into the algorithm

991
00:54:56,850 --> 00:55:02,610
because it only understand numbers one

992
00:54:59,190 --> 00:55:07,890
hold encoding it would say but one is a

993
00:55:02,610 --> 00:55:10,070
zero and the good one is a one these are

994
00:55:07,890 --> 00:55:12,210
very low numbers by the way you were

995
00:55:10,070 --> 00:55:15,120
similar in value so it doesn't really

996
00:55:12,210 --> 00:55:18,020
work k-means doesn't work with states

997
00:55:15,120 --> 00:55:24,040
that are have several dimensions

998
00:55:18,020 --> 00:55:24,040
number of rooms etc so picture this case

999
00:55:25,150 --> 00:55:31,280
machine learning on network traffic we

1000
00:55:28,400 --> 00:55:33,650
can get so many features out of this

1001
00:55:31,280 --> 00:55:36,530
package we can see number of incoming

1002
00:55:33,650 --> 00:55:38,900
bytes or a number of the package we get

1003
00:55:36,530 --> 00:55:41,800
that and we can get the number of bytes

1004
00:55:38,900 --> 00:55:48,470
through the floating coming outgoing TCP

1005
00:55:41,800 --> 00:55:51,740
UDP and so on and so forth so what if we

1006
00:55:48,470 --> 00:55:54,819
have many columns that algorithm won't

1007
00:55:51,740 --> 00:55:57,919
be working properly we would be needing

1008
00:55:54,820 --> 00:56:00,020
dimension reduction algorithms I'm not

1009
00:55:57,920 --> 00:56:05,180
going to go into detail because those

1010
00:56:00,020 --> 00:56:11,290
are more advanced so k-means works

1011
00:56:05,180 --> 00:56:15,379
better if the centroids are randomized

1012
00:56:11,290 --> 00:56:19,610
most implementations they're initialized

1013
00:56:15,380 --> 00:56:23,350
randomly and very importantly caming

1014
00:56:19,610 --> 00:56:27,859
k-means it takes it for granted that

1015
00:56:23,350 --> 00:56:30,440
there is very cool look at this example

1016
00:56:27,859 --> 00:56:33,799
look at those clusters how they look

1017
00:56:30,440 --> 00:56:36,320
like circles right when i say it's fira

1018
00:56:33,800 --> 00:56:40,850
co i'm talking about a graphic

1019
00:56:36,320 --> 00:56:44,359
representation so that is an assumption

1020
00:56:40,850 --> 00:56:49,279
and if when they're not spherical then

1021
00:56:44,359 --> 00:56:50,990
this technique for k-means grouping or

1022
00:56:49,280 --> 00:56:52,820
clustering it just won't be working

1023
00:56:50,990 --> 00:56:54,950
properly

1024
00:56:52,820 --> 00:56:58,340
let's look at a different type of

1025
00:56:54,950 --> 00:56:59,000
clustering because clustering is quite

1026
00:56:58,340 --> 00:57:03,290
generic

1027
00:56:59,000 --> 00:57:05,630
is based on proximity and k-means uses

1028
00:57:03,290 --> 00:57:08,870
euclidean distance but there are other

1029
00:57:05,630 --> 00:57:11,840
possible distances if we have problems

1030
00:57:08,870 --> 00:57:14,839
with that in this case we're going to

1031
00:57:11,840 --> 00:57:19,930
look at DB scan a clustering algorithm

1032
00:57:14,840 --> 00:57:25,490
that doesn't use euclidean density but

1033
00:57:19,930 --> 00:57:29,390
actually it works really well with non

1034
00:57:25,490 --> 00:57:33,529
spherical distributions let's see how

1035
00:57:29,390 --> 00:57:37,129
the algorithm works for a DB scan here

1036
00:57:33,530 --> 00:57:41,210
we have four clusters and we can see

1037
00:57:37,130 --> 00:57:44,060
this round cluster that has been colored

1038
00:57:41,210 --> 00:57:49,670
in red but we have three more smaller

1039
00:57:44,060 --> 00:57:53,299
clusters with Euclidean distance

1040
00:57:49,670 --> 00:57:56,480
centroids and k-means well if we start

1041
00:57:53,300 --> 00:57:59,270
to assign values and it all moves to the

1042
00:57:56,480 --> 00:58:02,600
center the outer cluster won't be

1043
00:57:59,270 --> 00:58:06,770
detected why why not because it's not a

1044
00:58:02,600 --> 00:58:10,130
circuit that's particularly spherical DB

1045
00:58:06,770 --> 00:58:13,660
scan works differently because we have

1046
00:58:10,130 --> 00:58:17,240
to indicate first an epsilon parameter

1047
00:58:13,660 --> 00:58:21,980
that would mean is like a radius around

1048
00:58:17,240 --> 00:58:24,950
which it looks for neighboring values so

1049
00:58:21,980 --> 00:58:28,520
we would initialize a point on our data

1050
00:58:24,950 --> 00:58:32,149
set which is get one of those points and

1051
00:58:28,520 --> 00:58:35,860
research within that radius so if it has

1052
00:58:32,150 --> 00:58:38,870
four main points or over that number

1053
00:58:35,860 --> 00:58:42,650
neighboring points that will be a core

1054
00:58:38,870 --> 00:58:45,799
point as the start of a cluster and that

1055
00:58:42,650 --> 00:58:49,570
is done again for other neighboring

1056
00:58:45,800 --> 00:58:54,080
points if we if there are four of them

1057
00:58:49,570 --> 00:58:56,270
again we go on and on until the point

1058
00:58:54,080 --> 00:59:00,560
comes when no more neighboring values

1059
00:58:56,270 --> 00:59:03,890
are found so we then look for points

1060
00:59:00,560 --> 00:59:06,499
that have not been added to that cluster

1061
00:59:03,890 --> 00:59:08,420
we start again we initialize a different

1062
00:59:06,499 --> 00:59:12,430
point this one for instance it doesn't

1063
00:59:08,420 --> 00:59:16,880
have four points so that is an anomaly

1064
00:59:12,430 --> 00:59:19,249
again here we see that it's got the

1065
00:59:16,880 --> 00:59:23,150
number of number of neighboring values

1066
00:59:19,249 --> 00:59:25,910
that we defined and recurrently doing so

1067
00:59:23,150 --> 00:59:29,450
until we find no more and then we would

1068
00:59:25,910 --> 00:59:35,299
move to on to a different point we see

1069
00:59:29,450 --> 00:59:39,230
then a density for clustering to be able

1070
00:59:35,299 --> 00:59:42,499
to cluster points even if they're not a

1071
00:59:39,230 --> 00:59:46,390
spherical DB scan doesn't work properly

1072
00:59:42,499 --> 00:59:50,299
when we have different densities

1073
00:59:46,390 --> 00:59:53,720
logically then the selection of epsilon

1074
00:59:50,299 --> 00:59:56,410
and main points parameters are the basis

1075
00:59:53,720 --> 00:59:59,988
for correct functioning of this model

1076
00:59:56,410 --> 01:00:04,069
the radius and the number of neighboring

1077
00:59:59,989 --> 01:00:05,809
values that define a cluster that's what

1078
01:00:04,069 --> 01:00:08,269
we need we don't need to define the

1079
01:00:05,809 --> 01:00:11,989
clusters beforehand but we do need to

1080
01:00:08,269 --> 01:00:13,788
define those parameters so that will be

1081
01:00:11,989 --> 01:00:17,319
the basis for the algorithm to work

1082
01:00:13,789 --> 01:00:19,849
properly or not it doesn't work properly

1083
01:00:17,319 --> 01:00:24,980
where we have many different dimensions

1084
01:00:19,849 --> 01:00:26,900
for that type of data set before I show

1085
01:00:24,980 --> 01:00:29,059
you an example I want to warn you that

1086
01:00:26,900 --> 01:00:32,839
would you need to be very careful not to

1087
01:00:29,059 --> 01:00:37,190
alter the density if the set is

1088
01:00:32,839 --> 01:00:40,700
subdivided into subsets we have our data

1089
01:00:37,190 --> 01:00:42,650
set we need to check how the algorithm

1090
01:00:40,700 --> 01:00:46,730
works so that means we need to leave

1091
01:00:42,650 --> 01:00:49,759
some data for supervision or monitoring

1092
01:00:46,730 --> 01:00:52,930
without a tag or at least the algorithm

1093
01:00:49,759 --> 01:00:55,519
doesn't need the tag for clustering

1094
01:00:52,930 --> 01:00:59,569
classifications in this case but we

1095
01:00:55,519 --> 01:01:03,669
could have the tag and then a feed both

1096
01:00:59,569 --> 01:01:07,160
the data and the tag and then on testing

1097
01:01:03,670 --> 01:01:09,739
during that phase we could check to see

1098
01:01:07,160 --> 01:01:13,399
how the classification into clusters

1099
01:01:09,739 --> 01:01:15,360
want to see for malicious examples it

1100
01:01:13,400 --> 01:01:18,530
went into a cluster and then

1101
01:01:15,360 --> 01:01:25,920
into a different cluster or if they're

1102
01:01:18,530 --> 01:01:28,350
mixed together so if we you in those

1103
01:01:25,920 --> 01:01:31,320
cases we need to be very careful again

1104
01:01:28,350 --> 01:01:34,259
not to alter the density of the dataset

1105
01:01:31,320 --> 01:01:37,530
because if we split it is split them

1106
01:01:34,260 --> 01:01:40,620
randomly we might end up with data that

1107
01:01:37,530 --> 01:01:43,710
are very good representations for some

1108
01:01:40,620 --> 01:01:46,500
of those data but not for the rest why

1109
01:01:43,710 --> 01:01:47,280
because we altered the density and that

1110
01:01:46,500 --> 01:01:51,240
would be wrong

1111
01:01:47,280 --> 01:01:55,170
there's look at a practical case an

1112
01:01:51,240 --> 01:01:58,919
example that is a little bit like threat

1113
01:01:55,170 --> 01:02:01,770
hunting in a company a team working on

1114
01:01:58,920 --> 01:02:05,370
threat hunting what is the normal cycle

1115
01:02:01,770 --> 01:02:09,900
then well starting with hypothesis of

1116
01:02:05,370 --> 01:02:12,450
what we're looking for or presupposing

1117
01:02:09,900 --> 01:02:15,930
that attacks will be of a certain type

1118
01:02:12,450 --> 01:02:19,830
and see how it works with certain data

1119
01:02:15,930 --> 01:02:22,890
sets we then collect the data that we

1120
01:02:19,830 --> 01:02:26,130
think will be interesting for the

1121
01:02:22,890 --> 01:02:30,029
attacking techniques for us to explore

1122
01:02:26,130 --> 01:02:34,890
that and then we start discovering the

1123
01:02:30,030 --> 01:02:38,610
TTP's sometimes datasets are huge with

1124
01:02:34,890 --> 01:02:41,040
so many different imports and where can

1125
01:02:38,610 --> 01:02:45,330
we start well that's what clustering is

1126
01:02:41,040 --> 01:02:49,520
useful if malicious activities happen

1127
01:02:45,330 --> 01:02:52,830
with cholesteric we can start analyzing

1128
01:02:49,520 --> 01:02:55,350
smaller groups or clusters to see how

1129
01:02:52,830 --> 01:02:58,940
the activity works there let's have a

1130
01:02:55,350 --> 01:02:58,940
look then at the example

1131
01:03:08,680 --> 01:03:16,910
and let them look at the Moblin delicacy

1132
01:03:10,970 --> 01:03:21,980
in them here is our example I start with

1133
01:03:16,910 --> 01:03:24,730
a set of URLs legit URLs and then I said

1134
01:03:21,980 --> 01:03:30,020
of URLs that are used for malware

1135
01:03:24,730 --> 01:03:33,770
distribution overall it's about half a

1136
01:03:30,020 --> 01:03:36,560
million URLs which one's belong to

1137
01:03:33,770 --> 01:03:40,220
malware distribution which ones are

1138
01:03:36,560 --> 01:03:44,299
legit that's the exercise the threat

1139
01:03:40,220 --> 01:03:49,310
hunting exercise might be caused because

1140
01:03:44,300 --> 01:03:52,460
some devices have been infected and we

1141
01:03:49,310 --> 01:03:55,400
want to find which malware have been

1142
01:03:52,460 --> 01:03:58,070
used imagine half a million pieces of

1143
01:03:55,400 --> 01:03:59,900
information where can we start well if

1144
01:03:58,070 --> 01:04:03,050
there has been classification the

1145
01:03:59,900 --> 01:04:06,620
algorithm clustering algorithm won't see

1146
01:04:03,050 --> 01:04:09,680
this URL this has been tagged so that

1147
01:04:06,620 --> 01:04:13,490
once I have my clusters I can get their

1148
01:04:09,680 --> 01:04:17,569
tags and show you which URLs are

1149
01:04:13,490 --> 01:04:19,549
malicious in a cluster which legit URLs

1150
01:04:17,570 --> 01:04:23,780
are in a different cluster does this

1151
01:04:19,550 --> 01:04:27,670
work properly or not very importantly we

1152
01:04:23,780 --> 01:04:27,670
have the pre-processing step

1153
01:04:38,010 --> 01:04:45,820
here we have a URL set with text

1154
01:04:43,150 --> 01:04:48,610
characters numbers and with the same

1155
01:04:45,820 --> 01:04:51,150
problem we saw earlier the machine

1156
01:04:48,610 --> 01:04:55,390
learning algorithm gets numerical

1157
01:04:51,150 --> 01:04:57,910
information but what we have is text

1158
01:04:55,390 --> 01:05:02,080
change so we can't just feed that and

1159
01:04:57,910 --> 01:05:05,770
expect a perfect perfect URL clustering

1160
01:05:02,080 --> 01:05:10,540
how can we turn the URLs into numerical

1161
01:05:05,770 --> 01:05:13,830
information representing legit URLs and

1162
01:05:10,540 --> 01:05:17,740
malware URLs we need to do

1163
01:05:13,830 --> 01:05:20,770
pre-processing of our data set and get

1164
01:05:17,740 --> 01:05:24,279
certain features that we call dimensions

1165
01:05:20,770 --> 01:05:27,190
those are the different columns look at

1166
01:05:24,280 --> 01:05:30,510
the ones I've extracted firstly Anthropy

1167
01:05:27,190 --> 01:05:30,510
what is entropy

1168
01:05:32,250 --> 01:05:38,890
Fannin's entropy says the following well

1169
01:05:36,940 --> 01:05:46,600
it finds the amount of information on it

1170
01:05:38,890 --> 01:05:51,310
takes chain which premise can I use this

1171
01:05:46,600 --> 01:05:56,319
is my own idea extracting the entropy

1172
01:05:51,310 --> 01:05:59,259
from the URL and that will finally give

1173
01:05:56,320 --> 01:06:01,960
me a number that's the idea so the

1174
01:05:59,260 --> 01:06:04,840
premise is the following well we have no

1175
01:06:01,960 --> 01:06:10,930
more behavior on a website on an domain

1176
01:06:04,840 --> 01:06:13,990
by a user the idea is that the user will

1177
01:06:10,930 --> 01:06:18,160
be browsing from one area to the other

1178
01:06:13,990 --> 01:06:21,430
for instance opening a YouTube videos

1179
01:06:18,160 --> 01:06:24,339
what would change just some numbers

1180
01:06:21,430 --> 01:06:27,730
right because information is quite

1181
01:06:24,340 --> 01:06:31,570
similar for those URLs when we calculate

1182
01:06:27,730 --> 01:06:35,610
entropy we see how and that result is

1183
01:06:31,570 --> 01:06:39,820
then clustered quite nicely for legit

1184
01:06:35,610 --> 01:06:43,490
URL but when we exit a malware URL and

1185
01:06:39,820 --> 01:06:48,240
we download dog fire whatever Kanta

1186
01:06:43,490 --> 01:06:52,350
we would click and we would end up on a

1187
01:06:48,240 --> 01:06:54,330
weird URL with a very long address with

1188
01:06:52,350 --> 01:06:57,360
a lot of numbers with very little

1189
01:06:54,330 --> 01:07:00,330
information and the entropy is quite

1190
01:06:57,360 --> 01:07:03,390
weird right err there might be some

1191
01:07:00,330 --> 01:07:05,430
legit ones that will read like that as

1192
01:07:03,390 --> 01:07:08,670
well but very few II this could be a

1193
01:07:05,430 --> 01:07:12,450
good start for classification as a

1194
01:07:08,670 --> 01:07:18,450
feature to measure distance what else

1195
01:07:12,450 --> 01:07:23,189
can I use TLB o TLD sorry to see if it's

1196
01:07:18,450 --> 01:07:26,310
a part of malware TLD set so I would

1197
01:07:23,190 --> 01:07:28,530
have my column with TLD malware and I

1198
01:07:26,310 --> 01:07:32,070
would score a zero otherwise it would

1199
01:07:28,530 --> 01:07:36,270
score as one I would also extract the

1200
01:07:32,070 --> 01:07:40,110
type of resource that is accessed with

1201
01:07:36,270 --> 01:07:43,800
numerical coding I would see if it's an

1202
01:07:40,110 --> 01:07:47,760
IP address or a domain because sometimes

1203
01:07:43,800 --> 01:07:50,400
well we download malware it's an IP

1204
01:07:47,760 --> 01:07:55,560
because they haven't even got their own

1205
01:07:50,400 --> 01:07:59,460
domain so we with the data I would then

1206
01:07:55,560 --> 01:08:03,150
need to make it readable by pandas or by

1207
01:07:59,460 --> 01:08:06,660
CSV in this case look at this here this

1208
01:08:03,150 --> 01:08:09,720
is a way to justify to you here what I

1209
01:08:06,660 --> 01:08:14,520
explained earlier the functions I

1210
01:08:09,720 --> 01:08:18,858
defined are applied on the URL set with

1211
01:08:14,520 --> 01:08:18,859
the apply panda function

1212
01:08:19,849 --> 01:08:25,529
I'm not a developer right I work with

1213
01:08:23,698 --> 01:08:28,528
cybersecurity but the first thing that

1214
01:08:25,529 --> 01:08:31,859
came to mind is that I have defined my

1215
01:08:28,529 --> 01:08:35,670
functions I could have my loop for loop

1216
01:08:31,859 --> 01:08:38,219
so that I can extract the entropy of the

1217
01:08:35,670 --> 01:08:42,569
function the entropy of the URL and so

1218
01:08:38,219 --> 01:08:46,408
on well when I did it I estimated 13

1219
01:08:42,569 --> 01:08:51,750
days to cover those half a million URLs

1220
01:08:46,408 --> 01:08:56,518
but using panda it took how long four

1221
01:08:51,750 --> 01:08:59,429
minutes for the half a million URLs can

1222
01:08:56,519 --> 01:09:04,380
you see how having good frameworks for

1223
01:08:59,429 --> 01:09:07,589
data processing loading for a large data

1224
01:09:04,380 --> 01:09:10,828
sense like half a million right which is

1225
01:09:07,589 --> 01:09:16,469
not the biggest we can come across at

1226
01:09:10,828 --> 01:09:20,130
all this is the script that we use on

1227
01:09:16,469 --> 01:09:29,069
our URLs and this is the result this

1228
01:09:20,130 --> 01:09:33,630
data set just see how the data set now

1229
01:09:29,069 --> 01:09:36,869
looks much better it looks much more

1230
01:09:33,630 --> 01:09:41,038
numerical still some categorical data

1231
01:09:36,868 --> 01:09:43,679
that we will later work with not horse

1232
01:09:41,038 --> 01:09:46,170
encoding because k-means doesn't work

1233
01:09:43,679 --> 01:09:49,139
with that very well and also we're

1234
01:09:46,170 --> 01:09:52,309
including URLs here why because when I

1235
01:09:49,139 --> 01:09:55,320
process with the algorithm I won't be

1236
01:09:52,309 --> 01:09:59,489
classifying only the clusters with

1237
01:09:55,320 --> 01:10:03,449
numerical values on top of that I will

1238
01:09:59,489 --> 01:10:06,299
then have the URL on the corresponding

1239
01:10:03,449 --> 01:10:08,759
hosta so that URLs will be in different

1240
01:10:06,300 --> 01:10:11,510
files belonging to different plastic

1241
01:10:08,760 --> 01:10:14,429
clusters and I can then analyze that

1242
01:10:11,510 --> 01:10:18,019
let's have a look at the file in charge

1243
01:10:14,429 --> 01:10:18,019
of that analysis

1244
01:10:26,110 --> 01:10:31,549
is the same thing we saw for linear

1245
01:10:28,670 --> 01:10:37,940
regression if the same frameworks I used

1246
01:10:31,550 --> 01:10:42,580
earlier this is our new URL data set

1247
01:10:37,940 --> 01:10:48,169
that we calculated earlier we use get

1248
01:10:42,580 --> 01:10:52,790
dummies which is a panda resource for

1249
01:10:48,170 --> 01:10:54,949
binary encoding we have loaded some

1250
01:10:52,790 --> 01:10:59,330
variables that were not interested in so

1251
01:10:54,949 --> 01:11:03,650
I'm going to do a drop of the URL the

1252
01:10:59,330 --> 01:11:11,809
entropy now not the entropy I'm going to

1253
01:11:03,650 --> 01:11:15,920
leave that out and the label so for the

1254
01:11:11,810 --> 01:11:18,410
URL and the label so that the data set

1255
01:11:15,920 --> 01:11:20,690
can understand it it can't understand

1256
01:11:18,410 --> 01:11:24,199
the URLs right and then we will leave

1257
01:11:20,690 --> 01:11:26,870
the label out later because is if it's a

1258
01:11:24,199 --> 01:11:30,110
malicious URL because it's not

1259
01:11:26,870 --> 01:11:33,170
supervised learning they don't need the

1260
01:11:30,110 --> 01:11:36,830
label it just works with a structure so

1261
01:11:33,170 --> 01:11:39,560
we do reprocessing we've done scaling

1262
01:11:36,830 --> 01:11:42,559
before because some entropy is standard

1263
01:11:39,560 --> 01:11:45,080
for point something six point something

1264
01:11:42,560 --> 01:11:48,830
then we have the encoding with values of

1265
01:11:45,080 --> 01:11:52,400
roundabout one so there's quite a

1266
01:11:48,830 --> 01:11:58,010
difference right if the scaling will

1267
01:11:52,400 --> 01:12:02,420
help equate equalize that because we're

1268
01:11:58,010 --> 01:12:04,850
using k-means and it might work wrong

1269
01:12:02,420 --> 01:12:08,600
when the data set is not properly

1270
01:12:04,850 --> 01:12:13,489
balanced so once that's ready all we do

1271
01:12:08,600 --> 01:12:14,390
is we instance Kami k-means very simple

1272
01:12:13,489 --> 01:12:17,150
low garage

1273
01:12:14,390 --> 01:12:19,019
we need to indicate the number of

1274
01:12:17,150 --> 01:12:20,938
clusters

1275
01:12:19,019 --> 01:12:24,389
looking and trying to differentiate

1276
01:12:20,939 --> 01:12:27,959
between alleged and malicious URLs I

1277
01:12:24,389 --> 01:12:30,359
have between one and three times the

1278
01:12:27,959 --> 01:12:33,809
number of URLs that's reasonable so I'm

1279
01:12:30,359 --> 01:12:36,029
going to say that's four clusters so

1280
01:12:33,809 --> 01:12:39,689
once we have the number of clusters here

1281
01:12:36,029 --> 01:12:43,409
we do feed predict that means we don't

1282
01:12:39,689 --> 01:12:47,849
need to feed the labels just the data

1283
01:12:43,409 --> 01:12:52,759
set and then that will contain the

1284
01:12:47,849 --> 01:12:57,260
clusters and the examples the loop then

1285
01:12:52,760 --> 01:13:00,719
gets the URLs into the right cluster and

1286
01:12:57,260 --> 01:13:04,939
we end up with the files and this is the

1287
01:13:00,719 --> 01:13:07,739
metrics that's a bit weird

1288
01:13:04,939 --> 01:13:12,269
what are we measuring at the end of the

1289
01:13:07,739 --> 01:13:14,848
day we're not measuring probability of

1290
01:13:12,269 --> 01:13:19,109
it being right or not we're measuring

1291
01:13:14,849 --> 01:13:23,069
how well the clusters are defined that

1292
01:13:19,109 --> 01:13:26,519
is containing as many instances of a

1293
01:13:23,069 --> 01:13:29,399
particular type and as few instances of

1294
01:13:26,519 --> 01:13:34,949
a different type so that's where we use

1295
01:13:29,399 --> 01:13:39,199
purity score getting both clusters that

1296
01:13:34,949 --> 01:13:42,388
have been predicted and also real labels

1297
01:13:39,199 --> 01:13:44,339
that's why I have my labels and it will

1298
01:13:42,389 --> 01:13:47,849
check if the clustering is correct

1299
01:13:44,339 --> 01:13:50,549
there's another type of metrics that I

1300
01:13:47,849 --> 01:13:53,729
can show you in a minute there's a

1301
01:13:50,549 --> 01:13:58,169
silhouette coefficient to see if

1302
01:13:53,729 --> 01:14:00,149
clusters have been found correctly on

1303
01:13:58,169 --> 01:14:02,189
the basis of some mathematical

1304
01:14:00,149 --> 01:14:04,679
properties and it doesn't even need the

1305
01:14:02,189 --> 01:14:08,159
labels for the clusters to know if

1306
01:14:04,679 --> 01:14:08,579
they've been described or found properly

1307
01:14:08,159 --> 01:14:11,398
or not

1308
01:14:08,579 --> 01:14:15,268
if we have time later time later we will

1309
01:14:11,399 --> 01:14:19,049
see how this compares with labels in one

1310
01:14:15,269 --> 01:14:23,869
case without in a separate example and

1311
01:14:19,049 --> 01:14:23,869
the final outcome is pretty similar

1312
01:14:25,940 --> 01:14:30,349
we're going to run the scripts now

1313
01:14:34,620 --> 01:14:42,150
here it is and look how it predicts

1314
01:14:38,790 --> 01:14:45,070
k-means is often used in production

1315
01:14:42,150 --> 01:14:47,980
environments because is ever so fast so

1316
01:14:45,070 --> 01:14:51,210
fast this is how long it took for half a

1317
01:14:47,980 --> 01:14:56,110
million URLs this is the classification

1318
01:14:51,210 --> 01:14:57,810
it says there is a cluster with two

1319
01:14:56,110 --> 01:15:01,950
thousand seven hundred and eight

1320
01:14:57,810 --> 01:15:06,280
instances where over two thousand are

1321
01:15:01,950 --> 01:15:09,639
malicious as an amazing classification

1322
01:15:06,280 --> 01:15:11,410
most of the examples are malicious then

1323
01:15:09,640 --> 01:15:13,780
we have three hundred and seventy-nine

1324
01:15:11,410 --> 01:15:17,559
thousand euros worth thirty four

1325
01:15:13,780 --> 01:15:21,070
thousand are malicious URLs that have

1326
01:15:17,560 --> 01:15:24,430
been included this is with simple

1327
01:15:21,070 --> 01:15:26,769
parameters right the only complex one is

1328
01:15:24,430 --> 01:15:29,440
entropy the rest of the parameters are

1329
01:15:26,770 --> 01:15:32,440
very simple and in two more clusters one

1330
01:15:29,440 --> 01:15:38,440
with six hundred and forty instances

1331
01:15:32,440 --> 01:15:42,160
where 628 are URLs for malware to be

1332
01:15:38,440 --> 01:15:47,320
downloaded and then another cluster with

1333
01:15:42,160 --> 01:15:50,730
over six thousand URLs with again most

1334
01:15:47,320 --> 01:15:56,160
of them being more where excellent

1335
01:15:50,730 --> 01:15:59,530
classification where we have less data

1336
01:15:56,160 --> 01:16:03,630
we will find URLs with common features

1337
01:15:59,530 --> 01:16:10,300
that have been used to download malware

1338
01:16:03,630 --> 01:16:14,640
and always on a real data set the purity

1339
01:16:10,300 --> 01:16:18,810
score is north point nine percent

1340
01:16:14,640 --> 01:16:24,030
logically so and this one is 34,000 but

1341
01:16:18,810 --> 01:16:30,870
as opposed to almost 400,000 original

1342
01:16:24,030 --> 01:16:36,210
ones here we have our clusters it's open

1343
01:16:30,870 --> 01:16:38,090
data set zero that had some 2,000

1344
01:16:36,210 --> 01:16:43,140
malware URLs

1345
01:16:38,090 --> 01:16:47,270
these are URLs that have been used at

1346
01:16:43,140 --> 01:16:52,400
different times for malware distribution

1347
01:16:47,270 --> 01:16:57,350
they all have IP addresses as domains

1348
01:16:52,400 --> 01:16:57,349
look at the resources that are accessed

1349
01:16:58,640 --> 01:17:06,470
I'm going to show you now a different

1350
01:17:01,230 --> 01:17:10,740
one another cluster cluster number two

1351
01:17:06,470 --> 01:17:13,790
how was that group together look at that

1352
01:17:10,740 --> 01:17:16,980
URLs that were distributed for malware

1353
01:17:13,790 --> 01:17:23,130
distribution this is the indicator the

1354
01:17:16,980 --> 01:17:32,730
real data set it has the number for

1355
01:17:23,130 --> 01:17:36,030
every instance Kansas City hotel dot XA

1356
01:17:32,730 --> 01:17:39,599
for some of them or dot zip that might

1357
01:17:36,030 --> 01:17:42,570
not look very obvious right probably

1358
01:17:39,600 --> 01:17:46,280
because of the domain so the algorithm

1359
01:17:42,570 --> 01:17:48,719
was fed some parameters and it then uses

1360
01:17:46,280 --> 01:17:51,240
function just like we saw for linear

1361
01:17:48,720 --> 01:17:53,470
regression earlier but not the same one

1362
01:17:51,240 --> 01:17:57,530
but in

1363
01:17:53,470 --> 01:18:01,900
similar operation mode it adjusts

1364
01:17:57,530 --> 01:18:06,380
parameters it clusters URLs together and

1365
01:18:01,900 --> 01:18:09,230
with a brute-force attack you don't need

1366
01:18:06,380 --> 01:18:12,560
to say well over ten predictions that

1367
01:18:09,230 --> 01:18:15,919
will be considered as brute force no it

1368
01:18:12,560 --> 01:18:18,710
just needs mathematical calculations so

1369
01:18:15,920 --> 01:18:22,400
that's it for clustering and now let's

1370
01:18:18,710 --> 01:18:25,700
see something well the last section in

1371
01:18:22,400 --> 01:18:30,250
my presentation that's very popular in

1372
01:18:25,700 --> 01:18:34,720
cyber security and is anomaly detection

1373
01:18:30,250 --> 01:18:38,180
I'm going to start talking about a

1374
01:18:34,720 --> 01:18:42,250
intrusion detection which falls into two

1375
01:18:38,180 --> 01:18:46,090
different categories when we find an

1376
01:18:42,250 --> 01:18:50,570
intrusion it can be based on heuristics

1377
01:18:46,090 --> 01:18:54,410
that type of rule that most companies

1378
01:18:50,570 --> 01:18:57,370
use it finds false positives but it

1379
01:18:54,410 --> 01:19:02,470
doesn't work very well for unknown or

1380
01:18:57,370 --> 01:19:07,340
totally new attacks through heuristics

1381
01:19:02,470 --> 01:19:09,920
rules help this happen and then there's

1382
01:19:07,340 --> 01:19:13,850
a different way of doing this based on

1383
01:19:09,920 --> 01:19:18,260
animal anomalies profiling normal

1384
01:19:13,850 --> 01:19:23,150
behavior in a system could be a website

1385
01:19:18,260 --> 01:19:25,670
host whatever and it can find new

1386
01:19:23,150 --> 01:19:29,620
attacks because it can assess events

1387
01:19:25,670 --> 01:19:33,890
that deviate from the norm that is

1388
01:19:29,620 --> 01:19:36,320
anomaly evaluation or assessment so when

1389
01:19:33,890 --> 01:19:40,070
we talk about anomaly detection we need

1390
01:19:36,320 --> 01:19:43,130
to first define what an anomaly is it's

1391
01:19:40,070 --> 01:19:48,200
an event that deviates from normal or

1392
01:19:43,130 --> 01:19:51,640
expected behavior expected from a

1393
01:19:48,200 --> 01:19:51,639
security perspective

1394
01:19:51,690 --> 01:19:59,429
because anomalies don't just have to do

1395
01:19:55,200 --> 01:20:02,010
with security sometimes they have to do

1396
01:19:59,430 --> 01:20:08,400
with performance problems for instance

1397
01:20:02,010 --> 01:20:11,930
we have NIH D s finding anomalies on a

1398
01:20:08,400 --> 01:20:14,940
host for instance a system might have a

1399
01:20:11,930 --> 01:20:16,650
huge load because we're rendering a

1400
01:20:14,940 --> 01:20:19,469
video well that's an anomaly

1401
01:20:16,650 --> 01:20:23,089
compared with no more behavior for the

1402
01:20:19,470 --> 01:20:24,690
processor not caused in this case by a

1403
01:20:23,090 --> 01:20:29,280
security breach

1404
01:20:24,690 --> 01:20:35,639
how did anomaly detection start well to

1405
01:20:29,280 --> 01:20:39,269
detect intrusions proposed in 1976 by

1406
01:20:35,640 --> 01:20:43,100
Dorothy Jennings in her famous paper

1407
01:20:39,270 --> 01:20:47,040
people are still saying this is new but

1408
01:20:43,100 --> 01:20:49,880
back in the day this paper proposed a

1409
01:20:47,040 --> 01:20:53,220
anomaly detection for this very purpose

1410
01:20:49,880 --> 01:20:56,640
so how is machine learning connected

1411
01:20:53,220 --> 01:20:59,910
with anomaly detection here we have a

1412
01:20:56,640 --> 01:21:04,500
definition in English and it says that a

1413
01:20:59,910 --> 01:21:07,889
machine learning algorithm attempts to

1414
01:21:04,500 --> 01:21:11,400
recognize complex patterns in existing

1415
01:21:07,890 --> 01:21:17,070
data sets to help make intelligent

1416
01:21:11,400 --> 01:21:21,299
decisions or predictions when the

1417
01:21:17,070 --> 01:21:24,860
pattern deviates from the norm and what

1418
01:21:21,300 --> 01:21:29,430
it does is finding non-trivial methods

1419
01:21:24,860 --> 01:21:33,120
to find them so this a new category of

1420
01:21:29,430 --> 01:21:38,030
machine learning that is semi-supervised

1421
01:21:33,120 --> 01:21:42,120
learning because some algorithms will be

1422
01:21:38,030 --> 01:21:45,590
completely fully non supervised but some

1423
01:21:42,120 --> 01:21:55,159
of them need

1424
01:21:45,590 --> 01:21:58,800
at least your set of alleged data not

1425
01:21:55,159 --> 01:22:03,589
labeled with alleged or malicious

1426
01:21:58,800 --> 01:22:07,769
examples but a set of alleged examples

1427
01:22:03,590 --> 01:22:13,079
the system profiles a function and

1428
01:22:07,769 --> 01:22:16,679
whenever there is an out layer then that

1429
01:22:13,079 --> 01:22:17,820
will be studied as a possible security

1430
01:22:16,679 --> 01:22:19,679
breach

1431
01:22:17,820 --> 01:22:22,530
so is semi-supervised because the

1432
01:22:19,679 --> 01:22:25,110
supervisor system needs both legit and

1433
01:22:22,530 --> 01:22:29,099
malicious examples from the real world

1434
01:22:25,110 --> 01:22:31,499
the non supervisor examples doesn't need

1435
01:22:29,099 --> 01:22:35,099
either of them we don't need to

1436
01:22:31,499 --> 01:22:39,059
differentiate between them and in this

1437
01:22:35,099 --> 01:22:42,329
semi-supervised learning it needs to

1438
01:22:39,059 --> 01:22:46,559
know at least some of the legit examples

1439
01:22:42,329 --> 01:22:51,179
are legit legitimate the most simple

1440
01:22:46,559 --> 01:22:56,070
example is a Gaussian distribution which

1441
01:22:51,179 --> 01:22:58,949
is a probability distribution for

1442
01:22:56,070 --> 01:23:02,969
continuous variables here we can see the

1443
01:22:58,949 --> 01:23:06,360
density graph representing a Gaussian

1444
01:23:02,969 --> 01:23:08,309
distribution poisson distribution says

1445
01:23:06,360 --> 01:23:11,579
the following don't panic when you see

1446
01:23:08,309 --> 01:23:14,419
that formula it says for this particular

1447
01:23:11,579 --> 01:23:17,070
point if I get an example their

1448
01:23:14,419 --> 01:23:19,499
probability will be far higher than if I

1449
01:23:17,070 --> 01:23:22,438
get a point here at this end of the

1450
01:23:19,499 --> 01:23:26,039
curve so if our data are distributed

1451
01:23:22,439 --> 01:23:29,909
according to Gaussian distribution then

1452
01:23:26,039 --> 01:23:34,650
we will adjust parameters just like the

1453
01:23:29,909 --> 01:23:38,320
linear function we saw earlier but in

1454
01:23:34,650 --> 01:23:41,740
this case a parameter is a

1455
01:23:38,320 --> 01:23:44,259
depend on variants the metrics and

1456
01:23:41,740 --> 01:23:47,950
typical distribution so let's do

1457
01:23:44,260 --> 01:23:52,780
something similar now but starting with

1458
01:23:47,950 --> 01:23:55,269
legit examples a-brewing air the curve

1459
01:23:52,780 --> 01:24:04,139
the function then adjusting the

1460
01:23:55,270 --> 01:24:04,140
parameters and then this function this

1461
01:24:05,190 --> 01:24:14,589
will then give back a probability we

1462
01:24:11,470 --> 01:24:16,960
would then set a threshold a limit so

1463
01:24:14,590 --> 01:24:19,180
that when there's a new example it will

1464
01:24:16,960 --> 01:24:21,490
be assessed according to Gaussian

1465
01:24:19,180 --> 01:24:26,530
distribution according to the model that

1466
01:24:21,490 --> 01:24:29,889
I've described and adjusted for my data

1467
01:24:26,530 --> 01:24:33,130
set and if the probability is too low or

1468
01:24:29,890 --> 01:24:38,110
under my threshold then it will be an

1469
01:24:33,130 --> 01:24:40,930
anomaly but if it's within the limit or

1470
01:24:38,110 --> 01:24:43,150
larger than the threshold it will be

1471
01:24:40,930 --> 01:24:44,980
considered as a normal example something

1472
01:24:43,150 --> 01:24:48,250
the threshold is here to the left if

1473
01:24:44,980 --> 01:24:52,540
most data are to the right then the ones

1474
01:24:48,250 --> 01:24:57,070
to the left will be unusual or non

1475
01:24:52,540 --> 01:24:59,650
typical so malicious behavior the idea

1476
01:24:57,070 --> 01:25:03,910
is that is normally quite different from

1477
01:24:59,650 --> 01:25:08,740
legitimate behavior I bet you've often

1478
01:25:03,910 --> 01:25:12,099
seen situations where you're you have

1479
01:25:08,740 --> 01:25:15,550
the logs of our Apache server for

1480
01:25:12,100 --> 01:25:21,100
instance and we try to do detect SQL

1481
01:25:15,550 --> 01:25:26,230
injections and normally when that

1482
01:25:21,100 --> 01:25:29,890
happens the requests are well defined

1483
01:25:26,230 --> 01:25:33,879
for instance with the same reserved

1484
01:25:29,890 --> 01:25:37,900
words quite predictable but in an attack

1485
01:25:33,880 --> 01:25:41,320
in an attack SQL SQL injections are

1486
01:25:37,900 --> 01:25:44,769
longer they might use unions they might

1487
01:25:41,320 --> 01:25:47,929
add other requests for the database or

1488
01:25:44,770 --> 01:25:51,830
using reserved words that are often

1489
01:25:47,930 --> 01:25:55,550
not used so logically if we have

1490
01:25:51,830 --> 01:25:57,830
profiled our normal requests then we

1491
01:25:55,550 --> 01:26:00,740
will easily find anomalies and that will

1492
01:25:57,830 --> 01:26:05,809
tell us something odd or nasty is

1493
01:26:00,740 --> 01:26:09,440
happening how how does the algorithm

1494
01:26:05,810 --> 01:26:12,740
work with Gaussian distribution we would

1495
01:26:09,440 --> 01:26:15,799
select characteristics or features that

1496
01:26:12,740 --> 01:26:19,160
can determine an example or define it as

1497
01:26:15,800 --> 01:26:21,110
anomalous then we would adjust the

1498
01:26:19,160 --> 01:26:23,660
parameters for the model well the

1499
01:26:21,110 --> 01:26:27,259
algorithm will do that and then given an

1500
01:26:23,660 --> 01:26:30,320
example we would compute the probability

1501
01:26:27,260 --> 01:26:33,800
if it's below epsilon or threshold then

1502
01:26:30,320 --> 01:26:36,080
it will be classed as an anomaly let's

1503
01:26:33,800 --> 01:26:41,020
see a practical example for anomaly

1504
01:26:36,080 --> 01:26:46,730
detection applied in this case to

1505
01:26:41,020 --> 01:26:49,580
banking fraud with credit cards this is

1506
01:26:46,730 --> 01:26:52,549
the normal follow when we pay with a

1507
01:26:49,580 --> 01:26:55,940
credit card this is how I've imagined it

1508
01:26:52,550 --> 01:27:02,510
I don't really know the details we pay

1509
01:26:55,940 --> 01:27:06,440
with a card for authorization then the

1510
01:27:02,510 --> 01:27:08,840
merchant bank is involved the issuer

1511
01:27:06,440 --> 01:27:12,349
approves the pavement and then the

1512
01:27:08,840 --> 01:27:15,260
reverse route until the payment is

1513
01:27:12,350 --> 01:27:17,930
confirmed we can use the anomaly

1514
01:27:15,260 --> 01:27:23,630
detection algorithm where here at this

1515
01:27:17,930 --> 01:27:27,950
point with an anomaly detection

1516
01:27:23,630 --> 01:27:30,530
algorithm if the transaction is classed

1517
01:27:27,950 --> 01:27:32,840
as anomalous on the basis of the

1518
01:27:30,530 --> 01:27:37,009
transactions that individual normally

1519
01:27:32,840 --> 01:27:42,790
makes then it has a high probability of

1520
01:27:37,010 --> 01:27:42,790
being malicious or a fruit

1521
01:27:47,210 --> 01:27:58,830
very far Esteban's under that I'm gonna

1522
01:27:55,440 --> 01:28:03,839
use this set of data this this set of

1523
01:27:58,830 --> 01:28:07,050
data here is real we got several

1524
01:28:03,840 --> 01:28:12,960
features of a transaction using a credit

1525
01:28:07,050 --> 01:28:14,760
card this was published by a company but

1526
01:28:12,960 --> 01:28:16,890
mainly those our number is very close to

1527
01:28:14,760 --> 01:28:19,770
get that and why is that and how can

1528
01:28:16,890 --> 01:28:22,800
that up there we have visas and v7

1529
01:28:19,770 --> 01:28:25,560
because they follow a dimensionality

1530
01:28:22,800 --> 01:28:28,620
reduction algorithm which is PZ a PCA

1531
01:28:25,560 --> 01:28:31,170
gets our data set and builds the main

1532
01:28:28,620 --> 01:28:33,090
components and with those it produces

1533
01:28:31,170 --> 01:28:35,969
the same columns that we had with the

1534
01:28:33,090 --> 01:28:37,949
same data and the Trent is somehow kept

1535
01:28:35,969 --> 01:28:42,330
but different values which are scale

1536
01:28:37,949 --> 01:28:44,460
Aidid standardized and the data set is

1537
01:28:42,330 --> 01:28:46,410
an anonymized because if it were an

1538
01:28:44,460 --> 01:28:51,210
anonymous we could not get the data

1539
01:28:46,410 --> 01:28:53,730
because there was a favorite data but

1540
01:28:51,210 --> 01:28:58,850
once of an anonymized we have all these

1541
01:28:53,730 --> 01:29:03,169
red Sanchez classified as zero legit one

1542
01:28:58,850 --> 01:29:06,420
malicious we will not be using malicious

1543
01:29:03,170 --> 01:29:09,239
crooks actions but is still good to see

1544
01:29:06,420 --> 01:29:11,160
the result actually this is an easy

1545
01:29:09,239 --> 01:29:14,940
classification for a bank because if a

1546
01:29:11,160 --> 01:29:17,699
client comes and is not complaining it

1547
01:29:14,940 --> 01:29:19,230
is legit and if it claims that they been

1548
01:29:17,699 --> 01:29:22,370
stolen or they've been robbed the first

1549
01:29:19,230 --> 01:29:25,049
thing they do is call and say some words

1550
01:29:22,370 --> 01:29:28,230
but something with my credit card and so

1551
01:29:25,050 --> 01:29:29,820
you can classify it as a fraud and then

1552
01:29:28,230 --> 01:29:32,540
you use it for training your own

1553
01:29:29,820 --> 01:29:32,540
algorithm

1554
01:29:37,880 --> 01:29:42,320
let me show you what should be done

1555
01:29:48,230 --> 01:29:51,679
whatever

1556
01:29:49,790 --> 01:29:54,019
in this region we said that one of the

1557
01:29:51,680 --> 01:29:56,030
premises is that our dataset has to be

1558
01:29:54,020 --> 01:29:59,510
distributed according to a Gaussian

1559
01:29:56,030 --> 01:30:02,509
distribution which is a gauze Bell shape

1560
01:29:59,510 --> 01:30:05,510
is called Bell because the density

1561
01:30:02,510 --> 01:30:09,050
distribution as represented shares like

1562
01:30:05,510 --> 01:30:14,690
a panel and what we also said is that

1563
01:30:09,050 --> 01:30:17,030
for it to work we need to use features

1564
01:30:14,690 --> 01:30:20,059
where the malicious examples are

1565
01:30:17,030 --> 01:30:22,250
different from the legit ones to the

1566
01:30:20,060 --> 01:30:24,620
naked eye this is good when we see the

1567
01:30:22,250 --> 01:30:26,480
data but when we have pz8 and everything

1568
01:30:24,620 --> 01:30:28,130
has been anonymized to the naked eye

1569
01:30:26,480 --> 01:30:31,219
it's a bit more difficult

1570
01:30:28,130 --> 01:30:33,020
usually we graphically represent the

1571
01:30:31,220 --> 01:30:37,010
distribution of data here I've

1572
01:30:33,020 --> 01:30:42,260
represented each of the feature Orange

1573
01:30:37,010 --> 01:30:45,860
you see the examples for the malicious

1574
01:30:42,260 --> 01:30:48,290
feature and in blue that between legit

1575
01:30:45,860 --> 01:30:50,389
sir I see what feature follows the

1576
01:30:48,290 --> 01:30:53,300
Gaussian distribution almost all of them

1577
01:30:50,390 --> 01:30:58,340
and we also see or I also see which of

1578
01:30:53,300 --> 01:31:00,290
these features are more differentiating

1579
01:30:58,340 --> 01:31:02,900
between a positive or a negative example

1580
01:31:00,290 --> 01:31:04,760
here the negative examples are very

1581
01:31:02,900 --> 01:31:07,250
different but this one here is always

1582
01:31:04,760 --> 01:31:10,400
the same this would be a full featured

1583
01:31:07,250 --> 01:31:12,650
she used for an anomaly detection these

1584
01:31:10,400 --> 01:31:15,230
would be for well this one would be

1585
01:31:12,650 --> 01:31:17,750
quite quite good with this here what I

1586
01:31:15,230 --> 01:31:20,150
do is okay I will keep those that I know

1587
01:31:17,750 --> 01:31:22,550
are good because they are differentiated

1588
01:31:20,150 --> 01:31:24,469
and follow a Gaussian distribution all

1589
01:31:22,550 --> 01:31:28,420
others will be removed from the dataset

1590
01:31:24,470 --> 01:31:28,420
and so I train my algorithm

1591
01:31:36,280 --> 01:31:52,219
okay but so that you'd see that this is

1592
01:31:50,629 --> 01:31:54,260
the Gaussian distribution that has not

1593
01:31:52,219 --> 01:31:56,449
been implemented as a machine learning

1594
01:31:54,260 --> 01:31:58,159
and saying you need to implement it

1595
01:31:56,449 --> 01:32:01,699
yourselves but it's pretty simple

1596
01:31:58,159 --> 01:32:05,049
actually if you want tape to test it

1597
01:32:01,699 --> 01:32:07,699
just email me and I will send it to you

1598
01:32:05,050 --> 01:32:11,079
but we do what we saw from previous

1599
01:32:07,699 --> 01:32:11,079
example so let's run it

1600
01:32:20,120 --> 01:32:25,450
this is something I haven't told you

1601
01:32:21,890 --> 01:32:29,170
since I need to have a dressy Oh a limit

1602
01:32:25,450 --> 01:32:31,160
above which I would think it's an

1603
01:32:29,170 --> 01:32:33,170
anomaly but I don't really know what the

1604
01:32:31,160 --> 01:32:36,320
limit is here so what I do is I try

1605
01:32:33,170 --> 01:32:38,690
different limit I get my data set and

1606
01:32:36,320 --> 01:32:42,620
split into two or two or three and one

1607
01:32:38,690 --> 01:32:45,650
of them I use this for testing so this

1608
01:32:42,620 --> 01:32:47,930
is forecasting I see the labels I see if

1609
01:32:45,650 --> 01:32:50,240
they much a malicious example and I say

1610
01:32:47,930 --> 01:32:51,890
yes isn't much here so this is a good

1611
01:32:50,240 --> 01:32:53,599
for us we know there's too much a

1612
01:32:51,890 --> 01:32:56,000
difference this is a bad one so I keep

1613
01:32:53,600 --> 01:32:59,150
the best one since their days I said

1614
01:32:56,000 --> 01:33:01,220
I've been using it for deliberate

1615
01:32:59,150 --> 01:33:03,500
selection I cannot use it afterwards for

1616
01:33:01,220 --> 01:33:06,230
testing and why is that because it might

1617
01:33:03,500 --> 01:33:09,320
be there's been what's known as

1618
01:33:06,230 --> 01:33:12,500
overtraining which means that the limit

1619
01:33:09,320 --> 01:33:16,700
is nicely adjusted to the subset of data

1620
01:33:12,500 --> 01:33:19,310
but not for the generic subset and so I

1621
01:33:16,700 --> 01:33:21,230
get just the same set for the limit or a

1622
01:33:19,310 --> 01:33:23,240
throw so and observe my protesting so

1623
01:33:21,230 --> 01:33:25,639
it's a good test because it's been

1624
01:33:23,240 --> 01:33:27,800
selected for it but when deployed real

1625
01:33:25,640 --> 01:33:30,200
time nothing is detected and that's why

1626
01:33:27,800 --> 01:33:33,020
it is important that when any of the

1627
01:33:30,200 --> 01:33:35,389
parameters are gathered like that you

1628
01:33:33,020 --> 01:33:38,150
need to use a different subset for

1629
01:33:35,390 --> 01:33:41,060
testing afterwards when you're going to

1630
01:33:38,150 --> 01:33:48,589
assess it final result if you check it

1631
01:33:41,060 --> 01:33:50,870
out and you will see that it got sorry

1632
01:33:48,590 --> 01:33:53,600
it actually detected the anomalies with

1633
01:33:50,870 --> 01:33:54,349
an 80 plus percent accuracy not so bad

1634
01:33:53,600 --> 01:33:56,990
is it

1635
01:33:54,350 --> 01:33:59,750
so these are the different sections that

1636
01:33:56,990 --> 01:34:02,900
belong to real back in 82 percent of all

1637
01:33:59,750 --> 01:34:04,730
cases so so many because there's been

1638
01:34:02,900 --> 01:34:08,500
thousands and thousands of transactions

1639
01:34:04,730 --> 01:34:08,500
it's been a match and the anomaly

1640
01:34:08,699 --> 01:34:15,928
it was actually a perfect match for a

1641
01:34:11,489 --> 01:34:17,940
fraud or legitimacy so if it's a fraud

1642
01:34:15,929 --> 01:34:20,209
or malicious we can follow other

1643
01:34:17,940 --> 01:34:23,308
techniques we've seen before for

1644
01:34:20,209 --> 01:34:26,039
supervised learning that helps us truly

1645
01:34:23,309 --> 01:34:29,039
verify that it is a fraud and reduce the

1646
01:34:26,039 --> 01:34:33,780
number of false positives not so many

1647
01:34:29,039 --> 01:34:36,900
here and so the last algorithm I got you

1648
01:34:33,780 --> 01:34:43,079
here we have 25 minutes left from

1649
01:34:36,900 --> 01:34:44,848
questions so this is for an early

1650
01:34:43,079 --> 01:34:48,749
detection and country it's a Gaussian

1651
01:34:44,849 --> 01:34:52,709
detection does not need to have the

1652
01:34:48,749 --> 01:34:56,909
legit or the label data set for

1653
01:34:52,709 --> 01:35:03,329
profiling here we have a an algorithm

1654
01:34:56,909 --> 01:35:07,440
which is for trees following two

1655
01:35:03,329 --> 01:35:10,170
premises here the abnormal subset it's a

1656
01:35:07,440 --> 01:35:12,478
minority can curve to the regular data

1657
01:35:10,170 --> 01:35:17,039
and the features are very different from

1658
01:35:12,479 --> 01:35:20,130
normal data before I show you in a graph

1659
01:35:17,039 --> 01:35:22,590
how it works and see the practical

1660
01:35:20,130 --> 01:35:26,099
example that I have for these network

1661
01:35:22,590 --> 01:35:27,599
attack detention detection let me tell

1662
01:35:26,099 --> 01:35:31,199
you that this algorithm is very good

1663
01:35:27,599 --> 01:35:33,749
when you want to use it for network

1664
01:35:31,199 --> 01:35:36,150
traffic abnormalities real time or for

1665
01:35:33,749 --> 01:35:39,090
detection processes without high load

1666
01:35:36,150 --> 01:35:42,360
why is that because it only uses a few

1667
01:35:39,090 --> 01:35:45,150
training data for good results and even

1668
01:35:42,360 --> 01:35:47,848
if we use a very large set there is a

1669
01:35:45,150 --> 01:35:50,518
subsample with just one to examine with

1670
01:35:47,849 --> 01:35:52,440
one example it can decide which ones are

1671
01:35:50,519 --> 01:35:56,729
abnormal and which ones are not so it's

1672
01:35:52,440 --> 01:35:59,009
very fast and it is not many data and

1673
01:35:56,729 --> 01:36:01,170
doesn't need labels

1674
01:35:59,010 --> 01:36:03,510
it's unsupervised so it's good for

1675
01:36:01,170 --> 01:36:06,860
real-time Department and in order to

1676
01:36:03,510 --> 01:36:09,300
have the real time process same in

1677
01:36:06,860 --> 01:36:13,199
reasonable within reasonable deadlines

1678
01:36:09,300 --> 01:36:15,599
how does how does it work first the

1679
01:36:13,199 --> 01:36:18,209
graphic representation how it works and

1680
01:36:15,599 --> 01:36:19,910
you can find on any site or anything

1681
01:36:18,209 --> 01:36:23,510
doing with isolation

1682
01:36:19,910 --> 01:36:27,860
and then shortly I'll explain what this

1683
01:36:23,510 --> 01:36:32,060
is all about let's see let's say we have

1684
01:36:27,860 --> 01:36:34,460
this data set we have two features and

1685
01:36:32,060 --> 01:36:38,330
it shows like that on our table and we

1686
01:36:34,460 --> 01:36:40,970
see human being is quite good at

1687
01:36:38,330 --> 01:36:42,260
spotting abnormalities in sets and

1688
01:36:40,970 --> 01:36:44,990
everything and we see there's a large

1689
01:36:42,260 --> 01:36:47,410
group this one can be thought of as an

1690
01:36:44,990 --> 01:36:50,360
abnormality and up there probably it is

1691
01:36:47,410 --> 01:36:54,530
compared to ours dataset so what about

1692
01:36:50,360 --> 01:36:59,089
isolation first first horizontal and

1693
01:36:54,530 --> 01:37:02,480
vertical cuts or slices so we split it

1694
01:36:59,090 --> 01:37:05,150
here then this one here then this one

1695
01:37:02,480 --> 01:37:07,370
here right here it would be done as

1696
01:37:05,150 --> 01:37:10,040
isolation as a function I salute in

1697
01:37:07,370 --> 01:37:12,410
first counts the number of lines

1698
01:37:10,040 --> 01:37:16,670
vertical and horizontal needed to

1699
01:37:12,410 --> 01:37:18,920
isolate an item from all others based on

1700
01:37:16,670 --> 01:37:21,560
the number of the lines needed for the

1701
01:37:18,920 --> 01:37:23,900
isolation it's considered at animality

1702
01:37:21,560 --> 01:37:27,110
or not the threshold here would be the

1703
01:37:23,900 --> 01:37:29,389
number of lines or katzie's said to say

1704
01:37:27,110 --> 01:37:30,679
this is an end up not animality and only

1705
01:37:29,390 --> 01:37:32,590
animal in Emily because these are

1706
01:37:30,680 --> 01:37:35,360
isolated this one's been left outside

1707
01:37:32,590 --> 01:37:37,580
but this is not because they've not yet

1708
01:37:35,360 --> 01:37:41,240
been isolated by the cats it would take

1709
01:37:37,580 --> 01:37:44,660
many more if we were to choose this one

1710
01:37:41,240 --> 01:37:46,639
here and we would use random lines we

1711
01:37:44,660 --> 01:37:50,990
would need many more to isolate it

1712
01:37:46,640 --> 01:37:52,940
compared to this other one so we've

1713
01:37:50,990 --> 01:37:55,280
already said probably you cannot say

1714
01:37:52,940 --> 01:38:00,200
that but we've said it already that

1715
01:37:55,280 --> 01:38:02,420
actually isolation forest is using

1716
01:38:00,200 --> 01:38:05,360
decision-making forests and how does it

1717
01:38:02,420 --> 01:38:09,200
work you get your data set you get the

1718
01:38:05,360 --> 01:38:11,420
features of that size it looks into the

1719
01:38:09,200 --> 01:38:14,240
features following some scores and

1720
01:38:11,420 --> 01:38:17,740
metrics this would be the din score

1721
01:38:14,240 --> 01:38:17,740
which is another score

1722
01:38:18,020 --> 01:38:22,310
and I will get this value and I will

1723
01:38:21,440 --> 01:38:27,620
split it

1724
01:38:22,310 --> 01:38:29,210
following this feature and valuable this

1725
01:38:27,620 --> 01:38:30,920
is on this one side and then the other

1726
01:38:29,210 --> 01:38:32,840
side and this is branch and another

1727
01:38:30,920 --> 01:38:35,360
branch and all the way up to the

1728
01:38:32,840 --> 01:38:38,060
absolute purity in one branch that is

1729
01:38:35,360 --> 01:38:40,910
would be a Pierce part this is a

1730
01:38:38,060 --> 01:38:43,400
classification algorithm that is super

1731
01:38:40,910 --> 01:38:48,290
fast so you need to know what's legit

1732
01:38:43,400 --> 01:38:50,769
and what's not to keep assessing it so

1733
01:38:48,290 --> 01:38:54,469
sorry it's unsupervised because it does

1734
01:38:50,770 --> 01:38:56,450
calculate the gender score based on the

1735
01:38:54,469 --> 01:39:00,170
cluster density and everything else as

1736
01:38:56,450 --> 01:39:02,870
we were doing so what does it do it

1737
01:39:00,170 --> 01:39:06,080
builds decision trees based on our set

1738
01:39:02,870 --> 01:39:08,599
of data data set and so it will think

1739
01:39:06,080 --> 01:39:10,519
that the cats here so what we represent

1740
01:39:08,600 --> 01:39:13,100
it as Scots well you see these are

1741
01:39:10,520 --> 01:39:16,280
actually separations from our dataset

1742
01:39:13,100 --> 01:39:19,460
and so it becomes subset so we have two

1743
01:39:16,280 --> 01:39:20,960
dataset is split into two subsets we

1744
01:39:19,460 --> 01:39:23,510
have this one here and there's one here

1745
01:39:20,960 --> 01:39:25,370
translate it into a decision tree we

1746
01:39:23,510 --> 01:39:28,070
would have two branches one here one

1747
01:39:25,370 --> 01:39:29,800
here with this data with another card

1748
01:39:28,070 --> 01:39:32,960
actually we are translated it into

1749
01:39:29,800 --> 01:39:35,630
having a card on previous cards and so

1750
01:39:32,960 --> 01:39:38,390
we have more branches created with this

1751
01:39:35,630 --> 01:39:42,020
algorithm under interfaces our normal is

1752
01:39:38,390 --> 01:39:44,810
all those sets that have a lower depth

1753
01:39:42,020 --> 01:39:48,670
or a few branches all the way up to

1754
01:39:44,810 --> 01:39:48,670
purity and that's why it is tree based

1755
01:39:50,620 --> 01:39:55,230
how can we

1756
01:39:52,530 --> 01:39:59,219
kind of algorithms on a real example for

1757
01:39:55,230 --> 01:40:02,428
animal detection on a network we can use

1758
01:39:59,219 --> 01:40:05,880
our network our local network small one

1759
01:40:02,429 --> 01:40:09,179
at home wherever use a tab between the

1760
01:40:05,880 --> 01:40:12,630
router and the devices or use a switch

1761
01:40:09,179 --> 01:40:14,699
through V fry and redirect all the

1762
01:40:12,630 --> 01:40:17,190
traffic of the internet on a machine

1763
01:40:14,699 --> 01:40:19,769
where this algorithm is run real time

1764
01:40:17,190 --> 01:40:22,980
and see whether the traffic sets are

1765
01:40:19,770 --> 01:40:24,420
abnormal anomalies or not we will not do

1766
01:40:22,980 --> 01:40:26,250
it package by package because it would

1767
01:40:24,420 --> 01:40:29,460
be pointless but maybe some closed

1768
01:40:26,250 --> 01:40:32,400
traffic flows with a 10000 network

1769
01:40:29,460 --> 01:40:34,440
packages or sessions or login sessions

1770
01:40:32,400 --> 01:40:37,949
that is and we can group them anywhere

1771
01:40:34,440 --> 01:40:40,888
and any way we like we can have net flow

1772
01:40:37,949 --> 01:40:43,098
standards that starts and many other

1773
01:40:40,889 --> 01:40:43,099
mechanisms

1774
01:40:45,560 --> 01:40:51,860
so all the traffic in and out of the

1775
01:40:48,620 --> 01:40:54,860
network we can check and see when there

1776
01:40:51,860 --> 01:41:00,950
can be some anomaly on the network our

1777
01:40:54,860 --> 01:41:04,070
network so the mechanism we I have not

1778
01:41:00,950 --> 01:41:06,340
deployed real-time but I got here a set

1779
01:41:04,070 --> 01:41:06,340
of data

1780
01:41:14,690 --> 01:41:20,460
so I've got a dataset here which is

1781
01:41:18,210 --> 01:41:23,310
already ready with some features of

1782
01:41:20,460 --> 01:41:26,040
several Network flows and it's

1783
01:41:23,310 --> 01:41:28,320
classified as well here we see this flow

1784
01:41:26,040 --> 01:41:30,960
here which is a regular one you see it

1785
01:41:28,320 --> 01:41:33,509
says Numa regular and then the features

1786
01:41:30,960 --> 01:41:38,250
that got here would be let's see

1787
01:41:33,510 --> 01:41:39,440
incoming or outgoing iTunes many more

1788
01:41:38,250 --> 01:41:40,940
things

1789
01:41:39,440 --> 01:41:46,589
[Music]

1790
01:41:40,940 --> 01:41:49,589
TCP passive open outgoing requests from

1791
01:41:46,590 --> 01:41:53,160
T as if he so so many features and

1792
01:41:49,590 --> 01:41:55,860
different flows for the network traffic

1793
01:41:53,160 --> 01:41:58,320
you just get it pass it as we did with

1794
01:41:55,860 --> 01:42:01,130
the emails to get the features are you

1795
01:41:58,320 --> 01:42:03,900
dump them into your data set and this is

1796
01:42:01,130 --> 01:42:06,210
offline but online you would get the

1797
01:42:03,900 --> 01:42:09,629
flow and send it into our system real

1798
01:42:06,210 --> 01:42:13,850
time and it would give us the odds that

1799
01:42:09,630 --> 01:42:13,850
it is an anomaly or that it is an

1800
01:42:14,450 --> 01:42:24,630
isolation forest is easily implemented

1801
01:42:17,970 --> 01:42:29,520
as well see here I've used a feature

1802
01:42:24,630 --> 01:42:31,710
selection algorithm priorly which do

1803
01:42:29,520 --> 01:42:35,820
what we've done with the Gaussian

1804
01:42:31,710 --> 01:42:37,470
distribution see what the best

1805
01:42:35,820 --> 01:42:40,679
distribution is but here it follows some

1806
01:42:37,470 --> 01:42:42,270
statistical mathematical method says so

1807
01:42:40,680 --> 01:42:45,210
this one is the one with the best

1808
01:42:42,270 --> 01:42:48,330
results are usually and I kept this one

1809
01:42:45,210 --> 01:42:51,900
so we change our isolation first

1810
01:42:48,330 --> 01:42:54,980
algorithm same as we did before and here

1811
01:42:51,900 --> 01:43:02,190
we haven't implemented and we fit it

1812
01:42:54,980 --> 01:43:04,730
into our dataset and the result are some

1813
01:43:02,190 --> 01:43:04,730
scores

1814
01:43:08,270 --> 01:43:15,720
come here I'm detecting brute force and

1815
01:43:12,240 --> 01:43:17,670
brute force in a web app I have it on

1816
01:43:15,720 --> 01:43:20,930
the network and these are easily

1817
01:43:17,670 --> 01:43:24,930
detected because you have someone trying

1818
01:43:20,930 --> 01:43:27,330
user password and everything so this is

1819
01:43:24,930 --> 01:43:29,160
very different from regular queries and

1820
01:43:27,330 --> 01:43:31,230
requests and those flows if I choose

1821
01:43:29,160 --> 01:43:35,330
those flows so if it's a five-minute

1822
01:43:31,230 --> 01:43:38,280
flow that flow work there's some brute

1823
01:43:35,330 --> 01:43:39,750
force brute force in this different form

1824
01:43:38,280 --> 01:43:42,000
what's normal because it might be one

1825
01:43:39,750 --> 01:43:44,640
per minute or even less than that so

1826
01:43:42,000 --> 01:43:49,070
here I find that there's brute force in

1827
01:43:44,640 --> 01:43:53,760
on my network and for my set of data I

1828
01:43:49,070 --> 01:43:56,580
had some two hundred or thousand items

1829
01:43:53,760 --> 01:44:01,760
or lines but then I split the dataset

1830
01:43:56,580 --> 01:44:06,720
into 800 for verification and 200 for

1831
01:44:01,760 --> 01:44:09,090
later purposes and then f1 score that

1832
01:44:06,720 --> 01:44:11,910
it's the score or metrics that will give

1833
01:44:09,090 --> 01:44:15,240
us the percentage of yes the percentage

1834
01:44:11,910 --> 01:44:17,250
that is for the proper classification of

1835
01:44:15,240 --> 01:44:19,050
anomalies that were malicious examples

1836
01:44:17,250 --> 01:44:21,260
and anomalous that were false positive

1837
01:44:19,050 --> 01:44:24,960
there were legit example studies a 92%

1838
01:44:21,260 --> 01:44:27,810
which means the 92% of all the flows

1839
01:44:24,960 --> 01:44:29,970
that have been considered abnormal where

1840
01:44:27,810 --> 01:44:36,060
match two flows that came from full

1841
01:44:29,970 --> 01:44:38,190
brute forcing so that was eight so we've

1842
01:44:36,060 --> 01:44:40,950
actually seen that there was an abnormal

1843
01:44:38,190 --> 01:44:44,160
behavior so we need to give it real-time

1844
01:44:40,950 --> 01:44:46,730
well all well profiles and when we do

1845
01:44:44,160 --> 01:44:48,780
see that there are some real

1846
01:44:46,730 --> 01:44:51,660
abnormalities we need to suspect there

1847
01:44:48,780 --> 01:44:54,570
is something going on and so you get the

1848
01:44:51,660 --> 01:44:56,490
flows you get the network traffic you

1849
01:44:54,570 --> 01:44:58,139
redirected elsewhere to a different

1850
01:44:56,490 --> 01:44:59,990
system and so we have other mechanism

1851
01:44:58,140 --> 01:45:02,750
for machine learning and

1852
01:44:59,990 --> 01:45:05,040
reclassification are all based on

1853
01:45:02,750 --> 01:45:06,930
malware ramzan were for example that

1854
01:45:05,040 --> 01:45:09,240
it's distributed on the network it will

1855
01:45:06,930 --> 01:45:10,389
be easily classified or also brute force

1856
01:45:09,240 --> 01:45:14,409
inner denial of

1857
01:45:10,389 --> 01:45:17,349
service or RPS roofing tax or all those

1858
01:45:14,409 --> 01:45:19,539
attacks that actually Eve we go into it

1859
01:45:17,349 --> 01:45:21,189
we will be able to classify it by

1860
01:45:19,539 --> 01:45:23,228
properly selecting features but if we

1861
01:45:21,189 --> 01:45:25,780
use a classifier just on a large network

1862
01:45:23,229 --> 01:45:27,579
we've got plenty of data we will get

1863
01:45:25,780 --> 01:45:30,489
many of false positives really taken

1864
01:45:27,579 --> 01:45:32,889
abnormal ease we move that elsewhere we

1865
01:45:30,489 --> 01:45:34,309
can classify it and the results are way

1866
01:45:32,889 --> 01:45:36,239
better

1867
01:45:34,310 --> 01:45:41,320
[Music]

1868
01:45:36,239 --> 01:45:43,239
that's me know for me folks thank you

1869
01:45:41,320 --> 01:45:47,530
very much for your attendance I hope I

1870
01:45:43,239 --> 01:45:49,509
did not bore you with my lecture and

1871
01:45:47,530 --> 01:45:54,719
formula and everything else and so I

1872
01:45:49,510 --> 01:45:57,909
have 10 minutes left for questions as

1873
01:45:54,719 --> 01:46:00,070
requested if you want examples of or

1874
01:45:57,909 --> 01:46:02,530
anything of this kind or advanced

1875
01:46:00,070 --> 01:46:07,960
questions you can ask me on Twitter or

1876
01:46:02,530 --> 01:46:14,889
in my email thank you very much

1877
01:46:07,960 --> 01:46:14,890
[Applause]

1878
01:46:18,030 --> 01:46:25,599
analyst Odin he gave us an example for

1879
01:46:22,059 --> 01:46:28,239
isolation forests and you said that was

1880
01:46:25,599 --> 01:46:29,649
about isolating a node right that's one

1881
01:46:28,239 --> 01:46:32,468
it is something no there is

1882
01:46:29,649 --> 01:46:35,050
decision-making trees and it would

1883
01:46:32,469 --> 01:46:41,229
consider that an anomaly is what follows

1884
01:46:35,050 --> 01:46:44,530
and depth very shorter so when does it

1885
01:46:41,229 --> 01:46:46,899
stop building the trees it depends you

1886
01:46:44,530 --> 01:46:49,509
can indicate it well maybe when the know

1887
01:46:46,899 --> 01:46:53,800
disappeared so you only have one kind of

1888
01:46:49,510 --> 01:46:56,229
an example or else so you might have a

1889
01:46:53,800 --> 01:46:57,429
set of data set with lots of times it

1890
01:46:56,229 --> 01:47:01,179
would be a huge tree

1891
01:46:57,429 --> 01:47:04,329
huge and unmanageable tree so you can

1892
01:47:01,179 --> 01:47:06,879
say I don't need the data set or node to

1893
01:47:04,329 --> 01:47:10,719
be pure but I needed to have an 80%

1894
01:47:06,879 --> 01:47:13,209
purity so when it says that in through a

1895
01:47:10,719 --> 01:47:16,629
series of techniques such as the ciliary

1896
01:47:13,209 --> 01:47:18,489
coefficient using genus core and it says

1897
01:47:16,629 --> 01:47:21,399
that the cluster is pure enough it stops

1898
01:47:18,489 --> 01:47:23,169
and stops producing anymore branches are

1899
01:47:21,399 --> 01:47:24,959
normally since they are very much

1900
01:47:23,169 --> 01:47:27,999
different from everything else or

1901
01:47:24,959 --> 01:47:30,608
isolated first because that's a

1902
01:47:27,999 --> 01:47:32,379
difference that's associated when we

1903
01:47:30,609 --> 01:47:35,590
said isolation forests there were two

1904
01:47:32,379 --> 01:47:39,309
premises first the abnormal subset has

1905
01:47:35,590 --> 01:47:41,530
to be minority and then they are very

1906
01:47:39,309 --> 01:47:44,530
different compared to ordinary datum and

1907
01:47:41,530 --> 01:47:47,409
so they're isolated in just a few

1908
01:47:44,530 --> 01:47:49,780
branches all right but graphically those

1909
01:47:47,409 --> 01:47:51,669
are the cuts that we follow right there

1910
01:47:49,780 --> 01:47:55,090
or your card second one is more than

1911
01:47:51,669 --> 01:47:58,959
enough right that's right there's what I

1912
01:47:55,090 --> 01:48:00,459
didn't follow yes here the second one

1913
01:47:58,959 --> 01:48:03,849
would have been enough for isolation

1914
01:48:00,459 --> 01:48:06,059
purposes but usually it would take on an

1915
01:48:03,849 --> 01:48:06,059
arm

1916
01:48:08,860 --> 01:48:15,019
if these were a tree we would keep with

1917
01:48:13,640 --> 01:48:16,250
cats all the way to the end to see

1918
01:48:15,020 --> 01:48:25,180
what's significant because maybe out

1919
01:48:16,250 --> 01:48:25,180
here it would work any more questions

1920
01:48:30,500 --> 01:48:37,550
guess what I said must defeat it what's

1921
01:48:34,200 --> 01:48:40,170
more difficult for machine learning

1922
01:48:37,550 --> 01:48:44,930
information classification are choosing

1923
01:48:40,170 --> 01:48:44,930
an algorithm to tackle a problem whoa

1924
01:48:46,670 --> 01:48:51,390
the most complicated part of it would be

1925
01:48:49,230 --> 01:48:54,120
to get a good dataset good quality

1926
01:48:51,390 --> 01:48:57,080
getting a good dataset it is so very

1927
01:48:54,120 --> 01:49:00,000
complicated for example you want him

1928
01:48:57,080 --> 01:49:02,430
here's isolation forest it could it

1929
01:49:00,000 --> 01:49:05,460
could be done for example to detect

1930
01:49:02,430 --> 01:49:09,480
anomalies on a corporate network so what

1931
01:49:05,460 --> 01:49:11,700
about your home how do you get data

1932
01:49:09,480 --> 01:49:14,759
because you don't have access to your

1933
01:49:11,700 --> 01:49:16,320
business network and at home with a

1934
01:49:14,760 --> 01:49:19,950
virtual environment you cannot have

1935
01:49:16,320 --> 01:49:21,809
something graphic as on a corporation

1936
01:49:19,950 --> 01:49:25,139
because they have priorities and they

1937
01:49:21,810 --> 01:49:27,000
have more nodes and Habs and have

1938
01:49:25,140 --> 01:49:29,850
different structures within the network

1939
01:49:27,000 --> 01:49:32,100
so in the end problem is that you don't

1940
01:49:29,850 --> 01:49:34,140
know whether you've got a good model

1941
01:49:32,100 --> 01:49:35,370
well there won't be implemented because

1942
01:49:34,140 --> 01:49:38,550
you don't have access to information

1943
01:49:35,370 --> 01:49:40,890
data to test your algorithm so if I were

1944
01:49:38,550 --> 01:49:42,630
to say something I would say the most

1945
01:49:40,890 --> 01:49:47,690
critical thing would be to get good data

1946
01:49:42,630 --> 01:49:49,770
and there are many researchers in

1947
01:49:47,690 --> 01:49:52,980
artificial intelligence think that

1948
01:49:49,770 --> 01:49:55,320
artificial intelligence has been a hit

1949
01:49:52,980 --> 01:49:57,240
for the last ten years ago - before that

1950
01:49:55,320 --> 01:49:59,490
we could not get a good data there were

1951
01:49:57,240 --> 01:50:01,800
no mechanisms for a good data collection

1952
01:49:59,490 --> 01:50:04,410
and it really hadn't had enough time to

1953
01:50:01,800 --> 01:50:09,650
have big enough data or good enough data

1954
01:50:04,410 --> 01:50:12,660
now everything is faster and it's all

1955
01:50:09,650 --> 01:50:15,210
expedited and it's easier to get larger

1956
01:50:12,660 --> 01:50:19,730
volumes than in the past that's why I

1957
01:50:15,210 --> 01:50:19,730
say that data are critical now

1958
01:50:28,849 --> 01:50:32,679
what's your name my friend us if there

1959
01:50:31,129 --> 01:50:34,700
are no more questions

1960
01:50:32,679 --> 01:50:40,360
that's me I don't thank you very much

1961
01:50:34,700 --> 01:50:42,280
[Applause]

1962
01:50:40,360 --> 01:50:42,599
[Music]

1963
01:50:42,280 --> 01:50:48,519
you

1964
01:50:42,600 --> 01:50:48,520
[Music]

