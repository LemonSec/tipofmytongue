1
00:00:02,060 --> 00:00:06,629
first of all thank you very much for

2
00:00:04,140 --> 00:00:09,540
being here this is my second time at

3
00:00:06,629 --> 00:00:11,160
cyber camp I really like it because you

4
00:00:09,540 --> 00:00:13,739
get to meet so many people who are

5
00:00:11,160 --> 00:00:15,420
curious about everything and I willing

6
00:00:13,740 --> 00:00:18,150
to learn that's the best thing you can

7
00:00:15,420 --> 00:00:23,939
take with you you share ideas and that's

8
00:00:18,150 --> 00:00:27,019
interesting as it's been said I my IT

9
00:00:23,939 --> 00:00:31,109
engineer I've working on pen tests in

10
00:00:27,019 --> 00:00:33,540
all my professional career this is my

11
00:00:31,109 --> 00:00:36,750
presentation maybe you know me because

12
00:00:33,540 --> 00:00:41,670
I've been pod to moon Dahaka this is a

13
00:00:36,750 --> 00:00:44,160
TV series and my professional life has

14
00:00:41,670 --> 00:00:46,110
revolved around pen testing and hacking

15
00:00:44,160 --> 00:00:49,019
offensive hacking mainly but ever since

16
00:00:46,110 --> 00:00:51,780
of my school years I've been interested

17
00:00:49,020 --> 00:00:55,440
in artificial intelligence what happened

18
00:00:51,780 --> 00:00:57,360
that was 2003 2004 there were no many

19
00:00:55,440 --> 00:01:01,530
options to work on that and that but

20
00:00:57,360 --> 00:01:05,989
then 2010 2011 that's when I started

21
00:01:01,530 --> 00:01:05,989
hearing this word that was big data

22
00:01:07,369 --> 00:01:12,150
could someone define big data does it

23
00:01:10,470 --> 00:01:16,679
ring a bell have you heard it

24
00:01:12,150 --> 00:01:19,140
hands up yes and of you this one you the

25
00:01:16,680 --> 00:01:30,180
one closest to me can you define big

26
00:01:19,140 --> 00:01:33,329
data that's right I will repeat it

27
00:01:30,180 --> 00:01:36,600
big data would be algorithms and

28
00:01:33,329 --> 00:01:39,298
architectures that will let me process

29
00:01:36,600 --> 00:01:43,429
large amount of data in an efficient and

30
00:01:39,299 --> 00:01:47,340
fast way I will see you in that name

31
00:01:43,430 --> 00:01:49,320
more often and my interest grew and I

32
00:01:47,340 --> 00:01:53,399
realized that big data was closely

33
00:01:49,320 --> 00:01:55,258
related to I a that I've so very much

34
00:01:53,399 --> 00:02:00,119
enjoyed in my school years but I could

35
00:01:55,259 --> 00:02:02,310
not define I would look into it

36
00:02:00,119 --> 00:02:06,869
I got all the papers I could find I got

37
00:02:02,310 --> 00:02:10,199
my certificate and so it started growing

38
00:02:06,869 --> 00:02:13,760
started growing on me and

39
00:02:10,199 --> 00:02:18,329
machine learning algorithms AI I a

40
00:02:13,760 --> 00:02:21,630
algorithms we're being used in the

41
00:02:18,330 --> 00:02:29,610
defensive security sector not so much an

42
00:02:21,630 --> 00:02:34,890
offensive side and that's what I wanted

43
00:02:29,610 --> 00:02:37,470
to bring here for my agenda scientists

44
00:02:34,890 --> 00:02:39,059
say that Dib learn and the basis of my

45
00:02:37,470 --> 00:02:41,670
presentation will be the future for the

46
00:02:39,060 --> 00:02:44,989
Signum Valley technology so what's the

47
00:02:41,670 --> 00:02:48,208
basis here what will I explain here

48
00:02:44,989 --> 00:02:50,250
first let me tell you why you should be

49
00:02:48,209 --> 00:02:53,400
interested in deep learning you might

50
00:02:50,250 --> 00:02:55,500
ask well I'm not interesting this first

51
00:02:53,400 --> 00:02:58,799
time I hear about it I'm not interested

52
00:02:55,500 --> 00:03:01,739
at all but we will see why we should all

53
00:02:58,799 --> 00:03:07,380
be interested finding out what this

54
00:03:01,739 --> 00:03:10,590
technology is about then we will move on

55
00:03:07,380 --> 00:03:12,750
and see what a professional working in

56
00:03:10,590 --> 00:03:15,390
this technology does so what's the life

57
00:03:12,750 --> 00:03:17,670
cycle for machine learning because I

58
00:03:15,390 --> 00:03:21,208
will be referring to different concepts

59
00:03:17,670 --> 00:03:24,000
here but as I go in deeper I will define

60
00:03:21,209 --> 00:03:25,590
them and I will show you how to work

61
00:03:24,000 --> 00:03:30,660
with that kind of technology

62
00:03:25,590 --> 00:03:33,390
next brief introduction to deep learning

63
00:03:30,660 --> 00:03:35,780
that's neural networks when we say deep

64
00:03:33,390 --> 00:03:40,708
learning these are artificial neural

65
00:03:35,780 --> 00:03:45,510
networks or pathways ones I've shared it

66
00:03:40,709 --> 00:03:47,400
all we will have the basis and the

67
00:03:45,510 --> 00:03:52,620
knowledge to understand what is being

68
00:03:47,400 --> 00:03:55,769
done then I'll explain a specific type

69
00:03:52,620 --> 00:03:59,910
of dividend that is super fast and since

70
00:03:55,769 --> 00:04:05,280
we need some movement we will run a demo

71
00:03:59,910 --> 00:04:09,120
using this supervised supervised deep

72
00:04:05,280 --> 00:04:12,680
learning to spot bank fraud when we buy

73
00:04:09,120 --> 00:04:16,108
on the web such as the bitter card

74
00:04:12,680 --> 00:04:17,940
companies such as Visa or MasterCard

75
00:04:16,108 --> 00:04:20,159
that are interested in this kind of

76
00:04:17,940 --> 00:04:21,839
student in technologies you should know

77
00:04:20,159 --> 00:04:24,010
about them as well because it's not the

78
00:04:21,839 --> 00:04:27,280
technology for the future but it is

79
00:04:24,010 --> 00:04:30,510
parent if you are still not too bored

80
00:04:27,280 --> 00:04:33,700
and I have time I will explain

81
00:04:30,510 --> 00:04:38,950
unsupervised deep learning and autumn

82
00:04:33,700 --> 00:04:41,469
colors and again we need to get our

83
00:04:38,950 --> 00:04:43,090
hands dirty that's what's interesting we

84
00:04:41,470 --> 00:04:46,570
need to be practical about it and we

85
00:04:43,090 --> 00:04:49,419
will see a demo on how to use autumn and

86
00:04:46,570 --> 00:04:52,780
cutters this other deep learning neural

87
00:04:49,420 --> 00:04:56,800
networks for cancer prognosis our

88
00:04:52,780 --> 00:05:00,070
prediction we see it as defensive that

89
00:04:56,800 --> 00:05:06,040
is front detection and then something

90
00:05:00,070 --> 00:05:07,930
else and that is a these other side

91
00:05:06,040 --> 00:05:12,910
would be that these technologies are

92
00:05:07,930 --> 00:05:15,880
helpful in the medical field why are we

93
00:05:12,910 --> 00:05:19,930
interested in deep learning here is a

94
00:05:15,880 --> 00:05:24,850
bender handling notes but it helps us

95
00:05:19,930 --> 00:05:27,670
take better decisions if we are not CEO

96
00:05:24,850 --> 00:05:33,160
at a large company we might not care

97
00:05:27,670 --> 00:05:36,870
that let me spend some use cases where

98
00:05:33,160 --> 00:05:43,000
the technology is used for this neural

99
00:05:36,870 --> 00:05:46,690
networks or AI hence app have you ever

100
00:05:43,000 --> 00:05:48,460
googled today thumbs up if you've

101
00:05:46,690 --> 00:05:52,330
googled anything today if you've gone

102
00:05:48,460 --> 00:05:54,370
into Facebook or Twitter no one no one's

103
00:05:52,330 --> 00:05:55,859
been to Twitter today well great I love

104
00:05:54,370 --> 00:06:00,250
it

105
00:05:55,860 --> 00:06:02,620
basically we are interested so I have

106
00:06:00,250 --> 00:06:07,680
some some candy to bribe you by the way

107
00:06:02,620 --> 00:06:10,450
I will distribute it later I thought of

108
00:06:07,680 --> 00:06:11,980
getting something better but I missed my

109
00:06:10,450 --> 00:06:16,750
train I didn't have time to buy anything

110
00:06:11,980 --> 00:06:19,270
else we are interested in this

111
00:06:16,750 --> 00:06:25,720
technology because Google Amazon

112
00:06:19,270 --> 00:06:29,020
Microsoft Facebook or similar large

113
00:06:25,720 --> 00:06:31,900
technology companies are investing most

114
00:06:29,020 --> 00:06:34,810
of their capital into further developing

115
00:06:31,900 --> 00:06:37,760
this kind of technologies and this large

116
00:06:34,810 --> 00:06:40,130
technology companies are the one

117
00:06:37,760 --> 00:06:40,940
moving most of the data available on the

118
00:06:40,130 --> 00:06:45,280
Internet

119
00:06:40,940 --> 00:06:48,350
and if they are using those technologies

120
00:06:45,280 --> 00:06:52,309
it means it they have an impact on you

121
00:06:48,350 --> 00:06:54,940
and why is that because you go to a job

122
00:06:52,310 --> 00:06:57,740
interview and HR will be using

123
00:06:54,940 --> 00:07:02,660
information big data that I mentioned

124
00:06:57,740 --> 00:07:07,220
and all the AI algorithms to take a

125
00:07:02,660 --> 00:07:09,470
profile of our behavior another uses use

126
00:07:07,220 --> 00:07:11,600
it Cage insurance company where the

127
00:07:09,470 --> 00:07:14,210
medical or risk insurance companies in

128
00:07:11,600 --> 00:07:17,810
the end they use this kind of match in

129
00:07:14,210 --> 00:07:20,960
machine learning technologies as well to

130
00:07:17,810 --> 00:07:26,470
do what they call sentiment analysis

131
00:07:20,960 --> 00:07:30,080
they profile our behavior the same as

132
00:07:26,470 --> 00:07:34,180
profilers FBI profilers for behavior

133
00:07:30,080 --> 00:07:34,180
this algorithms their work even better

134
00:07:36,310 --> 00:07:42,130
last year I showed how this kind of

135
00:07:39,890 --> 00:07:46,310
technology could be used for crime

136
00:07:42,130 --> 00:07:48,080
prediction in San Francisco Chicago and

137
00:07:46,310 --> 00:07:51,440
other US cities they are using deep

138
00:07:48,080 --> 00:07:54,580
learning which I've not defined so far

139
00:07:51,440 --> 00:07:56,630
which is just an abstract is a buzzword

140
00:07:54,580 --> 00:07:59,719
but this has been used by law

141
00:07:56,630 --> 00:08:03,590
enforcement bodies for terrorism and

142
00:07:59,720 --> 00:08:06,590
crime detection there's been officers

143
00:08:03,590 --> 00:08:09,739
going to different homes saying you are

144
00:08:06,590 --> 00:08:12,229
subject of an investigation because you

145
00:08:09,740 --> 00:08:14,660
are suspicious because of these kind of

146
00:08:12,230 --> 00:08:16,970
movements and movements have been

147
00:08:14,660 --> 00:08:20,360
detected by this algorithms getting data

148
00:08:16,970 --> 00:08:23,960
from different sources IOT and and

149
00:08:20,360 --> 00:08:31,720
street cameras Commons searches cell

150
00:08:23,960 --> 00:08:35,780
phones Wi-Fi and Bluetooth when it's on

151
00:08:31,720 --> 00:08:38,660
or telephone antennas antennas that

152
00:08:35,780 --> 00:08:41,120
record where we move around that

153
00:08:38,659 --> 00:08:43,669
information has been stored I actually

154
00:08:41,120 --> 00:08:47,240
started working on a project that was a

155
00:08:43,669 --> 00:08:50,840
mad malls shopping centers where there

156
00:08:47,240 --> 00:08:51,529
would be beacons all over the center the

157
00:08:50,840 --> 00:08:54,290
shopping center

158
00:08:51,529 --> 00:08:58,040
this would be Bluetooth and also Wi-Fi

159
00:08:54,290 --> 00:09:00,560
beacons and with those probes and the

160
00:08:58,040 --> 00:09:03,949
Bluetooth cars they would start

161
00:09:00,560 --> 00:09:07,579
identifying us so in the end with that

162
00:09:03,949 --> 00:09:10,309
information we would see were the users

163
00:09:07,579 --> 00:09:13,670
were what how much time they spend in

164
00:09:10,309 --> 00:09:16,100
every area of the shopping center if

165
00:09:13,670 --> 00:09:19,309
they ever came back what time of the day

166
00:09:16,100 --> 00:09:22,149
how often they would come back to the

167
00:09:19,309 --> 00:09:25,100
shopping center and those would be

168
00:09:22,149 --> 00:09:27,529
behavioral patterns these are used in

169
00:09:25,100 --> 00:09:29,809
every single sector as I was saying for

170
00:09:27,529 --> 00:09:32,809
one of the demos will be used for

171
00:09:29,809 --> 00:09:35,689
medicine medicine is also highly

172
00:09:32,809 --> 00:09:37,100
benefiting from this kind of

173
00:09:35,689 --> 00:09:40,699
technologies and why is that well

174
00:09:37,100 --> 00:09:45,170
there's some pictures there first one is

175
00:09:40,699 --> 00:09:47,059
the way a deep learning algorithm would

176
00:09:45,170 --> 00:09:49,969
behave even if you don't see it there

177
00:09:47,059 --> 00:09:55,059
are some red black spots red spots could

178
00:09:49,970 --> 00:09:59,930
be fraud cases black could be regular

179
00:09:55,059 --> 00:10:03,079
proper transactions and so the deep

180
00:09:59,930 --> 00:10:06,138
learning algorithm can tell apart

181
00:10:03,079 --> 00:10:09,920
one spot from the next one duck from the

182
00:10:06,139 --> 00:10:12,459
net so it really add justice or

183
00:10:09,920 --> 00:10:15,019
accommodates gee understand what

184
00:10:12,459 --> 00:10:19,040
fraudulent and what's not so it can be

185
00:10:15,019 --> 00:10:20,480
curves and it extracts all the features

186
00:10:19,040 --> 00:10:23,240
so that we understand about the

187
00:10:20,480 --> 00:10:26,569
transaction is good or bad this is a

188
00:10:23,240 --> 00:10:29,209
traditional AI algorithm this is awesome

189
00:10:26,569 --> 00:10:30,860
boom machine learning I will not dwell

190
00:10:29,209 --> 00:10:34,699
on that but this is a bit more

191
00:10:30,860 --> 00:10:37,759
traditional one and it is outperformed

192
00:10:34,699 --> 00:10:43,040
by the first one fraudulent transactions

193
00:10:37,759 --> 00:10:46,040
cannot be told apart so easily here

194
00:10:43,040 --> 00:10:48,939
random forests decision-making trees and

195
00:10:46,040 --> 00:10:51,800
is another kind of AI then it's

196
00:10:48,939 --> 00:10:55,579
outperformed by all others you see that

197
00:10:51,800 --> 00:10:58,429
there is a mess here this is a linear

198
00:10:55,579 --> 00:11:01,489
progression using lines to tell apart

199
00:10:58,429 --> 00:11:03,470
one examples one kind of examples from

200
00:11:01,490 --> 00:11:04,500
the other so the performance is even

201
00:11:03,470 --> 00:11:07,080
worse like

202
00:11:04,500 --> 00:11:09,390
of a I these presentation will be the

203
00:11:07,080 --> 00:11:15,000
best performing one because of the one

204
00:11:09,390 --> 00:11:19,580
that accommodates the best there's a

205
00:11:15,000 --> 00:11:22,980
case of a traditional algorithm which is

206
00:11:19,580 --> 00:11:25,110
the one where you have different

207
00:11:22,980 --> 00:11:30,090
features to define a problem if it's

208
00:11:25,110 --> 00:11:33,930
fraud that would be some features those

209
00:11:30,090 --> 00:11:36,120
are kind of predicted factors for a

210
00:11:33,930 --> 00:11:40,109
fraudulent transaction when you get many

211
00:11:36,120 --> 00:11:43,890
of those predictors such as name of the

212
00:11:40,110 --> 00:11:45,080
car holder place of the transaction and

213
00:11:43,890 --> 00:11:49,310
Mound

214
00:11:45,080 --> 00:11:51,900
when you've got many of those you use

215
00:11:49,310 --> 00:11:55,319
principal component analysis technique

216
00:11:51,900 --> 00:11:57,780
to narrow them down and keep only those

217
00:11:55,320 --> 00:11:59,700
predictors that actually have an

218
00:11:57,780 --> 00:12:01,740
influence on the nature of the

219
00:11:59,700 --> 00:12:04,110
transaction it might look like a math

220
00:12:01,740 --> 00:12:07,860
snap it seems that it cannot be poorly

221
00:12:04,110 --> 00:12:10,740
narrowed down you see all the dots blend

222
00:12:07,860 --> 00:12:12,990
together these are autumn colors and

223
00:12:10,740 --> 00:12:14,370
they're really really good at telling

224
00:12:12,990 --> 00:12:17,550
them apart each color is a different

225
00:12:14,370 --> 00:12:21,170
kind and it really helps make better

226
00:12:17,550 --> 00:12:24,240
decisions and that's why large

227
00:12:21,170 --> 00:12:26,849
technology companies are investing so

228
00:12:24,240 --> 00:12:29,310
much such high capital into developing

229
00:12:26,850 --> 00:12:32,010
this kind of algorithms Google has this

230
00:12:29,310 --> 00:12:34,260
two technique I find it or a study I

231
00:12:32,010 --> 00:12:36,660
find interesting they publish all

232
00:12:34,260 --> 00:12:38,069
research papers they they have all of

233
00:12:36,660 --> 00:12:43,260
them free of charge

234
00:12:38,070 --> 00:12:46,920
they've got one on nature on a powerful

235
00:12:43,260 --> 00:12:48,360
and neural networks evolution and it's

236
00:12:46,920 --> 00:12:52,800
all made published it's all about

237
00:12:48,360 --> 00:12:54,960
monetizing and it would be about leasing

238
00:12:52,800 --> 00:12:57,510
space on the internet to train

239
00:12:54,960 --> 00:13:01,290
algorithms I'll explain what algorithm

240
00:12:57,510 --> 00:13:04,170
training is later on what I wanted is to

241
00:13:01,290 --> 00:13:08,430
show how important this is and something

242
00:13:04,170 --> 00:13:11,520
else I wanted to share and that the last

243
00:13:08,430 --> 00:13:15,750
summer upper had the cyber challenge

244
00:13:11,520 --> 00:13:16,770
does it ring a bell and the idea was to

245
00:13:15,750 --> 00:13:19,620
him

246
00:13:16,770 --> 00:13:23,399
have a contest and kind of capture the

247
00:13:19,620 --> 00:13:27,570
flag but it wouldn't be auditors or pen

248
00:13:23,399 --> 00:13:31,560
testers hacking the challenges the idea

249
00:13:27,570 --> 00:13:37,290
is to have data scientists developing AI

250
00:13:31,560 --> 00:13:39,989
algorithms that could hock so it's not

251
00:13:37,290 --> 00:13:44,040
people hacking anymore but the hackers

252
00:13:39,990 --> 00:13:47,160
are the AI algorithms themselves so

253
00:13:44,040 --> 00:13:49,260
within three four years the one doing

254
00:13:47,160 --> 00:13:54,839
the ones doing reversing it won't be a

255
00:13:49,260 --> 00:13:57,720
person there will be AI programs

256
00:13:54,839 --> 00:13:59,220
predictive programs that can work on

257
00:13:57,720 --> 00:14:01,320
reversion and find and exploit

258
00:13:59,220 --> 00:14:03,660
vulnerabilities as we saw from previous

259
00:14:01,320 --> 00:14:08,220
presentations so it's a change in

260
00:14:03,660 --> 00:14:10,230
paradigm that will mean that we will for

261
00:14:08,220 --> 00:14:13,589
example need a basic salary Universal

262
00:14:10,230 --> 00:14:16,700
basic salary because we would have as

263
00:14:13,589 --> 00:14:20,730
the self-driven cars

264
00:14:16,700 --> 00:14:22,500
so either we recycled jobs or we'll have

265
00:14:20,730 --> 00:14:28,260
some problem there is some discussion

266
00:14:22,500 --> 00:14:29,880
there I wanted to highlight it because I

267
00:14:28,260 --> 00:14:34,920
guess these technologies are quite quite

268
00:14:29,880 --> 00:14:37,920
important so I have a video this is a

269
00:14:34,920 --> 00:14:41,790
Google Google algorithm so that you see

270
00:14:37,920 --> 00:14:44,610
its power this is a reinforcement neural

271
00:14:41,790 --> 00:14:47,250
network when you learn how to do

272
00:14:44,610 --> 00:14:50,970
something and you do it right you get a

273
00:14:47,250 --> 00:14:56,970
reward you are reinforced so we use this

274
00:14:50,970 --> 00:15:00,540
for gaming so this is the algorithm

275
00:14:56,970 --> 00:15:03,149
starts from scratch the only thing that

276
00:15:00,540 --> 00:15:06,360
it knows is what they find its finds on

277
00:15:03,149 --> 00:15:08,520
the environment you see it not very good

278
00:15:06,360 --> 00:15:14,040
at it and doesn't a score match this so

279
00:15:08,520 --> 00:15:17,870
doesn't know what to do but as there

280
00:15:14,040 --> 00:15:20,939
they have it has more scores it gets

281
00:15:17,870 --> 00:15:22,770
points it gets a reward and it learns

282
00:15:20,940 --> 00:15:28,110
how to do it better and better faster

283
00:15:22,770 --> 00:15:29,220
and faster in d the ball is cut more

284
00:15:28,110 --> 00:15:33,139
often and

285
00:15:29,220 --> 00:15:36,120
and they just did it just do it just by

286
00:15:33,139 --> 00:15:37,949
what it finds on the screen no standards

287
00:15:36,120 --> 00:15:40,350
have been defined just tries and see

288
00:15:37,949 --> 00:15:44,008
where the regards rewards come from and

289
00:15:40,350 --> 00:15:45,439
after over 100 minutes training magic

290
00:15:44,009 --> 00:15:49,050
happens

291
00:15:45,439 --> 00:15:53,389
so strategies are developed so there

292
00:15:49,050 --> 00:15:55,829
would be a tunnel here as you can see to

293
00:15:53,389 --> 00:15:58,379
place it in here

294
00:15:55,829 --> 00:16:00,719
it starts with bouncing and it is scores

295
00:15:58,379 --> 00:16:04,620
it is learning strategies as a a regular

296
00:16:00,720 --> 00:16:09,379
person for gym so these kind of

297
00:16:04,620 --> 00:16:13,980
algorithm try to simulate our learning

298
00:16:09,379 --> 00:16:16,620
method for people's learning method but

299
00:16:13,980 --> 00:16:19,649
and translate it into him I see

300
00:16:16,620 --> 00:16:22,560
algorithms computer algorithms now that

301
00:16:19,649 --> 00:16:25,589
you've found the context for it all let

302
00:16:22,560 --> 00:16:30,060
me show you how it works when uses this

303
00:16:25,589 --> 00:16:35,699
kind of algorithm usually what we do is

304
00:16:30,060 --> 00:16:37,800
we get a history of data if it's fraud

305
00:16:35,699 --> 00:16:42,359
you see all the bank transactions and

306
00:16:37,800 --> 00:16:46,199
every transaction is a data set that we

307
00:16:42,360 --> 00:16:50,779
visualize a stable with rows and columns

308
00:16:46,199 --> 00:16:54,389
columns a power holder transaction date

309
00:16:50,779 --> 00:16:59,180
amount where it happened

310
00:16:54,389 --> 00:17:02,160
so regular data and then rows will be

311
00:16:59,180 --> 00:17:04,589
various examples these are the kind of

312
00:17:02,160 --> 00:17:09,539
data we work with these are the data

313
00:17:04,589 --> 00:17:12,708
that will help the algorithm learn with

314
00:17:09,539 --> 00:17:16,289
this data we have feature engineering

315
00:17:12,709 --> 00:17:19,970
else so we clean them because they are

316
00:17:16,289 --> 00:17:23,730
full of a shade when we get data from

317
00:17:19,970 --> 00:17:26,730
websites or sensors usually we get lost

318
00:17:23,730 --> 00:17:30,419
values we get doubled values or

319
00:17:26,730 --> 00:17:34,559
duplicated values there even mistakes in

320
00:17:30,419 --> 00:17:37,380
columns that can be excessive or give us

321
00:17:34,559 --> 00:17:41,070
a similar information and just one

322
00:17:37,380 --> 00:17:43,700
column between of so we clean it

323
00:17:41,070 --> 00:17:48,689
and this part of feature engineering

324
00:17:43,700 --> 00:17:55,710
takes us 70% of all the time that's

325
00:17:48,690 --> 00:18:00,290
proper clean enough data then we divide

326
00:17:55,710 --> 00:18:06,630
it into three sets right training

327
00:18:00,290 --> 00:18:08,730
validation and test this one gets 16 of

328
00:18:06,630 --> 00:18:09,900
the present of data twenty percent and

329
00:18:08,730 --> 00:18:11,580
then twenty percent each

330
00:18:09,900 --> 00:18:16,200
it depending on how you want to use it

331
00:18:11,580 --> 00:18:19,949
and let's focus on the training set why

332
00:18:16,200 --> 00:18:22,740
is this useful this will help us get the

333
00:18:19,950 --> 00:18:27,290
examples so here we see bank

334
00:18:22,740 --> 00:18:30,320
transactions and for example per row we

335
00:18:27,290 --> 00:18:33,659
targeted to see if it's a fraudulent or

336
00:18:30,320 --> 00:18:36,149
regular transaction same as learning

337
00:18:33,660 --> 00:18:41,210
with examples this is what's known as

338
00:18:36,150 --> 00:18:45,390
supervised surveilled machine learning

339
00:18:41,210 --> 00:18:50,690
so we define a value set for predictors

340
00:18:45,390 --> 00:18:54,240
and we say that for that kind of values

341
00:18:50,690 --> 00:18:56,960
whether there is fraud or not fraud

342
00:18:54,240 --> 00:19:00,150
would be the target or the pretty

343
00:18:56,960 --> 00:19:02,610
variable to predict so we set an example

344
00:19:00,150 --> 00:19:05,160
and explain how to behave for those

345
00:19:02,610 --> 00:19:08,580
examples it is example based learning

346
00:19:05,160 --> 00:19:11,940
same as if we were back in college and

347
00:19:08,580 --> 00:19:14,159
we were getting ready for an exam and we

348
00:19:11,940 --> 00:19:16,470
would check out exams from previous

349
00:19:14,160 --> 00:19:20,520
years and we would see the typical

350
00:19:16,470 --> 00:19:22,500
problems or exercises and we would say

351
00:19:20,520 --> 00:19:24,750
well this is a typical problem and it

352
00:19:22,500 --> 00:19:30,050
solved this way this is what what

353
00:19:24,750 --> 00:19:32,970
happens here this training set will be

354
00:19:30,050 --> 00:19:38,399
transformed to the machine learning

355
00:19:32,970 --> 00:19:42,960
algorithm here we set hyper parameters

356
00:19:38,400 --> 00:19:45,050
which means that the training the

357
00:19:42,960 --> 00:19:47,880
learning process will happen faster

358
00:19:45,050 --> 00:19:51,600
we'll see what they are all about but

359
00:19:47,880 --> 00:19:54,770
this is kind of magic there are some

360
00:19:51,600 --> 00:19:59,300
bottoms we can push so that it learns

361
00:19:54,770 --> 00:20:01,030
strong once we've passed this training

362
00:19:59,300 --> 00:20:05,540
data there will be a predictive model

363
00:20:01,030 --> 00:20:09,290
produced there will be a model that can

364
00:20:05,540 --> 00:20:13,430
get new UNTAC data without saying

365
00:20:09,290 --> 00:20:16,520
whether it's right or not and it will

366
00:20:13,430 --> 00:20:18,980
reply whether it is a fraud or not which

367
00:20:16,520 --> 00:20:21,170
is in the end our targets same with the

368
00:20:18,980 --> 00:20:28,520
same demo here there will be a series of

369
00:20:21,170 --> 00:20:32,360
markers that will explain whether there

370
00:20:28,520 --> 00:20:34,970
is cancer or not it generates a

371
00:20:32,360 --> 00:20:38,719
predictive model but we still don't have

372
00:20:34,970 --> 00:20:41,750
a clear way to see if that predictive

373
00:20:38,720 --> 00:20:46,430
model works right or not or if it's a

374
00:20:41,750 --> 00:20:49,010
poor predicting method so to understand

375
00:20:46,430 --> 00:20:52,550
whether we are good at it or not we will

376
00:20:49,010 --> 00:20:54,710
try it out with the validation set which

377
00:20:52,550 --> 00:20:58,700
has been tacked as well so the

378
00:20:54,710 --> 00:21:02,210
predictive model has the rows for the

379
00:20:58,700 --> 00:21:04,640
validation set and the predictive model

380
00:21:02,210 --> 00:21:07,190
would save it whether for this said it

381
00:21:04,640 --> 00:21:12,920
is a fraud or not and since this been

382
00:21:07,190 --> 00:21:15,350
talked for every row we know whether if

383
00:21:12,920 --> 00:21:19,490
it's fraud or not and we compare it to

384
00:21:15,350 --> 00:21:22,490
what we need to get is it clear is it

385
00:21:19,490 --> 00:21:24,290
too messy we need to compare whether

386
00:21:22,490 --> 00:21:27,760
there is proper prediction or not and

387
00:21:24,290 --> 00:21:33,560
this will help us change it correct

388
00:21:27,760 --> 00:21:38,450
behavior and to find in training once we

389
00:21:33,560 --> 00:21:41,720
have properly trained the AI algorithm

390
00:21:38,450 --> 00:21:43,670
once it's fully perfected and we have a

391
00:21:41,720 --> 00:21:45,620
predictive model that it's good enough

392
00:21:43,670 --> 00:21:48,500
we're trying to see whether it can be

393
00:21:45,620 --> 00:21:52,000
widespread and there will be another run

394
00:21:48,500 --> 00:21:55,910
to see if we can use it to production

395
00:21:52,000 --> 00:21:58,670
for which we get this data set that it

396
00:21:55,910 --> 00:22:01,310
start when I say tag I mean that for

397
00:21:58,670 --> 00:22:05,840
every data set I know whether it is

398
00:22:01,310 --> 00:22:08,269
fraud or not and we give it to the

399
00:22:05,840 --> 00:22:12,320
system and see whether it can be

400
00:22:08,269 --> 00:22:15,200
spread we'll see whether the final model

401
00:22:12,320 --> 00:22:17,479
that I produce gives me back a good

402
00:22:15,200 --> 00:22:19,849
result and compare it to the real result

403
00:22:17,479 --> 00:22:24,200
and with different measurements I will

404
00:22:19,849 --> 00:22:27,489
see whether it is a easily scaled up or

405
00:22:24,200 --> 00:22:32,719
replicated that whether it can be used

406
00:22:27,489 --> 00:22:34,609
efficiently and then you use this model

407
00:22:32,719 --> 00:22:37,849
and that's the way to work with this

408
00:22:34,609 --> 00:22:41,718
kind of algorithms you train algorithms

409
00:22:37,849 --> 00:22:46,339
to generate predictive models that get

410
00:22:41,719 --> 00:22:48,519
untack data and with those and models

411
00:22:46,339 --> 00:22:53,239
and data they can give us forecast

412
00:22:48,519 --> 00:22:56,419
prediction for automated driving where

413
00:22:53,239 --> 00:23:01,359
you use neural networks that highly

414
00:22:56,419 --> 00:23:08,950
simulate the visual cortex same as with

415
00:23:01,359 --> 00:23:08,949
any other filter classifiers or

416
00:23:09,669 --> 00:23:14,989
automated players that learn how to

417
00:23:12,379 --> 00:23:17,629
solve problems this has been used

418
00:23:14,989 --> 00:23:21,249
specially for traffic jam problem

419
00:23:17,629 --> 00:23:24,079
solution to explain how signs and and

420
00:23:21,249 --> 00:23:26,779
lights need to move to make sure there

421
00:23:24,079 --> 00:23:32,899
is no Jam for any kind of problem you

422
00:23:26,779 --> 00:23:36,049
can think of I've explained a few use

423
00:23:32,899 --> 00:23:39,619
cases so far let me explain what's the

424
00:23:36,049 --> 00:23:41,179
basis for our AI and that's deep

425
00:23:39,619 --> 00:23:44,779
learning which is based on neural

426
00:23:41,179 --> 00:23:48,019
networks in order to understand neural

427
00:23:44,779 --> 00:23:50,959
networks the basics would be it let me

428
00:23:48,019 --> 00:23:54,529
plane your dab regular neuron looks like

429
00:23:50,959 --> 00:23:57,799
this is a picture there is some dendrite

430
00:23:54,529 --> 00:24:01,879
that is the in fit and these stimulus

431
00:23:57,799 --> 00:24:05,389
goes in this way said stimulus going

432
00:24:01,879 --> 00:24:07,609
into the dendrites goes to the soma to

433
00:24:05,389 --> 00:24:10,248
the cell body what the information is

434
00:24:07,609 --> 00:24:13,699
processed and once processed the

435
00:24:10,249 --> 00:24:16,159
information goes out this axial exit

436
00:24:13,700 --> 00:24:18,019
channel that it is the axon and from

437
00:24:16,159 --> 00:24:21,389
there it goes to the dendrites of

438
00:24:18,019 --> 00:24:24,209
another neuron this way

439
00:24:21,389 --> 00:24:27,829
we have a neural network and the

440
00:24:24,209 --> 00:24:31,019
connection of an axon and the

441
00:24:27,829 --> 00:24:33,178
consecutive a dendrite is this an optic

442
00:24:31,019 --> 00:24:37,049
connection and the strength of the

443
00:24:33,179 --> 00:24:39,989
connection is what helps people makes

444
00:24:37,049 --> 00:24:44,450
people learn one way or another so our

445
00:24:39,989 --> 00:24:47,219
neurons have given strength and

446
00:24:44,450 --> 00:24:52,409
depending on the strength we remember

447
00:24:47,219 --> 00:24:55,529
something or not and by joining together

448
00:24:52,409 --> 00:25:00,479
lots of neurons we get a biological

449
00:24:55,529 --> 00:25:03,440
neural network once we have it what we

450
00:25:00,479 --> 00:25:11,809
wanted to export it into an IT system

451
00:25:03,440 --> 00:25:14,690
let's try it this was done with this

452
00:25:11,809 --> 00:25:18,178
artificial neuron but it's not as simple

453
00:25:14,690 --> 00:25:21,709
perceptron actually they studied out in

454
00:25:18,179 --> 00:25:24,539
the 40s moved on in the 56 days it was

455
00:25:21,709 --> 00:25:27,719
promoted in the eighties but the problem

456
00:25:24,539 --> 00:25:30,658
is well if it started out in the foetus

457
00:25:27,719 --> 00:25:32,279
why are we hearing about it now how come

458
00:25:30,659 --> 00:25:34,529
of that it's technology companies that

459
00:25:32,279 --> 00:25:37,940
are investing into it now because you

460
00:25:34,529 --> 00:25:37,940
know it's older than all of us together

461
00:25:38,389 --> 00:25:43,829
for neural network training you need

462
00:25:41,099 --> 00:25:48,658
lots of data which we have now because

463
00:25:43,829 --> 00:25:50,789
the big data and you need new

464
00:25:48,659 --> 00:25:54,899
architectures to process information in

465
00:25:50,789 --> 00:25:57,149
a fast efficient way how to spark does

466
00:25:54,899 --> 00:25:59,728
it ring a bell but those are big data

467
00:25:57,149 --> 00:26:04,439
architectures that help us process all

468
00:25:59,729 --> 00:26:06,899
those data running parallely environment

469
00:26:04,440 --> 00:26:09,329
and efficiently so all those that have

470
00:26:06,899 --> 00:26:12,869
we produced with social media cellphone

471
00:26:09,329 --> 00:26:14,940
use and the IOT with this kind of New

472
00:26:12,869 --> 00:26:17,339
York ax textures it's making it possible

473
00:26:14,940 --> 00:26:20,070
for this kind of algorithms to be used

474
00:26:17,339 --> 00:26:22,200
and implemented you need to realize that

475
00:26:20,070 --> 00:26:25,168
up in our brains we've got millions of

476
00:26:22,200 --> 00:26:28,049
neurons neurons it is amazing and we're

477
00:26:25,169 --> 00:26:31,519
trying to get somewhere near eight but

478
00:26:28,049 --> 00:26:35,930
we're so far away still it still just

479
00:26:31,519 --> 00:26:38,790
starting research that we are far from

480
00:26:35,930 --> 00:26:41,580
imitating a human brain it is trying

481
00:26:38,790 --> 00:26:45,620
down like this as I said the entry

482
00:26:41,580 --> 00:26:50,970
channels are dendrites the drains here

483
00:26:45,620 --> 00:26:54,510
this is X sub one sub T sub L that the

484
00:26:50,970 --> 00:26:59,130
nervous impulse the pulse all the fraud

485
00:26:54,510 --> 00:27:03,480
features it would be the car holder the

486
00:26:59,130 --> 00:27:05,580
amount that's the entry into our what's

487
00:27:03,480 --> 00:27:07,290
entered into the new room that makes it

488
00:27:05,580 --> 00:27:10,980
all the way to the dendrites as I was

489
00:27:07,290 --> 00:27:13,610
saying and when that neural pulse goes

490
00:27:10,980 --> 00:27:19,340
to the vent right there is a synaptic

491
00:27:13,610 --> 00:27:22,080
connection or linked that has a wait an

492
00:27:19,340 --> 00:27:25,970
hour artificial neural network will

493
00:27:22,080 --> 00:27:28,949
learn one way or the other based on the

494
00:27:25,970 --> 00:27:31,260
amount of those values so we want to

495
00:27:28,950 --> 00:27:35,130
train our artificial neuron networks so

496
00:27:31,260 --> 00:27:38,100
that it finds the appropriate W values

497
00:27:35,130 --> 00:27:41,190
of the neural network behaves as I as we

498
00:27:38,100 --> 00:27:43,500
wanted so the target you need to remind

499
00:27:41,190 --> 00:27:45,270
is getting the right values for weights

500
00:27:43,500 --> 00:27:53,540
and this is all achieved through

501
00:27:45,270 --> 00:27:58,260
training so every X sub one we multiply

502
00:27:53,540 --> 00:28:02,610
by W sub 1 all the way to sub n and

503
00:27:58,260 --> 00:28:05,910
those weight entries will make it to the

504
00:28:02,610 --> 00:28:07,129
cell body or so ma'am and here we add it

505
00:28:05,910 --> 00:28:11,040
up

506
00:28:07,130 --> 00:28:15,980
so we multiplied it and upload it and

507
00:28:11,040 --> 00:28:25,200
and add it all up this one this one and

508
00:28:15,980 --> 00:28:28,920
then we add B B stands for bias it will

509
00:28:25,200 --> 00:28:32,100
be a threshold we said or something that

510
00:28:28,920 --> 00:28:35,250
like a barrier that we come that we set

511
00:28:32,100 --> 00:28:39,629
up so that the neuron finds it easy or

512
00:28:35,250 --> 00:28:43,890
not so easy to fire so it will fired it

513
00:28:39,630 --> 00:28:47,310
will be aroused and it will return value

514
00:28:43,890 --> 00:28:47,750
one easily if values are very negative

515
00:28:47,310 --> 00:28:50,480
on the

516
00:28:47,750 --> 00:28:53,660
bias the neuron will find it more

517
00:28:50,480 --> 00:28:56,840
difficult to fire to have the light go

518
00:28:53,660 --> 00:28:59,090
on and with these bias and weight values

519
00:28:56,840 --> 00:29:02,740
we will play around so that our neural

520
00:28:59,090 --> 00:29:04,790
network learns that we wanted to learn

521
00:29:02,740 --> 00:29:10,700
so it's all about playing around with

522
00:29:04,790 --> 00:29:12,830
values so with this formula we get it

523
00:29:10,700 --> 00:29:16,600
out through the accent through the exit

524
00:29:12,830 --> 00:29:19,429
channel but before that we use a firing

525
00:29:16,600 --> 00:29:20,780
formula and you might say wait much why

526
00:29:19,430 --> 00:29:24,380
we do we need it

527
00:29:20,780 --> 00:29:27,920
well we need it because a firing formula

528
00:29:24,380 --> 00:29:32,840
narrows down within an interval we find

529
00:29:27,920 --> 00:29:34,940
useful the result the exit value because

530
00:29:32,840 --> 00:29:37,970
if the neural network can work with any

531
00:29:34,940 --> 00:29:40,190
kind of number it'd be much more

532
00:29:37,970 --> 00:29:43,450
difficult in the end with it later to

533
00:29:40,190 --> 00:29:47,330
Train it it will be way easier if we

534
00:29:43,450 --> 00:29:53,200
find a well defined and limited value

535
00:29:47,330 --> 00:29:56,659
set 1 to 0 minus 1 plus 1 minus 5 plus 5

536
00:29:53,200 --> 00:30:00,050
but if we narrow down potential results

537
00:29:56,660 --> 00:30:02,630
we will find it way easier to find the

538
00:30:00,050 --> 00:30:06,820
values for doubled years so that the

539
00:30:02,630 --> 00:30:06,820
neural network does what we wanted to do

540
00:30:07,960 --> 00:30:15,200
before you rush out of this room let me

541
00:30:12,140 --> 00:30:17,210
explain you some far in formulas I'll

542
00:30:15,200 --> 00:30:19,670
explain those because I will use them my

543
00:30:17,210 --> 00:30:24,200
demos and it's interesting so that you

544
00:30:19,670 --> 00:30:28,310
know what it's all about first one is

545
00:30:24,200 --> 00:30:31,760
sigmoid firing function so the exit the

546
00:30:28,310 --> 00:30:34,040
output is somewhere between 0 and 5 and

547
00:30:31,760 --> 00:30:39,290
this is the formula and we see the

548
00:30:34,040 --> 00:30:44,389
output it will be from 0 to 5 and this

549
00:30:39,290 --> 00:30:46,659
is an one sorry and this is the graph so

550
00:30:44,390 --> 00:30:50,600
we work within a range that could be

551
00:30:46,660 --> 00:30:53,960
interesting but this Sigma firing

552
00:30:50,600 --> 00:30:57,949
function was outperformed by the

553
00:30:53,960 --> 00:31:01,580
hyperbolic tangent line the narrows down

554
00:30:57,950 --> 00:31:05,180
to minus 1 plus 1 and

555
00:31:01,580 --> 00:31:07,760
descender around zero so it is more

556
00:31:05,180 --> 00:31:10,270
interesting to use this based on

557
00:31:07,760 --> 00:31:14,830
different problems and this hyperbolic

558
00:31:10,270 --> 00:31:18,920
tangent line is an escalated sigmoid

559
00:31:14,830 --> 00:31:21,649
function this is the one that we

560
00:31:18,920 --> 00:31:24,370
multiply and so it's narrow down between

561
00:31:21,650 --> 00:31:28,570
minus 1 and plus 1

562
00:31:24,370 --> 00:31:34,129
but then in 2014 one of the most used

563
00:31:28,570 --> 00:31:41,139
Fyren formulas this one we are oh I

564
00:31:34,130 --> 00:31:45,170
removed it well it it would be real ooh

565
00:31:41,140 --> 00:31:52,340
is one of the the ones I used and we'll

566
00:31:45,170 --> 00:31:55,160
see oh this is the state if the

567
00:31:52,340 --> 00:31:57,139
activation function is below zero result

568
00:31:55,160 --> 00:31:59,330
is zero is above zero the result is the

569
00:31:57,140 --> 00:32:01,550
value itself and why is there much used

570
00:31:59,330 --> 00:32:05,270
because this is the activation function

571
00:32:01,550 --> 00:32:08,000
that looks most like our the one that

572
00:32:05,270 --> 00:32:10,790
our neural neurons is that's the way our

573
00:32:08,000 --> 00:32:14,420
neurons fire when they get of stimulus F

574
00:32:10,790 --> 00:32:17,330
following that graph so if the if it is

575
00:32:14,420 --> 00:32:21,500
minus zero return zero and if it's

576
00:32:17,330 --> 00:32:24,439
positive returns the value itself is

577
00:32:21,500 --> 00:32:26,870
planked explained already the

578
00:32:24,440 --> 00:32:29,990
architecture of the artificial neuron

579
00:32:26,870 --> 00:32:35,719
the simple perceptron but how does it

580
00:32:29,990 --> 00:32:38,570
learn that perceptron this is frame you

581
00:32:35,720 --> 00:32:43,760
see if you agree with fray or not well

582
00:32:38,570 --> 00:32:47,689
what we do is we use an algorithm known

583
00:32:43,760 --> 00:32:50,870
as gradient decrease algorithm if you're

584
00:32:47,690 --> 00:32:54,890
working on AI is quite quite common this

585
00:32:50,870 --> 00:32:58,459
one is used to handle all those

586
00:32:54,890 --> 00:33:01,850
decisions I said already car-driving

587
00:32:58,460 --> 00:33:08,330
medical questions everything so it's

588
00:33:01,850 --> 00:33:12,959
good to know this it start with T Sub

589
00:33:08,330 --> 00:33:16,260
Zero with a random mask setup of weight

590
00:33:12,960 --> 00:33:19,170
I want to find the fine value to the

591
00:33:16,260 --> 00:33:21,740
weight so that our renewal networks do

592
00:33:19,170 --> 00:33:26,400
what we want we start with this

593
00:33:21,740 --> 00:33:30,450
randomized one and it can be it the

594
00:33:26,400 --> 00:33:34,310
bell-shaped but we have that starting

595
00:33:30,450 --> 00:33:34,310
randomized and this would be it right

596
00:33:36,080 --> 00:33:45,980
and we need to have an error function

597
00:33:41,040 --> 00:33:48,620
define to know how much a mistake our

598
00:33:45,980 --> 00:33:54,450
algorithm is with the predictive model

599
00:33:48,620 --> 00:33:58,709
as we said it we change it into a

600
00:33:54,450 --> 00:34:00,300
predictive model which we need to have a

601
00:33:58,710 --> 00:34:06,200
way to understand when it's making a

602
00:34:00,300 --> 00:34:12,989
mistake and this is an error function

603
00:34:06,200 --> 00:34:15,000
this one here this one and we need to

604
00:34:12,989 --> 00:34:19,859
minimize the error function there are

605
00:34:15,000 --> 00:34:22,679
needs to be as little as possible so we

606
00:34:19,860 --> 00:34:26,420
are here starting point of the weights

607
00:34:22,679 --> 00:34:31,710
of initial value and what we need to do

608
00:34:26,420 --> 00:34:34,740
is calculate the direction where the

609
00:34:31,710 --> 00:34:39,929
error will grow the most we would

610
00:34:34,739 --> 00:34:43,529
calculate that how do we do it we use

611
00:34:39,929 --> 00:34:47,460
what's known gradient for the error

612
00:34:43,530 --> 00:34:50,610
function so that's the very very very

613
00:34:47,460 --> 00:34:52,800
but if so this is a function an error

614
00:34:50,610 --> 00:34:55,110
function telling us when there is a

615
00:34:52,800 --> 00:34:58,230
mistake and we need to know where it

616
00:34:55,110 --> 00:35:00,480
grows the most starting here and this is

617
00:34:58,230 --> 00:35:03,750
calculated through the derivative of

618
00:35:00,480 --> 00:35:06,270
said function it would be the slope or

619
00:35:03,750 --> 00:35:08,640
gradient of the tangent line to the

620
00:35:06,270 --> 00:35:11,100
curve back in high school derivatives

621
00:35:08,640 --> 00:35:14,129
were studied and what's a point well a

622
00:35:11,100 --> 00:35:17,130
derivatives help us now develop

623
00:35:14,130 --> 00:35:20,190
algorithms making decisions that have an

624
00:35:17,130 --> 00:35:26,180
influence in our daily lives so quite

625
00:35:20,190 --> 00:35:28,099
quite important we calculated the error

626
00:35:26,180 --> 00:35:33,319
grading it would be this straight line

627
00:35:28,099 --> 00:35:37,579
this is a slope and what we do is update

628
00:35:33,319 --> 00:35:41,900
those weights following the opposite

629
00:35:37,579 --> 00:35:46,700
direction to the largest point or where

630
00:35:41,900 --> 00:35:49,339
it grows the most so we go downwards we

631
00:35:46,700 --> 00:35:50,960
would go downward and update waves so

632
00:35:49,339 --> 00:35:53,710
these are the new weights that would

633
00:35:50,960 --> 00:35:57,349
equal the weights available now plus

634
00:35:53,710 --> 00:35:59,960
hear you sing this is the Alpha learning

635
00:35:57,349 --> 00:36:05,750
constant that would be the step that I

636
00:35:59,960 --> 00:36:09,650
use or rather the gap the the play so

637
00:36:05,750 --> 00:36:12,020
and we add the error function derivative

638
00:36:09,650 --> 00:36:13,579
for those weights this way we get the

639
00:36:12,020 --> 00:36:17,900
new weight that will be somewhere around

640
00:36:13,579 --> 00:36:21,530
here and so the goal would be to reach

641
00:36:17,900 --> 00:36:24,500
this point here which is the point where

642
00:36:21,530 --> 00:36:28,010
the error function is the smallest where

643
00:36:24,500 --> 00:36:30,890
the error made by our function is the

644
00:36:28,010 --> 00:36:32,599
smallest how do we start it here we

645
00:36:30,890 --> 00:36:35,540
would have gone down and down and down

646
00:36:32,599 --> 00:36:39,369
all the way down here this is called a

647
00:36:35,540 --> 00:36:42,349
local minimum so it is not the smallest

648
00:36:39,369 --> 00:36:44,059
minimum for the error function because

649
00:36:42,349 --> 00:36:45,799
there's a smaller one this one here but

650
00:36:44,059 --> 00:36:48,109
there are some techniques such as hyper

651
00:36:45,799 --> 00:36:51,980
parameters that I mentioned that will

652
00:36:48,109 --> 00:36:54,890
help us reset it and go down here all

653
00:36:51,980 --> 00:36:57,470
the way down to the global minimum value

654
00:36:54,890 --> 00:37:00,520
the minute we get this this is the

655
00:36:57,470 --> 00:37:03,348
smallest volume that's when our

656
00:37:00,520 --> 00:37:05,270
algorithm will know how to do what we're

657
00:37:03,349 --> 00:37:09,170
interested in we need to minimize the

658
00:37:05,270 --> 00:37:11,839
error function to minimize if we're

659
00:37:09,170 --> 00:37:15,400
really mistaken in our predictions are

660
00:37:11,839 --> 00:37:15,400
you're holding up quite quite well

661
00:37:15,670 --> 00:37:21,950
I've shown you a new run so far an

662
00:37:18,740 --> 00:37:25,660
artificial neuron and I got back not

663
00:37:21,950 --> 00:37:28,970
just a one neuron but several neural

664
00:37:25,660 --> 00:37:31,640
networks artificial networks we've got

665
00:37:28,970 --> 00:37:36,350
different neural layers first we have an

666
00:37:31,640 --> 00:37:40,850
entry layer get in the infant the entry

667
00:37:36,350 --> 00:37:45,799
values so maybe we have a picture that

668
00:37:40,850 --> 00:37:49,970
is 2256 per 256 pixels and there is a 0

669
00:37:45,800 --> 00:37:52,670
9 digit that is handwritten right how

670
00:37:49,970 --> 00:37:58,990
many neurons do we need for this entry

671
00:37:52,670 --> 00:38:02,030
layer 256 x 256 is the number of pixels

672
00:37:58,990 --> 00:38:04,100
that would be the entry point for the

673
00:38:02,030 --> 00:38:07,480
neural network that would be the

674
00:38:04,100 --> 00:38:10,279
starting point of the neural network and

675
00:38:07,480 --> 00:38:12,710
what we want is for the neural network

676
00:38:10,280 --> 00:38:16,430
to tell us what's the digit that we've

677
00:38:12,710 --> 00:38:18,980
handwritten into the picture I said that

678
00:38:16,430 --> 00:38:21,529
can be it can range from 0 to 9 so how

679
00:38:18,980 --> 00:38:25,430
many neurons can we find on the exit

680
00:38:21,530 --> 00:38:28,550
layer that would be from 0 to 9 we're

681
00:38:25,430 --> 00:38:31,129
shaping it we know how many neurons we

682
00:38:28,550 --> 00:38:34,670
need on the entry level and the exit

683
00:38:31,130 --> 00:38:37,850
level or layer rubber it is clear but

684
00:38:34,670 --> 00:38:41,390
what about concealed hidden layers in

685
00:38:37,850 --> 00:38:44,180
between the ones working magic so that

686
00:38:41,390 --> 00:38:47,120
in the end the neural network can tell

687
00:38:44,180 --> 00:38:49,660
apart which one is the handwritten digit

688
00:38:47,120 --> 00:38:53,830
in our picture these are the layers

689
00:38:49,660 --> 00:38:56,839
hidden layers every layer hidden layer

690
00:38:53,830 --> 00:38:59,810
will learn from the set of features a

691
00:38:56,840 --> 00:39:03,380
set of attributes that it's the output

692
00:38:59,810 --> 00:39:05,450
from the previous night yam so what does

693
00:39:03,380 --> 00:39:08,330
it mean it's a bit weird right now but

694
00:39:05,450 --> 00:39:11,629
the minute we move forward throughout

695
00:39:08,330 --> 00:39:14,029
the network neurons will be able to

696
00:39:11,630 --> 00:39:18,260
detect features that are a bit more

697
00:39:14,030 --> 00:39:23,090
complex so picture this first neurons

698
00:39:18,260 --> 00:39:27,500
can only tell apart corners address then

699
00:39:23,090 --> 00:39:28,820
next layer silhouette may be the deepest

700
00:39:27,500 --> 00:39:31,130
ones

701
00:39:28,820 --> 00:39:35,690
depending on how we make them specialize

702
00:39:31,130 --> 00:39:39,560
they can fly spot phases that's a

703
00:39:35,690 --> 00:39:42,830
feature higher game and it's a higher

704
00:39:39,560 --> 00:39:45,799
key that it's growing in complexity and

705
00:39:42,830 --> 00:39:49,130
what's most important growing in

706
00:39:45,800 --> 00:39:53,240
abstraction so as the network moves

707
00:39:49,130 --> 00:39:55,580
forward it works on concept abstraction

708
00:39:53,240 --> 00:39:58,580
which means after training that network

709
00:39:55,580 --> 00:40:03,170
for car detection for example if there

710
00:39:58,580 --> 00:40:06,500
is a fear panel or a Ferrari it will say

711
00:40:03,170 --> 00:40:08,690
both our cars and a donkey it will say

712
00:40:06,500 --> 00:40:11,500
it's not a car because it's been trained

713
00:40:08,690 --> 00:40:19,670
for that purpose the neural network can

714
00:40:11,500 --> 00:40:25,900
get up comes up abstraction and right &

715
00:40:19,670 --> 00:40:25,900
dicie's whirring neuron and network

716
00:40:26,320 --> 00:40:33,410
architecture is and then then I told you

717
00:40:30,440 --> 00:40:36,110
about the learning mode of these

718
00:40:33,410 --> 00:40:38,600
artificial neurons and now I will tell

719
00:40:36,110 --> 00:40:41,090
you how a whole network of neurons learn

720
00:40:38,600 --> 00:40:43,370
by using the technique of gradient

721
00:40:41,090 --> 00:40:45,020
descent that we saw before but this time

722
00:40:43,370 --> 00:40:48,380
we will be adding other elements and

723
00:40:45,020 --> 00:40:54,110
these other things are referred to as

724
00:40:48,380 --> 00:40:55,280
back preparation algorithm and even if

725
00:40:54,110 --> 00:40:57,470
these set formula

726
00:40:55,280 --> 00:41:00,640
look a bit strange how we'll explain

727
00:40:57,470 --> 00:41:04,160
them to you and we start off from

728
00:41:00,640 --> 00:41:06,109
initial values of random weights I've

729
00:41:04,160 --> 00:41:07,910
repeated again these are the values that

730
00:41:06,110 --> 00:41:10,010
we want to find the values of the

731
00:41:07,910 --> 00:41:13,850
weights and then what we will be doing

732
00:41:10,010 --> 00:41:16,910
is to take that set of training data we

733
00:41:13,850 --> 00:41:19,700
will just get a row of that set of data

734
00:41:16,910 --> 00:41:22,730
of the set of features and then we will

735
00:41:19,700 --> 00:41:24,919
put them through here we will go through

736
00:41:22,730 --> 00:41:27,740
this entry layer and we will also

737
00:41:24,920 --> 00:41:30,320
include the value of the target the

738
00:41:27,740 --> 00:41:33,200
value of the objective for that set of

739
00:41:30,320 --> 00:41:36,020
features so for instance imagine a bank

740
00:41:33,200 --> 00:41:37,790
transaction go and say well for that

741
00:41:36,020 --> 00:41:41,330
bank transaction that is going through

742
00:41:37,790 --> 00:41:42,109
here we are going to tell whether it is

743
00:41:41,330 --> 00:41:47,749
legit

744
00:41:42,109 --> 00:41:50,058
or fraud then the neuron your neural

745
00:41:47,749 --> 00:41:53,089
network for that set of features will

746
00:41:50,059 --> 00:41:56,089
calculate the output that is to say it

747
00:41:53,089 --> 00:41:58,279
would try to see whether it can predict

748
00:41:56,089 --> 00:42:02,058
whether the future values have fraud or

749
00:41:58,279 --> 00:42:04,309
not and for that it does what I told you

750
00:42:02,059 --> 00:42:08,029
before that is to say multiplying the

751
00:42:04,309 --> 00:42:10,279
entries but wait then they are added to

752
00:42:08,029 --> 00:42:13,489
each other and then that's the way it

753
00:42:10,279 --> 00:42:16,220
moves forward in the network and then

754
00:42:13,489 --> 00:42:18,739
this is what we saw before for a simple

755
00:42:16,220 --> 00:42:21,049
neuron and now we multiply the weight

756
00:42:18,739 --> 00:42:23,779
about the entries and then we sum it up

757
00:42:21,049 --> 00:42:26,599
and sum it up up until we've reached the

758
00:42:23,779 --> 00:42:29,299
entry exit layer and then we get the

759
00:42:26,599 --> 00:42:32,900
result for each of the neurons here and

760
00:42:29,299 --> 00:42:38,089
what we have to do now I told you that

761
00:42:32,900 --> 00:42:41,480
the network received the entry pattern

762
00:42:38,089 --> 00:42:43,369
and it calculated the target value that

763
00:42:41,480 --> 00:42:45,769
is to say told us whether this is fraud

764
00:42:43,369 --> 00:42:48,230
or not and therefore we will compare it

765
00:42:45,769 --> 00:42:51,348
with a real value that we had for the

766
00:42:48,230 --> 00:42:53,660
set of training values and we make

767
00:42:51,349 --> 00:42:56,299
comparison as we see here on the screen

768
00:42:53,660 --> 00:42:59,180
let us calculate the error that's X

769
00:42:56,299 --> 00:43:02,809
placing each neuron and we will starting

770
00:42:59,180 --> 00:43:05,328
off with this layer and this time you

771
00:43:02,809 --> 00:43:09,339
will understand why it is named back

772
00:43:05,329 --> 00:43:12,049
propagation we have these values 1 2 and

773
00:43:09,339 --> 00:43:14,690
these are the values that we have given

774
00:43:12,049 --> 00:43:15,200
to it and then we compare that with the

775
00:43:14,690 --> 00:43:17,089
D

776
00:43:15,200 --> 00:43:19,730
these are sent for the real value and

777
00:43:17,089 --> 00:43:22,400
take it from there and we calculate we

778
00:43:19,730 --> 00:43:25,160
work out the error this is Delta letter

779
00:43:22,400 --> 00:43:28,700
is the error that's expressing each of

780
00:43:25,160 --> 00:43:30,410
the entry layers so the value that we

781
00:43:28,700 --> 00:43:34,009
should obtain for this new term minus

782
00:43:30,410 --> 00:43:38,480
the actual value multiplied by the

783
00:43:34,009 --> 00:43:41,089
derivative of the ever function and then

784
00:43:38,480 --> 00:43:43,099
we will calculate the error that takes

785
00:43:41,089 --> 00:43:45,380
place in this noodle and this other one

786
00:43:43,099 --> 00:43:48,410
and this other one and for me to know

787
00:43:45,380 --> 00:43:50,539
what are the right weights I really want

788
00:43:48,410 --> 00:43:52,640
to find out about the error that took

789
00:43:50,539 --> 00:43:55,460
place in each of the neurons because

790
00:43:52,640 --> 00:43:57,440
weights are all over the network

791
00:43:55,460 --> 00:43:59,660
I want to know the other in each name

792
00:43:57,440 --> 00:44:03,050
you know so therefore I want to

793
00:43:59,660 --> 00:44:05,270
propagate the error backwards to

794
00:44:03,050 --> 00:44:07,339
calculate the errors that took place in

795
00:44:05,270 --> 00:44:11,000
these intermediate layers this will be

796
00:44:07,339 --> 00:44:14,150
derivative of the gradient of faction

797
00:44:11,000 --> 00:44:16,730
ever multiplied by the sum of these

798
00:44:14,150 --> 00:44:20,390
neurons multiplied by this weight plus

799
00:44:16,730 --> 00:44:24,579
the error of this neuron + neuron by

800
00:44:20,390 --> 00:44:27,589
this weight so we are propagating we are

801
00:44:24,579 --> 00:44:29,990
propagating Everest backwards then we

802
00:44:27,589 --> 00:44:33,410
take a look at all the layers up until

803
00:44:29,990 --> 00:44:36,140
we get to the start of the network and

804
00:44:33,410 --> 00:44:38,509
now we know how what are the errors made

805
00:44:36,140 --> 00:44:42,470
in each mistake and this will help us

806
00:44:38,510 --> 00:44:45,440
update the weights and ultimately to

807
00:44:42,470 --> 00:44:48,140
minimize the error and we will minimize

808
00:44:45,440 --> 00:44:51,430
the error by modifying the weights and

809
00:44:48,140 --> 00:44:54,348
this is how I updated the new weight of

810
00:44:51,430 --> 00:44:57,618
neuron in the exit layer would be this

811
00:44:54,349 --> 00:45:01,520
new weight plus this gradient which is

812
00:44:57,619 --> 00:45:06,230
alpha which is the learning rate that is

813
00:45:01,520 --> 00:45:08,569
to say the speed that we with which we

814
00:45:06,230 --> 00:45:13,099
travel this curve multiplied by the

815
00:45:08,569 --> 00:45:16,609
error and by the output the error of

816
00:45:13,099 --> 00:45:19,400
this neuron plus the output generated by

817
00:45:16,609 --> 00:45:21,828
the neuron and these will help us get

818
00:45:19,400 --> 00:45:26,450
the new weights and we do it exactly the

819
00:45:21,829 --> 00:45:30,770
same but backwards and this process will

820
00:45:26,450 --> 00:45:34,279
be repeated and you will wonder when do

821
00:45:30,770 --> 00:45:37,220
you stop yes because I calculate the

822
00:45:34,280 --> 00:45:38,960
errors and I go backwards and then in

823
00:45:37,220 --> 00:45:40,700
the end I end up updating all the

824
00:45:38,960 --> 00:45:43,250
weights and I said when do I stop going

825
00:45:40,700 --> 00:45:46,339
back and forth well in the end we will

826
00:45:43,250 --> 00:45:52,220
be using a matrix for that whether it

827
00:45:46,339 --> 00:45:55,190
could be the mean square average mini

828
00:45:52,220 --> 00:45:59,058
square and then we calculate this error

829
00:45:55,190 --> 00:46:02,180
we elevated to the power of two then we

830
00:45:59,059 --> 00:46:07,670
calculate the error and then if the

831
00:46:02,180 --> 00:46:09,230
result of that root square average is

832
00:46:07,670 --> 00:46:12,890
sufficiently small for

833
00:46:09,230 --> 00:46:14,869
as we stop therefore you may go and say

834
00:46:12,890 --> 00:46:17,000
well this is you have to be like really

835
00:46:14,869 --> 00:46:19,910
you have to follow your intuition

836
00:46:17,000 --> 00:46:24,670
perhaps well you will stop by the time

837
00:46:19,910 --> 00:46:24,670
the probability error will be very small

838
00:46:24,970 --> 00:46:30,439
you calculate the error to propagate

839
00:46:27,980 --> 00:46:32,690
them backwards so that you can calculate

840
00:46:30,440 --> 00:46:35,780
the new weights up until the error

841
00:46:32,690 --> 00:46:42,910
becomes small enough for it to be

842
00:46:35,780 --> 00:46:45,950
acceptable and I say medium square root

843
00:46:42,910 --> 00:46:49,220
but there are other other mean other

844
00:46:45,950 --> 00:46:52,580
ways to calculate this root mean square

845
00:46:49,220 --> 00:46:56,000
and to do this we also use other

846
00:46:52,580 --> 00:46:59,119
algorithms so for instance to find out

847
00:46:56,000 --> 00:47:00,740
whether and back transaction is flawed

848
00:46:59,119 --> 00:47:02,750
or not that would be type of

849
00:47:00,740 --> 00:47:04,910
classification or for instance I don't

850
00:47:02,750 --> 00:47:07,550
know predicting the value of a home

851
00:47:04,910 --> 00:47:09,980
which is located in a specific area in

852
00:47:07,550 --> 00:47:13,010
these cases one of the metrics that is

853
00:47:09,980 --> 00:47:16,369
used to find out how good an algorithm

854
00:47:13,010 --> 00:47:23,380
is is the area under the curve so you

855
00:47:16,369 --> 00:47:23,380
may see here a 45-degree straight line

856
00:47:23,680 --> 00:47:31,190
and this is kind of like getting it

857
00:47:27,800 --> 00:47:35,390
right out of chance when you toss a coin

858
00:47:31,190 --> 00:47:38,090
you have 50% chances that to get one

859
00:47:35,390 --> 00:47:40,790
side or the other so these are the 50%

860
00:47:38,090 --> 00:47:43,430
likeliness that my algorithm tells me

861
00:47:40,790 --> 00:47:48,859
that my that transaction is see the

862
00:47:43,430 --> 00:47:51,160
widget Oh fraud we want this metric to

863
00:47:48,859 --> 00:47:52,369
tell us that this curve is above that

864
00:47:51,160 --> 00:47:55,100
50%

865
00:47:52,369 --> 00:47:57,080
therefore this is the result of a

866
00:47:55,100 --> 00:48:00,618
predictive model and as it says here

867
00:47:57,080 --> 00:48:03,770
this is good but here in jello I don't

868
00:48:00,619 --> 00:48:06,050
know whether you can see it that it is

869
00:48:03,770 --> 00:48:08,810
tells us that it is excellent we want

870
00:48:06,050 --> 00:48:11,300
that the area and the curve is as big as

871
00:48:08,810 --> 00:48:13,190
possible as large as possible if it is

872
00:48:11,300 --> 00:48:17,240
really big it means that the algorithm

873
00:48:13,190 --> 00:48:20,510
is never wrong this is not easy but what

874
00:48:17,240 --> 00:48:22,279
we see here the area under the curve we

875
00:48:20,510 --> 00:48:23,030
want it to be as close as possible to

876
00:48:22,280 --> 00:48:25,790
one

877
00:48:23,030 --> 00:48:27,560
and how do you calculate that well there

878
00:48:25,790 --> 00:48:30,590
is a formula for that I wanted to

879
00:48:27,560 --> 00:48:34,549
mention that to you because surely you

880
00:48:30,590 --> 00:48:38,180
will hear these from people who are into

881
00:48:34,550 --> 00:48:40,520
a a they will talk about true positives

882
00:48:38,180 --> 00:48:43,370
true negatives can we set a true

883
00:48:40,520 --> 00:48:46,490
positive rate true positive rate it's

884
00:48:43,370 --> 00:48:48,680
also known as sensitivity so the rate of

885
00:48:46,490 --> 00:48:51,290
true positive is calculated by dividing

886
00:48:48,680 --> 00:48:53,390
the true positives and then you may go

887
00:48:51,290 --> 00:48:55,850
and say what is a true positive well a

888
00:48:53,390 --> 00:48:59,210
true positive is when my algorithm tells

889
00:48:55,850 --> 00:49:01,610
me that intersection is fraught and the

890
00:48:59,210 --> 00:49:06,260
prediction is correct divided by the

891
00:49:01,610 --> 00:49:08,210
true positive plus d2 a false negatives

892
00:49:06,260 --> 00:49:10,220
false negatives is when my algorithm

893
00:49:08,210 --> 00:49:12,290
tells me that a transaction is not

894
00:49:10,220 --> 00:49:15,049
flawed but it actually it is actually

895
00:49:12,290 --> 00:49:18,560
fun so we are dividing the true positive

896
00:49:15,050 --> 00:49:21,830
by all the false positives and on the

897
00:49:18,560 --> 00:49:25,400
x-axis will have a positive ray and we

898
00:49:21,830 --> 00:49:27,860
have the false positives and this is

899
00:49:25,400 --> 00:49:29,930
when our algorithm tells us that at this

900
00:49:27,860 --> 00:49:32,630
action is fraud but it is not actually

901
00:49:29,930 --> 00:49:34,490
fraud so decided through negatives my

902
00:49:32,630 --> 00:49:39,110
algorithm predicts that it is not fraud

903
00:49:34,490 --> 00:49:41,120
and actually it is not plus the false

904
00:49:39,110 --> 00:49:44,720
positives that I mentioned before

905
00:49:41,120 --> 00:49:47,630
so with this formula this formula allow

906
00:49:44,720 --> 00:49:51,350
us to generate to plot this graph the

907
00:49:47,630 --> 00:49:57,410
bigger the area the closer to value 1

908
00:49:51,350 --> 00:50:00,860
what area under curve is close to 1 let

909
00:49:57,410 --> 00:50:04,940
us say 0.9 this tells us that we are on

910
00:50:00,860 --> 00:50:09,680
the right path done so well acceptable

911
00:50:04,940 --> 00:50:13,250
rate 80 percent 85 percent depending on

912
00:50:09,680 --> 00:50:16,009
the purpose of that of the network

913
00:50:13,250 --> 00:50:18,560
neural network and other metrics that

914
00:50:16,010 --> 00:50:20,650
are useful in my demo I just wanted to

915
00:50:18,560 --> 00:50:23,299
share that with you so that you don't

916
00:50:20,650 --> 00:50:26,720
get lost when you see it is the lock

917
00:50:23,300 --> 00:50:29,240
loss so these measures the lack of

918
00:50:26,720 --> 00:50:32,089
certainty of the probabilities of each

919
00:50:29,240 --> 00:50:35,149
class that is to say my algorithm would

920
00:50:32,090 --> 00:50:37,850
assign a priority to each classroom

921
00:50:35,150 --> 00:50:39,740
class of possibility of it being fraud

922
00:50:37,850 --> 00:50:41,299
or not being front and then it mentions

923
00:50:39,740 --> 00:50:43,970
the lack of sentences of those

924
00:50:41,300 --> 00:50:47,000
probabilities and compared that to the

925
00:50:43,970 --> 00:50:49,129
real or actual possibilities what is

926
00:50:47,000 --> 00:50:52,100
what happens with the luck loss our

927
00:50:49,130 --> 00:50:54,470
objective here is that that value is

928
00:50:52,100 --> 00:50:57,589
close as possible to zero

929
00:50:54,470 --> 00:50:59,299
well before we wanted to that value to

930
00:50:57,590 --> 00:51:01,070
be closer to one with the area and the

931
00:50:59,300 --> 00:51:06,260
curve and now with low clouds we want it

932
00:51:01,070 --> 00:51:08,900
to be closer to zero so when I will skip

933
00:51:06,260 --> 00:51:14,710
the formula now and now I just wanted to

934
00:51:08,900 --> 00:51:14,710
share fast demo demo for fraud detection

935
00:51:16,300 --> 00:51:19,600
all right

936
00:51:32,190 --> 00:51:45,240
all right I have to change screens give

937
00:51:42,880 --> 00:51:45,240
me a minute

938
00:52:04,060 --> 00:52:14,710
so I didn't know when I could be well or

939
00:52:08,660 --> 00:52:19,750
not so I just wanted to do a live demo

940
00:52:14,710 --> 00:52:19,750
and I also recorded it on video

941
00:52:20,290 --> 00:52:26,740
so then I will show you the video the

942
00:52:24,110 --> 00:52:26,740
demo on video

943
00:52:40,180 --> 00:52:43,109
better now

944
00:52:43,380 --> 00:52:51,870
and we'll tell you what we do okay so

945
00:52:48,990 --> 00:52:56,390
this is an algorithm that will be using

946
00:52:51,870 --> 00:53:00,000
everything that I explained with you to

947
00:52:56,390 --> 00:53:02,759
say whether these bank transactions are

948
00:53:00,000 --> 00:53:06,060
further not so when someone uses our

949
00:53:02,760 --> 00:53:14,040
cards make a transaction this is what

950
00:53:06,060 --> 00:53:18,049
large companies do to detect fraud when

951
00:53:14,040 --> 00:53:20,940
I use the the following platform our

952
00:53:18,050 --> 00:53:24,390
programming language and we also I also

953
00:53:20,940 --> 00:53:28,260
use the deep learning library h2o and

954
00:53:24,390 --> 00:53:31,890
this is the set of data banking

955
00:53:28,260 --> 00:53:35,240
transactions okay I will slow down a bit

956
00:53:31,890 --> 00:53:35,240
so I will stop here

957
00:53:47,670 --> 00:53:58,840
okay these hard data that we use this is

958
00:53:51,760 --> 00:54:00,970
a set of banking transaction data it is

959
00:53:58,840 --> 00:54:04,300
very difficult to find these sets of

960
00:54:00,970 --> 00:54:05,950
data because what credit card companies

961
00:54:04,300 --> 00:54:06,340
are not interested in disclosing this

962
00:54:05,950 --> 00:54:08,740
data

963
00:54:06,340 --> 00:54:11,650
well I found these data sets which is

964
00:54:08,740 --> 00:54:13,479
anonymous it means that these columns

965
00:54:11,650 --> 00:54:17,410
the features characteristics or

966
00:54:13,480 --> 00:54:21,850
predictors or synonyms so I prefer to us

967
00:54:17,410 --> 00:54:24,609
v1 v2 where there is one style before v3

968
00:54:21,850 --> 00:54:27,880
very many V letters that have made

969
00:54:24,610 --> 00:54:35,200
having made anonymous so the names of

970
00:54:27,880 --> 00:54:38,200
the features have been removed yeah but

971
00:54:35,200 --> 00:54:42,009
here you will see at last column that

972
00:54:38,200 --> 00:54:46,899
tells us if each of the rows is a fraud

973
00:54:42,010 --> 00:54:52,450
or not alabanza on the bottom Pokhara

974
00:54:46,900 --> 00:54:56,560
Garrido so we do some parsing and then

975
00:54:52,450 --> 00:54:58,120
the last column is class if it says zero

976
00:54:56,560 --> 00:55:00,820
it means that the transaction is no

977
00:54:58,120 --> 00:55:08,740
fraud and then if it is one means that

978
00:55:00,820 --> 00:55:10,720
it is fraud so these commands I do the

979
00:55:08,740 --> 00:55:14,709
feature engineering that is to say I

980
00:55:10,720 --> 00:55:17,259
clean a firstly clean data I take a look

981
00:55:14,710 --> 00:55:20,020
at the current data characteristics I

982
00:55:17,260 --> 00:55:23,800
draw statistical maximum minimum first

983
00:55:20,020 --> 00:55:28,450
quartile third quartile to see how

984
00:55:23,800 --> 00:55:31,860
features are distributed so here we see

985
00:55:28,450 --> 00:55:36,129
the number of rows the number of columns

986
00:55:31,860 --> 00:55:39,610
so this huge number of cells so this is

987
00:55:36,130 --> 00:55:43,500
the data set we will be working with so

988
00:55:39,610 --> 00:55:46,810
the statistic data the median the mean

989
00:55:43,500 --> 00:55:49,300
very important information to see how

990
00:55:46,810 --> 00:55:53,200
each of features we have behaved and

991
00:55:49,300 --> 00:55:55,869
then these data cleaning that's why I

992
00:55:53,200 --> 00:55:58,980
this time consuming one of the things

993
00:55:55,869 --> 00:56:02,170
that we often do our is to detect

994
00:55:58,980 --> 00:56:04,779
irrelevant features and for that we take

995
00:56:02,170 --> 00:56:09,549
a look at those who have serum variants

996
00:56:04,779 --> 00:56:11,680
if all the values of a feature are very

997
00:56:09,549 --> 00:56:14,740
close to zero that tells us that that

998
00:56:11,680 --> 00:56:16,868
feature does is not giving an

999
00:56:14,740 --> 00:56:19,029
information because they their features

1000
00:56:16,869 --> 00:56:21,039
are all the time using the same value we

1001
00:56:19,029 --> 00:56:22,599
are interested in having features

1002
00:56:21,039 --> 00:56:25,180
working with different values so

1003
00:56:22,599 --> 00:56:27,700
therefore the output is different so

1004
00:56:25,180 --> 00:56:30,220
therefore I eliminate those features

1005
00:56:27,700 --> 00:56:32,799
with zero variance this is what I do

1006
00:56:30,220 --> 00:56:35,890
with that code then I also look for

1007
00:56:32,799 --> 00:56:38,680
those features that are related that is

1008
00:56:35,890 --> 00:56:42,279
to say those features that deliver the

1009
00:56:38,680 --> 00:56:44,799
same information I do that through given

1010
00:56:42,279 --> 00:56:47,859
a correlation coefficient as well as

1011
00:56:44,799 --> 00:56:51,130
covariance and this tells us that I take

1012
00:56:47,859 --> 00:56:52,930
into two features and then I see if one

1013
00:56:51,130 --> 00:56:55,180
increases whether the other one is

1014
00:56:52,930 --> 00:56:58,149
increases or not that will be a direct

1015
00:56:55,180 --> 00:57:00,160
linear election relation or if one

1016
00:56:58,150 --> 00:57:04,329
increases the other one decreases that

1017
00:57:00,160 --> 00:57:07,839
will be in direct correlation and if I

1018
00:57:04,329 --> 00:57:13,779
find out that they are related

1019
00:57:07,839 --> 00:57:15,880
I just eliminate one of them so that

1020
00:57:13,779 --> 00:57:18,759
data cleaning that I mentioned the

1021
00:57:15,880 --> 00:57:21,539
beginning is about removing the

1022
00:57:18,759 --> 00:57:24,759
information that is not meaningful for

1023
00:57:21,539 --> 00:57:26,410
learning it is the same as if you are I

1024
00:57:24,759 --> 00:57:29,349
don't know reading a book and then you

1025
00:57:26,410 --> 00:57:34,420
have your notes and both say the same

1026
00:57:29,349 --> 00:57:39,819
thing when you take either of the two so

1027
00:57:34,420 --> 00:57:43,630
after that cleaning I build a base model

1028
00:57:39,819 --> 00:57:52,779
that's where you do the first training I

1029
00:57:43,630 --> 00:57:55,410
would stop it what do you do first first

1030
00:57:52,779 --> 00:58:02,500
you create the most possible basic

1031
00:57:55,410 --> 00:58:04,930
neural network in this case you cannot

1032
00:58:02,500 --> 00:58:07,070
see very well here we have two hidden

1033
00:58:04,930 --> 00:58:09,770
layers we have the

1034
00:58:07,070 --> 00:58:13,670
entry and the exit layer the first one

1035
00:58:09,770 --> 00:58:20,300
has 200 neurons and the other one also

1036
00:58:13,670 --> 00:58:24,080
has 200 hidden neurons so I divided as a

1037
00:58:20,300 --> 00:58:27,110
60% training 40% validation and the

1038
00:58:24,080 --> 00:58:32,230
remaining 20% for tests however I say

1039
00:58:27,110 --> 00:58:37,010
here epochs and I say one run one run

1040
00:58:32,230 --> 00:58:42,620
over the whole set of data with a very

1041
00:58:37,010 --> 00:58:45,260
simple and neuron neuronal and network I

1042
00:58:42,620 --> 00:58:47,960
do this run and then I take a look at

1043
00:58:45,260 --> 00:58:52,180
see what the starting point is I'll take

1044
00:58:47,960 --> 00:58:52,180
it from there and I improve the network

1045
00:58:54,700 --> 00:59:00,859
so it's being trained

1046
00:58:57,080 --> 00:59:05,210
here it's being trained to see this 0%

1047
00:59:00,860 --> 00:59:09,460
here I stopped it because it but it

1048
00:59:05,210 --> 00:59:13,460
takes this time and because I don't have

1049
00:59:09,460 --> 00:59:16,370
GPU in this laptop I had to had a

1050
00:59:13,460 --> 00:59:20,030
computer training during the training

1051
00:59:16,370 --> 00:59:22,040
for a week then these are the outputs

1052
00:59:20,030 --> 00:59:25,760
and what is interesting for me is

1053
00:59:22,040 --> 00:59:29,509
running these 2/9 this is the area under

1054
00:59:25,760 --> 00:59:32,480
the ROC curve that helps me for

1055
00:59:29,510 --> 00:59:36,740
validation and also here we have the con

1056
00:59:32,480 --> 00:59:37,930
funding matrix that we would not wanna

1057
00:59:36,740 --> 00:59:40,339
measure it could be a bit complicated

1058
00:59:37,930 --> 00:59:42,859
but these are indicators that are

1059
00:59:40,340 --> 00:59:45,220
telling me about the training behavior

1060
00:59:42,860 --> 00:59:49,670
this is output information that we are

1061
00:59:45,220 --> 00:59:53,870
getting from the learning process and I

1062
00:59:49,670 --> 00:59:56,090
will be interested in what I get when I

1063
00:59:53,870 --> 00:59:58,910
get here we want these values to be very

1064
00:59:56,090 --> 01:00:01,490
close to one the greater the area and

1065
00:59:58,910 --> 01:00:04,580
the curve the better the more accurate

1066
01:00:01,490 --> 01:00:07,759
my algorithm and let us take a look at

1067
01:00:04,580 --> 01:00:11,090
the starting values for the evaluation

1068
01:00:07,760 --> 01:00:15,800
set of data helping us to see how good

1069
01:00:11,090 --> 01:00:19,250
our validation method was well it is

1070
01:00:15,800 --> 01:00:20,900
point 93 very close to 1 well the

1071
01:00:19,250 --> 01:00:23,839
network is doing very well

1072
01:00:20,900 --> 01:00:28,220
and then for the test one it is

1073
01:00:23,839 --> 01:00:31,609
generalizing very well 95% we have only

1074
01:00:28,220 --> 01:00:36,078
we've only done data cleaning and our

1075
01:00:31,609 --> 01:00:41,058
network is simple as one of the simplest

1076
01:00:36,079 --> 01:00:44,839
and I will stop here to tell you that

1077
01:00:41,059 --> 01:00:46,670
the data that I used are not balanced

1078
01:00:44,839 --> 01:00:51,799
that means that there are many other

1079
01:00:46,670 --> 01:00:55,579
examples of legit transaction and those

1080
01:00:51,799 --> 01:00:59,029
data fraudulent so when we give them

1081
01:00:55,579 --> 01:01:01,339
examples but one of the two types of

1082
01:00:59,029 --> 01:01:04,249
transactions it will learn it will learn

1083
01:01:01,339 --> 01:01:07,069
to better differentiate one versus the

1084
01:01:04,249 --> 01:01:08,240
other and that is not desirable so

1085
01:01:07,069 --> 01:01:12,279
therefore there are a number of

1086
01:01:08,240 --> 01:01:16,160
techniques that are used to balance that

1087
01:01:12,279 --> 01:01:18,799
differentiation and one of them is takes

1088
01:01:16,160 --> 01:01:21,170
care of under sampling of the majority

1089
01:01:18,799 --> 01:01:25,069
class that is to say non frontal and

1090
01:01:21,170 --> 01:01:26,930
examples so do look at how many fraud

1091
01:01:25,069 --> 01:01:31,160
examples you have imagine that you have

1092
01:01:26,930 --> 01:01:33,859
n number then you take n random examples

1093
01:01:31,160 --> 01:01:35,420
of non fraud and then you mix them and

1094
01:01:33,859 --> 01:01:37,549
at the end of the day you have the same

1095
01:01:35,420 --> 01:01:40,339
number of examples of fraud and not

1096
01:01:37,549 --> 01:01:43,099
fraud and what is the problem here well

1097
01:01:40,339 --> 01:01:46,038
I am reducing the set of data that I'm

1098
01:01:43,099 --> 01:01:49,130
training so therefore well less example

1099
01:01:46,039 --> 01:01:52,999
let's lower learning but another

1100
01:01:49,130 --> 01:01:57,349
technique that we use is over sampling

1101
01:01:52,999 --> 01:02:01,629
of the minority class that means that we

1102
01:01:57,349 --> 01:02:05,720
will be doubling the examples of

1103
01:02:01,630 --> 01:02:08,299
fraudulent transactions here this is the

1104
01:02:05,720 --> 01:02:09,649
minority class we will be doubling it

1105
01:02:08,299 --> 01:02:13,700
and then there is another technique is

1106
01:02:09,650 --> 01:02:15,440
mud it generates and it looks at the

1107
01:02:13,700 --> 01:02:17,299
distribution of the examples of the

1108
01:02:15,440 --> 01:02:20,029
minority class in this case the

1109
01:02:17,299 --> 01:02:23,420
fraudulent class and it generates new

1110
01:02:20,029 --> 01:02:25,579
examples in a synthetic manner examples

1111
01:02:23,420 --> 01:02:27,680
that follow that distribution it takes a

1112
01:02:25,579 --> 01:02:29,539
look at the fraud cases and it generates

1113
01:02:27,680 --> 01:02:31,999
new examples following that distribution

1114
01:02:29,539 --> 01:02:33,470
following that pattern and this is what

1115
01:02:31,999 --> 01:02:34,700
we will be doing now we will be

1116
01:02:33,470 --> 01:02:37,578
inserting toasted

1117
01:02:34,700 --> 01:02:39,890
so that this dataset is balanced and let

1118
01:02:37,579 --> 01:02:42,109
us see if after balancing the data set

1119
01:02:39,890 --> 01:02:54,109
the results that the output that we get

1120
01:02:42,109 --> 01:02:57,890
is better than the prior output so that

1121
01:02:54,109 --> 01:03:00,950
model wasn't trained and am i I am

1122
01:02:57,890 --> 01:03:03,049
uploading it for you to go fast you can

1123
01:03:00,950 --> 01:03:05,328
do it once you train a model you store

1124
01:03:03,050 --> 01:03:11,150
it in this land then you import it later

1125
01:03:05,329 --> 01:03:14,510
on and after loading it we will take a

1126
01:03:11,150 --> 01:03:17,630
look at the area under curve it has

1127
01:03:14,510 --> 01:03:21,260
improved it was 93 percent before now 25

1128
01:03:17,630 --> 01:03:24,440
percent and here for test before we have

1129
01:03:21,260 --> 01:03:26,930
95 now we have 96 we are improving

1130
01:03:24,440 --> 01:03:27,890
however our algorithm is doing better

1131
01:03:26,930 --> 01:03:31,640
and better

1132
01:03:27,890 --> 01:03:33,589
so the I'm finishing the demo with

1133
01:03:31,640 --> 01:03:37,009
forward little bits and basically the

1134
01:03:33,589 --> 01:03:39,290
next thing that I do is what is known as

1135
01:03:37,010 --> 01:03:41,150
high parameter grid that is to say

1136
01:03:39,290 --> 01:03:45,050
random search with very many parameters

1137
01:03:41,150 --> 01:03:47,260
I try different architectures and I see

1138
01:03:45,050 --> 01:03:50,270
what is the output of each of

1139
01:03:47,260 --> 01:03:52,579
architecture and I stick to the best I

1140
01:03:50,270 --> 01:03:54,829
did it with a small network but the next

1141
01:03:52,579 --> 01:03:57,050
one would be a grid search that is to

1142
01:03:54,829 --> 01:04:00,290
save testing and trying very many hyper

1143
01:03:57,050 --> 01:04:03,050
parameters for it to be trained better

1144
01:04:00,290 --> 01:04:05,750
and faster and Elia and as I say at the

1145
01:04:03,050 --> 01:04:09,190
end I stick to the architecture that has

1146
01:04:05,750 --> 01:04:09,190
the best performance

1147
01:04:15,090 --> 01:04:20,950
and these behalf of the hyperparameters

1148
01:04:18,520 --> 01:04:23,350
I don't have the time to explain them to

1149
01:04:20,950 --> 01:04:25,720
you decide the activation functions that

1150
01:04:23,350 --> 01:04:28,240
I try I try the conditions for is to

1151
01:04:25,720 --> 01:04:32,620
stop when the training is not improving

1152
01:04:28,240 --> 01:04:34,359
I tell it to stop training here we see

1153
01:04:32,620 --> 01:04:37,299
all the models that have been generated

1154
01:04:34,360 --> 01:04:40,390
14 models here they are arranged from

1155
01:04:37,300 --> 01:04:43,810
the best to the worst and I take the

1156
01:04:40,390 --> 01:04:48,120
best and this is what you do with this

1157
01:04:43,810 --> 01:04:58,060
type of projects I keep the best one

1158
01:04:48,120 --> 01:05:02,290
here and this is now going to show you

1159
01:04:58,060 --> 01:05:07,480
the area under curve so that you see the

1160
01:05:02,290 --> 01:05:11,560
improvement now again finishing

1161
01:05:07,480 --> 01:05:13,540
calculating and for validation and the

1162
01:05:11,560 --> 01:05:16,810
six percentage was ninety five percent

1163
01:05:13,540 --> 01:05:22,180
before and for test now we have 97

1164
01:05:16,810 --> 01:05:28,480
percent Wow it is quite quite accurate

1165
01:05:22,180 --> 01:05:32,410
and last I predict number of predictions

1166
01:05:28,480 --> 01:05:37,570
to see how it behaves all with regard to

1167
01:05:32,410 --> 01:05:39,549
the set of tests here I points out the

1168
01:05:37,570 --> 01:05:41,590
most relevant features the one that have

1169
01:05:39,550 --> 01:05:44,470
the greatest impact in telling me

1170
01:05:41,590 --> 01:05:48,490
whether the transaction is fraudulent or

1171
01:05:44,470 --> 01:05:51,879
not because these data are hidden okay

1172
01:05:48,490 --> 01:05:53,709
we cannot really are anonymous this time

1173
01:05:51,880 --> 01:05:56,110
we don't have the names but it were very

1174
01:05:53,710 --> 01:06:00,310
helpful to find out about the actual

1175
01:05:56,110 --> 01:06:02,650
causes causing a transaction to be a

1176
01:06:00,310 --> 01:06:06,580
Fretilin one so that we can mitigate

1177
01:06:02,650 --> 01:06:09,840
them here I make predictions I predict

1178
01:06:06,580 --> 01:06:12,880
cases and I say a fraudulent case

1179
01:06:09,840 --> 01:06:16,630
algorithm tells me that it is fraud run

1180
01:06:12,880 --> 01:06:20,260
when I compare it with the set of tests

1181
01:06:16,630 --> 01:06:22,390
I can see that it is an actual fraud and

1182
01:06:20,260 --> 01:06:24,760
then this is class one it predicts it

1183
01:06:22,390 --> 01:06:27,129
will be it is class one and he

1184
01:06:24,760 --> 01:06:30,550
it tells me that this is class one

1185
01:06:27,130 --> 01:06:35,740
therefore if that it gets it right so we

1186
01:06:30,550 --> 01:06:39,190
just give him examples of the set of

1187
01:06:35,740 --> 01:06:42,580
tests and then we compare it to the real

1188
01:06:39,190 --> 01:06:46,450
label and then we see the level of match

1189
01:06:42,580 --> 01:06:49,930
or the level of times that it gets it

1190
01:06:46,450 --> 01:06:52,060
right up until we get that 97% I just

1191
01:06:49,930 --> 01:06:55,450
wanted to share with you how this could

1192
01:06:52,060 --> 01:06:57,160
help benefit cancer prevention so this

1193
01:06:55,450 --> 01:07:00,939
time you just don't give it labeled

1194
01:06:57,160 --> 01:07:03,640
cases you just let it let it to find

1195
01:07:00,940 --> 01:07:05,950
patterns you are not going to tell the

1196
01:07:03,640 --> 01:07:08,080
system whether it is fraud or not it

1197
01:07:05,950 --> 01:07:10,750
will look for abnormalities and

1198
01:07:08,080 --> 01:07:13,180
normality in the data and then these

1199
01:07:10,750 --> 01:07:15,220
anomalies will stand for these fraud

1200
01:07:13,180 --> 01:07:20,080
cases in our daily life we follow

1201
01:07:15,220 --> 01:07:23,620
patterns whenever you go out far from

1202
01:07:20,080 --> 01:07:27,880
the front those patterns that's clearly

1203
01:07:23,620 --> 01:07:31,060
a fraudulent course and I had prepared

1204
01:07:27,880 --> 01:07:34,510
case for autoencoders non supervisor

1205
01:07:31,060 --> 01:07:37,509
algorithms for cancer detection but

1206
01:07:34,510 --> 01:07:40,000
because I've run over time and apologies

1207
01:07:37,510 --> 01:07:41,830
I cannot share that with you have a new

1208
01:07:40,000 --> 01:07:45,960
questions further information and

1209
01:07:41,830 --> 01:07:45,960
questions yes I say I'll be

