1
00:00:25,600 --> 00:00:30,020
ellis cose nephews

2
00:00:27,890 --> 00:00:32,390
good morning this is for signifier

3
00:00:30,020 --> 00:00:36,040
Scrivener I'm Alfonso Moniz and we're

4
00:00:32,390 --> 00:00:40,250
gonna talk to you a little bit about AI

5
00:00:36,040 --> 00:00:48,410
which seems an important subject for

6
00:00:40,250 --> 00:00:51,290
everybody but related to computer

7
00:00:48,410 --> 00:00:53,089
security we're talking about offensive

8
00:00:51,290 --> 00:00:59,930
and defensive security with machine

9
00:00:53,090 --> 00:01:04,250
learning that's the title of offensive

10
00:00:59,930 --> 00:01:05,810
security because we're seeing that many

11
00:01:04,250 --> 00:01:07,970
attacks arriving we'll look and see

12
00:01:05,810 --> 00:01:09,530
whether we can be defended from them

13
00:01:07,970 --> 00:01:10,640
that's a little bit of information about

14
00:01:09,530 --> 00:01:13,670
who we are

15
00:01:10,640 --> 00:01:15,830
I'm Alfonzo and you can see the other

16
00:01:13,670 --> 00:01:19,250
person with me is for Cigna Thea so

17
00:01:15,830 --> 00:01:22,250
there are contact details if you need to

18
00:01:19,250 --> 00:01:24,490
contact us you can contact us by email

19
00:01:22,250 --> 00:01:33,860
or on our Twitter accounts

20
00:01:24,490 --> 00:01:37,640
what's our aim a github published if you

21
00:01:33,860 --> 00:01:41,090
want to replicate anything that we've

22
00:01:37,640 --> 00:01:45,800
done what we want to do is to remove all

23
00:01:41,090 --> 00:01:49,459
the worries the concerns the mist that

24
00:01:45,800 --> 00:01:53,300
surrounds this subject we'll look at the

25
00:01:49,459 --> 00:01:55,670
different issues theoretical and

26
00:01:53,300 --> 00:02:02,060
practical and we're going to spend some

27
00:01:55,670 --> 00:02:04,429
time especially talking about all the

28
00:02:02,060 --> 00:02:06,800
different attacks on artificial

29
00:02:04,429 --> 00:02:09,530
intelligence systems some are practical

30
00:02:06,800 --> 00:02:11,630
they're tools that can be downloaded and

31
00:02:09,530 --> 00:02:16,300
can be plugged in and others that are

32
00:02:11,630 --> 00:02:16,299
still theoretical but just go to show

33
00:02:16,569 --> 00:02:22,579
what can be done so we're going to do

34
00:02:19,700 --> 00:02:27,079
the following firstly we're gonna give

35
00:02:22,580 --> 00:02:29,040
you an overview of the context of

36
00:02:27,080 --> 00:02:30,840
artificial intelligence so

37
00:02:29,040 --> 00:02:32,489
if somebody's new to this world and

38
00:02:30,840 --> 00:02:34,890
there's nothing about artificial

39
00:02:32,489 --> 00:02:38,430
intelligence just a brief context

40
00:02:34,890 --> 00:02:44,548
setting we'll talk about what makes a

41
00:02:38,430 --> 00:02:48,319
right unique will then have Jose giving

42
00:02:44,549 --> 00:02:51,209
you a little more practical approach

43
00:02:48,319 --> 00:02:55,760
we're talking about machine learning or

44
00:02:51,209 --> 00:02:55,760
give some demos we'll see how your own

45
00:03:05,359 --> 00:03:10,590
once we've done this context will then

46
00:03:08,010 --> 00:03:14,010
move on to security we'll talk about the

47
00:03:10,590 --> 00:03:16,530
limitations of context security and

48
00:03:14,010 --> 00:03:19,790
we'll give you a brief overview what's

49
00:03:16,530 --> 00:03:23,129
being done in defensive security and

50
00:03:19,790 --> 00:03:26,459
with those limitations and context we'll

51
00:03:23,129 --> 00:03:29,298
move on to what they what's less spoken

52
00:03:26,459 --> 00:03:33,060
about which is offensive as security

53
00:03:29,299 --> 00:03:42,180
we'll talk about all difference black

54
00:03:33,060 --> 00:03:44,760
box white box attacks a I be safe from a

55
00:03:42,180 --> 00:03:52,530
computer safety viewpoint well then draw

56
00:03:44,760 --> 00:03:55,858
some conclusions so the context as you

57
00:03:52,530 --> 00:04:03,030
probably know artificial intelligence is

58
00:03:55,859 --> 00:04:04,500
a machine learning it is part of

59
00:04:03,030 --> 00:04:07,769
artificial intelligence and what it

60
00:04:04,500 --> 00:04:11,370
allows you to do is via software allow

61
00:04:07,769 --> 00:04:13,769
machines to learn independently in to a

62
00:04:11,370 --> 00:04:16,560
certain extent we're trying to imitate

63
00:04:13,769 --> 00:04:20,820
the reasoning of human behavior but

64
00:04:16,560 --> 00:04:22,800
automatically and in fact this kind of

65
00:04:20,820 --> 00:04:30,630
technology is so important and I don't

66
00:04:22,800 --> 00:04:33,810
know if you see companies in many

67
00:04:30,630 --> 00:04:36,590
sectors are trying to use this

68
00:04:33,810 --> 00:04:36,590
technology to

69
00:04:46,780 --> 00:04:54,859
which is how to play AI to cybersecurity

70
00:04:50,930 --> 00:04:57,260
and I say such ly anecdotic all because

71
00:04:54,860 --> 00:05:00,830
the control we have it is very limited

72
00:04:57,260 --> 00:05:03,590
because there are many things that mean

73
00:05:00,830 --> 00:05:05,090
that in our field it's very difficult to

74
00:05:03,590 --> 00:05:08,229
apply this so if you look at this table

75
00:05:05,090 --> 00:05:13,940
there's a lot of information about tools

76
00:05:08,230 --> 00:05:15,650
environments etc if you know and you've

77
00:05:13,940 --> 00:05:17,690
worked with these technologies this is a

78
00:05:15,650 --> 00:05:21,679
very good summary of how they work what

79
00:05:17,690 --> 00:05:24,920
is AI and why is everybody so excited

80
00:05:21,680 --> 00:05:27,530
about this technology we think this

81
00:05:24,920 --> 00:05:30,640
slide summarizes it quite clearly

82
00:05:27,530 --> 00:05:34,390
there's a whole series of apps that

83
00:05:30,640 --> 00:05:39,349
actually profit from the possibility of

84
00:05:34,390 --> 00:05:41,180
automating certain human behavior with

85
00:05:39,350 --> 00:05:43,100
certain different techniques that are

86
00:05:41,180 --> 00:05:45,380
already known there are many scenarios

87
00:05:43,100 --> 00:05:48,290
where this is successful so we've got

88
00:05:45,380 --> 00:05:53,000
the classics fraud detection image

89
00:05:48,290 --> 00:06:31,550
classification recommendation systems a

90
00:05:53,000 --> 00:06:43,700
whole series in the prying to reach

91
00:06:31,550 --> 00:06:45,529
common style they show many of the

92
00:06:43,700 --> 00:06:52,759
mistakes of a

93
00:06:45,529 --> 00:06:54,799
recent decades cybersecurity how things

94
00:06:52,759 --> 00:07:08,569
could go badly wrong if we don't do

95
00:06:54,799 --> 00:07:12,919
things properly typical written about

96
00:07:08,569 --> 00:07:17,949
machine learning they try to bring

97
00:07:12,919 --> 00:07:21,349
together information about this subject

98
00:07:17,949 --> 00:07:27,649
and another problem that many people are

99
00:07:21,349 --> 00:07:31,009
concerned about is its impact of impact

100
00:07:27,649 --> 00:07:34,669
of AI on the economy and society and in

101
00:07:31,009 --> 00:07:39,699
fact if if you've read some people are

102
00:07:34,669 --> 00:07:42,079
talking about charging robots taxes and

103
00:07:39,699 --> 00:07:45,589
where the robots are going to take our

104
00:07:42,079 --> 00:07:47,629
work away from us but what we're worried

105
00:07:45,589 --> 00:07:50,409
about is not just the economic side of

106
00:07:47,629 --> 00:07:55,699
things but the societal side of things

107
00:07:50,409 --> 00:07:58,539
whether these areas may bring out the

108
00:07:55,699 --> 00:08:02,360
worst in human beings they may end up

109
00:07:58,539 --> 00:08:08,719
becoming us making us more racist

110
00:08:02,360 --> 00:08:11,289
because it's been shown that terrible

111
00:08:08,719 --> 00:08:14,629
things may happen for example BOTS that

112
00:08:11,289 --> 00:08:16,699
identify people by race and that

113
00:08:14,629 --> 00:08:17,989
encourage them to be racist so what

114
00:08:16,699 --> 00:08:19,759
we're saying is that we need to be very

115
00:08:17,989 --> 00:08:22,578
careful with the technology and at the

116
00:08:19,759 --> 00:08:25,509
end of this of the day what it's

117
00:08:22,579 --> 00:08:31,189
summarizing is the following

118
00:08:25,509 --> 00:08:37,009
because a eye is what's called a

119
00:08:31,189 --> 00:08:40,429
singularity so that's the time when this

120
00:08:37,009 --> 00:08:43,129
kind of AI can solve a problem as

121
00:08:40,429 --> 00:08:45,410
efficiently as a human being when is

122
00:08:43,129 --> 00:08:48,910
this going to happen well it depends on

123
00:08:45,410 --> 00:08:51,800
many things the problem that we have is

124
00:08:48,910 --> 00:08:53,929
as human beings that we're very bad at

125
00:08:51,800 --> 00:08:56,809
predicting the future we virtually

126
00:08:53,929 --> 00:08:57,439
always get things wrong in fact nobody

127
00:08:56,809 --> 00:08:59,360
knows when this

128
00:08:57,440 --> 00:09:04,010
is really going to happen the only thing

129
00:08:59,360 --> 00:09:05,530
that we can refer to is some service

130
00:09:04,010 --> 00:09:08,300
carried out by the future of humanity

131
00:09:05,530 --> 00:09:11,300
Institute and here what they've said is

132
00:09:08,300 --> 00:09:16,040
that between now and 40 years hence many

133
00:09:11,300 --> 00:09:21,380
activities that humans do today will be

134
00:09:16,040 --> 00:09:24,560
done by AI at least equally as well as

135
00:09:21,380 --> 00:09:29,180
humans and they've given for example a

136
00:09:24,560 --> 00:09:31,939
20 year time span for more or less at

137
00:09:29,180 --> 00:09:33,530
the time when the majority of tasks that

138
00:09:31,940 --> 00:09:37,040
humans carried out will be able to be

139
00:09:33,530 --> 00:09:38,689
carried out by a AI my robots we're

140
00:09:37,040 --> 00:09:42,530
talking about surgery talk about all

141
00:09:38,690 --> 00:09:44,840
sorts of things so this is the uncertain

142
00:09:42,530 --> 00:09:46,880
scenario that we find ourselves in and

143
00:09:44,840 --> 00:09:52,100
what makes people so concerned about

144
00:09:46,880 --> 00:09:55,280
these kinds of technologies we we don't

145
00:09:52,100 --> 00:09:59,060
know is when these kind of algorithms

146
00:09:55,280 --> 00:10:01,640
are going to be able to replace the

147
00:09:59,060 --> 00:10:08,689
ability of a senior analyst for example

148
00:10:01,640 --> 00:10:10,880
of testing reversing etc so our specific

149
00:10:08,690 --> 00:10:14,480
opinion on this is that if you've got a

150
00:10:10,880 --> 00:10:16,700
good security and analysts normally AI

151
00:10:14,480 --> 00:10:20,060
isn't going to replace that person but

152
00:10:16,700 --> 00:10:22,880
what AI has is it can be replicated for

153
00:10:20,060 --> 00:10:24,739
example I may have a very good person

154
00:10:22,880 --> 00:10:26,540
that's very good at doing reversing but

155
00:10:24,740 --> 00:10:27,980
I don't have five people or ten people

156
00:10:26,540 --> 00:10:29,240
or 100 people that could twenty

157
00:10:27,980 --> 00:10:31,430
four-hours-a-day

158
00:10:29,240 --> 00:10:35,020
repeat that so what's good about a is

159
00:10:31,430 --> 00:10:37,670
its scalability it may not be as good as

160
00:10:35,020 --> 00:10:38,810
human beings but as long as they're

161
00:10:37,670 --> 00:10:40,670
reasonably good too they're gonna be

162
00:10:38,810 --> 00:10:43,430
able to replace many tasks that we do as

163
00:10:40,670 --> 00:10:49,490
I said earlier the main problem is that

164
00:10:43,430 --> 00:10:54,069
people don't really know where this road

165
00:10:49,490 --> 00:10:56,690
is heading in fact the onus of major

166
00:10:54,070 --> 00:10:58,970
organization such as Tesla and Facebook

167
00:10:56,690 --> 00:11:00,800
are continually debating whether this is

168
00:10:58,970 --> 00:11:03,410
gonna have an impact or not and what its

169
00:11:00,800 --> 00:11:06,650
reach will be and there's a great deal

170
00:11:03,410 --> 00:11:08,199
of uncertainty once again let's now we

171
00:11:06,650 --> 00:11:13,029
quickly look

172
00:11:08,200 --> 00:11:18,700
with a couple of overheads of justice or

173
00:11:13,029 --> 00:11:22,360
fairness are you to what extent AI is so

174
00:11:18,700 --> 00:11:24,010
powerful and whether it will affect

175
00:11:22,360 --> 00:11:26,560
security here

176
00:11:24,010 --> 00:11:29,019
we've given you a series of examples

177
00:11:26,560 --> 00:11:32,109
especially from video games which is the

178
00:11:29,019 --> 00:11:35,529
typical one that people always struck by

179
00:11:32,110 --> 00:11:38,829
to see how this progress has developed

180
00:11:35,529 --> 00:11:43,449
and here you can see how in I think the

181
00:11:38,829 --> 00:11:47,170
most recent 20 years AI has made fairly

182
00:11:43,449 --> 00:11:48,939
slow progress it's dealt with certain

183
00:11:47,170 --> 00:11:54,160
specific problems some of them it's

184
00:11:48,940 --> 00:12:03,250
resolved very well but it can't be used

185
00:11:54,160 --> 00:12:06,069
as generally as human intelligence so if

186
00:12:03,250 --> 00:12:07,540
you look at artificial intelligence you

187
00:12:06,070 --> 00:12:09,760
can see actually really dates from the

188
00:12:07,540 --> 00:12:11,829
1950s but it's over the last 20 years

189
00:12:09,760 --> 00:12:14,410
that things have been done but not

190
00:12:11,829 --> 00:12:18,069
perhaps as quickly as many people might

191
00:12:14,410 --> 00:12:20,589
think in fact this is a typical example

192
00:12:18,070 --> 00:12:22,630
that people are always struck by these

193
00:12:20,589 --> 00:12:26,290
are different simulations that's really

194
00:12:22,630 --> 00:12:30,810
made of AI playing video games and you

195
00:12:26,290 --> 00:12:34,000
can prove that for many video games ai

196
00:12:30,810 --> 00:12:37,589
plays better than a human being so you

197
00:12:34,000 --> 00:12:40,540
can have algorithms made that actually

198
00:12:37,589 --> 00:12:42,190
play better than a human so for many

199
00:12:40,540 --> 00:12:45,730
scenario Keith's are persecuted maybe

200
00:12:42,190 --> 00:12:47,410
for security these are interesting we're

201
00:12:45,730 --> 00:12:49,089
not worried about these games because

202
00:12:47,410 --> 00:12:51,880
it's just an anecdote but what people

203
00:12:49,089 --> 00:12:57,010
begin to get worried about is the

204
00:12:51,880 --> 00:12:59,980
ability to play games and develop AI

205
00:12:57,010 --> 00:13:03,640
algorithms that actually reflect human

206
00:12:59,980 --> 00:13:11,500
intelligence and an example of this or

207
00:13:03,640 --> 00:13:15,040
or a disruptive point was with the

208
00:13:11,500 --> 00:13:19,260
different bets that are made to beat for

209
00:13:15,040 --> 00:13:19,260
example alpha go zero

210
00:13:21,200 --> 00:13:30,740
so today things are evolving of former

211
00:13:28,080 --> 00:13:32,910
rapidly we can do very good specific

212
00:13:30,740 --> 00:13:34,830
algorithms we can develop them and we

213
00:13:32,910 --> 00:13:38,790
get the results that we all know about

214
00:13:34,830 --> 00:13:40,860
with them I like this study carried out

215
00:13:38,790 --> 00:13:44,219
by Google especially I don't know if

216
00:13:40,860 --> 00:13:48,750
you've seen it what Google did was an

217
00:13:44,220 --> 00:13:51,660
experiment with urine neuronal networks

218
00:13:48,750 --> 00:13:54,470
in blue and red you've got two neurons

219
00:13:51,660 --> 00:13:58,290
neuronal networks that are competing

220
00:13:54,470 --> 00:14:00,899
they represent apples what happens is

221
00:13:58,290 --> 00:14:02,610
the neuron neuronal networks as there

222
00:14:00,899 --> 00:14:07,200
are lots of apples they collaborate that

223
00:14:02,610 --> 00:14:08,970
is they they decide to collaborate if

224
00:14:07,200 --> 00:14:12,720
there are lots of apples but when there

225
00:14:08,970 --> 00:14:15,890
are few apples they decide they have to

226
00:14:12,720 --> 00:14:20,399
compete so they attack each other these

227
00:14:15,890 --> 00:14:22,949
red dots are represent but there's

228
00:14:20,399 --> 00:14:25,230
another scenario that is if you're faced

229
00:14:22,950 --> 00:14:28,110
with different neuronal networks one

230
00:14:25,230 --> 00:14:30,329
hears greater computation or ability

231
00:14:28,110 --> 00:14:32,910
other so therefore it can train rapidly

232
00:14:30,329 --> 00:14:34,109
than the other one it doesn't matter how

233
00:14:32,910 --> 00:14:38,040
many apples there are they'll always

234
00:14:34,110 --> 00:14:42,420
attack then never collaborate and that

235
00:14:38,040 --> 00:14:44,010
is deduced by the network and that's

236
00:14:42,420 --> 00:14:47,040
what many people are concerned about

237
00:14:44,010 --> 00:14:51,569
what extent a I can be controlled and

238
00:14:47,040 --> 00:14:53,730
from a security viewpoint it's okay i if

239
00:14:51,570 --> 00:14:56,130
I use this kind of method for defensive

240
00:14:53,730 --> 00:14:59,850
security is it going to turn on me

241
00:14:56,130 --> 00:15:04,250
that's what we want to know and here I'm

242
00:14:59,850 --> 00:15:08,779
gonna give you a couple of references to

243
00:15:04,250 --> 00:15:12,570
articles talking about these subjects I

244
00:15:08,779 --> 00:15:15,779
actually recommend you watch this a

245
00:15:12,570 --> 00:15:18,870
video a video published by C seek

246
00:15:15,779 --> 00:15:22,230
there's also an article about how Google

247
00:15:18,870 --> 00:15:25,230
has got things wrong and ask about I

248
00:15:22,230 --> 00:15:28,230
mentioned earlier AI is useful at the

249
00:15:25,230 --> 00:15:30,760
end of the day but we need to remember

250
00:15:28,230 --> 00:15:35,010
in what context in what domain

251
00:15:30,760 --> 00:15:38,890
it should be applied or not and

252
00:15:35,010 --> 00:15:40,480
finishing this section about context and

253
00:15:38,890 --> 00:15:43,240
I'll move on to the more technical side

254
00:15:40,480 --> 00:15:47,230
of things there are some aspects which

255
00:15:43,240 --> 00:15:49,570
some people think about forget which for

256
00:15:47,230 --> 00:15:50,830
us in the security world is very

257
00:15:49,570 --> 00:15:54,150
important which is everything related to

258
00:15:50,830 --> 00:15:57,640
legislation there are scenarios where

259
00:15:54,150 --> 00:15:59,709
even if we had AI algorithms that were

260
00:15:57,640 --> 00:16:02,170
better than human behavior they cannot

261
00:15:59,710 --> 00:16:04,420
be used because legislation is going to

262
00:16:02,170 --> 00:16:08,560
prevent us from doing from let's imagine

263
00:16:04,420 --> 00:16:13,120
a bank that has to give a loan the data

264
00:16:08,560 --> 00:16:16,949
protection law may force that those

265
00:16:13,120 --> 00:16:21,010
decisions have that impact a person's

266
00:16:16,950 --> 00:16:22,660
normal daily life have to be reviewed

267
00:16:21,010 --> 00:16:26,770
especially if they impact for example a

268
00:16:22,660 --> 00:16:29,650
URL and that's because you can have a

269
00:16:26,770 --> 00:16:32,400
black box made you can have inputs but

270
00:16:29,650 --> 00:16:34,900
not outputs and you don't know why

271
00:16:32,400 --> 00:16:36,370
inputs don't lead to outputs because

272
00:16:34,900 --> 00:16:38,770
there's no reasoning so there many

273
00:16:36,370 --> 00:16:41,320
scenarios in which this technology

274
00:16:38,770 --> 00:16:43,990
cannot be used if we want the best

275
00:16:41,320 --> 00:16:45,250
security system it may well be that we

276
00:16:43,990 --> 00:16:49,210
can actually use it because of legal

277
00:16:45,250 --> 00:16:51,010
problems having given you context let's

278
00:16:49,210 --> 00:16:56,160
now move on to the technical side of

279
00:16:51,010 --> 00:17:00,240
things let's talk about machine learning

280
00:16:56,160 --> 00:17:03,280
machine learning are basically data and

281
00:17:00,240 --> 00:17:06,520
algorithms you have seen data and

282
00:17:03,280 --> 00:17:09,609
algorithm is applied to a data and we

283
00:17:06,520 --> 00:17:13,510
obtain a model a model that allows us to

284
00:17:09,609 --> 00:17:16,060
carry out different tasks the data can

285
00:17:13,510 --> 00:17:18,730
be in any kind here they represent as

286
00:17:16,060 --> 00:17:24,190
binary data but they can be text images

287
00:17:18,730 --> 00:17:28,449
audio recordings whatever we want takes

288
00:17:24,190 --> 00:17:31,360
us onto algorithms we can classify the

289
00:17:28,449 --> 00:17:33,400
algorithms depending upon the type of

290
00:17:31,360 --> 00:17:35,199
learning and the task that they saw

291
00:17:33,400 --> 00:17:37,960
let's look at learning first there are

292
00:17:35,200 --> 00:17:40,220
different times there's supervised non

293
00:17:37,960 --> 00:17:42,650
supervised and back up learning

294
00:17:40,220 --> 00:17:48,140
the supervisor and what it tries to do

295
00:17:42,650 --> 00:17:51,760
is to learn a task through tagged data

296
00:17:48,140 --> 00:17:54,560
if we want to identify for example

297
00:17:51,760 --> 00:18:00,110
pictures of dogs and cats in each

298
00:17:54,560 --> 00:18:04,040
picture we add a tag a tag to say

299
00:18:00,110 --> 00:18:06,439
whether it's a dog or a cat so what this

300
00:18:04,040 --> 00:18:10,730
function does is it learns from these

301
00:18:06,440 --> 00:18:14,450
tax unlike non supervised learning where

302
00:18:10,730 --> 00:18:20,840
those tags don't exist the aim of this

303
00:18:14,450 --> 00:18:22,580
kind of learning is to obtain the

304
00:18:20,840 --> 00:18:26,929
internal structure of the data and

305
00:18:22,580 --> 00:18:32,179
finally back up learning which is

306
00:18:26,930 --> 00:18:35,270
different to the other two and in this

307
00:18:32,180 --> 00:18:39,530
case we've got a robot in a certain

308
00:18:35,270 --> 00:18:41,780
environment and this agent of robots

309
00:18:39,530 --> 00:18:44,720
knows the states and carries out

310
00:18:41,780 --> 00:18:46,940
different actions for every action that

311
00:18:44,720 --> 00:18:51,500
it carries out in a state it receives a

312
00:18:46,940 --> 00:19:00,080
reward and the idea of this agent is to

313
00:18:51,500 --> 00:19:02,390
maximize the reward we can also classify

314
00:19:00,080 --> 00:19:05,990
things according to the task that's

315
00:19:02,390 --> 00:19:13,370
solved machine learning can be applied

316
00:19:05,990 --> 00:19:16,100
to different kind of applications but it

317
00:19:13,370 --> 00:19:18,409
can be grouped into four different types

318
00:19:16,100 --> 00:19:21,139
which are called tasks classification

319
00:19:18,410 --> 00:19:25,510
regression clustering and dimensionality

320
00:19:21,140 --> 00:19:31,220
reduction classification and regression

321
00:19:25,510 --> 00:19:35,180
our part of supervised learning and the

322
00:19:31,220 --> 00:19:38,680
other 2 are non supervised learning so

323
00:19:35,180 --> 00:19:43,250
what happens here you have a series of

324
00:19:38,680 --> 00:19:45,389
predefined objects or classes like the

325
00:19:43,250 --> 00:19:49,320
cats and dogs

326
00:19:45,389 --> 00:19:51,779
as we said previously and the idea is to

327
00:19:49,320 --> 00:19:56,899
differentiate between these two classes

328
00:19:51,779 --> 00:19:59,369
between the cats and the dogs regression

329
00:19:56,899 --> 00:20:02,339
in the case this case would have a

330
00:19:59,369 --> 00:20:09,149
finite group of classes but rather an

331
00:20:02,339 --> 00:20:12,379
infinite interval so we try to approach

332
00:20:09,149 --> 00:20:16,109
a function to these data cholesterin

333
00:20:12,379 --> 00:20:18,629
involves grouping together objects

334
00:20:16,109 --> 00:20:21,389
depending upon their properties to give

335
00:20:18,629 --> 00:20:23,039
a typical example we've got market

336
00:20:21,389 --> 00:20:24,988
segmentation for example if we've got a

337
00:20:23,039 --> 00:20:29,940
million customers now we want to offer

338
00:20:24,989 --> 00:20:32,179
these customers a tailor-made tariff for

339
00:20:29,940 --> 00:20:35,729
each one that's impossible

340
00:20:32,179 --> 00:20:40,139
so we segment the market into four or

341
00:20:35,729 --> 00:20:42,029
five user groups and each user group is

342
00:20:40,139 --> 00:20:45,678
offered a certain tariff a certain rate

343
00:20:42,029 --> 00:20:52,950
and then dimensionality reduction

344
00:20:45,679 --> 00:20:55,139
normally we have data with some

345
00:20:52,950 --> 00:20:57,899
variables but there's lots of data some

346
00:20:55,139 --> 00:21:00,988
may be redundant and in the case of

347
00:20:57,899 --> 00:21:06,268
machine learning training this can take

348
00:21:00,989 --> 00:21:09,029
its time because if you reduce all these

349
00:21:06,269 --> 00:21:13,499
variables you can lose information but

350
00:21:09,029 --> 00:21:19,979
this is a way of making training more

351
00:21:13,499 --> 00:21:25,440
efficient each type has several

352
00:21:19,979 --> 00:21:29,690
algorithms so what we've done is search

353
00:21:25,440 --> 00:21:32,039
in kaggle for a page with different

354
00:21:29,690 --> 00:21:34,649
machine learning competitions are

355
00:21:32,039 --> 00:21:41,700
carried out and here they've carried out

356
00:21:34,649 --> 00:21:43,559
a survey where many questions are asked

357
00:21:41,700 --> 00:21:46,280
here they're just two the first question

358
00:21:43,559 --> 00:21:49,170
is what

359
00:21:46,280 --> 00:21:51,149
data science methods are used at work

360
00:21:49,170 --> 00:21:56,190
for those that use Kegel and what tools

361
00:21:51,150 --> 00:21:58,680
are used at work and if we look the ones

362
00:21:56,190 --> 00:22:02,340
that are most use are up there on the

363
00:21:58,680 --> 00:22:05,160
screen you've got logistic regression

364
00:22:02,340 --> 00:22:12,230
and decision trees random forest neural

365
00:22:05,160 --> 00:22:16,950
networks and as for programming

366
00:22:12,230 --> 00:22:19,230
languages the one that's mostly used is

367
00:22:16,950 --> 00:22:22,380
Python followed by R which is used by

368
00:22:19,230 --> 00:22:27,630
mathematicians and statisticians then

369
00:22:22,380 --> 00:22:34,150
we've got others such as Jupiter

370
00:22:27,630 --> 00:22:37,010
notebooks or elearning frames

371
00:22:34,150 --> 00:22:40,590
[Music]

372
00:22:37,010 --> 00:22:42,900
the major the problem of machine

373
00:22:40,590 --> 00:22:45,330
learning isn't so much algorithms but

374
00:22:42,900 --> 00:22:50,580
where they get their data from either

375
00:22:45,330 --> 00:22:52,379
because we need to apply or we want to

376
00:22:50,580 --> 00:22:53,340
make an application for something but we

377
00:22:52,380 --> 00:22:55,740
don't have the data because the data

378
00:22:53,340 --> 00:23:00,830
aren't publish because a very bad dirty

379
00:22:55,740 --> 00:23:04,230
data so there are a series of data

380
00:23:00,830 --> 00:23:11,780
repositories where you can see the kinds

381
00:23:04,230 --> 00:23:11,780
of data each data type has different

382
00:23:11,960 --> 00:23:18,270
categories maybe the the most

383
00:23:16,020 --> 00:23:21,080
interesting one is is the cattle data

384
00:23:18,270 --> 00:23:23,639
sets because they do all kinds of

385
00:23:21,080 --> 00:23:28,350
competitions and all kinds of themes

386
00:23:23,640 --> 00:23:31,580
also the data from the web service from

387
00:23:28,350 --> 00:23:36,000
the US and from Amazon is also

388
00:23:31,580 --> 00:23:40,169
interesting and also sick repo has also

389
00:23:36,000 --> 00:23:42,690
have several security data sets one of

390
00:23:40,170 --> 00:23:45,800
the data kinds most used in security or

391
00:23:42,690 --> 00:23:50,790
machine learning in general are pictures

392
00:23:45,800 --> 00:23:52,889
images and here you can see a series of

393
00:23:50,790 --> 00:23:56,909
repositories on the scheme screen and

394
00:23:52,890 --> 00:23:58,380
normally in these data sets what's

395
00:23:56,910 --> 00:24:01,350
happened is you get a comparison between

396
00:23:58,380 --> 00:24:03,480
different algorithms there's a benchmark

397
00:24:01,350 --> 00:24:06,629
to see whether the algorithms are

398
00:24:03,480 --> 00:24:08,670
working or any good one of the most

399
00:24:06,630 --> 00:24:11,910
important monomers typical which is all

400
00:24:08,670 --> 00:24:14,220
very small which is M NIST which are

401
00:24:11,910 --> 00:24:17,310
handwritten digits you can see it on the

402
00:24:14,220 --> 00:24:19,560
screen here what we try to do is to

403
00:24:17,310 --> 00:24:23,250
classify each of these digits from

404
00:24:19,560 --> 00:24:25,679
naught to 9 for example also see for 10

405
00:24:23,250 --> 00:24:26,910
and see 400 or we don't have any digits

406
00:24:25,680 --> 00:24:30,300
but in this case we've got pictures of

407
00:24:26,910 --> 00:24:33,030
objects and depending on whether it's a

408
00:24:30,300 --> 00:24:35,310
C for 10 or C 400 you've got either 10

409
00:24:33,030 --> 00:24:39,420
types of objects or 100 types of objects

410
00:24:35,310 --> 00:24:43,950
another set is celibate with 200,000

411
00:24:39,420 --> 00:24:46,590
images of famous faces with over 40

412
00:24:43,950 --> 00:24:47,680
attributes and other databases such as

413
00:24:46,590 --> 00:24:51,459
image net

414
00:24:47,680 --> 00:24:56,550
Cocco dataset okay

415
00:24:51,460 --> 00:25:00,250
now we've seen that machine learning is

416
00:24:56,550 --> 00:25:02,500
datasets and algorithms but then how do

417
00:25:00,250 --> 00:25:07,180
we make something out of that can we

418
00:25:02,500 --> 00:25:10,270
apply the algorithm you can have data

419
00:25:07,180 --> 00:25:15,400
and algorithms okay on the screen you

420
00:25:10,270 --> 00:25:20,770
can see that so during the data stage

421
00:25:15,400 --> 00:25:22,330
you have to obtain the data from date

422
00:25:20,770 --> 00:25:26,940
that we generate ourselves through

423
00:25:22,330 --> 00:25:33,939
public repositories of other sources

424
00:25:26,940 --> 00:25:37,960
normally when a database has a set of

425
00:25:33,940 --> 00:25:40,690
datas you have to clean them because

426
00:25:37,960 --> 00:25:44,320
some data are dirty you have to get rid

427
00:25:40,690 --> 00:25:49,260
of data that are atypical some data are

428
00:25:44,320 --> 00:25:49,260
missing once you've cleaned the data

429
00:25:51,150 --> 00:25:57,010
what we have to do is to reduce their

430
00:25:53,860 --> 00:26:00,010
size through the algorithms through

431
00:25:57,010 --> 00:26:03,970
statistical applications or through

432
00:26:00,010 --> 00:26:05,620
expert intervention so that's the data

433
00:26:03,970 --> 00:26:10,380
once you've got the data and then you

434
00:26:05,620 --> 00:26:12,610
move on to algorithms and depending upon

435
00:26:10,380 --> 00:26:14,920
the applications you want to create

436
00:26:12,610 --> 00:26:17,290
whether we want to solve a

437
00:26:14,920 --> 00:26:21,430
classification or regression problem

438
00:26:17,290 --> 00:26:25,450
what we just recreate an algorithm and

439
00:26:21,430 --> 00:26:27,280
normally these algorithms have several

440
00:26:25,450 --> 00:26:31,200
one or several parameters that need to

441
00:26:27,280 --> 00:26:33,340
be set initially how do you do that

442
00:26:31,200 --> 00:26:37,810
either through trial and error or

443
00:26:33,340 --> 00:26:42,970
through initial studying and here we're

444
00:26:37,810 --> 00:26:45,669
ready to train our algorithm then we can

445
00:26:42,970 --> 00:26:51,070
divide our datasets into two or three

446
00:26:45,670 --> 00:26:53,520
parts the majority of data I used it for

447
00:26:51,070 --> 00:26:53,520
training

448
00:26:53,660 --> 00:27:01,940
and the remainders used for test and

449
00:26:58,400 --> 00:27:04,900
validation although of course validation

450
00:27:01,940 --> 00:27:04,900
depends on what we're going to be doing

451
00:27:06,100 --> 00:27:10,909
and those parts that are used to train

452
00:27:09,830 --> 00:27:13,850
the model are called the training

453
00:27:10,910 --> 00:27:19,010
reports and for the other two parts of

454
00:27:13,850 --> 00:27:22,159
our data set that's what our trained

455
00:27:19,010 --> 00:27:25,340
model is based on and we get a series of

456
00:27:22,160 --> 00:27:28,130
metrics that we've pre-established early

457
00:27:25,340 --> 00:27:33,139
and depending upon the application may

458
00:27:28,130 --> 00:27:35,240
be error positive data phases depending

459
00:27:33,140 --> 00:27:37,960
on the metrics that we achieve what if

460
00:27:35,240 --> 00:27:43,220
they're correct if we've got for example

461
00:27:37,960 --> 00:27:46,190
you know if we decide that our model

462
00:27:43,220 --> 00:27:50,360
will be valid if we get 90% precision

463
00:27:46,190 --> 00:27:53,180
and if from the metrics so that we get

464
00:27:50,360 --> 00:27:54,350
from where we've got 95% precision then

465
00:27:53,180 --> 00:27:58,270
we'll deploy the model if they're not

466
00:27:54,350 --> 00:28:01,550
valid then you have to either choose

467
00:27:58,270 --> 00:28:04,820
algorithm or change the parameters just

468
00:28:01,550 --> 00:28:07,820
something to add to what Jorge said and

469
00:28:04,820 --> 00:28:09,770
somebody may be wondering what do I want

470
00:28:07,820 --> 00:28:15,740
to know this for I'm not interested in

471
00:28:09,770 --> 00:28:17,840
hacking into an AE system it's like you

472
00:28:15,740 --> 00:28:19,850
have to study all sorts of things

473
00:28:17,840 --> 00:28:21,199
protocols and architecture and it's

474
00:28:19,850 --> 00:28:22,189
seriously you can understand what's

475
00:28:21,200 --> 00:28:24,560
going to happen after you need to

476
00:28:22,190 --> 00:28:25,670
understand this first because this cycle

477
00:28:24,560 --> 00:28:29,300
is very important because if we

478
00:28:25,670 --> 00:28:31,490
understand the cycle we can see where

479
00:28:29,300 --> 00:28:34,790
the access points are for attacks so

480
00:28:31,490 --> 00:28:36,560
it's important to have this mix of

481
00:28:34,790 --> 00:28:38,060
people in your tape team of those that

482
00:28:36,560 --> 00:28:46,870
are AI experts and those are the

483
00:28:38,060 --> 00:28:46,870
security experts so as I said earlier

484
00:28:46,970 --> 00:28:51,450
majority problem at data with algorithms

485
00:28:49,740 --> 00:29:00,779
ransom any problem because the majority

486
00:28:51,450 --> 00:29:02,279
of algorithms in several libraries the

487
00:29:00,779 --> 00:29:03,690
majority you can see some of them of the

488
00:29:02,279 --> 00:29:06,360
majority are in Python because that's

489
00:29:03,690 --> 00:29:09,090
the language that are used pythons are

490
00:29:06,360 --> 00:29:12,120
used to program it's the majority are in

491
00:29:09,090 --> 00:29:21,449
Python but one of the ones that we like

492
00:29:12,120 --> 00:29:25,289
a lot is ticket learn and hto AI if

493
00:29:21,450 --> 00:29:27,720
you're more big data or it is you've got

494
00:29:25,289 --> 00:29:29,610
this spark em lib and then you've got

495
00:29:27,720 --> 00:29:32,250
other libraries such as WIC I'm at my

496
00:29:29,610 --> 00:29:35,639
house well so here they are up on the

497
00:29:32,250 --> 00:29:43,230
screen for you you've got deep learning

498
00:29:35,639 --> 00:29:45,479
frameworks as well wear classic machine

499
00:29:43,230 --> 00:29:46,919
learn algorithms can be applied although

500
00:29:45,480 --> 00:29:49,080
they're not specifically designed for

501
00:29:46,919 --> 00:29:57,450
that there's not what the one that we

502
00:29:49,080 --> 00:30:04,320
like a lot which is chaos and chaos as

503
00:29:57,450 --> 00:30:06,269
some celeritas of deep learning together

504
00:30:04,320 --> 00:30:09,090
with some frameworks such as tensor fern

505
00:30:06,269 --> 00:30:12,179
theano so for the things that we're

506
00:30:09,090 --> 00:30:14,158
going to be seeing afterwards for deep

507
00:30:12,179 --> 00:30:17,629
learning this is a very interesting free

508
00:30:14,159 --> 00:30:17,629
mug framework got a very simple one

509
00:30:18,049 --> 00:30:23,850
apart from a thing that we've said we

510
00:30:20,970 --> 00:30:31,889
can run this locally as you see later

511
00:30:23,850 --> 00:30:34,889
these algorithms are difficult so we've

512
00:30:31,889 --> 00:30:39,959
got specific services in the specific

513
00:30:34,889 --> 00:30:42,870
cloud services where we can train models

514
00:30:39,960 --> 00:30:46,889
the transparent cloud for us or we can

515
00:30:42,870 --> 00:30:51,840
type in the code upload it into the

516
00:30:46,889 --> 00:30:54,260
cloud with our data and run it all on

517
00:30:51,840 --> 00:30:56,510
the cloud

518
00:30:54,260 --> 00:31:02,060
that happens with cloud platform Amazon

519
00:30:56,510 --> 00:31:05,030
Web service and other specific service

520
00:31:02,060 --> 00:31:07,520
such as flight hub and big ml we'll see

521
00:31:05,030 --> 00:31:12,200
something later with fluid hub which

522
00:31:07,520 --> 00:31:15,430
allows you a free account to trial

523
00:31:12,200 --> 00:31:15,430
things but we'll see that afterwards

524
00:31:19,059 --> 00:31:24,289
then we'll see a demo so this is the

525
00:31:22,429 --> 00:31:29,409
part of the deep learning which is a

526
00:31:24,289 --> 00:31:34,249
subsets of machine learning and then

527
00:31:29,409 --> 00:31:36,700
we'll talk about a gang neural network

528
00:31:34,249 --> 00:31:42,230
that is basically used in security

529
00:31:36,700 --> 00:31:45,710
what is the deep learning it's new

530
00:31:42,230 --> 00:31:51,399
neural networks basically let's see it

531
00:31:45,710 --> 00:31:55,070
demo to understand what this is in this

532
00:31:51,399 --> 00:32:00,529
demo you see the basic concepts of a

533
00:31:55,070 --> 00:32:04,428
neural network in deep learning

534
00:32:00,529 --> 00:32:07,039
especially for offensive attacks you

535
00:32:04,429 --> 00:32:11,289
will see the second part are the tool to

536
00:32:07,039 --> 00:32:16,580
see how we can cheat classification

537
00:32:11,289 --> 00:32:20,360
systems a neural networks is a set of

538
00:32:16,580 --> 00:32:22,730
layers composed of neurons what is a

539
00:32:20,360 --> 00:32:25,850
neuron will define it very quickly it's

540
00:32:22,730 --> 00:32:28,580
a set of entries and in inputs and

541
00:32:25,850 --> 00:32:32,748
outputs and the outputs go to the next

542
00:32:28,580 --> 00:32:36,199
layer these entries and exits are

543
00:32:32,749 --> 00:32:41,840
weights and the goal of the new world

544
00:32:36,200 --> 00:32:45,080
networks is to adjust these waves so the

545
00:32:41,840 --> 00:32:47,840
loss functions are minimized in this

546
00:32:45,080 --> 00:32:54,759
will depend on the algorithm that we

547
00:32:47,840 --> 00:32:58,789
apply we have three types of layers the

548
00:32:54,759 --> 00:33:04,429
injuries basically the data that are in

549
00:32:58,789 --> 00:33:07,360
put into the algorithm yeah outputs and

550
00:33:04,429 --> 00:33:13,899
interestingly user of the hidden layers

551
00:33:07,360 --> 00:33:18,709
and this allows us to learn complex

552
00:33:13,899 --> 00:33:20,748
concepts that more layers we have the

553
00:33:18,710 --> 00:33:23,950
deepest the learning that's why it's

554
00:33:20,749 --> 00:33:23,950
called deep learning

555
00:33:26,450 --> 00:33:35,010
and the more complex concepts we can

556
00:33:31,679 --> 00:33:37,650
learn you can use the URL veces

557
00:33:35,010 --> 00:33:41,070
simulator for neural networks if you

558
00:33:37,650 --> 00:33:43,800
want to understand concepts much better

559
00:33:41,070 --> 00:33:46,110
you can use this simulator you can

560
00:33:43,800 --> 00:33:54,570
design neural networks and see the

561
00:33:46,110 --> 00:33:58,469
effects what the algorithm does is to

562
00:33:54,570 --> 00:34:01,260
minimize a loss function here it doesn't

563
00:33:58,470 --> 00:34:05,809
converge but it what should happen here

564
00:34:01,260 --> 00:34:05,809
is that where you can see here in blue

565
00:34:07,490 --> 00:34:13,199
composes a complex circle separating the

566
00:34:10,860 --> 00:34:15,060
orange and the blue classes here they

567
00:34:13,199 --> 00:34:18,239
should be separated let's change the

568
00:34:15,060 --> 00:34:21,600
architecture of this network and

569
00:34:18,239 --> 00:34:23,609
normally something that is quite

570
00:34:21,600 --> 00:34:25,918
difficult to adjust is the number of

571
00:34:23,610 --> 00:34:28,260
hidden layers because we don't know how

572
00:34:25,918 --> 00:34:32,339
many we need how many neurons we need on

573
00:34:28,260 --> 00:34:36,510
on each layer this is one of the biggest

574
00:34:32,340 --> 00:34:39,320
problems we do it with a test and a

575
00:34:36,510 --> 00:34:41,850
trial and error and we adjust the

576
00:34:39,320 --> 00:34:54,330
different number of neurons on each

577
00:34:41,850 --> 00:34:55,889
layer etc let's remove layers let's see

578
00:34:54,330 --> 00:35:00,270
if this works

579
00:34:55,889 --> 00:35:04,369
to train a neural network what we do is

580
00:35:00,270 --> 00:35:07,290
to apply a pox it's the training test

581
00:35:04,369 --> 00:35:11,550
process in whole that we apply we will

582
00:35:07,290 --> 00:35:13,109
get to 500 a pox and 500 times the

583
00:35:11,550 --> 00:35:17,520
training so there's a clear separation

584
00:35:13,109 --> 00:35:20,160
here from the two classes you see the

585
00:35:17,520 --> 00:35:24,810
colors the the orange and the blue class

586
00:35:20,160 --> 00:35:27,868
and the error is 0 so this is a good

587
00:35:24,810 --> 00:35:29,580
classifying system for defensive

588
00:35:27,869 --> 00:35:31,980
security and offensive security the

589
00:35:29,580 --> 00:35:34,529
problems is a pox this is done very

590
00:35:31,980 --> 00:35:36,240
quickly by the similar simulator but

591
00:35:34,530 --> 00:35:37,570
this is that our very high computational

592
00:35:36,240 --> 00:35:41,799
course

593
00:35:37,570 --> 00:35:44,200
so we can do it very many times and and

594
00:35:41,800 --> 00:35:46,900
test and train the model as an offender

595
00:35:44,200 --> 00:35:49,029
and as a defense defenders we can be a

596
00:35:46,900 --> 00:35:51,790
better offender or a better defender we

597
00:35:49,030 --> 00:35:57,640
can play with various parameters for

598
00:35:51,790 --> 00:36:01,210
instance the the training data ratio we

599
00:35:57,640 --> 00:36:04,359
can include noise so that it's not that

600
00:36:01,210 --> 00:36:06,760
easy to classify here you will see that

601
00:36:04,360 --> 00:36:11,460
this is not separated into two

602
00:36:06,760 --> 00:36:14,110
concentric circles here they are just

603
00:36:11,460 --> 00:36:16,510
near the blue ones it's difficult to

604
00:36:14,110 --> 00:36:20,220
classify it with this algorithm we can

605
00:36:16,510 --> 00:36:24,550
also play with the training rage how the

606
00:36:20,220 --> 00:36:27,720
algorithm learns the activation rate we

607
00:36:24,550 --> 00:36:35,590
can also play with the type of problems

608
00:36:27,720 --> 00:36:39,629
of classification and training and let's

609
00:36:35,590 --> 00:36:46,720
see the convergence here this is just

610
00:36:39,630 --> 00:36:50,350
basic new your neural network there's a

611
00:36:46,720 --> 00:36:53,140
training process going on but we haven't

612
00:36:50,350 --> 00:36:55,990
given you any technical details so this

613
00:36:53,140 --> 00:36:58,420
is not too boring otherwise we should

614
00:36:55,990 --> 00:37:00,189
enter into mathematical details that we

615
00:36:58,420 --> 00:37:02,200
don't want to give you here today with

616
00:37:00,190 --> 00:37:05,590
that simulator we've seen the same the

617
00:37:02,200 --> 00:37:09,069
simple model the multi-layer training

618
00:37:05,590 --> 00:37:10,420
but we have other types of neural

619
00:37:09,070 --> 00:37:13,870
networks called

620
00:37:10,420 --> 00:37:16,330
architectures and there are many of them

621
00:37:13,870 --> 00:37:19,240
one of the most popular ones are

622
00:37:16,330 --> 00:37:21,819
recurrent networks even though they're

623
00:37:19,240 --> 00:37:26,259
not used in that way we use variants

624
00:37:21,820 --> 00:37:31,630
like PS the HTML group the convolute

625
00:37:26,260 --> 00:37:34,720
conversational networks and there's Jam

626
00:37:31,630 --> 00:37:39,010
Network that we use in security the OT

627
00:37:34,720 --> 00:37:44,109
encoders here's the link where you can

628
00:37:39,010 --> 00:37:46,330
find the detail on the full architecture

629
00:37:44,110 --> 00:37:49,390
of these networks the description and

630
00:37:46,330 --> 00:37:49,920
what the purpose is let's see what are

631
00:37:49,390 --> 00:37:55,770
the

632
00:37:49,920 --> 00:37:57,840
ganz j-a ends this is an interesting

633
00:37:55,770 --> 00:38:00,720
concept for attack and defense it's not

634
00:37:57,840 --> 00:38:04,550
a neural networks that you generator and

635
00:38:00,720 --> 00:38:04,549
discriminator how does it work

636
00:38:05,720 --> 00:38:17,879
when the generator gets some noise and

637
00:38:11,040 --> 00:38:22,650
it generates false examples and the

638
00:38:17,880 --> 00:38:25,260
discriminator who gets the data

639
00:38:22,650 --> 00:38:27,180
generated by the generator plus real

640
00:38:25,260 --> 00:38:30,180
examples in such a way that a generator

641
00:38:27,180 --> 00:38:31,770
and discriminator compete the general

642
00:38:30,180 --> 00:38:33,299
rated tries to cheat on the

643
00:38:31,770 --> 00:38:36,750
discrimination and the discriminated

644
00:38:33,300 --> 00:38:40,470
tribes to defend from the generator the

645
00:38:36,750 --> 00:38:44,690
generator tries to generate more and

646
00:38:40,470 --> 00:38:47,459
better examples real examples real

647
00:38:44,690 --> 00:38:49,740
generated samples and the discriminator

648
00:38:47,460 --> 00:38:54,470
tries to distinguish between the real

649
00:38:49,740 --> 00:38:57,899
and the generated this game finishes

650
00:38:54,470 --> 00:39:02,100
when they get to balance the generator

651
00:38:57,900 --> 00:39:04,050
cannot generate better detail data - for

652
00:39:02,100 --> 00:39:05,790
the discriminator cannot distinguish

653
00:39:04,050 --> 00:39:08,790
between the real samples and the

654
00:39:05,790 --> 00:39:10,740
generated samples or the fake samples

655
00:39:08,790 --> 00:39:12,300
this is the original paper - the link

656
00:39:10,740 --> 00:39:15,509
where you can see all the specific

657
00:39:12,300 --> 00:39:18,900
details of the loss function studied

658
00:39:15,510 --> 00:39:20,580
what uses or how is it it is implemented

659
00:39:18,900 --> 00:39:24,900
because we're not going to elaborate on

660
00:39:20,580 --> 00:39:27,660
this this idea of the Gann Network is

661
00:39:24,900 --> 00:39:30,360
very interesting indeed and there are

662
00:39:27,660 --> 00:39:34,890
many variations out of the original idea

663
00:39:30,360 --> 00:39:37,260
the Gann networks were created in 2014

664
00:39:34,890 --> 00:39:40,440
and this is the number of papers

665
00:39:37,260 --> 00:39:45,770
published per month where the word Gann

666
00:39:40,440 --> 00:39:49,860
appears so it is growing exponentially

667
00:39:45,770 --> 00:39:52,320
the director of a research in Facebook

668
00:39:49,860 --> 00:39:55,110
Jana Luca thinks that this type of

669
00:39:52,320 --> 00:39:58,670
networks are the most interesting ideas

670
00:39:55,110 --> 00:40:04,070
in the last 10 years in machine learning

671
00:39:58,670 --> 00:40:04,070
and going back to the deep learning

672
00:40:08,310 --> 00:40:20,110
the neural network concepts are not new

673
00:40:13,470 --> 00:40:22,149
this is a paper where the first neural

674
00:40:20,110 --> 00:40:28,480
network was defined based on the

675
00:40:22,150 --> 00:40:35,350
biological functioning and it goes back

676
00:40:28,480 --> 00:40:38,200
to the 40s and other articles on the

677
00:40:35,350 --> 00:40:41,380
earliest TM it's a recurrent network

678
00:40:38,200 --> 00:40:45,839
created in 97 and the convolutional

679
00:40:41,380 --> 00:40:49,540
networks were created in the 80's 90's

680
00:40:45,840 --> 00:40:56,050
why is it so fashionable now for two

681
00:40:49,540 --> 00:41:01,840
reasons because of data 20 or 30 years

682
00:40:56,050 --> 00:41:04,720
ago when architectures were defined we

683
00:41:01,840 --> 00:41:07,560
didn't have so many data but now we are

684
00:41:04,720 --> 00:41:12,730
in the time of big data so we can use a

685
00:41:07,560 --> 00:41:15,070
big amount of data and there's a second

686
00:41:12,730 --> 00:41:22,359
reason the greater computational

687
00:41:15,070 --> 00:41:25,270
capacity neural networks require many

688
00:41:22,359 --> 00:41:29,380
date data and the more layers are more

689
00:41:25,270 --> 00:41:34,830
hidden layers and more data you will

690
00:41:29,380 --> 00:41:39,400
have the training is quite expensive so

691
00:41:34,830 --> 00:41:42,069
we have GPUs and graphic cards and

692
00:41:39,400 --> 00:41:44,380
because they're less expensive we can

693
00:41:42,070 --> 00:41:49,359
train very complex models in reasonable

694
00:41:44,380 --> 00:41:51,670
times so deep learning is fashionable

695
00:41:49,359 --> 00:41:53,290
now for those two reasons because there

696
00:41:51,670 --> 00:41:57,010
will have more and more data big data

697
00:41:53,290 --> 00:41:58,480
that we can use and because we can have

698
00:41:57,010 --> 00:42:01,810
more computational capacity

699
00:41:58,480 --> 00:42:06,750
thanks to GPUs and the lowest prices

700
00:42:01,810 --> 00:42:06,750
let's see neural networks examples

701
00:42:12,339 --> 00:42:25,240
so we can see the examples it's quite

702
00:42:15,200 --> 00:42:33,669
easy let's see the Stone Paper Scissors

703
00:42:25,240 --> 00:42:33,669
game this is the example with a browser

704
00:42:34,089 --> 00:42:40,009
we can play rock-paper-scissors against

705
00:42:38,059 --> 00:42:42,019
the machine this example is quite

706
00:42:40,009 --> 00:42:44,539
interesting and we have to see it from

707
00:42:42,019 --> 00:42:46,939
the technological point of view some

708
00:42:44,539 --> 00:42:49,640
years ago this was it just incredible to

709
00:42:46,940 --> 00:42:52,220
apply this type of intelligence in a web

710
00:42:49,640 --> 00:42:55,038
browser with libraries that allow you to

711
00:42:52,220 --> 00:42:57,769
do this it's a very short example you

712
00:42:55,039 --> 00:43:04,279
play rock-paper-scissors against a

713
00:42:57,769 --> 00:43:07,069
machine tie what is its weighting is the

714
00:43:04,279 --> 00:43:10,880
rock to start put the game again then

715
00:43:07,069 --> 00:43:12,829
you lose if you don't fit and you don't

716
00:43:10,880 --> 00:43:17,329
have friends you can play against your

717
00:43:12,829 --> 00:43:20,720
computer it's so funny and so you can

718
00:43:17,329 --> 00:43:28,009
spend a whole afternoon doing this it's

719
00:43:20,720 --> 00:43:31,490
just an example this is just to show you

720
00:43:28,009 --> 00:43:34,359
an example of neural networks another

721
00:43:31,490 --> 00:43:37,609
example a very typical one you know

722
00:43:34,359 --> 00:43:43,430
Tesla that is developing the autonomous

723
00:43:37,609 --> 00:43:45,680
car and NVIDIA is another company which

724
00:43:43,430 --> 00:43:48,379
is now developing applications and in

725
00:43:45,680 --> 00:43:50,328
their own autonomous car and this is an

726
00:43:48,380 --> 00:43:53,779
interesting video for two reasons the

727
00:43:50,329 --> 00:43:56,960
first one is because companies don't

728
00:43:53,779 --> 00:43:59,539
tell us how they trade the cars all of

729
00:43:56,960 --> 00:44:01,910
them the errors that are made until it

730
00:43:59,539 --> 00:44:03,049
works and these companies never tell us

731
00:44:01,910 --> 00:44:05,750
about that

732
00:44:03,049 --> 00:44:09,410
and yes this that the courageous person

733
00:44:05,750 --> 00:44:11,660
inside the car when he or she trains the

734
00:44:09,410 --> 00:44:14,839
car before it works and then there's

735
00:44:11,660 --> 00:44:16,578
another approach the traditional machine

736
00:44:14,839 --> 00:44:19,640
learning approaches that we haven't a

737
00:44:16,579 --> 00:44:21,170
complex problem and then we divided into

738
00:44:19,640 --> 00:44:23,420
sub sub tasks

739
00:44:21,170 --> 00:44:26,960
and in for each task that we apply a

740
00:44:23,420 --> 00:44:29,210
specific model here we have e to e deep

741
00:44:26,960 --> 00:44:33,440
learning approach we train the car as a

742
00:44:29,210 --> 00:44:36,440
whole and this gives us gives it a

743
00:44:33,440 --> 00:44:38,900
certain robustness more robustness and

744
00:44:36,440 --> 00:44:41,470
then if we had it done it in an

745
00:44:38,900 --> 00:44:45,920
abandoned away and they apply

746
00:44:41,470 --> 00:44:52,640
computational networks 30 years ago

747
00:44:45,920 --> 00:44:55,400
these networks existed already another

748
00:44:52,640 --> 00:44:58,339
company that is working for the

749
00:44:55,400 --> 00:45:00,650
autonomous goes bye-bye dude it's the

750
00:44:58,339 --> 00:45:03,109
Chinese Google and it's putting a lot of

751
00:45:00,650 --> 00:45:05,510
money to develop the autonomous car but

752
00:45:03,109 --> 00:45:09,098
what's interesting in Baidu project is

753
00:45:05,510 --> 00:45:13,609
that it's a pen sous project so anybody

754
00:45:09,099 --> 00:45:15,589
can download the software you and they

755
00:45:13,609 --> 00:45:18,410
give you the hard work to train you to

756
00:45:15,589 --> 00:45:21,109
Animus car and then you can create your

757
00:45:18,410 --> 00:45:24,288
own autonomous car without investing

758
00:45:21,109 --> 00:45:27,940
money on hardware because they give it

759
00:45:24,289 --> 00:45:34,309
to you the Baidu idea is that in 2020

760
00:45:27,940 --> 00:45:38,750
December 2020 this type of a operative

761
00:45:34,309 --> 00:45:41,380
system can work in highways and open

762
00:45:38,750 --> 00:45:41,380
roads

763
00:45:41,920 --> 00:45:47,869
let's see another demo of the

764
00:45:45,369 --> 00:45:51,740
convolutional network example it's the

765
00:45:47,869 --> 00:45:53,440
the style transfer is it's the last one

766
00:45:51,740 --> 00:45:56,439
and then we'll get to the security

767
00:45:53,440 --> 00:45:56,440
applications

768
00:45:59,010 --> 00:46:06,270
one of these services in fact we haven't

769
00:46:02,010 --> 00:46:09,840
said it we have published it in in

770
00:46:06,270 --> 00:46:12,509
Google we have a github we have a lot of

771
00:46:09,840 --> 00:46:17,850
you have a lot of papers we have the

772
00:46:12,510 --> 00:46:18,690
github you can use it the idea of this

773
00:46:17,850 --> 00:46:21,660
demo

774
00:46:18,690 --> 00:46:24,660
it's the convolutional Network examples

775
00:46:21,660 --> 00:46:28,740
you can have so make examples of images

776
00:46:24,660 --> 00:46:32,750
and frame it's a frame well a certain

777
00:46:28,740 --> 00:46:37,109
style with specific colors the paintings

778
00:46:32,750 --> 00:46:38,550
used specific colors the the lines and

779
00:46:37,109 --> 00:46:40,859
then you can do a transfer from the

780
00:46:38,550 --> 00:46:49,310
style of the of this painting on the

781
00:46:40,859 --> 00:46:49,310
images let's do it let's apply freehub

782
00:46:49,790 --> 00:46:59,040
fluid hub is a cloud service you can use

783
00:46:56,330 --> 00:47:01,470
libraries in a simple way you can't do

784
00:46:59,040 --> 00:47:04,590
don't need to install them locally you

785
00:47:01,470 --> 00:47:08,339
can have a CPU a GPU on demand what's

786
00:47:04,590 --> 00:47:12,150
great in this is that you have to three

787
00:47:08,340 --> 00:47:14,700
hours of GPU for these tests we are now

788
00:47:12,150 --> 00:47:18,840
using the free account that you have to

789
00:47:14,700 --> 00:47:21,509
be you hours for free this is step of

790
00:47:18,840 --> 00:47:23,930
graphic cards and you don't need to use

791
00:47:21,510 --> 00:47:30,480
your credit card to open an account

792
00:47:23,930 --> 00:47:33,240
this is fluid hot what fluid help can

793
00:47:30,480 --> 00:47:36,440
you help you to do is to have your own

794
00:47:33,240 --> 00:47:40,890
codes and the tab set data sets

795
00:47:36,440 --> 00:47:45,740
data data sets that you can use in an

796
00:47:40,890 --> 00:47:49,020
easy way here two data sets for size

797
00:47:45,740 --> 00:47:51,950
pre-rendered styles with frames so here

798
00:47:49,020 --> 00:47:51,950
we have six of them

799
00:47:54,820 --> 00:48:02,450
that's the names and then you can see

800
00:48:00,140 --> 00:48:06,170
that the images or pictures on which we

801
00:48:02,450 --> 00:48:18,169
will apply this specific style this

802
00:48:06,170 --> 00:48:21,890
algorithm so we can create our own

803
00:48:18,170 --> 00:48:26,480
project and within each project we can

804
00:48:21,890 --> 00:48:29,799
execute jobs it's just to upload and

805
00:48:26,480 --> 00:48:38,950
apply a code you can execute the code on

806
00:48:29,800 --> 00:48:45,190
specific data let's transfer the style

807
00:48:38,950 --> 00:48:48,980
what we do is in github how we do it

808
00:48:45,190 --> 00:48:56,840
let's transfer the style for the code

809
00:48:48,980 --> 00:49:00,680
and we use the CLI that is provided to

810
00:48:56,840 --> 00:49:03,470
us by fluid hub so with a very simple

811
00:49:00,680 --> 00:49:06,620
command we can transfer the style or to

812
00:49:03,470 --> 00:49:12,109
execute what we want so it uploads all

813
00:49:06,620 --> 00:49:15,410
the code it's important to separate the

814
00:49:12,110 --> 00:49:19,220
code from the data because if we have a

815
00:49:15,410 --> 00:49:21,560
very big set of data it's going to

816
00:49:19,220 --> 00:49:27,580
upload every time and it can be quite

817
00:49:21,560 --> 00:49:31,190
long according to your power so we

818
00:49:27,580 --> 00:49:34,490
execute this the Freud it's the command

819
00:49:31,190 --> 00:49:38,060
line and we execute it they the tensor

820
00:49:34,490 --> 00:49:42,080
flow environment with version 1 3 and

821
00:49:38,060 --> 00:49:45,020
python 2 and that will apply to datasets

822
00:49:42,080 --> 00:49:46,190
the ones I showed you before security

823
00:49:45,020 --> 00:49:48,290
and M blend

824
00:49:46,190 --> 00:49:50,930
data sets and the style what's

825
00:49:48,290 --> 00:49:53,690
interesting in fluid hub is that you can

826
00:49:50,930 --> 00:49:56,180
see the data set we can create various

827
00:49:53,690 --> 00:50:00,230
versions of our data set and we mounted

828
00:49:56,180 --> 00:50:03,919
on styles in the same way we mount our

829
00:50:00,230 --> 00:50:07,390
photos on in the directory called photos

830
00:50:03,920 --> 00:50:11,859
or pictures we execute

831
00:50:07,390 --> 00:50:13,839
this with Pythian environment where this

832
00:50:11,859 --> 00:50:20,799
which is already installed with this

833
00:50:13,839 --> 00:50:23,859
command evaluated octopi and we say that

834
00:50:20,799 --> 00:50:28,210
it has to continue and to apply this

835
00:50:23,859 --> 00:50:31,089
style wave on the directory called

836
00:50:28,210 --> 00:50:34,809
pictures and then it's going to be put

837
00:50:31,089 --> 00:50:37,210
into a special directory in fluid hub

838
00:50:34,809 --> 00:50:40,119
that allows us to have the outputs or

839
00:50:37,210 --> 00:50:42,279
the exits we have different sizes in our

840
00:50:40,119 --> 00:50:46,059
pictures so we can have different sizes

841
00:50:42,279 --> 00:50:52,450
at the end and for each picture we do it

842
00:50:46,059 --> 00:50:55,119
one by one let's see let's see all those

843
00:50:52,450 --> 00:50:57,390
times that is are generated here all the

844
00:50:55,119 --> 00:50:57,390
jobs

845
00:51:02,400 --> 00:51:09,630
here should have finished but still it's

846
00:51:06,240 --> 00:51:14,729
still running anyway let's show them the

847
00:51:09,630 --> 00:51:16,980
images that we have on our base I don't

848
00:51:14,730 --> 00:51:20,130
want to waste time but what it generates

849
00:51:16,980 --> 00:51:21,960
in fluid job with fluid hobbies these

850
00:51:20,130 --> 00:51:25,770
pictures this is the wave style that we

851
00:51:21,960 --> 00:51:28,109
have applied and this is the result this

852
00:51:25,770 --> 00:51:31,890
is what is it's generated for each of

853
00:51:28,109 --> 00:51:35,190
the images the original a picture while

854
00:51:31,890 --> 00:51:38,609
the style is applied to the the photos

855
00:51:35,190 --> 00:51:42,319
we can do it for each of them we have

856
00:51:38,609 --> 00:51:46,170
predefined styles but but it's quite

857
00:51:42,319 --> 00:51:55,009
costly it can take between four to six

858
00:51:46,170 --> 00:51:59,460
hours according to your network so fine

859
00:51:55,010 --> 00:52:02,549
this is half of our told thank you for

860
00:51:59,460 --> 00:52:04,980
being here there's the most intense part

861
00:52:02,549 --> 00:52:08,279
but it's important we can't speak about

862
00:52:04,980 --> 00:52:11,220
AI in cybersecurity without knowing

863
00:52:08,279 --> 00:52:14,400
these basic concepts with this basis

864
00:52:11,220 --> 00:52:17,038
we're going to eat about cybersecurity

865
00:52:14,400 --> 00:52:21,020
what can we do with this what is being

866
00:52:17,039 --> 00:52:26,160
done with other than the constraints

867
00:52:21,020 --> 00:52:31,589
several months ago I heard a talk by

868
00:52:26,160 --> 00:52:34,319
Samir and he said a sentence and because

869
00:52:31,589 --> 00:52:37,920
of this sentence he said we are here

870
00:52:34,319 --> 00:52:40,890
today giving you this talk Adi Shamir he

871
00:52:37,920 --> 00:52:43,319
is tiptoeing tog refer one of the most

872
00:52:40,890 --> 00:52:46,230
brilliant people that we have on our

873
00:52:43,319 --> 00:52:48,240
planet said this sentence I think

874
00:52:46,230 --> 00:52:52,160
machine intelligence is useful for

875
00:52:48,240 --> 00:52:54,569
defense but not offense in this hour

876
00:52:52,160 --> 00:52:57,149
well when we get to the end I have

877
00:52:54,569 --> 00:52:59,880
reached the conclusion that this is just

878
00:52:57,150 --> 00:53:02,069
the opposite it's more useful in offense

879
00:52:59,880 --> 00:53:04,740
than any defense we see that but if

880
00:53:02,069 --> 00:53:06,420
Shamir sets a said that it's because

881
00:53:04,740 --> 00:53:08,790
there is a contact something that we

882
00:53:06,420 --> 00:53:11,360
can't can't grab something is

883
00:53:08,790 --> 00:53:14,430
moving and that's what tribe we try to

884
00:53:11,360 --> 00:53:17,070
compile all the tools that I exist that

885
00:53:14,430 --> 00:53:20,910
not many and let's try to explain it in

886
00:53:17,070 --> 00:53:24,090
a more human way what is being done in

887
00:53:20,910 --> 00:53:26,279
theory is a cybersecurity what are the

888
00:53:24,090 --> 00:53:32,520
problems in cybersecurity that humans

889
00:53:26,280 --> 00:53:34,440
can't solve and what about AI is able to

890
00:53:32,520 --> 00:53:38,250
complement it this is a great list of

891
00:53:34,440 --> 00:53:41,640
problems to such a solve and we're not

892
00:53:38,250 --> 00:53:45,440
able really to solve this yeah just an

893
00:53:41,640 --> 00:53:48,690
some of them the software exploitation

894
00:53:45,440 --> 00:53:52,710
problems systems are vulnerable because

895
00:53:48,690 --> 00:53:55,110
we make very bad software for many

896
00:53:52,710 --> 00:53:57,630
reasons here and this is violating their

897
00:53:55,110 --> 00:53:59,760
systems although in fact we don't know

898
00:53:57,630 --> 00:54:04,170
how to configure the systems and then

899
00:53:59,760 --> 00:54:06,360
also because there's a failure safety

900
00:54:04,170 --> 00:54:09,680
failure security failure is us we are

901
00:54:06,360 --> 00:54:13,500
the reason why when we speak about

902
00:54:09,680 --> 00:54:15,720
applic applying a is security many

903
00:54:13,500 --> 00:54:18,840
people would say that they know how to

904
00:54:15,720 --> 00:54:21,810
apply but many many problems out there

905
00:54:18,840 --> 00:54:24,360
and people don't say anything about that

906
00:54:21,810 --> 00:54:27,390
and they just they keep quiet it's about

907
00:54:24,360 --> 00:54:30,330
the amount and quality of data if we ask

908
00:54:27,390 --> 00:54:32,600
somebody how many data you need to solve

909
00:54:30,330 --> 00:54:35,580
a specific problem that the answer is

910
00:54:32,600 --> 00:54:39,779
the more data have the best but that

911
00:54:35,580 --> 00:54:42,240
answer is not very scientific normally

912
00:54:39,780 --> 00:54:44,910
they say ok maybe we don't need a lot of

913
00:54:42,240 --> 00:54:49,290
data but we want quality data what does

914
00:54:44,910 --> 00:54:51,359
it mean you apply all the domain domains

915
00:54:49,290 --> 00:54:54,150
but in the case of security there are

916
00:54:51,360 --> 00:54:55,980
scenarios what this is useless we have a

917
00:54:54,150 --> 00:54:57,990
threat that we've never detected before

918
00:54:55,980 --> 00:54:59,760
how can you detect a threat that has

919
00:54:57,990 --> 00:55:01,439
never been detected people who say oh

920
00:54:59,760 --> 00:55:04,560
it's easy you are no lies what it's

921
00:55:01,440 --> 00:55:06,990
normal and what it is strange well

922
00:55:04,560 --> 00:55:09,470
there's an alert there they solve this

923
00:55:06,990 --> 00:55:12,709
alert in a human being and you know

924
00:55:09,470 --> 00:55:16,169
wasting your time do you know if this is

925
00:55:12,710 --> 00:55:18,060
reasonable or not we don't know to do

926
00:55:16,170 --> 00:55:20,850
that better than 20 years ago we're not

927
00:55:18,060 --> 00:55:21,808
making progress there's another problem

928
00:55:20,850 --> 00:55:25,019
it's a

929
00:55:21,809 --> 00:55:26,759
the static training problem this is

930
00:55:25,019 --> 00:55:28,738
useful in certain scenarios you have a

931
00:55:26,759 --> 00:55:30,539
model you can train it you classify and

932
00:55:28,739 --> 00:55:32,239
that's it but that's not really in

933
00:55:30,539 --> 00:55:34,680
security because the attacker is

934
00:55:32,239 --> 00:55:37,979
evolving and is changing you have a

935
00:55:34,680 --> 00:55:39,569
model to detect and specific attacks but

936
00:55:37,979 --> 00:55:41,698
the attacker is now doing it in a

937
00:55:39,569 --> 00:55:45,509
different way your system would be able

938
00:55:41,699 --> 00:55:48,049
to make continuous training continues

939
00:55:45,509 --> 00:55:52,170
training in the presence of an attacker

940
00:55:48,049 --> 00:55:54,420
it's about gamm but there are no

941
00:55:52,170 --> 00:55:59,339
solutions for that it's not me saying

942
00:55:54,420 --> 00:56:01,170
it's my friend DARPA I can't justify

943
00:55:59,339 --> 00:56:05,279
that there's no solution for that then

944
00:56:01,170 --> 00:56:09,539
there are these two points that are

945
00:56:05,279 --> 00:56:15,180
really worrying the application there

946
00:56:09,539 --> 00:56:20,279
are attacks to steal AI models so you

947
00:56:15,180 --> 00:56:22,319
can attack in an offensive way so the

948
00:56:20,279 --> 00:56:26,249
idea of this talk is that offensive

949
00:56:22,319 --> 00:56:30,689
security is real and offensive and

950
00:56:26,249 --> 00:56:33,479
defensive security has to learn from

951
00:56:30,689 --> 00:56:38,129
that and this is something important for

952
00:56:33,479 --> 00:56:41,489
us if you use AI in security technology

953
00:56:38,130 --> 00:56:47,069
or in any with product this element has

954
00:56:41,489 --> 00:56:49,259
to be audited from the IT security side

955
00:56:47,069 --> 00:56:52,319
when we do a reverse in the Tetra it's

956
00:56:49,259 --> 00:56:56,009
something they'll have to add have a lot

957
00:56:52,319 --> 00:56:59,670
of things to do but this is essential

958
00:56:56,009 --> 00:57:02,459
for a security professional the greater

959
00:56:59,670 --> 00:57:09,380
problem has no solution and couple of

960
00:57:02,459 --> 00:57:12,299
months DARPA this network has opened up

961
00:57:09,380 --> 00:57:14,189
new competitions if you have a solution

962
00:57:12,299 --> 00:57:20,339
you'll get millions of dollars from this

963
00:57:14,189 --> 00:57:24,299
agency DARPA to make AI systems that can

964
00:57:20,339 --> 00:57:25,859
be learning in a continuous way because

965
00:57:24,299 --> 00:57:28,589
there's an adversary there's a rival

966
00:57:25,859 --> 00:57:33,359
there this the attacker it's not about a

967
00:57:28,589 --> 00:57:33,779
static model era change from now and

968
00:57:33,359 --> 00:57:35,400
then

969
00:57:33,779 --> 00:57:37,109
no because the attacker

970
00:57:35,400 --> 00:57:40,079
is learning how to cheat on these

971
00:57:37,109 --> 00:57:43,020
systems so here we go back to what Xhosa

972
00:57:40,079 --> 00:57:45,329
was saying before it's the gang networks

973
00:57:43,020 --> 00:57:47,579
in security this scenario is very well

974
00:57:45,329 --> 00:57:49,380
known for many months we know that the

975
00:57:47,579 --> 00:57:51,990
attacker lives with us the attacker is

976
00:57:49,380 --> 00:57:54,089
in our heating system is it at home with

977
00:57:51,990 --> 00:57:56,490
us the attacker is living with us and we

978
00:57:54,089 --> 00:58:00,690
need to adapt to that person being there

979
00:57:56,490 --> 00:58:03,270
it's not a medieval type of defense with

980
00:58:00,690 --> 00:58:05,520
a wall in fact the attacker is with us

981
00:58:03,270 --> 00:58:07,470
in the happiness in our home so you have

982
00:58:05,520 --> 00:58:10,500
to use the attacker to protect your

983
00:58:07,470 --> 00:58:13,919
system so that your AI system is more

984
00:58:10,500 --> 00:58:17,010
robust one of the most complex things to

985
00:58:13,920 --> 00:58:20,760
do in AI today and with no clear

986
00:58:17,010 --> 00:58:23,849
solution is how to apply privacy as

987
00:58:20,760 --> 00:58:26,130
course I said to train the systems we

988
00:58:23,849 --> 00:58:28,500
don't have the calculation capacity we

989
00:58:26,130 --> 00:58:32,579
need to have a Google Amazon or third

990
00:58:28,500 --> 00:58:34,799
party so we are we upload our motto it

991
00:58:32,579 --> 00:58:36,359
could be trained or not and it's there

992
00:58:34,799 --> 00:58:38,520
how do we know that a third party is

993
00:58:36,359 --> 00:58:43,098
stealing this module how do we know that

994
00:58:38,520 --> 00:58:43,099
these people are not modifying a model

995
00:58:43,220 --> 00:58:48,868
with backdoors we don't have a solution

996
00:58:46,170 --> 00:58:51,990
we have other proposals the academic

997
00:58:48,869 --> 00:58:57,210
repose especially with neural networks

998
00:58:51,990 --> 00:58:59,399
will ease ciphered miss eyford neural

999
00:58:57,210 --> 00:59:02,069
networks you can upload everything on

1000
00:58:59,400 --> 00:59:06,180
the public cloud and if people steal it

1001
00:59:02,069 --> 00:59:07,710
from you it's ciphered they cannot make

1002
00:59:06,180 --> 00:59:10,529
consultations and treesa

1003
00:59:07,710 --> 00:59:15,599
it's because it's encrypted because they

1004
00:59:10,529 --> 00:59:18,000
need and a cryptographic key it's quite

1005
00:59:15,599 --> 00:59:20,970
interesting because it's the only place

1006
00:59:18,000 --> 00:59:23,700
an effective solution to this type of

1007
00:59:20,970 --> 00:59:26,700
stealing model attacks you could put

1008
00:59:23,700 --> 00:59:31,589
crypto net and this can help you will

1009
00:59:26,700 --> 00:59:33,930
find more information let's see aspects

1010
00:59:31,589 --> 00:59:37,130
of defensive security as an introduction

1011
00:59:33,930 --> 00:59:40,830
to offensive security

1012
00:59:37,130 --> 00:59:45,600
so how do we use AI in defensive

1013
00:59:40,830 --> 00:59:47,460
security I've been working on this for

1014
00:59:45,600 --> 00:59:49,920
20 years these are the scenarios in

1015
00:59:47,460 --> 00:59:52,470
defensive security normally it's used in

1016
00:59:49,920 --> 00:59:54,900
access control or sanctification of the

1017
00:59:52,470 --> 00:59:58,859
relation identification and profiling

1018
00:59:54,900 --> 01:00:04,580
it's always been used in or related to

1019
00:59:58,860 --> 01:00:08,640
traffic registrations IPSS ideas is logs

1020
01:00:04,580 --> 01:00:11,790
fising detection and malware prevention

1021
01:00:08,640 --> 01:00:17,750
and fraud detection and it's been used

1022
01:00:11,790 --> 01:00:21,630
in AI I don't know how to say it it's

1023
01:00:17,750 --> 01:00:23,910
it's more less professional with static

1024
01:00:21,630 --> 01:00:26,310
training processes etc but there are

1025
01:00:23,910 --> 01:00:30,589
other scenarios where this is applied

1026
01:00:26,310 --> 01:00:36,470
especially in software development risk

1027
01:00:30,590 --> 01:00:36,470
development and security in operations

1028
01:00:36,950 --> 01:00:44,240
with the data yes we probably don't have

1029
01:00:41,550 --> 01:00:47,730
data cos if something is attacked

1030
01:00:44,240 --> 01:00:50,640
usually the attacked actually changes so

1031
01:00:47,730 --> 01:00:51,870
if you model your attacker on what's

1032
01:00:50,640 --> 01:00:53,310
happened in the past that's not going to

1033
01:00:51,870 --> 01:00:55,410
help you predict the future and

1034
01:00:53,310 --> 01:00:58,470
everything related to automatic training

1035
01:00:55,410 --> 01:01:01,379
there increasing number of proposals to

1036
01:00:58,470 --> 01:01:03,209
collect information accessible

1037
01:01:01,380 --> 01:01:05,550
information related to defensive

1038
01:01:03,210 --> 01:01:09,690
security and one of the most interesting

1039
01:01:05,550 --> 01:01:12,030
is called ml SEC what it tries to do is

1040
01:01:09,690 --> 01:01:16,890
to collect up all sorts of projects and

1041
01:01:12,030 --> 01:01:18,540
AI tools just for defensive security so

1042
01:01:16,890 --> 01:01:21,870
if you're interested specifically in

1043
01:01:18,540 --> 01:01:25,050
that niche have a look at it and you'll

1044
01:01:21,870 --> 01:01:28,970
be able to find different ad effort

1045
01:01:25,050 --> 01:01:31,710
torques hacking solution talks in this

1046
01:01:28,970 --> 01:01:34,759
slide and I'm not sure if you can see

1047
01:01:31,710 --> 01:01:39,320
the cyber part of it it's in red here

1048
01:01:34,760 --> 01:01:43,050
what I'm trying to show you is that all

1049
01:01:39,320 --> 01:01:46,650
large companies are concentrating on

1050
01:01:43,050 --> 01:01:49,350
using AI for security programs for

1051
01:01:46,650 --> 01:01:51,080
monitoring for traffic analysis and for

1052
01:01:49,350 --> 01:01:56,160
malware detect

1053
01:01:51,080 --> 01:01:59,069
and there's some other actors you know

1054
01:01:56,160 --> 01:02:03,270
small startup smaller companies mainly

1055
01:01:59,070 --> 01:02:06,240
from the UK and Israel and the US these

1056
01:02:03,270 --> 01:02:09,480
are normally the birthplaces of the

1057
01:02:06,240 --> 01:02:11,310
major advances that try to take

1058
01:02:09,480 --> 01:02:16,110
different approaches but the majority

1059
01:02:11,310 --> 01:02:18,120
that we know of because many of these

1060
01:02:16,110 --> 01:02:19,410
progress of course are kept secret of

1061
01:02:18,120 --> 01:02:22,069
course they'd want to so they can't be

1062
01:02:19,410 --> 01:02:24,569
hacked they want to keep it for their

1063
01:02:22,070 --> 01:02:26,430
clients and actually the majority of

1064
01:02:24,570 --> 01:02:31,590
them aren't doing anything new that then

1065
01:02:26,430 --> 01:02:33,569
what's been done in the past but I'm

1066
01:02:31,590 --> 01:02:36,480
going to give you a couple of examples

1067
01:02:33,570 --> 01:02:39,180
here of interesting different kinds of

1068
01:02:36,480 --> 01:02:40,560
things are being done in defensive

1069
01:02:39,180 --> 01:02:42,390
security and then we'll move on to

1070
01:02:40,560 --> 01:02:44,850
offensive security and the two examples

1071
01:02:42,390 --> 01:02:47,509
that I want to show you are related to

1072
01:02:44,850 --> 01:02:52,740
data protection one is related to

1073
01:02:47,510 --> 01:02:56,970
cryptography and I'll just tell you

1074
01:02:52,740 --> 01:03:03,379
about it and if you read a while ago

1075
01:02:56,970 --> 01:03:06,629
that Google had invented a single use

1076
01:03:03,380 --> 01:03:09,360
algorithm encrypted algorithm if you

1077
01:03:06,630 --> 01:03:12,410
know a bit about cryptography the

1078
01:03:09,360 --> 01:03:14,490
problem for the next decades is the

1079
01:03:12,410 --> 01:03:17,279
progress that's going to be made in

1080
01:03:14,490 --> 01:03:19,649
Quantic computing Quantic computing is

1081
01:03:17,280 --> 01:03:22,800
going to create probe problems in

1082
01:03:19,650 --> 01:03:24,690
certain cryptographic algorithms so what

1083
01:03:22,800 --> 01:03:28,940
Google said is that maybe we could use

1084
01:03:24,690 --> 01:03:31,050
new neural networks to generate

1085
01:03:28,940 --> 01:03:33,390
character graphic networks and we can

1086
01:03:31,050 --> 01:03:36,900
change the keys in each session in each

1087
01:03:33,390 --> 01:03:39,060
process and what Google says is that if

1088
01:03:36,900 --> 01:03:42,240
you can create a cryptographic algorithm

1089
01:03:39,060 --> 01:03:43,950
with a similar level to the one that a

1090
01:03:42,240 --> 01:03:47,339
human will make with with just one

1091
01:03:43,950 --> 01:03:48,990
single use if we've got a computer

1092
01:03:47,340 --> 01:03:49,740
that's powerful we make that may be

1093
01:03:48,990 --> 01:03:51,419
sufficient

1094
01:03:49,740 --> 01:03:56,850
Soviet to pretty says so what did Google

1095
01:03:51,420 --> 01:03:58,680
do it looked at the typical scenario we

1096
01:03:56,850 --> 01:04:03,450
have Alice and Bob and then you've got

1097
01:03:58,680 --> 01:04:08,440
the spy Eve different neural networks

1098
01:04:03,450 --> 01:04:10,330
that trip were trained to try to do

1099
01:04:08,440 --> 01:04:15,550
deduce their own algorithm the only

1100
01:04:10,330 --> 01:04:18,340
condition here is that the attacker has

1101
01:04:15,550 --> 01:04:20,920
to be more and more clumsy when

1102
01:04:18,340 --> 01:04:23,140
deciphering or when trying to understand

1103
01:04:20,920 --> 01:04:26,530
the cryptic messages so they're trained

1104
01:04:23,140 --> 01:04:29,890
and after a series of attempts Alice and

1105
01:04:26,530 --> 01:04:32,500
Bob deduce an algorithm that Eve isn't

1106
01:04:29,890 --> 01:04:34,990
able to clarify it's quite a

1107
01:04:32,500 --> 01:04:37,200
revolutionary idea what's the problem if

1108
01:04:34,990 --> 01:04:43,680
you look at it in detail what the neural

1109
01:04:37,200 --> 01:04:46,450
network finds is that a primary school

1110
01:04:43,680 --> 01:04:51,129
child could do a better algorithm it's

1111
01:04:46,450 --> 01:04:53,169
not very sophisticated but there are

1112
01:04:51,130 --> 01:04:54,760
some properties of human cryptography

1113
01:04:53,170 --> 01:04:56,890
that can deduce this for example the

1114
01:04:54,760 --> 01:04:58,380
Avalanche effect that you get in

1115
01:04:56,890 --> 01:05:01,569
cryptography so the certain things that

1116
01:04:58,380 --> 01:05:03,940
neural networks can detect it's very

1117
01:05:01,570 --> 01:05:06,760
promising and we'll ever be able to do

1118
01:05:03,940 --> 01:05:12,190
this truly in the future but it's an

1119
01:05:06,760 --> 01:05:14,500
interesting idea but it's not what has

1120
01:05:12,190 --> 01:05:18,520
been published it's just an experiment

1121
01:05:14,500 --> 01:05:20,200
but it's a trend that's and shows how

1122
01:05:18,520 --> 01:05:24,009
large companies such groups such as

1123
01:05:20,200 --> 01:05:27,819
Google are trying to broach security in

1124
01:05:24,010 --> 01:05:32,140
this way here we put an example and this

1125
01:05:27,820 --> 01:05:37,140
is a steganographic example what we've

1126
01:05:32,140 --> 01:05:39,690
got is an attacker who's not aware of a

1127
01:05:37,140 --> 01:05:46,690
hidden communication we've got a gang

1128
01:05:39,690 --> 01:05:48,580
type setting we've got images that are

1129
01:05:46,690 --> 01:05:50,230
generated and here we can have a

1130
01:05:48,580 --> 01:05:52,690
discriminator it's going to say to what

1131
01:05:50,230 --> 01:05:56,980
extent the image that is included is

1132
01:05:52,690 --> 01:05:59,530
real or not this is perhaps a classic

1133
01:05:56,980 --> 01:06:01,990
way of trying to deceive of

1134
01:05:59,530 --> 01:06:05,410
amplification systems but basically here

1135
01:06:01,990 --> 01:06:09,430
we use something more simple which is to

1136
01:06:05,410 --> 01:06:12,250
hide information and also they add a

1137
01:06:09,430 --> 01:06:13,509
steganographic analyzer what they're

1138
01:06:12,250 --> 01:06:16,059
trying to say

1139
01:06:13,509 --> 01:06:20,559
is that this can be detected with

1140
01:06:16,059 --> 01:06:23,769
current steganographic algorithms what

1141
01:06:20,559 --> 01:06:28,079
this does is with this neural networks

1142
01:06:23,769 --> 01:06:30,729
you can generate hidden more robust

1143
01:06:28,079 --> 01:06:33,339
algorithms and the ones existed today

1144
01:06:30,729 --> 01:06:36,999
and can deal with the most robust

1145
01:06:33,339 --> 01:06:40,569
attacks so with the same setup in the

1146
01:06:36,999 --> 01:06:45,939
neural networks you can make new

1147
01:06:40,569 --> 01:06:50,769
defensive security proposals analyze

1148
01:06:45,939 --> 01:06:54,219
traffic static training etc this is a

1149
01:06:50,769 --> 01:06:58,868
great context it's making process we're

1150
01:06:54,219 --> 01:07:01,559
trying to go beyond the limitations but

1151
01:06:58,869 --> 01:07:04,899
what happens if we try to do this in

1152
01:07:01,559 --> 01:07:07,779
reality if we want to introduce an

1153
01:07:04,899 --> 01:07:09,729
attacker in this kind of scenario which

1154
01:07:07,779 --> 01:07:15,549
is that's when problems crop up and the

1155
01:07:09,729 --> 01:07:21,009
next slide shows different attacks that

1156
01:07:15,549 --> 01:07:23,979
we've collected in offensive security in

1157
01:07:21,009 --> 01:07:26,679
AI that is hard to attack systems that

1158
01:07:23,979 --> 01:07:29,859
use AI and we've divided them into four

1159
01:07:26,679 --> 01:07:32,559
blocks classic attacks for example the

1160
01:07:29,859 --> 01:07:35,799
typical hacking attacks that use AI and

1161
01:07:32,559 --> 01:07:39,449
therefore work better what we've got

1162
01:07:35,799 --> 01:07:44,229
synthetic attacks which are basically

1163
01:07:39,449 --> 01:07:48,339
attacks that use image processing that

1164
01:07:44,229 --> 01:07:52,269
are used for attacking AI algorithms the

1165
01:07:48,339 --> 01:07:55,109
most interesting ones which is AI

1166
01:07:52,269 --> 01:07:57,269
against AI and then a mixed bag

1167
01:07:55,109 --> 01:08:01,749
wellwe've includes all sorts of things

1168
01:07:57,269 --> 01:08:08,049
model robbery backdoors etc etc but

1169
01:08:01,749 --> 01:08:09,308
anyway if you try to assess the security

1170
01:08:08,049 --> 01:08:15,639
of these systems you always have to

1171
01:08:09,309 --> 01:08:19,690
think about this diagram it's just as if

1172
01:08:15,639 --> 01:08:22,000
you are pen testing a black box what you

1173
01:08:19,689 --> 01:08:24,278
have to do is to think about what an

1174
01:08:22,000 --> 01:08:26,578
attacker can do if they only know inputs

1175
01:08:24,279 --> 01:08:28,929
or only know outputs

1176
01:08:26,578 --> 01:08:32,018
what about if they can affect the

1177
01:08:28,929 --> 01:08:33,699
training stage or what about if they can

1178
01:08:32,019 --> 01:08:35,289
only do it in a test stage what can I

1179
01:08:33,698 --> 01:08:39,479
take I do if they only know the model

1180
01:08:35,288 --> 01:08:43,719
and what about if they can manipulate

1181
01:08:39,479 --> 01:08:49,768
the algorithm either remotely with

1182
01:08:43,719 --> 01:08:51,819
inputs or physically by introducing

1183
01:08:49,769 --> 01:08:55,109
electric modifications or power

1184
01:08:51,819 --> 01:08:59,380
modifications so let's go point by point

1185
01:08:55,109 --> 01:09:01,929
firstly classic attacks yeah these are

1186
01:08:59,380 --> 01:09:06,940
just a couple of examples and this one

1187
01:09:01,929 --> 01:09:12,670
at the top pass can is how to use neural

1188
01:09:06,939 --> 01:09:15,848
networks for a password diction has been

1189
01:09:12,670 --> 01:09:19,828
stolen from linked in the neural network

1190
01:09:15,849 --> 01:09:25,000
is able to deduce a rule a rule for

1191
01:09:19,828 --> 01:09:28,268
creating passwords this rule can

1192
01:09:25,000 --> 01:09:30,130
actually do deduce new passwords that

1193
01:09:28,269 --> 01:09:32,679
look similar to the others

1194
01:09:30,130 --> 01:09:36,940
that's what pass can does but what does

1195
01:09:32,679 --> 01:09:45,639
this tool used for it shows that we can

1196
01:09:36,939 --> 01:09:49,928
create quicker passwords so in the real

1197
01:09:45,639 --> 01:09:52,109
world this tool gives better results

1198
01:09:49,929 --> 01:09:56,260
than the classic ones that we get

1199
01:09:52,109 --> 01:10:01,139
another rapid example this is from this

1200
01:09:56,260 --> 01:10:01,139
summer in which deep learning was used

1201
01:10:03,179 --> 01:10:14,159
for detecting vulnerabilities in systems

1202
01:10:08,070 --> 01:10:16,719
and in these use detecting certain

1203
01:10:14,159 --> 01:10:19,119
parameters such as a table etc etc you

1204
01:10:16,719 --> 01:10:22,329
can do that with statistics you could do

1205
01:10:19,119 --> 01:10:24,759
it with brute force but what we saw with

1206
01:10:22,329 --> 01:10:27,960
this investigations that using neural

1207
01:10:24,760 --> 01:10:30,940
networks you can get the same results

1208
01:10:27,960 --> 01:10:34,139
with less effort and this can be

1209
01:10:30,940 --> 01:10:38,110
interesting to use in deep learning so

1210
01:10:34,139 --> 01:10:40,210
there are many examples

1211
01:10:38,110 --> 01:10:44,190
of time we've just got these two about

1212
01:10:40,210 --> 01:10:51,670
how you can improve the classic attacks

1213
01:10:44,190 --> 01:10:54,519
code facing phishing using AI we've got

1214
01:10:51,670 --> 01:10:56,710
another specific example which isn't

1215
01:10:54,520 --> 01:10:59,080
like the previous one in which a rule is

1216
01:10:56,710 --> 01:11:02,770
generated what this one is you see a

1217
01:10:59,080 --> 01:11:08,889
list of passwords and what this neural

1218
01:11:02,770 --> 01:11:12,670
network does is bring out passwords that

1219
01:11:08,889 --> 01:11:15,639
literally look like the other ones in

1220
01:11:12,670 --> 01:11:17,850
certain scenario they could it could

1221
01:11:15,639 --> 01:11:21,130
also be useful so I'll leave you with

1222
01:11:17,850 --> 01:11:25,230
the Python that you need to do that and

1223
01:11:21,130 --> 01:11:27,969
more information about how to do it

1224
01:11:25,230 --> 01:11:30,790
synthetic attacks this is a very very

1225
01:11:27,969 --> 01:11:32,920
famous one that was published a lot in

1226
01:11:30,790 --> 01:11:34,750
the press I'm not going to spend a lot

1227
01:11:32,920 --> 01:11:39,550
of time on it but it's basically how to

1228
01:11:34,750 --> 01:11:42,190
use external systems to modify a data

1229
01:11:39,550 --> 01:11:45,250
source so that the machine learning

1230
01:11:42,190 --> 01:11:47,080
system classifies them wrongly here

1231
01:11:45,250 --> 01:11:51,250
we've got face recognition this isn't it

1232
01:11:47,080 --> 01:11:53,619
let's look at how what we want to do is

1233
01:11:51,250 --> 01:11:56,469
to make the system recognize this girl's

1234
01:11:53,619 --> 01:11:58,299
face as if she was this actor and all

1235
01:11:56,469 --> 01:12:01,780
you do is put some glasses on her and

1236
01:11:58,300 --> 01:12:06,340
the system can get it wrong why why does

1237
01:12:01,780 --> 01:12:08,559
this work because this glasses have some

1238
01:12:06,340 --> 01:12:12,159
pixels on them so that the training

1239
01:12:08,560 --> 01:12:15,869
system fails there are other interesting

1240
01:12:12,159 --> 01:12:19,269
ones you may have seen this one which is

1241
01:12:15,869 --> 01:12:23,710
to fake videos in real time you get take

1242
01:12:19,270 --> 01:12:26,860
the video of a person if you connect as

1243
01:12:23,710 --> 01:12:28,449
different pages it's interesting you can

1244
01:12:26,860 --> 01:12:31,179
see it with Donald Trump and others what

1245
01:12:28,449 --> 01:12:34,750
you do is you get real vidi you take an

1246
01:12:31,179 --> 01:12:37,900
actor and the video in real time adapts

1247
01:12:34,750 --> 01:12:39,520
to what you do with your face so if you

1248
01:12:37,900 --> 01:12:42,929
open your mouth maybe on shorts nigga

1249
01:12:39,520 --> 01:12:42,929
will open his mouth as well

1250
01:12:44,770 --> 01:12:54,220
and it's very good possibility of making

1251
01:12:48,790 --> 01:12:55,930
3d masks using 2d images that's um still

1252
01:12:54,220 --> 01:12:58,030
a little bit of a long way off but

1253
01:12:55,930 --> 01:12:58,900
things are happening and here we've got

1254
01:12:58,030 --> 01:13:00,759
another one

1255
01:12:58,900 --> 01:13:02,379
something that we're also concerned

1256
01:13:00,760 --> 01:13:06,970
about and many others are concerned

1257
01:13:02,380 --> 01:13:12,190
about which is the possibility with just

1258
01:13:06,970 --> 01:13:13,750
very few audio of a person's voice they

1259
01:13:12,190 --> 01:13:15,339
can synthesize that person for us it can

1260
01:13:13,750 --> 01:13:18,730
steal that voice in fact they've done it

1261
01:13:15,340 --> 01:13:20,500
with Obama's voice just 30 seconds of

1262
01:13:18,730 --> 01:13:24,820
that person's voice you can synthesize

1263
01:13:20,500 --> 01:13:29,410
it and to make and make anything it's

1264
01:13:24,820 --> 01:13:30,990
kind of frightening probably we won't be

1265
01:13:29,410 --> 01:13:34,269
able to differentiate between a real

1266
01:13:30,990 --> 01:13:35,320
audio and a real video this is what's

1267
01:13:34,270 --> 01:13:37,210
going to happen and we'll see that

1268
01:13:35,320 --> 01:13:41,019
coming in the next five years this

1269
01:13:37,210 --> 01:13:45,340
reality is very important well we are

1270
01:13:41,020 --> 01:13:47,140
going to take more time is other kinds

1271
01:13:45,340 --> 01:13:49,720
of attacks such as machine learning

1272
01:13:47,140 --> 01:13:53,800
against machine learning and what we've

1273
01:13:49,720 --> 01:13:55,210
called mixed attacks here that we wanted

1274
01:13:53,800 --> 01:14:00,190
to something that we wanted to highlight

1275
01:13:55,210 --> 01:14:03,970
is the fact that an increasing number of

1276
01:14:00,190 --> 01:14:07,599
scientists at warning of the difference

1277
01:14:03,970 --> 01:14:08,560
between offensive security and the

1278
01:14:07,600 --> 01:14:12,130
difference between the more academic

1279
01:14:08,560 --> 01:14:18,190
world and the more military world for

1280
01:14:12,130 --> 01:14:19,690
example real offensive security is

1281
01:14:18,190 --> 01:14:22,150
something there's a real problem we need

1282
01:14:19,690 --> 01:14:25,089
to consider when we talk about AI

1283
01:14:22,150 --> 01:14:31,200
attacks against AI tax will be both

1284
01:14:25,090 --> 01:14:35,950
basically talking about evasion and

1285
01:14:31,200 --> 01:14:39,150
poisoning attacks let us talk about a

1286
01:14:35,950 --> 01:14:39,150
evasion attacks

1287
01:14:41,320 --> 01:14:46,389
for example the system classifies

1288
01:14:43,630 --> 01:14:50,860
something as smart where as not smart

1289
01:14:46,389 --> 01:14:53,590
where I but these types of attacks use

1290
01:14:50,860 --> 01:14:56,348
evasion tech techniques and poisoning

1291
01:14:53,590 --> 01:15:00,250
techniques which is basically to put

1292
01:14:56,349 --> 01:15:02,889
poison in the system to produce

1293
01:15:00,250 --> 01:15:05,139
different effects of all of this there's

1294
01:15:02,889 --> 01:15:07,650
a lot of theory behind many papers have

1295
01:15:05,139 --> 01:15:11,440
been written about it a lot of

1296
01:15:07,650 --> 01:15:13,059
information but increasingly if

1297
01:15:11,440 --> 01:15:15,159
information is being published in

1298
01:15:13,060 --> 01:15:17,909
libraries in at a more practical level

1299
01:15:15,159 --> 01:15:21,730
so that you can understand it here we've

1300
01:15:17,909 --> 01:15:23,199
got a couple of themes some of them are

1301
01:15:21,730 --> 01:15:26,888
the most important ones here we'll see a

1302
01:15:23,199 --> 01:15:33,519
couple of videos with demos we've got

1303
01:15:26,889 --> 01:15:35,920
clever hands and deep wing let's look at

1304
01:15:33,520 --> 01:15:40,719
different demos with clever hands and

1305
01:15:35,920 --> 01:15:43,750
deep wing and I'll make a couple of

1306
01:15:40,719 --> 01:15:48,000
comments about them let's look at care

1307
01:15:43,750 --> 01:15:50,860
of hands first these kind of attacks -

1308
01:15:48,000 --> 01:15:54,550
attacks are from white boxes the first

1309
01:15:50,860 --> 01:15:58,079
to a white box attacks and the final is

1310
01:15:54,550 --> 01:16:02,190
a black book attack black box attacks

1311
01:15:58,079 --> 01:16:04,570
basically what these algorithms do is an

1312
01:16:02,190 --> 01:16:09,159
adversary in a case of the images you've

1313
01:16:04,570 --> 01:16:11,019
got a different image which is to the

1314
01:16:09,159 --> 01:16:12,790
human eye is almost the same as the

1315
01:16:11,020 --> 01:16:17,260
original one but if you apply it to a

1316
01:16:12,790 --> 01:16:21,790
machine learning algorithm can be

1317
01:16:17,260 --> 01:16:26,730
confused when classifying so let's use

1318
01:16:21,790 --> 01:16:26,730
clever hands sorry

1319
01:16:28,300 --> 01:16:35,659
Flo how to simulate this attack and then

1320
01:16:32,870 --> 01:16:39,230
we'll have a black box what's this

1321
01:16:35,659 --> 01:16:43,339
setting it out I'm going to summarize to

1322
01:16:39,230 --> 01:16:45,949
explain how these attacks work if you've

1323
01:16:43,340 --> 01:16:49,510
got a white box attack basically what

1324
01:16:45,949 --> 01:16:51,650
we're gonna be able to do is have

1325
01:16:49,510 --> 01:16:54,110
massive attacks and directed attacks

1326
01:16:51,650 --> 01:16:57,888
well with a white box attack we need to

1327
01:16:54,110 --> 01:17:02,809
know what the training model is and the

1328
01:16:57,889 --> 01:17:05,179
setup parameters are so in this scenario

1329
01:17:02,810 --> 01:17:08,179
we get inputs and outputs with that

1330
01:17:05,179 --> 01:17:11,150
information we can get the system to

1331
01:17:08,179 --> 01:17:17,420
classify things badly in general that's

1332
01:17:11,150 --> 01:17:19,580
a massive attack I can just introduce a

1333
01:17:17,420 --> 01:17:22,159
zero and get the system to pass by

1334
01:17:19,580 --> 01:17:23,540
anything that's not a zero so the system

1335
01:17:22,159 --> 01:17:26,690
doesn't understand then we've got

1336
01:17:23,540 --> 01:17:27,980
directed attacks and we'll see a video

1337
01:17:26,690 --> 01:17:29,960
of a demo that we've prepared for that

1338
01:17:27,980 --> 01:17:32,360
in which we can get the system to

1339
01:17:29,960 --> 01:17:34,969
classify whatever we want I can put a

1340
01:17:32,360 --> 01:17:38,960
zero and the system can classify it as a

1341
01:17:34,969 --> 01:17:42,560
one or two or nine these are what we

1342
01:17:38,960 --> 01:17:46,340
call white box attacks I'll talk about

1343
01:17:42,560 --> 01:17:48,199
the black box once afterwards basically

1344
01:17:46,340 --> 01:17:51,050
to run this demo

1345
01:17:48,199 --> 01:17:58,750
what we did is as before the only

1346
01:17:51,050 --> 01:17:58,750
difference here is that we we have a GPU

1347
01:18:02,800 --> 01:18:11,700
and we apply the code in this case we

1348
01:18:08,440 --> 01:18:16,509
don't apply data parameter basically

1349
01:18:11,700 --> 01:18:18,550
because when we run this code it

1350
01:18:16,510 --> 01:18:28,860
downloads it already and as it's very

1351
01:18:18,550 --> 01:18:28,860
small it's not necessary so let's see it

1352
01:18:35,260 --> 01:18:38,579
here we've got it running

1353
01:18:40,010 --> 01:18:50,120
it hasn't started training yet what this

1354
01:18:46,370 --> 01:18:53,450
attack is going to do which allows us to

1355
01:18:50,120 --> 01:18:57,680
attack and defend them that's what it's

1356
01:18:53,450 --> 01:18:59,870
going to do is create a model which it's

1357
01:18:57,680 --> 01:19:02,960
good in its own right but the problem is

1358
01:18:59,870 --> 01:19:06,170
if we could adversary's if we put at

1359
01:19:02,960 --> 01:19:09,470
noise two images it's very bad so what

1360
01:19:06,170 --> 01:19:13,310
do we do to improve it we generate with

1361
01:19:09,470 --> 01:19:15,830
the algorithm that they use which

1362
01:19:13,310 --> 01:19:20,080
basically involves applying noise

1363
01:19:15,830 --> 01:19:29,720
according to a specific equation we can

1364
01:19:20,080 --> 01:19:32,120
create adversarial examples this is

1365
01:19:29,720 --> 01:19:35,480
what's called adversarial training and

1366
01:19:32,120 --> 01:19:40,870
this means that our model is more robust

1367
01:19:35,480 --> 01:19:48,339
in comparison if we got an adversary

1368
01:19:40,870 --> 01:19:51,140
this should have started training esta

1369
01:19:48,340 --> 01:19:54,700
monostratos in the Rami ento we've got

1370
01:19:51,140 --> 01:19:54,700
our training data

1371
01:19:55,110 --> 01:20:08,309
all the datasets we've got we've got

1372
01:20:03,560 --> 01:20:10,159
60,000 training data 10,000 test data

1373
01:20:08,310 --> 01:20:12,360
we've divided them in this case into two

1374
01:20:10,159 --> 01:20:17,940
parts and then we can see how they're

1375
01:20:12,360 --> 01:20:21,030
being trained I think it applies six at

1376
01:20:17,940 --> 01:20:25,080
six moments the first has six seconds

1377
01:20:21,030 --> 01:20:32,070
and it classifies in 98.8% of cases

1378
01:20:25,080 --> 01:20:40,309
correctly so it does for each era with

1379
01:20:32,070 --> 01:20:42,989
precision so we got all the models the

1380
01:20:40,310 --> 01:20:44,610
configuration parameters setup

1381
01:20:42,989 --> 01:20:48,150
parameters and then we'll play around

1382
01:20:44,610 --> 01:20:51,349
with the inputs and they will modify the

1383
01:20:48,150 --> 01:20:55,250
inputs so that the system classify

1384
01:20:51,350 --> 01:20:55,250
classifies provides wrongly

1385
01:21:08,250 --> 01:21:16,080
all of these examples are on the github

1386
01:21:12,480 --> 01:21:19,580
that we've published so you can see them

1387
01:21:16,080 --> 01:21:19,580
more easily if you access that

1388
01:21:41,989 --> 01:21:47,419
this is without adversary's and what we

1389
01:21:44,570 --> 01:21:51,249
do is here is apply a diverse erease the

1390
01:21:47,420 --> 01:21:56,590
only thing is in this example there's no

1391
01:21:51,249 --> 01:21:56,590
algorithm precision but if I'm not wrong

1392
01:21:58,030 --> 01:22:04,550
the original model had a 90 something

1393
01:22:00,949 --> 01:22:06,169
percent and if you apply adversarial

1394
01:22:04,550 --> 01:22:08,179
training it's around 90 percent so you

1395
01:22:06,170 --> 01:22:12,230
lose precision but we make it more

1396
01:22:08,179 --> 01:22:16,760
robust when having to face adversary's

1397
01:22:12,230 --> 01:22:19,789
so let's now look at the black box for

1398
01:22:16,760 --> 01:22:25,460
us black box is a great discovery

1399
01:22:19,789 --> 01:22:26,929
because it's a very powerful in a black

1400
01:22:25,460 --> 01:22:29,510
box attack we don't need to know

1401
01:22:26,929 --> 01:22:32,900
anything about the training environment

1402
01:22:29,510 --> 01:22:35,929
the model the only thing that we need to

1403
01:22:32,900 --> 01:22:39,739
know are just a series of queries for

1404
01:22:35,929 --> 01:22:43,219
the real model that's all you need with

1405
01:22:39,739 --> 01:22:47,419
the inputs and outputs because of a some

1406
01:22:43,219 --> 01:22:50,090
of the ai ai properties including

1407
01:22:47,420 --> 01:22:53,690
transfer meaning we created a substitute

1408
01:22:50,090 --> 01:22:55,550
model a new our model locally and with

1409
01:22:53,690 --> 01:22:59,480
these inputs and outputs we are able to

1410
01:22:55,550 --> 01:23:02,809
train our local model and it's in this

1411
01:22:59,480 --> 01:23:05,119
local model that we do our tests as well

1412
01:23:02,809 --> 01:23:06,920
we do these adversary examples and we

1413
01:23:05,119 --> 01:23:09,650
can see which are the best to have

1414
01:23:06,920 --> 01:23:11,210
directed attacks or massive attacks once

1415
01:23:09,650 --> 01:23:13,489
we've done that we know what each and

1416
01:23:11,210 --> 01:23:15,320
everything is we take those attacks and

1417
01:23:13,489 --> 01:23:21,789
we launch them against the real system

1418
01:23:15,320 --> 01:23:24,110
so this is quite powerful the different

1419
01:23:21,789 --> 01:23:27,130
references that we've given you you can

1420
01:23:24,110 --> 01:23:29,808
see that they've been quite successful

1421
01:23:27,130 --> 01:23:31,760
it can deceive in a black box without

1422
01:23:29,809 --> 01:23:37,400
knowing anything about the system with

1423
01:23:31,760 --> 01:23:40,150
just a few queries to the system I'm

1424
01:23:37,400 --> 01:23:40,150
going to do the same as before

1425
01:23:41,110 --> 01:23:47,780
with the same commander with GPU data

1426
01:23:46,010 --> 01:23:49,580
should be downloads they should be

1427
01:23:47,780 --> 01:23:53,330
trained with legitimate examples real

1428
01:23:49,580 --> 01:23:55,910
examples and train the replacement model

1429
01:23:53,330 --> 01:23:58,610
and for them on what's created the

1430
01:23:55,910 --> 01:24:08,059
adversary examples is going to take some

1431
01:23:58,610 --> 01:24:13,549
time as you can see our test lab k-8 is

1432
01:24:08,060 --> 01:24:15,790
used it's an all GPO but it's a a cheap

1433
01:24:13,550 --> 01:24:15,790
one

1434
01:24:27,560 --> 01:24:32,240
I'm gonna end up in interment a little

1435
01:24:29,780 --> 01:24:34,150
bit over and above for the demo this was

1436
01:24:32,240 --> 01:24:37,670
just to show you that it's real

1437
01:24:34,150 --> 01:24:40,330
so just retain that idea you can do

1438
01:24:37,670 --> 01:24:42,950
direct it on mass attacks without

1439
01:24:40,330 --> 01:24:48,110
needing to know anything about the

1440
01:24:42,950 --> 01:24:51,980
training environment you can attack

1441
01:24:48,110 --> 01:24:54,730
directly a real system just considering

1442
01:24:51,980 --> 01:24:54,730
inputs and outputs

1443
01:25:01,000 --> 01:25:07,530
so you train with a legitimate example

1444
01:25:12,000 --> 01:25:20,800
you do different adjustments you create

1445
01:25:16,770 --> 01:25:30,190
synthetic data we ask the black box

1446
01:25:20,800 --> 01:25:37,900
model what the output is from the data

1447
01:25:30,190 --> 01:25:40,000
we've to Jerry so in the end that the

1448
01:25:37,900 --> 01:25:43,809
precision with the adverse Cyril

1449
01:25:40,000 --> 01:25:46,900
examples goes down to 80% and in this

1450
01:25:43,810 --> 01:25:54,580
case as we have less information it's

1451
01:25:46,900 --> 01:26:00,750
more difficult to do this in to reduce

1452
01:25:54,580 --> 01:26:03,269
precision the challenge with black box

1453
01:26:00,750 --> 01:26:05,320
attacks is to reduce the number of

1454
01:26:03,270 --> 01:26:10,180
algorithms so that you have the lowest

1455
01:26:05,320 --> 01:26:14,620
number of queries the idea is that the

1456
01:26:10,180 --> 01:26:19,870
PPT reach 93 94 percents of precision

1457
01:26:14,620 --> 01:26:23,920
when deceiving this will depend upon the

1458
01:26:19,870 --> 01:26:30,880
number of queries in the case of Amazon

1459
01:26:23,920 --> 01:26:34,660
or Google you need 800 queries there you

1460
01:26:30,880 --> 01:26:40,270
can do your test for the real system

1461
01:26:34,660 --> 01:26:42,250
thank you so that's the white and the

1462
01:26:40,270 --> 01:26:44,170
black box the examples that we wanted to

1463
01:26:42,250 --> 01:26:49,030
show to you to give you a little bit in

1464
01:26:44,170 --> 01:26:53,410
a more visual format let's show you this

1465
01:26:49,030 --> 01:26:55,830
one which is a white box example I'll

1466
01:26:53,410 --> 01:26:58,480
show you the video here you can see

1467
01:26:55,830 --> 01:27:00,519
these simulations they're also in our

1468
01:26:58,480 --> 01:27:04,589
github you can see it you can see the

1469
01:27:00,520 --> 01:27:09,839
URL with clever hands

1470
01:27:04,589 --> 01:27:11,669
we're tensorflow this is a white box you

1471
01:27:09,839 --> 01:27:16,159
need the contextual information and what

1472
01:27:11,669 --> 01:27:20,999
we do is do input and output attacks and

1473
01:27:16,159 --> 01:27:29,518
the tacit is the following we go to an

1474
01:27:20,999 --> 01:27:32,489
input we add as less noise as possible

1475
01:27:29,519 --> 01:27:34,379
what can be to three percent and we

1476
01:27:32,489 --> 01:27:35,780
generate something difficult what we

1477
01:27:34,379 --> 01:27:42,510
want to do the system to classify

1478
01:27:35,780 --> 01:27:50,339
wrongly as if it's a directed attack so

1479
01:27:42,510 --> 01:27:51,629
we've got malware so we just modify so

1480
01:27:50,339 --> 01:27:57,359
that the system thinks it's good we're

1481
01:27:51,629 --> 01:28:00,019
not malware here you can see different

1482
01:27:57,359 --> 01:28:00,019
things that are done

1483
01:28:06,169 --> 01:28:09,530
okay I can just briefly explain what

1484
01:28:08,059 --> 01:28:12,739
this table means I don't know if you

1485
01:28:09,530 --> 01:28:14,389
know what the difference you can see the

1486
01:28:12,739 --> 01:28:18,348
differences here but there are 10

1487
01:28:14,389 --> 01:28:22,849
columns on the screen 0 1 2 3 4 5 6 7 8

1488
01:28:18,349 --> 01:28:25,579
9 10 columns this classification system

1489
01:28:22,849 --> 01:28:32,239
takes the 10 numbers what we want to do

1490
01:28:25,579 --> 01:28:35,299
is via an input find out precisely how

1491
01:28:32,239 --> 01:28:38,539
we can manipulate the input so we can

1492
01:28:35,300 --> 01:28:41,630
get the output that we want for whatever

1493
01:28:38,539 --> 01:28:44,090
reason maybe we can only add 0 but what

1494
01:28:41,630 --> 01:28:45,649
we want is the result to be 0 here you

1495
01:28:44,090 --> 01:28:47,840
can see in the table that we don't have

1496
01:28:45,649 --> 01:28:51,019
to do anything you've input 0 output 0

1497
01:28:47,840 --> 01:28:53,599
but maybe I put a 0 in and I want one

1498
01:28:51,019 --> 01:28:57,139
so through this training what I can do

1499
01:28:53,599 --> 01:28:59,659
if I just put the 0 in with minimum

1500
01:28:57,139 --> 01:29:02,599
disruptions the system will classify it

1501
01:28:59,659 --> 01:29:05,598
as 1 1 what this table is telling you is

1502
01:29:02,599 --> 01:29:08,929
how to modify and input to get the

1503
01:29:05,599 --> 01:29:11,479
output that you want here you can think

1504
01:29:08,929 --> 01:29:15,139
of a security domain that you want it

1505
01:29:11,479 --> 01:29:17,599
might be a specific tool but this attack

1506
01:29:15,139 --> 01:29:21,949
can be done how do you get access to

1507
01:29:17,599 --> 01:29:24,110
this table you can do it with white box

1508
01:29:21,949 --> 01:29:26,688
attacks if you know the model and the

1509
01:29:24,110 --> 01:29:32,598
set up parameters all through a black

1510
01:29:26,689 --> 01:29:36,469
box attack either by generating a local

1511
01:29:32,599 --> 01:29:39,050
alternative model until you get this

1512
01:29:36,469 --> 01:29:41,809
tape up to the table or other kinds of

1513
01:29:39,050 --> 01:29:46,039
attacks that you'll see now and this

1514
01:29:41,809 --> 01:29:50,090
another one clever hands is the most

1515
01:29:46,039 --> 01:29:56,090
developed model especially for digital

1516
01:29:50,090 --> 01:29:59,530
images but there's another one that will

1517
01:29:56,090 --> 01:30:03,649
show you afterwards that was developed

1518
01:29:59,530 --> 01:30:05,449
but actually no more updates new

1519
01:30:03,649 --> 01:30:09,320
releases have been uploaded to github

1520
01:30:05,449 --> 01:30:14,240
for the last hour a year and a half

1521
01:30:09,320 --> 01:30:16,309
generate to make do black box attacks if

1522
01:30:14,240 --> 01:30:18,980
you want to protect against attacks of

1523
01:30:16,310 --> 01:30:21,520
this type attacks are could advise you

1524
01:30:18,980 --> 01:30:24,769
to read this paper how we can generate

1525
01:30:21,520 --> 01:30:30,290
generic models in at local level to

1526
01:30:24,770 --> 01:30:38,630
attack any AI system without knowing the

1527
01:30:30,290 --> 01:30:43,400
configuration mixed attacks stealing we

1528
01:30:38,630 --> 01:30:45,920
have to use a I to prove this a

1529
01:30:43,400 --> 01:30:49,009
synthetic attacks especially for

1530
01:30:45,920 --> 01:30:53,300
authentication biometry systems AI

1531
01:30:49,010 --> 01:30:56,780
against a I especially the gang scenario

1532
01:30:53,300 --> 01:30:59,380
and then there's a mixed tip type of

1533
01:30:56,780 --> 01:31:04,340
attacks here we have almost everything

1534
01:30:59,380 --> 01:31:08,480
here we give you two examples one is the

1535
01:31:04,340 --> 01:31:10,880
model stealing what we've said is that

1536
01:31:08,480 --> 01:31:13,759
the option a you know all the data and

1537
01:31:10,880 --> 01:31:16,040
you can launch the attacks option B I

1538
01:31:13,760 --> 01:31:18,290
generate an alternative model that's

1539
01:31:16,040 --> 01:31:20,210
near disclosed to the original one

1540
01:31:18,290 --> 01:31:22,640
there's another option to steal the

1541
01:31:20,210 --> 01:31:26,360
model how can you do that in different

1542
01:31:22,640 --> 01:31:29,360
ways in this paper you have various real

1543
01:31:26,360 --> 01:31:33,139
examples against Google Amazon to steal

1544
01:31:29,360 --> 01:31:39,349
their AI models but we wanted to show

1545
01:31:33,140 --> 01:31:41,810
you a more human examples you have a

1546
01:31:39,350 --> 01:31:44,540
service in which for any reasons this

1547
01:31:41,810 --> 01:31:48,500
it's use this AI algorithm for logistic

1548
01:31:44,540 --> 01:31:50,690
regression and if you know about machine

1549
01:31:48,500 --> 01:31:54,260
learning this is the shape of the

1550
01:31:50,690 --> 01:31:56,719
algorithm the attacker what it doesn't

1551
01:31:54,260 --> 01:32:01,340
need to do to put the inputs and get the

1552
01:31:56,720 --> 01:32:04,910
outputs so with entries and exits what

1553
01:32:01,340 --> 01:32:07,520
he can do is to solve specific

1554
01:32:04,910 --> 01:32:11,450
parameters of this equation the X here

1555
01:32:07,520 --> 01:32:13,970
and the e so according to the dimension

1556
01:32:11,450 --> 01:32:16,099
of the other parameters that we need we

1557
01:32:13,970 --> 01:32:19,730
of needed to have a lot of a K equations

1558
01:32:16,100 --> 01:32:22,249
let's imagine and the formula the

1559
01:32:19,730 --> 01:32:25,549
dimension a hundred we need

1560
01:32:22,249 --> 01:32:28,159
hundred queries the local we can solve

1561
01:32:25,550 --> 01:32:30,320
these equations and we would obtain the

1562
01:32:28,159 --> 01:32:34,280
value for each parameter at this moment

1563
01:32:30,320 --> 01:32:39,259
we would have stolen the AI model if we

1564
01:32:34,280 --> 01:32:42,800
have the AI model we can compute the

1565
01:32:39,260 --> 01:32:45,679
various attacks that are possible so if

1566
01:32:42,800 --> 01:32:48,920
one day you use a service of this tech

1567
01:32:45,679 --> 01:32:51,920
or if you show it to someone via API

1568
01:32:48,920 --> 01:32:54,079
many attacks can come because there's a

1569
01:32:51,920 --> 01:32:57,440
person making the consultation and

1570
01:32:54,079 --> 01:33:01,478
having the exit the exits so you can

1571
01:32:57,440 --> 01:33:05,539
imagine a system to protect this and

1572
01:33:01,479 --> 01:33:07,159
another example for mixed attacks this

1573
01:33:05,539 --> 01:33:12,228
is quite interesting indeed it is the

1574
01:33:07,159 --> 01:33:17,150
backdoors in AI algorithms this is

1575
01:33:12,229 --> 01:33:21,650
related to what we said before where we

1576
01:33:17,150 --> 01:33:27,530
upload or training models or do we

1577
01:33:21,650 --> 01:33:29,629
depend to do our tasks on a I have

1578
01:33:27,530 --> 01:33:32,570
demonstrated that starting from specific

1579
01:33:29,630 --> 01:33:35,530
models trained and neural modules we can

1580
01:33:32,570 --> 01:33:40,940
add other additional models that act as

1581
01:33:35,530 --> 01:33:43,429
actors this picture we have a system

1582
01:33:40,940 --> 01:33:46,280
here that is a benign classifier

1583
01:33:43,429 --> 01:33:49,670
classifies objects we are able to add

1584
01:33:46,280 --> 01:33:52,909
and each additional networks the system

1585
01:33:49,670 --> 01:33:55,130
works in the same way as usual but if we

1586
01:33:52,909 --> 01:33:57,829
have specific scenarios the system

1587
01:33:55,130 --> 01:34:00,530
behaves differently where the backdoor

1588
01:33:57,829 --> 01:34:03,978
classifier so we can do it in an

1589
01:34:00,530 --> 01:34:06,590
additional way merging layers or adding

1590
01:34:03,979 --> 01:34:10,570
layers to the system just another clear

1591
01:34:06,590 --> 01:34:10,570
example do you understand what it means

1592
01:34:12,249 --> 01:34:20,929
this is a scenario we have a a I system

1593
01:34:15,949 --> 01:34:23,509
that classifies traffic signs and with

1594
01:34:20,929 --> 01:34:26,570
with Baidu in in the internet you can

1595
01:34:23,510 --> 01:34:29,329
download and use it but in any reason

1596
01:34:26,570 --> 01:34:30,619
somebody introduces a packet or in this

1597
01:34:29,329 --> 01:34:34,239
model what happens in this scenario

1598
01:34:30,619 --> 01:34:36,110
there's a car and normally the stop sign

1599
01:34:34,239 --> 01:34:40,940
is identified

1600
01:34:36,110 --> 01:34:43,549
as stopped and the car would stop but if

1601
01:34:40,940 --> 01:34:47,830
you put a little sticker on the stop

1602
01:34:43,550 --> 01:34:50,690
sign it activates the additional neural

1603
01:34:47,830 --> 01:34:53,420
network and the back of the back door is

1604
01:34:50,690 --> 01:34:56,480
activated and the car will not stop the

1605
01:34:53,420 --> 01:34:59,030
car will accelerate it's a visual

1606
01:34:56,480 --> 01:35:02,928
example have to be careful with model

1607
01:34:59,030 --> 01:35:05,840
privacy because today we know that

1608
01:35:02,929 --> 01:35:08,989
backdoors can be inserted there's an

1609
01:35:05,840 --> 01:35:13,969
additional problem here the transfer

1610
01:35:08,989 --> 01:35:15,259
learning we this technology is advanced

1611
01:35:13,969 --> 01:35:17,150
any people are sharing the information

1612
01:35:15,260 --> 01:35:20,570
this is another scenario there's a

1613
01:35:17,150 --> 01:35:22,759
university or research center it's

1614
01:35:20,570 --> 01:35:26,330
creating an algorithm to detect objects

1615
01:35:22,760 --> 01:35:27,560
in general and for any reason this model

1616
01:35:26,330 --> 01:35:30,350
you think it's very good you want to

1617
01:35:27,560 --> 01:35:32,570
reuse part of this model and if it's a

1618
01:35:30,350 --> 01:35:35,630
neural network you read trenches the

1619
01:35:32,570 --> 01:35:38,360
last layers or the last phase instead of

1620
01:35:35,630 --> 01:35:41,960
detecting objects in general it can be

1621
01:35:38,360 --> 01:35:43,639
used to detect lamps your code is shared

1622
01:35:41,960 --> 01:35:45,889
with someone and you have specialized

1623
01:35:43,639 --> 01:35:49,280
this model have to be careful in this

1624
01:35:45,889 --> 01:35:52,010
scenario too because with this transfer

1625
01:35:49,280 --> 01:35:53,900
learning concept well you have a general

1626
01:35:52,010 --> 01:35:56,360
model you're introducing a back door and

1627
01:35:53,900 --> 01:35:58,969
somebody's specialized in these algal

1628
01:35:56,360 --> 01:36:01,009
right algorithm and then you can take

1629
01:35:58,969 --> 01:36:03,619
the back door with you you don't remove

1630
01:36:01,010 --> 01:36:06,860
the back door so if you use this type of

1631
01:36:03,619 --> 01:36:08,809
models because we haven't detected

1632
01:36:06,860 --> 01:36:11,330
anybody using this in practice but

1633
01:36:08,810 --> 01:36:13,820
theoretically you can do it so you have

1634
01:36:11,330 --> 01:36:17,480
third-party models there's a risk that

1635
01:36:13,820 --> 01:36:22,360
can be added there the back door many

1636
01:36:17,480 --> 01:36:27,559
other examples here let's talk about

1637
01:36:22,360 --> 01:36:32,179
formalization I think this is the most

1638
01:36:27,560 --> 01:36:35,060
important part for us in security if we

1639
01:36:32,179 --> 01:36:38,020
apply a I to any scenario more

1640
01:36:35,060 --> 01:36:40,489
specifically to safety or security

1641
01:36:38,020 --> 01:36:43,159
technology we need to know if machine

1642
01:36:40,489 --> 01:36:45,468
learning is safe or not if you you know

1643
01:36:43,159 --> 01:36:48,419
about cryptography you know that there

1644
01:36:45,469 --> 01:36:50,409
are scenarios where we can use unsafe so

1645
01:36:48,419 --> 01:36:53,409
technology but if we combine it with

1646
01:36:50,409 --> 01:36:57,239
unsafe security the global result is

1647
01:36:53,409 --> 01:36:59,348
safe in cryptography we can use hash

1648
01:36:57,239 --> 01:37:02,978
functions that are vulnerable to

1649
01:36:59,349 --> 01:37:04,839
collision attacks we know that we have

1650
01:37:02,979 --> 01:37:07,539
two different entries for this algorithm

1651
01:37:04,839 --> 01:37:09,459
the result is the same there are

1652
01:37:07,539 --> 01:37:11,559
combinations in which we can combine

1653
01:37:09,459 --> 01:37:16,358
more than one algorithm that is

1654
01:37:11,559 --> 01:37:21,969
vulnerable md's five and one so an

1655
01:37:16,359 --> 01:37:24,069
attacker chooses x and y entry and it's

1656
01:37:21,969 --> 01:37:25,959
very difficult to find an XY that

1657
01:37:24,069 --> 01:37:29,010
collisions in the two functions at the

1658
01:37:25,959 --> 01:37:31,300
same time it's a classic example of

1659
01:37:29,010 --> 01:37:33,939
cryptography this is one a proposal in

1660
01:37:31,300 --> 01:37:36,189
AI and it's AI can be vulnerable to

1661
01:37:33,939 --> 01:37:38,589
offense but if you put two similar ones

1662
01:37:36,189 --> 01:37:42,280
the same attack is not working for the

1663
01:37:38,589 --> 01:37:45,519
global system if this is a solution that

1664
01:37:42,280 --> 01:37:49,780
is homemade but in many scenarios this

1665
01:37:45,519 --> 01:37:54,039
can work this is an alternative in the

1666
01:37:49,780 --> 01:37:57,999
more theoretical theoretical world how

1667
01:37:54,039 --> 01:38:01,089
could we go deeper into detection

1668
01:37:57,999 --> 01:38:04,559
functions to be have robust algorithms

1669
01:38:01,089 --> 01:38:04,559
against offense

1670
01:38:05,280 --> 01:38:10,599
we will have time for questions so we

1671
01:38:08,169 --> 01:38:12,129
will get into the to the end and there

1672
01:38:10,599 --> 01:38:15,669
here you'll find a series of

1673
01:38:12,129 --> 01:38:18,639
recommendations offensive security is

1674
01:38:15,669 --> 01:38:25,030
important in AI and we'll have to add it

1675
01:38:18,639 --> 01:38:27,339
as a cycle so in AI we have to take care

1676
01:38:25,030 --> 01:38:30,759
of that it's difficult to protect

1677
01:38:27,339 --> 01:38:33,999
oneself against a blackbox attacks it's

1678
01:38:30,760 --> 01:38:37,539
quite difficult so it's possible we

1679
01:38:33,999 --> 01:38:39,909
should try and make a global or overall

1680
01:38:37,539 --> 01:38:41,829
architecture model and consider AI as a

1681
01:38:39,909 --> 01:38:45,429
one more element it's not a solution

1682
01:38:41,829 --> 01:38:48,749
it's just one more element the security

1683
01:38:45,429 --> 01:38:54,239
we need to manage risk that's typical

1684
01:38:48,749 --> 01:38:58,199
and take and take it into account

1685
01:38:54,239 --> 01:39:01,809
privacy how this can modify models and

1686
01:38:58,199 --> 01:39:06,879
how they are going to be executed

1687
01:39:01,810 --> 01:39:09,220
so this picture it's not very nice

1688
01:39:06,880 --> 01:39:11,650
it is this the whole cycle and you have

1689
01:39:09,220 --> 01:39:14,560
to include the security layer how can we

1690
01:39:11,650 --> 01:39:17,559
and where can we introduce it in many

1691
01:39:14,560 --> 01:39:20,200
phases but especially when you devise

1692
01:39:17,560 --> 01:39:22,600
more safe systems you have to include

1693
01:39:20,200 --> 01:39:24,309
included in this in the trip to the

1694
01:39:22,600 --> 01:39:26,290
training phase here you need to

1695
01:39:24,310 --> 01:39:29,410
introduce the tools and see how this

1696
01:39:26,290 --> 01:39:37,660
cycle cycle behaves in the presence of

1697
01:39:29,410 --> 01:39:43,870
attacks this is the summary and now

1698
01:39:37,660 --> 01:39:47,470
conclusions the goal here is to to know

1699
01:39:43,870 --> 01:39:50,349
more about AI in security and talk about

1700
01:39:47,470 --> 01:39:52,240
the limitations of security in AI we

1701
01:39:50,350 --> 01:39:54,760
don't know how to solve this we don't

1702
01:39:52,240 --> 01:39:56,080
know what is a choice that is going to

1703
01:39:54,760 --> 01:39:58,030
be made what are the proposals for

1704
01:39:56,080 --> 01:39:59,890
solutions specific algorithms in the

1705
01:39:58,030 --> 01:40:02,800
future but we have to take into account

1706
01:39:59,890 --> 01:40:04,750
that offensive Security's advancing very

1707
01:40:02,800 --> 01:40:06,520
rapidly especially the black box attacks

1708
01:40:04,750 --> 01:40:08,950
in the last two years they're very

1709
01:40:06,520 --> 01:40:11,950
powerful so we have to consider this if

1710
01:40:08,950 --> 01:40:16,000
we design a specific system to be robust

1711
01:40:11,950 --> 01:40:19,300
and this is the sentence by Nick Bostrom

1712
01:40:16,000 --> 01:40:23,230
that we like very much so we depend on

1713
01:40:19,300 --> 01:40:25,630
AI because it's true more and more but

1714
01:40:23,230 --> 01:40:32,769
we have to work on it little by little

1715
01:40:25,630 --> 01:40:35,020
and then this is the series of workshops

1716
01:40:32,770 --> 01:40:37,780
where these topics are being dealt with

1717
01:40:35,020 --> 01:40:44,620
it's about machine learning and IT

1718
01:40:37,780 --> 01:40:47,650
security one has has already taken place

1719
01:40:44,620 --> 01:40:50,320
in November the third of November and

1720
01:40:47,650 --> 01:40:54,759
another one next week it's one of the

1721
01:40:50,320 --> 01:40:58,630
most important conferences for neural

1722
01:40:54,760 --> 01:41:01,890
networks nibs well it's about attack

1723
01:40:58,630 --> 01:41:01,890
defense privacy

1724
01:41:03,100 --> 01:41:12,050
so there's an emphasis that is put on

1725
01:41:07,660 --> 01:41:14,630
not not just security but machine

1726
01:41:12,050 --> 01:41:17,020
learning experts are realizing that

1727
01:41:14,630 --> 01:41:20,870
these algorithms should be protected

1728
01:41:17,020 --> 01:41:24,980
because they are all over the place in

1729
01:41:20,870 --> 01:41:28,790
the future they'll be with us so we need

1730
01:41:24,980 --> 01:41:31,580
to design new ways and today they're

1731
01:41:28,790 --> 01:41:33,290
more theoretical but maybe in some

1732
01:41:31,580 --> 01:41:35,320
months or years they'll be more

1733
01:41:33,290 --> 01:41:42,790
practical we have to take into account

1734
01:41:35,320 --> 01:41:46,160
security for AI and this is a list of

1735
01:41:42,790 --> 01:41:50,450
topics about machine learning what you

1736
01:41:46,160 --> 01:41:53,900
want to learn more this is the detail

1737
01:41:50,450 --> 01:41:59,510
the details of the attacks that we have

1738
01:41:53,900 --> 01:42:02,059
describes here you have courses -

1739
01:41:59,510 --> 01:42:05,420
tutorials about machine load the most

1740
01:42:02,060 --> 01:42:07,250
famous one is done in Stanford they have

1741
01:42:05,420 --> 01:42:10,610
deep learning specialization with five

1742
01:42:07,250 --> 01:42:13,700
courses where the neural neural networks

1743
01:42:10,610 --> 01:42:14,559
are included recurrent convolutional

1744
01:42:13,700 --> 01:42:20,809
networks

1745
01:42:14,560 --> 01:42:22,730
Jude acity more specialized if you want

1746
01:42:20,810 --> 01:42:24,350
to learn more about deep learning or

1747
01:42:22,730 --> 01:42:28,580
machine learning you'll find information

1748
01:42:24,350 --> 01:42:33,140
here in this courses there are

1749
01:42:28,580 --> 01:42:38,800
conferences world level to know what is

1750
01:42:33,140 --> 01:42:41,950
being done and what are the last

1751
01:42:38,800 --> 01:42:45,530
developments in machine learning nips

1752
01:42:41,950 --> 01:42:50,050
2017 ml comes that's more theoretical

1753
01:42:45,530 --> 01:42:53,440
and I see ml that's the companies and

1754
01:42:50,050 --> 01:42:57,620
this is a list of companies that are in

1755
01:42:53,440 --> 01:43:02,960
investing quite a lot of money in AI and

1756
01:42:57,620 --> 01:43:10,269
machine learning and that's all folks

1757
01:43:02,960 --> 01:43:12,740
and if you work in security especially

1758
01:43:10,270 --> 01:43:14,600
global and architectural and design

1759
01:43:12,740 --> 01:43:15,989
security the problem is the appearance

1760
01:43:14,600 --> 01:43:19,170
of new elements

1761
01:43:15,989 --> 01:43:22,860
ai is one of them and to understand the

1762
01:43:19,170 --> 01:43:25,560
tax you need to be specialized so many

1763
01:43:22,860 --> 01:43:33,030
of these courses are basic but this is

1764
01:43:25,560 --> 01:43:34,770
not enough you need specialists for

1765
01:43:33,030 --> 01:43:37,500
instance we deal with security but we

1766
01:43:34,770 --> 01:43:40,140
need experts and work together how to

1767
01:43:37,500 --> 01:43:42,090
and learn how to defend yourself and how

1768
01:43:40,140 --> 01:43:44,370
to attack because you need to attack in

1769
01:43:42,090 --> 01:43:47,280
order to learn how to defend yourself to

1770
01:43:44,370 --> 01:43:49,349
have more robust systems thank you very

1771
01:43:47,280 --> 01:43:52,019
much for your attention I hope this has

1772
01:43:49,350 --> 01:43:54,330
been interesting now we have some

1773
01:43:52,020 --> 01:43:57,180
minutes for questions and answers but

1774
01:43:54,330 --> 01:43:59,910
we'll be there you can write to us by

1775
01:43:57,180 --> 01:44:03,920
email or by Twitter thank you

1776
01:43:59,910 --> 01:44:03,920
[Applause]

1777
01:44:08,830 --> 01:44:10,890
you

