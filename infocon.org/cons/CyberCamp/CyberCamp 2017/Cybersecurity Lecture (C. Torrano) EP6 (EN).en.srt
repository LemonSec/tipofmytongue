1
00:00:19,009 --> 00:00:26,309
good afternoon everybody

2
00:00:21,480 --> 00:00:30,480
thanks very much to Ethan for inviting

3
00:00:26,309 --> 00:00:35,909
me to participate and make this

4
00:00:30,480 --> 00:00:39,480
presentation playing pleased and thieves

5
00:00:35,909 --> 00:00:42,269
to detect anomalies in the network and

6
00:00:39,480 --> 00:00:45,680
in the internet with machine learning my

7
00:00:42,269 --> 00:00:48,420
name is Carlos Turin I hold a piece in

8
00:00:45,680 --> 00:00:56,159
commuting my study de Universidad Carlos

9
00:00:48,420 --> 00:00:58,909
de Madrid and I've done my PhD work in

10
00:00:56,159 --> 00:01:03,239
it artificial intelligence and the

11
00:00:58,909 --> 00:01:06,150
scientific research board and the so

12
00:01:03,239 --> 00:01:10,048
called physic in Spain I currently work

13
00:01:06,150 --> 00:01:13,409
as a senior researcher and 11 pets which

14
00:01:10,049 --> 00:01:15,180
is a Philip Annika cybersecurity unit

15
00:01:13,409 --> 00:01:19,710
and there you have my contact details in

16
00:01:15,180 --> 00:01:21,780
case you want to email me what am I

17
00:01:19,710 --> 00:01:23,640
going to talk to you about today we're

18
00:01:21,780 --> 00:01:25,920
going to talk about many things and also

19
00:01:23,640 --> 00:01:29,100
found machine learning which is very

20
00:01:25,920 --> 00:01:32,850
much topical issue these days so you

21
00:01:29,100 --> 00:01:35,339
probably ask yourself why why are we

22
00:01:32,850 --> 00:01:37,169
talking about machine learning so much

23
00:01:35,340 --> 00:01:40,679
these days one of the reasons behind

24
00:01:37,170 --> 00:01:45,630
this is because we live in a world full

25
00:01:40,679 --> 00:01:46,079
of Satyam we're overwhelmed with data we

26
00:01:45,630 --> 00:01:49,259
have

27
00:01:46,079 --> 00:01:51,389
IOT we have sense disappoint but we

28
00:01:49,259 --> 00:01:53,689
everywhere we have the wearables you

29
00:01:51,389 --> 00:01:57,619
have the water the other smartphones

30
00:01:53,689 --> 00:02:00,329
even a home appliances have sensors cars

31
00:01:57,619 --> 00:02:03,420
everything virtually everything will be

32
00:02:00,329 --> 00:02:06,149
connected and this translates into a

33
00:02:03,420 --> 00:02:09,240
huge amount of info that we will have to

34
00:02:06,149 --> 00:02:11,190
process and store appropriately and it

35
00:02:09,240 --> 00:02:14,080
is here where it's very important that

36
00:02:11,190 --> 00:02:18,160
we talk about Big Data and

37
00:02:14,080 --> 00:02:22,510
of this and needs to be duly processed

38
00:02:18,160 --> 00:02:24,390
we need a great computation capabilities

39
00:02:22,510 --> 00:02:27,130
in order to process all this information

40
00:02:24,390 --> 00:02:30,220
and it is here where the machine

41
00:02:27,130 --> 00:02:32,440
learning will play a role we can use m/l

42
00:02:30,220 --> 00:02:35,320
techniques to process huge amounts of

43
00:02:32,440 --> 00:02:38,880
data because human beings alone cannot

44
00:02:35,320 --> 00:02:42,670
process them we don't have the resources

45
00:02:38,880 --> 00:02:44,829
capabilities to manage such a huge

46
00:02:42,670 --> 00:02:46,929
amounts of data that's why we need

47
00:02:44,830 --> 00:02:49,930
machines to give us a hand and to help

48
00:02:46,930 --> 00:02:52,630
them help us process data the goal of my

49
00:02:49,930 --> 00:02:54,400
presentation is to introduce you to the

50
00:02:52,630 --> 00:02:56,590
world of machine learning tell you how

51
00:02:54,400 --> 00:02:58,720
it works and for you to understand how

52
00:02:56,590 --> 00:03:01,270
we can apply machine learning and how we

53
00:02:58,720 --> 00:03:07,630
can start using machine learning

54
00:03:01,270 --> 00:03:09,760
algorithms there are several multiple

55
00:03:07,630 --> 00:03:12,700
applications to machine learning and one

56
00:03:09,760 --> 00:03:16,570
of them which is well known is anomaly

57
00:03:12,700 --> 00:03:20,738
detection if you allow me this would be

58
00:03:16,570 --> 00:03:24,940
like looking for the red X spotting the

59
00:03:20,739 --> 00:03:27,250
red egg among all the white X I eat to

60
00:03:24,940 --> 00:03:32,070
be able to detect something that stands

61
00:03:27,250 --> 00:03:35,140
out we can profile what we understand by

62
00:03:32,070 --> 00:03:38,019
normal and what commonly happens and

63
00:03:35,140 --> 00:03:42,010
then try to detect anomalies we can

64
00:03:38,020 --> 00:03:45,370
detect malware issues and what sorts of

65
00:03:42,010 --> 00:03:48,690
anomalous behaviors we know what fired

66
00:03:45,370 --> 00:03:48,690
it could be an anomaly

67
00:03:48,790 --> 00:03:56,230
to do so it's part of anomaly detection

68
00:03:52,090 --> 00:03:59,709
I'm going to focus on mmm anomaly

69
00:03:56,230 --> 00:04:02,649
detection and network or web traffic and

70
00:03:59,709 --> 00:04:05,409
we have ideas and IPS devices and these

71
00:04:02,650 --> 00:04:07,689
are systems for the detection of

72
00:04:05,409 --> 00:04:11,379
intuitions and I'm also going to talk

73
00:04:07,689 --> 00:04:12,519
about WAFS that work at an application

74
00:04:11,379 --> 00:04:14,078
level how do they work

75
00:04:12,519 --> 00:04:15,730
they are basically positioned between

76
00:04:14,079 --> 00:04:17,168
the customer and the server that we want

77
00:04:15,730 --> 00:04:20,259
to protect and basically what these

78
00:04:17,168 --> 00:04:24,270
systems to us monitor all the traffic so

79
00:04:20,259 --> 00:04:29,280
when an anomalous behavior occurs it can

80
00:04:24,270 --> 00:04:32,849
be detected let me continue with

81
00:04:29,280 --> 00:04:35,440
metaphors and as I was saying and these

82
00:04:32,849 --> 00:04:37,570
systems would be like watchdog sir

83
00:04:35,440 --> 00:04:40,930
policeman we have many police officers

84
00:04:37,570 --> 00:04:44,349
here and this venue and it's almost like

85
00:04:40,930 --> 00:04:47,590
a bouncer these people are trying to see

86
00:04:44,349 --> 00:04:51,039
or are on the lookout for the good guys

87
00:04:47,590 --> 00:04:53,679
and the bad guys were the bad guys are

88
00:04:51,039 --> 00:04:56,860
to stop them or were the good guys

89
00:04:53,680 --> 00:05:01,210
harder to let them access almost like a

90
00:04:56,860 --> 00:05:03,880
traffic officer even good these systems

91
00:05:01,210 --> 00:05:06,489
can be classified in many different ways

92
00:05:03,880 --> 00:05:08,860
depending on different variables among

93
00:05:06,490 --> 00:05:13,719
them functionality on the one hand we

94
00:05:08,860 --> 00:05:16,569
have the ideas and the ideas

95
00:05:13,719 --> 00:05:20,139
issue an alarm or an alert whenever an

96
00:05:16,569 --> 00:05:24,429
anomaly is detected an alarm is emitted

97
00:05:20,139 --> 00:05:26,619
and the IPS act in a more proactive

98
00:05:24,429 --> 00:05:28,989
manner basically what they do is

99
00:05:26,619 --> 00:05:32,619
interrupt traffic whenever an anomaly is

100
00:05:28,989 --> 00:05:34,388
detected an anomaly the traffic sorry is

101
00:05:32,619 --> 00:05:37,329
kept and this can be useful in some

102
00:05:34,389 --> 00:05:43,599
cases but it can also be problematic in

103
00:05:37,329 --> 00:05:48,969
other cases and then we have the systems

104
00:05:43,599 --> 00:05:53,128
that protect a specific host and n IDS's

105
00:05:48,969 --> 00:05:56,610
that protect the traffic circulates

106
00:05:53,129 --> 00:05:59,800
through the net and the other

107
00:05:56,610 --> 00:06:01,599
classification that is SS based on the

108
00:05:59,800 --> 00:06:03,579
approach adopted thee we have the

109
00:06:01,599 --> 00:06:05,949
signature approach that has been used

110
00:06:03,579 --> 00:06:08,379
commonly during the past years basically

111
00:06:05,949 --> 00:06:11,349
it collects the unknown malware

112
00:06:08,379 --> 00:06:15,849
signatures but the problem you're facing

113
00:06:11,349 --> 00:06:20,860
here is of a different nature problem is

114
00:06:15,849 --> 00:06:23,050
that you don't adapt well to variations

115
00:06:20,860 --> 00:06:27,089
in anomalies for example whenever we

116
00:06:23,050 --> 00:06:31,659
have mutations or when there are new

117
00:06:27,089 --> 00:06:34,089
behaviors that are not registered in the

118
00:06:31,659 --> 00:06:36,039
form of signatures and therefore go

119
00:06:34,089 --> 00:06:37,839
undetected considering the great amount

120
00:06:36,039 --> 00:06:40,659
of variability it's very important to be

121
00:06:37,839 --> 00:06:43,239
able to react to all these threats and

122
00:06:40,659 --> 00:06:45,009
to do so we have the so called anomaly

123
00:06:43,239 --> 00:06:48,369
approach which is a positive approach

124
00:06:45,009 --> 00:06:50,800
because what it does is to define a

125
00:06:48,369 --> 00:06:53,469
profile of normal behavior and all

126
00:06:50,800 --> 00:06:55,090
deviations or divergence are considered

127
00:06:53,469 --> 00:06:58,659
to be anomalies

128
00:06:55,090 --> 00:07:01,659
this allows us to detect all those

129
00:06:58,660 --> 00:07:04,449
unexpected behaviors that haven't been

130
00:07:01,660 --> 00:07:09,370
considered in the past with a signature

131
00:07:04,449 --> 00:07:13,590
approach and we commonly use machine

132
00:07:09,370 --> 00:07:16,960
learning tools when applying these

133
00:07:13,590 --> 00:07:21,400
approach machine learning algorithms

134
00:07:16,960 --> 00:07:23,919
work very well in detecting patterns

135
00:07:21,400 --> 00:07:26,770
when we want to detect patterns and data

136
00:07:23,919 --> 00:07:28,599
or paterson behavior you need to

137
00:07:26,770 --> 00:07:31,060
understand that these algorithms have

138
00:07:28,600 --> 00:07:33,690
been designed to identify patterns to

139
00:07:31,060 --> 00:07:37,300
characterize them and to detect all

140
00:07:33,690 --> 00:07:42,370
deviations or anomalies from these to

141
00:07:37,300 --> 00:07:45,490
find patterns on this side I have wanted

142
00:07:42,370 --> 00:07:48,100
to explain to you the process follow to

143
00:07:45,490 --> 00:07:51,430
apply a machine learning algorithm to

144
00:07:48,100 --> 00:07:54,610
solve a specific problem in this case to

145
00:07:51,430 --> 00:07:56,530
detect anomalies but here you can see

146
00:07:54,610 --> 00:08:00,400
the different steps of the process that

147
00:07:56,530 --> 00:08:03,638
need to be followed first we have a data

148
00:08:00,400 --> 00:08:05,440
set basically the ensembl data that

149
00:08:03,639 --> 00:08:08,410
we're going to use in our system in

150
00:08:05,440 --> 00:08:11,430
order to feed the system and these are

151
00:08:08,410 --> 00:08:14,260
the data without were going to analyze

152
00:08:11,430 --> 00:08:18,190
and it can be appropriate perhaps to

153
00:08:14,260 --> 00:08:20,789
have a pre-processing stage regarding

154
00:08:18,190 --> 00:08:25,599
this data and then we will extract or

155
00:08:20,789 --> 00:08:27,700
the features so what we do is extract or

156
00:08:25,599 --> 00:08:30,550
characterize all those features that

157
00:08:27,700 --> 00:08:33,718
allow us to profile the normal behavior

158
00:08:30,550 --> 00:08:37,390
and therefore this will allow us to

159
00:08:33,719 --> 00:08:39,669
detect deviations at this stage and the

160
00:08:37,390 --> 00:08:43,409
work of the analyst is very important

161
00:08:39,669 --> 00:08:45,939
because he or she will help us know more

162
00:08:43,409 --> 00:08:49,329
features characterize normally

163
00:08:45,940 --> 00:08:51,550
supposed to unexpected behavior and in

164
00:08:49,330 --> 00:08:56,170
some domains it's very important to have

165
00:08:51,550 --> 00:08:58,209
the expert knowledge or expertise in

166
00:08:56,170 --> 00:08:59,860
that particular field you need to

167
00:08:58,210 --> 00:09:03,190
consider that there can be highly

168
00:08:59,860 --> 00:09:05,860
complex problems with many variables or

169
00:09:03,190 --> 00:09:08,620
features and in terms of computational

170
00:09:05,860 --> 00:09:11,110
performance this can have a negative

171
00:09:08,620 --> 00:09:14,380
impact on the system to streamline this

172
00:09:11,110 --> 00:09:16,750
we can select some features ie you can

173
00:09:14,380 --> 00:09:20,170
choose those features that really

174
00:09:16,750 --> 00:09:23,260
provide relevant info and that are not

175
00:09:20,170 --> 00:09:25,510
redundant that don't disturb us so to

176
00:09:23,260 --> 00:09:31,660
say so that we can make this process

177
00:09:25,510 --> 00:09:35,890
much more agile then we move on to the

178
00:09:31,660 --> 00:09:39,189
training stage when the algorithm learns

179
00:09:35,890 --> 00:09:41,530
it's like when we human beings read a

180
00:09:39,190 --> 00:09:43,950
book or study a book and we learn we

181
00:09:41,530 --> 00:09:46,750
acquire all the knowledge required

182
00:09:43,950 --> 00:09:48,670
regarding what is normal and that's when

183
00:09:46,750 --> 00:09:54,760
we extract all the patterns that will

184
00:09:48,670 --> 00:09:57,849
help us characterize and then we move on

185
00:09:54,760 --> 00:10:00,400
to the test stage in order to verify and

186
00:09:57,850 --> 00:10:02,560
shake how well our system has learned

187
00:10:00,400 --> 00:10:05,079
this and what the capabilities for the

188
00:10:02,560 --> 00:10:08,469
system are or this is Ammar to detect

189
00:10:05,080 --> 00:10:12,300
unexpected behaviors unseen behaviors

190
00:10:08,470 --> 00:10:16,450
and last we conduct our results analysis

191
00:10:12,300 --> 00:10:18,400
to know to what extent the algorithm has

192
00:10:16,450 --> 00:10:21,990
been accurate and the detection of

193
00:10:18,400 --> 00:10:25,480
anomalies and we can even consider reap

194
00:10:21,990 --> 00:10:28,020
iterating and fine-tuning the stages in

195
00:10:25,480 --> 00:10:32,440
order to be able to improve the results

196
00:10:28,020 --> 00:10:36,460
if needed my presentation is divided

197
00:10:32,440 --> 00:10:40,480
into a series of sections and I'm going

198
00:10:36,460 --> 00:10:43,570
to try to analyze in further detail that

199
00:10:40,480 --> 00:10:46,150
components of this flow chart so let's

200
00:10:43,570 --> 00:10:49,260
begin with the data say that data said

201
00:10:46,150 --> 00:10:52,689
that we can want to use

202
00:10:49,260 --> 00:10:56,170
visit itself is a challenge at least for

203
00:10:52,690 --> 00:10:58,230
the time being I've included this quote

204
00:10:56,170 --> 00:11:00,400
from authors that say that the most

205
00:10:58,230 --> 00:11:04,530
insignificant challenge that an

206
00:11:00,400 --> 00:11:08,110
evaluation faces is the lack of

207
00:11:04,530 --> 00:11:11,800
appropriate public datasets for

208
00:11:08,110 --> 00:11:15,850
assessing anomaly detection systems so

209
00:11:11,800 --> 00:11:18,160
we need to have a public datasets that

210
00:11:15,850 --> 00:11:20,950
are labeled and that is very costly is

211
00:11:18,160 --> 00:11:23,439
very difficult and we need to consider

212
00:11:20,950 --> 00:11:28,030
it there the datasets will be essential

213
00:11:23,440 --> 00:11:29,860
because this will be what will feed the

214
00:11:28,030 --> 00:11:34,870
algorithm so we need to be able to work

215
00:11:29,860 --> 00:11:39,460
with adequate data since this is a table

216
00:11:34,870 --> 00:11:43,600
that are prepared for my research and I

217
00:11:39,460 --> 00:11:48,130
extracted the parameters that the data

218
00:11:43,600 --> 00:11:51,780
says sheered have well they can be

219
00:11:48,130 --> 00:11:54,220
public or not this can be convenient

220
00:11:51,780 --> 00:11:58,750
when you wish to compare different

221
00:11:54,220 --> 00:12:00,340
systems if we're going to use supervised

222
00:11:58,750 --> 00:12:02,380
machine learning algorithms it's

223
00:12:00,340 --> 00:12:04,450
important and that they are labeled or

224
00:12:02,380 --> 00:12:07,510
having updated attacks because some of

225
00:12:04,450 --> 00:12:12,370
the data sets are criticized because

226
00:12:07,510 --> 00:12:17,550
they have very old attacks that don't

227
00:12:12,370 --> 00:12:21,250
really help us assess systems well

228
00:12:17,550 --> 00:12:25,209
effectively so there are some data sets

229
00:12:21,250 --> 00:12:29,500
that are out there that can be used but

230
00:12:25,210 --> 00:12:34,380
as I was saying there are always some

231
00:12:29,500 --> 00:12:37,450
drawbacks some pitiful some

232
00:12:34,380 --> 00:12:40,320
inconveniences to them so one of the

233
00:12:37,450 --> 00:12:43,060
options is to use the existing data sets

234
00:12:40,320 --> 00:12:44,000
sometimes what happens is that they have

235
00:12:43,060 --> 00:12:46,369
synthetic

236
00:12:44,000 --> 00:12:52,910
data and another option is to create our

237
00:12:46,370 --> 00:12:56,600
own data set data sets depending on the

238
00:12:52,910 --> 00:13:00,050
lab layer we're working on could be HTTP

239
00:12:56,600 --> 00:13:05,300
if we're in the application layer or we

240
00:13:00,050 --> 00:13:07,849
get can work at layer 3 I network

241
00:13:05,300 --> 00:13:09,859
traffic we can even work on traffic

242
00:13:07,850 --> 00:13:13,420
statistics considering the high volume

243
00:13:09,860 --> 00:13:17,150
of data network

244
00:13:13,420 --> 00:13:19,430
NetFlow is a protocol commonly used

245
00:13:17,150 --> 00:13:22,819
protocol for the detection of anomalies

246
00:13:19,430 --> 00:13:26,959
and that worker traffic it was developed

247
00:13:22,820 --> 00:13:31,750
by Cisco in 1996 we're currently at

248
00:13:26,960 --> 00:13:37,010
version 9 that specified in RCC 39:54

249
00:13:31,750 --> 00:13:39,920
natural basically collects per compiled

250
00:13:37,010 --> 00:13:43,340
statem regarding IP traffic when it

251
00:13:39,920 --> 00:13:46,459
enters or leaves the interface it does

252
00:13:43,340 --> 00:13:51,800
stats it defines how the route or

253
00:13:46,460 --> 00:13:53,870
exports data there are many different

254
00:13:51,800 --> 00:13:55,880
fields and NetFlow but i've brought some

255
00:13:53,870 --> 00:14:00,500
for you to see and understand what we're

256
00:13:55,880 --> 00:14:02,990
talking about for example data regarding

257
00:14:00,500 --> 00:14:06,950
the date and hour of the beginning of

258
00:14:02,990 --> 00:14:09,370
the film that's in asia and IP ports of

259
00:14:06,950 --> 00:14:13,339
origin and destination type of particle

260
00:14:09,370 --> 00:14:15,740
the interface and the bytes are a number

261
00:14:13,339 --> 00:14:18,110
of packages and both incoming and

262
00:14:15,740 --> 00:14:21,650
outgoing and this are some of the data

263
00:14:18,110 --> 00:14:25,460
with which an inner flow works in this

264
00:14:21,650 --> 00:14:31,339
case we have working with at say ross

265
00:14:25,460 --> 00:14:35,810
crude data and this has pros and cons as

266
00:14:31,339 --> 00:14:38,900
we will see shortly these systems have

267
00:14:35,810 --> 00:14:41,660
three main components first we have an

268
00:14:38,900 --> 00:14:43,910
exporter basically what it does is

269
00:14:41,660 --> 00:14:46,870
aggregate all of these packages and

270
00:14:43,910 --> 00:14:49,670
several flows and exports these i'm

271
00:14:46,870 --> 00:14:50,750
packages to collectors and the

272
00:14:49,670 --> 00:14:55,040
collectors

273
00:14:50,750 --> 00:14:59,210
see this data and store and the analyzer

274
00:14:55,040 --> 00:15:03,319
then analyzes this info using the

275
00:14:59,210 --> 00:15:04,910
processes that we have defined pros of

276
00:15:03,320 --> 00:15:08,660
using net fault

277
00:15:04,910 --> 00:15:12,040
instead of let's say traffic as it is as

278
00:15:08,660 --> 00:15:16,449
it says well it's lied it's Universal

279
00:15:12,040 --> 00:15:20,890
and it's adopted as a standard and many

280
00:15:16,450 --> 00:15:24,710
devices from most important

281
00:15:20,890 --> 00:15:28,790
manufacturers among them fiscal Jupiter

282
00:15:24,710 --> 00:15:31,340
they have old integrated NetFlow so it

283
00:15:28,790 --> 00:15:33,650
serves this purses purpose very well

284
00:15:31,340 --> 00:15:36,020
when talking about putting into dialogue

285
00:15:33,650 --> 00:15:40,250
different devices one of the main

286
00:15:36,020 --> 00:15:42,380
advantages of NetFlow is how they

287
00:15:40,250 --> 00:15:44,890
abstract from detail see if we consider

288
00:15:42,380 --> 00:15:47,390
the huge volume of data that can

289
00:15:44,890 --> 00:15:50,750
circulate through a network it's pretty

290
00:15:47,390 --> 00:15:53,720
hard to see what is happening so in a

291
00:15:50,750 --> 00:15:55,339
snapshot very fast you can see and

292
00:15:53,720 --> 00:15:59,240
analyze what is happening in the network

293
00:15:55,339 --> 00:16:01,520
for example if it's used in a strange

294
00:15:59,240 --> 00:16:04,490
time of the day where there is some sort

295
00:16:01,520 --> 00:16:07,760
of attack with Ninfa we analyze all this

296
00:16:04,490 --> 00:16:12,190
data we were able to detect this very

297
00:16:07,760 --> 00:16:14,330
easily and aside from this I can help us

298
00:16:12,190 --> 00:16:18,740
regarding privacy or confidentiality

299
00:16:14,330 --> 00:16:26,030
issues to make sure that no personal

300
00:16:18,740 --> 00:16:27,370
data are disseminated and we believe it

301
00:16:26,030 --> 00:16:32,680
can be used

302
00:16:27,370 --> 00:16:36,130
useful to flag alerts in level 1 but

303
00:16:32,680 --> 00:16:38,229
thanks to this detailed obstruction well

304
00:16:36,130 --> 00:16:41,110
this is an advantage as I said a prophet

305
00:16:38,230 --> 00:16:43,420
also O'Connor or disadvantage because in

306
00:16:41,110 --> 00:16:48,070
net foe we would be missing a great

307
00:16:43,420 --> 00:16:49,810
amount of info and some attacks if they

308
00:16:48,070 --> 00:16:52,660
can only be detected with the

309
00:16:49,810 --> 00:16:54,939
application layers data they would go

310
00:16:52,660 --> 00:16:57,910
undetected this way we wouldn't have a

311
00:16:54,940 --> 00:17:00,850
sufficient information or data to detect

312
00:16:57,910 --> 00:17:05,500
them so it can be useful in some cases

313
00:17:00,850 --> 00:17:08,560
to fine-tune this with dpi with a more

314
00:17:05,500 --> 00:17:10,750
detailed inspection of packages for some

315
00:17:08,560 --> 00:17:19,929
times where the features of the

316
00:17:10,750 --> 00:17:22,829
application layer your sexist and open

317
00:17:19,930 --> 00:17:27,069
most amazing family car nose you see

318
00:17:22,829 --> 00:17:30,690
Thomas first option now if we choose

319
00:17:27,069 --> 00:17:33,940
this option we will have

320
00:17:30,690 --> 00:17:37,480
legitimate traffic which is background

321
00:17:33,940 --> 00:17:40,240
and then we need the anomalies that will

322
00:17:37,480 --> 00:17:44,710
be detected so we're going to see how

323
00:17:40,240 --> 00:17:51,010
this is implemented I'm going to focus

324
00:17:44,710 --> 00:17:53,770
now in the legitimate traffic in a

325
00:17:51,010 --> 00:17:57,790
network that is free of attacks we would

326
00:17:53,770 --> 00:18:00,340
do it with the following scheme based on

327
00:17:57,790 --> 00:18:03,310
the switch we would do a mirroring of

328
00:18:00,340 --> 00:18:09,899
the traffic as is the case in some ideas

329
00:18:03,310 --> 00:18:14,240
and this will send the traffic to the

330
00:18:09,900 --> 00:18:18,440
NetFlow prove then we can handle

331
00:18:14,240 --> 00:18:21,890
NetFlow traffic to it device in the

332
00:18:18,440 --> 00:18:24,640
cloud and then we can start the data

333
00:18:21,890 --> 00:18:27,130
science and machine learning process

334
00:18:24,640 --> 00:18:30,910
reporter necesitamos tambien locusts

335
00:18:27,130 --> 00:18:37,220
malware then we need the malware or

336
00:18:30,910 --> 00:18:41,210
anomalies one of the options is to use

337
00:18:37,220 --> 00:18:45,910
real malware samples in a controlled

338
00:18:41,210 --> 00:18:48,740
environment if we don't have these

339
00:18:45,910 --> 00:18:52,400
samples another possibility is to

340
00:18:48,740 --> 00:18:55,340
download some as you can see in this

341
00:18:52,400 --> 00:18:59,290
screen the name is malware traffic

342
00:18:55,340 --> 00:19:04,280
analysis I don't know if you know this

343
00:18:59,290 --> 00:19:08,649
web page but it has a dated structure

344
00:19:04,280 --> 00:19:12,440
that links different types of malware

345
00:19:08,650 --> 00:19:15,530
explaining how it can be detected or the

346
00:19:12,440 --> 00:19:18,610
anomalous behavior and the take ups can

347
00:19:15,530 --> 00:19:21,830
be directly downloaded from this website

348
00:19:18,610 --> 00:19:24,290
this could be a source of information to

349
00:19:21,830 --> 00:19:28,689
get the malware in order to test our

350
00:19:24,290 --> 00:19:32,330
system this is just an example of

351
00:19:28,690 --> 00:19:36,220
downloaded my well said oh darling and

352
00:19:32,330 --> 00:19:38,570
he's a group of malware that finally

353
00:19:36,220 --> 00:19:42,260
starts ransomware

354
00:19:38,570 --> 00:19:46,629
if with the open the page with the words

355
00:19:42,260 --> 00:19:51,559
that the traces of this malware flow

356
00:19:46,630 --> 00:19:56,360
indicates that from the same ip we have

357
00:19:51,559 --> 00:20:01,700
consecutive destination IDs which give

358
00:19:56,360 --> 00:20:05,750
us a clue to detect the malware and to

359
00:20:01,700 --> 00:20:09,980
set the features that will allow us to

360
00:20:05,750 --> 00:20:12,770
detect anomalies in our system so an

361
00:20:09,980 --> 00:20:16,010
analyst once in this screen could see

362
00:20:12,770 --> 00:20:20,389
there is not a normal trace of the Nets

363
00:20:16,010 --> 00:20:24,770
flow or traffic this

364
00:20:20,390 --> 00:20:27,920
our conservative IPS happened in only a

365
00:20:24,770 --> 00:20:35,799
second all of these is data that can be

366
00:20:27,920 --> 00:20:39,440
used to set the system's specificities

367
00:20:35,799 --> 00:20:42,260
in the pre-processing step

368
00:20:39,440 --> 00:20:45,980
we might need several actions for

369
00:20:42,260 --> 00:20:50,299
example making data anonymous erasing

370
00:20:45,980 --> 00:20:55,070
some null data that can interfere with

371
00:20:50,299 --> 00:20:59,120
the algorithm and also this great

372
00:20:55,070 --> 00:21:01,659
ization some of the algorithm cannot

373
00:20:59,120 --> 00:21:05,239
work with long streams or with

374
00:21:01,660 --> 00:21:08,360
alphanumeric chains and we need to do

375
00:21:05,240 --> 00:21:11,299
the conversion into numeric data behind

376
00:21:08,360 --> 00:21:14,770
the machine learning algorithm we only

377
00:21:11,299 --> 00:21:18,460
have numbers and mathematics is not

378
00:21:14,770 --> 00:21:18,460
magic by all means

379
00:21:18,880 --> 00:21:28,220
in this step we would get features

380
00:21:22,820 --> 00:21:31,668
extraction that will allow us to see

381
00:21:28,220 --> 00:21:35,000
what's normal from validation and in

382
00:21:31,669 --> 00:21:39,620
order to illustrate the importance of

383
00:21:35,000 --> 00:21:42,679
this I present you a quote by Pedro

384
00:21:39,620 --> 00:21:45,559
Domingues a researcher and a professor

385
00:21:42,679 --> 00:21:48,890
at Washington University that says the

386
00:21:45,559 --> 00:21:51,559
following at the end of the day some

387
00:21:48,890 --> 00:21:57,460
machine learning projects succeed and

388
00:21:51,559 --> 00:22:01,250
some fail what makes the difference as

389
00:21:57,460 --> 00:22:04,880
researchers or experts in those areas

390
00:22:01,250 --> 00:22:07,520
sometimes we wonder what makes the

391
00:22:04,880 --> 00:22:11,240
difference what made a process

392
00:22:07,520 --> 00:22:15,110
successful or a failure his answer is

393
00:22:11,240 --> 00:22:17,830
that easily the most important factor is

394
00:22:15,110 --> 00:22:22,149
the features used

395
00:22:17,830 --> 00:22:26,199
this is what will allow us to learn with

396
00:22:22,149 --> 00:22:28,500
the algorithm being able to distinguish

397
00:22:26,200 --> 00:22:30,970
an attack from what is not

398
00:22:28,500 --> 00:22:35,529
he's also important to highlight that

399
00:22:30,970 --> 00:22:40,240
during this stage most of the effort is

400
00:22:35,529 --> 00:22:43,149
devoted around the 80% of the projects

401
00:22:40,240 --> 00:22:47,010
effort is invested in the data science

402
00:22:43,149 --> 00:22:52,320
stage everything related to data

403
00:22:47,010 --> 00:22:55,570
manipulation fine tuning fitting

404
00:22:52,320 --> 00:22:59,879
constructing the necessary columns so

405
00:22:55,570 --> 00:23:05,168
this is a very important step therefore

406
00:22:59,880 --> 00:23:07,090
now in order to extract the features we

407
00:23:05,169 --> 00:23:11,139
should take into account different

408
00:23:07,090 --> 00:23:14,980
factors here the analyst role is

409
00:23:11,139 --> 00:23:18,729
essential there are a few things that

410
00:23:14,980 --> 00:23:22,450
give us information about where can we

411
00:23:18,730 --> 00:23:25,149
start detecting anomalies internet if an

412
00:23:22,450 --> 00:23:29,769
element communicates in a very brief

413
00:23:25,149 --> 00:23:33,580
period of time communicates with many

414
00:23:29,769 --> 00:23:36,549
different public eyepiece on the other

415
00:23:33,580 --> 00:23:39,960
hand they will work in a more detailed

416
00:23:36,549 --> 00:23:44,918
level we should take into account a URL

417
00:23:39,960 --> 00:23:46,990
length and structure we can see if they

418
00:23:44,919 --> 00:23:52,779
have run files

419
00:23:46,990 --> 00:23:56,049
many of the malware downloader excel

420
00:23:52,779 --> 00:23:59,889
files we can see whether there are

421
00:23:56,049 --> 00:24:01,779
anomalous parameters that are important

422
00:23:59,889 --> 00:24:04,120
to take into account in relation to

423
00:24:01,779 --> 00:24:08,470
attacks and we can analyze the

424
00:24:04,120 --> 00:24:12,370
parameters of the URL with one of the

425
00:24:08,470 --> 00:24:16,320
proposals we made in a paper take into

426
00:24:12,370 --> 00:24:19,689
account the characters distribution and

427
00:24:16,320 --> 00:24:22,090
the rates of letters digits and non

428
00:24:19,690 --> 00:24:24,399
alphanumeric characters that will help

429
00:24:22,090 --> 00:24:27,289
us to differentiate between

430
00:24:24,399 --> 00:24:30,070
an attack from what is not those are

431
00:24:27,289 --> 00:24:30,070
just a few examples

432
00:24:33,669 --> 00:24:43,190
after features obstruction and depending

433
00:24:40,279 --> 00:24:47,600
on the problem some of them are very

434
00:24:43,190 --> 00:24:52,880
complex we speak about hundreds of

435
00:24:47,600 --> 00:24:55,100
features we need to select the most

436
00:24:52,880 --> 00:24:58,850
important ones those that are really

437
00:24:55,100 --> 00:25:01,668
relevant and provide information to sort

438
00:24:58,850 --> 00:25:04,428
out our problems so everything that is

439
00:25:01,669 --> 00:25:09,220
relevant or redundant or my half

440
00:25:04,429 --> 00:25:14,149
correlations is not needed we have to

441
00:25:09,220 --> 00:25:16,610
erase it to improve our performance when

442
00:25:14,149 --> 00:25:19,699
we don't have to process such a huge

443
00:25:16,610 --> 00:25:24,258
amount of data the algorithm will work

444
00:25:19,700 --> 00:25:26,330
better and in some locations we can just

445
00:25:24,259 --> 00:25:31,340
play and see what happens with the

446
00:25:26,330 --> 00:25:35,899
results here the challenge is to know

447
00:25:31,340 --> 00:25:37,730
how much we should erase if we raise too

448
00:25:35,899 --> 00:25:43,610
much we might not sort out the problem

449
00:25:37,730 --> 00:25:46,700
so we should see one two arrays and

450
00:25:43,610 --> 00:25:49,158
water the core features that will

451
00:25:46,700 --> 00:25:52,129
provide the solution to our problem so

452
00:25:49,159 --> 00:25:54,200
the challenge is to have the fewer

453
00:25:52,129 --> 00:25:58,240
number of features that will allow to

454
00:25:54,200 --> 00:26:04,820
sort out our problem without impacting

455
00:25:58,240 --> 00:26:08,840
to our results I've seen cases where the

456
00:26:04,820 --> 00:26:11,299
algorithmic properly functioning helped

457
00:26:08,840 --> 00:26:14,379
us to improve our results so he would be

458
00:26:11,299 --> 00:26:14,379
a win-win

459
00:26:15,570 --> 00:26:25,450
those feature selection algorithm are of

460
00:26:20,919 --> 00:26:29,019
three types first the rapper's they use

461
00:26:25,450 --> 00:26:32,230
the power of a classifier or to see

462
00:26:29,019 --> 00:26:38,980
whether a features be included or not

463
00:26:32,230 --> 00:26:42,510
the drawback I worried is that they are

464
00:26:38,980 --> 00:26:46,830
less efficient and need more time

465
00:26:42,510 --> 00:26:49,899
those that are based in filters they are

466
00:26:46,830 --> 00:26:53,500
independent from the classifier they use

467
00:26:49,899 --> 00:26:56,649
different techniques under four consume

468
00:26:53,500 --> 00:27:01,330
less resources and thirdly the hybrids

469
00:26:56,649 --> 00:27:06,279
that will be built during the training

470
00:27:01,330 --> 00:27:13,090
stage of the classifier they are midway

471
00:27:06,279 --> 00:27:17,159
between the two first about ML algorithm

472
00:27:13,090 --> 00:27:21,699
there are many he's incredible I

473
00:27:17,159 --> 00:27:23,740
discovered new ones every day and the

474
00:27:21,700 --> 00:27:27,399
whole field is very interesting there

475
00:27:23,740 --> 00:27:32,350
are different types here you have some

476
00:27:27,399 --> 00:27:36,969
of the best known as the K nearest

477
00:27:32,350 --> 00:27:39,760
neighbors SVM decision trees rather

478
00:27:36,970 --> 00:27:42,250
neuronal s cluster in isolation forests

479
00:27:39,760 --> 00:27:46,950
and for a wide range of Albury and each

480
00:27:42,250 --> 00:27:52,360
work differently and they are based on

481
00:27:46,950 --> 00:27:55,809
mathematical criteria when we speak

482
00:27:52,360 --> 00:27:59,529
about the machine learning how to make

483
00:27:55,809 --> 00:28:02,760
machines to learn I'm sure you've seen

484
00:27:59,529 --> 00:28:06,210
or heard about supervised and

485
00:28:02,760 --> 00:28:09,100
unsupervised algorithm

486
00:28:06,210 --> 00:28:13,570
those that are supervisor are used when

487
00:28:09,100 --> 00:28:16,539
we have a label for our data group that

488
00:28:13,570 --> 00:28:22,320
means all data set when we're detecting

489
00:28:16,539 --> 00:28:25,510
anomalies if we have the network traffic

490
00:28:22,320 --> 00:28:29,139
we might not either because it's done by

491
00:28:25,510 --> 00:28:32,470
a human or by any other means we know

492
00:28:29,140 --> 00:28:34,990
the classification or Sultan we know

493
00:28:32,470 --> 00:28:38,650
there is something common I in the

494
00:28:34,990 --> 00:28:42,370
network orifice an anomaly or my world

495
00:28:38,650 --> 00:28:45,460
if we don't have the label then we go to

496
00:28:42,370 --> 00:28:48,428
non supervised algorithm are those that

497
00:28:45,460 --> 00:28:51,390
without this provisional can help us

498
00:28:48,429 --> 00:28:51,390
process the information

499
00:28:51,720 --> 00:28:59,559
now this graph about machine learning

500
00:28:56,710 --> 00:29:03,159
algorithm is here in this presentation

501
00:28:59,559 --> 00:29:07,840
because we might think where do I start

502
00:29:03,159 --> 00:29:11,230
I don't really know how to do it so I

503
00:29:07,840 --> 00:29:14,289
thought that this graph was quite

504
00:29:11,230 --> 00:29:19,809
interesting in order to give you an idea

505
00:29:14,289 --> 00:29:23,770
that on the first site to know what you

506
00:29:19,809 --> 00:29:28,678
should do first is to choose machine

507
00:29:23,770 --> 00:29:32,020
learning algorithm that is from excuse

508
00:29:28,679 --> 00:29:35,940
learn from Python first Wizards in the

509
00:29:32,020 --> 00:29:39,100
number of samples and depending on

510
00:29:35,940 --> 00:29:41,700
whether we're gonna work with categories

511
00:29:39,100 --> 00:29:45,820
or with quantities we would be a

512
00:29:41,700 --> 00:29:48,700
regression approach or classification

513
00:29:45,820 --> 00:29:50,799
one and there are different algorithms

514
00:29:48,700 --> 00:29:53,080
there are different criteria use but

515
00:29:50,799 --> 00:29:55,600
here you have a few examples then

516
00:29:53,080 --> 00:29:59,189
depending on whether we have the labels

517
00:29:55,600 --> 00:30:04,480
or not we would be working with levels

518
00:29:59,190 --> 00:30:05,179
in or we can use clustering algorithm

519
00:30:04,480 --> 00:30:07,729
that

520
00:30:05,179 --> 00:30:11,479
we'll commence for example is well known

521
00:30:07,729 --> 00:30:16,399
will allow us to do the Sortino without

522
00:30:11,479 --> 00:30:19,070
the labels or we can use size reduction

523
00:30:16,399 --> 00:30:23,449
algorithm which are related to the

524
00:30:19,070 --> 00:30:26,119
feature selection among all machine

525
00:30:23,450 --> 00:30:30,409
learning algorithm I will speak about

526
00:30:26,119 --> 00:30:33,668
one in particular is quite new and the

527
00:30:30,409 --> 00:30:40,399
name is isolation forest this is an

528
00:30:33,669 --> 00:30:43,700
unsupervised algorithm usually used for

529
00:30:40,399 --> 00:30:48,758
anomalies detection to detect all those

530
00:30:43,700 --> 00:30:51,589
anomalies is proven to be quite good

531
00:30:48,759 --> 00:30:57,379
we're gonna say it's a structure in

532
00:30:51,589 --> 00:31:01,029
broad terms the main idea is that

533
00:30:57,379 --> 00:31:04,789
anomalous data are easier to isolate

534
00:31:01,029 --> 00:31:08,869
through the recursive partitioning of

535
00:31:04,789 --> 00:31:11,599
datasets we choose at random one of the

536
00:31:08,869 --> 00:31:15,379
variables of the data set and also

537
00:31:11,599 --> 00:31:19,908
randomly some limit values so the data

538
00:31:15,379 --> 00:31:23,478
set becomes divided we have different

539
00:31:19,909 --> 00:31:27,830
lines that will divide the data set into

540
00:31:23,479 --> 00:31:30,559
parts we might end up designing the tree

541
00:31:27,830 --> 00:31:35,119
to your right on the second round we

542
00:31:30,559 --> 00:31:38,269
will build the second line or use the

543
00:31:35,119 --> 00:31:40,879
second variable in such a way that we

544
00:31:38,269 --> 00:31:46,279
can build the tree it's based on

545
00:31:40,879 --> 00:31:49,428
decision making trees when we make the

546
00:31:46,279 --> 00:31:54,830
partition of the data set we see that a

547
00:31:49,429 --> 00:31:57,919
that was an anomaly was isolated and is

548
00:31:54,830 --> 00:32:01,059
based on the idea that anomalous data

549
00:31:57,919 --> 00:32:03,940
are easier to isolate the normal data

550
00:32:01,059 --> 00:32:07,120
normal data will require

551
00:32:03,940 --> 00:32:10,179
higher number of cuts and based on the

552
00:32:07,120 --> 00:32:14,139
depth of the tree that makes reference

553
00:32:10,179 --> 00:32:18,909
to the number of times we've divided we

554
00:32:14,139 --> 00:32:21,668
can see with parameters are anomalies or

555
00:32:18,909 --> 00:32:25,659
not and then we will use some other

556
00:32:21,669 --> 00:32:29,019
processes as contamination then we get

557
00:32:25,659 --> 00:32:32,500
to the training stage when the machine

558
00:32:29,019 --> 00:32:35,860
learns when the algorithm with all this

559
00:32:32,500 --> 00:32:40,080
information starts learning about the

560
00:32:35,860 --> 00:32:44,769
model that will allow us to predict data

561
00:32:40,080 --> 00:32:47,710
this is the stage before the algorithm

562
00:32:44,769 --> 00:32:48,460
starts detecting before it is in

563
00:32:47,710 --> 00:32:54,580
production

564
00:32:48,460 --> 00:32:57,820
once all patterns are incorporated we

565
00:32:54,580 --> 00:33:01,689
then go to the test stage when the

566
00:32:57,820 --> 00:33:06,039
algorithm is already detecting the Nets

567
00:33:01,690 --> 00:33:09,549
traffic and as I said police officer

568
00:33:06,039 --> 00:33:14,379
those that have a positive pattern can

569
00:33:09,549 --> 00:33:17,110
go and we isolate those that not what we

570
00:33:14,379 --> 00:33:20,769
do here is to analyze cases that haven't

571
00:33:17,110 --> 00:33:22,629
been visible before not to analyze the

572
00:33:20,769 --> 00:33:26,230
same data that we've seen at the

573
00:33:22,629 --> 00:33:28,509
training stage what is interesting of

574
00:33:26,230 --> 00:33:33,879
those algorithm is that they have the

575
00:33:28,509 --> 00:33:36,490
capacity to generalize so to detect data

576
00:33:33,879 --> 00:33:39,668
an anomaly T's that were not seen before

577
00:33:36,490 --> 00:33:43,480
and this is their strength compared to

578
00:33:39,669 --> 00:33:47,980
signatures approach here is where we see

579
00:33:43,480 --> 00:33:52,000
the algorithms predictive capacity we

580
00:33:47,980 --> 00:33:55,059
should be aware of not over adjusting

581
00:33:52,000 --> 00:33:59,500
things this can be seen in this drawing

582
00:33:55,059 --> 00:34:04,740
well we might have data of the blue and

583
00:33:59,500 --> 00:34:06,970
red class the green line if you see

584
00:34:04,740 --> 00:34:09,790
distinguish them

585
00:34:06,970 --> 00:34:14,380
and adjust the data but what is the

586
00:34:09,790 --> 00:34:17,409
problem here when we have it dataset

587
00:34:14,380 --> 00:34:18,790
that is a slightly different without and

588
00:34:17,409 --> 00:34:22,210
Cindy done

589
00:34:18,790 --> 00:34:24,630
the algorithm might make mistakes in the

590
00:34:22,210 --> 00:34:28,179
prediction because she's been over

591
00:34:24,630 --> 00:34:30,670
adjusted to a specific training model so

592
00:34:28,179 --> 00:34:33,279
when new data arrive he will not have an

593
00:34:30,670 --> 00:34:37,480
afternoon rising capacity to detect

594
00:34:33,280 --> 00:34:40,750
anomalies lives in a set that is not

595
00:34:37,480 --> 00:34:43,510
exactly the same as the tray as the one

596
00:34:40,750 --> 00:34:46,780
used during the training stage and here

597
00:34:43,510 --> 00:34:49,750
we have to see that the algorithm us

598
00:34:46,780 --> 00:34:59,710
should learn and make generalizations in

599
00:34:49,750 --> 00:35:02,740
the right way another stage is

600
00:34:59,710 --> 00:35:08,830
cross-validation as I said what is

601
00:35:02,740 --> 00:35:13,060
interesting is to have the capacity for

602
00:35:08,830 --> 00:35:16,509
data detection that had been previously

603
00:35:13,060 --> 00:35:19,529
unseen until evolved over adjustment we

604
00:35:16,510 --> 00:35:23,280
have to divide the data set in two parts

605
00:35:19,530 --> 00:35:26,530
part of the set is used only to train

606
00:35:23,280 --> 00:35:31,050
the algorithm and the other part to test

607
00:35:26,530 --> 00:35:35,320
it and evaluation is about data that has

608
00:35:31,050 --> 00:35:38,140
been previously seen another approach is

609
00:35:35,320 --> 00:35:41,530
the use validation data divide the

610
00:35:38,140 --> 00:35:47,080
dataset in three different sets or

611
00:35:41,530 --> 00:35:50,200
subsets and here is the careful we have

612
00:35:47,080 --> 00:35:53,170
the careful cross-validation is a

613
00:35:50,200 --> 00:35:56,319
technique used to give more reliability

614
00:35:53,170 --> 00:35:59,050
to the evaluation stage as I said the

615
00:35:56,320 --> 00:36:03,070
data set would be divided in different

616
00:35:59,050 --> 00:36:08,020
subsets training and evaluation but this

617
00:36:03,070 --> 00:36:09,060
is quite random in order to avoid bias

618
00:36:08,020 --> 00:36:12,090
in our

619
00:36:09,060 --> 00:36:15,299
classification we have to repeat this

620
00:36:12,090 --> 00:36:19,110
process and in number of occasions

621
00:36:15,300 --> 00:36:22,320
especially okay number of times what

622
00:36:19,110 --> 00:36:26,490
good do we do we divide the data set in

623
00:36:22,320 --> 00:36:29,940
K number of different blocks all those

624
00:36:26,490 --> 00:36:33,240
blocks minus one are used for the

625
00:36:29,940 --> 00:36:35,820
training and one of them here the yellow

626
00:36:33,240 --> 00:36:39,509
one would be used for evaluation

627
00:36:35,820 --> 00:36:42,120
purposes in the next round what would we

628
00:36:39,510 --> 00:36:45,660
do we might use the next block and

629
00:36:42,120 --> 00:36:48,150
through iteration we'll be using the

630
00:36:45,660 --> 00:36:51,720
following block for validation and the

631
00:36:48,150 --> 00:36:55,860
rest for training we might repeat this

632
00:36:51,720 --> 00:37:00,270
process until complete in completing a

633
00:36:55,860 --> 00:37:07,200
take a number of iterations this gives

634
00:37:00,270 --> 00:37:10,470
more reliability to the results once

635
00:37:07,200 --> 00:37:15,149
we've carried out this process we can go

636
00:37:10,470 --> 00:37:18,540
and analyze our results depending on the

637
00:37:15,150 --> 00:37:22,470
labels and the results provided by the

638
00:37:18,540 --> 00:37:27,210
algorithm we build a contingency charged

639
00:37:22,470 --> 00:37:29,370
with false positive or negative here we

640
00:37:27,210 --> 00:37:33,300
have a visual representation in this

641
00:37:29,370 --> 00:37:35,250
chart with ticks and crosses green ticks

642
00:37:33,300 --> 00:37:38,460
and red process depending on those

643
00:37:35,250 --> 00:37:40,980
values we can build or design different

644
00:37:38,460 --> 00:37:45,690
metrics to evaluate those systems would

645
00:37:40,980 --> 00:37:48,900
have accuracy recall many different

646
00:37:45,690 --> 00:37:53,670
measures can be applied to the system

647
00:37:48,900 --> 00:37:56,550
and we can see how accurate is it we are

648
00:37:53,670 --> 00:38:02,180
evaluating its capacity to detect new

649
00:37:56,550 --> 00:38:05,220
threats another system is drop curves

650
00:38:02,180 --> 00:38:08,310
here in the x-axis

651
00:38:05,220 --> 00:38:12,529
it will be the rate of false positives

652
00:38:08,310 --> 00:38:18,210
and in the y-axis the rate of true

653
00:38:12,530 --> 00:38:22,140
positives so that you can see it in the

654
00:38:18,210 --> 00:38:24,780
graph false positive rates has to be

655
00:38:22,140 --> 00:38:27,390
really low they are false warnings or

656
00:38:24,780 --> 00:38:33,480
alarms in the system and the true

657
00:38:27,390 --> 00:38:36,029
positives the anomalies themselves that

658
00:38:33,480 --> 00:38:40,920
are being detected we want them to be

659
00:38:36,030 --> 00:38:44,310
very numerous and we can draw a curve as

660
00:38:40,920 --> 00:38:47,220
you see in the graph the closer to the X

661
00:38:44,310 --> 00:38:51,900
end of the graph the better results we

662
00:38:47,220 --> 00:38:55,560
may have a way to measure this is using

663
00:38:51,900 --> 00:38:58,560
the area under the curve we might

664
00:38:55,560 --> 00:39:03,619
calculate the area below this curve and

665
00:38:58,560 --> 00:39:12,509
as I said the higher the better them

666
00:39:03,619 --> 00:39:15,170
higher they a u G will be the expected

667
00:39:12,510 --> 00:39:15,170
result

668
00:39:17,680 --> 00:39:25,210
in this slide I wanted to present

669
00:39:22,720 --> 00:39:29,500
information about a study conducted by

670
00:39:25,210 --> 00:39:31,990
researchers giving reasons why machine

671
00:39:29,500 --> 00:39:37,059
learning especially for anomaly

672
00:39:31,990 --> 00:39:42,180
detection and in this particular case

673
00:39:37,059 --> 00:39:46,200
and for data traffic is more complex

674
00:39:42,180 --> 00:39:50,609
machine learning can be applied to a

675
00:39:46,200 --> 00:39:55,169
numerous number of fields biology

676
00:39:50,609 --> 00:39:59,799
healthcare business intelligence but

677
00:39:55,170 --> 00:40:04,359
what they say is that specifically in

678
00:39:59,799 --> 00:40:06,910
this area there are specific

679
00:40:04,359 --> 00:40:10,270
circumstances that make machine learning

680
00:40:06,910 --> 00:40:12,598
more complex they are more difficult

681
00:40:10,270 --> 00:40:17,259
than in other areas

682
00:40:12,599 --> 00:40:21,640
first due to the high cost of errors we

683
00:40:17,260 --> 00:40:24,970
should take into account that when this

684
00:40:21,640 --> 00:40:28,629
type of algorithm is not detecting an

685
00:40:24,970 --> 00:40:32,490
anomaly we can have serious consequences

686
00:40:28,630 --> 00:40:38,109
for example and a tactic that is

687
00:40:32,490 --> 00:40:41,589
unnoticed this week I had a meeting with

688
00:40:38,109 --> 00:40:43,900
some partners and they told me how they

689
00:40:41,589 --> 00:40:48,160
use all those techniques in an

690
00:40:43,900 --> 00:40:51,880
industrial context and let us think what

691
00:40:48,160 --> 00:40:54,490
might happen in an industrial plant if

692
00:40:51,880 --> 00:40:57,130
the algorithm does not detect an anomaly

693
00:40:54,490 --> 00:41:01,538
that is taking place in the plant and

694
00:40:57,130 --> 00:41:06,210
the cost of these errors might be very

695
00:41:01,539 --> 00:41:10,109
high so it's very specific

696
00:41:06,210 --> 00:41:13,740
as I said the recess Casa T of training

697
00:41:10,109 --> 00:41:17,279
data related to the difficulty to have

698
00:41:13,740 --> 00:41:25,399
the right data and data sets to test the

699
00:41:17,280 --> 00:41:29,070
system of course the how different the

700
00:41:25,400 --> 00:41:31,589
Internet traffic is they all have many

701
00:41:29,070 --> 00:41:34,910
different variables we have many

702
00:41:31,589 --> 00:41:40,250
different protocols each of them has its

703
00:41:34,910 --> 00:41:43,290
specificities and therefore there can be

704
00:41:40,250 --> 00:41:48,150
different simultaneous events in the net

705
00:41:43,290 --> 00:41:50,250
so it's difficult to describe what's

706
00:41:48,150 --> 00:41:51,990
going on in the network and even more

707
00:41:50,250 --> 00:41:55,710
difficult when we want to detect

708
00:41:51,990 --> 00:41:58,490
anomalies there are also difficulties at

709
00:41:55,710 --> 00:42:00,680
the evaluation stage for example

710
00:41:58,490 --> 00:42:04,080
interpreting the results or

711
00:42:00,680 --> 00:42:07,529
understanding what type of attacks on

712
00:42:04,080 --> 00:42:10,080
system can detect we are on occasions

713
00:42:07,530 --> 00:42:13,080
very ambitious we want to have a system

714
00:42:10,080 --> 00:42:18,230
that might detect all the attacks but

715
00:42:13,080 --> 00:42:25,710
maybe we can be come more specialized

716
00:42:18,230 --> 00:42:28,859
and I will now move to the practical

717
00:42:25,710 --> 00:42:32,280
part of my presentation everything I've

718
00:42:28,859 --> 00:42:35,130
introduced can be done in a nice way

719
00:42:32,280 --> 00:42:38,820
in Python we're going to use two

720
00:42:35,130 --> 00:42:43,020
libraries that would be very useful one

721
00:42:38,820 --> 00:42:46,230
is pandas for the data science and data

722
00:42:43,020 --> 00:42:49,530
manipulation process and then ask is it

723
00:42:46,230 --> 00:42:51,750
run from machine learning algorithm

724
00:42:49,530 --> 00:42:55,200
pandas is a library that will provide

725
00:42:51,750 --> 00:43:00,300
different tools and data structures that

726
00:42:55,200 --> 00:43:03,598
will allow us a high performance this is

727
00:43:00,300 --> 00:43:06,349
important when we want to you have a

728
00:43:03,599 --> 00:43:06,349
real time

729
00:43:06,800 --> 00:43:13,950
analysis then we have the data frame

730
00:43:10,589 --> 00:43:16,950
class useful for data representation and

731
00:43:13,950 --> 00:43:23,609
data manipulation and it will allow us

732
00:43:16,950 --> 00:43:27,299
to load data from an SQL file or other

733
00:43:23,609 --> 00:43:30,180
type of formats then we also have

734
00:43:27,300 --> 00:43:35,579
squishy turn that provides different

735
00:43:30,180 --> 00:43:38,910
machine learning algorithm and has API

736
00:43:35,579 --> 00:43:42,180
with the estimator which is the

737
00:43:38,910 --> 00:43:46,710
particular algorithm being used they

738
00:43:42,180 --> 00:43:50,069
have different methods and IP parameters

739
00:43:46,710 --> 00:43:53,070
or settings but usually they provide the

740
00:43:50,070 --> 00:43:56,609
training method in this case the name is

741
00:43:53,070 --> 00:44:00,000
feet that sets the status of the

742
00:43:56,609 --> 00:44:04,910
estimator based on the data and then the

743
00:44:00,000 --> 00:44:09,750
predictor data to predict unknown data

744
00:44:04,910 --> 00:44:12,480
so if we do a summary of what the steps

745
00:44:09,750 --> 00:44:16,589
of the process would be therefore the

746
00:44:12,480 --> 00:44:20,780
way we might choose the model we want to

747
00:44:16,589 --> 00:44:25,369
use and select the hyper parameters and

748
00:44:20,780 --> 00:44:29,040
adjust them for this specific algorithm

749
00:44:25,369 --> 00:44:32,460
after we would distract the matrix with

750
00:44:29,040 --> 00:44:34,619
features and the predictive vector we

751
00:44:32,460 --> 00:44:37,380
will adjust a model to the training data

752
00:44:34,619 --> 00:44:39,680
and then we will finally predict a known

753
00:44:37,380 --> 00:44:39,680
data

754
00:44:42,150 --> 00:44:48,839
here I will speak about something that

755
00:44:46,619 --> 00:44:51,509
is more specifically related to

756
00:44:48,839 --> 00:44:54,869
implementation and I would like to speak

757
00:44:51,509 --> 00:44:56,880
about pipelines untransformed this is

758
00:44:54,869 --> 00:45:01,559
not well known and it might be really

759
00:44:56,880 --> 00:45:05,999
useful especially for the implementation

760
00:45:01,559 --> 00:45:09,660
of data transformation or data science I

761
00:45:05,999 --> 00:45:12,959
urge you to use those pipelines so that

762
00:45:09,660 --> 00:45:16,649
you can experiment and explore and your

763
00:45:12,960 --> 00:45:21,410
life might be easier to understand about

764
00:45:16,650 --> 00:45:27,269
this I will introduce them now pipelines

765
00:45:21,410 --> 00:45:30,299
is leased sequential list of transformed

766
00:45:27,269 --> 00:45:34,439
features with a final estimator they

767
00:45:30,299 --> 00:45:40,769
carry out data manipulation with feet

768
00:45:34,440 --> 00:45:44,609
and the transformer I will show you

769
00:45:40,769 --> 00:45:46,680
Morris Pacifica details later on to

770
00:45:44,609 --> 00:45:48,920
speak about some of the advantages of

771
00:45:46,680 --> 00:45:52,288
using pipelines and trans front ones

772
00:45:48,920 --> 00:45:57,690
first we can group everything we don't

773
00:45:52,289 --> 00:46:00,239
need to go one by one executing them if

774
00:45:57,690 --> 00:46:02,940
we have a huge group of transformed

775
00:46:00,239 --> 00:46:06,150
features in the data science process we

776
00:46:02,940 --> 00:46:10,710
can do or use it as a single block if we

777
00:46:06,150 --> 00:46:14,239
wish to add new features rodeo for

778
00:46:10,710 --> 00:46:17,940
animals everybody it will allow us to

779
00:46:14,239 --> 00:46:21,210
avoid repetitions it can also simplify

780
00:46:17,940 --> 00:46:25,249
the close reading making it more

781
00:46:21,210 --> 00:46:35,369
reusable and making the execution stage

782
00:46:25,249 --> 00:46:42,509
easier before showing you the rest of my

783
00:46:35,369 --> 00:46:49,619
presentation some examples open we

784
00:46:42,509 --> 00:46:54,630
implement this in titan manner so that

785
00:46:49,619 --> 00:46:57,390
everything that i've mentioned you see

786
00:46:54,630 --> 00:47:00,359
how it can be implemented and what i

787
00:46:57,390 --> 00:47:06,720
said is not so difficult and i urge you

788
00:47:00,359 --> 00:47:10,410
to tested this is a notebook in jupiter

789
00:47:06,720 --> 00:47:14,279
in is present in anaconda word you can

790
00:47:10,410 --> 00:47:18,690
use any other first i import several

791
00:47:14,279 --> 00:47:24,720
libraries and first i build the data set

792
00:47:18,690 --> 00:47:27,660
I selected the malware this pseudo

793
00:47:24,720 --> 00:47:29,999
Darlie and I will mix it with the

794
00:47:27,660 --> 00:47:32,670
legitimate background traffic that was

795
00:47:29,999 --> 00:47:35,730
compiled from Internet I'm speaking

796
00:47:32,670 --> 00:47:41,789
about net flow this is where everything

797
00:47:35,730 --> 00:47:46,680
I told you starts being meaningful once

798
00:47:41,789 --> 00:47:49,470
we have our data set together with the

799
00:47:46,680 --> 00:47:52,680
data traffic and anomalies this is a

800
00:47:49,470 --> 00:47:56,759
very simple example so that you can see

801
00:47:52,680 --> 00:47:58,799
its practical use the real life is more

802
00:47:56,759 --> 00:48:02,160
complex instead this is just an

803
00:47:58,799 --> 00:48:05,130
illustration of what I have introduced

804
00:48:02,160 --> 00:48:11,420
once we have our set with this library

805
00:48:05,130 --> 00:48:11,420
pandas we will read this CSV

806
00:48:15,750 --> 00:48:23,050
vamos al al-faisal bet we're gonna read

807
00:48:18,850 --> 00:48:27,910
it because we're using NetFlow we're

808
00:48:23,050 --> 00:48:32,290
going to choose the columns we have in

809
00:48:27,910 --> 00:48:38,379
our file date and time of beginning of

810
00:48:32,290 --> 00:48:42,100
the flow and flow time origin IP

811
00:48:38,380 --> 00:48:48,040
destination IP original port destination

812
00:48:42,100 --> 00:48:54,610
port the protocol plugs packets and

813
00:48:48,040 --> 00:48:59,730
entry bytes in addition to this I've

814
00:48:54,610 --> 00:49:02,980
added the label here the name is type

815
00:48:59,730 --> 00:49:06,010
because I'm gonna use an unsupervised

816
00:49:02,980 --> 00:49:09,880
algorithm we might not need this Doug

817
00:49:06,010 --> 00:49:12,550
but I think loses it to see and analyze

818
00:49:09,880 --> 00:49:16,030
the results and to save me my algorithm

819
00:49:12,550 --> 00:49:19,510
is really working well in detecting

820
00:49:16,030 --> 00:49:24,010
anomalies if we introduce this

821
00:49:19,510 --> 00:49:29,640
information this is what we get here we

822
00:49:24,010 --> 00:49:36,150
see in different columns the traffic and

823
00:49:29,640 --> 00:49:36,150
data compiled we see TCP port 80

824
00:49:38,610 --> 00:49:48,250
laughter he will start the manipulation

825
00:49:45,180 --> 00:49:52,000
process data science and the features

826
00:49:48,250 --> 00:49:56,770
extraction that we will use in our

827
00:49:52,000 --> 00:50:00,910
algorithm and to that aim one of the

828
00:49:56,770 --> 00:50:04,620
first things you have to do is the group

829
00:50:00,910 --> 00:50:06,879
that flows depending on the frequency

830
00:50:04,620 --> 00:50:09,609
those flows that might have different

831
00:50:06,880 --> 00:50:12,430
time rates can be grouped in five

832
00:50:09,610 --> 00:50:18,820
seconds intervals in this particular

833
00:50:12,430 --> 00:50:24,399
example so that this example is devoted

834
00:50:18,820 --> 00:50:29,880
to malware detection so based uncertain

835
00:50:24,400 --> 00:50:32,260
origin IP it was connect into many AI

836
00:50:29,880 --> 00:50:34,900
destination IP is in a really brief

837
00:50:32,260 --> 00:50:39,160
period of time so I group those flows by

838
00:50:34,900 --> 00:50:42,130
time intervals once I do it we see that

839
00:50:39,160 --> 00:50:48,339
the flows that are now grouped in five

840
00:50:42,130 --> 00:50:51,940
seconds flows they go from eleven open

841
00:50:48,340 --> 00:50:54,480
five four point 0.15 in five seconds

842
00:50:51,940 --> 00:50:54,480
intervals

843
00:50:58,810 --> 00:51:08,210
from now on we will start extracting the

844
00:51:02,720 --> 00:51:12,350
teachers here are just the one I

845
00:51:08,210 --> 00:51:15,530
mentioned to detect malware and we do

846
00:51:12,350 --> 00:51:19,430
different groupings by eye bees by

847
00:51:15,530 --> 00:51:23,420
origin eyepiece destination ibis and all

848
00:51:19,430 --> 00:51:28,370
of this would be data manipulation

849
00:51:23,420 --> 00:51:32,600
process and we might end up with the

850
00:51:28,370 --> 00:51:34,910
column count column that is counting the

851
00:51:32,600 --> 00:51:37,660
number of flows during this time

852
00:51:34,910 --> 00:51:43,160
interval with a certain origin and

853
00:51:37,660 --> 00:51:45,319
destination IP for similar a pharisee

854
00:51:43,160 --> 00:51:52,490
come on it could be possible to do it

855
00:51:45,320 --> 00:51:54,820
this way or alternatively yes I was

856
00:51:52,490 --> 00:52:01,069
saying earlier we can do this using

857
00:51:54,820 --> 00:52:03,920
pipelines you've defined an estimator

858
00:52:01,070 --> 00:52:07,760
that is going to use the pipeline the

859
00:52:03,920 --> 00:52:11,560
what we do and I blind a set of

860
00:52:07,760 --> 00:52:16,660
transforms the pipeline is like a

861
00:52:11,560 --> 00:52:21,560
sequence of transformations and datum ie

862
00:52:16,660 --> 00:52:25,399
manipulations and data as you can see

863
00:52:21,560 --> 00:52:30,140
here what I'm doing well first I order

864
00:52:25,400 --> 00:52:33,770
the index I group flows in the 5-second

865
00:52:30,140 --> 00:52:36,379
sequences the index is reset then we

866
00:52:33,770 --> 00:52:38,450
select specific columns with which we're

867
00:52:36,380 --> 00:52:42,710
going to work that have to be numerical

868
00:52:38,450 --> 00:52:45,649
for this an isolation force algorithm

869
00:52:42,710 --> 00:52:48,470
unless we eliminate all the columns that

870
00:52:45,650 --> 00:52:51,140
we won't have to use this is a very

871
00:52:48,470 --> 00:52:53,509
simple straightforward example so that

872
00:52:51,140 --> 00:52:56,240
you understand the role of the pipeline

873
00:52:53,510 --> 00:52:58,570
and sort of doing all these operations

874
00:52:56,240 --> 00:53:02,290
separately I group them together

875
00:52:58,570 --> 00:53:05,170
as an estimator I will use isolation

876
00:53:02,290 --> 00:53:08,790
force with 100 trees and contamination

877
00:53:05,170 --> 00:53:14,710
value of 0.1 one basically these values

878
00:53:08,790 --> 00:53:19,540
sets a threshold a cap between anomalies

879
00:53:14,710 --> 00:53:24,840
and normal situations is the threshold

880
00:53:19,540 --> 00:53:24,840
level and 266 samples are used

881
00:53:30,210 --> 00:53:39,390
after this what we do is divide and

882
00:53:35,400 --> 00:53:45,190
separate data from tax or labels and

883
00:53:39,390 --> 00:53:49,330
then I apply the estimator to do so

884
00:53:45,190 --> 00:53:54,340
first fit first I train the algorithm

885
00:53:49,330 --> 00:53:56,770
with the training ensemble in this case

886
00:53:54,340 --> 00:53:58,840
have used X the ensemble of data but

887
00:53:56,770 --> 00:54:01,570
this isn't the most appropriate option

888
00:53:58,840 --> 00:54:04,270
it'd be better to do this with

889
00:54:01,570 --> 00:54:07,390
cross-validation as we will see

890
00:54:04,270 --> 00:54:12,600
so once the algorithm has learned we

891
00:54:07,390 --> 00:54:16,980
predict or forecast the new data I the

892
00:54:12,600 --> 00:54:20,680
assessment of the algorithm and this

893
00:54:16,980 --> 00:54:23,170
yields the following results for each of

894
00:54:20,680 --> 00:54:25,270
the flow lines we will have a one of its

895
00:54:23,170 --> 00:54:29,800
normal or minus one if an anomaly has

896
00:54:25,270 --> 00:54:34,150
been detected this way we extract the

897
00:54:29,800 --> 00:54:36,790
metrics regarding the performance of the

898
00:54:34,150 --> 00:54:39,430
system I have the false positives and

899
00:54:36,790 --> 00:54:43,480
the true negatives there and there you

900
00:54:39,430 --> 00:54:46,149
have the results these results are

901
00:54:43,480 --> 00:54:49,720
pretty good or acceptable but we need to

902
00:54:46,150 --> 00:54:54,580
consider that I've been working with a

903
00:54:49,720 --> 00:54:58,660
very reduced or limited data set and we

904
00:54:54,580 --> 00:55:00,549
reduced time in real scenario we would

905
00:54:58,660 --> 00:55:03,180
have to adjust this further in order to

906
00:55:00,550 --> 00:55:03,180
obtain good results

907
00:55:04,080 --> 00:55:11,450
couple things I could add

908
00:55:07,700 --> 00:55:13,549
on the one hand the selection of

909
00:55:11,450 --> 00:55:19,879
features or characteristics in this case

910
00:55:13,549 --> 00:55:23,690
I have used a classifier extra tree

911
00:55:19,880 --> 00:55:27,109
classifier to be more exact and I select

912
00:55:23,690 --> 00:55:31,930
some specific characteristics instead of

913
00:55:27,109 --> 00:55:36,319
using oh I use a more reduced set of

914
00:55:31,930 --> 00:55:37,250
characteristics or features and we would

915
00:55:36,319 --> 00:55:41,000
do the same

916
00:55:37,250 --> 00:55:42,859
fits predict and in this case you can

917
00:55:41,000 --> 00:55:44,690
see the results

918
00:55:42,859 --> 00:55:49,150
well we've detected the malware by the

919
00:55:44,690 --> 00:55:52,130
way hey the results obtained are

920
00:55:49,150 --> 00:55:52,640
decreased slight decrease it's still

921
00:55:52,130 --> 00:55:55,250
pretty good

922
00:55:52,640 --> 00:55:57,379
but the selection of characteristics or

923
00:55:55,250 --> 00:56:00,500
features has to reduce significantly the

924
00:55:57,380 --> 00:56:04,760
number of characteristics used which is

925
00:56:00,500 --> 00:56:08,380
good in terms of efficiency but it has

926
00:56:04,760 --> 00:56:12,650
reduced the result slightly in this case

927
00:56:08,380 --> 00:56:15,319
so it's worth the effort but there are

928
00:56:12,650 --> 00:56:18,049
some cases in which even in you select

929
00:56:15,319 --> 00:56:22,009
features on them the results can improve

930
00:56:18,049 --> 00:56:26,990
so keep an eye on this because this can

931
00:56:22,010 --> 00:56:33,619
happen it can be interesting and for you

932
00:56:26,990 --> 00:56:38,270
to understand cross-validation I'm

933
00:56:33,619 --> 00:56:40,579
working here with a group of digits and

934
00:56:38,270 --> 00:56:44,210
I wanted to show you that normally what

935
00:56:40,579 --> 00:56:46,339
we do is not to train the system with

936
00:56:44,210 --> 00:56:49,640
all the set of data and assess the

937
00:56:46,339 --> 00:56:54,140
entire system but divided so to do this

938
00:56:49,640 --> 00:56:57,170
I use the split feature the device the

939
00:56:54,140 --> 00:57:00,379
whole on solve and training data

940
00:56:57,170 --> 00:57:02,780
scientist and and 30% which is this o

941
00:57:00,380 --> 00:57:07,130
dot three which means that thirty

942
00:57:02,780 --> 00:57:08,960
percent of the data said will be aimed

943
00:57:07,130 --> 00:57:11,880
and

944
00:57:08,960 --> 00:57:14,210
assessment or devoted to assessment

945
00:57:11,880 --> 00:57:17,460
we're using the car neighbors and

946
00:57:14,210 --> 00:57:19,620
algorithm and what we do is the fit with

947
00:57:17,460 --> 00:57:23,490
the training data exclusively and

948
00:57:19,620 --> 00:57:26,100
project with test data so we've divided

949
00:57:23,490 --> 00:57:30,450
or split the data set and there is some

950
00:57:26,100 --> 00:57:34,980
once obtained are more trustworthy more

951
00:57:30,450 --> 00:57:37,710
reliable and will more accurately assess

952
00:57:34,980 --> 00:57:42,390
the capacity for prediction of the

953
00:57:37,710 --> 00:57:47,630
algorithm and face up an own data and we

954
00:57:42,390 --> 00:57:52,500
can apply k-fold cross-validation here

955
00:57:47,630 --> 00:57:57,450
am i playing it here with a 10-fold

956
00:57:52,500 --> 00:58:00,390
are you divided into ten blocks and I've

957
00:57:57,450 --> 00:58:03,870
also used different estimators to see

958
00:58:00,390 --> 00:58:10,470
the results and in this case we see that

959
00:58:03,870 --> 00:58:16,230
KN n is the one that provides us with

960
00:58:10,470 --> 00:58:20,250
the best results I have to finish off

961
00:58:16,230 --> 00:58:24,690
because I guess you're hungry and I

962
00:58:20,250 --> 00:58:26,610
don't want to take too much time the

963
00:58:24,690 --> 00:58:28,320
people there's one person that is going

964
00:58:26,610 --> 00:58:32,370
to take the floor now as a conclusion

965
00:58:28,320 --> 00:58:34,350
what can I say the selection of the data

966
00:58:32,370 --> 00:58:36,839
set is very important it's very

967
00:58:34,350 --> 00:58:41,250
important to you work with appropriate

968
00:58:36,840 --> 00:58:44,010
data this is it's always easy to do as

969
00:58:41,250 --> 00:58:47,190
for data size manipulation extraction of

970
00:58:44,010 --> 00:58:48,570
features that's a very important part of

971
00:58:47,190 --> 00:58:51,060
the project you need to pay special

972
00:58:48,570 --> 00:58:53,340
attention to this because the results

973
00:58:51,060 --> 00:58:56,400
you obtained will depend largely on this

974
00:58:53,340 --> 00:58:58,260
and this can consume around 80% of the

975
00:58:56,400 --> 00:59:01,770
time of the project so you need to

976
00:58:58,260 --> 00:59:05,510
devote great amount of attention train

977
00:59:01,770 --> 00:59:07,310
training is also very important

978
00:59:05,510 --> 00:59:09,590
because that's when the algorithm will

979
00:59:07,310 --> 00:59:13,690
learn and determine the prediction in

980
00:59:09,590 --> 00:59:16,820
capacity and when we're talking about

981
00:59:13,690 --> 00:59:18,859
bar scale projects it's advisable to

982
00:59:16,820 --> 00:59:21,500
have different types of experts to have

983
00:59:18,859 --> 00:59:23,869
a multidisciplinary team made up of

984
00:59:21,500 --> 00:59:29,260
analysts and help us extract features

985
00:59:23,869 --> 00:59:32,720
also security experts and also data

986
00:59:29,260 --> 00:59:35,330
scientists because the fields can be

987
00:59:32,720 --> 00:59:38,810
very different and maybe a dieter

988
00:59:35,330 --> 00:59:41,569
scientist from the biology world has no

989
00:59:38,810 --> 00:59:43,150
security knowledge so it's very

990
00:59:41,570 --> 00:59:47,060
important to have different profiles

991
00:59:43,150 --> 00:59:54,140
when developers people with machine

992
00:59:47,060 --> 00:59:58,460
learning knowledge and I encourage you

993
00:59:54,140 --> 01:00:00,910
to try this out it's not that difficult

994
00:59:58,460 --> 01:00:03,260
you can play around with this and learn

995
01:00:00,910 --> 01:00:06,109
and without further ado I'd like to

996
01:00:03,260 --> 01:00:08,420
thank you for your attention and I'm

997
01:00:06,109 --> 01:00:11,020
here you are availa view have any

998
01:00:08,420 --> 01:00:17,939
questions or queries

999
01:00:11,020 --> 01:00:17,939
[Applause]

1000
01:00:20,730 --> 01:00:27,070
hola CKD abrir una tarde

1001
01:00:23,800 --> 01:00:29,560
I mean delay are there any tools in the

1002
01:00:27,070 --> 01:00:33,430
cloud that we can use to send the

1003
01:00:29,560 --> 01:00:38,590
NetFlow to do something similar to what

1004
01:00:33,430 --> 01:00:42,520
you've described free of charge as

1005
01:00:38,590 --> 01:00:48,250
possible there are several that can be

1006
01:00:42,520 --> 01:00:50,050
used some work pretty well and not for

1007
01:00:48,250 --> 01:00:52,240
your charts n table is one of them it's

1008
01:00:50,050 --> 01:00:55,380
a paid tool and it's interesting because

1009
01:00:52,240 --> 01:00:58,750
it has many puddings

1010
01:00:55,380 --> 01:01:01,660
and this allows us to capture NetFlow

1011
01:00:58,750 --> 01:01:07,480
that has many setting parameters that

1012
01:01:01,660 --> 01:01:12,009
are very interesting it also allows you

1013
01:01:07,480 --> 01:01:19,930
with plugins to obtain features from the

1014
01:01:12,010 --> 01:01:22,800
application layer HTTP E&S etc it says

1015
01:01:19,930 --> 01:01:27,089
you're asking for something for free

1016
01:01:22,800 --> 01:01:30,130
there are other options for example and

1017
01:01:27,090 --> 01:01:33,300
fkd saw flow I can give you later the

1018
01:01:30,130 --> 01:01:38,080
list there's some tools that you can use

1019
01:01:33,300 --> 01:01:42,240
for the proof and to be able to extract

1020
01:01:38,080 --> 01:01:42,240
traffic statistics thank you

1021
01:01:53,310 --> 01:02:00,360
well you have no further questions

1022
01:01:55,570 --> 01:02:00,360
thanks very much for being here and

1023
01:02:06,500 --> 01:02:08,560
you

