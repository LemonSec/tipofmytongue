1
00:00:03,620 --> 00:00:07,919
my name is Angelo and I'm here today to

2
00:00:07,919 --> 00:00:09,420
present you

3
00:00:09,420 --> 00:00:12,300
my talk it's a data science and machine

4
00:00:12,300 --> 00:00:15,500
learning cyber security hyper reality

5
00:00:15,500 --> 00:00:17,880
and uh

6
00:00:17,880 --> 00:00:20,039
I'm a cyber security data Science Tech

7
00:00:20,039 --> 00:00:20,939
lead

8
00:00:20,939 --> 00:00:23,939
in the data security team for Mercado

9
00:00:23,939 --> 00:00:24,980
Libre

10
00:00:24,980 --> 00:00:27,840
Mercado Libre is the largest e-commerce

11
00:00:27,840 --> 00:00:29,760
platform in Latin America

12
00:00:29,760 --> 00:00:32,460
and it provides a great environment for

13
00:00:32,460 --> 00:00:35,700
buyers and sellers to make business

14
00:00:35,700 --> 00:00:37,640
if you want to know more about me please

15
00:00:37,640 --> 00:00:40,440
you can follow that link that's going to

16
00:00:40,440 --> 00:00:42,480
take you to my LinkedIn profile

17
00:00:42,480 --> 00:00:43,980
and if you want to know more about

18
00:00:43,980 --> 00:00:47,239
Mercado Libre you can follow

19
00:00:47,340 --> 00:00:51,800
here this link so great

20
00:00:53,700 --> 00:00:57,000
so today I want to present you three use

21
00:00:57,000 --> 00:00:58,980
cases in which

22
00:00:58,980 --> 00:01:01,140
machine learning and data science were

23
00:01:01,140 --> 00:01:03,239
successfully applied to cyber security

24
00:01:03,239 --> 00:01:05,220
problems

25
00:01:05,220 --> 00:01:07,140
and uh

26
00:01:07,140 --> 00:01:10,200
also I want to focus on rediff instead

27
00:01:10,200 --> 00:01:12,659
of adaptive because I want to expose you

28
00:01:12,659 --> 00:01:14,119
to several different

29
00:01:14,119 --> 00:01:16,560
techniques and choosing data science and

30
00:01:16,560 --> 00:01:18,180
machine learning and how they were

31
00:01:18,180 --> 00:01:21,240
applied to cyber security problems

32
00:01:21,240 --> 00:01:22,680
and the

33
00:01:22,680 --> 00:01:23,939
finally

34
00:01:23,939 --> 00:01:26,100
I want to share with you my experience

35
00:01:26,100 --> 00:01:27,540
in field notes because that might save

36
00:01:27,540 --> 00:01:29,400
you a lot of time in the future and also

37
00:01:29,400 --> 00:01:31,500
headaches

38
00:01:31,500 --> 00:01:33,979
okay

39
00:01:34,560 --> 00:01:36,119
uh

40
00:01:36,119 --> 00:01:38,460
what is data science

41
00:01:38,460 --> 00:01:40,200
so there is no formal definition for

42
00:01:40,200 --> 00:01:41,520
data science

43
00:01:41,520 --> 00:01:43,439
but I like thinking about data science

44
00:01:43,439 --> 00:01:45,840
as the meeting point for computer

45
00:01:45,840 --> 00:01:47,000
science

46
00:01:47,000 --> 00:01:49,380
mathematics and statistics and the

47
00:01:49,380 --> 00:01:52,320
domain slash business knowledge to solve

48
00:01:52,320 --> 00:01:54,200
data problems

49
00:01:54,200 --> 00:01:57,899
and in particular in our case today to

50
00:01:57,899 --> 00:02:02,240
solve cyber security data problems

51
00:02:04,439 --> 00:02:07,200
so uh

52
00:02:07,200 --> 00:02:09,538
as as usual we need to have a cat

53
00:02:09,538 --> 00:02:11,520
picture in any successful presentation

54
00:02:11,520 --> 00:02:13,819
right

55
00:02:13,920 --> 00:02:17,099
but wait there will be more

56
00:02:17,099 --> 00:02:21,260
so is data science for everyone

57
00:02:22,739 --> 00:02:24,660
let's see

58
00:02:24,660 --> 00:02:26,780
okay

59
00:02:28,340 --> 00:02:30,540
you know in my original presentation

60
00:02:30,540 --> 00:02:32,220
presentation there was some animations

61
00:02:32,220 --> 00:02:33,959
here so please don't don't be

62
00:02:33,959 --> 00:02:38,160
disappointed uh so we got here data

63
00:02:38,160 --> 00:02:41,220
science so data science is data plus

64
00:02:41,220 --> 00:02:45,900
science so seems uh fair enough

65
00:02:45,900 --> 00:02:50,640
uh okay so no data node science no data

66
00:02:50,640 --> 00:02:52,200
science

67
00:02:52,200 --> 00:02:55,260
right so you you must have data to do

68
00:02:55,260 --> 00:02:58,680
data science so this point is is related

69
00:02:58,680 --> 00:03:02,280
to the data maturity of the company

70
00:03:02,280 --> 00:03:04,400
so we need to have

71
00:03:04,400 --> 00:03:06,840
data we need to have a maturity

72
00:03:06,840 --> 00:03:09,000
concerning

73
00:03:09,000 --> 00:03:11,340
storage and management

74
00:03:11,340 --> 00:03:14,120
okay in the next science what is science

75
00:03:14,120 --> 00:03:17,519
so in science usually what we do we

76
00:03:17,519 --> 00:03:20,340
formulate hypothesis and test those

77
00:03:20,340 --> 00:03:22,260
hypotheses

78
00:03:22,260 --> 00:03:25,500
and tests how can we perform the tests

79
00:03:25,500 --> 00:03:26,879
we need to perform computational

80
00:03:26,879 --> 00:03:29,640
experiments and computation experiments

81
00:03:29,640 --> 00:03:30,780
are

82
00:03:30,780 --> 00:03:32,760
research consuming they're consuming

83
00:03:32,760 --> 00:03:36,420
they consume time processing power

84
00:03:36,420 --> 00:03:38,940
um memory storage and depending on your

85
00:03:38,940 --> 00:03:42,120
cloud provider this might cost you a lot

86
00:03:42,120 --> 00:03:43,680
really a lot

87
00:03:43,680 --> 00:03:47,159
but here's the trickest point

88
00:03:47,159 --> 00:03:51,299
you know this scientific approach

89
00:03:51,299 --> 00:03:54,900
has a high degree of failure failure but

90
00:03:54,900 --> 00:03:57,659
what does that mean failure it means

91
00:03:57,659 --> 00:04:00,840
that most of the hypothesis we formulate

92
00:04:00,840 --> 00:04:03,659
they they will end up being wrong and

93
00:04:03,659 --> 00:04:06,540
not useful to solve the problem at hand

94
00:04:06,540 --> 00:04:09,019
and also this might seem

95
00:04:09,019 --> 00:04:12,599
negative problematic depending on the

96
00:04:12,599 --> 00:04:15,720
data maturity concerning Analytics

97
00:04:15,720 --> 00:04:18,238
for a company

98
00:04:18,238 --> 00:04:21,899
so okay so you let's ask let's answer

99
00:04:21,899 --> 00:04:25,400
the that question

100
00:04:26,340 --> 00:04:28,860
so is that a size for F1

101
00:04:28,860 --> 00:04:31,320
well it depends on your data maturity if

102
00:04:31,320 --> 00:04:33,780
your data maturity concerning storage

103
00:04:33,780 --> 00:04:37,340
and data storage management and

104
00:04:37,340 --> 00:04:42,360
analytics is low so no you cannot you

105
00:04:42,360 --> 00:04:45,300
cannot has cheeseburger and what if it

106
00:04:45,300 --> 00:04:46,680
is medium

107
00:04:46,680 --> 00:04:49,560
like you have some CN but you don't

108
00:04:49,560 --> 00:04:52,139
manage your that data well and so on so

109
00:04:52,139 --> 00:04:54,560
maybe we can try I will cook for you

110
00:04:54,560 --> 00:04:56,580
cheeseburger I'll give you and then you

111
00:04:56,580 --> 00:05:00,120
tell me oh yes I like it so let's do it

112
00:05:00,120 --> 00:05:01,259
again

113
00:05:01,259 --> 00:05:05,900
and if it's high so definitely you can

114
00:05:06,300 --> 00:05:09,900
okay next

115
00:05:10,620 --> 00:05:12,600
and what about machine learning what is

116
00:05:12,600 --> 00:05:14,639
machine learning

117
00:05:14,639 --> 00:05:16,860
so there is a definition here that that

118
00:05:16,860 --> 00:05:19,500
is uh the field of study that gives

119
00:05:19,500 --> 00:05:21,300
gives computers the ability to learn

120
00:05:21,300 --> 00:05:24,120
without being explicitly programmed so

121
00:05:24,120 --> 00:05:26,880
this definition is due to the AI Pioneer

122
00:05:26,880 --> 00:05:29,940
Arthur Samuel Circle 1950 and this is a

123
00:05:29,940 --> 00:05:32,940
quite good one actually and quite up to

124
00:05:32,940 --> 00:05:35,520
date because we that that's exactly what

125
00:05:35,520 --> 00:05:37,800
we've been trying to do since then

126
00:05:37,800 --> 00:05:40,919
and next

127
00:05:40,919 --> 00:05:43,680
so if this knowledge is not programmed

128
00:05:43,680 --> 00:05:45,900
explicitly programmed

129
00:05:45,900 --> 00:05:48,840
from where and how will they learn and

130
00:05:48,840 --> 00:05:51,240
by day I mean the machines

131
00:05:51,240 --> 00:05:54,539
well they will learn from data plus

132
00:05:54,539 --> 00:05:57,240
learning algorithms and data

133
00:05:57,240 --> 00:05:58,380
is

134
00:05:58,380 --> 00:06:00,900
best you can you can look at the data

135
00:06:00,900 --> 00:06:03,600
that the past experiences you know we

136
00:06:03,600 --> 00:06:06,000
have the past there and we want to

137
00:06:06,000 --> 00:06:08,340
extract patterns from the path from the

138
00:06:08,340 --> 00:06:10,919
past data to try to predict the future

139
00:06:10,919 --> 00:06:12,960
so it's more or less

140
00:06:12,960 --> 00:06:15,180
that this is the idea and learning

141
00:06:15,180 --> 00:06:17,160
algorithms and learning algorithms they

142
00:06:17,160 --> 00:06:19,320
make this possible to extract those

143
00:06:19,320 --> 00:06:22,020
patterns and learn with them okay

144
00:06:22,020 --> 00:06:23,880
so here we have an example of a

145
00:06:23,880 --> 00:06:25,620
classification problem

146
00:06:25,620 --> 00:06:27,600
and we are using a supervised learning

147
00:06:27,600 --> 00:06:30,360
algorithm to solve that problem

148
00:06:30,360 --> 00:06:33,419
so um suppose that you you want to build

149
00:06:33,419 --> 00:06:34,560
an app

150
00:06:34,560 --> 00:06:36,780
and then you point that app to a cat or

151
00:06:36,780 --> 00:06:41,460
to a dog and that uh answers ah this is

152
00:06:41,460 --> 00:06:43,500
a cat this is a dog okay so this is

153
00:06:43,500 --> 00:06:45,960
called classification problem

154
00:06:45,960 --> 00:06:47,460
how can we solve that using machine

155
00:06:47,460 --> 00:06:49,919
learning so first you need to build what

156
00:06:49,919 --> 00:06:51,960
we call the training set this is the

157
00:06:51,960 --> 00:06:53,759
training set what's a training set

158
00:06:53,759 --> 00:06:55,979
training sets a data set with several

159
00:06:55,979 --> 00:06:57,419
instances of

160
00:06:57,419 --> 00:06:59,759
cat images and dog images

161
00:06:59,759 --> 00:07:02,039
and most importantly you need to have a

162
00:07:02,039 --> 00:07:04,139
label attached to it

163
00:07:04,139 --> 00:07:05,520
image

164
00:07:05,520 --> 00:07:09,360
to identify it as a cat or or as a dog

165
00:07:09,360 --> 00:07:10,740
next

166
00:07:10,740 --> 00:07:13,139
we choose a suitable machine learning

167
00:07:13,139 --> 00:07:15,120
algorithm for that task

168
00:07:15,120 --> 00:07:17,400
then we start the process of training

169
00:07:17,400 --> 00:07:18,720
the training process

170
00:07:18,720 --> 00:07:20,759
in the training process we present the

171
00:07:20,759 --> 00:07:24,300
algorithm a list of image and label

172
00:07:24,300 --> 00:07:26,400
how the images in the training set we

173
00:07:26,400 --> 00:07:29,160
enter respective labels

174
00:07:29,160 --> 00:07:31,800
and this is the training process so once

175
00:07:31,800 --> 00:07:33,419
the training is done

176
00:07:33,419 --> 00:07:35,580
we want to

177
00:07:35,580 --> 00:07:38,400
evaluate the model performance and to

178
00:07:38,400 --> 00:07:40,560
evaluate the model performance

179
00:07:40,560 --> 00:07:43,860
we use another test sorry another data

180
00:07:43,860 --> 00:07:46,440
set that's called test set and what is

181
00:07:46,440 --> 00:07:48,539
the test set is a data set

182
00:07:48,539 --> 00:07:52,860
that contains unseen images and what

183
00:07:52,860 --> 00:07:55,380
does that mean it means images that were

184
00:07:55,380 --> 00:07:57,479
not used to train the model

185
00:07:57,479 --> 00:07:58,680
because you want to understand how the

186
00:07:58,680 --> 00:08:01,080
model generalized

187
00:08:01,080 --> 00:08:03,060
and from that we can assess the model

188
00:08:03,060 --> 00:08:04,919
performance

189
00:08:04,919 --> 00:08:07,620
if the model has a high accuracy it

190
00:08:07,620 --> 00:08:09,780
means that the model learned

191
00:08:09,780 --> 00:08:12,660
how to extract correctly the patterns

192
00:08:12,660 --> 00:08:14,880
from the images tend to correlate them

193
00:08:14,880 --> 00:08:17,880
to the labels

194
00:08:17,880 --> 00:08:20,360
okay

195
00:08:20,879 --> 00:08:23,520
now there is a different problem here

196
00:08:23,520 --> 00:08:26,639
and as you can see

197
00:08:26,639 --> 00:08:31,020
we don't have labels okay so here we

198
00:08:31,020 --> 00:08:32,700
have what is called a clustering problem

199
00:08:32,700 --> 00:08:35,039
and for that we use an unsupervised

200
00:08:35,039 --> 00:08:37,740
learning algorithm unsupervised and why

201
00:08:37,740 --> 00:08:40,559
is that unsupervised because there are

202
00:08:40,559 --> 00:08:42,659
no labels attached so it's the machine

203
00:08:42,659 --> 00:08:45,420
learning algorithm's job two

204
00:08:45,420 --> 00:08:48,779
extract the similarities

205
00:08:48,779 --> 00:08:52,019
of this images and put them together

206
00:08:52,019 --> 00:08:55,519
into clusters or groups

207
00:08:55,519 --> 00:08:59,040
with similar structural patterns so here

208
00:08:59,040 --> 00:09:00,660
we can see that we have only cats and

209
00:09:00,660 --> 00:09:02,880
little dogs so this is the idea of a

210
00:09:02,880 --> 00:09:06,300
result we expect and then once again if

211
00:09:06,300 --> 00:09:08,760
you want to evaluate the model we just

212
00:09:08,760 --> 00:09:11,300
present the model

213
00:09:11,300 --> 00:09:15,480
test set with some unseen images and

214
00:09:15,480 --> 00:09:17,160
from there we can assess its performance

215
00:09:17,160 --> 00:09:22,699
by the way this is my cat Okay Misty

216
00:09:26,279 --> 00:09:27,120
um

217
00:09:27,120 --> 00:09:31,800
here there was a animation okay

218
00:09:31,800 --> 00:09:34,580
and garbage in garbage Auto should show

219
00:09:34,580 --> 00:09:38,640
up later but anyways I'll explain

220
00:09:38,640 --> 00:09:42,620
uh so what's the first step in the

221
00:09:42,620 --> 00:09:45,000
machine learning pipeline so you want to

222
00:09:45,000 --> 00:09:46,680
build the project what's the first step

223
00:09:46,680 --> 00:09:49,860
first step is behind this it's called

224
00:09:49,860 --> 00:09:52,019
Problem definition we need to define the

225
00:09:52,019 --> 00:09:53,160
problem we need to understand the

226
00:09:53,160 --> 00:09:56,820
problem and for that we need to actively

227
00:09:56,820 --> 00:09:59,580
involve the stakeholders because we need

228
00:09:59,580 --> 00:10:01,019
to understand what they want to solve

229
00:10:01,019 --> 00:10:02,940
and then translate that

230
00:10:02,940 --> 00:10:07,920
to data science project okay and in the

231
00:10:07,920 --> 00:10:09,720
end we need to ask a question that every

232
00:10:09,720 --> 00:10:11,880
data science hates

233
00:10:11,880 --> 00:10:14,279
do we really need data science to solve

234
00:10:14,279 --> 00:10:16,680
that problem sometimes we don't

235
00:10:16,680 --> 00:10:21,839
and not actually often we don't

236
00:10:21,839 --> 00:10:24,480
okay supposing that we we need okay we

237
00:10:24,480 --> 00:10:25,320
need

238
00:10:25,320 --> 00:10:28,380
and the next step is data extraction we

239
00:10:28,380 --> 00:10:31,939
need to understand where the data is

240
00:10:32,420 --> 00:10:37,620
logs CM combination of those apis and we

241
00:10:37,620 --> 00:10:39,360
need to establish a way to stack that

242
00:10:39,360 --> 00:10:41,640
data efficiently

243
00:10:41,640 --> 00:10:45,120
next next we have a phase that's called

244
00:10:45,120 --> 00:10:47,880
pre-processing okay data preprocessing

245
00:10:47,880 --> 00:10:49,740
data preprocessing we do cleaning and

246
00:10:49,740 --> 00:10:51,600
imputation

247
00:10:51,600 --> 00:10:54,899
uh remove noise sometimes we remove the

248
00:10:54,899 --> 00:10:56,459
outliers

249
00:10:56,459 --> 00:10:59,459
and the input missing data

250
00:10:59,459 --> 00:11:01,260
and finally

251
00:11:01,260 --> 00:11:04,200
for spark is concerning data we have a

252
00:11:04,200 --> 00:11:06,480
feature Transformations and in feature

253
00:11:06,480 --> 00:11:08,820
Transformations usually we do feature

254
00:11:08,820 --> 00:11:10,380
engineering and what is feature

255
00:11:10,380 --> 00:11:11,579
engineering

256
00:11:11,579 --> 00:11:13,920
well this is arguably the most important

257
00:11:13,920 --> 00:11:17,399
step in this data science pipeline

258
00:11:17,399 --> 00:11:20,160
it it's where we include business and

259
00:11:20,160 --> 00:11:24,120
domain knowledge in into our model

260
00:11:24,120 --> 00:11:28,019
by crafting by crafting smart features

261
00:11:28,019 --> 00:11:31,019
okay next so so far we've been we've

262
00:11:31,019 --> 00:11:32,940
been dealing with data now it's time to

263
00:11:32,940 --> 00:11:34,980
deal with models

264
00:11:34,980 --> 00:11:37,320
first first thing we need to do is to

265
00:11:37,320 --> 00:11:38,940
establish a base model we need to choose

266
00:11:38,940 --> 00:11:42,540
a baseline model amongst several models

267
00:11:42,540 --> 00:11:46,860
then we need to perform what's called

268
00:11:46,860 --> 00:11:48,920
um

269
00:11:48,920 --> 00:11:51,060
optimization hyper parameter

270
00:11:51,060 --> 00:11:53,760
optimization and then we do feature

271
00:11:53,760 --> 00:11:55,500
selection to choose the features we

272
00:11:55,500 --> 00:11:58,200
really need for the model

273
00:11:58,200 --> 00:12:00,899
okay so next

274
00:12:00,899 --> 00:12:04,320
we have model

275
00:12:04,320 --> 00:12:06,420
um evaluation so what is model

276
00:12:06,420 --> 00:12:09,240
evaluation I I showed before right we

277
00:12:09,240 --> 00:12:11,640
need to evaluate the model using some

278
00:12:11,640 --> 00:12:15,120
test sets and most importantly here we

279
00:12:15,120 --> 00:12:17,160
need to understand how to correlate the

280
00:12:17,160 --> 00:12:18,660
model performance metrics to business

281
00:12:18,660 --> 00:12:20,279
performance metrics because they

282
00:12:20,279 --> 00:12:23,700
stakeholders they are interested in this

283
00:12:23,700 --> 00:12:26,700
metrics and us as a data scientists we

284
00:12:26,700 --> 00:12:28,500
are interested in this so we need to

285
00:12:28,500 --> 00:12:30,200
understand how we can translate

286
00:12:30,200 --> 00:12:33,019
make a translation between them

287
00:12:33,019 --> 00:12:35,940
so modulator interpretability answers

288
00:12:35,940 --> 00:12:38,339
the question what features affect most

289
00:12:38,339 --> 00:12:40,260
of the model's predictions

290
00:12:40,260 --> 00:12:42,240
sometimes this is a business requirement

291
00:12:42,240 --> 00:12:44,040
you know we need to know which features

292
00:12:44,040 --> 00:12:46,279
are more important and

293
00:12:46,279 --> 00:12:50,040
for more important for model prediction

294
00:12:50,040 --> 00:12:52,260
productionization

295
00:12:52,260 --> 00:12:54,839
uh depending on on your company you you

296
00:12:54,839 --> 00:12:57,000
can have your internal ecosystem so you

297
00:12:57,000 --> 00:13:00,180
need to refactor all your code

298
00:13:00,180 --> 00:13:03,600
that so far we usually Implement in a

299
00:13:03,600 --> 00:13:05,459
lab environment

300
00:13:05,459 --> 00:13:08,519
so you use your internal system that

301
00:13:08,519 --> 00:13:10,740
could be based in Cloud environments or

302
00:13:10,740 --> 00:13:12,500
multi-cloud

303
00:13:12,500 --> 00:13:14,519
and then finally

304
00:13:14,519 --> 00:13:16,860
we have the monitoring what is the

305
00:13:16,860 --> 00:13:19,560
monitoring phase monitoring phase we

306
00:13:19,560 --> 00:13:21,660
want to keep checking the health of the

307
00:13:21,660 --> 00:13:23,940
system as a whole so we want to check

308
00:13:23,940 --> 00:13:29,519
the system resources like memory CPU

309
00:13:29,519 --> 00:13:32,339
consumption storage Etc we want to

310
00:13:32,339 --> 00:13:34,920
constantly monitor the model

311
00:13:34,920 --> 00:13:38,100
we want to keep an eye on this Matrix

312
00:13:38,100 --> 00:13:39,899
to understand if the model is working

313
00:13:39,899 --> 00:13:43,200
and we need to keep an eye on the data

314
00:13:43,200 --> 00:13:44,700
distributions because if the

315
00:13:44,700 --> 00:13:46,740
distribution changes

316
00:13:46,740 --> 00:13:49,740
maybe the model stops working so this is

317
00:13:49,740 --> 00:13:51,660
called Data drifting so you need to keep

318
00:13:51,660 --> 00:13:53,760
an eye on the model and the distribution

319
00:13:53,760 --> 00:13:56,700
of the future the features

320
00:13:56,700 --> 00:13:58,019
great

321
00:13:58,019 --> 00:13:59,399
so

322
00:13:59,399 --> 00:14:02,040
it this brings us to the first case so

323
00:14:02,040 --> 00:14:03,300
the first case

324
00:14:03,300 --> 00:14:05,880
Security operation Center socks receive

325
00:14:05,880 --> 00:14:07,920
huge amounts of alerts per day

326
00:14:07,920 --> 00:14:10,139
how to prioritize the analysis of those

327
00:14:10,139 --> 00:14:12,480
alerts the triaging process

328
00:14:12,480 --> 00:14:14,220
to decrease the mean time to detection

329
00:14:14,220 --> 00:14:16,980
in the meantime to response

330
00:14:16,980 --> 00:14:22,200
so what we have here is basically uh

331
00:14:22,200 --> 00:14:24,600
problem in which we want to minimize

332
00:14:24,600 --> 00:14:27,420
some metrics right some business metrics

333
00:14:27,420 --> 00:14:31,260
when to minimize mttd and mttr

334
00:14:31,260 --> 00:14:35,779
okay how can how can we do that

335
00:14:36,540 --> 00:14:39,180
so here's a simplified scenario

336
00:14:39,180 --> 00:14:41,100
of our problem

337
00:14:41,100 --> 00:14:45,060
we have a CN the CM triggers an alert

338
00:14:45,060 --> 00:14:47,399
this goes to the stock analyst the stock

339
00:14:47,399 --> 00:14:48,660
analyst

340
00:14:48,660 --> 00:14:50,880
performs his job and

341
00:14:50,880 --> 00:14:53,360
finally he decides if that

342
00:14:53,360 --> 00:14:56,279
alert is a false positive and provides a

343
00:14:56,279 --> 00:14:58,860
report or if it's a true positive and he

344
00:14:58,860 --> 00:15:00,420
creates an incident

345
00:15:00,420 --> 00:15:03,360
and also report and that goes to the

346
00:15:03,360 --> 00:15:05,040
incident response team that's

347
00:15:05,040 --> 00:15:07,680
responsible to handle the incident and

348
00:15:07,680 --> 00:15:09,240
also in the end

349
00:15:09,240 --> 00:15:11,760
we got we got a report so this is this

350
00:15:11,760 --> 00:15:16,500
simplified flow that of the problem and

351
00:15:16,500 --> 00:15:18,360
we want to

352
00:15:18,360 --> 00:15:21,300
optimize this decision making process

353
00:15:21,300 --> 00:15:23,599
here

354
00:15:26,820 --> 00:15:29,339
so let's formulate some hypothesis to

355
00:15:29,339 --> 00:15:31,440
help us solving that problem

356
00:15:31,440 --> 00:15:34,320
so the first hypothesis and the only one

357
00:15:34,320 --> 00:15:36,839
for this problem is that prioritizing

358
00:15:36,839 --> 00:15:39,060
alerts based on their probability of

359
00:15:39,060 --> 00:15:41,279
generating incidents reduces the mean

360
00:15:41,279 --> 00:15:42,959
time to detection in the meantime to

361
00:15:42,959 --> 00:15:44,480
response

362
00:15:44,480 --> 00:15:47,399
we want to put that hypothesis into test

363
00:15:47,399 --> 00:15:51,360
right so we need to be the model

364
00:15:51,360 --> 00:15:54,240
and here you can see there is this guy

365
00:15:54,240 --> 00:15:56,579
here this angry sock analyst and why is

366
00:15:56,579 --> 00:15:57,839
he angry

367
00:15:57,839 --> 00:15:59,699
he's angry because he knows that

368
00:15:59,699 --> 00:16:01,019
although

369
00:16:01,019 --> 00:16:02,220
he can

370
00:16:02,220 --> 00:16:06,120
use this alert level

371
00:16:06,120 --> 00:16:08,519
to choose which ality should handle

372
00:16:08,519 --> 00:16:10,079
first

373
00:16:10,079 --> 00:16:12,060
he knows that if he only takes into

374
00:16:12,060 --> 00:16:14,339
consideration that information he's

375
00:16:14,339 --> 00:16:15,839
going to end up with a lot of false

376
00:16:15,839 --> 00:16:18,540
positives so he needs to take into

377
00:16:18,540 --> 00:16:19,920
consideration this order information

378
00:16:19,920 --> 00:16:23,339
about the alert and we know that usually

379
00:16:23,339 --> 00:16:25,560
it's overwhelming

380
00:16:25,560 --> 00:16:28,620
so the data science guys kick in

381
00:16:28,620 --> 00:16:32,459
and then offer a solution in which we

382
00:16:32,459 --> 00:16:35,100
have an additional column here and this

383
00:16:35,100 --> 00:16:37,079
additional column here has a number that

384
00:16:37,079 --> 00:16:39,720
represents the probability or likelihood

385
00:16:39,720 --> 00:16:41,040
of that

386
00:16:41,040 --> 00:16:43,680
particular Alert in generating an

387
00:16:43,680 --> 00:16:46,139
incident based on what based on the

388
00:16:46,139 --> 00:16:48,060
alert level and based on this other

389
00:16:48,060 --> 00:16:50,639
information and more importantly based

390
00:16:50,639 --> 00:16:53,759
on the alerts that generated and didn't

391
00:16:53,759 --> 00:16:57,019
generate alerts in the past so we we

392
00:16:57,019 --> 00:16:59,940
want we need to learn what happened and

393
00:16:59,940 --> 00:17:02,339
then associate to this label

394
00:17:02,339 --> 00:17:05,160
and here so now he's more cool why

395
00:17:05,160 --> 00:17:07,260
because he can just take a look at this

396
00:17:07,260 --> 00:17:08,459
number

397
00:17:08,459 --> 00:17:11,299
and use that number as the main

398
00:17:11,299 --> 00:17:14,359
guideline for him to choose the alerts

399
00:17:14,359 --> 00:17:17,640
and quickly review this information as a

400
00:17:17,640 --> 00:17:20,280
sanity check

401
00:17:20,280 --> 00:17:22,260
okay

402
00:17:22,260 --> 00:17:24,980
how can we implement this solution using

403
00:17:24,980 --> 00:17:27,359
data science we need to implement what's

404
00:17:27,359 --> 00:17:29,640
called data science pipeline

405
00:17:29,640 --> 00:17:32,820
and the data science pipeline in this

406
00:17:32,820 --> 00:17:33,840
case

407
00:17:33,840 --> 00:17:36,120
we have the date distraction and the

408
00:17:36,120 --> 00:17:38,039
date extraction

409
00:17:38,039 --> 00:17:41,280
all the data we needed came from the CN

410
00:17:41,280 --> 00:17:44,700
it management software for for the sock

411
00:17:44,700 --> 00:17:48,960
and rest apis we connected using rest

412
00:17:48,960 --> 00:17:50,760
apis

413
00:17:50,760 --> 00:17:52,620
feature engineering so feature

414
00:17:52,620 --> 00:17:56,700
engineering is usually is useful to ask

415
00:17:56,700 --> 00:17:59,760
those questions you know who the user

416
00:17:59,760 --> 00:18:02,520
involved in the alert what the

417
00:18:02,520 --> 00:18:05,940
application or original destination when

418
00:18:05,940 --> 00:18:10,620
timestamp of the events of the alert how

419
00:18:10,620 --> 00:18:12,840
and how here is a

420
00:18:12,840 --> 00:18:17,039
is related to the user actions that were

421
00:18:17,039 --> 00:18:19,320
that were three that that triggered the

422
00:18:19,320 --> 00:18:21,120
alert

423
00:18:21,120 --> 00:18:23,400
and how much so how much inbounded

424
00:18:23,400 --> 00:18:25,559
outbound traffic

425
00:18:25,559 --> 00:18:27,419
and there is this this very very

426
00:18:27,419 --> 00:18:29,220
important Point here why

427
00:18:29,220 --> 00:18:31,740
so unfortunately we can't answer that

428
00:18:31,740 --> 00:18:33,539
because data science is not a crystal

429
00:18:33,539 --> 00:18:37,440
ball so we we need to to live with that

430
00:18:37,440 --> 00:18:38,580
okay

431
00:18:38,580 --> 00:18:41,460
and there is one feature that's very

432
00:18:41,460 --> 00:18:44,580
very important that we came out

433
00:18:44,580 --> 00:18:46,799
after some time

434
00:18:46,799 --> 00:18:49,799
that is called content score and what is

435
00:18:49,799 --> 00:18:52,860
content score content score is is a

436
00:18:52,860 --> 00:18:55,080
feature based on the words in the alerts

437
00:18:55,080 --> 00:18:57,900
that generated incidents so we got the

438
00:18:57,900 --> 00:19:00,059
Alex that generate incidents and use

439
00:19:00,059 --> 00:19:01,380
some

440
00:19:01,380 --> 00:19:05,580
um natural language processing technique

441
00:19:05,580 --> 00:19:08,640
called TF IDF it's term term frequency

442
00:19:08,640 --> 00:19:10,919
inverse document frequency to get that

443
00:19:10,919 --> 00:19:12,059
score

444
00:19:12,059 --> 00:19:14,400
and we use that information as a feature

445
00:19:14,400 --> 00:19:16,559
in our model

446
00:19:16,559 --> 00:19:19,620
so next we need to

447
00:19:19,620 --> 00:19:21,720
perform model development and for model

448
00:19:21,720 --> 00:19:23,480
development

449
00:19:23,480 --> 00:19:26,880
first we need to decide what kind of

450
00:19:26,880 --> 00:19:28,200
model we need

451
00:19:28,200 --> 00:19:31,080
and we know we have we got the past day

452
00:19:31,080 --> 00:19:33,559
in the past data we have the the

453
00:19:33,559 --> 00:19:37,320
sorry we have the labels so and since we

454
00:19:37,320 --> 00:19:39,120
have the labels we have a supervised

455
00:19:39,120 --> 00:19:40,559
learning model

456
00:19:40,559 --> 00:19:44,000
and therefore we can use

457
00:19:44,400 --> 00:19:47,100
uh supervised learning model sorry and

458
00:19:47,100 --> 00:19:48,740
then we need to perform what's called

459
00:19:48,740 --> 00:19:51,780
dimensionality reduction why

460
00:19:51,780 --> 00:19:56,039
we have this TF IDF technique and this

461
00:19:56,039 --> 00:19:57,600
technique introduces a lot of features

462
00:19:57,600 --> 00:20:00,000
in the data set so we need somehow to

463
00:20:00,000 --> 00:20:02,000
reduce those features because models

464
00:20:02,000 --> 00:20:04,200
usually they

465
00:20:04,200 --> 00:20:07,020
they suffer from high dimensionality and

466
00:20:07,020 --> 00:20:09,480
suffer from noise when we have a lot of

467
00:20:09,480 --> 00:20:11,220
features so we need to some way to

468
00:20:11,220 --> 00:20:13,440
reduce and to reduce that we can use a

469
00:20:13,440 --> 00:20:14,640
technique called

470
00:20:14,640 --> 00:20:16,320
dimensionality reduction more

471
00:20:16,320 --> 00:20:18,960
specifically PCA principal component

472
00:20:18,960 --> 00:20:20,640
analysis it reduces the number of

473
00:20:20,640 --> 00:20:24,179
features while keeping the variance High

474
00:20:24,179 --> 00:20:27,780
next we need to choose a baseline model

475
00:20:27,780 --> 00:20:29,580
and to choose a baseline model there is

476
00:20:29,580 --> 00:20:31,620
a very nice tool called laser predict

477
00:20:31,620 --> 00:20:35,940
that can be used to uh actually these

478
00:20:35,940 --> 00:20:37,080
two

479
00:20:37,080 --> 00:20:41,760
gets our our data set and builds

480
00:20:41,760 --> 00:20:44,760
50 models I I think from that

481
00:20:44,760 --> 00:20:47,460
scikit-learn library and give us a

482
00:20:47,460 --> 00:20:49,559
summary of the results using several

483
00:20:49,559 --> 00:20:51,360
metrics performance metrics and then

484
00:20:51,360 --> 00:20:52,860
from there we can choose the best ones

485
00:20:52,860 --> 00:20:55,679
and understand of course why they were

486
00:20:55,679 --> 00:20:57,240
the best ones

487
00:20:57,240 --> 00:20:59,580
and uh it turns out that

488
00:20:59,580 --> 00:21:01,440
for this problem we chose the light

489
00:21:01,440 --> 00:21:04,520
gradient boosting machines which is a

490
00:21:04,520 --> 00:21:08,539
Ensemble three state three state of art

491
00:21:08,539 --> 00:21:11,039
machine learning method

492
00:21:11,039 --> 00:21:12,720
and then we perform hyper parameter

493
00:21:12,720 --> 00:21:14,400
optimization

494
00:21:14,400 --> 00:21:16,919
using a basin approach

495
00:21:16,919 --> 00:21:18,660
wire Basin approach because there are

496
00:21:18,660 --> 00:21:21,660
too many the search space here is too

497
00:21:21,660 --> 00:21:25,200
large and if if we do brute force or

498
00:21:25,200 --> 00:21:27,600
grid search or random search

499
00:21:27,600 --> 00:21:29,400
it's going to take a lot of time and it

500
00:21:29,400 --> 00:21:31,260
won't bring us good results so we use

501
00:21:31,260 --> 00:21:33,120
the base and optimization this is Mark

502
00:21:33,120 --> 00:21:35,940
that is a smarter way to to search for

503
00:21:35,940 --> 00:21:37,440
hyper parameters

504
00:21:37,440 --> 00:21:39,780
and then you see there is this number

505
00:21:39,780 --> 00:21:42,179
here 80 and 20. it means that we should

506
00:21:42,179 --> 00:21:44,400
you usually spend eighty percent of our

507
00:21:44,400 --> 00:21:47,520
time doing data pre-processing and

508
00:21:47,520 --> 00:21:49,320
feature engineering and so on and 20

509
00:21:49,320 --> 00:21:49,680
percent

510
00:21:49,680 --> 00:21:51,200
[Music]

511
00:21:51,200 --> 00:21:53,760
uh playing with the models why because

512
00:21:53,760 --> 00:21:56,340
of what I showed before garbage in

513
00:21:56,340 --> 00:21:59,340
garbage out if this is bad no matter

514
00:21:59,340 --> 00:22:00,960
what kind of model you do or

515
00:22:00,960 --> 00:22:04,080
optimization the result also will be

516
00:22:04,080 --> 00:22:05,700
not so good

517
00:22:05,700 --> 00:22:07,020
so please

518
00:22:07,020 --> 00:22:09,600
uh pay attention to that

519
00:22:09,600 --> 00:22:10,679
okay

520
00:22:10,679 --> 00:22:13,080
using that approach

521
00:22:13,080 --> 00:22:15,179
we got this results

522
00:22:15,179 --> 00:22:17,100
we got the model performance in six

523
00:22:17,100 --> 00:22:19,080
months in three months in production we

524
00:22:19,080 --> 00:22:21,960
got the Precision of 90 percent and

525
00:22:21,960 --> 00:22:23,640
precision means the higher the Precision

526
00:22:23,640 --> 00:22:26,460
the lower the number of false positives

527
00:22:26,460 --> 00:22:30,000
so we got a recall of 80 percent so the

528
00:22:30,000 --> 00:22:32,039
higher the recall there were the false

529
00:22:32,039 --> 00:22:34,860
negatives and we got of F1 score of

530
00:22:34,860 --> 00:22:37,080
eighty percent what is F1 score F1

531
00:22:37,080 --> 00:22:39,240
scores the harmonic mean between

532
00:22:39,240 --> 00:22:41,360
precision and recall and show how they

533
00:22:41,360 --> 00:22:44,940
balance how how uh how is the balance

534
00:22:44,940 --> 00:22:47,480
between them

535
00:22:47,700 --> 00:22:49,260
um so

536
00:22:49,260 --> 00:22:52,200
actually if I ask you what what do you

537
00:22:52,200 --> 00:22:54,419
want more Precision or more equal you

538
00:22:54,419 --> 00:22:57,720
say both I want I want everything

539
00:22:57,720 --> 00:23:00,539
so then we can use F1 score to to get

540
00:23:00,539 --> 00:23:02,340
The Sweet Spot between them because

541
00:23:02,340 --> 00:23:04,440
precision and recall they there are two

542
00:23:04,440 --> 00:23:05,940
metrics that fight against each other

543
00:23:05,940 --> 00:23:08,039
the higher the Precision the lower the

544
00:23:08,039 --> 00:23:10,440
recall and vice versa so the F1 score is

545
00:23:10,440 --> 00:23:12,179
a good metric to use when you're

546
00:23:12,179 --> 00:23:13,919
optimizing your model

547
00:23:13,919 --> 00:23:16,799
and how that translates to

548
00:23:16,799 --> 00:23:18,539
business metrics

549
00:23:18,539 --> 00:23:20,360
main decrease of

550
00:23:20,360 --> 00:23:24,780
mttg 20 and mean decrease of mttr 15

551
00:23:24,780 --> 00:23:27,139
percent

552
00:23:28,980 --> 00:23:31,860
case two case two

553
00:23:31,860 --> 00:23:34,140
we have

554
00:23:34,140 --> 00:23:37,440
this story so once upon a time a user

555
00:23:37,440 --> 00:23:38,940
used his credentials to make direct

556
00:23:38,940 --> 00:23:41,880
requests to some apis used by a

557
00:23:41,880 --> 00:23:45,059
company's app it worked then he created

558
00:23:45,059 --> 00:23:48,539
a bot to automate and scale that process

559
00:23:48,539 --> 00:23:51,419
finally he created a product based on

560
00:23:51,419 --> 00:23:55,020
that data and started monetizing on it

561
00:23:55,020 --> 00:23:56,940
and that's a problem so how can we

562
00:23:56,940 --> 00:23:59,880
detect what users are using the Bots and

563
00:23:59,880 --> 00:24:02,400
consequently the bots so here we have a

564
00:24:02,400 --> 00:24:04,440
detection problem right we want to find

565
00:24:04,440 --> 00:24:09,179
the bad guys the Bots and the guys

566
00:24:09,179 --> 00:24:12,740
who are using the Bots

567
00:24:14,400 --> 00:24:18,360
so again it's it's useful to to draw a

568
00:24:18,360 --> 00:24:20,940
simplified scenario of the situation

569
00:24:20,940 --> 00:24:24,480
so here we have the good guys good guy

570
00:24:24,480 --> 00:24:28,200
using the Acme Corporation app and of

571
00:24:28,200 --> 00:24:30,299
course he's using his authentic

572
00:24:30,299 --> 00:24:32,460
credentials get a token and this is how

573
00:24:32,460 --> 00:24:33,900
it should be

574
00:24:33,900 --> 00:24:36,120
then there is this guy and he has this

575
00:24:36,120 --> 00:24:38,760
idea let's reverse engineer the

576
00:24:38,760 --> 00:24:41,460
authentication procedure procedure from

577
00:24:41,460 --> 00:24:44,820
this app understand how it works

578
00:24:44,820 --> 00:24:48,240
and using my own uh credentials I can of

579
00:24:48,240 --> 00:24:50,340
course get a token

580
00:24:50,340 --> 00:24:53,760
because I'm a client from uh Acme in the

581
00:24:53,760 --> 00:24:56,400
umbrella and then I can forward this

582
00:24:56,400 --> 00:24:59,100
token to a bot and that bot can extract

583
00:24:59,100 --> 00:25:02,280
huge huge amounts of information

584
00:25:02,280 --> 00:25:04,320
bring that information back to the app

585
00:25:04,320 --> 00:25:07,320
and here we can like provide some

586
00:25:07,320 --> 00:25:09,840
products some reports or some some

587
00:25:09,840 --> 00:25:11,520
things like that

588
00:25:11,520 --> 00:25:14,520
that Alchemy Corporation doesn't doesn't

589
00:25:14,520 --> 00:25:16,559
provide

590
00:25:16,559 --> 00:25:20,039
and then you can so basically invite

591
00:25:20,039 --> 00:25:22,860
the customers here from Acme and

592
00:25:22,860 --> 00:25:25,740
they pay just a small fee and subscribe

593
00:25:25,740 --> 00:25:27,000
to my service

594
00:25:27,000 --> 00:25:30,360
and this guy becomes rich

595
00:25:30,360 --> 00:25:32,880
you know using data that he can't use

596
00:25:32,880 --> 00:25:35,520
well this is this looks criminal right

597
00:25:35,520 --> 00:25:37,440
this is a real problem

598
00:25:37,440 --> 00:25:41,220
and how can we solve that

599
00:25:41,220 --> 00:25:44,299
we can detect these guys in Block

600
00:25:44,299 --> 00:25:46,799
how can we detect

601
00:25:46,799 --> 00:25:49,500
so that's the problem that we have

602
00:25:49,500 --> 00:25:52,320
so let's formulate some hypothesis

603
00:25:52,320 --> 00:25:54,840
first one human user behavior is

604
00:25:54,840 --> 00:25:56,940
different from bot Behavior we need to

605
00:25:56,940 --> 00:25:59,100
this hypothesis must be true if you want

606
00:25:59,100 --> 00:26:03,360
to use behavior that behavioral analysis

607
00:26:03,360 --> 00:26:05,940
second human user behavior is more

608
00:26:05,940 --> 00:26:08,460
diverse than both Behavior what does

609
00:26:08,460 --> 00:26:10,620
that mean so let's think about that for

610
00:26:10,620 --> 00:26:12,620
a minute

611
00:26:12,620 --> 00:26:16,799
you get a bot what is a bot is it it's a

612
00:26:16,799 --> 00:26:19,799
script basically code what's code it's a

613
00:26:19,799 --> 00:26:23,039
sequence of instructions we can map that

614
00:26:23,039 --> 00:26:24,779
into a graph

615
00:26:24,779 --> 00:26:27,240
for example control flow graph and it's

616
00:26:27,240 --> 00:26:30,000
going to be very stable now think about

617
00:26:30,000 --> 00:26:31,679
human behavior

618
00:26:31,679 --> 00:26:33,960
human behavior is a bit more random you

619
00:26:33,960 --> 00:26:35,580
know every time you you access your app

620
00:26:35,580 --> 00:26:38,159
you do something slightly different

621
00:26:38,159 --> 00:26:40,320
you take more time less time you do

622
00:26:40,320 --> 00:26:42,539
something before you do some other thing

623
00:26:42,539 --> 00:26:44,820
and every time it's a bit different so

624
00:26:44,820 --> 00:26:47,039
it's less stable we can try to compare

625
00:26:47,039 --> 00:26:49,620
this the stability of these behaviors

626
00:26:49,620 --> 00:26:52,140
and use it as a feature in our model to

627
00:26:52,140 --> 00:26:53,400
try to capture

628
00:26:53,400 --> 00:26:57,299
body behavior in human user Behavior

629
00:26:57,299 --> 00:27:00,500
so let's try

630
00:27:00,840 --> 00:27:03,900
here's an example of

631
00:27:03,900 --> 00:27:06,600
of behavior but first we need to

632
00:27:06,600 --> 00:27:09,000
Define behavior what is behavior in this

633
00:27:09,000 --> 00:27:10,140
context

634
00:27:10,140 --> 00:27:12,900
well behavior is

635
00:27:12,900 --> 00:27:15,299
temporal sequence of API operation

636
00:27:15,299 --> 00:27:17,400
request is made by a user and here we

637
00:27:17,400 --> 00:27:19,820
have some examples of behavior see login

638
00:27:19,820 --> 00:27:23,159
gets a balance transfer and logs out we

639
00:27:23,159 --> 00:27:24,120
can

640
00:27:24,120 --> 00:27:28,080
track this Behavior using a table and

641
00:27:28,080 --> 00:27:29,700
this table contains

642
00:27:29,700 --> 00:27:32,100
the the ID user ID the sequence

643
00:27:32,100 --> 00:27:35,940
sequences of actions and timestamps

644
00:27:35,940 --> 00:27:39,539
so from that graph we extract some

645
00:27:39,539 --> 00:27:40,320
um

646
00:27:40,320 --> 00:27:42,600
almost

647
00:27:42,600 --> 00:27:43,500
um

648
00:27:43,500 --> 00:27:47,000
tabular data right

649
00:27:47,760 --> 00:27:49,799
so what's next

650
00:27:49,799 --> 00:27:52,559
well this is La this is how a behavioral

651
00:27:52,559 --> 00:27:53,820
graph

652
00:27:53,820 --> 00:27:56,820
usually looks like in reality okay it's

653
00:27:56,820 --> 00:27:58,679
much more complicated than my example

654
00:27:58,679 --> 00:28:01,500
but for machine learning algorithms

655
00:28:01,500 --> 00:28:04,380
that's a piece of cake so no fear not

656
00:28:04,380 --> 00:28:06,840
about that

657
00:28:06,840 --> 00:28:10,559
again data science pipeline

658
00:28:10,559 --> 00:28:14,100
data science pipeline in this case we

659
00:28:14,100 --> 00:28:16,260
need to to do data extraction as usual

660
00:28:16,260 --> 00:28:18,419
all the data necessary

661
00:28:18,419 --> 00:28:20,340
came from the CM

662
00:28:20,340 --> 00:28:23,700
again we ask those questions that that

663
00:28:23,700 --> 00:28:25,260
are very useful for building features

664
00:28:25,260 --> 00:28:26,820
you know who

665
00:28:26,820 --> 00:28:30,000
what when here we are concerning

666
00:28:30,000 --> 00:28:32,940
concerned about sequences of

667
00:28:32,940 --> 00:28:36,960
of actions and when those actions happen

668
00:28:36,960 --> 00:28:40,140
how much so this this feature is

669
00:28:40,140 --> 00:28:42,900
important why because frequency is

670
00:28:42,900 --> 00:28:44,039
something very

671
00:28:44,039 --> 00:28:46,740
uh important when we are comparing

672
00:28:46,740 --> 00:28:48,659
machine behavior in the human behavior

673
00:28:48,659 --> 00:28:50,279
because machines as I said they're more

674
00:28:50,279 --> 00:28:53,159
stable they are more precise the time

675
00:28:53,159 --> 00:28:55,260
the timing is better the amount of time

676
00:28:55,260 --> 00:28:57,720
to extract information is lower than

677
00:28:57,720 --> 00:28:59,700
people it's more random so maybe we can

678
00:28:59,700 --> 00:29:02,299
use that feature as a

679
00:29:02,299 --> 00:29:06,120
as information for our model to decide

680
00:29:06,120 --> 00:29:08,520
okay and what else

681
00:29:08,520 --> 00:29:10,740
how much there is one thing that you

682
00:29:10,740 --> 00:29:12,299
want to measure according to our

683
00:29:12,299 --> 00:29:16,020
hypothesis is that how diverse are the

684
00:29:16,020 --> 00:29:17,760
sweetened sequences

685
00:29:17,760 --> 00:29:19,860
and to measure that we can use sequence

686
00:29:19,860 --> 00:29:22,380
entropy and to use to measure sequence

687
00:29:22,380 --> 00:29:25,020
entropy we can use a metric called

688
00:29:25,020 --> 00:29:27,299
Channel entropy

689
00:29:27,299 --> 00:29:30,919
okay and next one more question how much

690
00:29:30,919 --> 00:29:33,960
now a sequence similarity

691
00:29:33,960 --> 00:29:37,020
we can somehow compare the sequences

692
00:29:37,020 --> 00:29:38,940
and there are several methods for that

693
00:29:38,940 --> 00:29:41,580
okay but one method worked quite well

694
00:29:41,580 --> 00:29:42,779
here

695
00:29:42,779 --> 00:29:45,720
was this one for it for it was a the DNA

696
00:29:45,720 --> 00:29:47,700
sequence similarity algorithms from the

697
00:29:47,700 --> 00:29:49,500
bioinformatics

698
00:29:49,500 --> 00:29:52,020
from a tool called Blast basic local

699
00:29:52,020 --> 00:29:54,059
align Search tool and it's fully

700
00:29:54,059 --> 00:29:57,980
implemented in the library bio python

701
00:29:58,559 --> 00:30:00,419
so obviously we got a lot of interesting

702
00:30:00,419 --> 00:30:02,279
features there

703
00:30:02,279 --> 00:30:04,860
yes and the

704
00:30:04,860 --> 00:30:06,960
now modeling

705
00:30:06,960 --> 00:30:08,820
we know that we have an unsupervised

706
00:30:08,820 --> 00:30:10,740
problem because we don't have the labels

707
00:30:10,740 --> 00:30:13,080
we want to figure them out

708
00:30:13,080 --> 00:30:16,440
the bad guys the good guys so what what

709
00:30:16,440 --> 00:30:18,539
we should do we should use some

710
00:30:18,539 --> 00:30:21,480
unsupervised model to solve this problem

711
00:30:21,480 --> 00:30:24,779
and in what we want is to clusterize and

712
00:30:24,779 --> 00:30:27,059
create cluster of human behaviors in

713
00:30:27,059 --> 00:30:29,880
both Behavior

714
00:30:29,880 --> 00:30:32,279
and after some research we chose this

715
00:30:32,279 --> 00:30:35,220
algorithm it's called the VDB scan that

716
00:30:35,220 --> 00:30:37,140
stands it stands for variable density

717
00:30:37,140 --> 00:30:39,600
based spatial clustering applications

718
00:30:39,600 --> 00:30:41,520
with noise

719
00:30:41,520 --> 00:30:45,000
yeah that's a mouthful

720
00:30:45,000 --> 00:30:47,940
and uh but actually it's a quite good

721
00:30:47,940 --> 00:30:49,740
one to

722
00:30:49,740 --> 00:30:53,580
um detect clusters that have variable

723
00:30:53,580 --> 00:30:56,760
density and different shapes

724
00:30:56,760 --> 00:30:59,039
so it's a nice algorithm

725
00:30:59,039 --> 00:31:02,700
and then we got each cluster and look in

726
00:31:02,700 --> 00:31:05,640
within them for anomalies

727
00:31:05,640 --> 00:31:07,559
hoping that those anomalies will

728
00:31:07,559 --> 00:31:10,980
represent anomalous behaviors or Bots

729
00:31:10,980 --> 00:31:17,779
and human beings beings doing bad things

730
00:31:18,539 --> 00:31:21,419
so it turns out that we got this one

731
00:31:21,419 --> 00:31:23,279
this result

732
00:31:23,279 --> 00:31:24,779
and take a look

733
00:31:24,779 --> 00:31:28,799
this big blobs here is this this this it

734
00:31:28,799 --> 00:31:32,059
represents human user Behavior

735
00:31:32,059 --> 00:31:35,760
and this points in between the Clusters

736
00:31:35,760 --> 00:31:38,880
they represent bot Behavior

737
00:31:38,880 --> 00:31:41,399
that's very interesting and

738
00:31:41,399 --> 00:31:43,740
it turns out that

739
00:31:43,740 --> 00:31:46,500
if we we measured of course the

740
00:31:46,500 --> 00:31:48,779
performance of the model in six months

741
00:31:48,779 --> 00:31:49,980
in production

742
00:31:49,980 --> 00:31:52,860
and we got the Precision of 75 percent

743
00:31:52,860 --> 00:31:55,380
what does that mean

744
00:31:55,380 --> 00:31:57,000
it means that

745
00:31:57,000 --> 00:31:59,820
out of 100 data points that were

746
00:31:59,820 --> 00:32:02,720
classified by the model as a positive or

747
00:32:02,720 --> 00:32:05,059
Bots

748
00:32:05,059 --> 00:32:07,860
70.75 percent of them were really

749
00:32:07,860 --> 00:32:11,700
confirmed as bots so the model worked

750
00:32:11,700 --> 00:32:13,140
well

751
00:32:13,140 --> 00:32:15,419
and the business Matrix

752
00:32:15,419 --> 00:32:17,580
this model is not in production okay

753
00:32:17,580 --> 00:32:20,399
it's used to check the data from time to

754
00:32:20,399 --> 00:32:23,159
time by the security team

755
00:32:23,159 --> 00:32:26,399
and uh but interestingly enough this

756
00:32:26,399 --> 00:32:28,620
result led to a budget approval to

757
00:32:28,620 --> 00:32:30,480
redesign the whole security mechanism of

758
00:32:30,480 --> 00:32:32,940
the apis and the apps they can take into

759
00:32:32,940 --> 00:32:35,820
account this modern adversaries so it

760
00:32:35,820 --> 00:32:37,380
helped their architecture in The Blue

761
00:32:37,380 --> 00:32:38,460
Team to

762
00:32:38,460 --> 00:32:41,279
to build a better model sorry a better

763
00:32:41,279 --> 00:32:43,880
application

764
00:32:46,559 --> 00:32:49,919
in case three so cases three

765
00:32:49,919 --> 00:32:52,740
we have a here problem about data

766
00:32:52,740 --> 00:32:55,080
exfiltration so

767
00:32:55,080 --> 00:32:58,620
preventing data exfiltration by internal

768
00:32:58,620 --> 00:33:02,039
users is a key problem in cyber security

769
00:33:02,039 --> 00:33:04,140
but also very interesting one from the

770
00:33:04,140 --> 00:33:06,600
data science perspective

771
00:33:06,600 --> 00:33:08,940
again how the user Behavior could be

772
00:33:08,940 --> 00:33:11,460
used for that purpose

773
00:33:11,460 --> 00:33:13,679
we have a detection problem again right

774
00:33:13,679 --> 00:33:15,419
we want to detect

775
00:33:15,419 --> 00:33:19,679
what users are exfiltrating data in so

776
00:33:19,679 --> 00:33:22,820
this is a detection problem

777
00:33:25,740 --> 00:33:28,440
again let's take a look at simplified

778
00:33:28,440 --> 00:33:32,120
scenario of this problem

779
00:33:32,340 --> 00:33:34,740
we have a user here that in principle he

780
00:33:34,740 --> 00:33:37,019
was a he was not black okay he was white

781
00:33:37,019 --> 00:33:39,600
and then in the end he changes

782
00:33:39,600 --> 00:33:42,659
so imagine this is white and then

783
00:33:42,659 --> 00:33:44,519
we have this Behavior profile this is

784
00:33:44,519 --> 00:33:46,500
all what we want to build Behavior

785
00:33:46,500 --> 00:33:49,080
profile and this Behavior profile

786
00:33:49,080 --> 00:33:50,880
must contain

787
00:33:50,880 --> 00:33:55,019
activities and data usage because we are

788
00:33:55,019 --> 00:33:57,179
concerned about latex filtration so we

789
00:33:57,179 --> 00:34:00,320
need to have data usage

790
00:34:00,840 --> 00:34:02,960
and out from this

791
00:34:02,960 --> 00:34:05,700
activities and this data usage you want

792
00:34:05,700 --> 00:34:07,620
to find anomalous activities and

793
00:34:07,620 --> 00:34:10,139
anomalous data usage and then we want to

794
00:34:10,139 --> 00:34:14,219
come to correlate them somehow

795
00:34:14,219 --> 00:34:18,239
to find for possible data exfiltration

796
00:34:18,239 --> 00:34:19,820
cases

797
00:34:19,820 --> 00:34:23,599
so that's the idea

798
00:34:24,060 --> 00:34:27,780
let's formulate some hypothesis

799
00:34:27,780 --> 00:34:30,320
so the first hypothesis

800
00:34:30,320 --> 00:34:33,239
normal activities sorry the normal

801
00:34:33,239 --> 00:34:35,820
activities of a user differs from his

802
00:34:35,820 --> 00:34:38,219
activities when exfiltrating data

803
00:34:38,219 --> 00:34:42,359
and the normal data usage of

804
00:34:42,359 --> 00:34:44,460
and user differs from his data usage

805
00:34:44,460 --> 00:34:48,300
when x filter 18 date so it's what we

806
00:34:48,300 --> 00:34:53,460
withdrawal here right I I want to

807
00:34:53,460 --> 00:34:55,440
find a normal list

808
00:34:55,440 --> 00:34:58,380
in in the user activities and data usage

809
00:34:58,380 --> 00:34:59,940
and then correlate

810
00:34:59,940 --> 00:35:02,460
then somehow

811
00:35:02,460 --> 00:35:04,440
so we can find

812
00:35:04,440 --> 00:35:08,540
data exploitations and cases

813
00:35:11,099 --> 00:35:13,500
here we need a more comprehensive

814
00:35:13,500 --> 00:35:16,740
definition of behavior we need to add

815
00:35:16,740 --> 00:35:19,200
more information there

816
00:35:19,200 --> 00:35:22,380
what can we do we can Define behavior

817
00:35:22,380 --> 00:35:27,000
now as a temporal sequence of tuples

818
00:35:27,000 --> 00:35:29,400
there's a list like API operation

819
00:35:29,400 --> 00:35:32,339
requests inbound data outbound data

820
00:35:32,339 --> 00:35:34,380
origin and destination they are all

821
00:35:34,380 --> 00:35:37,320
useful to detect data exfiltration okay

822
00:35:37,320 --> 00:35:40,020
made by a new data made by a user

823
00:35:40,020 --> 00:35:41,160
foreign

824
00:35:41,160 --> 00:35:44,640
here we have the behavioral graph as I

825
00:35:44,640 --> 00:35:48,359
showed before and now this table

826
00:35:48,359 --> 00:35:52,140
has the data related to this Behavior

827
00:35:52,140 --> 00:35:54,420
graph but also takes into consideration

828
00:35:54,420 --> 00:35:56,400
some other variables

829
00:35:56,400 --> 00:35:58,320
they are all sequences

830
00:35:58,320 --> 00:35:59,820
all sequences

831
00:35:59,820 --> 00:36:01,920
sequential data

832
00:36:01,920 --> 00:36:03,720
is very very powerful to detect

833
00:36:03,720 --> 00:36:06,119
behavioral anomalies so we should take

834
00:36:06,119 --> 00:36:08,040
that into consideration whenever we try

835
00:36:08,040 --> 00:36:10,619
to do anything that is related to

836
00:36:10,619 --> 00:36:13,160
behavior

837
00:36:13,200 --> 00:36:16,279
we have the

838
00:36:16,619 --> 00:36:18,839
data science pipeline for this third

839
00:36:18,839 --> 00:36:19,880
case

840
00:36:19,880 --> 00:36:23,339
so as usual

841
00:36:23,339 --> 00:36:25,079
data extraction

842
00:36:25,079 --> 00:36:27,359
again all the necessary data we needed

843
00:36:27,359 --> 00:36:29,640
came from the CM so you want to do data

844
00:36:29,640 --> 00:36:34,200
science in cyber security yes please uh

845
00:36:34,200 --> 00:36:36,000
if you don't have a CM you should get

846
00:36:36,000 --> 00:36:37,920
one okay

847
00:36:37,920 --> 00:36:41,460
they are very very rich sources of

848
00:36:41,460 --> 00:36:44,160
information for cyber security and data

849
00:36:44,160 --> 00:36:46,740
science projects

850
00:36:46,740 --> 00:36:49,500
feature engineering

851
00:36:49,500 --> 00:36:51,599
so future engineering

852
00:36:51,599 --> 00:36:54,480
usually we start asking those questions

853
00:36:54,480 --> 00:36:57,660
right what user was involved in the in

854
00:36:57,660 --> 00:37:00,260
that situation

855
00:37:00,260 --> 00:37:03,359
also the entities we are dealing with

856
00:37:03,359 --> 00:37:05,579
like the sequences all the sequences we

857
00:37:05,579 --> 00:37:08,760
need to take into consideration

858
00:37:08,760 --> 00:37:09,380
um

859
00:37:09,380 --> 00:37:13,320
timestamps when those events happened

860
00:37:13,320 --> 00:37:15,440
this is important very important because

861
00:37:15,440 --> 00:37:19,619
think about order in time of some event

862
00:37:19,619 --> 00:37:22,079
you know if we do something in a

863
00:37:22,079 --> 00:37:24,240
different order

864
00:37:24,240 --> 00:37:26,540
our purpose can be completely different

865
00:37:26,540 --> 00:37:30,180
so it's a very good track when the

866
00:37:30,180 --> 00:37:33,240
events happen and what happened in that

867
00:37:33,240 --> 00:37:36,598
particular moment

868
00:37:37,440 --> 00:37:40,380
and how much of course data usage here

869
00:37:40,380 --> 00:37:43,520
is crucial

870
00:37:44,040 --> 00:37:47,460
what else so model development

871
00:37:47,460 --> 00:37:50,640
in model development here

872
00:37:50,640 --> 00:37:53,339
we should ask a question so what's the

873
00:37:53,339 --> 00:37:55,200
question the question is How likely is a

874
00:37:55,200 --> 00:37:58,560
given user plus user action Plus data

875
00:37:58,560 --> 00:38:00,240
access pattern

876
00:38:00,240 --> 00:38:02,099
we want to understand the probability of

877
00:38:02,099 --> 00:38:04,260
that action

878
00:38:04,260 --> 00:38:07,320
happening right if it's rare or if it's

879
00:38:07,320 --> 00:38:11,599
common and when you want to measure it

880
00:38:11,880 --> 00:38:14,099
and for that we need to do this okay

881
00:38:14,099 --> 00:38:17,099
it's called a mod we need to model The

882
00:38:17,099 --> 00:38:20,099
Joint probability distribution

883
00:38:20,099 --> 00:38:23,940
of this C of this scenario and for that

884
00:38:23,940 --> 00:38:27,660
we can use a method called Markov Markov

885
00:38:27,660 --> 00:38:29,760
chains which is a probabilistic

886
00:38:29,760 --> 00:38:31,859
stochastic model it's a very old one

887
00:38:31,859 --> 00:38:34,260
it's a very simple one but it's very

888
00:38:34,260 --> 00:38:38,099
very powerful and underrated

889
00:38:38,099 --> 00:38:40,079
okay nobody likes it because it's too

890
00:38:40,079 --> 00:38:40,859
simple

891
00:38:40,859 --> 00:38:44,280
people like a lot of fancy methods no

892
00:38:44,280 --> 00:38:47,339
deep learning all that stuff forget

893
00:38:47,339 --> 00:38:48,900
about that

894
00:38:48,900 --> 00:38:51,380
forget

895
00:38:52,260 --> 00:38:53,220
um

896
00:38:53,220 --> 00:38:57,119
and out from this Markov chains

897
00:38:57,119 --> 00:38:58,560
we hope

898
00:38:58,560 --> 00:39:00,900
we get a bunch of vectors six

899
00:39:00,900 --> 00:39:03,000
dimensional vectors

900
00:39:03,000 --> 00:39:06,000
and we will find look for anomalies

901
00:39:06,000 --> 00:39:09,540
there in those vectors using uh anomaly

902
00:39:09,540 --> 00:39:11,720
detection algorithm algorithm called

903
00:39:11,720 --> 00:39:13,920
isolation Forest

904
00:39:13,920 --> 00:39:16,099
foreign

905
00:39:21,839 --> 00:39:24,599
so How likely is that behavior actions

906
00:39:24,599 --> 00:39:27,300
Plus data usage

907
00:39:27,300 --> 00:39:30,359
Let's see we know that each time every

908
00:39:30,359 --> 00:39:33,060
time the user does something using the

909
00:39:33,060 --> 00:39:35,040
app

910
00:39:35,040 --> 00:39:37,500
several graphs are generated we saw

911
00:39:37,500 --> 00:39:40,260
before so we have a user activity graph

912
00:39:40,260 --> 00:39:43,500
we have a user inbound data usage graph

913
00:39:43,500 --> 00:39:46,440
outbound origin destination and

914
00:39:46,440 --> 00:39:50,520
timestamp so we have six graphs to track

915
00:39:50,520 --> 00:39:52,740
and you want to we want to extract

916
00:39:52,740 --> 00:39:54,480
information from that graph some use it

917
00:39:54,480 --> 00:39:57,900
for information for our model

918
00:39:57,900 --> 00:39:59,640
so in order to do that we can use the

919
00:39:59,640 --> 00:40:02,099
Markov chain so what is the Markov chain

920
00:40:02,099 --> 00:40:06,839
so let's ask Mr Markov so Markov chain

921
00:40:06,839 --> 00:40:09,300
is a stochastic model describing a

922
00:40:09,300 --> 00:40:11,280
sequence of possible events in which the

923
00:40:11,280 --> 00:40:13,200
probability of each event depends only

924
00:40:13,200 --> 00:40:16,140
on the state of the previous event so

925
00:40:16,140 --> 00:40:17,700
it's a very very simple stochastic model

926
00:40:17,700 --> 00:40:19,200
that assumes that the events are

927
00:40:19,200 --> 00:40:21,599
independent

928
00:40:21,599 --> 00:40:22,920
so let's

929
00:40:22,920 --> 00:40:27,060
imagine that this graph is this one okay

930
00:40:27,060 --> 00:40:29,579
but here's like in a more pretty format

931
00:40:29,579 --> 00:40:32,880
and each node represents one action user

932
00:40:32,880 --> 00:40:35,240
action and each

933
00:40:35,240 --> 00:40:37,820
translation represents the probability

934
00:40:37,820 --> 00:40:41,400
of the user transition from one state to

935
00:40:41,400 --> 00:40:43,680
another so for example this one could be

936
00:40:43,680 --> 00:40:46,800
login and this one could be like check

937
00:40:46,800 --> 00:40:49,380
the balance if if a user does that

938
00:40:49,380 --> 00:40:51,119
several several times

939
00:40:51,119 --> 00:40:53,160
the probabilities transition probability

940
00:40:53,160 --> 00:40:54,540
is high

941
00:40:54,540 --> 00:40:57,900
right so it's a way to

942
00:40:57,900 --> 00:41:00,060
it's a way to connect the events and

943
00:41:00,060 --> 00:41:02,220
their probabilities

944
00:41:02,220 --> 00:41:04,320
it turns out that

945
00:41:04,320 --> 00:41:05,579
each

946
00:41:05,579 --> 00:41:08,420
Matrix sorry each graph can be

947
00:41:08,420 --> 00:41:11,400
represented by a matrix and this Matrix

948
00:41:11,400 --> 00:41:13,380
is called transition Matrix

949
00:41:13,380 --> 00:41:15,240
now what's more interesting more

950
00:41:15,240 --> 00:41:18,240
interesting is that we can infer this

951
00:41:18,240 --> 00:41:20,339
Matrix from the data from the training

952
00:41:20,339 --> 00:41:22,740
data so you can get the training data

953
00:41:22,740 --> 00:41:24,960
the past data of that user the behavior

954
00:41:24,960 --> 00:41:28,920
his behavior and then we can build out

955
00:41:28,920 --> 00:41:31,500
from that the transition matrices

956
00:41:31,500 --> 00:41:34,079
for each user and we are building six

957
00:41:34,079 --> 00:41:36,480
right one here another here and the

958
00:41:36,480 --> 00:41:39,060
other and the other

959
00:41:39,060 --> 00:41:41,880
measurements we have

960
00:41:41,880 --> 00:41:45,060
so how can we use that so whenever the

961
00:41:45,060 --> 00:41:47,579
user does perform some action

962
00:41:47,579 --> 00:41:48,420
okay

963
00:41:48,420 --> 00:41:50,040
he

964
00:41:50,040 --> 00:41:52,140
he will

965
00:41:52,140 --> 00:41:54,960
create this graphs right and this graphs

966
00:41:54,960 --> 00:41:57,359
we can just feed it to the Markov chain

967
00:41:57,359 --> 00:42:00,300
and the Markov chain will output the

968
00:42:00,300 --> 00:42:05,060
probability of that event of that action

969
00:42:05,460 --> 00:42:07,800
and since we have six

970
00:42:07,800 --> 00:42:10,920
transition Matrix you know in the end we

971
00:42:10,920 --> 00:42:13,920
will have six data points six uh one

972
00:42:13,920 --> 00:42:16,320
dimensional data points that we join and

973
00:42:16,320 --> 00:42:18,839
we have a six dimensional data point or

974
00:42:18,839 --> 00:42:22,260
six dimensional probability space

975
00:42:22,260 --> 00:42:24,180
and then we need to look for our normal

976
00:42:24,180 --> 00:42:27,119
is there using for example the isolation

977
00:42:27,119 --> 00:42:30,380
Forest algorithm

978
00:42:30,839 --> 00:42:33,980
and that was the solution

979
00:42:36,540 --> 00:42:37,680
so

980
00:42:37,680 --> 00:42:38,760
um

981
00:42:38,760 --> 00:42:41,760
we have here some data science in the

982
00:42:41,760 --> 00:42:43,980
business Matrix for this case and six

983
00:42:43,980 --> 00:42:45,540
months in production

984
00:42:45,540 --> 00:42:48,540
the model achieved 70 percent of

985
00:42:48,540 --> 00:42:49,920
precision

986
00:42:49,920 --> 00:42:51,900
so you remember precision

987
00:42:51,900 --> 00:42:54,000
is a higher the Precision the lower the

988
00:42:54,000 --> 00:42:56,220
number of false positives

989
00:42:56,220 --> 00:42:59,280
and uh how those how this metrics

990
00:42:59,280 --> 00:43:01,200
translated to business metrics so

991
00:43:01,200 --> 00:43:03,000
several attempts attempts of data

992
00:43:03,000 --> 00:43:05,520
exfiltration were identified and

993
00:43:05,520 --> 00:43:07,920
interrupted because sometimes the user

994
00:43:07,920 --> 00:43:10,020
acts filtrate in a lot of data so we

995
00:43:10,020 --> 00:43:14,180
have time to to detect it and interrupt

996
00:43:14,180 --> 00:43:17,940
it's really really hard to make this

997
00:43:17,940 --> 00:43:22,020
thing work in real time okay if some

998
00:43:22,020 --> 00:43:23,280
vendor

999
00:43:23,280 --> 00:43:25,260
says you ah buy my tool because it's

1000
00:43:25,260 --> 00:43:27,240
possible to I'll detect this latex

1001
00:43:27,240 --> 00:43:30,240
filtration's real time

1002
00:43:30,240 --> 00:43:33,660
um don't believe him okay don't believe

1003
00:43:33,660 --> 00:43:35,819
you show you so show me show me your

1004
00:43:35,819 --> 00:43:38,640
proofs no okay I want to point the proof

1005
00:43:38,640 --> 00:43:42,259
I want evidences

1006
00:43:43,200 --> 00:43:46,380
um yeah yes and to this day the models

1007
00:43:46,380 --> 00:43:48,480
in production alive and well helping to

1008
00:43:48,480 --> 00:43:52,040
detect data ex filtration attempts

1009
00:43:54,240 --> 00:43:57,000
so it's not showing there but I said

1010
00:43:57,000 --> 00:44:00,680
merci beaucoup thank you very much

1011
00:44:00,839 --> 00:44:03,660
and if you have some questions I I'm

1012
00:44:03,660 --> 00:44:06,259
here to answer

1013
00:44:07,550 --> 00:44:10,830
[Applause]

1014
00:44:10,830 --> 00:44:13,980
[Music]

1015
00:44:14,099 --> 00:44:16,859
okay I'll take two oh you guys want me

1016
00:44:16,859 --> 00:44:19,160
to run

1017
00:44:20,040 --> 00:44:23,040
foreign

1018
00:44:29,839 --> 00:44:31,440
thanks

1019
00:44:31,440 --> 00:44:33,000
um what would you recommend someone who

1020
00:44:33,000 --> 00:44:34,740
has security experience but knows

1021
00:44:34,740 --> 00:44:36,240
absolutely nothing about machine

1022
00:44:36,240 --> 00:44:38,480
learning data science to get started

1023
00:44:38,480 --> 00:44:41,160
because I wanted to go to a course of

1024
00:44:41,160 --> 00:44:43,500
machine learning it was very math no

1025
00:44:43,500 --> 00:44:45,599
application but from the other side you

1026
00:44:45,599 --> 00:44:47,520
don't want to just run random models

1027
00:44:47,520 --> 00:44:48,660
without having a clue of what you're

1028
00:44:48,660 --> 00:44:50,280
doing right so how would you recommend

1029
00:44:50,280 --> 00:44:51,359
starting

1030
00:44:51,359 --> 00:44:54,300
so to get started with machine learning

1031
00:44:54,300 --> 00:44:57,119
I really recommend you to First Take a

1032
00:44:57,119 --> 00:44:58,800
good course in mathematics and the

1033
00:44:58,800 --> 00:45:00,060
statistics

1034
00:45:00,060 --> 00:45:02,880
there are several courses in the for

1035
00:45:02,880 --> 00:45:05,880
example edx Coursera they have several

1036
00:45:05,880 --> 00:45:07,859
Great Courses there why this is

1037
00:45:07,859 --> 00:45:09,660
important because you saw in the

1038
00:45:09,660 --> 00:45:12,540
beginning that data data science is the

1039
00:45:12,540 --> 00:45:14,240
meeting point for computer science

1040
00:45:14,240 --> 00:45:16,980
mathematics and statistics

1041
00:45:16,980 --> 00:45:19,560
and business and domain knowledge so we

1042
00:45:19,560 --> 00:45:22,619
need to have that knowledge to

1043
00:45:22,619 --> 00:45:24,839
to get to understand the basics of

1044
00:45:24,839 --> 00:45:27,540
machine learning after that also in this

1045
00:45:27,540 --> 00:45:29,760
in those platforms you can find very

1046
00:45:29,760 --> 00:45:31,380
very good courses

1047
00:45:31,380 --> 00:45:34,319
and they are not expensive and they are

1048
00:45:34,319 --> 00:45:37,680
very comprehensive okay so that's what I

1049
00:45:37,680 --> 00:45:40,940
I recommend to everyone just make a

1050
00:45:40,940 --> 00:45:43,319
research on those platforms you will

1051
00:45:43,319 --> 00:45:45,839
find very very good courses and of

1052
00:45:45,839 --> 00:45:48,180
course you can contact me if you if you

1053
00:45:48,180 --> 00:45:50,160
need more detail you know I can send you

1054
00:45:50,160 --> 00:45:53,660
uh suggestions and links Etc

1055
00:45:53,660 --> 00:45:57,359
I'll share this presentation after a

1056
00:45:57,359 --> 00:45:59,520
while and there is a link for my

1057
00:45:59,520 --> 00:46:01,800
LinkedIn profile you can connect and we

1058
00:46:01,800 --> 00:46:04,020
can keep talking about that this is

1059
00:46:04,020 --> 00:46:06,720
valid for everyone here okay

1060
00:46:06,720 --> 00:46:08,760
I will not take more questions because I

1061
00:46:08,760 --> 00:46:10,260
see some people have the the belly

1062
00:46:10,260 --> 00:46:12,480
calling so there is a lunch starting in

1063
00:46:12,480 --> 00:46:13,800
a few minutes

1064
00:46:13,800 --> 00:46:15,300
I guess you can still ask questions

1065
00:46:15,300 --> 00:46:17,819
Angelo I think your yourself still a few

1066
00:46:17,819 --> 00:46:19,500
minutes if people want to ask you some

1067
00:46:19,500 --> 00:46:22,020
things otherwise thank you very much

1068
00:46:22,020 --> 00:46:24,440
thank you

