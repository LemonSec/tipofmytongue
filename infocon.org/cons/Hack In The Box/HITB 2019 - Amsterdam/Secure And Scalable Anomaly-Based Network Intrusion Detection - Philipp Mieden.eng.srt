1
00:00:19,320 --> 00:00:26,310
so hi everyone my name is philip i

2
00:00:23,750 --> 00:00:28,969
recently finished my bachelor at the

3
00:00:26,310 --> 00:00:30,899
Ludwig Maximilian University in Munich

4
00:00:28,969 --> 00:00:32,339
working as a security and back-end

5
00:00:30,899 --> 00:00:35,970
engineer at the company best bytes

6
00:00:32,339 --> 00:00:39,390
currently and I plan to do my master

7
00:00:35,970 --> 00:00:41,399
here in Amsterdam at the UVA so I did a

8
00:00:39,390 --> 00:00:43,430
bit of research on network intrusion

9
00:00:41,399 --> 00:00:45,390
detection in the last few years and

10
00:00:43,430 --> 00:00:48,149
especially with the use of machine

11
00:00:45,390 --> 00:00:50,489
learning for that purpose I do a lot of

12
00:00:48,149 --> 00:00:56,100
programming mainly go but also other

13
00:00:50,489 --> 00:00:57,689
languages as needed so I'm also

14
00:00:56,100 --> 00:00:59,670
interested in hardware security not only

15
00:00:57,689 --> 00:01:05,549
software security and I do a bit of

16
00:00:59,670 --> 00:01:07,380
reverse engineering as well P so just to

17
00:01:05,549 --> 00:01:10,020
give you an overview of what my talk is

18
00:01:07,380 --> 00:01:11,820
about today so first of all I want to

19
00:01:10,020 --> 00:01:15,030
make clear what kind of problems I try

20
00:01:11,820 --> 00:01:16,829
to solve here and we're going to you get

21
00:01:15,030 --> 00:01:18,570
you're gonna get a brief overview over

22
00:01:16,829 --> 00:01:21,179
the net net cap framework what it can do

23
00:01:18,570 --> 00:01:23,339
for you and I'm gonna give you a brief

24
00:01:21,180 --> 00:01:25,830
wrap-up on what I did in my experiments

25
00:01:23,340 --> 00:01:27,540
and then finally I'm going to introduce

26
00:01:25,830 --> 00:01:31,950
some of the new features of implemented

27
00:01:27,540 --> 00:01:34,350
for the hack in the books so let's start

28
00:01:31,950 --> 00:01:36,479
by talking about the really big problem

29
00:01:34,350 --> 00:01:41,070
for this software industry currently and

30
00:01:36,479 --> 00:01:42,600
that is memory safety there are there

31
00:01:41,070 --> 00:01:45,000
was a thought from Matt Miller who is an

32
00:01:42,600 --> 00:01:46,908
engineer at Microsoft security team and

33
00:01:45,000 --> 00:01:49,619
they published some really interesting

34
00:01:46,909 --> 00:01:51,510
internal statistics they had about how

35
00:01:49,619 --> 00:01:53,850
many of the fixes they pushed out as

36
00:01:51,510 --> 00:01:57,240
Windows updates were addressing problems

37
00:01:53,850 --> 00:01:59,070
with memory corruption and almost 70% of

38
00:01:57,240 --> 00:02:00,839
the vulnerabilities were related to

39
00:01:59,070 --> 00:02:02,610
memory corruption and that is a pretty

40
00:02:00,840 --> 00:02:06,060
strong number I think it's just a huge

41
00:02:02,610 --> 00:02:09,689
attack vector and well we have to work

42
00:02:06,060 --> 00:02:12,180
on getting rid of this and it's not only

43
00:02:09,689 --> 00:02:14,579
for Windows of course it affects

44
00:02:12,180 --> 00:02:15,689
basically every major platform and if

45
00:02:14,580 --> 00:02:17,760
you're interested in this there's a

46
00:02:15,689 --> 00:02:20,250
really nice Twitter feed I stumbled upon

47
00:02:17,760 --> 00:02:22,048
it's called fish in a barrel and they do

48
00:02:20,250 --> 00:02:24,750
basically exactly that they monitor

49
00:02:22,049 --> 00:02:27,629
updates for all kind of big software's

50
00:02:24,750 --> 00:02:29,099
and then they point out how many of the

51
00:02:27,629 --> 00:02:31,720
fixes that were included in the update

52
00:02:29,099 --> 00:02:33,700
were related to memories corruption

53
00:02:31,720 --> 00:02:36,310
and if you look at this it's really it's

54
00:02:33,700 --> 00:02:37,989
really frightening so like and it

55
00:02:36,310 --> 00:02:40,750
affects every major platform like in

56
00:02:37,990 --> 00:02:44,020
this case 22 out of 23 and Michael has

57
00:02:40,750 --> 00:02:47,380
Mojave update eight out of ten in Safari

58
00:02:44,020 --> 00:02:49,540
update 27 out of 31 in iOS so it

59
00:02:47,380 --> 00:02:51,220
basically this goes for all kind of

60
00:02:49,540 --> 00:03:01,810
major platforms no matter if Android

61
00:02:51,220 --> 00:03:02,890
major browsers whatever so and so what

62
00:03:01,810 --> 00:03:04,720
we're doing currently is like we're

63
00:03:02,890 --> 00:03:07,089
using mitigations but as the name

64
00:03:04,720 --> 00:03:08,859
mitigation implies it's not addressing

65
00:03:07,090 --> 00:03:13,959
the root cause it's just making it

66
00:03:08,860 --> 00:03:16,480
harder to exploit and you might wonder

67
00:03:13,959 --> 00:03:18,730
how does this memory safety topic affect

68
00:03:16,480 --> 00:03:20,350
network security monitoring because we

69
00:03:18,730 --> 00:03:23,530
have to parse a lot of complex protocols

70
00:03:20,350 --> 00:03:25,239
and most of the time these frameworks

71
00:03:23,530 --> 00:03:29,650
are implemented in low-level languages

72
00:03:25,240 --> 00:03:31,300
such as C or C++ and as you know things

73
00:03:29,650 --> 00:03:35,110
can go wrong especially if we parse

74
00:03:31,300 --> 00:03:38,769
untrusted input and so let's have a

75
00:03:35,110 --> 00:03:41,170
brief look just looking up some

76
00:03:38,769 --> 00:03:42,940
well-known intrusion detection systems

77
00:03:41,170 --> 00:03:45,040
and throwing them into the CV database

78
00:03:42,940 --> 00:03:46,959
and for example for those

79
00:03:45,040 --> 00:03:48,190
vulnerabilities we get for bro half of

80
00:03:46,959 --> 00:03:51,250
them is related to memory corruption

81
00:03:48,190 --> 00:03:55,930
problems and for surakarta we find also

82
00:03:51,250 --> 00:03:58,600
a lot of memory issues and even more if

83
00:03:55,930 --> 00:04:00,670
we look at for example the release logs

84
00:03:58,600 --> 00:04:02,769
so a lot of these issues don't get CVE

85
00:04:00,670 --> 00:04:05,559
numbers assigned so the dark digit is

86
00:04:02,769 --> 00:04:07,080
probably much higher and also in their

87
00:04:05,560 --> 00:04:09,850
recent sericata

88
00:04:07,080 --> 00:04:13,000
update there was it was published like

89
00:04:09,850 --> 00:04:15,340
one and a half weeks ago and it

90
00:04:13,000 --> 00:04:17,140
contained like four out of the ten fixes

91
00:04:15,340 --> 00:04:20,320
were addressing memory corruption

92
00:04:17,140 --> 00:04:21,909
problems and if we dig a little bit

93
00:04:20,320 --> 00:04:23,260
deeper and look at their commit history

94
00:04:21,910 --> 00:04:26,410
we will find even more that's not

95
00:04:23,260 --> 00:04:28,330
mentioned in the release locks and most

96
00:04:26,410 --> 00:04:33,280
of the things are just memory corruption

97
00:04:28,330 --> 00:04:35,020
problems while parsing Network data so

98
00:04:33,280 --> 00:04:36,760
that's of a brief look at the

99
00:04:35,020 --> 00:04:39,669
mitigations and current intrusion

100
00:04:36,760 --> 00:04:41,680
detection solutions so so ricotta is

101
00:04:39,669 --> 00:04:43,419
using rust for several parcels they

102
00:04:41,680 --> 00:04:45,190
started using rest for the DNS parlor

103
00:04:43,419 --> 00:04:48,219
and by now there are certain other

104
00:04:45,190 --> 00:04:50,440
protocols also using rust or pausing but

105
00:04:48,220 --> 00:04:53,200
if you look at the ratio from C code to

106
00:04:50,440 --> 00:04:56,770
rust code well the majority is still C

107
00:04:53,200 --> 00:04:58,780
so two weeks to be precise it's the 340

108
00:04:56,770 --> 00:05:02,849
thousand lines of C to 17,000 lines of

109
00:04:58,780 --> 00:05:02,849
rust so still is a long way to go

110
00:05:03,520 --> 00:05:08,349
what's nice although is that for the

111
00:05:06,210 --> 00:05:11,349
next version of sericata

112
00:05:08,349 --> 00:05:13,300
they make rust mandatory so before that

113
00:05:11,349 --> 00:05:15,310
it currently it is still optional and

114
00:05:13,300 --> 00:05:20,169
this way they don't have to have

115
00:05:15,310 --> 00:05:22,810
separate implementations bro for example

116
00:05:20,169 --> 00:05:27,070
now they renamed to Zeke which is a pity

117
00:05:22,810 --> 00:05:29,110
because I really like the name bro so

118
00:05:27,070 --> 00:05:31,539
they are using a parser generator to

119
00:05:29,110 --> 00:05:33,250
address this issues this is probably the

120
00:05:31,539 --> 00:05:35,070
most simple illustration of a parser

121
00:05:33,250 --> 00:05:37,990
generator you will ever see and

122
00:05:35,070 --> 00:05:40,180
basically it's just reading a formal

123
00:05:37,990 --> 00:05:42,330
specification and generating a C++ code

124
00:05:40,180 --> 00:05:46,180
for passing the format's that you expect

125
00:05:42,330 --> 00:05:47,950
but yeah this is supposed to be safe but

126
00:05:46,180 --> 00:05:50,770
though there have been problems as well

127
00:05:47,950 --> 00:05:52,690
and so in the generated code

128
00:05:50,770 --> 00:05:54,280
cogeneration general is hard and it's

129
00:05:52,690 --> 00:05:56,469
even harder to generate good parsing

130
00:05:54,280 --> 00:05:57,940
code so yeah also there have been

131
00:05:56,470 --> 00:06:03,340
numerous problems with memory

132
00:05:57,940 --> 00:06:05,650
corruptions for the pin pack so another

133
00:06:03,340 --> 00:06:07,419
major problem is our dependency on

134
00:06:05,650 --> 00:06:09,789
signature based detection methods

135
00:06:07,419 --> 00:06:11,979
because as you know a signature is made

136
00:06:09,789 --> 00:06:13,539
to detect an own thread and if you're

137
00:06:11,979 --> 00:06:16,630
being attacked with something new then

138
00:06:13,539 --> 00:06:18,729
it's not known so well

139
00:06:16,630 --> 00:06:21,070
machine learning is quite a nice idea to

140
00:06:18,729 --> 00:06:23,650
fix to address these problems and the

141
00:06:21,070 --> 00:06:25,930
idea is basically you would train an

142
00:06:23,650 --> 00:06:27,460
algorithm to to know what is normal in

143
00:06:25,930 --> 00:06:31,710
your network and then you let it report

144
00:06:27,460 --> 00:06:35,159
on things that are new or suspicious and

145
00:06:31,710 --> 00:06:37,780
yeah like Kaspersky Lab is talking about

146
00:06:35,159 --> 00:06:39,909
330,000 new unique malware samples they

147
00:06:37,780 --> 00:06:42,070
get every day and if each one of those

148
00:06:39,909 --> 00:06:44,289
gets is a new signature your database

149
00:06:42,070 --> 00:06:45,750
with signatures is growing exponentially

150
00:06:44,289 --> 00:06:49,349
I would say

151
00:06:45,750 --> 00:06:51,639
so and of course we can obfuscate

152
00:06:49,349 --> 00:06:55,479
existing malware to make it undetectable

153
00:06:51,639 --> 00:06:56,770
again so we need to find a way of

154
00:06:55,479 --> 00:07:01,930
dealing with these issues without

155
00:06:56,770 --> 00:07:03,639
independent on signatures and yeah I

156
00:07:01,930 --> 00:07:07,000
think we have a problem here and we need

157
00:07:03,639 --> 00:07:09,250
to address this and so to fix the first

158
00:07:07,000 --> 00:07:12,189
issue with the memory safety

159
00:07:09,250 --> 00:07:13,360
the most obvious thing here would be

160
00:07:12,189 --> 00:07:14,770
just using a language for the

161
00:07:13,360 --> 00:07:17,110
implementation that provides memory

162
00:07:14,770 --> 00:07:18,818
safety and my opinion there are two good

163
00:07:17,110 --> 00:07:22,689
candidates for this currently it is like

164
00:07:18,819 --> 00:07:25,719
go and rust and in my case I chose go

165
00:07:22,689 --> 00:07:28,509
because I like the language it is as a

166
00:07:25,719 --> 00:07:30,789
garbage collected runtime and it has not

167
00:07:28,509 --> 00:07:32,229
been not yet been successfully attacked

168
00:07:30,789 --> 00:07:34,270
which is quite interesting so you can

169
00:07:32,229 --> 00:07:36,250
also throw a golang in the CV database

170
00:07:34,270 --> 00:07:37,870
and check for results and all the bugs

171
00:07:36,250 --> 00:07:41,949
that are listed there currently our

172
00:07:37,870 --> 00:07:46,060
logic arose and no memory corruption

173
00:07:41,949 --> 00:07:48,810
attacks against the runtime so nap gap

174
00:07:46,060 --> 00:07:51,099
stands for a network capture and

175
00:07:48,810 --> 00:07:54,400
basically decodes network packets and

176
00:07:51,099 --> 00:07:55,990
generates audit records from those I'm

177
00:07:54,400 --> 00:07:57,638
using the NGO packet library for

178
00:07:55,990 --> 00:08:01,000
decoding the packets it's well

179
00:07:57,639 --> 00:08:02,800
maintained it's owned by Google net cap

180
00:08:01,000 --> 00:08:04,960
has a concurrent design it's using a

181
00:08:02,800 --> 00:08:09,370
worker pool that's a very common pattern

182
00:08:04,960 --> 00:08:12,190
in photo applications and the the audit

183
00:08:09,370 --> 00:08:15,819
records that you get I up with them as

184
00:08:12,190 --> 00:08:20,500
protocol buffers weight protocol buffers

185
00:08:15,819 --> 00:08:24,099
this is weird RPC format for google why

186
00:08:20,500 --> 00:08:26,529
am i using this so it's really nice for

187
00:08:24,099 --> 00:08:29,349
this purpose because it gives you type

188
00:08:26,529 --> 00:08:31,659
safe structured data and protocols are

189
00:08:29,349 --> 00:08:33,819
not always flat and they do not always

190
00:08:31,659 --> 00:08:36,159
only consists out of scalar scalar

191
00:08:33,820 --> 00:08:37,990
values so we needed a way to present

192
00:08:36,159 --> 00:08:41,979
that to represent complex nested

193
00:08:37,990 --> 00:08:43,779
structures and the the other nice effect

194
00:08:41,979 --> 00:08:45,399
here is that protocol buffers are

195
00:08:43,779 --> 00:08:47,680
platform neutral so it works a bit

196
00:08:45,399 --> 00:08:49,420
similar to the parser generator but it's

197
00:08:47,680 --> 00:08:51,880
more of a type definition generator so

198
00:08:49,420 --> 00:08:54,099
you can basically write protocol

199
00:08:51,880 --> 00:08:56,770
definitions buffer definitions and then

200
00:08:54,100 --> 00:08:58,190
you can generate the type code for the

201
00:08:56,770 --> 00:08:59,960
language you want to use them in and

202
00:08:58,190 --> 00:09:04,640
can use them as typesafe structured data

203
00:08:59,960 --> 00:09:07,460
which is really powerful so just a brief

204
00:09:04,640 --> 00:09:09,439
overview what net cup does basically

205
00:09:07,460 --> 00:09:11,870
we're fetching packets from an input

206
00:09:09,440 --> 00:09:13,850
source it could be live from a network

207
00:09:11,870 --> 00:09:16,310
interface could be from a dump file and

208
00:09:13,850 --> 00:09:18,170
then these are these packets are equally

209
00:09:16,310 --> 00:09:20,150
distributed to a pool of workers with

210
00:09:18,170 --> 00:09:24,079
the round-robin strategy so nothing

211
00:09:20,150 --> 00:09:27,560
special here and each worker basically

212
00:09:24,080 --> 00:09:29,300
dissects all the layers and makes writes

213
00:09:27,560 --> 00:09:31,729
all the audit records in the the

214
00:09:29,300 --> 00:09:33,560
corresponding files and also filters out

215
00:09:31,730 --> 00:09:38,090
traffic that we cannot decode or that

216
00:09:33,560 --> 00:09:40,609
had arose when decoding workers are

217
00:09:38,090 --> 00:09:42,590
buffered so each worker has like an

218
00:09:40,610 --> 00:09:46,490
input queue and you can configure the

219
00:09:42,590 --> 00:09:49,910
size of the queue so it's I try to get

220
00:09:46,490 --> 00:09:51,650
keep it as a configurable as possible so

221
00:09:49,910 --> 00:09:53,930
you can really tweak it and look what

222
00:09:51,650 --> 00:09:57,350
works best on your system where you get

223
00:09:53,930 --> 00:09:59,719
the best performance all so let's talk a

224
00:09:57,350 --> 00:10:02,810
bit about the audit record records that

225
00:09:59,720 --> 00:10:04,760
you can get I'm dividing here between

226
00:10:02,810 --> 00:10:08,420
two things so one of the layer types and

227
00:10:04,760 --> 00:10:10,970
one of the customer records the custom

228
00:10:08,420 --> 00:10:12,110
audit records currently I have a flow

229
00:10:10,970 --> 00:10:14,500
model which is just a unidirectional

230
00:10:12,110 --> 00:10:17,150
connection then the connection type

231
00:10:14,500 --> 00:10:20,000
offers the same field as the flow but

232
00:10:17,150 --> 00:10:22,670
it's just be directional and there's for

233
00:10:20,000 --> 00:10:24,530
example also in order to record type for

234
00:10:22,670 --> 00:10:26,630
TLS handshakes that also includes the

235
00:10:24,530 --> 00:10:28,250
joffrey hash and like all the ciphers

236
00:10:26,630 --> 00:10:31,340
tools that have been negotiated in the

237
00:10:28,250 --> 00:10:33,050
handshake for example and then there's

238
00:10:31,340 --> 00:10:37,130
an autoworker type for HTTP which is a

239
00:10:33,050 --> 00:10:39,260
special case because an HTTP request or

240
00:10:37,130 --> 00:10:41,330
an HTTP response can be split up upon

241
00:10:39,260 --> 00:10:42,740
multiple network packets so that's why

242
00:10:41,330 --> 00:10:44,870
it's not included in the go packet

243
00:10:42,740 --> 00:10:47,150
library by default and we need to do is

244
00:10:44,870 --> 00:10:51,950
TCP stream reassembly to basically parse

245
00:10:47,150 --> 00:10:54,079
the HTTP requests so these are the layer

246
00:10:51,950 --> 00:10:56,210
types so these are protocol specific

247
00:10:54,080 --> 00:10:58,310
audit records and as you can see I just

248
00:10:56,210 --> 00:11:01,580
highlighted the most important ones but

249
00:10:58,310 --> 00:11:06,800
yeah although the basic protocols are

250
00:11:01,580 --> 00:11:10,540
supported from TCP UDP ipv6 DHCP DNS ARP

251
00:11:06,800 --> 00:11:15,579
and since version 0 3 9 there are

252
00:11:10,540 --> 00:11:17,949
of more how one witches might be

253
00:11:15,580 --> 00:11:23,590
interesting is the USB protocol I'll get

254
00:11:17,950 --> 00:11:25,120
to that in a second so what does an

255
00:11:23,590 --> 00:11:27,550
audit record look like now that record

256
00:11:25,120 --> 00:11:30,640
has a timestamp when it was observed and

257
00:11:27,550 --> 00:11:32,740
then it has several fields and the field

258
00:11:30,640 --> 00:11:36,880
could also be a substructure for example

259
00:11:32,740 --> 00:11:39,130
and then I'm writing all audit records

260
00:11:36,880 --> 00:11:41,920
of a certain type into one single file

261
00:11:39,130 --> 00:11:43,270
and each file contains a header that has

262
00:11:41,920 --> 00:11:45,069
some meta information like for example

263
00:11:43,270 --> 00:11:46,689
what was the the source where we got

264
00:11:45,070 --> 00:11:48,760
those audit records from where was it

265
00:11:46,690 --> 00:11:51,550
captured from at what time was it

266
00:11:48,760 --> 00:11:56,200
captured and of course what type of

267
00:11:51,550 --> 00:11:58,030
audit records are in the file so the

268
00:11:56,200 --> 00:12:00,310
file then gets the extension dot n cap

269
00:11:58,030 --> 00:12:06,699
for a net gap audit records or rotten

270
00:12:00,310 --> 00:12:07,089
cabbage is it if it's compressed this is

271
00:12:06,700 --> 00:12:09,790
just a

272
00:12:07,089 --> 00:12:11,439
example for a flower connection ordered

273
00:12:09,790 --> 00:12:13,990
record and on the left you can see the

274
00:12:11,439 --> 00:12:18,748
field names and on the right side I've

275
00:12:13,990 --> 00:12:18,749
populated them with some dummy data so

276
00:12:21,300 --> 00:12:27,309
protocol buffers are a message based

277
00:12:23,939 --> 00:12:30,509
protocol so they are not made to be

278
00:12:27,309 --> 00:12:32,860
written to disk by default so we have to

279
00:12:30,509 --> 00:12:34,689
implement a little trick for that so and

280
00:12:32,860 --> 00:12:38,439
the most common approach here is just to

281
00:12:34,689 --> 00:12:40,209
prefix each serialized message with its

282
00:12:38,439 --> 00:12:42,490
own length so we can write that to disk

283
00:12:40,209 --> 00:12:44,529
and parse it back quite easily and you

284
00:12:42,490 --> 00:12:49,779
just think about the length as a variant

285
00:12:44,529 --> 00:12:51,939
and then can write it to disk so net cap

286
00:12:49,779 --> 00:12:54,490
I call it the data pipe here because

287
00:12:51,939 --> 00:12:57,639
basically if one wants a layer type or

288
00:12:54,490 --> 00:12:59,499
once you have a decoded protocol it's

289
00:12:57,639 --> 00:13:04,389
being passed in the data pipe and this

290
00:12:59,499 --> 00:13:06,730
is basically a thread safe mechanism for

291
00:13:04,389 --> 00:13:08,499
writing those to disk and you can write

292
00:13:06,730 --> 00:13:10,540
into it from several threads it's

293
00:13:08,499 --> 00:13:12,490
synchronized and then they audit to

294
00:13:10,540 --> 00:13:15,219
record will be serialized delimited

295
00:13:12,490 --> 00:13:21,790
compressed buffered and finally written

296
00:13:15,220 --> 00:13:26,470
to disk so in order to make it easy to

297
00:13:21,790 --> 00:13:28,748
work with the with the records I have a

298
00:13:26,470 --> 00:13:32,079
mechanism for exporting them for example

299
00:13:28,749 --> 00:13:33,399
as CSV so you can select certain fields

300
00:13:32,079 --> 00:13:35,769
and you can select the order of the

301
00:13:33,399 --> 00:13:38,439
fields you want to have and by the way

302
00:13:35,769 --> 00:13:40,420
this is a mistake it should be net dot

303
00:13:38,439 --> 00:13:43,089
dump I did not play the slider so

304
00:13:40,420 --> 00:13:44,920
interesting noticed but yeah anyway so

305
00:13:43,089 --> 00:13:46,959
you have a lot of command-line tools and

306
00:13:44,920 --> 00:13:50,469
you can use it to operate on those audit

307
00:13:46,959 --> 00:13:53,290
records files then there's functionality

308
00:13:50,470 --> 00:13:55,149
for creating label data sets because for

309
00:13:53,290 --> 00:13:58,629
my experiments I was using a supervised

310
00:13:55,149 --> 00:14:00,550
machine learning strategy and I needed a

311
00:13:58,629 --> 00:14:02,379
way to label my audit records and I

312
00:14:00,550 --> 00:14:04,300
implemented that quick and easy with the

313
00:14:02,379 --> 00:14:06,429
using sericata so basically I'm

314
00:14:04,300 --> 00:14:08,649
generating audit records from an input

315
00:14:06,429 --> 00:14:10,420
pcap file then I'm using Surakarta to

316
00:14:08,649 --> 00:14:11,980
obtain alerts for the pickup file and

317
00:14:10,420 --> 00:14:14,160
then I'm mapping those alerts back onto

318
00:14:11,980 --> 00:14:20,199
my audit records and generate that a CSV

319
00:14:14,160 --> 00:14:23,410
so also you can use it in a distributed

320
00:14:20,199 --> 00:14:26,139
set up so with like a central collection

321
00:14:23,410 --> 00:14:28,019
server and the agents that are exporting

322
00:14:26,139 --> 00:14:30,459
data and you can collect them and these

323
00:14:28,019 --> 00:14:32,259
these audit records are being sent as

324
00:14:30,459 --> 00:14:36,849
batch GDP data grams and they are also

325
00:14:32,259 --> 00:14:38,649
encrypted in transit so quick use cases

326
00:14:36,850 --> 00:14:39,850
well I think it's quite important

327
00:14:38,649 --> 00:14:43,149
interesting for now

328
00:14:39,850 --> 00:14:45,429
for monitoring honeypot maybe for

329
00:14:43,149 --> 00:14:47,649
forensic analysis as well and of course

330
00:14:45,429 --> 00:14:53,199
for research that's why I created it in

331
00:14:47,649 --> 00:14:54,790
the first place it's open source so now

332
00:14:53,199 --> 00:14:56,979
just a very quick wrap-up what I did in

333
00:14:54,790 --> 00:14:58,540
my thesis basically I picked a very

334
00:14:56,980 --> 00:15:00,819
recent dataset from the Canadian

335
00:14:58,540 --> 00:15:02,170
Institute for cyber security I spent a

336
00:15:00,819 --> 00:15:04,540
lot of time looking at different data

337
00:15:02,170 --> 00:15:08,519
sets please don't use an etheric ID

338
00:15:04,540 --> 00:15:10,509
anymore it's totally outdated and the

339
00:15:08,519 --> 00:15:12,129
Canadian Institute for cyber security

340
00:15:10,509 --> 00:15:13,809
they have really really good data sets

341
00:15:12,129 --> 00:15:16,089
that are well documented and up-to-date

342
00:15:13,809 --> 00:15:18,699
and also for different scenarios from

343
00:15:16,089 --> 00:15:21,399
tournant or VPN non VPN so there are

344
00:15:18,699 --> 00:15:23,439
really a lot of good stuff I chose the

345
00:15:21,399 --> 00:15:25,629
intrusion detection data set from 2017

346
00:15:23,439 --> 00:15:27,910
it's really well documented it's around

347
00:15:25,629 --> 00:15:30,339
50 gigabyte of original pcaps

348
00:15:27,910 --> 00:15:32,079
that's important because many

349
00:15:30,339 --> 00:15:34,360
universities for example they stripped

350
00:15:32,079 --> 00:15:37,839
their data sets the remove IP addresses

351
00:15:34,360 --> 00:15:39,850
hardware addresses whatever and I wanted

352
00:15:37,839 --> 00:15:42,069
the full data though basically to work

353
00:15:39,850 --> 00:15:43,809
with that so it's five days of traffic

354
00:15:42,069 --> 00:15:45,209
Monday is the normal day Tuesday's

355
00:15:43,809 --> 00:15:47,649
brute-force attacks Wednesday does

356
00:15:45,209 --> 00:15:53,529
Thursday web attacks and Friday a bit of

357
00:15:47,649 --> 00:15:56,620
traffic so key takeaways for me from the

358
00:15:53,529 --> 00:15:58,660
experiments before we pass the data to

359
00:15:56,620 --> 00:16:01,809
the neural network we need to encode it

360
00:15:58,660 --> 00:16:05,709
and because the neural network can only

361
00:16:01,809 --> 00:16:08,410
work with numeric data and the strategy

362
00:16:05,709 --> 00:16:12,069
you choose for encoding the data has a

363
00:16:08,410 --> 00:16:13,629
really big impact on the performance so

364
00:16:12,069 --> 00:16:15,610
for example at first I tried the

365
00:16:13,629 --> 00:16:18,220
encoding alphanumeric values with a

366
00:16:15,610 --> 00:16:21,430
dummy variable approach and

367
00:16:18,220 --> 00:16:23,200
this lets this basically insert a new

368
00:16:21,430 --> 00:16:25,300
column for each unique string that you

369
00:16:23,200 --> 00:16:28,830
have in your data set so it grows it

370
00:16:25,300 --> 00:16:33,430
will and that was a bad idea

371
00:16:28,830 --> 00:16:35,230
so another interesting observation is

372
00:16:33,430 --> 00:16:38,229
that I got a really high tection

373
00:16:35,230 --> 00:16:41,950
accuracy with just a handful of

374
00:16:38,230 --> 00:16:43,570
extracted features so extracted or

375
00:16:41,950 --> 00:16:47,350
generated features are those that we

376
00:16:43,570 --> 00:16:51,880
have to compute so it costs resources to

377
00:16:47,350 --> 00:16:54,490
to get children to generate them and the

378
00:16:51,880 --> 00:16:56,380
other ones were collected features that

379
00:16:54,490 --> 00:17:00,370
are directly available like port numbers

380
00:16:56,380 --> 00:17:02,410
now in a packet header for example so I

381
00:17:00,370 --> 00:17:05,109
was only using a handful of features

382
00:17:02,410 --> 00:17:07,720
like connections for flows or connection

383
00:17:05,109 --> 00:17:09,819
or the payload size for certain audit

384
00:17:07,720 --> 00:17:14,439
record types and the payload entropy in

385
00:17:09,819 --> 00:17:17,109
some cases so another interesting

386
00:17:14,439 --> 00:17:18,910
takeaway was for me that we can increase

387
00:17:17,109 --> 00:17:21,010
the value of the alerts for the analyst

388
00:17:18,910 --> 00:17:24,130
by using different strategies for the

389
00:17:21,010 --> 00:17:26,680
labeling so think of using the attack

390
00:17:24,130 --> 00:17:29,710
classes as label we could also use the

391
00:17:26,680 --> 00:17:31,510
concrete attack name as label so in that

392
00:17:29,710 --> 00:17:33,370
case the the alert would not say okay

393
00:17:31,510 --> 00:17:34,960
it's a web application attack but it

394
00:17:33,370 --> 00:17:39,360
would be more precise okay it's a

395
00:17:34,960 --> 00:17:39,360
cross-site scripting or whatever and

396
00:17:39,720 --> 00:17:45,010
yeah the last thing is the the accuracy

397
00:17:42,910 --> 00:17:46,750
for the protocol specific approach so

398
00:17:45,010 --> 00:17:48,970
for just looking at the audit records

399
00:17:46,750 --> 00:17:50,830
for certain protocol types was really

400
00:17:48,970 --> 00:17:52,330
high on average so that was quite

401
00:17:50,830 --> 00:17:53,860
interesting for me as well and it

402
00:17:52,330 --> 00:17:57,210
performed better in a lot of cases than

403
00:17:53,860 --> 00:17:57,209
the flow or connection model for example

404
00:17:57,840 --> 00:18:04,209
yeah so development on that gap has not

405
00:18:02,650 --> 00:18:07,170
stopped since I released it in December

406
00:18:04,210 --> 00:18:07,170
2018

407
00:18:09,640 --> 00:18:13,480
an interesting thing to notice that I

408
00:18:11,860 --> 00:18:15,370
was able to improve the serialization

409
00:18:13,480 --> 00:18:17,140
performance for the protocol buffers

410
00:18:15,370 --> 00:18:19,389
just by exchanging the code generator

411
00:18:17,140 --> 00:18:20,770
the Jade that generates the code from

412
00:18:19,390 --> 00:18:22,660
packing and unpacking the protocol

413
00:18:20,770 --> 00:18:24,850
buffers and this was a super easy

414
00:18:22,660 --> 00:18:27,670
performance gain and marshalling them is

415
00:18:24,850 --> 00:18:29,409
now twice as fast and unmarshal it also

416
00:18:27,670 --> 00:18:30,330
got a big speed-up from that so that was

417
00:18:29,410 --> 00:18:32,490
a really easy fix

418
00:18:30,330 --> 00:18:36,240
and I was you I'm now using the it's

419
00:18:32,490 --> 00:18:39,620
called gogo code generator for the for

420
00:18:36,240 --> 00:18:42,450
the protocol buffers it's really nice

421
00:18:39,620 --> 00:18:44,129
also it's now possible to capture packet

422
00:18:42,450 --> 00:18:45,929
payloads that wasn't intended in the

423
00:18:44,130 --> 00:18:47,670
first place but since we since we

424
00:18:45,929 --> 00:18:49,620
already got it offered by go packet I

425
00:18:47,670 --> 00:18:51,540
made it optional and you can now use a

426
00:18:49,620 --> 00:18:53,850
flag to capture the payloads as well and

427
00:18:51,540 --> 00:19:00,450
preserve it for some audit records for

428
00:18:53,850 --> 00:19:02,909
example like TCP UDP then it's also

429
00:19:00,450 --> 00:19:04,590
possible to decode USB traffic now so

430
00:19:02,910 --> 00:19:06,960
this audit record type is also offered

431
00:19:04,590 --> 00:19:10,439
by a go packet which got me here yes of

432
00:19:06,960 --> 00:19:12,630
course and if you install Wireshark you

433
00:19:10,440 --> 00:19:14,550
can bring up a special interface and you

434
00:19:12,630 --> 00:19:16,170
can also attach to it and then decode

435
00:19:14,550 --> 00:19:17,850
they use be traffic life which might be

436
00:19:16,170 --> 00:19:20,490
interested for some kind of applications

437
00:19:17,850 --> 00:19:22,559
and it's also possible to capture the

438
00:19:20,490 --> 00:19:28,410
payload or for the USB or the record

439
00:19:22,559 --> 00:19:30,120
type this is V output that you can

440
00:19:28,410 --> 00:19:32,010
generate from the audit records is

441
00:19:30,120 --> 00:19:35,010
highly configurable you can choose a

442
00:19:32,010 --> 00:19:37,890
custom separator symbol and you can now

443
00:19:35,010 --> 00:19:39,870
also configure the way that I flatten

444
00:19:37,890 --> 00:19:43,320
substructures when putting them into the

445
00:19:39,870 --> 00:19:45,360
CSV so for example we have a message

446
00:19:43,320 --> 00:19:47,189
with certain field and then you can

447
00:19:45,360 --> 00:19:49,678
choose what should be the delimiting

448
00:19:47,190 --> 00:19:51,270
symbols around the the structure and

449
00:19:49,679 --> 00:19:54,450
what should be the internal deliberative

450
00:19:51,270 --> 00:19:59,879
and you can configure the way structures

451
00:19:54,450 --> 00:20:01,980
appear in the generated CSV output I

452
00:19:59,880 --> 00:20:05,400
restructured the command-line interface

453
00:20:01,980 --> 00:20:08,190
so there are now eight standalone

454
00:20:05,400 --> 00:20:10,440
command-line tools one for capturing one

455
00:20:08,190 --> 00:20:13,050
for working with audit record dump files

456
00:20:10,440 --> 00:20:15,900
and exporting them to CSV for example so

457
00:20:13,050 --> 00:20:17,490
that's dump two then there's a tool for

458
00:20:15,900 --> 00:20:19,380
doing the labeling I've just talked

459
00:20:17,490 --> 00:20:21,600
about there's a tool for starting the

460
00:20:19,380 --> 00:20:23,880
collection server there's the tool for

461
00:20:21,600 --> 00:20:26,730
vehicle for the sensor agent that is

462
00:20:23,880 --> 00:20:29,970
exporting data then there's a new tool

463
00:20:26,730 --> 00:20:32,970
is called Neto proxy it's a basically a

464
00:20:29,970 --> 00:20:35,700
simple HTTP reverse proxy and go in the

465
00:20:32,970 --> 00:20:37,980
bit more detail in a second there's a

466
00:20:35,700 --> 00:20:40,080
general util and there's a tool for

467
00:20:37,980 --> 00:20:42,190
exporting in Prometheus metrics which is

468
00:20:40,080 --> 00:20:44,050
also new

469
00:20:42,190 --> 00:20:46,000
I did a bit of improvements on the

470
00:20:44,050 --> 00:20:48,310
library side so now you also have a

471
00:20:46,000 --> 00:20:50,140
writer type previously only either

472
00:20:48,310 --> 00:20:51,490
a structure for reading audit records

473
00:20:50,140 --> 00:20:53,950
back and now you have also a writer that

474
00:20:51,490 --> 00:20:57,520
basically implements the data pipe just

475
00:20:53,950 --> 00:20:58,720
been talking about and also the I made

476
00:20:57,520 --> 00:21:01,240
the go packet decode options

477
00:20:58,720 --> 00:21:06,370
configurable so you don't have a bit

478
00:21:01,240 --> 00:21:09,880
more options to play with apart from

479
00:21:06,370 --> 00:21:11,800
that I decided to generate pre generate

480
00:21:09,880 --> 00:21:13,120
the type definitions for a few common

481
00:21:11,800 --> 00:21:14,440
languages and include them in each

482
00:21:13,120 --> 00:21:15,790
release so you don't have to do this

483
00:21:14,440 --> 00:21:17,650
yourself you can just download the

484
00:21:15,790 --> 00:21:19,690
latest release and and if you want to

485
00:21:17,650 --> 00:21:21,370
work with the data than in rust or Java

486
00:21:19,690 --> 00:21:23,140
or whatever you can just use the pre

487
00:21:21,370 --> 00:21:26,889
generated ones without having to setup

488
00:21:23,140 --> 00:21:29,890
the proto compiler there's no Python

489
00:21:26,890 --> 00:21:33,100
support and what's interesting here

490
00:21:29,890 --> 00:21:35,470
probably is that you can really direct

491
00:21:33,100 --> 00:21:36,879
into a panda's data frame which might be

492
00:21:35,470 --> 00:21:38,920
interesting for some kind of machine

493
00:21:36,880 --> 00:21:41,640
learning applications so since it's a

494
00:21:38,920 --> 00:21:44,020
really common library used there I

495
00:21:41,640 --> 00:21:48,400
published this in a separate repository

496
00:21:44,020 --> 00:21:50,800
make it up it's called pine nut cap so

497
00:21:48,400 --> 00:21:53,350
yeah let's talk about the proxy so net

498
00:21:50,800 --> 00:21:58,240
cap currently has a small problem and

499
00:21:53,350 --> 00:22:01,179
that is go packet implements TCP stream

500
00:21:58,240 --> 00:22:03,820
reassembly but only for TCP packets that

501
00:22:01,180 --> 00:22:06,880
are transported over ipv4 currently so

502
00:22:03,820 --> 00:22:08,710
efficiently this means net cap is going

503
00:22:06,880 --> 00:22:12,310
to miss all HTTP traffic that's being

504
00:22:08,710 --> 00:22:14,350
sent over ipv6 and that's bad so in

505
00:22:12,310 --> 00:22:16,980
order to tackle that problem until there

506
00:22:14,350 --> 00:22:19,929
is a working ipv6 tree reassembly

507
00:22:16,980 --> 00:22:22,000
implementation available I just decided

508
00:22:19,930 --> 00:22:24,130
to use a simple reverse proxy setup so

509
00:22:22,000 --> 00:22:25,900
you can either spawn one or multiple

510
00:22:24,130 --> 00:22:27,580
reverse proxies put them in front of

511
00:22:25,900 --> 00:22:30,130
your web applications or web services

512
00:22:27,580 --> 00:22:32,379
and then you're just going to extract

513
00:22:30,130 --> 00:22:37,360
HTTP audit records and the nice thing

514
00:22:32,380 --> 00:22:39,850
here is that while doing it this way I

515
00:22:37,360 --> 00:22:41,979
can use the tracing functionality for

516
00:22:39,850 --> 00:22:44,260
the hcg from the go HTTP standard

517
00:22:41,980 --> 00:22:45,910
library and I can give you a couple of

518
00:22:44,260 --> 00:22:48,790
more interesting fields here so for

519
00:22:45,910 --> 00:22:51,730
example the total time it took from

520
00:22:48,790 --> 00:22:55,180
request to response the time it took to

521
00:22:51,730 --> 00:22:57,550
resolve the DNS query the time took

522
00:22:55,180 --> 00:22:59,260
until we received the first byte of the

523
00:22:57,550 --> 00:23:01,510
response which might be interesting for

524
00:22:59,260 --> 00:23:05,640
some timing attacks and also the time it

525
00:23:01,510 --> 00:23:05,640
took for the TLS handshake to complete

526
00:23:08,280 --> 00:23:14,350
metrics so I've instrumented the

527
00:23:11,710 --> 00:23:17,650
application and you get three types of

528
00:23:14,350 --> 00:23:20,770
metrics currently one our net cap

529
00:23:17,650 --> 00:23:22,450
internal metrics so it's about how what

530
00:23:20,770 --> 00:23:24,610
kind of protocols that we see how many

531
00:23:22,450 --> 00:23:27,970
decoding errors have been there stuff

532
00:23:24,610 --> 00:23:29,678
like that then we have metrics for the

533
00:23:27,970 --> 00:23:32,200
go runtime this is just a standard

534
00:23:29,679 --> 00:23:33,760
Prometheus go exporter and it gives you

535
00:23:32,200 --> 00:23:35,590
some interesting insights about memory

536
00:23:33,760 --> 00:23:36,760
usage the number of core routines that

537
00:23:35,590 --> 00:23:39,850
are currently running and stuff like

538
00:23:36,760 --> 00:23:42,309
that and and also this is the most

539
00:23:39,850 --> 00:23:44,020
interesting part there are no metrics

540
00:23:42,309 --> 00:23:46,000
for the audit directors which means we

541
00:23:44,020 --> 00:23:47,470
can visualize the audit records and this

542
00:23:46,000 --> 00:23:49,630
is quite interesting because it's very

543
00:23:47,470 --> 00:23:53,500
abstract machine data and I wanted to

544
00:23:49,630 --> 00:23:54,880
have a way to visually see and look at

545
00:23:53,500 --> 00:23:59,080
my data set and basically try to

546
00:23:54,880 --> 00:24:01,120
understand it this way as well and so

547
00:23:59,080 --> 00:24:03,189
the way this works is quite simple so

548
00:24:01,120 --> 00:24:04,870
there you have the net dot export

549
00:24:03,190 --> 00:24:06,580
command line application you can attach

550
00:24:04,870 --> 00:24:10,178
it either to the network interface into

551
00:24:06,580 --> 00:24:12,370
it life or read a pcap file or you can

552
00:24:10,179 --> 00:24:15,970
export audit records files that you

553
00:24:12,370 --> 00:24:18,129
approve of previously captured and you

554
00:24:15,970 --> 00:24:22,420
just expose at a certain part you have a

555
00:24:18,130 --> 00:24:24,040
handler for that exports metrics and you

556
00:24:22,420 --> 00:24:26,380
configure a Prometheus daemon to scrape

557
00:24:24,040 --> 00:24:27,909
that one and then you configure a graph

558
00:24:26,380 --> 00:24:32,070
on or to scrape the Prometheus and then

559
00:24:27,910 --> 00:24:34,780
you can visualize it in your funnel and

560
00:24:32,070 --> 00:24:37,149
I'm currently working on building a nice

561
00:24:34,780 --> 00:24:40,270
dashboard for this this is just a quick

562
00:24:37,150 --> 00:24:43,870
preview I'm going to release it probably

563
00:24:40,270 --> 00:24:46,450
somewhere in the next week so you can

564
00:24:43,870 --> 00:24:48,669
see the number of audit records that

565
00:24:46,450 --> 00:24:51,610
have been generated what kind of art did

566
00:24:48,670 --> 00:24:54,309
red records and the total number of

567
00:24:51,610 --> 00:24:55,689
packets how many unknown protocols were

568
00:24:54,309 --> 00:24:58,059
in there

569
00:24:55,690 --> 00:25:00,580
how many decoding errors were in there

570
00:24:58,059 --> 00:25:05,410
and stuff like that this is just the

571
00:25:00,580 --> 00:25:07,178
overview page and I started also

572
00:25:05,410 --> 00:25:08,410
building one for HTTP so basically now

573
00:25:07,179 --> 00:25:08,980
you can do it for every kind of audit

574
00:25:08,410 --> 00:25:11,350
record that

575
00:25:08,980 --> 00:25:13,870
being offered by a net cap you can start

576
00:25:11,350 --> 00:25:16,659
building metrics for it and visualizing

577
00:25:13,870 --> 00:25:18,939
those and I were just looking at the

578
00:25:16,660 --> 00:25:20,860
distribution of user agents what kind of

579
00:25:18,940 --> 00:25:24,460
call has been dialed up how many times

580
00:25:20,860 --> 00:25:27,820
and stuff like that and this is the one

581
00:25:24,460 --> 00:25:31,120
for TCP is quite interesting you can

582
00:25:27,820 --> 00:25:34,659
also see this is like the payload size

583
00:25:31,120 --> 00:25:38,350
over time and this one shows the payload

584
00:25:34,660 --> 00:25:39,669
entropy over time and yeah so this way

585
00:25:38,350 --> 00:25:41,830
we can get a bit of a better

586
00:25:39,669 --> 00:25:44,650
understanding on what kind of data we

587
00:25:41,830 --> 00:25:49,600
operate on and what's in there and yeah

588
00:25:44,650 --> 00:25:51,309
so to help you get started quickly I've

589
00:25:49,600 --> 00:25:53,530
also created an outline you know

590
00:25:51,309 --> 00:25:55,059
structure container that can just pull

591
00:25:53,530 --> 00:25:57,040
the container run it and all the tools

592
00:25:55,059 --> 00:26:01,240
are pre-installed in there so you can

593
00:25:57,040 --> 00:26:02,918
just start hacking there's now a simple

594
00:26:01,240 --> 00:26:04,960
website it gives just a brief overview

595
00:26:02,919 --> 00:26:08,380
of the project so it's easier to find

596
00:26:04,960 --> 00:26:10,270
and the the major new changes now

597
00:26:08,380 --> 00:26:12,580
there's a final finally real

598
00:26:10,270 --> 00:26:15,190
documentation available so for a long

599
00:26:12,580 --> 00:26:18,070
time there was only my thesis and so it

600
00:26:15,190 --> 00:26:19,780
was quite extensive but I knew that

601
00:26:18,070 --> 00:26:21,939
people wanted the proper documentation

602
00:26:19,780 --> 00:26:23,740
so now you have you know you have one I

603
00:26:21,940 --> 00:26:25,720
used the get book for that it's quite

604
00:26:23,740 --> 00:26:28,809
nice and it's responsive you can use it

605
00:26:25,720 --> 00:26:31,270
on your phone or whatever and I try to

606
00:26:28,809 --> 00:26:35,470
make it easy to get started and to

607
00:26:31,270 --> 00:26:38,830
understand what you can do so let's talk

608
00:26:35,470 --> 00:26:41,169
a bit about future plans one of the next

609
00:26:38,830 --> 00:26:43,570
things I'm going to do is implement

610
00:26:41,169 --> 00:26:45,280
support for using jar or rules for

611
00:26:43,570 --> 00:26:47,678
labeling so I think that would be kind

612
00:26:45,280 --> 00:26:49,540
of interesting because for so for my

613
00:26:47,679 --> 00:26:51,880
first experiments I didn't have a huge

614
00:26:49,540 --> 00:26:54,399
Tower DB rule base but I think some do

615
00:26:51,880 --> 00:26:57,220
so for them I think it might be

616
00:26:54,400 --> 00:26:59,260
interesting I'm going to do a bit of

617
00:26:57,220 --> 00:27:01,150
benchmarking and the next iteration or

618
00:26:59,260 --> 00:27:02,770
working on a project just to get an

619
00:27:01,150 --> 00:27:06,700
impression and how well it performs

620
00:27:02,770 --> 00:27:08,530
compared to a simple mutation and I've

621
00:27:06,700 --> 00:27:10,900
also been thinking about building like

622
00:27:08,530 --> 00:27:13,899
like a deep packet inspection module

623
00:27:10,900 --> 00:27:16,299
that you can could use to run certain

624
00:27:13,900 --> 00:27:18,309
pattern matches or analysis on the palos

625
00:27:16,299 --> 00:27:20,620
before they are being discarded because

626
00:27:18,309 --> 00:27:22,720
for a lot of protocols currently we

627
00:27:20,620 --> 00:27:24,669
cannot set the application

628
00:27:22,720 --> 00:27:28,090
a protocol type because we just don't

629
00:27:24,670 --> 00:27:32,460
know it but this way you could implement

630
00:27:28,090 --> 00:27:32,459
it yourself and start doing this

631
00:27:32,790 --> 00:27:40,540
ipv6 stream reassembly for TCP would be

632
00:27:36,610 --> 00:27:42,570
really really interesting and yeah and

633
00:27:40,540 --> 00:27:44,889
also like a generic interface for

634
00:27:42,570 --> 00:27:46,450
application layer decoders that require

635
00:27:44,890 --> 00:27:49,420
stream reassembly because there are more

636
00:27:46,450 --> 00:27:50,800
protocols that rely on this and so it

637
00:27:49,420 --> 00:27:53,170
would be really nice to have a generic

638
00:27:50,800 --> 00:27:55,210
interface for like a decoder that just

639
00:27:53,170 --> 00:27:56,890
gets as input the stream of all the

640
00:27:55,210 --> 00:27:58,270
decoded data and then he can work on

641
00:27:56,890 --> 00:28:03,910
this and create the audit records that

642
00:27:58,270 --> 00:28:06,610
you actually what I extract that's it

643
00:28:03,910 --> 00:28:10,180
for now if you have any questions yes

644
00:28:06,610 --> 00:28:16,870
please and otherwise yeah just contact

645
00:28:10,180 --> 00:28:19,020
me by mail or whatever so similar thank

646
00:28:16,870 --> 00:28:19,020
you

647
00:28:19,369 --> 00:28:24,779
okay Thank You Philip so if there are

648
00:28:22,679 --> 00:28:27,659
any questions from the audience let's

649
00:28:24,779 --> 00:28:34,950
start from maybe here you have a

650
00:28:27,659 --> 00:28:38,070
microphone thank you for the

651
00:28:34,950 --> 00:28:42,119
presentation Russell and question is can

652
00:28:38,070 --> 00:28:44,460
you just briefly talk about the deep

653
00:28:42,119 --> 00:28:49,918
learning model that you used for the 95

654
00:28:44,460 --> 00:29:00,299
to 99 accuracy and on how many data

655
00:28:49,919 --> 00:29:06,479
samples did you get those numbers yeah

656
00:29:00,299 --> 00:29:10,679
that's like okay so the experiments were

657
00:29:06,479 --> 00:29:12,960
operating on the dump file for each day

658
00:29:10,679 --> 00:29:14,729
so I don't have the exact number of

659
00:29:12,960 --> 00:29:17,039
audit records that were generated for

660
00:29:14,729 --> 00:29:19,649
this but it's roughly 10 gigabytes of

661
00:29:17,039 --> 00:29:24,629
traffic that were observed for each for

662
00:29:19,649 --> 00:29:25,859
each day and basically I have the thesis

663
00:29:24,629 --> 00:29:27,629
here as well if you want to take a look

664
00:29:25,859 --> 00:29:29,489
at side of a printed version with me so

665
00:29:27,629 --> 00:29:31,199
I have all the tables with all the

666
00:29:29,489 --> 00:29:33,359
values from the experiment so maybe that

667
00:29:31,200 --> 00:29:35,039
will answer a question better yeah but I

668
00:29:33,359 --> 00:29:36,809
was curious about the module itself that

669
00:29:35,039 --> 00:29:38,879
gave that performance is like is it

670
00:29:36,809 --> 00:29:41,729
fully connected near Network random

671
00:29:38,879 --> 00:29:45,539
forests what is it yeah so I just used

672
00:29:41,729 --> 00:29:47,789
Karis and the model type was sequential

673
00:29:45,539 --> 00:29:50,639
so five layers like that for medium

674
00:29:47,789 --> 00:29:56,158
optimizer oh that's nothing nothing

675
00:29:50,639 --> 00:29:58,799
really fancy here and always like thank

676
00:29:56,159 --> 00:30:05,219
you thank you well okay so next question

677
00:29:58,799 --> 00:30:08,519
over there yeah thank you

678
00:30:05,219 --> 00:30:11,519
oh it's loud my question was actually

679
00:30:08,519 --> 00:30:13,679
follow-up to this one so can we download

680
00:30:11,519 --> 00:30:15,929
your thesis to go further into this

681
00:30:13,679 --> 00:30:18,119
because that 95 to 99 percent really

682
00:30:15,929 --> 00:30:20,039
triggered me that's great it's so the

683
00:30:18,119 --> 00:30:22,259
thesis is available in the root of the

684
00:30:20,039 --> 00:30:25,679
repository ok so it's on get up

685
00:30:22,259 --> 00:30:29,059
it's also on ResearchGate please have a

686
00:30:25,679 --> 00:30:31,710
look excellent thank you

687
00:30:29,059 --> 00:30:33,720
by the way the slides of this present

688
00:30:31,710 --> 00:30:36,570
we'll be available on the other website

689
00:30:33,720 --> 00:30:38,490
and you can really download them of

690
00:30:36,570 --> 00:30:42,860
course any other question we have still

691
00:30:38,490 --> 00:30:45,899
time for one probably hi thank you oh

692
00:30:42,860 --> 00:30:49,049
it's on okay thank you for presentation

693
00:30:45,899 --> 00:30:52,199
it seems like a really lot of work that

694
00:30:49,049 --> 00:30:54,870
you put it put into this project are

695
00:30:52,200 --> 00:30:56,610
currently the only one yeah it's a

696
00:30:54,870 --> 00:30:58,229
one-man project so it's a bit of an

697
00:30:56,610 --> 00:30:59,639
evolution of my tool chain that they've

698
00:30:58,230 --> 00:31:01,980
built over the years so I've been

699
00:30:59,640 --> 00:31:04,049
working a lot with a cold packet and I

700
00:31:01,980 --> 00:31:06,539
tried to unify it in a way that also

701
00:31:04,049 --> 00:31:09,059
others could use it and that also like

702
00:31:06,539 --> 00:31:11,490
it wasn't better suited from the use

703
00:31:09,059 --> 00:31:13,230
case I wanted to work with it so yeah

704
00:31:11,490 --> 00:31:14,880
currently it's a one-man project but

705
00:31:13,230 --> 00:31:16,320
contributions are welcome so if you're

706
00:31:14,880 --> 00:31:20,220
interested in hacking on this please

707
00:31:16,320 --> 00:31:22,320
contact me I have a lot of ideas for new

708
00:31:20,220 --> 00:31:22,950
features but very little time you know

709
00:31:22,320 --> 00:31:25,610
how it goes

710
00:31:22,950 --> 00:31:30,779
so yeah let's see where this is going

711
00:31:25,610 --> 00:31:34,830
kudos okay so thank you very much Philip

712
00:31:30,779 --> 00:31:40,170
for your answers and yep

713
00:31:34,830 --> 00:31:40,169
[Applause]

714
00:31:45,170 --> 00:31:47,230
you

