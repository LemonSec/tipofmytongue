1
00:00:08,519 --> 00:00:16,610
I'll give you a little bit of background
about who I am because I were a lot of

2
00:00:16,610 --> 00:00:22,300
different hats spent about eight years
and the enterprise security application

3
00:00:22,300 --> 00:00:27,969
security world starting out down in the
trenches of code review and increasingly

4
00:00:27,969 --> 00:00:31,550
moved up into architecture and threat
modeling and security engineering work

5
00:00:31,550 --> 00:00:38,460
about four years ago I jumped over and
started doing more work with NGOs and

6
00:00:38,460 --> 00:00:43,070
news organizations targeted by nation
states and I realized when I started

7
00:00:43,070 --> 00:00:46,770
dealing with people who had actual
problems and had actual attackers that

8
00:00:46,770 --> 00:00:55,899
we don't actually know very much about
security we are far less able to deal

9
00:00:55,899 --> 00:01:01,130
with security in the real world than we
think we are and it's been a few years

10
00:01:01,130 --> 00:01:04,719
it kind of dealing with that realization
and starting to figure out how to fix it

11
00:01:04,718 --> 00:01:11,750
and where I am now it's kind of starting
to be the result of that so we have this

12
00:01:11,750 --> 00:01:16,909
idea that computer security is about
computers it's not computers have

13
00:01:16,909 --> 00:01:25,619
actually relatively little to do with
what our job is many many years ago

14
00:01:25,619 --> 00:01:28,979
people started building computers
because they had work that they needed

15
00:01:28,979 --> 00:01:32,719
to do and get done and then they started
building more computers they started

16
00:01:32,719 --> 00:01:37,119
networking on because they needed to get
more work done and at a certain point

17
00:01:37,119 --> 00:01:42,020
these computers started getting
compromised and then folks decided well

18
00:01:42,020 --> 00:01:47,130
let's pay these these people to fix this
problem and we made a little mistake

19
00:01:47,130 --> 00:01:52,329
that we thought that they meant that we
should go fix the computers that was not

20
00:01:52,329 --> 00:01:53,770
what they needed

21
00:01:53,770 --> 00:01:58,889
having made this mistake of thinking
that we were supposed to fix computers

22
00:01:58,889 --> 00:02:05,020
we built an entire industry around the
wrong problem of fixing computers people

23
00:02:05,020 --> 00:02:11,030
built yet more computers and get more
networks and we started realizing from

24
00:02:11,030 --> 00:02:15,050
this perspective of trying to fix the
wrong problem that we couldn't secure

25
00:02:15,050 --> 00:02:20,000
them all individually and now we're
where we are where we are now where

26
00:02:20,000 --> 00:02:21,880
we're starting to look at

27
00:02:21,880 --> 00:02:27,060
no large data analysis you know looking
at populations of machines starting to

28
00:02:27,060 --> 00:02:31,820
look finally at epidemiological
approaches you know basically how do we

29
00:02:31,820 --> 00:02:38,340
scale fixing the wrong problem we never
fixed the right problem

30
00:02:38,340 --> 00:02:43,690
security is the set of activities that
reduce the likelihood of a set of

31
00:02:43,690 --> 00:02:48,329
adversaries successfully frustrating the
goals and a set of users will notice

32
00:02:48,330 --> 00:02:54,200
that nowhere in this definition does it
say anything about computers because

33
00:02:54,200 --> 00:02:58,799
security does not have anything to do
with computers it has to do with what

34
00:02:58,800 --> 00:03:02,570
people with what the things that people
are trying to accomplish in the real

35
00:03:02,570 --> 00:03:08,769
world the ability to define and
determine what a technical system will

36
00:03:08,770 --> 00:03:13,410
and will not do is sufficient is
necessary but not sufficient to

37
00:03:13,410 --> 00:03:18,210
determine whether that system is secure
so if you can determine all of the code

38
00:03:18,210 --> 00:03:22,940
paths that a system will take in
response to execution both all of the

39
00:03:22,940 --> 00:03:26,530
the intentional ones and all the weird
machines that got built into it you

40
00:03:26,530 --> 00:03:30,880
still don't know whether that system is
secure because if you want to understand

41
00:03:30,880 --> 00:03:35,200
whether it's secure or not you have to
understand what people are trying to use

42
00:03:35,200 --> 00:03:44,119
it to do the mismatch between a set of
human goals and a set of executing code

43
00:03:44,120 --> 00:03:50,750
is where almost all of the bugs that we
act that actually matter lie right the

44
00:03:50,750 --> 00:03:55,730
the does this you know can I compromise
this Android device at a very low level

45
00:03:55,730 --> 00:04:00,940
that's interesting but that's not
actually the problem that a user of that

46
00:04:00,940 --> 00:04:06,380
system has the problem of a user that
system houses I need to I need the

47
00:04:06,380 --> 00:04:10,579
system to maintain certain variants as I
work one of those invariants might be

48
00:04:10,580 --> 00:04:20,239
confidentiality of data various other
things this leaves us with basically a

49
00:04:20,238 --> 00:04:25,700
missing filled with insecurity security
design as the process of understanding

50
00:04:25,700 --> 00:04:31,330
user culture goals and workflows
organizational technical capabilities

51
00:04:31,330 --> 00:04:34,330
and adversary capabilities and
dispositions

52
00:04:34,330 --> 00:04:40,849
and synthesizing a solution to this
problem right so we start off with not

53
00:04:40,849 --> 00:04:44,930
actually even the goals of the user but
the culture of the user how did your

54
00:04:44,930 --> 00:04:50,689
homework you know if you have a group of
people who you were trying to to provide

55
00:04:50,689 --> 00:04:55,310
security support for whether that's the
user base of you know a fortune 500

56
00:04:55,310 --> 00:05:00,949
company or all of the customers of a
major bank or you know these four

57
00:05:00,949 --> 00:05:05,360
journalists and you know some god
forsaken country her trying to not get

58
00:05:05,360 --> 00:05:11,069
killed you need to understand the ways
in which they work the ways in which

59
00:05:11,069 --> 00:05:17,710
trust flows the ways in which these
teams interact the ways in which you

60
00:05:17,710 --> 00:05:21,810
know let's say you're talking about
banking customers do your banking

61
00:05:21,810 --> 00:05:25,650
customers own their own machines are
they must be working for mobile devices

62
00:05:25,650 --> 00:05:30,128
are those mobile mobile devices shared
right these are these are basic

63
00:05:30,129 --> 00:05:33,639
questions about the culture of their
interaction with their computers that

64
00:05:33,639 --> 00:05:38,509
you're not going to to be able to have
two functionally accomplished security

65
00:05:38,509 --> 00:05:43,620
without understanding but it goes deeper
than that you know what you know do bank

66
00:05:43,620 --> 00:05:47,710
accounts represent a single-family ok
that implies certain things about

67
00:05:47,710 --> 00:05:52,859
authentication systems do they represent
you know groups ok that implies a very

68
00:05:52,860 --> 00:05:58,860
different thing about authentication
systems so it starts there and then you

69
00:05:58,860 --> 00:06:03,800
start looking at the goals and then you
start looking at how people work and now

70
00:06:03,800 --> 00:06:08,849
you may be know enough about your users
to be able to stay something meaningful

71
00:06:08,849 --> 00:06:16,180
about their problems and what they want
you have to understand obviously the the

72
00:06:16,180 --> 00:06:20,330
capabilities what you know what you can
field on your side what you can actually

73
00:06:20,330 --> 00:06:24,359
build and then you have to understand
what your adversaries can do and what

74
00:06:24,360 --> 00:06:27,449
they want to do and what they want to do
is almost certainly much more

75
00:06:27,449 --> 00:06:32,990
interesting than what they can do right
because that's where you say okay if our

76
00:06:32,990 --> 00:06:37,190
adversaries are malware writers are
malware authors who are looking to write

77
00:06:37,190 --> 00:06:41,690
banking Trojans so they have some very
specific goals and those goals are not

78
00:06:41,690 --> 00:06:45,589
necessarily just simple financial
transactions

79
00:06:45,589 --> 00:06:46,569
there's a lot of

80
00:06:46,569 --> 00:06:49,749
of nuance and complexity there that's
going to shape the responses that they

81
00:06:49,749 --> 00:06:56,580
take and is going to shape whether or
not your counter measures are useful one

82
00:06:56,580 --> 00:07:01,029
of the things that working in the field
taught me is that outcomes are messy so

83
00:07:01,029 --> 00:07:09,949
we have this idea that insurance is a
goal for security organizations right

84
00:07:09,949 --> 00:07:15,069
that you want to provide an enterprise
with assurance that the systems that

85
00:07:15,069 --> 00:07:23,270
they operate will not get compromised it
turns out that especially when you're

86
00:07:23,270 --> 00:07:27,308
dealing with high risk users are people
who were specifically targeted

87
00:07:27,309 --> 00:07:30,759
assurances kind of a joke right

88
00:07:30,759 --> 00:07:34,719
you know if you're dealing with with
journalists who may get killed for the

89
00:07:34,719 --> 00:07:41,080
reporting work they're doing sometimes
you just get unlucky and we have this

90
00:07:41,080 --> 00:07:45,308
idea that we can provide more guarantees
that we are actually capable of doing

91
00:07:45,309 --> 00:07:51,379
and this goal of insurance which is
basically something that's coming out of

92
00:07:51,379 --> 00:07:55,180
and I said this is a as a consultant is
coming out of consulting marketing

93
00:07:55,180 --> 00:08:01,979
material right it's ridiculous we can't
actually do it but we've built this

94
00:08:01,979 --> 00:08:07,789
culture around it so now we promise it
even though we know we can't deliver it

95
00:08:07,789 --> 00:08:11,469
and it prevents us from acknowledging
the fact that all of the systems we

96
00:08:11,469 --> 00:08:15,879
interact with our categorically broken
they're basically just piles of bugs

97
00:08:15,879 --> 00:08:21,860
built on top of each other and the hope
that in some way be structural and it

98
00:08:21,860 --> 00:08:26,610
also because we were only willing to
evaluate solutions that give us

99
00:08:26,610 --> 00:08:33,399
assurance it limits radically the kinds
of medications were willing to look at

100
00:08:33,399 --> 00:08:39,339
for instance one of the one of the
really useful tools if you're looking at

101
00:08:39,339 --> 00:08:46,040
and user security systems is you've got
this really great pattern matching

102
00:08:46,040 --> 00:08:51,790
processor connected to that system is
called the user the user actually has a

103
00:08:51,790 --> 00:08:56,550
pretty good idea at any point in time
what things their system should probably

104
00:08:56,550 --> 00:08:59,699
be doing so if you can give the user
more

105
00:08:59,699 --> 00:09:04,029
feedback about what their system is
actually doing you can then give them

106
00:09:04,029 --> 00:09:09,259
ways to react not technically they're
not gonna say oh I you know I better

107
00:09:09,259 --> 00:09:13,369
reinstall the operating system but they
may say oh it looks like you know this

108
00:09:13,369 --> 00:09:17,209
thing is telling me that this machine is
transmitting audio when I don't think it

109
00:09:17,209 --> 00:09:21,248
should be maybe I'm just gonna close
that machine leave it out of the room or

110
00:09:21,249 --> 00:09:25,809
you know it gives them ways to adapt
rate and is that have a tactician and be

111
00:09:25,809 --> 00:09:29,799
perfect as the user going to recognize
this all the time no course not but that

112
00:09:29,799 --> 00:09:34,329
doesn't matter because we're not going
for insurance but we're going for is

113
00:09:34,329 --> 00:09:42,998
improvement in outcomes statistical
improvement in outcomes over time so as

114
00:09:42,999 --> 00:09:49,339
I said I learned a lot about the things
we don't know about this process by

115
00:09:49,339 --> 00:09:56,259
working with users who have actual
problems one of the core things there is

116
00:09:56,259 --> 00:10:02,309
that if you don't understand how your
users plan for and think around security

117
00:10:02,309 --> 00:10:08,919
you can't help them now if your users
are not targeted if they're not at

118
00:10:08,919 --> 00:10:13,899
significant risk maybe they don't think
about securities certainly maybe they

119
00:10:13,899 --> 00:10:18,609
don't think about security as much as
all of us whose you know hobbies our

120
00:10:18,609 --> 00:10:23,079
lives are thinking about security would
like but there is still a planning

121
00:10:23,079 --> 00:10:26,758
process that happens and when you're
working with people who are targeted

122
00:10:26,759 --> 00:10:29,449
dinner where they're targeted that
planning process becomes more obvious

123
00:10:29,449 --> 00:10:36,008
and so it's easier to study and
understand how that that shifts over so

124
00:10:36,009 --> 00:10:41,230
the general structure of the task is
planning in the presence of an adversary

125
00:10:41,230 --> 00:10:43,989
and there's literature in various
contexts here

126
00:10:43,989 --> 00:10:48,699
unsurprisingly various militaries have a
lot to say about this stuff although

127
00:10:48,699 --> 00:10:51,099
they're planning processes are more
heavy weight than you can possibly

128
00:10:51,100 --> 00:10:59,239
imagine but there's one really useful
thing that came out of that there's a

129
00:10:59,239 --> 00:11:04,429
guy named John Boyd USAir force colonel
and he came up with this thing called in

130
00:11:04,429 --> 00:11:09,749
to leap and this was gosh we've been in
the fifties and it was kind of one of

131
00:11:09,749 --> 00:11:12,769
the first structural representations of
up

132
00:11:12,769 --> 00:11:18,749
of the planning process in the presence
of an adversary and so a new loop jump

133
00:11:18,749 --> 00:11:23,639
foot jump forward here in Italy is the
core of this process right observe

134
00:11:23,639 --> 00:11:27,540
orient decide act observe the world
around you

135
00:11:27,540 --> 00:11:32,420
orient yourself within the possible
actions that you can take decide on an

136
00:11:32,420 --> 00:11:37,498
action implemented go back to observing
right that's the core and you can you

137
00:11:37,499 --> 00:11:41,089
can shift that around depending on
exactly how you want to divide up the

138
00:11:41,089 --> 00:11:48,299
steps but that's the core of every
oppositional planning process there is a

139
00:11:48,299 --> 00:11:55,339
notion of turning inside your adversary
zulu the idea that if you if you in your

140
00:11:55,339 --> 00:11:59,689
adversary are both trying to plan your
reactions if you can plan more quickly

141
00:11:59,689 --> 00:12:05,589
than them at the same death you will end
up all else being equal which it isn't

142
00:12:05,589 --> 00:12:09,959
but if it were equal you know you will
end up basically being able to tout

143
00:12:09,959 --> 00:12:16,199
plein them and and winning the conflict
so one of the things that this brings us

144
00:12:16,199 --> 00:12:22,179
to his cognitive overhead so if you are
trying to build security time to design

145
00:12:22,179 --> 00:12:27,619
security countermeasures security tools
for users and they are complex tools

146
00:12:27,619 --> 00:12:33,220
they require a lot of thinking about how
they're going to be deployed chances are

147
00:12:33,220 --> 00:12:38,980
you are actively hurting their security
outcomes because you are absorbing all

148
00:12:38,980 --> 00:12:42,579
of the space and they're planning
process

149
00:12:42,579 --> 00:12:46,299
thinking about your tools that are
either going to decide that I have time

150
00:12:46,299 --> 00:12:46,869
for it

151
00:12:46,869 --> 00:12:54,019
misuse it or shortcut all of the other
things and there's a there's a whole

152
00:12:54,019 --> 00:13:03,230
list of stuff around that circle all of
the different task domains that users

153
00:13:03,230 --> 00:13:10,720
need to be thinking about of which you
know like digital security practices for

154
00:13:10,720 --> 00:13:17,830
any real process is just one tiny little
thing

155
00:13:17,830 --> 00:13:22,180
operational utility and functional the
playability of the two other things that

156
00:13:22,180 --> 00:13:28,540
we come out of the the operations
process if you don't have a very good

157
00:13:28,540 --> 00:13:35,640
idea of whether or not the system that
you're building is going to support user

158
00:13:35,640 --> 00:13:39,189
goals and let them get more done

159
00:13:39,190 --> 00:13:45,240
chances are it's not useful and that is
not a question of is it going to keep

160
00:13:45,240 --> 00:13:48,540
them from being alone it's a question of
are they going to get more work done

161
00:13:48,540 --> 00:13:55,060
when we talk about security a lot of
what we're talking about really is

162
00:13:55,060 --> 00:13:59,859
efficacy has remember our our definition
beginning to doubt whether or not the

163
00:13:59,860 --> 00:14:04,350
adversaries can frustrate the goals of
the users can they stop those users from

164
00:14:04,350 --> 00:14:08,030
getting that thing done making them
spend all of their time thinking about

165
00:14:08,030 --> 00:14:12,459
security systems is a very effective way
to stop them from getting anything done

166
00:14:12,460 --> 00:14:18,090
this is the worst is better monitor
right in many cases if the system is

167
00:14:18,090 --> 00:14:24,340
technically less secure but functionally
lets a user get that much more work done

168
00:14:24,340 --> 00:14:29,300
it is a better system from an efficacy
perspective and from an outcomes

169
00:14:29,300 --> 00:14:34,300
perspective because the user's goals are
to get work done at an acceptable risk

170
00:14:34,300 --> 00:14:40,979
in an acceptable cost not to maintain
perfect security we as security

171
00:14:40,980 --> 00:14:46,320
professionals are responsible for
helping them to maximize their goals not

172
00:14:46,320 --> 00:14:55,180
the goals that we think they should have
no liability is another interesting line

173
00:14:55,180 --> 00:15:01,079
this is culture match this is context
match does this tool give them the

174
00:15:01,080 --> 00:15:07,380
things that they need to work in a
specific place you know the number of

175
00:15:07,380 --> 00:15:12,340
times we've seen systems which in theory
are really secure but in practice

176
00:15:12,340 --> 00:15:18,730
require things on the ground that none
of the relevant users have you know this

177
00:15:18,730 --> 00:15:24,630
is this is why johnnie can encrypt this
is much of the story and user facing

178
00:15:24,630 --> 00:15:29,030
security tools and the same process
repeats in the enterprise just with much

179
00:15:29,030 --> 00:15:29,670
bigger

180
00:15:29,670 --> 00:15:35,939
checks attached to it so invariants are
one of the tools that we used to think

181
00:15:35,940 --> 00:15:39,800
about this stuff so this this is kind of
the set of invariance that you guys are

182
00:15:39,800 --> 00:15:44,410
probably familiar with seeing the idea
of an invariant is this is something

183
00:15:44,410 --> 00:15:48,790
that you may wish to preserve in the
operation of the system to varying

184
00:15:48,790 --> 00:15:53,930
degrees and with varying resource
commitments right so you know I want to

185
00:15:53,930 --> 00:15:58,199
preserve availability within the system
and I'm willing to you know I want you

186
00:15:58,200 --> 00:16:01,560
know this many 9's about time and I'm
willing to spend this much money to get

187
00:16:01,560 --> 00:16:06,300
it the same you know I need to preserve
the integrity and I'm willing to spend

188
00:16:06,300 --> 00:16:10,089
this much money I need to preserve
interoperability and I'm willing to

189
00:16:10,090 --> 00:16:15,270
spend this much money now in this case
we don't think of that as a you know as

190
00:16:15,270 --> 00:16:19,949
like an uptime metric failover
interoperability systems

191
00:16:19,950 --> 00:16:24,600
well except when you've got a massive
enterprise deployment and it turns out

192
00:16:24,600 --> 00:16:28,420
that when you try to to get it working
and production it doesn't work and you

193
00:16:28,420 --> 00:16:33,860
roll back right that is that is
interoperability failover we just don't

194
00:16:33,860 --> 00:16:37,900
think of it in those terms it's the same
kind of invariant it turns out there's a

195
00:16:37,900 --> 00:16:42,980
lot of invariants that you may want to
preserve here's a list of a few of them

196
00:16:42,980 --> 00:16:51,350
the design of invariance has a lot to do
with cognitive overload right everything

197
00:16:51,350 --> 00:16:57,830
on this list that you tried to give
users tool to preserve or to change the

198
00:16:57,830 --> 00:17:03,950
structure of they have to take into
account and they're planning process you

199
00:17:03,950 --> 00:17:09,069
also have to make it legible to them
they have to be able to understand what

200
00:17:09,069 --> 00:17:15,030
the tools they have available to them or
they can't plan with them now if we're

201
00:17:15,030 --> 00:17:20,200
talking about users who are not a
specific risk you are not targeted or

202
00:17:20,200 --> 00:17:25,400
who were not aware that they're targeted
legibility can become less important

203
00:17:25,400 --> 00:17:29,800
because you know maybe oh you get
confidentiality for free and it doesn't

204
00:17:29,800 --> 00:17:33,930
matter if you don't know about you still
sort of get the benefit except in almost

205
00:17:33,930 --> 00:17:37,460
every case where you have illegible
invariance the

206
00:17:37,460 --> 00:17:42,880
the user is going to act as though the
system was less secure right if they're

207
00:17:42,880 --> 00:17:46,470
you know if they're aware that there
might be a problem and they don't know

208
00:17:46,470 --> 00:17:50,260
that the system can help them then
they're gonna you know trust their

209
00:17:50,260 --> 00:17:55,059
ecommerce merchants less or whatever you
know whatever the whatever the specific

210
00:17:55,059 --> 00:18:00,010
cases so even for users who are targeted
if they don't understand the ways in

211
00:18:00,010 --> 00:18:03,490
which the system can help them you were
doing them a disservice and you are

212
00:18:03,490 --> 00:18:09,919
hurting their ability to accomplish
their goals so if we say that this is a

213
00:18:09,919 --> 00:18:17,640
design process what does design mean and
we'll talk about what security designed

214
00:18:17,640 --> 00:18:26,070
specifically so if you are going to
design anything right

215
00:18:26,070 --> 00:18:31,210
you need to to understand and document
what the constraints are and what the

216
00:18:31,210 --> 00:18:34,830
capabilities are right you have to you
have to understand the problem you also

217
00:18:34,830 --> 00:18:38,730
have to be able to tell people about the
problem if you're looking at a software

218
00:18:38,730 --> 00:18:41,820
development lifecycle you looking at you
know kind of your front-loaded

219
00:18:41,820 --> 00:18:47,908
requirements work if you don't if you
can't communicate to business drivers

220
00:18:47,909 --> 00:18:54,820
you know this is why we need to deploy
tell us 12 in this context than you are

221
00:18:54,820 --> 00:18:58,500
unlikely to get the budget for testing
or for dads are for whatever you need

222
00:18:58,500 --> 00:19:05,140
what we're talking about here is up a
few levels though this is looking at

223
00:19:05,140 --> 00:19:09,429
what are the requirements for this
system what are the tasks that we intend

224
00:19:09,429 --> 00:19:14,299
this system to support right so
synthesized invalidate potential

225
00:19:14,299 --> 00:19:19,770
solutions you know so we think we have
an understanding of what might work does

226
00:19:19,770 --> 00:19:21,309
it actually work

227
00:19:21,309 --> 00:19:25,820
justify them support the development
process and prevent drift you know where

228
00:19:25,820 --> 00:19:29,899
somebody further down the line says well
we don't actually have time for I don't

229
00:19:29,899 --> 00:19:33,689
know doing any encryption whatsoever

230
00:19:33,690 --> 00:19:37,570
the number of times I've seen he
security features pulled out because

231
00:19:37,570 --> 00:19:39,250
schedule

232
00:19:39,250 --> 00:19:46,750
so when we talk about security design
I'm at least talking specifically about

233
00:19:46,750 --> 00:19:50,190
participatory design and actually in the
jump ahead a few slides and we'll come

234
00:19:50,190 --> 00:19:56,570
back to it cause I wanna Scituate this a
bit better so this is a development

235
00:19:56,570 --> 00:20:00,679
staff requirements of architectural
design development testing operations

236
00:20:00,680 --> 00:20:05,340
you know let's pretend it's a nice
waterfall it isn't but and so when we

237
00:20:05,340 --> 00:20:09,770
when we think normally about security
operations through the stacks you got

238
00:20:09,770 --> 00:20:14,490
architectural analysis you've got
standards and frameworks for you know

239
00:20:14,490 --> 00:20:18,150
working for developers to build more
secure solutions you've got security

240
00:20:18,150 --> 00:20:21,810
testing which is where a lot of us spend
a lot of our time and then he got

241
00:20:21,810 --> 00:20:24,980
monitoring an instant response and
catching and all the other kind of

242
00:20:24,980 --> 00:20:32,940
operation stuff so the core of a lot of
thinking about security and requirements

243
00:20:32,940 --> 00:20:36,110
as threat modeling right so threat
modeling done correctly

244
00:20:36,110 --> 00:20:44,129
drives most of your SCLC remodeling and
security architectural analysis heavily

245
00:20:44,130 --> 00:20:50,800
overlap there basically 90% the same
task it specify you know it it drives

246
00:20:50,800 --> 00:20:58,129
your specs for what security requirement
what what things need to be maintained

247
00:20:58,130 --> 00:21:02,500
during the development process it drives
testing because you now have a list of

248
00:21:02,500 --> 00:21:06,800
everything that can go wrong if you have
a proper threat model and it's a

249
00:21:06,800 --> 00:21:12,740
reference document for monitoring an
instant response when you're talking

250
00:21:12,740 --> 00:21:16,380
about security design we add on to that
an adversary modeling stage which a lot

251
00:21:16,380 --> 00:21:21,040
of ask it because we think we understand
who our attackers are so you spent some

252
00:21:21,040 --> 00:21:25,520
time and actually understand what the
problem is and then we bring in this

253
00:21:25,520 --> 00:21:30,700
participatory design and security design
steps up front and this is where we

254
00:21:30,700 --> 00:21:36,050
figure out are we actually designing the
system that is going to provide users

255
00:21:36,050 --> 00:21:42,500
with the things that it means that they
need to know the difference with

256
00:21:42,500 --> 00:21:52,550
participatory design and the reason that
we care is that security the the goals

257
00:21:52,550 --> 00:21:58,669
that users have are not are often not
obvious right if you're designing escape

258
00:21:58,670 --> 00:22:03,230
you know what are the one of these cases
for Skype what are the goals that users

259
00:22:03,230 --> 00:22:08,390
have escaped how much is confidentiality
important in what cases is important

260
00:22:08,390 --> 00:22:15,980
what does confidentiality mean again you
know the narrower the deployment

261
00:22:15,980 --> 00:22:20,480
scenario the better an idea you gonna
have this one of the more the really

262
00:22:20,480 --> 00:22:25,090
interesting things that came up
supporting some folks in Syria was that

263
00:22:25,090 --> 00:22:30,570
given the option of secure text chat
tools that we could say fairly strongly

264
00:22:30,570 --> 00:22:34,220
weren't gonna be intercepted assuming
they didn't have money on their machines

265
00:22:34,220 --> 00:22:39,230
etc and could provide better data
protection etcetera they were much more

266
00:22:39,230 --> 00:22:44,140
interested in continuing to use Skype
because it gave him video and this

267
00:22:44,140 --> 00:22:49,230
didn't make any sense to a lot of people
like why why do these people you know

268
00:22:49,230 --> 00:22:52,870
and it was a question training or
anything else and it turned out that one

269
00:22:52,870 --> 00:22:55,709
of the first things you do when he
started a Skype calls you take the

270
00:22:55,710 --> 00:23:00,680
laptop up and you show the whole room
and you prove to them that it was nobody

271
00:23:00,680 --> 00:23:03,370
standing in the corner with a gun
pointed at you telling you what to say

272
00:23:03,370 --> 00:23:12,449
and that ability to surface to to
basically do remote attestation of some

273
00:23:12,450 --> 00:23:20,230
degree of coercion and a conversation
was much more important then security

274
00:23:20,230 --> 00:23:27,570
then confidentiality or integrity in
that context so this is this is the kind

275
00:23:27,570 --> 00:23:32,659
of thing where if you don't actually
know your users and know their culture

276
00:23:32,660 --> 00:23:36,530
and know the things that are there
actually worried about it's very

277
00:23:36,530 --> 00:23:41,960
difficult to do these kinds of design
tasks what we've seen in the field is

278
00:23:41,960 --> 00:23:46,240
that basically the only way to do that
is to actually bring users into the

279
00:23:46,240 --> 00:23:53,380
product design process directly and into
the security design process directly now

280
00:23:53,380 --> 00:23:58,810
the Securities I'm process starts a lot
earlier than a lot of people think it's

281
00:23:58,810 --> 00:24:03,190
not it is it the requirement stage but
that's also ok so you're going to deploy

282
00:24:03,190 --> 00:24:07,030
a bunch of cryptographic protocols that
it's just you know that's just a base

283
00:24:07,030 --> 00:24:10,690
layer clearly that's not part of
security design

284
00:24:10,690 --> 00:24:16,300
well the invariance that you're
deploying is determined at the protocol

285
00:24:16,300 --> 00:24:21,550
level right so if you don't have users
in the room when you're starting to

286
00:24:21,550 --> 00:24:27,389
think about the invariance that you want
to have supported in your cryptographic

287
00:24:27,390 --> 00:24:35,380
protocols you are already too late but i
dont have it here there's this great

288
00:24:35,380 --> 00:24:46,140
curve cost of what it costs to fix above
and it's an exponential curve and the

289
00:24:46,140 --> 00:24:51,830
cost to fix so it's it's basically from
time an introduction to time effects and

290
00:24:51,830 --> 00:24:54,929
if you've got a requirements level bug
that you have to fix and deployment

291
00:24:54,930 --> 00:24:59,790
costs at least a hundred times more to
fix it you know something that would

292
00:24:59,790 --> 00:25:05,020
have been at the early requirements
stage literally a no no we can't do that

293
00:25:05,020 --> 00:25:09,740
because acts you know probably just a
conversation at most you know a couple

294
00:25:09,740 --> 00:25:16,660
rounds in a Word doc can become in one
case axing an entire corporate division

295
00:25:16,660 --> 00:25:21,310
in firing everyone involved because it
turned out that the products that the

296
00:25:21,310 --> 00:25:26,200
company was trying to deploy was
fundamentally unsecured apple and had

297
00:25:26,200 --> 00:25:32,670
fundamentally conflicting requirements
you know if you if you have to do

298
00:25:32,670 --> 00:25:36,170
cryptographic protocol redevelopment
it's probably even more expensive than

299
00:25:36,170 --> 00:25:40,540
that because now you're not just talking
about one product you're talking about

300
00:25:40,540 --> 00:25:46,159
multiple products and standards
processes and you know it it becomes

301
00:25:46,160 --> 00:25:52,410
nightmarish so if you want to do
security design in a meaningful way it

302
00:25:52,410 --> 00:25:58,230
has to be participatory participatory
design means recognizing users of shit

303
00:25:58,230 --> 00:26:03,550
thirties on their goals so instead of
assuming that you know what they want

304
00:26:03,550 --> 00:26:05,250
you know bring them in

305
00:26:05,250 --> 00:26:11,640
working with them to describe things
that they're trying to optimize for you

306
00:26:11,640 --> 00:26:14,640
know it means that kind of deep cultural
engagement that would let you see oh

307
00:26:14,640 --> 00:26:18,670
right this is why these guys care more
about video then they care about

308
00:26:18,670 --> 00:26:27,370
confidentiality users really everyone is
horrible at telling people what they

309
00:26:27,370 --> 00:26:32,330
know and how they know it especially
when you're talking about

310
00:26:32,330 --> 00:26:40,419
group operations or or communities in
and community support for each other to

311
00:26:40,420 --> 00:26:44,550
working with users over time let's use
surface that kind of tacit an embodied

312
00:26:44,550 --> 00:26:52,879
knowledge that is often really critical
to operational outcomes you know knowing

313
00:26:52,880 --> 00:26:57,830
for instance oh you know yes this is a
chat system but a lot of the work what's

314
00:26:57,830 --> 00:27:03,409
happening over it is arranging financial
transfers ok what are the you know what

315
00:27:03,410 --> 00:27:06,850
is the tacit knowledge around financial
transfers it's not just that it needs to

316
00:27:06,850 --> 00:27:13,290
be confidential building long-term
community trust this is critical if you

317
00:27:13,290 --> 00:27:17,230
aren't a scenario where you cannot
simply force users to use whatever it is

318
00:27:17,230 --> 00:27:22,380
you provide to them and even if you
think you can force users to use

319
00:27:22,380 --> 00:27:25,990
whatever it is you provide to them
you're probably wrong unless you're in

320
00:27:25,990 --> 00:27:32,940
the military even then you know if you
have a system that is designed with user

321
00:27:32,940 --> 00:27:39,800
participation that will end up doing a
lot of subtle ways through the way you

322
00:27:39,800 --> 00:27:41,490
talk about things that way

323
00:27:41,490 --> 00:27:47,320
features introduced the way the entire
process works and that's really trust is

324
00:27:47,320 --> 00:27:51,700
often critical for adoption and remember
the entire point of this process is to

325
00:27:51,700 --> 00:27:55,770
improve security outcomes so if you're
going to build a security tool that no

326
00:27:55,770 --> 00:27:58,260
one's going to use because you didn't
bother gaining the trust of the

327
00:27:58,260 --> 00:28:04,120
community during the development process
you have wasted all of your time if you

328
00:28:04,120 --> 00:28:07,780
had users involved up front this lets
you short-circuit long development

329
00:28:07,780 --> 00:28:14,920
processes an interesting example for
this there's now a joint interagency

330
00:28:14,920 --> 00:28:17,020
field experimentation

331
00:28:17,020 --> 00:28:21,560
which is a USDOT thing where they get
people together and they take early

332
00:28:21,560 --> 00:28:25,320
prototypes of kit I think they're going
to ship in different contexts I've

333
00:28:25,320 --> 00:28:29,770
interacted with it the humanitarian
assistance world and they go out into

334
00:28:29,770 --> 00:28:35,030
this abandoned their National Guard base
and they try stuff and I try it when

335
00:28:35,030 --> 00:28:38,050
it's still an early prototype in the
trial with engineers and what people who

336
00:28:38,050 --> 00:28:42,290
can use it in the ground for real and
they find out things like oh this radio

337
00:28:42,290 --> 00:28:46,540
doesn't work with this battery pack
system instead of buying a hundred and

338
00:28:46,540 --> 00:28:50,020
fifty thousand units and shipping them
around the world to a conflict region

339
00:28:50,020 --> 00:28:53,139
and then plugging them in and finding
out this radio doesn't work with his

340
00:28:53,140 --> 00:28:58,880
battery pack well I guess that was you
know you know like an hour 35 million

341
00:28:58,880 --> 00:29:04,280
dollars we just weren't you know we
don't normally deal in software

342
00:29:04,280 --> 00:29:07,560
development cycles that are that long
without expensive although sometimes we

343
00:29:07,560 --> 00:29:13,020
do the earlier you can get people in the
loop the faster your iteration speed is

344
00:29:13,020 --> 00:29:21,020
you know even just versus more
traditional design processes blended

345
00:29:21,020 --> 00:29:23,860
countermeasures are another really
interesting thing this is basically

346
00:29:23,860 --> 00:29:29,899
getting us out of that digital security
is a digital computer security is about

347
00:29:29,900 --> 00:29:31,040
computers

348
00:29:31,040 --> 00:29:36,590
this is the place where we start looking
at how we build countermeasures that

349
00:29:36,590 --> 00:29:42,189
combined cultural aspects and workflow
aspects and digital aspects and build

350
00:29:42,190 --> 00:29:47,550
that support and it's really difficult
to do unless you've got people in the

351
00:29:47,550 --> 00:29:53,070
room and the last one is minimizing team
ago so one of the one of the fundamental

352
00:29:53,070 --> 00:29:59,990
notions of participatory design is that
it is completely centers the user and

353
00:29:59,990 --> 00:30:06,660
what the users goals are and you know i
i joke sometimes you know you can either

354
00:30:06,660 --> 00:30:11,900
if you're if you're trying to develop
secure systems for high-risk users you

355
00:30:11,900 --> 00:30:19,830
can either kill your ego or users the
the issue is that basically as long as

356
00:30:19,830 --> 00:30:26,600
you are focus more on what your product
is going to do as long as you're focused

357
00:30:26,600 --> 00:30:29,639
more on the things that you're going to
build

358
00:30:29,639 --> 00:30:36,189
rather than the things that your users
need and those goals you're going to end

359
00:30:36,190 --> 00:30:41,489
up in our saying how our Users thinks
this but they're wrong and they should

360
00:30:41,489 --> 00:30:44,869
do it the other way and you're not going
to do that analysis to understand why

361
00:30:44,869 --> 00:30:53,320
that doesn't work any process that
convinces you to spend less time

362
00:30:53,320 --> 00:30:57,369
thinking about you and more time
thinking about your user is going to

363
00:30:57,369 --> 00:31:11,580
result in better security outcomes

364
00:31:11,580 --> 00:31:15,699
I don't know how many of you have tried
to deal with process change for real

365
00:31:15,700 --> 00:31:23,150
to complete nightmare here are some
practical hints that I found work at

366
00:31:23,150 --> 00:31:28,240
least some of the time so find the
people who were doing the other parts of

367
00:31:28,240 --> 00:31:32,850
early product design right cuz what
you're trying to do is basically jump

368
00:31:32,850 --> 00:31:39,280
forward you know several steps we
normally engage like if we're really

369
00:31:39,280 --> 00:31:43,660
lucky we get to engage in architectural
design often and you know when we're

370
00:31:43,660 --> 00:31:47,740
down here or down it testing you know
somebody's built some software and they

371
00:31:47,740 --> 00:31:50,320
say hey coming tested this is secure

372
00:31:50,320 --> 00:31:54,300
you know we're now trying to jump all
the way up to you like requirements

373
00:31:54,300 --> 00:31:58,340
analysis that's normally where the
business analysts are sitting maybe

374
00:31:58,340 --> 00:32:01,439
where some of the UX guys are sitting
depending on if you're kind of more

375
00:32:01,440 --> 00:32:08,760
product team or more and are priced team
find this guy's insist on going to all

376
00:32:08,760 --> 00:32:12,770
of their meetings maybe not all of them
but you know just start showing up at

377
00:32:12,770 --> 00:32:16,090
their meetings and listening right

378
00:32:16,090 --> 00:32:23,970
learn their language and their processes
it will be very painful I'm not I'm not

379
00:32:23,970 --> 00:32:29,240
gonna say that it's not going to be but
if you do that then when you push back

380
00:32:29,240 --> 00:32:34,110
and you say okay there's a security
implication here or there's an

381
00:32:34,110 --> 00:32:39,000
opportunity here to do something that
will improve user security you can tell

382
00:32:39,000 --> 00:32:42,590
them what their do you know what you
want to do when what you care about in a

383
00:32:42,590 --> 00:32:45,899
way that they're actually going to
understand the security community is

384
00:32:45,900 --> 00:32:50,300
terrible about this we insist that
everyone adapt to our language and our

385
00:32:50,300 --> 00:32:54,919
process you know when sometimes there
are good reasons for that and sometimes

386
00:32:54,920 --> 00:32:59,310
it's just well you know we have all the
bugs and will make your company's life

387
00:32:59,310 --> 00:33:03,899
hell if you don't adapt to our process
you know and and you know this is this

388
00:33:03,900 --> 00:33:07,760
maybe it's justified maybe it isn't but
for this kind of stuff if you want

389
00:33:07,760 --> 00:33:11,120
people to let you into that early design
process that you can actually improve

390
00:33:11,120 --> 00:33:18,310
security outcomes you don't get to do
that you know walk through that walk

391
00:33:18,310 --> 00:33:22,759
through that design lifecycle carefully
give yourself

392
00:33:22,759 --> 00:33:31,199
room to fail if you can and this is
always difficult for your efforts were

393
00:33:31,199 --> 00:33:34,989
expected to kind of show ROI and
everything else is one of the reasons

394
00:33:34,989 --> 00:33:39,709
why Anchorage if you're trying to build
a security design effort don't get

395
00:33:39,709 --> 00:33:45,749
permission to start with just start
showing up two things it lets you did

396
00:33:45,749 --> 00:33:49,529
you that much more buffer before you
have to actually be accountable and

397
00:33:49,529 --> 00:33:58,889
prove ROI and there is you know this is
there is not a lot of existing knowledge

398
00:33:58,889 --> 00:34:03,039
in existing understanding on how we
structure of this stuff right

399
00:34:03,039 --> 00:34:11,969
the the security design process is not
well understood yet it is not something

400
00:34:11,969 --> 00:34:19,418
that is deployed its scale and and
really any head shops for real

401
00:34:19,418 --> 00:34:24,089
we're still struggling to even get
threat modeling done how many people

402
00:34:24,089 --> 00:34:27,250
here have built a threat model for real

403
00:34:27,250 --> 00:34:37,849
12 I'm gonna jump in and do an ad hoc
this is why and how you should threat

404
00:34:37,849 --> 00:34:44,319
model because I forget that despite it
being 2015 and the fact that threat

405
00:34:44,319 --> 00:34:48,940
models were in very kind of yesterday's
news in 2004 everybody has forgotten

406
00:34:48,940 --> 00:34:55,399
about them or forgotten to start doing
them

407
00:34:55,399 --> 00:35:05,089
model is how you define coherently what
security is so it's the thing that it's

408
00:35:05,089 --> 00:35:12,730
the carrier for your security design
it's the carrier for the requirements

409
00:35:12,730 --> 00:35:20,859
for security for a system it is how you
communicate this requirement that's how

410
00:35:20,859 --> 00:35:25,369
you show that you finished your analysis
so good secure a good threat model is

411
00:35:25,369 --> 00:35:41,670
slide deck not a good model shows you
when you're done into this is this is

412
00:35:41,670 --> 00:35:45,740
one of the problems that we often have
when we're doing security architecture

413
00:35:45,740 --> 00:35:51,509
analysis you know security architecture
analysis often is we get a bunches

414
00:35:51,510 --> 00:35:55,420
hopefully smart people and they sit
around a room together and we try to

415
00:35:55,420 --> 00:36:02,309
figure out how an application might
break and when everybody's Board or the

416
00:36:02,309 --> 00:36:07,299
beer runs out we assume that we found
all the potential ways it can fail this

417
00:36:07,299 --> 00:36:14,950
doesn't work so the idea of a threat
model good threat model you build a

418
00:36:14,950 --> 00:36:18,109
model the requirements in a model of the
architecture that that are separate and

419
00:36:18,109 --> 00:36:22,420
our formal not and the lake said code
probability sense but in the sense that

420
00:36:22,420 --> 00:36:27,520
you can actually say have we modeled how
we understood all of the ways that the

421
00:36:27,520 --> 00:36:31,450
parts of the system can interact have we
understood all of the ways that the

422
00:36:31,450 --> 00:36:36,339
system can break in can go wrong you
know at a high enough level in with

423
00:36:36,339 --> 00:36:44,099
enough formality that we can actually
then say plan tests from that you know

424
00:36:44,099 --> 00:36:52,779
tell developers this is the list of
things that you need to build to a good

425
00:36:52,779 --> 00:36:58,460
threat model is a lot of your security
specification and security requirements

426
00:36:58,460 --> 00:37:03,800
documentation so many people here have
interacted with

427
00:37:03,800 --> 00:37:09,200
security requirements documentation and
systems at all

428
00:37:09,200 --> 00:37:18,589
ok a few more not that many more that's
terrifying you know if you if you ship

429
00:37:18,590 --> 00:37:22,700
the system and it doesn't have security
requirements

430
00:37:22,700 --> 00:37:28,680
it's broken fundamentally because that
means that you do not know what you do

431
00:37:28,680 --> 00:37:34,680
not know whether security doesn't have
any security requirements so therefore

432
00:37:34,680 --> 00:37:36,750
it cannot be secure

433
00:37:36,750 --> 00:37:40,840
you know you can't you have no way of
telling whether or not it secure

434
00:37:40,840 --> 00:37:50,260
you know there's a lot of

435
00:37:50,260 --> 00:37:54,940
there's a lot of work in the development
process that we end up skipping when we

436
00:37:54,940 --> 00:38:02,060
focus entirely on the low level entirely
on the testing you know the idea that

437
00:38:02,060 --> 00:38:08,680
finding finding bugs in the system is
meaning fall is really kind of you know

438
00:38:08,680 --> 00:38:12,230
as a meaningful indicator of whether or
not secure doesn't really tell us

439
00:38:12,230 --> 00:38:18,160
anything you know if you if you know
that you found a bug that's grade but it

440
00:38:18,160 --> 00:38:21,990
doesn't tell you how many more might be
there if you don't have security

441
00:38:21,990 --> 00:38:23,689
requirements you're screwed

442
00:38:23,690 --> 00:38:28,910
you know you have no way of telling them
the threat model should be the thing

443
00:38:28,910 --> 00:38:34,049
that's driving that are building three
models first if you're not building

444
00:38:34,050 --> 00:38:38,390
models for every piece of code you write
you know every piece of code the org

445
00:38:38,390 --> 00:38:44,220
rights you're not ready to do security
design yet once you are at that point

446
00:38:44,220 --> 00:38:50,149
you know and once you have a more mature
process this is when you can start

447
00:38:50,150 --> 00:38:54,080
thinking about whether or not you are
actually supporting the things that your

448
00:38:54,080 --> 00:39:02,200
users are trying to do so I'm gonna go
ahead and jump to questions I've got

449
00:39:02,200 --> 00:39:08,640
more stuff that I can talk about but I
know that I'm kind of a good ways from

450
00:39:08,640 --> 00:39:14,379
where a lot of the content and this
conference has been and I think that'd

451
00:39:14,380 --> 00:39:21,540
be more useful

452
00:39:21,540 --> 00:39:31,230
I guess you had a quite clear story and
thank you very much

