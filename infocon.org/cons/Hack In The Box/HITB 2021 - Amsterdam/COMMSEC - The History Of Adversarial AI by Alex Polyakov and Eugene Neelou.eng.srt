1
00:00:16,800 --> 00:00:17,440
all right

2
00:00:17,440 --> 00:00:19,600
hi everyone and thank you for visiting

3
00:00:19,600 --> 00:00:21,039
our presentation

4
00:00:21,039 --> 00:00:23,439
uh i'm eugene and together with alex

5
00:00:23,439 --> 00:00:25,519
we'll tell you about the new immersion

6
00:00:25,519 --> 00:00:27,760
field of securing trusted ti

7
00:00:27,760 --> 00:00:29,599
and in particular adversarial machine

8
00:00:29,599 --> 00:00:31,840
learning we have been doing security

9
00:00:31,840 --> 00:00:34,160
research more than decade and these days

10
00:00:34,160 --> 00:00:35,440
we are founders

11
00:00:35,440 --> 00:00:37,920
of adverse ai a security research

12
00:00:37,920 --> 00:00:40,320
company focusing on protecting a systems

13
00:00:40,320 --> 00:00:42,559
from cyber threats privacy issues and

14
00:00:42,559 --> 00:00:44,000
safety incidents

15
00:00:44,000 --> 00:00:45,840
we have recently released a report the

16
00:00:45,840 --> 00:00:47,200
part that covers

17
00:00:47,200 --> 00:00:49,920
the past ten years of progress uh

18
00:00:49,920 --> 00:00:51,840
towards securities systems

19
00:00:51,840 --> 00:00:54,320
and it includes signals from uh

20
00:00:54,320 --> 00:00:56,079
governments academia and

21
00:00:56,079 --> 00:00:58,480
industry so we collect all those signals

22
00:00:58,480 --> 00:00:59,199
together

23
00:00:59,199 --> 00:01:02,640
to make the problem uh more tangible and

24
00:01:02,640 --> 00:01:05,600
our findings demonstrate that a industry

25
00:01:05,600 --> 00:01:07,840
is totally unprepared for security

26
00:01:07,840 --> 00:01:09,040
threats

27
00:01:09,040 --> 00:01:10,720
and with this agenda with this

28
00:01:10,720 --> 00:01:13,360
presentation i want to prove this

29
00:01:13,360 --> 00:01:16,080
and in the end alex will present a case

30
00:01:16,080 --> 00:01:17,360
study of

31
00:01:17,360 --> 00:01:20,560
security assessment assessing

32
00:01:20,560 --> 00:01:26,240
ai facial recognition systems

33
00:01:26,240 --> 00:01:28,000
uh in the first section i want to

34
00:01:28,000 --> 00:01:30,320
introduce the problem of securing a

35
00:01:30,320 --> 00:01:31,200
systems

36
00:01:31,200 --> 00:01:32,640
i want to prove that it's not a

37
00:01:32,640 --> 00:01:34,720
theoretical threat

38
00:01:34,720 --> 00:01:37,920
but the real one and the industry

39
00:01:37,920 --> 00:01:41,360
is totally losing this game so far

40
00:01:41,360 --> 00:01:43,439
if you take a look at the history of

41
00:01:43,439 --> 00:01:45,920
cyber security and technology in general

42
00:01:45,920 --> 00:01:47,520
we can see that every decade was

43
00:01:47,520 --> 00:01:49,680
dominated by specific technology

44
00:01:49,680 --> 00:01:53,200
and attackers started to exploit it and

45
00:01:53,200 --> 00:01:54,000
then the security

46
00:01:54,000 --> 00:01:56,960
solutions emerged so we've had an era of

47
00:01:56,960 --> 00:01:59,040
network security of endpoint security

48
00:01:59,040 --> 00:02:00,960
application security and now

49
00:02:00,960 --> 00:02:02,719
the focus is shifting from traditional

50
00:02:02,719 --> 00:02:04,719
software to ai

51
00:02:04,719 --> 00:02:09,280
software or software 2.0

52
00:02:09,280 --> 00:02:11,599
so we believe that the next decade will

53
00:02:11,599 --> 00:02:13,280
be dedicated to

54
00:02:13,280 --> 00:02:16,400
uh securing ei systems

55
00:02:16,400 --> 00:02:18,480
but of course uh these are not changing

56
00:02:18,480 --> 00:02:19,840
errors these are

57
00:02:19,840 --> 00:02:22,480
uh extensions so we still need networks

58
00:02:22,480 --> 00:02:24,800
and points and applications secure

59
00:02:24,800 --> 00:02:27,599
and we just cannot ignore ai security

60
00:02:27,599 --> 00:02:29,360
anymore

61
00:02:29,360 --> 00:02:32,959
and besides all the hype around ti

62
00:02:32,959 --> 00:02:36,239
it's proven its value in the real world

63
00:02:36,239 --> 00:02:38,319
we have a lot of use cases in our daily

64
00:02:38,319 --> 00:02:40,480
lives like smart assistance

65
00:02:40,480 --> 00:02:43,519
that process speech or text some other

66
00:02:43,519 --> 00:02:45,040
smart applications

67
00:02:45,040 --> 00:02:48,800
used by various industries and

68
00:02:48,800 --> 00:02:52,080
we cannot avoid using ai anymore it's

69
00:02:52,080 --> 00:02:55,920
just doing real uh providing real value

70
00:02:55,920 --> 00:02:58,400
uh but the problem is that we are

71
00:02:58,400 --> 00:03:00,400
familiar with how to protect within

72
00:03:00,400 --> 00:03:03,440
traditional software and both attackers

73
00:03:03,440 --> 00:03:05,599
and defenders have clear motivations

74
00:03:05,599 --> 00:03:07,280
they know how to do their jobs

75
00:03:07,280 --> 00:03:08,959
but the same approach cannot be

76
00:03:08,959 --> 00:03:11,200
transferred to ai software

77
00:03:11,200 --> 00:03:13,120
because it's totally new paradigm and

78
00:03:13,120 --> 00:03:15,599
requires new approaches

79
00:03:15,599 --> 00:03:18,080
and the problem is that traditional

80
00:03:18,080 --> 00:03:19,120
solutions

81
00:03:19,120 --> 00:03:22,159
uh cannot transfer to ai models there

82
00:03:22,159 --> 00:03:24,239
are no anti-malware for

83
00:03:24,239 --> 00:03:27,840
uh model trojans there is no uh

84
00:03:27,840 --> 00:03:30,400
firewall product for input manipulation

85
00:03:30,400 --> 00:03:32,640
attacks during the threat detection

86
00:03:32,640 --> 00:03:36,080
uh for data exfiltration from ia models

87
00:03:36,080 --> 00:03:38,159
uh their early attempts to do some

88
00:03:38,159 --> 00:03:40,080
vulnerability analysis but this

89
00:03:40,080 --> 00:03:43,200
open source tools are very useful

90
00:03:43,200 --> 00:03:45,120
in production environments especially at

91
00:03:45,120 --> 00:03:46,720
scale

92
00:03:46,720 --> 00:03:49,840
so ai systems can be both attack target

93
00:03:49,840 --> 00:03:52,319
and attack vector because even if you

94
00:03:52,319 --> 00:03:54,239
don't care about securing

95
00:03:54,239 --> 00:03:57,120
a given system because it's low priority

96
00:03:57,120 --> 00:03:58,959
it's a new interface

97
00:03:58,959 --> 00:04:02,560
it's a new attack vector because it

98
00:04:02,560 --> 00:04:05,439
increases your attack surface so

99
00:04:05,439 --> 00:04:09,200
it opens doors to attackers

100
00:04:09,200 --> 00:04:12,319
and we cannot ignore

101
00:04:12,319 --> 00:04:15,439
any systems in production anymore uh

102
00:04:15,439 --> 00:04:17,120
so here i want to show that it's not a

103
00:04:17,120 --> 00:04:19,358
theoretical concept but a real threat

104
00:04:19,358 --> 00:04:22,400
there are several incidents uh

105
00:04:22,400 --> 00:04:25,600
uh with the real uh products um that

106
00:04:25,600 --> 00:04:26,080
relate

107
00:04:26,080 --> 00:04:29,600
to ai security and even besides

108
00:04:29,600 --> 00:04:32,720
that in many academic works

109
00:04:32,720 --> 00:04:36,160
researchers try to target a real world

110
00:04:36,160 --> 00:04:38,639
system such as smart assistance

111
00:04:38,639 --> 00:04:41,600
such as online services apis of tech

112
00:04:41,600 --> 00:04:42,400
giants

113
00:04:42,400 --> 00:04:44,080
so it's only question of time when we

114
00:04:44,080 --> 00:04:46,240
see an explosion of

115
00:04:46,240 --> 00:04:49,600
real attacks against ai systems and they

116
00:04:49,600 --> 00:04:51,440
will become as common as spam or

117
00:04:51,440 --> 00:04:52,800
ransomware

118
00:04:52,800 --> 00:04:56,000
and if we take uh look at

119
00:04:56,000 --> 00:04:59,280
a higher level we can see that there is

120
00:04:59,280 --> 00:05:00,000
a problem

121
00:05:00,000 --> 00:05:02,880
of building reliable ai systems and this

122
00:05:02,880 --> 00:05:04,000
problem is real

123
00:05:04,000 --> 00:05:07,280
and security is just one part of uh

124
00:05:07,280 --> 00:05:10,400
many problems in ai and if you group all

125
00:05:10,400 --> 00:05:12,479
the problems together we'll see

126
00:05:12,479 --> 00:05:15,680
an immersion field of trust with the eye

127
00:05:15,680 --> 00:05:18,880
and all of these areas uh we have

128
00:05:18,880 --> 00:05:19,440
grouped them

129
00:05:19,440 --> 00:05:23,360
into three big categories about so

130
00:05:23,360 --> 00:05:26,400
the ai should be functional or reliable

131
00:05:26,400 --> 00:05:28,639
it must be resilient and responsible

132
00:05:28,639 --> 00:05:31,199
all of these areas are equally important

133
00:05:31,199 --> 00:05:32,160
to build

134
00:05:32,160 --> 00:05:35,120
uh beneficial and trustworthy eye but in

135
00:05:35,120 --> 00:05:36,880
this report we've only focused on

136
00:05:36,880 --> 00:05:39,280
security part of the

137
00:05:39,280 --> 00:05:42,479
trustworthy ai with the emphasis

138
00:05:42,479 --> 00:05:46,320
on adversarial machine learning

139
00:05:46,320 --> 00:05:50,160
so the field adversarial machine

140
00:05:50,160 --> 00:05:52,000
learning is about practical attacks

141
00:05:52,000 --> 00:05:55,520
and defenses for ai systems and this

142
00:05:55,520 --> 00:05:56,880
field may

143
00:05:56,880 --> 00:05:59,919
sound like a new thing to many people

144
00:05:59,919 --> 00:06:01,759
some people never heard about this some

145
00:06:01,759 --> 00:06:03,440
security experts

146
00:06:03,440 --> 00:06:06,560
anticipated that ai might be

147
00:06:06,560 --> 00:06:10,319
a new attack target but even for them it

148
00:06:10,319 --> 00:06:12,960
it was hard to grasp how huge this field

149
00:06:12,960 --> 00:06:14,880
is nowadays

150
00:06:14,880 --> 00:06:17,440
historically ideas of adversarial

151
00:06:17,440 --> 00:06:20,000
machine learning can be traced back to

152
00:06:20,000 --> 00:06:23,520
early uh 90s with the papers

153
00:06:23,520 --> 00:06:27,680
about learning under malicious uh noise

154
00:06:27,680 --> 00:06:30,319
then there is the interest was reborn

155
00:06:30,319 --> 00:06:31,400
around

156
00:06:31,400 --> 00:06:35,039
2004 with the some

157
00:06:35,039 --> 00:06:38,319
attacks on classification algorithms but

158
00:06:38,319 --> 00:06:40,560
it remained a very small area of

159
00:06:40,560 --> 00:06:42,639
research for a long time

160
00:06:42,639 --> 00:06:46,240
until around 2014 when the first

161
00:06:46,240 --> 00:06:49,680
attacks against diplomat algorithms

162
00:06:49,680 --> 00:06:53,120
were published around the same time

163
00:06:53,120 --> 00:06:56,240
we've seen an explosion of computer

164
00:06:56,240 --> 00:06:59,360
vision algorithms and of

165
00:06:59,360 --> 00:07:02,319
generative adversarial networks so all

166
00:07:02,319 --> 00:07:03,440
of this together

167
00:07:03,440 --> 00:07:06,319
motivated some research about securing

168
00:07:06,319 --> 00:07:06,800
and

169
00:07:06,800 --> 00:07:10,400
attacking a systems

170
00:07:10,400 --> 00:07:14,880
and also the growth in the recent years

171
00:07:14,880 --> 00:07:18,000
uh is goes

172
00:07:18,000 --> 00:07:20,400
in line with the national initiatives

173
00:07:20,400 --> 00:07:22,319
which also

174
00:07:22,319 --> 00:07:25,440
motivate some academic research

175
00:07:25,440 --> 00:07:29,199
we have collected around 4 000

176
00:07:29,199 --> 00:07:32,800
research papers and we have found out

177
00:07:32,800 --> 00:07:33,840
that

178
00:07:33,840 --> 00:07:36,160
around 50 countries contributed to this

179
00:07:36,160 --> 00:07:37,199
field

180
00:07:37,199 --> 00:07:40,000
and most researchers are affiliated with

181
00:07:40,000 --> 00:07:42,240
academic labs rather than

182
00:07:42,240 --> 00:07:44,879
corporate research labs and this map

183
00:07:44,879 --> 00:07:45,440
shows

184
00:07:45,440 --> 00:07:47,840
uh all countries which contributed at

185
00:07:47,840 --> 00:07:49,039
least one percent

186
00:07:49,039 --> 00:07:50,800
to a total number of research

187
00:07:50,800 --> 00:07:52,400
publications

188
00:07:52,400 --> 00:07:54,879
and we see that the volume of ai

189
00:07:54,879 --> 00:07:57,039
security research

190
00:07:57,039 --> 00:07:59,599
is correlated to the maturity of tech

191
00:07:59,599 --> 00:08:01,520
industries in these countries

192
00:08:01,520 --> 00:08:04,080
uh so basically it also correlates with

193
00:08:04,080 --> 00:08:04,879
the

194
00:08:04,879 --> 00:08:08,000
adoption of ai in these countries and

195
00:08:08,000 --> 00:08:09,440
predictably

196
00:08:09,440 --> 00:08:12,800
us and china two ai superpowers

197
00:08:12,800 --> 00:08:15,599
have released more papers than the rest

198
00:08:15,599 --> 00:08:17,680
of the world combined

199
00:08:17,680 --> 00:08:20,000
and the uk and germany shared their

200
00:08:20,000 --> 00:08:21,360
third place

201
00:08:21,360 --> 00:08:24,000
however if you consider per capita

202
00:08:24,000 --> 00:08:25,120
numbers

203
00:08:25,120 --> 00:08:26,879
in millions the top three countries

204
00:08:26,879 --> 00:08:29,199
would be singapore switland

205
00:08:29,199 --> 00:08:32,479
and israel and i also want to mention

206
00:08:32,479 --> 00:08:33,679
that due to

207
00:08:33,679 --> 00:08:35,440
isolated research environment and

208
00:08:35,440 --> 00:08:37,360
language differences

209
00:08:37,360 --> 00:08:40,559
we expect that the chinese contribution

210
00:08:40,559 --> 00:08:44,080
is much bigger however

211
00:08:44,080 --> 00:08:46,080
our research methodology cannot show

212
00:08:46,080 --> 00:08:48,000
this

213
00:08:48,000 --> 00:08:50,160
so regarding the progress we can see it

214
00:08:50,160 --> 00:08:52,399
from different angles from academia from

215
00:08:52,399 --> 00:08:53,279
governments from

216
00:08:53,279 --> 00:08:56,800
industry and the history shows that

217
00:08:56,800 --> 00:08:58,800
for various cyber security domains it

218
00:08:58,800 --> 00:09:00,880
takes around 10 years to go from the

219
00:09:00,880 --> 00:09:02,320
academic concepts

220
00:09:02,320 --> 00:09:04,640
uh to a complete adoption by the

221
00:09:04,640 --> 00:09:06,800
industry by organizations

222
00:09:06,800 --> 00:09:09,279
so clearly adversarial machine learning

223
00:09:09,279 --> 00:09:10,720
is on a similar path

224
00:09:10,720 --> 00:09:12,080
and is quite close to becoming

225
00:09:12,080 --> 00:09:14,160
mainstream uh

226
00:09:14,160 --> 00:09:16,959
it's also backed by national ai policies

227
00:09:16,959 --> 00:09:18,640
and industry demand

228
00:09:18,640 --> 00:09:20,720
and talking about national policies the

229
00:09:20,720 --> 00:09:22,160
first policy

230
00:09:22,160 --> 00:09:25,680
on ai has been released uh in

231
00:09:25,680 --> 00:09:28,720
uh 2016 and

232
00:09:28,720 --> 00:09:31,760
in the u.s and the next policy focused

233
00:09:31,760 --> 00:09:33,600
on security of the ai

234
00:09:33,600 --> 00:09:36,320
uh it was released in 2019 so it took

235
00:09:36,320 --> 00:09:36,800
around

236
00:09:36,800 --> 00:09:38,880
three years to shift focus from a

237
00:09:38,880 --> 00:09:41,279
regular from basic ai policies to the

238
00:09:41,279 --> 00:09:43,200
trusted ai policies

239
00:09:43,200 --> 00:09:45,519
and we believe that this trend will

240
00:09:45,519 --> 00:09:46,240
continue

241
00:09:46,240 --> 00:09:50,880
as more than several dozen countries

242
00:09:50,880 --> 00:09:53,440
will release their follow-up uh policies

243
00:09:53,440 --> 00:09:56,000
on trusted ti after they have released

244
00:09:56,000 --> 00:09:58,959
uh regular a policies and what gives me

245
00:09:58,959 --> 00:10:01,279
hope is the industry progress

246
00:10:01,279 --> 00:10:04,399
uh we can see that there are first

247
00:10:04,399 --> 00:10:08,800
demand and to address this demand

248
00:10:08,800 --> 00:10:10,480
there are startups and consulting

249
00:10:10,480 --> 00:10:13,120
companies that offer trusted a services

250
00:10:13,120 --> 00:10:16,720
the gardener started covering this topic

251
00:10:16,720 --> 00:10:19,600
recently and we can see the growth of ai

252
00:10:19,600 --> 00:10:21,040
red teams

253
00:10:21,040 --> 00:10:23,600
across uh big tech companies like

254
00:10:23,600 --> 00:10:24,800
facebook microsoft

255
00:10:24,800 --> 00:10:29,839
media open ai metre and others

256
00:10:29,839 --> 00:10:33,839
uh so talking about ai attacks

257
00:10:33,839 --> 00:10:36,640
uh i want to present you some in science

258
00:10:36,640 --> 00:10:37,040
uh

259
00:10:37,040 --> 00:10:39,440
based on uh more than two thousand

260
00:10:39,440 --> 00:10:40,800
attack cases

261
00:10:40,800 --> 00:10:45,680
uh against around uh 100 applications

262
00:10:45,680 --> 00:10:48,959
so let's take a look uh basically with

263
00:10:48,959 --> 00:10:50,320
the growth of ai

264
00:10:50,320 --> 00:10:53,839
uh cyber attacks will focus on fulham

265
00:10:53,839 --> 00:10:57,600
uh uh those part of smart software

266
00:10:57,600 --> 00:11:01,600
like vision or analytical or language

267
00:11:01,600 --> 00:11:04,880
parts because the i has introduced

268
00:11:04,880 --> 00:11:06,399
the new type of cognitive user

269
00:11:06,399 --> 00:11:08,560
interfaces because we can

270
00:11:08,560 --> 00:11:11,760
make speech commands

271
00:11:11,760 --> 00:11:15,200
we can make gesture commands so it will

272
00:11:15,200 --> 00:11:16,399
be easy

273
00:11:16,399 --> 00:11:19,200
to fool those systems and as we can see

274
00:11:19,200 --> 00:11:20,079
there are many

275
00:11:20,079 --> 00:11:22,640
applications that already attacked and

276
00:11:22,640 --> 00:11:23,680
the largest share

277
00:11:23,680 --> 00:11:25,519
goes to vision and there are a few

278
00:11:25,519 --> 00:11:28,160
reasons behind this

279
00:11:28,160 --> 00:11:31,120
large chair first of all computer vision

280
00:11:31,120 --> 00:11:32,560
is the most mature

281
00:11:32,560 --> 00:11:35,920
area of ai so

282
00:11:36,079 --> 00:11:39,040
that's why it's

283
00:11:39,440 --> 00:11:42,240
gaining some interest the second it's

284
00:11:42,240 --> 00:11:45,040
easier to demonstrate the vulnerability

285
00:11:45,040 --> 00:11:48,079
uh with visual changes to inputs

286
00:11:48,079 --> 00:11:50,560
and third due to structure of image

287
00:11:50,560 --> 00:11:51,120
files

288
00:11:51,120 --> 00:11:53,600
it's easier to modify them and to attack

289
00:11:53,600 --> 00:11:56,000
an audio or something else

290
00:11:56,000 --> 00:11:59,839
but we're confident that as adoption

291
00:11:59,839 --> 00:12:03,040
of and maturity of other domains grow

292
00:12:03,040 --> 00:12:05,920
they also will become a more lucrative

293
00:12:05,920 --> 00:12:07,120
target

294
00:12:07,120 --> 00:12:10,959
for attackers and data

295
00:12:10,959 --> 00:12:14,639
is a vital part of every ai system

296
00:12:14,639 --> 00:12:17,680
adversarial scenarios around data uh

297
00:12:17,680 --> 00:12:20,480
may vary from infecting data used for a

298
00:12:20,480 --> 00:12:21,360
training

299
00:12:21,360 --> 00:12:24,399
uh to crafting malicious inputs during

300
00:12:24,399 --> 00:12:26,639
inference or attackers can simply steal

301
00:12:26,639 --> 00:12:29,519
private data through data extraction

302
00:12:29,519 --> 00:12:34,959
attacks and images is the most targeted

303
00:12:34,959 --> 00:12:38,000
data type because it's easier to attack

304
00:12:38,000 --> 00:12:39,920
and it's more convincing convincing to

305
00:12:39,920 --> 00:12:41,920
demonstrate vulnerabilities

306
00:12:41,920 --> 00:12:44,399
and this also correlates with their

307
00:12:44,399 --> 00:12:45,279
maturity

308
00:12:45,279 --> 00:12:48,320
of computer vision however

309
00:12:48,320 --> 00:12:50,079
uh none of these reasons mean that the

310
00:12:50,079 --> 00:12:52,800
computer vision is more vulnerable

311
00:12:52,800 --> 00:12:55,279
i would say quite the opposite a big

312
00:12:55,279 --> 00:12:56,480
number of attacks

313
00:12:56,480 --> 00:13:00,720
in research literature motivates

314
00:13:00,720 --> 00:13:02,800
development defenses and those

315
00:13:02,800 --> 00:13:04,959
applications with fewer attacks may be

316
00:13:04,959 --> 00:13:06,560
even more vulnerable because there is no

317
00:13:06,560 --> 00:13:07,680
motivation to

318
00:13:07,680 --> 00:13:11,200
develop defenses against them but after

319
00:13:11,200 --> 00:13:11,680
all

320
00:13:11,680 --> 00:13:14,399
we can see that all existing data types

321
00:13:14,399 --> 00:13:17,760
have been attacked already

322
00:13:19,040 --> 00:13:21,440
so this is also closely correlated to

323
00:13:21,440 --> 00:13:22,800
the applications because

324
00:13:22,800 --> 00:13:26,160
uh data fills those applications

325
00:13:26,160 --> 00:13:29,440
and talking about ti adoption we

326
00:13:29,440 --> 00:13:31,839
don't even notice how much we rely on

327
00:13:31,839 --> 00:13:34,000
algorithms already

328
00:13:34,000 --> 00:13:36,480
let's say with siri or alexa or just

329
00:13:36,480 --> 00:13:37,920
face id

330
00:13:37,920 --> 00:13:41,600
many applications have become natural

331
00:13:41,600 --> 00:13:43,360
part of our lives

332
00:13:43,360 --> 00:13:46,160
and some other use cases like medical

333
00:13:46,160 --> 00:13:48,800
imaging or malware detection

334
00:13:48,800 --> 00:13:51,680
have disrupted their industries and we

335
00:13:51,680 --> 00:13:53,760
are all aware that

336
00:13:53,760 --> 00:13:56,800
popular applications become a target for

337
00:13:56,800 --> 00:13:59,519
cyber criminals quite quickly

338
00:13:59,519 --> 00:14:03,440
and ai is not going to be an exception

339
00:14:03,440 --> 00:14:05,839
and the problem is that we think that

340
00:14:05,839 --> 00:14:07,199
the most applications

341
00:14:07,199 --> 00:14:09,600
are fundamentally vulnerable in all

342
00:14:09,600 --> 00:14:11,839
adversarial cases

343
00:14:11,839 --> 00:14:14,000
unprotected models now exhibited

344
00:14:14,000 --> 00:14:15,519
inherent robustness

345
00:14:15,519 --> 00:14:18,160
and always failed against attacks on ai

346
00:14:18,160 --> 00:14:19,440
algorithms

347
00:14:19,440 --> 00:14:22,560
also from all the cases we can conclude

348
00:14:22,560 --> 00:14:23,680
that

349
00:14:23,680 --> 00:14:25,680
the fundamental reasons why

350
00:14:25,680 --> 00:14:27,040
vulnerabilities exist

351
00:14:27,040 --> 00:14:30,000
are of the same nature so most other

352
00:14:30,000 --> 00:14:32,480
applications that are not tested yet

353
00:14:32,480 --> 00:14:35,440
are also anticipated to be vulnerable

354
00:14:35,440 --> 00:14:36,720
and besides that

355
00:14:36,720 --> 00:14:39,120
there is an attempt to create universal

356
00:14:39,120 --> 00:14:40,880
attack that transfers

357
00:14:40,880 --> 00:14:44,399
across models and applications

358
00:14:44,399 --> 00:14:47,440
so these numbers will only grow and

359
00:14:47,440 --> 00:14:50,880
if we talk about real uh real world

360
00:14:50,880 --> 00:14:55,120
scenarios and industry applicability

361
00:14:55,120 --> 00:14:57,279
we can see that the most purple

362
00:14:57,279 --> 00:14:58,160
applications

363
00:14:58,160 --> 00:15:01,360
are the most scrutinized uh and some

364
00:15:01,360 --> 00:15:02,560
applications such as

365
00:15:02,560 --> 00:15:05,440
uh image classification they span across

366
00:15:05,440 --> 00:15:06,240
industries

367
00:15:06,240 --> 00:15:09,120
and put them these industries at risk

368
00:15:09,120 --> 00:15:11,279
others like facial recognition

369
00:15:11,279 --> 00:15:14,480
or malware detection they undermine

370
00:15:14,480 --> 00:15:16,720
trust in their applications in such

371
00:15:16,720 --> 00:15:18,880
mission critical products

372
00:15:18,880 --> 00:15:21,120
so here we determine industries based on

373
00:15:21,120 --> 00:15:23,199
the purpose of the model and the data

374
00:15:23,199 --> 00:15:23,920
set

375
00:15:23,920 --> 00:15:26,480
for instance facial recognition systems

376
00:15:26,480 --> 00:15:28,720
would go to biometrics

377
00:15:28,720 --> 00:15:31,440
and attacks against uh semantic

378
00:15:31,440 --> 00:15:32,639
segmentation

379
00:15:32,639 --> 00:15:34,880
uh with the data sets from their

380
00:15:34,880 --> 00:15:37,680
self-driving cars dashboard cameras

381
00:15:37,680 --> 00:15:40,320
accounted towards automotive industry so

382
00:15:40,320 --> 00:15:42,720
that's how we got this rating

383
00:15:42,720 --> 00:15:45,279
and it's based purely on the number of

384
00:15:45,279 --> 00:15:46,399
uh

385
00:15:46,399 --> 00:15:49,600
attacks in research literature and

386
00:15:49,600 --> 00:15:51,199
attack research column shows the

387
00:15:51,199 --> 00:15:52,880
interest of researchers to attack such

388
00:15:52,880 --> 00:15:54,320
applications

389
00:15:54,320 --> 00:15:57,680
but we also want to

390
00:15:58,320 --> 00:16:01,680
pay attention to transfer risks because

391
00:16:01,680 --> 00:16:04,720
there are many attacks that are

392
00:16:04,720 --> 00:16:07,279
similar or transferable both across

393
00:16:07,279 --> 00:16:08,079
applications

394
00:16:08,079 --> 00:16:10,880
and across across industries so we've

395
00:16:10,880 --> 00:16:11,680
made this

396
00:16:11,680 --> 00:16:15,199
uh risk correlation and counted

397
00:16:15,199 --> 00:16:17,040
attacks from one industry to another

398
00:16:17,040 --> 00:16:19,199
industry when they're transferable

399
00:16:19,199 --> 00:16:20,800
for instance attacks against facial

400
00:16:20,800 --> 00:16:22,480
recognition system

401
00:16:22,480 --> 00:16:24,720
in biometric systems are transferred to

402
00:16:24,720 --> 00:16:26,079
surveillance systems

403
00:16:26,079 --> 00:16:28,079
so we counted these numbers in both

404
00:16:28,079 --> 00:16:29,920
industries and that's

405
00:16:29,920 --> 00:16:34,639
made our rating for transfer risk

406
00:16:35,600 --> 00:16:38,320
and now let's look at adversarial

407
00:16:38,320 --> 00:16:38,959
attacks

408
00:16:38,959 --> 00:16:42,000
in more details uh first of all

409
00:16:42,000 --> 00:16:44,720
to develop trusted ti equal attention

410
00:16:44,720 --> 00:16:47,120
should be paid to all stages of security

411
00:16:47,120 --> 00:16:48,000
life cycle

412
00:16:48,000 --> 00:16:50,959
of va systems and here you can see how

413
00:16:50,959 --> 00:16:51,360
the

414
00:16:51,360 --> 00:16:53,759
interest of researchers is distributed

415
00:16:53,759 --> 00:16:54,880
across these

416
00:16:54,880 --> 00:16:58,480
domains uh close numbers for attacking

417
00:16:58,480 --> 00:17:00,079
defenses

418
00:17:00,079 --> 00:17:03,040
reflect the common cat and mouse game in

419
00:17:03,040 --> 00:17:04,400
cyber security

420
00:17:04,400 --> 00:17:06,880
and despite differences on the technical

421
00:17:06,880 --> 00:17:08,000
level

422
00:17:08,000 --> 00:17:10,160
of attacking and defending machine

423
00:17:10,160 --> 00:17:12,240
learning algorithms

424
00:17:12,240 --> 00:17:14,160
adversarial machine learning follows the

425
00:17:14,160 --> 00:17:16,240
same cyber security laws so the same

426
00:17:16,240 --> 00:17:17,280
concepts

427
00:17:17,280 --> 00:17:21,039
and ideas can be applied to securing

428
00:17:21,039 --> 00:17:24,720
ai systems however

429
00:17:24,720 --> 00:17:27,039
while we can see the number of attacks

430
00:17:27,039 --> 00:17:27,839
and defenses

431
00:17:27,839 --> 00:17:31,679
is quite close it's worth noting that

432
00:17:31,679 --> 00:17:34,880
there are dozens of effective attacks

433
00:17:34,880 --> 00:17:37,120
and next to nothing efficient defenses

434
00:17:37,120 --> 00:17:40,159
for ai systems

435
00:17:41,280 --> 00:17:44,960
so let's take a look at attacks

436
00:17:44,960 --> 00:17:48,559
at a high level uh in this section

437
00:17:48,559 --> 00:17:51,600
uh we show uh the big categories of

438
00:17:51,600 --> 00:17:53,280
adversarial attacks

439
00:17:53,280 --> 00:17:55,440
uh manipulation attacks allow

440
00:17:55,440 --> 00:17:57,520
adversaries to bypass expected a

441
00:17:57,520 --> 00:17:58,480
behavior

442
00:17:58,480 --> 00:18:01,120
or make a systems perform unexpected

443
00:18:01,120 --> 00:18:02,400
jobs

444
00:18:02,400 --> 00:18:04,559
with maliciously crafted inputs

445
00:18:04,559 --> 00:18:07,440
attackers can conduct evasion attacks

446
00:18:07,440 --> 00:18:09,600
or adversarial reprogramming of va

447
00:18:09,600 --> 00:18:11,919
systems in real time

448
00:18:11,919 --> 00:18:15,120
infection attacks sabotage the quality

449
00:18:15,120 --> 00:18:15,440
of

450
00:18:15,440 --> 00:18:18,000
air decisions and enables self-control

451
00:18:18,000 --> 00:18:19,280
of va systems

452
00:18:19,280 --> 00:18:21,679
and attackers contaminate data used for

453
00:18:21,679 --> 00:18:22,320
training

454
00:18:22,320 --> 00:18:24,720
they can exploit hidden air back doors

455
00:18:24,720 --> 00:18:26,000
or distribute

456
00:18:26,000 --> 00:18:29,440
malicious air trojans on that hand

457
00:18:29,440 --> 00:18:31,200
exfiltration attacks

458
00:18:31,200 --> 00:18:34,240
aim to steal data from ia systems uh

459
00:18:34,240 --> 00:18:37,360
data samples uh used for a training

460
00:18:37,360 --> 00:18:41,280
private a inputs internal algorithms

461
00:18:41,280 --> 00:18:44,559
can be exfiltrated with attacks such as

462
00:18:44,559 --> 00:18:47,440
model inversion membership inference

463
00:18:47,440 --> 00:18:49,679
attribute inference or model extraction

464
00:18:49,679 --> 00:18:53,360
which is also called model stealing

465
00:18:53,360 --> 00:18:56,240
this is our top 10 attacks based on

466
00:18:56,240 --> 00:18:56,799
around

467
00:18:56,799 --> 00:18:59,840
2 000 attack cases

468
00:18:59,840 --> 00:19:02,720
in our research data set what is worth

469
00:19:02,720 --> 00:19:03,280
noting

470
00:19:03,280 --> 00:19:06,400
is that evasion is around 81 percent of

471
00:19:06,400 --> 00:19:07,520
all attacks

472
00:19:07,520 --> 00:19:10,080
and poisoning is around seven percent

473
00:19:10,080 --> 00:19:11,760
and all the rest attacks

474
00:19:11,760 --> 00:19:14,320
uh take from three percent to zero point

475
00:19:14,320 --> 00:19:16,320
three percent

476
00:19:16,320 --> 00:19:19,760
and i want to mention that most attacks

477
00:19:19,760 --> 00:19:21,600
against ta systems

478
00:19:21,600 --> 00:19:23,520
uh remind me traditional application

479
00:19:23,520 --> 00:19:25,200
security attacks

480
00:19:25,200 --> 00:19:28,480
is traceable how attack vectors are

481
00:19:28,480 --> 00:19:30,480
methodically taken from general

482
00:19:30,480 --> 00:19:31,520
cybersecurity

483
00:19:31,520 --> 00:19:33,280
and applied to adversarial machine

484
00:19:33,280 --> 00:19:34,559
learning

485
00:19:34,559 --> 00:19:36,799
and now alex will show you the case

486
00:19:36,799 --> 00:19:39,679
study about how we attacked

487
00:19:39,679 --> 00:19:42,160
facial recognition systems and conducted

488
00:19:42,160 --> 00:19:45,360
comprehensive security evaluation

489
00:19:45,360 --> 00:19:48,400
with the evasion attacks

490
00:19:48,400 --> 00:19:50,799
hi everyone my part will be a little

491
00:19:50,799 --> 00:19:51,600
more

492
00:19:51,600 --> 00:19:53,919
practical so i'm going to show you the

493
00:19:53,919 --> 00:19:55,440
case study

494
00:19:55,440 --> 00:19:57,520
that we perform against a patch

495
00:19:57,520 --> 00:19:59,280
recognition system

496
00:19:59,280 --> 00:20:02,559
and as you are there the batch

497
00:20:02,559 --> 00:20:03,760
recognition systems

498
00:20:03,760 --> 00:20:06,799
are on the second place by the number of

499
00:20:06,799 --> 00:20:12,000
research papers devoted to this topic

500
00:20:12,400 --> 00:20:15,840
and i'll start with the example of

501
00:20:15,840 --> 00:20:20,000
evasion attack which is a

502
00:20:20,000 --> 00:20:22,640
on the first place in the top ten so

503
00:20:22,640 --> 00:20:24,480
invasion is the most common attack

504
00:20:24,480 --> 00:20:27,280
basically uh you can see the

505
00:20:27,280 --> 00:20:28,320
demonstration

506
00:20:28,320 --> 00:20:31,600
in the slide when we have a some kind of

507
00:20:31,600 --> 00:20:34,960
uh image specification uh

508
00:20:34,960 --> 00:20:38,320
engine and and we have a

509
00:20:38,320 --> 00:20:40,960
image of pig and we let's say we want to

510
00:20:40,960 --> 00:20:43,120
change this image to something else

511
00:20:43,120 --> 00:20:46,840
so we may add a certain

512
00:20:46,840 --> 00:20:50,000
pixels uh to this image so that the

513
00:20:50,000 --> 00:20:52,720
ai system will recognize it now as an

514
00:20:52,720 --> 00:20:53,600
airliner

515
00:20:53,600 --> 00:20:57,039
but we as humans uh won't be able to see

516
00:20:57,039 --> 00:21:00,080
any difference so basically uh

517
00:21:00,080 --> 00:21:02,159
this attack can be performed in two

518
00:21:02,159 --> 00:21:04,640
steps first step is to understand what

519
00:21:04,640 --> 00:21:05,840
are the most

520
00:21:05,840 --> 00:21:10,240
important pixels and the next step is to

521
00:21:10,240 --> 00:21:13,679
somehow change those pixels uh

522
00:21:13,679 --> 00:21:17,840
using various types of mathematical

523
00:21:17,840 --> 00:21:22,159
uh algorithms and there are hundreds of

524
00:21:22,159 --> 00:21:25,440
papers describing those algebras

525
00:21:25,440 --> 00:21:28,480
but this is a theory

526
00:21:28,480 --> 00:21:31,360
and of course it works in a digital

527
00:21:31,360 --> 00:21:32,000
world

528
00:21:32,000 --> 00:21:35,440
but what about first of all the physical

529
00:21:35,440 --> 00:21:37,600
world and secondly about the

530
00:21:37,600 --> 00:21:39,520
uh real applications like facial

531
00:21:39,520 --> 00:21:42,640
recognition or not just

532
00:21:42,640 --> 00:21:46,880
image classification so

533
00:21:47,440 --> 00:21:50,720
once we were uh contacted by the company

534
00:21:50,720 --> 00:21:54,080
uh and the uh

535
00:21:54,080 --> 00:21:55,919
the smartphone solution provider and

536
00:21:55,919 --> 00:21:59,200
they um asked us to

537
00:21:59,200 --> 00:22:02,559
perform the tests of their

538
00:22:02,559 --> 00:22:04,640
factory function system and the goal was

539
00:22:04,640 --> 00:22:06,720
to understand what are the best

540
00:22:06,720 --> 00:22:09,760
combinations of software

541
00:22:09,760 --> 00:22:13,840
and hardware

542
00:22:14,640 --> 00:22:16,880
so

543
00:22:17,840 --> 00:22:22,559
what we've done uh the first thing

544
00:22:22,720 --> 00:22:26,240
was to actually the first problem

545
00:22:26,240 --> 00:22:27,840
is that there are four thousand

546
00:22:27,840 --> 00:22:29,440
different research papers about

547
00:22:29,440 --> 00:22:31,440
adversarial attacks

548
00:22:31,440 --> 00:22:34,000
okay if we can if we will take only

549
00:22:34,000 --> 00:22:34,559
those

550
00:22:34,559 --> 00:22:37,280
which are devoted to a patch recognition

551
00:22:37,280 --> 00:22:38,080
we still have

552
00:22:38,080 --> 00:22:41,200
more than 100 research papers

553
00:22:41,200 --> 00:22:45,200
each one of more than 10 pages

554
00:22:45,200 --> 00:22:48,000
but the biggest problem is that each of

555
00:22:48,000 --> 00:22:48,480
those

556
00:22:48,480 --> 00:22:51,120
research papers focus on the different

557
00:22:51,120 --> 00:22:51,919
types of

558
00:22:51,919 --> 00:22:55,200
models different data sets

559
00:22:55,200 --> 00:22:58,720
and you cannot really compare those

560
00:22:58,720 --> 00:23:01,280
results

561
00:23:01,919 --> 00:23:05,200
so you still don't have a clue like

562
00:23:05,200 --> 00:23:08,880
what is the real thread and what is just

563
00:23:08,880 --> 00:23:12,000
an academia uh research

564
00:23:12,000 --> 00:23:16,240
so we perform um how we create our own

565
00:23:16,240 --> 00:23:19,360
framework to perform

566
00:23:19,360 --> 00:23:23,120
adversarial uh testing

567
00:23:23,120 --> 00:23:26,960
uh so first you should under

568
00:23:26,960 --> 00:23:29,760
understand what is your uh the biggest

569
00:23:29,760 --> 00:23:30,240
uh

570
00:23:30,240 --> 00:23:33,440
risk so basically what is the attacking

571
00:23:33,440 --> 00:23:34,080
goal

572
00:23:34,080 --> 00:23:36,559
if attackers just want to misclassify

573
00:23:36,559 --> 00:23:37,840
the uh

574
00:23:37,840 --> 00:23:40,880
image or he wants to

575
00:23:40,880 --> 00:23:43,919
hide from the factual cognition system

576
00:23:43,919 --> 00:23:47,520
or he wants to pretend that he is

577
00:23:47,520 --> 00:23:52,080
a different person then depending on the

578
00:23:52,080 --> 00:23:56,639
conditions uh like

579
00:23:56,960 --> 00:24:00,320
if uh attacker can wear glasses

580
00:24:00,320 --> 00:24:02,640
we may try to perform adversarial attack

581
00:24:02,640 --> 00:24:04,400
on glasses

582
00:24:04,400 --> 00:24:08,400
if he cannot wear glasses we can

583
00:24:08,400 --> 00:24:11,120
wear a mask and print adversarial cycle

584
00:24:11,120 --> 00:24:11,520
mask

585
00:24:11,520 --> 00:24:15,600
because everybody should wear masks

586
00:24:15,600 --> 00:24:18,159
the next step is to understand like uh

587
00:24:18,159 --> 00:24:19,120
the

588
00:24:19,120 --> 00:24:22,240
knowledge of attacker if it's black box

589
00:24:22,240 --> 00:24:22,720
or

590
00:24:22,720 --> 00:24:25,919
file box attack

591
00:24:25,919 --> 00:24:27,200
then we have some kind of time

592
00:24:27,200 --> 00:24:29,120
constraints so we should understand what

593
00:24:29,120 --> 00:24:31,200
kind of attacks

594
00:24:31,200 --> 00:24:34,559
can be performed uh in a

595
00:24:34,559 --> 00:24:37,600
certain period of time

596
00:24:37,600 --> 00:24:41,520
the next is a very important part

597
00:24:41,520 --> 00:24:45,200
it's uh different conditions uh because

598
00:24:45,200 --> 00:24:48,559
if you will print your uh let's say

599
00:24:48,559 --> 00:24:50,480
adversarial uh glasses

600
00:24:50,480 --> 00:24:52,960
the colors on on those glasses won't be

601
00:24:52,960 --> 00:24:53,600
the same

602
00:24:53,600 --> 00:24:56,159
as you expected so your adversarial

603
00:24:56,159 --> 00:24:57,520
attack may not work

604
00:24:57,520 --> 00:25:01,039
so you need to adjust your attack

605
00:25:01,039 --> 00:25:03,360
to various printing conditions and

606
00:25:03,360 --> 00:25:05,039
consistency of colors

607
00:25:05,039 --> 00:25:07,840
and also the position of glasses on your

608
00:25:07,840 --> 00:25:08,799
face

609
00:25:08,799 --> 00:25:12,080
your face position behind the camera the

610
00:25:12,080 --> 00:25:16,640
the angle of the size and so on

611
00:25:16,640 --> 00:25:19,600
and only finally after that uh you may

612
00:25:19,600 --> 00:25:20,159
perform

613
00:25:20,159 --> 00:25:23,360
various uh

614
00:25:23,360 --> 00:25:26,880
algorithms to perform this attack

615
00:25:26,880 --> 00:25:30,400
in certain limitations

616
00:25:30,400 --> 00:25:31,919
and there are hundreds of different

617
00:25:31,919 --> 00:25:34,080
algorithms that you may

618
00:25:34,080 --> 00:25:38,400
perform so of course we successfully

619
00:25:38,400 --> 00:25:41,760
uh performed the physical attack

620
00:25:41,760 --> 00:25:43,520
and we're able to full pressure ignition

621
00:25:43,520 --> 00:25:45,120
system uh

622
00:25:45,120 --> 00:25:48,320
with glasses uh glasses

623
00:25:48,320 --> 00:25:50,960
uh together with bandanas have the

624
00:25:50,960 --> 00:25:51,679
biggest

625
00:25:51,679 --> 00:25:55,120
uh miscensification rate and

626
00:25:55,120 --> 00:25:57,919
uh it's also possible to make other

627
00:25:57,919 --> 00:25:59,520
optimizations

628
00:25:59,520 --> 00:26:02,799
uh to those glasses uh for example to

629
00:26:02,799 --> 00:26:03,520
make them

630
00:26:03,520 --> 00:26:07,120
a little bit uh smaller in size and so

631
00:26:07,120 --> 00:26:08,480
on

632
00:26:08,480 --> 00:26:12,320
but uh this is uh uh

633
00:26:12,320 --> 00:26:15,440
was out of soul but what's one

634
00:26:15,440 --> 00:26:18,799
what's more important here is

635
00:26:18,799 --> 00:26:21,760
that um

636
00:26:21,840 --> 00:26:24,000
if we want to protect our system from

637
00:26:24,000 --> 00:26:25,760
such attack

638
00:26:25,760 --> 00:26:28,960
it's much harder than to protect from

639
00:26:28,960 --> 00:26:33,200
let's say sql injection because okay

640
00:26:33,200 --> 00:26:36,240
the pen test demonstrated that you can

641
00:26:36,240 --> 00:26:36,960
pull

642
00:26:36,960 --> 00:26:39,760
pressure pressure system by that such

643
00:26:39,760 --> 00:26:41,679
that in such glasses

644
00:26:41,679 --> 00:26:44,720
but uh comparing to

645
00:26:44,720 --> 00:26:46,880
a sql injection you cannot just you know

646
00:26:46,880 --> 00:26:48,960
filter it

647
00:26:48,960 --> 00:26:52,960
because there are different

648
00:26:52,960 --> 00:26:56,480
uh types of attacks and once you

649
00:26:56,480 --> 00:26:59,279
uh protect your system from this attack

650
00:26:59,279 --> 00:27:02,000
it may be vulnerable to other attacks so

651
00:27:02,000 --> 00:27:04,880
there are other steps that you need to

652
00:27:04,880 --> 00:27:05,440
check

653
00:27:05,440 --> 00:27:08,559
your model parameters you need to check

654
00:27:08,559 --> 00:27:11,360
this or other attacks in various

655
00:27:11,360 --> 00:27:12,799
physical conditions

656
00:27:12,799 --> 00:27:14,720
and of course you need to check uh

657
00:27:14,720 --> 00:27:16,080
different types of

658
00:27:16,080 --> 00:27:18,960
protections and uh performance that

659
00:27:18,960 --> 00:27:19,520
there are no

660
00:27:19,520 --> 00:27:21,840
one-size-fits-all protections but each

661
00:27:21,840 --> 00:27:23,279
of them has their advantages and

662
00:27:23,279 --> 00:27:24,559
disadvantages

663
00:27:24,559 --> 00:27:26,559
but usually we'll tell you more about

664
00:27:26,559 --> 00:27:27,919
this

665
00:27:27,919 --> 00:27:30,000
all right in this talk we don't cover

666
00:27:30,000 --> 00:27:31,679
specific defense techniques

667
00:27:31,679 --> 00:27:34,159
in details because efficient defenses

668
00:27:34,159 --> 00:27:35,520
for ai systems don't exist

669
00:27:35,520 --> 00:27:38,080
yet and common defenses either fail to

670
00:27:38,080 --> 00:27:39,919
protect from modified attacks

671
00:27:39,919 --> 00:27:42,240
or affect the performance and accuracy

672
00:27:42,240 --> 00:27:44,080
of va systems

673
00:27:44,080 --> 00:27:46,240
and while it's nearly impossible to

674
00:27:46,240 --> 00:27:48,480
create a universal ai defense

675
00:27:48,480 --> 00:27:50,559
it's still feasible to maximize the

676
00:27:50,559 --> 00:27:52,720
difficulty of attacking a single

677
00:27:52,720 --> 00:27:55,039
application against a clearly defined

678
00:27:55,039 --> 00:27:55,840
threat model

679
00:27:55,840 --> 00:27:57,360
in a particular environment with

680
00:27:57,360 --> 00:27:59,360
particular restrictions

681
00:27:59,360 --> 00:28:01,120
so it's more than it's more an

682
00:28:01,120 --> 00:28:03,279
operational issue to

683
00:28:03,279 --> 00:28:06,480
evaluate such defenses and trade-offs

684
00:28:06,480 --> 00:28:09,440
also uh additional efforts are required

685
00:28:09,440 --> 00:28:11,600
in securing supply chain software models

686
00:28:11,600 --> 00:28:12,960
and data sets

687
00:28:12,960 --> 00:28:15,200
and securing production data pipelines

688
00:28:15,200 --> 00:28:17,120
and ai infrastructure

689
00:28:17,120 --> 00:28:18,960
so i want to finish with the suggested

690
00:28:18,960 --> 00:28:20,640
operational steps

691
00:28:20,640 --> 00:28:22,480
and as part of these steps it would be

692
00:28:22,480 --> 00:28:23,840
possible to find

693
00:28:23,840 --> 00:28:27,360
suitable protections so to deal with the

694
00:28:27,360 --> 00:28:28,159
new threats

695
00:28:28,159 --> 00:28:30,480
covered in this talk we suggest the

696
00:28:30,480 --> 00:28:32,559
following secure air life cycle to jump

697
00:28:32,559 --> 00:28:33,039
start

698
00:28:33,039 --> 00:28:36,799
a ai security program this life cycle

699
00:28:36,799 --> 00:28:39,039
is based on nice cyber security

700
00:28:39,039 --> 00:28:40,080
framework

701
00:28:40,080 --> 00:28:42,840
and also gartner's adaptive security

702
00:28:42,840 --> 00:28:44,640
architecture these are two

703
00:28:44,640 --> 00:28:46,880
popular reference frameworks for cyber

704
00:28:46,880 --> 00:28:49,360
security life cycle management

705
00:28:49,360 --> 00:28:51,840
so you can use it too so if you are

706
00:28:51,840 --> 00:28:53,039
interested in

707
00:28:53,039 --> 00:28:54,960
getting more details about such life

708
00:28:54,960 --> 00:28:57,520
cycle with the documented steps

709
00:28:57,520 --> 00:29:00,799
feel free to download our report

710
00:29:00,799 --> 00:29:03,760
and if this is your area of concern what

711
00:29:03,760 --> 00:29:04,320
you do

712
00:29:04,320 --> 00:29:07,120
what should you do next first of all you

713
00:29:07,120 --> 00:29:08,720
should raise awareness of all the

714
00:29:08,720 --> 00:29:10,559
stakeholders

715
00:29:10,559 --> 00:29:14,080
involved in uh creation of va systems

716
00:29:14,080 --> 00:29:17,120
uh second uh you should conduct a

717
00:29:17,120 --> 00:29:18,399
security assessment

718
00:29:18,399 --> 00:29:21,360
of your ai system based on your unique

719
00:29:21,360 --> 00:29:22,960
threat model

720
00:29:22,960 --> 00:29:25,600
and finally after you respond to those

721
00:29:25,600 --> 00:29:27,200
security findings

722
00:29:27,200 --> 00:29:29,520
you should implement regular ai security

723
00:29:29,520 --> 00:29:31,039
checks as part of

724
00:29:31,039 --> 00:29:34,320
ai development life cycle so this is

725
00:29:34,320 --> 00:29:37,919
it thank you for your attention

726
00:29:37,919 --> 00:29:39,600
if you're interested in this topic feel

727
00:29:39,600 --> 00:29:41,279
free to reach out whether you're

728
00:29:41,279 --> 00:29:44,320
independent researcher or organization

729
00:29:44,320 --> 00:29:46,240
and if you want to learn more download

730
00:29:46,240 --> 00:29:48,720
the original report and subscribe to our

731
00:29:48,720 --> 00:29:50,000
open newsletter

732
00:29:50,000 --> 00:29:52,720
while we cover a vulnerabilities and

733
00:29:52,720 --> 00:29:53,360
incidents

734
00:29:53,360 --> 00:30:03,439
in a security thank you

