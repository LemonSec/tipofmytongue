1
00:00:05,440 --> 00:00:08,080
hi everyone welcome to this simcon

2
00:00:08,080 --> 00:00:10,400
presentation where i will be talking

3
00:00:10,400 --> 00:00:12,960
about side channel attacks at the age of

4
00:00:12,960 --> 00:00:14,400
deep learning

5
00:00:14,400 --> 00:00:16,640
so my goal during this talk is to give

6
00:00:16,640 --> 00:00:19,520
an overview of how deep learning is

7
00:00:19,520 --> 00:00:22,240
really changing the way we perform side

8
00:00:22,240 --> 00:00:24,400
channel analysis of cryptographic

9
00:00:24,400 --> 00:00:26,400
implementations

10
00:00:26,400 --> 00:00:28,960
so before we get started just a few

11
00:00:28,960 --> 00:00:32,000
words about myself and about ishard so

12
00:00:32,000 --> 00:00:34,640
my name is benjamin i work as a security

13
00:00:34,640 --> 00:00:36,559
engineer at ishard

14
00:00:36,559 --> 00:00:39,600
ishard's mission is to help companies

15
00:00:39,600 --> 00:00:42,399
mastering the security risk in embedded

16
00:00:42,399 --> 00:00:43,600
devices

17
00:00:43,600 --> 00:00:45,840
and for that we provide different types

18
00:00:45,840 --> 00:00:47,360
of services

19
00:00:47,360 --> 00:00:49,760
we provide penetration testing services

20
00:00:49,760 --> 00:00:52,800
but we also provide training and we

21
00:00:52,800 --> 00:00:55,280
develop security tools

22
00:00:55,280 --> 00:00:58,320
so our team covers different layers in

23
00:00:58,320 --> 00:01:01,520
security we have people working more on

24
00:01:01,520 --> 00:01:05,119
cryptography and on the hardware side

25
00:01:05,119 --> 00:01:07,680
and for that we have a flagship software

26
00:01:07,680 --> 00:01:09,520
called estanamic

27
00:01:09,520 --> 00:01:12,000
which is a data science platform to

28
00:01:12,000 --> 00:01:13,040
perform

29
00:01:13,040 --> 00:01:14,640
side channel attacks and fault

30
00:01:14,640 --> 00:01:16,880
injections on cryptographic

31
00:01:16,880 --> 00:01:18,479
implementations

32
00:01:18,479 --> 00:01:20,799
so as you can see this is a tool that i

33
00:01:20,799 --> 00:01:23,200
will use today for this talk on deep

34
00:01:23,200 --> 00:01:26,560
learning applied to side channel and we

35
00:01:26,560 --> 00:01:28,720
also have people working more on reverse

36
00:01:28,720 --> 00:01:30,880
engineering and on the security of

37
00:01:30,880 --> 00:01:32,560
mobile applications

38
00:01:32,560 --> 00:01:34,799
and for that part we have a new tool

39
00:01:34,799 --> 00:01:37,520
called es checker that we are

40
00:01:37,520 --> 00:01:39,280
introducing during

41
00:01:39,280 --> 00:01:41,759
the simcon conference

42
00:01:41,759 --> 00:01:44,159
so for people who might be interested in

43
00:01:44,159 --> 00:01:46,479
that i will just say a few words about

44
00:01:46,479 --> 00:01:47,920
this new tool

45
00:01:47,920 --> 00:01:50,399
so especially is a tool

46
00:01:50,399 --> 00:01:52,479
to help businesses

47
00:01:52,479 --> 00:01:55,439
safeguard their mobile applications

48
00:01:55,439 --> 00:01:58,000
by automatically validating that the

49
00:01:58,000 --> 00:02:00,560
security protections of the app are

50
00:02:00,560 --> 00:02:01,840
functional

51
00:02:01,840 --> 00:02:04,320
so the thing is without the right level

52
00:02:04,320 --> 00:02:06,640
of protections mobile applications they

53
00:02:06,640 --> 00:02:10,160
are vulnerable and can be targeted

54
00:02:10,160 --> 00:02:12,720
by cyber criminals

55
00:02:12,720 --> 00:02:15,200
and so this can be prevented by ensuring

56
00:02:15,200 --> 00:02:18,160
that the app protection shield is always

57
00:02:18,160 --> 00:02:19,520
functional

58
00:02:19,520 --> 00:02:20,720
and so

59
00:02:20,720 --> 00:02:22,800
yes checker provide with a

60
00:02:22,800 --> 00:02:25,200
series of tests

61
00:02:25,200 --> 00:02:27,840
and it provides a comprehensive range of

62
00:02:27,840 --> 00:02:30,959
static and dynamic tests which are based

63
00:02:30,959 --> 00:02:33,680
on oasp recommendations

64
00:02:33,680 --> 00:02:36,480
all these tests can be automatically

65
00:02:36,480 --> 00:02:39,599
run within your ci cd pipeline

66
00:02:39,599 --> 00:02:42,800
and so esteka provides an efficient and

67
00:02:42,800 --> 00:02:46,239
cost effective solution

68
00:02:46,239 --> 00:02:49,599
for people to run comprehensive security

69
00:02:49,599 --> 00:02:53,120
audits on their app and get protection

70
00:02:53,120 --> 00:02:55,920
recommendations

71
00:02:56,400 --> 00:02:58,879
okay so now let's dive into today's

72
00:02:58,879 --> 00:03:01,360
topic so if we look at cryptography

73
00:03:01,360 --> 00:03:02,400
today

74
00:03:02,400 --> 00:03:04,159
we have algorithms that are

75
00:03:04,159 --> 00:03:06,400
theoretically unbreakable

76
00:03:06,400 --> 00:03:08,560
if we take a plain text and we encrypt

77
00:03:08,560 --> 00:03:11,840
it with a cipher like aes for example

78
00:03:11,840 --> 00:03:13,840
even if the ciphertext is

79
00:03:13,840 --> 00:03:16,000
intercepted we know that with just the

80
00:03:16,000 --> 00:03:18,239
knowledge of this cipher text

81
00:03:18,239 --> 00:03:20,959
it is impossible to recover the original

82
00:03:20,959 --> 00:03:25,040
message because the encryption is secure

83
00:03:25,040 --> 00:03:27,120
but in practice what happened is that

84
00:03:27,120 --> 00:03:29,760
this cryptographic algorithm they are

85
00:03:29,760 --> 00:03:33,040
executed on devices and these devices

86
00:03:33,040 --> 00:03:35,920
leak information through multiple

87
00:03:35,920 --> 00:03:38,720
channels that we call side channels

88
00:03:38,720 --> 00:03:41,680
and this can be exploited to recover

89
00:03:41,680 --> 00:03:43,680
secret keys

90
00:03:43,680 --> 00:03:45,599
common channels that we usually can

91
00:03:45,599 --> 00:03:47,840
exploit are for example the time

92
00:03:47,840 --> 00:03:49,920
execution of the algorithm

93
00:03:49,920 --> 00:03:52,319
the power consumption of the device or

94
00:03:52,319 --> 00:03:55,040
also the electromagnetic emanations of

95
00:03:55,040 --> 00:03:56,879
the chip

96
00:03:56,879 --> 00:03:58,400
so the problem is not really on the

97
00:03:58,400 --> 00:04:01,200
algorithm itself which is secure

98
00:04:01,200 --> 00:04:04,080
the issue is on the endpoint device that

99
00:04:04,080 --> 00:04:07,360
is executing the algorithm and which

100
00:04:07,360 --> 00:04:09,439
leaks information

101
00:04:09,439 --> 00:04:12,239
and because devices leak side channel

102
00:04:12,239 --> 00:04:14,959
information there is always a risk

103
00:04:14,959 --> 00:04:16,959
especially in the context where the

104
00:04:16,959 --> 00:04:20,639
device can be accessed by the attacker

105
00:04:20,639 --> 00:04:22,960
so inside channel we don't try to defeat

106
00:04:22,960 --> 00:04:25,759
the security of the algorithm itself

107
00:04:25,759 --> 00:04:28,720
what we do is to attack from the side

108
00:04:28,720 --> 00:04:32,000
and we target the implementation of the

109
00:04:32,000 --> 00:04:33,759
cryptographic algorithm which is

110
00:04:33,759 --> 00:04:35,040
executed

111
00:04:35,040 --> 00:04:36,400
on the device

112
00:04:36,400 --> 00:04:38,960
and we exploit the leakages of this

113
00:04:38,960 --> 00:04:43,680
device to recover the secret key

114
00:04:43,680 --> 00:04:45,600
so in this presentation today i won't

115
00:04:45,600 --> 00:04:48,320
have time to describe precisely how side

116
00:04:48,320 --> 00:04:49,919
channel attacks work

117
00:04:49,919 --> 00:04:52,720
but i will try to make a quick summary

118
00:04:52,720 --> 00:04:54,960
so the idea behind side channel attacks

119
00:04:54,960 --> 00:04:58,240
is to exploit a relationship between the

120
00:04:58,240 --> 00:05:01,280
data manipulated by the algorithm and

121
00:05:01,280 --> 00:05:03,120
the side channel signal that we

122
00:05:03,120 --> 00:05:04,880
collected from the chip

123
00:05:04,880 --> 00:05:07,199
and so for that we usually collect a lot

124
00:05:07,199 --> 00:05:08,720
of traces

125
00:05:08,720 --> 00:05:11,600
corresponding to several execution of

126
00:05:11,600 --> 00:05:14,560
the algorithm with inputs that are

127
00:05:14,560 --> 00:05:17,680
different from one execution to another

128
00:05:17,680 --> 00:05:20,080
and after that we use a statistical

129
00:05:20,080 --> 00:05:21,440
distinguisher

130
00:05:21,440 --> 00:05:25,199
to detect and exploit the relationship

131
00:05:25,199 --> 00:05:28,160
between the data and the signal

132
00:05:28,160 --> 00:05:30,000
so the first attack you will usually

133
00:05:30,000 --> 00:05:32,080
find in the in textbooks is called the

134
00:05:32,080 --> 00:05:35,840
correlation power analysis or cp in

135
00:05:35,840 --> 00:05:39,759
short and this attack use a correlation

136
00:05:39,759 --> 00:05:41,759
as a distinguisher

137
00:05:41,759 --> 00:05:43,039
the idea

138
00:05:43,039 --> 00:05:46,320
behind cpa is to guess the value of the

139
00:05:46,320 --> 00:05:47,360
key

140
00:05:47,360 --> 00:05:50,000
to make some predictions about

141
00:05:50,000 --> 00:05:52,560
intermediate values that are manipulated

142
00:05:52,560 --> 00:05:54,639
by the by the algorithm

143
00:05:54,639 --> 00:05:55,440
so

144
00:05:55,440 --> 00:05:57,759
after that we use correlation

145
00:05:57,759 --> 00:06:00,400
to measure whether or not

146
00:06:00,400 --> 00:06:03,600
the predictions that we made are correct

147
00:06:03,600 --> 00:06:06,639
and we simply select the key guess

148
00:06:06,639 --> 00:06:10,000
that leads to the highest correlation

149
00:06:10,000 --> 00:06:13,120
so here we can run an example in

150
00:06:13,120 --> 00:06:16,639
of cpa in python we have a data set here

151
00:06:16,639 --> 00:06:19,600
composed of multiple traces

152
00:06:19,600 --> 00:06:22,560
corresponding to aes encryptions

153
00:06:22,560 --> 00:06:26,160
and below we defined an attack object

154
00:06:26,160 --> 00:06:29,600
where as you can see we use the cpa

155
00:06:29,600 --> 00:06:32,000
distinguisher to exploit

156
00:06:32,000 --> 00:06:33,440
the leakages

157
00:06:33,440 --> 00:06:37,280
and so if we run this attack

158
00:06:37,280 --> 00:06:38,880
as a result

159
00:06:38,880 --> 00:06:41,600
we can see that for each

160
00:06:41,600 --> 00:06:43,440
byte of the key

161
00:06:43,440 --> 00:06:46,720
there is a specific value which leads to

162
00:06:46,720 --> 00:06:48,479
a high correlation

163
00:06:48,479 --> 00:06:50,160
and this is a value actually that we

164
00:06:50,160 --> 00:06:52,000
will select

165
00:06:52,000 --> 00:06:52,960
for

166
00:06:52,960 --> 00:06:56,400
the key bytes that we want to recover

167
00:06:56,400 --> 00:06:58,400
so here it's a very straightforward

168
00:06:58,400 --> 00:07:01,120
attack we are collecting data and we are

169
00:07:01,120 --> 00:07:03,520
using a statistical distinguisher like

170
00:07:03,520 --> 00:07:06,000
the example the pearson correlation

171
00:07:06,000 --> 00:07:08,880
to extract information about the secret

172
00:07:08,880 --> 00:07:11,120
key

173
00:07:11,199 --> 00:07:13,759
so to build more complex and powerful

174
00:07:13,759 --> 00:07:16,479
attacks we can leverage techniques from

175
00:07:16,479 --> 00:07:18,800
machine learning and in this case we

176
00:07:18,800 --> 00:07:21,360
have a methodology in two steps

177
00:07:21,360 --> 00:07:24,080
first there is a learning phase and

178
00:07:24,080 --> 00:07:26,319
during this phase we will use

179
00:07:26,319 --> 00:07:29,039
a machine learning algorithm to build

180
00:07:29,039 --> 00:07:31,840
a model that will characterize the

181
00:07:31,840 --> 00:07:33,440
leakage of the chip

182
00:07:33,440 --> 00:07:35,840
for all the possible values that a

183
00:07:35,840 --> 00:07:38,479
secret target can can take

184
00:07:38,479 --> 00:07:40,960
and then there is the attack phase where

185
00:07:40,960 --> 00:07:43,759
we use the machine learning model that

186
00:07:43,759 --> 00:07:44,879
we built

187
00:07:44,879 --> 00:07:47,360
to infer some information about the

188
00:07:47,360 --> 00:07:48,800
secret value

189
00:07:48,800 --> 00:07:51,440
corresponding to a trace that we want to

190
00:07:51,440 --> 00:07:52,560
attack

191
00:07:52,560 --> 00:07:54,400
so during the learning phase what we

192
00:07:54,400 --> 00:07:57,360
want is to build a classifier that is

193
00:07:57,360 --> 00:07:58,479
able to

194
00:07:58,479 --> 00:08:01,919
classify traces based on a secret value

195
00:08:01,919 --> 00:08:04,560
and during the attack phase we want to

196
00:08:04,560 --> 00:08:07,759
use this classifier to recover the

197
00:08:07,759 --> 00:08:09,360
secret information

198
00:08:09,360 --> 00:08:12,319
from a trace that we want to attack so

199
00:08:12,319 --> 00:08:16,319
for that once the classifier is trained

200
00:08:16,319 --> 00:08:18,960
we can give a trace as input to the

201
00:08:18,960 --> 00:08:19,919
model

202
00:08:19,919 --> 00:08:22,080
and the model will tell us to what

203
00:08:22,080 --> 00:08:24,840
secret value this trace

204
00:08:24,840 --> 00:08:27,599
corresponds so for this kind of machine

205
00:08:27,599 --> 00:08:30,000
learning attacks the methodology is

206
00:08:30,000 --> 00:08:32,559
always the same and the only parameter

207
00:08:32,559 --> 00:08:34,320
that changes

208
00:08:34,320 --> 00:08:36,640
from one attack to another

209
00:08:36,640 --> 00:08:40,080
is what machine learning algorithm do we

210
00:08:40,080 --> 00:08:43,039
use to build the classifier

211
00:08:43,039 --> 00:08:43,919
so

212
00:08:43,919 --> 00:08:46,080
in the state of the art you you can find

213
00:08:46,080 --> 00:08:47,839
several techniques

214
00:08:47,839 --> 00:08:50,080
with different types of machine learning

215
00:08:50,080 --> 00:08:52,080
algorithm and different types of

216
00:08:52,080 --> 00:08:54,000
classifier

217
00:08:54,000 --> 00:08:55,600
historically

218
00:08:55,600 --> 00:08:58,000
one of the most famous technique

219
00:08:58,000 --> 00:09:01,200
used in side channel is the so-called

220
00:09:01,200 --> 00:09:04,800
template attack where leakages are

221
00:09:04,800 --> 00:09:07,120
characterized using

222
00:09:07,120 --> 00:09:10,160
multivariate gaussian distributions

223
00:09:10,160 --> 00:09:12,720
so it's it's a very simple type of of

224
00:09:12,720 --> 00:09:16,000
model but in many cases it's efficient

225
00:09:16,000 --> 00:09:16,880
because

226
00:09:16,880 --> 00:09:20,320
um side channel leakages usually follow

227
00:09:20,320 --> 00:09:23,440
gaussian distributions but because the

228
00:09:23,440 --> 00:09:26,399
model is very simple there is also some

229
00:09:26,399 --> 00:09:27,839
limitations

230
00:09:27,839 --> 00:09:30,640
uh for example there is some there are

231
00:09:30,640 --> 00:09:33,120
some types of leakages that cannot

232
00:09:33,120 --> 00:09:35,760
properly be characterized by by this

233
00:09:35,760 --> 00:09:39,040
kind of models and also sometimes it

234
00:09:39,040 --> 00:09:41,920
lacks it lacks some flexibility

235
00:09:41,920 --> 00:09:44,399
and so later on in the presentation we

236
00:09:44,399 --> 00:09:45,839
will see how

237
00:09:45,839 --> 00:09:47,920
deep learning actually provides some

238
00:09:47,920 --> 00:09:49,440
improvements

239
00:09:49,440 --> 00:09:54,160
compared to this previous attack methods

240
00:09:54,160 --> 00:09:56,880
so now if we look at the evolution of

241
00:09:56,880 --> 00:09:59,440
machine learning algorithms

242
00:09:59,440 --> 00:10:03,440
since 2014 we can observe a clear trend

243
00:10:03,440 --> 00:10:06,399
where deep learning has progressively

244
00:10:06,399 --> 00:10:07,519
become

245
00:10:07,519 --> 00:10:10,079
dominant for many applications in

246
00:10:10,079 --> 00:10:12,079
machine learning especially for

247
00:10:12,079 --> 00:10:14,000
everything which is related to data

248
00:10:14,000 --> 00:10:15,680
classification

249
00:10:15,680 --> 00:10:19,519
today deep learning usually outperforms

250
00:10:19,519 --> 00:10:21,040
other techniques

251
00:10:21,040 --> 00:10:22,640
and what is interesting with deep

252
00:10:22,640 --> 00:10:24,800
learning and neural networks is that

253
00:10:24,800 --> 00:10:26,399
these techniques

254
00:10:26,399 --> 00:10:28,399
they have been actually around for for

255
00:10:28,399 --> 00:10:31,440
more than 30 years but the whole thing

256
00:10:31,440 --> 00:10:34,240
started to take off only in the second

257
00:10:34,240 --> 00:10:37,279
half of the 2010s decade

258
00:10:37,279 --> 00:10:38,079
and

259
00:10:38,079 --> 00:10:40,240
this comeback actually can be explained

260
00:10:40,240 --> 00:10:42,880
by by multiple factors but

261
00:10:42,880 --> 00:10:45,519
the two main factors that we we usually

262
00:10:45,519 --> 00:10:47,680
see usually are listed

263
00:10:47,680 --> 00:10:51,360
um to explain this this comeback

264
00:10:51,360 --> 00:10:54,000
are the following so first

265
00:10:54,000 --> 00:10:56,720
there is the increase of computational

266
00:10:56,720 --> 00:10:57,839
power

267
00:10:57,839 --> 00:11:01,040
especially the amount of operations that

268
00:11:01,040 --> 00:11:04,800
can be performed on recent gpus and

269
00:11:04,800 --> 00:11:08,160
other dedicated hardware and this makes

270
00:11:08,160 --> 00:11:10,560
possible to train bigger and bigger

271
00:11:10,560 --> 00:11:12,240
network

272
00:11:12,240 --> 00:11:15,040
and as a result this networks can

273
00:11:15,040 --> 00:11:17,120
achieve better and better

274
00:11:17,120 --> 00:11:18,720
accuracy

275
00:11:18,720 --> 00:11:21,680
and the second point is a faster and

276
00:11:21,680 --> 00:11:23,760
faster access to

277
00:11:23,760 --> 00:11:26,399
bigger and bigger data sets that we can

278
00:11:26,399 --> 00:11:31,279
use to train also this big and large

279
00:11:31,279 --> 00:11:33,519
neural networks

280
00:11:33,519 --> 00:11:36,399
so these two factors and other factors

281
00:11:36,399 --> 00:11:38,480
can explain why

282
00:11:38,480 --> 00:11:39,760
suddenly

283
00:11:39,760 --> 00:11:42,720
deep learning became dominant so there

284
00:11:42,720 --> 00:11:45,040
is a clear trend in machine learning and

285
00:11:45,040 --> 00:11:47,040
following following this trend in

286
00:11:47,040 --> 00:11:48,640
machine learning

287
00:11:48,640 --> 00:11:51,279
the site channel community also started

288
00:11:51,279 --> 00:11:53,519
to investigate

289
00:11:53,519 --> 00:11:55,680
the potential applications of deep

290
00:11:55,680 --> 00:11:58,880
learning for site channel analysis

291
00:11:58,880 --> 00:11:59,600
so

292
00:11:59,600 --> 00:12:00,720
there was

293
00:12:00,720 --> 00:12:03,360
the first publications about that around

294
00:12:03,360 --> 00:12:04,959
2016

295
00:12:04,959 --> 00:12:08,560
and the first publication demonstrated

296
00:12:08,560 --> 00:12:09,680
a clear

297
00:12:09,680 --> 00:12:11,600
interest of using

298
00:12:11,600 --> 00:12:14,639
deep learning and neural networks

299
00:12:14,639 --> 00:12:17,760
the experiments in this first paper

300
00:12:17,760 --> 00:12:20,560
papers show that deep learning usually

301
00:12:20,560 --> 00:12:22,240
outperform

302
00:12:22,240 --> 00:12:25,360
previous attack techniques and so since

303
00:12:25,360 --> 00:12:26,639
then uh

304
00:12:26,639 --> 00:12:29,200
we are we have seen many many articles

305
00:12:29,200 --> 00:12:32,000
presented every year in in security

306
00:12:32,000 --> 00:12:33,279
conferences

307
00:12:33,279 --> 00:12:36,079
and it's definitely one of the most

308
00:12:36,079 --> 00:12:41,199
active topic today in side channel

309
00:12:41,279 --> 00:12:43,760
okay so now let's take a look at how

310
00:12:43,760 --> 00:12:46,480
deep learning can be applied for side

311
00:12:46,480 --> 00:12:49,360
channel analysis so in my notebook today

312
00:12:49,360 --> 00:12:52,160
i will showcase an example where we will

313
00:12:52,160 --> 00:12:54,959
build a neural network that is able to

314
00:12:54,959 --> 00:12:56,880
classify traces

315
00:12:56,880 --> 00:12:58,880
collected from a device

316
00:12:58,880 --> 00:13:01,040
based on a sensitive value which is

317
00:13:01,040 --> 00:13:04,639
computed during the aes encryption

318
00:13:04,639 --> 00:13:06,720
so in this example today we will target

319
00:13:06,720 --> 00:13:09,760
a specific value computing during the

320
00:13:09,760 --> 00:13:12,399
algorithm which is the output of the

321
00:13:12,399 --> 00:13:14,560
xbox operation

322
00:13:14,560 --> 00:13:17,600
at the first round of the aes

323
00:13:17,600 --> 00:13:19,839
it is a sensitive operation because this

324
00:13:19,839 --> 00:13:21,760
operation involves

325
00:13:21,760 --> 00:13:24,880
the master key of the encryption and so

326
00:13:24,880 --> 00:13:27,600
we want to build a neural network that

327
00:13:27,600 --> 00:13:30,079
is able to classify traces

328
00:13:30,079 --> 00:13:33,040
based on the value of this sensitive

329
00:13:33,040 --> 00:13:36,079
target so i don't go in the details of

330
00:13:36,079 --> 00:13:38,880
the cryptanalysis part but the idea is

331
00:13:38,880 --> 00:13:41,519
that if we have a neural network which

332
00:13:41,519 --> 00:13:43,440
is able to

333
00:13:43,440 --> 00:13:44,800
predict

334
00:13:44,800 --> 00:13:47,920
this sensitive value for several traces

335
00:13:47,920 --> 00:13:50,720
then this knowledge can be exploited

336
00:13:50,720 --> 00:13:53,360
with differential attacks

337
00:13:53,360 --> 00:13:55,839
to recover the secret key

338
00:13:55,839 --> 00:13:58,800
so here we will target only one bit of

339
00:13:58,800 --> 00:14:01,600
the xbox output and we want the neural

340
00:14:01,600 --> 00:14:04,399
network to classify the traces

341
00:14:04,399 --> 00:14:06,880
based on the value of this

342
00:14:06,880 --> 00:14:08,560
targeted bit

343
00:14:08,560 --> 00:14:11,440
so here below we have a data set which

344
00:14:11,440 --> 00:14:12,240
is

345
00:14:12,240 --> 00:14:15,440
composed of 10 000 traces corresponding

346
00:14:15,440 --> 00:14:18,639
to 10 000 encryptions with different

347
00:14:18,639 --> 00:14:19,680
inputs

348
00:14:19,680 --> 00:14:23,360
and here this is what we call a label

349
00:14:23,360 --> 00:14:26,480
data set meaning that for each trace of

350
00:14:26,480 --> 00:14:27,760
the data set

351
00:14:27,760 --> 00:14:31,440
we know the inputs of the encryption so

352
00:14:31,440 --> 00:14:33,760
meaning the plain text and the key

353
00:14:33,760 --> 00:14:36,480
this means that we can assign a label

354
00:14:36,480 --> 00:14:38,399
for each trace

355
00:14:38,399 --> 00:14:40,480
and here the label will correspond to

356
00:14:40,480 --> 00:14:44,560
the value of the bit that we target okay

357
00:14:44,560 --> 00:14:48,320
and below here for our neural network

358
00:14:48,320 --> 00:14:51,600
we define a multi-layer perceptron which

359
00:14:51,600 --> 00:14:55,279
is a simple type of neural network with

360
00:14:55,279 --> 00:14:57,360
linear layers which are connected

361
00:14:57,360 --> 00:14:58,480
together

362
00:14:58,480 --> 00:14:59,680
and between

363
00:14:59,680 --> 00:15:01,680
linear layers there is activation

364
00:15:01,680 --> 00:15:04,480
functions that provide

365
00:15:04,480 --> 00:15:06,959
non-linearity in the model

366
00:15:06,959 --> 00:15:08,959
so here the mlp

367
00:15:08,959 --> 00:15:12,800
is composed of four layers of 50 nodes

368
00:15:12,800 --> 00:15:13,760
each

369
00:15:13,760 --> 00:15:16,639
the input of the network is composed of

370
00:15:16,639 --> 00:15:19,360
400 nodes which correspond to

371
00:15:19,360 --> 00:15:22,800
the 400 time sample that we have in the

372
00:15:22,800 --> 00:15:24,480
input traces

373
00:15:24,480 --> 00:15:27,199
and the output of the network

374
00:15:27,199 --> 00:15:29,199
is composed of two nodes

375
00:15:29,199 --> 00:15:31,680
and uh this is because in this case we

376
00:15:31,680 --> 00:15:34,639
want to predict the value of a single

377
00:15:34,639 --> 00:15:38,079
bit of the xbox output so we have two

378
00:15:38,079 --> 00:15:41,839
output nodes one output for the targeted

379
00:15:41,839 --> 00:15:45,440
bit being equal to zero and one for

380
00:15:45,440 --> 00:15:46,480
the bit

381
00:15:46,480 --> 00:15:48,639
being equal to one

382
00:15:48,639 --> 00:15:50,800
so in practice it's also possible to

383
00:15:50,800 --> 00:15:53,360
predict more bits if we want for example

384
00:15:53,360 --> 00:15:56,560
we we might want to predict a full byte

385
00:15:56,560 --> 00:15:59,279
so 8 bits and in this case we will have

386
00:15:59,279 --> 00:16:02,560
just more outputs we will have 256

387
00:16:02,560 --> 00:16:04,560
outputs in the network

388
00:16:04,560 --> 00:16:06,639
one output for each

389
00:16:06,639 --> 00:16:10,160
possible value of of the targeted byte

390
00:16:10,160 --> 00:16:12,399
but so here today we keep

391
00:16:12,399 --> 00:16:13,279
things

392
00:16:13,279 --> 00:16:15,920
a bit simple we will target just a bit

393
00:16:15,920 --> 00:16:18,639
of the value and so we have two output

394
00:16:18,639 --> 00:16:20,800
notes

395
00:16:20,800 --> 00:16:23,279
so we have defined our network now we

396
00:16:23,279 --> 00:16:25,120
can train it

397
00:16:25,120 --> 00:16:27,519
as we have a label data set what we can

398
00:16:27,519 --> 00:16:30,160
do is just apply a supervised learning

399
00:16:30,160 --> 00:16:31,360
method

400
00:16:31,360 --> 00:16:33,759
we feed the network with the traces and

401
00:16:33,759 --> 00:16:36,000
for each trace we indicate to the

402
00:16:36,000 --> 00:16:39,440
network what is corresponding bit value

403
00:16:39,440 --> 00:16:41,279
so that the network can learn to

404
00:16:41,279 --> 00:16:43,199
classify the traces

405
00:16:43,199 --> 00:16:46,560
based on the targeted bit

406
00:16:46,560 --> 00:16:48,720
here you can see in live the evolution

407
00:16:48,720 --> 00:16:51,920
of the training with the accuracy and

408
00:16:51,920 --> 00:16:53,839
loss matrix

409
00:16:53,839 --> 00:16:56,560
the accuracy on the left plot

410
00:16:56,560 --> 00:16:59,519
measures the proportion of traces that

411
00:16:59,519 --> 00:17:02,720
the network is able to properly classify

412
00:17:02,720 --> 00:17:04,959
and so we can see that over time the

413
00:17:04,959 --> 00:17:07,679
accuracy increases

414
00:17:07,679 --> 00:17:10,400
the accuracy starts at around 50 percent

415
00:17:10,400 --> 00:17:12,959
when the network is not trained

416
00:17:12,959 --> 00:17:15,439
and this is just a probability uh to

417
00:17:15,439 --> 00:17:18,880
successfully classify inputs with two

418
00:17:18,880 --> 00:17:20,799
levels at random

419
00:17:20,799 --> 00:17:22,480
but over time we can see that the

420
00:17:22,480 --> 00:17:24,720
accuracy increases

421
00:17:24,720 --> 00:17:27,280
which means the network is learning and

422
00:17:27,280 --> 00:17:29,840
at the end we can see that the training

423
00:17:29,840 --> 00:17:31,440
at the end of the training the accuracy

424
00:17:31,440 --> 00:17:35,520
which is a level of around 65 percent

425
00:17:35,520 --> 00:17:37,440
meaning that the network is able to

426
00:17:37,440 --> 00:17:39,200
properly classify

427
00:17:39,200 --> 00:17:41,760
uh 65 percent of the traces which is

428
00:17:41,760 --> 00:17:44,720
already already better than a random

429
00:17:44,720 --> 00:17:46,320
guess

430
00:17:46,320 --> 00:17:49,039
and so once the network is trained using

431
00:17:49,039 --> 00:17:50,720
a label

432
00:17:50,720 --> 00:17:53,039
data set we can use the network to

433
00:17:53,039 --> 00:17:54,160
predict

434
00:17:54,160 --> 00:17:55,679
the value

435
00:17:55,679 --> 00:17:58,480
of the secret information from traces

436
00:17:58,480 --> 00:18:00,400
that we want to attack

437
00:18:00,400 --> 00:18:02,880
for that it's simple we just pass

438
00:18:02,880 --> 00:18:06,080
as input to the neural network a trace

439
00:18:06,080 --> 00:18:07,919
that we want to

440
00:18:07,919 --> 00:18:10,559
attack and the networks the network

441
00:18:10,559 --> 00:18:12,559
output two value

442
00:18:12,559 --> 00:18:15,200
one corresponding to the probability

443
00:18:15,200 --> 00:18:16,640
that the traces

444
00:18:16,640 --> 00:18:19,760
matches with an encryption where the bit

445
00:18:19,760 --> 00:18:22,000
is equal to zero and the other one

446
00:18:22,000 --> 00:18:24,400
corresponding to the probability that

447
00:18:24,400 --> 00:18:26,000
the sensitive

448
00:18:26,000 --> 00:18:28,480
bit is equal to 1

449
00:18:28,480 --> 00:18:31,200
and after that if we take a separate

450
00:18:31,200 --> 00:18:32,320
data set

451
00:18:32,320 --> 00:18:34,720
on which we don't know the value of the

452
00:18:34,720 --> 00:18:37,039
key we can use a network

453
00:18:37,039 --> 00:18:40,240
to predict the value of the sensitive

454
00:18:40,240 --> 00:18:42,480
bit for different traces

455
00:18:42,480 --> 00:18:45,039
and using differential analysis we can

456
00:18:45,039 --> 00:18:48,080
recover the corresponding byte of the

457
00:18:48,080 --> 00:18:49,679
key

458
00:18:49,679 --> 00:18:51,520
and for that we don't need to reach a

459
00:18:51,520 --> 00:18:54,400
perfect accuracy as you saw

460
00:18:54,400 --> 00:18:57,760
we have an accuracy of only 65 percent

461
00:18:57,760 --> 00:19:00,559
but as long as the network is able to

462
00:19:00,559 --> 00:19:04,320
properly predict uh the bit value for

463
00:19:04,320 --> 00:19:07,200
some traces it is enough information to

464
00:19:07,200 --> 00:19:08,320
break

465
00:19:08,320 --> 00:19:10,480
the implementation using using

466
00:19:10,480 --> 00:19:13,679
differential analysis

467
00:19:13,679 --> 00:19:16,720
so here we saw this is a standard way to

468
00:19:16,720 --> 00:19:19,039
apply deep learning to perform

469
00:19:19,039 --> 00:19:21,360
differential side channel attack

470
00:19:21,360 --> 00:19:23,679
the methodology is pretty much the same

471
00:19:23,679 --> 00:19:26,080
when you use other machine learning

472
00:19:26,080 --> 00:19:27,200
algorithm

473
00:19:27,200 --> 00:19:29,200
but here what is really interesting with

474
00:19:29,200 --> 00:19:31,679
neural networks is that you can

475
00:19:31,679 --> 00:19:35,520
leverage the advantages of deep learning

476
00:19:35,520 --> 00:19:36,480
first

477
00:19:36,480 --> 00:19:38,559
with deep learning what is very

478
00:19:38,559 --> 00:19:40,960
interesting is that you have the freedom

479
00:19:40,960 --> 00:19:43,919
to design your network and so you can

480
00:19:43,919 --> 00:19:44,799
build

481
00:19:44,799 --> 00:19:47,760
more complex models and what is really

482
00:19:47,760 --> 00:19:49,760
interesting is that you can adapt the

483
00:19:49,760 --> 00:19:52,559
network architecture to the dataset that

484
00:19:52,559 --> 00:19:53,840
you target

485
00:19:53,840 --> 00:19:55,520
and later in the presentation we will

486
00:19:55,520 --> 00:19:58,640
see a very interesting example of that

487
00:19:58,640 --> 00:20:01,679
with a cnn with convolutional neural

488
00:20:01,679 --> 00:20:03,039
networks

489
00:20:03,039 --> 00:20:04,640
and in general what is what is

490
00:20:04,640 --> 00:20:07,280
interesting is that we can leverage all

491
00:20:07,280 --> 00:20:09,679
the work that is done in the deep

492
00:20:09,679 --> 00:20:11,200
learning field

493
00:20:11,200 --> 00:20:13,039
where many many techniques and

494
00:20:13,039 --> 00:20:15,760
architectures are designed

495
00:20:15,760 --> 00:20:16,640
and

496
00:20:16,640 --> 00:20:19,120
we can reuse all this work for side

497
00:20:19,120 --> 00:20:21,840
channel analysis

498
00:20:22,240 --> 00:20:24,720
okay so one other interest that we can

499
00:20:24,720 --> 00:20:26,960
mention is that neural networks can

500
00:20:26,960 --> 00:20:30,240
naturally detect and exploit

501
00:20:30,240 --> 00:20:33,039
multivariate leakages which is something

502
00:20:33,039 --> 00:20:35,120
that most of the other technique cannot

503
00:20:35,120 --> 00:20:37,200
do by default

504
00:20:37,200 --> 00:20:39,520
so a multivator leakage is a type of

505
00:20:39,520 --> 00:20:41,840
leakage which is composed of

506
00:20:41,840 --> 00:20:44,480
two or more components

507
00:20:44,480 --> 00:20:48,640
splitted over several instants in time

508
00:20:48,640 --> 00:20:50,880
an example of that is for example when

509
00:20:50,880 --> 00:20:53,280
developers implement

510
00:20:53,280 --> 00:20:56,799
what we call a masking contour measure

511
00:20:56,799 --> 00:20:59,200
masking is a contour measure where

512
00:20:59,200 --> 00:21:01,120
sensitive values like the keys for

513
00:21:01,120 --> 00:21:04,880
example are split into multiple shells

514
00:21:04,880 --> 00:21:07,280
that are manipulated separately

515
00:21:07,280 --> 00:21:09,600
so it protects the implementation

516
00:21:09,600 --> 00:21:11,679
against site channel attacks because the

517
00:21:11,679 --> 00:21:13,840
sensitive value itself is never

518
00:21:13,840 --> 00:21:15,679
manipulated

519
00:21:15,679 --> 00:21:17,520
by the algorithm

520
00:21:17,520 --> 00:21:20,320
the algorithm actually operates on the

521
00:21:20,320 --> 00:21:22,320
on the shares instead

522
00:21:22,320 --> 00:21:23,039
so

523
00:21:23,039 --> 00:21:25,600
the simplest example of that is when a

524
00:21:25,600 --> 00:21:29,200
sensitive data is split into two shares

525
00:21:29,200 --> 00:21:32,400
a mask and a masked value

526
00:21:32,400 --> 00:21:34,320
and in this case if we want to perform

527
00:21:34,320 --> 00:21:38,480
an attack targeting this protected value

528
00:21:38,480 --> 00:21:41,120
what needs to be done is to identify and

529
00:21:41,120 --> 00:21:43,919
combine the leakage of the first share

530
00:21:43,919 --> 00:21:46,799
with the leakage of the second share

531
00:21:46,799 --> 00:21:48,960
so it this is what we call actually a

532
00:21:48,960 --> 00:21:50,960
second order attack it's something that

533
00:21:50,960 --> 00:21:53,520
is usually quite challenging to do

534
00:21:53,520 --> 00:21:56,080
because the two leakages must be

535
00:21:56,080 --> 00:21:59,120
identified first and then the leakages

536
00:21:59,120 --> 00:22:01,280
must be combined together

537
00:22:01,280 --> 00:22:04,559
before the attack is performed

538
00:22:04,559 --> 00:22:06,960
what is nice with neural network is that

539
00:22:06,960 --> 00:22:09,280
during the training process

540
00:22:09,280 --> 00:22:11,360
when the gradient descent algorithm is

541
00:22:11,360 --> 00:22:12,480
applied

542
00:22:12,480 --> 00:22:15,120
to optimize the network weights

543
00:22:15,120 --> 00:22:18,559
the network can automatically detect

544
00:22:18,559 --> 00:22:20,400
the different shares the different

545
00:22:20,400 --> 00:22:22,799
shares leakages and combine them

546
00:22:22,799 --> 00:22:24,559
together

547
00:22:24,559 --> 00:22:26,240
so it means that neural networks they

548
00:22:26,240 --> 00:22:28,799
are naturally able to exploit this this

549
00:22:28,799 --> 00:22:31,600
type of of multivariate leakages and so

550
00:22:31,600 --> 00:22:33,120
it's a huge

551
00:22:33,120 --> 00:22:36,799
plus compared to other techniques

552
00:22:36,799 --> 00:22:39,039
so let's take an example of that here we

553
00:22:39,039 --> 00:22:41,600
have a data set which correspond to a

554
00:22:41,600 --> 00:22:42,880
masked

555
00:22:42,880 --> 00:22:45,760
aes implementation where the sensitive

556
00:22:45,760 --> 00:22:47,840
bytes are split

557
00:22:47,840 --> 00:22:49,919
into two shares

558
00:22:49,919 --> 00:22:52,000
at this stage we don't know where the

559
00:22:52,000 --> 00:22:54,720
leakages of the two shares are located

560
00:22:54,720 --> 00:22:56,799
and we also don't know how to combine

561
00:22:56,799 --> 00:22:57,840
them

562
00:22:57,840 --> 00:22:59,600
but so what we can do is to take a

563
00:22:59,600 --> 00:23:02,559
neural network for example a mlp

564
00:23:02,559 --> 00:23:04,880
like in in the in the previous example

565
00:23:04,880 --> 00:23:08,640
and we can run a training exactly like

566
00:23:08,640 --> 00:23:10,720
in the previous example

567
00:23:10,720 --> 00:23:12,960
but this time in addition to the

568
00:23:12,960 --> 00:23:16,000
accuracy and loss metric we will observe

569
00:23:16,000 --> 00:23:17,600
another metric which is called the

570
00:23:17,600 --> 00:23:20,960
sensitivity of the network input

571
00:23:20,960 --> 00:23:22,400
and this metric

572
00:23:22,400 --> 00:23:23,679
indicates

573
00:23:23,679 --> 00:23:26,000
which input nodes

574
00:23:26,000 --> 00:23:28,159
contribute the most to the

575
00:23:28,159 --> 00:23:30,960
classification of the input data okay

576
00:23:30,960 --> 00:23:32,240
so if we want

577
00:23:32,240 --> 00:23:35,520
the training with this metric

578
00:23:35,520 --> 00:23:37,600
what we can observe is that

579
00:23:37,600 --> 00:23:39,760
the network during the training

580
00:23:39,760 --> 00:23:43,440
automatically detects and focuses on two

581
00:23:43,440 --> 00:23:47,039
specific areas which actually correspond

582
00:23:47,039 --> 00:23:47,919
to

583
00:23:47,919 --> 00:23:52,159
the time samples of the two shares

584
00:23:52,159 --> 00:23:53,600
the two shares of the data that we

585
00:23:53,600 --> 00:23:54,640
target

586
00:23:54,640 --> 00:23:57,600
so the network detect the two shares and

587
00:23:57,600 --> 00:24:00,640
combine their respective leakages

588
00:24:00,640 --> 00:24:02,400
inside the network

589
00:24:02,400 --> 00:24:05,520
to classify the the input data based on

590
00:24:05,520 --> 00:24:08,159
the combine information which is

591
00:24:08,159 --> 00:24:09,919
the sensitive information that we want

592
00:24:09,919 --> 00:24:11,360
to exploit

593
00:24:11,360 --> 00:24:14,240
so it is very powerful here everything

594
00:24:14,240 --> 00:24:15,679
is automatically

595
00:24:15,679 --> 00:24:19,039
done by the neural network

596
00:24:19,039 --> 00:24:21,600
in comparison to other methods where the

597
00:24:21,600 --> 00:24:23,279
leakage combination

598
00:24:23,279 --> 00:24:24,799
must be done

599
00:24:24,799 --> 00:24:26,720
before the

600
00:24:26,720 --> 00:24:28,559
before the attack it must be done by

601
00:24:28,559 --> 00:24:32,760
manually by the attacker

602
00:24:33,200 --> 00:24:35,200
okay so another example that we can

603
00:24:35,200 --> 00:24:38,480
study is how we can use specific network

604
00:24:38,480 --> 00:24:40,960
architectures like convolutional neural

605
00:24:40,960 --> 00:24:43,360
networks for example to solve some

606
00:24:43,360 --> 00:24:47,520
problems from side channel analysis

607
00:24:47,520 --> 00:24:49,760
one common problem that we face in side

608
00:24:49,760 --> 00:24:51,360
channel is what we call

609
00:24:51,360 --> 00:24:53,360
signal desynchronization

610
00:24:53,360 --> 00:24:55,039
and so this happens

611
00:24:55,039 --> 00:24:57,039
for example when we collect multiple

612
00:24:57,039 --> 00:24:59,360
traces from an oscilloscope

613
00:24:59,360 --> 00:25:01,440
most of the time there is a time

614
00:25:01,440 --> 00:25:03,279
desynchronization between the different

615
00:25:03,279 --> 00:25:06,159
traces meaning that from one trace to

616
00:25:06,159 --> 00:25:07,120
another

617
00:25:07,120 --> 00:25:10,320
the same time sample does not correspond

618
00:25:10,320 --> 00:25:12,799
to exactly the same leakage

619
00:25:12,799 --> 00:25:15,440
and the effect of of desynchronization

620
00:25:15,440 --> 00:25:18,559
is that it adds some noise to the signal

621
00:25:18,559 --> 00:25:21,279
and it reduces the efficiency of the

622
00:25:21,279 --> 00:25:22,720
statistical

623
00:25:22,720 --> 00:25:24,880
side channel analysis

624
00:25:24,880 --> 00:25:26,799
so here you can see an example of what

625
00:25:26,799 --> 00:25:28,640
is the impact of

626
00:25:28,640 --> 00:25:31,440
desynchronization on

627
00:25:31,440 --> 00:25:34,880
on an attack that uses the correlation

628
00:25:34,880 --> 00:25:36,559
distinguisher

629
00:25:36,559 --> 00:25:38,640
at the top you can see the overview of

630
00:25:38,640 --> 00:25:42,240
the traces used for the attack and at

631
00:25:42,240 --> 00:25:45,120
the bottom you can see the result of the

632
00:25:45,120 --> 00:25:46,159
attack

633
00:25:46,159 --> 00:25:49,440
with a corresponding correlation result

634
00:25:49,440 --> 00:25:51,200
so at the beginning here there is no

635
00:25:51,200 --> 00:25:53,039
desynchronization and we can see that

636
00:25:53,039 --> 00:25:56,159
the attack works pretty well because for

637
00:25:56,159 --> 00:25:58,080
one of the key guests

638
00:25:58,080 --> 00:26:00,720
one of the key guesses we observe a very

639
00:26:00,720 --> 00:26:03,039
high correlation

640
00:26:03,039 --> 00:26:05,440
but now what we can do is see what

641
00:26:05,440 --> 00:26:08,159
happens if we artificially

642
00:26:08,159 --> 00:26:11,200
add desynchronization to the signal

643
00:26:11,200 --> 00:26:12,320
so here

644
00:26:12,320 --> 00:26:14,000
at each step

645
00:26:14,000 --> 00:26:16,480
we add one time sample of

646
00:26:16,480 --> 00:26:20,000
desynchronization to each input trace

647
00:26:20,000 --> 00:26:22,159
and we can see that very quickly

648
00:26:22,159 --> 00:26:26,320
it reduces the efficiency of the attack

649
00:26:26,320 --> 00:26:28,480
because the correlation becomes smaller

650
00:26:28,480 --> 00:26:29,679
and smaller

651
00:26:29,679 --> 00:26:32,000
and at the end with just a few samples

652
00:26:32,000 --> 00:26:34,559
of desynchronization it becomes

653
00:26:34,559 --> 00:26:36,240
impossible

654
00:26:36,240 --> 00:26:37,679
to recover

655
00:26:37,679 --> 00:26:40,240
the secret key byte

656
00:26:40,240 --> 00:26:41,520
so

657
00:26:41,520 --> 00:26:43,279
the goal in sidechain is always to

658
00:26:43,279 --> 00:26:45,760
reduce this desynchronization

659
00:26:45,760 --> 00:26:47,600
and so there are many techniques to do

660
00:26:47,600 --> 00:26:48,559
that

661
00:26:48,559 --> 00:26:50,320
most of the time we use techniques from

662
00:26:50,320 --> 00:26:54,159
signal processing that we use to realign

663
00:26:54,159 --> 00:26:58,960
the traces and reduce this effect

664
00:26:58,960 --> 00:27:00,880
but again this is a processing that

665
00:27:00,880 --> 00:27:02,480
usually must be done

666
00:27:02,480 --> 00:27:04,880
manually by the analyst

667
00:27:04,880 --> 00:27:06,880
it can be very time consuming it can be

668
00:27:06,880 --> 00:27:09,440
challenging because it requires to

669
00:27:09,440 --> 00:27:12,720
analyze the signal and to adapt the

670
00:27:12,720 --> 00:27:14,159
alignment

671
00:27:14,159 --> 00:27:16,799
procedure to the data set

672
00:27:16,799 --> 00:27:19,279
so now i want to present how

673
00:27:19,279 --> 00:27:21,039
convolutional neural networks they

674
00:27:21,039 --> 00:27:25,039
provide a very nice solution to this

675
00:27:25,039 --> 00:27:28,559
common issue inside channel

676
00:27:28,559 --> 00:27:31,039
okay so convolutional neural networks

677
00:27:31,039 --> 00:27:33,840
are cnn in short it's it's a popular

678
00:27:33,840 --> 00:27:36,880
type of network architecture that was

679
00:27:36,880 --> 00:27:38,960
originally designed for image

680
00:27:38,960 --> 00:27:40,720
classification

681
00:27:40,720 --> 00:27:43,120
and the core of the cnn architecture is

682
00:27:43,120 --> 00:27:46,320
a combination of two types of layer

683
00:27:46,320 --> 00:27:47,279
first

684
00:27:47,279 --> 00:27:49,440
the first type of layer is convolution

685
00:27:49,440 --> 00:27:50,480
layers

686
00:27:50,480 --> 00:27:54,080
where a convolution kernel is slided

687
00:27:54,080 --> 00:27:56,080
along the input data

688
00:27:56,080 --> 00:27:58,240
and where the weights of the of the

689
00:27:58,240 --> 00:28:00,640
convolution kernel are actually the

690
00:28:00,640 --> 00:28:03,200
trainable parameters of the network

691
00:28:03,200 --> 00:28:05,039
and the second type of layer is a

692
00:28:05,039 --> 00:28:06,720
pulling layers

693
00:28:06,720 --> 00:28:07,600
where

694
00:28:07,600 --> 00:28:10,080
pulling opera a pulling operator is

695
00:28:10,080 --> 00:28:11,200
slided

696
00:28:11,200 --> 00:28:13,840
along the output of the previous

697
00:28:13,840 --> 00:28:15,360
convolution layer

698
00:28:15,360 --> 00:28:18,720
and this pooling layer produces a local

699
00:28:18,720 --> 00:28:22,320
summary of the convoluted data

700
00:28:22,320 --> 00:28:25,840
so the cnn is composed of a series of

701
00:28:25,840 --> 00:28:28,799
convolution plus pulling blocks

702
00:28:28,799 --> 00:28:31,600
chained together and at the end of this

703
00:28:31,600 --> 00:28:34,880
convolution and pulling blocks

704
00:28:34,880 --> 00:28:36,880
there is a series of fully connected

705
00:28:36,880 --> 00:28:40,320
layers or linear layers that are used to

706
00:28:40,320 --> 00:28:43,279
classify the data into the different

707
00:28:43,279 --> 00:28:45,520
classes okay

708
00:28:45,520 --> 00:28:48,159
so now we can have a look at how

709
00:28:48,159 --> 00:28:51,760
cnn a cnn transforms

710
00:28:51,760 --> 00:28:55,440
an input data into what we call abstract

711
00:28:55,440 --> 00:28:57,200
features

712
00:28:57,200 --> 00:28:59,520
so if we take the example of image

713
00:28:59,520 --> 00:29:01,679
classification

714
00:29:01,679 --> 00:29:04,240
in image classification we start with a

715
00:29:04,240 --> 00:29:07,520
picture which is composed of a single

716
00:29:07,520 --> 00:29:09,600
feature if it's a black and white

717
00:29:09,600 --> 00:29:13,279
picture or is composed of three features

718
00:29:13,279 --> 00:29:15,919
uh three color features red green and

719
00:29:15,919 --> 00:29:19,679
blue if we we use a color picture

720
00:29:19,679 --> 00:29:23,039
and so in the input image the size of

721
00:29:23,039 --> 00:29:23,919
each

722
00:29:23,919 --> 00:29:24,960
feature

723
00:29:24,960 --> 00:29:29,360
is quite large in the in the input image

724
00:29:29,360 --> 00:29:31,760
the image is basically composed of

725
00:29:31,760 --> 00:29:35,360
hundreds or thousands of pixels in each

726
00:29:35,360 --> 00:29:37,360
of the color feature

727
00:29:37,360 --> 00:29:40,240
and what happened is that the cnn will

728
00:29:40,240 --> 00:29:44,000
transform this small number of large

729
00:29:44,000 --> 00:29:45,039
features

730
00:29:45,039 --> 00:29:46,080
that

731
00:29:46,080 --> 00:29:47,760
compose the

732
00:29:47,760 --> 00:29:49,120
input image

733
00:29:49,120 --> 00:29:52,320
into more and more abstract features

734
00:29:52,320 --> 00:29:55,360
that become smaller and smaller the

735
00:29:55,360 --> 00:29:58,240
deeper we go in the network okay

736
00:29:58,240 --> 00:30:00,640
so you can see in this diagram

737
00:30:00,640 --> 00:30:03,039
the deeper we go in the network the

738
00:30:03,039 --> 00:30:05,200
higher the number of

739
00:30:05,200 --> 00:30:07,760
features that are outputted by the

740
00:30:07,760 --> 00:30:09,679
convolution layers

741
00:30:09,679 --> 00:30:12,559
and these features usually becomes

742
00:30:12,559 --> 00:30:15,200
become smaller and smaller because at

743
00:30:15,200 --> 00:30:16,559
each step

744
00:30:16,559 --> 00:30:19,600
we apply a pulling operation

745
00:30:19,600 --> 00:30:22,799
that compresses the information and it

746
00:30:22,799 --> 00:30:24,159
reduces

747
00:30:24,159 --> 00:30:27,440
the dimension of the features

748
00:30:27,440 --> 00:30:29,120
so at the end at the end of the

749
00:30:29,120 --> 00:30:31,919
convolution blocks what we get is a

750
00:30:31,919 --> 00:30:34,000
large number

751
00:30:34,000 --> 00:30:36,720
of usually pretty small

752
00:30:36,720 --> 00:30:38,799
compressed features that we call

753
00:30:38,799 --> 00:30:40,399
abstract features

754
00:30:40,399 --> 00:30:43,520
and at the end of the network the

755
00:30:43,520 --> 00:30:46,159
final final layer the

756
00:30:46,159 --> 00:30:47,919
fully connected layers

757
00:30:47,919 --> 00:30:48,960
will

758
00:30:48,960 --> 00:30:52,880
classify the data into categories

759
00:30:52,880 --> 00:30:55,760
based on the value of these abstract

760
00:30:55,760 --> 00:30:58,158
features

761
00:30:58,320 --> 00:31:01,120
so the combination of convolution and

762
00:31:01,120 --> 00:31:03,600
pulling layers in cnn

763
00:31:03,600 --> 00:31:06,640
it offers a very interesting property

764
00:31:06,640 --> 00:31:09,600
to these networks which is a translation

765
00:31:09,600 --> 00:31:11,600
in variance property

766
00:31:11,600 --> 00:31:13,200
this is something that is

767
00:31:13,200 --> 00:31:15,279
quite simple to illustrate and

768
00:31:15,279 --> 00:31:18,640
understand with image classification

769
00:31:18,640 --> 00:31:22,399
when we build a cnn to classify images

770
00:31:22,399 --> 00:31:25,360
we want that the network is able to

771
00:31:25,360 --> 00:31:28,559
detect and classify the object

772
00:31:28,559 --> 00:31:32,640
whatever its position in the picture and

773
00:31:32,640 --> 00:31:35,279
this is a property which is naturally

774
00:31:35,279 --> 00:31:38,799
provided by the cnn architecture

775
00:31:38,799 --> 00:31:41,039
if we look at the convolution layer

776
00:31:41,039 --> 00:31:43,039
used in cnn

777
00:31:43,039 --> 00:31:45,919
the convolution kernel is slided along

778
00:31:45,919 --> 00:31:49,039
the input data and along the features

779
00:31:49,039 --> 00:31:50,559
inside the network

780
00:31:50,559 --> 00:31:52,640
which means that the relevant

781
00:31:52,640 --> 00:31:55,840
information is always captured

782
00:31:55,840 --> 00:31:57,120
whatever

783
00:31:57,120 --> 00:32:00,720
its position in the data and the second

784
00:32:00,720 --> 00:32:02,799
important point is that the pulling

785
00:32:02,799 --> 00:32:04,799
operations

786
00:32:04,799 --> 00:32:08,000
are used as a compression method

787
00:32:08,000 --> 00:32:10,720
and it also reduces the effect of the

788
00:32:10,720 --> 00:32:14,000
original translation the deeper we go in

789
00:32:14,000 --> 00:32:15,600
the network

790
00:32:15,600 --> 00:32:18,399
and so thanks to this property it means

791
00:32:18,399 --> 00:32:21,840
that cnns are also naturally efficient

792
00:32:21,840 --> 00:32:23,360
to classify

793
00:32:23,360 --> 00:32:25,200
side channel traces

794
00:32:25,200 --> 00:32:28,320
that are desynchronized and it's for the

795
00:32:28,320 --> 00:32:30,880
same reason um as in image

796
00:32:30,880 --> 00:32:32,159
classification

797
00:32:32,159 --> 00:32:35,679
even if the relevant information is

798
00:32:35,679 --> 00:32:37,519
slightly shifted

799
00:32:37,519 --> 00:32:39,760
from one trace to another

800
00:32:39,760 --> 00:32:42,640
the cnn architecture will naturally be

801
00:32:42,640 --> 00:32:46,720
able to capture this information and

802
00:32:46,720 --> 00:32:49,440
properly classify the traces

803
00:32:49,440 --> 00:32:50,880
and so it's very very interesting

804
00:32:50,880 --> 00:32:54,720
because it means that again in this case

805
00:32:54,720 --> 00:32:57,679
everything is done by the network itself

806
00:32:57,679 --> 00:33:00,159
we don't need to re-synchronize and

807
00:33:00,159 --> 00:33:03,440
realign the traces before the attack

808
00:33:03,440 --> 00:33:05,760
everything is done by the network and

809
00:33:05,760 --> 00:33:07,120
cnn

810
00:33:07,120 --> 00:33:09,279
a convolutional neural network they are

811
00:33:09,279 --> 00:33:11,360
able to

812
00:33:11,360 --> 00:33:15,200
handle this kind of desynchronization

813
00:33:15,200 --> 00:33:17,600
okay so now let's observe this

814
00:33:17,600 --> 00:33:20,000
translation in variance property on a

815
00:33:20,000 --> 00:33:23,679
real example with side channel traces

816
00:33:23,679 --> 00:33:25,440
so here we have a

817
00:33:25,440 --> 00:33:28,000
convolutional neural network with four

818
00:33:28,000 --> 00:33:29,760
convolution blocks

819
00:33:29,760 --> 00:33:32,720
and this network was trained on a

820
00:33:32,720 --> 00:33:34,720
desynchronized data set

821
00:33:34,720 --> 00:33:36,399
so we reload

822
00:33:36,399 --> 00:33:39,600
the parameters of this trained network

823
00:33:39,600 --> 00:33:42,720
and below here we have a small widget

824
00:33:42,720 --> 00:33:44,799
that we will use to observe

825
00:33:44,799 --> 00:33:47,600
the translation in variance of this

826
00:33:47,600 --> 00:33:48,720
network

827
00:33:48,720 --> 00:33:50,720
so with this widget we are able to

828
00:33:50,720 --> 00:33:54,480
observe the features

829
00:33:54,480 --> 00:33:57,440
at the different layers of the cnn okay

830
00:33:57,440 --> 00:34:00,720
so we start at the input of the network

831
00:34:00,720 --> 00:34:03,519
and here we display two input data

832
00:34:03,519 --> 00:34:07,519
we display in blue an input trace and in

833
00:34:07,519 --> 00:34:10,639
orange we display the same input data

834
00:34:10,639 --> 00:34:13,760
but that we artificially desynchronize

835
00:34:13,760 --> 00:34:16,159
so in orange it's the same

836
00:34:16,159 --> 00:34:20,399
trace as in blue but just shifted 10

837
00:34:20,399 --> 00:34:22,960
samples to the right

838
00:34:22,960 --> 00:34:23,760
and

839
00:34:23,760 --> 00:34:26,000
what we will do is to observe

840
00:34:26,000 --> 00:34:28,639
how the network processes these two

841
00:34:28,639 --> 00:34:29,839
inputs

842
00:34:29,839 --> 00:34:32,639
across the different layers

843
00:34:32,639 --> 00:34:35,679
so if we observe the output of the first

844
00:34:35,679 --> 00:34:37,280
convolution block

845
00:34:37,280 --> 00:34:39,199
we can observe that

846
00:34:39,199 --> 00:34:42,159
basically the layer output pretty much

847
00:34:42,159 --> 00:34:45,520
the same information for both traces

848
00:34:45,520 --> 00:34:48,239
and this is what we expect from from

849
00:34:48,239 --> 00:34:50,399
what we explained earlier

850
00:34:50,399 --> 00:34:52,480
because of the convolution kernel which

851
00:34:52,480 --> 00:34:55,839
is slided along the input data

852
00:34:55,839 --> 00:34:58,079
it captures pretty much the same

853
00:34:58,079 --> 00:34:59,280
information

854
00:34:59,280 --> 00:35:01,760
for both input traces

855
00:35:01,760 --> 00:35:04,720
but with just a small offset

856
00:35:04,720 --> 00:35:07,040
but what is interesting is that this

857
00:35:07,040 --> 00:35:09,680
offset between the the convoluted

858
00:35:09,680 --> 00:35:10,720
features

859
00:35:10,720 --> 00:35:13,760
will gradually decrease

860
00:35:13,760 --> 00:35:16,480
the deeper we go in the network and this

861
00:35:16,480 --> 00:35:19,599
is because at each step we will apply a

862
00:35:19,599 --> 00:35:22,640
pulling operation and this pulling

863
00:35:22,640 --> 00:35:25,520
operation compress will compress the

864
00:35:25,520 --> 00:35:27,599
information and reduce

865
00:35:27,599 --> 00:35:29,920
the dimension of

866
00:35:29,920 --> 00:35:31,520
the features

867
00:35:31,520 --> 00:35:34,560
so if we observe the output

868
00:35:34,560 --> 00:35:36,400
of the second

869
00:35:36,400 --> 00:35:38,320
convolution block

870
00:35:38,320 --> 00:35:41,200
again we can observe that the data is

871
00:35:41,200 --> 00:35:45,839
pretty similar for both inputs

872
00:35:45,920 --> 00:35:47,760
because of the because of the pulling

873
00:35:47,760 --> 00:35:49,680
operation we can observe that there is

874
00:35:49,680 --> 00:35:52,000
actually some loss some information loss

875
00:35:52,000 --> 00:35:54,160
between the two uh

876
00:35:54,160 --> 00:35:55,760
the two data

877
00:35:55,760 --> 00:35:58,880
but the information is pretty similar

878
00:35:58,880 --> 00:36:00,960
and here we can observe that the offset

879
00:36:00,960 --> 00:36:03,040
between the two information

880
00:36:03,040 --> 00:36:05,280
was clearly reduced

881
00:36:05,280 --> 00:36:08,240
we started with an offset of 10 samples

882
00:36:08,240 --> 00:36:09,680
in the input

883
00:36:09,680 --> 00:36:12,960
and after the second layer we only have

884
00:36:12,960 --> 00:36:16,480
two samples of offset between the two

885
00:36:16,480 --> 00:36:18,079
signals

886
00:36:18,079 --> 00:36:20,720
so here we are using a pulling layer

887
00:36:20,720 --> 00:36:23,280
with a window size of two so

888
00:36:23,280 --> 00:36:26,400
we divide the dimension by two at each

889
00:36:26,400 --> 00:36:28,000
layer

890
00:36:28,000 --> 00:36:31,200
meaning that the original offset is

891
00:36:31,200 --> 00:36:33,040
reduced by two

892
00:36:33,040 --> 00:36:36,720
at each layer of the network okay

893
00:36:36,720 --> 00:36:39,200
so if we look at the output of the third

894
00:36:39,200 --> 00:36:40,640
block

895
00:36:40,640 --> 00:36:43,520
we can see that there is still some loss

896
00:36:43,520 --> 00:36:46,560
because of the of the pulling operation

897
00:36:46,560 --> 00:36:47,599
but

898
00:36:47,599 --> 00:36:48,640
the two

899
00:36:48,640 --> 00:36:52,320
signals are pretty again pretty similar

900
00:36:52,320 --> 00:36:55,520
and at this stage the original offset

901
00:36:55,520 --> 00:36:58,320
almost disappeared

902
00:36:58,320 --> 00:37:00,560
if we look at the output of the last

903
00:37:00,560 --> 00:37:03,760
convolution block this time

904
00:37:03,760 --> 00:37:05,760
the original offset completely

905
00:37:05,760 --> 00:37:08,400
disappeared and the two signals

906
00:37:08,400 --> 00:37:12,000
are still pretty similar

907
00:37:12,560 --> 00:37:13,760
the

908
00:37:13,760 --> 00:37:16,160
convoluted features for the two signals

909
00:37:16,160 --> 00:37:18,160
are pretty similar

910
00:37:18,160 --> 00:37:20,800
and the original offset was

911
00:37:20,800 --> 00:37:22,400
completely

912
00:37:22,400 --> 00:37:24,240
removed

913
00:37:24,240 --> 00:37:27,359
so here we started with two signals that

914
00:37:27,359 --> 00:37:29,839
were desynchronized and what we can see

915
00:37:29,839 --> 00:37:32,160
is that the output produced by the

916
00:37:32,160 --> 00:37:33,839
convolution blocks

917
00:37:33,839 --> 00:37:36,400
are very similar for both signal and so

918
00:37:36,400 --> 00:37:39,839
after that it's pretty pretty simple for

919
00:37:39,839 --> 00:37:41,040
the last

920
00:37:41,040 --> 00:37:44,560
layers of the cnn to classify the two

921
00:37:44,560 --> 00:37:47,280
inputs in the same categories because

922
00:37:47,280 --> 00:37:50,720
the two inputs have pretty similar

923
00:37:50,720 --> 00:37:52,480
abstract features

924
00:37:52,480 --> 00:37:53,680
at the end

925
00:37:53,680 --> 00:37:56,480
of the convolution blocks

926
00:37:56,480 --> 00:37:57,280
so

927
00:37:57,280 --> 00:37:58,720
this example

928
00:37:58,720 --> 00:38:00,800
illustrates how

929
00:38:00,800 --> 00:38:03,040
the translation in variance

930
00:38:03,040 --> 00:38:05,839
works with cnn

931
00:38:05,839 --> 00:38:09,119
and why it is able to efficiently

932
00:38:09,119 --> 00:38:11,839
process side channel choices that are

933
00:38:11,839 --> 00:38:13,359
desynchronized

934
00:38:13,359 --> 00:38:16,640
and so as as i explained earlier

935
00:38:16,640 --> 00:38:18,400
because desynchronization is one of the

936
00:38:18,400 --> 00:38:21,520
of the biggest challenge in side channel

937
00:38:21,520 --> 00:38:23,440
it's very interesting to have neural

938
00:38:23,440 --> 00:38:25,359
network like cnns

939
00:38:25,359 --> 00:38:27,119
that are able to

940
00:38:27,119 --> 00:38:28,280
handle

941
00:38:28,280 --> 00:38:30,880
desynchronization and still

942
00:38:30,880 --> 00:38:33,839
be efficient to classify side channel

943
00:38:33,839 --> 00:38:34,880
choices

944
00:38:34,880 --> 00:38:35,839
and so

945
00:38:35,839 --> 00:38:38,000
it's very interesting because here

946
00:38:38,000 --> 00:38:39,920
everything is automatically

947
00:38:39,920 --> 00:38:42,640
automatically done by the network

948
00:38:42,640 --> 00:38:45,760
the attacker the analyst does not need

949
00:38:45,760 --> 00:38:48,880
to re-synchronize the traces manually

950
00:38:48,880 --> 00:38:52,560
before before the attack

951
00:38:53,760 --> 00:38:56,160
okay so finally i just want to mention a

952
00:38:56,160 --> 00:38:57,680
few other techniques

953
00:38:57,680 --> 00:39:00,240
that for me illustrates

954
00:39:00,240 --> 00:39:01,200
why

955
00:39:01,200 --> 00:39:04,160
deep learning is opening the door to

956
00:39:04,160 --> 00:39:07,200
pretty exciting possibilities for side

957
00:39:07,200 --> 00:39:09,040
channel analysis

958
00:39:09,040 --> 00:39:11,200
so the fact is deep learning is a very

959
00:39:11,200 --> 00:39:14,400
vibrant field there are many many new

960
00:39:14,400 --> 00:39:17,119
techniques being published almost every

961
00:39:17,119 --> 00:39:18,079
day

962
00:39:18,079 --> 00:39:19,119
in this

963
00:39:19,119 --> 00:39:21,200
in this field and what is great is that

964
00:39:21,200 --> 00:39:22,880
many of these techniques they can be

965
00:39:22,880 --> 00:39:25,760
reused for other applications in other

966
00:39:25,760 --> 00:39:26,800
fields

967
00:39:26,800 --> 00:39:28,720
like for example for side channel

968
00:39:28,720 --> 00:39:30,240
analysis

969
00:39:30,240 --> 00:39:33,119
so the first example i wanted to mention

970
00:39:33,119 --> 00:39:35,200
is the use of auto encoders for

971
00:39:35,200 --> 00:39:38,320
denoising side channel traces

972
00:39:38,320 --> 00:39:40,560
so auto encoders is a type of neural

973
00:39:40,560 --> 00:39:43,599
network that is composed of two parts

974
00:39:43,599 --> 00:39:45,280
the first part of the network is called

975
00:39:45,280 --> 00:39:48,880
the encoder and it compresses the input

976
00:39:48,880 --> 00:39:52,400
into an encoded form which is of a lower

977
00:39:52,400 --> 00:39:56,079
dimension than the input data and then

978
00:39:56,079 --> 00:39:57,680
the second part of the network is what

979
00:39:57,680 --> 00:39:59,440
we call the decoder

980
00:39:59,440 --> 00:40:01,760
this part reconstructs the original

981
00:40:01,760 --> 00:40:05,119
signal from the encoded data

982
00:40:05,119 --> 00:40:07,839
so the goal when training an autoencoder

983
00:40:07,839 --> 00:40:08,640
is

984
00:40:08,640 --> 00:40:11,119
to get a network that is able to

985
00:40:11,119 --> 00:40:13,839
efficiently reconstruct the original

986
00:40:13,839 --> 00:40:14,960
input

987
00:40:14,960 --> 00:40:18,319
from the compressed encoded data

988
00:40:18,319 --> 00:40:20,319
and so there are many applications of

989
00:40:20,319 --> 00:40:23,280
auto encoders one of them is

990
00:40:23,280 --> 00:40:26,800
denoising auto encoders that are used to

991
00:40:26,800 --> 00:40:30,400
remove or reduce the noise from an input

992
00:40:30,400 --> 00:40:31,440
data

993
00:40:31,440 --> 00:40:34,079
and so exactly like desynchronization

994
00:40:34,079 --> 00:40:36,640
noise it's a it's a very common issue in

995
00:40:36,640 --> 00:40:38,560
side channel analysis

996
00:40:38,560 --> 00:40:41,119
when you collect signal from a device

997
00:40:41,119 --> 00:40:43,680
you always want to reduce the noise

998
00:40:43,680 --> 00:40:47,520
level so that attacks are more efficient

999
00:40:47,520 --> 00:40:49,920
and so as you can as you can see below i

1000
00:40:49,920 --> 00:40:52,880
listed one article published in 2020

1001
00:40:52,880 --> 00:40:55,040
which explores

1002
00:40:55,040 --> 00:40:56,880
the application of denoising auto

1003
00:40:56,880 --> 00:40:59,920
encoders for site channel analysis and

1004
00:40:59,920 --> 00:41:01,200
it's

1005
00:41:01,200 --> 00:41:03,520
pretty interesting

1006
00:41:03,520 --> 00:41:06,000
and so the second example i wanted to

1007
00:41:06,000 --> 00:41:08,240
give is

1008
00:41:08,240 --> 00:41:10,720
the usage of generative adversarial

1009
00:41:10,720 --> 00:41:12,000
networks

1010
00:41:12,000 --> 00:41:15,599
to generate artificial site channel data

1011
00:41:15,599 --> 00:41:17,040
and improve

1012
00:41:17,040 --> 00:41:19,599
the efficiency of training when working

1013
00:41:19,599 --> 00:41:22,400
on side channel data sets

1014
00:41:22,400 --> 00:41:25,119
so guns it's a it's a special type of

1015
00:41:25,119 --> 00:41:27,760
network architecture that can be used

1016
00:41:27,760 --> 00:41:28,560
to

1017
00:41:28,560 --> 00:41:31,040
generate artificial data

1018
00:41:31,040 --> 00:41:35,680
from real data examples so in a gun you

1019
00:41:35,680 --> 00:41:38,160
have two actually two networks that are

1020
00:41:38,160 --> 00:41:40,880
competing against each other

1021
00:41:40,880 --> 00:41:42,960
you have a first network which is called

1022
00:41:42,960 --> 00:41:45,760
a generator and the goal of this network

1023
00:41:45,760 --> 00:41:50,160
is to generate new artificial data from

1024
00:41:50,160 --> 00:41:51,440
a small

1025
00:41:51,440 --> 00:41:53,119
source of entropy

1026
00:41:53,119 --> 00:41:54,960
and the second network

1027
00:41:54,960 --> 00:41:57,599
called the discriminator

1028
00:41:57,599 --> 00:41:58,800
the second network is called the

1029
00:41:58,800 --> 00:42:01,839
discriminator and the goal of this

1030
00:42:01,839 --> 00:42:04,079
network is to be able to

1031
00:42:04,079 --> 00:42:06,319
distinguish between the real data

1032
00:42:06,319 --> 00:42:09,599
example and the artificial data

1033
00:42:09,599 --> 00:42:13,200
generated by the generator network

1034
00:42:13,200 --> 00:42:16,640
so both networks are trained in parallel

1035
00:42:16,640 --> 00:42:18,640
and actually they are competing against

1036
00:42:18,640 --> 00:42:19,839
each other

1037
00:42:19,839 --> 00:42:22,240
and so ideally

1038
00:42:22,240 --> 00:42:24,560
what happens at the end is that over

1039
00:42:24,560 --> 00:42:27,839
time the generator becomes so good at

1040
00:42:27,839 --> 00:42:30,160
generating new data that the the

1041
00:42:30,160 --> 00:42:32,720
discriminator is not able to

1042
00:42:32,720 --> 00:42:36,000
properly distinguish the real data from

1043
00:42:36,000 --> 00:42:38,800
the fake generated data

1044
00:42:38,800 --> 00:42:42,000
and so one application of this

1045
00:42:42,000 --> 00:42:44,319
is for example to use uh

1046
00:42:44,319 --> 00:42:46,720
gans to generate artificial data for

1047
00:42:46,720 --> 00:42:49,200
what we call data augmentation okay

1048
00:42:49,200 --> 00:42:51,599
so in deep learning usually the more

1049
00:42:51,599 --> 00:42:53,760
data we have to train the network the

1050
00:42:53,760 --> 00:42:54,720
better

1051
00:42:54,720 --> 00:42:56,000
and so in deep learning you have a

1052
00:42:56,000 --> 00:42:58,400
technique a set of techniques which is

1053
00:42:58,400 --> 00:43:01,839
called data augmentation where the idea

1054
00:43:01,839 --> 00:43:02,720
is to

1055
00:43:02,720 --> 00:43:05,920
from a small or limited data set we

1056
00:43:05,920 --> 00:43:08,640
increase artificially increase the size

1057
00:43:08,640 --> 00:43:10,240
of the data set

1058
00:43:10,240 --> 00:43:13,119
to improve the overall performance of

1059
00:43:13,119 --> 00:43:16,240
the network during the training

1060
00:43:16,240 --> 00:43:19,200
and so gun generative adversarial

1061
00:43:19,200 --> 00:43:20,560
networks

1062
00:43:20,560 --> 00:43:22,720
it provides a way to generate this type

1063
00:43:22,720 --> 00:43:24,640
of artificial data and so it's an

1064
00:43:24,640 --> 00:43:26,000
interesting technique for data

1065
00:43:26,000 --> 00:43:27,359
augmentation

1066
00:43:27,359 --> 00:43:29,920
and so below i listed one article from

1067
00:43:29,920 --> 00:43:33,599
2021 which explores

1068
00:43:33,599 --> 00:43:35,200
this type of technique for data

1069
00:43:35,200 --> 00:43:36,880
augmentation

1070
00:43:36,880 --> 00:43:39,680
in the context of site channel analysis

1071
00:43:39,680 --> 00:43:43,680
and again it's it's pretty interesting

1072
00:43:44,000 --> 00:43:45,680
and so that's it for today's

1073
00:43:45,680 --> 00:43:49,119
presentation i hope this talk gave you a

1074
00:43:49,119 --> 00:43:50,640
good overview of

1075
00:43:50,640 --> 00:43:52,319
how and why

1076
00:43:52,319 --> 00:43:54,960
deep learning is really changing the way

1077
00:43:54,960 --> 00:43:57,440
we perform side channel analysis

1078
00:43:57,440 --> 00:44:00,240
and why it's one of the most active

1079
00:44:00,240 --> 00:44:03,599
topic in in this field

1080
00:44:03,599 --> 00:44:04,640
today

1081
00:44:04,640 --> 00:44:07,119
thanks a lot for listening and i will

1082
00:44:07,119 --> 00:44:09,839
see you next time

