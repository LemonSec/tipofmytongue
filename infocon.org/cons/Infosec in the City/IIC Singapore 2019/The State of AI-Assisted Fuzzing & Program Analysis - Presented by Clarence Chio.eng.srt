1
00:00:01,250 --> 00:00:04,380
my background is that I I spent some

2
00:00:04,380 --> 00:00:08,010
time working in in ml and then I went to

3
00:00:08,010 --> 00:00:10,200
went to Stanford for my undergrad and

4
00:00:10,200 --> 00:00:13,650
master's in AI and then worked at a

5
00:00:13,650 --> 00:00:15,120
security company for a while mainly

6
00:00:15,120 --> 00:00:17,760
working on bought intrusion and

7
00:00:17,760 --> 00:00:21,630
automation attack detection so after

8
00:00:21,630 --> 00:00:23,760
that then I decided to write a book on

9
00:00:23,760 --> 00:00:25,230
this subject because I thought that

10
00:00:25,230 --> 00:00:28,410
there was so much noise in security in

11
00:00:28,410 --> 00:00:30,689
general and any any other vertical that

12
00:00:30,689 --> 00:00:32,969
uses ml or claims to use ml for anything

13
00:00:32,969 --> 00:00:34,559
I'm sure if you walk around a conference

14
00:00:34,559 --> 00:00:36,600
floors you'll find that everyone is

15
00:00:36,600 --> 00:00:38,520
claiming to have some kind of a solution

16
00:00:38,520 --> 00:00:40,680
but if you just try to poke a little

17
00:00:40,680 --> 00:00:42,420
deeper than you realize that it's it's a

18
00:00:42,420 --> 00:00:45,210
lot of BS so the purpose of this book is

19
00:00:45,210 --> 00:00:47,309
really to try to uncover some of that

20
00:00:47,309 --> 00:00:50,039
and see what is really possible so it's

21
00:00:50,039 --> 00:00:53,090
a technical book there's code inside and

22
00:00:53,090 --> 00:00:55,890
the book is the basis of the class I'm

23
00:00:55,890 --> 00:00:57,840
teaching in Berkeley right now in a

24
00:00:57,840 --> 00:01:01,109
graduate program and it's making use of

25
00:01:01,109 --> 00:01:04,250
data science and ml in security context

26
00:01:04,250 --> 00:01:06,990
beside that I started a security company

27
00:01:06,990 --> 00:01:10,110
I started a company last year that's

28
00:01:10,110 --> 00:01:12,689
funded by Google that works in the

29
00:01:12,689 --> 00:01:14,610
financial crimes space so we are looking

30
00:01:14,610 --> 00:01:16,590
for money laundering in banks and

31
00:01:16,590 --> 00:01:18,630
financial institutions and we're a small

32
00:01:18,630 --> 00:01:20,189
team if interested in finding out more

33
00:01:20,189 --> 00:01:23,070
and potentially joining us in SF let me

34
00:01:23,070 --> 00:01:26,310
know and then would love to chat aside

35
00:01:26,310 --> 00:01:28,290
from that I spend my spare time

36
00:01:28,290 --> 00:01:31,380
consulting at Facebook for the Russian

37
00:01:31,380 --> 00:01:33,079
interference problem and the bot problem

38
00:01:33,079 --> 00:01:36,270
that's where my co-authors works and so

39
00:01:36,270 --> 00:01:40,740
I'm just helping out so a lot of the a

40
00:01:40,740 --> 00:01:42,930
lot of the context of this talk came

41
00:01:42,930 --> 00:01:46,049
from just conversations of people I used

42
00:01:46,049 --> 00:01:47,520
to go to a lot of security conferences a

43
00:01:47,520 --> 00:01:49,290
lot more than than I do now

44
00:01:49,290 --> 00:01:52,439
and talking to people who are just

45
00:01:52,439 --> 00:01:54,600
spending a lot of time looking for

46
00:01:54,600 --> 00:01:56,430
vulnerabilities in software spending a

47
00:01:56,430 --> 00:01:59,310
lot of time looking for ways to exploit

48
00:01:59,310 --> 00:02:02,880
software to get the bug bounty rewards

49
00:02:02,880 --> 00:02:06,689
to get any kind of recognition for

50
00:02:06,689 --> 00:02:09,030
finding crashes and programs I realized

51
00:02:09,030 --> 00:02:11,400
that the way that people were

52
00:02:11,400 --> 00:02:13,860
approaching the problem of

53
00:02:13,860 --> 00:02:16,440
finding exploits looks very similar to

54
00:02:16,440 --> 00:02:19,140
how people approached antivirus how

55
00:02:19,140 --> 00:02:22,590
people look at trying to brute-force a

56
00:02:22,590 --> 00:02:23,400
problem

57
00:02:23,400 --> 00:02:26,520
a decade ago 20 years ago insecurity and

58
00:02:26,520 --> 00:02:28,200
I realized that the only the only

59
00:02:28,200 --> 00:02:30,660
sensible way forward is for this field

60
00:02:30,660 --> 00:02:32,820
to be a lot more data-driven and so I

61
00:02:32,820 --> 00:02:34,290
started doing some research in the field

62
00:02:34,290 --> 00:02:37,380
into how different data techniques have

63
00:02:37,380 --> 00:02:39,750
been used to improve the processes of

64
00:02:39,750 --> 00:02:42,660
program analysis in general not only for

65
00:02:42,660 --> 00:02:44,580
finding exploits but also just for

66
00:02:44,580 --> 00:02:47,130
static analysis in general understanding

67
00:02:47,130 --> 00:02:48,630
how programs work understanding the

68
00:02:48,630 --> 00:02:53,580
nature of code execution so buck

69
00:02:53,580 --> 00:02:55,590
discovery is all about fuzzing to find

70
00:02:55,590 --> 00:02:58,170
crashes and vulnerabilities symbolic

71
00:02:58,170 --> 00:02:59,880
execution has become a huge thing in the

72
00:02:59,880 --> 00:03:02,010
past ten years or so particularly

73
00:03:02,010 --> 00:03:03,630
because there's been lots of

74
00:03:03,630 --> 00:03:05,340
computational improvements in solvers

75
00:03:05,340 --> 00:03:08,250
and so today we'll be looking at a

76
00:03:08,250 --> 00:03:09,750
plethora of different kinds of

77
00:03:09,750 --> 00:03:12,239
approaches that have been recent works

78
00:03:12,239 --> 00:03:15,900
in in the field and then also looking at

79
00:03:15,900 --> 00:03:17,700
what potential improvements that can be

80
00:03:17,700 --> 00:03:21,989
as we go on in the coming years so just

81
00:03:21,989 --> 00:03:23,670
so we can get a feel of the room how

82
00:03:23,670 --> 00:03:25,230
many folks are kind of familiar with

83
00:03:25,230 --> 00:03:28,200
program analysis with fuzzing have done

84
00:03:28,200 --> 00:03:30,510
fuzzing of any sort okay cool

85
00:03:30,510 --> 00:03:34,590
that's good that's great so for for

86
00:03:34,590 --> 00:03:35,420
those who are a little bit more

87
00:03:35,420 --> 00:03:38,610
unfamiliar this is kind of a very

88
00:03:38,610 --> 00:03:42,870
simplistic overview of the different the

89
00:03:42,870 --> 00:03:44,100
different levels of access of

90
00:03:44,100 --> 00:03:45,630
information you have to a piece of

91
00:03:45,630 --> 00:03:48,420
software or the ability for you to reach

92
00:03:48,420 --> 00:03:50,040
your hand inside a piece of software and

93
00:03:50,040 --> 00:03:52,140
a code execution flow or the compilation

94
00:03:52,140 --> 00:03:54,840
flow to interfere with how code is being

95
00:03:54,840 --> 00:03:58,290
executed in runtime and in general from

96
00:03:58,290 --> 00:04:00,930
left to right you'll see that the less

97
00:04:00,930 --> 00:04:03,209
access you have to interfere with the

98
00:04:03,209 --> 00:04:06,390
way that code is being run the less

99
00:04:06,390 --> 00:04:08,940
you're able to kind of use that insight

100
00:04:08,940 --> 00:04:11,250
to generate more intelligent ways of

101
00:04:11,250 --> 00:04:13,200
finding out how the software is being

102
00:04:13,200 --> 00:04:16,440
run and consequently you have less data

103
00:04:16,440 --> 00:04:19,160
to work off so blackbox random fuzzing

104
00:04:19,160 --> 00:04:23,070
kind of simple you have only the ability

105
00:04:23,070 --> 00:04:25,650
to execute and observe the output

106
00:04:25,650 --> 00:04:27,570
so you only see the input and output

107
00:04:27,570 --> 00:04:29,040
it's just a black box you feed in

108
00:04:29,040 --> 00:04:30,240
whatever you want to feed in and then

109
00:04:30,240 --> 00:04:31,500
you see what comes out on the other side

110
00:04:31,500 --> 00:04:33,870
it's kind of the canonical fuzzing

111
00:04:33,870 --> 00:04:36,810
approach grammar based fuzzing leverages

112
00:04:36,810 --> 00:04:39,570
some knowledge that you have about the

113
00:04:39,570 --> 00:04:43,230
software's input schema so if you know

114
00:04:43,230 --> 00:04:46,710
for example it's a PDF parser then you

115
00:04:46,710 --> 00:04:48,720
would know that okay there are certain

116
00:04:48,720 --> 00:04:50,550
formats according to the PDF spec which

117
00:04:50,550 --> 00:04:54,360
is surprisingly large that you can that

118
00:04:54,360 --> 00:04:55,950
you can shape your input so that it's

119
00:04:55,950 --> 00:04:58,800
considered valid input and and then you

120
00:04:58,800 --> 00:05:01,140
can use that as a schema for you to

121
00:05:01,140 --> 00:05:03,240
think about how you can tweak the input

122
00:05:03,240 --> 00:05:04,560
in ways that will allow you to

123
00:05:04,560 --> 00:05:06,150
potentially crash your software for

124
00:05:06,150 --> 00:05:08,700
higher probability then not great box

125
00:05:08,700 --> 00:05:10,770
instrumented fuzzing is I think becoming

126
00:05:10,770 --> 00:05:13,440
a lot more popular because of this one

127
00:05:13,440 --> 00:05:15,420
particular thing called AFL American

128
00:05:15,420 --> 00:05:17,670
fuzzy law which I'm sure everyone anyone

129
00:05:17,670 --> 00:05:19,140
who's looked into fuzzing what have

130
00:05:19,140 --> 00:05:21,330
experience working with just because of

131
00:05:21,330 --> 00:05:24,060
the generalizability of this piece of

132
00:05:24,060 --> 00:05:27,210
software the promise of it and and the

133
00:05:27,210 --> 00:05:28,800
the kind of potential for it to find

134
00:05:28,800 --> 00:05:30,390
bugs in an interesting way more

135
00:05:30,390 --> 00:05:31,650
interesting than we've seen before and

136
00:05:31,650 --> 00:05:33,180
other kinds of generalizable software

137
00:05:33,180 --> 00:05:35,430
and it uses dynamic both time

138
00:05:35,430 --> 00:05:36,630
instrumentation so during a compilation

139
00:05:36,630 --> 00:05:38,880
process you can you can think of it as

140
00:05:38,880 --> 00:05:40,620
just injecting certain hooks into the

141
00:05:40,620 --> 00:05:42,390
code execution flow so that you can

142
00:05:42,390 --> 00:05:44,040
figure out what exactly is going on and

143
00:05:44,040 --> 00:05:45,570
don't don't just have to rely on the

144
00:05:45,570 --> 00:05:47,190
input and output let's start in the end

145
00:05:47,190 --> 00:05:49,260
you can realize in intermediate chapters

146
00:05:49,260 --> 00:05:51,420
within the program execution flow so

147
00:05:51,420 --> 00:05:53,160
that you can actually find out what is

148
00:05:53,160 --> 00:05:54,570
going on at each time in the program

149
00:05:54,570 --> 00:05:58,050
execution runtime and of course the more

150
00:05:58,050 --> 00:06:01,110
access you have what white box fuzzing

151
00:06:01,110 --> 00:06:02,610
is when you have access to the source

152
00:06:02,610 --> 00:06:04,080
code when you have a lot more

153
00:06:04,080 --> 00:06:06,090
information about whatever is going on

154
00:06:06,090 --> 00:06:07,410
in the piece of code you're running and

155
00:06:07,410 --> 00:06:11,330
you can then use some other more more

156
00:06:11,330 --> 00:06:14,040
enhanced approaches to figuring out how

157
00:06:14,040 --> 00:06:15,510
you should change your input how you

158
00:06:15,510 --> 00:06:17,130
should how you should tweak the software

159
00:06:17,130 --> 00:06:18,930
execution so that you can then increase

160
00:06:18,930 --> 00:06:22,080
your likelihood of finding crashes so

161
00:06:22,080 --> 00:06:24,150
just examples of doing this like

162
00:06:24,150 --> 00:06:25,770
blackbox random passing of course like

163
00:06:25,770 --> 00:06:27,150
you can just generate random strings

164
00:06:27,150 --> 00:06:28,890
generate random bytes string them

165
00:06:28,890 --> 00:06:29,850
together and then put them to your

166
00:06:29,850 --> 00:06:32,190
software this is I don't think I don't

167
00:06:32,190 --> 00:06:33,030
think we need to talk too much about

168
00:06:33,030 --> 00:06:35,790
this grammar based fuzzing there's some

169
00:06:35,790 --> 00:06:38,040
examples not too many I think Ram fast

170
00:06:38,040 --> 00:06:39,810
is one of the more popular ones and of

171
00:06:39,810 --> 00:06:41,439
course intrumental fuzzing AFL

172
00:06:41,439 --> 00:06:44,529
with us they are the most popular ones

173
00:06:44,529 --> 00:06:46,389
and then constraint based housing wet

174
00:06:46,389 --> 00:06:49,149
box housing you know more popularly

175
00:06:49,149 --> 00:06:50,889
because of CDC the past couple of years

176
00:06:50,889 --> 00:06:52,779
anger has been coming up a lot see it's

177
00:06:52,779 --> 00:06:55,749
becoming a blot so just some examples of

178
00:06:55,749 --> 00:06:59,649
what's going on so the process of

179
00:06:59,649 --> 00:07:01,300
today's session is that we want to go

180
00:07:01,300 --> 00:07:04,269
through about eight to ten different

181
00:07:04,269 --> 00:07:07,059
papers and I only spend about one minute

182
00:07:07,059 --> 00:07:08,709
on each of them the purpose of this is

183
00:07:08,709 --> 00:07:10,389
not to give you kind of the complete

184
00:07:10,389 --> 00:07:12,069
picture of what exactly the paper is

185
00:07:12,069 --> 00:07:13,899
talking about and I'm sure I'll be doing

186
00:07:13,899 --> 00:07:16,599
a lot of these in justice because it's

187
00:07:16,599 --> 00:07:18,399
kind of impossible to cover everything

188
00:07:18,399 --> 00:07:21,459
that the researchers did to put into a

189
00:07:21,459 --> 00:07:23,860
15 page paper over a period of one year

190
00:07:23,860 --> 00:07:25,449
and then summarized in one minute but

191
00:07:25,449 --> 00:07:28,360
the general takeaway that I think I

192
00:07:28,360 --> 00:07:30,279
myself personally got and I feel like

193
00:07:30,279 --> 00:07:32,559
available to share is the trends of

194
00:07:32,559 --> 00:07:34,449
research and a trends of how these

195
00:07:34,449 --> 00:07:37,360
different snippets of time in the these

196
00:07:37,360 --> 00:07:39,249
different pieces of research have shown

197
00:07:39,249 --> 00:07:42,189
that data science in general or any kind

198
00:07:42,189 --> 00:07:45,369
of data driven approaches to use in the

199
00:07:45,369 --> 00:07:47,469
vulnerabilities cover this covering

200
00:07:47,469 --> 00:07:50,860
process has been helpful and then maybe

201
00:07:50,860 --> 00:07:53,529
influenced future research and stuff

202
00:07:53,529 --> 00:07:55,389
that I've done as well I'll talk in the

203
00:07:55,389 --> 00:07:57,699
end and and also stuff that I think

204
00:07:57,699 --> 00:07:59,199
anyone can do even if you're completely

205
00:07:59,199 --> 00:08:02,949
new to fuzzing so this is stuff that's

206
00:08:02,949 --> 00:08:04,899
kind of old 2010-2011 this is like eight

207
00:08:04,899 --> 00:08:08,169
nine year old research but I think it's

208
00:08:08,169 --> 00:08:09,339
particularly interesting when I was

209
00:08:09,339 --> 00:08:11,739
looking around in the field for how in

210
00:08:11,739 --> 00:08:15,729
particular how to define a starting

211
00:08:15,729 --> 00:08:19,239
point of when the process of fuzzing

212
00:08:19,239 --> 00:08:22,059
really became a data science subject

213
00:08:22,059 --> 00:08:23,829
instead of because in instead of just

214
00:08:23,829 --> 00:08:26,559
staying in the realm of systems then I

215
00:08:26,559 --> 00:08:29,289
think these two papers stood out in

216
00:08:29,289 --> 00:08:31,239
particular the one on the left it's by

217
00:08:31,239 --> 00:08:36,339
stefan savage at UCSD famous guy and the

218
00:08:36,339 --> 00:08:38,500
main the main idea around is is that

219
00:08:38,500 --> 00:08:40,360
it's the very canonical machine learning

220
00:08:40,360 --> 00:08:42,969
approach which is that hey over the past

221
00:08:42,969 --> 00:08:44,920
20 years there's been a bunch of people

222
00:08:44,920 --> 00:08:46,779
that spend their entire lives pulling

223
00:08:46,779 --> 00:08:47,980
their hair out looking for

224
00:08:47,980 --> 00:08:50,920
vulnerabilities by studying nightly

225
00:08:50,920 --> 00:08:53,500
releases of software and then looking at

226
00:08:53,500 --> 00:08:54,769
how does different

227
00:08:54,769 --> 00:08:55,939
software has been changing and they're

228
00:08:55,939 --> 00:08:57,559
looking for four bucks like this now

229
00:08:57,559 --> 00:08:59,300
this is obviously not scalable as

230
00:08:59,300 --> 00:09:01,579
software becomes more complicated as it

231
00:09:01,579 --> 00:09:02,929
becomes more and more of such software

232
00:09:02,929 --> 00:09:04,999
being used in critical systems and so

233
00:09:04,999 --> 00:09:06,889
how do we scale this up maybe we can

234
00:09:06,889 --> 00:09:08,149
automate this process a little bit by

235
00:09:08,149 --> 00:09:10,790
looking for the patterns and previous

236
00:09:10,790 --> 00:09:12,589
vulnerabilities we're found and then

237
00:09:12,589 --> 00:09:14,239
trying to see if we can find some

238
00:09:14,239 --> 00:09:15,709
patterns in them and then automatically

239
00:09:15,709 --> 00:09:17,389
discover them in the future so this is

240
00:09:17,389 --> 00:09:19,369
the very traditional kind of machine

241
00:09:19,369 --> 00:09:20,540
learning approach right if a bunch of

242
00:09:20,540 --> 00:09:23,029
data they're by nature labeled because

243
00:09:23,029 --> 00:09:24,589
there are things that people have found

244
00:09:24,589 --> 00:09:26,749
to be said to be suspect or vulnerable

245
00:09:26,749 --> 00:09:29,379
then you use this to find new things

246
00:09:29,379 --> 00:09:31,309
maybe it works maybe it doesn't but

247
00:09:31,309 --> 00:09:33,920
let's see the other the other study is

248
00:09:33,920 --> 00:09:36,170
kind of a there's a white side and a

249
00:09:36,170 --> 00:09:38,350
black side so this is more of the

250
00:09:38,350 --> 00:09:41,720
success that's more of a failure but

251
00:09:41,720 --> 00:09:43,759
then we'll see the differences in the

252
00:09:43,759 --> 00:09:45,439
approaches and then try to see if we can

253
00:09:45,439 --> 00:09:46,939
come up with some pattern they published

254
00:09:46,939 --> 00:09:48,499
very different results even though

255
00:09:48,499 --> 00:09:50,119
they're basically doing the same kind of

256
00:09:50,119 --> 00:09:53,149
study they are looking for vulnerability

257
00:09:53,149 --> 00:09:54,559
they're predicting vulnerability and

258
00:09:54,559 --> 00:09:56,509
also predicting time to exploit

259
00:09:56,509 --> 00:09:59,029
vulnerabilities by learning from these

260
00:09:59,029 --> 00:10:00,799
public exploit databases public

261
00:10:00,799 --> 00:10:03,829
vulnerable databases so the sandwich

262
00:10:03,829 --> 00:10:05,990
paper basically use a very very simple

263
00:10:05,990 --> 00:10:08,689
SVM model support vector machines it's

264
00:10:08,689 --> 00:10:11,569
taught in you know ml 101 classes in

265
00:10:11,569 --> 00:10:13,869
your freshman computer science class and

266
00:10:13,869 --> 00:10:17,240
it uses a bunch of different manually

267
00:10:17,240 --> 00:10:18,860
generated features so right remember

268
00:10:18,860 --> 00:10:20,749
this is 2010 there's a bunch of people

269
00:10:20,749 --> 00:10:22,339
who are all systems expert they've spent

270
00:10:22,339 --> 00:10:24,649
their entire careers analyzing binaries

271
00:10:24,649 --> 00:10:27,529
analyzing like any elf files right so

272
00:10:27,529 --> 00:10:31,160
they are they're not only using the data

273
00:10:31,160 --> 00:10:32,509
science approach but they're also

274
00:10:32,509 --> 00:10:34,279
combining it with the expert knowledge

275
00:10:34,279 --> 00:10:37,009
of how likely is this feature able to

276
00:10:37,009 --> 00:10:39,740
help me to find a vulnerability and then

277
00:10:39,740 --> 00:10:41,660
they're using that to kind of build this

278
00:10:41,660 --> 00:10:46,279
model so the kinds of features that are

279
00:10:46,279 --> 00:10:50,089
using hopefully can see this but there's

280
00:10:50,089 --> 00:10:51,919
a bunch of interesting stuff as much of

281
00:10:51,919 --> 00:10:54,379
not so interesting stuff but there are

282
00:10:54,379 --> 00:10:56,089
things that if you are researcher

283
00:10:56,089 --> 00:10:59,540
looking for bugs you would probably try

284
00:10:59,540 --> 00:11:01,160
to generate these yourselves anyway

285
00:11:01,160 --> 00:11:03,230
maybe plot them out maybe just look at

286
00:11:03,230 --> 00:11:04,699
the values of them and try to see how

287
00:11:04,699 --> 00:11:06,410
likely is it that this software is to be

288
00:11:06,410 --> 00:11:07,760
buggy given your your

289
00:11:07,760 --> 00:11:13,190
experience so they use that they use

290
00:11:13,190 --> 00:11:15,050
this this approach they use this mainly

291
00:11:15,050 --> 00:11:16,940
generated nine ninety plus thousand

292
00:11:16,940 --> 00:11:19,310
features and then they claim to have

293
00:11:19,310 --> 00:11:20,390
really good results for generating

294
00:11:20,390 --> 00:11:24,230
predictions the other one claims a very

295
00:11:24,230 --> 00:11:25,520
bad results for generating predictions

296
00:11:25,520 --> 00:11:27,890
they use the same source of information

297
00:11:27,890 --> 00:11:30,920
it's the nvd CBS CBS s public

298
00:11:30,920 --> 00:11:33,290
vulnerability reports and the things

299
00:11:33,290 --> 00:11:36,110
that there is much less talk on data

300
00:11:36,110 --> 00:11:39,830
validation on cleaning and it cites data

301
00:11:39,830 --> 00:11:41,500
quality actually as the main reason for

302
00:11:41,500 --> 00:11:44,720
the failure to predict reliably the time

303
00:11:44,720 --> 00:11:46,490
to exploit so the time to export

304
00:11:46,490 --> 00:11:48,770
prediction is basically given that you

305
00:11:48,770 --> 00:11:50,630
have a vulnerability being reported and

306
00:11:50,630 --> 00:11:52,850
then you have exploits being reported as

307
00:11:52,850 --> 00:11:55,250
well what is the time range between them

308
00:11:55,250 --> 00:11:57,350
and given that in the past you have

309
00:11:57,350 --> 00:11:59,330
visibility over a time range when a

310
00:11:59,330 --> 00:12:00,620
vulnerability is reported and then when

311
00:12:00,620 --> 00:12:02,390
it's exploited then maybe you can

312
00:12:02,390 --> 00:12:04,880
predict how likely it is that a yet to

313
00:12:04,880 --> 00:12:06,590
be explored that vulnerability is going

314
00:12:06,590 --> 00:12:07,880
to be exploited and then you naturally

315
00:12:07,880 --> 00:12:09,710
have that feedback loop when that when

316
00:12:09,710 --> 00:12:11,540
you then get the exploit cycle

317
00:12:11,540 --> 00:12:15,860
reported so I think there's only about a

318
00:12:15,860 --> 00:12:18,110
40% success so I less than random chance

319
00:12:18,110 --> 00:12:22,400
and the general takeaway of comparing

320
00:12:22,400 --> 00:12:24,020
just these two papers is that data

321
00:12:24,020 --> 00:12:26,600
quality is everything so there tends to

322
00:12:26,600 --> 00:12:28,700
be a lot of you know but whenever I

323
00:12:28,700 --> 00:12:30,680
whenever I talk to folks about okay this

324
00:12:30,680 --> 00:12:33,620
is this is what what I do like I I look

325
00:12:33,620 --> 00:12:35,060
at different algorithms and and figure

326
00:12:35,060 --> 00:12:36,470
out if it's good for this problem or not

327
00:12:36,470 --> 00:12:38,750
and and then they say okay so so like

328
00:12:38,750 --> 00:12:40,490
well how do you how do you write your

329
00:12:40,490 --> 00:12:42,110
machine learning algorithms and the

330
00:12:42,110 --> 00:12:43,610
truth that no one no one really writes

331
00:12:43,610 --> 00:12:45,110
new algorithms all the time they're just

332
00:12:45,110 --> 00:12:47,180
using the same few algorithms using the

333
00:12:47,180 --> 00:12:48,440
neural nets are just using literally the

334
00:12:48,440 --> 00:12:49,910
same piece of software and understanding

335
00:12:49,910 --> 00:12:51,340
parameters to make things happen and

336
00:12:51,340 --> 00:12:53,870
most of the time is spent on cleaning

337
00:12:53,870 --> 00:12:56,930
data for a good reason just because it

338
00:12:56,930 --> 00:12:58,910
matters a lot more what kind of a they

339
00:12:58,910 --> 00:13:00,380
are using how your wrangling our data

340
00:13:00,380 --> 00:13:01,970
and what kind of what kind of input

341
00:13:01,970 --> 00:13:04,850
you're putting into these systems in

342
00:13:04,850 --> 00:13:07,160
order to train semi-intelligent or

343
00:13:07,160 --> 00:13:09,350
pseudo intelligent systems that you

344
00:13:09,350 --> 00:13:11,990
think are doing automation in a in a

345
00:13:11,990 --> 00:13:15,140
less than that right way so that is the

346
00:13:15,140 --> 00:13:17,000
that is the first kind of basis that

347
00:13:17,000 --> 00:13:18,740
that we want to set on course we're just

348
00:13:18,740 --> 00:13:21,080
the other quality is everything so given

349
00:13:21,080 --> 00:13:21,260
the

350
00:13:21,260 --> 00:13:24,620
all of the all of the research in using

351
00:13:24,620 --> 00:13:26,480
for using data science for program

352
00:13:26,480 --> 00:13:28,850
analysis has been looked at the the only

353
00:13:28,850 --> 00:13:33,110
ones that kind of are are valid in my

354
00:13:33,110 --> 00:13:35,060
mind or the only ones that kind of make

355
00:13:35,060 --> 00:13:39,080
sense as a grounds of off of comparison

356
00:13:39,080 --> 00:13:41,570
between each other and to use as a data

357
00:13:41,570 --> 00:13:43,190
point in this in this kind of tenure

358
00:13:43,190 --> 00:13:46,730
trajectory is that we want to look at

359
00:13:46,730 --> 00:13:51,110
only research pieces that have put more

360
00:13:51,110 --> 00:13:53,750
thought into how to generating their

361
00:13:53,750 --> 00:13:55,610
data sets into what kind of data sources

362
00:13:55,610 --> 00:13:57,980
start using and in general I think this

363
00:13:57,980 --> 00:13:59,780
has improved a lot in the last 10 years

364
00:13:59,780 --> 00:14:04,060
but then early on it wasn't the case so

365
00:14:04,060 --> 00:14:06,350
let's take a step back and look at the

366
00:14:06,350 --> 00:14:08,030
problems to solve in fuzzing in general

367
00:14:08,030 --> 00:14:09,860
right so you have this piece of software

368
00:14:09,860 --> 00:14:11,750
it doesn't matter how much access you

369
00:14:11,750 --> 00:14:15,770
have to it but the main problems are

370
00:14:15,770 --> 00:14:19,160
that fuzzing takes too long right if you

371
00:14:19,160 --> 00:14:20,480
have a simple piece of software to us

372
00:14:20,480 --> 00:14:22,370
sure you can come up with some random

373
00:14:22,370 --> 00:14:23,810
seats even if you know nothing about the

374
00:14:23,810 --> 00:14:26,030
program and you can find something

375
00:14:26,030 --> 00:14:29,030
pretty reliably pretty simply but the

376
00:14:29,030 --> 00:14:31,010
truth is that most of these things are

377
00:14:31,010 --> 00:14:32,240
found on people if they're worth any

378
00:14:32,240 --> 00:14:36,020
money at all already and you don't

379
00:14:36,020 --> 00:14:37,130
really want to find these simple cases

380
00:14:37,130 --> 00:14:38,600
you want to find an interesting case as

381
00:14:38,600 --> 00:14:41,180
the ones that kind of are meaningful and

382
00:14:41,180 --> 00:14:44,510
are not you know quote unquote trivial

383
00:14:44,510 --> 00:14:46,700
enough that they can be found by just

384
00:14:46,700 --> 00:14:48,170
someone looking at the code or just

385
00:14:48,170 --> 00:14:51,160
someone playing around the program so

386
00:14:51,160 --> 00:14:53,660
the main causes test case generation

387
00:14:53,660 --> 00:14:56,300
right the entire the entire idea around

388
00:14:56,300 --> 00:14:58,250
is is that you want to feed test cases

389
00:14:58,250 --> 00:15:00,410
into this software such that you cause

390
00:15:00,410 --> 00:15:04,220
some unexpected output and then taking a

391
00:15:04,220 --> 00:15:06,560
step back from there the test cases have

392
00:15:06,560 --> 00:15:08,060
to be generated somehow automatically

393
00:15:08,060 --> 00:15:09,380
right because you're not the one that's

394
00:15:09,380 --> 00:15:10,760
generating the test cases yourself

395
00:15:10,760 --> 00:15:13,730
manually the some kind of algorithm is

396
00:15:13,730 --> 00:15:16,430
doing that so this this is kind of

397
00:15:16,430 --> 00:15:18,740
coarsely named input mutation strategy

398
00:15:18,740 --> 00:15:21,800
where you basically have a seed input or

399
00:15:21,800 --> 00:15:24,850
a set of seed inputs that you use as as

400
00:15:24,850 --> 00:15:27,710
kind of your starter set and then from

401
00:15:27,710 --> 00:15:29,540
there you're changing this input in in

402
00:15:29,540 --> 00:15:32,630
some ways that allow you to to maximize

403
00:15:32,630 --> 00:15:34,640
your likelihood of finding crashes

404
00:15:34,640 --> 00:15:36,950
sooner rather than later and then also

405
00:15:36,950 --> 00:15:38,560
if you look at it a little bit more

406
00:15:38,560 --> 00:15:41,240
philosophically and and zoom out into

407
00:15:41,240 --> 00:15:43,370
what it means to be executing a program

408
00:15:43,370 --> 00:15:47,330
or writing code and each of each of your

409
00:15:47,330 --> 00:15:49,640
lines of code or or or each line of

410
00:15:49,640 --> 00:15:52,520
logic is executed then you can actually

411
00:15:52,520 --> 00:15:54,680
visualize it as some kind of execution

412
00:15:54,680 --> 00:15:57,350
tree where each conditional Clause each

413
00:15:57,350 --> 00:16:01,430
each kind of loop statement actually can

414
00:16:01,430 --> 00:16:03,230
be branched out and you can follow that

415
00:16:03,230 --> 00:16:07,670
logic sequentially imperatively so the

416
00:16:07,670 --> 00:16:09,440
idea is that if you can exercise all

417
00:16:09,440 --> 00:16:12,830
possible routes exhaustive exhaustively

418
00:16:12,830 --> 00:16:16,630
then you can with with caveats

419
00:16:16,630 --> 00:16:19,370
potentially find all kinds of edge cases

420
00:16:19,370 --> 00:16:21,920
in the software right and so why don't

421
00:16:21,920 --> 00:16:23,870
people do that because most non-trivial

422
00:16:23,870 --> 00:16:27,670
software of course have huge huge huge

423
00:16:27,670 --> 00:16:30,860
execution trees and so you're not able

424
00:16:30,860 --> 00:16:34,130
to do that all the time and so the issue

425
00:16:34,130 --> 00:16:35,990
is if you have unlimited time of course

426
00:16:35,990 --> 00:16:38,510
you can you can enumerate over all of

427
00:16:38,510 --> 00:16:41,180
these cases but that's always not the

428
00:16:41,180 --> 00:16:42,470
case you don't have a little that time

429
00:16:42,470 --> 00:16:44,120
you don't have a lot of resources so

430
00:16:44,120 --> 00:16:46,610
it's really getting to the crash we have

431
00:16:46,610 --> 00:16:48,770
higher likelihood as soon as possible as

432
00:16:48,770 --> 00:16:49,700
quickly as possible

433
00:16:49,700 --> 00:16:52,880
and then the last thing is you know the

434
00:16:52,880 --> 00:16:54,140
difference between exploit ability and

435
00:16:54,140 --> 00:16:56,000
vulnerability only I'll go too deeply

436
00:16:56,000 --> 00:16:57,890
into this but there's some interesting

437
00:16:57,890 --> 00:17:02,270
work done in this area too so it's kind

438
00:17:02,270 --> 00:17:04,040
of an intimidating diagram but the way I

439
00:17:04,040 --> 00:17:06,949
split up the the field is that there

440
00:17:06,949 --> 00:17:08,869
tends to be two general classes of

441
00:17:08,869 --> 00:17:10,459
research one is generative one is

442
00:17:10,459 --> 00:17:12,829
predictive and the generative ones I

443
00:17:12,829 --> 00:17:14,890
think are generally a little bit more

444
00:17:14,890 --> 00:17:17,209
exciting just because it allows you to

445
00:17:17,209 --> 00:17:20,000
action on it in some way generative

446
00:17:20,000 --> 00:17:23,720
approaches mainly mean that you are able

447
00:17:23,720 --> 00:17:26,030
to run the algorithm that generates some

448
00:17:26,030 --> 00:17:28,490
input that helps you to come up with

449
00:17:28,490 --> 00:17:31,280
crashed test cases a lot faster than

450
00:17:31,280 --> 00:17:32,120
without

451
00:17:32,120 --> 00:17:35,000
so it's measurable because you can try

452
00:17:35,000 --> 00:17:36,650
using these test cases generated by

453
00:17:36,650 --> 00:17:38,780
these algorithms you can try using test

454
00:17:38,780 --> 00:17:39,740
cases generated by some other

455
00:17:39,740 --> 00:17:41,660
implementation strategy then it gonna be

456
00:17:41,660 --> 00:17:43,790
compare the time to crash or the time

457
00:17:43,790 --> 00:17:46,130
defining a vulnerability or when you

458
00:17:46,130 --> 00:17:47,059
crash and

459
00:17:47,059 --> 00:17:49,309
then that's that's your metric in

460
00:17:49,309 --> 00:17:51,740
predictive approaches it's a little bit

461
00:17:51,740 --> 00:17:52,549
different it's a little bit more

462
00:17:52,549 --> 00:17:55,999
indirect so it's more of the analytical

463
00:17:55,999 --> 00:17:57,440
side of the problem where you're looking

464
00:17:57,440 --> 00:17:59,570
for parts of the software are more

465
00:17:59,570 --> 00:18:01,220
likely to contain crashes or

466
00:18:01,220 --> 00:18:03,019
vulnerabilities you're looking for

467
00:18:03,019 --> 00:18:04,820
exploit ability of a crash you're

468
00:18:04,820 --> 00:18:07,519
looking for the time to exploit and then

469
00:18:07,519 --> 00:18:10,039
the other side of it is is a little bit

470
00:18:10,039 --> 00:18:11,749
more in the in a theoretical realm where

471
00:18:11,749 --> 00:18:14,629
you're looking for how you how you

472
00:18:14,629 --> 00:18:17,330
select state when you are choosing which

473
00:18:17,330 --> 00:18:19,990
package which path to branch down into

474
00:18:19,990 --> 00:18:22,970
when you switch between fuzzing and the

475
00:18:22,970 --> 00:18:25,220
other approach when you are using a kind

476
00:18:25,220 --> 00:18:27,919
of a dual engine buzzing methodology

477
00:18:27,919 --> 00:18:28,970
which we'll look at a little later as

478
00:18:28,970 --> 00:18:34,009
well so let's start with with the first

479
00:18:34,009 --> 00:18:36,049
one which personally was the first paper

480
00:18:36,049 --> 00:18:38,299
that I that I read in this area which

481
00:18:38,299 --> 00:18:41,869
was pretty interesting so you can

482
00:18:41,869 --> 00:18:43,100
actually try the software out it's

483
00:18:43,100 --> 00:18:46,460
called V discover or the visa and and if

484
00:18:46,460 --> 00:18:47,720
you run it actually that doesn't work

485
00:18:47,720 --> 00:18:49,460
that well in practice but but the idea

486
00:18:49,460 --> 00:18:52,340
around it is it's interesting so the

487
00:18:52,340 --> 00:18:55,100
thing is that they split this entire did

488
00:18:55,100 --> 00:18:56,899
this entire process of extracting

489
00:18:56,899 --> 00:18:59,059
features from binaries up in two steps

490
00:18:59,059 --> 00:19:02,090
the first step is you want to extract

491
00:19:02,090 --> 00:19:04,100
static and binary features from from

492
00:19:04,100 --> 00:19:06,259
binaries static and dynamic features

493
00:19:06,259 --> 00:19:08,299
from binaries by this imploding in some

494
00:19:08,299 --> 00:19:09,830
kind of sandbox and then just analyzing

495
00:19:09,830 --> 00:19:13,029
static features and then you use this to

496
00:19:13,029 --> 00:19:16,100
prioritize your test cases so the main

497
00:19:16,100 --> 00:19:17,899
thing is that you have a bunch of test

498
00:19:17,899 --> 00:19:20,389
cases already generated beforehand and

499
00:19:20,389 --> 00:19:22,519
you want to figure out how you can queue

500
00:19:22,519 --> 00:19:24,019
them up in a way that allows you to find

501
00:19:24,019 --> 00:19:26,360
things better so the assumption behind

502
00:19:26,360 --> 00:19:27,470
this is that you have some way of

503
00:19:27,470 --> 00:19:28,999
already generating pretty good test

504
00:19:28,999 --> 00:19:30,769
cases exhaustively and you just want to

505
00:19:30,769 --> 00:19:33,440
you just want to prioritize them into

506
00:19:33,440 --> 00:19:35,509
the ones that will likely cause you to

507
00:19:35,509 --> 00:19:37,669
find interesting things and not just run

508
00:19:37,669 --> 00:19:42,619
and die so it used the mayhem teams in

509
00:19:42,619 --> 00:19:45,590
ball executors and then it use pretty

510
00:19:45,590 --> 00:19:47,779
pretty simple models NLP multi-layer

511
00:19:47,779 --> 00:19:50,149
perceptrons the kind of precursor to

512
00:19:50,149 --> 00:19:52,159
neural networks and random force to do

513
00:19:52,159 --> 00:19:55,250
predictions and it was able to find some

514
00:19:55,250 --> 00:19:57,440
pretty interesting test cases and yeah

515
00:19:57,440 --> 00:19:59,760
the thing the thing around is is that

516
00:19:59,760 --> 00:20:02,970
they used a kind of naive way to look at

517
00:20:02,970 --> 00:20:04,889
the entire problem because if you think

518
00:20:04,889 --> 00:20:07,350
about if you think about the process of

519
00:20:07,350 --> 00:20:09,659
let's say malware malware classification

520
00:20:09,659 --> 00:20:11,490
this is kind of the way you look at it

521
00:20:11,490 --> 00:20:13,529
right because you want to look for

522
00:20:13,529 --> 00:20:15,690
different artifacts in a binary to allow

523
00:20:15,690 --> 00:20:19,200
you to classify this binary in in into

524
00:20:19,200 --> 00:20:21,480
some category the category and a very

525
00:20:21,480 --> 00:20:22,799
coarse nature could be whether it's

526
00:20:22,799 --> 00:20:25,470
malicious or not in in a more

527
00:20:25,470 --> 00:20:27,090
fine-grained nature it could be does

528
00:20:27,090 --> 00:20:28,620
this binary belong to this malware a

529
00:20:28,620 --> 00:20:30,779
family that just exhibits this kind of

530
00:20:30,779 --> 00:20:34,380
characteristic but the step back from

531
00:20:34,380 --> 00:20:36,990
from this paper is is asking a deeper

532
00:20:36,990 --> 00:20:39,870
question which is that is there any kind

533
00:20:39,870 --> 00:20:42,419
of is there any kind of characteristic

534
00:20:42,419 --> 00:20:46,529
of input cases or software features that

535
00:20:46,529 --> 00:20:49,559
indicate its vulnerability or is that

536
00:20:49,559 --> 00:20:52,380
part is the possible vault is there even

537
00:20:52,380 --> 00:20:55,409
a pattern that can kind of you can kind

538
00:20:55,409 --> 00:20:59,519
of look for in general such that you

539
00:20:59,519 --> 00:21:02,789
will know with some some surety that

540
00:21:02,789 --> 00:21:04,860
this piece of software contains a bug

541
00:21:04,860 --> 00:21:07,950
and I think the conclusion from that

542
00:21:07,950 --> 00:21:10,710
paper was that it is an inconclusive no

543
00:21:10,710 --> 00:21:14,070
and and the idea behind it is that the

544
00:21:14,070 --> 00:21:16,529
the entire class of vulnerable O's of

545
00:21:16,529 --> 00:21:19,350
vulnerable software is so huge that it

546
00:21:19,350 --> 00:21:20,610
becomes more of an anomaly detection

547
00:21:20,610 --> 00:21:22,500
problem what you're looking for

548
00:21:22,500 --> 00:21:24,659
classifying the unknowns and the knowns

549
00:21:24,659 --> 00:21:26,760
where the unknown class is so huge and

550
00:21:26,760 --> 00:21:29,279
so massive that you cannot possibly come

551
00:21:29,279 --> 00:21:31,350
up with characteristics to classify them

552
00:21:31,350 --> 00:21:33,750
and so that I think in general is the

553
00:21:33,750 --> 00:21:36,149
conclusion of this this paper which is

554
00:21:36,149 --> 00:21:38,220
that it proved you cannot use such an

555
00:21:38,220 --> 00:21:39,659
approach to think about the problem of

556
00:21:39,659 --> 00:21:43,740
input test case generation the other

557
00:21:43,740 --> 00:21:48,950
thing is thinking of input cases less of

558
00:21:49,039 --> 00:21:52,470
thinking of input cases less in a sense

559
00:21:52,470 --> 00:21:54,809
of randomly changing things about but

560
00:21:54,809 --> 00:21:57,899
combining fuzzing with with grammar

561
00:21:57,899 --> 00:22:00,779
based learning so when you look at for

562
00:22:00,779 --> 00:22:02,279
example the PDF parser case that I

563
00:22:02,279 --> 00:22:04,950
mentioned earlier then you can see that

564
00:22:04,950 --> 00:22:06,330
they're actually certain ways they can

565
00:22:06,330 --> 00:22:09,210
mutate the input Det can generate more

566
00:22:09,210 --> 00:22:12,600
likely than not valid PDFs or valid

567
00:22:12,600 --> 00:22:13,340
schemas

568
00:22:13,340 --> 00:22:15,980
for the for for this for this particular

569
00:22:15,980 --> 00:22:18,020
program that you are feeding it into and

570
00:22:18,020 --> 00:22:23,000
its it kind of uses the kind of already

571
00:22:23,000 --> 00:22:25,610
old idea of sequence of sequence which

572
00:22:25,610 --> 00:22:27,590
is what's being used for most natural

573
00:22:27,590 --> 00:22:31,250
language translation so if you if you go

574
00:22:31,250 --> 00:22:33,380
to Google Translate today they actually

575
00:22:33,380 --> 00:22:35,810
shifted from more you know intricately

576
00:22:35,810 --> 00:22:38,680
built language models to this kind of

577
00:22:38,680 --> 00:22:40,940
seemingly dumb approach of sequence

578
00:22:40,940 --> 00:22:43,490
sequence which is the idea that instead

579
00:22:43,490 --> 00:22:45,770
of instead of trying to understand what

580
00:22:45,770 --> 00:22:47,360
exactly ascended is saying and then

581
00:22:47,360 --> 00:22:49,400
translating this sentence into from

582
00:22:49,400 --> 00:22:52,340
English to French what you're doing is

583
00:22:52,340 --> 00:22:54,530
that you're feeding the english-language

584
00:22:54,530 --> 00:22:56,600
version of the of the sentence and the

585
00:22:56,600 --> 00:22:58,610
French version of the sentence side by

586
00:22:58,610 --> 00:23:01,610
side into a model and then you're

587
00:23:01,610 --> 00:23:03,620
letting the model learn that hey if this

588
00:23:03,620 --> 00:23:06,020
if this sentence comes first then the

589
00:23:06,020 --> 00:23:07,310
French sentence this is the French

590
00:23:07,310 --> 00:23:09,740
sentence that comes next so that's

591
00:23:09,740 --> 00:23:12,620
that's kind of using brute force data to

592
00:23:12,620 --> 00:23:14,600
solve the problem and that's in general

593
00:23:14,600 --> 00:23:16,250
the trend of the last 20 years which is

594
00:23:16,250 --> 00:23:18,260
that there's no point investing in

595
00:23:18,260 --> 00:23:19,850
better algorithms when we have so much

596
00:23:19,850 --> 00:23:23,000
data and improving data by 10% gets you

597
00:23:23,000 --> 00:23:25,130
a 10x improvement improving algorithms

598
00:23:25,130 --> 00:23:28,270
by 10% gets you a 10% improvement so

599
00:23:28,270 --> 00:23:31,280
that's why that's why everything is

600
00:23:31,280 --> 00:23:34,850
going this direction but this paper is

601
00:23:34,850 --> 00:23:35,900
interesting because it's kind of the

602
00:23:35,900 --> 00:23:37,340
first approach the first meaningful

603
00:23:37,340 --> 00:23:39,560
approach of using neural networks the

604
00:23:39,560 --> 00:23:42,680
disco learning to to automate the

605
00:23:42,680 --> 00:23:45,710
traditional input generation and and it

606
00:23:45,710 --> 00:23:47,120
has pretty interesting results so it was

607
00:23:47,120 --> 00:23:49,850
able to mutate a complex PDF file if you

608
00:23:49,850 --> 00:23:51,590
think about a PDF file right it's it's

609
00:23:51,590 --> 00:23:54,410
actually a lot more calm more complex in

610
00:23:54,410 --> 00:23:59,410
structure than most languages and well

611
00:23:59,410 --> 00:24:02,600
that may not be true I think it's a lot

612
00:24:02,600 --> 00:24:05,440
more complex and difficult to get right

613
00:24:05,440 --> 00:24:09,950
because it is the the the criterion for

614
00:24:09,950 --> 00:24:12,470
getting a PDF a sequence of bytes that

615
00:24:12,470 --> 00:24:16,670
constitute valid PDF is a lot less is a

616
00:24:16,670 --> 00:24:18,770
lot it's a lot more sensitive to

617
00:24:18,770 --> 00:24:20,600
correctness and in correctness than in

618
00:24:20,600 --> 00:24:22,820
English sentence so you can generate an

619
00:24:22,820 --> 00:24:24,440
English sentence with an extra word in

620
00:24:24,440 --> 00:24:26,390
there and people would still recognize

621
00:24:26,390 --> 00:24:26,670
it

622
00:24:26,670 --> 00:24:28,140
English sentence whereas if you had an

623
00:24:28,140 --> 00:24:30,180
extra random bite somewhere then it'll

624
00:24:30,180 --> 00:24:33,630
be corrupted in a file so this was able

625
00:24:33,630 --> 00:24:36,120
to mutate complex PDF files pretty

626
00:24:36,120 --> 00:24:38,280
reliably and pretty well which led to a

627
00:24:38,280 --> 00:24:41,610
future work in this space so quantifying

628
00:24:41,610 --> 00:24:43,740
software exploitability uses some of

629
00:24:43,740 --> 00:24:47,070
this idea and generate a score for how

630
00:24:47,070 --> 00:24:49,500
exploitable a piece of software is it

631
00:24:49,500 --> 00:24:51,630
uses the features generated and that use

632
00:24:51,630 --> 00:24:54,510
a sequential input sequential input

633
00:24:54,510 --> 00:24:56,550
schemas and grammar in order to generate

634
00:24:56,550 --> 00:25:00,330
a priori beliefs into how likely this

635
00:25:00,330 --> 00:25:03,690
piece of binary is to be to be able to

636
00:25:03,690 --> 00:25:05,160
be exploitable and then it feeds into

637
00:25:05,160 --> 00:25:09,030
this system and and it really kind of

638
00:25:09,030 --> 00:25:11,310
puts puts the idea that you can you can

639
00:25:11,310 --> 00:25:13,860
rank software by how vulnerable they are

640
00:25:13,860 --> 00:25:16,920
before even running them with a phaser

641
00:25:16,920 --> 00:25:20,310
and then use this to kind of determine

642
00:25:20,310 --> 00:25:22,020
how many how much resources you want to

643
00:25:22,020 --> 00:25:25,620
spend on on on passing this piece of

644
00:25:25,620 --> 00:25:27,990
software so for example if if this if

645
00:25:27,990 --> 00:25:30,330
this algorithm tells you that it's only

646
00:25:30,330 --> 00:25:32,250
20 percent likely that you're gonna find

647
00:25:32,250 --> 00:25:34,140
some exploit then maybe you only spend

648
00:25:34,140 --> 00:25:36,240
20 percent of your resources and fuzz it

649
00:25:36,240 --> 00:25:37,860
for a fifth of the time that you file

650
00:25:37,860 --> 00:25:39,600
some other software it is most likely to

651
00:25:39,600 --> 00:25:43,950
before you find something and then whose

652
00:25:43,950 --> 00:25:49,500
heart of gans here cool yeah so Gant's

653
00:25:49,500 --> 00:25:51,960
are kind of this this this concept of

654
00:25:51,960 --> 00:25:53,430
generative extra networks

655
00:25:53,430 --> 00:25:55,380
it's the idea behind all that big stuff

656
00:25:55,380 --> 00:25:57,570
you're seeing it's it is there is the

657
00:25:57,570 --> 00:26:01,260
idea behind generating realistic images

658
00:26:01,260 --> 00:26:04,050
by putting two neural networks at at

659
00:26:04,050 --> 00:26:05,850
competition of one another or an

660
00:26:05,850 --> 00:26:08,010
opposition of one another where the two

661
00:26:08,010 --> 00:26:12,090
neural networks are basically set up

662
00:26:12,090 --> 00:26:14,490
with different objectives one is setting

663
00:26:14,490 --> 00:26:16,380
it one is set up with the objective of

664
00:26:16,380 --> 00:26:19,440
generating input that you can think of

665
00:26:19,440 --> 00:26:21,510
as counterfeit dollar bills and the

666
00:26:21,510 --> 00:26:22,950
other one is sort of the objective of

667
00:26:22,950 --> 00:26:24,900
determining which dollar bills that the

668
00:26:24,900 --> 00:26:26,490
other network generator are counterfeit

669
00:26:26,490 --> 00:26:28,710
so if you think of the process of

670
00:26:28,710 --> 00:26:30,270
training these two networks over and

671
00:26:30,270 --> 00:26:32,940
over again then you can imagine

672
00:26:32,940 --> 00:26:35,670
potentially that these will eventually

673
00:26:35,670 --> 00:26:38,700
form a self-improving loop and generate

674
00:26:38,700 --> 00:26:39,450
input

675
00:26:39,450 --> 00:26:42,269
generate samples that are so realistic

676
00:26:42,269 --> 00:26:44,720
and that that's exactly what happened

677
00:26:44,720 --> 00:26:46,889
people just threw a bunch of data at a

678
00:26:46,889 --> 00:26:48,510
problem through a bunch of GPUs at a

679
00:26:48,510 --> 00:26:50,130
problem and realized that they could

680
00:26:50,130 --> 00:26:52,289
generate super realistic looking fake

681
00:26:52,289 --> 00:26:55,529
human faces and think everything that

682
00:26:55,529 --> 00:26:57,480
that looks exactly like what you might

683
00:26:57,480 --> 00:27:00,809
imagine to be non computer-generated so

684
00:27:00,809 --> 00:27:02,730
this is a little bit less interesting of

685
00:27:02,730 --> 00:27:04,139
a problem we're not generating the fix

686
00:27:04,139 --> 00:27:06,899
here but this improves the performance

687
00:27:06,899 --> 00:27:10,019
of AFL by generating seed files based on

688
00:27:10,019 --> 00:27:14,159
based on Gans that's the idea so you

689
00:27:14,159 --> 00:27:16,320
know maybe some kind of some kind of

690
00:27:16,320 --> 00:27:19,019
program analysis Nord is trying to do

691
00:27:19,019 --> 00:27:20,639
this is the you know doing something

692
00:27:20,639 --> 00:27:22,500
that can actually show up in papers but

693
00:27:22,500 --> 00:27:25,500
I mean newspapers but this helps a FX

694
00:27:25,500 --> 00:27:28,049
exercise deeper paths and and and fine

695
00:27:28,049 --> 00:27:30,840
fine more fine quicker exploits compared

696
00:27:30,840 --> 00:27:32,760
to the default random strategy and then

697
00:27:32,760 --> 00:27:35,490
we'll go a little bit into ASL later

698
00:27:35,490 --> 00:27:39,510
we're where we want to try to dig a

699
00:27:39,510 --> 00:27:41,519
little bit into into what AFL actually

700
00:27:41,519 --> 00:27:43,950
is and then and then try to see if we

701
00:27:43,950 --> 00:27:46,529
can actually improve it using this as as

702
00:27:46,529 --> 00:27:52,320
a as kind of a seed before we do that

703
00:27:52,320 --> 00:27:54,389
there's a couple more interesting papers

704
00:27:54,389 --> 00:27:55,440
that I thought interesting to mention

705
00:27:55,440 --> 00:28:00,570
this is part of anger this is part of of

706
00:28:00,570 --> 00:28:02,809
the the UCS B's

707
00:28:02,809 --> 00:28:07,230
CGC approach where they actually develop

708
00:28:07,230 --> 00:28:09,840
this thing called driller which doesn't

709
00:28:09,840 --> 00:28:12,330
use any machine learning but it's an

710
00:28:12,330 --> 00:28:13,260
interesting approach to think about a

711
00:28:13,260 --> 00:28:15,149
problem so if you think about the

712
00:28:15,149 --> 00:28:16,740
problem of fuzzing what I'm actually

713
00:28:16,740 --> 00:28:20,909
doing is starting from the the the input

714
00:28:20,909 --> 00:28:24,620
of the program you're trying to trace a

715
00:28:24,620 --> 00:28:28,559
increasingly X exploding path execution

716
00:28:28,559 --> 00:28:30,659
tree and you're trying to figure out how

717
00:28:30,659 --> 00:28:33,210
you can optimize your fuzzing approach

718
00:28:33,210 --> 00:28:35,730
your fuzzing strategy your inputs such

719
00:28:35,730 --> 00:28:37,529
that you can exercise more of this tree

720
00:28:37,529 --> 00:28:39,809
and then you don't have to spend more

721
00:28:39,809 --> 00:28:42,029
resources fussing more time fuzzing the

722
00:28:42,029 --> 00:28:44,490
problem is that there are some are

723
00:28:44,490 --> 00:28:47,399
process that allow you to to you know

724
00:28:47,399 --> 00:28:49,500
generate analytical outputs from

725
00:28:49,500 --> 00:28:51,570
programs a lot more reliably compared to

726
00:28:51,570 --> 00:28:52,670
fuzzing

727
00:28:52,670 --> 00:28:55,150
this approach called symbolic execution

728
00:28:55,150 --> 00:28:58,100
basically at a very high level tries to

729
00:28:58,100 --> 00:29:01,340
convert a program into an analytical

730
00:29:01,340 --> 00:29:04,730
problem so that you can you can you can

731
00:29:04,730 --> 00:29:07,630
express the output of the program as a

732
00:29:07,630 --> 00:29:11,390
algebraic expression constituting of the

733
00:29:11,390 --> 00:29:14,179
input and then using that you can then

734
00:29:14,179 --> 00:29:16,520
use mathematical equations to solve it

735
00:29:16,520 --> 00:29:18,860
and solve for what kind of input will

736
00:29:18,860 --> 00:29:21,590
cause a discontinuity in this equation

737
00:29:21,590 --> 00:29:24,080
and cause of kind of crash and the

738
00:29:24,080 --> 00:29:26,360
problem with that of course is that most

739
00:29:26,360 --> 00:29:28,850
large programs cannot be solved with

740
00:29:28,850 --> 00:29:30,860
this because it's just too too

741
00:29:30,860 --> 00:29:32,750
computationally intensive to to

742
00:29:32,750 --> 00:29:34,850
analytically intensive and you know the

743
00:29:34,850 --> 00:29:37,970
XIII solver is one of the the more

744
00:29:37,970 --> 00:29:40,660
popular and more successful

745
00:29:40,660 --> 00:29:43,880
implementations of this but still it

746
00:29:43,880 --> 00:29:46,190
only manages to deal with fairly simple

747
00:29:46,190 --> 00:29:48,440
programs so the idea is that maybe we

748
00:29:48,440 --> 00:29:50,270
can fast to a certain point a certain

749
00:29:50,270 --> 00:29:52,790
sub tree in this in this program and

750
00:29:52,790 --> 00:29:56,179
then from that point onwards we we run

751
00:29:56,179 --> 00:29:58,070
some tsubame execution on that much

752
00:29:58,070 --> 00:30:00,740
smaller subtree in a program if we see

753
00:30:00,740 --> 00:30:01,970
something that could indicate that

754
00:30:01,970 --> 00:30:04,460
there's something very likely to be

755
00:30:04,460 --> 00:30:07,309
found here and so in that case then we

756
00:30:07,309 --> 00:30:10,010
don't have to potentially run the solver

757
00:30:10,010 --> 00:30:12,380
on the much larger global tree and can

758
00:30:12,380 --> 00:30:15,140
run it on a smaller smaller set of trees

759
00:30:15,140 --> 00:30:17,480
which we can then paralyze which we can

760
00:30:17,480 --> 00:30:20,240
then spend much fewer resources solving

761
00:30:20,240 --> 00:30:24,850
for and it actually works pretty well so

762
00:30:25,840 --> 00:30:28,370
in the interest of time let's go in a

763
00:30:28,370 --> 00:30:31,520
little bit this is this is perhaps the

764
00:30:31,520 --> 00:30:33,410
paper that still has the best results

765
00:30:33,410 --> 00:30:36,110
for for for for fuzzing so far for

766
00:30:36,110 --> 00:30:39,049
program analysis in general there's no

767
00:30:39,049 --> 00:30:41,570
machinery knows about fusion but it uses

768
00:30:41,570 --> 00:30:45,860
taint tracking to to improve the process

769
00:30:45,860 --> 00:30:47,270
of program analysis to improve the

770
00:30:47,270 --> 00:30:50,240
process of finding bugs and the main

771
00:30:50,240 --> 00:30:52,760
idea is that most path constraints

772
00:30:52,760 --> 00:30:54,710
depend on only part of the input so for

773
00:30:54,710 --> 00:30:57,679
example if if you look at a conditional

774
00:30:57,679 --> 00:31:00,740
statement like this if s 0 if F F of 0

775
00:31:00,740 --> 00:31:02,809
equals x then your border program then

776
00:31:02,809 --> 00:31:06,410
if if your algorithm is able to for exam

777
00:31:06,410 --> 00:31:09,620
detect that only a certain part of your

778
00:31:09,620 --> 00:31:12,440
input is relevant to exercising this

779
00:31:12,440 --> 00:31:14,240
code path then maybe you should just

780
00:31:14,240 --> 00:31:17,900
change that and so that that's what it

781
00:31:17,900 --> 00:31:20,420
does exactly it tries to look for it

782
00:31:20,420 --> 00:31:21,980
translate for patterns like that it

783
00:31:21,980 --> 00:31:23,240
tries to look for context-sensitive

784
00:31:23,240 --> 00:31:26,750
branch coverage this is a little bit of

785
00:31:26,750 --> 00:31:28,910
a mind twister but this is pretty simple

786
00:31:28,910 --> 00:31:30,440
so the first time the flag is set to

787
00:31:30,440 --> 00:31:35,180
false and then and then if the flag is

788
00:31:35,180 --> 00:31:37,100
forced it won't abort and then after you

789
00:31:37,100 --> 00:31:38,900
set the flag to true and then the second

790
00:31:38,900 --> 00:31:40,910
time you run this exact same you run

791
00:31:40,910 --> 00:31:42,740
this exact same function it actually

792
00:31:42,740 --> 00:31:44,570
aborts because the flag was people to

793
00:31:44,570 --> 00:31:48,500
set the true already so so the general

794
00:31:48,500 --> 00:31:50,060
idea is that most programs have some

795
00:31:50,060 --> 00:31:51,440
kind of global state like that have some

796
00:31:51,440 --> 00:31:53,330
kind of shared state where the state can

797
00:31:53,330 --> 00:31:56,450
be mutated and and most sponsors don't

798
00:31:56,450 --> 00:31:59,030
actually take into consideration the the

799
00:31:59,030 --> 00:32:00,590
context in which the function is being

800
00:32:00,590 --> 00:32:03,320
executed like if you think about static

801
00:32:03,320 --> 00:32:05,690
analysis even most static analyzers

802
00:32:05,690 --> 00:32:07,040
don't look at context when they're

803
00:32:07,040 --> 00:32:09,410
executing what when analyzing a program

804
00:32:09,410 --> 00:32:11,570
they only look at the function in

805
00:32:11,570 --> 00:32:13,520
general and then they look at what the

806
00:32:13,520 --> 00:32:14,810
current state is what they expect that

807
00:32:14,810 --> 00:32:16,700
state is and don't look at changing

808
00:32:16,700 --> 00:32:17,720
state because then there'll be too many

809
00:32:17,720 --> 00:32:20,210
too many variables to keep track of but

810
00:32:20,210 --> 00:32:25,220
the idea behind this is that you want to

811
00:32:25,220 --> 00:32:27,770
look for some kind of way to keep track

812
00:32:27,770 --> 00:32:31,220
off of branch coverage that are

813
00:32:31,220 --> 00:32:33,710
influenced by context and then they're

814
00:32:33,710 --> 00:32:34,760
keeping track of that we're using a

815
00:32:34,760 --> 00:32:36,860
simple dictionary and then that improves

816
00:32:36,860 --> 00:32:40,190
the results quite significantly and the

817
00:32:40,190 --> 00:32:42,140
next idea is kind of a more ml based

818
00:32:42,140 --> 00:32:43,760
approach the entire machine learning

819
00:32:43,760 --> 00:32:45,740
feel is based off convex optimization

820
00:32:45,740 --> 00:32:48,130
which is trying to look for a

821
00:32:48,130 --> 00:32:51,200
optimization point minimizing some kind

822
00:32:51,200 --> 00:32:52,820
of cost function by going down some

823
00:32:52,820 --> 00:32:55,030
gradient and this gradient is

824
00:32:55,030 --> 00:32:57,230
representative of optimizing some

825
00:32:57,230 --> 00:32:59,600
equation computationally not at all

826
00:32:59,600 --> 00:33:02,450
analytically and they are searching for

827
00:33:02,450 --> 00:33:05,150
input that can help them to improve the

828
00:33:05,150 --> 00:33:06,980
scores or improve the likelihood of

829
00:33:06,980 --> 00:33:10,520
finding a crash more quickly and and and

830
00:33:10,520 --> 00:33:12,460
this improves the search process and

831
00:33:12,460 --> 00:33:15,830
input Lex pleurae ssin so it just

832
00:33:15,830 --> 00:33:18,410
improves the fuzzers working with longer

833
00:33:18,410 --> 00:33:19,440
inputs

834
00:33:19,440 --> 00:33:21,419
because they realized that the

835
00:33:21,419 --> 00:33:23,190
low-hanging fruit bears that most visors

836
00:33:23,190 --> 00:33:25,500
work well looking for experts involving

837
00:33:25,500 --> 00:33:27,990
short input but then with longer input

838
00:33:27,990 --> 00:33:30,330
that maybe don't fit in that don't fit

839
00:33:30,330 --> 00:33:32,909
in it register for it for example then

840
00:33:32,909 --> 00:33:34,200
it just doesn't work so they were able

841
00:33:34,200 --> 00:33:35,610
to find a lot more exploits that were

842
00:33:35,610 --> 00:33:38,009
missed by traditional feathers using

843
00:33:38,009 --> 00:33:40,919
this approach okay

844
00:33:40,919 --> 00:33:43,139
now going back to AFL which we kind of

845
00:33:43,139 --> 00:33:44,309
mentioned a few times in any entire

846
00:33:44,309 --> 00:33:46,409
approach I think this is deeper the most

847
00:33:46,409 --> 00:33:48,690
popular generalizable fuzzer over the

848
00:33:48,690 --> 00:33:50,789
last five years or so and it uses

849
00:33:50,789 --> 00:33:53,970
comprise some instrumentation by by

850
00:33:53,970 --> 00:33:55,799
adding hooks into your program execution

851
00:33:55,799 --> 00:33:57,299
so that you can figure out what's going

852
00:33:57,299 --> 00:33:58,710
on at what time you can get gather some

853
00:33:58,710 --> 00:34:00,600
metrics on what's going on and it uses

854
00:34:00,600 --> 00:34:02,159
this thing that quote-unquote everyone

855
00:34:02,159 --> 00:34:03,659
kind of calls the genetic algorithm to

856
00:34:03,659 --> 00:34:06,509
mutate input and the idea is you know

857
00:34:06,509 --> 00:34:07,909
kind of simple if you think about

858
00:34:07,909 --> 00:34:10,649
evolution it's a similar concept where

859
00:34:10,649 --> 00:34:13,260
if you change things in a certain way or

860
00:34:13,260 --> 00:34:14,699
if you change certain input if you're

861
00:34:14,699 --> 00:34:16,050
changing bite values if you change

862
00:34:16,050 --> 00:34:20,399
certain certain certain artifacts about

863
00:34:20,399 --> 00:34:24,510
a string or a program then it gives you

864
00:34:24,510 --> 00:34:26,639
a positive outcome then you want to keep

865
00:34:26,639 --> 00:34:29,609
changing that right so so maybe maybe

866
00:34:29,609 --> 00:34:31,199
the program is looking for a certain

867
00:34:31,199 --> 00:34:33,810
bite in that location and and changing

868
00:34:33,810 --> 00:34:35,099
that constantly will allow you to

869
00:34:35,099 --> 00:34:36,989
extract more code paths so that's a

870
00:34:36,989 --> 00:34:39,599
simple algorithm behind it it uses a

871
00:34:39,599 --> 00:34:41,040
simple feedback loop to generate a

872
00:34:41,040 --> 00:34:43,079
goodness of input and then it retains

873
00:34:43,079 --> 00:34:46,560
input that that helps to exercise new

874
00:34:46,560 --> 00:34:48,329
code paths and how it knows that is by

875
00:34:48,329 --> 00:34:50,849
instrumentation it knows what code paths

876
00:34:50,849 --> 00:34:53,849
are newly exercised and what are what

877
00:34:53,849 --> 00:34:54,690
are seen before

878
00:34:54,690 --> 00:34:56,460
the great thing is that this application

879
00:34:56,460 --> 00:34:58,980
agnostic it works with with very few

880
00:34:58,980 --> 00:35:01,650
seeds the bad thing is that if you look

881
00:35:01,650 --> 00:35:03,150
at the algorithm itself it actually

882
00:35:03,150 --> 00:35:06,660
still relies a lot of luck and so this

883
00:35:06,660 --> 00:35:08,369
is the core algorithm for AFL got it

884
00:35:08,369 --> 00:35:11,339
from a paper that looked at AFL and the

885
00:35:11,339 --> 00:35:16,319
the idea around this is that basically

886
00:35:16,319 --> 00:35:20,640
it's a it's a loop there are it's a

887
00:35:20,640 --> 00:35:22,319
nested loop you are picking an input

888
00:35:22,319 --> 00:35:24,960
from a queue and then you are sampling

889
00:35:24,960 --> 00:35:26,940
the number of mutations in order to

890
00:35:26,940 --> 00:35:30,000
apply it to apply on this input and then

891
00:35:30,000 --> 00:35:32,640
you apply the mutation and then you feed

892
00:35:32,640 --> 00:35:33,390
this mutation

893
00:35:33,390 --> 00:35:35,700
- in this mutation which is the

894
00:35:35,700 --> 00:35:37,470
candidate into the program and see what

895
00:35:37,470 --> 00:35:39,869
happens right so you just loop through

896
00:35:39,869 --> 00:35:41,819
this entire thing multiple times and

897
00:35:41,819 --> 00:35:44,400
realize that in this general outline

898
00:35:44,400 --> 00:35:47,279
algorithm there's a few things that no

899
00:35:47,279 --> 00:35:49,799
one really talks about there's the pic

900
00:35:49,799 --> 00:35:52,049
input which a paper that we saw

901
00:35:52,049 --> 00:35:53,569
previously talked about like

902
00:35:53,569 --> 00:35:55,289
prioritizing input based on the

903
00:35:55,289 --> 00:35:57,420
likelihood of affects portability of the

904
00:35:57,420 --> 00:35:58,650
likelihood of extracting new code paths

905
00:35:58,650 --> 00:36:00,690
and then there's a sample numb you

906
00:36:00,690 --> 00:36:02,069
tations basically the number of times

907
00:36:02,069 --> 00:36:04,260
the mutate the parent input and this

908
00:36:04,260 --> 00:36:06,089
will sort of looked at before in this

909
00:36:06,089 --> 00:36:08,279
sector sack paper and you pick a

910
00:36:08,279 --> 00:36:09,660
mutation operator which we'll look at in

911
00:36:09,660 --> 00:36:12,180
next couple slides and then you apply

912
00:36:12,180 --> 00:36:15,420
the mutations to the site so this

913
00:36:15,420 --> 00:36:17,130
probably requires a little bit of

914
00:36:17,130 --> 00:36:19,859
elaboration where AFL mutates input

915
00:36:19,859 --> 00:36:22,410
pretty simply this is not all of them

916
00:36:22,410 --> 00:36:24,089
but this is a representative list of

917
00:36:24,089 --> 00:36:26,099
them basically you can flip a bit and

918
00:36:26,099 --> 00:36:27,599
then you have got a new input you can

919
00:36:27,599 --> 00:36:31,049
add a byte at some random value to your

920
00:36:31,049 --> 00:36:32,609
input and then you get a new input and

921
00:36:32,609 --> 00:36:35,579
then you can overwrite insert deletions

922
00:36:35,579 --> 00:36:39,539
things like that simple and then we're

923
00:36:39,539 --> 00:36:41,279
just applying the concepts that we saw

924
00:36:41,279 --> 00:36:43,079
in the papers before and trying to try

925
00:36:43,079 --> 00:36:45,359
to see if we get better results and try

926
00:36:45,359 --> 00:36:49,890
to see if we can improve AFL and so what

927
00:36:49,890 --> 00:36:54,599
I did was I looked at nice small data

928
00:36:54,599 --> 00:36:56,819
set of programs that are well analyzed

929
00:36:56,819 --> 00:36:58,890
and well studied the ASCII content

930
00:36:58,890 --> 00:37:01,140
server in the CDC data set from a couple

931
00:37:01,140 --> 00:37:03,480
years ago and and this is this is

932
00:37:03,480 --> 00:37:06,119
basically a a content serving server and

933
00:37:06,119 --> 00:37:09,930
so it received commands from from some

934
00:37:09,930 --> 00:37:13,319
interface and then it responds with some

935
00:37:13,319 --> 00:37:18,299
kind of output so the idea is that if

936
00:37:18,299 --> 00:37:20,519
you look at all of these if you look at

937
00:37:20,519 --> 00:37:22,349
all of all of these different mutation

938
00:37:22,349 --> 00:37:24,390
strategies you realize that some of them

939
00:37:24,390 --> 00:37:26,549
will work better in this context

940
00:37:26,549 --> 00:37:29,039
compared to others and so if you apply a

941
00:37:29,039 --> 00:37:30,960
random mutation strategy selection which

942
00:37:30,960 --> 00:37:32,819
is what AFL does it just literally

943
00:37:32,819 --> 00:37:35,069
selects one at random applies them to a

944
00:37:35,069 --> 00:37:38,130
site then you realize that that that may

945
00:37:38,130 --> 00:37:39,390
not be the best approach at doing that

946
00:37:39,390 --> 00:37:40,710
maybe there is a better approach to

947
00:37:40,710 --> 00:37:44,250
doing this if if we apply kind of human

948
00:37:44,250 --> 00:37:46,049
intelligence to this problem and realize

949
00:37:46,049 --> 00:37:46,890
that

950
00:37:46,890 --> 00:37:49,170
since this is a Content server the input

951
00:37:49,170 --> 00:37:50,760
is expected to be in some kind of schema

952
00:37:50,760 --> 00:37:52,619
the input could be a little long the

953
00:37:52,619 --> 00:37:54,329
input needs to start with for example

954
00:37:54,329 --> 00:37:57,990
one of those five prefixes that you see

955
00:37:57,990 --> 00:37:59,460
that requests queries and visualize

956
00:37:59,460 --> 00:38:01,559
interact and if not it will just be

957
00:38:01,559 --> 00:38:04,079
thrown out so maybe we want to enhance

958
00:38:04,079 --> 00:38:05,970
the program enhance the impute mutations

959
00:38:05,970 --> 00:38:09,269
selection such that it is more likely to

960
00:38:09,269 --> 00:38:11,789
generate those rather than then just

961
00:38:11,789 --> 00:38:13,980
some random string so if you look at

962
00:38:13,980 --> 00:38:17,579
this bit flips randomly that that's not

963
00:38:17,579 --> 00:38:19,680
too promising because flipping bits

964
00:38:19,680 --> 00:38:23,099
randomly will unlikely in in in most

965
00:38:23,099 --> 00:38:24,869
unlikely so now most likely scenarios

966
00:38:24,869 --> 00:38:27,630
not generate valid strings that meet

967
00:38:27,630 --> 00:38:29,970
those criterion but I thought things do

968
00:38:29,970 --> 00:38:31,619
so for example if we want to clone

969
00:38:31,619 --> 00:38:34,859
strings then they come from valid inputs

970
00:38:34,859 --> 00:38:37,260
already and thus are likely to generate

971
00:38:37,260 --> 00:38:40,500
you know valid valid S key input that

972
00:38:40,500 --> 00:38:42,390
could then be accepted by this this

973
00:38:42,390 --> 00:38:45,150
server and could then be used to execute

974
00:38:45,150 --> 00:38:48,059
more code paths so taking this taking

975
00:38:48,059 --> 00:38:50,369
this as kind of the intuition for for

976
00:38:50,369 --> 00:38:52,289
the process of improving AFL for

977
00:38:52,289 --> 00:38:54,269
improving the implementation strategy

978
00:38:54,269 --> 00:38:56,789
selection process then we actually

979
00:38:56,789 --> 00:38:59,190
realized that we can improve them we can

980
00:38:59,190 --> 00:39:01,259
improve the time it takes to find

981
00:39:01,259 --> 00:39:04,259
crashes by about 20% or so and only

982
00:39:04,259 --> 00:39:06,329
require me up 50 50 lines of code to

983
00:39:06,329 --> 00:39:10,170
patch AFL so there's some interesting

984
00:39:10,170 --> 00:39:15,930
stuff I thought to to also look at this

985
00:39:15,930 --> 00:39:19,140
other this other paper which is which

986
00:39:19,140 --> 00:39:20,519
which I thought was pretty mind-blowing

987
00:39:20,519 --> 00:39:23,640
when I first read about it it's a couple

988
00:39:23,640 --> 00:39:26,250
years already and they actually already

989
00:39:26,250 --> 00:39:28,920
have released some some tool for you to

990
00:39:28,920 --> 00:39:31,470
reproduce the results in very

991
00:39:31,470 --> 00:39:34,200
constrained environments but the idea is

992
00:39:34,200 --> 00:39:38,430
that if you can if you can formulate a

993
00:39:38,430 --> 00:39:41,099
data science problem around looking for

994
00:39:41,099 --> 00:39:43,470
exploits then can you formulate a

995
00:39:43,470 --> 00:39:45,180
similar data science problem around

996
00:39:45,180 --> 00:39:47,579
looking for patches to fix that to fix

997
00:39:47,579 --> 00:39:51,180
that bug right and so they went about

998
00:39:51,180 --> 00:39:53,190
doing this you know with I think low

999
00:39:53,190 --> 00:39:55,170
expectations girls come in who would

1000
00:39:55,170 --> 00:39:57,120
think that this might actually work

1001
00:39:57,120 --> 00:39:59,850
the idea is that it learns from code

1002
00:39:59,850 --> 00:40:02,730
transformation patterns looking at other

1003
00:40:02,730 --> 00:40:05,790
bug fixes in open-source software so you

1004
00:40:05,790 --> 00:40:07,920
can think of the way you're thinking

1005
00:40:07,920 --> 00:40:09,810
about is probably right you look at you

1006
00:40:09,810 --> 00:40:13,410
know issues or bugs filed in open source

1007
00:40:13,410 --> 00:40:16,080
software and you look at the petrous or

1008
00:40:16,080 --> 00:40:18,750
the changes the diffs meet to meet that

1009
00:40:18,750 --> 00:40:20,490
that line of code or the area of code

1010
00:40:20,490 --> 00:40:22,650
such that that bugs mark is fixed and

1011
00:40:22,650 --> 00:40:24,210
then they use that extra data set their

1012
00:40:24,210 --> 00:40:26,220
label data set and try to train some

1013
00:40:26,220 --> 00:40:28,200
model that can generate these these

1014
00:40:28,200 --> 00:40:31,950
fixes automatically and they realized

1015
00:40:31,950 --> 00:40:34,440
that okay they were able to generate

1016
00:40:34,440 --> 00:40:39,180
fixes pretty reliably and I mean more

1017
00:40:39,180 --> 00:40:40,910
reliably than you would think but

1018
00:40:40,910 --> 00:40:43,920
correct patches for similar bugs are the

1019
00:40:43,920 --> 00:40:46,050
same Universal features across apps if

1020
00:40:46,050 --> 00:40:48,120
you find a sequel injection in one place

1021
00:40:48,120 --> 00:40:50,970
and then you learn the properties of the

1022
00:40:50,970 --> 00:40:53,370
fix that went into fixing this equal

1023
00:40:53,370 --> 00:40:55,440
injection more likely than not it comes

1024
00:40:55,440 --> 00:40:57,300
from a certain distribution that a model

1025
00:40:57,300 --> 00:40:59,370
can learn and therefore can come up with

1026
00:40:59,370 --> 00:41:01,830
based on the context of this new this

1027
00:41:01,830 --> 00:41:04,590
new situation select based on one of the

1028
00:41:04,590 --> 00:41:06,480
previously seen before patches that can

1029
00:41:06,480 --> 00:41:08,970
then fix the they can then fix the code

1030
00:41:08,970 --> 00:41:14,300
with a certain reliability and and

1031
00:41:14,300 --> 00:41:16,680
generating a correct patch that fixes

1032
00:41:16,680 --> 00:41:19,290
both the bug and does not break under

1033
00:41:19,290 --> 00:41:20,580
functioning the program is a different

1034
00:41:20,580 --> 00:41:23,640
matter altogether so that I think they

1035
00:41:23,640 --> 00:41:25,320
still are pretty pretty explicitly in

1036
00:41:25,320 --> 00:41:26,970
the program but there just goes to show

1037
00:41:26,970 --> 00:41:28,320
how much extra work we have to do in

1038
00:41:28,320 --> 00:41:30,870
this area and if data can be thrown at a

1039
00:41:30,870 --> 00:41:33,780
problem to kind of do this do something

1040
00:41:33,780 --> 00:41:36,420
about things then I think there may be a

1041
00:41:36,420 --> 00:41:38,040
lot more that we're not imagining today

1042
00:41:38,040 --> 00:41:41,520
that can be done in this area so the

1043
00:41:41,520 --> 00:41:44,250
future directions for this are that

1044
00:41:44,250 --> 00:41:46,770
there's quite a number of them but if

1045
00:41:46,770 --> 00:41:48,150
you look at the cadence of the different

1046
00:41:48,150 --> 00:41:49,410
papers that the different pieces of

1047
00:41:49,410 --> 00:41:50,820
research are being put out by this

1048
00:41:50,820 --> 00:41:54,240
community I think you can see that first

1049
00:41:54,240 --> 00:41:57,360
of all did enhancing data quality of any

1050
00:41:57,360 --> 00:41:59,820
kind of vulnerability data set any kind

1051
00:41:59,820 --> 00:42:02,580
of bug data set is is crucial right so

1052
00:42:02,580 --> 00:42:05,610
any kind of reporting any kind of

1053
00:42:05,610 --> 00:42:07,200
tagging that we can do to any

1054
00:42:07,200 --> 00:42:09,600
vulnerabilities that we find today

1055
00:42:09,600 --> 00:42:11,730
can help to influence the next

1056
00:42:11,730 --> 00:42:13,980
generation of algorithms the next

1057
00:42:13,980 --> 00:42:15,450
generation of research is being put onto

1058
00:42:15,450 --> 00:42:18,210
the field and then the the

1059
00:42:18,210 --> 00:42:19,980
state-of-the-art fuzzing research are

1060
00:42:19,980 --> 00:42:23,550
still kind of siloed in most systems in

1061
00:42:23,550 --> 00:42:25,740
most kind of systems arms of academia

1062
00:42:25,740 --> 00:42:28,710
today doesn't seem to use any kind of ML

1063
00:42:28,710 --> 00:42:30,780
approach but then there always seems to

1064
00:42:30,780 --> 00:42:33,660
be a tick and a talk where there is a

1065
00:42:33,660 --> 00:42:36,630
next there's a next wave of research

1066
00:42:36,630 --> 00:42:39,090
being put out that uses ideas from those

1067
00:42:39,090 --> 00:42:40,980
past papers and then enhances them with

1068
00:42:40,980 --> 00:42:43,770
with data techniques and then there's a

1069
00:42:43,770 --> 00:42:45,450
bunch of low-hanging fruit in generative

1070
00:42:45,450 --> 00:42:47,550
and generative methods that that I

1071
00:42:47,550 --> 00:42:50,100
realized when I started to just quickly

1072
00:42:50,100 --> 00:42:51,600
I fell a little bit and realized that

1073
00:42:51,600 --> 00:42:53,550
there's there's very there's very simple

1074
00:42:53,550 --> 00:42:54,960
things that you can do to push up the

1075
00:42:54,960 --> 00:42:56,730
push up the efficiency that doesn't

1076
00:42:56,730 --> 00:42:58,440
require complicated systems knowledge

1077
00:42:58,440 --> 00:43:01,050
and then the last thing is that there's

1078
00:43:01,050 --> 00:43:02,940
a lot of work being done in symbolic

1079
00:43:02,940 --> 00:43:05,820
execution and and taint tracking being

1080
00:43:05,820 --> 00:43:08,220
combined with fuzzing that can be that

1081
00:43:08,220 --> 00:43:11,510
can be maybe and pretty intuitively

1082
00:43:11,510 --> 00:43:13,710
improved with some kind of data

1083
00:43:13,710 --> 00:43:15,390
techniques once we have the right data

1084
00:43:15,390 --> 00:43:19,290
sets so program analysis is kind of a

1085
00:43:19,290 --> 00:43:21,090
relatively untainted field by the other

1086
00:43:21,090 --> 00:43:23,550
techniques compared to other other areas

1087
00:43:23,550 --> 00:43:26,100
in code execution analysis in in binary

1088
00:43:26,100 --> 00:43:29,000
analysis and what you realize is that

1089
00:43:29,000 --> 00:43:31,350
this means that there is gonna be a lot

1090
00:43:31,350 --> 00:43:32,940
of work done and this means that you

1091
00:43:32,940 --> 00:43:34,110
know potentially do some work yourself

1092
00:43:34,110 --> 00:43:37,040
too so

1093
00:43:37,040 --> 00:43:40,140
thanks hope this was a little

1094
00:43:40,140 --> 00:43:44,370
interesting but if you are inspired in

1095
00:43:44,370 --> 00:43:46,110
some way and and if you feel like you

1096
00:43:46,110 --> 00:43:48,570
wanna you want to jump in into this area

1097
00:43:48,570 --> 00:43:50,220
and you feel like this could be

1098
00:43:50,220 --> 00:43:52,590
interesting to look at then you should

1099
00:43:52,590 --> 00:43:54,630
come talk to me and and I have a bunch

1100
00:43:54,630 --> 00:43:57,270
of interesting interesting stuff that

1101
00:43:57,270 --> 00:43:59,190
then I think what me would be good to

1102
00:43:59,190 --> 00:44:01,410
start with and and interesting papers to

1103
00:44:01,410 --> 00:44:03,990
read and I think looking at this kind of

1104
00:44:03,990 --> 00:44:05,910
plenary study of the field is always a

1105
00:44:05,910 --> 00:44:08,880
good way to think about what what's next

1106
00:44:08,880 --> 00:44:10,290
what you can do and what's been done

1107
00:44:10,290 --> 00:44:12,180
before what you think could be done that

1108
00:44:12,180 --> 00:44:14,820
no one else has done yet so that's it

1109
00:44:14,820 --> 00:44:17,420
thank you

1110
00:44:20,760 --> 00:44:50,350
any questions yeah yeah that's that's

1111
00:44:50,350 --> 00:44:53,140
actually interesting so a lot of a lot

1112
00:44:53,140 --> 00:44:55,300
of the field thinks of the products of

1113
00:44:55,300 --> 00:44:58,600
the process of classifying exports of

1114
00:44:58,600 --> 00:45:01,360
generating exploits using a supervised

1115
00:45:01,360 --> 00:45:03,100
learning approach which means that

1116
00:45:03,100 --> 00:45:04,720
you're able to characterize both sides

1117
00:45:04,720 --> 00:45:06,670
of the equation and you're able to come

1118
00:45:06,670 --> 00:45:10,660
up with come up with a data set of equal

1119
00:45:10,660 --> 00:45:13,930
representation on both sides and the

1120
00:45:13,930 --> 00:45:17,140
fact is that I think as we discuss they

1121
00:45:17,140 --> 00:45:19,570
just touch on a low in one of the

1122
00:45:19,570 --> 00:45:22,900
earlier papers we saw that most most

1123
00:45:22,900 --> 00:45:26,830
bugs that are not maybe OWASP top 10 or

1124
00:45:26,830 --> 00:45:27,970
one of those common things that you're

1125
00:45:27,970 --> 00:45:29,800
looking for cannot actually be

1126
00:45:29,800 --> 00:45:32,530
effectively class classified using one

1127
00:45:32,530 --> 00:45:34,270
of those approaches right so I'm looking

1128
00:45:34,270 --> 00:45:35,650
for something out of the ordinary and

1129
00:45:35,650 --> 00:45:36,700
most of the time these are the

1130
00:45:36,700 --> 00:45:37,780
interesting things that people are

1131
00:45:37,780 --> 00:45:38,950
actually looking for you're not

1132
00:45:38,950 --> 00:45:40,030
interested in finding some random

1133
00:45:40,030 --> 00:45:43,870
sequence somewhere but the idea is that

1134
00:45:43,870 --> 00:45:47,710
by looking for intricacies in different

1135
00:45:47,710 --> 00:45:50,350
kinds of input by looking for for

1136
00:45:50,350 --> 00:45:52,780
patterns that may be humans studying the

1137
00:45:52,780 --> 00:45:54,760
program heaven yet realize how to

1138
00:45:54,760 --> 00:45:57,610
generate then there could be a potential

1139
00:45:57,610 --> 00:46:00,640
for generating input off of this nature

1140
00:46:00,640 --> 00:46:04,900
so I think the one of the more tried and

1141
00:46:04,900 --> 00:46:08,230
tested open-source tools that is covered

1142
00:46:08,230 --> 00:46:11,620
in this paper is the visa software and I

1143
00:46:11,620 --> 00:46:13,990
think it has consistently demonstrated

1144
00:46:13,990 --> 00:46:15,880
its ability to find interesting bugs

1145
00:46:15,880 --> 00:46:19,210
that that haven't yet been found even by

1146
00:46:19,210 --> 00:46:21,460
you know human analyzers the program

1147
00:46:21,460 --> 00:46:23,590
which have studied you know for example

1148
00:46:23,590 --> 00:46:25,840
like GAAP for a long time it's able to

1149
00:46:25,840 --> 00:46:27,880
find something that approaches the

1150
00:46:27,880 --> 00:46:29,440
problem from a different angle that a

1151
00:46:29,440 --> 00:46:30,940
human's

1152
00:46:30,940 --> 00:46:33,730
and an expert in this field would so I

1153
00:46:33,730 --> 00:46:35,020
think there's some potential for doing

1154
00:46:35,020 --> 00:46:37,000
that but still the problem space is not

1155
00:46:37,000 --> 00:46:39,010
entirely set up for that problem to be

1156
00:46:39,010 --> 00:46:47,579
solved very efficiently new questions

1157
00:46:48,690 --> 00:46:54,400
cool so I'll be around here but if

1158
00:46:54,400 --> 00:46:56,500
anyone has questions about what I'm

1159
00:46:56,500 --> 00:46:59,020
doing about about this fuel in general

1160
00:46:59,020 --> 00:47:00,760
and I think it's a good time to look

1161
00:47:00,760 --> 00:47:01,180
into it

1162
00:47:01,180 --> 00:47:03,550
thank you

1163
00:47:03,550 --> 00:47:06,859
[Applause]

