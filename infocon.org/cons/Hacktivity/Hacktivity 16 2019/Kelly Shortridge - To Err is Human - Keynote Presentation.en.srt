1
00:00:00,180 --> 00:00:04,529
thank you so much for coming out at 9:00

2
00:00:02,430 --> 00:00:05,850
a.m. on a Saturday I'm not sure if I

3
00:00:04,530 --> 00:00:08,730
would be willing to do it so it's very

4
00:00:05,850 --> 00:00:10,889
appreciated so welcome to to err is

5
00:00:08,730 --> 00:00:14,699
human the complexity of security

6
00:00:10,889 --> 00:00:16,560
failures I'm Kelly Shortridge I'm VP of

7
00:00:14,699 --> 00:00:18,509
product strategy at capsule 8 which is a

8
00:00:16,560 --> 00:00:21,000
Linux visibility and protection startup

9
00:00:18,509 --> 00:00:22,619
based in New York City my spare time I

10
00:00:21,000 --> 00:00:26,189
research the intersection of behavioral

11
00:00:22,619 --> 00:00:28,050
economics and InfoSec I always like to

12
00:00:26,189 --> 00:00:30,029
set the tone of the talk with a quote

13
00:00:28,050 --> 00:00:32,580
and here I'm gonna be quoting Alexander

14
00:00:30,029 --> 00:00:37,110
Pope who said to err is human to forgive

15
00:00:32,580 --> 00:00:39,239
is divine humans make mistakes it's part

16
00:00:37,110 --> 00:00:40,800
of our nature I think it's more of a

17
00:00:39,240 --> 00:00:42,810
feature than a bug it's good for

18
00:00:40,800 --> 00:00:44,070
evolution because many of our mistakes

19
00:00:42,810 --> 00:00:46,140
are the result of thinking that was

20
00:00:44,070 --> 00:00:49,620
actually rational in the moment but

21
00:00:46,140 --> 00:00:51,330
later on doesn't seem so right info sex

22
00:00:49,620 --> 00:00:53,519
in the stake however is operating as if

23
00:00:51,330 --> 00:00:55,739
humans can be forced to never ever make

24
00:00:53,520 --> 00:00:57,870
errors we punish those who deviate from

25
00:00:55,740 --> 00:00:59,760
our policies and scoff at security teams

26
00:00:57,870 --> 00:01:02,910
who are pwned in like silly ways by

27
00:00:59,760 --> 00:01:04,890
script kiddies we also say that security

28
00:01:02,910 --> 00:01:07,890
might be solved if only users or it's so

29
00:01:04,890 --> 00:01:09,870
stupid to click on links the problem is

30
00:01:07,890 --> 00:01:11,550
this mentality undermines our strategies

31
00:01:09,870 --> 00:01:13,590
and it forces us into a futile war

32
00:01:11,550 --> 00:01:15,539
against human nature which doesn't make

33
00:01:13,590 --> 00:01:18,360
a lot of sense interest that kind of

34
00:01:15,540 --> 00:01:20,550
hopes to bend reality to its own will

35
00:01:18,360 --> 00:01:24,180
not realizing that we're all so mortal

36
00:01:20,550 --> 00:01:25,890
and we also can make these mistakes so

37
00:01:24,180 --> 00:01:27,780
to achieve the goal of building secure

38
00:01:25,890 --> 00:01:30,300
systems we have to evolve our thinking

39
00:01:27,780 --> 00:01:31,320
we have to work with human nature rather

40
00:01:30,300 --> 00:01:33,360
than against it

41
00:01:31,320 --> 00:01:35,850
so in this talk my goal is to show you

42
00:01:33,360 --> 00:01:39,120
why the shift is so essential and also

43
00:01:35,850 --> 00:01:40,860
how to begin down this path so first

44
00:01:39,120 --> 00:01:42,840
we'll explore what do we actually mean

45
00:01:40,860 --> 00:01:44,909
by human error because it's a highly

46
00:01:42,840 --> 00:01:46,530
subjective term then we'll delve into

47
00:01:44,909 --> 00:01:48,360
some of the cognitive biases

48
00:01:46,530 --> 00:01:51,210
specifically hindsight bias and outcome

49
00:01:48,360 --> 00:01:53,100
bias that play into this then we'll

50
00:01:51,210 --> 00:01:54,929
discuss how InfoSec is currently coping

51
00:01:53,100 --> 00:01:57,119
with this sort of failure in human error

52
00:01:54,930 --> 00:01:59,670
and then finally we'll delve into how

53
00:01:57,119 --> 00:02:01,770
InfoSec can actually create the shift in

54
00:01:59,670 --> 00:02:06,950
its approach and stop fighting against

55
00:02:01,770 --> 00:02:09,239
nature so first what do we mean by error

56
00:02:06,950 --> 00:02:10,739
when we invoke human error in the

57
00:02:09,239 --> 00:02:11,639
discussion of security failure what do

58
00:02:10,739 --> 00:02:13,380
we actually mean by it

59
00:02:11,639 --> 00:02:15,390
an era can reflect

60
00:02:13,380 --> 00:02:16,590
an action that leads to a failure or

61
00:02:15,390 --> 00:02:18,540
it's an action that deviates from

62
00:02:16,590 --> 00:02:21,360
expected behavior this is true for

63
00:02:18,540 --> 00:02:22,709
humans and systems alike failure is

64
00:02:21,360 --> 00:02:24,480
quite obviously the opposite of a

65
00:02:22,710 --> 00:02:27,810
success when the objective is not

66
00:02:24,480 --> 00:02:29,340
actually achieved we experience failure

67
00:02:27,810 --> 00:02:31,500
all the time to varying degrees of

68
00:02:29,340 --> 00:02:33,420
severity and consequence through the

69
00:02:31,500 --> 00:02:35,370
lens of complex systems failure can also

70
00:02:33,420 --> 00:02:38,540
represent the inability to cope with

71
00:02:35,370 --> 00:02:40,770
complexity so I think a security failure

72
00:02:38,540 --> 00:02:44,700
represents the breakdown in our security

73
00:02:40,770 --> 00:02:46,530
coping mechanisms the phrase human

74
00:02:44,700 --> 00:02:49,320
errors specifically involves subjective

75
00:02:46,530 --> 00:02:51,750
expectations often societal on behavior

76
00:02:49,320 --> 00:02:54,720
is it human error if a sales engineer

77
00:02:51,750 --> 00:02:57,630
download some executable so they can run

78
00:02:54,720 --> 00:03:00,060
a demo call and that leads to a sale a

79
00:02:57,630 --> 00:03:02,700
security person may think so but the

80
00:03:00,060 --> 00:03:04,140
sales leader may not think so is it

81
00:03:02,700 --> 00:03:06,359
human error to overlook one of a

82
00:03:04,140 --> 00:03:09,480
thousand alerts 90% of which are false

83
00:03:06,360 --> 00:03:11,490
positives and that was about potentially

84
00:03:09,480 --> 00:03:13,920
malicious endpoint activity the sock

85
00:03:11,490 --> 00:03:16,320
analyst might think it's justifiable but

86
00:03:13,920 --> 00:03:19,010
then an incident response consultant may

87
00:03:16,320 --> 00:03:21,239
think it was a stupid mistake

88
00:03:19,010 --> 00:03:23,340
understanding why an incident happened

89
00:03:21,240 --> 00:03:25,200
is obviously essential for process

90
00:03:23,340 --> 00:03:26,820
improvement the problem is if human

91
00:03:25,200 --> 00:03:28,290
error means the human performed an

92
00:03:26,820 --> 00:03:30,420
action that contributed to failure

93
00:03:28,290 --> 00:03:32,489
that's not that useful but at least it's

94
00:03:30,420 --> 00:03:34,320
more constructive because it encourages

95
00:03:32,490 --> 00:03:36,480
an exploration of all the things that

96
00:03:34,320 --> 00:03:38,730
led to the failure but if human error

97
00:03:36,480 --> 00:03:41,280
means the blame for the error rests on

98
00:03:38,730 --> 00:03:42,989
the human that's entirely unproductive

99
00:03:41,280 --> 00:03:46,230
you're really unlikely to learn much

100
00:03:42,990 --> 00:03:48,270
from your incident review aviation

101
00:03:46,230 --> 00:03:50,070
manufacturing and health care have all

102
00:03:48,270 --> 00:03:52,380
experienced this revolution and thinking

103
00:03:50,070 --> 00:03:54,180
healthcare there's tons of literature on

104
00:03:52,380 --> 00:03:56,940
this and I have my sources at the very

105
00:03:54,180 --> 00:03:58,950
last slide is still struggling to change

106
00:03:56,940 --> 00:04:01,760
its ways in light of the fact that human

107
00:03:58,950 --> 00:04:04,260
error empirically doesn't really exist

108
00:04:01,760 --> 00:04:06,299
the problem as much of health care views

109
00:04:04,260 --> 00:04:07,799
all areas as mistakes which are

110
00:04:06,300 --> 00:04:10,350
inappropriate intentions rather than

111
00:04:07,800 --> 00:04:12,420
slips which are unintended actions

112
00:04:10,350 --> 00:04:14,579
I think InfoSec is pretty similar in

113
00:04:12,420 --> 00:04:16,409
that way so across industries although

114
00:04:14,580 --> 00:04:17,940
human error usually shows of that a

115
00:04:16,410 --> 00:04:19,739
human was involved in the failure

116
00:04:17,940 --> 00:04:22,079
somehow which isn't very illuminating

117
00:04:19,738 --> 00:04:25,140
until we have everything driven by AI

118
00:04:22,079 --> 00:04:26,400
and some sort of horrible dystopia so

119
00:04:25,140 --> 00:04:27,360
ultimately human error

120
00:04:26,400 --> 00:04:29,849
means different things to different

121
00:04:27,360 --> 00:04:32,400
people the term is much less grounded in

122
00:04:29,850 --> 00:04:34,080
reality than we think so in truth what

123
00:04:32,400 --> 00:04:36,198
most of what we deem human error is

124
00:04:34,080 --> 00:04:38,849
really tainted by our cognitive biases

125
00:04:36,199 --> 00:04:44,639
specifically hindsight bias and outcome

126
00:04:38,850 --> 00:04:46,650
bias cognitive biases represent mental

127
00:04:44,639 --> 00:04:48,600
shortcuts that are optimal for evolution

128
00:04:46,650 --> 00:04:50,008
but not necessarily for the cognitive

129
00:04:48,600 --> 00:04:52,979
demands that pervade our modern

130
00:04:50,009 --> 00:04:54,750
environments we have to learn from the

131
00:04:52,979 --> 00:04:56,520
past in order to progress the problem is

132
00:04:54,750 --> 00:04:58,650
our lizard brains I call them the kind

133
00:04:56,520 --> 00:05:00,719
of evolutionary thinking we have can

134
00:04:58,650 --> 00:05:03,599
take things too far in veer into the

135
00:05:00,720 --> 00:05:06,000
territory of hindsight bias so what is

136
00:05:03,600 --> 00:05:07,830
hindsight bias it involves our present

137
00:05:06,000 --> 00:05:09,510
knowledge influencing our consideration

138
00:05:07,830 --> 00:05:11,430
of past events the I knew it all along

139
00:05:09,510 --> 00:05:14,370
effect or what's also called the curse

140
00:05:11,430 --> 00:05:16,080
of knowledge for example think about all

141
00:05:14,370 --> 00:05:17,669
the armchair experts that come out on

142
00:05:16,080 --> 00:05:19,859
Twitter or LinkedIn or everywhere else

143
00:05:17,669 --> 00:05:21,690
after every breach talking about how

144
00:05:19,860 --> 00:05:25,289
obvious it was that a breach would

145
00:05:21,690 --> 00:05:26,610
happen repeated experiments show that

146
00:05:25,289 --> 00:05:28,650
people tend to overestimate the

147
00:05:26,610 --> 00:05:30,120
predictive abilities when they don't

148
00:05:28,650 --> 00:05:32,280
have the benefit of this future

149
00:05:30,120 --> 00:05:34,590
knowledge does the practical example

150
00:05:32,280 --> 00:05:36,960
people condemned financial analysts who

151
00:05:34,590 --> 00:05:38,609
didn't predict the 2008 financial crisis

152
00:05:36,960 --> 00:05:40,169
across the world they deemed it as

153
00:05:38,610 --> 00:05:41,669
inevitable and that all the signs were

154
00:05:40,169 --> 00:05:42,349
there and how could anyone have missed

155
00:05:41,669 --> 00:05:44,969
it

156
00:05:42,349 --> 00:05:46,889
in InfoSec I remember when the Sony

157
00:05:44,970 --> 00:05:48,150
Pictures leak happened many people

158
00:05:46,889 --> 00:05:50,240
didn't actually believe it was North

159
00:05:48,150 --> 00:05:53,489
Korea they believed this now a defunct

160
00:05:50,240 --> 00:05:55,919
Norse company which was the one with the

161
00:05:53,490 --> 00:05:57,360
PPU map if you remember that / US

162
00:05:55,919 --> 00:05:59,490
officials that said it was North Korea

163
00:05:57,360 --> 00:06:01,289
of course now if you ask anyone it was

164
00:05:59,490 --> 00:06:04,919
so obvious that it was North Korea all

165
00:06:01,289 --> 00:06:07,260
along outcome bias is when we judged the

166
00:06:04,919 --> 00:06:09,659
decision the quality of a decision based

167
00:06:07,260 --> 00:06:11,550
on its eventual outcome we tend to judge

168
00:06:09,659 --> 00:06:13,560
a decision far more harshly if it led to

169
00:06:11,550 --> 00:06:14,550
a negative outcome than if it led to a

170
00:06:13,560 --> 00:06:16,409
positive outcome

171
00:06:14,550 --> 00:06:18,419
so outcome bias really weighs the

172
00:06:16,409 --> 00:06:20,729
ultimate outcome is the most important

173
00:06:18,419 --> 00:06:24,389
factor when evaluating the correctness

174
00:06:20,729 --> 00:06:26,280
of decisions in both cases of bias we

175
00:06:24,389 --> 00:06:28,080
really should evaluate what was the best

176
00:06:26,280 --> 00:06:29,698
decision at the time given what was

177
00:06:28,080 --> 00:06:31,919
known at the time without the benefit of

178
00:06:29,699 --> 00:06:34,229
hindsight knowing more information than

179
00:06:31,919 --> 00:06:35,849
you did in the past or outcome which is

180
00:06:34,229 --> 00:06:38,130
knowing what you would result for those

181
00:06:35,849 --> 00:06:39,990
of you who are familiar with the

182
00:06:38,130 --> 00:06:42,449
philosopher David Hume he

183
00:06:39,990 --> 00:06:46,139
work on causation very much fits into

184
00:06:42,449 --> 00:06:48,810
this mold the thing is all decisions in

185
00:06:46,139 --> 00:06:50,910
life involves some level of risk they're

186
00:06:48,810 --> 00:06:54,120
pretty complex the resulting outcome of

187
00:06:50,910 --> 00:06:55,410
a decision is largely based on chance so

188
00:06:54,120 --> 00:06:57,720
if you subscribe to the multiverse

189
00:06:55,410 --> 00:07:00,120
theory whatever your most recent stupid

190
00:06:57,720 --> 00:07:02,370
decision was like coming to a 9:00 a.m.

191
00:07:00,120 --> 00:07:04,590
talk on Saturday and that resulted in

192
00:07:02,370 --> 00:07:06,000
something terrible in our world actually

193
00:07:04,590 --> 00:07:08,270
results in something great in another

194
00:07:06,000 --> 00:07:10,860
world which maybe is a little comforting

195
00:07:08,270 --> 00:07:12,240
so when we succumb to outcome bias or

196
00:07:10,860 --> 00:07:13,889
really unfairly holding people

197
00:07:12,240 --> 00:07:16,259
accountable for things that are outside

198
00:07:13,889 --> 00:07:18,180
of their control the classic example on

199
00:07:16,259 --> 00:07:21,060
out of outcome biases that people rate

200
00:07:18,180 --> 00:07:23,160
the decisions operate far more harshly

201
00:07:21,060 --> 00:07:25,590
if the patient died than if the patient

202
00:07:23,160 --> 00:07:27,570
lives but logically the decision to

203
00:07:25,590 --> 00:07:28,919
operate should be the same regardless of

204
00:07:27,570 --> 00:07:30,900
outcome because you had the same

205
00:07:28,919 --> 00:07:33,570
probability of success going into it

206
00:07:30,900 --> 00:07:35,698
each time so this sort of evaluation

207
00:07:33,570 --> 00:07:37,080
really punishes practitioners for only

208
00:07:35,699 --> 00:07:41,190
holding knowledge of the present not the

209
00:07:37,080 --> 00:07:43,139
future which is super unfair so how many

210
00:07:41,190 --> 00:07:47,039
of you heard about the latest capital

211
00:07:43,139 --> 00:07:49,860
one breach okay a few of you so Capital

212
00:07:47,039 --> 00:07:51,690
One was hacked kind of by an insider who

213
00:07:49,860 --> 00:07:54,000
at least had some familiarity there

214
00:07:51,690 --> 00:07:56,639
seemed to be a misconfigured woth etc

215
00:07:54,000 --> 00:07:58,770
etc but when Capital One was breached

216
00:07:56,639 --> 00:08:00,449
there's a lot of talk about how Capital

217
00:07:58,770 --> 00:08:03,389
One's kind of cloudy and DevOps II

218
00:08:00,449 --> 00:08:05,400
strategy didn't work and they had been

219
00:08:03,389 --> 00:08:07,530
pretty vocal about pursuing this kind of

220
00:08:05,400 --> 00:08:08,789
approach with modern infrastructure they

221
00:08:07,530 --> 00:08:10,679
definitely experienced the security

222
00:08:08,789 --> 00:08:12,690
failure but I think any of us who were

223
00:08:10,680 --> 00:08:14,820
external to them can't really judge if

224
00:08:12,690 --> 00:08:18,210
it was a failure of strategy or not I

225
00:08:14,820 --> 00:08:21,060
personally don't think it is these

226
00:08:18,210 --> 00:08:23,820
cognitive biases outcome and hindsight

227
00:08:21,060 --> 00:08:25,590
bias change how we cope with failure and

228
00:08:23,820 --> 00:08:27,180
they also put guileless into believing

229
00:08:25,590 --> 00:08:29,039
that human error was the root cause of

230
00:08:27,180 --> 00:08:31,500
something that's really a misguided

231
00:08:29,039 --> 00:08:34,110
notion about how systems fundamentally

232
00:08:31,500 --> 00:08:35,849
work so now let's examine how InfoSec

233
00:08:34,110 --> 00:08:40,050
currently copes with security failure

234
00:08:35,849 --> 00:08:42,709
and this so-called human error which

235
00:08:40,049 --> 00:08:45,240
brings us to unhealthy coping mechanisms

236
00:08:42,708 --> 00:08:47,640
so the first unhealthy coping mechanism

237
00:08:45,240 --> 00:08:50,490
is blaming human error I would argue

238
00:08:47,640 --> 00:08:52,500
InfoSec favorite pastimes are picnic and

239
00:08:50,490 --> 00:08:53,740
pep kak which is problem and share not

240
00:08:52,500 --> 00:08:55,390
in computer and

241
00:08:53,740 --> 00:08:58,209
problem exists between keyboard and

242
00:08:55,390 --> 00:09:00,130
chair respectively so blaming failure on

243
00:08:58,209 --> 00:09:03,010
humans is pretty convenient but really

244
00:09:00,130 --> 00:09:04,810
it's kind of a faulty endeavor it

245
00:09:03,010 --> 00:09:06,130
doesn't mean that humans are absolved of

246
00:09:04,810 --> 00:09:07,630
accountability there are definitely

247
00:09:06,130 --> 00:09:09,970
malicious individuals that aim to

248
00:09:07,630 --> 00:09:11,950
inflict harm particularly when you're

249
00:09:09,970 --> 00:09:13,420
dealing with security stuff but what it

250
00:09:11,950 --> 00:09:15,130
means is that you cannot settle on

251
00:09:13,420 --> 00:09:17,229
blaming the individual closest to the

252
00:09:15,130 --> 00:09:18,930
event that led to failure you have to

253
00:09:17,230 --> 00:09:22,440
examine all of the contributing factors

254
00:09:18,930 --> 00:09:24,880
if you want to productively investigate

255
00:09:22,440 --> 00:09:26,649
the fundamental attribution error is the

256
00:09:24,880 --> 00:09:28,360
tendency for people to assume other

257
00:09:26,649 --> 00:09:30,820
people's characteristics or innate

258
00:09:28,360 --> 00:09:33,010
traits but anything that you do is just

259
00:09:30,820 --> 00:09:36,760
situational that's holding yourself to a

260
00:09:33,010 --> 00:09:38,439
different standard so for example if and

261
00:09:36,760 --> 00:09:40,569
this has happened to me um if we have

262
00:09:38,440 --> 00:09:42,550
InfoSec professionals fell victim to a

263
00:09:40,570 --> 00:09:44,140
phishing test an organization will

264
00:09:42,550 --> 00:09:46,599
probably laugh it off and be like oh I

265
00:09:44,140 --> 00:09:49,180
haven't had my coffee or you know I have

266
00:09:46,600 --> 00:09:51,160
a zillion things on my plate but then if

267
00:09:49,180 --> 00:09:52,930
we see someone else clicking on a link

268
00:09:51,160 --> 00:09:54,730
in a phishing exercise it's like oh well

269
00:09:52,930 --> 00:09:57,160
they're lazy or sloppy or naive or

270
00:09:54,730 --> 00:09:59,079
inattentive and all of that stuff so

271
00:09:57,160 --> 00:10:00,850
believing someone deliberately made the

272
00:09:59,079 --> 00:10:03,489
choice to go on this kind of error-prone

273
00:10:00,850 --> 00:10:07,420
path Ferries us towards the strategy of

274
00:10:03,490 --> 00:10:09,040
blaming humans which isn't great really

275
00:10:07,420 --> 00:10:11,290
an error occurring represents the

276
00:10:09,040 --> 00:10:14,500
starting point for an investigation not

277
00:10:11,290 --> 00:10:16,750
its conclusion errors are a symptom of

278
00:10:14,500 --> 00:10:18,850
failure not a cause because they are

279
00:10:16,750 --> 00:10:21,130
real I may arise between the

280
00:10:18,850 --> 00:10:24,070
relationships between components in a

281
00:10:21,130 --> 00:10:25,510
complex system when someone clicks on a

282
00:10:24,070 --> 00:10:27,220
link they shouldn't the first question

283
00:10:25,510 --> 00:10:29,920
should be why did they click on the link

284
00:10:27,220 --> 00:10:31,779
and another relevant question is why did

285
00:10:29,920 --> 00:10:34,719
clicking on the link actually lead to an

286
00:10:31,779 --> 00:10:36,880
incident but these questions go

287
00:10:34,720 --> 00:10:40,000
unanswered if we are satisfied that the

288
00:10:36,880 --> 00:10:41,170
incident is explained by human error we

289
00:10:40,000 --> 00:10:43,180
tend to follow it by a proposed

290
00:10:41,170 --> 00:10:44,770
mitigation like user education which

291
00:10:43,180 --> 00:10:48,550
really doesn't have much of a chance of

292
00:10:44,770 --> 00:10:50,410
succeeding so focus on human areas of

293
00:10:48,550 --> 00:10:52,270
root cause leads to solutions that fail

294
00:10:50,410 --> 00:10:55,149
to understand the real priorities for

295
00:10:52,270 --> 00:10:56,589
users in question for example expecting

296
00:10:55,149 --> 00:10:58,149
DevOps to suddenly care about

297
00:10:56,589 --> 00:10:59,860
implementing security in the software

298
00:10:58,149 --> 00:11:02,470
development lifecycle after a training

299
00:10:59,860 --> 00:11:06,040
module is unrealistic and unlikely to

300
00:11:02,470 --> 00:11:07,670
work some some of your InfoSec teams use

301
00:11:06,040 --> 00:11:10,009
a five why's approach

302
00:11:07,670 --> 00:11:12,740
which is just asking why five times

303
00:11:10,009 --> 00:11:14,269
getting deeper and deeper and it's

304
00:11:12,740 --> 00:11:15,860
because they seek to understand the

305
00:11:14,269 --> 00:11:18,290
underlying factors that contributed to

306
00:11:15,860 --> 00:11:19,970
events to determine what other systems

307
00:11:18,290 --> 00:11:22,040
might have the same issue and how they

308
00:11:19,970 --> 00:11:23,930
can improve their systems but there's

309
00:11:22,040 --> 00:11:26,059
still too many instances of blaming

310
00:11:23,930 --> 00:11:28,638
humans and firing them to solve the

311
00:11:26,059 --> 00:11:32,180
problem guessing everyone here probably

312
00:11:28,639 --> 00:11:33,139
heard about the Equifax breach right no

313
00:11:32,180 --> 00:11:36,019
one's raising their hand but I'm gonna

314
00:11:33,139 --> 00:11:38,180
seem so so the ex-ceo of Equifax blamed

315
00:11:36,019 --> 00:11:40,879
human error for the breach specifically

316
00:11:38,180 --> 00:11:42,498
one individual was identified as not

317
00:11:40,879 --> 00:11:45,069
ensuring communication got to the right

318
00:11:42,499 --> 00:11:47,120
person to manually patch the application

319
00:11:45,069 --> 00:11:48,680
but there didn't appear to be an

320
00:11:47,120 --> 00:11:50,470
exploration of the friction filled

321
00:11:48,680 --> 00:11:52,459
workflow for patching the orgonite

322
00:11:50,470 --> 00:11:55,160
organizational pressures to maximize

323
00:11:52,459 --> 00:11:57,469
uptime the reliance on legacy systems

324
00:11:55,160 --> 00:12:01,850
nor the inappropriate workflow placement

325
00:11:57,470 --> 00:12:04,850
of the vulnerability scanner itself so

326
00:12:01,850 --> 00:12:06,860
this isn't just a problem in InfoSec and

327
00:12:04,850 --> 00:12:08,839
in the investigation of 27 major

328
00:12:06,860 --> 00:12:11,180
aviation accidents by the National

329
00:12:08,839 --> 00:12:15,079
Transportation Safety Board in the

330
00:12:11,180 --> 00:12:17,628
United States 96 percent cited humans as

331
00:12:15,079 --> 00:12:20,300
the probable cause of the accident an 81

332
00:12:17,629 --> 00:12:23,480
percent of those accidents it--is human

333
00:12:20,300 --> 00:12:26,000
cited humans as the sole cause similar

334
00:12:23,480 --> 00:12:28,160
stats are found in InfoSec so 90% of the

335
00:12:26,000 --> 00:12:31,279
breaches reported to the US Information

336
00:12:28,160 --> 00:12:33,980
Commissioner's Office between 2017 and

337
00:12:31,279 --> 00:12:36,350
2018 cited human error is the cause of a

338
00:12:33,980 --> 00:12:37,790
breach Verizon if you're familiar with

339
00:12:36,350 --> 00:12:41,149
their data breach investigations report

340
00:12:37,790 --> 00:12:44,360
tends to dig a little deeper and they

341
00:12:41,149 --> 00:12:46,189
cited that 21% of breaches were caused

342
00:12:44,360 --> 00:12:47,689
by human error which is better but

343
00:12:46,189 --> 00:12:49,459
they're still probably not digging deep

344
00:12:47,689 --> 00:12:52,699
enough into the systemic factors that

345
00:12:49,459 --> 00:12:54,679
led to the breach when we blame human

346
00:12:52,699 --> 00:12:57,019
error were really tacitly absolving

347
00:12:54,679 --> 00:12:59,480
technolo technology from any culpability

348
00:12:57,019 --> 00:13:02,600
which could have sneaked lien courage us

349
00:12:59,480 --> 00:13:04,759
us towards behavioral control technology

350
00:13:02,600 --> 00:13:06,620
accumulation and misguided automation

351
00:13:04,759 --> 00:13:08,059
the underlying theories that if the

352
00:13:06,620 --> 00:13:09,709
human can be removed from the process

353
00:13:08,059 --> 00:13:11,149
and we just get rid of the you know

354
00:13:09,709 --> 00:13:13,729
stupid idiot who's making all these

355
00:13:11,149 --> 00:13:16,610
mistakes then era doesn't have a chance

356
00:13:13,730 --> 00:13:18,139
to occur I think our eternal hope is

357
00:13:16,610 --> 00:13:20,490
that there's some tool around the corner

358
00:13:18,139 --> 00:13:23,130
that's suddenly going to save us from

359
00:13:20,490 --> 00:13:25,470
you know all of these humans which

360
00:13:23,130 --> 00:13:28,490
brings us to unhealthy coping mechanism

361
00:13:25,470 --> 00:13:31,500
number two which is behavioral control

362
00:13:28,490 --> 00:13:33,089
so as karma Henrickson and his

363
00:13:31,500 --> 00:13:34,620
colleagues said and approached aimed at

364
00:13:33,089 --> 00:13:37,050
the individual is the equivalent of

365
00:13:34,620 --> 00:13:38,910
swatting individual mosquitoes rather

366
00:13:37,050 --> 00:13:40,890
than draining the swamp to address the

367
00:13:38,910 --> 00:13:43,529
source of the problem it's not very

368
00:13:40,890 --> 00:13:46,290
productive a sneaky way to tell yourself

369
00:13:43,529 --> 00:13:48,570
you're not blaming humans is to focus on

370
00:13:46,290 --> 00:13:50,459
protocol and policy violations which

371
00:13:48,570 --> 00:13:52,500
feel neutrally stated but still really

372
00:13:50,459 --> 00:13:54,930
at a fundamental level are blaming

373
00:13:52,500 --> 00:13:56,820
humans if you believe the sorts of

374
00:13:54,930 --> 00:13:59,099
failures by someone violating a single

375
00:13:56,820 --> 00:14:01,410
policy then the typical conclusion is to

376
00:13:59,100 --> 00:14:03,810
resort to disciplinary measures training

377
00:14:01,410 --> 00:14:06,240
programs and writing new procedures that

378
00:14:03,810 --> 00:14:09,719
specify even narrower bands of behavior

379
00:14:06,240 --> 00:14:11,459
I think evidence of info sex

380
00:14:09,720 --> 00:14:13,649
predilection towards believe is

381
00:14:11,459 --> 00:14:15,569
believing policy violations are the

382
00:14:13,649 --> 00:14:17,220
cause of a lot of incidents is the fact

383
00:14:15,570 --> 00:14:19,860
that we have this cornucopia of security

384
00:14:17,220 --> 00:14:21,690
education and awareness programs there

385
00:14:19,860 --> 00:14:25,230
security awareness training exercises

386
00:14:21,690 --> 00:14:26,730
videos even at escape rooms now not to

387
00:14:25,230 --> 00:14:29,070
mention the existence of cyber security

388
00:14:26,730 --> 00:14:29,550
awareness month itself which is this

389
00:14:29,070 --> 00:14:31,290
month

390
00:14:29,550 --> 00:14:33,899
I think telling people to care about

391
00:14:31,290 --> 00:14:35,189
security is about as effective as what

392
00:14:33,899 --> 00:14:37,470
I'll tell you later which is to think in

393
00:14:35,190 --> 00:14:39,420
systems so I'm aware of the irony but

394
00:14:37,470 --> 00:14:42,720
really you can't just tell people to

395
00:14:39,420 --> 00:14:46,020
think differently restricting human

396
00:14:42,720 --> 00:14:47,670
behavior alone just will never work

397
00:14:46,020 --> 00:14:49,920
the study of human factors I think is

398
00:14:47,670 --> 00:14:51,479
quietly growing in InfoSec but most

399
00:14:49,920 --> 00:14:53,360
security programs still fail to

400
00:14:51,480 --> 00:14:57,120
incorporate an understanding of human

401
00:14:53,360 --> 00:14:59,220
strengths and limitations I think the

402
00:14:57,120 --> 00:15:01,560
focus is too often on trying to force

403
00:14:59,220 --> 00:15:03,660
humans to fit the mold of ideal behavior

404
00:15:01,560 --> 00:15:05,969
rather than designing systems that

405
00:15:03,660 --> 00:15:07,949
minimize errors and inefficiency based

406
00:15:05,970 --> 00:15:10,470
on people how people actually behave

407
00:15:07,950 --> 00:15:13,320
their visual cognitive and even motor

408
00:15:10,470 --> 00:15:15,060
ability motor abilities part of this I

409
00:15:13,320 --> 00:15:16,770
think is due to status quo bias which is

410
00:15:15,060 --> 00:15:19,459
the tendency to keep doing things the

411
00:15:16,770 --> 00:15:22,020
same way as they've always been done

412
00:15:19,459 --> 00:15:23,609
formal policies in general are rarely

413
00:15:22,020 --> 00:15:26,040
written by people who are in the flow of

414
00:15:23,610 --> 00:15:27,540
the work being policed seldom do you

415
00:15:26,040 --> 00:15:29,189
release these security professionals

416
00:15:27,540 --> 00:15:31,020
shadowing the workers who behavior

417
00:15:29,190 --> 00:15:33,150
they're regulating to understand their

418
00:15:31,020 --> 00:15:33,850
priorities and challenges their goals

419
00:15:33,150 --> 00:15:37,810
and their kin

420
00:15:33,850 --> 00:15:39,519
streets for the most part security

421
00:15:37,810 --> 00:15:41,949
professionals rests at what's called the

422
00:15:39,519 --> 00:15:44,440
blunt end of systems thank policymakers

423
00:15:41,949 --> 00:15:47,229
administrators regulators tech creators

424
00:15:44,440 --> 00:15:49,389
or tech suppliers the sharp end in

425
00:15:47,230 --> 00:15:51,670
contrast is made up the made up of the

426
00:15:49,389 --> 00:15:53,290
individual operators of systems for an

427
00:15:51,670 --> 00:15:55,300
analogy think of in health care the

428
00:15:53,290 --> 00:15:56,740
surgeon is at the sharp end while the

429
00:15:55,300 --> 00:15:59,130
healthcare policy maker and the

430
00:15:56,740 --> 00:16:01,149
government is at the blunt end

431
00:15:59,130 --> 00:16:03,189
unfortunately people tend to blame

432
00:16:01,149 --> 00:16:05,680
whoever lies closest to the error which

433
00:16:03,190 --> 00:16:07,480
tends to be at the sharp end think of

434
00:16:05,680 --> 00:16:09,099
the pilot in the cockpit the surgeon as

435
00:16:07,480 --> 00:16:11,199
I mentioned we're an InfoSec the

436
00:16:09,100 --> 00:16:13,540
developer the executives assistant or

437
00:16:11,199 --> 00:16:15,880
the accountant but these operators tend

438
00:16:13,540 --> 00:16:19,599
to be the inheritors of system defects

439
00:16:15,880 --> 00:16:20,949
not the creators of them these defects

440
00:16:19,600 --> 00:16:22,630
were really created by factors well

441
00:16:20,949 --> 00:16:24,930
outside of the control of the operators

442
00:16:22,630 --> 00:16:27,459
themselves such as bad implementation

443
00:16:24,930 --> 00:16:30,089
unsatisfactory design poor maintenance

444
00:16:27,459 --> 00:16:32,859
and undesirable management decisions as

445
00:16:30,089 --> 00:16:34,899
James reason put it the human error by

446
00:16:32,860 --> 00:16:36,819
operators adds a final garnish to the

447
00:16:34,899 --> 00:16:39,959
lethal brew whose ingredients have

448
00:16:36,819 --> 00:16:43,269
already been long in the cooking

449
00:16:39,959 --> 00:16:45,880
returning to equifax an example they did

450
00:16:43,269 --> 00:16:48,220
have a 48-hour patching policy which

451
00:16:45,880 --> 00:16:50,529
obviously was not followed I think for

452
00:16:48,220 --> 00:16:52,899
months on end it wasn't followed for the

453
00:16:50,529 --> 00:16:54,100
Apache stress vulnerability but there

454
00:16:52,899 --> 00:16:56,589
appeared to be no real enforcement

455
00:16:54,100 --> 00:16:58,689
mechanism for the policy nor some sort

456
00:16:56,589 --> 00:17:01,360
of reminder and work work streams and

457
00:16:58,689 --> 00:17:02,800
definitely no automation I think

458
00:17:01,360 --> 00:17:04,870
creating words on a piece of paper and

459
00:17:02,800 --> 00:17:07,178
expecting them to be followed is kind of

460
00:17:04,869 --> 00:17:09,339
an overly ambitious strategy but maybe

461
00:17:07,179 --> 00:17:11,799
that's just me for example integration

462
00:17:09,339 --> 00:17:13,569
testing its automated in CI ACD

463
00:17:11,799 --> 00:17:15,939
pipelines because it's a mandatory

464
00:17:13,569 --> 00:17:17,879
hurdle given its importance but if you

465
00:17:15,939 --> 00:17:20,380
just told people like oh yeah implement

466
00:17:17,880 --> 00:17:21,970
integration testing on your own manually

467
00:17:20,380 --> 00:17:25,390
they probably wouldn't follow it every

468
00:17:21,970 --> 00:17:27,760
time enforcement itself is really a

469
00:17:25,390 --> 00:17:30,610
byproduct of the focus on policy

470
00:17:27,760 --> 00:17:33,370
violations being the root cause of

471
00:17:30,610 --> 00:17:35,500
failure so disciplinary actions legal

472
00:17:33,370 --> 00:17:37,719
investigations cancellation of bonuses

473
00:17:35,500 --> 00:17:39,970
and other tactics to hold people solely

474
00:17:37,720 --> 00:17:41,590
accountable for failure don't really

475
00:17:39,970 --> 00:17:44,120
support the goal of modifying behavior

476
00:17:41,590 --> 00:17:45,830
it encourages fear and

477
00:17:44,120 --> 00:17:47,870
sentiment suppression of information

478
00:17:45,830 --> 00:17:48,980
that could actually be valuable to

479
00:17:47,870 --> 00:17:50,209
detect things early

480
00:17:48,980 --> 00:17:52,790
they could be seeking other employment

481
00:17:50,210 --> 00:17:55,400
as well as quieter ways of them working

482
00:17:52,790 --> 00:18:00,470
around whatever your security standards

483
00:17:55,400 --> 00:18:02,750
are this is a case study in 2016 there

484
00:18:00,470 --> 00:18:04,820
is a lawsuit filed against s s and C

485
00:18:02,750 --> 00:18:06,740
which is a financial technology company

486
00:18:04,820 --> 00:18:08,480
alleging that a failure to follow their

487
00:18:06,740 --> 00:18:11,270
own policies led to a business email

488
00:18:08,480 --> 00:18:13,490
compromised scam and millions of dollars

489
00:18:11,270 --> 00:18:15,470
were lost in it s essence he mayn't

490
00:18:13,490 --> 00:18:17,540
maintained a plenty of written policies

491
00:18:15,470 --> 00:18:18,470
just like Equifax regarding the need for

492
00:18:17,540 --> 00:18:20,510
four people

493
00:18:18,470 --> 00:18:23,150
to verify a transaction before it could

494
00:18:20,510 --> 00:18:25,640
be completed and also to check the email

495
00:18:23,150 --> 00:18:28,809
for signs of fraud but these policies

496
00:18:25,640 --> 00:18:31,040
obviously failed to prevent the incident

497
00:18:28,809 --> 00:18:32,210
policy violations was deemed the root

498
00:18:31,040 --> 00:18:34,370
cause of it but a closer examination

499
00:18:32,210 --> 00:18:36,410
shows that there is little process or

500
00:18:34,370 --> 00:18:38,629
technology to help deter business email

501
00:18:36,410 --> 00:18:41,120
compromised solely implementing controls

502
00:18:38,630 --> 00:18:45,080
to regulate the human behavior doesn't

503
00:18:41,120 --> 00:18:47,300
actually beget resilience a classic

504
00:18:45,080 --> 00:18:49,428
example outside of InfoSec takes us to

505
00:18:47,300 --> 00:18:51,559
right after world war two so these two

506
00:18:49,429 --> 00:18:53,780
guys Paul Fitz and Richard Jones were

507
00:18:51,559 --> 00:18:56,270
like hey pilots didn't seem totally

508
00:18:53,780 --> 00:18:58,870
efficient during the war how can we

509
00:18:56,270 --> 00:19:01,160
actually help support better decisions

510
00:18:58,870 --> 00:19:02,600
the problem that they discovered is that

511
00:19:01,160 --> 00:19:05,240
the location of controls between

512
00:19:02,600 --> 00:19:06,500
cockpits were different it would change

513
00:19:05,240 --> 00:19:09,020
between cockpits and that would

514
00:19:06,500 --> 00:19:11,360
obviously lead to confusion and use of

515
00:19:09,020 --> 00:19:13,340
the wrong control so they decided to dig

516
00:19:11,360 --> 00:19:15,199
deeper and they focus on improving the

517
00:19:13,340 --> 00:19:17,330
design of the displays and the controls

518
00:19:15,200 --> 00:19:19,370
rather than trying to train away human

519
00:19:17,330 --> 00:19:21,980
error in getting the pilots comfortable

520
00:19:19,370 --> 00:19:23,659
switching between different planes so

521
00:19:21,980 --> 00:19:25,640
this design overall helped pilots

522
00:19:23,660 --> 00:19:29,600
process more information more quickly

523
00:19:25,640 --> 00:19:31,429
which supported smarter action that

524
00:19:29,600 --> 00:19:33,320
isn't to say policies aren't useful I

525
00:19:31,429 --> 00:19:35,300
know security loves its policies so I

526
00:19:33,320 --> 00:19:37,610
don't want anyone to be too mad at me

527
00:19:35,300 --> 00:19:39,919
and communicating expert guidance is

528
00:19:37,610 --> 00:19:41,959
absolutely important but it has to be

529
00:19:39,920 --> 00:19:45,050
tethered to reality you can't ignore the

530
00:19:41,960 --> 00:19:46,880
demands dilemmas and workflows of the

531
00:19:45,050 --> 00:19:51,080
users who fall under the policies

532
00:19:46,880 --> 00:19:52,580
purview as a good example checklist can

533
00:19:51,080 --> 00:19:54,169
be a valuable aid and this is shown

534
00:19:52,580 --> 00:19:56,010
across a bunch of industries to help

535
00:19:54,170 --> 00:19:58,470
practitioners remember import'

536
00:19:56,010 --> 00:20:00,150
steps in their work but checklists are

537
00:19:58,470 --> 00:20:01,890
only productive if they're built on an

538
00:20:00,150 --> 00:20:03,720
understanding of the actual work flows

539
00:20:01,890 --> 00:20:06,530
that are in place and they insert

540
00:20:03,720 --> 00:20:09,690
security at the appropriate point

541
00:20:06,530 --> 00:20:12,030
ultimately policies must encourage safer

542
00:20:09,690 --> 00:20:14,309
context not lured over human behavior

543
00:20:12,030 --> 00:20:16,080
with an iron fist across other

544
00:20:14,309 --> 00:20:18,389
disciplines which have wrestled with

545
00:20:16,080 --> 00:20:20,250
this problem of human error engineering

546
00:20:18,390 --> 00:20:22,530
controls end up being the preferred

547
00:20:20,250 --> 00:20:23,940
method of reducing failure and this will

548
00:20:22,530 --> 00:20:25,590
frame our discussion of how to cope with

549
00:20:23,940 --> 00:20:26,600
security failure better in the next

550
00:20:25,590 --> 00:20:29,399
section

551
00:20:26,600 --> 00:20:31,110
but finally for the last unhealthy

552
00:20:29,400 --> 00:20:34,620
coping mechanism is the just world

553
00:20:31,110 --> 00:20:37,168
hypothesis attempting to find the

554
00:20:34,620 --> 00:20:38,790
ultimate seed of failure whether the

555
00:20:37,169 --> 00:20:41,130
seed is an errant human or a policy

556
00:20:38,790 --> 00:20:43,290
violation is really a mechanism for

557
00:20:41,130 --> 00:20:44,760
coping with fear if we accept that

558
00:20:43,290 --> 00:20:47,159
everything kind of hangs in this

559
00:20:44,760 --> 00:20:48,720
delicate balance and that the same kind

560
00:20:47,160 --> 00:20:52,919
of things can lead to both success and

561
00:20:48,720 --> 00:20:54,900
failure we realize that it's not due to

562
00:20:52,919 --> 00:20:56,669
a single event that failure happens it's

563
00:20:54,900 --> 00:20:58,169
this kind of interrelation of factors

564
00:20:56,669 --> 00:21:00,540
the fact that all of these components

565
00:20:58,169 --> 00:21:02,370
align in a way to create failure and

566
00:21:00,540 --> 00:21:05,178
that means the world feels like a more

567
00:21:02,370 --> 00:21:07,770
disordered and dangerous place

568
00:21:05,179 --> 00:21:10,830
this concept obviously naturally fuels

569
00:21:07,770 --> 00:21:12,629
fear humans tend to prefer believing

570
00:21:10,830 --> 00:21:14,610
that the world is an orderly just and

571
00:21:12,630 --> 00:21:17,610
consequential place which is what's

572
00:21:14,610 --> 00:21:19,110
known as the just world hypothesis so

573
00:21:17,610 --> 00:21:21,000
believing that the same components and

574
00:21:19,110 --> 00:21:23,280
procedures can lead to success in

575
00:21:21,000 --> 00:21:26,640
failure that there's not just a single

576
00:21:23,280 --> 00:21:30,330
event that sets destiny violates this

577
00:21:26,640 --> 00:21:34,710
just world hypothesis so how many of you

578
00:21:30,330 --> 00:21:36,059
have seen the HBO series Chernobyl okay

579
00:21:34,710 --> 00:21:38,370
a lot of you perfect

580
00:21:36,059 --> 00:21:39,690
so the contributors to failure existed

581
00:21:38,370 --> 00:21:41,489
well before the days or even weeks

582
00:21:39,690 --> 00:21:44,040
leading up to the meltdown and it was

583
00:21:41,490 --> 00:21:45,660
far from a linear process as the

584
00:21:44,040 --> 00:21:48,030
researcher Kim Vincente points out

585
00:21:45,660 --> 00:21:49,950
reduced operating power a disabled

586
00:21:48,030 --> 00:21:52,350
safety system the lack of information

587
00:21:49,950 --> 00:21:54,200
about system state missing feedback for

588
00:21:52,350 --> 00:21:56,668
operator actions unfamiliar

589
00:21:54,200 --> 00:21:58,470
unfamiliarity among operators for such

590
00:21:56,669 --> 00:22:00,299
an anomalous situation and the

591
00:21:58,470 --> 00:22:02,490
complexity of the nuclear reactor itself

592
00:22:00,299 --> 00:22:05,158
all contributed to failure it just

593
00:22:02,490 --> 00:22:07,169
wasn't one single event so any one of

594
00:22:05,159 --> 00:22:08,280
these factors in isolation may not have

595
00:22:07,169 --> 00:22:10,020
actually led to a melt

596
00:22:08,280 --> 00:22:11,760
down it was alignment of these kind of

597
00:22:10,020 --> 00:22:16,590
unfortunate stars that led to this

598
00:22:11,760 --> 00:22:19,379
disaster errors she'd probably seen as

599
00:22:16,590 --> 00:22:21,360
the kind of to be expected by-product or

600
00:22:19,380 --> 00:22:24,480
side effect of the proceed of success

601
00:22:21,360 --> 00:22:26,280
under resource constraints the routes to

602
00:22:24,480 --> 00:22:28,170
failure are generally longer and more

603
00:22:26,280 --> 00:22:30,300
circuitous than we conceived which

604
00:22:28,170 --> 00:22:33,120
requires us to zoom way out and start

605
00:22:30,300 --> 00:22:35,370
looking at a systems level which brings

606
00:22:33,120 --> 00:22:37,590
us to how we can more productively deal

607
00:22:35,370 --> 00:22:39,810
with security failures discarding human

608
00:22:37,590 --> 00:22:43,770
error behavioral control mechanisms and

609
00:22:39,810 --> 00:22:53,520
the just world hypothesis to do so now

610
00:22:43,770 --> 00:22:55,080
it's time to make failure epic my view

611
00:22:53,520 --> 00:22:56,700
is that InfoSec will start making

612
00:22:55,080 --> 00:22:58,649
progress when we make sure that the easy

613
00:22:56,700 --> 00:23:00,650
way is also the secure way that the

614
00:22:58,650 --> 00:23:02,850
insecure way is exceptionally difficult

615
00:23:00,650 --> 00:23:05,160
we need to offer our hard-earned

616
00:23:02,850 --> 00:23:06,899
expertise to our colleagues to help them

617
00:23:05,160 --> 00:23:10,290
navigate the complexity of systems to

618
00:23:06,900 --> 00:23:11,820
reach their goals so to do so we have to

619
00:23:10,290 --> 00:23:14,370
embrace a system perspective the

620
00:23:11,820 --> 00:23:16,350
importance of security UX security chaos

621
00:23:14,370 --> 00:23:19,530
engineering in a blameless culture so

622
00:23:16,350 --> 00:23:22,770
I'll be covering all of these now first

623
00:23:19,530 --> 00:23:24,600
up is the system perspective again

624
00:23:22,770 --> 00:23:26,639
security failure is never the result of

625
00:23:24,600 --> 00:23:29,040
one factor it's never solely because of

626
00:23:26,640 --> 00:23:31,140
one vulnerability or a single alert

627
00:23:29,040 --> 00:23:32,909
being dismissed failure works more like

628
00:23:31,140 --> 00:23:37,140
a symphony with multiple multiple

629
00:23:32,910 --> 00:23:38,850
factors interacting and changing ways so

630
00:23:37,140 --> 00:23:40,470
security teams must adopt a systems

631
00:23:38,850 --> 00:23:43,199
perspective when seeking to understand

632
00:23:40,470 --> 00:23:44,790
security failure expanding your focus to

633
00:23:43,200 --> 00:23:47,070
really look at the relationships between

634
00:23:44,790 --> 00:23:51,030
components rather than pinning the blame

635
00:23:47,070 --> 00:23:52,500
to a single factor so probably most of

636
00:23:51,030 --> 00:23:54,960
you know what a system is but just in

637
00:23:52,500 --> 00:23:57,000
case a simplified definition is as a set

638
00:23:54,960 --> 00:23:59,060
of interdependent components interacting

639
00:23:57,000 --> 00:24:01,500
to achieve a common overarching goal

640
00:23:59,060 --> 00:24:03,090
systems thinking is not natural to most

641
00:24:01,500 --> 00:24:06,120
humans there's plenty of experimental

642
00:24:03,090 --> 00:24:07,770
evidence about that my experience has

643
00:24:06,120 --> 00:24:10,050
definitely extends to InfoSec

644
00:24:07,770 --> 00:24:11,400
professionals there are some generalists

645
00:24:10,050 --> 00:24:13,710
and InfoSec but I think as an industry

646
00:24:11,400 --> 00:24:15,000
we tend to be pretty specialized which

647
00:24:13,710 --> 00:24:16,770
means that you're really only seeing a

648
00:24:15,000 --> 00:24:18,810
narrow slice of kind of the overall

649
00:24:16,770 --> 00:24:19,590
security system you're not seeing how

650
00:24:18,810 --> 00:24:20,429
all of the different components

651
00:24:19,590 --> 00:24:22,740
interrelate

652
00:24:20,430 --> 00:24:26,160
the problem is only looking at one

653
00:24:22,740 --> 00:24:27,720
component will leave you blind Nancy

654
00:24:26,160 --> 00:24:29,880
Leveson said and narrow focus on

655
00:24:27,720 --> 00:24:31,890
operator actions physical component

656
00:24:29,880 --> 00:24:33,570
failures and technology may lead to

657
00:24:31,890 --> 00:24:35,640
ignoring some of the most important

658
00:24:33,570 --> 00:24:40,230
factors in terms of preventing future

659
00:24:35,640 --> 00:24:42,210
accidents a systems perspective can't

660
00:24:40,230 --> 00:24:44,460
just be limited to technology the way

661
00:24:42,210 --> 00:24:46,710
humans use technology also involves

662
00:24:44,460 --> 00:24:48,510
economic and social factors which are

663
00:24:46,710 --> 00:24:52,500
almost entirely ignored by traditional

664
00:24:48,510 --> 00:24:54,360
InfoSec teams so economic factors in an

665
00:24:52,500 --> 00:24:56,250
organization include revenue and profit

666
00:24:54,360 --> 00:24:58,850
goals how compensation schemes are

667
00:24:56,250 --> 00:25:00,930
structured or other budgetary decisions

668
00:24:58,850 --> 00:25:02,639
social factors in an organization

669
00:25:00,930 --> 00:25:04,200
include its key performance indicators

670
00:25:02,640 --> 00:25:06,090
the performance expectations on

671
00:25:04,200 --> 00:25:08,010
employees what sort of behavior is

672
00:25:06,090 --> 00:25:11,459
rewarded or punished or other cultural

673
00:25:08,010 --> 00:25:13,680
facets as an industry we tend to

674
00:25:11,460 --> 00:25:15,900
conceive of vulnerabilities as some sort

675
00:25:13,680 --> 00:25:18,120
of flaw borne by software but we

676
00:25:15,900 --> 00:25:19,380
overlook the incentives the

677
00:25:18,120 --> 00:25:22,320
vulnerability and incentivizing

678
00:25:19,380 --> 00:25:23,730
employees do more work and faster we

679
00:25:22,320 --> 00:25:25,770
overlook the vulnerability and giving

680
00:25:23,730 --> 00:25:28,740
bonuses to yes people and people who are

681
00:25:25,770 --> 00:25:30,900
politically savvy these vulnerabilities

682
00:25:28,740 --> 00:25:32,970
reduce the organization's resilience

683
00:25:30,900 --> 00:25:35,000
just as much as the software flaws but

684
00:25:32,970 --> 00:25:37,260
they rarely appear in our threat models

685
00:25:35,000 --> 00:25:39,090
occasionally we'll identify disgruntled

686
00:25:37,260 --> 00:25:40,740
employees as a potential insider threat

687
00:25:39,090 --> 00:25:43,129
without exploring the factors that

688
00:25:40,740 --> 00:25:45,360
actually led them to be so disgruntled

689
00:25:43,130 --> 00:25:48,360
so we buy tools with machine learning

690
00:25:45,360 --> 00:25:50,899
natural language processing AI to figure

691
00:25:48,360 --> 00:25:53,669
out which employee sound angry or upset

692
00:25:50,900 --> 00:25:56,250
detect insider threats early or

693
00:25:53,670 --> 00:25:58,140
reinstall what's essentially spyware to

694
00:25:56,250 --> 00:26:00,000
see if they're looking for other jobs

695
00:25:58,140 --> 00:26:01,860
and they're experiencing other kind of

696
00:26:00,000 --> 00:26:03,630
arbitrary symptoms in this blackbox

697
00:26:01,860 --> 00:26:05,790
module that would indicate they're an

698
00:26:03,630 --> 00:26:07,410
insider threat so I think we'd rather

699
00:26:05,790 --> 00:26:09,960
treat humans like their schrödinger's

700
00:26:07,410 --> 00:26:11,880
attacker then actually start to untangle

701
00:26:09,960 --> 00:26:15,390
all those factors that lead them to

702
00:26:11,880 --> 00:26:17,070
become an insider threat so I think this

703
00:26:15,390 --> 00:26:19,380
sort of strategy that exists today may

704
00:26:17,070 --> 00:26:20,909
placate those who are in charge but it's

705
00:26:19,380 --> 00:26:24,720
not actually helping improve our

706
00:26:20,910 --> 00:26:26,700
organizational security its mean the

707
00:26:24,720 --> 00:26:28,820
main point is that security is something

708
00:26:26,700 --> 00:26:31,399
a system does not something

709
00:26:28,820 --> 00:26:33,740
system has security has to become a

710
00:26:31,399 --> 00:26:36,110
value rather than a commodity something

711
00:26:33,740 --> 00:26:38,240
we continually strive to uphold in our

712
00:26:36,110 --> 00:26:42,168
organizations rather than something with

713
00:26:38,240 --> 00:26:43,549
an expected end state I think we have to

714
00:26:42,169 --> 00:26:45,320
think in terms of helping our system

715
00:26:43,549 --> 00:26:48,080
operate safely rather than adding

716
00:26:45,320 --> 00:26:49,939
security to our systems with this view

717
00:26:48,080 --> 00:26:50,720
you can see how failure unfolds within

718
00:26:49,940 --> 00:26:52,009
the system

719
00:26:50,720 --> 00:26:55,159
which allows you to begin identifying

720
00:26:52,009 --> 00:26:56,389
points at which the progression of the

721
00:26:55,159 --> 00:26:58,519
failure might have been stopped or

722
00:26:56,389 --> 00:27:00,168
somehow diverted and I think this helps

723
00:26:58,519 --> 00:27:01,759
inform what signals we can start

724
00:27:00,169 --> 00:27:05,600
collecting early to help identify

725
00:27:01,759 --> 00:27:07,279
failure earlier this is the risk of

726
00:27:05,600 --> 00:27:09,289
poorly chosen health metrics and other

727
00:27:07,279 --> 00:27:10,669
security vanity metrics they don't

728
00:27:09,289 --> 00:27:13,009
provide you with the full picture of

729
00:27:10,669 --> 00:27:14,779
organizational security the percent of

730
00:27:13,009 --> 00:27:16,100
users clicking on phishing links in your

731
00:27:14,779 --> 00:27:17,870
organization doesn't tell you whether

732
00:27:16,100 --> 00:27:19,490
your organization will experience a

733
00:27:17,870 --> 00:27:21,379
horrific incident if someone actually

734
00:27:19,490 --> 00:27:23,330
clicks on the link the percent of

735
00:27:21,379 --> 00:27:25,219
phishing that bypasses controls is far

736
00:27:23,330 --> 00:27:28,699
more valuable or even the percent of

737
00:27:25,220 --> 00:27:29,750
people that are reporting phishing also

738
00:27:28,700 --> 00:27:31,820
perhaps the number of vulnerabilities

739
00:27:29,750 --> 00:27:33,740
found matters less than their severity

740
00:27:31,820 --> 00:27:35,120
and how quickly they're being remediated

741
00:27:33,740 --> 00:27:39,799
how early in the software development

742
00:27:35,120 --> 00:27:41,899
lifecycle they're being fixed part of

743
00:27:39,799 --> 00:27:44,059
this is InfoSec needs to really analyze

744
00:27:41,899 --> 00:27:47,600
the mismatch between the organization's

745
00:27:44,059 --> 00:27:49,639
self perception and its reality as Tobey

746
00:27:47,600 --> 00:27:52,100
: Berg who's head of red teaming at

747
00:27:49,639 --> 00:27:54,590
Dropbox opined red teaming done properly

748
00:27:52,100 --> 00:27:56,509
adopts the mental of devil's advocate

749
00:27:54,590 --> 00:27:58,939
performing alternative analysis to

750
00:27:56,509 --> 00:28:00,620
enhance decision-making security teams

751
00:27:58,940 --> 00:28:02,240
not just red team should perform

752
00:28:00,620 --> 00:28:04,489
alternative analysis for their

753
00:28:02,240 --> 00:28:06,440
organization helping it view problems

754
00:28:04,490 --> 00:28:07,970
through a different lens and pushing

755
00:28:06,440 --> 00:28:12,590
back against the kool-aid drinking that

756
00:28:07,970 --> 00:28:15,049
can lead to complacency the alternative

757
00:28:12,590 --> 00:28:17,389
analysis must also include consideration

758
00:28:15,049 --> 00:28:18,889
of outside perspectives and red teaming

759
00:28:17,389 --> 00:28:21,199
this manifests is thinking like your

760
00:28:18,889 --> 00:28:23,000
adversary for defenders I think that

761
00:28:21,200 --> 00:28:25,129
should additionally manifest in user

762
00:28:23,000 --> 00:28:27,139
research understanding of the users

763
00:28:25,129 --> 00:28:29,209
building maintaining and using the

764
00:28:27,139 --> 00:28:30,860
systems in your organization so that the

765
00:28:29,210 --> 00:28:33,019
security program is not built on this

766
00:28:30,860 --> 00:28:36,258
fantasy view of how your organization

767
00:28:33,019 --> 00:28:38,659
works so adopting a systems perspective

768
00:28:36,259 --> 00:28:40,100
is the first step to the better coping

769
00:28:38,659 --> 00:28:40,820
with security failure because it allows

770
00:28:40,100 --> 00:28:42,320
you to see how

771
00:28:40,820 --> 00:28:44,799
combination of long-standing problems

772
00:28:42,320 --> 00:28:49,210
with acute events leads to failure

773
00:28:44,799 --> 00:28:49,210
security ux is the next step

774
00:28:49,240 --> 00:28:53,509
security has to recognize that much of

775
00:28:51,350 --> 00:28:54,769
security failure originates in competing

776
00:28:53,509 --> 00:28:57,440
goals and the harrowing pressure

777
00:28:54,769 --> 00:28:59,419
required to meet those goals the account

778
00:28:57,440 --> 00:29:01,279
new wires money out to attackers

779
00:28:59,419 --> 00:29:03,139
due to business email compromised does

780
00:29:01,279 --> 00:29:05,480
so for a reason and it's not because

781
00:29:03,139 --> 00:29:07,100
they're stupid what drives their

782
00:29:05,480 --> 00:29:08,870
promotion opportunities what drives

783
00:29:07,100 --> 00:29:10,459
their firing does the accounting team

784
00:29:08,870 --> 00:29:13,428
measure success based on minimizing

785
00:29:10,460 --> 00:29:15,409
transaction time many but certainly not

786
00:29:13,429 --> 00:29:17,750
all performance goals in an organization

787
00:29:15,409 --> 00:29:19,429
will be in conflict with the security so

788
00:29:17,750 --> 00:29:22,610
your job as defenders is to help your

789
00:29:19,429 --> 00:29:24,320
colleagues cope with this mess a core

790
00:29:22,610 --> 00:29:26,240
principle to understand is that human

791
00:29:24,320 --> 00:29:28,519
attention is a finite and precious

792
00:29:26,240 --> 00:29:30,559
resource just because you believe that

793
00:29:28,519 --> 00:29:32,360
security should be the priority it

794
00:29:30,559 --> 00:29:33,590
doesn't mean it will when so many other

795
00:29:32,360 --> 00:29:37,279
things are competing for limited

796
00:29:33,590 --> 00:29:40,189
cognitive bandwidth what sort of events

797
00:29:37,279 --> 00:29:42,379
draw practitioners attention how can you

798
00:29:40,190 --> 00:29:44,720
instead draw attention towards security

799
00:29:42,379 --> 00:29:46,370
concerns instead you can't answer these

800
00:29:44,720 --> 00:29:49,700
questions without user research

801
00:29:46,370 --> 00:29:51,139
otherwise it will just be guesswork UX

802
00:29:49,700 --> 00:29:53,389
is more than just figuring out like

803
00:29:51,139 --> 00:29:55,459
button placement which is more UI anyway

804
00:29:53,389 --> 00:29:57,678
explores how information should be

805
00:29:55,460 --> 00:29:59,740
presented and how to help practitioners

806
00:29:57,679 --> 00:30:03,649
better perform their work

807
00:29:59,740 --> 00:30:05,330
so Katie McCaffrey at Microsoft had this

808
00:30:03,649 --> 00:30:07,039
great tweet recently she said daily

809
00:30:05,330 --> 00:30:09,289
reminder for DevOps and InfoSec people

810
00:30:07,039 --> 00:30:11,029
design any tools alerts that always show

811
00:30:09,289 --> 00:30:13,639
up red don't make your systems more

812
00:30:11,029 --> 00:30:15,470
reliable or secure they just teach

813
00:30:13,639 --> 00:30:17,870
people to ignore alerts

814
00:30:15,470 --> 00:30:19,549
I think her words are pretty wise we

815
00:30:17,870 --> 00:30:21,199
have to always consider what behavior

816
00:30:19,549 --> 00:30:23,899
we're encouraging what other cognitive

817
00:30:21,200 --> 00:30:25,580
demands are present on the user and how

818
00:30:23,899 --> 00:30:29,600
we can incentivize the user to turn

819
00:30:25,580 --> 00:30:31,220
their intention towards security so I

820
00:30:29,600 --> 00:30:33,350
recently read an article that Raytheon

821
00:30:31,220 --> 00:30:35,120
designed an aircraft safety system to

822
00:30:33,350 --> 00:30:37,908
flash a big red alert that proclaimed

823
00:30:35,120 --> 00:30:40,279
cyber anomaly in all caps she's kind of

824
00:30:37,909 --> 00:30:42,710
ridiculous big red alerts may attract

825
00:30:40,279 --> 00:30:44,210
attention but what is the pilot really

826
00:30:42,710 --> 00:30:46,429
supposed to do with this information

827
00:30:44,210 --> 00:30:48,379
how should the pilot react what are they

828
00:30:46,429 --> 00:30:50,240
supposed to investigate does it apply

829
00:30:48,379 --> 00:30:51,918
solely to life or death scenarios should

830
00:30:50,240 --> 00:30:54,410
they assume there are no false positive

831
00:30:51,919 --> 00:30:57,830
creates a lot more questions I think

832
00:30:54,410 --> 00:30:59,390
sirs all these considerations can be

833
00:30:57,830 --> 00:31:01,490
realized through choice architecture

834
00:30:59,390 --> 00:31:04,010
which i think is sorely underutilized

835
00:31:01,490 --> 00:31:05,930
tool with an InfoSec trace architects

836
00:31:04,010 --> 00:31:07,250
have the responsibility for organizing

837
00:31:05,930 --> 00:31:10,280
the context in which people make

838
00:31:07,250 --> 00:31:11,960
decisions nudging users towards desired

839
00:31:10,280 --> 00:31:14,240
behavior through smart design can

840
00:31:11,960 --> 00:31:16,280
encourage security in a very natural way

841
00:31:14,240 --> 00:31:20,210
rather than feeling like some sort of

842
00:31:16,280 --> 00:31:21,649
bolted on intrusion one of the more

843
00:31:20,210 --> 00:31:23,450
powerful tools in the choice

844
00:31:21,650 --> 00:31:25,970
architecture Arsenal is the use of

845
00:31:23,450 --> 00:31:28,700
defaults placing the ideal behavior on

846
00:31:25,970 --> 00:31:30,590
the path of least resistance so

847
00:31:28,700 --> 00:31:32,180
requiring two-factor authentication to

848
00:31:30,590 --> 00:31:34,459
set up an account represents the use of

849
00:31:32,180 --> 00:31:36,500
default implementing security tests in

850
00:31:34,460 --> 00:31:38,780
to see ICD pipelines as another form of

851
00:31:36,500 --> 00:31:40,130
default users of course will probably

852
00:31:38,780 --> 00:31:41,710
still be upset about some security

853
00:31:40,130 --> 00:31:44,330
policies getting in the way of your work

854
00:31:41,710 --> 00:31:46,520
but your job is to balance reasonable

855
00:31:44,330 --> 00:31:50,360
reality grounded policy with ensuring

856
00:31:46,520 --> 00:31:51,590
systems operate safely once you

857
00:31:50,360 --> 00:31:53,090
understand that humans tend to make

858
00:31:51,590 --> 00:31:55,040
slips which are unintended actions

859
00:31:53,090 --> 00:31:57,139
rather than mistakes which are

860
00:31:55,040 --> 00:31:59,120
inappropriate intentions you start to

861
00:31:57,140 --> 00:32:00,890
focus your intention attention on the

862
00:31:59,120 --> 00:32:02,989
design of systems with which humans

863
00:32:00,890 --> 00:32:06,770
interact that includes software devices

864
00:32:02,990 --> 00:32:08,630
protocols and also work environments so

865
00:32:06,770 --> 00:32:10,400
tactics to reduce slippage include

866
00:32:08,630 --> 00:32:12,290
checklist as memory aids forcing

867
00:32:10,400 --> 00:32:14,390
functions and default defaults to reduce

868
00:32:12,290 --> 00:32:16,280
workarounds moving complexity and

869
00:32:14,390 --> 00:32:18,620
functionality sprawl from software and

870
00:32:16,280 --> 00:32:19,879
eliminating distractions at points of

871
00:32:18,620 --> 00:32:22,429
workflow that require the utmost

872
00:32:19,880 --> 00:32:24,500
attention think of when the you know

873
00:32:22,430 --> 00:32:26,960
stack ops analyst first sees that

874
00:32:24,500 --> 00:32:28,820
there's some sort of major horrible

875
00:32:26,960 --> 00:32:32,060
alert that happens they need their full

876
00:32:28,820 --> 00:32:34,580
attention on that when you design

877
00:32:32,060 --> 00:32:35,870
security procedures security software or

878
00:32:34,580 --> 00:32:37,460
Security's tests you're probably

879
00:32:35,870 --> 00:32:39,860
thinking of some sort of predetermined

880
00:32:37,460 --> 00:32:42,020
usage pattern for how people should be

881
00:32:39,860 --> 00:32:43,969
using it if it's poorly designed it

882
00:32:42,020 --> 00:32:46,280
doesn't cover all use cases that arise

883
00:32:43,970 --> 00:32:48,440
though users will be inventive and

884
00:32:46,280 --> 00:32:50,690
they'll find workarounds so strong

885
00:32:48,440 --> 00:32:53,360
security design conceives the potential

886
00:32:50,690 --> 00:32:55,880
for deviations supporting workarounds

887
00:32:53,360 --> 00:32:58,100
that allow flexibility but also minimize

888
00:32:55,880 --> 00:33:00,950
the risk arising from non-standard user

889
00:32:58,100 --> 00:33:02,600
strategies so in security this could

890
00:33:00,950 --> 00:33:05,210
take the form of letting the user select

891
00:33:02,600 --> 00:33:07,250
an option for a security exception for a

892
00:33:05,210 --> 00:33:08,100
short window of time which would trigger

893
00:33:07,250 --> 00:33:10,280
an alert to this

894
00:33:08,100 --> 00:33:12,840
dirty' team and additional monitoring

895
00:33:10,280 --> 00:33:14,910
dual security for example uses a

896
00:33:12,840 --> 00:33:17,428
self-service approval path to allow

897
00:33:14,910 --> 00:33:18,570
users to run a newer unknown binary that

898
00:33:17,429 --> 00:33:20,789
is not currently allowed by their

899
00:33:18,570 --> 00:33:22,860
whitelisting they paying the user with a

900
00:33:20,789 --> 00:33:24,140
slack bot to confirm that the user

901
00:33:22,860 --> 00:33:27,178
indeed wanted to run the application

902
00:33:24,140 --> 00:33:29,220
which helps detect unauthorized usage of

903
00:33:27,179 --> 00:33:31,559
the workaround the good news is they'll

904
00:33:29,220 --> 00:33:33,090
probably be open sourcing this soon but

905
00:33:31,559 --> 00:33:35,850
there are a few other companies included

906
00:33:33,090 --> 00:33:39,570
Dropbox that use slack bots similarly to

907
00:33:35,850 --> 00:33:42,330
this the reality is sometimes there

908
00:33:39,570 --> 00:33:44,070
really no clean workarounds and no clean

909
00:33:42,330 --> 00:33:46,110
solutions that don't create just a

910
00:33:44,070 --> 00:33:48,720
proper loophole that attackers could

911
00:33:46,110 --> 00:33:50,908
take advantage of so this really makes

912
00:33:48,720 --> 00:33:53,549
it an exercise and acceptable trade-offs

913
00:33:50,909 --> 00:33:55,380
the bypass is brief enough that only a

914
00:33:53,549 --> 00:33:57,120
motivated drive-by attacker could

915
00:33:55,380 --> 00:34:00,330
leverage it maybe that's an acceptable

916
00:33:57,120 --> 00:34:02,039
level of risk even so providing alerts

917
00:34:00,330 --> 00:34:03,840
to the user in question for example with

918
00:34:02,039 --> 00:34:06,150
a slack bot and the security team

919
00:34:03,840 --> 00:34:07,949
getting alert as well can help fight

920
00:34:06,150 --> 00:34:11,369
when workarounds are actually malicious

921
00:34:07,950 --> 00:34:13,109
activity so to recap security UX

922
00:34:11,369 --> 00:34:15,240
involves understanding user workflows

923
00:34:13,109 --> 00:34:17,129
what is capturing their attention and

924
00:34:15,239 --> 00:34:18,868
how to guide more secure behavior that

925
00:34:17,129 --> 00:34:21,149
still allows them to meet their primary

926
00:34:18,869 --> 00:34:22,740
goals the arrests are faith in these

927
00:34:21,149 --> 00:34:24,529
excuse of human error

928
00:34:22,739 --> 00:34:26,819
you'll never be forced to understand

929
00:34:24,530 --> 00:34:28,800
untangle all of this complexity and

930
00:34:26,820 --> 00:34:32,280
understand the systemic factors that

931
00:34:28,800 --> 00:34:34,050
lead to success or failure so this

932
00:34:32,280 --> 00:34:35,820
brings us to chaos security engineering

933
00:34:34,050 --> 00:34:38,250
to improve your understanding of your

934
00:34:35,820 --> 00:34:39,740
systems the context they create and how

935
00:34:38,250 --> 00:34:42,330
that context can lead to failure

936
00:34:39,739 --> 00:34:45,259
security chaos engineering is a very

937
00:34:42,330 --> 00:34:45,259
worthy pursuits

