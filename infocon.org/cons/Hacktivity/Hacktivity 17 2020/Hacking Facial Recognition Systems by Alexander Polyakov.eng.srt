1
00:00:02,399 --> 00:00:03,919
hi um

2
00:00:03,919 --> 00:00:06,319
so the the name of this topic is testing

3
00:00:06,319 --> 00:00:09,040
facial recognition system security

4
00:00:09,040 --> 00:00:11,360
so what i'm gonna tell you is like how

5
00:00:11,360 --> 00:00:13,440
to how you can actually test

6
00:00:13,440 --> 00:00:17,119
uh various ai solutions and the fetch

7
00:00:17,119 --> 00:00:17,920
recognition

8
00:00:17,920 --> 00:00:20,320
system is just one example so you can

9
00:00:20,320 --> 00:00:21,039
apply

10
00:00:21,039 --> 00:00:24,800
those methods to other solutions

11
00:00:24,800 --> 00:00:27,279
as you want

12
00:00:28,960 --> 00:00:32,000
my name is alex and i spent like 15

13
00:00:32,000 --> 00:00:32,558
years

14
00:00:32,558 --> 00:00:36,399
in cyber security and i'm not a big

15
00:00:36,399 --> 00:00:37,040
expert

16
00:00:37,040 --> 00:00:40,640
in ai however my first ai

17
00:00:40,640 --> 00:00:43,920
application was intrusion detection

18
00:00:43,920 --> 00:00:44,559
system

19
00:00:44,559 --> 00:00:47,760
which i wrote when i was a student

20
00:00:47,760 --> 00:00:50,559
so i pretty much understand like ai and

21
00:00:50,559 --> 00:00:51,680
cyber

22
00:00:51,680 --> 00:00:55,199
i'm a founder of adversary ai it's a

23
00:00:55,199 --> 00:00:58,719
ai security company based in israel

24
00:00:58,719 --> 00:01:01,680
i'm also author of two different books

25
00:01:01,680 --> 00:01:02,239
in

26
00:01:02,239 --> 00:01:05,280
cyber security one about the

27
00:01:05,280 --> 00:01:07,200
databases and another one about the

28
00:01:07,200 --> 00:01:08,799
applications

29
00:01:08,799 --> 00:01:12,080
and also a big fan of basically hacking

30
00:01:12,080 --> 00:01:15,200
uh everything from like biohacking

31
00:01:15,200 --> 00:01:17,439
uh to scientific biology like

32
00:01:17,439 --> 00:01:19,119
genetically modifying

33
00:01:19,119 --> 00:01:22,240
uh cells and bacterias

34
00:01:22,240 --> 00:01:24,080
and also being kind of neuroscience and

35
00:01:24,080 --> 00:01:26,240
so on so i i'm

36
00:01:26,240 --> 00:01:29,439
really excited of you know

37
00:01:29,439 --> 00:01:33,360
hacking various things and ai is just

38
00:01:33,360 --> 00:01:36,799
one of them and so our mission

39
00:01:36,799 --> 00:01:40,240
in adversar is to increase trust in ai

40
00:01:40,240 --> 00:01:41,119
systems by

41
00:01:41,119 --> 00:01:43,520
protecting from attacks and enhancing

42
00:01:43,520 --> 00:01:46,159
the security privacy and so on

43
00:01:46,159 --> 00:01:49,439
um and how i came to this

44
00:01:49,439 --> 00:01:52,560
uh to the importance of

45
00:01:52,560 --> 00:01:56,960
trust is basically um i spent a year

46
00:01:56,960 --> 00:01:59,680
in traveling all around the world and

47
00:01:59,680 --> 00:02:00,079
from

48
00:02:00,079 --> 00:02:03,280
various places from like deserts of oman

49
00:02:03,280 --> 00:02:04,799
to

50
00:02:04,799 --> 00:02:07,920
amazon and i was asking absolutely

51
00:02:07,920 --> 00:02:09,360
different people

52
00:02:09,360 --> 00:02:12,000
were they various questions about the

53
00:02:12,000 --> 00:02:13,599
future and one of the questions was

54
00:02:13,599 --> 00:02:16,160
would they sit in the self-driving car

55
00:02:16,160 --> 00:02:19,840
and most of people said no absolutely we

56
00:02:19,840 --> 00:02:20,319
don't

57
00:02:20,319 --> 00:02:23,920
trust it and then i realized that

58
00:02:23,920 --> 00:02:27,120
it it may sound funny you know uh

59
00:02:27,120 --> 00:02:30,000
that they don't trust it but i mean

60
00:02:30,000 --> 00:02:33,120
those are our potential lies

61
00:02:33,120 --> 00:02:36,000
and if people don't trust the new

62
00:02:36,000 --> 00:02:39,040
technologies it's the most important

63
00:02:39,040 --> 00:02:42,000
uh factor that we should solve because

64
00:02:42,000 --> 00:02:43,280
if we want

65
00:02:43,280 --> 00:02:45,599
solve it if we won't solve the problem

66
00:02:45,599 --> 00:02:46,400
of trust

67
00:02:46,400 --> 00:02:49,519
in ai we may experience the new ai

68
00:02:49,519 --> 00:02:50,480
winter

69
00:02:50,480 --> 00:02:52,480
so this is very important and that's why

70
00:02:52,480 --> 00:02:54,480
we decided to

71
00:02:54,480 --> 00:02:57,120
uh put our all our attention in

72
00:02:57,120 --> 00:02:58,800
increasing trust

73
00:02:58,800 --> 00:03:01,440
in ai and it's a very big topic so we

74
00:03:01,440 --> 00:03:02,319
mostly focus

75
00:03:02,319 --> 00:03:05,680
on the security part of ai transfer

76
00:03:05,680 --> 00:03:07,519
trustworthiness

77
00:03:07,519 --> 00:03:10,560
okay let's get back to the ground and

78
00:03:10,560 --> 00:03:13,680
this topic this presentation uh will be

79
00:03:13,680 --> 00:03:14,560
a little bit about

80
00:03:14,560 --> 00:03:18,000
ai security and then how to

81
00:03:18,000 --> 00:03:20,000
how we can basically assess the security

82
00:03:20,000 --> 00:03:21,519
of um

83
00:03:21,519 --> 00:03:23,360
facial recognition system and you can

84
00:03:23,360 --> 00:03:25,120
apply this approach to

85
00:03:25,120 --> 00:03:28,720
other solutions okay

86
00:03:28,720 --> 00:03:32,159
one more um important

87
00:03:32,159 --> 00:03:34,959
thing so if we take a traditional

88
00:03:34,959 --> 00:03:37,200
software

89
00:03:37,200 --> 00:03:39,519
uh right and we interact with this

90
00:03:39,519 --> 00:03:40,239
software

91
00:03:40,239 --> 00:03:43,280
using the graphical interface

92
00:03:43,280 --> 00:03:46,000
basically we have some inputs and we put

93
00:03:46,000 --> 00:03:46,400
some

94
00:03:46,400 --> 00:03:49,840
information some structured data in into

95
00:03:49,840 --> 00:03:50,239
those

96
00:03:50,239 --> 00:03:53,599
inputs and most of the vulnerabilities

97
00:03:53,599 --> 00:03:56,959
in the software industry is actually

98
00:03:56,959 --> 00:03:59,599
uh putting some kind of unstructured

99
00:03:59,599 --> 00:04:00,879
data

100
00:04:00,879 --> 00:04:04,000
um like sql injection so

101
00:04:04,000 --> 00:04:07,840
this the the system uh

102
00:04:07,840 --> 00:04:11,360
waiting for some data but we inject

103
00:04:11,360 --> 00:04:14,480
some unstructured data that the system

104
00:04:14,480 --> 00:04:18,320
uh don't want to see and if we take the

105
00:04:18,320 --> 00:04:20,079
ai solutions the all

106
00:04:20,079 --> 00:04:23,520
interaction between uh the systems

107
00:04:23,520 --> 00:04:27,199
will be like visual audio and

108
00:04:27,199 --> 00:04:31,759
some kind of uh like non-structured data

109
00:04:31,759 --> 00:04:34,479
and the thing is inside this

110
00:04:34,479 --> 00:04:37,120
unstructured data we also have the new

111
00:04:37,120 --> 00:04:39,199
types of attacks like evasion

112
00:04:39,199 --> 00:04:42,720
poisoning data exfiltration and

113
00:04:42,720 --> 00:04:46,320
this is a a big difference

114
00:04:46,320 --> 00:04:49,600
so we we don't even know how to

115
00:04:49,600 --> 00:04:52,800
basically solve the problem of

116
00:04:52,800 --> 00:04:55,840
uh vulnerabilities and injections in

117
00:04:55,840 --> 00:04:57,199
structured data

118
00:04:57,199 --> 00:05:00,880
but now in ai and actually in machine

119
00:05:00,880 --> 00:05:02,240
learning algorithms

120
00:05:02,240 --> 00:05:04,479
we have problems of filtering

121
00:05:04,479 --> 00:05:06,479
unstructured data

122
00:05:06,479 --> 00:05:09,840
and this is a new thread and if this is

123
00:05:09,840 --> 00:05:11,919
something where we need to

124
00:05:11,919 --> 00:05:14,479
focus a lot of attention because it's a

125
00:05:14,479 --> 00:05:15,440
completely

126
00:05:15,440 --> 00:05:17,840
new area with new types of attacks new

127
00:05:17,840 --> 00:05:18,880
types of

128
00:05:18,880 --> 00:05:22,000
security solutions and so on and this

129
00:05:22,000 --> 00:05:22,560
this

130
00:05:22,560 --> 00:05:25,120
now is just the beginning of of of this

131
00:05:25,120 --> 00:05:26,479
area actually

132
00:05:26,479 --> 00:05:30,400
so and even now we all already saw a

133
00:05:30,400 --> 00:05:31,759
number of examples

134
00:05:31,759 --> 00:05:34,080
uh affecting confidentiality integrity

135
00:05:34,080 --> 00:05:36,160
and availability of

136
00:05:36,160 --> 00:05:38,160
machine learning-based solution

137
00:05:38,160 --> 00:05:39,919
solutions so for example

138
00:05:39,919 --> 00:05:43,199
the uh the

139
00:05:43,199 --> 00:05:46,320
malware detection engine ai driven

140
00:05:46,320 --> 00:05:48,320
malware detection engine

141
00:05:48,320 --> 00:05:51,440
uh from silence was hacked

142
00:05:51,440 --> 00:05:54,560
and bypassed uh because of the

143
00:05:54,560 --> 00:05:56,000
vulnerability

144
00:05:56,000 --> 00:05:59,600
in the machine learning algorithm and

145
00:05:59,600 --> 00:06:02,880
there are various examples of of such

146
00:06:02,880 --> 00:06:06,960
attacks and also the number of research

147
00:06:06,960 --> 00:06:07,840
papers

148
00:06:07,840 --> 00:06:10,720
uh in in this space is really growing

149
00:06:10,720 --> 00:06:11,600
like we

150
00:06:11,600 --> 00:06:14,080
uh by the beginning of this year just by

151
00:06:14,080 --> 00:06:15,680
the beginning of this year we had

152
00:06:15,680 --> 00:06:17,680
two thousand research papers i think

153
00:06:17,680 --> 00:06:18,880
currently uh

154
00:06:18,880 --> 00:06:21,440
we don't have a final calculations but

155
00:06:21,440 --> 00:06:21,919
it's

156
00:06:21,919 --> 00:06:24,479
it's more than 3 000 research papers

157
00:06:24,479 --> 00:06:25,520
currently

158
00:06:25,520 --> 00:06:28,800
uh in this topic so this is really huge

159
00:06:28,800 --> 00:06:29,759
area

160
00:06:29,759 --> 00:06:33,759
and we can find examples of ai in every

161
00:06:33,759 --> 00:06:34,800
industry

162
00:06:34,800 --> 00:06:38,160
and basically why we choose the fashion

163
00:06:38,160 --> 00:06:39,360
recognition

164
00:06:39,360 --> 00:06:42,400
one of the uh facts that we choose it

165
00:06:42,400 --> 00:06:44,800
for for testing is because fashion

166
00:06:44,800 --> 00:06:46,560
recognition is basically

167
00:06:46,560 --> 00:06:49,759
almost everywhere you can you can

168
00:06:49,759 --> 00:06:52,880
you can see it on the slide uh

169
00:06:52,880 --> 00:06:56,240
then so what kind of attacks

170
00:06:56,240 --> 00:06:59,360
actually i'm i'm talking about right um

171
00:06:59,360 --> 00:07:02,400
so first one is evasion or

172
00:07:02,400 --> 00:07:06,720
it's also called adversarial examples

173
00:07:06,720 --> 00:07:09,360
and most of the research papers actually

174
00:07:09,360 --> 00:07:10,080
about the

175
00:07:10,080 --> 00:07:12,319
evasion attacks so basically when we

176
00:07:12,319 --> 00:07:13,120
full

177
00:07:13,120 --> 00:07:15,840
model detection or prediction and there

178
00:07:15,840 --> 00:07:16,639
will be

179
00:07:16,639 --> 00:07:20,080
examples uh in the news in another slide

180
00:07:20,080 --> 00:07:23,599
the second uh most popular attack is

181
00:07:23,599 --> 00:07:24,720
poisoning when we

182
00:07:24,720 --> 00:07:27,680
retrain a model on malicious data then

183
00:07:27,680 --> 00:07:29,199
we have a model inversion

184
00:07:29,199 --> 00:07:31,230
when we steal the data

185
00:07:31,230 --> 00:07:33,199
[Music]

186
00:07:33,199 --> 00:07:36,720
the the model uh data set

187
00:07:36,720 --> 00:07:40,080
actually and then we have a back drawers

188
00:07:40,080 --> 00:07:43,360
um when we inject the data

189
00:07:43,360 --> 00:07:46,400
in in the model uh inject

190
00:07:46,400 --> 00:07:49,440
like a new behavior in the model and and

191
00:07:49,440 --> 00:07:51,919
also we have a model extraction

192
00:07:51,919 --> 00:07:55,120
like we uh still model

193
00:07:55,120 --> 00:07:58,639
internal parameters

194
00:07:58,639 --> 00:08:01,919
so in this presentation we will only

195
00:08:01,919 --> 00:08:04,160
cover the evasion attack

196
00:08:04,160 --> 00:08:07,199
and i think most of you uh hopefully

197
00:08:07,199 --> 00:08:10,400
heard about that but very simply

198
00:08:10,400 --> 00:08:13,919
uh in this attack uh what we're trying

199
00:08:13,919 --> 00:08:14,639
to do

200
00:08:14,639 --> 00:08:17,919
is we're trying to change the

201
00:08:17,919 --> 00:08:19,919
picture if we talk about the computer

202
00:08:19,919 --> 00:08:21,039
vision we're trying to change the

203
00:08:21,039 --> 00:08:21,919
picture

204
00:08:21,919 --> 00:08:24,800
in such way that the changes will be

205
00:08:24,800 --> 00:08:26,639
imperceptible for human

206
00:08:26,639 --> 00:08:29,919
but the changes will be

207
00:08:29,919 --> 00:08:33,679
will affect the the model

208
00:08:33,679 --> 00:08:37,039
and actually how we can do that simply

209
00:08:37,039 --> 00:08:39,760
we need to discover the most important

210
00:08:39,760 --> 00:08:41,360
pixels

211
00:08:41,360 --> 00:08:43,839
somehow with different methods uh then

212
00:08:43,839 --> 00:08:45,920
we need to change those pixels

213
00:08:45,920 --> 00:08:48,880
to add them and actually the model uh

214
00:08:48,880 --> 00:08:51,040
will make a wrong prediction

215
00:08:51,040 --> 00:08:53,040
and there are a lot of different ways

216
00:08:53,040 --> 00:08:54,800
how to do that

217
00:08:54,800 --> 00:08:58,480
uh okay so now we're going back to

218
00:08:58,480 --> 00:09:01,920
our initial goal uh of testing

219
00:09:01,920 --> 00:09:04,800
a fascial recognition system and the

220
00:09:04,800 --> 00:09:05,600
first part

221
00:09:05,600 --> 00:09:10,160
is uh applying different attacks

222
00:09:10,160 --> 00:09:13,440
uh it all started with a

223
00:09:13,440 --> 00:09:16,640
case study uh the real story that one uh

224
00:09:16,640 --> 00:09:19,680
a smart home solution provider uh

225
00:09:19,680 --> 00:09:22,720
asked us to check

226
00:09:22,720 --> 00:09:26,399
so they their goal was to find

227
00:09:26,399 --> 00:09:29,040
the most reliable combination of

228
00:09:29,040 --> 00:09:31,200
hardware and software

229
00:09:31,200 --> 00:09:35,680
uh to apply to the uh critical facility

230
00:09:35,680 --> 00:09:38,880
so our goal was to check

231
00:09:38,880 --> 00:09:42,959
like how different software and hardware

232
00:09:42,959 --> 00:09:45,279
uh facial recognition solutions and

233
00:09:45,279 --> 00:09:46,880
their combinations

234
00:09:46,880 --> 00:09:50,560
um can be affected by adversarial

235
00:09:50,560 --> 00:09:51,440
attacks

236
00:09:51,440 --> 00:09:54,160
and the most important thing that it

237
00:09:54,160 --> 00:09:55,839
should be done in the physical

238
00:09:55,839 --> 00:09:59,360
um environment so it's not like

239
00:09:59,360 --> 00:10:02,399
just changing you know the uh

240
00:10:02,399 --> 00:10:05,760
pixels in the in the photo

241
00:10:05,760 --> 00:10:09,279
uh it must be a real physical attack

242
00:10:09,279 --> 00:10:12,160
so what is the problem first is that

243
00:10:12,160 --> 00:10:14,640
there are over 2 000 research papers in

244
00:10:14,640 --> 00:10:16,839
this area

245
00:10:16,839 --> 00:10:19,680
uh and you need to unders and you need

246
00:10:19,680 --> 00:10:21,519
to really understand like

247
00:10:21,519 --> 00:10:24,399
all of them or maybe not all of them but

248
00:10:24,399 --> 00:10:25,200
you need to

249
00:10:25,200 --> 00:10:27,760
at least understand how to filter them

250
00:10:27,760 --> 00:10:28,560
secondly

251
00:10:28,560 --> 00:10:32,480
uh around 100 research papers were

252
00:10:32,480 --> 00:10:35,600
on the test facial recognition systems

253
00:10:35,600 --> 00:10:40,560
uh then the next problem is there is a

254
00:10:40,560 --> 00:10:43,120
a countless combination of different

255
00:10:43,120 --> 00:10:45,279
types of attacks models and environment

256
00:10:45,279 --> 00:10:47,250
and basically i think

257
00:10:47,250 --> 00:10:48,800
[Music]

258
00:10:48,800 --> 00:10:52,000
you know like most of the research

259
00:10:52,000 --> 00:10:52,880
papers

260
00:10:52,880 --> 00:10:56,240
they test something on a specific

261
00:10:56,240 --> 00:10:59,920
specific environment so you have 100

262
00:10:59,920 --> 00:11:01,760
research papers but all of them

263
00:11:01,760 --> 00:11:03,200
have different types of attacks

264
00:11:03,200 --> 00:11:04,959
different models different environments

265
00:11:04,959 --> 00:11:05,440
and

266
00:11:05,440 --> 00:11:07,279
different everything so you cannot

267
00:11:07,279 --> 00:11:09,040
really compare it

268
00:11:09,040 --> 00:11:13,120
so and finally you don't have real

269
00:11:13,120 --> 00:11:16,000
understanding of the of the real world

270
00:11:16,000 --> 00:11:16,640
risks

271
00:11:16,640 --> 00:11:18,240
because you have a lot of research

272
00:11:18,240 --> 00:11:20,720
papers but nobody knows like

273
00:11:20,720 --> 00:11:25,200
if this attack really can be applied to

274
00:11:25,200 --> 00:11:27,680
physical world and if it can be applied

275
00:11:27,680 --> 00:11:29,360
to physical world

276
00:11:29,360 --> 00:11:33,040
is it is it real is it uh it works with

277
00:11:33,040 --> 00:11:33,519
a

278
00:11:33,519 --> 00:11:36,640
multiple uh can it uh

279
00:11:36,640 --> 00:11:39,120
be applied to the multiple uh facial

280
00:11:39,120 --> 00:11:40,079
recognition systems

281
00:11:40,079 --> 00:11:43,839
and and whatever

282
00:11:43,839 --> 00:11:47,360
so we decided to um

283
00:11:47,360 --> 00:11:51,120
apply the structure approach how to

284
00:11:51,120 --> 00:11:54,720
test it and finally uh

285
00:11:54,720 --> 00:11:57,839
help the customer to find the right uh

286
00:11:57,839 --> 00:12:01,279
solution so first of all

287
00:12:01,279 --> 00:12:04,480
uh before we start to filter

288
00:12:04,480 --> 00:12:06,959
all the research papers we should answer

289
00:12:06,959 --> 00:12:09,600
the main question the attacker go

290
00:12:09,600 --> 00:12:13,839
so it can be uh

291
00:12:13,839 --> 00:12:16,160
the ques the first question is why so

292
00:12:16,160 --> 00:12:17,760
what is the

293
00:12:17,760 --> 00:12:20,800
why attacker trying to

294
00:12:20,800 --> 00:12:23,200
attack this system it's either

295
00:12:23,200 --> 00:12:25,040
confidence reduction

296
00:12:25,040 --> 00:12:27,920
or it's a misclassification so just to

297
00:12:27,920 --> 00:12:28,800
avoid

298
00:12:28,800 --> 00:12:31,760
uh detection or it's target

299
00:12:31,760 --> 00:12:33,440
misclassification when we

300
00:12:33,440 --> 00:12:36,480
pretend to be another person so so first

301
00:12:36,480 --> 00:12:37,279
of all

302
00:12:37,279 --> 00:12:41,040
if you test something you need to uh

303
00:12:41,040 --> 00:12:43,440
choose what is the goal after you choose

304
00:12:43,440 --> 00:12:44,240
the goal

305
00:12:44,240 --> 00:12:48,160
you need to analyze the what can you do

306
00:12:48,160 --> 00:12:51,600
and in here you can say that it's

307
00:12:51,600 --> 00:12:54,639
actually attack constraints

308
00:12:54,639 --> 00:12:57,440
so one of the most popular attack

309
00:12:57,440 --> 00:12:58,639
constraints

310
00:12:58,639 --> 00:13:01,920
is different l norms so

311
00:13:01,920 --> 00:13:05,360
simply can we

312
00:13:05,360 --> 00:13:08,639
change every pixel or

313
00:13:08,639 --> 00:13:12,880
can we change only a few pixels

314
00:13:12,880 --> 00:13:16,560
uh because in the digital environment

315
00:13:16,560 --> 00:13:19,600
most of the adversarial attacks are

316
00:13:19,600 --> 00:13:23,360
performed by a little by changing

317
00:13:23,360 --> 00:13:25,920
a lot of pixels a little bit in the

318
00:13:25,920 --> 00:13:28,320
physical world

319
00:13:28,320 --> 00:13:32,160
we most of the attacks actually change

320
00:13:32,160 --> 00:13:36,000
a few number of pixels but change them

321
00:13:36,000 --> 00:13:39,920
a lot uh so the second

322
00:13:39,920 --> 00:13:41,680
thing we need to understand that what

323
00:13:41,680 --> 00:13:43,600
are the constraints

324
00:13:43,600 --> 00:13:46,880
uh in our environment then

325
00:13:46,880 --> 00:13:51,040
uh it's more detailed uh uh

326
00:13:51,040 --> 00:13:53,920
question about constraints like which

327
00:13:53,920 --> 00:13:55,279
form

328
00:13:55,279 --> 00:13:58,399
we can apply uh

329
00:13:58,399 --> 00:14:01,680
because uh if we talk about the facial

330
00:14:01,680 --> 00:14:03,040
recognition system

331
00:14:03,040 --> 00:14:06,079
yeah right we can do different things we

332
00:14:06,079 --> 00:14:07,279
can uh

333
00:14:07,279 --> 00:14:11,120
create a mask we can create a

334
00:14:11,120 --> 00:14:14,160
like um special glasses

335
00:14:14,160 --> 00:14:17,519
uh we can create a head a mustache a

336
00:14:17,519 --> 00:14:20,160
band-aid so everything but we need to

337
00:14:20,160 --> 00:14:21,199
understand

338
00:14:21,199 --> 00:14:24,079
like in our particular environment uh

339
00:14:24,079 --> 00:14:25,519
what

340
00:14:25,519 --> 00:14:28,800
what kind of uh things we can do

341
00:14:28,800 --> 00:14:32,000
can we uh use

342
00:14:32,000 --> 00:14:34,720
glasses or it's some kind of facial

343
00:14:34,720 --> 00:14:36,000
finish system

344
00:14:36,000 --> 00:14:38,800
which asking you to took off your

345
00:14:38,800 --> 00:14:39,600
glasses so

346
00:14:39,600 --> 00:14:42,959
if it's this system uh we only can do

347
00:14:42,959 --> 00:14:46,160
like other modifications like probably

348
00:14:46,160 --> 00:14:47,680
band-aid or probably

349
00:14:47,680 --> 00:14:50,880
uh some kind of retina

350
00:14:50,880 --> 00:14:54,079
changes or something like that so the

351
00:14:54,079 --> 00:14:54,880
next

352
00:14:54,880 --> 00:14:59,920
question is the form then

353
00:14:59,920 --> 00:15:03,600
we have attack conditions so basically

354
00:15:03,600 --> 00:15:07,360
uh it's a various approaches

355
00:15:07,360 --> 00:15:10,079
to for example optimize the quality of

356
00:15:10,079 --> 00:15:11,120
patches

357
00:15:11,120 --> 00:15:14,240
for printing because when you physically

358
00:15:14,240 --> 00:15:15,600
print something

359
00:15:15,600 --> 00:15:18,959
uh the color of pixels may be changed

360
00:15:18,959 --> 00:15:20,959
you can adjust you need to adjust your

361
00:15:20,959 --> 00:15:23,040
attack to inconsistency of

362
00:15:23,040 --> 00:15:26,560
of colors because it can be uh

363
00:15:26,560 --> 00:15:28,560
different lights in the physical

364
00:15:28,560 --> 00:15:29,920
environment

365
00:15:29,920 --> 00:15:32,480
you can you need to apply uh adjust it

366
00:15:32,480 --> 00:15:33,759
to the various uh

367
00:15:33,759 --> 00:15:36,240
patch positions sizes angles and so on

368
00:15:36,240 --> 00:15:38,079
so a lot of

369
00:15:38,079 --> 00:15:41,360
different constraints and only then

370
00:15:41,360 --> 00:15:45,040
the final thing is like how you

371
00:15:45,040 --> 00:15:48,079
uh perform this attack

372
00:15:48,079 --> 00:15:51,120
and there are various methods like

373
00:15:51,120 --> 00:15:54,160
fg same the the most simple one

374
00:15:54,160 --> 00:15:57,040
fast gradient science method or they can

375
00:15:57,040 --> 00:15:58,880
be more complex

376
00:15:58,880 --> 00:16:02,240
uh like momentum fgsm

377
00:16:02,240 --> 00:16:05,600
or uh any other approaches

378
00:16:05,600 --> 00:16:08,880
that basically doing the same thing but

379
00:16:08,880 --> 00:16:13,279
maybe faster and uh more effectively

380
00:16:13,279 --> 00:16:17,199
uh then after we

381
00:16:17,199 --> 00:16:20,320
understand like how we can

382
00:16:20,320 --> 00:16:23,839
perform the attack we need to uh

383
00:16:23,839 --> 00:16:26,880
have some attacks of success criterias

384
00:16:26,880 --> 00:16:30,399
like what is

385
00:16:30,560 --> 00:16:33,680
what is a perfect attack for us is this

386
00:16:33,680 --> 00:16:35,360
attack with the

387
00:16:35,360 --> 00:16:38,480
best misclassification rate or if this

388
00:16:38,480 --> 00:16:39,120
attack

389
00:16:39,120 --> 00:16:42,240
with the best imperceptibility uh

390
00:16:42,240 --> 00:16:45,279
like uh

391
00:16:45,360 --> 00:16:47,600
which is the attack which is hard to

392
00:16:47,600 --> 00:16:48,399
detect

393
00:16:48,399 --> 00:16:51,839
right or if our goal is to make

394
00:16:51,839 --> 00:16:55,680
the most robust attack for example if we

395
00:16:55,680 --> 00:16:56,959
talk about the

396
00:16:56,959 --> 00:16:59,920
the glasses yeah uh we want those

397
00:16:59,920 --> 00:17:01,120
glasses

398
00:17:01,120 --> 00:17:04,959
to work not in in the situation when we

399
00:17:04,959 --> 00:17:05,760
just you know

400
00:17:05,760 --> 00:17:08,319
show those glasses like directly but we

401
00:17:08,319 --> 00:17:09,199
want

402
00:17:09,199 --> 00:17:11,679
this attack to be super robust we want

403
00:17:11,679 --> 00:17:12,880
those glasses to

404
00:17:12,880 --> 00:17:16,480
be uh to work on different

405
00:17:16,480 --> 00:17:18,550
uh different

406
00:17:18,550 --> 00:17:19,919
[Music]

407
00:17:19,919 --> 00:17:23,039
different sizes different angles and so

408
00:17:23,039 --> 00:17:24,160
on

409
00:17:24,160 --> 00:17:29,039
and the computation so it is possible to

410
00:17:29,039 --> 00:17:32,480
create some super uh attack

411
00:17:32,480 --> 00:17:35,600
but some of the attacks like

412
00:17:35,600 --> 00:17:38,720
uh may cost a lot of

413
00:17:38,720 --> 00:17:42,240
uh money because you spend a lot of

414
00:17:42,240 --> 00:17:46,160
risk computer resources to uh

415
00:17:46,160 --> 00:17:48,640
calculate this attack and it's also very

416
00:17:48,640 --> 00:17:49,520
important

417
00:17:49,520 --> 00:17:53,520
success criteria uh and finally

418
00:17:53,520 --> 00:17:56,770
so what what we

419
00:17:56,770 --> 00:17:58,840
[Music]

420
00:17:58,840 --> 00:18:01,919
found that first of all

421
00:18:01,919 --> 00:18:04,799
it's possible to perform a physical

422
00:18:04,799 --> 00:18:05,760
attack

423
00:18:05,760 --> 00:18:08,240
against special recognition system you

424
00:18:08,240 --> 00:18:11,200
can see the example on the slide

425
00:18:11,200 --> 00:18:14,240
secondly we understood that glasses

426
00:18:14,240 --> 00:18:17,760
and bandanas uh have the best

427
00:18:17,760 --> 00:18:21,120
uh misclassification rate uh

428
00:18:21,120 --> 00:18:24,160
so this form of attack is

429
00:18:24,160 --> 00:18:27,520
the best one uh we also understood that

430
00:18:27,520 --> 00:18:28,400
there are

431
00:18:28,400 --> 00:18:31,440
it's it can be it's possible to further

432
00:18:31,440 --> 00:18:32,640
optimize

433
00:18:32,640 --> 00:18:36,000
it to make those glasses smaller and so

434
00:18:36,000 --> 00:18:37,280
on but it was not the

435
00:18:37,280 --> 00:18:40,400
the goal of this uh of this

436
00:18:40,400 --> 00:18:43,679
uh test the goal was to find some

437
00:18:43,679 --> 00:18:47,360
uh universal the most precise attack

438
00:18:47,360 --> 00:18:51,200
on which we will test solutions

439
00:18:51,200 --> 00:18:55,039
so the second part of research was to

440
00:18:55,039 --> 00:18:58,400
test models and compare them

441
00:18:58,400 --> 00:19:01,440
against this uh

442
00:19:01,440 --> 00:19:04,480
universal super attack

443
00:19:04,480 --> 00:19:08,080
and they can be different

444
00:19:08,080 --> 00:19:11,200
goals why we can why we test models

445
00:19:11,200 --> 00:19:14,240
uh we can test models against uh

446
00:19:14,240 --> 00:19:16,400
white box attacks just just to test

447
00:19:16,400 --> 00:19:17,679
different models

448
00:19:17,679 --> 00:19:21,600
or uh we can test uh

449
00:19:21,600 --> 00:19:24,080
models against the black box attacks to

450
00:19:24,080 --> 00:19:25,600
understand

451
00:19:25,600 --> 00:19:28,720
how models are protected from

452
00:19:28,720 --> 00:19:31,679
uh transferability so to transfer

453
00:19:31,679 --> 00:19:32,720
ability is

454
00:19:32,720 --> 00:19:34,430
the thing that

455
00:19:34,430 --> 00:19:35,840
[Music]

456
00:19:35,840 --> 00:19:39,440
in most cases if you

457
00:19:39,440 --> 00:19:42,480
calculate attack against

458
00:19:42,480 --> 00:19:44,400
some machine learning model with some

459
00:19:44,400 --> 00:19:46,400
data set

460
00:19:46,400 --> 00:19:49,760
this attack may be transferred to

461
00:19:49,760 --> 00:19:52,960
another module in another data set

462
00:19:52,960 --> 00:19:55,799
and depending on various parameters this

463
00:19:55,799 --> 00:19:57,200
transferability

464
00:19:57,200 --> 00:20:00,640
will be better or worse

465
00:20:00,640 --> 00:20:02,880
and the goal of this test was to

466
00:20:02,880 --> 00:20:05,120
understand

467
00:20:05,120 --> 00:20:08,880
what are the what factors affect uh

468
00:20:08,880 --> 00:20:13,200
transferability so for example is it the

469
00:20:13,200 --> 00:20:16,320
uh model size the data set size and so

470
00:20:16,320 --> 00:20:17,360
on

471
00:20:17,360 --> 00:20:21,280
uh and there are different uh

472
00:20:21,280 --> 00:20:24,320
model conditions starting from the

473
00:20:24,320 --> 00:20:25,679
architecture there are different

474
00:20:25,679 --> 00:20:27,360
architecture types

475
00:20:27,360 --> 00:20:29,600
different data sets and different uh

476
00:20:29,600 --> 00:20:31,600
types of outputs

477
00:20:31,600 --> 00:20:33,120
uh if we talk about the facial

478
00:20:33,120 --> 00:20:34,880
recognition system

479
00:20:34,880 --> 00:20:38,080
and uh the goal of this

480
00:20:38,080 --> 00:20:41,360
part was to understand how

481
00:20:41,360 --> 00:20:44,720
each uh part how the architecture

482
00:20:44,720 --> 00:20:48,080
affects transferability right so

483
00:20:48,080 --> 00:20:51,280
uh for example if you uh

484
00:20:51,280 --> 00:20:55,360
apply attack on vgg if this attack will

485
00:20:55,360 --> 00:20:55,840
be

486
00:20:55,840 --> 00:20:59,039
uh relevant for capsule network

487
00:20:59,039 --> 00:21:03,440
system uh and so on

488
00:21:03,440 --> 00:21:06,559
so the testing approaches are the

489
00:21:06,559 --> 00:21:07,760
following so

490
00:21:07,760 --> 00:21:11,280
we can prepare attack on random model

491
00:21:11,280 --> 00:21:14,960
or on the same model we can prepare

492
00:21:14,960 --> 00:21:16,000
attack on the different

493
00:21:16,000 --> 00:21:18,559
or the same conditions and different on

494
00:21:18,559 --> 00:21:20,080
the same output and so on

495
00:21:20,080 --> 00:21:21,919
and finally we can prepare attack on the

496
00:21:21,919 --> 00:21:23,840
ensemble of models

497
00:21:23,840 --> 00:21:26,080
uh which is the most the the best

498
00:21:26,080 --> 00:21:27,200
approach if we

499
00:21:27,200 --> 00:21:30,240
if you apply attack

500
00:21:30,240 --> 00:21:33,520
uh simultaneously against

501
00:21:33,520 --> 00:21:36,400
different models so the more models you

502
00:21:36,400 --> 00:21:37,280
have

503
00:21:37,280 --> 00:21:40,240
actually it's better and so here are the

504
00:21:40,240 --> 00:21:41,200
results

505
00:21:41,200 --> 00:21:44,799
um very quickly so some of the results

506
00:21:44,799 --> 00:21:46,799
because most of the results are under

507
00:21:46,799 --> 00:21:48,720
nga but just to

508
00:21:48,720 --> 00:21:50,799
to show you like what kind of results

509
00:21:50,799 --> 00:21:51,840
can be

510
00:21:51,840 --> 00:21:54,960
achieved after this testing

511
00:21:54,960 --> 00:21:57,600
uh in our environment like models with

512
00:21:57,600 --> 00:21:59,120
more layers

513
00:21:59,120 --> 00:22:02,400
were more secure uh preparing attacks on

514
00:22:02,400 --> 00:22:03,679
random models is

515
00:22:03,679 --> 00:22:08,000
not effective attacks on the same models

516
00:22:08,000 --> 00:22:08,320
but

517
00:22:08,320 --> 00:22:11,039
different actually not even landmarks

518
00:22:11,039 --> 00:22:11,360
but

519
00:22:11,360 --> 00:22:15,280
outputs in general are effective

520
00:22:15,280 --> 00:22:17,520
and of course if you prepare a attack on

521
00:22:17,520 --> 00:22:19,039
the ensemble of models

522
00:22:19,039 --> 00:22:22,400
it's it's more effective with each new

523
00:22:22,400 --> 00:22:26,559
model uh the thing is the results

524
00:22:26,559 --> 00:22:29,600
may be absolutely different for your

525
00:22:29,600 --> 00:22:32,880
uh machine learning solution

526
00:22:32,880 --> 00:22:36,559
and here i'm just giving uh

527
00:22:36,559 --> 00:22:40,480
like uh the approach right and the

528
00:22:40,480 --> 00:22:42,799
the results that we experienced in in

529
00:22:42,799 --> 00:22:44,880
our environment

530
00:22:44,880 --> 00:22:47,840
uh just just to give an idea how it

531
00:22:47,840 --> 00:22:49,679
works

532
00:22:49,679 --> 00:22:52,799
then after we tested the

533
00:22:52,799 --> 00:22:56,480
models basically the algorithms

534
00:22:56,480 --> 00:22:59,760
we need to test the environment because

535
00:22:59,760 --> 00:23:02,880
okay we can find that before testing

536
00:23:02,880 --> 00:23:05,120
environment we can only say that

537
00:23:05,120 --> 00:23:08,159
this algorithm is more protected from

538
00:23:08,159 --> 00:23:09,120
attacks

539
00:23:09,120 --> 00:23:12,000
but we have no idea how this algorithm

540
00:23:12,000 --> 00:23:13,039
will work

541
00:23:13,039 --> 00:23:16,640
in a particular device

542
00:23:16,640 --> 00:23:19,360
maybe like if we combine it with a

543
00:23:19,360 --> 00:23:20,840
different device

544
00:23:20,840 --> 00:23:24,640
the algorithm will be more

545
00:23:24,640 --> 00:23:27,520
secure or less secure so we need to test

546
00:23:27,520 --> 00:23:29,760
devices environment

547
00:23:29,760 --> 00:23:32,960
why because reality and it's it's

548
00:23:32,960 --> 00:23:36,159
much more complicated

549
00:23:36,159 --> 00:23:40,320
then they just research and test against

550
00:23:40,320 --> 00:23:44,799
some models without environment

551
00:23:44,799 --> 00:23:46,400
because there are different environment

552
00:23:46,400 --> 00:23:49,279
features camera features and so on

553
00:23:49,279 --> 00:23:52,640
simply uh they can be different

554
00:23:52,640 --> 00:23:54,080
environment

555
00:23:54,080 --> 00:23:56,480
like distance to objects lights

556
00:23:56,480 --> 00:23:58,480
brightness and so on

557
00:23:58,480 --> 00:24:02,640
in in some environments

558
00:24:02,640 --> 00:24:05,360
attacks can be better in some

559
00:24:05,360 --> 00:24:08,000
environment attacks can be

560
00:24:08,000 --> 00:24:11,360
less better then we have device features

561
00:24:11,360 --> 00:24:12,000
like

562
00:24:12,000 --> 00:24:15,360
resolution quality or color rendering

563
00:24:15,360 --> 00:24:18,480
and different colors will can be

564
00:24:18,480 --> 00:24:20,480
rendered absolutely differently

565
00:24:20,480 --> 00:24:23,600
in in different devices

566
00:24:23,600 --> 00:24:26,880
so it may happen that you have perfect

567
00:24:26,880 --> 00:24:30,640
uh attack for one device

568
00:24:30,640 --> 00:24:33,279
with like good resolution but then you

569
00:24:33,279 --> 00:24:34,960
apply this attack

570
00:24:34,960 --> 00:24:38,159
uh this mask or glasses on another

571
00:24:38,159 --> 00:24:41,200
device and it just don't work completely

572
00:24:41,200 --> 00:24:44,240
uh and also there are

573
00:24:44,240 --> 00:24:46,960
less important but existing thing like

574
00:24:46,960 --> 00:24:49,120
coded compression

575
00:24:49,120 --> 00:24:52,480
and data transfer compression which may

576
00:24:52,480 --> 00:24:53,039
also

577
00:24:53,039 --> 00:24:57,279
affect uh stability of attack

578
00:24:57,279 --> 00:25:00,880
and defense um

579
00:25:00,880 --> 00:25:04,320
so there are different approaches

580
00:25:04,320 --> 00:25:05,930
how you can

581
00:25:05,930 --> 00:25:07,440
[Music]

582
00:25:07,440 --> 00:25:10,320
test your physical attacks against

583
00:25:10,320 --> 00:25:12,960
different environment

584
00:25:12,960 --> 00:25:16,400
like you may have some fine function

585
00:25:16,400 --> 00:25:20,480
for a big pixel difference so

586
00:25:20,480 --> 00:25:23,520
uh for example when you calculate the

587
00:25:23,520 --> 00:25:24,799
attack

588
00:25:24,799 --> 00:25:28,320
uh you may tell your uh

589
00:25:28,320 --> 00:25:31,679
function to not make

590
00:25:31,679 --> 00:25:35,679
a big pixel difference because

591
00:25:35,679 --> 00:25:39,120
uh pixel difference in uh

592
00:25:39,120 --> 00:25:42,159
so they should not be two

593
00:25:42,159 --> 00:25:46,000
pixels together with very big difference

594
00:25:46,000 --> 00:25:49,840
because uh in the real world the camera

595
00:25:49,840 --> 00:25:52,880
will not um detect

596
00:25:52,880 --> 00:25:55,760
this difference right then you need to

597
00:25:55,760 --> 00:25:57,440
train your attacks

598
00:25:57,440 --> 00:26:00,400
with various sizes and angles to be so

599
00:26:00,400 --> 00:26:02,159
that the tech will be prepared for

600
00:26:02,159 --> 00:26:04,320
physical environment

601
00:26:04,320 --> 00:26:07,600
you can add or subtract color changes

602
00:26:07,600 --> 00:26:09,840
for for example you you may know that

603
00:26:09,840 --> 00:26:11,200
this camera

604
00:26:11,200 --> 00:26:14,480
changing the red color

605
00:26:14,480 --> 00:26:18,000
to pink color so you may

606
00:26:18,000 --> 00:26:21,200
extract some uh

607
00:26:21,200 --> 00:26:24,799
values from the red color before making

608
00:26:24,799 --> 00:26:25,600
attack

609
00:26:25,600 --> 00:26:28,960
and then uh add this

610
00:26:28,960 --> 00:26:33,360
uh value while you know making attack

611
00:26:33,360 --> 00:26:36,159
and different gaussian blur black and

612
00:26:36,159 --> 00:26:37,760
white and so on there are a lot of

613
00:26:37,760 --> 00:26:40,640
different approaches how you can

614
00:26:40,640 --> 00:26:43,600
make attack in such way that it can

615
00:26:43,600 --> 00:26:46,559
bypass different environment

616
00:26:46,559 --> 00:26:50,080
so our results are those

617
00:26:50,080 --> 00:26:52,799
like cameras with better quality are

618
00:26:52,799 --> 00:26:54,559
more vulnerable

619
00:26:54,559 --> 00:26:58,400
set true cameras

620
00:26:58,400 --> 00:27:01,039
rendering color rendering may seriously

621
00:27:01,039 --> 00:27:02,000
affect

622
00:27:02,000 --> 00:27:05,840
attack success black and white attacks

623
00:27:05,840 --> 00:27:08,639
are more stable

624
00:27:09,360 --> 00:27:13,200
light brightness may slightly affect

625
00:27:13,200 --> 00:27:14,640
color patches

626
00:27:14,640 --> 00:27:17,840
but in general it's not a super big

627
00:27:17,840 --> 00:27:19,200
problem

628
00:27:19,200 --> 00:27:21,679
so those are some of the most uh

629
00:27:21,679 --> 00:27:22,640
important

630
00:27:22,640 --> 00:27:27,360
results uh it's very important for

631
00:27:27,360 --> 00:27:30,240
was for our goal to find the best

632
00:27:30,240 --> 00:27:31,679
combination of software

633
00:27:31,679 --> 00:27:35,200
and hardware so because of the camera

634
00:27:35,200 --> 00:27:36,159
quality

635
00:27:36,159 --> 00:27:40,159
and attacks uh and our goal

636
00:27:40,159 --> 00:27:45,120
in in this in this task was to find

637
00:27:45,120 --> 00:27:49,440
a solution like so that the camera will

638
00:27:49,440 --> 00:27:49,919
have

639
00:27:49,919 --> 00:27:53,039
good quality and at the same time the

640
00:27:53,039 --> 00:27:54,240
algorithm

641
00:27:54,240 --> 00:27:58,080
will be not vulnerable even with the

642
00:27:58,080 --> 00:28:01,279
camera with good quality so it's a quite

643
00:28:01,279 --> 00:28:04,240
challenging uh

644
00:28:04,320 --> 00:28:09,360
task and finally uh

645
00:28:09,520 --> 00:28:12,720
we should test uh different defenses

646
00:28:12,720 --> 00:28:16,799
uh and uh

647
00:28:16,799 --> 00:28:19,360
there can be different defenses

648
00:28:19,360 --> 00:28:20,559
depending on

649
00:28:20,559 --> 00:28:23,760
your uh so who you are

650
00:28:23,760 --> 00:28:27,440
because if your uh enterprise

651
00:28:27,440 --> 00:28:30,559
and you're buying those devices like

652
00:28:30,559 --> 00:28:34,399
uh in our example it was a

653
00:28:34,399 --> 00:28:37,679
solution provider who who is buying

654
00:28:37,679 --> 00:28:38,320
devices

655
00:28:38,320 --> 00:28:42,000
they cannot uh change

656
00:28:42,000 --> 00:28:45,039
uh something in those devices the only

657
00:28:45,039 --> 00:28:46,640
way for them is

658
00:28:46,640 --> 00:28:49,919
is just to choose the right combination

659
00:28:49,919 --> 00:28:51,760
of software and hardware

660
00:28:51,760 --> 00:28:54,960
uh or they can be a company

661
00:28:54,960 --> 00:28:57,919
who provide apis like cloud platform so

662
00:28:57,919 --> 00:28:58,559
they

663
00:28:58,559 --> 00:29:01,679
may have better more options they may

664
00:29:01,679 --> 00:29:02,240
apply

665
00:29:02,240 --> 00:29:06,080
some defenses on top of uh

666
00:29:06,080 --> 00:29:10,559
algorithm uh or

667
00:29:10,559 --> 00:29:13,440
you may be a hardware vendor and you're

668
00:29:13,440 --> 00:29:15,440
buying the software

669
00:29:15,440 --> 00:29:18,159
from someone and this software may be

670
00:29:18,159 --> 00:29:19,360
vulnerable but

671
00:29:19,360 --> 00:29:21,520
and you cannot do something with that

672
00:29:21,520 --> 00:29:22,399
but you can

673
00:29:22,399 --> 00:29:26,559
also apply some uh additional

674
00:29:26,559 --> 00:29:29,600
filters and so on or

675
00:29:29,600 --> 00:29:31,360
if you're a software vendor if you

676
00:29:31,360 --> 00:29:32,880
develop your own

677
00:29:32,880 --> 00:29:36,480
machine learning uh solution you can

678
00:29:36,480 --> 00:29:39,840
apply other approaches to defend your

679
00:29:39,840 --> 00:29:40,640
solution so

680
00:29:40,640 --> 00:29:43,230
it's the defense really um

681
00:29:43,230 --> 00:29:45,200
[Music]

682
00:29:45,200 --> 00:29:48,399
differs uh

683
00:29:48,399 --> 00:29:51,840
by the type of of the company

684
00:29:51,840 --> 00:29:54,960
so those are the most

685
00:29:54,960 --> 00:29:58,240
common categories of defenses uh

686
00:29:58,240 --> 00:30:02,240
first is to modify a training phase

687
00:30:02,240 --> 00:30:04,880
basically it's adversarial training so

688
00:30:04,880 --> 00:30:05,520
you

689
00:30:05,520 --> 00:30:07,520
create adversarial examples and then you

690
00:30:07,520 --> 00:30:09,120
train your system

691
00:30:09,120 --> 00:30:12,799
uh with adversarial examples

692
00:30:12,799 --> 00:30:15,600
and hope that it will work but in

693
00:30:15,600 --> 00:30:16,559
general

694
00:30:16,559 --> 00:30:20,480
it's not uh you can modify model

695
00:30:20,480 --> 00:30:24,080
at add new layers and change

696
00:30:24,080 --> 00:30:26,159
it to specific activation functions

697
00:30:26,159 --> 00:30:27,840
there are multiple

698
00:30:27,840 --> 00:30:30,960
research papers on how you can modify

699
00:30:30,960 --> 00:30:32,799
module

700
00:30:32,799 --> 00:30:35,679
you can modify

701
00:30:35,840 --> 00:30:39,200
jpeg compression the very simple and

702
00:30:39,200 --> 00:30:41,039
obvious approach but

703
00:30:41,039 --> 00:30:45,440
there are at least like dozens of papers

704
00:30:45,440 --> 00:30:48,880
uh with different types of

705
00:30:48,880 --> 00:30:52,159
you know model input changes

706
00:30:52,159 --> 00:30:54,240
or you can apply different detection

707
00:30:54,240 --> 00:30:55,480
approaches

708
00:30:55,480 --> 00:30:56,960
[Music]

709
00:30:56,960 --> 00:30:59,039
supervised detection or unsupervised

710
00:30:59,039 --> 00:31:01,679
detection

711
00:31:03,120 --> 00:31:07,600
so finally what um

712
00:31:08,240 --> 00:31:11,600
what we uh understood after uh

713
00:31:11,600 --> 00:31:14,640
applying different uh defenses

714
00:31:14,640 --> 00:31:17,159
first of all like there are no

715
00:31:17,159 --> 00:31:19,120
one-size-fits-all protection

716
00:31:19,120 --> 00:31:22,399
this is a big problem and i think it

717
00:31:22,399 --> 00:31:24,240
won't be

718
00:31:24,240 --> 00:31:27,519
some universal solution for

719
00:31:27,519 --> 00:31:30,640
each you know model and each data set

720
00:31:30,640 --> 00:31:30,960
and

721
00:31:30,960 --> 00:31:34,159
each algorithm

722
00:31:34,159 --> 00:31:37,519
so you always need to test

723
00:31:37,519 --> 00:31:40,799
various defense approaches which will

724
00:31:40,799 --> 00:31:44,080
work for your solution

725
00:31:44,559 --> 00:31:46,480
why there are no one-size-fits-all

726
00:31:46,480 --> 00:31:48,559
solutions because

727
00:31:48,559 --> 00:31:51,200
various protection matters have their

728
00:31:51,200 --> 00:31:51,919
own

729
00:31:51,919 --> 00:31:55,200
issues like modifying model

730
00:31:55,200 --> 00:31:58,640
may be really a

731
00:31:58,640 --> 00:32:01,679
good solution but in most cases is it

732
00:32:01,679 --> 00:32:02,240
decreased

733
00:32:02,240 --> 00:32:06,159
accuracy uh so your solution may be like

734
00:32:06,159 --> 00:32:09,039
super protected but the accuracy can

735
00:32:09,039 --> 00:32:09,440
drop

736
00:32:09,440 --> 00:32:12,640
like significantly uh modified training

737
00:32:12,640 --> 00:32:14,080
like adversarial training

738
00:32:14,080 --> 00:32:17,760
and other examples it's very expensive

739
00:32:17,760 --> 00:32:20,000
uh because you need to retrain system

740
00:32:20,000 --> 00:32:22,240
and in mostly adversarial training it's

741
00:32:22,240 --> 00:32:23,200
not like

742
00:32:23,200 --> 00:32:25,600
just retraining the system it's

743
00:32:25,600 --> 00:32:26,320
basically

744
00:32:26,320 --> 00:32:29,360
it can be like 10 times more

745
00:32:29,360 --> 00:32:31,919
complex than just training because you

746
00:32:31,919 --> 00:32:33,360
need to add a lot of

747
00:32:33,360 --> 00:32:36,320
examples and

748
00:32:36,640 --> 00:32:38,799
so adversarial training is really

749
00:32:38,799 --> 00:32:39,840
expensive

750
00:32:39,840 --> 00:32:42,480
and in most cases it can be easily

751
00:32:42,480 --> 00:32:43,840
bypassed again

752
00:32:43,840 --> 00:32:47,120
just by applying new attack modifying

753
00:32:47,120 --> 00:32:50,559
input it's uh also good approach

754
00:32:50,559 --> 00:32:53,200
but it's complicated and it's very test

755
00:32:53,200 --> 00:32:54,640
specific so

756
00:32:54,640 --> 00:32:58,159
you may create your uh it may work

757
00:32:58,159 --> 00:33:01,360
for a particular model but

758
00:33:01,360 --> 00:33:06,080
once you will change your model to do

759
00:33:06,399 --> 00:33:10,480
a little bit your all your defenses

760
00:33:10,480 --> 00:33:13,519
uh should be

761
00:33:13,919 --> 00:33:17,679
changed as well detection uh

762
00:33:17,679 --> 00:33:21,440
is also in most cases it's it's

763
00:33:21,440 --> 00:33:24,799
the best methods but

764
00:33:24,799 --> 00:33:27,679
unfortunately

765
00:33:28,880 --> 00:33:33,760
this method applied when attack already

766
00:33:33,760 --> 00:33:37,120
executed so

767
00:33:37,120 --> 00:33:41,120
so you only can detect it afterwards

768
00:33:41,120 --> 00:33:43,470
uh so the takeaways um

769
00:33:43,470 --> 00:33:45,760
[Music]

770
00:33:45,760 --> 00:33:48,880
the best approach is to have the secure

771
00:33:48,880 --> 00:33:50,960
ai lifecycle if we have

772
00:33:50,960 --> 00:33:53,760
in the software development we have sdlc

773
00:33:53,760 --> 00:33:54,399
so

774
00:33:54,399 --> 00:33:57,279
in uh ai we also need to have some kind

775
00:33:57,279 --> 00:33:57,840
of

776
00:33:57,840 --> 00:34:02,159
sdlc and it consists of four

777
00:34:02,159 --> 00:34:05,200
stages predict prevent attack respond

778
00:34:05,200 --> 00:34:08,960
so we need to conduct a security test

779
00:34:08,960 --> 00:34:12,480
first then apply some

780
00:34:12,480 --> 00:34:16,879
basic protection measures

781
00:34:16,879 --> 00:34:18,719
then apply some detection to monitor

782
00:34:18,719 --> 00:34:21,440
threats and respond

783
00:34:21,440 --> 00:34:24,639
so continuously react on the changing

784
00:34:24,639 --> 00:34:27,760
environment and and retest it

785
00:34:27,760 --> 00:34:31,599
so a few next steps

786
00:34:31,599 --> 00:34:35,199
just like train your ai and security

787
00:34:35,199 --> 00:34:36,159
teams

788
00:34:36,159 --> 00:34:39,199
uh on this topic

789
00:34:39,199 --> 00:34:41,918
so that the the teams will be aware of

790
00:34:41,918 --> 00:34:42,239
it

791
00:34:42,239 --> 00:34:45,839
and can work together uh because the

792
00:34:45,839 --> 00:34:48,960
protecting ai systems it's really a

793
00:34:48,960 --> 00:34:52,159
very complex task and it involves

794
00:34:52,159 --> 00:34:55,679
experts from ai from

795
00:34:55,679 --> 00:35:00,400
security at least from

796
00:35:00,400 --> 00:35:05,359
experts from those two sides um

797
00:35:05,359 --> 00:35:09,520
then uh after you you train your team

798
00:35:09,520 --> 00:35:11,920
you need to conduct first ai security

799
00:35:11,920 --> 00:35:14,320
tests even

800
00:35:14,320 --> 00:35:17,280
by yourself just to understand and get

801
00:35:17,280 --> 00:35:17,520
an

802
00:35:17,520 --> 00:35:21,119
idea uh address your first security

803
00:35:21,119 --> 00:35:22,480
findings

804
00:35:22,480 --> 00:35:24,960
and then integrate those steps in your

805
00:35:24,960 --> 00:35:27,040
security life cycle

806
00:35:27,040 --> 00:35:29,839
and you know automate it and do it

807
00:35:29,839 --> 00:35:32,000
better better and better

808
00:35:32,000 --> 00:35:34,800
so uh thanks thanks a lot for uh

809
00:35:34,800 --> 00:35:36,400
listening i hope you enjoyed

810
00:35:36,400 --> 00:35:39,599
if you want to talk about

811
00:35:39,599 --> 00:35:43,440
how to secure ai systems you always can

812
00:35:43,440 --> 00:35:46,400
write us email contact us on the website

813
00:35:46,400 --> 00:35:48,480
you can write me directly

814
00:35:48,480 --> 00:35:51,680
uh what else so

815
00:35:51,680 --> 00:35:55,200
yeah i'm very uh interested in

816
00:35:55,200 --> 00:35:58,880
any feedback and we hiring

817
00:35:58,880 --> 00:36:02,880
we hiring like uh continuously

818
00:36:02,880 --> 00:36:06,560
uh and we will be happy to

819
00:36:06,560 --> 00:36:10,839
talk with anyone who interested in

820
00:36:10,839 --> 00:36:13,920
protecting ai solutions and

821
00:36:13,920 --> 00:36:18,160
like making ai more trustworthy uh

822
00:36:18,160 --> 00:36:22,800
we can find ways how we can partner with

823
00:36:22,800 --> 00:36:27,359
each other so again hope you enjoyed and

824
00:36:27,359 --> 00:36:30,880
have a have a good day

825
00:36:32,000 --> 00:36:41,839
thank you and bye

826
00:36:42,000 --> 00:36:44,079
you

