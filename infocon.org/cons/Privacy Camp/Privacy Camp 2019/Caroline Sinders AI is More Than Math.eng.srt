1
00:00:03,140 --> 00:00:06,249
[Music]

2
00:00:07,160 --> 00:00:09,160
a

3
00:00:10,520 --> 00:00:15,540
[Music]

4
00:00:13,480 --> 00:00:15,540
you

5
00:00:24,540 --> 00:00:29,919
hi everyone my name is Caroline senders

6
00:00:27,400 --> 00:00:33,160
and this is my talk AI is more than math

7
00:00:29,920 --> 00:00:35,830
um I'm an artist and design researcher

8
00:00:33,160 --> 00:00:38,199
um I've been studying how people

9
00:00:35,830 --> 00:00:40,059
understand to relate and then create

10
00:00:38,200 --> 00:00:42,430
within large systems and platforms for

11
00:00:40,059 --> 00:00:44,140
the past seven years I've worked as a UX

12
00:00:42,430 --> 00:00:46,570
designer in advertising a design

13
00:00:44,140 --> 00:00:48,510
researcher for IBM Watson I've held arts

14
00:00:46,570 --> 00:00:50,980
and writing residency's with BuzzFeed

15
00:00:48,510 --> 00:00:52,690
IBM and Google's pair people an

16
00:00:50,980 --> 00:00:54,160
artificial intelligence group and I'm

17
00:00:52,690 --> 00:00:56,709
currently a fellow with Harvard

18
00:00:54,160 --> 00:00:59,769
University Mozilla exploring explainable

19
00:00:56,710 --> 00:01:03,550
AI and I often look like this a lot when

20
00:00:59,770 --> 00:01:05,290
I'm studying people on the internet and

21
00:01:03,550 --> 00:01:07,810
I've been thinking a lot about how do we

22
00:01:05,290 --> 00:01:09,910
make AI pretty understandable for

23
00:01:07,810 --> 00:01:12,520
everyone and I'm really narrowly focused

24
00:01:09,910 --> 00:01:14,649
on this one goal how can we make as

25
00:01:12,520 --> 00:01:17,619
designers technologists researchers and

26
00:01:14,649 --> 00:01:20,050
creators more understandable how we make

27
00:01:17,619 --> 00:01:23,020
AI more engaged how we make it feel more

28
00:01:20,050 --> 00:01:25,360
human and I think it's with really

29
00:01:23,020 --> 00:01:27,550
really good design and when I say design

30
00:01:25,360 --> 00:01:29,530
I don't just mean product design or UX

31
00:01:27,550 --> 00:01:31,690
design I mean design holistically as a

32
00:01:29,530 --> 00:01:33,700
whole design as a medium design as a

33
00:01:31,690 --> 00:01:37,860
methodology and the ways in which design

34
00:01:33,700 --> 00:01:40,479
touches every part of our everyday lives

35
00:01:37,860 --> 00:01:42,729
so my title is actually a little bit of

36
00:01:40,479 --> 00:01:44,590
a misnomer because algorithms AI and

37
00:01:42,729 --> 00:01:46,929
machine learning they are math and

38
00:01:44,590 --> 00:01:49,750
they're code but they're also data sets

39
00:01:46,929 --> 00:01:52,030
and intentional design algorithms are

40
00:01:49,750 --> 00:01:54,340
designed to do things from translating

41
00:01:52,030 --> 00:01:56,830
text into speech to creating predictions

42
00:01:54,340 --> 00:01:59,050
from data from recognizing and sorting

43
00:01:56,830 --> 00:02:00,970
images together algorithms are also

44
00:01:59,050 --> 00:02:03,580
really about how information is

45
00:02:00,970 --> 00:02:06,580
processed Kate Devlin who I believe is a

46
00:02:03,580 --> 00:02:09,639
speaker also here at Repubblica defines

47
00:02:06,580 --> 00:02:12,040
AI in her book as the concepts of the

48
00:02:09,639 --> 00:02:15,010
concept of machines being able to carry

49
00:02:12,040 --> 00:02:15,640
out tasks in an intelligible manner at

50
00:02:15,010 --> 00:02:18,099
present

51
00:02:15,640 --> 00:02:20,019
none of the AI and used today involves

52
00:02:18,099 --> 00:02:22,329
machines actually being sentient or

53
00:02:20,020 --> 00:02:24,970
conscious nor do they have general

54
00:02:22,330 --> 00:02:27,069
intelligence rather they use data to

55
00:02:24,970 --> 00:02:29,010
understand patterns of outcomes and that

56
00:02:27,069 --> 00:02:31,750
way learn from previous situations

57
00:02:29,010 --> 00:02:33,609
really AI machine learning and

58
00:02:31,750 --> 00:02:38,260
algorithms are just pattern matching

59
00:02:33,610 --> 00:02:40,240
sorting but really really what is a I me

60
00:02:38,260 --> 00:02:41,679
me Oh Noah and mother cyborg describe AI

61
00:02:40,240 --> 00:02:44,080
in their book the people's guide

62
00:02:41,680 --> 00:02:48,010
artificial intelligence as AI being a

63
00:02:44,080 --> 00:02:49,690
bit more like salt on its own it's not

64
00:02:48,010 --> 00:02:51,989
really a product and it's not really a

65
00:02:49,690 --> 00:02:54,490
feature it doesn't really do much alone

66
00:02:51,990 --> 00:02:57,310
but it's a transformative ingredient and

67
00:02:54,490 --> 00:02:59,170
it's technology salt for example on its

68
00:02:57,310 --> 00:03:01,180
own is not a meal I don't know if you

69
00:02:59,170 --> 00:03:03,760
should eat salt actually alone by itself

70
00:03:01,180 --> 00:03:05,950
in large quantities it's not really

71
00:03:03,760 --> 00:03:08,440
anything other than a flavor how

72
00:03:05,950 --> 00:03:10,390
interesting really is salt it's singular

73
00:03:08,440 --> 00:03:12,760
but when combined with other ingredients

74
00:03:10,390 --> 00:03:15,760
to make a dish it completely changes an

75
00:03:12,760 --> 00:03:17,769
entire meal ai is like that AI is

76
00:03:15,760 --> 00:03:21,940
transformative and product design as

77
00:03:17,770 --> 00:03:24,700
well as everyday life but AI is also

78
00:03:21,940 --> 00:03:26,050
data and I think data is as important as

79
00:03:24,700 --> 00:03:28,329
the algorithm in the code that is

80
00:03:26,050 --> 00:03:30,790
written because data can determine what

81
00:03:28,330 --> 00:03:31,840
an algorithm does data is used to train

82
00:03:30,790 --> 00:03:34,420
an algorithm and machine learning

83
00:03:31,840 --> 00:03:37,470
systems an algorithm that's designed to

84
00:03:34,420 --> 00:03:40,268
analyze social media data is trained on

85
00:03:37,470 --> 00:03:44,200
social media data so data is incredibly

86
00:03:40,269 --> 00:03:46,360
important and data is key and data

87
00:03:44,200 --> 00:03:49,089
inside of machine learning can be things

88
00:03:46,360 --> 00:03:51,580
like language or conversation or images

89
00:03:49,090 --> 00:03:53,830
or texts or any kind of output for

90
00:03:51,580 --> 00:03:56,290
example from social media can be things

91
00:03:53,830 --> 00:03:58,030
like the frequency of interactions it's

92
00:03:56,290 --> 00:04:00,370
a lot of things related to SEO and

93
00:03:58,030 --> 00:04:03,700
marketing but it's all those things are

94
00:04:00,370 --> 00:04:05,290
actually human outputs let's think about

95
00:04:03,700 --> 00:04:06,970
that for a second because any kind of

96
00:04:05,290 --> 00:04:08,950
data out there is created for and by

97
00:04:06,970 --> 00:04:12,130
people even if it fuels inherently

98
00:04:08,950 --> 00:04:15,390
mechanical or inherently technical what

99
00:04:12,130 --> 00:04:18,070
about data about shipping or any kind of

100
00:04:15,390 --> 00:04:19,870
shipping information when something is

101
00:04:18,070 --> 00:04:22,150
sent from one place to another is that

102
00:04:19,870 --> 00:04:24,400
human who's sending something and who is

103
00:04:22,150 --> 00:04:27,400
receiving something shipping data for

104
00:04:24,400 --> 00:04:28,780
example is inherently human and I think

105
00:04:27,400 --> 00:04:31,120
it's important to think about this when

106
00:04:28,780 --> 00:04:34,539
it comes to data there's nothing cold or

107
00:04:31,120 --> 00:04:36,010
mechanical about data it's all people so

108
00:04:34,540 --> 00:04:38,260
we have to treat data then as a

109
00:04:36,010 --> 00:04:40,719
sensitive object as a really kind of

110
00:04:38,260 --> 00:04:43,810
precious material needs to be handled

111
00:04:40,720 --> 00:04:45,700
with care I want to highlight some

112
00:04:43,810 --> 00:04:46,310
interesting projects that focus on data

113
00:04:45,700 --> 00:04:48,620
and

114
00:04:46,310 --> 00:04:50,900
and this is one of my favorites this is

115
00:04:48,620 --> 00:04:52,990
by Mimi Oh Noah and it's her project the

116
00:04:50,900 --> 00:04:56,000
library of missing data sets

117
00:04:52,990 --> 00:04:58,639
missing data sets are blank spots and

118
00:04:56,000 --> 00:05:00,650
otherwise data saturated spaces the

119
00:04:58,639 --> 00:05:03,950
library of missing data sets started in

120
00:05:00,650 --> 00:05:05,960
2016 is an ongoing physical repository

121
00:05:03,950 --> 00:05:07,880
of those things that have been excluded

122
00:05:05,960 --> 00:05:11,120
in a society where so much else is

123
00:05:07,880 --> 00:05:11,750
collected I don't think about that for a

124
00:05:11,120 --> 00:05:14,180
second

125
00:05:11,750 --> 00:05:16,730
what are times when perhaps not being

126
00:05:14,180 --> 00:05:18,919
seen or collected as good for example

127
00:05:16,730 --> 00:05:21,650
facial recognition software has a really

128
00:05:18,919 --> 00:05:24,650
hard time recognizing different races

129
00:05:21,650 --> 00:05:26,570
particularly black faces this can be

130
00:05:24,650 --> 00:05:29,120
good if facial recognition software is

131
00:05:26,570 --> 00:05:31,700
sort of deployed ad hoc or holistically

132
00:05:29,120 --> 00:05:34,100
across any kind of CCTV camera and this

133
00:05:31,700 --> 00:05:36,640
kind of space sure being unrecognized by

134
00:05:34,100 --> 00:05:39,650
the system is actually really positive

135
00:05:36,640 --> 00:05:41,690
but when is it bad that a system doesn't

136
00:05:39,650 --> 00:05:43,489
recognize you well what happens with

137
00:05:41,690 --> 00:05:45,590
facial recognition software deployed at

138
00:05:43,490 --> 00:05:48,200
a border crossing what happens if that

139
00:05:45,590 --> 00:05:51,049
system doesn't actually recognize your

140
00:05:48,200 --> 00:05:53,330
face and you can't cross the border what

141
00:05:51,050 --> 00:05:55,250
happens if you're not seen are you

142
00:05:53,330 --> 00:05:57,890
detained where do you go and what

143
00:05:55,250 --> 00:06:00,080
happens to you in this space and was a

144
00:05:57,890 --> 00:06:02,990
system designed with this kind of error

145
00:06:00,080 --> 00:06:07,010
in mind imagine how scary that would be

146
00:06:02,990 --> 00:06:08,360
right this is another art project I

147
00:06:07,010 --> 00:06:10,780
really like that sort of builds upon

148
00:06:08,360 --> 00:06:13,039
this idea of algorithms being fallible

149
00:06:10,780 --> 00:06:15,859
this is Heather doing hag works work

150
00:06:13,039 --> 00:06:17,780
probably Chelsea what we're looking at

151
00:06:15,860 --> 00:06:19,640
are 30 different possible portraits of

152
00:06:17,780 --> 00:06:22,099
Chelsea Manning that are algorithmically

153
00:06:19,640 --> 00:06:25,370
generated by an analysis of her DNA and

154
00:06:22,100 --> 00:06:27,440
then 3d printed well it's interesting is

155
00:06:25,370 --> 00:06:29,030
technically the system Heather is using

156
00:06:27,440 --> 00:06:30,410
to analyze her DNA is one that's used

157
00:06:29,030 --> 00:06:33,799
pretty frequently especially in

158
00:06:30,410 --> 00:06:36,350
bioengineering but none of these

159
00:06:33,800 --> 00:06:38,390
portraits are actual portraits of

160
00:06:36,350 --> 00:06:40,640
Chelsea she doesn't look like any of

161
00:06:38,390 --> 00:06:42,979
these faces but technically really

162
00:06:40,640 --> 00:06:46,099
technically none of these portraits are

163
00:06:42,979 --> 00:06:48,039
wrong but the way that we see them none

164
00:06:46,100 --> 00:06:50,870
of them are actually correct are they

165
00:06:48,039 --> 00:06:53,210
this project really isn't about AI but

166
00:06:50,870 --> 00:06:55,700
I'm striving to build here an idea about

167
00:06:53,210 --> 00:06:57,830
how products and data can work together

168
00:06:55,700 --> 00:06:59,630
and how an algorithm especially when it

169
00:06:57,830 --> 00:07:02,050
learns from human data from

170
00:06:59,630 --> 00:07:06,050
changing data from data fed into our

171
00:07:02,050 --> 00:07:08,270
daily products can fail and that's why

172
00:07:06,050 --> 00:07:09,530
we need to deeply investigate data

173
00:07:08,270 --> 00:07:11,450
within these systems and how these

174
00:07:09,530 --> 00:07:14,809
systems are designed I want to give you

175
00:07:11,450 --> 00:07:16,820
a few examples so what we're seeing here

176
00:07:14,810 --> 00:07:19,790
are two different Google search results

177
00:07:16,820 --> 00:07:21,830
this is one this is the other these were

178
00:07:19,790 --> 00:07:25,780
done in 2016

179
00:07:21,830 --> 00:07:27,950
get anyone sort of guess what they are

180
00:07:25,780 --> 00:07:30,049
this is what happens when you google

181
00:07:27,950 --> 00:07:34,490
professional hair and this is

182
00:07:30,050 --> 00:07:39,430
unprofessional hair as you can see one

183
00:07:34,490 --> 00:07:41,860
is overwhelmingly white and one is not

184
00:07:39,430 --> 00:07:44,120
this is uncovered by the Guardian in

185
00:07:41,860 --> 00:07:46,850
2016 I think specifically by Lee

186
00:07:44,120 --> 00:07:49,070
Alexander it's important in this case

187
00:07:46,850 --> 00:07:51,530
that we also think about the intentions

188
00:07:49,070 --> 00:07:53,000
of this is Google search a product well

189
00:07:51,530 --> 00:07:55,700
it is how many of you have used Google

190
00:07:53,000 --> 00:07:57,260
search today or yesterday or in the last

191
00:07:55,700 --> 00:07:58,849
week or the last month and think about

192
00:07:57,260 --> 00:08:03,230
the amount of times you use that in your

193
00:07:58,850 --> 00:08:04,580
daily life looking at these examples one

194
00:08:03,230 --> 00:08:06,650
thing I want to highlight I don't think

195
00:08:04,580 --> 00:08:09,109
the Google search team set out to make a

196
00:08:06,650 --> 00:08:11,390
shitty product I don't think they set

197
00:08:09,110 --> 00:08:13,790
out to also make something that is sort

198
00:08:11,390 --> 00:08:15,440
of this erroneous but we can look at

199
00:08:13,790 --> 00:08:17,840
this and maybe start to hypothesize the

200
00:08:15,440 --> 00:08:19,910
diversity of the data set what image

201
00:08:17,840 --> 00:08:21,799
data sets were they pulling from how are

202
00:08:19,910 --> 00:08:24,110
they training these search queries we

203
00:08:21,800 --> 00:08:26,870
can even hypothesize the diversity of

204
00:08:24,110 --> 00:08:29,270
the engineering team no one really sets

205
00:08:26,870 --> 00:08:33,349
out to make a bad product but it's on a

206
00:08:29,270 --> 00:08:36,130
bug it's a feature until it's fixed let

207
00:08:33,349 --> 00:08:39,200
me give you another example this is a

208
00:08:36,130 --> 00:08:41,900
product called face ception made by a

209
00:08:39,200 --> 00:08:45,650
lee company that is using algorithms to

210
00:08:41,900 --> 00:08:47,569
detect emotions and faces now stepping

211
00:08:45,650 --> 00:08:48,740
outside of the problem the most obvious

212
00:08:47,570 --> 00:08:50,750
problem with this other than the

213
00:08:48,740 --> 00:08:54,080
surveillance aspect let's think about

214
00:08:50,750 --> 00:08:55,970
how wrong facial detection could be how

215
00:08:54,080 --> 00:09:00,590
many of you have heard of resting

216
00:08:55,970 --> 00:09:04,370
bitchface one of my best friends is what

217
00:09:00,590 --> 00:09:06,170
I call resting bitchface so when I text

218
00:09:04,370 --> 00:09:07,970
her a lot like oh I'm coming into town

219
00:09:06,170 --> 00:09:11,089
she'll be like oh I'm so excited to see

220
00:09:07,970 --> 00:09:12,889
you but in person she's like I've missed

221
00:09:11,089 --> 00:09:16,040
you oh it's been so long since

222
00:09:12,889 --> 00:09:18,769
we've hung out but she's really really

223
00:09:16,040 --> 00:09:21,170
excited to see me but if you were to

224
00:09:18,769 --> 00:09:22,639
analyze any of these like emotion

225
00:09:21,170 --> 00:09:26,029
registering in her face she would look

226
00:09:22,639 --> 00:09:27,769
probably really pissed off or maybe

227
00:09:26,029 --> 00:09:29,029
let's give like another example maybe

228
00:09:27,769 --> 00:09:31,489
when that's also kind of a little bit

229
00:09:29,029 --> 00:09:34,699
more realistic how many have you have

230
00:09:31,489 --> 00:09:36,559
ever smiled or laughed when you've been

231
00:09:34,699 --> 00:09:38,979
really scared or uncomfortable to defeat

232
00:09:36,559 --> 00:09:41,660
to defuse an uncomfortable situation

233
00:09:38,980 --> 00:09:43,699
about a lot of women or marginalized

234
00:09:41,660 --> 00:09:46,730
groups in this room have felt that right

235
00:09:43,699 --> 00:09:49,910
were you actually happy in that moment

236
00:09:46,730 --> 00:09:52,609
is that moment like the evocative most

237
00:09:49,910 --> 00:09:55,759
truthful moment of you in that time no

238
00:09:52,609 --> 00:09:58,519
it's not can we a moat all the things

239
00:09:55,759 --> 00:10:01,100
that we're feeling in real time should

240
00:09:58,519 --> 00:10:03,709
we I don't think so we should have the

241
00:10:01,100 --> 00:10:05,239
ability right to mask or hide what we're

242
00:10:03,709 --> 00:10:07,518
feeling it sometimes social situations

243
00:10:05,239 --> 00:10:09,230
call for that I sound probably really

244
00:10:07,519 --> 00:10:11,569
funny and gregarious right now you have

245
00:10:09,230 --> 00:10:13,009
no idea if I'm having a good day or a

246
00:10:11,569 --> 00:10:18,229
bad day nor should you because I'm

247
00:10:13,009 --> 00:10:20,809
onstage like lecturing at you but one of

248
00:10:18,230 --> 00:10:22,699
the things I want to highlight is if

249
00:10:20,809 --> 00:10:24,319
this is being used to train on the ways

250
00:10:22,699 --> 00:10:26,389
that we classically think of emotions

251
00:10:24,319 --> 00:10:27,949
sort of presenting themselves it's

252
00:10:26,389 --> 00:10:30,589
important to think about how wrong that

253
00:10:27,949 --> 00:10:32,508
can be and how this could be used to

254
00:10:30,589 --> 00:10:34,759
serve in for any kind of deeper findings

255
00:10:32,509 --> 00:10:36,199
or meanings from an event and then on

256
00:10:34,759 --> 00:10:37,730
the other side of things it's always

257
00:10:36,199 --> 00:10:41,479
important to think about do we express

258
00:10:37,730 --> 00:10:43,519
happiness culturally the same do I as an

259
00:10:41,480 --> 00:10:45,949
American Express happiness the same as a

260
00:10:43,519 --> 00:10:48,860
German or is someone from mainland China

261
00:10:45,949 --> 00:10:51,019
or is someone from India does happiness

262
00:10:48,860 --> 00:10:55,879
look the same universally do we express

263
00:10:51,019 --> 00:10:57,350
happiness in that way so a lot of this

264
00:10:55,879 --> 00:10:59,269
talk is trying to highlight this idea of

265
00:10:57,350 --> 00:11:01,459
what is bad what is good and how do you

266
00:10:59,269 --> 00:11:04,189
serve more deeply dive into artificial

267
00:11:01,459 --> 00:11:06,378
intelligence that AI isn't just because

268
00:11:04,189 --> 00:11:08,539
algorithms are broken and tech is awful

269
00:11:06,379 --> 00:11:10,579
it's really I think trying to understand

270
00:11:08,539 --> 00:11:13,339
the nuances between what's in your data

271
00:11:10,579 --> 00:11:15,799
set how it can unintentionally and train

272
00:11:13,339 --> 00:11:18,399
your algorithms how that can change the

273
00:11:15,799 --> 00:11:20,749
product and how they can then harm users

274
00:11:18,399 --> 00:11:23,179
but it's also what you intend to do with

275
00:11:20,749 --> 00:11:25,549
the product as we saw with face ception

276
00:11:23,179 --> 00:11:26,180
and how the intentions of that design

277
00:11:25,549 --> 00:11:28,579
can harm

278
00:11:26,180 --> 00:11:30,979
people and it's also for anything about

279
00:11:28,580 --> 00:11:36,050
what how are we defining harm how can we

280
00:11:30,980 --> 00:11:37,970
also fix harm how I think a great way to

281
00:11:36,050 --> 00:11:40,579
try to mitigate this idea of harm sort

282
00:11:37,970 --> 00:11:43,160
of holistically is moving towards making

283
00:11:40,580 --> 00:11:45,920
AI more accessible and we can look at

284
00:11:43,160 --> 00:11:48,439
open-source as a way for this like

285
00:11:45,920 --> 00:11:51,079
should there be code releases that are

286
00:11:48,440 --> 00:11:53,920
open to external audits datasets that

287
00:11:51,080 --> 00:11:56,240
the public can dig into and actually see

288
00:11:53,920 --> 00:11:58,880
explanations that are easy to understand

289
00:11:56,240 --> 00:12:00,980
how the algorithm was made how how old

290
00:11:58,880 --> 00:12:03,350
it was and who made it this is what I

291
00:12:00,980 --> 00:12:04,760
mean by explainable AI let's take what

292
00:12:03,350 --> 00:12:06,649
artificial intelligence is when it's

293
00:12:04,760 --> 00:12:08,360
inserted into a space and actually break

294
00:12:06,649 --> 00:12:10,550
down and explain what it's doing when

295
00:12:08,360 --> 00:12:12,050
and how I think there are some pretty

296
00:12:10,550 --> 00:12:15,109
good examples that start to push towards

297
00:12:12,050 --> 00:12:16,670
us especially with data and this is

298
00:12:15,110 --> 00:12:17,959
where data is important and I want to

299
00:12:16,670 --> 00:12:19,520
highlight this project that I really

300
00:12:17,959 --> 00:12:23,140
love and I love it not because I'm a

301
00:12:19,520 --> 00:12:25,310
Mozilla fellow but this is common voice

302
00:12:23,140 --> 00:12:27,470
common voice is a project by the Mozilla

303
00:12:25,310 --> 00:12:30,020
Foundation and Corporation as an

304
00:12:27,470 --> 00:12:32,150
open-source data set common voice is one

305
00:12:30,020 --> 00:12:35,329
of the largest public domain transcribed

306
00:12:32,150 --> 00:12:37,939
voice data sets ever made it has more

307
00:12:35,330 --> 00:12:41,510
than 1400 hours of voice data with 18

308
00:12:37,940 --> 00:12:43,730
languages represented now why would we

309
00:12:41,510 --> 00:12:46,250
need an open-source voice data set what

310
00:12:43,730 --> 00:12:49,040
does it do what could it possibly solve

311
00:12:46,250 --> 00:12:50,839
well voice detects automation is the

312
00:12:49,040 --> 00:12:53,000
frontier of creating more accessible

313
00:12:50,839 --> 00:12:55,250
products this is AI helping and

314
00:12:53,000 --> 00:12:56,990
disability and accessibility and common

315
00:12:55,250 --> 00:12:59,060
voice supports multiple languages and

316
00:12:56,990 --> 00:13:01,400
that means multiple dialects and accents

317
00:12:59,060 --> 00:13:04,160
that's something that Siri doesn't quite

318
00:13:01,400 --> 00:13:08,089
do very well but anyone any company

319
00:13:04,160 --> 00:13:09,529
could use common voice right now or as I

320
00:13:08,089 --> 00:13:11,660
mentioned the description of this talk a

321
00:13:09,529 --> 00:13:12,980
project I've been working on it which is

322
00:13:11,660 --> 00:13:15,380
an open-source project in the making

323
00:13:12,980 --> 00:13:16,430
which is feminist data set the

324
00:13:15,380 --> 00:13:17,660
mysterious that comes out of the

325
00:13:16,430 --> 00:13:18,949
culmination of a lot of different work

326
00:13:17,660 --> 00:13:21,439
I've been doing over the past couple of

327
00:13:18,950 --> 00:13:23,180
years and I had this idea when I was

328
00:13:21,440 --> 00:13:25,100
focusing mainly on toxicity and

329
00:13:23,180 --> 00:13:26,930
harassment in social networks and

330
00:13:25,100 --> 00:13:29,510
looking at toxicity in language and

331
00:13:26,930 --> 00:13:30,949
algorithms part of what I do outside of

332
00:13:29,510 --> 00:13:32,810
looking at explainable AI is I actually

333
00:13:30,950 --> 00:13:34,760
study online harass but in the alt-right

334
00:13:32,810 --> 00:13:37,910
which may make the first gift you saw a

335
00:13:34,760 --> 00:13:39,500
lot more easy to understand this means

336
00:13:37,910 --> 00:13:43,550
is I look at a lot of traumatic

337
00:13:39,500 --> 00:13:45,530
data I look at how people organize and

338
00:13:43,550 --> 00:13:48,050
congregate especially inside of white

339
00:13:45,530 --> 00:13:51,410
supremacy in white nationalist groups on

340
00:13:48,050 --> 00:13:54,740
spaces like discord Twitter 4chan HN and

341
00:13:51,410 --> 00:13:57,050
reddit and outside and within this work

342
00:13:54,740 --> 00:13:59,060
I started to make different kinds of

343
00:13:57,050 --> 00:14:01,790
publications to support the work I was

344
00:13:59,060 --> 00:14:04,099
looking at for example looking at how

345
00:14:01,790 --> 00:14:05,599
the alt-right when when they're kicked

346
00:14:04,100 --> 00:14:07,490
off of a platform or a different space

347
00:14:05,600 --> 00:14:09,170
will make an alt write version of that

348
00:14:07,490 --> 00:14:12,020
so when they're kicked off patreon

349
00:14:09,170 --> 00:14:15,410
they'll go to a place that is actually

350
00:14:12,020 --> 00:14:17,510
called hay Tree on and I also made a

351
00:14:15,410 --> 00:14:19,610
dictionary that was trying to codify and

352
00:14:17,510 --> 00:14:22,189
define different slang terms that were

353
00:14:19,610 --> 00:14:24,500
being used by the alright this hate

354
00:14:22,190 --> 00:14:26,150
speech dictionary is currently being

355
00:14:24,500 --> 00:14:27,980
used by the Southern Poverty Law Center

356
00:14:26,150 --> 00:14:31,579
to serve support a lot of the hate

357
00:14:27,980 --> 00:14:33,350
speech work that they're looking at it's

358
00:14:31,580 --> 00:14:35,540
important in this in this scenario does

359
00:14:33,350 --> 00:14:37,130
like know and highlight that language

360
00:14:35,540 --> 00:14:39,740
especially once in a digital space is

361
00:14:37,130 --> 00:14:41,810
data so even this act of creating this

362
00:14:39,740 --> 00:14:44,420
hate speech data set I was contributing

363
00:14:41,810 --> 00:14:47,599
to I was contributing to creating data

364
00:14:44,420 --> 00:14:50,479
to study larger larger data sets of hate

365
00:14:47,600 --> 00:14:52,730
speech and I got to a point where I

366
00:14:50,480 --> 00:14:54,230
wanted to have a bit of a break from

367
00:14:52,730 --> 00:14:56,060
what the majority of my work was

368
00:14:54,230 --> 00:14:58,220
focusing on I wanted to make something

369
00:14:56,060 --> 00:14:59,839
that could counteract the hate speech I

370
00:14:58,220 --> 00:15:01,490
was studying I wanted to make something

371
00:14:59,839 --> 00:15:03,470
that would sort of challenge our

372
00:15:01,490 --> 00:15:05,990
concepts of how inequity and bias or of

373
00:15:03,470 --> 00:15:09,320
manifests inside of a data set hence the

374
00:15:05,990 --> 00:15:10,760
creation of feminist data set feminist

375
00:15:09,320 --> 00:15:12,740
data set is a multi year long project

376
00:15:10,760 --> 00:15:15,470
that's collecting and archiving written

377
00:15:12,740 --> 00:15:17,750
feminist works be it song lyrics podcast

378
00:15:15,470 --> 00:15:19,700
transcripts interviews essays and blog

379
00:15:17,750 --> 00:15:21,980
posts which will be put into a public

380
00:15:19,700 --> 00:15:23,870
repository that also functions as an

381
00:15:21,980 --> 00:15:25,310
archive that can then be used to try to

382
00:15:23,870 --> 00:15:27,650
imagine what a feminist chat interface

383
00:15:25,310 --> 00:15:28,819
could look like and what this data is

384
00:15:27,650 --> 00:15:30,920
that can sort of really do within

385
00:15:28,820 --> 00:15:32,839
natural image processing perimeters it's

386
00:15:30,920 --> 00:15:35,150
my way of intervening in AI and it's

387
00:15:32,839 --> 00:15:37,070
also my way of trying to wonder how can

388
00:15:35,150 --> 00:15:40,880
we epically create with data how can we

389
00:15:37,070 --> 00:15:41,960
protest with data so feminist ASA is

390
00:15:40,880 --> 00:15:44,120
trying to offer this new way to think

391
00:15:41,960 --> 00:15:45,860
about data sets and data collection by

392
00:15:44,120 --> 00:15:47,570
specifically looking at intersectional

393
00:15:45,860 --> 00:15:50,030
feminism as a framework and structure

394
00:15:47,570 --> 00:15:51,860
for data interrogation and community

395
00:15:50,030 --> 00:15:54,810
collection of

396
00:15:51,860 --> 00:15:56,280
and this is an example of feminist

397
00:15:54,810 --> 00:15:58,500
dataset being installed at the Victoria

398
00:15:56,280 --> 00:16:00,510
and Albert Museum this project is also

399
00:15:58,500 --> 00:16:02,190
really trying to push the idea that

400
00:16:00,510 --> 00:16:04,230
something as benign as data collection

401
00:16:02,190 --> 00:16:05,670
can be utilized and activated as a

402
00:16:04,230 --> 00:16:07,650
community protest and community

403
00:16:05,670 --> 00:16:09,719
collaboration we live in a data

404
00:16:07,650 --> 00:16:11,430
saturated environment and if we're ever

405
00:16:09,720 --> 00:16:13,260
gonna figure out if ethical data and

406
00:16:11,430 --> 00:16:15,150
ethical ethical data collection as

407
00:16:13,260 --> 00:16:17,010
possible I believe this has to come from

408
00:16:15,150 --> 00:16:19,410
the community and community organized

409
00:16:17,010 --> 00:16:22,050
events and this is really what feminist

410
00:16:19,410 --> 00:16:23,850
dataset is trying to explore Fitness

411
00:16:22,050 --> 00:16:26,120
data set is also deeper part of work

412
00:16:23,850 --> 00:16:29,990
that I've been looking at around where

413
00:16:26,120 --> 00:16:32,310
where design fits in creating equity

414
00:16:29,990 --> 00:16:34,560
especially within software design in

415
00:16:32,310 --> 00:16:35,699
products before I get into the next

416
00:16:34,560 --> 00:16:37,890
couple slides I need to give you some

417
00:16:35,700 --> 00:16:41,490
quick design background how many of you

418
00:16:37,890 --> 00:16:43,560
have heard of human centered design okay

419
00:16:41,490 --> 00:16:45,770
okay some so I'm glad to include this

420
00:16:43,560 --> 00:16:48,599
slide because I added that this morning

421
00:16:45,770 --> 00:16:52,319
human centered design is a methodology

422
00:16:48,600 --> 00:16:54,000
that is used is used or a variation of

423
00:16:52,320 --> 00:16:56,640
it is used in almost every product that

424
00:16:54,000 --> 00:16:59,370
you're touching right now it's an idea

425
00:16:56,640 --> 00:17:02,069
that Anna framework that was created by

426
00:16:59,370 --> 00:17:03,870
the design firm IDEO and it comes from a

427
00:17:02,070 --> 00:17:06,660
paper that was published in 1989 on

428
00:17:03,870 --> 00:17:08,609
human centered systems human centered

429
00:17:06,660 --> 00:17:09,990
design is defined as a design and

430
00:17:08,609 --> 00:17:12,329
management framework that develops

431
00:17:09,990 --> 00:17:14,099
solutions to problems by involving the

432
00:17:12,329 --> 00:17:16,919
human perspective in all steps of the

433
00:17:14,099 --> 00:17:18,359
problem-solving process so human

434
00:17:16,920 --> 00:17:20,010
centered design is just really this

435
00:17:18,359 --> 00:17:22,109
deeper framework that asks how does this

436
00:17:20,010 --> 00:17:24,930
affect people and tries to break people

437
00:17:22,109 --> 00:17:26,069
into different kinds of groups no this

438
00:17:24,930 --> 00:17:28,680
probably doesn't actually sound very

439
00:17:26,069 --> 00:17:31,530
revolutionary to us right now but in the

440
00:17:28,680 --> 00:17:33,840
1980s it was and so all the different

441
00:17:31,530 --> 00:17:35,940
products in tech were using a lot of the

442
00:17:33,840 --> 00:17:38,939
way that its design comes from this

443
00:17:35,940 --> 00:17:41,280
space but I think to create with AI we

444
00:17:38,940 --> 00:17:43,140
need a new kind of framework one that

445
00:17:41,280 --> 00:17:44,820
moves beyond human Center design and

446
00:17:43,140 --> 00:17:47,220
focuses on Human Rights centered design

447
00:17:44,820 --> 00:17:49,620
one that focuses on data accountability

448
00:17:47,220 --> 00:17:51,450
and creation so the idea of human rights

449
00:17:49,620 --> 00:17:53,189
Center design is inspired by the United

450
00:17:51,450 --> 00:17:55,320
Nations International Declaration on

451
00:17:53,190 --> 00:17:57,030
Human Rights frameworks which outlines

452
00:17:55,320 --> 00:17:59,250
basic Nabal rights afforded to all

453
00:17:57,030 --> 00:18:00,810
people we can think about this maybe

454
00:17:59,250 --> 00:18:03,000
more deeply as sovereignty for the

455
00:18:00,810 --> 00:18:05,370
individual but I want to argue that

456
00:18:03,000 --> 00:18:06,960
Verde and dignity is user agency

457
00:18:05,370 --> 00:18:08,699
especially in relationship to data

458
00:18:06,960 --> 00:18:11,940
ownership when it's enacted in

459
00:18:08,700 --> 00:18:13,680
technology and design right now we

460
00:18:11,940 --> 00:18:16,230
aren't giving consumers any ways to

461
00:18:13,680 --> 00:18:17,910
opt-in opt-out change or just how

462
00:18:16,230 --> 00:18:19,920
algorithms affect them and how

463
00:18:17,910 --> 00:18:22,230
algorithms affect them within a product

464
00:18:19,920 --> 00:18:24,300
design space how many of you can really

465
00:18:22,230 --> 00:18:26,490
necessarily opt out of your facebook

466
00:18:24,300 --> 00:18:29,010
timeline or the problems of a facebook

467
00:18:26,490 --> 00:18:31,260
timeline inside of your inside of your

468
00:18:29,010 --> 00:18:33,270
actual Facebook profile if someone is

469
00:18:31,260 --> 00:18:35,010
using timeline and you're not not that

470
00:18:33,270 --> 00:18:37,560
you can opt out of it it would still

471
00:18:35,010 --> 00:18:40,290
affect you so we aren't giving consumers

472
00:18:37,560 --> 00:18:42,659
any way to say yes or no inside of these

473
00:18:40,290 --> 00:18:46,320
spaces we are giving users anyway to

474
00:18:42,660 --> 00:18:48,870
remove their data from a data model so

475
00:18:46,320 --> 00:18:50,639
Human Rights Center design is a part of

476
00:18:48,870 --> 00:18:52,610
steeper project I'm starting to work on

477
00:18:50,640 --> 00:18:54,420
for the Mozilla Foundation which is this

478
00:18:52,610 --> 00:18:59,310
methodology called designing for

479
00:18:54,420 --> 00:19:00,900
transparency so my project outlines

480
00:18:59,310 --> 00:19:02,730
three principles for designing with

481
00:19:00,900 --> 00:19:05,370
transparency machine learning which is

482
00:19:02,730 --> 00:19:07,800
legibility the ability to audit and

483
00:19:05,370 --> 00:19:10,350
creating spaces for impact interaction

484
00:19:07,800 --> 00:19:12,060
and then using design to explain what is

485
00:19:10,350 --> 00:19:15,120
happening inside of an algorithm a data

486
00:19:12,060 --> 00:19:16,770
model or a product what does it mean to

487
00:19:15,120 --> 00:19:18,870
really call for transparency machine

488
00:19:16,770 --> 00:19:20,730
learning and more importantly was it

489
00:19:18,870 --> 00:19:22,949
mean to design for transparency

490
00:19:20,730 --> 00:19:25,050
transparency can mean many different

491
00:19:22,950 --> 00:19:27,480
things it and it can be used or

492
00:19:25,050 --> 00:19:30,379
weaponized for marketing campaigns or

493
00:19:27,480 --> 00:19:32,750
for this Platonic ideal of safety right

494
00:19:30,380 --> 00:19:35,040
transparency often gets related to trust

495
00:19:32,750 --> 00:19:36,690
ideals in this space then can exist

496
00:19:35,040 --> 00:19:38,730
purely as talking points and are not

497
00:19:36,690 --> 00:19:42,180
necessarily not necessarily implemented

498
00:19:38,730 --> 00:19:43,740
in the product or the service and Trust

499
00:19:42,180 --> 00:19:46,320
is tied to transparency then in this

500
00:19:43,740 --> 00:19:48,180
kind of transaction and I think within

501
00:19:46,320 --> 00:19:50,669
product design software design there are

502
00:19:48,180 --> 00:19:52,890
two different versions of trust Trust is

503
00:19:50,670 --> 00:19:54,330
deployed as a modern design trope as a

504
00:19:52,890 --> 00:19:57,150
way to describe relationships between

505
00:19:54,330 --> 00:19:59,610
the user and the software was it mean to

506
00:19:57,150 --> 00:20:03,600
design for trust when trust can exist as

507
00:19:59,610 --> 00:20:05,010
implicit trust or explicit trust it's

508
00:20:03,600 --> 00:20:06,840
important to highlight how Trust can be

509
00:20:05,010 --> 00:20:08,820
gamed but that's but that's what's key

510
00:20:06,840 --> 00:20:11,189
here is how Trust is articulated and

511
00:20:08,820 --> 00:20:12,600
shown to users explicit trust is

512
00:20:11,190 --> 00:20:14,340
seemingly clear and simulated

513
00:20:12,600 --> 00:20:15,719
transparent because an action or policy

514
00:20:14,340 --> 00:20:19,260
seems to be clearly

515
00:20:15,720 --> 00:20:21,150
and seems to be as key explicit trust

516
00:20:19,260 --> 00:20:21,960
can be gained as we've seen with gdpr

517
00:20:21,150 --> 00:20:23,850
notices

518
00:20:21,960 --> 00:20:26,760
while privacy policies can be confusing

519
00:20:23,850 --> 00:20:28,649
and GDP GDP our websites can force

520
00:20:26,760 --> 00:20:30,780
consent by forcing dark patterns that

521
00:20:28,650 --> 00:20:32,490
really obscure our choices the

522
00:20:30,780 --> 00:20:34,470
appearance of clearly stating these

523
00:20:32,490 --> 00:20:36,600
policies gives the illusion and the

524
00:20:34,470 --> 00:20:38,970
feeling of this being explicit trust

525
00:20:36,600 --> 00:20:39,539
because this choice now seems to be out

526
00:20:38,970 --> 00:20:41,820
in the open

527
00:20:39,539 --> 00:20:43,620
instead of being implied this clear

528
00:20:41,820 --> 00:20:46,980
statement with direct links can create

529
00:20:43,620 --> 00:20:48,059
the this illusion of transparency and I

530
00:20:46,980 --> 00:20:50,250
when the highlights of interesting book

531
00:20:48,059 --> 00:20:53,158
or an interesting book about trust and

532
00:20:50,250 --> 00:20:54,929
technology in her book calm technology

533
00:20:53,159 --> 00:20:57,510
amber case highlights how privacy

534
00:20:54,929 --> 00:20:59,010
policies can be systems that create

535
00:20:57,510 --> 00:21:02,190
trust in products

536
00:20:59,010 --> 00:21:03,870
she writes great privacy user experience

537
00:21:02,190 --> 00:21:05,909
means that your users will understand

538
00:21:03,870 --> 00:21:08,189
the privacy policy you've created when

539
00:21:05,909 --> 00:21:10,049
they start to use your app a well-built

540
00:21:08,190 --> 00:21:11,400
app will let users know what they're

541
00:21:10,049 --> 00:21:13,679
opting into when they're using your

542
00:21:11,400 --> 00:21:15,179
software so design can really be this

543
00:21:13,679 --> 00:21:17,880
space to show those policies such as

544
00:21:15,179 --> 00:21:21,539
designing in an opt-in UI and user flow

545
00:21:17,880 --> 00:21:23,220
and some GDP our noses do have this case

546
00:21:21,539 --> 00:21:25,770
continues by describing how privacy

547
00:21:23,220 --> 00:21:27,539
policies to users how to explain privacy

548
00:21:25,770 --> 00:21:29,549
policies to users by using something

549
00:21:27,539 --> 00:21:31,669
called plain language and offering

550
00:21:29,549 --> 00:21:33,840
multiple opt-out points such as

551
00:21:31,669 --> 00:21:36,990
explaining what the product or service

552
00:21:33,840 --> 00:21:39,510
is collecting and why how that data will

553
00:21:36,990 --> 00:21:41,510
be used allowing users to download their

554
00:21:39,510 --> 00:21:43,710
own data for the service or product

555
00:21:41,510 --> 00:21:45,330
allowing for the option to permanently

556
00:21:43,710 --> 00:21:47,730
delete accounts and remove their data

557
00:21:45,330 --> 00:21:49,678
from a company's servers case argues

558
00:21:47,730 --> 00:21:51,480
that design considerations are that

559
00:21:49,679 --> 00:21:53,070
these design considerations are actually

560
00:21:51,480 --> 00:21:55,650
transparency and that this form of

561
00:21:53,070 --> 00:21:58,320
transparency will create trust that

562
00:21:55,650 --> 00:22:00,179
companies want with the users that

563
00:21:58,320 --> 00:22:02,189
they're interacting with and in fact we

564
00:22:00,179 --> 00:22:04,260
could argue case isn't just describing

565
00:22:02,190 --> 00:22:07,200
well-articulated privacy policies she's

566
00:22:04,260 --> 00:22:09,929
really also advocating for user agency

567
00:22:07,200 --> 00:22:11,850
the opt-in isn't necessarily a form of

568
00:22:09,929 --> 00:22:14,010
transparency but an interaction a user

569
00:22:11,850 --> 00:22:16,439
can take to have some control agency and

570
00:22:14,010 --> 00:22:18,240
even equity inside of a system the user

571
00:22:16,440 --> 00:22:20,039
gets multiple choices in this case and

572
00:22:18,240 --> 00:22:23,760
not one specific flow or interaction

573
00:22:20,039 --> 00:22:25,770
that they're forced into but in some

574
00:22:23,760 --> 00:22:27,690
cases we really need implicit trust

575
00:22:25,770 --> 00:22:29,700
while working as an online harassment

576
00:22:27,690 --> 00:22:31,800
researcher for the Wikimedia Foundation

577
00:22:29,700 --> 00:22:33,690
my research had to be open-source and

578
00:22:31,800 --> 00:22:35,550
shared with the community but online

579
00:22:33,690 --> 00:22:37,230
harassment data are people's personal

580
00:22:35,550 --> 00:22:39,600
stories about trauma that an individual

581
00:22:37,230 --> 00:22:41,220
has experienced sharing specific

582
00:22:39,600 --> 00:22:43,679
takeaways are linking to a harassment

583
00:22:41,220 --> 00:22:45,510
report can actually out an individual

584
00:22:43,680 --> 00:22:48,930
reach Ramat eyes them or restart a

585
00:22:45,510 --> 00:22:50,790
harassment case so instead my team and i

586
00:22:48,930 --> 00:22:52,950
created this form of semi public

587
00:22:50,790 --> 00:22:55,860
research we called being transparently

588
00:22:52,950 --> 00:22:58,170
opaque which sounds kind of like a

589
00:22:55,860 --> 00:23:00,240
misnomer but bear with me instead of

590
00:22:58,170 --> 00:23:02,400
showing exact cases of harassment or

591
00:23:00,240 --> 00:23:04,020
linking to a specific wiki page we would

592
00:23:02,400 --> 00:23:06,870
generally describe the data we had

593
00:23:04,020 --> 00:23:08,879
collected such as we ran three surveys

594
00:23:06,870 --> 00:23:10,919
over the course of X months that X

595
00:23:08,880 --> 00:23:12,510
individuals filled out we then spoke to

596
00:23:10,920 --> 00:23:15,300
20 users about harassment they

597
00:23:12,510 --> 00:23:17,070
experienced from this kind of harassment

598
00:23:15,300 --> 00:23:18,450
case to this kind of harassment case and

599
00:23:17,070 --> 00:23:21,240
then we generally read a series of

600
00:23:18,450 --> 00:23:23,640
different wiki pages then any takeaway

601
00:23:21,240 --> 00:23:29,030
we shared would be anonymized if used in

602
00:23:23,640 --> 00:23:31,560
a report or truck or presentation so

603
00:23:29,030 --> 00:23:33,950
Alice I'd like to argue that Trust isn't

604
00:23:31,560 --> 00:23:36,270
transparency that can be a byproduct of

605
00:23:33,950 --> 00:23:38,490
transparency which leads me back to an

606
00:23:36,270 --> 00:23:39,930
original but this original question was

607
00:23:38,490 --> 00:23:41,460
it mean to design for transparency

608
00:23:39,930 --> 00:23:44,910
machine learning and how do we do it

609
00:23:41,460 --> 00:23:46,530
especially using sensitive data because

610
00:23:44,910 --> 00:23:48,030
data and machine learning but all

611
00:23:46,530 --> 00:23:51,030
digital products and software is

612
00:23:48,030 --> 00:23:52,200
important data comes from people so I

613
00:23:51,030 --> 00:23:54,660
covered this earlier but to reiterate

614
00:23:52,200 --> 00:23:56,940
I'm divining I'm defining transparencies

615
00:23:54,660 --> 00:23:58,470
these three things transparency is a

616
00:23:56,940 --> 00:24:00,960
mixture of legibility which is the

617
00:23:58,470 --> 00:24:02,670
ability to understand audit ability or

618
00:24:00,960 --> 00:24:05,060
the ability to audit a process data

619
00:24:02,670 --> 00:24:07,290
point or intention so building upon

620
00:24:05,060 --> 00:24:09,629
legibility and then understand it enough

621
00:24:07,290 --> 00:24:12,180
to request changes or give feedback and

622
00:24:09,630 --> 00:24:14,010
then interaction or or agency the

623
00:24:12,180 --> 00:24:17,190
ability to affect change or decision

624
00:24:14,010 --> 00:24:18,870
making from auditability legibility

625
00:24:17,190 --> 00:24:20,880
really means that something has to be

626
00:24:18,870 --> 00:24:24,060
stated and revealed to us but the

627
00:24:20,880 --> 00:24:25,620
process has to be understandable let's

628
00:24:24,060 --> 00:24:27,899
examine bureaucratic procedures as a

629
00:24:25,620 --> 00:24:29,610
metaphor for AI for a second this

630
00:24:27,900 --> 00:24:31,710
example the processes are revealed

631
00:24:29,610 --> 00:24:33,179
almost explicitly shared but that

632
00:24:31,710 --> 00:24:34,500
doesn't mean we understand it that

633
00:24:33,180 --> 00:24:36,720
doesn't mean that common users

634
00:24:34,500 --> 00:24:39,630
understand why a bureaucratic procedure

635
00:24:36,720 --> 00:24:41,430
happens or what it actually means this

636
00:24:39,630 --> 00:24:42,210
key takeaway is important one could

637
00:24:41,430 --> 00:24:44,220
share or

638
00:24:42,210 --> 00:24:46,260
say what a policy process is but for it

639
00:24:44,220 --> 00:24:48,150
to be understandable it has to it has to

640
00:24:46,260 --> 00:24:52,170
have legibility to be really transparent

641
00:24:48,150 --> 00:24:53,790
in this case so what does legibility

642
00:24:52,170 --> 00:24:55,410
look like in action this is when it's

643
00:24:53,790 --> 00:24:58,500
important to think of design not just as

644
00:24:55,410 --> 00:25:00,000
UX uija graphic it's content strategy

645
00:24:58,500 --> 00:25:02,040
and it's communication design its

646
00:25:00,000 --> 00:25:04,050
distilling or rather translating

647
00:25:02,040 --> 00:25:06,540
complexity complexities into an artifact

648
00:25:04,050 --> 00:25:09,840
an artifact can be an app or it can be

649
00:25:06,540 --> 00:25:11,550
clear and concise documentation 18f a

650
00:25:09,840 --> 00:25:12,620
design and technology firm centered in

651
00:25:11,550 --> 00:25:15,480
the arm of the US federal government

652
00:25:12,620 --> 00:25:17,100
designed a Content guide and curriculum

653
00:25:15,480 --> 00:25:19,440
on how to use plain language and

654
00:25:17,100 --> 00:25:20,879
governmental and federal services when

655
00:25:19,440 --> 00:25:22,560
they created what they created was an

656
00:25:20,880 --> 00:25:24,210
asset that was flexible and then it

657
00:25:22,560 --> 00:25:26,399
could scale from a local level all the

658
00:25:24,210 --> 00:25:27,210
way up to a national level and it was

659
00:25:26,400 --> 00:25:29,280
flexible the sense that it was

660
00:25:27,210 --> 00:25:31,290
guidelines and principles this is what

661
00:25:29,280 --> 00:25:35,580
they thought about scalability across a

662
00:25:31,290 --> 00:25:37,379
federal system legibility is important

663
00:25:35,580 --> 00:25:39,870
when it manifests though as designed

664
00:25:37,380 --> 00:25:41,310
user experience in a system when it

665
00:25:39,870 --> 00:25:43,800
fails it can have incredibly dire

666
00:25:41,310 --> 00:25:45,750
consequences because legibly requires

667
00:25:43,800 --> 00:25:47,280
analyzing distilling these systems and

668
00:25:45,750 --> 00:25:49,550
when those systems fail and we rely

669
00:25:47,280 --> 00:25:53,010
solely on documentation problems happen

670
00:25:49,550 --> 00:25:54,899
so in January 13th 2018 a federal

671
00:25:53,010 --> 00:25:58,650
employee in Hawaii accidentally sent out

672
00:25:54,900 --> 00:26:00,570
this message and this is what everyone

673
00:25:58,650 --> 00:26:04,860
within the state of Hawaii saw on their

674
00:26:00,570 --> 00:26:07,860
phones and this was up for 30 minutes

675
00:26:04,860 --> 00:26:10,320
and this was a false alarm there were no

676
00:26:07,860 --> 00:26:12,149
missiles enroute but people believed

677
00:26:10,320 --> 00:26:14,639
this for 30 minutes as this is the

678
00:26:12,150 --> 00:26:16,410
notification that they had received but

679
00:26:14,640 --> 00:26:19,050
what had actually happened to sort of

680
00:26:16,410 --> 00:26:20,760
caused this trigger was a an internal

681
00:26:19,050 --> 00:26:25,770
group was testing and updating crisis

682
00:26:20,760 --> 00:26:27,300
software that looked like this so it's

683
00:26:25,770 --> 00:26:29,670
important to note here that this is

684
00:26:27,300 --> 00:26:31,260
actual software an actual interface and

685
00:26:29,670 --> 00:26:34,980
this is what a lot of software looks

686
00:26:31,260 --> 00:26:36,960
like for civil servants Sid Harrell a

687
00:26:34,980 --> 00:26:38,900
civic designer and former head of code

688
00:26:36,960 --> 00:26:41,670
of head of product at Code for America

689
00:26:38,900 --> 00:26:42,900
retweeted this in response the greatest

690
00:26:41,670 --> 00:26:44,910
trick the devil ever pulled was

691
00:26:42,900 --> 00:26:46,710
convincing the world that enterprise

692
00:26:44,910 --> 00:26:47,950
software complexities properly addressed

693
00:26:46,710 --> 00:26:49,720
via training

694
00:26:47,950 --> 00:26:51,640
and that kind of explains how the

695
00:26:49,720 --> 00:26:53,980
previous slide came to be designed

696
00:26:51,640 --> 00:26:56,350
doesn't it the legibility here was that

697
00:26:53,980 --> 00:26:59,290
training could solve and overcome any

698
00:26:56,350 --> 00:27:02,740
kind of complexity but we know that this

699
00:26:59,290 --> 00:27:04,360
isn't true because of what happened it's

700
00:27:02,740 --> 00:27:06,100
important to look at this that nothing

701
00:27:04,360 --> 00:27:08,469
in the UI is really legible or

702
00:27:06,100 --> 00:27:10,810
understandable whereas the logic the

703
00:27:08,470 --> 00:27:13,300
hierarchy the selection was there a

704
00:27:10,810 --> 00:27:15,310
conformation UI that popped up prompting

705
00:27:13,300 --> 00:27:17,590
the user to really reaffirm the choice

706
00:27:15,310 --> 00:27:19,810
they were making in an interview with me

707
00:27:17,590 --> 00:27:21,220
Harrell said I think it's true in the

708
00:27:19,810 --> 00:27:22,629
commercial world that interfaces for

709
00:27:21,220 --> 00:27:25,030
employees are rarely as good as

710
00:27:22,630 --> 00:27:26,500
interfaces for customers or users and in

711
00:27:25,030 --> 00:27:29,050
particular when departments are strapped

712
00:27:26,500 --> 00:27:30,880
by resources and constrained if you read

713
00:27:29,050 --> 00:27:32,800
any of the sources about Hawaii the

714
00:27:30,880 --> 00:27:35,200
government there had only three vendors

715
00:27:32,800 --> 00:27:36,970
you you could choose to work with and

716
00:27:35,200 --> 00:27:40,090
those requirements don't include things

717
00:27:36,970 --> 00:27:41,590
like usability usability and legibility

718
00:27:40,090 --> 00:27:43,510
or what help makes products and

719
00:27:41,590 --> 00:27:46,270
processes transparent and not

720
00:27:43,510 --> 00:27:48,430
adversarial while these examples are not

721
00:27:46,270 --> 00:27:49,900
explicitly about machine learning it

722
00:27:48,430 --> 00:27:51,550
works really well as a descriptor

723
00:27:49,900 --> 00:27:54,370
specifically on legibility throughout

724
00:27:51,550 --> 00:27:55,810
the entire product and design cycle but

725
00:27:54,370 --> 00:27:57,459
seeing something and understanding

726
00:27:55,810 --> 00:28:00,760
something are not quite transparency

727
00:27:57,460 --> 00:28:03,250
it's really just the first step which

728
00:28:00,760 --> 00:28:05,260
brings us to audit ability mall solder a

729
00:28:03,250 --> 00:28:07,630
researcher and author explained audit

730
00:28:05,260 --> 00:28:08,830
ability in an interview they say what

731
00:28:07,630 --> 00:28:10,510
we've lost in this rush towards

732
00:28:08,830 --> 00:28:13,720
transparency is we now have this ability

733
00:28:10,510 --> 00:28:15,070
to just see things but seeing something

734
00:28:13,720 --> 00:28:16,870
isn't really having an impact on

735
00:28:15,070 --> 00:28:19,360
something but we still need to see

736
00:28:16,870 --> 00:28:21,520
something to understand it right so

737
00:28:19,360 --> 00:28:23,620
auditability builds upon legibility to

738
00:28:21,520 --> 00:28:25,060
offer a space to audit for machine

739
00:28:23,620 --> 00:28:27,340
learning this is really important

740
00:28:25,060 --> 00:28:28,810
removing bias and discussing data steps

741
00:28:27,340 --> 00:28:30,850
are often the suggestions for

742
00:28:28,810 --> 00:28:32,950
transparency but being able to audit and

743
00:28:30,850 --> 00:28:34,870
provide feedback that will be taken into

744
00:28:32,950 --> 00:28:36,490
consideration or actually having the

745
00:28:34,870 --> 00:28:39,429
ability to change the product as

746
00:28:36,490 --> 00:28:41,860
necessary we see audit ability and open

747
00:28:39,430 --> 00:28:43,720
source that can be filing a bug report

748
00:28:41,860 --> 00:28:45,760
or forking code it can be volunteering

749
00:28:43,720 --> 00:28:47,560
to create a process or product then

750
00:28:45,760 --> 00:28:49,090
having that thing you've suggested

751
00:28:47,560 --> 00:28:51,429
accepted and acknowledged in the

752
00:28:49,090 --> 00:28:53,560
ecosystem or service auditability can

753
00:28:51,430 --> 00:28:55,390
also be public forums where users or

754
00:28:53,560 --> 00:28:57,720
volunteers can voice concerns and see a

755
00:28:55,390 --> 00:28:59,740
response and impact from that

756
00:28:57,720 --> 00:29:01,990
but couldn't that also just be a better

757
00:28:59,740 --> 00:29:03,730
description of interaction or impact we

758
00:29:01,990 --> 00:29:05,680
need to go a step further which is you

759
00:29:03,730 --> 00:29:07,930
need to design the space for interaction

760
00:29:05,680 --> 00:29:10,180
and agency and again solder reinforces

761
00:29:07,930 --> 00:29:11,980
this is impact a firm can announce we're

762
00:29:10,180 --> 00:29:13,570
doing X at this location and that is a

763
00:29:11,980 --> 00:29:15,400
form of transparency because they're

764
00:29:13,570 --> 00:29:17,169
revealing the location and announcing

765
00:29:15,400 --> 00:29:19,120
what they're doing but can people

766
00:29:17,170 --> 00:29:20,800
meaningfully interact or change its

767
00:29:19,120 --> 00:29:22,719
situation you have to have the ability

768
00:29:20,800 --> 00:29:24,760
to impact it and for that impact to be

769
00:29:22,720 --> 00:29:26,740
meaningful meaning the system needs to

770
00:29:24,760 --> 00:29:29,080
be designed to take what was audited and

771
00:29:26,740 --> 00:29:31,470
respond to that and and then there have

772
00:29:29,080 --> 00:29:34,030
an emphasis emphasis that some kind of

773
00:29:31,470 --> 00:29:35,790
implementation will happen or could

774
00:29:34,030 --> 00:29:38,170
happen at some point

775
00:29:35,790 --> 00:29:39,790
so as this mean in practice to design

776
00:29:38,170 --> 00:29:42,910
transparency for machine learning it's

777
00:29:39,790 --> 00:29:44,770
applying those standards to looking at

778
00:29:42,910 --> 00:29:47,410
how data is used how an algorithm is

779
00:29:44,770 --> 00:29:48,790
trained what the data models training

780
00:29:47,410 --> 00:29:50,820
the algorithms are doing and that how

781
00:29:48,790 --> 00:29:54,100
the product itself is using an algorithm

782
00:29:50,820 --> 00:29:55,720
the above is what I'm sort of calling

783
00:29:54,100 --> 00:29:57,010
data ingredients calls out of this

784
00:29:55,720 --> 00:29:59,530
methodology for designing for

785
00:29:57,010 --> 00:30:03,360
transparency should datasets have

786
00:29:59,530 --> 00:30:05,590
ratings explanations or readable labels

787
00:30:03,360 --> 00:30:08,159
drugs in the United States have many

788
00:30:05,590 --> 00:30:10,120
approval processes they're tested

789
00:30:08,160 --> 00:30:11,500
they're looked at the different kinds of

790
00:30:10,120 --> 00:30:12,909
ingredients that lurked at how they

791
00:30:11,500 --> 00:30:16,780
would be able to solve or impact

792
00:30:12,910 --> 00:30:19,240
different kinds of injuries or illnesses

793
00:30:16,780 --> 00:30:20,710
etc food for example also has

794
00:30:19,240 --> 00:30:23,410
ingredients and caloric information

795
00:30:20,710 --> 00:30:24,700
listed well this is it perfect we should

796
00:30:23,410 --> 00:30:27,010
be thinking about ratings and

797
00:30:24,700 --> 00:30:28,480
accountability for data what if

798
00:30:27,010 --> 00:30:30,970
algorithms came with this kind of

799
00:30:28,480 --> 00:30:32,650
labeling fee this is a good idea you're

800
00:30:30,970 --> 00:30:34,300
maybe not the only one a similar

801
00:30:32,650 --> 00:30:36,820
methodology has also been proposed by

802
00:30:34,300 --> 00:30:38,620
Kate Crawford the head of AI now a group

803
00:30:36,820 --> 00:30:41,530
of collaborators in a paper called data

804
00:30:38,620 --> 00:30:44,800
sheets or better ways to label data sets

805
00:30:41,530 --> 00:30:49,090
as this our data future

806
00:30:44,800 --> 00:30:51,280
it could be this is where I think design

807
00:30:49,090 --> 00:30:53,860
is really really useful in in this

808
00:30:51,280 --> 00:30:56,620
conversation design is what can distill

809
00:30:53,860 --> 00:30:59,169
policy and code into a digestible and

810
00:30:56,620 --> 00:31:00,909
interactable interface for users design

811
00:30:59,170 --> 00:31:03,130
is really that thing that can explain

812
00:31:00,910 --> 00:31:04,870
what Co and policy are doing especially

813
00:31:03,130 --> 00:31:06,670
when it comes to machine learning design

814
00:31:04,870 --> 00:31:08,979
is important and it's really integral to

815
00:31:06,670 --> 00:31:10,919
this bigger idea of Human Rights

816
00:31:08,980 --> 00:31:13,629
centered design

817
00:31:10,919 --> 00:31:16,509
so let's imagine these frameworks I've

818
00:31:13,629 --> 00:31:19,480
sort of outlined more in action let me

819
00:31:16,509 --> 00:31:22,749
tell you this weird adjacent but funny

820
00:31:19,480 --> 00:31:25,960
Side Story I dated someone for years ago

821
00:31:22,749 --> 00:31:30,669
who had really beautiful like Disney

822
00:31:25,960 --> 00:31:33,009
Prince style hair and he broke my heart

823
00:31:30,669 --> 00:31:34,869
and I'm sure some of you have done here

824
00:31:33,009 --> 00:31:36,489
you probably I don't know if you

825
00:31:34,869 --> 00:31:39,639
listened to a lot of music during a

826
00:31:36,489 --> 00:31:41,489
breakup but I definitely did and my

827
00:31:39,639 --> 00:31:44,279
Spotify drug of choice was this

828
00:31:41,489 --> 00:31:48,249
embarrassing band called Mumford & Sons

829
00:31:44,279 --> 00:31:51,220
and I listened to that band every single

830
00:31:48,249 --> 00:31:52,690
day for about like two months on repeat

831
00:31:51,220 --> 00:31:56,289
and do you have an idea of like what

832
00:31:52,690 --> 00:31:58,149
that did it like up my discover

833
00:31:56,289 --> 00:32:02,109
weekly and it's still up and that

834
00:31:58,149 --> 00:32:04,629
was four years ago and the worst part

835
00:32:02,109 --> 00:32:05,949
about this is like I it up like I

836
00:32:04,629 --> 00:32:07,769
I don't know if I can really blame

837
00:32:05,950 --> 00:32:12,820
anybody else like I'm completely

838
00:32:07,769 --> 00:32:15,820
culpable in this and to this day I still

839
00:32:12,820 --> 00:32:17,619
get recommendations that are like I call

840
00:32:15,820 --> 00:32:19,629
them like light poppy versions of

841
00:32:17,619 --> 00:32:23,379
Mumford & Sons if you could imagine such

842
00:32:19,629 --> 00:32:25,418
bands existing and this maybe seems like

843
00:32:23,379 --> 00:32:26,709
a really innocuous and silly example but

844
00:32:25,419 --> 00:32:28,749
I think it's important to highlight

845
00:32:26,710 --> 00:32:30,730
because I actually can't really

846
00:32:28,749 --> 00:32:33,039
intervene and might discover weakly to

847
00:32:30,730 --> 00:32:35,289
change it there's not really anything I

848
00:32:33,039 --> 00:32:38,950
can do in the algorithm of that product

849
00:32:35,289 --> 00:32:41,049
to retrain it and I use Spotify every

850
00:32:38,950 --> 00:32:44,470
single day because I spend a lot of time

851
00:32:41,049 --> 00:32:46,359
working listening to music so why can't

852
00:32:44,470 --> 00:32:48,399
I make my discover weekly forget why

853
00:32:46,359 --> 00:32:50,470
can't I as a user suggest what I'm

854
00:32:48,399 --> 00:32:52,359
actually interested in right now can I

855
00:32:50,470 --> 00:32:55,809
just please block like Mumford &

856
00:32:52,359 --> 00:32:57,189
Sons from my discover weekly and I

857
00:32:55,809 --> 00:32:59,139
highlight this because if an algorithm

858
00:32:57,190 --> 00:33:01,419
is looking for user flags and queues

859
00:32:59,139 --> 00:33:03,998
some of those cues may not be good eg

860
00:33:01,419 --> 00:33:06,100
Mumford & Sons some of those Flags may

861
00:33:03,999 --> 00:33:08,379
also be an accident so what you see here

862
00:33:06,100 --> 00:33:10,269
is a suggestion I made for Harvard

863
00:33:08,379 --> 00:33:12,998
Shorenstein centers publication called

864
00:33:10,269 --> 00:33:14,950
privacy by design and it pulls from my

865
00:33:12,999 --> 00:33:17,200
designing for transparency methodology

866
00:33:14,950 --> 00:33:19,960
skive imagines well what if what if this

867
00:33:17,200 --> 00:33:21,399
kind of label really did exist in my

868
00:33:19,960 --> 00:33:23,800
Discover Weekly what if I could just

869
00:33:21,399 --> 00:33:26,560
click on an eye for ingredients

870
00:33:23,800 --> 00:33:28,540
and have some kind of breakdown as to

871
00:33:26,560 --> 00:33:31,720
maybe how their discover weekly was made

872
00:33:28,540 --> 00:33:34,330
and some kind of way for me to forget

873
00:33:31,720 --> 00:33:36,640
things or suggest things or like design

874
00:33:34,330 --> 00:33:40,300
would be a better version of my discover

875
00:33:36,640 --> 00:33:42,790
weekly any go this is important because

876
00:33:40,300 --> 00:33:45,100
it's really just trying to explain what

877
00:33:42,790 --> 00:33:47,020
it's doing and allowing space for a user

878
00:33:45,100 --> 00:33:49,000
agency in a way that could also like fit

879
00:33:47,020 --> 00:33:50,889
inside the product I think this really

880
00:33:49,000 --> 00:33:54,760
embodies this human right Center design

881
00:33:50,890 --> 00:33:57,280
mindset mister brings us to what is

882
00:33:54,760 --> 00:33:59,560
human rights in our design well it puts

883
00:33:57,280 --> 00:34:02,410
user agency first by always focusing on

884
00:33:59,560 --> 00:34:05,169
consent everything should have a way for

885
00:34:02,410 --> 00:34:06,760
a user to say yes or no human rights

886
00:34:05,170 --> 00:34:08,500
Center design is data protection first

887
00:34:06,760 --> 00:34:11,440
we're recognizing that data is human

888
00:34:08,500 --> 00:34:13,510
inherently always this methodology

889
00:34:11,440 --> 00:34:14,980
doesn't design only with opt-out in mind

890
00:34:13,510 --> 00:34:18,190
Human Rights Center design is

891
00:34:14,980 --> 00:34:19,990
participatory by nature it designs for

892
00:34:18,190 --> 00:34:22,000
the global South first centering on a

893
00:34:19,989 --> 00:34:23,379
diversity of experiences and understands

894
00:34:22,000 --> 00:34:26,050
that localization is different than

895
00:34:23,380 --> 00:34:28,990
translation that language and access is

896
00:34:26,050 --> 00:34:30,490
key and design is integral in that it

897
00:34:28,989 --> 00:34:33,159
actively asked what could possibly go

898
00:34:30,489 --> 00:34:35,259
wrong in this product designs for all

899
00:34:33,159 --> 00:34:37,480
shades of the problem from the benign to

900
00:34:35,260 --> 00:34:40,720
the extreme and then plans for those use

901
00:34:37,480 --> 00:34:42,760
cases because it views use cases not as

902
00:34:40,719 --> 00:34:45,189
edge cases Human Rights Center design

903
00:34:42,760 --> 00:34:48,580
knows that a bug is a feature until you

904
00:34:45,190 --> 00:34:50,080
fix it it's also important to ask then

905
00:34:48,580 --> 00:34:52,449
or look at our society and think of a

906
00:34:50,080 --> 00:34:55,330
wetter design based human rights

907
00:34:52,449 --> 00:34:57,640
violations and to come to mind one is a

908
00:34:55,330 --> 00:34:59,200
bit more extreme and the other is a bit

909
00:34:57,640 --> 00:35:00,850
more benign but nonetheless we should

910
00:34:59,200 --> 00:35:03,189
view both of them as Human Rights Center

911
00:35:00,850 --> 00:35:05,250
design violations should also just view

912
00:35:03,190 --> 00:35:07,780
them perhaps as human rights violations

913
00:35:05,250 --> 00:35:10,420
the first one is this one the systems

914
00:35:07,780 --> 00:35:12,760
and protocols for content moderators the

915
00:35:10,420 --> 00:35:14,140
tools the policies and time limits in

916
00:35:12,760 --> 00:35:15,910
addition to the content the content

917
00:35:14,140 --> 00:35:18,069
moderators are folks are forced to look

918
00:35:15,910 --> 00:35:19,750
at on social networks in the systems

919
00:35:18,070 --> 00:35:22,330
they are forced to work within our human

920
00:35:19,750 --> 00:35:24,400
rights violations most content

921
00:35:22,330 --> 00:35:25,810
moderators have to under have under a

922
00:35:24,400 --> 00:35:27,660
handful of seconds to make a decision

923
00:35:25,810 --> 00:35:30,220
looking at some of the most grotesque

924
00:35:27,660 --> 00:35:32,759
violent traumatic and upsetting content

925
00:35:30,220 --> 00:35:35,649
posts and images all having to memorize

926
00:35:32,760 --> 00:35:37,810
specific policy and trainings from

927
00:35:35,650 --> 00:35:39,940
platforms and it's important that we

928
00:35:37,810 --> 00:35:41,200
see this entirety the tools that they

929
00:35:39,940 --> 00:35:42,640
use as well as the content that they're

930
00:35:41,200 --> 00:35:43,930
looking at the policies that they're

931
00:35:42,640 --> 00:35:45,549
engaging with and the lack of support

932
00:35:43,930 --> 00:35:49,419
that they face from Facebook and Twitter

933
00:35:45,550 --> 00:35:51,160
as a human rights violation this is a

934
00:35:49,420 --> 00:35:55,390
pretty serious one so what is a more

935
00:35:51,160 --> 00:35:57,339
benign one I think we could argue

936
00:35:55,390 --> 00:36:01,060
location services automatically turned

937
00:35:57,340 --> 00:36:02,740
on so does location services

938
00:36:01,060 --> 00:36:04,870
automatically turn on on platform and

939
00:36:02,740 --> 00:36:06,129
images violate human rights well let's

940
00:36:04,870 --> 00:36:08,500
start to look at it and ask does it

941
00:36:06,130 --> 00:36:09,910
endanger sovereignty when you're

942
00:36:08,500 --> 00:36:12,190
designing it's always important to ask

943
00:36:09,910 --> 00:36:16,390
who does it serve and who does it harm

944
00:36:12,190 --> 00:36:18,040
does this deploy dignity we can have

945
00:36:16,390 --> 00:36:19,480
opt-in for location services in the

946
00:36:18,040 --> 00:36:22,509
design and users should be able to

947
00:36:19,480 --> 00:36:24,400
choose location individually not

948
00:36:22,510 --> 00:36:26,320
automatically on images and posts and

949
00:36:24,400 --> 00:36:28,180
that choosing of location can be

950
00:36:26,320 --> 00:36:31,060
something really silly that can be

951
00:36:28,180 --> 00:36:32,410
enjoyable maybe I want to say that I was

952
00:36:31,060 --> 00:36:34,360
hanging out in the woods yesterday in

953
00:36:32,410 --> 00:36:36,460
Moose Jaw Canada which is a very real

954
00:36:34,360 --> 00:36:37,020
place in Canada that I really want to

955
00:36:36,460 --> 00:36:39,340
visit

956
00:36:37,020 --> 00:36:41,980
what is it harm when my location

957
00:36:39,340 --> 00:36:44,980
services are revealed though are like my

958
00:36:41,980 --> 00:36:47,260
actual real location will ask any victim

959
00:36:44,980 --> 00:36:49,150
of domestic violence ask any victim of

960
00:36:47,260 --> 00:36:50,680
online harassment maybe ask any

961
00:36:49,150 --> 00:36:53,110
marginalized person who's faced death

962
00:36:50,680 --> 00:36:55,270
threats does Instagram really really

963
00:36:53,110 --> 00:36:59,860
need my real locations to make a better

964
00:36:55,270 --> 00:37:01,990
product does Apple I think we can make

965
00:36:59,860 --> 00:37:03,790
better things if we think about human

966
00:37:01,990 --> 00:37:05,649
rights center design as this kind of

967
00:37:03,790 --> 00:37:08,080
methodology inside of large systems

968
00:37:05,650 --> 00:37:10,240
especially with machine learning because

969
00:37:08,080 --> 00:37:14,910
it's looking at policy design and code

970
00:37:10,240 --> 00:37:19,200
as ways to express user sovereignty so

971
00:37:14,910 --> 00:37:21,850
TLDR or spoiler alert rather this wasn't

972
00:37:19,200 --> 00:37:24,460
necessarily talk just about AI it was

973
00:37:21,850 --> 00:37:26,020
really a design talk about ethics um

974
00:37:24,460 --> 00:37:28,270
thank you for your time if you have any

975
00:37:26,020 --> 00:37:29,020
questions I'd love to chat and feel free

976
00:37:28,270 --> 00:37:31,990
to drop me a note

977
00:37:29,020 --> 00:37:42,009
my Mozilla email thank you

978
00:37:31,990 --> 00:37:44,740
[Music]

979
00:37:42,010 --> 00:37:53,619
I think we have time for questions and

980
00:37:44,740 --> 00:37:55,209
there's someone with a mic yes we have

981
00:37:53,619 --> 00:37:56,560
time for questions and there's a

982
00:37:55,210 --> 00:37:58,780
microphone in the middle so queue up for

983
00:37:56,560 --> 00:38:00,700
questions if questions arise we have

984
00:37:58,780 --> 00:38:04,030
plenty of time for upcoming questions so

985
00:38:00,700 --> 00:38:06,839
feel free to ask you questions and we

986
00:38:04,030 --> 00:38:06,840
will have time for them

987
00:38:13,820 --> 00:38:17,570
thank you very much for this really

988
00:38:15,740 --> 00:38:19,640
interesting talk and could you talk a

989
00:38:17,570 --> 00:38:21,920
bit more about the uptick of the

990
00:38:19,640 --> 00:38:27,350
feminist dataset is this used somewhere

991
00:38:21,920 --> 00:38:29,780
and how so it's not used yet and it's

992
00:38:27,350 --> 00:38:31,610
like a multi-year long process so right

993
00:38:29,780 --> 00:38:34,790
now I'm still in the process of of

994
00:38:31,610 --> 00:38:36,140
collecting the data but the next step is

995
00:38:34,790 --> 00:38:38,420
sort of looking at well what what would

996
00:38:36,140 --> 00:38:40,730
be the system for training that data how

997
00:38:38,420 --> 00:38:42,920
would you make a data model right now a

998
00:38:40,730 --> 00:38:44,150
very popular way to sort of train any

999
00:38:42,920 --> 00:38:46,280
kind of data model for machine learning

1000
00:38:44,150 --> 00:38:48,290
would be using Amazon's Mechanical Turk

1001
00:38:46,280 --> 00:38:50,300
but because the project is sort of

1002
00:38:48,290 --> 00:38:52,850
guided by an intersectional feminist

1003
00:38:50,300 --> 00:38:56,780
framework you know one has to ask is

1004
00:38:52,850 --> 00:38:58,190
Mechanical Turk ethical does the

1005
00:38:56,780 --> 00:39:00,440
existence of Mechanical Turk or the way

1006
00:38:58,190 --> 00:39:03,590
that that that workers are treated on

1007
00:39:00,440 --> 00:39:06,410
that platform a manifestation of like

1008
00:39:03,590 --> 00:39:08,150
feminist ideology and it's not so the

1009
00:39:06,410 --> 00:39:10,850
next step is trying to think about well

1010
00:39:08,150 --> 00:39:14,810
what what would be an ethical way to

1011
00:39:10,850 --> 00:39:17,240
train data with people is it inherently

1012
00:39:14,810 --> 00:39:19,820
like this system that Mechanical Turk is

1013
00:39:17,240 --> 00:39:21,890
designed or is it that Amazon has

1014
00:39:19,820 --> 00:39:23,870
optimized it towards payment towards

1015
00:39:21,890 --> 00:39:25,670
Amazon not payment towards the employee

1016
00:39:23,870 --> 00:39:27,319
so the next step of this project is

1017
00:39:25,670 --> 00:39:31,010
thinking about is that something that I

1018
00:39:27,320 --> 00:39:34,040
have to redesign or partner with like a

1019
00:39:31,010 --> 00:39:36,530
Groupon like should there be an open

1020
00:39:34,040 --> 00:39:39,710
source like small-scale version of

1021
00:39:36,530 --> 00:39:42,350
Mechanical Turk we have another question

1022
00:39:39,710 --> 00:39:44,150
from there yes thank you very for your

1023
00:39:42,350 --> 00:39:46,069
insightful talk I got another question

1024
00:39:44,150 --> 00:39:48,200
from all the things and projects you

1025
00:39:46,070 --> 00:39:50,600
work on you talked about design ethics

1026
00:39:48,200 --> 00:39:54,109
essentially as you mentioned where you

1027
00:39:50,600 --> 00:39:57,680
think how these are features where I

1028
00:39:54,110 --> 00:40:00,560
expect that a law body or accounting

1029
00:39:57,680 --> 00:40:02,950
body should we should enforce them or

1030
00:40:00,560 --> 00:40:05,120
did something you would like to see

1031
00:40:02,950 --> 00:40:08,870
alright are you asking about like

1032
00:40:05,120 --> 00:40:10,819
regulation maybe okay um I am always

1033
00:40:08,870 --> 00:40:12,950
hesitant this is probably because I am

1034
00:40:10,820 --> 00:40:17,270
an American to sort of stay where

1035
00:40:12,950 --> 00:40:19,490
regulations should exist and I feel like

1036
00:40:17,270 --> 00:40:23,330
when when we start to look at the ways

1037
00:40:19,490 --> 00:40:24,529
in which a lot of representatives of

1038
00:40:23,330 --> 00:40:25,319
different countries or different states

1039
00:40:24,530 --> 00:40:27,140
understand

1040
00:40:25,320 --> 00:40:29,820
there seems to be very much like a big

1041
00:40:27,140 --> 00:40:32,009
knowledge gap from like the way in which

1042
00:40:29,820 --> 00:40:33,660
perhaps a lawmaker understands working

1043
00:40:32,010 --> 00:40:36,000
can conceptualize of AI from like a

1044
00:40:33,660 --> 00:40:40,319
technologist and I think there are small

1045
00:40:36,000 --> 00:40:43,380
steps that can happen and I like the

1046
00:40:40,320 --> 00:40:45,300
idea of GD P R is a good one in practice

1047
00:40:43,380 --> 00:40:47,130
maybe it needs to be refined because a

1048
00:40:45,300 --> 00:40:51,660
lot of companies are figuring out ways

1049
00:40:47,130 --> 00:40:54,840
around like having people actually opt

1050
00:40:51,660 --> 00:40:56,790
out of tracking right so I think that

1051
00:40:54,840 --> 00:40:58,470
there is a space for regulation

1052
00:40:56,790 --> 00:41:01,259
I really would hesitate to say that

1053
00:40:58,470 --> 00:41:03,810
that's one we should take right now but

1054
00:41:01,260 --> 00:41:05,670
I do think that there needs to be like a

1055
00:41:03,810 --> 00:41:07,770
moratorium for example on facial

1056
00:41:05,670 --> 00:41:09,660
recognition software like that's one

1057
00:41:07,770 --> 00:41:11,610
thing where a lot of different

1058
00:41:09,660 --> 00:41:13,170
researchers and experts who look

1059
00:41:11,610 --> 00:41:15,600
specifically at facial recognition have

1060
00:41:13,170 --> 00:41:17,850
called for this moratorium even on like

1061
00:41:15,600 --> 00:41:19,200
the deployment or creation of algorithms

1062
00:41:17,850 --> 00:41:21,210
like let's not even try to make them

1063
00:41:19,200 --> 00:41:23,189
more accurate this is not necessarily

1064
00:41:21,210 --> 00:41:28,440
something that needs to exist so I think

1065
00:41:23,190 --> 00:41:30,180
there's a space between like regulation

1066
00:41:28,440 --> 00:41:31,170
and also experts coming together and

1067
00:41:30,180 --> 00:41:33,270
saying like this is actually something

1068
00:41:31,170 --> 00:41:40,620
we won't work on I'm not sure quite what

1069
00:41:33,270 --> 00:41:44,070
that sweet spot is though and all I also

1070
00:41:40,620 --> 00:41:47,130
really appreciate your work and research

1071
00:41:44,070 --> 00:41:51,180
I'm pretty sure it can be very

1072
00:41:47,130 --> 00:41:53,790
frustrating and complex but I am into

1073
00:41:51,180 --> 00:41:58,680
ethics and I wanted to ask about the

1074
00:41:53,790 --> 00:42:01,470
specific feminist datas design would you

1075
00:41:58,680 --> 00:42:05,940
then more opt for like splitting

1076
00:42:01,470 --> 00:42:07,230
platforms to create specific designs for

1077
00:42:05,940 --> 00:42:09,990
them because we know like you know

1078
00:42:07,230 --> 00:42:11,490
Facebook and Twitter and whatever they

1079
00:42:09,990 --> 00:42:13,890
include everybody

1080
00:42:11,490 --> 00:42:18,569
you know everybody's ethics and of

1081
00:42:13,890 --> 00:42:20,549
course then it's quite difficult to take

1082
00:42:18,570 --> 00:42:23,880
a car to get a common ground you know on

1083
00:42:20,550 --> 00:42:27,060
a global scale even like ethics depend

1084
00:42:23,880 --> 00:42:29,970
on countries cultures and all those

1085
00:42:27,060 --> 00:42:32,240
things so do your up more in a prayer

1086
00:42:29,970 --> 00:42:35,069
I'm looking at a practical you know

1087
00:42:32,240 --> 00:42:36,839
implementation are you then moral our

1088
00:42:35,070 --> 00:42:37,980
exploding community is splitting

1089
00:42:36,840 --> 00:42:41,550
platforms and

1090
00:42:37,980 --> 00:42:45,150
what you really think it's realistic to

1091
00:42:41,550 --> 00:42:46,230
implement that on a global platform are

1092
00:42:45,150 --> 00:42:47,550
you talking about

1093
00:42:46,230 --> 00:42:51,270
some of the different frameworks I've

1094
00:42:47,550 --> 00:42:53,730
suggested or like feminist data set yeah

1095
00:42:51,270 --> 00:42:57,450
I would just take feminist data set as

1096
00:42:53,730 --> 00:43:00,050
an template or example like do you think

1097
00:42:57,450 --> 00:43:04,350
it would work for all social media

1098
00:43:00,050 --> 00:43:07,530
platforms do you what is your your end

1099
00:43:04,350 --> 00:43:09,630
product or your ant vision so feminist

1100
00:43:07,530 --> 00:43:12,660
data set is an art project that's

1101
00:43:09,630 --> 00:43:17,100
pulling a lot of like English centered

1102
00:43:12,660 --> 00:43:18,420
data and language yeah I mean there are

1103
00:43:17,100 --> 00:43:21,630
some different kinds of data sets that

1104
00:43:18,420 --> 00:43:23,010
aren't like not every algorithm or

1105
00:43:21,630 --> 00:43:24,869
machine learning system is using every

1106
00:43:23,010 --> 00:43:27,450
data set in existence right yeah of

1107
00:43:24,869 --> 00:43:29,070
course yeah right or like they'll use a

1108
00:43:27,450 --> 00:43:30,210
part of a data set that can trainer and

1109
00:43:29,070 --> 00:43:32,100
form something and then they're pulling

1110
00:43:30,210 --> 00:43:33,570
from different kinds of data sets the

1111
00:43:32,100 --> 00:43:35,220
creation of Emma's data set is not to

1112
00:43:33,570 --> 00:43:36,619
say like everyone should use one data

1113
00:43:35,220 --> 00:43:40,410
set that also I think would be like very

1114
00:43:36,619 --> 00:43:42,720
irresponsible and not a solution but the

1115
00:43:40,410 --> 00:43:44,819
idea behind it is more how can we think

1116
00:43:42,720 --> 00:43:46,680
about community-created data sets and

1117
00:43:44,820 --> 00:43:48,660
then how can we look at like the

1118
00:43:46,680 --> 00:43:50,819
ideology of intersectional feminism as a

1119
00:43:48,660 --> 00:43:54,420
framework because it is a framework it

1120
00:43:50,820 --> 00:43:56,010
defines what is like what are politics

1121
00:43:54,420 --> 00:43:58,310
we can focus on or the idea of

1122
00:43:56,010 --> 00:44:01,500
sovereignty amongst like FEM feminine

1123
00:43:58,310 --> 00:44:03,779
gender binary and trans people right

1124
00:44:01,500 --> 00:44:06,030
it's sort of it's it's about that kind

1125
00:44:03,780 --> 00:44:07,980
of emphasis but you understand my point

1126
00:44:06,030 --> 00:44:10,050
of questioning it's a very specific

1127
00:44:07,980 --> 00:44:11,910
right but I'm responding to you and

1128
00:44:10,050 --> 00:44:14,520
saying that like I don't think any data

1129
00:44:11,910 --> 00:44:17,339
set one data set should be used globally

1130
00:44:14,520 --> 00:44:21,540
inside of one large system that's my

1131
00:44:17,340 --> 00:44:23,220
counterpoint yeah yeah okay thanks so we

1132
00:44:21,540 --> 00:44:26,970
have time for more questions do we have

1133
00:44:23,220 --> 00:44:28,890
another question okay though I think we

1134
00:44:26,970 --> 00:44:31,100
wrap this up here thanks for the great

1135
00:44:28,890 --> 00:44:31,319
speech Caroline cinders

1136
00:44:31,100 --> 00:44:34,669
[Music]

1137
00:44:31,320 --> 00:44:34,669
[Applause]

1138
00:44:38,150 --> 00:44:41,299
thank you

1139
00:44:50,309 --> 00:44:53,309
Hey

