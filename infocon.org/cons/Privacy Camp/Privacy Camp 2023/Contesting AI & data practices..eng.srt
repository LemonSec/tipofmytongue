1
00:00:03,380 --> 00:00:06,480
welcome everybody this room is now

2
00:00:06,480 --> 00:00:08,580
having the session on contesting Ai and

3
00:00:08,580 --> 00:00:10,920
data and data practices if you want to

4
00:00:10,920 --> 00:00:13,799
stay here please do take a seat uh

5
00:00:13,799 --> 00:00:16,260
slowly finish up your conversations for

6
00:00:16,260 --> 00:00:17,760
everyone who's still outside and can

7
00:00:17,760 --> 00:00:21,060
hear me please come in now thank you all

8
00:00:21,060 --> 00:00:21,900
right

9
00:00:21,900 --> 00:00:23,820
and then we can start this session and I

10
00:00:23,820 --> 00:00:26,160
hand over the mic

11
00:00:26,160 --> 00:00:28,859
thank you very much uh thank you for for

12
00:00:28,859 --> 00:00:31,920
being here with us today in this session

13
00:00:31,920 --> 00:00:34,260
we're going to speak about contesting Ai

14
00:00:34,260 --> 00:00:37,320
and data practices so

15
00:00:37,320 --> 00:00:39,360
um we want to to speak about very

16
00:00:39,360 --> 00:00:41,640
practical and effective approaches to

17
00:00:41,640 --> 00:00:44,219
responsible data practices what can we

18
00:00:44,219 --> 00:00:47,640
do to do this and looking at platforms

19
00:00:47,640 --> 00:00:50,579
like privacy camp or cpdp we see these

20
00:00:50,579 --> 00:00:52,739
are platforms that are not only driving

21
00:00:52,739 --> 00:00:55,020
the discourse on civil rights in the

22
00:00:55,020 --> 00:00:57,840
digital age they create the necessary

23
00:00:57,840 --> 00:01:00,420
awareness present Concepts and connect

24
00:01:00,420 --> 00:01:03,600
people this year's topics act now is

25
00:01:03,600 --> 00:01:05,280
something that really resonates with

26
00:01:05,280 --> 00:01:07,560
what we are doing here and the speakers

27
00:01:07,560 --> 00:01:10,380
that we have here today

28
00:01:10,380 --> 00:01:14,580
practically to act to do ethics as Willy

29
00:01:14,580 --> 00:01:17,100
would say or to implement something that

30
00:01:17,100 --> 00:01:20,340
would work we are here today with three

31
00:01:20,340 --> 00:01:21,720
speakers

32
00:01:21,720 --> 00:01:24,180
um with Iris Mouse from the retract data

33
00:01:24,180 --> 00:01:27,180
School whose data ethicist and once a

34
00:01:27,180 --> 00:01:29,280
team that implements actual data

35
00:01:29,280 --> 00:01:32,759
practices really tadema who's the chief

36
00:01:32,759 --> 00:01:36,180
data and IIs assist in the ministry for

37
00:01:36,180 --> 00:01:38,220
the interior and Kingdom relations in

38
00:01:38,220 --> 00:01:40,979
the Netherlands and yosten who is a

39
00:01:40,979 --> 00:01:44,280
privacy and AI lawyer in the Netherlands

40
00:01:44,280 --> 00:01:47,119
and has his own Law Firm

41
00:01:47,119 --> 00:01:50,159
my name is Sheffer and I'll be the

42
00:01:50,159 --> 00:01:52,200
moderator and trying to connect the

43
00:01:52,200 --> 00:01:54,360
talks to each other and connect to the

44
00:01:54,360 --> 00:01:58,259
audience for discussion and questions

45
00:01:58,259 --> 00:02:00,600
the session is built in a way that we

46
00:02:00,600 --> 00:02:03,119
will have three lightning Talks by each

47
00:02:03,119 --> 00:02:05,460
of our speakers where they present how

48
00:02:05,460 --> 00:02:08,699
they help organizations to tackle issues

49
00:02:08,699 --> 00:02:11,480
around responsible data and AI practices

50
00:02:11,480 --> 00:02:14,700
and what works and what is an obstacle

51
00:02:14,700 --> 00:02:17,760
they see in their day-to-day practice

52
00:02:17,760 --> 00:02:19,860
we will then have a discussion about

53
00:02:19,860 --> 00:02:22,860
that and include you the audience as

54
00:02:22,860 --> 00:02:25,620
much as possible we don't have any cups

55
00:02:25,620 --> 00:02:27,660
to give away or stickers to share with

56
00:02:27,660 --> 00:02:29,520
you so I hope that the intrinsic

57
00:02:29,520 --> 00:02:31,560
motivation will be sufficient to

58
00:02:31,560 --> 00:02:33,540
participate in this panel

59
00:02:33,540 --> 00:02:36,780
so without further Ado I give the floor

60
00:02:36,780 --> 00:02:39,120
to eres Morris who's going to speak from

61
00:02:39,120 --> 00:02:40,920
her perspective

62
00:02:40,920 --> 00:02:43,260
thank you so much and first of all I

63
00:02:43,260 --> 00:02:45,120
have a technical question could we have

64
00:02:45,120 --> 00:02:50,580
our slides up on the screen perhaps

65
00:02:50,700 --> 00:02:54,239
great thank you so much

66
00:02:54,239 --> 00:02:55,440
um

67
00:02:55,440 --> 00:03:00,120
so let me start off first my name is

68
00:03:00,120 --> 00:03:03,360
Iris Maize I work at utak University in

69
00:03:03,360 --> 00:03:06,599
the field of data ethics we work at

70
00:03:06,599 --> 00:03:09,660
utter data School Mirko and I some of

71
00:03:09,660 --> 00:03:11,340
our colleagues are also present here

72
00:03:11,340 --> 00:03:14,159
today and what we do is we research the

73
00:03:14,159 --> 00:03:18,060
impact of technology on society we do it

74
00:03:18,060 --> 00:03:20,519
across different domains but our main

75
00:03:20,519 --> 00:03:23,340
focus area is government governmental

76
00:03:23,340 --> 00:03:25,700
organizations in the Netherlands

77
00:03:25,700 --> 00:03:29,400
how we work is that we do a lot of

78
00:03:29,400 --> 00:03:32,239
publicly engaged research and teaching

79
00:03:32,239 --> 00:03:35,819
so that means that we work in the field

80
00:03:35,819 --> 00:03:38,640
we develop instruments that can be used

81
00:03:38,640 --> 00:03:41,700
by the sector and that we in turn can

82
00:03:41,700 --> 00:03:46,379
use to harvest research data

83
00:03:46,379 --> 00:03:48,360
um first a little bit about my

84
00:03:48,360 --> 00:03:50,700
background I studied law international

85
00:03:50,700 --> 00:03:54,420
relations and then I worked at a

86
00:03:54,420 --> 00:03:57,180
municipality city of Amsterdam I worked

87
00:03:57,180 --> 00:04:00,000
at an NGO in a digital education for

88
00:04:00,000 --> 00:04:03,599
children and now I'm working at uh data

89
00:04:03,599 --> 00:04:06,738
school as lead operations

90
00:04:06,900 --> 00:04:07,980
um

91
00:04:07,980 --> 00:04:11,819
okay so I'm a delving right into the

92
00:04:11,819 --> 00:04:13,220
Practical

93
00:04:13,220 --> 00:04:15,959
implementation of data ethics that's of

94
00:04:15,959 --> 00:04:18,600
course the focus of this panel we will

95
00:04:18,600 --> 00:04:21,120
share with you lots of practices from

96
00:04:21,120 --> 00:04:24,840
the Dutch context specifically

97
00:04:24,840 --> 00:04:27,120
um and first I would like to share with

98
00:04:27,120 --> 00:04:29,759
you two very practical instruments that

99
00:04:29,759 --> 00:04:32,160
we as uttar University have developed

100
00:04:32,160 --> 00:04:35,820
and that have been used for the past six

101
00:04:35,820 --> 00:04:38,180
seven years extensively by the

102
00:04:38,180 --> 00:04:41,400
governmental sector

103
00:04:41,400 --> 00:04:43,620
so a few of them are already on the

104
00:04:43,620 --> 00:04:47,220
screen actually I will dive into two of

105
00:04:47,220 --> 00:04:49,320
those which is the one on the left the

106
00:04:49,320 --> 00:04:51,720
fundamental rights and algorithms impact

107
00:04:51,720 --> 00:04:55,139
assessment and the run on the top right

108
00:04:55,139 --> 00:04:57,479
corner the data ethics decision AIDS

109
00:04:57,479 --> 00:05:00,419
which are dialogical tools for project

110
00:05:00,419 --> 00:05:04,259
teams of data projects to discuss ethics

111
00:05:04,259 --> 00:05:07,320
deliberate on ethical dilemmas make

112
00:05:07,320 --> 00:05:11,100
decisions and document those decisions

113
00:05:11,100 --> 00:05:13,979
okay so first of all uh the fundamental

114
00:05:13,979 --> 00:05:16,680
rights and algorithms impact assessment

115
00:05:16,680 --> 00:05:21,600
which can by the way be seen here so you

116
00:05:21,600 --> 00:05:24,240
can take a look for yourself it's an

117
00:05:24,240 --> 00:05:25,759
instrument

118
00:05:25,759 --> 00:05:28,979
aimed at government organizations that

119
00:05:28,979 --> 00:05:32,639
want to use data or digital technology

120
00:05:32,639 --> 00:05:36,259
for instance algorithms to predict

121
00:05:36,259 --> 00:05:40,100
social benefit fraud

122
00:05:40,100 --> 00:05:42,199
to monitor

123
00:05:42,199 --> 00:05:46,320
safety all kinds of algorithms and we

124
00:05:46,320 --> 00:05:50,100
really want to make sure that no human

125
00:05:50,100 --> 00:05:52,380
rights or fundamental rights are

126
00:05:52,380 --> 00:05:55,199
breached so we've been asked by the

127
00:05:55,199 --> 00:05:57,240
ministry for the interior to develop

128
00:05:57,240 --> 00:05:59,940
this impact assessments

129
00:05:59,940 --> 00:06:03,000
let me quickly go through the different

130
00:06:03,000 --> 00:06:04,979
steps of course you can take a look at

131
00:06:04,979 --> 00:06:07,139
the instrument yourself but just to get

132
00:06:07,139 --> 00:06:10,800
an idea first the first step is to

133
00:06:10,800 --> 00:06:13,919
answer the why question so why do we

134
00:06:13,919 --> 00:06:16,199
need this algorithm what kind of

135
00:06:16,199 --> 00:06:18,300
societal issue is this algorithm going

136
00:06:18,300 --> 00:06:19,639
to solve

137
00:06:19,639 --> 00:06:24,840
does it have a a legal ground can is it

138
00:06:24,840 --> 00:06:26,880
even allowed to use this kind of

139
00:06:26,880 --> 00:06:29,400
algorithm for this kind of problem

140
00:06:29,400 --> 00:06:32,840
then the second block is

141
00:06:32,840 --> 00:06:36,120
involving technology so what kind of

142
00:06:36,120 --> 00:06:38,699
Technology are we going to use is it a

143
00:06:38,699 --> 00:06:41,039
very complex algorithm does it involve

144
00:06:41,039 --> 00:06:45,180
training data what kind of input data

145
00:06:45,180 --> 00:06:48,419
are we feeding the algorithm the third

146
00:06:48,419 --> 00:06:51,300
question is context so how is it

147
00:06:51,300 --> 00:06:53,539
implemented because of a very

148
00:06:53,539 --> 00:06:56,340
technically sound algorithm can still

149
00:06:56,340 --> 00:06:59,160
cause harmful effects if it's

150
00:06:59,160 --> 00:07:01,740
implemented in the in the wrong ways or

151
00:07:01,740 --> 00:07:03,020
in a certain

152
00:07:03,020 --> 00:07:06,660
vulnerable context so it's about you

153
00:07:06,660 --> 00:07:09,660
know what what citizens are affected are

154
00:07:09,660 --> 00:07:12,259
those vulnerable groups

155
00:07:12,259 --> 00:07:15,180
is there a potential for discrimination

156
00:07:15,180 --> 00:07:18,600
or other harmful effects and the fourth

157
00:07:18,600 --> 00:07:20,940
and final part of the fundamental rights

158
00:07:20,940 --> 00:07:22,560
and algorithms impact assessments

159
00:07:22,560 --> 00:07:25,199
surrounds fundamental rights so it's

160
00:07:25,199 --> 00:07:27,180
it's really about the question you know

161
00:07:27,180 --> 00:07:30,660
what fundamental rights can be impact by

162
00:07:30,660 --> 00:07:33,120
this algorithm both negatively and

163
00:07:33,120 --> 00:07:36,060
positively and do the benefits of this

164
00:07:36,060 --> 00:07:39,479
algorithm way up to the potential harms

165
00:07:39,479 --> 00:07:42,840
on human rights it can cause so it

166
00:07:42,840 --> 00:07:46,139
really allows for you know teams data

167
00:07:46,139 --> 00:07:49,620
teams interdisciplinary teams to make

168
00:07:49,620 --> 00:07:53,460
these kinds of decisions

169
00:07:53,520 --> 00:07:54,240
um

170
00:07:54,240 --> 00:07:57,720
okay so that's the first one uh uh the

171
00:07:57,720 --> 00:08:01,139
second one I would like to introduce is

172
00:08:01,139 --> 00:08:04,380
the data ethics decision AIDS which has

173
00:08:04,380 --> 00:08:06,900
been around for I think six or seven

174
00:08:06,900 --> 00:08:10,979
years now uh it's also in use in in

175
00:08:10,979 --> 00:08:15,660
Germany Sweden uh Greece so it has more

176
00:08:15,660 --> 00:08:19,259
of a European use as well and this is

177
00:08:19,259 --> 00:08:21,680
also meant for

178
00:08:21,680 --> 00:08:24,960
dialogue facilitating dialogue in a

179
00:08:24,960 --> 00:08:27,720
project team so imagine

180
00:08:27,720 --> 00:08:28,740
um

181
00:08:28,740 --> 00:08:32,339
you know uh um someone with a technical

182
00:08:32,339 --> 00:08:34,260
background sitting at the table the

183
00:08:34,260 --> 00:08:37,320
Project Lead sitting at the table uh a

184
00:08:37,320 --> 00:08:40,919
legal expert or DPO having a place at

185
00:08:40,919 --> 00:08:43,080
the table and and talking about ethical

186
00:08:43,080 --> 00:08:46,380
dilemma surrounding data together

187
00:08:46,380 --> 00:08:48,779
um making decisions and documenting

188
00:08:48,779 --> 00:08:51,839
these decisions and all of this leads to

189
00:08:51,839 --> 00:08:55,760
you know accountability responsibility

190
00:08:55,760 --> 00:08:58,680
enabling us as Citizens but also the

191
00:08:58,680 --> 00:09:01,019
fourth estate the media to ask questions

192
00:09:01,019 --> 00:09:03,779
and to be able to get answers you know

193
00:09:03,779 --> 00:09:06,500
why have you chosen to do or implement

194
00:09:06,500 --> 00:09:10,019
this technology in a certain way so

195
00:09:10,019 --> 00:09:12,540
that's what we've been working with for

196
00:09:12,540 --> 00:09:15,740
the past six or seven years

197
00:09:15,740 --> 00:09:19,200
we've seen a lot about how data ethics

198
00:09:19,200 --> 00:09:20,720
Works in practice

199
00:09:20,720 --> 00:09:24,480
how organizations implement or try to

200
00:09:24,480 --> 00:09:27,300
implement data ethics so I look forward

201
00:09:27,300 --> 00:09:30,300
on to sharing some of these findings

202
00:09:30,300 --> 00:09:32,940
with you today and also to discuss these

203
00:09:32,940 --> 00:09:35,459
with my my fellow panelists

204
00:09:35,459 --> 00:09:37,260
thank you

205
00:09:37,260 --> 00:09:40,320
thank you Iris well that was in

206
00:09:40,320 --> 00:09:42,360
perspective from from Academia but you

207
00:09:42,360 --> 00:09:45,240
can see this was a do and research

208
00:09:45,240 --> 00:09:48,360
perspective because we do not only think

209
00:09:48,360 --> 00:09:50,399
of Concepts and drop them at the bottom

210
00:09:50,399 --> 00:09:52,080
of the Ivory Tower we are actually out

211
00:09:52,080 --> 00:09:54,480
in the field and and try to develop

212
00:09:54,480 --> 00:09:57,300
things that can be applied and we always

213
00:09:57,300 --> 00:10:00,360
do so for collecting insights so let's

214
00:10:00,360 --> 00:10:03,180
now turn to to Jose who will share his

215
00:10:03,180 --> 00:10:05,880
experience as a privacy and AI lawyer

216
00:10:05,880 --> 00:10:08,880
working with a wide range of companies

217
00:10:08,880 --> 00:10:12,060
but also public entities

218
00:10:12,060 --> 00:10:14,240
thank you Miko for the introduction

219
00:10:14,240 --> 00:10:17,760
hello everyone my name is Jose from The

220
00:10:17,760 --> 00:10:20,040
Firm legal Beetle

221
00:10:20,040 --> 00:10:22,940
um indeed I'm a lawyer since 2010 now

222
00:10:22,940 --> 00:10:26,339
specialized in privacy laws and solving

223
00:10:26,339 --> 00:10:28,920
legal issues surrounding emerging

224
00:10:28,920 --> 00:10:30,360
Technologies

225
00:10:30,360 --> 00:10:33,839
and in 2016 I started my own law firm in

226
00:10:33,839 --> 00:10:36,240
2016 of course it's the year when the

227
00:10:36,240 --> 00:10:38,660
gdpr was published

228
00:10:38,660 --> 00:10:42,959
and my daily practice consists mostly of

229
00:10:42,959 --> 00:10:45,480
advisory work but I also conduct

230
00:10:45,480 --> 00:10:50,459
research and in that respect I wrote

231
00:10:50,459 --> 00:10:52,500
several Publications regarding

232
00:10:52,500 --> 00:10:54,240
artificial intelligence

233
00:10:54,240 --> 00:10:55,680
I think you can see them on the screen

234
00:10:55,680 --> 00:10:57,720
one is called human rights in a robot

235
00:10:57,720 --> 00:11:00,500
age or better protected against

236
00:11:00,500 --> 00:11:03,540
biometric Technologies and another one

237
00:11:03,540 --> 00:11:06,120
is called tackling deep fakes in the

238
00:11:06,120 --> 00:11:08,579
European policy space

239
00:11:08,579 --> 00:11:11,820
I also co-founded the Dutch Association

240
00:11:11,820 --> 00:11:15,779
for AI lawyers that's something we my

241
00:11:15,779 --> 00:11:18,540
other founder invented that phrase AI

242
00:11:18,540 --> 00:11:22,019
lawyers and next to that in addition I

243
00:11:22,019 --> 00:11:24,540
maintain a newsletter and if you

244
00:11:24,540 --> 00:11:25,920
subscribe to the newsletter you can

245
00:11:25,920 --> 00:11:29,160
follow you can get updates on pending

246
00:11:29,160 --> 00:11:31,560
cases before the court of justice of the

247
00:11:31,560 --> 00:11:33,360
EU

248
00:11:33,360 --> 00:11:35,160
um I was talking about the the

249
00:11:35,160 --> 00:11:37,380
Publications and I think they all have

250
00:11:37,380 --> 00:11:39,620
one thing in common

251
00:11:39,620 --> 00:11:43,380
I think every publication and by the way

252
00:11:43,380 --> 00:11:45,480
I I write them with the router now

253
00:11:45,480 --> 00:11:48,660
Institute most of the time almost each

254
00:11:48,660 --> 00:11:50,940
publication ends with the notion that

255
00:11:50,940 --> 00:11:53,519
technology puts public values under

256
00:11:53,519 --> 00:11:54,480
pressure

257
00:11:54,480 --> 00:11:57,240
and we send those reports those

258
00:11:57,240 --> 00:12:00,480
Publications to uh for instance Dutch

259
00:12:00,480 --> 00:12:02,700
government or the European Parliament

260
00:12:02,700 --> 00:12:05,760
and the Council of Europe and we advise

261
00:12:05,760 --> 00:12:07,860
them on how to deal with that pressure

262
00:12:07,860 --> 00:12:10,980
and how to maintain the public values

263
00:12:10,980 --> 00:12:13,560
so my point uh for now for a couple of

264
00:12:13,560 --> 00:12:16,320
next minutes is the following I think

265
00:12:16,320 --> 00:12:19,320
that implementing data practices that

266
00:12:19,320 --> 00:12:22,440
preserve public values should be

267
00:12:22,440 --> 00:12:25,500
something profitable and I have a few

268
00:12:25,500 --> 00:12:28,620
ideas how that can be done

269
00:12:28,620 --> 00:12:31,140
and yeah America already said it I I

270
00:12:31,140 --> 00:12:34,680
advise companies and my focus will be on

271
00:12:34,680 --> 00:12:37,320
companies for the next minutes

272
00:12:37,320 --> 00:12:40,279
but let's address the question first

273
00:12:40,279 --> 00:12:42,899
what are companies because we all have

274
00:12:42,899 --> 00:12:45,779
ideas about it but let's look at facts

275
00:12:45,779 --> 00:12:47,639
um did you know for instance that in

276
00:12:47,639 --> 00:12:49,459
Europe

277
00:12:49,459 --> 00:12:52,740
98.9 are small companies so those are

278
00:12:52,740 --> 00:12:55,700
companies who employ less than 50 people

279
00:12:55,700 --> 00:13:00,060
and only 0.2 percent are large ones and

280
00:13:00,060 --> 00:13:02,839
they have more than 250 people

281
00:13:02,839 --> 00:13:04,860
working for them

282
00:13:04,860 --> 00:13:07,260
but then again those large companies

283
00:13:07,260 --> 00:13:09,600
they almost over 50 percent of the

284
00:13:09,600 --> 00:13:11,160
economic value so they are quite

285
00:13:11,160 --> 00:13:14,540
important to us you can think of

286
00:13:14,540 --> 00:13:17,040
asml which is a Dutch Semiconductor

287
00:13:17,040 --> 00:13:19,320
Company is very well known or a software

288
00:13:19,320 --> 00:13:22,500
company like sap also known as sub or

289
00:13:22,500 --> 00:13:25,380
agen which is a Dutch payment services

290
00:13:25,380 --> 00:13:27,660
providers they are really those are

291
00:13:27,660 --> 00:13:28,980
really big

292
00:13:28,980 --> 00:13:31,200
companies and well do they use

293
00:13:31,200 --> 00:13:34,380
artificial intelligence yes they do I

294
00:13:34,380 --> 00:13:37,740
think in 2019 our national statistics

295
00:13:37,740 --> 00:13:40,560
Institute and what I mean our I mean the

296
00:13:40,560 --> 00:13:41,660
Dutch one

297
00:13:41,660 --> 00:13:44,040
conducted research and discovered that

298
00:13:44,040 --> 00:13:46,620
more than half of the big companies use

299
00:13:46,620 --> 00:13:50,399
artificial intelligence and in practice

300
00:13:50,399 --> 00:13:52,800
that means they use machine learning or

301
00:13:52,800 --> 00:13:57,600
facial recognition or image recognition

302
00:13:57,600 --> 00:13:59,940
and yeah this is the background of

303
00:13:59,940 --> 00:14:03,480
companies a lot of small ones a few

304
00:14:03,480 --> 00:14:06,660
large ones but who make the headlines in

305
00:14:06,660 --> 00:14:08,820
the international press when public

306
00:14:08,820 --> 00:14:12,000
values are infringed well if you ask me

307
00:14:12,000 --> 00:14:14,940
that's mostly the abbreviation known as

308
00:14:14,940 --> 00:14:18,300
Gotham or Google Amazon Facebook apple

309
00:14:18,300 --> 00:14:20,940
and Microsoft and indeed if you look at

310
00:14:20,940 --> 00:14:24,200
the the top 10 of the biggest gdpr fines

311
00:14:24,200 --> 00:14:28,579
nine of them are a concerned government

312
00:14:28,579 --> 00:14:32,220
related companies in fact only three of

313
00:14:32,220 --> 00:14:35,519
them and those are Google Facebook and

314
00:14:35,519 --> 00:14:37,200
Amazon

315
00:14:37,200 --> 00:14:39,060
so we have to keep in mind that if we

316
00:14:39,060 --> 00:14:42,000
talk about companies those gavum

317
00:14:42,000 --> 00:14:44,579
companies are not a fair representation

318
00:14:44,579 --> 00:14:47,100
of the companies in Europe

319
00:14:47,100 --> 00:14:50,160
however all these companies being from

320
00:14:50,160 --> 00:14:52,320
America or Europe have one thing in

321
00:14:52,320 --> 00:14:53,160
common

322
00:14:53,160 --> 00:14:56,699
they are for profit and sometimes uh

323
00:14:56,699 --> 00:14:58,920
with a mission to add a value for

324
00:14:58,920 --> 00:15:00,360
shareholders

325
00:15:00,360 --> 00:15:03,480
so those are companies and I use the

326
00:15:03,480 --> 00:15:06,899
phrase public values a few times

327
00:15:06,899 --> 00:15:08,760
what are public values where you can

328
00:15:08,760 --> 00:15:10,440
answer that question I think from

329
00:15:10,440 --> 00:15:13,500
different perspectives uh I'm a lawyer

330
00:15:13,500 --> 00:15:16,260
so I have a legal perspective and when I

331
00:15:16,260 --> 00:15:18,959
think of public values I think of the

332
00:15:18,959 --> 00:15:23,360
the EU Charter of fundamental rights

333
00:15:23,360 --> 00:15:26,699
enshrined in the charter are uh yeah

334
00:15:26,699 --> 00:15:31,380
various values translated into rights we

335
00:15:31,380 --> 00:15:33,540
know privacy writes which is basically a

336
00:15:33,540 --> 00:15:36,000
combination of data protection rights

337
00:15:36,000 --> 00:15:39,139
and respect for private family right

338
00:15:39,139 --> 00:15:43,380
sorry live but there are of course a lot

339
00:15:43,380 --> 00:15:46,500
of other rights think of the rights to

340
00:15:46,500 --> 00:15:49,320
free expression or the right to be not

341
00:15:49,320 --> 00:15:52,500
to be discriminated against for instance

342
00:15:52,500 --> 00:15:55,079
and all those rights which you just told

343
00:15:55,079 --> 00:15:57,660
you about they have one thing in common

344
00:15:57,660 --> 00:16:00,180
they are not absolute they are relative

345
00:16:00,180 --> 00:16:03,680
rights so that means that uh you have to

346
00:16:03,680 --> 00:16:07,199
outweigh them against other rights and

347
00:16:07,199 --> 00:16:10,500
they can be over uh being overridden by

348
00:16:10,500 --> 00:16:14,220
other rights such as rice maintained by

349
00:16:14,220 --> 00:16:15,720
companies

350
00:16:15,720 --> 00:16:19,459
companies also have rights for instance

351
00:16:19,459 --> 00:16:22,620
the right to free the uh to to have the

352
00:16:22,620 --> 00:16:24,959
freedom to conduct a business uh the

353
00:16:24,959 --> 00:16:26,699
right to Peaceful enjoyment of

354
00:16:26,699 --> 00:16:29,399
possessions and overall there's an

355
00:16:29,399 --> 00:16:32,279
important interest namely the economic

356
00:16:32,279 --> 00:16:34,260
well-being of a country and all those

357
00:16:34,260 --> 00:16:36,839
rights in certain circumstances can

358
00:16:36,839 --> 00:16:40,759
outweigh privacy rights for instance

359
00:16:40,759 --> 00:16:44,279
in practice this means that companies uh

360
00:16:44,279 --> 00:16:47,459
have to before perform a balancing act

361
00:16:47,459 --> 00:16:52,680
with the rights and interests of you the

362
00:16:52,680 --> 00:16:55,380
the citizens people and they have to

363
00:16:55,380 --> 00:16:57,720
balance them against their own rights

364
00:16:57,720 --> 00:17:00,540
and it can be quite a hassle but of

365
00:17:00,540 --> 00:17:03,779
course thanks to instruments like data

366
00:17:03,779 --> 00:17:08,819
which Iris just described it can be done

367
00:17:08,819 --> 00:17:11,359
but the thing is that

368
00:17:11,359 --> 00:17:14,520
we're talking about data practices uh

369
00:17:14,520 --> 00:17:18,079
preserving public values such as data

370
00:17:18,079 --> 00:17:21,419
how can we make them attractive for

371
00:17:21,419 --> 00:17:23,220
those companies because yeah it's an

372
00:17:23,220 --> 00:17:25,260
instrument but a company is like I just

373
00:17:25,260 --> 00:17:28,199
said they are for profit so back to my

374
00:17:28,199 --> 00:17:31,080
points those data practices have to be

375
00:17:31,080 --> 00:17:33,299
profitable somehow and how can we make

376
00:17:33,299 --> 00:17:35,580
that an attractive option for them for

377
00:17:35,580 --> 00:17:37,919
the companies

378
00:17:37,919 --> 00:17:39,179
um I think

379
00:17:39,179 --> 00:17:42,480
of course there are a

380
00:17:42,480 --> 00:17:44,400
whoop

381
00:17:44,400 --> 00:17:47,640
ah I'm astonished by my own

382
00:17:47,640 --> 00:17:51,200
PowerPoint skills

383
00:17:51,360 --> 00:17:52,280
um

384
00:17:52,280 --> 00:17:55,020
of course there are two Simple Solutions

385
00:17:55,020 --> 00:17:58,020
if we talk about how to implement those

386
00:17:58,020 --> 00:17:59,360
data practices

387
00:17:59,360 --> 00:18:02,460
at companies one is actually a message

388
00:18:02,460 --> 00:18:05,100
addressed to the supervisory authorities

389
00:18:05,100 --> 00:18:09,440
enforce the law so if you enforce them

390
00:18:09,440 --> 00:18:12,120
when they breach the gdpr for instance

391
00:18:12,120 --> 00:18:14,900
you do it

392
00:18:14,900 --> 00:18:17,760
you do it more often for instance then

393
00:18:17,760 --> 00:18:20,760
it can help establishing a more equal

394
00:18:20,760 --> 00:18:24,020
Level Playing Field so to speak another

395
00:18:24,020 --> 00:18:26,700
solution would be addressed to the court

396
00:18:26,700 --> 00:18:29,220
of justice I think the court of justice

397
00:18:29,220 --> 00:18:33,720
should allow uh non-material damages uh

398
00:18:33,720 --> 00:18:36,120
to that you can claim non-material

399
00:18:36,120 --> 00:18:38,220
damages when the gdpr for instance is

400
00:18:38,220 --> 00:18:41,340
breached or the courts can also allow

401
00:18:41,340 --> 00:18:43,620
competitors to claim

402
00:18:43,620 --> 00:18:46,080
um to make claims against each other

403
00:18:46,080 --> 00:18:48,660
when the gdpr are breached and maybe you

404
00:18:48,660 --> 00:18:51,480
think this is not the case right now now

405
00:18:51,480 --> 00:18:54,480
if it comes to no material damages or a

406
00:18:54,480 --> 00:18:57,900
competitor it cannot go to the court and

407
00:18:57,900 --> 00:18:59,880
saying hey my my competitor is

408
00:18:59,880 --> 00:19:02,520
infringing the gdpr their judge do

409
00:19:02,520 --> 00:19:04,200
something about it it's right now it's

410
00:19:04,200 --> 00:19:07,020
not possible but in the near future it

411
00:19:07,020 --> 00:19:09,240
may change

412
00:19:09,240 --> 00:19:12,240
um yeah those are two in my few simple

413
00:19:12,240 --> 00:19:15,480
things maybe you heard them all before

414
00:19:15,480 --> 00:19:17,220
so let's dive into the more interesting

415
00:19:17,220 --> 00:19:21,720
stuff what can you do well that depends

416
00:19:21,720 --> 00:19:26,780
if you work at an NGO like adri

417
00:19:27,059 --> 00:19:29,580
um I think you have to start thinking

418
00:19:29,580 --> 00:19:31,679
about who are your enemies and who are

419
00:19:31,679 --> 00:19:35,400
your allies so uh I in from my

420
00:19:35,400 --> 00:19:38,100
experience only really a small portion

421
00:19:38,100 --> 00:19:41,940
of companies are highly dubious uh for

422
00:19:41,940 --> 00:19:45,000
instance Tick Tock or clear view they

423
00:19:45,000 --> 00:19:48,299
don't have our best interests in mind uh

424
00:19:48,299 --> 00:19:51,179
you should focus on on the majority uh

425
00:19:51,179 --> 00:19:54,660
seek allies with them because in my few

426
00:19:54,660 --> 00:19:57,780
they are well willing oh two minutes I

427
00:19:57,780 --> 00:20:00,000
can't speed a little bit up

428
00:20:00,000 --> 00:20:02,100
um yes they make mistakes the majority

429
00:20:02,100 --> 00:20:04,140
like accidental data leaks but they are

430
00:20:04,140 --> 00:20:05,640
not evil

431
00:20:05,640 --> 00:20:08,940
and are not faceless entities uh humans

432
00:20:08,940 --> 00:20:11,760
work there like you and me and they

433
00:20:11,760 --> 00:20:13,679
really want to do the right thing so

434
00:20:13,679 --> 00:20:15,600
what can you do you can befriend them

435
00:20:15,600 --> 00:20:18,240
like data protection officers or other

436
00:20:18,240 --> 00:20:20,700
influential people for a bottom-up

437
00:20:20,700 --> 00:20:21,799
approach

438
00:20:21,799 --> 00:20:24,480
convincing that their directors that

439
00:20:24,480 --> 00:20:26,760
public value preserving practices are

440
00:20:26,760 --> 00:20:28,679
the way to go

441
00:20:28,679 --> 00:20:30,780
so you want people within companies to

442
00:20:30,780 --> 00:20:32,640
challenge the High Reps are we doing

443
00:20:32,640 --> 00:20:34,020
this ethically

444
00:20:34,020 --> 00:20:37,140
but how can we make them the people in

445
00:20:37,140 --> 00:20:39,360
charge the director's list

446
00:20:39,360 --> 00:20:41,820
uh well in my view you should make

447
00:20:41,820 --> 00:20:43,919
public value preserving data practices

448
00:20:43,919 --> 00:20:47,220
uh a unique selling point a USP so

449
00:20:47,220 --> 00:20:49,380
companies enough to be inter industry

450
00:20:49,380 --> 00:20:51,780
leaders and ngos should help them with

451
00:20:51,780 --> 00:20:53,940
that and my idea is that you should as

452
00:20:53,940 --> 00:20:56,760
an NGO invite them to co-write a code of

453
00:20:56,760 --> 00:20:58,500
conduct and maybe you would think oh

454
00:20:58,500 --> 00:21:00,900
code of conduct self-regulatory it won't

455
00:21:00,900 --> 00:21:03,840
help it's it's too vague no in practice

456
00:21:03,840 --> 00:21:05,880
code of conducts are well respected by

457
00:21:05,880 --> 00:21:08,280
the courts if you are a company who

458
00:21:08,280 --> 00:21:10,500
adhere to such accounts and you infringe

459
00:21:10,500 --> 00:21:13,080
the code then the judge suspects that

460
00:21:13,080 --> 00:21:15,539
you act unlawful so it's quite a serious

461
00:21:15,539 --> 00:21:17,460
instrument and it's I think quite

462
00:21:17,460 --> 00:21:19,020
underrated

463
00:21:19,020 --> 00:21:21,900
such codes of conduct are encouraged by

464
00:21:21,900 --> 00:21:24,780
the gdpr and the AI act and I think they

465
00:21:24,780 --> 00:21:26,700
are a perfect opportunity to introduce

466
00:21:26,700 --> 00:21:29,700
ethical data practices my final sentence

467
00:21:29,700 --> 00:21:33,299
uh Miracle because okay now we have a

468
00:21:33,299 --> 00:21:35,280
code a code of conduct writing together

469
00:21:35,280 --> 00:21:37,620
with let's say a bunch of companies now

470
00:21:37,620 --> 00:21:40,020
what so here's my idea to put in the

471
00:21:40,020 --> 00:21:41,700
code of conduct at least one provision

472
00:21:41,700 --> 00:21:43,039
that says

473
00:21:43,039 --> 00:21:46,080
that companies only make their budget

474
00:21:46,080 --> 00:21:49,679
available to an AI product or a AI

475
00:21:49,679 --> 00:21:52,620
service once they successfully completed

476
00:21:52,620 --> 00:21:54,900
certain data practices that preserve

477
00:21:54,900 --> 00:21:57,900
public values for instance data in a

478
00:21:57,900 --> 00:21:59,880
company it's all about money and if a

479
00:21:59,880 --> 00:22:03,200
company if a project is not green lit

480
00:22:03,200 --> 00:22:06,419
because it doesn't pass successfully the

481
00:22:06,419 --> 00:22:09,120
data assessment yeah then there's no

482
00:22:09,120 --> 00:22:11,220
budget available and we are protected

483
00:22:11,220 --> 00:22:14,220
from potentially harmful AI I have a lot

484
00:22:14,220 --> 00:22:16,500
of more ideas but I'm out of time so I

485
00:22:16,500 --> 00:22:18,600
conclude with this thank you so much

486
00:22:18,600 --> 00:22:20,280
we'll pick your ideas up in the

487
00:22:20,280 --> 00:22:22,380
discussion and we like the idea of

488
00:22:22,380 --> 00:22:25,799
connecting data mandatory likewise to to

489
00:22:25,799 --> 00:22:29,100
codes of conducts now we turn to a

490
00:22:29,100 --> 00:22:30,780
perspective from the government sector

491
00:22:30,780 --> 00:22:32,820
really please

492
00:22:32,820 --> 00:22:36,139
yeah thank you that's

493
00:22:36,900 --> 00:22:38,640
[Music]

494
00:22:38,640 --> 00:22:40,799
but I have to do it's how you come

495
00:22:40,799 --> 00:22:43,380
across so it's just the question of my

496
00:22:43,380 --> 00:22:46,880
respect not yet at least so my name is

497
00:22:46,880 --> 00:22:51,299
I'm a data scientist and AI ethicist

498
00:22:51,299 --> 00:22:53,820
working for the Dutch Ministry of the

499
00:22:53,820 --> 00:22:54,960
interior

500
00:22:54,960 --> 00:22:59,220
my team is working with governments a

501
00:22:59,220 --> 00:23:01,440
governmental organization trying to

502
00:23:01,440 --> 00:23:04,020
realize the potential of data and I AI

503
00:23:04,020 --> 00:23:05,880
in a trustworthy way

504
00:23:05,880 --> 00:23:09,600
so accountable transparent fair safe and

505
00:23:09,600 --> 00:23:12,500
secure and without infringing

506
00:23:12,500 --> 00:23:15,240
privacy if it's not necessary and

507
00:23:15,240 --> 00:23:16,860
disproportionately

508
00:23:16,860 --> 00:23:19,440
so we are mainly working for Central

509
00:23:19,440 --> 00:23:22,260
governments on a national level like

510
00:23:22,260 --> 00:23:25,260
Ministries and the tax Authority and the

511
00:23:25,260 --> 00:23:26,700
Judiciary

512
00:23:26,700 --> 00:23:30,679
we provide numerous Services we assist

513
00:23:30,679 --> 00:23:32,720
governments in

514
00:23:32,720 --> 00:23:36,419
designing building testing deploying and

515
00:23:36,419 --> 00:23:39,840
using AI but we also conduct a lot of

516
00:23:39,840 --> 00:23:42,559
Assessments so

517
00:23:42,559 --> 00:23:44,820
fundamental rights impact assessments

518
00:23:44,820 --> 00:23:46,740
for instance bias assessments

519
00:23:46,740 --> 00:23:50,340
pre-ordered assessments and we also on a

520
00:23:50,340 --> 00:23:52,980
strategical level advice organization

521
00:23:52,980 --> 00:23:56,340
how to prepare for new legislation and

522
00:23:56,340 --> 00:23:59,539
policies like the AI act or the national

523
00:23:59,539 --> 00:24:02,820
algorithmic register

524
00:24:02,820 --> 00:24:05,880
so but I think the the main focus of our

525
00:24:05,880 --> 00:24:07,380
team is

526
00:24:07,380 --> 00:24:08,059
um

527
00:24:08,059 --> 00:24:10,740
putting ethics into practice

528
00:24:10,740 --> 00:24:15,380
so helping governments and thinking

529
00:24:15,380 --> 00:24:18,480
helping them

530
00:24:18,480 --> 00:24:22,260
um doing ethics what should I do so when

531
00:24:22,260 --> 00:24:27,000
I started this job almost five years ago

532
00:24:27,000 --> 00:24:27,720
um

533
00:24:27,720 --> 00:24:30,780
things were like we had a lot of trust

534
00:24:30,780 --> 00:24:34,620
in in data and AI it was almost like a

535
00:24:34,620 --> 00:24:37,860
holy grail and it was going to solve all

536
00:24:37,860 --> 00:24:40,380
our problems data science and AI was

537
00:24:40,380 --> 00:24:43,200
very much mystified like in this picture

538
00:24:43,200 --> 00:24:46,740
so governments were setting up data Labs

539
00:24:46,740 --> 00:24:49,080
everywhere they were recruiting data

540
00:24:49,080 --> 00:24:51,179
scientists they were experimenting and

541
00:24:51,179 --> 00:24:55,620
prototyping and also the the data

542
00:24:55,620 --> 00:24:57,419
scientists they were doing their work in

543
00:24:57,419 --> 00:25:00,000
isolation and people think we're

544
00:25:00,000 --> 00:25:02,520
thinking well we better leave those data

545
00:25:02,520 --> 00:25:04,679
scientists alone we give them the data

546
00:25:04,679 --> 00:25:07,200
and we give them the computing power and

547
00:25:07,200 --> 00:25:09,539
then they can perform their magic

548
00:25:09,539 --> 00:25:13,500
so um yeah to be honest of course there

549
00:25:13,500 --> 00:25:15,539
were data protection impact assessments

550
00:25:15,539 --> 00:25:17,340
being done because that was mandatory

551
00:25:17,340 --> 00:25:21,059
under the gdpr but on the whole not a

552
00:25:21,059 --> 00:25:23,340
lot of attention was paid to the illegal

553
00:25:23,340 --> 00:25:25,020
and ethical risks

554
00:25:25,020 --> 00:25:28,500
and also when you're not a lot of

555
00:25:28,500 --> 00:25:31,260
attention was paid to the to the the

556
00:25:31,260 --> 00:25:34,020
organizational and Technical aspects of

557
00:25:34,020 --> 00:25:37,559
of testing and deploying and using

558
00:25:37,559 --> 00:25:39,120
algorithms

559
00:25:39,120 --> 00:25:42,840
uh and also data data issues like data

560
00:25:42,840 --> 00:25:44,520
Access Data quality data

561
00:25:44,520 --> 00:25:47,039
representativeness well that that wasn't

562
00:25:47,039 --> 00:25:49,559
really a thing we talked a lot about

563
00:25:49,559 --> 00:25:53,580
so the data scientist was doing his work

564
00:25:53,580 --> 00:25:56,820
was optimizing for accuracy and not so

565
00:25:56,820 --> 00:25:59,700
much for fairness and transparency and

566
00:25:59,700 --> 00:26:04,380
also accountability no well because

567
00:26:04,380 --> 00:26:06,860
not really paid a lot of attention to

568
00:26:06,860 --> 00:26:10,200
and it's not I don't mean to bash those

569
00:26:10,200 --> 00:26:12,299
organizations because I think in

570
00:26:12,299 --> 00:26:16,080
hindsight it is what you could expect it

571
00:26:16,080 --> 00:26:18,360
was a new technology it was very much

572
00:26:18,360 --> 00:26:21,840
hyped and of course our um yeah the the

573
00:26:21,840 --> 00:26:24,360
organizations had a really low maturity

574
00:26:24,360 --> 00:26:27,179
level when we look at Ai and ethics so

575
00:26:27,179 --> 00:26:29,820
you have to keep that in mind

576
00:26:29,820 --> 00:26:32,760
but I have to be honest sometimes it

577
00:26:32,760 --> 00:26:36,299
really scared me what I was seeing

578
00:26:36,299 --> 00:26:38,220
but then things started to change

579
00:26:38,220 --> 00:26:40,320
because we had a number of scandals in

580
00:26:40,320 --> 00:26:41,520
the Netherlands

581
00:26:41,520 --> 00:26:44,940
we had for instance the judge ruling

582
00:26:44,940 --> 00:26:47,880
that the fraud uh the wealth fraud

583
00:26:47,880 --> 00:26:51,059
detection system Siri was unlawfully and

584
00:26:51,059 --> 00:26:52,799
of course we had a child care benefits

585
00:26:52,799 --> 00:26:53,760
scandal

586
00:26:53,760 --> 00:26:56,460
that was enormous it was was really

587
00:26:56,460 --> 00:26:57,539
awful

588
00:26:57,539 --> 00:26:58,799
so

589
00:26:58,799 --> 00:27:01,740
um it that was marking the the hand was

590
00:27:01,740 --> 00:27:03,840
marked in the beginning of a lot of new

591
00:27:03,840 --> 00:27:06,840
policies being drafted and uh new

592
00:27:06,840 --> 00:27:09,000
instruments new tools being developed

593
00:27:09,000 --> 00:27:11,220
for trustworthy AI

594
00:27:11,220 --> 00:27:13,200
and of course that is a really good

595
00:27:13,200 --> 00:27:14,520
thing

596
00:27:14,520 --> 00:27:17,100
so where are we now

597
00:27:17,100 --> 00:27:20,760
um okay well I think that

598
00:27:20,760 --> 00:27:21,900
um

599
00:27:21,900 --> 00:27:23,419
next cheat

600
00:27:23,419 --> 00:27:28,080
I think a lot of governments now are

601
00:27:28,080 --> 00:27:31,559
hesitant hesitant maybe reluctant to use

602
00:27:31,559 --> 00:27:32,820
AI

603
00:27:32,820 --> 00:27:36,559
I see a lot of projects being stopped

604
00:27:36,559 --> 00:27:39,960
prototypes not making it into production

605
00:27:39,960 --> 00:27:43,020
and all this without sound reasoning

606
00:27:43,020 --> 00:27:46,559
without good arguments and that is a bad

607
00:27:46,559 --> 00:27:48,659
thing because it means we are not

608
00:27:48,659 --> 00:27:50,580
learning we are not learning from our

609
00:27:50,580 --> 00:27:53,299
failures we are just wasting

610
00:27:53,299 --> 00:27:55,860
taxpayers money

611
00:27:55,860 --> 00:27:59,760
we are talking a lot more about ethics

612
00:27:59,760 --> 00:28:02,520
and that's also a very good thing but I

613
00:28:02,520 --> 00:28:05,039
think it's limited to ethical dilemmas

614
00:28:05,039 --> 00:28:07,380
so it's limited to the question should

615
00:28:07,380 --> 00:28:09,500
we be even building this algorithm

616
00:28:09,500 --> 00:28:13,080
algorithm what we have designed decided

617
00:28:13,080 --> 00:28:15,419
that we need an algorithm the the

618
00:28:15,419 --> 00:28:18,659
ethical discussions they just stop

619
00:28:18,659 --> 00:28:21,779
but we also have to talk about ethical

620
00:28:21,779 --> 00:28:23,220
decisions

621
00:28:23,220 --> 00:28:26,640
um and the the how do we translate uh

622
00:28:26,640 --> 00:28:29,580
public values into design requirements

623
00:28:29,580 --> 00:28:32,658
design decisions

624
00:28:32,820 --> 00:28:36,360
um we also are still focusing on the

625
00:28:36,360 --> 00:28:38,700
algorithm and not on the algorithmic

626
00:28:38,700 --> 00:28:40,260
system

627
00:28:40,260 --> 00:28:44,520
hey of course it's true a lot of buyers

628
00:28:44,520 --> 00:28:47,820
can stem from and the algorithm the

629
00:28:47,820 --> 00:28:51,000
algorithm and also the data but like we

630
00:28:51,000 --> 00:28:53,779
have seen in child care benefits scandal

631
00:28:53,779 --> 00:28:57,840
you can also create a do a lot of harm

632
00:28:57,840 --> 00:29:01,860
by using an algorithm in the wrong way

633
00:29:01,860 --> 00:29:04,440
so and as a consequence of zooming in on

634
00:29:04,440 --> 00:29:07,320
the algorithm the responsibility for

635
00:29:07,320 --> 00:29:11,159
trustworthy AI is still put on the

636
00:29:11,159 --> 00:29:15,000
shoulders of the technical team

637
00:29:15,000 --> 00:29:17,340
and this is also a really bad thing

638
00:29:17,340 --> 00:29:20,880
because ethics is is something it's for

639
00:29:20,880 --> 00:29:23,279
the whole organization not only for the

640
00:29:23,279 --> 00:29:25,279
engineers

641
00:29:25,279 --> 00:29:28,559
so I think on the whole what we are

642
00:29:28,559 --> 00:29:30,299
seeing is that

643
00:29:30,299 --> 00:29:32,940
um yeah government organizations are

644
00:29:32,940 --> 00:29:34,440
overwhelmed

645
00:29:34,440 --> 00:29:38,640
and they are they are very insecure they

646
00:29:38,640 --> 00:29:41,640
um they they need to take uh the lead

647
00:29:41,640 --> 00:29:45,480
they need to prioritize they need to to

648
00:29:45,480 --> 00:29:48,779
chart a course but they fail to do so so

649
00:29:48,779 --> 00:29:51,240
they look at external parties for

650
00:29:51,240 --> 00:29:53,659
guidance and that is a really bad thing

651
00:29:53,659 --> 00:29:57,720
because ethics should be ingrained in

652
00:29:57,720 --> 00:29:59,820
the organization it's not something that

653
00:29:59,820 --> 00:30:02,220
you can Outsource

654
00:30:02,220 --> 00:30:04,860
so what should we do to to

655
00:30:04,860 --> 00:30:05,580
um

656
00:30:05,580 --> 00:30:07,039
to to

657
00:30:07,039 --> 00:30:10,140
bridge the gap with the between theory

658
00:30:10,140 --> 00:30:11,460
and practice

659
00:30:11,460 --> 00:30:13,500
but well for one

660
00:30:13,500 --> 00:30:17,120
we should stop placing the the the

661
00:30:17,120 --> 00:30:20,700
burden of moral decision on the uh

662
00:30:20,700 --> 00:30:23,399
engineering teams on a tech technical

663
00:30:23,399 --> 00:30:24,960
teams

664
00:30:24,960 --> 00:30:27,539
and we should um when we look at

665
00:30:27,539 --> 00:30:30,299
trustworthy AI it's not only technical

666
00:30:30,299 --> 00:30:33,600
or robust but it's also legal ethical

667
00:30:33,600 --> 00:30:36,360
and organizational robust so we should

668
00:30:36,360 --> 00:30:40,020
bring in the lawyers the ethicists uh

669
00:30:40,020 --> 00:30:41,779
the system Engineers but also

670
00:30:41,779 --> 00:30:45,600
psychologists communication experts many

671
00:30:45,600 --> 00:30:48,899
more people and especially when we are

672
00:30:48,899 --> 00:30:51,840
looking at high stake algorithms we

673
00:30:51,840 --> 00:30:53,880
should have all stakeholders at the

674
00:30:53,880 --> 00:30:57,679
table right from the beginning so also

675
00:30:57,679 --> 00:31:01,380
people representing those groups within

676
00:31:01,380 --> 00:31:03,480
society that are impacted by the

677
00:31:03,480 --> 00:31:06,120
algorithm we should create a Level

678
00:31:06,120 --> 00:31:08,159
Playing Field so the information

679
00:31:08,159 --> 00:31:10,799
position of every stakeholder should be

680
00:31:10,799 --> 00:31:13,200
the same and we should pay attention to

681
00:31:13,200 --> 00:31:15,299
language develop a Common Language

682
00:31:15,299 --> 00:31:19,080
otherwise we can have and can't have a

683
00:31:19,080 --> 00:31:21,960
meaningful deliberation

684
00:31:21,960 --> 00:31:25,020
but I think well my most important point

685
00:31:25,020 --> 00:31:26,820
would be

686
00:31:26,820 --> 00:31:27,960
um

687
00:31:27,960 --> 00:31:32,279
if we we need to adopt a learning

688
00:31:32,279 --> 00:31:33,779
approach

689
00:31:33,779 --> 00:31:37,380
we need to bring the outside world in we

690
00:31:37,380 --> 00:31:40,640
have to invest in people and teams

691
00:31:40,640 --> 00:31:43,919
ethics is is not something that you can

692
00:31:43,919 --> 00:31:47,100
buy or Outsource ethics needs to be in

693
00:31:47,100 --> 00:31:50,399
the DNA of your organization meaning the

694
00:31:50,399 --> 00:31:51,299
people

695
00:31:51,299 --> 00:31:55,380
and also ethics is knowing the right

696
00:31:55,380 --> 00:31:57,539
thing to do in every situation that's

697
00:31:57,539 --> 00:31:59,340
not something that you can learn learn

698
00:31:59,340 --> 00:32:02,580
by reading a book or or looking at a

699
00:32:02,580 --> 00:32:06,179
policy or at a regulation it's ethics is

700
00:32:06,179 --> 00:32:08,220
something that you should do and you

701
00:32:08,220 --> 00:32:10,620
should practice it to get better at it

702
00:32:10,620 --> 00:32:12,779
and it's not ethics is not a

703
00:32:12,779 --> 00:32:16,559
one-size-fits all and we do not want uh

704
00:32:16,559 --> 00:32:18,779
government organizations that run

705
00:32:18,779 --> 00:32:20,760
through checklists and take all the

706
00:32:20,760 --> 00:32:22,980
boxes we want government organizations

707
00:32:22,980 --> 00:32:25,740
that are Guided by their by their by

708
00:32:25,740 --> 00:32:29,399
their values and um and that create and

709
00:32:29,399 --> 00:32:32,460
use algorithms accordingly so

710
00:32:32,460 --> 00:32:36,120
so I think as as a government we should

711
00:32:36,120 --> 00:32:39,240
lead by example we should adopt a

712
00:32:39,240 --> 00:32:40,580
learning approach

713
00:32:40,580 --> 00:32:43,559
get the outside world in

714
00:32:43,559 --> 00:32:46,799
and uh well I I it's my hope that we can

715
00:32:46,799 --> 00:32:50,039
do this with you with the with industry

716
00:32:50,039 --> 00:32:52,080
with Academia and of course with with

717
00:32:52,080 --> 00:32:55,380
the ngos so thank you really thank you

718
00:32:55,380 --> 00:33:00,740
this was a great um call to action

719
00:33:01,320 --> 00:33:04,080
with the night to the time I I

720
00:33:04,080 --> 00:33:06,679
um for that

721
00:33:06,679 --> 00:33:11,340
yes come on everybody fits also with an

722
00:33:11,340 --> 00:33:13,799
eye to the time we don't want to keep

723
00:33:13,799 --> 00:33:16,559
the audience also from engaging with the

724
00:33:16,559 --> 00:33:18,659
experts here on the podium so please

725
00:33:18,659 --> 00:33:21,419
feel free to to raise a hand that's a

726
00:33:21,419 --> 00:33:24,720
nice uh gentleman who is going to share

727
00:33:24,720 --> 00:33:26,580
a microphone

728
00:33:26,580 --> 00:33:27,659
um

729
00:33:27,659 --> 00:33:31,460
and there is already the first question

730
00:33:32,399 --> 00:33:33,720
thank you

731
00:33:33,720 --> 00:33:35,580
hello thank you very much for the

732
00:33:35,580 --> 00:33:38,279
organization I am a data scientist and

733
00:33:38,279 --> 00:33:41,580
also a master's student at k11 in data

734
00:33:41,580 --> 00:33:42,779
science program

735
00:33:42,779 --> 00:33:45,960
and in one of the courses which is price

736
00:33:45,960 --> 00:33:48,720
and big data we learned that we have a

737
00:33:48,720 --> 00:33:52,380
tool called Lindon developed by K Levin

738
00:33:52,380 --> 00:33:55,799
it is similar to Dida I think I at least

739
00:33:55,799 --> 00:33:58,260
I find it similar

740
00:33:58,260 --> 00:34:01,260
yeah I I think I think there will be

741
00:34:01,260 --> 00:34:04,679
some advantages and maybe some pros and

742
00:34:04,679 --> 00:34:07,740
cons of following one or the other tool

743
00:34:07,740 --> 00:34:11,879
framework for applying in practice I am

744
00:34:11,879 --> 00:34:15,540
wondering whether there is a project or

745
00:34:15,540 --> 00:34:20,040
at work to merge this kind of Frameworks

746
00:34:20,040 --> 00:34:23,760
in your EU because we have one low GT

747
00:34:23,760 --> 00:34:27,599
per gdpr which is the state of thought I

748
00:34:27,599 --> 00:34:29,699
believe in in the world maybe we can

749
00:34:29,699 --> 00:34:33,119
come up with a one framework maybe dark

750
00:34:33,119 --> 00:34:35,580
can be other obstacles I don't know but

751
00:34:35,580 --> 00:34:37,099
it may be a good

752
00:34:37,099 --> 00:34:41,339
project to Foster the and help the data

753
00:34:41,339 --> 00:34:43,679
scientists in this way what do you think

754
00:34:43,679 --> 00:34:46,020
about this

755
00:34:46,020 --> 00:34:48,020
um

756
00:34:50,239 --> 00:34:53,520
thank you so much

757
00:34:53,520 --> 00:34:56,099
so uh there are

758
00:34:56,099 --> 00:34:58,280
there's a plethora of

759
00:34:58,280 --> 00:35:02,220
Frameworks deliberation methods only in

760
00:35:02,220 --> 00:35:04,859
the Netherlands alone we have uh we have

761
00:35:04,859 --> 00:35:07,980
several I mean there's an NGO algorithm

762
00:35:07,980 --> 00:35:10,079
watch that collects all of these

763
00:35:10,079 --> 00:35:13,260
different manifestos and and methods and

764
00:35:13,260 --> 00:35:16,079
they are over 100 now you can see them

765
00:35:16,079 --> 00:35:18,359
that it's an endless list on their on

766
00:35:18,359 --> 00:35:21,960
their website so I understand the the

767
00:35:21,960 --> 00:35:25,380
confusion and uh you know where do you

768
00:35:25,380 --> 00:35:28,859
start which framework do you choose

769
00:35:28,859 --> 00:35:31,740
um it's it's very difficult and I

770
00:35:31,740 --> 00:35:34,680
present it to you two Frameworks we

771
00:35:34,680 --> 00:35:37,200
developed of course I'm very biased uh

772
00:35:37,200 --> 00:35:39,900
being the co-author of both of them

773
00:35:39,900 --> 00:35:43,140
um but um I think the core point of of

774
00:35:43,140 --> 00:35:46,320
all of these methods is an ethical

775
00:35:46,320 --> 00:35:48,300
deliberation fostering an ethical

776
00:35:48,300 --> 00:35:51,920
deliberation between stakeholders of

777
00:35:51,920 --> 00:35:54,200
interdisciplinary background

778
00:35:54,200 --> 00:35:57,420
and ingraining public values or

779
00:35:57,420 --> 00:35:59,700
organizational values translating them

780
00:35:59,700 --> 00:36:04,740
into your technology design so how you

781
00:36:04,740 --> 00:36:05,880
do that

782
00:36:05,880 --> 00:36:08,579
doesn't really matter which which method

783
00:36:08,579 --> 00:36:10,500
you choose doesn't really matter as long

784
00:36:10,500 --> 00:36:14,660
as you do it with stakeholders involved

785
00:36:14,660 --> 00:36:17,640
an interdisciplinary team I think that's

786
00:36:17,640 --> 00:36:21,359
that's very important to not just uh uh

787
00:36:21,359 --> 00:36:24,420
get blindsided or or miss or have blind

788
00:36:24,420 --> 00:36:26,040
spots

789
00:36:26,040 --> 00:36:28,560
um and also to Foster in a shared

790
00:36:28,560 --> 00:36:31,920
responsibility to not just focus on like

791
00:36:31,920 --> 00:36:34,079
Billy mentioned the the technical people

792
00:36:34,079 --> 00:36:36,780
as being responsible for data ethics or

793
00:36:36,780 --> 00:36:39,119
maybe the the project leader but to

794
00:36:39,119 --> 00:36:41,220
really share this responsibility within

795
00:36:41,220 --> 00:36:42,540
the organization

796
00:36:42,540 --> 00:36:44,460
and also

797
00:36:44,460 --> 00:36:45,119
um

798
00:36:45,119 --> 00:36:49,260
what we encounter a lot uh we've been to

799
00:36:49,260 --> 00:36:51,420
over a hundred institutions in the

800
00:36:51,420 --> 00:36:53,640
Netherlands to conduct or lead these

801
00:36:53,640 --> 00:36:56,640
kinds of ethical deliberations

802
00:36:56,640 --> 00:36:58,859
um every organization has different

803
00:36:58,859 --> 00:37:00,240
values

804
00:37:00,240 --> 00:37:05,420
so especially in in the case of

805
00:37:05,420 --> 00:37:08,099
government or organizations for instance

806
00:37:08,099 --> 00:37:10,339
like cities or municipalities

807
00:37:10,339 --> 00:37:14,160
every four years there are

808
00:37:14,160 --> 00:37:17,940
the politicians change so they may have

809
00:37:17,940 --> 00:37:21,880
uh in one term a very liberal

810
00:37:21,880 --> 00:37:23,359
[Music]

811
00:37:23,359 --> 00:37:27,359
head of the the government and they

812
00:37:27,359 --> 00:37:29,820
choose liberal algorithms but then

813
00:37:29,820 --> 00:37:32,280
there's elections maybe Democratic or

814
00:37:32,280 --> 00:37:35,700
you know left-wing uh party comes to

815
00:37:35,700 --> 00:37:38,880
power and you should really review all

816
00:37:38,880 --> 00:37:41,099
of these data practices again to make

817
00:37:41,099 --> 00:37:43,140
sure they're still in line with the the

818
00:37:43,140 --> 00:37:45,140
values so

819
00:37:45,140 --> 00:37:48,480
whichever method you you choose the the

820
00:37:48,480 --> 00:37:50,579
translation of these public values is

821
00:37:50,579 --> 00:37:52,619
very important and of course there is

822
00:37:52,619 --> 00:37:55,339
some harmonization of course we have the

823
00:37:55,339 --> 00:37:58,640
data protection impact Assessments

824
00:37:58,640 --> 00:38:02,460
in the AI act some sort of Human Rights

825
00:38:02,460 --> 00:38:05,180
impact assessment is is probably being

826
00:38:05,180 --> 00:38:09,180
installed and

827
00:38:09,599 --> 00:38:12,540
it would be good to have one harmonized

828
00:38:12,540 --> 00:38:14,700
human rights impact assessment but still

829
00:38:14,700 --> 00:38:17,520
it needs to leave some room for

830
00:38:17,520 --> 00:38:21,300
organizational values to be ingrained

831
00:38:21,300 --> 00:38:23,700
in that assessment

832
00:38:23,700 --> 00:38:25,680
thank you thank you and you said you're

833
00:38:25,680 --> 00:38:29,000
studying applied data science

834
00:38:29,520 --> 00:38:32,940
yes I'm managing that I applied data

835
00:38:32,940 --> 00:38:34,560
science at UTech University and your

836
00:38:34,560 --> 00:38:37,320
questions are awfully familiar I get

837
00:38:37,320 --> 00:38:39,540
that very often from the students so I

838
00:38:39,540 --> 00:38:42,180
think what what you will recognize very

839
00:38:42,180 --> 00:38:44,520
often is that these ethical questions

840
00:38:44,520 --> 00:38:47,040
are delegated to you

841
00:38:47,040 --> 00:38:48,480
um but I think that data scientists

842
00:38:48,480 --> 00:38:50,760
should be should recognize them and

843
00:38:50,760 --> 00:38:52,500
assign responsibility and give it back

844
00:38:52,500 --> 00:38:55,020
to the organization and take them on the

845
00:38:55,020 --> 00:38:57,240
table and say you know it's not only the

846
00:38:57,240 --> 00:38:59,700
law that regulates this as Iris just

847
00:38:59,700 --> 00:39:01,680
pointed out it's also the values that

848
00:39:01,680 --> 00:39:03,480
might differ from one municipality to

849
00:39:03,480 --> 00:39:06,000
the other and the data scientists should

850
00:39:06,000 --> 00:39:07,740
shouldn't accept to be left alone with

851
00:39:07,740 --> 00:39:09,300
this

852
00:39:09,300 --> 00:39:11,460
yeah maybe

853
00:39:11,460 --> 00:39:13,560
I can add something to that because I'm

854
00:39:13,560 --> 00:39:16,260
also a data scientist so I I totally

855
00:39:16,260 --> 00:39:19,920
understand the position that you are in

856
00:39:19,920 --> 00:39:22,680
um and and you were saying like um it's

857
00:39:22,680 --> 00:39:25,920
important uh to to recognize the values

858
00:39:25,920 --> 00:39:27,900
and and uh for instance when we are

859
00:39:27,900 --> 00:39:29,460
looking at equality and

860
00:39:29,460 --> 00:39:31,680
non-discrimination of course and we want

861
00:39:31,680 --> 00:39:34,920
the the algorithm to be fair and we

862
00:39:34,920 --> 00:39:37,260
should talk about that but in my

863
00:39:37,260 --> 00:39:39,300
experience okay so we're saying the

864
00:39:39,300 --> 00:39:41,280
algorithms should be fair you are the

865
00:39:41,280 --> 00:39:44,280
data scientist you check that and it's

866
00:39:44,280 --> 00:39:46,740
not not something that I was prepared

867
00:39:46,740 --> 00:39:48,859
for at University

868
00:39:48,859 --> 00:39:51,900
so we were taught how to train models

869
00:39:51,900 --> 00:39:54,960
but not how to translate fairness into a

870
00:39:54,960 --> 00:39:57,300
metric that we can optimize our models

871
00:39:57,300 --> 00:39:58,079
on

872
00:39:58,079 --> 00:40:00,900
so I think this is also something that

873
00:40:00,900 --> 00:40:04,820
lacks in in our training and education

874
00:40:04,820 --> 00:40:07,680
so and and luckily we're seeing in the

875
00:40:07,680 --> 00:40:08,820
Netherlands there are a lot of

876
00:40:08,820 --> 00:40:11,160
universities and and um

877
00:40:11,160 --> 00:40:13,160
are our

878
00:40:13,160 --> 00:40:16,680
adding this to their curriculum so but I

879
00:40:16,680 --> 00:40:20,419
think that's also a very important point

880
00:40:21,300 --> 00:40:22,619
yeah

881
00:40:22,619 --> 00:40:24,000
um me

882
00:40:24,000 --> 00:40:27,060
yes hello thank you very much my name is

883
00:40:27,060 --> 00:40:29,579
Javier Seitz I work for the theme

884
00:40:29,579 --> 00:40:34,079
secretary is an NGO I work in against

885
00:40:34,079 --> 00:40:37,079
discrimination against Roma community in

886
00:40:37,079 --> 00:40:40,859
Spain and I would like to ask a problem

887
00:40:40,859 --> 00:40:44,760
to Mrs Stadium if or maybe on the other

888
00:40:44,760 --> 00:40:47,520
colleagues if you have experienced on

889
00:40:47,520 --> 00:40:48,980
how to prevent

890
00:40:48,980 --> 00:40:53,280
bias on algorithms racial bias because I

891
00:40:53,280 --> 00:40:55,880
remember a case in the Netherlands was

892
00:40:55,880 --> 00:41:00,119
in royalmond and when the predicting

893
00:41:00,119 --> 00:41:03,140
police finally criminalized Roma

894
00:41:03,140 --> 00:41:06,119
communities from our people but we have

895
00:41:06,119 --> 00:41:08,220
also examples in Spain and we are trying

896
00:41:08,220 --> 00:41:11,220
to work with authorities uh on how to

897
00:41:11,220 --> 00:41:12,260
prevent

898
00:41:12,260 --> 00:41:14,880
artificial intelligence or algorithms

899
00:41:14,880 --> 00:41:19,140
regarding racial discrimination but the

900
00:41:19,140 --> 00:41:21,839
question is do you have the protocols or

901
00:41:21,839 --> 00:41:24,720
ideas about how to prevent that and

902
00:41:24,720 --> 00:41:26,880
secondly if you think it's interesting

903
00:41:26,880 --> 00:41:30,420
to get on board the communities that can

904
00:41:30,420 --> 00:41:34,380
be targeted or affected racial or other

905
00:41:34,380 --> 00:41:37,740
minorities Within These procedures thank

906
00:41:37,740 --> 00:41:39,119
you

907
00:41:39,119 --> 00:41:41,579
yeah it's a it's a very good question I

908
00:41:41,579 --> 00:41:42,720
think

909
00:41:42,720 --> 00:41:44,280
um well

910
00:41:44,280 --> 00:41:46,200
um for instance yeah when you are

911
00:41:46,200 --> 00:41:50,099
looking at uh discrimination it's it's

912
00:41:50,099 --> 00:41:53,220
um we don't have the the awareness is

913
00:41:53,220 --> 00:41:55,859
there but the best practices and the

914
00:41:55,859 --> 00:41:58,079
standards they're not there so we have

915
00:41:58,079 --> 00:42:01,079
to mature on that level and it's also

916
00:42:01,079 --> 00:42:03,720
it's it's a really difficult topic

917
00:42:03,720 --> 00:42:05,339
because when you are talking about

918
00:42:05,339 --> 00:42:08,339
discrimination depending on whether you

919
00:42:08,339 --> 00:42:12,119
are an engineer or an ethicist or a

920
00:42:12,119 --> 00:42:15,780
lawyer or just a worried citizen you may

921
00:42:15,780 --> 00:42:18,000
assign that a very different meaning

922
00:42:18,000 --> 00:42:20,940
discrimination I think within law hey

923
00:42:20,940 --> 00:42:23,220
it's really good defined what is

924
00:42:23,220 --> 00:42:25,800
discrimination and what is not I can add

925
00:42:25,800 --> 00:42:27,240
something okay well maybe it could

926
00:42:27,240 --> 00:42:30,119
reflect on that but

927
00:42:30,119 --> 00:42:32,280
um it's yeah

928
00:42:32,280 --> 00:42:33,000
um

929
00:42:33,000 --> 00:42:37,140
it's really tough for a data scientist I

930
00:42:37,140 --> 00:42:41,099
had to to say I I get asked quite a lot

931
00:42:41,099 --> 00:42:42,900
uh hey can you can you check whether

932
00:42:42,900 --> 00:42:45,720
this algorithm is discriminating and

933
00:42:45,720 --> 00:42:47,700
it's a very very difficult question and

934
00:42:47,700 --> 00:42:49,500
it's not a question that I should answer

935
00:42:49,500 --> 00:42:53,160
I can provide you with information

936
00:42:53,160 --> 00:42:55,859
about the bias the bias in the data the

937
00:42:55,859 --> 00:42:58,260
Bison algorithm maybe we could even

938
00:42:58,260 --> 00:43:00,720
monitor the bias in the usage of the

939
00:43:00,720 --> 00:43:04,079
algorithm but whether the the algorithm

940
00:43:04,079 --> 00:43:06,560
is is racist whether it is

941
00:43:06,560 --> 00:43:09,660
discriminatory is not something that a

942
00:43:09,660 --> 00:43:11,819
data scientist can decide and it's it's

943
00:43:11,819 --> 00:43:13,680
something that we should do I think

944
00:43:13,680 --> 00:43:18,380
together and we definitely need to

945
00:43:18,380 --> 00:43:21,540
develop best practices and standards and

946
00:43:21,540 --> 00:43:23,940
I don't think they are here yet so this

947
00:43:23,940 --> 00:43:26,040
this is a topic that we really want to

948
00:43:26,040 --> 00:43:31,200
work together with Academia and ngos

949
00:43:31,200 --> 00:43:34,680
yeah no for me it's not easy at all or

950
00:43:34,680 --> 00:43:37,800
not black and white she suggests no I

951
00:43:37,800 --> 00:43:40,200
think that's not my suggestion oh sorry

952
00:43:40,200 --> 00:43:43,680
no no no no no okay oh well I think like

953
00:43:43,680 --> 00:43:45,720
whether

954
00:43:45,720 --> 00:43:48,720
um people ask me whether the algorithm

955
00:43:48,720 --> 00:43:51,900
is discriminatory but uh I think when

956
00:43:51,900 --> 00:43:54,119
you look at non-discrimination law it is

957
00:43:54,119 --> 00:43:56,700
contextual it is depending on a

958
00:43:56,700 --> 00:43:58,500
circumstance oh yeah okay whether it is

959
00:43:58,500 --> 00:44:00,119
discriminatory so it's not something

960
00:44:00,119 --> 00:44:01,920
that you can measure but not something

961
00:44:01,920 --> 00:44:04,260
that you can automate so it's really

962
00:44:04,260 --> 00:44:06,060
important to be aware of that so you

963
00:44:06,060 --> 00:44:09,000
can't defer that decision that

964
00:44:09,000 --> 00:44:11,880
assessment to a detectives a technical

965
00:44:11,880 --> 00:44:13,740
person oh yeah but yeah then I totally

966
00:44:13,740 --> 00:44:15,540
agree with you sorry I just

967
00:44:15,540 --> 00:44:17,819
misunderstood now I think that the

968
00:44:17,819 --> 00:44:19,920
non-discriminate non-discrimination law

969
00:44:19,920 --> 00:44:22,619
is probably the hardest legal field uh

970
00:44:22,619 --> 00:44:25,859
uh to be active in so last year I was

971
00:44:25,859 --> 00:44:28,079
invited to write an advice paper small

972
00:44:28,079 --> 00:44:30,140
paper to the Dutch Parliament about

973
00:44:30,140 --> 00:44:34,079
non-discrimination in a AI context and

974
00:44:34,079 --> 00:44:36,780
uh boy I almost fell off my chair I mean

975
00:44:36,780 --> 00:44:40,020
there's not there's not only a lot of

976
00:44:40,020 --> 00:44:42,119
case law uh but the case law is also

977
00:44:42,119 --> 00:44:43,339
about

978
00:44:43,339 --> 00:44:46,020
non-ai situations but you have to

979
00:44:46,020 --> 00:44:50,280
somehow apply that to a modern situation

980
00:44:50,280 --> 00:44:52,260
um and what I also think what's really

981
00:44:52,260 --> 00:44:55,020
hard is that um when we talk to each

982
00:44:55,020 --> 00:44:57,119
other and what someone says I I am

983
00:44:57,119 --> 00:45:00,000
discriminated most of the time you think

984
00:45:00,000 --> 00:45:01,920
yeah that's illegal that shouldn't be

985
00:45:01,920 --> 00:45:05,160
done but from a legal perspective uh

986
00:45:05,160 --> 00:45:07,200
discrimination like I said in my Opening

987
00:45:07,200 --> 00:45:09,960
space each it's it's a relative right so

988
00:45:09,960 --> 00:45:12,839
yes some forms of discrimination are

989
00:45:12,839 --> 00:45:15,599
always forbidden but are also forms that

990
00:45:15,599 --> 00:45:17,579
can be justified and that makes it

991
00:45:17,579 --> 00:45:19,800
really difficult that sometimes you feel

992
00:45:19,800 --> 00:45:21,480
this shouldn't be done this is

993
00:45:21,480 --> 00:45:23,220
discriminatory we should get rid of it

994
00:45:23,220 --> 00:45:26,099
but if you uh ask a judge the judge will

995
00:45:26,099 --> 00:45:28,319
say yeah well okay but in this situation

996
00:45:28,319 --> 00:45:31,500
it can be done so um yeah part of the

997
00:45:31,500 --> 00:45:34,319
advice paper I dive uh dove in the um

998
00:45:34,319 --> 00:45:36,720
also in the academic work maybe you know

999
00:45:36,720 --> 00:45:39,180
the name Sandra wachter she's active I

1000
00:45:39,180 --> 00:45:41,400
think Oxford University she did a lot of

1001
00:45:41,400 --> 00:45:44,700
academic papers and if I understand her

1002
00:45:44,700 --> 00:45:46,740
correctly she says yeah well you cannot

1003
00:45:46,740 --> 00:45:49,920
eradicate uh bias in an algorithm it

1004
00:45:49,920 --> 00:45:52,859
simply cannot be done as she has several

1005
00:45:52,859 --> 00:45:54,839
Arguments for that

1006
00:45:54,839 --> 00:45:58,319
um but uh that that message we try to

1007
00:45:58,319 --> 00:45:59,880
convey to the Dutch Parliament as well

1008
00:45:59,880 --> 00:46:02,280
because the parliament said I think it

1009
00:46:02,280 --> 00:46:04,560
was even our prime minister he said I

1010
00:46:04,560 --> 00:46:06,480
want zero discrimination in our

1011
00:46:06,480 --> 00:46:08,760
algorithms I want once I don't want

1012
00:46:08,760 --> 00:46:11,160
anything but we have to say to him sorry

1013
00:46:11,160 --> 00:46:14,579
Mr haruta uh yeah of course not

1014
00:46:14,579 --> 00:46:17,760
literally but we said we that's

1015
00:46:17,760 --> 00:46:20,700
impossible uh thanks to Sandra we know

1016
00:46:20,700 --> 00:46:23,760
this and uh for other reasons so our

1017
00:46:23,760 --> 00:46:26,160
question to the politicians were what

1018
00:46:26,160 --> 00:46:28,140
level of discrimination are you willing

1019
00:46:28,140 --> 00:46:30,780
to accept and that's a real not a nice

1020
00:46:30,780 --> 00:46:32,460
discussion to have but I think a really

1021
00:46:32,460 --> 00:46:34,800
important discussion because from my

1022
00:46:34,800 --> 00:46:37,200
legal perspective

1023
00:46:37,200 --> 00:46:40,319
um it's really hard to prevent harmful

1024
00:46:40,319 --> 00:46:43,920
Ai and yeah it's like I said it's it's

1025
00:46:43,920 --> 00:46:45,839
difficult most difficult legal field

1026
00:46:45,839 --> 00:46:47,880
that I know of

1027
00:46:47,880 --> 00:46:52,280
if I met please go ahead

1028
00:46:59,819 --> 00:47:02,400
and I would like to add something on

1029
00:47:02,400 --> 00:47:03,420
um

1030
00:47:03,420 --> 00:47:07,079
so in the Netherlands you you see a very

1031
00:47:07,079 --> 00:47:10,560
clear shift where at first politicians

1032
00:47:10,560 --> 00:47:13,020
were like okay we

1033
00:47:13,020 --> 00:47:15,060
I'm not a technical person I don't know

1034
00:47:15,060 --> 00:47:16,920
anything about technology so please

1035
00:47:16,920 --> 00:47:20,339
leave the decisions regarding technology

1036
00:47:20,339 --> 00:47:23,520
to the data scientists

1037
00:47:23,520 --> 00:47:26,400
um we don't want to do anything uh

1038
00:47:26,400 --> 00:47:28,800
we don't even have a standpoint on

1039
00:47:28,800 --> 00:47:31,500
technology so this has really changed

1040
00:47:31,500 --> 00:47:35,160
also due to the the Scandal um uh really

1041
00:47:35,160 --> 00:47:38,339
mentioned earlier uh it is really awoken

1042
00:47:38,339 --> 00:47:39,900
politicians

1043
00:47:39,900 --> 00:47:42,540
um to form a standpoint political

1044
00:47:42,540 --> 00:47:44,700
standpoint on the use of technology and

1045
00:47:44,700 --> 00:47:46,220
data ethics

1046
00:47:46,220 --> 00:47:50,339
but the way they are getting involved is

1047
00:47:50,339 --> 00:47:54,599
sometimes not very productive because

1048
00:47:54,599 --> 00:47:57,480
they might lack the the knowledge to

1049
00:47:57,480 --> 00:48:00,000
provide at a good control over

1050
00:48:00,000 --> 00:48:01,339
technology

1051
00:48:01,339 --> 00:48:04,500
and that's also where Education and

1052
00:48:04,500 --> 00:48:07,859
Training comes in again to provide these

1053
00:48:07,859 --> 00:48:09,300
politicians with the right information

1054
00:48:09,300 --> 00:48:12,740
to adequately control and steer

1055
00:48:12,740 --> 00:48:15,540
technological developments within their

1056
00:48:15,540 --> 00:48:17,660
their organization

1057
00:48:17,660 --> 00:48:22,380
so that's also a development we see and

1058
00:48:22,380 --> 00:48:27,359
and we see getting stronger so I I

1059
00:48:27,359 --> 00:48:28,940
highly suggest

1060
00:48:28,940 --> 00:48:32,099
stimulating your local politician even

1061
00:48:32,099 --> 00:48:34,500
to get involved in informing a

1062
00:48:34,500 --> 00:48:37,020
standpoint on digital technology

1063
00:48:37,020 --> 00:48:38,940
thank you there was another question

1064
00:48:38,940 --> 00:48:41,940
there hi I'm Judith and I'm conducting

1065
00:48:41,940 --> 00:48:44,400
an action research on menstruation

1066
00:48:44,400 --> 00:48:46,619
tracking and the role of Technologies

1067
00:48:46,619 --> 00:48:49,319
and so I'm interviewing a lot of people

1068
00:48:49,319 --> 00:48:51,599
who use apps for instance to track their

1069
00:48:51,599 --> 00:48:54,119
Administration and I'm also looking with

1070
00:48:54,119 --> 00:48:56,460
them how they if the app Zone exactly

1071
00:48:56,460 --> 00:48:59,040
work like they want them to work how

1072
00:48:59,040 --> 00:49:00,900
they can contact the companies Behind

1073
00:49:00,900 --> 00:49:03,960
these apps and tell them hey can you

1074
00:49:03,960 --> 00:49:06,599
change this maybe for instance one

1075
00:49:06,599 --> 00:49:08,099
person that I interviewed

1076
00:49:08,099 --> 00:49:10,440
keeps receiving this pop-up where it

1077
00:49:10,440 --> 00:49:12,660
says do you want to conceive and she

1078
00:49:12,660 --> 00:49:13,980
really doesn't and she has very

1079
00:49:13,980 --> 00:49:16,980
sensitive reasons why she doesn't so she

1080
00:49:16,980 --> 00:49:18,960
really would like that this this pop-up

1081
00:49:18,960 --> 00:49:21,240
to be to disappear which is a really so

1082
00:49:21,240 --> 00:49:23,160
I have a really practical question how

1083
00:49:23,160 --> 00:49:26,280
can we Empower users to successfully

1084
00:49:26,280 --> 00:49:28,680
have some sort of meaningful contact

1085
00:49:28,680 --> 00:49:31,140
with companies because now we together

1086
00:49:31,140 --> 00:49:33,960
send an email to this company and we got

1087
00:49:33,960 --> 00:49:36,660
an automatic reply which was like we're

1088
00:49:36,660 --> 00:49:39,359
working on our services right now please

1089
00:49:39,359 --> 00:49:42,000
rate US on the app you know so that's

1090
00:49:42,000 --> 00:49:45,240
not very helpful for users and yeah so

1091
00:49:45,240 --> 00:49:46,800
that's my question how can we Empower

1092
00:49:46,800 --> 00:49:49,859
users to do this

1093
00:49:49,859 --> 00:49:52,140
yours can you say something from the

1094
00:49:52,140 --> 00:49:55,460
consumer perspective yeah

1095
00:49:55,800 --> 00:49:58,500
the thing is that the this topic is

1096
00:49:58,500 --> 00:50:00,780
already dealt with by the gdpr if it

1097
00:50:00,780 --> 00:50:02,940
comes to data protection oh sorry data

1098
00:50:02,940 --> 00:50:04,980
protection and processing activities so

1099
00:50:04,980 --> 00:50:06,780
yeah you probably heard of the data

1100
00:50:06,780 --> 00:50:10,380
subject and his or her rights

1101
00:50:10,380 --> 00:50:12,960
um so from my perspective I would say

1102
00:50:12,960 --> 00:50:14,400
you know that this is not a problem at

1103
00:50:14,400 --> 00:50:16,859
all the law is already there but yeah I

1104
00:50:16,859 --> 00:50:19,079
I know as well also from a personal uh

1105
00:50:19,079 --> 00:50:21,900
experience I try to or I did actually I

1106
00:50:21,900 --> 00:50:23,839
sued T-Mobile

1107
00:50:23,839 --> 00:50:27,420
because they use my location data or

1108
00:50:27,420 --> 00:50:32,420
cell phone data to build apps

1109
00:50:32,420 --> 00:50:35,220
analytical apps and I think okay sure

1110
00:50:35,220 --> 00:50:37,020
but they did it with the government and

1111
00:50:37,020 --> 00:50:38,579
didn't tell anyone I just learned it

1112
00:50:38,579 --> 00:50:39,960
from the newspaper

1113
00:50:39,960 --> 00:50:42,200
so I was a bit uh

1114
00:50:42,200 --> 00:50:44,880
yeah excited in the wrong way I I

1115
00:50:44,880 --> 00:50:47,579
emailed them and then indeed I

1116
00:50:47,579 --> 00:50:50,520
recognized the story at first I had no

1117
00:50:50,520 --> 00:50:52,859
reaction as well and then okay I'm going

1118
00:50:52,859 --> 00:50:54,960
to tell them I'm a lawyer and then yes I

1119
00:50:54,960 --> 00:50:57,300
get a response

1120
00:50:57,300 --> 00:50:59,579
um and it takes a lot of time so even if

1121
00:50:59,579 --> 00:51:03,300
you can find a contact person it takes a

1122
00:51:03,300 --> 00:51:04,619
lot of time I actually took them to

1123
00:51:04,619 --> 00:51:07,440
court because my um but she cannot take

1124
00:51:07,440 --> 00:51:10,079
them to quote can't she yeah you can uh

1125
00:51:10,079 --> 00:51:11,880
well if you are the data subject

1126
00:51:11,880 --> 00:51:14,400
yourself you can and of course you have

1127
00:51:14,400 --> 00:51:18,420
ngos specialized in class actions so

1128
00:51:18,420 --> 00:51:19,940
they can

1129
00:51:19,940 --> 00:51:23,460
gather all those complaints and well you

1130
00:51:23,460 --> 00:51:25,020
can go to the court for several reasons

1131
00:51:25,020 --> 00:51:28,140
but you have to think about the the

1132
00:51:28,140 --> 00:51:31,319
costs involved it's it's very costly and

1133
00:51:31,319 --> 00:51:33,599
also the the duration of it it took me

1134
00:51:33,599 --> 00:51:35,640
almost two years and I know my way

1135
00:51:35,640 --> 00:51:37,760
around the court so and then I thought

1136
00:51:37,760 --> 00:51:40,640
if it's quite difficult for me

1137
00:51:40,640 --> 00:51:43,200
how is it for people who didn't study

1138
00:51:43,200 --> 00:51:45,240
law or who are not a lawyer so yes the

1139
00:51:45,240 --> 00:51:47,160
gdpr helps

1140
00:51:47,160 --> 00:51:50,520
um also the gdpr says something about EU

1141
00:51:50,520 --> 00:51:53,280
representative so if the app is built by

1142
00:51:53,280 --> 00:51:55,260
an American or Chinese company or

1143
00:51:55,260 --> 00:51:57,420
whatever they should have someone in

1144
00:51:57,420 --> 00:51:59,520
Europe which you can contact

1145
00:51:59,520 --> 00:52:02,040
and last but not least the AI act

1146
00:52:02,040 --> 00:52:05,280
because uh the ai ai Act is coming the

1147
00:52:05,280 --> 00:52:07,079
first X in the world that regulates

1148
00:52:07,079 --> 00:52:10,440
technology uh also on the level of the

1149
00:52:10,440 --> 00:52:12,180
the programmers the programs have to

1150
00:52:12,180 --> 00:52:13,920
think about a lot of stuff when they are

1151
00:52:13,920 --> 00:52:16,680
making AI in the near future uh just

1152
00:52:16,680 --> 00:52:18,240
like uh you are building a project

1153
00:52:18,240 --> 00:52:21,599
product now you're making AI

1154
00:52:21,599 --> 00:52:25,800
and the AIX also has provisions on you

1155
00:52:25,800 --> 00:52:27,660
should have a contact person available

1156
00:52:27,660 --> 00:52:30,780
Etc yeah so I think I don't have a

1157
00:52:30,780 --> 00:52:32,640
really good answer to the question other

1158
00:52:32,640 --> 00:52:35,119
than yes you have rights and

1159
00:52:35,119 --> 00:52:38,040
unfortunately they are really hard to uh

1160
00:52:38,040 --> 00:52:43,099
to enforce but it is possible

1161
00:52:43,260 --> 00:52:48,720
okay there's a next question yes hi

1162
00:52:48,720 --> 00:52:51,000
I'm Fran I work in applied machine

1163
00:52:51,000 --> 00:52:54,300
learning cyber security I would like to

1164
00:52:54,300 --> 00:52:57,359
start my question with an anecdote so on

1165
00:52:57,359 --> 00:52:59,880
on occasion I got passed down a

1166
00:52:59,880 --> 00:53:02,160
regulatory form from one of our clients

1167
00:53:02,160 --> 00:53:04,680
in the financial sector about they had

1168
00:53:04,680 --> 00:53:06,960
they had to report all the machine

1169
00:53:06,960 --> 00:53:08,760
learning models in their environment so

1170
00:53:08,760 --> 00:53:11,640
we had to provide information on that I

1171
00:53:11,640 --> 00:53:14,040
was kind of suggested to just for any

1172
00:53:14,040 --> 00:53:16,260
complicated stuff to just say down its

1173
00:53:16,260 --> 00:53:19,140
proprietary so it's easier but I found

1174
00:53:19,140 --> 00:53:21,480
the regulatory form actually quite well

1175
00:53:21,480 --> 00:53:23,940
written uh it covered most of the best

1176
00:53:23,940 --> 00:53:26,760
practices as someone who does his work

1177
00:53:26,760 --> 00:53:28,440
well I could basically give a decent

1178
00:53:28,440 --> 00:53:31,200
answer on most questions without having

1179
00:53:31,200 --> 00:53:33,000
to delve into actual proprietary stuff

1180
00:53:33,000 --> 00:53:37,020
so in the end I did and the feedback I

1181
00:53:37,020 --> 00:53:39,240
got was mostly about I didn't answer

1182
00:53:39,240 --> 00:53:43,559
some questions and I felt like the the

1183
00:53:43,559 --> 00:53:46,079
answers I did provide were not really

1184
00:53:46,079 --> 00:53:48,180
evaluated and they kind of focused on

1185
00:53:48,180 --> 00:53:52,079
how if if they if it's more filled out

1186
00:53:52,079 --> 00:53:54,180
completely rather than what I actually

1187
00:53:54,180 --> 00:53:57,839
answered so my question is even if we do

1188
00:53:57,839 --> 00:54:00,300
our best about being open about our

1189
00:54:00,300 --> 00:54:03,059
practices do everything right what is

1190
00:54:03,059 --> 00:54:04,740
the capacity of some regulatory

1191
00:54:04,740 --> 00:54:07,200
framework to actually evaluate and

1192
00:54:07,200 --> 00:54:08,940
absorb

1193
00:54:08,940 --> 00:54:10,680
the information we provide about our

1194
00:54:10,680 --> 00:54:12,599
work

1195
00:54:12,599 --> 00:54:15,540
and with regulatory framework you mean

1196
00:54:15,540 --> 00:54:18,480
something like like the fundamental

1197
00:54:18,480 --> 00:54:19,460
white

1198
00:54:19,460 --> 00:54:23,040
algorithm impact assessment or any of

1199
00:54:23,040 --> 00:54:25,680
the other 160 on the algorithm watch

1200
00:54:25,680 --> 00:54:27,980
list

1201
00:54:28,920 --> 00:54:32,280
I I can answer on that I think you make

1202
00:54:32,280 --> 00:54:34,619
a very very valid point because uh all

1203
00:54:34,619 --> 00:54:36,420
of these Frameworks

1204
00:54:36,420 --> 00:54:39,260
um whichever you choose are

1205
00:54:39,260 --> 00:54:42,119
self-regulation basically so

1206
00:54:42,119 --> 00:54:45,420
um there is no capacity to

1207
00:54:45,420 --> 00:54:49,020
you know have someone check if the

1208
00:54:49,020 --> 00:54:52,319
answers are correct and with ethics it's

1209
00:54:52,319 --> 00:54:54,599
a very new field so

1210
00:54:54,599 --> 00:54:58,079
correct is still under debate with what

1211
00:54:58,079 --> 00:54:59,880
is correct and what isn't

1212
00:54:59,880 --> 00:55:02,640
um so I think that's where oversight and

1213
00:55:02,640 --> 00:55:05,460
inspection really comes in uh that

1214
00:55:05,460 --> 00:55:08,520
that's that's the the stage in which uh

1215
00:55:08,520 --> 00:55:09,420
you know

1216
00:55:09,420 --> 00:55:11,760
um regulation or or authorities

1217
00:55:11,760 --> 00:55:14,819
inspection authorities should really

1218
00:55:14,819 --> 00:55:17,460
step in and

1219
00:55:17,460 --> 00:55:21,059
um uh in their respective domains you

1220
00:55:21,059 --> 00:55:24,420
have so many uh inspection authorities

1221
00:55:24,420 --> 00:55:26,940
in the Netherlands all overseeing their

1222
00:55:26,940 --> 00:55:28,859
field they have a lot of domain

1223
00:55:28,859 --> 00:55:31,680
expertise they've built this expertise

1224
00:55:31,680 --> 00:55:35,880
in knowledge for over uh decades they

1225
00:55:35,880 --> 00:55:39,119
should really also oversee Ai and the

1226
00:55:39,119 --> 00:55:41,579
use of algorithms and Technology within

1227
00:55:41,579 --> 00:55:44,000
their domains they should be the ones

1228
00:55:44,000 --> 00:55:48,359
qualitatively checking uh what companies

1229
00:55:48,359 --> 00:55:50,280
and and other organizations are doing

1230
00:55:50,280 --> 00:55:52,920
and not just if they have filled out the

1231
00:55:52,920 --> 00:55:55,800
form completely but really look at the

1232
00:55:55,800 --> 00:55:57,839
contents you know what is happening in

1233
00:55:57,839 --> 00:56:00,119
our domain is that happening in in the

1234
00:56:00,119 --> 00:56:02,339
right way doesn't it hard does it harm

1235
00:56:02,339 --> 00:56:06,000
human rights or or fundamental rights

1236
00:56:06,000 --> 00:56:08,280
um and what we're seeing in the

1237
00:56:08,280 --> 00:56:09,599
Netherlands right now I have no

1238
00:56:09,599 --> 00:56:11,700
experience in in other countries when it

1239
00:56:11,700 --> 00:56:14,940
comes to inspection authorities but we

1240
00:56:14,940 --> 00:56:18,380
see that those authorities are

1241
00:56:18,380 --> 00:56:21,000
stepping up and taking their

1242
00:56:21,000 --> 00:56:24,180
responsibility trying to keep oversight

1243
00:56:24,180 --> 00:56:27,119
on on AI you have a couple of Frameworks

1244
00:56:27,119 --> 00:56:30,260
with the which they are using already

1245
00:56:30,260 --> 00:56:32,940
but it's it is difficult for them

1246
00:56:32,940 --> 00:56:35,160
especially in domains you know in in

1247
00:56:35,160 --> 00:56:38,040
domains such as the Telecom industry

1248
00:56:38,040 --> 00:56:41,760
they're very technology heavy already so

1249
00:56:41,760 --> 00:56:44,160
they have a lot of capacities to do

1250
00:56:44,160 --> 00:56:46,920
these kinds of of checks but in in other

1251
00:56:46,920 --> 00:56:49,440
domains like like food safety for

1252
00:56:49,440 --> 00:56:51,900
instance the people working at such an

1253
00:56:51,900 --> 00:56:54,660
authority don't have any experience with

1254
00:56:54,660 --> 00:56:58,859
you know auditing AI systems so that's

1255
00:56:58,859 --> 00:57:01,280
really something that should be built up

1256
00:57:01,280 --> 00:57:05,099
uh and should be uh formalized

1257
00:57:05,099 --> 00:57:07,680
um that mandate but that's another point

1258
00:57:07,680 --> 00:57:10,020
to it and if I understood uh yoast

1259
00:57:10,020 --> 00:57:11,700
correctly here the gdpr really

1260
00:57:11,700 --> 00:57:14,220
encourages the organizations to develop

1261
00:57:14,220 --> 00:57:16,800
their own maturity to develop their own

1262
00:57:16,800 --> 00:57:19,920
codes of conduct and adhere to them so

1263
00:57:19,920 --> 00:57:22,740
this is something that I feel across the

1264
00:57:22,740 --> 00:57:25,740
board of our experts here today that

1265
00:57:25,740 --> 00:57:27,720
they really call for the development of

1266
00:57:27,720 --> 00:57:30,180
a mature approach to artificial

1267
00:57:30,180 --> 00:57:32,460
intelligence and data practices and to

1268
00:57:32,460 --> 00:57:35,760
develop that within any organization and

1269
00:57:35,760 --> 00:57:38,640
that does not have to be prescribed

1270
00:57:38,640 --> 00:57:41,520
necessarily in detail by law it is

1271
00:57:41,520 --> 00:57:43,800
something that has to be embedded much

1272
00:57:43,800 --> 00:57:46,319
more in detail into the practice in the

1273
00:57:46,319 --> 00:57:49,380
respective organizations so my questions

1274
00:57:49,380 --> 00:57:52,380
to to the panel would be what is the

1275
00:57:52,380 --> 00:57:54,119
next step in achieving that because

1276
00:57:54,119 --> 00:57:55,859
everything you presented they are good

1277
00:57:55,859 --> 00:57:57,900
ideas their best practices we can point

1278
00:57:57,900 --> 00:58:00,240
to there is a legal framework that seems

1279
00:58:00,240 --> 00:58:02,579
to be really helpful and encouraging but

1280
00:58:02,579 --> 00:58:04,920
what is the next step to to actually

1281
00:58:04,920 --> 00:58:08,579
make a responsible data practice and I

1282
00:58:08,579 --> 00:58:12,079
an AI possible

1283
00:58:12,960 --> 00:58:15,960
shall I shall I start

1284
00:58:15,960 --> 00:58:18,059
um of course I'm I work at a university

1285
00:58:18,059 --> 00:58:20,640
so I'm very biased in saying that

1286
00:58:20,640 --> 00:58:22,040
education

1287
00:58:22,040 --> 00:58:25,520
obviously plays a big part

1288
00:58:25,520 --> 00:58:29,400
not only educating students for instance

1289
00:58:29,400 --> 00:58:31,819
data science students but also

1290
00:58:31,819 --> 00:58:34,200
professionals working in the field right

1291
00:58:34,200 --> 00:58:35,359
now

1292
00:58:35,359 --> 00:58:38,480
educating politicians

1293
00:58:38,480 --> 00:58:42,540
involving ngos also in this

1294
00:58:42,540 --> 00:58:45,480
um and and really stimulating and

1295
00:58:45,480 --> 00:58:47,579
encouraging that Democratic control and

1296
00:58:47,579 --> 00:58:49,859
deliberation

1297
00:58:49,859 --> 00:58:52,680
um I think that's that's very important

1298
00:58:52,680 --> 00:58:55,920
and I'm sure you have a lot to add to

1299
00:58:55,920 --> 00:58:57,059
this

1300
00:58:57,059 --> 00:59:00,900
well I think yeah I I agree for me the

1301
00:59:00,900 --> 00:59:03,960
best thing to do now is to uh especially

1302
00:59:03,960 --> 00:59:05,579
when we are looking at high stake

1303
00:59:05,579 --> 00:59:07,200
algorithms

1304
00:59:07,200 --> 00:59:10,280
we should develop an inclusive

1305
00:59:10,280 --> 00:59:13,559
participatory accountable Democratic

1306
00:59:13,559 --> 00:59:16,319
approach to designing and building those

1307
00:59:16,319 --> 00:59:17,760
algorithms

1308
00:59:17,760 --> 00:59:20,400
and uh well we've done a pilot with the

1309
00:59:20,400 --> 00:59:22,559
province of freestand and with Roberto

1310
00:59:22,559 --> 00:59:26,579
sicari trying to do the first steps and

1311
00:59:26,579 --> 00:59:28,319
it's really hard it's really difficult

1312
00:59:28,319 --> 00:59:31,440
and there aren't a lot of examples so I

1313
00:59:31,440 --> 00:59:33,359
would really yeah

1314
00:59:33,359 --> 00:59:37,200
would really love you to help us doing

1315
00:59:37,200 --> 00:59:39,540
this and and and

1316
00:59:39,540 --> 00:59:42,240
um wow yeah proceed on that way it's

1317
00:59:42,240 --> 00:59:44,640
it's really important okay you clearly

1318
00:59:44,640 --> 00:59:46,319
make a point here for the Civil Society

1319
00:59:46,319 --> 00:59:49,400
it's our media advocacy and ngos and

1320
00:59:49,400 --> 00:59:52,400
educational institutions stepping in and

1321
00:59:52,400 --> 00:59:55,140
I've talked also to companies about this

1322
00:59:55,140 --> 00:59:56,760
and they were saying oh it's much to

1323
00:59:56,760 --> 00:59:59,160
it's way too expensive and it costs way

1324
00:59:59,160 --> 01:00:00,180
too much time

1325
01:00:00,180 --> 01:00:01,520
I understand

1326
01:00:01,520 --> 01:00:05,579
from profitable yes but I think for us

1327
01:00:05,579 --> 01:00:06,900
as a government

1328
01:00:06,900 --> 01:00:10,740
like I I already said or we should set

1329
01:00:10,740 --> 01:00:13,559
an example and when we look at the child

1330
01:00:13,559 --> 01:00:15,780
care benefits Scandal well I do think

1331
01:00:15,780 --> 01:00:19,020
that well those costs and and the harm

1332
01:00:19,020 --> 01:00:22,260
that has done that we can't undo they

1333
01:00:22,260 --> 01:00:25,020
are much much larger so it's well worth

1334
01:00:25,020 --> 01:00:27,599
the investment

1335
01:00:27,599 --> 01:00:28,859
yeah

1336
01:00:28,859 --> 01:00:33,240
um if you ask me what can we do uh let

1337
01:00:33,240 --> 01:00:37,500
me start with uh a bias I have my bias

1338
01:00:37,500 --> 01:00:41,760
is that ngos are a bit too much reactive

1339
01:00:41,760 --> 01:00:45,780
so if there is an act published or ideas

1340
01:00:45,780 --> 01:00:48,859
one act you have all the ngos

1341
01:00:48,859 --> 01:00:51,720
reacting oh that's stupid oh that's

1342
01:00:51,720 --> 01:00:53,400
hindering fundamental rights and I think

1343
01:00:53,400 --> 01:00:55,920
of probably they are right but the

1344
01:00:55,920 --> 01:00:57,839
problem with that approach is is that

1345
01:00:57,839 --> 01:01:01,020
you do not set the tone and I think if

1346
01:01:01,020 --> 01:01:04,380
you start a discussion uh or open up the

1347
01:01:04,380 --> 01:01:06,059
debate it's really important to set the

1348
01:01:06,059 --> 01:01:08,579
tone and how do you do that just be the

1349
01:01:08,579 --> 01:01:12,960
first party to uh to draft the ACT you

1350
01:01:12,960 --> 01:01:14,099
can do that you can do that together

1351
01:01:14,099 --> 01:01:17,160
with parliamentarians or if it comes to

1352
01:01:17,160 --> 01:01:19,500
my idea of the code of conduct you do it

1353
01:01:19,500 --> 01:01:22,200
together with uh with the businesses but

1354
01:01:22,200 --> 01:01:25,260
if you want to have influence you start

1355
01:01:25,260 --> 01:01:26,760
with the draft on your own and you you

1356
01:01:26,760 --> 01:01:28,500
go to the company say we already made

1357
01:01:28,500 --> 01:01:31,380
this also uh it will save you time and

1358
01:01:31,380 --> 01:01:34,140
thus costs and we can make you an

1359
01:01:34,140 --> 01:01:36,540
industry leader come join us

1360
01:01:36,540 --> 01:01:39,119
um however this is my bias and if you

1361
01:01:39,119 --> 01:01:42,359
think yoast what are you saying ngos are

1362
01:01:42,359 --> 01:01:46,020
are not proactive feel free to challenge

1363
01:01:46,020 --> 01:01:48,259
me

1364
01:01:48,599 --> 01:01:50,460
so they asked the question right there I

1365
01:01:50,460 --> 01:01:51,780
don't know whether that's response to

1366
01:01:51,780 --> 01:01:54,059
that but please go ahead hi my name is

1367
01:01:54,059 --> 01:01:56,160
Leva thank you so much I come from a

1368
01:01:56,160 --> 01:01:58,079
active NGO

1369
01:01:58,079 --> 01:02:00,359
um but I had a question connect in

1370
01:02:00,359 --> 01:02:01,859
connection to the scandals you mentioned

1371
01:02:01,859 --> 01:02:03,480
happened in Netherlands and also the

1372
01:02:03,480 --> 01:02:06,240
fact that it's impossible by input data

1373
01:02:06,240 --> 01:02:08,460
to fix the algorithms to make sure that

1374
01:02:08,460 --> 01:02:10,619
they're non-discriminatory so in your

1375
01:02:10,619 --> 01:02:13,260
opinion what's the right recourse or is

1376
01:02:13,260 --> 01:02:15,540
there an effective recourse system that

1377
01:02:15,540 --> 01:02:17,460
can be put in place for people who are

1378
01:02:17,460 --> 01:02:20,579
affected discriminated by the algorithms

1379
01:02:20,579 --> 01:02:23,240
and systems

1380
01:02:23,700 --> 01:02:28,079
so nice yeah so so that great question I

1381
01:02:28,079 --> 01:02:30,540
mean it's um

1382
01:02:30,540 --> 01:02:32,579
I try to connect it to the few things

1383
01:02:32,579 --> 01:02:34,140
that were already said for instance the

1384
01:02:34,140 --> 01:02:36,000
case in woman the gentleman mentioned

1385
01:02:36,000 --> 01:02:37,520
where the police started to

1386
01:02:37,520 --> 01:02:40,559
non-discriminatory uh scan all cars

1387
01:02:40,559 --> 01:02:43,440
approaching a certain shopping mall

1388
01:02:43,440 --> 01:02:45,660
um it was Amnesty International that

1389
01:02:45,660 --> 01:02:47,460
really did the investigation and put it

1390
01:02:47,460 --> 01:02:49,440
on to the agenda of the public discourse

1391
01:02:49,440 --> 01:02:51,599
that was really helpful in the case of

1392
01:02:51,599 --> 01:02:53,940
the Dutch benefit child benefit Scandal

1393
01:02:53,940 --> 01:02:56,880
I would like to point out we always look

1394
01:02:56,880 --> 01:02:59,579
at the technology here but in the that

1395
01:02:59,579 --> 01:03:01,680
particular case it wasn't artificial

1396
01:03:01,680 --> 01:03:03,359
intelligence it was probably a

1397
01:03:03,359 --> 01:03:05,640
regression analysis that was carried out

1398
01:03:05,640 --> 01:03:07,680
there was no machine learning involved

1399
01:03:07,680 --> 01:03:10,140
but what we can learn from the case is

1400
01:03:10,140 --> 01:03:12,960
that the context also matters in that

1401
01:03:12,960 --> 01:03:15,720
particular case it was usually the

1402
01:03:15,720 --> 01:03:18,960
political pressure that stopped people

1403
01:03:18,960 --> 01:03:22,160
working on the cases to

1404
01:03:22,160 --> 01:03:24,480
voice their concerns about the

1405
01:03:24,480 --> 01:03:26,400
effectiveness of the wool based

1406
01:03:26,400 --> 01:03:29,400
algorithm big word here but that was

1407
01:03:29,400 --> 01:03:33,180
applied so I I think um to to answer

1408
01:03:33,180 --> 01:03:35,099
your question a bit that all

1409
01:03:35,099 --> 01:03:37,740
encompassing approach should consider

1410
01:03:37,740 --> 01:03:40,200
both technology and the social context

1411
01:03:40,200 --> 01:03:42,480
and the Frameworks that we've seen today

1412
01:03:42,480 --> 01:03:45,720
try to implement exactly that to look at

1413
01:03:45,720 --> 01:03:47,880
the developers context of any digital

1414
01:03:47,880 --> 01:03:50,400
system whether it is in machine learning

1415
01:03:50,400 --> 01:03:53,220
trained algorithm or whether it is just

1416
01:03:53,220 --> 01:03:56,700
a form of data analysis or a decision

1417
01:03:56,700 --> 01:04:00,119
tree or whatsoever and to to look in

1418
01:04:00,119 --> 01:04:02,640
particular in the use context how is it

1419
01:04:02,640 --> 01:04:06,299
used how do you care about appeal

1420
01:04:06,299 --> 01:04:08,640
processes how do you deal with errors

1421
01:04:08,640 --> 01:04:10,859
that are quite apparent and that is

1422
01:04:10,859 --> 01:04:12,420
something that was missing in the child

1423
01:04:12,420 --> 01:04:15,359
benefit Scandal Affair that there was no

1424
01:04:15,359 --> 01:04:18,059
appeal process there was no way to

1425
01:04:18,059 --> 01:04:20,520
report the errors and have a response to

1426
01:04:20,520 --> 01:04:22,980
that now that would be my part on it but

1427
01:04:22,980 --> 01:04:25,079
maybe you want to add something from

1428
01:04:25,079 --> 01:04:27,240
yeah I think when you're I'm not an

1429
01:04:27,240 --> 01:04:29,160
expert on a childcare benefit Scandal

1430
01:04:29,160 --> 01:04:32,819
but I think yes the algorithm it was

1431
01:04:32,819 --> 01:04:35,880
faulty it was discriminatory but even it

1432
01:04:35,880 --> 01:04:39,780
hadn't me the harm done would have been

1433
01:04:39,780 --> 01:04:43,200
yeah also huge and because

1434
01:04:43,200 --> 01:04:46,440
um when we look at what how the

1435
01:04:46,440 --> 01:04:48,660
predictions were being used and the

1436
01:04:48,660 --> 01:04:51,920
interventions that were

1437
01:04:51,920 --> 01:04:55,500
were done and those uh and we could have

1438
01:04:55,500 --> 01:04:58,260
said people were being flagged as

1439
01:04:58,260 --> 01:05:00,540
potential fraudsters and their benefits

1440
01:05:00,540 --> 01:05:02,520
were stopped

1441
01:05:02,520 --> 01:05:05,099
we could have said okay this is a person

1442
01:05:05,099 --> 01:05:08,400
who who forgot to file a form or put

1443
01:05:08,400 --> 01:05:10,859
signature on a form we we can call them

1444
01:05:10,859 --> 01:05:13,140
and we can we can tell them to correct

1445
01:05:13,140 --> 01:05:15,059
their error no we didn't do that we

1446
01:05:15,059 --> 01:05:18,260
stopped their benefits and also the the

1447
01:05:18,260 --> 01:05:21,299
predictions of the algorithm they were

1448
01:05:21,299 --> 01:05:22,980
being shared with other government

1449
01:05:22,980 --> 01:05:25,980
agencies and they were being reused in a

1450
01:05:25,980 --> 01:05:28,260
completely different context and new

1451
01:05:28,260 --> 01:05:31,339
decisions being made upon those uh

1452
01:05:31,339 --> 01:05:34,020
predictions so

1453
01:05:34,020 --> 01:05:36,660
as I said before we really need to look

1454
01:05:36,660 --> 01:05:39,480
at the system systemic level

1455
01:05:39,480 --> 01:05:42,839
if you want to control these harms being

1456
01:05:42,839 --> 01:05:45,240
done by a AI it's not enough to look at

1457
01:05:45,240 --> 01:05:48,299
the algorithm and to add on that what

1458
01:05:48,299 --> 01:05:50,520
we've seen over the past 10 years in the

1459
01:05:50,520 --> 01:05:52,980
Netherlands is that any change has been

1460
01:05:52,980 --> 01:05:55,260
really driven by two things on the one

1461
01:05:55,260 --> 01:05:58,020
hand uh the the policy pressure coming

1462
01:05:58,020 --> 01:06:00,480
from Brussels so many municipalities

1463
01:06:00,480 --> 01:06:02,760
started structurally thinking about the

1464
01:06:02,760 --> 01:06:06,180
practices when when the gdpr was

1465
01:06:06,180 --> 01:06:08,160
approaching that was the moment they

1466
01:06:08,160 --> 01:06:09,720
really stepped up the game and the other

1467
01:06:09,720 --> 01:06:12,540
thing was Eros that were reported in the

1468
01:06:12,540 --> 01:06:15,180
media so looking at the Digital Society

1469
01:06:15,180 --> 01:06:17,400
we really need a forces state that is

1470
01:06:17,400 --> 01:06:19,500
tech savvy we need a really good Tech

1471
01:06:19,500 --> 01:06:22,380
journalism that is capable in

1472
01:06:22,380 --> 01:06:23,780
investigating

1473
01:06:23,780 --> 01:06:26,940
this kind of of our society and report

1474
01:06:26,940 --> 01:06:28,799
on it in an effective and appropriate

1475
01:06:28,799 --> 01:06:30,900
way and I think we're lucky in the

1476
01:06:30,900 --> 01:06:32,640
Netherlands that we have really some

1477
01:06:32,640 --> 01:06:35,400
excellent Tech journalists there and and

1478
01:06:35,400 --> 01:06:38,579
a vested interest of of a public reading

1479
01:06:38,579 --> 01:06:42,539
such coverage and as for ngos so we've

1480
01:06:42,539 --> 01:06:44,339
seen quite some shifts over the past

1481
01:06:44,339 --> 01:06:46,319
five years I mean prior to this meeting

1482
01:06:46,319 --> 01:06:48,359
we we discussed what has happened in

1483
01:06:48,359 --> 01:06:50,339
those five years and one thing is that

1484
01:06:50,339 --> 01:06:52,680
we see ngos also changing that they

1485
01:06:52,680 --> 01:06:54,720
start Amnesty International for instance

1486
01:06:54,720 --> 01:06:57,839
concerning itself actively with uh

1487
01:06:57,839 --> 01:06:59,700
issues of artificial intelligence and

1488
01:06:59,700 --> 01:07:01,559
human rights to what extent is that

1489
01:07:01,559 --> 01:07:03,599
pre-breaching

1490
01:07:03,599 --> 01:07:06,240
um this is something that can really I

1491
01:07:06,240 --> 01:07:08,760
think from from the perspective of uh of

1492
01:07:08,760 --> 01:07:11,280
our position in Academia really helped

1493
01:07:11,280 --> 01:07:13,799
building a mature Digital Society where

1494
01:07:13,799 --> 01:07:16,079
we have different stakeholders a forces

1495
01:07:16,079 --> 01:07:18,839
state that is tech savvy and able to to

1496
01:07:18,839 --> 01:07:21,119
ask the the needed and hard questions

1497
01:07:21,119 --> 01:07:23,940
about how technology is used is able to

1498
01:07:23,940 --> 01:07:25,859
investigate that even on a local level

1499
01:07:25,859 --> 01:07:28,440
don't underestimate the local aspect so

1500
01:07:28,440 --> 01:07:30,900
in the Netherlands we see algorithms

1501
01:07:30,900 --> 01:07:33,359
really deployed on a local level like

1502
01:07:33,359 --> 01:07:37,319
social benefit for detection algorithms

1503
01:07:37,319 --> 01:07:39,720
that are used by smaller municipality

1504
01:07:39,720 --> 01:07:41,700
cities and are developed by small and

1505
01:07:41,700 --> 01:07:43,980
medium companies locally not the big

1506
01:07:43,980 --> 01:07:46,079
companies that we see there so local

1507
01:07:46,079 --> 01:07:48,180
journalism is here important as well to

1508
01:07:48,180 --> 01:07:50,700
ask questions about that and then most

1509
01:07:50,700 --> 01:07:53,059
importantly I think the last mile after

1510
01:07:53,059 --> 01:07:56,819
trained bureaucrats people in in in our

1511
01:07:56,819 --> 01:08:00,900
government sector a legal sector that is

1512
01:08:00,900 --> 01:08:03,900
a robust we we need the last mile of

1513
01:08:03,900 --> 01:08:06,839
stakeholders uh the local electives we

1514
01:08:06,839 --> 01:08:09,539
would say something that Eris tries to

1515
01:08:09,539 --> 01:08:12,359
approach in her work in training

1516
01:08:12,359 --> 01:08:14,819
elected people in municipalities and

1517
01:08:14,819 --> 01:08:17,040
that is something I believe would build

1518
01:08:17,040 --> 01:08:19,799
a robust and mature Digital Society that

1519
01:08:19,799 --> 01:08:22,738
can deal with these challenges

1520
01:08:22,738 --> 01:08:25,259
there's another question so yes thank

1521
01:08:25,259 --> 01:08:27,420
you I work for a citizen engagement

1522
01:08:27,420 --> 01:08:30,120
platform and so most of our clients are

1523
01:08:30,120 --> 01:08:31,738
from the public sector but we also have

1524
01:08:31,738 --> 01:08:36,000
private orgs and uh and we do use AI to

1525
01:08:36,000 --> 01:08:38,580
summarize categorize and give insights

1526
01:08:38,580 --> 01:08:41,120
about all the consultations that we

1527
01:08:41,120 --> 01:08:45,299
organize for the clients and um

1528
01:08:45,299 --> 01:08:47,880
as you say it's extremely difficult to

1529
01:08:47,880 --> 01:08:49,799
say whether especially also because we

1530
01:08:49,799 --> 01:08:52,198
use external services to do that to say

1531
01:08:52,198 --> 01:08:53,460
whether or not the algorithm is

1532
01:08:53,460 --> 01:08:57,420
discriminatory and somehow it's also we

1533
01:08:57,420 --> 01:09:01,380
we as a tech subcontractor uh play a

1534
01:09:01,380 --> 01:09:03,120
role into what tech we use and how

1535
01:09:03,120 --> 01:09:04,380
transparent we are with the clients and

1536
01:09:04,380 --> 01:09:08,299
the users and so one way we kind of were

1537
01:09:08,299 --> 01:09:12,000
do for now it's also show on the

1538
01:09:12,000 --> 01:09:14,279
platforms which decision have been made

1539
01:09:14,279 --> 01:09:16,500
or categorized or summarized by an AI

1540
01:09:16,500 --> 01:09:18,899
service so that at least the person

1541
01:09:18,899 --> 01:09:22,020
knows that it needs to to pay attention

1542
01:09:22,020 --> 01:09:24,719
to to potential bias or something else

1543
01:09:24,719 --> 01:09:29,640
or and react to that but in general my

1544
01:09:29,640 --> 01:09:31,020
question is then

1545
01:09:31,020 --> 01:09:33,359
um what what level of transparency and

1546
01:09:33,359 --> 01:09:35,880
maybe traceability and especially in the

1547
01:09:35,880 --> 01:09:38,399
public sector can citizen users or

1548
01:09:38,399 --> 01:09:42,120
clients expect or should have uh to

1549
01:09:42,120 --> 01:09:45,299
counter these potential bias and have a

1550
01:09:45,299 --> 01:09:48,000
way to react and so yeah what's what's

1551
01:09:48,000 --> 01:09:50,219
the expected level of transparency is

1552
01:09:50,219 --> 01:09:52,380
there something already in place for

1553
01:09:52,380 --> 01:09:55,800
determining this or is it like our on

1554
01:09:55,800 --> 01:09:57,660
the per situation basis or something

1555
01:09:57,660 --> 01:09:59,340
well I would like to have your opinion

1556
01:09:59,340 --> 01:10:01,860
on this thank you excellent question I

1557
01:10:01,860 --> 01:10:05,340
think we we had a letter by a Secretary

1558
01:10:05,340 --> 01:10:07,739
of State uh demanding transparency

1559
01:10:07,739 --> 01:10:09,260
unless otherwise

1560
01:10:09,260 --> 01:10:12,780
I stopped by with our lawyer here what

1561
01:10:12,780 --> 01:10:16,260
what can citizens expect in terms of

1562
01:10:16,260 --> 01:10:17,840
transparency

1563
01:10:17,840 --> 01:10:21,120
well I know there are a lot of methods

1564
01:10:21,120 --> 01:10:23,520
around and I don't know anything about

1565
01:10:23,520 --> 01:10:24,500
it maybe

1566
01:10:24,500 --> 01:10:29,460
next to me but oh I see five minutes

1567
01:10:29,460 --> 01:10:30,300
um

1568
01:10:30,300 --> 01:10:32,460
now transparency is an important topic

1569
01:10:32,460 --> 01:10:35,640
why because without it you wouldn't know

1570
01:10:35,640 --> 01:10:37,440
for instance whether or not you are

1571
01:10:37,440 --> 01:10:39,360
discriminated if you have no idea how

1572
01:10:39,360 --> 01:10:41,760
the algorithm works so to a certain

1573
01:10:41,760 --> 01:10:44,940
extent transparency is needed in the

1574
01:10:44,940 --> 01:10:48,540
upcoming AIX that's totally understood

1575
01:10:48,540 --> 01:10:50,880
and the legislator has some Provisions

1576
01:10:50,880 --> 01:10:57,000
about that however a legislator also uh

1577
01:10:57,000 --> 01:11:00,060
puts a lot of references to are you

1578
01:11:00,060 --> 01:11:02,580
aware of something called ISO Norms so

1579
01:11:02,580 --> 01:11:05,580
those are technical Norms about how you

1580
01:11:05,580 --> 01:11:07,860
shoot in this case uh provide

1581
01:11:07,860 --> 01:11:10,140
transparency and you see that the the

1582
01:11:10,140 --> 01:11:13,199
legislator I think it's a person like me

1583
01:11:13,199 --> 01:11:15,780
once it gets too Technical and too

1584
01:11:15,780 --> 01:11:18,179
detailed it says well go look at the

1585
01:11:18,179 --> 01:11:20,820
delegated acts so there will be so you

1586
01:11:20,820 --> 01:11:22,440
have the AI act but in the future we

1587
01:11:22,440 --> 01:11:24,179
have will have a lot of surrounding

1588
01:11:24,179 --> 01:11:29,580
delegated acts and ISO Norms as well uh

1589
01:11:29,580 --> 01:11:32,760
and my expectation is that as part of

1590
01:11:32,760 --> 01:11:35,600
those Norms you would know as an AI

1591
01:11:35,600 --> 01:11:39,060
provider to what level or what level of

1592
01:11:39,060 --> 01:11:41,520
transparency you have to maintain but so

1593
01:11:41,520 --> 01:11:44,040
my answer is only yes this is a known

1594
01:11:44,040 --> 01:11:46,980
problem and no we do not know yet we

1595
01:11:46,980 --> 01:11:49,380
lawyers do not know yet the exact

1596
01:11:49,380 --> 01:11:53,760
details on how transparent ufcb

1597
01:11:53,760 --> 01:11:56,820
really have a better answer sorry I

1598
01:11:56,820 --> 01:11:58,940
think

1599
01:11:59,219 --> 01:12:01,679
a lot of work transparency is a word

1600
01:12:01,679 --> 01:12:04,020
that we use quite often but what it

1601
01:12:04,020 --> 01:12:06,360
really means that's that's very

1602
01:12:06,360 --> 01:12:08,400
difficult and when you start thinking

1603
01:12:08,400 --> 01:12:10,260
about it and when we when we were

1604
01:12:10,260 --> 01:12:12,719
talking about transparent algorithms you

1605
01:12:12,719 --> 01:12:14,640
can how you can think about okay the

1606
01:12:14,640 --> 01:12:16,560
process should be transparent and

1607
01:12:16,560 --> 01:12:17,940
accountable

1608
01:12:17,940 --> 01:12:20,760
and the or you could say well the model

1609
01:12:20,760 --> 01:12:22,920
and the behavior of the model needs to

1610
01:12:22,920 --> 01:12:25,620
be transparent because uh well when we

1611
01:12:25,620 --> 01:12:26,600
look at

1612
01:12:26,600 --> 01:12:29,100
non-discriminatory legislation and we

1613
01:12:29,100 --> 01:12:31,020
need to know how the model behaves and

1614
01:12:31,020 --> 01:12:33,719
differences between uh demographic

1615
01:12:33,719 --> 01:12:37,260
groups and of course uh when when we are

1616
01:12:37,260 --> 01:12:39,420
using the model for individual

1617
01:12:39,420 --> 01:12:42,780
predictions and decisions well citizens

1618
01:12:42,780 --> 01:12:46,020
they want to contest hand those

1619
01:12:46,020 --> 01:12:49,140
decisions and need transparency so those

1620
01:12:49,140 --> 01:12:51,659
are very different kinds of transparency

1621
01:12:51,659 --> 01:12:53,460
I think so

1622
01:12:53,460 --> 01:12:55,560
um yeah I don't have a good answer

1623
01:12:55,560 --> 01:12:59,460
either and I I do think that well yeah

1624
01:12:59,460 --> 01:13:02,880
we need to dive into this and and I also

1625
01:13:02,880 --> 01:13:05,820
think that standards are really

1626
01:13:05,820 --> 01:13:08,820
important and but what we should not

1627
01:13:08,820 --> 01:13:12,840
leave that to a small group of uh yeah

1628
01:13:12,840 --> 01:13:15,540
of people lawyers no well not only

1629
01:13:15,540 --> 01:13:18,060
lawyers it's also people from industry

1630
01:13:18,060 --> 01:13:20,400
actually there has been some criticism

1631
01:13:20,400 --> 01:13:23,100
on the on the development of the

1632
01:13:23,100 --> 01:13:24,659
standards because it's not really a

1633
01:13:24,659 --> 01:13:27,659
demographic Democratic process so it

1634
01:13:27,659 --> 01:13:30,480
should be more involvement I guess from

1635
01:13:30,480 --> 01:13:33,600
uh from government from administrators

1636
01:13:33,600 --> 01:13:36,659
and from ngos

1637
01:13:36,659 --> 01:13:38,760
you have a minute

1638
01:13:38,760 --> 01:13:41,520
uh there's just one small thing I wanted

1639
01:13:41,520 --> 01:13:42,780
to add

1640
01:13:42,780 --> 01:13:46,860
um regarding transparency since uh the

1641
01:13:46,860 --> 01:13:48,840
first of this month in the Netherlands

1642
01:13:48,840 --> 01:13:52,020
there's a national algorithm register in

1643
01:13:52,020 --> 01:13:54,000
which all all government organizations

1644
01:13:54,000 --> 01:13:57,120
are asked to disclose the algorithms

1645
01:13:57,120 --> 01:13:58,280
they use

1646
01:13:58,280 --> 01:14:02,100
which they have done I think you you can

1647
01:14:02,100 --> 01:14:03,840
check it later but I think there are

1648
01:14:03,840 --> 01:14:06,719
over 200 on there now

1649
01:14:06,719 --> 01:14:10,739
um which really means a huge step for

1650
01:14:10,739 --> 01:14:13,860
for public transparency everyone can

1651
01:14:13,860 --> 01:14:16,380
just go on there and and see what kind

1652
01:14:16,380 --> 01:14:19,080
of algorithms are used uh what what

1653
01:14:19,080 --> 01:14:22,020
their purpose is so I'm I'm very curious

1654
01:14:22,020 --> 01:14:24,840
to see how that development will go in

1655
01:14:24,840 --> 01:14:26,400
the future and if other countries will

1656
01:14:26,400 --> 01:14:30,060
adopt the same practices thank you

1657
01:14:30,060 --> 01:14:32,159
well we are at the end of this panel

1658
01:14:32,159 --> 01:14:34,500
thank you all very much for being here

1659
01:14:34,500 --> 01:14:37,260
and for and the questions and the

1660
01:14:37,260 --> 01:14:39,719
fruitful discussion we had it has been a

1661
01:14:39,719 --> 01:14:41,190
blast thank you all

1662
01:14:41,190 --> 01:14:41,760
[Applause]

1663
01:14:41,760 --> 01:14:45,020
[Music]

