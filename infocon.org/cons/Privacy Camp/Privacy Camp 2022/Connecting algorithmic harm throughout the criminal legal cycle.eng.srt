1
00:00:07,440 --> 00:00:11,599
okay hi all uh so i am ben winters i'm

2
00:00:11,599 --> 00:00:13,200
counsel at the electronic privacy

3
00:00:13,200 --> 00:00:15,360
information center or epic

4
00:00:15,360 --> 00:00:17,520
where i lead our project on ai and human

5
00:00:17,520 --> 00:00:18,480
rights

6
00:00:18,480 --> 00:00:20,240
i'll be moderating this panel today of

7
00:00:20,240 --> 00:00:22,320
excellent experts that i'm excited to

8
00:00:22,320 --> 00:00:24,480
hear from during this panel

9
00:00:24,480 --> 00:00:25,840
we're going to be discussing how many

10
00:00:25,840 --> 00:00:28,240
tools used both directly in the criminal

11
00:00:28,240 --> 00:00:30,320
legal system like predictive policing

12
00:00:30,320 --> 00:00:31,840
facial recognition tools risk

13
00:00:31,840 --> 00:00:34,640
assessments and more and outside of it

14
00:00:34,640 --> 00:00:36,559
like algorithms that help decide loan

15
00:00:36,559 --> 00:00:38,719
rates school eligibility housing

16
00:00:38,719 --> 00:00:40,800
eligibility and more are related to each

17
00:00:40,800 --> 00:00:42,399
other occasionally exact further

18
00:00:42,399 --> 00:00:44,079
exacerbating the negative effects of any

19
00:00:44,079 --> 00:00:46,239
single one system and furthering us from

20
00:00:46,239 --> 00:00:47,200
meaningful understanding and

21
00:00:47,200 --> 00:00:48,800
accountability we're going to try to

22
00:00:48,800 --> 00:00:50,879
zoom out and consider how advocates

23
00:00:50,879 --> 00:00:52,239
can keep this in mind when trying to

24
00:00:52,239 --> 00:00:53,760
deal with each system as a problem come

25
00:00:53,760 --> 00:00:55,840
up how the current regulatory scheme

26
00:00:55,840 --> 00:00:57,600
anywhere in the world is really not well

27
00:00:57,600 --> 00:00:59,680
keyed to handle this dynamic and what's

28
00:00:59,680 --> 00:01:00,559
next

29
00:01:00,559 --> 00:01:01,520
so

30
00:01:01,520 --> 00:01:03,039
logistically we're going to hear from

31
00:01:03,039 --> 00:01:04,479
each panelist briefly just for about

32
00:01:04,479 --> 00:01:05,760
five minutes then have a guided

33
00:01:05,760 --> 00:01:06,799
discussion

34
00:01:06,799 --> 00:01:08,240
among the participants and take some

35
00:01:08,240 --> 00:01:10,479
questions at the end so please use the

36
00:01:10,479 --> 00:01:12,320
the public chat for q a throughout we'll

37
00:01:12,320 --> 00:01:13,920
work it in and then make sure to take

38
00:01:13,920 --> 00:01:16,320
questions at the end um but for now i'm

39
00:01:16,320 --> 00:01:17,680
going to introduce each panelist and

40
00:01:17,680 --> 00:01:19,040
then hand it off to them for their short

41
00:01:19,040 --> 00:01:19,920
talks

42
00:01:19,920 --> 00:01:22,479
so first we're going to have silky carlo

43
00:01:22,479 --> 00:01:24,159
who's the executive director of big

44
00:01:24,159 --> 00:01:27,680
brother watch uk uh clarence oko who's

45
00:01:27,680 --> 00:01:30,400
the civil rights legal fellow at naacp

46
00:01:30,400 --> 00:01:32,640
legal defense fund and dr nakima

47
00:01:32,640 --> 00:01:35,600
steffelbauer who's the ceo at frauntloop

48
00:01:35,600 --> 00:01:36,320
so

49
00:01:36,320 --> 00:01:39,040
for now i will hand it off to silky to

50
00:01:39,040 --> 00:01:41,840
present briefly

51
00:01:42,560 --> 00:01:45,119
great thank you so much ben and it's

52
00:01:45,119 --> 00:01:46,960
wonderful to be

53
00:01:46,960 --> 00:01:48,720
in this session with you and with

54
00:01:48,720 --> 00:01:51,040
clarence and the chemo also it's a real

55
00:01:51,040 --> 00:01:52,720
it's a real pleasure

56
00:01:52,720 --> 00:01:55,840
um i want to talk about some work that

57
00:01:55,840 --> 00:01:57,119
we've been doing in

58
00:01:57,119 --> 00:01:58,240
the uk

59
00:01:58,240 --> 00:02:01,520
uh on a particular algorithm in criminal

60
00:02:01,520 --> 00:02:04,719
justice um at the outset actually i just

61
00:02:04,719 --> 00:02:06,840
want to check if i have slides coming

62
00:02:06,840 --> 00:02:08,560
through uh

63
00:02:08,560 --> 00:02:11,280
hopefully so because then i can make

64
00:02:11,280 --> 00:02:12,879
reference

65
00:02:12,879 --> 00:02:14,400
just to a couple of pictures but it's

66
00:02:14,400 --> 00:02:16,160
actually not essential

67
00:02:16,160 --> 00:02:17,280
but if they're coming through that's

68
00:02:17,280 --> 00:02:18,480
great

69
00:02:18,480 --> 00:02:19,360
um

70
00:02:19,360 --> 00:02:22,000
meanwhile just let you know uh i'm the

71
00:02:22,000 --> 00:02:23,360
director of big brother watch big

72
00:02:23,360 --> 00:02:25,120
brother watch is a cross-party

73
00:02:25,120 --> 00:02:27,440
non-profit campaign organization

74
00:02:27,440 --> 00:02:30,480
um in the uk we work to protect rights

75
00:02:30,480 --> 00:02:32,080
and liberties particularly in the

76
00:02:32,080 --> 00:02:33,519
context of

77
00:02:33,519 --> 00:02:36,560
growing technological change

78
00:02:36,560 --> 00:02:38,800
um so the

79
00:02:38,800 --> 00:02:41,200
algorithm that i want to talk about uh

80
00:02:41,200 --> 00:02:43,200
briefly in the time that i have is an

81
00:02:43,200 --> 00:02:45,519
interesting one because um

82
00:02:45,519 --> 00:02:47,200
you will see multi

83
00:02:47,200 --> 00:02:49,360
the the multi-layered

84
00:02:49,360 --> 00:02:51,360
possibilities for harm and

85
00:02:51,360 --> 00:02:52,959
discrimination

86
00:02:52,959 --> 00:02:54,480
in this tool

87
00:02:54,480 --> 00:02:56,239
which is

88
00:02:56,239 --> 00:02:57,440
what you're going to come on to first of

89
00:02:57,440 --> 00:02:59,519
all i just want to show you um so my

90
00:02:59,519 --> 00:03:02,560
opening slide i've taken a screenshot

91
00:03:02,560 --> 00:03:04,879
here from a film called coded bias which

92
00:03:04,879 --> 00:03:07,280
is a documentary um that i really

93
00:03:07,280 --> 00:03:09,040
recommend you see it features some of

94
00:03:09,040 --> 00:03:11,760
your brother watch's work um

95
00:03:11,760 --> 00:03:13,200
monitoring and

96
00:03:13,200 --> 00:03:14,720
fighting police use of facial

97
00:03:14,720 --> 00:03:16,000
recognition

98
00:03:16,000 --> 00:03:20,159
and in this photo um

99
00:03:20,159 --> 00:03:22,080
it kind of almost looks like an and on

100
00:03:22,080 --> 00:03:24,560
the ground it looks like a bunch of men

101
00:03:24,560 --> 00:03:26,640
just jumped this poor kid

102
00:03:26,640 --> 00:03:29,519
um but actually they are plain clothed

103
00:03:29,519 --> 00:03:30,879
police officers

104
00:03:30,879 --> 00:03:34,799
um and the boy on the far left

105
00:03:34,799 --> 00:03:36,879
whose face is blurred out

106
00:03:36,879 --> 00:03:39,519
was misidentified by police facial

107
00:03:39,519 --> 00:03:41,440
facial recognition the police there

108
00:03:41,440 --> 00:03:42,799
actually trying to take a fingerprint

109
00:03:42,799 --> 00:03:44,959
from him he was completely innocent

110
00:03:44,959 --> 00:03:48,560
schoolboy aged 14 years old

111
00:03:48,560 --> 00:03:51,680
and that was a really horrendous uh

112
00:03:51,680 --> 00:03:52,959
incident i'm sure we're going to talk

113
00:03:52,959 --> 00:03:54,879
more more about facial recognition so

114
00:03:54,879 --> 00:03:56,879
i'm not going to talk too much about

115
00:03:56,879 --> 00:03:57,920
that

116
00:03:57,920 --> 00:04:00,159
um if you could go to the next slide

117
00:04:00,159 --> 00:04:01,680
please

118
00:04:01,680 --> 00:04:03,360
so this is what i want to talk about

119
00:04:03,360 --> 00:04:05,599
it's a system called heart

120
00:04:05,599 --> 00:04:08,400
which is the harm assessment risk tool

121
00:04:08,400 --> 00:04:10,400
it was used by a police force in the uk

122
00:04:10,400 --> 00:04:12,879
by durham police

123
00:04:12,879 --> 00:04:13,680
which

124
00:04:13,680 --> 00:04:15,680
is an in-house machine learning

125
00:04:15,680 --> 00:04:16,880
algorithm

126
00:04:16,880 --> 00:04:19,918
that profiles suspects so this is before

127
00:04:19,918 --> 00:04:21,440
anyone has been

128
00:04:21,440 --> 00:04:22,240
uh

129
00:04:22,240 --> 00:04:23,919
charged certainly not convicted of any

130
00:04:23,919 --> 00:04:25,520
crime whatsoever

131
00:04:25,520 --> 00:04:28,400
um it profiles suspects to predict their

132
00:04:28,400 --> 00:04:30,400
risk of reoffending

133
00:04:30,400 --> 00:04:31,919
this is before you know if they even

134
00:04:31,919 --> 00:04:33,600
have offended

135
00:04:33,600 --> 00:04:36,720
um it's an it is an ai generated risk

136
00:04:36,720 --> 00:04:37,919
score

137
00:04:37,919 --> 00:04:40,240
that's used to advise whether uh the

138
00:04:40,240 --> 00:04:43,120
police should charge the suspect or

139
00:04:43,120 --> 00:04:45,600
release them onto her rehabilitation

140
00:04:45,600 --> 00:04:46,639
program

141
00:04:46,639 --> 00:04:48,080
so this is

142
00:04:48,080 --> 00:04:50,720
um an ai tool that's involved in making

143
00:04:50,720 --> 00:04:52,400
clearly what could be quite a

144
00:04:52,400 --> 00:04:54,479
life-changing decision for an individual

145
00:04:54,479 --> 00:04:55,520
whether they're going to be put through

146
00:04:55,520 --> 00:04:57,040
the criminal justice system or whether

147
00:04:57,040 --> 00:04:58,880
they're going to receive support and

148
00:04:58,880 --> 00:05:00,560
that's based on how likely they think it

149
00:05:00,560 --> 00:05:02,800
is that this person is going to reoffend

150
00:05:02,800 --> 00:05:04,639
before they've even been convicted of

151
00:05:04,639 --> 00:05:06,800
anything whatsoever

152
00:05:06,800 --> 00:05:07,840
we

153
00:05:07,840 --> 00:05:09,919
there was not much publicity around this

154
00:05:09,919 --> 00:05:12,720
we investigated it and found that there

155
00:05:12,720 --> 00:05:15,520
were two variables going into the system

156
00:05:15,520 --> 00:05:18,000
that were both postcode variables

157
00:05:18,000 --> 00:05:20,639
one was a straightforward uh half of

158
00:05:20,639 --> 00:05:24,800
that person's postcode and the other um

159
00:05:24,800 --> 00:05:28,320
is a postcode identifier based on

160
00:05:28,320 --> 00:05:30,720
commercial marketing data

161
00:05:30,720 --> 00:05:33,440
from experian the big credit scoring

162
00:05:33,440 --> 00:05:34,800
agency

163
00:05:34,800 --> 00:05:36,720
and they have a

164
00:05:36,720 --> 00:05:38,479
geodemographic

165
00:05:38,479 --> 00:05:40,320
segmentation tool

166
00:05:40,320 --> 00:05:41,919
known as mosaic

167
00:05:41,919 --> 00:05:43,919
which is uh for anyone who isn't

168
00:05:43,919 --> 00:05:46,160
familiar with this it's a

169
00:05:46,160 --> 00:05:48,639
very prolonged way of saying

170
00:05:48,639 --> 00:05:51,520
stereotyping based on your postcode

171
00:05:51,520 --> 00:05:53,520
and i think all of us can imagine

172
00:05:53,520 --> 00:05:55,440
or based on our own postcodes what kind

173
00:05:55,440 --> 00:05:57,120
of assumptions might be made and quite

174
00:05:57,120 --> 00:05:59,840
often those assumptions would be wrong

175
00:05:59,840 --> 00:06:02,880
and even if they're right

176
00:06:02,880 --> 00:06:04,639
the product of what happens in this

177
00:06:04,639 --> 00:06:08,960
system is is clearly problematic

178
00:06:09,440 --> 00:06:11,840
so mosaic consists of postcode

179
00:06:11,840 --> 00:06:13,479
stereotypes built on

180
00:06:13,479 --> 00:06:17,199
850 million pieces of data

181
00:06:17,199 --> 00:06:20,080
and that data spans health data exam

182
00:06:20,080 --> 00:06:23,680
results child benefits income support

183
00:06:23,680 --> 00:06:27,520
names names are scanned for

184
00:06:27,600 --> 00:06:29,919
the likelihood that they relate to

185
00:06:29,919 --> 00:06:31,759
ethnicities

186
00:06:31,759 --> 00:06:34,880
data script from online sources

187
00:06:34,880 --> 00:06:37,120
and an awful lot more and the tool is

188
00:06:37,120 --> 00:06:40,080
used to profile all 50 million adults in

189
00:06:40,080 --> 00:06:43,680
the uk into each of these stereotypes

190
00:06:43,680 --> 00:06:47,800
could you go to the next slide please

191
00:06:48,800 --> 00:06:50,080
some of these stereotypes are

192
00:06:50,080 --> 00:06:52,800
particularly sorry it's very small but

193
00:06:52,800 --> 00:06:54,160
i'm going to explain

194
00:06:54,160 --> 00:06:55,599
um some of these

195
00:06:55,599 --> 00:06:58,319
stereotypes are incredibly crude i mean

196
00:06:58,319 --> 00:07:00,840
they all are but some are just

197
00:07:00,840 --> 00:07:03,360
ridiculous um and bear in mind again

198
00:07:03,360 --> 00:07:05,840
this is based on postcode so they built

199
00:07:05,840 --> 00:07:08,160
these profiles for example one was

200
00:07:08,160 --> 00:07:10,479
called asian heritage

201
00:07:10,479 --> 00:07:13,759
another was called disconnected youth

202
00:07:13,759 --> 00:07:15,440
since our investigation some of those

203
00:07:15,440 --> 00:07:17,759
names have now been changed but they're

204
00:07:17,759 --> 00:07:19,599
just euphemisms for the same kind of

205
00:07:19,599 --> 00:07:21,199
stereotype

206
00:07:21,199 --> 00:07:24,160
and so experian attributes democrat

207
00:07:24,160 --> 00:07:26,080
demographic characteristics to each

208
00:07:26,080 --> 00:07:29,520
stereotype so for example

209
00:07:29,520 --> 00:07:32,400
uh their description of asian heritage

210
00:07:32,400 --> 00:07:34,639
is uh people who live in extended

211
00:07:34,639 --> 00:07:37,440
families in inexpensive i'm

212
00:07:37,440 --> 00:07:40,319
quoting here inexpensive closed-packed

213
00:07:40,319 --> 00:07:42,560
victorian terraces

214
00:07:42,560 --> 00:07:44,720
and experience says that when these

215
00:07:44,720 --> 00:07:47,360
people do have jobs they are generally

216
00:07:47,360 --> 00:07:50,000
in low-paid routine occupations in

217
00:07:50,000 --> 00:07:52,720
transport or food service

218
00:07:52,720 --> 00:07:54,560
let me remind you this data is going

219
00:07:54,560 --> 00:07:56,000
into deciding whether someone is going

220
00:07:56,000 --> 00:07:57,440
to go through the criminal justice

221
00:07:57,440 --> 00:07:59,440
system or not

222
00:07:59,440 --> 00:08:02,639
incredibly serious so it raises new uh

223
00:08:02,639 --> 00:08:04,960
could you go to the next slide please

224
00:08:04,960 --> 00:08:08,319
it raises new questions um about big

225
00:08:08,319 --> 00:08:12,000
data and and privacy this is a their

226
00:08:12,000 --> 00:08:14,800
their uh long explanation of what asian

227
00:08:14,800 --> 00:08:16,800
heritage is in terms of a postcode

228
00:08:16,800 --> 00:08:19,520
variable um

229
00:08:19,520 --> 00:08:21,759
this really raises big questions not

230
00:08:21,759 --> 00:08:23,280
only about algorithms in criminal

231
00:08:23,280 --> 00:08:24,400
justice

232
00:08:24,400 --> 00:08:27,039
but about how

233
00:08:27,039 --> 00:08:30,720
the accumulation of big data can go into

234
00:08:30,720 --> 00:08:32,559
building a system like this that you

235
00:08:32,559 --> 00:08:34,000
actually even have

236
00:08:34,000 --> 00:08:34,880
um

237
00:08:34,880 --> 00:08:37,519
on sale these kinds of profiling tools

238
00:08:37,519 --> 00:08:39,519
in the first place that then authorities

239
00:08:39,519 --> 00:08:42,719
can use to kind of retroactively justify

240
00:08:42,719 --> 00:08:45,440
some of their practices and the types of

241
00:08:45,440 --> 00:08:46,880
people that they target and the way that

242
00:08:46,880 --> 00:08:49,200
they they treat those people

243
00:08:49,200 --> 00:08:50,880
um and i think this this is a

244
00:08:50,880 --> 00:08:52,160
particularly interesting case study

245
00:08:52,160 --> 00:08:53,680
because it's indicative of the way that

246
00:08:53,680 --> 00:08:55,920
authorities in general in the uk

247
00:08:55,920 --> 00:08:58,080
are approaching new technologies

248
00:08:58,080 --> 00:08:59,360
algorithms

249
00:08:59,360 --> 00:09:00,800
for decision making and artificial

250
00:09:00,800 --> 00:09:02,240
intelligence

251
00:09:02,240 --> 00:09:03,519
could you just go to the next slide

252
00:09:03,519 --> 00:09:07,279
please and i know i have to wrap up

253
00:09:07,600 --> 00:09:09,519
again this is very small this this is my

254
00:09:09,519 --> 00:09:10,560
attempt

255
00:09:10,560 --> 00:09:13,600
a draft model um to try and understand

256
00:09:13,600 --> 00:09:16,080
um the kind of

257
00:09:16,080 --> 00:09:16,880
the

258
00:09:16,880 --> 00:09:17,920
uh

259
00:09:17,920 --> 00:09:20,080
impact of algorithmic decision making

260
00:09:20,080 --> 00:09:21,839
and how the harms relate to one another

261
00:09:21,839 --> 00:09:24,000
so i've got data privacy at the center

262
00:09:24,000 --> 00:09:25,519
with all of these systems you see that

263
00:09:25,519 --> 00:09:26,880
it's the loss of privacy and the

264
00:09:26,880 --> 00:09:30,160
accumulation of often granular data on

265
00:09:30,160 --> 00:09:33,839
all of us that enables subsequent harms

266
00:09:33,839 --> 00:09:35,440
and that affects civil and political

267
00:09:35,440 --> 00:09:37,360
rights socio-economic rights i'm sure

268
00:09:37,360 --> 00:09:38,240
we'll hear more about some of the

269
00:09:38,240 --> 00:09:41,440
welfare systems and algorithms um and

270
00:09:41,440 --> 00:09:43,839
discrimination really wraps around all

271
00:09:43,839 --> 00:09:46,320
of this um you can find it present at

272
00:09:46,320 --> 00:09:48,240
every stage of the process

273
00:09:48,240 --> 00:09:48,839
and

274
00:09:48,839 --> 00:09:51,760
ultimately it's democratic processes and

275
00:09:51,760 --> 00:09:53,920
also systems of justice

276
00:09:53,920 --> 00:09:55,440
that suffer

277
00:09:55,440 --> 00:09:58,240
um i think i'm out of time but uh thanks

278
00:09:58,240 --> 00:10:00,080
for i hope that

279
00:10:00,080 --> 00:10:02,800
uh whistle stop tour through the heart

280
00:10:02,800 --> 00:10:05,040
system uh was was clear enough and i

281
00:10:05,040 --> 00:10:06,240
look forward to continuing the

282
00:10:06,240 --> 00:10:08,800
conversation

283
00:10:09,440 --> 00:10:10,480
thank you

284
00:10:10,480 --> 00:10:13,279
um it was really helpful um and we're

285
00:10:13,279 --> 00:10:15,200
gonna hand it off to clarence to to

286
00:10:15,200 --> 00:10:16,880
explain a few other systems and and

287
00:10:16,880 --> 00:10:20,640
continue the sort of table setting here

288
00:10:20,880 --> 00:10:22,320
hey good morning everybody can you all

289
00:10:22,320 --> 00:10:22,750
hear me

290
00:10:22,750 --> 00:10:24,560
[Laughter]

291
00:10:24,560 --> 00:10:27,360
great so my name is clarence unko uh ucm

292
00:10:27,360 --> 00:10:29,440
pronouns i am an equal justice works

293
00:10:29,440 --> 00:10:31,839
fellow here at the naacp legal defense

294
00:10:31,839 --> 00:10:34,240
fund where i lead a project uh that

295
00:10:34,240 --> 00:10:36,480
challenges the racialized consequences

296
00:10:36,480 --> 00:10:38,000
of emerging technology such as

297
00:10:38,000 --> 00:10:40,160
artificial intelligence machine learning

298
00:10:40,160 --> 00:10:42,160
on the civil and human rights of black

299
00:10:42,160 --> 00:10:43,440
and brown communities in the united

300
00:10:43,440 --> 00:10:45,920
states uh for those of you who who may

301
00:10:45,920 --> 00:10:48,399
not be familiar uh ldf is one of the

302
00:10:48,399 --> 00:10:49,760
nation's oldest civil rights

303
00:10:49,760 --> 00:10:52,160
organizations we pursue litigation and

304
00:10:52,160 --> 00:10:54,320
policy advocacy to advance racial

305
00:10:54,320 --> 00:10:56,560
justice in the united states and in

306
00:10:56,560 --> 00:10:58,640
recent years we have uh developed deep

307
00:10:58,640 --> 00:11:01,120
concerns about the emergence of these

308
00:11:01,120 --> 00:11:02,720
technologies and their implications on

309
00:11:02,720 --> 00:11:04,480
the rights of the communities that we

310
00:11:04,480 --> 00:11:05,680
serve

311
00:11:05,680 --> 00:11:07,600
we're in particular concerned about the

312
00:11:07,600 --> 00:11:10,079
development of surveillance

313
00:11:10,079 --> 00:11:11,920
technologies by the use by law

314
00:11:11,920 --> 00:11:13,519
enforcement as well as other car school

315
00:11:13,519 --> 00:11:15,600
technologies that are deployed

316
00:11:15,600 --> 00:11:16,959
again to burden the rights of our

317
00:11:16,959 --> 00:11:19,440
communities uh so in collaboration with

318
00:11:19,440 --> 00:11:21,680
a number of advocates activist attorneys

319
00:11:21,680 --> 00:11:23,200
we have been uh challenging these

320
00:11:23,200 --> 00:11:24,480
practices across a number of

321
00:11:24,480 --> 00:11:26,880
jurisdictions in the united states and

322
00:11:26,880 --> 00:11:28,800
in each instance we try to kind of blend

323
00:11:28,800 --> 00:11:29,920
together

324
00:11:29,920 --> 00:11:32,000
interdisciplinary advocacy strategies so

325
00:11:32,000 --> 00:11:34,240
relying on litigation policy advocacy

326
00:11:34,240 --> 00:11:36,720
and organizing strategies uh to advance

327
00:11:36,720 --> 00:11:37,680
our work

328
00:11:37,680 --> 00:11:40,399
so i want to spend uh just a few seconds

329
00:11:40,399 --> 00:11:42,240
walking through a couple of examples

330
00:11:42,240 --> 00:11:43,760
from some of the work that we've been

331
00:11:43,760 --> 00:11:46,640
doing in recent years related to both

332
00:11:46,640 --> 00:11:48,240
partial technologies as well as

333
00:11:48,240 --> 00:11:51,279
algorithm discrimination um

334
00:11:51,279 --> 00:11:53,040
i actually don't have to do that much of

335
00:11:53,040 --> 00:11:53,760
a

336
00:11:53,760 --> 00:11:55,760
detailed analysis because essentially

337
00:11:55,760 --> 00:11:57,200
everything that silky described is

338
00:11:57,200 --> 00:11:59,519
happening in the uk i was almost shocked

339
00:11:59,519 --> 00:12:01,120
like the the actual graphs were very

340
00:12:01,120 --> 00:12:03,360
similar to what we've seen in some of

341
00:12:03,360 --> 00:12:04,800
our work so i'll just grab some of those

342
00:12:04,800 --> 00:12:06,959
cases briefly but uh at the onset i just

343
00:12:06,959 --> 00:12:08,000
want to make a couple of quick

344
00:12:08,000 --> 00:12:09,680
observations i think are really

345
00:12:09,680 --> 00:12:12,320
important to ground the conversation um

346
00:12:12,320 --> 00:12:13,839
the first is that the community level

347
00:12:13,839 --> 00:12:16,240
consequences of carson technologies are

348
00:12:16,240 --> 00:12:18,399
important and that we can't lose focus

349
00:12:18,399 --> 00:12:20,720
uh on those consequences so oftentimes

350
00:12:20,720 --> 00:12:23,360
we discuss um the impact of carson

351
00:12:23,360 --> 00:12:25,680
technologies we focus on the threat to

352
00:12:25,680 --> 00:12:27,760
particular individuals right the threat

353
00:12:27,760 --> 00:12:29,440
of misidentification from a racial

354
00:12:29,440 --> 00:12:31,760
recognition system or the individualized

355
00:12:31,760 --> 00:12:33,680
consequences of digital profiling for a

356
00:12:33,680 --> 00:12:35,920
predictive policing system but it's also

357
00:12:35,920 --> 00:12:37,760
important to understand that when you

358
00:12:37,760 --> 00:12:39,680
aggregate those experiences there are

359
00:12:39,680 --> 00:12:42,000
unique harms that can be surfaced right

360
00:12:42,000 --> 00:12:43,839
when you look at the kind of impact of

361
00:12:43,839 --> 00:12:46,079
those individualized harms at scale and

362
00:12:46,079 --> 00:12:48,399
so in particular we're concerned about

363
00:12:48,399 --> 00:12:50,160
this development of surveillance

364
00:12:50,160 --> 00:12:53,120
redlining that this kind of layering on

365
00:12:53,120 --> 00:12:54,880
the police surveillance technologies

366
00:12:54,880 --> 00:12:56,399
within black and brown communities in

367
00:12:56,399 --> 00:12:58,720
the united states essentially operates

368
00:12:58,720 --> 00:13:00,320
as a form of containment

369
00:13:00,320 --> 00:13:02,720
it directs the increased interactions

370
00:13:02,720 --> 00:13:04,639
between law enforcement and communities

371
00:13:04,639 --> 00:13:07,120
of color uh that reinforces historic

372
00:13:07,120 --> 00:13:08,880
patterns that can lead to those

373
00:13:08,880 --> 00:13:11,680
communities being spatially isolated uh

374
00:13:11,680 --> 00:13:13,120
and that is a really important

375
00:13:13,120 --> 00:13:14,800
development that i think we want to kind

376
00:13:14,800 --> 00:13:16,320
of keep in mind as we're thinking about

377
00:13:16,320 --> 00:13:18,720
these technologies um

378
00:13:18,720 --> 00:13:20,399
obviously the second observation is that

379
00:13:20,399 --> 00:13:22,639
these harms are differentiated by race

380
00:13:22,639 --> 00:13:24,560
um and that these technologies are often

381
00:13:24,560 --> 00:13:27,519
designed to pursue racial ends even if

382
00:13:27,519 --> 00:13:29,760
they're not intentional again you know

383
00:13:29,760 --> 00:13:31,600
technologies that reinforce the status

384
00:13:31,600 --> 00:13:34,160
quo are building upon histories of

385
00:13:34,160 --> 00:13:35,839
racial inequality and injustice in the

386
00:13:35,839 --> 00:13:38,720
united states there was a um

387
00:13:38,720 --> 00:13:41,040
a uh study conducted by buzzfeed that

388
00:13:41,040 --> 00:13:42,959
examined the prevalence of a certain

389
00:13:42,959 --> 00:13:45,199
facial recognition platform being used

390
00:13:45,199 --> 00:13:46,480
by law enforcement i think there were

391
00:13:46,480 --> 00:13:48,480
over 2 000 agencies found

392
00:13:48,480 --> 00:13:51,440
um to have used the technology and when

393
00:13:51,440 --> 00:13:53,760
we kind of did a quick analysis we found

394
00:13:53,760 --> 00:13:55,440
that almost every jurisdiction that has

395
00:13:55,440 --> 00:13:57,839
had a pattern and practice investigation

396
00:13:57,839 --> 00:13:59,839
by the department of justice

397
00:13:59,839 --> 00:14:00,639
for

398
00:14:00,639 --> 00:14:02,399
systemic civil rights and constitutional

399
00:14:02,399 --> 00:14:03,680
violations

400
00:14:03,680 --> 00:14:05,440
they have been using facial recognition

401
00:14:05,440 --> 00:14:07,839
technology oftentimes unauthorized by

402
00:14:07,839 --> 00:14:09,760
the locality that they work in

403
00:14:09,760 --> 00:14:11,360
and finally

404
00:14:11,360 --> 00:14:12,880
the two examples i'm going to provide

405
00:14:12,880 --> 00:14:14,480
the first kind of looks at a predictive

406
00:14:14,480 --> 00:14:16,560
policing system in florida the second

407
00:14:16,560 --> 00:14:19,040
looks at a fintech lending platform that

408
00:14:19,040 --> 00:14:20,240
uses

409
00:14:20,240 --> 00:14:22,399
education related data

410
00:14:22,399 --> 00:14:24,079
to feed a machine learning system to

411
00:14:24,079 --> 00:14:26,480
price loans for applicants

412
00:14:26,480 --> 00:14:28,320
these two examples may feel disparate

413
00:14:28,320 --> 00:14:30,079
but what i want to suggest is that the

414
00:14:30,079 --> 00:14:32,399
common link between the two is that both

415
00:14:32,399 --> 00:14:35,600
of them kind of reinforce this um

416
00:14:35,600 --> 00:14:37,519
the status of second-class citizenship

417
00:14:37,519 --> 00:14:39,040
for black and brown folks that we kind

418
00:14:39,040 --> 00:14:41,600
of see these technologies doing uh the

419
00:14:41,600 --> 00:14:43,760
work that we saw in the early

420
00:14:43,760 --> 00:14:45,120
20th century in the united states that

421
00:14:45,120 --> 00:14:46,639
we're seeing this kind of re-emergence

422
00:14:46,639 --> 00:14:49,519
of uh two different systems uh through

423
00:14:49,519 --> 00:14:50,800
which people are able to access

424
00:14:50,800 --> 00:14:52,240
opportunity two different systems

425
00:14:52,240 --> 00:14:54,480
through which people are able to access

426
00:14:54,480 --> 00:14:57,199
benefits through which their citizenship

427
00:14:57,199 --> 00:14:59,680
is being experienced so um really

428
00:14:59,680 --> 00:15:01,600
quickly on the two examples the first is

429
00:15:01,600 --> 00:15:03,760
a pasco county it's a county down in

430
00:15:03,760 --> 00:15:05,040
florida

431
00:15:05,040 --> 00:15:06,959
where a tampa bay times investigative

432
00:15:06,959 --> 00:15:09,120
report found that the local sheriff's

433
00:15:09,120 --> 00:15:11,120
office was working in collaboration with

434
00:15:11,120 --> 00:15:12,720
the school district

435
00:15:12,720 --> 00:15:15,680
to operate a predictive policing program

436
00:15:15,680 --> 00:15:17,199
the way that the system works that

437
00:15:17,199 --> 00:15:19,199
there's a data sharing agreement whereby

438
00:15:19,199 --> 00:15:21,600
the school district provides access to

439
00:15:21,600 --> 00:15:23,360
confidential student records including a

440
00:15:23,360 --> 00:15:25,199
student's grades their discipline

441
00:15:25,199 --> 00:15:27,440
records their attendance records and

442
00:15:27,440 --> 00:15:30,160
others they grant those documents over

443
00:15:30,160 --> 00:15:32,800
to the pasco county sheriff's office the

444
00:15:32,800 --> 00:15:35,120
sheriff then aggregates those data and

445
00:15:35,120 --> 00:15:36,639
feeds them through a predictive policy

446
00:15:36,639 --> 00:15:38,240
excuse me a predictive policing

447
00:15:38,240 --> 00:15:39,759
algorithm

448
00:15:39,759 --> 00:15:41,279
scores students

449
00:15:41,279 --> 00:15:43,040
based off of all those different

450
00:15:43,040 --> 00:15:44,560
criteria that i mentioned and labels

451
00:15:44,560 --> 00:15:46,959
them as either at risk off excuse me at

452
00:15:46,959 --> 00:15:49,040
risk on track or off track

453
00:15:49,040 --> 00:15:50,480
and the goal of the program is to try

454
00:15:50,480 --> 00:15:52,639
and identify students who the sheriff's

455
00:15:52,639 --> 00:15:55,120
office believes uh is at risk of

456
00:15:55,120 --> 00:15:56,720
developing into prolific criminal

457
00:15:56,720 --> 00:15:57,839
offenders

458
00:15:57,839 --> 00:15:58,639
um

459
00:15:58,639 --> 00:16:01,120
based off of our kind of cursory um

460
00:16:01,120 --> 00:16:02,880
honesty cursing based off of our work

461
00:16:02,880 --> 00:16:04,000
that we've been doing over the last year

462
00:16:04,000 --> 00:16:08,320
and a half uh in pasco we have um we've

463
00:16:08,320 --> 00:16:10,880
arrived at is a concern that the

464
00:16:10,880 --> 00:16:12,639
practices that they are engaged in down

465
00:16:12,639 --> 00:16:14,720
in pasco are having a disparate impact

466
00:16:14,720 --> 00:16:16,880
on black and brown youth uh low-income

467
00:16:16,880 --> 00:16:18,560
youth and youth with disabilities and

468
00:16:18,560 --> 00:16:21,040
often that is driven by the fact

469
00:16:21,040 --> 00:16:22,399
that the data that they're relying

470
00:16:22,399 --> 00:16:24,240
abroad are all racially biased and

471
00:16:24,240 --> 00:16:25,199
skewed

472
00:16:25,199 --> 00:16:26,160
um

473
00:16:26,160 --> 00:16:28,079
so that's what's happening in pasco and

474
00:16:28,079 --> 00:16:29,680
i'm happy to provide additional details

475
00:16:29,680 --> 00:16:31,360
if we kind of continue the conversation

476
00:16:31,360 --> 00:16:33,440
really quickly on upstart upside as i

477
00:16:33,440 --> 00:16:35,040
mentioned before is a fintech company

478
00:16:35,040 --> 00:16:38,000
that provides loan services um they they

479
00:16:38,000 --> 00:16:41,120
do so by relying on alternative data

480
00:16:41,120 --> 00:16:43,519
so non-traditional criteria to determine

481
00:16:43,519 --> 00:16:46,240
credit worthiness um and they do that

482
00:16:46,240 --> 00:16:47,839
they feed that data into their

483
00:16:47,839 --> 00:16:50,399
proprietary machine learning system

484
00:16:50,399 --> 00:16:52,079
and they used the combination of the two

485
00:16:52,079 --> 00:16:54,160
to actually price loans we had a partner

486
00:16:54,160 --> 00:16:55,440
organization the student borrower

487
00:16:55,440 --> 00:16:57,839
protection center that ran an analysis

488
00:16:57,839 --> 00:17:00,800
of uh start's lending platform and found

489
00:17:00,800 --> 00:17:03,680
that there were deep concerns uh related

490
00:17:03,680 --> 00:17:05,760
to the ways in which the lending

491
00:17:05,760 --> 00:17:07,679
platform was treating students who

492
00:17:07,679 --> 00:17:09,359
attended historically black colleges and

493
00:17:09,359 --> 00:17:11,039
universities versus students who were

494
00:17:11,039 --> 00:17:12,079
attending predominantly white

495
00:17:12,079 --> 00:17:14,799
institutions uh over the course of the

496
00:17:14,799 --> 00:17:16,559
life of a loan uh students who were

497
00:17:16,559 --> 00:17:18,480
attending hbcus could be charged

498
00:17:18,480 --> 00:17:20,160
hundreds if not thousands of more

499
00:17:20,160 --> 00:17:22,640
dollars um than uh students were

500
00:17:22,640 --> 00:17:24,880
attending pwis so what we were able to

501
00:17:24,880 --> 00:17:26,640
do is enter into a two-year monitoring

502
00:17:26,640 --> 00:17:29,440
agreement with upstart um to essentially

503
00:17:29,440 --> 00:17:31,200
conduct a kind of fair lending analysis

504
00:17:31,200 --> 00:17:33,440
of their algorithm to understand have a

505
00:17:33,440 --> 00:17:34,720
better understanding about the fair

506
00:17:34,720 --> 00:17:36,799
lending applications of their platform

507
00:17:36,799 --> 00:17:39,360
uh but i will stop there um because i

508
00:17:39,360 --> 00:17:40,960
know i'm probably over the five limit

509
00:17:40,960 --> 00:17:43,039
i'm in the limit and i will turn it over

510
00:17:43,039 --> 00:17:46,200
to mckinney

511
00:17:50,000 --> 00:17:53,360
okay thank you i'm not sure if my slide

512
00:17:53,360 --> 00:17:54,960
is going to be

513
00:17:54,960 --> 00:17:58,160
presented it is great if it's not

514
00:17:58,160 --> 00:18:00,960
that's also fine and i'll try to be

515
00:18:00,960 --> 00:18:02,880
mindful of the time as i quickly go

516
00:18:02,880 --> 00:18:04,799
through the points that i wanted to

517
00:18:04,799 --> 00:18:06,000
uh cover

518
00:18:06,000 --> 00:18:09,840
um first i'm nikki mustafa i am a

519
00:18:09,840 --> 00:18:12,559
digital transformation

520
00:18:12,559 --> 00:18:14,559
executive or survivor

521
00:18:14,559 --> 00:18:16,640
depending on how you look at it i'm also

522
00:18:16,640 --> 00:18:20,080
the founder and um ceo of fraundu which

523
00:18:20,080 --> 00:18:22,880
is a non-profit computer programming

524
00:18:22,880 --> 00:18:23,919
school

525
00:18:23,919 --> 00:18:24,720
for

526
00:18:24,720 --> 00:18:26,320
resident eu

527
00:18:26,320 --> 00:18:29,440
immigrant and refugee status women here

528
00:18:29,440 --> 00:18:31,200
in berlin

529
00:18:31,200 --> 00:18:32,320
so

530
00:18:32,320 --> 00:18:34,400
in addition to what silky and clarence

531
00:18:34,400 --> 00:18:38,160
have been discussing um one

532
00:18:38,160 --> 00:18:41,520
fact that there is a significant

533
00:18:41,520 --> 00:18:43,280
interest amongst law enforcement

534
00:18:43,280 --> 00:18:44,799
agencies to

535
00:18:44,799 --> 00:18:47,520
rely on these um automated decision

536
00:18:47,520 --> 00:18:50,480
making or performing risk assessments

537
00:18:50,480 --> 00:18:51,840
and determining

538
00:18:51,840 --> 00:18:54,480
uh what

539
00:18:54,480 --> 00:18:58,559
the circumstances which it acceptable to

540
00:18:58,559 --> 00:19:00,480
target and surveilled people but they're

541
00:19:00,480 --> 00:19:03,120
really doing grants on

542
00:19:03,120 --> 00:19:06,160
these algorithmic solutions outside of

543
00:19:06,160 --> 00:19:08,480
law enforcement with the same approach

544
00:19:08,480 --> 00:19:11,039
to human classification

545
00:19:11,039 --> 00:19:13,200
i wanted to give the examples

546
00:19:13,200 --> 00:19:15,039
that i list in the slide i'm going to

547
00:19:15,039 --> 00:19:16,480
start from the bottom

548
00:19:16,480 --> 00:19:20,000
because some of the more interesting

549
00:19:20,000 --> 00:19:23,039
ways that this same type of targeting is

550
00:19:23,039 --> 00:19:25,280
has been taking place include the

551
00:19:25,280 --> 00:19:29,600
education sector which as everyone has

552
00:19:29,600 --> 00:19:32,480
the pandemic and all the disruption that

553
00:19:32,480 --> 00:19:36,000
is created such that we have two really

554
00:19:36,000 --> 00:19:39,840
um striking examples from 2020 where

555
00:19:39,840 --> 00:19:42,640
there were high profile cases that had a

556
00:19:42,640 --> 00:19:45,039
sector private actor

557
00:19:45,039 --> 00:19:46,160
on

558
00:19:46,160 --> 00:19:48,320
automated decision making software for

559
00:19:48,320 --> 00:19:49,919
great assessments

560
00:19:49,919 --> 00:19:53,679
i'm speaking specifically the uk

561
00:19:53,679 --> 00:19:54,480
um

562
00:19:54,480 --> 00:19:56,000
higher education

563
00:19:56,000 --> 00:19:58,640
boards responsible for administering the

564
00:19:58,640 --> 00:20:00,559
a level believe

565
00:20:00,559 --> 00:20:03,039
they're relieving exemptions

566
00:20:03,039 --> 00:20:06,559
and the intentional glory

567
00:20:18,960 --> 00:20:20,640
all was

568
00:20:20,640 --> 00:20:24,000
actors who outsourced the traditional um

569
00:20:24,000 --> 00:20:25,200
[Music]

570
00:20:25,200 --> 00:20:28,159
great assessment process for final

571
00:20:28,159 --> 00:20:31,120
examinations of high schoolers to

572
00:20:31,120 --> 00:20:33,600
external parties that

573
00:20:33,600 --> 00:20:35,120
customized

574
00:20:35,120 --> 00:20:40,159
algorithms to rank and to issue grading

575
00:20:40,159 --> 00:20:44,400
decisions for students in 2020

576
00:20:44,400 --> 00:20:45,919
what it turned out

577
00:20:45,919 --> 00:20:48,400
that happened was that vast numbers of

578
00:20:48,400 --> 00:20:51,120
students received unexpected and

579
00:20:51,120 --> 00:20:53,039
disparate grades from their predicted

580
00:20:53,039 --> 00:20:54,080
scores

581
00:20:54,080 --> 00:20:55,120
and

582
00:20:55,120 --> 00:20:56,880
upon investigation it was discovered

583
00:20:56,880 --> 00:20:59,600
that the historical average performances

584
00:20:59,600 --> 00:21:02,159
of the school institutions which many of

585
00:21:02,159 --> 00:21:04,559
these students had attended was being

586
00:21:04,559 --> 00:21:07,440
used in both cases in the uk case as

587
00:21:07,440 --> 00:21:08,960
well as in the international

588
00:21:08,960 --> 00:21:11,520
baccalaureate case to

589
00:21:11,520 --> 00:21:14,480
issue grades that were more

590
00:21:14,480 --> 00:21:17,200
demanded

591
00:21:17,200 --> 00:21:18,960
grades from

592
00:21:18,960 --> 00:21:20,400
the district

593
00:21:20,400 --> 00:21:23,200
schools that students had been attending

594
00:21:23,200 --> 00:21:25,520
rather than their actual performance on

595
00:21:25,520 --> 00:21:26,960
the tests

596
00:21:26,960 --> 00:21:28,720
this led to

597
00:21:28,720 --> 00:21:32,400
thousands of students globally

598
00:21:33,280 --> 00:21:36,080
and students in the uk who sat for a

599
00:21:36,080 --> 00:21:39,520
level exams in 2020 either receiving

600
00:21:39,520 --> 00:21:41,440
grades that did not correspond to their

601
00:21:41,440 --> 00:21:43,120
achievement level

602
00:21:43,120 --> 00:21:44,640
or

603
00:21:44,640 --> 00:21:47,840
and or losing scholarships in cases

604
00:21:47,840 --> 00:21:49,440
where they've been offered university

605
00:21:49,440 --> 00:21:51,520
places that were dependent on receiving

606
00:21:51,520 --> 00:21:54,000
a certain score and in many european

607
00:21:54,000 --> 00:21:56,880
cases led to students being denied entry

608
00:21:56,880 --> 00:22:00,080
to a whole area of study that required

609
00:22:00,080 --> 00:22:02,960
certain grades to be achieved in these

610
00:22:02,960 --> 00:22:05,840
final exams

611
00:22:06,799 --> 00:22:11,360
in the uk case we chose to utilize a

612
00:22:11,360 --> 00:22:14,240
resources underperforming rules

613
00:22:14,240 --> 00:22:17,919
their scores on average

614
00:22:17,919 --> 00:22:19,360
students were

615
00:22:19,360 --> 00:22:22,080
well released

616
00:22:23,600 --> 00:22:26,240
this is a really good people one example

617
00:22:26,240 --> 00:22:30,159
of how critical algorithmic used scale

618
00:22:30,159 --> 00:22:30,960
is

619
00:22:30,960 --> 00:22:33,760
very often

620
00:22:38,880 --> 00:22:42,080
another example is with regard to added

621
00:22:42,080 --> 00:22:44,000
um

622
00:22:44,000 --> 00:22:46,000
and automatic decision making

623
00:22:46,000 --> 00:22:47,840
algorithms on

624
00:22:47,840 --> 00:22:49,360
sites

625
00:22:49,360 --> 00:22:52,320
i'm specifically interested because of

626
00:22:52,320 --> 00:22:53,600
transparent

627
00:22:53,600 --> 00:22:56,480
that is that a bounce when it comes to

628
00:22:56,480 --> 00:23:00,720
employment and algorithmic ranking

629
00:23:00,720 --> 00:23:02,080
software

630
00:23:02,080 --> 00:23:03,200
we don't know

631
00:23:03,200 --> 00:23:06,080
for example on any major

632
00:23:06,080 --> 00:23:08,799
job search professional networking

633
00:23:08,799 --> 00:23:10,640
platform you can think of

634
00:23:10,640 --> 00:23:12,880
we don't know how people are treated

635
00:23:12,880 --> 00:23:14,799
based on having gaps

636
00:23:14,799 --> 00:23:15,760
having

637
00:23:15,760 --> 00:23:17,280
incarceration

638
00:23:17,280 --> 00:23:19,360
having any number of types of

639
00:23:19,360 --> 00:23:22,960
information present on their resume of

640
00:23:22,960 --> 00:23:24,559
former employment

641
00:23:24,559 --> 00:23:27,120
and we also don't know what impact

642
00:23:27,120 --> 00:23:29,039
characteristics such as age

643
00:23:29,039 --> 00:23:31,919
characteristics such as photographs

644
00:23:31,919 --> 00:23:34,880
names that are in any way identifiable

645
00:23:34,880 --> 00:23:37,600
with race or ethnicity are treated by

646
00:23:37,600 --> 00:23:40,640
ranking algorithms and the

647
00:23:40,640 --> 00:23:43,360
buying software that is powering how

648
00:23:43,360 --> 00:23:45,679
these platforms work

649
00:23:45,679 --> 00:23:48,080
to them

650
00:23:48,080 --> 00:23:50,880
where you can go through and check

651
00:23:50,880 --> 00:23:54,799
to see how many of the other candidates

652
00:23:54,799 --> 00:23:55,919
on

653
00:23:55,919 --> 00:23:59,520
a particular class without a gp

654
00:23:59,520 --> 00:24:01,120
rank

655
00:24:01,120 --> 00:24:02,720
are working in

656
00:24:02,720 --> 00:24:06,400
gig work are working in

657
00:24:07,760 --> 00:24:10,640
precarious circumstances

658
00:24:10,640 --> 00:24:12,559
it's extremely interesting to think

659
00:24:12,559 --> 00:24:15,039
about the degree of information that we

660
00:24:15,039 --> 00:24:16,720
don't have

661
00:24:16,720 --> 00:24:19,200
and how that may affect our ranking and

662
00:24:19,200 --> 00:24:22,320
how employers and others who deal with

663
00:24:22,320 --> 00:24:25,200
these employment sites engage with that

664
00:24:25,200 --> 00:24:27,520
information and the rankings that may

665
00:24:27,520 --> 00:24:29,520
ensue based on

666
00:24:29,520 --> 00:24:32,000
of a wealth of information

667
00:24:32,000 --> 00:24:34,480
that we're not quite sure

668
00:24:34,480 --> 00:24:37,279
how is integrated into these ranking

669
00:24:37,279 --> 00:24:38,480
systems

670
00:24:38,480 --> 00:24:41,760
similarly to this type of hidden or

671
00:24:41,760 --> 00:24:44,240
in transparent let's say

672
00:24:44,240 --> 00:24:46,480
software there's the

673
00:24:46,480 --> 00:24:48,080
situation that i wanted to point out

674
00:24:48,080 --> 00:24:50,159
which is um

675
00:24:50,159 --> 00:24:52,880
stores and businesses in the commercial

676
00:24:52,880 --> 00:24:54,640
sense businesses are under just as much

677
00:24:54,640 --> 00:24:57,360
pressure as law enforcement is to rely

678
00:24:57,360 --> 00:24:59,919
more and more on algorithmic and

679
00:24:59,919 --> 00:25:03,039
analytic analysis of their customers

680
00:25:03,039 --> 00:25:06,559
what this has turned into in the case of

681
00:25:06,559 --> 00:25:08,880
um two companies that were featured in

682
00:25:08,880 --> 00:25:13,600
an expose article and wired last year um

683
00:25:13,600 --> 00:25:15,360
they're not only

684
00:25:15,360 --> 00:25:17,679
in store not not only online but also

685
00:25:17,679 --> 00:25:20,240
in-store analytics that are being sold

686
00:25:20,240 --> 00:25:24,400
and evidently are being um hyped as ways

687
00:25:24,400 --> 00:25:29,760
to understand who is shopping at your

688
00:25:29,760 --> 00:25:32,240
bricks and mortar locations

689
00:25:32,240 --> 00:25:34,080
by means of

690
00:25:34,080 --> 00:25:36,000
attribute

691
00:25:36,000 --> 00:25:37,520
estimation

692
00:25:37,520 --> 00:25:38,960
which involves

693
00:25:38,960 --> 00:25:41,120
algorithmic determination of everything

694
00:25:41,120 --> 00:25:45,520
age gender to emotions and also

695
00:25:45,520 --> 00:25:48,799
allegedly racial or ethnic identity

696
00:25:48,799 --> 00:25:50,799
with the view of the time i won't

697
00:25:50,799 --> 00:25:52,320
continue

698
00:25:52,320 --> 00:25:54,880
but i want to make the point that

699
00:25:54,880 --> 00:25:57,200
all of this to say we're playing

700
00:25:57,200 --> 00:25:58,480
whack-a-mole

701
00:25:58,480 --> 00:26:00,880
in some sense with different sectors

702
00:26:00,880 --> 00:26:03,279
that are using automated decision-making

703
00:26:03,279 --> 00:26:06,960
software to target to classify to get to

704
00:26:06,960 --> 00:26:09,039
know to understand

705
00:26:09,039 --> 00:26:11,919
different sectors of society

706
00:26:11,919 --> 00:26:15,440
and based on this generalized aggregated

707
00:26:15,440 --> 00:26:18,400
data that all of these organizations

708
00:26:18,400 --> 00:26:22,320
companies entities tend to use

709
00:26:22,320 --> 00:26:24,400
as clarence said earlier we're very

710
00:26:24,400 --> 00:26:26,400
often seeing reproduction

711
00:26:26,400 --> 00:26:28,559
of exclusionary and discriminatory

712
00:26:28,559 --> 00:26:32,480
practices uh from the past

713
00:26:32,480 --> 00:26:35,279
thank you nikki men and thank you to

714
00:26:35,279 --> 00:26:37,360
all of you for for really

715
00:26:37,360 --> 00:26:39,679
setting the table uh and explaining some

716
00:26:39,679 --> 00:26:41,120
of these systems giving some more

717
00:26:41,120 --> 00:26:44,320
in-depth examples um because it can be

718
00:26:44,320 --> 00:26:46,159
really difficult to

719
00:26:46,159 --> 00:26:48,000
you know adequately

720
00:26:48,000 --> 00:26:49,360
tell all the different stories of the

721
00:26:49,360 --> 00:26:50,720
systems we want to talk about and how

722
00:26:50,720 --> 00:26:52,559
they're connected but i think it's you

723
00:26:52,559 --> 00:26:54,880
know obviously necessary to at least get

724
00:26:54,880 --> 00:26:56,320
that baseline

725
00:26:56,320 --> 00:26:57,520
so to start off going a little bit

726
00:26:57,520 --> 00:27:01,039
deeper uh clarence can you talk um about

727
00:27:01,039 --> 00:27:02,320
sort of the

728
00:27:02,320 --> 00:27:04,480
individualized harms that each tool

729
00:27:04,480 --> 00:27:06,240
might produce i mean you can't go for

730
00:27:06,240 --> 00:27:07,600
every single tool but you talked a

731
00:27:07,600 --> 00:27:10,559
little bit about how the how the fintech

732
00:27:10,559 --> 00:27:12,400
sort of alternative data has has

733
00:27:12,400 --> 00:27:13,919
disparate effects on black communities

734
00:27:13,919 --> 00:27:15,919
and how you know policing algorithms do

735
00:27:15,919 --> 00:27:17,200
that so if you could give like a little

736
00:27:17,200 --> 00:27:19,200
bit of sample of how some of those harms

737
00:27:19,200 --> 00:27:20,960
uh individually but also sort of

738
00:27:20,960 --> 00:27:23,039
compounded between systems i think that

739
00:27:23,039 --> 00:27:26,080
would be really helpful moving forward

740
00:27:26,080 --> 00:27:28,240
yeah so i think that there's a couple of

741
00:27:28,240 --> 00:27:29,760
good examples that can maybe ground some

742
00:27:29,760 --> 00:27:32,960
of this but um to just begin you know

743
00:27:32,960 --> 00:27:34,960
saying we see a variety of um police

744
00:27:34,960 --> 00:27:36,240
surveillance technologies and other

745
00:27:36,240 --> 00:27:38,240
carsonal technologies being used in

746
00:27:38,240 --> 00:27:39,760
jurisdictions around the united states

747
00:27:39,760 --> 00:27:41,600
so this is everything from you know

748
00:27:41,600 --> 00:27:43,360
common examples like predictive policing

749
00:27:43,360 --> 00:27:45,039
and facial recognition but other

750
00:27:45,039 --> 00:27:46,960
technologies like automated license

751
00:27:46,960 --> 00:27:48,880
plate readers aerial surveillance

752
00:27:48,880 --> 00:27:51,520
systems i think last year uh the new

753
00:27:51,520 --> 00:27:53,200
york city police department rolled out

754
00:27:53,200 --> 00:27:54,799
um robot dogs it sounded like

755
00:27:54,799 --> 00:27:57,039
terrestrial surveillance systems that

756
00:27:57,039 --> 00:27:59,120
assisted them in doing a variety of who

757
00:27:59,120 --> 00:28:01,440
knows what um and i think to what you

758
00:28:01,440 --> 00:28:02,960
know ben was saying there's both the

759
00:28:02,960 --> 00:28:04,559
kind of individualized harm so we've

760
00:28:04,559 --> 00:28:07,279
seen examples of uh using really tragic

761
00:28:07,279 --> 00:28:08,960
examples of the individualized

762
00:28:08,960 --> 00:28:11,039
consequences of these technologies so

763
00:28:11,039 --> 00:28:12,960
for example we know of cases out of

764
00:28:12,960 --> 00:28:14,840
michigan where facial recognition

765
00:28:14,840 --> 00:28:17,840
misidentification has led to uh black

766
00:28:17,840 --> 00:28:20,559
two at least two black men being um you

767
00:28:20,559 --> 00:28:22,880
know falsely incarcerated falsely jailed

768
00:28:22,880 --> 00:28:25,360
uh based off of those misidentifications

769
00:28:25,360 --> 00:28:27,440
we also know uh for example with adam

770
00:28:27,440 --> 00:28:31,039
toledo in chicago a young

771
00:28:31,039 --> 00:28:32,720
young man of color 14 years old i

772
00:28:32,720 --> 00:28:34,559
believe who was killed by the chicago

773
00:28:34,559 --> 00:28:36,880
police department um and was deployed

774
00:28:36,880 --> 00:28:38,240
the chicago police department was

775
00:28:38,240 --> 00:28:40,480
deployed to his neighborhood because of

776
00:28:40,480 --> 00:28:42,080
a shot spotter detection system that

777
00:28:42,080 --> 00:28:43,360
said that there was some gunshot that

778
00:28:43,360 --> 00:28:45,760
had gone off in this community um so we

779
00:28:45,760 --> 00:28:47,840
see the kind of lethal and the violence

780
00:28:47,840 --> 00:28:48,880
violent potential that these

781
00:28:48,880 --> 00:28:51,520
technologies can have with respect to on

782
00:28:51,520 --> 00:28:53,360
individuals we also see it not just in

783
00:28:53,360 --> 00:28:54,720
the you know the peer policing context

784
00:28:54,720 --> 00:28:55,760
but also

785
00:28:55,760 --> 00:28:58,000
with respect to public accommodation so

786
00:28:58,000 --> 00:28:59,679
there was another example out of detroit

787
00:28:59,679 --> 00:29:02,080
of a young teenage girl um who was

788
00:29:02,080 --> 00:29:04,320
kicked out of a skating rink because she

789
00:29:04,320 --> 00:29:06,240
had been misidentified by a facial

790
00:29:06,240 --> 00:29:08,240
recognition system um that said that she

791
00:29:08,240 --> 00:29:10,080
was on a list of people who should have

792
00:29:10,080 --> 00:29:11,520
been excluded from

793
00:29:11,520 --> 00:29:12,399
the

794
00:29:12,399 --> 00:29:14,480
the rink there are also

795
00:29:14,480 --> 00:29:16,880
a number of examples of retailers that

796
00:29:16,880 --> 00:29:18,799
are using facial recognition technology

797
00:29:18,799 --> 00:29:20,799
in similar ways so we see it both in the

798
00:29:20,799 --> 00:29:23,440
policing context and in the commercial

799
00:29:23,440 --> 00:29:24,799
sense as well

800
00:29:24,799 --> 00:29:26,559
but there's a kind of larger concern

801
00:29:26,559 --> 00:29:28,000
about the interoperability of these

802
00:29:28,000 --> 00:29:30,320
systems um so a good example of this has

803
00:29:30,320 --> 00:29:31,840
been some research by immigration

804
00:29:31,840 --> 00:29:33,440
advocates in the united states that have

805
00:29:33,440 --> 00:29:35,200
looked at the impact of data brokers

806
00:29:35,200 --> 00:29:37,039
these are companies that kind of take

807
00:29:37,039 --> 00:29:39,120
and aggregate a whole bunch of different

808
00:29:39,120 --> 00:29:41,760
data sets including like utility data

809
00:29:41,760 --> 00:29:43,440
and use that information package it up

810
00:29:43,440 --> 00:29:45,360
and sail it to law enforcement agencies

811
00:29:45,360 --> 00:29:47,679
to be able to go about uh and do their

812
00:29:47,679 --> 00:29:49,120
business including immigration

813
00:29:49,120 --> 00:29:51,440
enforcement so these technologies data

814
00:29:51,440 --> 00:29:52,880
programs are bringing together these

815
00:29:52,880 --> 00:29:55,200
different forms of data um using all of

816
00:29:55,200 --> 00:29:56,799
these different systems that are able to

817
00:29:56,799 --> 00:29:58,480
identify surveillance people to

818
00:29:58,480 --> 00:30:01,120
facilitate law enforcement purposes um

819
00:30:01,120 --> 00:30:02,960
and then there's the the

820
00:30:02,960 --> 00:30:05,440
the dynamic i was mentioning earlier

821
00:30:05,440 --> 00:30:06,799
with respect to the second class

822
00:30:06,799 --> 00:30:08,480
citizenship so when you look at

823
00:30:08,480 --> 00:30:10,640
technologies that facilitate algorithmic

824
00:30:10,640 --> 00:30:13,279
discrimination right this kind of um use

825
00:30:13,279 --> 00:30:15,279
of these technologies to reinforce

826
00:30:15,279 --> 00:30:17,120
existing historic patterns of

827
00:30:17,120 --> 00:30:19,360
discrimination seeing uh what you're

828
00:30:19,360 --> 00:30:20,960
seeing is a process that renders

829
00:30:20,960 --> 00:30:24,559
communities um vulnerable to the effects

830
00:30:24,559 --> 00:30:26,320
of economic inequality and economic

831
00:30:26,320 --> 00:30:28,480
precarity and that that precarity is

832
00:30:28,480 --> 00:30:30,720
really essential right to sustaining the

833
00:30:30,720 --> 00:30:32,799
ability of law enforcement to target

834
00:30:32,799 --> 00:30:35,360
surveil um and criminalize these

835
00:30:35,360 --> 00:30:37,760
communities and perpetuate patterns of

836
00:30:37,760 --> 00:30:40,159
police violence and mass incarceration

837
00:30:40,159 --> 00:30:42,000
directed to black and brown communities

838
00:30:42,000 --> 00:30:43,679
um so i think along those dimensions you

839
00:30:43,679 --> 00:30:45,279
can kind of get a good sense of all the

840
00:30:45,279 --> 00:30:47,279
different ways that uh carson

841
00:30:47,279 --> 00:30:49,600
technologies are really showing up and

842
00:30:49,600 --> 00:30:51,360
impacting our communities and the last

843
00:30:51,360 --> 00:30:53,679
thing i'll say is i think it's really

844
00:30:53,679 --> 00:30:54,799
important to kind of place these

845
00:30:54,799 --> 00:30:56,159
technologies within a particular

846
00:30:56,159 --> 00:30:58,480
historical context um in particular

847
00:30:58,480 --> 00:31:00,559
police surveillance technologies really

848
00:31:00,559 --> 00:31:04,000
kind of find their their origin story um

849
00:31:04,000 --> 00:31:06,559
in uh chattel slavery right that many of

850
00:31:06,559 --> 00:31:08,799
the surveillance techniques the ideas

851
00:31:08,799 --> 00:31:10,559
that um animate surveillance

852
00:31:10,559 --> 00:31:12,559
technologies today were really kind of

853
00:31:12,559 --> 00:31:14,159
developed and pioneered during the

854
00:31:14,159 --> 00:31:15,919
antebellum period so scholars like

855
00:31:15,919 --> 00:31:18,000
simone brown have looked at example

856
00:31:18,000 --> 00:31:19,919
examples like uh lantern laws in new

857
00:31:19,919 --> 00:31:22,480
york city uh that required uh enslaved

858
00:31:22,480 --> 00:31:24,480
people and black people to uh have

859
00:31:24,480 --> 00:31:26,399
lanterns up to their faces at night time

860
00:31:26,399 --> 00:31:28,080
so that any white person could be able

861
00:31:28,080 --> 00:31:30,000
to identify them right seeing that has

862
00:31:30,000 --> 00:31:31,760
come on the first processes in which

863
00:31:31,760 --> 00:31:34,159
we're um you can see facial recognition

864
00:31:34,159 --> 00:31:36,880
kind of being developed um similar uh

865
00:31:36,880 --> 00:31:38,480
similar things are true with techniques

866
00:31:38,480 --> 00:31:40,240
like branding and others that are

867
00:31:40,240 --> 00:31:42,399
designed to try and make black folks

868
00:31:42,399 --> 00:31:44,320
visible render us visible when they're

869
00:31:44,320 --> 00:31:46,559
subject to surveillance at any given

870
00:31:46,559 --> 00:31:48,880
time and so again the historical context

871
00:31:48,880 --> 00:31:51,440
is also really essential in grounding

872
00:31:51,440 --> 00:31:52,799
how we're thinking about how these

873
00:31:52,799 --> 00:31:54,799
technologies are impacting individuals

874
00:31:54,799 --> 00:31:58,360
and communities today

875
00:32:00,320 --> 00:32:02,000
thank you clarence um

876
00:32:02,000 --> 00:32:05,120
that i appreciate that answer um

877
00:32:05,120 --> 00:32:08,159
so uh silky i'd i'd love for you to talk

878
00:32:08,159 --> 00:32:11,120
a little bit about how you know sort of

879
00:32:11,120 --> 00:32:13,279
building off of of what's what clarence

880
00:32:13,279 --> 00:32:14,720
is talking about how these systems are

881
00:32:14,720 --> 00:32:17,360
are being layered onto sort of systems

882
00:32:17,360 --> 00:32:19,440
of stomach oppression throughout their

883
00:32:19,440 --> 00:32:21,519
time how how do the both criminal

884
00:32:21,519 --> 00:32:23,279
justice and law enforcement use but also

885
00:32:23,279 --> 00:32:25,279
sort of tools outside of the system that

886
00:32:25,279 --> 00:32:28,000
several of you have highlighted um

887
00:32:28,000 --> 00:32:30,720
exacerbate and and prey upon particular

888
00:32:30,720 --> 00:32:32,559
populations particularly

889
00:32:32,559 --> 00:32:34,799
um black and brown populations but also

890
00:32:34,799 --> 00:32:36,799
just um low income populations in

891
00:32:36,799 --> 00:32:38,880
general um and then how did that sort of

892
00:32:38,880 --> 00:32:42,799
drive a sort of an outsized impact on on

893
00:32:42,799 --> 00:32:46,720
on folks in in throughout their life

894
00:32:46,799 --> 00:32:49,120
thanks well it's a really good question

895
00:32:49,120 --> 00:32:51,200
and i think we are

896
00:32:51,200 --> 00:32:52,640
um

897
00:32:52,640 --> 00:32:55,760
on the brink of trying to understand the

898
00:32:55,760 --> 00:32:57,840
answer more in the uk

899
00:32:57,840 --> 00:32:59,519
one of the big problems that we have is

900
00:32:59,519 --> 00:33:01,760
a total lack of transparency i think

901
00:33:01,760 --> 00:33:03,519
this is a common problem but i mean it's

902
00:33:03,519 --> 00:33:05,679
incredibly hard to find how people's

903
00:33:05,679 --> 00:33:08,559
lives are being affected by ever more

904
00:33:08,559 --> 00:33:11,519
obscure and opaque systems

905
00:33:11,519 --> 00:33:14,399
we made a first attempt

906
00:33:14,399 --> 00:33:15,200
to

907
00:33:15,200 --> 00:33:16,559
look at

908
00:33:16,559 --> 00:33:18,320
in welfare and social care some of the

909
00:33:18,320 --> 00:33:21,279
algorithmic decision making big data

910
00:33:21,279 --> 00:33:23,440
related tools that are being used which

911
00:33:23,440 --> 00:33:25,679
i'm going to post a link to um in the

912
00:33:25,679 --> 00:33:27,360
chat it's a report called poverty

913
00:33:27,360 --> 00:33:28,799
panopticon

914
00:33:28,799 --> 00:33:31,679
um and it's interesting how you know we

915
00:33:31,679 --> 00:33:34,080
did find some examples of where the line

916
00:33:34,080 --> 00:33:35,360
between

917
00:33:35,360 --> 00:33:36,960
care and

918
00:33:36,960 --> 00:33:38,519
uh

919
00:33:38,519 --> 00:33:41,760
criminalization is very blurred um with

920
00:33:41,760 --> 00:33:45,440
some of these systems and um

921
00:33:45,440 --> 00:33:47,200
for example

922
00:33:47,200 --> 00:33:48,480
some of you might have heard of the

923
00:33:48,480 --> 00:33:52,240
gang's matrix database which was used by

924
00:33:52,240 --> 00:33:55,039
the metropolitan police here in london

925
00:33:55,039 --> 00:33:57,519
and this ostensibly was something that

926
00:33:57,519 --> 00:33:58,640
was

927
00:33:58,640 --> 00:34:01,760
kind of like a an i mean it is an

928
00:34:01,760 --> 00:34:03,760
intelligence database of

929
00:34:03,760 --> 00:34:04,960
basically

930
00:34:04,960 --> 00:34:06,399
young black male

931
00:34:06,399 --> 00:34:07,840
children

932
00:34:07,840 --> 00:34:11,199
overwhelmingly in london

933
00:34:11,199 --> 00:34:12,960
many of whom have never been involved in

934
00:34:12,960 --> 00:34:15,119
violent crime and the way the police

935
00:34:15,119 --> 00:34:18,159
justified it is through the lens of

936
00:34:18,159 --> 00:34:19,679
safeguarding

937
00:34:19,679 --> 00:34:21,839
and care which is not a role for the

938
00:34:21,839 --> 00:34:24,239
police um and not how it works in

939
00:34:24,239 --> 00:34:26,079
practice in practice it means that these

940
00:34:26,079 --> 00:34:28,560
boys are targeted um sometimes

941
00:34:28,560 --> 00:34:31,599
threatened with school exclusion or even

942
00:34:31,599 --> 00:34:33,199
loss of their homes

943
00:34:33,199 --> 00:34:35,918
and all kinds of sanctions and penalties

944
00:34:35,918 --> 00:34:38,480
affecting their lives um we found a

945
00:34:38,480 --> 00:34:40,480
similar system in our research that

946
00:34:40,480 --> 00:34:44,239
again was run by a council this time not

947
00:34:44,239 --> 00:34:45,679
police

948
00:34:45,679 --> 00:34:47,918
but it's very similar

949
00:34:47,918 --> 00:34:49,918
and again is trying to predict

950
00:34:49,918 --> 00:34:51,359
criminality

951
00:34:51,359 --> 00:34:52,480
in

952
00:34:52,480 --> 00:34:53,839
primarily

953
00:34:53,839 --> 00:34:57,040
young boys from minority groups

954
00:34:57,040 --> 00:34:58,320
and it's interesting how these

955
00:34:58,320 --> 00:35:00,000
predictive algorithms of course are

956
00:35:00,000 --> 00:35:03,119
always leveled at poor people

957
00:35:03,119 --> 00:35:04,560
um to

958
00:35:04,560 --> 00:35:07,680
to predict criminality um and not you

959
00:35:07,680 --> 00:35:11,119
know in canary wharf in the city or

960
00:35:11,119 --> 00:35:12,640
in downing street where there seems to

961
00:35:12,640 --> 00:35:14,240
be a lot of it at the moment

962
00:35:14,240 --> 00:35:15,119
um

963
00:35:15,119 --> 00:35:18,079
so yes certainly you can see that the

964
00:35:18,079 --> 00:35:19,839
uh the power imbalances are as they

965
00:35:19,839 --> 00:35:22,320
always are and the use of these opaque

966
00:35:22,320 --> 00:35:23,520
systems

967
00:35:23,520 --> 00:35:27,200
um is certainly overlapping now between

968
00:35:27,200 --> 00:35:30,800
criminality social care and welfare

969
00:35:30,800 --> 00:35:32,400
the problem that we face what we're up

970
00:35:32,400 --> 00:35:35,119
against in the uk is that it's not clear

971
00:35:35,119 --> 00:35:37,119
yet exactly

972
00:35:37,119 --> 00:35:38,800
what the impacts are and how that's

973
00:35:38,800 --> 00:35:40,079
happening

974
00:35:40,079 --> 00:35:41,680
and of course we're supposed to have the

975
00:35:41,680 --> 00:35:43,119
right to know

976
00:35:43,119 --> 00:35:45,119
if we have been subjected to a purely

977
00:35:45,119 --> 00:35:47,040
automated decision

978
00:35:47,040 --> 00:35:50,079
under gdpr we're supposed to uh under

979
00:35:50,079 --> 00:35:51,920
the uk data protection act

980
00:35:51,920 --> 00:35:53,359
we are supposed to have the right to be

981
00:35:53,359 --> 00:35:55,359
informed of that

982
00:35:55,359 --> 00:35:57,280
the best of my knowledge and i think

983
00:35:57,280 --> 00:35:59,599
anyone else who works on uk data rights

984
00:35:59,599 --> 00:36:00,720
here

985
00:36:00,720 --> 00:36:02,880
no one has ever been informed that

986
00:36:02,880 --> 00:36:04,400
they've been subjected to a purely

987
00:36:04,400 --> 00:36:06,320
automated decision i mean it's like a

988
00:36:06,320 --> 00:36:08,079
fictional data right that doesn't really

989
00:36:08,079 --> 00:36:10,800
work in practice because if you have

990
00:36:10,800 --> 00:36:12,480
an administrator or someone in a

991
00:36:12,480 --> 00:36:15,280
bureaucratic position or an officer who

992
00:36:15,280 --> 00:36:17,920
agrees with an outcome of them uh of an

993
00:36:17,920 --> 00:36:20,400
algorithm then in

994
00:36:20,400 --> 00:36:22,960
the view of the bureaucrats it's not

995
00:36:22,960 --> 00:36:25,280
purely automated it's a human decision

996
00:36:25,280 --> 00:36:27,599
informed by a computer

997
00:36:27,599 --> 00:36:30,079
um and so digging into this is really

998
00:36:30,079 --> 00:36:31,760
tough equally when some of these young

999
00:36:31,760 --> 00:36:34,079
boys in london were getting letters that

1000
00:36:34,079 --> 00:36:35,599
were threatening them with some of these

1001
00:36:35,599 --> 00:36:38,880
kinds of punishments and sanctions

1002
00:36:38,880 --> 00:36:40,240
and it turns out they were on the gang's

1003
00:36:40,240 --> 00:36:41,599
matrix

1004
00:36:41,599 --> 00:36:43,520
a lot of them didn't know and that comes

1005
00:36:43,520 --> 00:36:44,880
back as well to think what clarence was

1006
00:36:44,880 --> 00:36:47,440
saying about community impact

1007
00:36:47,440 --> 00:36:50,160
when a whole community starts to realize

1008
00:36:50,160 --> 00:36:51,520
and feel

1009
00:36:51,520 --> 00:36:52,400
that

1010
00:36:52,400 --> 00:36:54,880
they are being watched and you know

1011
00:36:54,880 --> 00:36:56,480
statements are exchanged like i'm

1012
00:36:56,480 --> 00:36:58,800
probably on the database or they've

1013
00:36:58,800 --> 00:37:01,359
probably targeted me for this reason or

1014
00:37:01,359 --> 00:37:02,880
that reason

1015
00:37:02,880 --> 00:37:05,440
uh people develop the view and sadly

1016
00:37:05,440 --> 00:37:06,960
quite rightly

1017
00:37:06,960 --> 00:37:09,839
that they are targeted and treated

1018
00:37:09,839 --> 00:37:11,440
differently and disadvantaged by

1019
00:37:11,440 --> 00:37:12,960
structures of power that they have no

1020
00:37:12,960 --> 00:37:16,160
control over and actually can't even see

1021
00:37:16,160 --> 00:37:18,720
feel or touch they just are subjected to

1022
00:37:18,720 --> 00:37:19,520
them

1023
00:37:19,520 --> 00:37:21,440
um so that's what i think why it's it's

1024
00:37:21,440 --> 00:37:22,400
so important

1025
00:37:22,400 --> 00:37:24,160
um and the transparency is so important

1026
00:37:24,160 --> 00:37:26,560
because we still know so little in terms

1027
00:37:26,560 --> 00:37:28,640
of as some of the systems that are being

1028
00:37:28,640 --> 00:37:31,359
used in the uk

1029
00:37:31,359 --> 00:37:33,520
yeah definitely i think i think a little

1030
00:37:33,520 --> 00:37:35,760
bit related to the the some of the

1031
00:37:35,760 --> 00:37:37,920
reason why there's not significant

1032
00:37:37,920 --> 00:37:40,160
transparency information about this uh

1033
00:37:40,160 --> 00:37:43,119
is that sort of the harms are felt on

1034
00:37:43,119 --> 00:37:45,599
the the folks that are already uh you

1035
00:37:45,599 --> 00:37:47,280
know most powerless and and and

1036
00:37:47,280 --> 00:37:49,920
marginalized um nikima can you talk a

1037
00:37:49,920 --> 00:37:52,880
little bit about uh that sort of dynamic

1038
00:37:52,880 --> 00:37:55,359
that since since the fact that uh these

1039
00:37:55,359 --> 00:37:58,000
harms are most felt on on those already

1040
00:37:58,000 --> 00:37:59,839
marginalized that they are not centered

1041
00:37:59,839 --> 00:38:01,760
they're not considered and and solutions

1042
00:38:01,760 --> 00:38:03,520
to them and not solutions but

1043
00:38:03,520 --> 00:38:07,760
interventions are not meaningfully made

1044
00:38:07,760 --> 00:38:10,000
absolutely i think the the main thing to

1045
00:38:10,000 --> 00:38:11,839
keep in mind is that we're dealing with

1046
00:38:11,839 --> 00:38:13,280
um

1047
00:38:13,280 --> 00:38:14,880
different kinds of

1048
00:38:14,880 --> 00:38:17,440
pressure on the administrative side the

1049
00:38:17,440 --> 00:38:19,440
governmental side to

1050
00:38:19,440 --> 00:38:20,400
appear

1051
00:38:20,400 --> 00:38:23,119
more digitally savvy and to incorporate

1052
00:38:23,119 --> 00:38:24,640
as much

1053
00:38:24,640 --> 00:38:27,760
time and money optimizing software as

1054
00:38:27,760 --> 00:38:30,800
possible in their

1055
00:38:31,040 --> 00:38:33,680
general administration of benefits and

1056
00:38:33,680 --> 00:38:34,880
of

1057
00:38:34,880 --> 00:38:37,280
all kinds of public services and at the

1058
00:38:37,280 --> 00:38:39,599
same time you have the groups that

1059
00:38:39,599 --> 00:38:40,960
typically

1060
00:38:40,960 --> 00:38:43,520
feel the impact first the negative

1061
00:38:43,520 --> 00:38:46,160
impact of these technologies and they

1062
00:38:46,160 --> 00:38:48,400
are groups like

1063
00:38:48,400 --> 00:38:52,240
under 18s who are subjected to you know

1064
00:38:52,240 --> 00:38:54,880
mass experimentation with algorithmic

1065
00:38:54,880 --> 00:38:56,240
grading

1066
00:38:56,240 --> 00:38:58,320
because there's no lobby because there's

1067
00:38:58,320 --> 00:39:00,640
no obvious defense against

1068
00:39:00,640 --> 00:39:03,599
these groups being targeted in this way

1069
00:39:03,599 --> 00:39:04,640
there's

1070
00:39:04,640 --> 00:39:05,920
uh

1071
00:39:05,920 --> 00:39:07,520
the most obvious

1072
00:39:07,520 --> 00:39:08,560
of

1073
00:39:08,560 --> 00:39:12,320
any type of surveillance or analytics

1074
00:39:12,320 --> 00:39:15,200
software in real time on

1075
00:39:15,200 --> 00:39:17,520
either the streets in greece

1076
00:39:17,520 --> 00:39:18,720
or

1077
00:39:18,720 --> 00:39:21,520
in stores and shops throughout the eu

1078
00:39:21,520 --> 00:39:23,599
is that the people who are going to be

1079
00:39:23,599 --> 00:39:25,839
flagged and the people who research

1080
00:39:25,839 --> 00:39:29,920
shows are going to feel unwelcome

1081
00:39:29,920 --> 00:39:32,560
threatened and potentially

1082
00:39:32,560 --> 00:39:34,720
brought to the attention of

1083
00:39:34,720 --> 00:39:37,200
law enforcement are the people who are

1084
00:39:37,200 --> 00:39:40,160
at the margins who are not pertaining to

1085
00:39:40,160 --> 00:39:42,960
the majority populations and the people

1086
00:39:42,960 --> 00:39:45,760
who are also as a result of that least

1087
00:39:45,760 --> 00:39:47,599
likely to

1088
00:39:47,599 --> 00:39:49,839
demand to know for what purpose they are

1089
00:39:49,839 --> 00:39:51,440
being filmed or

1090
00:39:51,440 --> 00:39:53,920
their information is being retained and

1091
00:39:53,920 --> 00:39:56,880
and this type of thing so i think it's

1092
00:39:56,880 --> 00:39:58,800
partly um

1093
00:39:58,800 --> 00:40:01,440
an encouraging factor at least in the eu

1094
00:40:01,440 --> 00:40:03,920
context so that we do have

1095
00:40:03,920 --> 00:40:06,960
gdpr in place that we are hoping to have

1096
00:40:06,960 --> 00:40:08,319
soon

1097
00:40:08,319 --> 00:40:11,359
a definitive biometric surveillance ban

1098
00:40:11,359 --> 00:40:15,200
the reality though is how do we get from

1099
00:40:15,200 --> 00:40:17,280
these high-level

1100
00:40:17,280 --> 00:40:19,200
protections that are understood in

1101
00:40:19,200 --> 00:40:21,440
concept by many

1102
00:40:21,440 --> 00:40:23,680
that no one seems

1103
00:40:23,680 --> 00:40:26,640
under any compulsion to implement when

1104
00:40:26,640 --> 00:40:29,119
it comes time to determine what types of

1105
00:40:29,119 --> 00:40:31,200
information what types of

1106
00:40:31,200 --> 00:40:34,079
dashboards or analytics will you collect

1107
00:40:34,079 --> 00:40:36,640
about your customers about

1108
00:40:36,640 --> 00:40:39,599
your community if you're policing it and

1109
00:40:39,599 --> 00:40:42,480
what are the requirements of basic human

1110
00:40:42,480 --> 00:40:45,359
rights when it comes to

1111
00:40:45,359 --> 00:40:47,200
making determinations about people's

1112
00:40:47,200 --> 00:40:50,319
future opportunities based on aggregated

1113
00:40:50,319 --> 00:40:52,720
data that has nothing to do with

1114
00:40:52,720 --> 00:40:55,439
individuals

1115
00:40:56,720 --> 00:40:57,760
thank you

1116
00:40:57,760 --> 00:41:00,560
um and just just watching the clock

1117
00:41:00,560 --> 00:41:02,400
things are definitely flying by so i

1118
00:41:02,400 --> 00:41:03,760
want to move

1119
00:41:03,760 --> 00:41:04,839
a little bit

1120
00:41:04,839 --> 00:41:07,440
towards um you know what's what's being

1121
00:41:07,440 --> 00:41:08,800
done and what what can be done how can

1122
00:41:08,800 --> 00:41:11,359
advocates sort of approach this with

1123
00:41:11,359 --> 00:41:14,319
this dynamic in mind that um this is all

1124
00:41:14,319 --> 00:41:16,880
sort of layered on top of sort of you

1125
00:41:16,880 --> 00:41:19,599
know matrix of domination and and power

1126
00:41:19,599 --> 00:41:22,800
imbalances and exacerbating um all sorts

1127
00:41:22,800 --> 00:41:26,560
of of uh marginalizations uh and so i

1128
00:41:26,560 --> 00:41:27,680
think with the understanding that we're

1129
00:41:27,680 --> 00:41:29,359
having a discussion that's relevant to

1130
00:41:29,359 --> 00:41:30,640
almost every country in every

1131
00:41:30,640 --> 00:41:32,480
jurisdiction but obviously has a little

1132
00:41:32,480 --> 00:41:35,760
bit of of differences uh in each you

1133
00:41:35,760 --> 00:41:37,760
know country or state or city you know

1134
00:41:37,760 --> 00:41:40,960
especially that's the case in the us um

1135
00:41:40,960 --> 00:41:43,280
what can can each of you maybe give an

1136
00:41:43,280 --> 00:41:46,079
example of a law or or a regulatory

1137
00:41:46,079 --> 00:41:47,359
action or

1138
00:41:47,359 --> 00:41:49,200
uh you know if the government's failed

1139
00:41:49,200 --> 00:41:50,640
that at every point which is

1140
00:41:50,640 --> 00:41:52,079
understandable a grassroots sort of

1141
00:41:52,079 --> 00:41:54,880
action that was successful um

1142
00:41:54,880 --> 00:41:58,480
that sort of took into account uh

1143
00:41:58,480 --> 00:42:01,040
adequately or at least you know

1144
00:42:01,040 --> 00:42:04,560
in a cursory extent uh this sort of

1145
00:42:04,560 --> 00:42:06,000
broader

1146
00:42:06,000 --> 00:42:06,880
um

1147
00:42:06,880 --> 00:42:09,280
connectivity between these systems um

1148
00:42:09,280 --> 00:42:11,680
and so so i'll i'll start i'll start

1149
00:42:11,680 --> 00:42:13,760
with asking clients um and would love

1150
00:42:13,760 --> 00:42:16,000
everyone to to to hit on this and

1151
00:42:16,000 --> 00:42:18,079
especially nikima in the in the eu i'd

1152
00:42:18,079 --> 00:42:20,400
love for you to talk about how uh you

1153
00:42:20,400 --> 00:42:21,680
know the potential

1154
00:42:21,680 --> 00:42:23,920
uh risk-based approach in the eua act

1155
00:42:23,920 --> 00:42:26,160
which the last event really dove into a

1156
00:42:26,160 --> 00:42:28,640
lot might be able to to cover this

1157
00:42:28,640 --> 00:42:31,640
adequately

1158
00:42:31,920 --> 00:42:34,160
yeah so i mean as ben was mentioning you

1159
00:42:34,160 --> 00:42:36,480
know the u.s is really um the problem

1160
00:42:36,480 --> 00:42:38,720
child when it comes to

1161
00:42:38,720 --> 00:42:40,640
a lot of these questions about building

1162
00:42:40,640 --> 00:42:43,119
policy solutions um we have a very kind

1163
00:42:43,119 --> 00:42:45,440
of fragmented approach to protecting

1164
00:42:45,440 --> 00:42:47,119
individual privacy as well as just kind

1165
00:42:47,119 --> 00:42:49,119
of safeguarding civil and human rights

1166
00:42:49,119 --> 00:42:50,720
generally but especially in uh

1167
00:42:50,720 --> 00:42:52,079
relationship to these emerging

1168
00:42:52,079 --> 00:42:54,880
technologies um so i will give an

1169
00:42:54,880 --> 00:42:56,560
example um

1170
00:42:56,560 --> 00:42:58,240
i got two quick examples i think that

1171
00:42:58,240 --> 00:43:00,640
show kind of the different approaches um

1172
00:43:00,640 --> 00:43:02,720
that communities are taking to begin to

1173
00:43:02,720 --> 00:43:05,040
develop some type of regulatory

1174
00:43:05,040 --> 00:43:07,359
regime over these technologies and the

1175
00:43:07,359 --> 00:43:08,480
reason why i'm going to give these two

1176
00:43:08,480 --> 00:43:10,000
examples is because i think in the

1177
00:43:10,000 --> 00:43:11,680
united states um

1178
00:43:11,680 --> 00:43:14,880
the prospects of federal legislation

1179
00:43:14,880 --> 00:43:17,119
anytime soon is pretty dumb at least

1180
00:43:17,119 --> 00:43:19,280
federal legislation that would i think

1181
00:43:19,280 --> 00:43:22,400
uh be as ambitious as many of the folks

1182
00:43:22,400 --> 00:43:24,960
in this conversation would like to see

1183
00:43:24,960 --> 00:43:26,800
but in the absence of that kind of

1184
00:43:26,800 --> 00:43:30,240
federal oversight um there are you know

1185
00:43:30,240 --> 00:43:31,920
there is movement happening at the local

1186
00:43:31,920 --> 00:43:34,560
level and state level so for example uh

1187
00:43:34,560 --> 00:43:37,040
a coalition down in new orleans

1188
00:43:37,040 --> 00:43:38,400
two years ago i believe now at this

1189
00:43:38,400 --> 00:43:39,200
point

1190
00:43:39,200 --> 00:43:40,640
came together

1191
00:43:40,640 --> 00:43:42,880
after a series of investigative reports

1192
00:43:42,880 --> 00:43:44,800
have found that the new orleans police

1193
00:43:44,800 --> 00:43:47,040
department have routinely lied about

1194
00:43:47,040 --> 00:43:48,640
their use of facial recognition and

1195
00:43:48,640 --> 00:43:49,920
other biometric surveillance

1196
00:43:49,920 --> 00:43:52,880
technologies um and so the coalition

1197
00:43:52,880 --> 00:43:54,640
came together and drafted an ordinance

1198
00:43:54,640 --> 00:43:57,359
that not only banned facial recognition

1199
00:43:57,359 --> 00:43:59,760
in the city of new orleans but also

1200
00:43:59,760 --> 00:44:01,920
set a ban for predictive policing and

1201
00:44:01,920 --> 00:44:04,480
set up this kind of process where

1202
00:44:04,480 --> 00:44:06,240
the new orleans police department would

1203
00:44:06,240 --> 00:44:07,520
have to

1204
00:44:07,520 --> 00:44:08,560
essentially

1205
00:44:08,560 --> 00:44:10,640
have democratic pre-authorization before

1206
00:44:10,640 --> 00:44:12,480
they can implement any new surveillance

1207
00:44:12,480 --> 00:44:14,160
technology so it's one of the most

1208
00:44:14,160 --> 00:44:16,400
robust and comprehensive measures that

1209
00:44:16,400 --> 00:44:18,560
we've seen from a locality

1210
00:44:18,560 --> 00:44:20,880
attempting to try and regulate the use

1211
00:44:20,880 --> 00:44:22,720
of police surveillance technologies

1212
00:44:22,720 --> 00:44:24,480
obviously the the law wasn't everything

1213
00:44:24,480 --> 00:44:26,240
that the advocates as a coalition's name

1214
00:44:26,240 --> 00:44:28,560
was eyes on surveillance um it's not

1215
00:44:28,560 --> 00:44:30,880
everything that they wanted um the model

1216
00:44:30,880 --> 00:44:32,079
bill that they presented was more

1217
00:44:32,079 --> 00:44:34,560
ambitious included more categorical bans

1218
00:44:34,560 --> 00:44:36,720
on various surveillance technologies but

1219
00:44:36,720 --> 00:44:39,040
it represents um the kind of progress

1220
00:44:39,040 --> 00:44:40,640
that i think a lot of jurisdictions

1221
00:44:40,640 --> 00:44:43,040
would like to see replicated

1222
00:44:43,040 --> 00:44:44,880
another example is through the courts

1223
00:44:44,880 --> 00:44:46,560
and this has been a far more perilous

1224
00:44:46,560 --> 00:44:47,440
path

1225
00:44:47,440 --> 00:44:49,839
but there are examples of success so one

1226
00:44:49,839 --> 00:44:52,480
example we're familiar with the ldf um

1227
00:44:52,480 --> 00:44:55,839
is in baltimore the city of baltimore

1228
00:44:55,839 --> 00:44:58,560
had piloted an aerial surveillance uh

1229
00:44:58,560 --> 00:45:00,079
aerial surveillance system it's kind of

1230
00:45:00,079 --> 00:45:02,160
a dragnet aerial surveillance program

1231
00:45:02,160 --> 00:45:03,839
where they had drones that flew over the

1232
00:45:03,839 --> 00:45:06,960
city um 24 hours a day seven days a week

1233
00:45:06,960 --> 00:45:09,680
gathering data um on uh all the

1234
00:45:09,680 --> 00:45:11,440
civilians i think was like 90 of people

1235
00:45:11,440 --> 00:45:13,200
could be surveilled by this aerial

1236
00:45:13,200 --> 00:45:15,680
surveillance system um

1237
00:45:15,680 --> 00:45:17,200
there were a group of community

1238
00:45:17,200 --> 00:45:19,280
organizers that came together and worked

1239
00:45:19,280 --> 00:45:22,079
with the aclu of maryland to bring bring

1240
00:45:22,079 --> 00:45:24,079
litigation challenging that aerial

1241
00:45:24,079 --> 00:45:25,680
surveillance saying that it was a

1242
00:45:25,680 --> 00:45:28,720
violation of the fourth amendment um

1243
00:45:28,720 --> 00:45:30,800
after rounds of litigation they were

1244
00:45:30,800 --> 00:45:32,720
able to actually achieve a really

1245
00:45:32,720 --> 00:45:34,800
landmark decision by

1246
00:45:34,800 --> 00:45:38,079
the fourth circuit holding that uh the

1247
00:45:38,079 --> 00:45:39,839
city's use of that aerial surveillance

1248
00:45:39,839 --> 00:45:42,319
technology actually was violative of the

1249
00:45:42,319 --> 00:45:43,520
fourth amendment and so they are

1250
00:45:43,520 --> 00:45:45,280
currently uh

1251
00:45:45,280 --> 00:45:47,599
well the the litigation actually

1252
00:45:47,599 --> 00:45:48,960
pressured the city to abandon the

1253
00:45:48,960 --> 00:45:51,760
program altogether um and it was just a

1254
00:45:51,760 --> 00:45:53,839
really kind of landmark case and so it

1255
00:45:53,839 --> 00:45:56,079
shows again you know in the absence of

1256
00:45:56,079 --> 00:45:58,160
federal legislation there are

1257
00:45:58,160 --> 00:45:59,920
alternative routes that um communities

1258
00:45:59,920 --> 00:46:01,599
and advocates can pursue

1259
00:46:01,599 --> 00:46:05,280
to try and build policy solutions

1260
00:46:12,480 --> 00:46:13,760
and it came off you can talk a little

1261
00:46:13,760 --> 00:46:17,040
bit about um sort of how you see the ua

1262
00:46:17,040 --> 00:46:19,839
act being able to um

1263
00:46:19,839 --> 00:46:21,359
you know

1264
00:46:21,359 --> 00:46:22,800
adequately

1265
00:46:22,800 --> 00:46:25,760
attack these issues

1266
00:46:25,839 --> 00:46:28,160
well the eu act is

1267
00:46:28,160 --> 00:46:31,200
taking a risk-based approach which is

1268
00:46:31,200 --> 00:46:33,040
categorizing the different types of

1269
00:46:33,040 --> 00:46:36,240
risks that are involved in public

1270
00:46:36,240 --> 00:46:38,800
deployment of large-scale

1271
00:46:38,800 --> 00:46:41,680
algorithmic decision-making systems and

1272
00:46:41,680 --> 00:46:44,400
also it takes the position of banning

1273
00:46:44,400 --> 00:46:47,040
certain specific uses

1274
00:46:47,040 --> 00:46:48,800
entirely

1275
00:46:48,800 --> 00:46:52,079
the good side of the ai act is that it

1276
00:46:52,079 --> 00:46:53,599
is

1277
00:46:53,599 --> 00:46:55,839
some type of framework it provides some

1278
00:46:55,839 --> 00:46:58,400
type of guidance with regard to

1279
00:46:58,400 --> 00:46:59,200
will

1280
00:46:59,200 --> 00:47:02,240
at a minimum require

1281
00:47:02,240 --> 00:47:06,079
audit and evaluation if a system is

1282
00:47:06,079 --> 00:47:08,160
implemented that's categorized it falls

1283
00:47:08,160 --> 00:47:11,680
under the high risk categorization

1284
00:47:11,680 --> 00:47:13,920
what's not so great about

1285
00:47:13,920 --> 00:47:16,960
a high level legislation of this type is

1286
00:47:16,960 --> 00:47:20,319
that you still have to ensure that at

1287
00:47:20,319 --> 00:47:22,240
all levels of administration law

1288
00:47:22,240 --> 00:47:25,280
enforcement and the general public that

1289
00:47:25,280 --> 00:47:28,000
the rights and privileges that it

1290
00:47:28,000 --> 00:47:31,359
protects are understood and can be

1291
00:47:31,359 --> 00:47:32,400
therefore

1292
00:47:32,400 --> 00:47:35,280
defended at the community and individual

1293
00:47:35,280 --> 00:47:38,559
level and right now i would say just

1294
00:47:38,559 --> 00:47:40,640
based on the fact that there is such a

1295
00:47:40,640 --> 00:47:45,040
heavy uh rel reliance on audits x post

1296
00:47:45,040 --> 00:47:46,640
facto and some k

1297
00:47:46,640 --> 00:47:51,680
um audits for fairness and bias um

1298
00:47:51,680 --> 00:47:53,680
there's a very long road ahead to

1299
00:47:53,680 --> 00:47:55,520
refining

1300
00:47:55,520 --> 00:47:58,000
this legislation such that it's actually

1301
00:47:58,000 --> 00:48:01,520
usable understandable and

1302
00:48:01,520 --> 00:48:03,440
defensive in courts

1303
00:48:03,440 --> 00:48:06,240
and um defensible

1304
00:48:06,240 --> 00:48:10,240
across the various eu localities

1305
00:48:11,040 --> 00:48:13,200
thanks um and and

1306
00:48:13,200 --> 00:48:15,040
oh my god

1307
00:48:15,040 --> 00:48:17,920
and and so uh if you have an example uh

1308
00:48:17,920 --> 00:48:19,520
briefly that you'd you'd like to share

1309
00:48:19,520 --> 00:48:21,280
that you think would be you know one

1310
00:48:21,280 --> 00:48:24,000
sort of exemplary example of of an

1311
00:48:24,000 --> 00:48:26,000
intervention or or

1312
00:48:26,000 --> 00:48:27,520
anything like that please please go

1313
00:48:27,520 --> 00:48:30,800
ahead but if not making fun

1314
00:48:31,520 --> 00:48:32,880
well thanks i mean i think this is the

1315
00:48:32,880 --> 00:48:34,720
most important question is why we're all

1316
00:48:34,720 --> 00:48:37,040
here isn't it and uh

1317
00:48:37,040 --> 00:48:38,079
there's no

1318
00:48:38,079 --> 00:48:39,920
one single way i mean every all the ways

1319
00:48:39,920 --> 00:48:42,079
that have been discussed in that we've

1320
00:48:42,079 --> 00:48:43,599
tackled different

1321
00:48:43,599 --> 00:48:47,520
kinds of issues uh within this scope

1322
00:48:47,520 --> 00:48:50,240
um are all really important and i share

1323
00:48:50,240 --> 00:48:53,119
uh clarence's uh

1324
00:48:53,119 --> 00:48:54,960
you know

1325
00:48:54,960 --> 00:48:57,119
just that um

1326
00:48:57,119 --> 00:48:59,200
you know the uncertainty of of legal

1327
00:48:59,200 --> 00:49:02,160
challenges um on on these issues with

1328
00:49:02,160 --> 00:49:04,720
algorithmic decision making

1329
00:49:04,720 --> 00:49:06,160
there's always the risk of course of

1330
00:49:06,160 --> 00:49:08,640
setting a negative precedent um this is

1331
00:49:08,640 --> 00:49:12,319
so new and you're you know

1332
00:49:12,319 --> 00:49:14,000
i maybe it's particularly here i don't

1333
00:49:14,000 --> 00:49:15,359
know but you know you're asking quite an

1334
00:49:15,359 --> 00:49:18,160
archaic system to grapple with and

1335
00:49:18,160 --> 00:49:20,079
totally novel concepts

1336
00:49:20,079 --> 00:49:21,520
um and

1337
00:49:21,520 --> 00:49:24,240
they are not sympathetic in general and

1338
00:49:24,240 --> 00:49:27,920
again the opacity is an issue uh we just

1339
00:49:27,920 --> 00:49:30,559
tried to um

1340
00:49:30,559 --> 00:49:34,800
bring a legal challenge against uh

1341
00:49:35,359 --> 00:49:37,920
so an element of the welfare system

1342
00:49:37,920 --> 00:49:38,800
um

1343
00:49:38,800 --> 00:49:40,800
where we think

1344
00:49:40,800 --> 00:49:41,920
an algorithm

1345
00:49:41,920 --> 00:49:44,559
an algorithm was involved in people not

1346
00:49:44,559 --> 00:49:47,119
being paid correctly

1347
00:49:47,119 --> 00:49:48,079
um

1348
00:49:48,079 --> 00:49:49,040
and

1349
00:49:49,040 --> 00:49:50,880
because we don't have any evidence and

1350
00:49:50,880 --> 00:49:53,359
we can't force that uh evidence to come

1351
00:49:53,359 --> 00:49:55,119
out you're shooting in the dark it's

1352
00:49:55,119 --> 00:49:57,839
really really hard

1353
00:49:57,839 --> 00:49:59,520
so it depends you know i think when

1354
00:49:59,520 --> 00:50:00,960
we're talking about algorithms in the

1355
00:50:00,960 --> 00:50:02,559
welfare system for example what you

1356
00:50:02,559 --> 00:50:03,920
really need is a claimant and you need a

1357
00:50:03,920 --> 00:50:05,200
claimant who's willing to do lots of

1358
00:50:05,200 --> 00:50:06,960
subject access requests get all of their

1359
00:50:06,960 --> 00:50:09,200
data have someone rifle through it i

1360
00:50:09,200 --> 00:50:11,040
don't know anyone on welfare that would

1361
00:50:11,040 --> 00:50:13,440
be willing to do that and i have to say

1362
00:50:13,440 --> 00:50:14,800
you know

1363
00:50:14,800 --> 00:50:16,960
you you can understand why we're talking

1364
00:50:16,960 --> 00:50:18,319
about people who are in precarious

1365
00:50:18,319 --> 00:50:20,720
vulnerable situations um where the

1366
00:50:20,720 --> 00:50:24,319
system is designed to to punish you um

1367
00:50:24,319 --> 00:50:26,480
and i know that's not that might sound

1368
00:50:26,480 --> 00:50:27,760
dramatic i mean it's supposed to be a

1369
00:50:27,760 --> 00:50:30,079
support net but also all of these

1370
00:50:30,079 --> 00:50:32,400
algorithms are designed to

1371
00:50:32,400 --> 00:50:35,839
uh with suspicion in mind the underlying

1372
00:50:35,839 --> 00:50:38,240
suspicion is that everyone

1373
00:50:38,240 --> 00:50:39,920
in receipt of welfare is committing

1374
00:50:39,920 --> 00:50:42,319
fraud or could be committing the fraud

1375
00:50:42,319 --> 00:50:44,319
and so in those circumstances the

1376
00:50:44,319 --> 00:50:45,760
expectation that people are going to be

1377
00:50:45,760 --> 00:50:47,839
able to challenge and this is like a

1378
00:50:47,839 --> 00:50:50,079
david and goliath thing um

1379
00:50:50,079 --> 00:50:52,559
but it's not courage enough courage is

1380
00:50:52,559 --> 00:50:54,640
not enough um you know it's not even

1381
00:50:54,640 --> 00:50:56,160
about courage at all it's about the fact

1382
00:50:56,160 --> 00:50:58,400
the system is so so stacked is that

1383
00:50:58,400 --> 00:50:59,920
something we've got to grapple with the

1384
00:50:59,920 --> 00:51:01,599
transparency is the most important thing

1385
00:51:01,599 --> 00:51:04,319
but the positive outcome of the um heart

1386
00:51:04,319 --> 00:51:06,240
investigation that we did is that

1387
00:51:06,240 --> 00:51:08,720
actually um first of all when we

1388
00:51:08,720 --> 00:51:11,520
published that investigation the mosaic

1389
00:51:11,520 --> 00:51:13,440
variable was taken out

1390
00:51:13,440 --> 00:51:14,720
um

1391
00:51:14,720 --> 00:51:17,440
so those crude postcode stereotypes were

1392
00:51:17,440 --> 00:51:19,599
taken out of the system and subsequently

1393
00:51:19,599 --> 00:51:22,880
the whole system has been dropped but i

1394
00:51:22,880 --> 00:51:24,480
suspect it's going to come back and it's

1395
00:51:24,480 --> 00:51:26,000
the same with facial recognition you

1396
00:51:26,000 --> 00:51:27,760
know we've had moratoriums we've had

1397
00:51:27,760 --> 00:51:29,920
pauses and i think they're probably

1398
00:51:29,920 --> 00:51:31,680
gathering their strength and they're

1399
00:51:31,680 --> 00:51:32,720
going to come back some of these

1400
00:51:32,720 --> 00:51:34,880
companies saying don't worry we fixed

1401
00:51:34,880 --> 00:51:36,720
the discrimination problem

1402
00:51:36,720 --> 00:51:37,760
um

1403
00:51:37,760 --> 00:51:39,040
because they're looking at through this

1404
00:51:39,040 --> 00:51:42,400
really narrow algorithmic lens about

1405
00:51:42,400 --> 00:51:44,640
um how the algorithm in isolation

1406
00:51:44,640 --> 00:51:47,040
performs and that will get better over

1407
00:51:47,040 --> 00:51:49,520
time but then we've got

1408
00:51:49,520 --> 00:51:52,319
just a different iteration of a very

1409
00:51:52,319 --> 00:51:54,000
similar problem because as we've

1410
00:51:54,000 --> 00:51:57,520
discussed discrimination runs through

1411
00:51:57,520 --> 00:51:59,839
and the harms run through every stage of

1412
00:51:59,839 --> 00:52:02,000
this process

1413
00:52:02,000 --> 00:52:03,280
where

1414
00:52:03,280 --> 00:52:06,160
marginalized people are routinely

1415
00:52:06,160 --> 00:52:09,760
targeted by by authorities or denied

1416
00:52:09,760 --> 00:52:13,280
basic rights and sustenance

1417
00:52:13,280 --> 00:52:15,440
so sometimes i think legal challenges

1418
00:52:15,440 --> 00:52:17,200
work transparency is the most important

1419
00:52:17,200 --> 00:52:19,839
thing and sometimes it's grassroots

1420
00:52:19,839 --> 00:52:21,280
and i think

1421
00:52:21,280 --> 00:52:22,800
international solidarity is really

1422
00:52:22,800 --> 00:52:25,119
important too i mean the connections

1423
00:52:25,119 --> 00:52:26,640
that uh

1424
00:52:26,640 --> 00:52:28,480
groups have had big brother watching

1425
00:52:28,480 --> 00:52:30,079
included with groups all around the

1426
00:52:30,079 --> 00:52:33,119
world on facial recognition um

1427
00:52:33,119 --> 00:52:35,359
we that matters so much because when we

1428
00:52:35,359 --> 00:52:37,839
sit in meetings with politicians we can

1429
00:52:37,839 --> 00:52:40,559
point to the u.s and we can point to the

1430
00:52:40,559 --> 00:52:42,000
countries all around the world and the

1431
00:52:42,000 --> 00:52:43,920
kind of backlash that

1432
00:52:43,920 --> 00:52:45,599
is happening in all kinds of different

1433
00:52:45,599 --> 00:52:46,559
um

1434
00:52:46,559 --> 00:52:47,839
places

1435
00:52:47,839 --> 00:52:49,119
um

1436
00:52:49,119 --> 00:52:50,480
and

1437
00:52:50,480 --> 00:52:52,079
educating i think

1438
00:52:52,079 --> 00:52:54,720
a grassroots level about the the future

1439
00:52:54,720 --> 00:52:56,240
risks before it's too late because of

1440
00:52:56,240 --> 00:52:58,400
course once the horse is bolted

1441
00:52:58,400 --> 00:53:01,200
and once you have something like facial

1442
00:53:01,200 --> 00:53:03,359
recognition used by every police force

1443
00:53:03,359 --> 00:53:05,119
which is something that we've genuinely

1444
00:53:05,119 --> 00:53:07,040
faced a few years ago and still face i

1445
00:53:07,040 --> 00:53:09,440
mean it's just a prolonged threat one

1446
00:53:09,440 --> 00:53:11,119
police force just a couple of weeks ago

1447
00:53:11,119 --> 00:53:13,280
now has hand-held facial recognition on

1448
00:53:13,280 --> 00:53:16,079
phones so they can just point and and

1449
00:53:16,079 --> 00:53:17,680
scan someone

1450
00:53:17,680 --> 00:53:18,720
um

1451
00:53:18,720 --> 00:53:21,200
with that's we have to not get there

1452
00:53:21,200 --> 00:53:23,040
because once we've gotten to that stage

1453
00:53:23,040 --> 00:53:25,119
i fear that it'll be too late so we have

1454
00:53:25,119 --> 00:53:27,680
to we have to have a kind of

1455
00:53:27,680 --> 00:53:30,079
entrepreneurial and multi-pronged

1456
00:53:30,079 --> 00:53:31,280
approach

1457
00:53:31,280 --> 00:53:33,200
and there's not enough of us

1458
00:53:33,200 --> 00:53:34,480
there are too many of these issues

1459
00:53:34,480 --> 00:53:36,400
coming up and not enough time not enough

1460
00:53:36,400 --> 00:53:39,400
people

1461
00:53:42,000 --> 00:53:43,760
yeah i wanted to add on to that i

1462
00:53:43,760 --> 00:53:45,440
absolutely agree that the solidarity

1463
00:53:45,440 --> 00:53:47,599
aspect is super important it's

1464
00:53:47,599 --> 00:53:49,280
especially important when we're looking

1465
00:53:49,280 --> 00:53:51,440
at different outcomes in different

1466
00:53:51,440 --> 00:53:53,520
geographies um

1467
00:53:53,520 --> 00:53:57,440
if you go back to uh or even before

1468
00:53:57,440 --> 00:53:59,040
the dutch

1469
00:53:59,040 --> 00:54:01,280
social benefits case that ended up

1470
00:54:01,280 --> 00:54:03,920
bringing down the dutch government

1471
00:54:03,920 --> 00:54:06,400
around the discriminatory use

1472
00:54:06,400 --> 00:54:08,400
in court to be

1473
00:54:08,400 --> 00:54:11,040
a violation of privacy and

1474
00:54:11,040 --> 00:54:13,599
fundamental human rights um same

1475
00:54:13,599 --> 00:54:15,760
scenario um

1476
00:54:15,760 --> 00:54:17,760
social welfare recipients were being

1477
00:54:17,760 --> 00:54:20,640
targeted based on an algorithm that took

1478
00:54:20,640 --> 00:54:22,960
into consideration or took into undue

1479
00:54:22,960 --> 00:54:25,599
consideration their nationality their

1480
00:54:25,599 --> 00:54:27,280
their um

1481
00:54:27,280 --> 00:54:30,559
the size of their families etc um and

1482
00:54:30,559 --> 00:54:33,040
essentially was proven to be victimizing

1483
00:54:33,040 --> 00:54:35,520
uh certain groups over others and

1484
00:54:35,520 --> 00:54:37,280
ultimately

1485
00:54:37,280 --> 00:54:38,400
predatory

1486
00:54:38,400 --> 00:54:41,040
in its approach to social welfare

1487
00:54:41,040 --> 00:54:43,280
recipients in general

1488
00:54:43,280 --> 00:54:44,960
i think that there's absolutely a need

1489
00:54:44,960 --> 00:54:45,839
to be

1490
00:54:45,839 --> 00:54:48,559
conscious of and to try to create some

1491
00:54:48,559 --> 00:54:49,920
high bar

1492
00:54:49,920 --> 00:54:52,480
across different geographies of the use

1493
00:54:52,480 --> 00:54:56,400
of these algorithms in the future and

1494
00:54:56,400 --> 00:54:57,359
so

1495
00:54:57,359 --> 00:55:00,079
alongside of the transparency demand

1496
00:55:00,079 --> 00:55:02,640
there needs to be more awareness i

1497
00:55:02,640 --> 00:55:03,599
believe

1498
00:55:03,599 --> 00:55:05,680
of

1499
00:55:05,680 --> 00:55:08,400
the risks of relying on the appearance

1500
00:55:08,400 --> 00:55:10,400
of modernity

1501
00:55:10,400 --> 00:55:12,319
and the appearance of agility and

1502
00:55:12,319 --> 00:55:14,319
flexibility in

1503
00:55:14,319 --> 00:55:17,119
introducing new technology the handheld

1504
00:55:17,119 --> 00:55:19,200
scanners that are being used i hope not

1505
00:55:19,200 --> 00:55:21,760
in the uk i hope it's the greek case

1506
00:55:21,760 --> 00:55:25,520
that i was referring to but if if if not

1507
00:55:25,520 --> 00:55:27,280
then here again we have you know what's

1508
00:55:27,280 --> 00:55:28,880
happening in the uk is happening in

1509
00:55:28,880 --> 00:55:31,200
greece and presumably in other parts of

1510
00:55:31,200 --> 00:55:34,000
the world where um people are just so

1511
00:55:34,000 --> 00:55:36,640
excited to innovate in the ways that

1512
00:55:36,640 --> 00:55:39,119
technology is utilized in the public

1513
00:55:39,119 --> 00:55:41,599
space but that means that you know these

1514
00:55:41,599 --> 00:55:43,839
experiments are happening at scale and

1515
00:55:43,839 --> 00:55:46,000
that means that even in the case i

1516
00:55:46,000 --> 00:55:47,920
wanted to just quickly go back to the

1517
00:55:47,920 --> 00:55:49,359
clarence mentioned

1518
00:55:49,359 --> 00:55:51,839
these innovative non-traditional types

1519
00:55:51,839 --> 00:55:54,240
of information that are being used as

1520
00:55:54,240 --> 00:55:56,319
part of an algorithmic assessment you

1521
00:55:56,319 --> 00:55:58,640
know very often that sounds good and it

1522
00:55:58,640 --> 00:56:00,240
sounds like a departure from the old

1523
00:56:00,240 --> 00:56:02,079
ways of doing things but when you

1524
00:56:02,079 --> 00:56:04,880
discover that the non-traditional data

1525
00:56:04,880 --> 00:56:07,200
that's being collected about subjects to

1526
00:56:07,200 --> 00:56:09,760
make an assessment of risk

1527
00:56:09,760 --> 00:56:12,400
credit worthiness or whatever it is are

1528
00:56:12,400 --> 00:56:14,720
data points such as how you scored on

1529
00:56:14,720 --> 00:56:17,040
standardized tests what

1530
00:56:17,040 --> 00:56:18,880
the brand recognition is of your

1531
00:56:18,880 --> 00:56:20,319
employer

1532
00:56:20,319 --> 00:56:22,799
how high you reached at whatever level

1533
00:56:22,799 --> 00:56:25,040
in employment you realize that actually

1534
00:56:25,040 --> 00:56:27,280
it's just an excuse to collect even more

1535
00:56:27,280 --> 00:56:30,160
invasive data about people who don't

1536
00:56:30,160 --> 00:56:32,400
have a choice in the collection and it

1537
00:56:32,400 --> 00:56:34,720
often ends up being just a smoke screen

1538
00:56:34,720 --> 00:56:37,200
for a greater and greater overreach in

1539
00:56:37,200 --> 00:56:41,399
the public's public space

1540
00:56:48,720 --> 00:56:52,919
then i cannot hear you

1541
00:57:04,799 --> 00:57:08,880
i'm afraid i can still not hear you

1542
00:57:08,880 --> 00:57:10,839
might be my own

1543
00:57:10,839 --> 00:57:14,240
issue you okay here rather

1544
00:57:14,240 --> 00:57:15,040
okay

1545
00:57:15,040 --> 00:57:19,520
then you can try to simply refresh

1546
00:57:22,960 --> 00:57:23,760
my

1547
00:57:23,760 --> 00:57:26,319
best advice would be to refresh the page

1548
00:57:26,319 --> 00:57:29,440
simply and get back

1549
00:57:32,079 --> 00:57:33,680
my parents

1550
00:57:33,680 --> 00:57:35,839
say

1551
00:57:36,480 --> 00:57:39,880
while we wait

1552
00:57:42,319 --> 00:57:45,359
yeah um i want there any questions in

1553
00:57:45,359 --> 00:57:46,799
the

1554
00:57:46,799 --> 00:57:49,040
chat let's see

1555
00:57:49,040 --> 00:57:50,880
inevitable way of making automated

1556
00:57:50,880 --> 00:57:54,000
decision making systems

1557
00:57:54,000 --> 00:57:55,280
yeah so

1558
00:57:55,280 --> 00:57:57,040
um i guess just a couple like quick

1559
00:57:57,040 --> 00:57:58,160
concluding thoughts and kind of

1560
00:57:58,160 --> 00:57:59,359
responding to one of the questions that

1561
00:57:59,359 --> 00:58:02,400
was raised um i do think that you know i

1562
00:58:02,400 --> 00:58:03,920
don't think that the development of

1563
00:58:03,920 --> 00:58:06,000
technology and civil rights are

1564
00:58:06,000 --> 00:58:07,760
fundamentally incompatible right but i

1565
00:58:07,760 --> 00:58:10,000
think that the interests that motivate

1566
00:58:10,000 --> 00:58:11,200
a lot of the companies that are

1567
00:58:11,200 --> 00:58:12,880
developing the technologies and many of

1568
00:58:12,880 --> 00:58:15,119
the jurisdictions that are adopting them

1569
00:58:15,119 --> 00:58:17,440
are very much rooted in at the very

1570
00:58:17,440 --> 00:58:19,920
least um a deep-seated investment in the

1571
00:58:19,920 --> 00:58:22,400
status quo which uh features all of the

1572
00:58:22,400 --> 00:58:24,559
inequalities that we discussed today and

1573
00:58:24,559 --> 00:58:26,480
so i think you know as we're thinking

1574
00:58:26,480 --> 00:58:28,720
about the kind of future of

1575
00:58:28,720 --> 00:58:30,240
the relationship between science and

1576
00:58:30,240 --> 00:58:32,880
technology um in society we have to look

1577
00:58:32,880 --> 00:58:34,720
at the past and i think we have to

1578
00:58:34,720 --> 00:58:36,880
really interrogate specifically looking

1579
00:58:36,880 --> 00:58:38,559
back at the kind of late 19th century

1580
00:58:38,559 --> 00:58:40,720
early 20th century but the advent of

1581
00:58:40,720 --> 00:58:43,520
scientific racism um and the use of

1582
00:58:43,520 --> 00:58:45,920
these pseudoscientific theories like

1583
00:58:45,920 --> 00:58:48,960
dractomania for example um this is a

1584
00:58:48,960 --> 00:58:50,880
phenomena that was developed by one of

1585
00:58:50,880 --> 00:58:52,400
the country's leading

1586
00:58:52,400 --> 00:58:55,280
medical pioneers uh that suggested that

1587
00:58:55,280 --> 00:58:58,880
uh enslaved people had a um impulse they

1588
00:58:58,880 --> 00:59:00,400
had this kind of disorder that would

1589
00:59:00,400 --> 00:59:02,480
cause them to want to flee and that the

1590
00:59:02,480 --> 00:59:05,040
medical the medical diagnosis the way

1591
00:59:05,040 --> 00:59:07,520
that you treated uh drank to mania was

1592
00:59:07,520 --> 00:59:10,240
by beating uh and whipping people that

1593
00:59:10,240 --> 00:59:12,480
that was how you treated dracula um and

1594
00:59:12,480 --> 00:59:15,599
we see that these kind of same you know

1595
00:59:15,599 --> 00:59:18,559
just baseless um theories are

1596
00:59:18,559 --> 00:59:20,240
undergirding many of the technologies

1597
00:59:20,240 --> 00:59:21,359
that are shaped in black and brown

1598
00:59:21,359 --> 00:59:23,839
people's lives today um and you can see

1599
00:59:23,839 --> 00:59:25,359
the ways in which you know ideas like

1600
00:59:25,359 --> 00:59:27,839
phrenology and physionomy are really you

1601
00:59:27,839 --> 00:59:29,680
know reimagining themselves and facial

1602
00:59:29,680 --> 00:59:31,599
recognition and affect recognition

1603
00:59:31,599 --> 00:59:33,280
technologies you being used both as we

1604
00:59:33,280 --> 00:59:35,520
described in the law enforcement context

1605
00:59:35,520 --> 00:59:38,160
but also um in the social welfare

1606
00:59:38,160 --> 00:59:40,559
context right to facilitate access to

1607
00:59:40,559 --> 00:59:42,559
employment opportunities to credit and

1608
00:59:42,559 --> 00:59:44,720
to others and so i think we really have

1609
00:59:44,720 --> 00:59:48,240
to um kind of return to our cornerstone

1610
00:59:48,240 --> 00:59:50,240
which is civil and human rights and um

1611
00:59:50,240 --> 00:59:52,640
democratic governance as the anchors as

1612
00:59:52,640 --> 00:59:54,640
we think about developing uh these

1613
00:59:54,640 --> 00:59:58,480
technologies and really reimagining um

1614
00:59:58,480 --> 01:00:00,559
the role that

1615
01:00:00,559 --> 01:00:03,440
uh public oversight can and must play in

1616
01:00:03,440 --> 01:00:04,799
ensuring that the development of these

1617
01:00:04,799 --> 01:00:06,880
technologies proceed along the lines of

1618
01:00:06,880 --> 01:00:08,880
civil and human rights um the last thing

1619
01:00:08,880 --> 01:00:10,000
i'll say is you know one of the things

1620
01:00:10,000 --> 01:00:11,760
that our executive director talks about

1621
01:00:11,760 --> 01:00:14,480
um frequently in this context is how

1622
01:00:14,480 --> 01:00:16,160
digital space has kind of reimagined

1623
01:00:16,160 --> 01:00:18,880
public space and that in the 1950s and

1624
01:00:18,880 --> 01:00:20,240
60s in the united states there was a

1625
01:00:20,240 --> 01:00:23,280
civil rights revolution to reimagine how

1626
01:00:23,280 --> 01:00:25,359
black people could exist in public

1627
01:00:25,359 --> 01:00:27,440
spaces to challenge you know the

1628
01:00:27,440 --> 01:00:30,079
practices of jim crow and segregation on

1629
01:00:30,079 --> 01:00:32,240
the racial uh apartheid in the united

1630
01:00:32,240 --> 01:00:34,160
states and that the result of that

1631
01:00:34,160 --> 01:00:35,599
struggle the result of many people who

1632
01:00:35,599 --> 01:00:37,200
had to die for

1633
01:00:37,200 --> 01:00:39,839
uh that struggle was our kind of modern

1634
01:00:39,839 --> 01:00:41,200
civil rights infrastructure in the

1635
01:00:41,200 --> 01:00:43,440
united states and that it is quite

1636
01:00:43,440 --> 01:00:45,839
alarming that as these digital as the

1637
01:00:45,839 --> 01:00:47,599
digital public is being reimagined in

1638
01:00:47,599 --> 01:00:49,680
real time there's this kind of impulse

1639
01:00:49,680 --> 01:00:51,680
to try and keep and prevent civil and

1640
01:00:51,680 --> 01:00:54,240
human rights uh victories from coming in

1641
01:00:54,240 --> 01:00:55,920
and shaping how those digital publics

1642
01:00:55,920 --> 01:00:58,400
will exist so um

1643
01:00:58,400 --> 01:00:59,839
yes i think that you know that's our

1644
01:00:59,839 --> 01:01:01,440
task is to figure out how can we make

1645
01:01:01,440 --> 01:01:03,200
sure that our our digital environment

1646
01:01:03,200 --> 01:01:05,760
our digital lives uh reflects the norms

1647
01:01:05,760 --> 01:01:07,119
that we have established over the last

1648
01:01:07,119 --> 01:01:09,680
century uh to protect the interests of

1649
01:01:09,680 --> 01:01:13,000
all of our communities

1650
01:01:24,799 --> 01:01:26,880
you

