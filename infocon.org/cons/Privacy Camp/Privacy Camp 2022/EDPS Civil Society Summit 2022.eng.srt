1
00:00:06,480 --> 00:00:08,400
say we're shifting to a very differing

2
00:00:08,400 --> 00:00:09,840
topic now

3
00:00:09,840 --> 00:00:11,679
uh please grab your lunch in front of

4
00:00:11,679 --> 00:00:13,440
your computer and in front of your

5
00:00:13,440 --> 00:00:15,920
screen and stay with us it's a very

6
00:00:15,920 --> 00:00:19,199
important topic it actually is one of

7
00:00:19,199 --> 00:00:20,640
the

8
00:00:20,640 --> 00:00:23,600
most important uh field where digital

9
00:00:23,600 --> 00:00:26,160
right it's under research or at least

10
00:00:26,160 --> 00:00:27,279
not

11
00:00:27,279 --> 00:00:29,599
doesn't get enough attention from the

12
00:00:29,599 --> 00:00:32,640
digital rights field um and it's a shame

13
00:00:32,640 --> 00:00:34,239
because a lot of fundamental rights

14
00:00:34,239 --> 00:00:36,000
violation are happening

15
00:00:36,000 --> 00:00:38,559
almost on a on a quasi-daily basis at

16
00:00:38,559 --> 00:00:40,800
the borders of europe but also on

17
00:00:40,800 --> 00:00:42,800
european territory and it concerns

18
00:00:42,800 --> 00:00:45,039
people on the move

19
00:00:45,039 --> 00:00:47,600
and everybody that deserves their

20
00:00:47,600 --> 00:00:50,399
migration rights to be respected by the

21
00:00:50,399 --> 00:00:53,039
european union and it has a lot of

22
00:00:53,039 --> 00:00:54,960
interconnection with data protection and

23
00:00:54,960 --> 00:00:57,600
privacy consideration

24
00:00:57,600 --> 00:01:00,879
and for uh this panel uh it's a very

25
00:01:00,879 --> 00:01:03,199
specific one it's the edps civil society

26
00:01:03,199 --> 00:01:05,600
summit it's kind of a tradition in

27
00:01:05,600 --> 00:01:07,680
privacy camp for some years that we

28
00:01:07,680 --> 00:01:10,720
co-organized this panel with uh edps the

29
00:01:10,720 --> 00:01:12,400
european data protection supervisor and

30
00:01:12,400 --> 00:01:14,960
we're super happy that it continues uh

31
00:01:14,960 --> 00:01:16,880
this year's as well it's very an

32
00:01:16,880 --> 00:01:18,000
important

33
00:01:18,000 --> 00:01:19,920
time where we can exchange with the

34
00:01:19,920 --> 00:01:22,560
institution and share our civil society

35
00:01:22,560 --> 00:01:25,280
point of view and also get to understand

36
00:01:25,280 --> 00:01:28,080
where the institutions uh are standing

37
00:01:28,080 --> 00:01:30,079
on certain issues and this year we we

38
00:01:30,079 --> 00:01:32,240
picked this topic and we are very much

39
00:01:32,240 --> 00:01:34,560
looking forward for this for this uh

40
00:01:34,560 --> 00:01:37,759
discussion i would like to

41
00:01:37,759 --> 00:01:40,079
invite the moderator who kindly agreed

42
00:01:40,079 --> 00:01:41,360
to

43
00:01:41,360 --> 00:01:43,280
to discuss and and moderate the

44
00:01:43,280 --> 00:01:45,920
discussion today lawrence mayer

45
00:01:45,920 --> 00:01:47,920
who is the racial and social justice

46
00:01:47,920 --> 00:01:50,479
lead from digital freedom fund thank you

47
00:01:50,479 --> 00:01:52,479
so much lawrence i will let you

48
00:01:52,479 --> 00:01:54,640
introduce the topic further and also the

49
00:01:54,640 --> 00:01:56,799
speakers and don't hesitate if you need

50
00:01:56,799 --> 00:01:58,880
any support have a great discussion

51
00:01:58,880 --> 00:02:00,320
thank you

52
00:02:00,320 --> 00:02:02,240
thank you chloe um

53
00:02:02,240 --> 00:02:04,719
as chloe said i'm laurence miya i'm the

54
00:02:04,719 --> 00:02:07,280
rations for justice league at digital

55
00:02:07,280 --> 00:02:09,440
freedom fund

56
00:02:09,440 --> 00:02:12,560
i see some of the panelists already are

57
00:02:12,560 --> 00:02:15,920
in place i will invite uh

58
00:02:15,920 --> 00:02:19,360
yeah exactly alina and teresa also

59
00:02:19,360 --> 00:02:22,640
are arriving so that is great um so

60
00:02:22,640 --> 00:02:26,080
thank you all so much for being there uh

61
00:02:26,080 --> 00:02:29,040
to discuss um as claude said this really

62
00:02:29,040 --> 00:02:30,959
central topic when we talk about like

63
00:02:30,959 --> 00:02:32,879
fundamental rights and

64
00:02:32,879 --> 00:02:34,239
people uh

65
00:02:34,239 --> 00:02:37,519
on the move uh we can't ignore the the

66
00:02:37,519 --> 00:02:38,879
impact that

67
00:02:38,879 --> 00:02:42,400
the use of digital tools have on uh

68
00:02:42,400 --> 00:02:44,239
potential infringement

69
00:02:44,239 --> 00:02:45,680
of their rights

70
00:02:45,680 --> 00:02:46,800
um

71
00:02:46,800 --> 00:02:49,440
i'll just do a quick introduction of all

72
00:02:49,440 --> 00:02:53,040
the panelists uh i'm just apologizing in

73
00:02:53,040 --> 00:02:55,840
advance if i mispronounce your name

74
00:02:55,840 --> 00:02:58,959
and uh for the shortness of the

75
00:02:58,959 --> 00:03:00,400
presentation

76
00:03:00,400 --> 00:03:03,840
um first on my list is

77
00:03:03,840 --> 00:03:07,760
vivarotki uh um he is the european data

78
00:03:07,760 --> 00:03:11,120
protection supervisor since december

79
00:03:11,120 --> 00:03:15,360
2019 and uh that for a term of five

80
00:03:15,360 --> 00:03:17,120
years

81
00:03:17,120 --> 00:03:20,159
doctor teresa quintel is a lecturer at

82
00:03:20,159 --> 00:03:22,480
the maastricht european center on

83
00:03:22,480 --> 00:03:24,640
privacy and cyber security

84
00:03:24,640 --> 00:03:27,519
and uh she joined the center in july

85
00:03:27,519 --> 00:03:28,959
2021

86
00:03:28,959 --> 00:03:29,760
her

87
00:03:29,760 --> 00:03:31,680
thesis looked at

88
00:03:31,680 --> 00:03:34,239
and this is a title managing migration

89
00:03:34,239 --> 00:03:35,840
flows by processing

90
00:03:35,840 --> 00:03:38,000
personal data within the adequate data

91
00:03:38,000 --> 00:03:39,360
protection

92
00:03:39,360 --> 00:03:42,159
instrument alina smith

93
00:03:42,159 --> 00:03:44,560
is deputy director at the platform of

94
00:03:44,560 --> 00:03:46,159
international cooperation on

95
00:03:46,159 --> 00:03:48,400
undocumented migrants

96
00:03:48,400 --> 00:03:51,360
she joined pycon picon i don't know how

97
00:03:51,360 --> 00:03:52,959
he pronounced it

98
00:03:52,959 --> 00:03:55,760
in 2015 and

99
00:03:55,760 --> 00:03:59,360
until 2021 she led becomes advocacy on

100
00:03:59,360 --> 00:04:01,200
access to health care and access to

101
00:04:01,200 --> 00:04:03,599
justice for undocumented

102
00:04:03,599 --> 00:04:06,319
people and uh

103
00:04:06,319 --> 00:04:07,920
last but not least

104
00:04:07,920 --> 00:04:10,159
sarah shonda

105
00:04:10,159 --> 00:04:12,560
who leads address policy work on

106
00:04:12,560 --> 00:04:15,200
artificial intelligence and specifically

107
00:04:15,200 --> 00:04:17,600
the eu's ai regulation

108
00:04:17,600 --> 00:04:19,680
work that is happening right now she

109
00:04:19,680 --> 00:04:22,400
also works on issues of discrimination

110
00:04:22,400 --> 00:04:24,560
in a digital context

111
00:04:24,560 --> 00:04:27,759
migration related technologies and works

112
00:04:27,759 --> 00:04:30,880
on the process the last little bit about

113
00:04:30,880 --> 00:04:33,199
uh of decolonizing the child rights

114
00:04:33,199 --> 00:04:36,639
field alongside uh digital freedom funds

115
00:04:36,639 --> 00:04:39,759
the organization i work for

116
00:04:39,759 --> 00:04:42,720
thank you all so much for being here um

117
00:04:42,720 --> 00:04:45,520
i'll obviously invite also the

118
00:04:45,520 --> 00:04:47,680
the people intelligence to share

119
00:04:47,680 --> 00:04:50,000
questions remark reactions

120
00:04:50,000 --> 00:04:52,960
uh in the chat and if uh

121
00:04:52,960 --> 00:04:56,000
yeah if at some point you also want

122
00:04:56,000 --> 00:04:58,240
mostly towards the end of the discussion

123
00:04:58,240 --> 00:05:00,560
to just join the discussion with like uh

124
00:05:00,560 --> 00:05:02,639
also open camera please feel free to do

125
00:05:02,639 --> 00:05:03,520
so

126
00:05:03,520 --> 00:05:04,960
um

127
00:05:04,960 --> 00:05:06,560
we have an hour

128
00:05:06,560 --> 00:05:07,440
yeah

129
00:05:07,440 --> 00:05:09,680
we have a bit less than an hour

130
00:05:09,680 --> 00:05:12,400
to discuss a topic that would deserve

131
00:05:12,400 --> 00:05:13,919
much more time

132
00:05:13,919 --> 00:05:14,639
so

133
00:05:14,639 --> 00:05:16,720
we'll try to just um

134
00:05:16,720 --> 00:05:17,919
give it

135
00:05:17,919 --> 00:05:19,280
justice

136
00:05:19,280 --> 00:05:21,199
um

137
00:05:21,199 --> 00:05:22,560
and

138
00:05:22,560 --> 00:05:23,840
i'll begin with

139
00:05:23,840 --> 00:05:26,639
this really clear uh fact

140
00:05:26,639 --> 00:05:29,199
uh that the eu the european union has

141
00:05:29,199 --> 00:05:32,240
the deadliest border in the world

142
00:05:32,240 --> 00:05:34,000
so according to the international

143
00:05:34,000 --> 00:05:36,160
organization for migration

144
00:05:36,160 --> 00:05:37,520
uh it

145
00:05:37,520 --> 00:05:40,320
we've lost around 23

146
00:05:40,320 --> 00:05:41,520
000

147
00:05:41,520 --> 00:05:44,080
uh people that were recording uh either

148
00:05:44,080 --> 00:05:46,240
missing or dead in the mediterranean

149
00:05:46,240 --> 00:05:49,360
since 2014.

150
00:05:49,360 --> 00:05:52,720
uh and we know that the use of digital

151
00:05:52,720 --> 00:05:54,560
tools plays a

152
00:05:54,560 --> 00:05:57,600
role in increasing the dangerousness of

153
00:05:57,600 --> 00:06:00,319
borders for people on the move

154
00:06:00,319 --> 00:06:01,120
so

155
00:06:01,120 --> 00:06:04,560
my first question

156
00:06:05,039 --> 00:06:07,120
is related to data protection obviously

157
00:06:07,120 --> 00:06:09,680
because we know that one of the tools

158
00:06:09,680 --> 00:06:12,080
uh that happens to

159
00:06:12,080 --> 00:06:14,400
make it more dangerous to cross borders

160
00:06:14,400 --> 00:06:16,160
uh for some people

161
00:06:16,160 --> 00:06:18,720
uh are among other things the data

162
00:06:18,720 --> 00:06:20,880
collection and the multiplication of

163
00:06:20,880 --> 00:06:22,720
databases

164
00:06:22,720 --> 00:06:28,000
alina uh smith and abolished vivoski uh

165
00:06:28,000 --> 00:06:28,880
how

166
00:06:28,880 --> 00:06:30,400
would you say

167
00:06:30,400 --> 00:06:32,840
or how do you see the impacts of data

168
00:06:32,840 --> 00:06:36,240
collection and exchange on the rights

169
00:06:36,240 --> 00:06:39,680
of people on on the move firstly and

170
00:06:39,680 --> 00:06:42,000
would you say that data protection laws

171
00:06:42,000 --> 00:06:44,479
in the eu at this stage have

172
00:06:44,479 --> 00:06:46,720
the potential to enhance

173
00:06:46,720 --> 00:06:48,720
the human rights protection and the

174
00:06:48,720 --> 00:06:51,120
safety uh for people on the move so i'm

175
00:06:51,120 --> 00:06:54,400
directing that to firstly elena and then

176
00:06:54,400 --> 00:06:55,120
uh

177
00:06:55,120 --> 00:06:58,160
uh elena smith and then like um voices

178
00:06:58,160 --> 00:07:01,039
divorce but obviously sarah shonda and

179
00:07:01,039 --> 00:07:03,599
teresa quintel feel free to add on to

180
00:07:03,599 --> 00:07:07,360
that uh afterwards the floor is yours uh

181
00:07:07,360 --> 00:07:10,639
mr vivoski

182
00:07:12,800 --> 00:07:15,199
so should i then start so let me first

183
00:07:15,199 --> 00:07:16,960
of all welcome all of you

184
00:07:16,960 --> 00:07:18,400
and uh

185
00:07:18,400 --> 00:07:21,840
say that this this is another time when

186
00:07:21,840 --> 00:07:23,440
i take part in the

187
00:07:23,440 --> 00:07:25,840
privacy camp which i recognize as one of

188
00:07:25,840 --> 00:07:27,919
the most important conferences and

189
00:07:27,919 --> 00:07:29,599
actually starting the year with privacy

190
00:07:29,599 --> 00:07:31,599
camp is an important thing for me that's

191
00:07:31,599 --> 00:07:33,199
something that you expect to hear from

192
00:07:33,199 --> 00:07:34,560
somebody who is

193
00:07:34,560 --> 00:07:36,639
a guest and who somehow

194
00:07:36,639 --> 00:07:38,400
at the same time is the organizer of

195
00:07:38,400 --> 00:07:41,520
this uh meeting and this

196
00:07:41,520 --> 00:07:44,240
forum that is always in the in the

197
00:07:44,240 --> 00:07:46,240
middle of the privacy camp

198
00:07:46,240 --> 00:07:49,840
uh but uh let me say also quite frankly

199
00:07:49,840 --> 00:07:52,160
uh at the beginning that while

200
00:07:52,160 --> 00:07:54,560
uh the discussion with the uh

201
00:07:54,560 --> 00:07:58,000
ngos with the specialists with academics

202
00:07:58,000 --> 00:07:59,759
dealing with human rights is an

203
00:07:59,759 --> 00:08:02,000
important part of our job and it's

204
00:08:02,000 --> 00:08:03,039
opening

205
00:08:03,039 --> 00:08:06,400
us eyes on what's going on in the world

206
00:08:06,400 --> 00:08:09,039
this is not an easy discussion and i'm

207
00:08:09,039 --> 00:08:11,039
not going to say that

208
00:08:11,039 --> 00:08:13,280
i came here in order to

209
00:08:13,280 --> 00:08:15,599
ask you for help to deal

210
00:08:15,599 --> 00:08:18,319
with the problem which is well

211
00:08:18,319 --> 00:08:20,879
understood and all the data protection

212
00:08:20,879 --> 00:08:22,960
authorities in the world

213
00:08:22,960 --> 00:08:25,840
are definitely in favor of

214
00:08:25,840 --> 00:08:27,440
reducing the

215
00:08:27,440 --> 00:08:31,919
uh burden which is put on the migrants

216
00:08:31,919 --> 00:08:32,640
and

217
00:08:32,640 --> 00:08:34,719
that we have the perfect law to deal

218
00:08:34,719 --> 00:08:37,039
with it but we have the bad execution

219
00:08:37,039 --> 00:08:38,559
this is what we are

220
00:08:38,559 --> 00:08:41,599
often hear and what we sometimes expect

221
00:08:41,599 --> 00:08:43,760
from the data protection commissioners

222
00:08:43,760 --> 00:08:44,560
and

223
00:08:44,560 --> 00:08:48,880
when we decided that this is going to be

224
00:08:48,880 --> 00:08:49,760
the

225
00:08:49,760 --> 00:08:52,160
subject of this year's summit

226
00:08:52,160 --> 00:08:54,480
i was really in favor but i was in favor

227
00:08:54,480 --> 00:08:56,240
because this is one of the most

228
00:08:56,240 --> 00:08:58,480
difficult subject that we are dealing

229
00:08:58,480 --> 00:08:59,519
with

230
00:08:59,519 --> 00:09:02,160
not from the technical point of view

231
00:09:02,160 --> 00:09:03,920
not from the organizational point of

232
00:09:03,920 --> 00:09:05,360
view even

233
00:09:05,360 --> 00:09:07,680
it's not that difficult to understand

234
00:09:07,680 --> 00:09:10,640
but it's extremely difficult to convince

235
00:09:10,640 --> 00:09:12,959
the people and also the authorities in

236
00:09:12,959 --> 00:09:15,120
the member states and authorities on the

237
00:09:15,120 --> 00:09:17,680
european level that here we have the

238
00:09:17,680 --> 00:09:19,120
real problem

239
00:09:19,120 --> 00:09:21,839
because the problem that is seen as far

240
00:09:21,839 --> 00:09:23,440
as the

241
00:09:23,440 --> 00:09:25,519
as migrants are concerned

242
00:09:25,519 --> 00:09:27,600
that's first of all the situation the

243
00:09:27,600 --> 00:09:29,839
the the

244
00:09:29,839 --> 00:09:31,279
accidents

245
00:09:31,279 --> 00:09:32,320
the

246
00:09:32,320 --> 00:09:35,120
uh problems to uh

247
00:09:35,120 --> 00:09:36,480
to keep them

248
00:09:36,480 --> 00:09:38,399
alive actually to

249
00:09:38,399 --> 00:09:40,080
to out their

250
00:09:40,080 --> 00:09:42,240
travel to europe the problems with the

251
00:09:42,240 --> 00:09:45,040
camps the problem with the organization

252
00:09:45,040 --> 00:09:46,560
of the

253
00:09:46,560 --> 00:09:48,399
administration

254
00:09:48,399 --> 00:09:50,480
ready for the migrants

255
00:09:50,480 --> 00:09:54,160
but not really the technical

256
00:09:54,160 --> 00:09:56,959
use of the data and the purpose for use

257
00:09:56,959 --> 00:09:59,040
for which the data is used

258
00:09:59,040 --> 00:10:00,480
in fact

259
00:10:00,480 --> 00:10:03,600
uh this is seen as a kind of solution to

260
00:10:03,600 --> 00:10:06,079
the problems which are visible so in

261
00:10:06,079 --> 00:10:08,160
this sense when the data protection

262
00:10:08,160 --> 00:10:11,120
authorities start to say that we have a

263
00:10:11,120 --> 00:10:13,040
problem with the purpose limitation for

264
00:10:13,040 --> 00:10:14,240
example

265
00:10:14,240 --> 00:10:16,959
we got a big pushback from

266
00:10:16,959 --> 00:10:19,839
many sites also from this from some of

267
00:10:19,839 --> 00:10:21,360
the ngos

268
00:10:21,360 --> 00:10:23,760
who say come on this is how we fight

269
00:10:23,760 --> 00:10:26,079
with the problem this is how we try to

270
00:10:26,079 --> 00:10:28,560
organize the things and you are trying

271
00:10:28,560 --> 00:10:32,399
to give these tools away from us so i

272
00:10:32,399 --> 00:10:35,440
would say that the all interoperability

273
00:10:35,440 --> 00:10:38,160
exercise that is done at the moment

274
00:10:38,160 --> 00:10:41,760
is actually done in order to solve the

275
00:10:41,760 --> 00:10:43,839
problem

276
00:10:43,839 --> 00:10:46,240
and our role as the data protection

277
00:10:46,240 --> 00:10:48,399
commissioners is to say that the

278
00:10:48,399 --> 00:10:49,760
solution

279
00:10:49,760 --> 00:10:52,320
is not the real

280
00:10:52,320 --> 00:10:54,560
the

281
00:10:54,640 --> 00:10:58,320
the solution is not to stop immigration

282
00:10:58,320 --> 00:11:00,399
the solution is not to get rid of these

283
00:11:00,399 --> 00:11:02,880
people the solution is not to transport

284
00:11:02,880 --> 00:11:04,800
these people the solution is to

285
00:11:04,800 --> 00:11:07,120
understand that they are the people

286
00:11:07,120 --> 00:11:08,800
and to understand that you are not

287
00:11:08,800 --> 00:11:10,800
crossing the data and you are not

288
00:11:10,800 --> 00:11:13,120
crossing the information you are simply

289
00:11:13,120 --> 00:11:15,519
crossing the people you are trying to

290
00:11:15,519 --> 00:11:17,279
cross the information about the people

291
00:11:17,279 --> 00:11:18,560
so the human

292
00:11:18,560 --> 00:11:22,000
dignity is really at stake so uh when we

293
00:11:22,000 --> 00:11:24,640
think why the data protection issues are

294
00:11:24,640 --> 00:11:26,959
important in this

295
00:11:26,959 --> 00:11:28,560
in this subject

296
00:11:28,560 --> 00:11:29,920
i would start from the general

297
00:11:29,920 --> 00:11:32,240
assumption

298
00:11:32,240 --> 00:11:34,640
all the tools that are prepared are

299
00:11:34,640 --> 00:11:37,680
prepared in the good with the goodwill

300
00:11:37,680 --> 00:11:39,760
the problem is that they are not

301
00:11:39,760 --> 00:11:41,120
respecting

302
00:11:41,120 --> 00:11:42,959
the main principles

303
00:11:42,959 --> 00:11:44,640
that have been done in the data

304
00:11:44,640 --> 00:11:47,279
protection law by purpose

305
00:11:47,279 --> 00:11:49,600
there was a reason to do that

306
00:11:49,600 --> 00:11:52,480
and the purpose limitation minimization

307
00:11:52,480 --> 00:11:55,360
the quality of the data the legality

308
00:11:55,360 --> 00:11:57,600
transparency of the processing

309
00:11:57,600 --> 00:11:59,519
this is all the things which are the

310
00:11:59,519 --> 00:12:02,160
principles of data protection and that

311
00:12:02,160 --> 00:12:04,639
were they were put into the european law

312
00:12:04,639 --> 00:12:06,800
also in the charter as the right to the

313
00:12:06,800 --> 00:12:09,760
personal data protection because we know

314
00:12:09,760 --> 00:12:11,839
that in such of the situations where we

315
00:12:11,839 --> 00:12:13,200
have a problem

316
00:12:13,200 --> 00:12:17,040
we have a tendency to solve it

317
00:12:17,040 --> 00:12:19,839
simply breaking the rights of the people

318
00:12:19,839 --> 00:12:20,880
and

319
00:12:20,880 --> 00:12:22,320
trying to

320
00:12:22,320 --> 00:12:24,639
secure their lives but not secure their

321
00:12:24,639 --> 00:12:27,040
dignity so that that would be the the

322
00:12:27,040 --> 00:12:29,839
initial point of the discussion

323
00:12:29,839 --> 00:12:30,800
on the

324
00:12:30,800 --> 00:12:33,600
use of different large-scale databases

325
00:12:33,600 --> 00:12:36,480
but also all different uh

326
00:12:36,480 --> 00:12:38,800
data that is coming directly from the

327
00:12:38,800 --> 00:12:42,000
from the migrants and in the end the the

328
00:12:42,000 --> 00:12:44,320
the last topic that i would

329
00:12:44,320 --> 00:12:45,519
put in

330
00:12:45,519 --> 00:12:47,120
is the problem that some of the

331
00:12:47,120 --> 00:12:49,279
solutions which we offer

332
00:12:49,279 --> 00:12:52,160
generally in data protection world may

333
00:12:52,160 --> 00:12:55,519
not work with immigrants and one of this

334
00:12:55,519 --> 00:12:57,600
is the consent

335
00:12:57,600 --> 00:13:00,320
the consent is

336
00:13:00,320 --> 00:13:02,160
for many people

337
00:13:02,160 --> 00:13:03,920
not with the good reasons

338
00:13:03,920 --> 00:13:06,160
the queen of the legal of the legal

339
00:13:06,160 --> 00:13:08,399
basis for data processing that's

340
00:13:08,399 --> 00:13:12,240
definitely not true with immigrants

341
00:13:12,240 --> 00:13:14,800
bearing in mind the fact that most of

342
00:13:14,800 --> 00:13:16,320
the people who are at the borders and

343
00:13:16,320 --> 00:13:17,920
try to cross the borders

344
00:13:17,920 --> 00:13:20,800
are ready to consent for anything

345
00:13:20,800 --> 00:13:23,040
to get out of the situation they are at

346
00:13:23,040 --> 00:13:25,839
the very moment so we have to use the

347
00:13:25,839 --> 00:13:27,839
principles of data protection but we

348
00:13:27,839 --> 00:13:30,480
have to use them in the uh reasonable

349
00:13:30,480 --> 00:13:31,600
way

350
00:13:31,600 --> 00:13:33,200
giving in mind that we are not

351
00:13:33,200 --> 00:13:34,880
protecting the data but protecting the

352
00:13:34,880 --> 00:13:37,200
people

353
00:13:38,240 --> 00:13:41,120
thank you very much and also to put the

354
00:13:41,120 --> 00:13:43,199
question of how we define consent in a

355
00:13:43,199 --> 00:13:46,240
con in the context where

356
00:13:46,240 --> 00:13:47,680
there is no

357
00:13:47,680 --> 00:13:50,959
real alternatives uh alina what is your

358
00:13:50,959 --> 00:13:54,880
what is your take on um how much the

359
00:13:54,880 --> 00:13:56,720
data protection is right now are

360
00:13:56,720 --> 00:13:57,920
protecting

361
00:13:57,920 --> 00:14:00,800
the people on the move and uh the impact

362
00:14:00,800 --> 00:14:03,839
of the data collection and data exchange

363
00:14:03,839 --> 00:14:06,720
on their rights

364
00:14:07,120 --> 00:14:09,680
thanks a lot lawrence um

365
00:14:09,680 --> 00:14:11,760
i mean i think i think it's

366
00:14:11,760 --> 00:14:13,839
i think it's striking that just the term

367
00:14:13,839 --> 00:14:16,480
data collection you know data collection

368
00:14:16,480 --> 00:14:18,800
that sounds so innocuous doesn't it uh

369
00:14:18,800 --> 00:14:21,040
sudden super banal you know data

370
00:14:21,040 --> 00:14:23,199
collection data processing

371
00:14:23,199 --> 00:14:25,680
and i think the the banality and kind of

372
00:14:25,680 --> 00:14:26,399
the

373
00:14:26,399 --> 00:14:27,760
um

374
00:14:27,760 --> 00:14:29,600
yeah the boringness in a way of that

375
00:14:29,600 --> 00:14:31,519
term just masks

376
00:14:31,519 --> 00:14:34,639
how incredibly invasive um

377
00:14:34,639 --> 00:14:36,800
this the processing of personal data can

378
00:14:36,800 --> 00:14:38,720
be in the context of migration

379
00:14:38,720 --> 00:14:41,279
and also just the enormous consequences

380
00:14:41,279 --> 00:14:44,399
for individuals of the ways in which

381
00:14:44,399 --> 00:14:46,160
and as we were just hearing the purposes

382
00:14:46,160 --> 00:14:48,480
for which data gets processed in the

383
00:14:48,480 --> 00:14:50,000
migration context

384
00:14:50,000 --> 00:14:52,399
so i guess in terms of i mean maybe just

385
00:14:52,399 --> 00:14:53,839
starting out talking about the harms

386
00:14:53,839 --> 00:14:55,440
like let's talk about the harms of this

387
00:14:55,440 --> 00:14:58,800
thing that sounds so innocuous um

388
00:14:58,800 --> 00:15:00,639
i think the first harm that we've seen

389
00:15:00,639 --> 00:15:01,920
is just

390
00:15:01,920 --> 00:15:04,240
um in terms of the ways in which this

391
00:15:04,240 --> 00:15:05,839
happens and the purposes is this

392
00:15:05,839 --> 00:15:07,440
reinforcement

393
00:15:07,440 --> 00:15:10,320
of this idea of people who are not from

394
00:15:10,320 --> 00:15:11,199
here

395
00:15:11,199 --> 00:15:13,600
not from europe is essentially being

396
00:15:13,600 --> 00:15:15,199
framed as risks that need to be

397
00:15:15,199 --> 00:15:17,680
monitored that need to be checked that

398
00:15:17,680 --> 00:15:20,000
need to be screened um so that's the

399
00:15:20,000 --> 00:15:21,199
default

400
00:15:21,199 --> 00:15:22,240
um

401
00:15:22,240 --> 00:15:24,839
and so of course this is profoundly

402
00:15:24,839 --> 00:15:28,560
discriminatory and stigmatizing

403
00:15:28,560 --> 00:15:30,079
and so you know we heard just a moment

404
00:15:30,079 --> 00:15:31,759
ago about these

405
00:15:31,759 --> 00:15:33,759
interoperable systems

406
00:15:33,759 --> 00:15:36,639
again a very mind-numbingly dull term

407
00:15:36,639 --> 00:15:38,560
but what does that actually mean

408
00:15:38,560 --> 00:15:41,600
um it means that um even outside of the

409
00:15:41,600 --> 00:15:44,079
borders this these processes of

410
00:15:44,079 --> 00:15:46,320
surveillance and checks are happening so

411
00:15:46,320 --> 00:15:48,160
your cousins who want to come to your

412
00:15:48,160 --> 00:15:50,639
wedding uh who are wanting to apply for

413
00:15:50,639 --> 00:15:52,959
visa or maybe for travel authorization

414
00:15:52,959 --> 00:15:55,680
under the new system this year ets

415
00:15:55,680 --> 00:15:57,839
well there's already a screening process

416
00:15:57,839 --> 00:15:59,199
they are putting their personal

417
00:15:59,199 --> 00:16:00,480
information

418
00:16:00,480 --> 00:16:02,160
a whole raft of personal information

419
00:16:02,160 --> 00:16:04,639
into either the visa information system

420
00:16:04,639 --> 00:16:07,680
the uh the travel authorization system

421
00:16:07,680 --> 00:16:08,800
ets

422
00:16:08,800 --> 00:16:10,800
and where is that going well that's

423
00:16:10,800 --> 00:16:11,920
going into

424
00:16:11,920 --> 00:16:13,600
an immense

425
00:16:13,600 --> 00:16:15,120
repository

426
00:16:15,120 --> 00:16:18,639
that can hold up to 300 million records

427
00:16:18,639 --> 00:16:20,639
um can we even get our minds around the

428
00:16:20,639 --> 00:16:22,880
size of that

429
00:16:22,880 --> 00:16:25,519
and being linked up with other databases

430
00:16:25,519 --> 00:16:27,519
that are part of this overarching system

431
00:16:27,519 --> 00:16:29,519
that have nothing to do with visas and

432
00:16:29,519 --> 00:16:31,680
nothing to do with travel authorizations

433
00:16:31,680 --> 00:16:34,000
that have you know maybe have to do with

434
00:16:34,000 --> 00:16:35,279
um

435
00:16:35,279 --> 00:16:36,880
criminal records that have to do with

436
00:16:36,880 --> 00:16:39,360
asylum applications um they're being

437
00:16:39,360 --> 00:16:40,880
linked up with interval interpol

438
00:16:40,880 --> 00:16:44,399
databases europol databases um and the

439
00:16:44,399 --> 00:16:46,320
intention of course is to screen people

440
00:16:46,320 --> 00:16:49,759
for migration health and security risks

441
00:16:49,759 --> 00:16:52,079
um that's that so

442
00:16:52,079 --> 00:16:54,560
that so i think this kind of takes me to

443
00:16:54,560 --> 00:16:57,120
the the second kind of harm in a sense

444
00:16:57,120 --> 00:16:57,839
is

445
00:16:57,839 --> 00:16:59,839
is the ways in which the purposes for

446
00:16:59,839 --> 00:17:00,639
which

447
00:17:00,639 --> 00:17:02,240
personal data is processed in the

448
00:17:02,240 --> 00:17:05,679
immigration context creates a pervasive

449
00:17:05,679 --> 00:17:08,160
web of surveillance and

450
00:17:08,160 --> 00:17:09,119
um

451
00:17:09,119 --> 00:17:11,039
and

452
00:17:11,039 --> 00:17:12,160
and

453
00:17:12,160 --> 00:17:13,760
suspicion

454
00:17:13,760 --> 00:17:15,760
and again it goes far beyond our borders

455
00:17:15,760 --> 00:17:17,760
but of course it includes our borders

456
00:17:17,760 --> 00:17:19,039
but it also includes even in our

457
00:17:19,039 --> 00:17:22,000
communities so that for example in the

458
00:17:22,000 --> 00:17:25,039
netherlands you get stopped by a police

459
00:17:25,039 --> 00:17:27,760
a police officer that police officer on

460
00:17:27,760 --> 00:17:30,799
their app on their mobile phone can

461
00:17:30,799 --> 00:17:33,600
immediately tell if you're undocumented

462
00:17:33,600 --> 00:17:35,200
so that encounter

463
00:17:35,200 --> 00:17:38,160
goes from whatever it started out as to

464
00:17:38,160 --> 00:17:39,840
a potential immigration enforcement

465
00:17:39,840 --> 00:17:42,480
encounter and not only do you face as an

466
00:17:42,480 --> 00:17:44,400
individual you know in the context of an

467
00:17:44,400 --> 00:17:45,840
encounter that

468
00:17:45,840 --> 00:17:48,160
already may be fraught depending on who

469
00:17:48,160 --> 00:17:49,840
you are you now face potential

470
00:17:49,840 --> 00:17:51,360
deportation

471
00:17:51,360 --> 00:17:52,320
so

472
00:17:52,320 --> 00:17:54,960
again where this is happening it's it's

473
00:17:54,960 --> 00:17:56,000
happening

474
00:17:56,000 --> 00:17:58,640
um in a very pervasive way i mean even

475
00:17:58,640 --> 00:18:01,120
again beyond law enforcement you know in

476
00:18:01,120 --> 00:18:02,960
germany for example if you're an

477
00:18:02,960 --> 00:18:05,280
undocumented person who wants to see a

478
00:18:05,280 --> 00:18:06,720
health professional

479
00:18:06,720 --> 00:18:07,679
um

480
00:18:07,679 --> 00:18:09,520
the authority that gives you the

481
00:18:09,520 --> 00:18:11,120
possibility of getting reimbursed for

482
00:18:11,120 --> 00:18:12,400
that treatment

483
00:18:12,400 --> 00:18:15,120
um has an obligation to share your data

484
00:18:15,120 --> 00:18:17,039
with the immigration authorities so

485
00:18:17,039 --> 00:18:18,240
immigration

486
00:18:18,240 --> 00:18:19,840
authorities and the processing of data

487
00:18:19,840 --> 00:18:21,120
for that purpose

488
00:18:21,120 --> 00:18:22,080
then

489
00:18:22,080 --> 00:18:24,240
becomes pervasive even in the health

490
00:18:24,240 --> 00:18:25,200
system

491
00:18:25,200 --> 00:18:26,000
so

492
00:18:26,000 --> 00:18:28,240
we just see the tentacles of this

493
00:18:28,240 --> 00:18:30,080
reaching far and wide and i think the

494
00:18:30,080 --> 00:18:31,440
harms of that

495
00:18:31,440 --> 00:18:35,440
are really hard to even articulate um

496
00:18:35,440 --> 00:18:37,679
and just the kind of oppressiveness of

497
00:18:37,679 --> 00:18:39,520
that web um

498
00:18:39,520 --> 00:18:42,400
which is obviously quite invisible um

499
00:18:42,400 --> 00:18:45,280
but again has enormous implications for

500
00:18:45,280 --> 00:18:46,480
individuals

501
00:18:46,480 --> 00:18:49,440
um and starts with the with the baseline

502
00:18:49,440 --> 00:18:51,520
that they there they are threats

503
00:18:51,520 --> 00:18:54,160
um and just just to briefly just recall

504
00:18:54,160 --> 00:18:55,760
that these whole interoperable systems

505
00:18:55,760 --> 00:18:57,520
so these different systems

506
00:18:57,520 --> 00:19:00,400
um that are now interconnected well why

507
00:19:00,400 --> 00:19:02,240
are they interconnected um the

508
00:19:02,240 --> 00:19:04,559
justification is that well we need to

509
00:19:04,559 --> 00:19:08,000
support efforts to better tackle well

510
00:19:08,000 --> 00:19:10,160
serious threats like terrorism and oh

511
00:19:10,160 --> 00:19:11,919
yes immigration

512
00:19:11,919 --> 00:19:14,160
irregular migration as if those two

513
00:19:14,160 --> 00:19:16,240
things were coextensive so again

514
00:19:16,240 --> 00:19:20,000
reinforcing um this this presumption

515
00:19:20,000 --> 00:19:20,880
that

516
00:19:20,880 --> 00:19:22,880
all foreigners because we're speaking of

517
00:19:22,880 --> 00:19:26,000
all foreigners these databases this big

518
00:19:26,000 --> 00:19:28,480
central repository only includes the

519
00:19:28,480 --> 00:19:30,880
personal data of non-europeans

520
00:19:30,880 --> 00:19:32,720
so i think those are the harms we're

521
00:19:32,720 --> 00:19:34,080
talking about

522
00:19:34,080 --> 00:19:35,760
um even before talking about

523
00:19:35,760 --> 00:19:37,600
surveillance technology literally at

524
00:19:37,600 --> 00:19:39,919
borders and in the mediterranean and so

525
00:19:39,919 --> 00:19:40,720
on

526
00:19:40,720 --> 00:19:43,280
and um and so in terms of are our rules

527
00:19:43,280 --> 00:19:45,200
working i think what we heard is exactly

528
00:19:45,200 --> 00:19:47,520
right we have i mean europe

529
00:19:47,520 --> 00:19:49,600
you know is the standard bearer globally

530
00:19:49,600 --> 00:19:51,679
in terms of data protection but what we

531
00:19:51,679 --> 00:19:53,600
see is the

532
00:19:53,600 --> 00:19:56,000
circumventing the skirting on purpose as

533
00:19:56,000 --> 00:19:59,600
we heard of those very robust rules

534
00:19:59,600 --> 00:20:01,039
in the migration context so the

535
00:20:01,039 --> 00:20:02,960
interoperability rule is passing one

536
00:20:02,960 --> 00:20:04,000
year

537
00:20:04,000 --> 00:20:05,039
after

538
00:20:05,039 --> 00:20:06,480
the gdpr

539
00:20:06,480 --> 00:20:07,520
um

540
00:20:07,520 --> 00:20:09,600
presenting a host of challenges around

541
00:20:09,600 --> 00:20:11,840
purpose limitation as we've just heard

542
00:20:11,840 --> 00:20:14,400
um but justified presumably because of

543
00:20:14,400 --> 00:20:16,960
the nature of the threat

544
00:20:16,960 --> 00:20:20,480
and so i think we have the tools um we

545
00:20:20,480 --> 00:20:23,360
just we're not enforcing them equally

546
00:20:23,360 --> 00:20:26,320
um we're not enforcing them equally um

547
00:20:26,320 --> 00:20:27,919
and of course as we already heard the

548
00:20:27,919 --> 00:20:29,520
bigger challenge of course is not the

549
00:20:29,520 --> 00:20:32,080
technical side of this but of course the

550
00:20:32,080 --> 00:20:33,520
the broader

551
00:20:33,520 --> 00:20:34,640
narrative

552
00:20:34,640 --> 00:20:36,880
and and

553
00:20:36,880 --> 00:20:40,000
accepted kind of um

554
00:20:40,000 --> 00:20:42,159
accepted narrative that actually if this

555
00:20:42,159 --> 00:20:43,200
technology is not creating

556
00:20:43,200 --> 00:20:45,039
discrimination it's not creating

557
00:20:45,039 --> 00:20:46,840
problems it's providing

558
00:20:46,840 --> 00:20:49,760
solutions but it's providing solutions

559
00:20:49,760 --> 00:20:51,760
to an understanding of migration that

560
00:20:51,760 --> 00:20:53,520
itself is something we need to unpack

561
00:20:53,520 --> 00:20:54,960
carefully together maybe in another

562
00:20:54,960 --> 00:20:57,360
session

563
00:20:59,280 --> 00:21:01,679
yeah maybe in another session but we can

564
00:21:01,679 --> 00:21:03,360
try and start now

565
00:21:03,360 --> 00:21:04,240
and

566
00:21:04,240 --> 00:21:05,840
um

567
00:21:05,840 --> 00:21:08,000
that leads me

568
00:21:08,000 --> 00:21:10,720
somehow to my uh to my next question

569
00:21:10,720 --> 00:21:13,039
that is more addressed to sarah

570
00:21:13,039 --> 00:21:15,679
chanda and tereza quintel but obviously

571
00:21:15,679 --> 00:21:17,679
everyone is welcome to

572
00:21:17,679 --> 00:21:19,520
join the conversation among the

573
00:21:19,520 --> 00:21:20,640
panelists

574
00:21:20,640 --> 00:21:21,520
um

575
00:21:21,520 --> 00:21:23,280
obviously one of the barrier is the way

576
00:21:23,280 --> 00:21:25,600
we frame the problem right

577
00:21:25,600 --> 00:21:28,640
and that's what uh elena smith both

578
00:21:28,640 --> 00:21:31,520
alina smith and uh voice vivoriusky were

579
00:21:31,520 --> 00:21:32,640
addressing

580
00:21:32,640 --> 00:21:36,320
uh in the um introductory uh

581
00:21:36,320 --> 00:21:38,559
uh

582
00:21:39,360 --> 00:21:42,000
introductory talk sorry for that

583
00:21:42,000 --> 00:21:43,600
i guess that's the word that is coming

584
00:21:43,600 --> 00:21:46,880
going to come um but what other buyers

585
00:21:46,880 --> 00:21:49,360
or do you want to expand on that barrier

586
00:21:49,360 --> 00:21:51,760
or other barriers that actually prevent

587
00:21:51,760 --> 00:21:55,200
uh a full vigorous uh impactful

588
00:21:55,200 --> 00:21:57,679
enforcement of human rights for

589
00:21:57,679 --> 00:21:59,280
people on the move

590
00:21:59,280 --> 00:22:01,360
specifically when we talk about

591
00:22:01,360 --> 00:22:04,960
data collection or use of ai

592
00:22:04,960 --> 00:22:06,960
in this context

593
00:22:06,960 --> 00:22:09,200
sarah perhaps you want to

594
00:22:09,200 --> 00:22:11,360
begin and then teresa

595
00:22:11,360 --> 00:22:14,639
couldn't tell you could yeah

596
00:22:14,720 --> 00:22:17,280
yeah thank you lahats and i can only say

597
00:22:17,280 --> 00:22:19,440
thank you for all of the intervention so

598
00:22:19,440 --> 00:22:20,720
far

599
00:22:20,720 --> 00:22:23,520
i definitely think that

600
00:22:23,520 --> 00:22:25,440
we have to take this concept further

601
00:22:25,440 --> 00:22:26,640
that

602
00:22:26,640 --> 00:22:28,880
is it that we're here to interrogate

603
00:22:28,880 --> 00:22:31,280
whether data protection can or cannot

604
00:22:31,280 --> 00:22:32,320
help us

605
00:22:32,320 --> 00:22:35,440
in the context of eu migration policies

606
00:22:35,440 --> 00:22:37,679
which are criminalizing people

607
00:22:37,679 --> 00:22:39,520
surveilling people monitoring people on

608
00:22:39,520 --> 00:22:40,559
the move

609
00:22:40,559 --> 00:22:42,480
or is it that such

610
00:22:42,480 --> 00:22:44,240
we have to accept to some degree that

611
00:22:44,240 --> 00:22:46,799
such principles such policies have been

612
00:22:46,799 --> 00:22:48,880
developed by designed

613
00:22:48,880 --> 00:22:51,120
uh in contravention to data protection

614
00:22:51,120 --> 00:22:53,600
principles but not only in contravention

615
00:22:53,600 --> 00:22:54,559
to

616
00:22:54,559 --> 00:22:57,520
basic human dignity as we've heard of

617
00:22:57,520 --> 00:22:59,679
people on the move basic fundamental

618
00:22:59,679 --> 00:23:00,880
rights

619
00:23:00,880 --> 00:23:02,960
and i think more and more from the the

620
00:23:02,960 --> 00:23:04,880
previous two interventions we can see

621
00:23:04,880 --> 00:23:07,840
it's the latter so then what it is

622
00:23:07,840 --> 00:23:09,919
what is it should we be doing about that

623
00:23:09,919 --> 00:23:12,400
once we accept this principle

624
00:23:12,400 --> 00:23:14,000
um i would like to speak a little bit

625
00:23:14,000 --> 00:23:15,440
more about

626
00:23:15,440 --> 00:23:18,240
the expansion of eu migration databases

627
00:23:18,240 --> 00:23:20,320
and expand on what elena has said but

628
00:23:20,320 --> 00:23:22,799
then also speak to some of the barriers

629
00:23:22,799 --> 00:23:25,039
in answer to your question about

630
00:23:25,039 --> 00:23:26,880
how um

631
00:23:26,880 --> 00:23:28,720
the use of surveillance technologies

632
00:23:28,720 --> 00:23:29,520
have

633
00:23:29,520 --> 00:23:32,240
been placed on top of that uh problem on

634
00:23:32,240 --> 00:23:34,000
top of that framework

635
00:23:34,000 --> 00:23:35,600
and what what does that mean for people

636
00:23:35,600 --> 00:23:37,039
on the move

637
00:23:37,039 --> 00:23:39,520
um so elena's already really eloquently

638
00:23:39,520 --> 00:23:41,279
talked about the interoperability

639
00:23:41,279 --> 00:23:43,120
framework which is great so i don't have

640
00:23:43,120 --> 00:23:46,080
to but i think in addition to that

641
00:23:46,080 --> 00:23:47,200
we

642
00:23:47,200 --> 00:23:49,520
would need to really have a good um

643
00:23:49,520 --> 00:23:51,919
understanding as the data protection

644
00:23:51,919 --> 00:23:53,760
community as a digital rights community

645
00:23:53,760 --> 00:23:55,360
of what it is that some of these

646
00:23:55,360 --> 00:23:57,200
databases are doing

647
00:23:57,200 --> 00:23:59,200
so in addition to

648
00:23:59,200 --> 00:24:01,039
visa information system in addition to

649
00:24:01,039 --> 00:24:04,480
ets we also have

650
00:24:04,640 --> 00:24:07,679
databases such as eurodac which are also

651
00:24:07,679 --> 00:24:10,000
expanding a number of very worrying

652
00:24:10,000 --> 00:24:11,840
trends when it comes to the data of

653
00:24:11,840 --> 00:24:13,600
people in the move

654
00:24:13,600 --> 00:24:15,679
some of them include for example

655
00:24:15,679 --> 00:24:17,760
entrenching this link between movement

656
00:24:17,760 --> 00:24:19,760
and criminality which we see to being

657
00:24:19,760 --> 00:24:21,919
done ideologically but we're also seeing

658
00:24:21,919 --> 00:24:24,240
embedded in the legal framework

659
00:24:24,240 --> 00:24:27,520
by focusing on returns or deportations

660
00:24:27,520 --> 00:24:29,679
to use framing particularly

661
00:24:29,679 --> 00:24:30,799
well

662
00:24:30,799 --> 00:24:33,440
such movements are justifying the

663
00:24:33,440 --> 00:24:35,679
exchange more and more exchange of data

664
00:24:35,679 --> 00:24:38,559
between migration authorities and

665
00:24:38,559 --> 00:24:40,559
law enforcement authorities and also

666
00:24:40,559 --> 00:24:42,240
authorities in third countries to

667
00:24:42,240 --> 00:24:44,559
facilitate deportations

668
00:24:44,559 --> 00:24:46,240
we're also seeing in the context of

669
00:24:46,240 --> 00:24:48,720
eurodac which is the database uh with

670
00:24:48,720 --> 00:24:50,240
information on

671
00:24:50,240 --> 00:24:53,200
asylum seekers and irregular migrants

672
00:24:53,200 --> 00:24:54,000
um

673
00:24:54,000 --> 00:24:56,320
being proposed to involve the collection

674
00:24:56,320 --> 00:24:58,880
of biometric data from children as long

675
00:24:58,880 --> 00:25:01,039
as as young as six

676
00:25:01,039 --> 00:25:03,919
we also see that um there is a shift

677
00:25:03,919 --> 00:25:06,000
towards the collection of more

678
00:25:06,000 --> 00:25:08,960
categories of sensitive data from people

679
00:25:08,960 --> 00:25:11,520
on the move asylum seekers including

680
00:25:11,520 --> 00:25:13,919
their facial images this is despite the

681
00:25:13,919 --> 00:25:16,159
fact that these databases already

682
00:25:16,159 --> 00:25:17,120
include

683
00:25:17,120 --> 00:25:19,279
invasive data such as fingerprint

684
00:25:19,279 --> 00:25:21,520
sensitive data such as fingerprints

685
00:25:21,520 --> 00:25:23,279
and we also see that

686
00:25:23,279 --> 00:25:25,200
with respect to these or the comments

687
00:25:25,200 --> 00:25:26,400
that we've already heard when it comes

688
00:25:26,400 --> 00:25:28,960
to consent a foundational principle

689
00:25:28,960 --> 00:25:30,960
index data protection world

690
00:25:30,960 --> 00:25:33,679
people subject to this database almost

691
00:25:33,679 --> 00:25:36,720
have no say no real say in the retrieval

692
00:25:36,720 --> 00:25:39,600
of the data from them so a far cry from

693
00:25:39,600 --> 00:25:41,840
principles of consent of the dataset

694
00:25:41,840 --> 00:25:44,240
the giver and conversations have already

695
00:25:44,240 --> 00:25:46,320
been going in the eu framework about

696
00:25:46,320 --> 00:25:47,279
whether

697
00:25:47,279 --> 00:25:49,760
and how acceptable it could be to use

698
00:25:49,760 --> 00:25:52,000
force to ensure the taking of people's

699
00:25:52,000 --> 00:25:53,840
biometric data

700
00:25:53,840 --> 00:25:56,080
so in essence in addition to all of what

701
00:25:56,080 --> 00:25:57,760
we've we've heard from elena we're

702
00:25:57,760 --> 00:25:59,919
seeing is this expansion of existing

703
00:25:59,919 --> 00:26:02,080
databases that we already have

704
00:26:02,080 --> 00:26:03,760
potentially expanding the extent to

705
00:26:03,760 --> 00:26:05,360
which we can consider them tools of

706
00:26:05,360 --> 00:26:07,600
violence against people

707
00:26:07,600 --> 00:26:10,799
against children also and we're seeing

708
00:26:10,799 --> 00:26:11,840
this like

709
00:26:11,840 --> 00:26:13,840
creation of a surveillance sort of super

710
00:26:13,840 --> 00:26:16,880
structure of information of all people

711
00:26:16,880 --> 00:26:19,600
on the move who happen to be non-eu

712
00:26:19,600 --> 00:26:21,200
nationals

713
00:26:21,200 --> 00:26:23,039
from a race perspective who are these

714
00:26:23,039 --> 00:26:25,360
people they are black and brown people

715
00:26:25,360 --> 00:26:28,240
across the world this is something that

716
00:26:28,240 --> 00:26:30,159
simply by understanding from a data

717
00:26:30,159 --> 00:26:32,480
protection perspective is important from

718
00:26:32,480 --> 00:26:33,919
a fundamental rights perspective is

719
00:26:33,919 --> 00:26:35,520
important but it's also a question of

720
00:26:35,520 --> 00:26:37,679
discrimination as alina says and racial

721
00:26:37,679 --> 00:26:38,799
justice

722
00:26:38,799 --> 00:26:41,440
why are we happy with this creation of

723
00:26:41,440 --> 00:26:44,000
this superstructure against racialized

724
00:26:44,000 --> 00:26:46,159
people across the world

725
00:26:46,159 --> 00:26:48,159
sec my second point is

726
00:26:48,159 --> 00:26:50,640
we're also seeing the

727
00:26:50,640 --> 00:26:53,200
increased interest of policy makers

728
00:26:53,200 --> 00:26:54,559
in this notion of artificial

729
00:26:54,559 --> 00:26:56,000
intelligence

730
00:26:56,000 --> 00:26:58,559
in the context of migration control

731
00:26:58,559 --> 00:27:00,000
so more and more we're seeing

732
00:27:00,000 --> 00:27:02,480
authorities policymakers endorsing the

733
00:27:02,480 --> 00:27:04,880
development the testing

734
00:27:04,880 --> 00:27:07,520
the deployment of ai systems on people

735
00:27:07,520 --> 00:27:09,200
on the news people that are already

736
00:27:09,200 --> 00:27:10,720
discriminated against already

737
00:27:10,720 --> 00:27:11,919
marginalized and have been

738
00:27:11,919 --> 00:27:13,919
systematically prevented from accessing

739
00:27:13,919 --> 00:27:15,279
their rights

740
00:27:15,279 --> 00:27:18,559
and eye border control is a famous

741
00:27:18,559 --> 00:27:20,640
example what many people were aware of

742
00:27:20,640 --> 00:27:22,799
where ai lie detectors were trialled on

743
00:27:22,799 --> 00:27:24,720
people in the course of their visa

744
00:27:24,720 --> 00:27:26,960
application process

745
00:27:26,960 --> 00:27:28,640
but there have been many many more

746
00:27:28,640 --> 00:27:30,799
examples of this and many many more

747
00:27:30,799 --> 00:27:33,679
examples in production in development

748
00:27:33,679 --> 00:27:36,880
particularly under the eu horizon 2020

749
00:27:36,880 --> 00:27:38,799
research program

750
00:27:38,799 --> 00:27:41,760
so basically in its attempt to

751
00:27:41,760 --> 00:27:43,520
ensure trustworthy

752
00:27:43,520 --> 00:27:45,679
ai and to regulate to ensure trust by

753
00:27:45,679 --> 00:27:47,520
the ai we've seen that the eu has

754
00:27:47,520 --> 00:27:49,840
proposed its ai regulation and in the

755
00:27:49,840 --> 00:27:52,080
course of that what he has done is

756
00:27:52,080 --> 00:27:54,080
classified some uses of artificial

757
00:27:54,080 --> 00:27:56,080
intelligence in the migration context as

758
00:27:56,080 --> 00:27:57,919
high risk

759
00:27:57,919 --> 00:27:59,919
and what this means is that

760
00:27:59,919 --> 00:28:01,520
developers of such high-risk

761
00:28:01,520 --> 00:28:03,520
technologies have to undergo a series of

762
00:28:03,520 --> 00:28:05,840
technical checks to make sure the data

763
00:28:05,840 --> 00:28:08,000
quality is strong to make sure that

764
00:28:08,000 --> 00:28:09,840
their use of such technology is in

765
00:28:09,840 --> 00:28:13,200
transparent edu's been investigating

766
00:28:13,200 --> 00:28:15,520
this proposal with a coalition of

767
00:28:15,520 --> 00:28:16,960
digital rights and migrant sites

768
00:28:16,960 --> 00:28:18,480
organizations and we think that there

769
00:28:18,480 --> 00:28:20,799
are a number of main flaws in that

770
00:28:20,799 --> 00:28:22,559
proposal and this could be one of the

771
00:28:22,559 --> 00:28:24,399
things that if we are concerned about

772
00:28:24,399 --> 00:28:26,880
this we could look to change

773
00:28:26,880 --> 00:28:28,799
the first alina is already talked about

774
00:28:28,799 --> 00:28:30,960
risk assessments um

775
00:28:30,960 --> 00:28:33,760
to score individuals for risk of effects

776
00:28:33,760 --> 00:28:37,039
of security illegality health risks

777
00:28:37,039 --> 00:28:38,799
foundationally discriminatory and

778
00:28:38,799 --> 00:28:40,960
foundationally in contravention to

779
00:28:40,960 --> 00:28:43,200
principles of purpose limitation and

780
00:28:43,200 --> 00:28:45,520
other data protection principles consent

781
00:28:45,520 --> 00:28:46,960
etc

782
00:28:46,960 --> 00:28:48,559
we think that the more needs to be done

783
00:28:48,559 --> 00:28:50,159
these are not systems are not just high

784
00:28:50,159 --> 00:28:52,720
risk but actually need to be prohibited

785
00:28:52,720 --> 00:28:55,440
and we need to not endorse this

786
00:28:55,440 --> 00:28:57,200
holy and non-consensual and

787
00:28:57,200 --> 00:28:58,960
discriminatory use of risk assessment

788
00:28:58,960 --> 00:29:01,440
systems as surveillance technologies as

789
00:29:01,440 --> 00:29:02,960
discriminatory technologies against

790
00:29:02,960 --> 00:29:05,360
people on the move

791
00:29:05,360 --> 00:29:08,080
the second issue is that the eu's ai

792
00:29:08,080 --> 00:29:10,799
proposal has a loophole in it for ai

793
00:29:10,799 --> 00:29:12,640
systems that are used in the course of

794
00:29:12,640 --> 00:29:13,840
large-scale

795
00:29:13,840 --> 00:29:17,279
it databases so if we're using ai in the

796
00:29:17,279 --> 00:29:20,240
context of neurodac or another

797
00:29:20,240 --> 00:29:22,880
large-scale database the proposal pretty

798
00:29:22,880 --> 00:29:24,640
much provides a loophole for those

799
00:29:24,640 --> 00:29:26,159
systems so that the rules in the

800
00:29:26,159 --> 00:29:29,279
proposed ai legislation do not apply

801
00:29:29,279 --> 00:29:32,640
obviously this is a vast

802
00:29:32,640 --> 00:29:34,799
flaw in the potential legislation and if

803
00:29:34,799 --> 00:29:36,559
we're talking about not creating a

804
00:29:36,559 --> 00:29:39,360
second class of data protection a second

805
00:29:39,360 --> 00:29:41,360
class of fundamental rights for people

806
00:29:41,360 --> 00:29:42,880
simply by the nature of the fact that

807
00:29:42,880 --> 00:29:44,960
they are moving that they are migrants

808
00:29:44,960 --> 00:29:46,640
that they are non-white this is

809
00:29:46,640 --> 00:29:48,159
something that we really need to attack

810
00:29:48,159 --> 00:29:50,080
to ensure the accountability of eu

811
00:29:50,080 --> 00:29:51,919
institutions

812
00:29:51,919 --> 00:29:53,679
and then lastly the and and this is

813
00:29:53,679 --> 00:29:54,960
where i'll stop there are a number of

814
00:29:54,960 --> 00:29:56,640
flaws in the legislative proposal in

815
00:29:56,640 --> 00:29:58,320
terms of the types of technologies that

816
00:29:58,320 --> 00:29:59,200
these

817
00:29:59,200 --> 00:30:00,640
um that aren't being currently

818
00:30:00,640 --> 00:30:02,000
unregulated

819
00:30:02,000 --> 00:30:04,480
so far in the legislative proposal on ai

820
00:30:04,480 --> 00:30:06,799
there is no mention of the use of

821
00:30:06,799 --> 00:30:09,200
predictive analytic systems

822
00:30:09,200 --> 00:30:11,120
so and would we and this is particularly

823
00:30:11,120 --> 00:30:12,480
worrying because we've seen some

824
00:30:12,480 --> 00:30:15,120
examples of such systems being used in

825
00:30:15,120 --> 00:30:17,600
even to facilitate the interdiction of

826
00:30:17,600 --> 00:30:18,720
votes

827
00:30:18,720 --> 00:30:20,480
of course completely undermining the

828
00:30:20,480 --> 00:30:22,080
right to asylum and the principle of

829
00:30:22,080 --> 00:30:24,000
normal fool among this will be a

830
00:30:24,000 --> 00:30:25,360
particularly interesting question

831
00:30:25,360 --> 00:30:27,440
because often this is not identifiable

832
00:30:27,440 --> 00:30:29,600
data this is not data about one

833
00:30:29,600 --> 00:30:31,919
particular person is that

834
00:30:31,919 --> 00:30:34,240
data about people on their aggregate how

835
00:30:34,240 --> 00:30:35,840
do we feel about this as a data

836
00:30:35,840 --> 00:30:37,679
protection community as a digital rights

837
00:30:37,679 --> 00:30:39,520
community and what resources and

838
00:30:39,520 --> 00:30:41,760
mechanisms do we have to contest this

839
00:30:41,760 --> 00:30:43,840
that aren't based on personally

840
00:30:43,840 --> 00:30:47,520
identifiable data but big data generally

841
00:30:47,520 --> 00:30:50,480
non-identified by the data the second

842
00:30:50,480 --> 00:30:52,799
point that we don't see in the ai

843
00:30:52,799 --> 00:30:54,799
regulation is that we see mass

844
00:30:54,799 --> 00:30:56,559
surveillance technologies being used at

845
00:30:56,559 --> 00:30:58,399
the border but that don't rely on that

846
00:30:58,399 --> 00:31:00,880
identification of people but rather

847
00:31:00,880 --> 00:31:02,960
simply the detection of whether a human

848
00:31:02,960 --> 00:31:05,519
is there of course we've seen this is

849
00:31:05,519 --> 00:31:07,200
being developed for the purposes of

850
00:31:07,200 --> 00:31:09,039
identifying people stopping people at

851
00:31:09,039 --> 00:31:11,039
the border and pushing back

852
00:31:11,039 --> 00:31:14,480
um the horizon 2020 fold-out program is

853
00:31:14,480 --> 00:31:16,720
one of them and this is again another

854
00:31:16,720 --> 00:31:18,559
question about the centrality of the

855
00:31:18,559 --> 00:31:20,320
concept of identification and data

856
00:31:20,320 --> 00:31:22,399
protection law if we center everything

857
00:31:22,399 --> 00:31:24,559
on that then we miss these broader uses

858
00:31:24,559 --> 00:31:26,720
of analytic ai technology which are

859
00:31:26,720 --> 00:31:28,159
still being used

860
00:31:28,159 --> 00:31:30,480
in the course of the surveillance

861
00:31:30,480 --> 00:31:32,480
mechanism the surveillance industrial

862
00:31:32,480 --> 00:31:34,960
complex that we have currently at eu

863
00:31:34,960 --> 00:31:37,039
borders how we're going to address that

864
00:31:37,039 --> 00:31:39,440
i think is a key question but we

865
00:31:39,440 --> 00:31:41,279
necessarily need the data protection

866
00:31:41,279 --> 00:31:43,279
community we necessarily the digital

867
00:31:43,279 --> 00:31:45,679
rights community to take hold of them if

868
00:31:45,679 --> 00:31:47,279
we're going to address them

869
00:31:47,279 --> 00:31:49,919
i'll stop there

870
00:31:51,360 --> 00:31:54,000
thank you very much for all these

871
00:31:54,000 --> 00:31:56,000
interventions and this very important

872
00:31:56,000 --> 00:31:59,919
point um i think i would rather speak to

873
00:31:59,919 --> 00:32:02,880
about the more practical issues of

874
00:32:02,880 --> 00:32:05,519
interoperability and the connection of

875
00:32:05,519 --> 00:32:07,360
all these large-scale informa

876
00:32:07,360 --> 00:32:09,360
information systems so when we speak

877
00:32:09,360 --> 00:32:11,360
about migrants and asylum seekers and

878
00:32:11,360 --> 00:32:12,880
the framework of

879
00:32:12,880 --> 00:32:15,120
eu large-scale databases and their

880
00:32:15,120 --> 00:32:16,640
interoperability

881
00:32:16,640 --> 00:32:18,559
i see the biggest limitation or the

882
00:32:18,559 --> 00:32:21,679
biggest barrier for those people and the

883
00:32:21,679 --> 00:32:23,360
complexity and the vagueness of the

884
00:32:23,360 --> 00:32:25,600
system and the difficulty to

885
00:32:25,600 --> 00:32:27,200
to understand

886
00:32:27,200 --> 00:32:30,559
how one's personal data is stored in the

887
00:32:30,559 --> 00:32:32,320
databases

888
00:32:32,320 --> 00:32:34,080
and how they are being connected and

889
00:32:34,080 --> 00:32:35,120
even

890
00:32:35,120 --> 00:32:37,519
i mean even people working in this topic

891
00:32:37,519 --> 00:32:39,360
since many years have difficulties

892
00:32:39,360 --> 00:32:41,240
understanding the system and the

893
00:32:41,240 --> 00:32:43,919
foreseeability requirement is basically

894
00:32:43,919 --> 00:32:46,320
non-existent where people do not

895
00:32:46,320 --> 00:32:48,880
understand the the system and cannot

896
00:32:48,880 --> 00:32:50,559
understand what is happening to their

897
00:32:50,559 --> 00:32:52,880
personal data and then this will of

898
00:32:52,880 --> 00:32:54,240
course

899
00:32:54,240 --> 00:32:57,279
affect how they can exercise their

900
00:32:57,279 --> 00:32:59,840
rights uh to understand

901
00:32:59,840 --> 00:33:02,159
uh how the processing in the system

902
00:33:02,159 --> 00:33:04,559
works because each database

903
00:33:04,559 --> 00:33:07,039
so alina already explained a bit that

904
00:33:07,039 --> 00:33:08,000
you have

905
00:33:08,000 --> 00:33:09,760
different databases some of them are

906
00:33:09,760 --> 00:33:12,880
already operational some of them are

907
00:33:12,880 --> 00:33:15,120
being developed right now and then they

908
00:33:15,120 --> 00:33:17,679
are supposed to be interconnected and

909
00:33:17,679 --> 00:33:20,080
each database has particular provisions

910
00:33:20,080 --> 00:33:22,399
for the exercise of data subject rights

911
00:33:22,399 --> 00:33:25,600
the interoperability uh regulations and

912
00:33:25,600 --> 00:33:28,320
yet yet other provisions

913
00:33:28,320 --> 00:33:30,240
and in some cases those provisions are

914
00:33:30,240 --> 00:33:32,880
very incoherent and it's difficult to

915
00:33:32,880 --> 00:33:35,039
to understand

916
00:33:35,039 --> 00:33:38,080
this highly technical setup of different

917
00:33:38,080 --> 00:33:39,919
databases

918
00:33:39,919 --> 00:33:41,919
of different systems

919
00:33:41,919 --> 00:33:44,240
that refer to other laws that you also

920
00:33:44,240 --> 00:33:46,240
have to then read so if you're not a

921
00:33:46,240 --> 00:33:48,720
lawyer and best case scenario would be

922
00:33:48,720 --> 00:33:51,039
an eu data protection lawyer specialized

923
00:33:51,039 --> 00:33:53,279
in large scale information system it

924
00:33:53,279 --> 00:33:54,799
will take you days or weeks to

925
00:33:54,799 --> 00:33:56,880
understand this

926
00:33:56,880 --> 00:33:58,720
you i also have to of course understand

927
00:33:58,720 --> 00:34:00,960
the law you have to understand the how

928
00:34:00,960 --> 00:34:03,360
the law is written and it will take you

929
00:34:03,360 --> 00:34:06,240
a lot of time to understand the system

930
00:34:06,240 --> 00:34:08,480
uh whom to contact in order to exercise

931
00:34:08,480 --> 00:34:10,239
your rights what will happen to your

932
00:34:10,239 --> 00:34:11,679
data

933
00:34:11,679 --> 00:34:12,480
and

934
00:34:12,480 --> 00:34:14,719
of course then we also have to look at

935
00:34:14,719 --> 00:34:16,879
additional actors that are processing

936
00:34:16,879 --> 00:34:19,280
personal data in the system especially

937
00:34:19,280 --> 00:34:22,239
if we were if we look at eu agencies

938
00:34:22,239 --> 00:34:24,960
such as uh frontiers or europoids

939
00:34:24,960 --> 00:34:26,800
becoming even more difficult that there

940
00:34:26,800 --> 00:34:29,440
are different levels different layers of

941
00:34:29,440 --> 00:34:31,520
actors and authorities that have access

942
00:34:31,520 --> 00:34:34,239
to the the systems uh because the

943
00:34:34,239 --> 00:34:37,040
procedures are different the different

944
00:34:37,040 --> 00:34:39,199
uh to the ones that apply when you're

945
00:34:39,199 --> 00:34:40,879
contacting national authorities to

946
00:34:40,879 --> 00:34:43,599
exercise your rights um

947
00:34:43,599 --> 00:34:45,440
and with regard to europe or what we've

948
00:34:45,440 --> 00:34:47,280
seen in the very recent

949
00:34:47,280 --> 00:34:49,199
past is that

950
00:34:49,199 --> 00:34:51,359
the trend of how they are processing the

951
00:34:51,359 --> 00:34:54,480
the personal data is very worrying uh

952
00:34:54,480 --> 00:34:57,599
and i wouldn't be

953
00:34:57,599 --> 00:34:59,200
very sure in

954
00:34:59,200 --> 00:35:02,079
respect of their being

955
00:35:02,079 --> 00:35:04,240
processing and data protection rules

956
00:35:04,240 --> 00:35:07,599
applied in a very fair and awful manner

957
00:35:07,599 --> 00:35:10,800
and if we look at frontex the agency was

958
00:35:10,800 --> 00:35:12,880
initially supposed to have only an

959
00:35:12,880 --> 00:35:14,400
assisting role

960
00:35:14,400 --> 00:35:16,320
to the member state authorities and is

961
00:35:16,320 --> 00:35:18,560
now developing into some kind of an

962
00:35:18,560 --> 00:35:20,720
information hub

963
00:35:20,720 --> 00:35:23,280
a bit like europol and the pro from

964
00:35:23,280 --> 00:35:25,839
problem with regard to frontex is that

965
00:35:25,839 --> 00:35:28,079
their data protection office consists of

966
00:35:28,079 --> 00:35:31,119
basically two people if not even europe

967
00:35:31,119 --> 00:35:33,760
would are managing their data in the

968
00:35:33,760 --> 00:35:36,800
correct manner how will frontex which

969
00:35:36,800 --> 00:35:39,760
now has to oversee not only staff and

970
00:35:39,760 --> 00:35:41,839
the the additional standing corps but

971
00:35:41,839 --> 00:35:43,920
also the data it receives from the

972
00:35:43,920 --> 00:35:45,920
member states the data in the in the

973
00:35:45,920 --> 00:35:47,200
databases

974
00:35:47,200 --> 00:35:48,400
uh

975
00:35:48,400 --> 00:35:49,839
some of which they are supposed to

976
00:35:49,839 --> 00:35:52,960
manage and be the controller of uh then

977
00:35:52,960 --> 00:35:54,960
manage to actually comply with all the

978
00:35:54,960 --> 00:35:56,480
rules that they should

979
00:35:56,480 --> 00:35:59,440
and finally for individuals it will be

980
00:35:59,440 --> 00:36:01,440
very difficult to understand

981
00:36:01,440 --> 00:36:02,320
um

982
00:36:02,320 --> 00:36:04,960
how to contact supervisory authorities

983
00:36:04,960 --> 00:36:07,119
in the member states of course that is

984
00:36:07,119 --> 00:36:10,320
not the case in all countries

985
00:36:10,320 --> 00:36:12,240
in some countries uh supervisory

986
00:36:12,240 --> 00:36:14,720
authorities reply to requests in a

987
00:36:14,720 --> 00:36:17,200
reasonable time frame but it really

988
00:36:17,200 --> 00:36:18,640
depends on the country where you're

989
00:36:18,640 --> 00:36:20,480
submitting your request so it could be a

990
00:36:20,480 --> 00:36:22,720
country like germany where each federal

991
00:36:22,720 --> 00:36:24,560
state has a different

992
00:36:24,560 --> 00:36:28,160
dpa and obviously it's better always to

993
00:36:28,160 --> 00:36:29,760
speak the language to understand the

994
00:36:29,760 --> 00:36:30,640
laws

995
00:36:30,640 --> 00:36:33,760
um so it will be extremely burdensome to

996
00:36:33,760 --> 00:36:35,440
exercise your rights in this

997
00:36:35,440 --> 00:36:37,200
interoperable system

998
00:36:37,200 --> 00:36:40,000
and if we look at the criminalization of

999
00:36:40,000 --> 00:36:42,640
migrants the country nationalists i see

1000
00:36:42,640 --> 00:36:45,440
another limitation or barrier in

1001
00:36:45,440 --> 00:36:47,760
especially in data protection law

1002
00:36:47,760 --> 00:36:48,720
for

1003
00:36:48,720 --> 00:36:51,200
which is uh that where data protection

1004
00:36:51,200 --> 00:36:53,359
rules that should only apply in the in

1005
00:36:53,359 --> 00:36:55,520
the criminal law enforcement context

1006
00:36:55,520 --> 00:36:57,280
suddenly are being applied in the

1007
00:36:57,280 --> 00:37:00,160
migration context and those rules leave

1008
00:37:00,160 --> 00:37:03,040
much more leeway to the authorities for

1009
00:37:03,040 --> 00:37:05,359
example to restrict data subject rights

1010
00:37:05,359 --> 00:37:06,560
or

1011
00:37:06,560 --> 00:37:08,640
to transfer personal data to third

1012
00:37:08,640 --> 00:37:10,800
countries so for example the countries

1013
00:37:10,800 --> 00:37:11,599
where

1014
00:37:11,599 --> 00:37:12,400
uh

1015
00:37:12,400 --> 00:37:14,560
these individuals come from and this

1016
00:37:14,560 --> 00:37:16,800
could have serious repercussions when

1017
00:37:16,800 --> 00:37:18,560
their their personal data are being

1018
00:37:18,560 --> 00:37:21,040
shared with their the authorities in

1019
00:37:21,040 --> 00:37:22,400
their home countries

1020
00:37:22,400 --> 00:37:24,240
and in the framework of my pdt i

1021
00:37:24,240 --> 00:37:26,000
contacted several border guards of

1022
00:37:26,000 --> 00:37:28,480
course guys border police in order to

1023
00:37:28,480 --> 00:37:31,200
ask in what kind of situations they

1024
00:37:31,200 --> 00:37:32,800
apply these law enforcement data

1025
00:37:32,800 --> 00:37:34,960
protection rules and they all replied

1026
00:37:34,960 --> 00:37:36,800
that the rules are being applied as soon

1027
00:37:36,800 --> 00:37:37,599
as

1028
00:37:37,599 --> 00:37:40,880
there's some suspicion of crime and in

1029
00:37:40,880 --> 00:37:42,880
this criminalization of migration

1030
00:37:42,880 --> 00:37:44,480
context this means

1031
00:37:44,480 --> 00:37:46,240
as soon as someone is crossing the

1032
00:37:46,240 --> 00:37:47,359
border

1033
00:37:47,359 --> 00:37:50,320
we call it irregularly which is not an

1034
00:37:50,320 --> 00:37:52,079
illegal crossing

1035
00:37:52,079 --> 00:37:54,000
but then does not for example apply for

1036
00:37:54,000 --> 00:37:55,200
asylum

1037
00:37:55,200 --> 00:37:57,680
this could then lead to the application

1038
00:37:57,680 --> 00:38:00,960
of the much more lenient rules

1039
00:38:00,960 --> 00:38:02,400
under which

1040
00:38:02,400 --> 00:38:04,640
authorities can also restrict data

1041
00:38:04,640 --> 00:38:07,680
subject rights much more easily and this

1042
00:38:07,680 --> 00:38:10,240
transparency principle that generally

1043
00:38:10,240 --> 00:38:12,320
exists under data protection law what

1044
00:38:12,320 --> 00:38:14,800
alina also mentioned

1045
00:38:14,800 --> 00:38:16,800
does not have to be respected anymore in

1046
00:38:16,800 --> 00:38:19,440
the same way so this is where i see the

1047
00:38:19,440 --> 00:38:20,800
practice

1048
00:38:20,800 --> 00:38:23,599
barriers for individuals whose personal

1049
00:38:23,599 --> 00:38:25,760
data are being stored and processed

1050
00:38:25,760 --> 00:38:28,960
in those databases

1051
00:38:31,040 --> 00:38:33,359
thank you very much um

1052
00:38:33,359 --> 00:38:36,880
yeah i i had thoughts when

1053
00:38:36,880 --> 00:38:40,400
uh sarah you were speaking about

1054
00:38:40,400 --> 00:38:42,320
the fact that there was like a control

1055
00:38:42,320 --> 00:38:43,040
of

1056
00:38:43,040 --> 00:38:44,720
people as a group

1057
00:38:44,720 --> 00:38:46,480
as communities and that's also like a

1058
00:38:46,480 --> 00:38:49,599
way of aggregating data that might not

1059
00:38:49,599 --> 00:38:50,800
also

1060
00:38:50,800 --> 00:38:52,079
um

1061
00:38:52,079 --> 00:38:55,119
lean into the more privacy centered way

1062
00:38:55,119 --> 00:38:56,880
of

1063
00:38:56,880 --> 00:38:58,839
conceiving data

1064
00:38:58,839 --> 00:39:01,599
protection and also when we talk about

1065
00:39:01,599 --> 00:39:03,680
access to justice and i think that's a

1066
00:39:03,680 --> 00:39:05,920
lot of what you are referring to as well

1067
00:39:05,920 --> 00:39:09,040
there is a practical barriers to access

1068
00:39:09,040 --> 00:39:11,200
rights when you are in a specific

1069
00:39:11,200 --> 00:39:12,400
position

1070
00:39:12,400 --> 00:39:14,160
um

1071
00:39:14,160 --> 00:39:15,280
that is

1072
00:39:15,280 --> 00:39:17,920
something that people on the move

1073
00:39:17,920 --> 00:39:20,240
know daily right they have this

1074
00:39:20,240 --> 00:39:23,200
experience daily uh elena you spoke

1075
00:39:23,200 --> 00:39:24,240
about it

1076
00:39:24,240 --> 00:39:26,240
quite eloquently it's something that is

1077
00:39:26,240 --> 00:39:28,160
part of like

1078
00:39:28,160 --> 00:39:30,320
their reality

1079
00:39:30,320 --> 00:39:31,680
in uh

1080
00:39:31,680 --> 00:39:34,000
in member states uh of the european

1081
00:39:34,000 --> 00:39:35,119
union

1082
00:39:35,119 --> 00:39:36,000
and

1083
00:39:36,000 --> 00:39:38,880
weirdly enough though we don't see them

1084
00:39:38,880 --> 00:39:41,280
uh often on the far front when it comes

1085
00:39:41,280 --> 00:39:43,119
to

1086
00:39:43,119 --> 00:39:46,000
doing advocacy work when it comes to

1087
00:39:46,000 --> 00:39:47,280
actually leading

1088
00:39:47,280 --> 00:39:50,160
a strategic litigation effort we don't

1089
00:39:50,160 --> 00:39:52,240
see them

1090
00:39:52,240 --> 00:39:56,000
also in key positions or in key places

1091
00:39:56,000 --> 00:39:57,359
to talk

1092
00:39:57,359 --> 00:39:58,320
with

1093
00:39:58,320 --> 00:40:01,520
relevant authorities and i was wondering

1094
00:40:01,520 --> 00:40:02,800
how

1095
00:40:02,800 --> 00:40:06,079
do you as organizations as

1096
00:40:06,079 --> 00:40:07,599
institutions

1097
00:40:07,599 --> 00:40:11,839
work with include uh people on the move

1098
00:40:11,839 --> 00:40:14,079
and their expertise and not only their

1099
00:40:14,079 --> 00:40:16,240
experience in the way you were uh you

1100
00:40:16,240 --> 00:40:19,680
talk and you work on those uh subjects

1101
00:40:19,680 --> 00:40:22,960
and as we didn't uh hear

1102
00:40:22,960 --> 00:40:25,119
you're the first person we heard and we

1103
00:40:25,119 --> 00:40:28,079
haven't heard uh you since then i'll

1104
00:40:28,079 --> 00:40:30,240
just invite perhaps uh

1105
00:40:30,240 --> 00:40:33,119
um the data protection uh supervisor um

1106
00:40:33,119 --> 00:40:39,640
mr vivoski uh to uh begin with that

1107
00:40:41,119 --> 00:40:43,680
okay i also now turn the thing that has

1108
00:40:43,680 --> 00:40:46,160
to work according to the law according

1109
00:40:46,160 --> 00:40:47,200
to the

1110
00:40:47,200 --> 00:40:50,240
rules which have been established by the

1111
00:40:50,240 --> 00:40:52,560
european union and us

1112
00:40:52,560 --> 00:40:55,119
the authority which is not the supra

1113
00:40:55,119 --> 00:40:57,359
european authority in data protection

1114
00:40:57,359 --> 00:41:00,160
but uh the one that is supervising only

1115
00:41:00,160 --> 00:41:02,000
the european institutions bodies and

1116
00:41:02,000 --> 00:41:04,720
agencies including those working on the

1117
00:41:04,720 --> 00:41:08,160
migrant issues and in the border issues

1118
00:41:08,160 --> 00:41:10,240
we do not have

1119
00:41:10,240 --> 00:41:13,200
the practical oversight over the whole

1120
00:41:13,200 --> 00:41:14,000
pro

1121
00:41:14,000 --> 00:41:16,240
problem we have to do it together with

1122
00:41:16,240 --> 00:41:18,000
the data protection authorities of the

1123
00:41:18,000 --> 00:41:19,440
member states

1124
00:41:19,440 --> 00:41:21,040
and it also means

1125
00:41:21,040 --> 00:41:23,520
that we do not have the

1126
00:41:23,520 --> 00:41:24,720
permanent

1127
00:41:24,720 --> 00:41:27,839
contact with the representatives of the

1128
00:41:27,839 --> 00:41:30,000
people of on the move and to be frank

1129
00:41:30,000 --> 00:41:32,000
one of the main reasons

1130
00:41:32,000 --> 00:41:34,400
for which we decided that this topic is

1131
00:41:34,400 --> 00:41:37,280
an interesting topic for the edps

1132
00:41:37,280 --> 00:41:38,720
was to ask

1133
00:41:38,720 --> 00:41:41,040
you as those who have the experience in

1134
00:41:41,040 --> 00:41:42,480
this field

1135
00:41:42,480 --> 00:41:45,200
where should we look this first hand

1136
00:41:45,200 --> 00:41:46,480
let's say

1137
00:41:46,480 --> 00:41:49,839
the the practical experiences because we

1138
00:41:49,839 --> 00:41:51,520
at the moment see them when we do

1139
00:41:51,520 --> 00:41:52,960
inspections

1140
00:41:52,960 --> 00:41:55,760
so we are actually inspecting one side

1141
00:41:55,760 --> 00:41:57,440
of the story

1142
00:41:57,440 --> 00:42:00,960
not having any concrete case against

1143
00:42:00,960 --> 00:42:03,200
that usually because the people are not

1144
00:42:03,200 --> 00:42:04,400
uh

1145
00:42:04,400 --> 00:42:07,119
complaining directly to the edps

1146
00:42:07,119 --> 00:42:10,400
so we are we see how organized it is

1147
00:42:10,400 --> 00:42:12,160
from the side of the institution which

1148
00:42:12,160 --> 00:42:13,920
is taking part in it and let me at this

1149
00:42:13,920 --> 00:42:15,119
moment

1150
00:42:15,119 --> 00:42:16,480
addressed something which has been

1151
00:42:16,480 --> 00:42:18,800
already addressed before by alina

1152
00:42:18,800 --> 00:42:21,760
but then later on was uh

1153
00:42:21,760 --> 00:42:24,640
was developed by teresa

1154
00:42:24,640 --> 00:42:27,760
the problem of the results which we are

1155
00:42:27,760 --> 00:42:28,960
inspecting

1156
00:42:28,960 --> 00:42:31,760
that's not that easy that we have one

1157
00:42:31,760 --> 00:42:35,760
huge database of millions billions or

1158
00:42:35,760 --> 00:42:37,119
trillions

1159
00:42:37,119 --> 00:42:39,920
of the records and the uh

1160
00:42:39,920 --> 00:42:41,760
and the

1161
00:42:41,760 --> 00:42:42,880
values

1162
00:42:42,880 --> 00:42:46,079
no we do have actually the ecosystem of

1163
00:42:46,079 --> 00:42:48,079
different sources of information an

1164
00:42:48,079 --> 00:42:50,480
ecosystem of different resources which

1165
00:42:50,480 --> 00:42:53,280
are managed by different uh authorities

1166
00:42:53,280 --> 00:42:55,520
so actually the whole concept of

1167
00:42:55,520 --> 00:42:57,280
interoperability

1168
00:42:57,280 --> 00:42:59,599
is good for those who are introducing it

1169
00:42:59,599 --> 00:43:00,720
because it

1170
00:43:00,720 --> 00:43:04,000
requires it allows to have an access to

1171
00:43:04,000 --> 00:43:06,240
the data which is stored

1172
00:43:06,240 --> 00:43:07,920
in different places

1173
00:43:07,920 --> 00:43:10,560
and managed by different controllers but

1174
00:43:10,560 --> 00:43:12,640
at the same time it means

1175
00:43:12,640 --> 00:43:14,960
that the possibility to oversight the

1176
00:43:14,960 --> 00:43:17,920
whole ecosystem does not exist

1177
00:43:17,920 --> 00:43:19,680
because being the data protection

1178
00:43:19,680 --> 00:43:20,880
commissioner

1179
00:43:20,880 --> 00:43:24,640
i have the right to to uh to

1180
00:43:24,640 --> 00:43:27,680
check from both sides the connections

1181
00:43:27,680 --> 00:43:30,960
the inter the this interoperable

1182
00:43:30,960 --> 00:43:33,839
databases and interrupt those large

1183
00:43:33,839 --> 00:43:35,440
resources

1184
00:43:35,440 --> 00:43:38,240
have with themselves but not necessarily

1185
00:43:38,240 --> 00:43:41,119
with the data which is in the national

1186
00:43:41,119 --> 00:43:42,880
capacities which is in the national

1187
00:43:42,880 --> 00:43:43,920
resources

1188
00:43:43,920 --> 00:43:45,359
so

1189
00:43:45,359 --> 00:43:50,000
we feel partly blind to what's going on

1190
00:43:50,000 --> 00:43:52,640
though at the same time i have to say

1191
00:43:52,640 --> 00:43:54,800
that we understand

1192
00:43:54,800 --> 00:43:56,160
why

1193
00:43:56,160 --> 00:43:58,960
there is no idea of creating the one

1194
00:43:58,960 --> 00:44:00,800
resource instead of all the databases

1195
00:44:00,800 --> 00:44:02,400
then that's obvious

1196
00:44:02,400 --> 00:44:03,280
and

1197
00:44:03,280 --> 00:44:05,200
the the last problem which is connected

1198
00:44:05,200 --> 00:44:07,680
with the fact that we have the different

1199
00:44:07,680 --> 00:44:11,119
resources it's also the fact that both

1200
00:44:11,119 --> 00:44:13,520
the technical solutions

1201
00:44:13,520 --> 00:44:15,359
and the law

1202
00:44:15,359 --> 00:44:17,920
concerning each of this

1203
00:44:17,920 --> 00:44:21,720
each of these resources is developing

1204
00:44:21,720 --> 00:44:23,280
simultaneously

1205
00:44:23,280 --> 00:44:25,359
but somehow independently one from

1206
00:44:25,359 --> 00:44:26,480
another

1207
00:44:26,480 --> 00:44:29,920
so while we have the changes in one of

1208
00:44:29,920 --> 00:44:31,680
the resources

1209
00:44:31,680 --> 00:44:34,319
we can have the effect

1210
00:44:34,319 --> 00:44:36,880
on the searches which are done for the

1211
00:44:36,880 --> 00:44:38,880
for the other purposes

1212
00:44:38,880 --> 00:44:40,480
but we don't see

1213
00:44:40,480 --> 00:44:41,920
immediately

1214
00:44:41,920 --> 00:44:43,839
that the result of the change of the

1215
00:44:43,839 --> 00:44:45,119
results

1216
00:44:45,119 --> 00:44:47,839
is the the the reason for the change of

1217
00:44:47,839 --> 00:44:50,960
the results is the change of one of the

1218
00:44:50,960 --> 00:44:52,720
elements which exists in such an

1219
00:44:52,720 --> 00:44:56,000
ecosystem so teresa said that it's

1220
00:44:56,000 --> 00:44:58,000
incredibly difficult it's simply

1221
00:44:58,000 --> 00:44:59,440
impossible

1222
00:44:59,440 --> 00:45:01,520
for the migrant to understand where the

1223
00:45:01,520 --> 00:45:04,720
data is coming from that we simply

1224
00:45:04,720 --> 00:45:06,880
let's not talk about impact

1225
00:45:06,880 --> 00:45:09,520
difficulties this is simply impossible

1226
00:45:09,520 --> 00:45:12,000
uh at the same time it's almost

1227
00:45:12,000 --> 00:45:13,200
impossible

1228
00:45:13,200 --> 00:45:15,680
to understand it for those who are

1229
00:45:15,680 --> 00:45:18,240
controlling controllers of each of these

1230
00:45:18,240 --> 00:45:19,599
resources

1231
00:45:19,599 --> 00:45:22,319
and those who are supervising so the

1232
00:45:22,319 --> 00:45:23,760
supervisors

1233
00:45:23,760 --> 00:45:26,319
and the main subject which we are trying

1234
00:45:26,319 --> 00:45:28,319
to deal with is to understand the whole

1235
00:45:28,319 --> 00:45:30,880
ecosystem and the relations which exist

1236
00:45:30,880 --> 00:45:33,440
there and we have to remember that the

1237
00:45:33,440 --> 00:45:34,319
the

1238
00:45:34,319 --> 00:45:36,880
vocabulary of this legal acts also

1239
00:45:36,880 --> 00:45:38,880
changes and i i'm happy that there is

1240
00:45:38,880 --> 00:45:40,400
notice the

1241
00:45:40,400 --> 00:45:42,160
predictive analytics

1242
00:45:42,160 --> 00:45:44,079
uh discussed in neither of these acts

1243
00:45:44,079 --> 00:45:46,000
because if there was one then i would

1244
00:45:46,000 --> 00:45:47,839
have to understand what does it mean

1245
00:45:47,839 --> 00:45:50,000
predictive analytics at all and then

1246
00:45:50,000 --> 00:45:51,920
what does it mean if we compare it with

1247
00:45:51,920 --> 00:45:52,800
with the

1248
00:45:52,800 --> 00:45:55,119
legal ground for eurodac or google

1249
00:45:55,119 --> 00:45:56,800
ground for uh

1250
00:45:56,800 --> 00:46:00,720
for vis system so uh they

1251
00:46:00,720 --> 00:46:03,440
work the european union works on

1252
00:46:03,440 --> 00:46:04,880
interoperability in public

1253
00:46:04,880 --> 00:46:06,480
administration

1254
00:46:06,480 --> 00:46:09,119
shown that there is a technical aspect

1255
00:46:09,119 --> 00:46:11,680
a technical layer of interoperability

1256
00:46:11,680 --> 00:46:13,440
there is a semantic one which we don't

1257
00:46:13,440 --> 00:46:15,200
have either

1258
00:46:15,200 --> 00:46:18,640
there is an organizational which we have

1259
00:46:18,640 --> 00:46:20,319
big problems with

1260
00:46:20,319 --> 00:46:22,960
and actually the legal and the political

1261
00:46:22,960 --> 00:46:27,160
are probably the most transparent

1262
00:46:31,599 --> 00:46:34,079
so we just talked about like the fight

1263
00:46:34,079 --> 00:46:37,040
a certain disconnection uh regarding the

1264
00:46:37,040 --> 00:46:39,440
the people that are affected

1265
00:46:39,440 --> 00:46:41,119
and

1266
00:46:41,119 --> 00:46:44,160
the data protection supervisor as a

1267
00:46:44,160 --> 00:46:46,480
function and the the the complication

1268
00:46:46,480 --> 00:46:49,119
that it entails and the possible then

1269
00:46:49,119 --> 00:46:50,720
like um

1270
00:46:50,720 --> 00:46:52,720
complication in overseeing also the

1271
00:46:52,720 --> 00:46:56,000
respect of uh human rights uh i just

1272
00:46:56,000 --> 00:46:58,720
want to invite also teresa sarah and

1273
00:46:58,720 --> 00:47:01,920
alina to share with us how

1274
00:47:01,920 --> 00:47:02,880
they

1275
00:47:02,880 --> 00:47:06,560
include our part of coalitions uh of

1276
00:47:06,560 --> 00:47:08,960
with people that are

1277
00:47:08,960 --> 00:47:11,920
directly affected by those policies

1278
00:47:11,920 --> 00:47:14,240
and how it impacts the work that they're

1279
00:47:14,240 --> 00:47:16,560
doing

1280
00:47:20,240 --> 00:47:22,319
i can maybe come and quickly

1281
00:47:22,319 --> 00:47:24,400
um i think

1282
00:47:24,400 --> 00:47:25,839
um

1283
00:47:25,839 --> 00:47:28,400
this is an enormous challenge and it's

1284
00:47:28,400 --> 00:47:30,079
at the same time it's so enormously

1285
00:47:30,079 --> 00:47:31,920
important um

1286
00:47:31,920 --> 00:47:34,880
so i think the the challenge is exactly

1287
00:47:34,880 --> 00:47:37,040
what we've heard a moment ago this is so

1288
00:47:37,040 --> 00:47:40,240
hard to understand and to articulate

1289
00:47:40,240 --> 00:47:41,599
and to see

1290
00:47:41,599 --> 00:47:43,200
really how all the different pieces

1291
00:47:43,200 --> 00:47:44,400
connect

1292
00:47:44,400 --> 00:47:46,720
that we just heard the data protection

1293
00:47:46,720 --> 00:47:48,319
supervisors speak of in terms of the

1294
00:47:48,319 --> 00:47:50,880
institutional piece the legal piece

1295
00:47:50,880 --> 00:47:52,319
these different legal frameworks which

1296
00:47:52,319 --> 00:47:54,000
are developing continuing to be

1297
00:47:54,000 --> 00:47:57,839
developed some of them in parallel

1298
00:47:57,839 --> 00:47:59,760
and then i think again the way this is

1299
00:47:59,760 --> 00:48:01,760
operationalized it's not always easy to

1300
00:48:01,760 --> 00:48:05,040
tease out the impact of a given

1301
00:48:05,040 --> 00:48:06,400
use of technology

1302
00:48:06,400 --> 00:48:08,880
on a given individual so i think there's

1303
00:48:08,880 --> 00:48:10,720
enormous challenges in terms of getting

1304
00:48:10,720 --> 00:48:12,400
our hands around what we're talking

1305
00:48:12,400 --> 00:48:14,240
about getting our hands around the

1306
00:48:14,240 --> 00:48:16,160
impact um

1307
00:48:16,160 --> 00:48:17,440
and so

1308
00:48:17,440 --> 00:48:19,040
i mean within our network for example

1309
00:48:19,040 --> 00:48:21,200
pico's work pikum is a network that

1310
00:48:21,200 --> 00:48:22,720
focuses on the rights of undocumented

1311
00:48:22,720 --> 00:48:24,720
people and the majority of our members

1312
00:48:24,720 --> 00:48:27,440
work directly with undocumented people

1313
00:48:27,440 --> 00:48:30,400
and they have their hands

1314
00:48:30,400 --> 00:48:31,920
full

1315
00:48:31,920 --> 00:48:33,200
with

1316
00:48:33,200 --> 00:48:34,880
the daily challenges confronted by

1317
00:48:34,880 --> 00:48:37,520
undocumented people and so getting their

1318
00:48:37,520 --> 00:48:39,839
hands around interoperability and around

1319
00:48:39,839 --> 00:48:42,240
data protection issues is really

1320
00:48:42,240 --> 00:48:44,160
challenging for organizations that

1321
00:48:44,160 --> 00:48:46,880
already are very often stretched

1322
00:48:46,880 --> 00:48:49,599
and so it's really important to

1323
00:48:49,599 --> 00:48:51,200
to to

1324
00:48:51,200 --> 00:48:54,559
to um embed this

1325
00:48:54,559 --> 00:48:58,000
in what it means for the individual

1326
00:48:58,000 --> 00:49:00,400
again this is increasingly the realities

1327
00:49:00,400 --> 00:49:03,440
of the individual but unpicking it is

1328
00:49:03,440 --> 00:49:05,040
important and understanding where it's

1329
00:49:05,040 --> 00:49:06,480
situated

1330
00:49:06,480 --> 00:49:07,920
and i think that's an ongoing work for

1331
00:49:07,920 --> 00:49:09,920
our network it really is an ongoing work

1332
00:49:09,920 --> 00:49:11,680
for our network and we've

1333
00:49:11,680 --> 00:49:12,640
um

1334
00:49:12,640 --> 00:49:14,559
benefited tremendously from working with

1335
00:49:14,559 --> 00:49:17,040
other organizations

1336
00:49:17,040 --> 00:49:19,119
like sarah's

1337
00:49:19,119 --> 00:49:20,319
in

1338
00:49:20,319 --> 00:49:21,760
learning from them

1339
00:49:21,760 --> 00:49:24,319
um and giving input where we can

1340
00:49:24,319 --> 00:49:25,839
in terms of what's happening at the eu

1341
00:49:25,839 --> 00:49:26,880
level

1342
00:49:26,880 --> 00:49:28,960
and then bringing those conversations to

1343
00:49:28,960 --> 00:49:31,119
our members and hearing from them but

1344
00:49:31,119 --> 00:49:33,920
also understanding that each of us even

1345
00:49:33,920 --> 00:49:35,760
even as non-experts

1346
00:49:35,760 --> 00:49:37,599
in terms of the digital rights piece

1347
00:49:37,599 --> 00:49:39,920
bring an expertise on this broader

1348
00:49:39,920 --> 00:49:43,040
context and this understanding of impact

1349
00:49:43,040 --> 00:49:44,880
that goes beyond strictly privacy and

1350
00:49:44,880 --> 00:49:46,800
that goes strictly it goes beyond

1351
00:49:46,800 --> 00:49:50,240
strictly data protection and i think

1352
00:49:50,240 --> 00:49:52,480
may not have been particularly present

1353
00:49:52,480 --> 00:49:53,680
in the whole run-up to the

1354
00:49:53,680 --> 00:49:56,319
interoperability regulations passage so

1355
00:49:56,319 --> 00:49:58,480
i think we we need to see

1356
00:49:58,480 --> 00:50:01,040
the the importance and the urgency of

1357
00:50:01,040 --> 00:50:03,119
involving those who can bring those

1358
00:50:03,119 --> 00:50:04,640
critical insights

1359
00:50:04,640 --> 00:50:06,640
those are expert insights even if the

1360
00:50:06,640 --> 00:50:09,839
expertise isn't per se on the technology

1361
00:50:09,839 --> 00:50:12,480
but then we need to be very proactive

1362
00:50:12,480 --> 00:50:14,720
in bringing them into the conversations

1363
00:50:14,720 --> 00:50:16,880
about the technology itself so that they

1364
00:50:16,880 --> 00:50:19,040
can engage so that they can understand

1365
00:50:19,040 --> 00:50:22,000
it's it's it's how it relates in a very

1366
00:50:22,000 --> 00:50:23,520
practical way

1367
00:50:23,520 --> 00:50:26,480
um even if a very complex way um to

1368
00:50:26,480 --> 00:50:28,160
their daily realities and their rights

1369
00:50:28,160 --> 00:50:29,440
on the ground so

1370
00:50:29,440 --> 00:50:30,880
it's an ongoing work for us in our

1371
00:50:30,880 --> 00:50:31,839
network

1372
00:50:31,839 --> 00:50:34,079
and for us in our secretariat

1373
00:50:34,079 --> 00:50:35,760
it's challenging

1374
00:50:35,760 --> 00:50:37,920
um but i think this work is so so

1375
00:50:37,920 --> 00:50:39,440
essential and it doesn't happen by

1376
00:50:39,440 --> 00:50:41,760
accident it requires really thoughtful

1377
00:50:41,760 --> 00:50:46,240
and proactive engagement on all sides

1378
00:50:48,559 --> 00:50:52,319
um just very quickly i i never really

1379
00:50:52,319 --> 00:50:54,559
engaged with people whose personal data

1380
00:50:54,559 --> 00:50:57,599
will be stored in the system i did work

1381
00:50:57,599 --> 00:50:59,839
on different projects stakeholder

1382
00:50:59,839 --> 00:51:02,400
meetings conferences i wrote articles in

1383
00:51:02,400 --> 00:51:04,240
order to explain interoperability but

1384
00:51:04,240 --> 00:51:06,800
the problem is that it's so complex and

1385
00:51:06,800 --> 00:51:09,359
that if you start with this is about

1386
00:51:09,359 --> 00:51:11,920
third country nationals people

1387
00:51:11,920 --> 00:51:13,680
are not really interested

1388
00:51:13,680 --> 00:51:16,079
the general public i mean not people who

1389
00:51:16,079 --> 00:51:18,720
are in this uh in this

1390
00:51:18,720 --> 00:51:21,200
privacy camp or in those conferences of

1391
00:51:21,200 --> 00:51:23,119
course but if you want to reach the

1392
00:51:23,119 --> 00:51:24,800
general public and explain what the

1393
00:51:24,800 --> 00:51:26,640
issue is and what are the consequences

1394
00:51:26,640 --> 00:51:29,359
it is it is extremely difficult

1395
00:51:29,359 --> 00:51:33,040
and it always boils down to this

1396
00:51:33,040 --> 00:51:36,000
i read in the chat this us and they and

1397
00:51:36,000 --> 00:51:38,240
this is i think this is extremely

1398
00:51:38,240 --> 00:51:40,240
important to note that it's actually not

1399
00:51:40,240 --> 00:51:43,920
because it also has its impact on

1400
00:51:43,920 --> 00:51:47,520
let's call it us although of course uh

1401
00:51:47,520 --> 00:51:49,520
fundamental rights and the right to data

1402
00:51:49,520 --> 00:51:51,359
protection applies to everyone in the eu

1403
00:51:51,359 --> 00:51:53,200
right the charter applies to everyone

1404
00:51:53,200 --> 00:51:55,200
who's here and whose personal data are

1405
00:51:55,200 --> 00:51:57,440
being processed um

1406
00:51:57,440 --> 00:51:58,480
and what

1407
00:51:58,480 --> 00:52:01,040
i i recently

1408
00:52:01,040 --> 00:52:03,680
was doing a training for those who will

1409
00:52:03,680 --> 00:52:04,960
process

1410
00:52:04,960 --> 00:52:08,240
the ats applications and what is very uh

1411
00:52:08,240 --> 00:52:10,559
important to note is that there are so

1412
00:52:10,559 --> 00:52:11,599
many

1413
00:52:11,599 --> 00:52:12,599
uh

1414
00:52:12,599 --> 00:52:14,160
unclarified

1415
00:52:14,160 --> 00:52:15,760
rules and

1416
00:52:15,760 --> 00:52:18,400
competencies and what are the roles of

1417
00:52:18,400 --> 00:52:21,280
whom and as has been mentioned like if

1418
00:52:21,280 --> 00:52:23,839
we look at eu lisa and protects

1419
00:52:23,839 --> 00:52:26,720
there are so many um difficulties in

1420
00:52:26,720 --> 00:52:28,880
building the system and who is actually

1421
00:52:28,880 --> 00:52:31,440
responsible to carry out let's say a

1422
00:52:31,440 --> 00:52:33,920
data protection impact assessment who

1423
00:52:33,920 --> 00:52:36,720
is responsible to reply to data subject

1424
00:52:36,720 --> 00:52:40,800
requests and as vojtech said um

1425
00:52:40,800 --> 00:52:42,800
dpas are then

1426
00:52:42,800 --> 00:52:45,040
in the end responsible to supervise oil

1427
00:52:45,040 --> 00:52:46,640
the system and this

1428
00:52:46,640 --> 00:52:49,680
how will this work out dpas are supposed

1429
00:52:49,680 --> 00:52:50,640
to then

1430
00:52:50,640 --> 00:52:53,680
uh follow the data flows and not look at

1431
00:52:53,680 --> 00:52:56,240
individual controllers but look at the

1432
00:52:56,240 --> 00:52:58,079
data and how they are being connected

1433
00:52:58,079 --> 00:53:01,040
and this needs to take place on on

1434
00:53:01,040 --> 00:53:03,119
different levels on eu level on national

1435
00:53:03,119 --> 00:53:05,280
level and there needs to be some kind of

1436
00:53:05,280 --> 00:53:07,680
cooperation in order to

1437
00:53:07,680 --> 00:53:11,040
establish effective supervision and if

1438
00:53:11,040 --> 00:53:13,440
dpas or if even the controllers do not

1439
00:53:13,440 --> 00:53:15,040
know really what they are doing because

1440
00:53:15,040 --> 00:53:16,720
what we need to keep in mind is that

1441
00:53:16,720 --> 00:53:19,200
this system is not yet operational and

1442
00:53:19,200 --> 00:53:21,680
we don't even know whether it will work

1443
00:53:21,680 --> 00:53:24,480
because once being connected

1444
00:53:24,480 --> 00:53:26,880
what happens to all the wrongful data in

1445
00:53:26,880 --> 00:53:28,480
the systems when they are leading to

1446
00:53:28,480 --> 00:53:31,119
wrong matches so how will this

1447
00:53:31,119 --> 00:53:33,359
supervision of this interoperable system

1448
00:53:33,359 --> 00:53:35,680
actually then work in practice and how

1449
00:53:35,680 --> 00:53:39,280
will supervisory authorities

1450
00:53:39,280 --> 00:53:42,160
inform and help individuals who are

1451
00:53:42,160 --> 00:53:44,319
being stored in those systems

1452
00:53:44,319 --> 00:53:48,558
in order to then enforce their rights

1453
00:53:52,160 --> 00:53:54,319
sarah

1454
00:53:54,319 --> 00:53:56,319
i will i will be quick um

1455
00:53:56,319 --> 00:53:58,960
i i think that

1456
00:53:58,960 --> 00:54:01,200
one particular challenge of looking at

1457
00:54:01,200 --> 00:54:03,200
digital technologies data protection in

1458
00:54:03,200 --> 00:54:06,319
the migration context

1459
00:54:06,319 --> 00:54:08,960
and perhaps in any context is that often

1460
00:54:08,960 --> 00:54:11,520
the technicality of these issues is used

1461
00:54:11,520 --> 00:54:13,119
to obfuscate

1462
00:54:13,119 --> 00:54:15,359
or avoid the fact that these are

1463
00:54:15,359 --> 00:54:18,720
the core very political issues

1464
00:54:18,720 --> 00:54:20,880
they're issues of exclusion violence

1465
00:54:20,880 --> 00:54:22,800
racism control

1466
00:54:22,800 --> 00:54:25,119
over people's day-to-day lives

1467
00:54:25,119 --> 00:54:25,839
and

1468
00:54:25,839 --> 00:54:28,720
the technicality of these files

1469
00:54:28,720 --> 00:54:31,599
is not just a reason for people to

1470
00:54:31,599 --> 00:54:32,720
disconnect

1471
00:54:32,720 --> 00:54:34,880
from these issues for a reason that

1472
00:54:34,880 --> 00:54:36,480
people do not feel that they can discuss

1473
00:54:36,480 --> 00:54:37,359
them and

1474
00:54:37,359 --> 00:54:39,760
be part of those discussions but is also

1475
00:54:39,760 --> 00:54:41,200
often used

1476
00:54:41,200 --> 00:54:43,119
perhaps by policymakers perhaps by the

1477
00:54:43,119 --> 00:54:45,839
digital rights community also to

1478
00:54:45,839 --> 00:54:47,440
uh distance them from the people that

1479
00:54:47,440 --> 00:54:49,520
are most affected and this is i think a

1480
00:54:49,520 --> 00:54:51,119
challenge that we should be very aware

1481
00:54:51,119 --> 00:54:53,520
of and we should confront head-on only

1482
00:54:53,520 --> 00:54:55,599
if we do this

1483
00:54:55,599 --> 00:54:57,119
only if we

1484
00:54:57,119 --> 00:54:58,720
acknowledge this and try to sort of

1485
00:54:58,720 --> 00:55:01,440
avoid falling into this cycle will we be

1486
00:55:01,440 --> 00:55:02,880
able to challenge these questions

1487
00:55:02,880 --> 00:55:04,559
head-on otherwise we will get lost in

1488
00:55:04,559 --> 00:55:07,280
technicalities and in answer to your

1489
00:55:07,280 --> 00:55:09,119
question lauren's what edri is doing

1490
00:55:09,119 --> 00:55:11,119
again is incredibly hard what we've

1491
00:55:11,119 --> 00:55:13,200
tried to do more and more is the first

1492
00:55:13,200 --> 00:55:16,079
midway step which is to engage and sort

1493
00:55:16,079 --> 00:55:18,400
of not

1494
00:55:18,400 --> 00:55:20,880
not play to this notion that there is a

1495
00:55:20,880 --> 00:55:23,119
very valid distinction between digital

1496
00:55:23,119 --> 00:55:24,799
rights organizations and migration

1497
00:55:24,799 --> 00:55:27,440
organizations and rather that both types

1498
00:55:27,440 --> 00:55:29,359
of organizations need to be connected on

1499
00:55:29,359 --> 00:55:30,720
these issues

1500
00:55:30,720 --> 00:55:32,319
so i think the second step is actually

1501
00:55:32,319 --> 00:55:33,920
how do you connect with people that are

1502
00:55:33,920 --> 00:55:35,680
directly affected and i do not want to

1503
00:55:35,680 --> 00:55:37,680
say that we have been been successful in

1504
00:55:37,680 --> 00:55:39,520
that for many of the reasons and the

1505
00:55:39,520 --> 00:55:41,680
barriers that we've talked about

1506
00:55:41,680 --> 00:55:43,440
and sort of the bread and butter issues

1507
00:55:43,440 --> 00:55:45,440
that alina has spoke about right if you

1508
00:55:45,440 --> 00:55:46,240
are

1509
00:55:46,240 --> 00:55:48,240
at risk of a pushback if you do not know

1510
00:55:48,240 --> 00:55:49,440
how you're going to have access to

1511
00:55:49,440 --> 00:55:51,280
public services

1512
00:55:51,280 --> 00:55:53,520
can you really have the time and

1513
00:55:53,520 --> 00:55:56,000
patience to speak with people in very

1514
00:55:56,000 --> 00:55:57,680
complex terms about data protection and

1515
00:55:57,680 --> 00:55:59,280
other issues that's a real challenge and

1516
00:55:59,280 --> 00:56:01,760
i think the burden is on us is to find a

1517
00:56:01,760 --> 00:56:03,839
concrete way to address that

1518
00:56:03,839 --> 00:56:06,160
beyond also civil society i think

1519
00:56:06,160 --> 00:56:09,520
crucially it's the place of academia and

1520
00:56:09,520 --> 00:56:11,280
of policy makers

1521
00:56:11,280 --> 00:56:13,119
to directly

1522
00:56:13,119 --> 00:56:16,400
develop schemes of how to include the

1523
00:56:16,400 --> 00:56:18,480
perspectives of people affected in their

1524
00:56:18,480 --> 00:56:19,280
work

1525
00:56:19,280 --> 00:56:21,440
to dedicate resources to do that to

1526
00:56:21,440 --> 00:56:23,280
compensate people for their time to do

1527
00:56:23,280 --> 00:56:25,040
that and to provide

1528
00:56:25,040 --> 00:56:26,720
mechanisms to ensure the safety of

1529
00:56:26,720 --> 00:56:28,480
people in order to participate in those

1530
00:56:28,480 --> 00:56:29,599
processes

1531
00:56:29,599 --> 00:56:31,359
civil society can try to do it but

1532
00:56:31,359 --> 00:56:33,680
ultimately if the policy makers who are

1533
00:56:33,680 --> 00:56:35,200
making

1534
00:56:35,200 --> 00:56:37,839
decisions and concrete uh

1535
00:56:37,839 --> 00:56:39,760
proposals for changes about people's

1536
00:56:39,760 --> 00:56:42,319
lives do not feel those

1537
00:56:42,319 --> 00:56:43,839
do not feel that it's appropriate to do

1538
00:56:43,839 --> 00:56:46,960
so we will be continue with the cycle of

1539
00:56:46,960 --> 00:56:49,440
making policy about people as opposed to

1540
00:56:49,440 --> 00:56:52,400
with people and i think that's what we

1541
00:56:52,400 --> 00:56:54,880
need to focus more of our energy on

1542
00:56:54,880 --> 00:56:57,119
especially on the topic of migration and

1543
00:56:57,119 --> 00:57:00,400
surveillance technologies

1544
00:57:01,920 --> 00:57:05,359
thank you so much to the great

1545
00:57:05,359 --> 00:57:08,559
words to conclude the panel

1546
00:57:08,559 --> 00:57:12,000
and we're on time i just want to flag

1547
00:57:12,000 --> 00:57:13,040
that

1548
00:57:13,040 --> 00:57:13,760
uh

1549
00:57:13,760 --> 00:57:16,319
some questions were asked in the chat uh

1550
00:57:16,319 --> 00:57:19,200
and notably uh by roma no

1551
00:57:19,200 --> 00:57:22,960
and uh they've let the email address in

1552
00:57:22,960 --> 00:57:27,040
case you wanted to answer them later on

1553
00:57:27,040 --> 00:57:29,599
and those are quite precise and

1554
00:57:29,599 --> 00:57:31,200
important questions

1555
00:57:31,200 --> 00:57:33,839
uh specifically regarding the fact of

1556
00:57:33,839 --> 00:57:36,000
the legislation being uh

1557
00:57:36,000 --> 00:57:37,359
being more

1558
00:57:37,359 --> 00:57:39,839
centered on accuracy from a technical

1559
00:57:39,839 --> 00:57:41,839
point of view uh

1560
00:57:41,839 --> 00:57:44,400
rather than unfairness and

1561
00:57:44,400 --> 00:57:46,559
human rights protection

1562
00:57:46,559 --> 00:57:49,520
of the people uh impacted

1563
00:57:49,520 --> 00:57:51,280
by this uh

1564
00:57:51,280 --> 00:57:54,000
by those measures uh i just want to

1565
00:57:54,000 --> 00:57:55,599
invite chloe

1566
00:57:55,599 --> 00:57:58,160
better than me from edri back

1567
00:57:58,160 --> 00:57:59,680
and

1568
00:57:59,680 --> 00:58:02,559
want to thank you for the time and just

1569
00:58:02,559 --> 00:58:04,480
yeah for the talk and the input thank

1570
00:58:04,480 --> 00:58:06,720
you so much

1571
00:58:06,720 --> 00:58:08,000
thank you so much lawrence for

1572
00:58:08,000 --> 00:58:10,319
moderating this great panel and to the

1573
00:58:10,319 --> 00:58:12,799
speakers that was actually a very

1574
00:58:12,799 --> 00:58:14,400
interesting conversation and i think

1575
00:58:14,400 --> 00:58:15,920
just the start

1576
00:58:15,920 --> 00:58:18,319
uh we only scratched the surface i think

1577
00:58:18,319 --> 00:58:22,400
uh i would like to restate our um big

1578
00:58:22,400 --> 00:58:24,240
thank you to the

1579
00:58:24,240 --> 00:58:27,200
edps for their continuous support over

1580
00:58:27,200 --> 00:58:28,480
the years

1581
00:58:28,480 --> 00:58:30,400
of privacy camp

1582
00:58:30,400 --> 00:58:31,440
they're

1583
00:58:31,440 --> 00:58:33,680
joining forces with us every time and

1584
00:58:33,680 --> 00:58:36,160
this is a pleasure to have them in a

1585
00:58:36,160 --> 00:58:38,319
common discussion and i hope we have

1586
00:58:38,319 --> 00:58:40,079
more discussion in the future about

1587
00:58:40,079 --> 00:58:42,160
these topics and other

1588
00:58:42,160 --> 00:58:44,319
difficult ones where maybe we disagree

1589
00:58:44,319 --> 00:58:47,680
but we are on the same

1590
00:58:47,680 --> 00:58:49,760
side

1591
00:58:49,760 --> 00:58:53,200
hoping to find a solution to solve

1592
00:58:53,200 --> 00:58:55,359
fundamental rights issues and and

1593
00:58:55,359 --> 00:58:58,359
problems

