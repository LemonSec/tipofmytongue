1
00:00:04,960 --> 00:00:06,960
joe and i as the moderators for today

2
00:00:06,960 --> 00:00:08,960
would like to make this session as

3
00:00:08,960 --> 00:00:11,440
interactive as possible

4
00:00:11,440 --> 00:00:13,679
so to raise your questions either to a

5
00:00:13,679 --> 00:00:16,320
specific speaker or the general panel

6
00:00:16,320 --> 00:00:18,640
please feel free to at any time always

7
00:00:18,640 --> 00:00:20,560
use the chat i see that people have

8
00:00:20,560 --> 00:00:22,000
already been using it a bit and that's

9
00:00:22,000 --> 00:00:24,640
very good keep up the good work

10
00:00:24,640 --> 00:00:26,880
um jill and i will manage and select the

11
00:00:26,880 --> 00:00:28,880
questions to pose

12
00:00:28,880 --> 00:00:30,720
and we will give you the opportunity to

13
00:00:30,720 --> 00:00:32,399
verbally elaborate on the question

14
00:00:32,399 --> 00:00:34,160
during the q a

15
00:00:34,160 --> 00:00:36,320
so overall we really hope to have a

16
00:00:36,320 --> 00:00:39,040
fruitful and above else respectful

17
00:00:39,040 --> 00:00:40,640
discussion

18
00:00:40,640 --> 00:00:43,040
so um to also introduce myself i am

19
00:00:43,040 --> 00:00:45,520
naomi apple mom and together with jill

20
00:00:45,520 --> 00:00:47,360
toe we will be moderating today's

21
00:00:47,360 --> 00:00:48,480
session

22
00:00:48,480 --> 00:00:50,239
we are together with thompson sparta who

23
00:00:50,239 --> 00:00:52,640
isn't here today at the co-founders of

24
00:00:52,640 --> 00:00:55,199
the racism and technology center that is

25
00:00:55,199 --> 00:00:57,039
responsible for the organization of

26
00:00:57,039 --> 00:00:58,960
today's session

27
00:00:58,960 --> 00:01:01,840
so as to the structure i will firstly be

28
00:01:01,840 --> 00:01:04,159
giving a brief introduction of the topic

29
00:01:04,159 --> 00:01:06,240
of this panel and the racism and

30
00:01:06,240 --> 00:01:09,360
technology center as the organizer of it

31
00:01:09,360 --> 00:01:12,000
then we will have a presentation by our

32
00:01:12,000 --> 00:01:14,720
three amazing speakers on the dutch

33
00:01:14,720 --> 00:01:17,200
child benefits scandal as a case study

34
00:01:17,200 --> 00:01:19,360
for how to make the debate on digital

35
00:01:19,360 --> 00:01:22,320
rights less technocentric

36
00:01:22,320 --> 00:01:25,040
after these presentations jill will post

37
00:01:25,040 --> 00:01:27,040
some further questions to the speakers

38
00:01:27,040 --> 00:01:29,520
with the aim to draw the broader lessons

39
00:01:29,520 --> 00:01:31,759
from this specific case study for the

40
00:01:31,759 --> 00:01:33,680
digital rights field and after which we

41
00:01:33,680 --> 00:01:36,799
hope to open it up for questions and

42
00:01:36,799 --> 00:01:38,159
hopefully there will be a lot of that

43
00:01:38,159 --> 00:01:41,759
and we can have a fruitful discussion

44
00:01:41,759 --> 00:01:45,200
so i think that most people are here by

45
00:01:45,200 --> 00:01:46,479
now

46
00:01:46,479 --> 00:01:49,200
and i think i'll just head into the

47
00:01:49,200 --> 00:01:51,280
substance

48
00:01:51,280 --> 00:01:54,000
so as we wrote in the panel description

49
00:01:54,000 --> 00:01:55,920
with this session we want to contribute

50
00:01:55,920 --> 00:01:58,240
to making the debate on digital rights

51
00:01:58,240 --> 00:02:00,159
and then specifically in relation to

52
00:02:00,159 --> 00:02:04,079
marginalized voices less technocentric

53
00:02:04,079 --> 00:02:06,560
so as is also indicated by the broader

54
00:02:06,560 --> 00:02:07,759
theme of

55
00:02:07,759 --> 00:02:10,239
this year's privacy camp

56
00:02:10,239 --> 00:02:12,959
it's necessary to re-evaluate how the

57
00:02:12,959 --> 00:02:16,319
digital right field sees technology as a

58
00:02:16,319 --> 00:02:18,720
driving role in exacerbating and

59
00:02:18,720 --> 00:02:22,160
perpetuating social injustices

60
00:02:22,160 --> 00:02:24,000
within this broader idea we have

61
00:02:24,000 --> 00:02:26,959
selected as a guiding question

62
00:02:26,959 --> 00:02:29,760
how we can identify the real harms

63
00:02:29,760 --> 00:02:32,640
automated systems can generate without

64
00:02:32,640 --> 00:02:34,640
disregarding the historical and social

65
00:02:34,640 --> 00:02:36,959
context that produce these systems in

66
00:02:36,959 --> 00:02:38,560
the first place and that these systems

67
00:02:38,560 --> 00:02:40,239
operate

68
00:02:40,239 --> 00:02:42,560
so in this panel we concretely want to

69
00:02:42,560 --> 00:02:44,879
use the dutch child benefits scandal as

70
00:02:44,879 --> 00:02:47,599
a specific and local case study

71
00:02:47,599 --> 00:02:49,599
which can function as a starting point

72
00:02:49,599 --> 00:02:52,000
to have this broader discussion so this

73
00:02:52,000 --> 00:02:54,000
specific and local case study opens up

74
00:02:54,000 --> 00:02:56,560
to a broader discussion on basis

75
00:02:56,560 --> 00:02:58,800
practices by governments

76
00:02:58,800 --> 00:03:01,519
the increasing use of new technologies

77
00:03:01,519 --> 00:03:03,120
such as automated decision making

78
00:03:03,120 --> 00:03:05,680
systems in government agencies

79
00:03:05,680 --> 00:03:07,599
and importantly

80
00:03:07,599 --> 00:03:09,920
the potentially outsized role that

81
00:03:09,920 --> 00:03:12,560
algorithms play in these discussions

82
00:03:12,560 --> 00:03:14,720
about these systems

83
00:03:14,720 --> 00:03:17,040
so we as the racism and technology

84
00:03:17,040 --> 00:03:19,280
center find it important to organize

85
00:03:19,280 --> 00:03:22,319
this panel as it connects very closely

86
00:03:22,319 --> 00:03:24,959
to the center's core mission

87
00:03:24,959 --> 00:03:27,360
jilto hansensvart and myself founded the

88
00:03:27,360 --> 00:03:29,360
racism and technology center to help

89
00:03:29,360 --> 00:03:31,680
bridge the gap between anti-racist

90
00:03:31,680 --> 00:03:33,040
organizations

91
00:03:33,040 --> 00:03:35,120
and organizations that focus on digital

92
00:03:35,120 --> 00:03:36,480
rights

93
00:03:36,480 --> 00:03:38,799
so we saw that where the former can

94
00:03:38,799 --> 00:03:40,720
often feel they lack the expertise or

95
00:03:40,720 --> 00:03:43,560
even the agency to address racism in a

96
00:03:43,560 --> 00:03:46,640
technological context the latter the

97
00:03:46,640 --> 00:03:48,560
digital rights organizations are more

98
00:03:48,560 --> 00:03:50,959
often than not have a blind spot for

99
00:03:50,959 --> 00:03:53,360
these type of questions on inequality or

100
00:03:53,360 --> 00:03:56,000
racial inequality and is also very much

101
00:03:56,000 --> 00:03:57,439
the theme of

102
00:03:57,439 --> 00:04:00,720
this year's privacy camp so

103
00:04:00,720 --> 00:04:02,720
what we overall hope to achieve with

104
00:04:02,720 --> 00:04:06,400
this panel is to identify concrete harms

105
00:04:06,400 --> 00:04:08,560
as well as to provide nuance to the

106
00:04:08,560 --> 00:04:09,680
discussion

107
00:04:09,680 --> 00:04:12,239
by decentral by decentering the role of

108
00:04:12,239 --> 00:04:15,920
technologies vis-a-vis social injustices

109
00:04:15,920 --> 00:04:18,160
so originally the ambition for this

110
00:04:18,160 --> 00:04:20,320
panel was to also center local knowledge

111
00:04:20,320 --> 00:04:22,880
and expertise by inviting anti-racist

112
00:04:22,880 --> 00:04:25,199
organizations from the netherlands

113
00:04:25,199 --> 00:04:26,639
however due to

114
00:04:26,639 --> 00:04:29,040
capacity issues both on our side from

115
00:04:29,040 --> 00:04:31,440
their bases and technology center

116
00:04:31,440 --> 00:04:32,800
and

117
00:04:32,800 --> 00:04:35,280
for the anti-racism organizations we

118
00:04:35,280 --> 00:04:37,280
approached and of course the short time

119
00:04:37,280 --> 00:04:39,600
span that privacy camp has we

120
00:04:39,600 --> 00:04:42,400
unfortunately weren't able to do so

121
00:04:42,400 --> 00:04:44,320
we feel this is also indicative of the

122
00:04:44,320 --> 00:04:48,000
broader issues and problems you run into

123
00:04:48,000 --> 00:04:50,080
trying to prioritize affected people in

124
00:04:50,080 --> 00:04:52,800
the discussions in primarily expert

125
00:04:52,800 --> 00:04:55,199
dominated spaces such as this one use as

126
00:04:55,199 --> 00:04:56,720
well

127
00:04:56,720 --> 00:04:58,960
so this this shows how within the

128
00:04:58,960 --> 00:05:00,880
digital rights field and of course much

129
00:05:00,880 --> 00:05:03,280
broader there's also a need to rethink

130
00:05:03,280 --> 00:05:05,759
more fundamentally who participates in

131
00:05:05,759 --> 00:05:07,600
discussions and how we can create space

132
00:05:07,600 --> 00:05:09,600
for the right people

133
00:05:09,600 --> 00:05:11,919
i'm sure that our discussions today and

134
00:05:11,919 --> 00:05:14,080
note that the inspiring talks by our

135
00:05:14,080 --> 00:05:16,160
speakers will give us a lot to work with

136
00:05:16,160 --> 00:05:18,240
and how to make the debate on digital

137
00:05:18,240 --> 00:05:20,400
rights less technocentric and more

138
00:05:20,400 --> 00:05:23,039
focused on the people affected

139
00:05:23,039 --> 00:05:25,199
so having said this i want to turn now

140
00:05:25,199 --> 00:05:27,680
to our three wonderful speakers of today

141
00:05:27,680 --> 00:05:29,680
nadia middle and sana

142
00:05:29,680 --> 00:05:32,240
and as they are most capable of

143
00:05:32,240 --> 00:05:34,479
introducing themselves i would like to

144
00:05:34,479 --> 00:05:36,880
ask them to do so before we move on to

145
00:05:36,880 --> 00:05:38,560
the presentations

146
00:05:38,560 --> 00:05:40,479
nadia can i start with you and ask you

147
00:05:40,479 --> 00:05:42,960
to introduce yourself

148
00:05:42,960 --> 00:05:44,800
yes of course thanks so much for having

149
00:05:44,800 --> 00:05:47,759
me i am nadia benesa i'm a lawyer i work

150
00:05:47,759 --> 00:05:49,440
at busy freedom which is a digital

151
00:05:49,440 --> 00:05:51,039
rights organization

152
00:05:51,039 --> 00:05:52,720
based in the netherlands and mainly

153
00:05:52,720 --> 00:05:53,840
focus

154
00:05:53,840 --> 00:05:56,240
on ai and gdpr

155
00:05:56,240 --> 00:05:58,720
issues

156
00:05:58,720 --> 00:06:00,880
thank you so much and

157
00:06:00,880 --> 00:06:02,880
amazing to have you here today

158
00:06:02,880 --> 00:06:06,319
and sonic can i ask you

159
00:06:06,319 --> 00:06:09,360
hi i'm sana i work for the justice

160
00:06:09,360 --> 00:06:11,759
equity and technology table and network

161
00:06:11,759 --> 00:06:13,440
that tries to address the impact of

162
00:06:13,440 --> 00:06:16,720
data-driven policing all over europe

163
00:06:16,720 --> 00:06:18,639
and i'm super happy to join you today

164
00:06:18,639 --> 00:06:22,639
from a quite gray amsterdam

165
00:06:23,840 --> 00:06:26,160
i'd say a very great amsterdam very

166
00:06:26,160 --> 00:06:28,160
happy to have you here and last but

167
00:06:28,160 --> 00:06:31,440
certainly not least middle

168
00:06:31,440 --> 00:06:33,440
hi everybody i'm miguel conning i'm a

169
00:06:33,440 --> 00:06:35,199
senior policy advisor at msd

170
00:06:35,199 --> 00:06:37,120
international and i lead the dutch

171
00:06:37,120 --> 00:06:40,400
technology program there

172
00:06:41,680 --> 00:06:42,960
thank you so much

173
00:06:42,960 --> 00:06:45,039
so now we can move on to the brief five

174
00:06:45,039 --> 00:06:47,039
to seven minute presentations by our

175
00:06:47,039 --> 00:06:49,680
three speakers inspired by specific

176
00:06:49,680 --> 00:06:52,800
questions we discussed beforehand

177
00:06:52,800 --> 00:06:54,560
allowing us to really zoom into the

178
00:06:54,560 --> 00:06:56,560
different aspects of this case study and

179
00:06:56,560 --> 00:06:59,680
to draw these broader questions later

180
00:06:59,680 --> 00:07:02,319
so to start with miro what we discussed

181
00:07:02,319 --> 00:07:03,520
that

182
00:07:03,520 --> 00:07:06,400
your talk would be inspired by is the

183
00:07:06,400 --> 00:07:08,960
question how the dutch child benefits

184
00:07:08,960 --> 00:07:12,720
scandal has been framed and discussed

185
00:07:12,720 --> 00:07:15,520
so can i please give you the floor

186
00:07:15,520 --> 00:07:17,680
yes yes and thank you

187
00:07:17,680 --> 00:07:19,520
um

188
00:07:19,520 --> 00:07:21,919
so um

189
00:07:21,919 --> 00:07:24,080
how the dutch childcare benefit scandal

190
00:07:24,080 --> 00:07:26,880
was framed and discussed i think the

191
00:07:26,880 --> 00:07:28,479
question should be slightly different

192
00:07:28,479 --> 00:07:30,880
how is it still framed and discussed

193
00:07:30,880 --> 00:07:34,000
because it's an ongoing problem that

194
00:07:34,000 --> 00:07:35,039
the dutch

195
00:07:35,039 --> 00:07:37,520
the childcare benefit scandal is seen as

196
00:07:37,520 --> 00:07:40,240
an administrative failure

197
00:07:40,240 --> 00:07:42,960
so um for those who are not familiar

198
00:07:42,960 --> 00:07:45,199
with the case i will give a brief

199
00:07:45,199 --> 00:07:46,800
introduction to it

200
00:07:46,800 --> 00:07:47,919
um

201
00:07:47,919 --> 00:07:49,160
from

202
00:07:49,160 --> 00:07:52,720
2013 onwards the dutch tax authorities

203
00:07:52,720 --> 00:07:55,759
increasingly used algorithmic systems to

204
00:07:55,759 --> 00:07:56,960
detect

205
00:07:56,960 --> 00:08:00,000
fraud or potential fraud with people who

206
00:08:00,000 --> 00:08:02,720
applied for child care benefits

207
00:08:02,720 --> 00:08:04,400
child care benefits is a system in the

208
00:08:04,400 --> 00:08:06,000
netherlands where you can apply and you

209
00:08:06,000 --> 00:08:08,080
can bring your child to check and your

210
00:08:08,080 --> 00:08:11,199
child to daycare it's great and then

211
00:08:11,199 --> 00:08:14,479
you get part of the uh the cost um

212
00:08:14,479 --> 00:08:17,680
reimbursed from the government

213
00:08:17,680 --> 00:08:19,599
in this whole system

214
00:08:19,599 --> 00:08:21,759
um there was a strong

215
00:08:21,759 --> 00:08:24,720
um idea with the tax authorities there

216
00:08:24,720 --> 00:08:26,639
that there was a lot of fraud so they

217
00:08:26,639 --> 00:08:29,120
used automated systems to detect this

218
00:08:29,120 --> 00:08:30,720
type of fraud

219
00:08:30,720 --> 00:08:33,839
in the initial algorithm

220
00:08:33,839 --> 00:08:36,399
one of the parameters that was used was

221
00:08:36,399 --> 00:08:38,880
whether or not you had a dutch

222
00:08:38,880 --> 00:08:42,399
nationality and that criteria

223
00:08:42,399 --> 00:08:44,959
mattered whether or not your risk score

224
00:08:44,959 --> 00:08:47,680
was increased

225
00:08:47,680 --> 00:08:49,360
and that's racist

226
00:08:49,360 --> 00:08:52,560
that is discriminatory and that has been

227
00:08:52,560 --> 00:08:54,880
under the radar for many years in the

228
00:08:54,880 --> 00:08:56,240
netherlands

229
00:08:56,240 --> 00:08:58,480
what has also been under the radar is

230
00:08:58,480 --> 00:09:01,680
that this automated system that was used

231
00:09:01,680 --> 00:09:04,000
used the self-learning algorithm and

232
00:09:04,000 --> 00:09:05,760
over the course of time

233
00:09:05,760 --> 00:09:07,279
this algorithm

234
00:09:07,279 --> 00:09:09,760
taught itself

235
00:09:09,760 --> 00:09:12,959
to focus primarily on people from low

236
00:09:12,959 --> 00:09:14,160
income

237
00:09:14,160 --> 00:09:15,600
groups

238
00:09:15,600 --> 00:09:17,760
so people who were in higher income

239
00:09:17,760 --> 00:09:20,560
groups they were less seen as potential

240
00:09:20,560 --> 00:09:23,040
fraudsters and people with low income

241
00:09:23,040 --> 00:09:24,560
were seen as

242
00:09:24,560 --> 00:09:27,440
potential fraudsters so they also

243
00:09:27,440 --> 00:09:30,720
got a higher risk score in the end

244
00:09:30,720 --> 00:09:33,040
so what happened was that people from

245
00:09:33,040 --> 00:09:34,640
low income

246
00:09:34,640 --> 00:09:35,760
families

247
00:09:35,760 --> 00:09:38,399
with migration backgrounds

248
00:09:38,399 --> 00:09:40,959
were the first people that were selected

249
00:09:40,959 --> 00:09:43,360
as potential fraudsters

250
00:09:43,360 --> 00:09:45,680
and what happened once you were selected

251
00:09:45,680 --> 00:09:47,600
as a potential fraudster is very

252
00:09:47,600 --> 00:09:50,160
devastating and

253
00:09:50,160 --> 00:09:53,040
well a black page in the history of uh

254
00:09:53,040 --> 00:09:54,480
dutch government

255
00:09:54,480 --> 00:09:56,720
because fall after fall after fault was

256
00:09:56,720 --> 00:09:58,880
made not only the algorithm was

257
00:09:58,880 --> 00:10:01,120
problematic but also the handling of the

258
00:10:01,120 --> 00:10:02,839
dutch tax authorities

259
00:10:02,839 --> 00:10:05,279
afterwards and

260
00:10:05,279 --> 00:10:07,920
quite um

261
00:10:07,920 --> 00:10:09,760
problematic not only the dutch tax

262
00:10:09,760 --> 00:10:11,440
authorities but also the courts that

263
00:10:11,440 --> 00:10:14,240
were supposed to protect these people

264
00:10:14,240 --> 00:10:15,680
um

265
00:10:15,680 --> 00:10:18,399
people were labeled as fraudsters

266
00:10:18,399 --> 00:10:19,839
where they were not

267
00:10:19,839 --> 00:10:22,000
they had to repay all the child care

268
00:10:22,000 --> 00:10:24,640
benefits at once

269
00:10:24,640 --> 00:10:26,959
people lost their houses they lost their

270
00:10:26,959 --> 00:10:30,079
jobs in the end 70 000

271
00:10:30,079 --> 00:10:32,240
children became the victim of this

272
00:10:32,240 --> 00:10:35,200
scandal and more than 10 well tens of

273
00:10:35,200 --> 00:10:37,600
thousands of families are impacted and

274
00:10:37,600 --> 00:10:39,120
until this day

275
00:10:39,120 --> 00:10:41,839
there is still no substantive

276
00:10:41,839 --> 00:10:44,959
solution for this

277
00:10:45,680 --> 00:10:48,320
how it was framed in the media was as an

278
00:10:48,320 --> 00:10:50,240
administrative failure

279
00:10:50,240 --> 00:10:52,640
how of course as amnesty international

280
00:10:52,640 --> 00:10:54,720
we think this should be framed is a

281
00:10:54,720 --> 00:10:56,560
human rights problem

282
00:10:56,560 --> 00:10:58,480
and not only a human rights problem

283
00:10:58,480 --> 00:11:00,560
stemming from the use of automated

284
00:11:00,560 --> 00:11:02,800
decision-making systems but a human

285
00:11:02,800 --> 00:11:05,200
rights problem stemming from

286
00:11:05,200 --> 00:11:08,000
institutional racism and

287
00:11:08,000 --> 00:11:09,279
sustained

288
00:11:09,279 --> 00:11:13,360
inequality and prolonged inequality and

289
00:11:13,360 --> 00:11:15,920
translated inequality into automated

290
00:11:15,920 --> 00:11:18,399
systems

291
00:11:18,720 --> 00:11:21,360
the system was both discriminatory on

292
00:11:21,360 --> 00:11:23,920
the base of race and ethnicity and on

293
00:11:23,920 --> 00:11:26,320
social and economic class

294
00:11:26,320 --> 00:11:27,440
and

295
00:11:27,440 --> 00:11:29,839
with regard to the last uh type of

296
00:11:29,839 --> 00:11:32,079
discrimination

297
00:11:32,079 --> 00:11:33,200
this is

298
00:11:33,200 --> 00:11:35,279
still um

299
00:11:35,279 --> 00:11:37,040
nobody's talking about this in the

300
00:11:37,040 --> 00:11:39,600
netherlands the whole child care benefit

301
00:11:39,600 --> 00:11:41,760
scandal is not framed as a social

302
00:11:41,760 --> 00:11:44,560
economic problem or social economic

303
00:11:44,560 --> 00:11:46,720
discrimination and the problem of

304
00:11:46,720 --> 00:11:48,720
discrimination as such

305
00:11:48,720 --> 00:11:50,720
but it is still seen as administrative

306
00:11:50,720 --> 00:11:52,959
failure we also see that in the way that

307
00:11:52,959 --> 00:11:55,040
the dutch government is picking

308
00:11:55,040 --> 00:11:58,160
this up and trying to repair this

309
00:11:58,160 --> 00:12:00,079
many

310
00:12:00,079 --> 00:12:01,839
guidance papers

311
00:12:01,839 --> 00:12:03,040
and

312
00:12:03,040 --> 00:12:05,040
quite frankly

313
00:12:05,040 --> 00:12:07,360
open and voluntary standards are now

314
00:12:07,360 --> 00:12:08,240
being

315
00:12:08,240 --> 00:12:10,320
discussed and developed within the

316
00:12:10,320 --> 00:12:13,360
government whereas strict mandatory

317
00:12:13,360 --> 00:12:14,800
legislation

318
00:12:14,800 --> 00:12:15,839
for

319
00:12:15,839 --> 00:12:18,399
people working in the government um to

320
00:12:18,399 --> 00:12:19,839
make sure that human rights are

321
00:12:19,839 --> 00:12:22,720
protected and to make sure that their

322
00:12:22,720 --> 00:12:25,519
practices are becoming less racist over

323
00:12:25,519 --> 00:12:26,880
time or

324
00:12:26,880 --> 00:12:28,160
of course at least

325
00:12:28,160 --> 00:12:30,959
immediately would be best

326
00:12:30,959 --> 00:12:33,519
this is still lacking and this is very

327
00:12:33,519 --> 00:12:36,560
problematic in framing

328
00:12:36,560 --> 00:12:40,320
i think i'll just stop here and then

329
00:12:40,320 --> 00:12:42,000
continue

330
00:12:42,000 --> 00:12:45,360
via questions later on

331
00:12:45,440 --> 00:12:47,839
thank you so much for giving this

332
00:12:47,839 --> 00:12:49,839
overview of what the scandal actually is

333
00:12:49,839 --> 00:12:50,880
in this

334
00:12:50,880 --> 00:12:52,560
great commentary to start with and also

335
00:12:52,560 --> 00:12:54,399
thank you very much for staying

336
00:12:54,399 --> 00:12:56,560
perfectly within the time

337
00:12:56,560 --> 00:13:00,000
then we will be moving on to our second

338
00:13:00,000 --> 00:13:02,000
speaker sauna

339
00:13:02,000 --> 00:13:04,800
and the guiding question that your

340
00:13:04,800 --> 00:13:07,200
presentation was inspired on

341
00:13:07,200 --> 00:13:09,760
is what are the shortcomings and missing

342
00:13:09,760 --> 00:13:14,160
perspectives in these discussions

343
00:13:15,440 --> 00:13:16,720
um

344
00:13:16,720 --> 00:13:18,839
yeah thanks mia also for this great

345
00:13:18,839 --> 00:13:20,800
introduction um

346
00:13:20,800 --> 00:13:23,040
i just want to start by saying that

347
00:13:23,040 --> 00:13:25,920
background is not only um

348
00:13:25,920 --> 00:13:28,800
in this topic but when i was 11 my

349
00:13:28,800 --> 00:13:30,959
father the breadth winner of the family

350
00:13:30,959 --> 00:13:32,560
was fired

351
00:13:32,560 --> 00:13:36,000
he appealed this and it was just unruly

352
00:13:36,000 --> 00:13:38,480
and he got some damage payments but he

353
00:13:38,480 --> 00:13:40,959
was also considered too old on the job

354
00:13:40,959 --> 00:13:43,760
market and his mental health spiraled

355
00:13:43,760 --> 00:13:44,720
down

356
00:13:44,720 --> 00:13:46,639
and we were on benefits for many many

357
00:13:46,639 --> 00:13:49,360
years and i know firsthand from this

358
00:13:49,360 --> 00:13:51,920
experience what it means

359
00:13:51,920 --> 00:13:54,079
though i must add that we did have some

360
00:13:54,079 --> 00:13:56,000
privileges

361
00:13:56,000 --> 00:13:57,839
that even though my father had mental

362
00:13:57,839 --> 00:14:00,240
health problems he was kind of sort of

363
00:14:00,240 --> 00:14:02,560
higher educated so he spent much time

364
00:14:02,560 --> 00:14:04,079
appealing

365
00:14:04,079 --> 00:14:06,240
the decisions of the

366
00:14:06,240 --> 00:14:08,240
text

367
00:14:08,240 --> 00:14:11,279
system and writing letters and so on and

368
00:14:11,279 --> 00:14:15,120
he had that capacity kind of and

369
00:14:15,120 --> 00:14:17,680
also we had some kind of safety net of

370
00:14:17,680 --> 00:14:20,160
some family that could come to aid

371
00:14:20,160 --> 00:14:22,959
when there was a real emergency so

372
00:14:22,959 --> 00:14:24,720
our situation was in many respects still

373
00:14:24,720 --> 00:14:26,800
one of certain privileges

374
00:14:26,800 --> 00:14:29,199
um but you can imagine that this

375
00:14:29,199 --> 00:14:31,760
experience prompted much interest for me

376
00:14:31,760 --> 00:14:32,800
and

377
00:14:32,800 --> 00:14:34,800
you know how

378
00:14:34,800 --> 00:14:37,680
how this whole benefit system works um

379
00:14:37,680 --> 00:14:39,839
it also made me i think a way more

380
00:14:39,839 --> 00:14:41,440
sensitive for

381
00:14:41,440 --> 00:14:43,519
what it feels like to be on benefits

382
00:14:43,519 --> 00:14:45,519
what what the impact is beyond the

383
00:14:45,519 --> 00:14:48,320
material impact the psychological impact

384
00:14:48,320 --> 00:14:50,000
etc

385
00:14:50,000 --> 00:14:52,639
so i closely followed the scandal and

386
00:14:52,639 --> 00:14:54,800
for me what i actually like to stress

387
00:14:54,800 --> 00:14:55,839
today

388
00:14:55,839 --> 00:14:59,279
is that i i the way i've read it and

389
00:14:59,279 --> 00:15:01,680
analyzed it and followed it is actually

390
00:15:01,680 --> 00:15:04,399
the minor role of technology in this

391
00:15:04,399 --> 00:15:06,639
case and this is where i think there's a

392
00:15:06,639 --> 00:15:08,560
difference between maybe me and how

393
00:15:08,560 --> 00:15:11,279
amnesty sees it i mean i definitely

394
00:15:11,279 --> 00:15:13,279
agree it was framed as an administrative

395
00:15:13,279 --> 00:15:16,639
issue and not a human rights issue

396
00:15:16,639 --> 00:15:19,040
um and i would add that it should not

397
00:15:19,040 --> 00:15:21,199
only be trained much more as a human

398
00:15:21,199 --> 00:15:23,279
rights issue but as a social justice

399
00:15:23,279 --> 00:15:24,839
issue

400
00:15:24,839 --> 00:15:26,800
um and

401
00:15:26,800 --> 00:15:29,279
i'm gonna try to explain as

402
00:15:29,279 --> 00:15:30,720
quick as i can

403
00:15:30,720 --> 00:15:32,959
uh what that means for me

404
00:15:32,959 --> 00:15:35,199
like reading the careful investigative

405
00:15:35,199 --> 00:15:38,880
research it was done by many journalists

406
00:15:38,880 --> 00:15:40,639
looking at the internal documents that

407
00:15:40,639 --> 00:15:42,880
were released as part of the big foia

408
00:15:42,880 --> 00:15:43,839
request

409
00:15:43,839 --> 00:15:44,720
that

410
00:15:44,720 --> 00:15:45,839
like

411
00:15:45,839 --> 00:15:46,800
tech

412
00:15:46,800 --> 00:15:49,040
was one of the least of the issues here

413
00:15:49,040 --> 00:15:51,120
unlike some other scandal like robo debt

414
00:15:51,120 --> 00:15:52,880
in australia or some or some of the

415
00:15:52,880 --> 00:15:55,839
great examples that are given in

416
00:15:55,839 --> 00:15:58,399
the book automating inequality and what

417
00:15:58,399 --> 00:16:00,959
is striking when looking into this not

418
00:16:00,959 --> 00:16:03,600
surprising and i will get back to why it

419
00:16:03,600 --> 00:16:06,959
was not surprising in the least

420
00:16:06,959 --> 00:16:09,680
but it was striking at the level of like

421
00:16:09,680 --> 00:16:12,320
the complete

422
00:16:12,320 --> 00:16:15,279
obsession with potential fraud and the

423
00:16:15,279 --> 00:16:17,759
stubborn persistence to hunt down people

424
00:16:17,759 --> 00:16:20,480
for fraud ignoring all the evidence the

425
00:16:20,480 --> 00:16:22,880
beneficiaries brought to the fore which

426
00:16:22,880 --> 00:16:24,959
proved there was no fraud the department

427
00:16:24,959 --> 00:16:27,120
that was tasked with fraud detection was

428
00:16:27,120 --> 00:16:30,480
choosing to follow the horrendous policy

429
00:16:30,480 --> 00:16:33,040
um in its most strict sense they were

430
00:16:33,040 --> 00:16:34,720
choosing to follow the policies

431
00:16:34,720 --> 00:16:37,600
furthermore in a horrendous very human

432
00:16:37,600 --> 00:16:39,759
driven insistence on the most

433
00:16:39,759 --> 00:16:41,759
denigrating humiliating racist

434
00:16:41,759 --> 00:16:44,320
stereotypes if you read their emails

435
00:16:44,320 --> 00:16:46,399
it's it's like

436
00:16:46,399 --> 00:16:48,480
it's very sickening

437
00:16:48,480 --> 00:16:50,800
the way they insisted to search their

438
00:16:50,800 --> 00:16:54,000
kind of shadow database for nationality

439
00:16:54,000 --> 00:16:57,040
on or and ethnicity was not just

440
00:16:57,040 --> 00:16:59,120
something automated or something that an

441
00:16:59,120 --> 00:17:02,079
algorithm uh kind of prompted it was

442
00:17:02,079 --> 00:17:04,160
very human-driven

443
00:17:04,160 --> 00:17:06,640
and the complete disregard and i would

444
00:17:06,640 --> 00:17:10,959
say sociopathic lack of empathy um was

445
00:17:10,959 --> 00:17:13,280
also very human driven

446
00:17:13,280 --> 00:17:15,520
and now and that's another thing that i

447
00:17:15,520 --> 00:17:17,839
found lacking in this debate is that

448
00:17:17,839 --> 00:17:20,319
this is of course nothing new there's a

449
00:17:20,319 --> 00:17:22,959
long historical lineage from the poor

450
00:17:22,959 --> 00:17:25,599
houses and the poor colonies in the

451
00:17:25,599 --> 00:17:27,919
netherlands for example fan housing and

452
00:17:27,919 --> 00:17:30,640
the pokemons that were invited mind you

453
00:17:30,640 --> 00:17:32,799
by a certain gentleman who also served

454
00:17:32,799 --> 00:17:34,960
the dutch overseas colonies where he

455
00:17:34,960 --> 00:17:37,039
repressed the local population there

456
00:17:37,039 --> 00:17:39,039
there's a direct link

457
00:17:39,039 --> 00:17:40,960
and if one wants to understand how this

458
00:17:40,960 --> 00:17:43,039
could have happened it's this

459
00:17:43,039 --> 00:17:45,520
fundamental hunting down and

460
00:17:45,520 --> 00:17:49,120
disciplining of the poor since centuries

461
00:17:49,120 --> 00:17:51,200
uh the malicious distinction made

462
00:17:51,200 --> 00:17:53,919
between the underserved

463
00:17:53,919 --> 00:17:54,960
war

464
00:17:54,960 --> 00:17:56,480
now all of this is not to say that

465
00:17:56,480 --> 00:17:59,360
technology has always been instrumental

466
00:17:59,360 --> 00:18:01,440
in these repressive efforts

467
00:18:01,440 --> 00:18:03,919
um one can see that also in history and

468
00:18:03,919 --> 00:18:05,840
we can talk about that more if it

469
00:18:05,840 --> 00:18:07,840
interests you and that there's this kind

470
00:18:07,840 --> 00:18:10,400
of specific role of technology that that

471
00:18:10,400 --> 00:18:13,600
makes it a very powerful weapon

472
00:18:13,600 --> 00:18:16,160
it's not only that technology is

473
00:18:16,160 --> 00:18:18,480
instrumental for power in its efficiency

474
00:18:18,480 --> 00:18:20,080
which

475
00:18:20,080 --> 00:18:23,039
the promise does not always fulfill

476
00:18:23,039 --> 00:18:24,400
but it's functioning as this

477
00:18:24,400 --> 00:18:26,160
rationalizing normalizing and

478
00:18:26,160 --> 00:18:28,559
neutralizing the shield for what's

479
00:18:28,559 --> 00:18:32,160
obviously not a technocratic matter

480
00:18:32,160 --> 00:18:34,720
of catching corrupt behavior but a form

481
00:18:34,720 --> 00:18:36,880
of glaswar i would say and aruha

482
00:18:36,880 --> 00:18:40,000
benjamin also describes this

483
00:18:40,000 --> 00:18:42,400
magic power of community of technology

484
00:18:42,400 --> 00:18:44,559
to make it something neutral something

485
00:18:44,559 --> 00:18:46,480
outside politics

486
00:18:46,480 --> 00:18:49,760
and what i'm struggling with um

487
00:18:49,760 --> 00:18:52,640
is that as soon as we localize the

488
00:18:52,640 --> 00:18:55,679
problem within the technology somehow as

489
00:18:55,679 --> 00:18:57,520
soon as we point to the algorithm and

490
00:18:57,520 --> 00:18:58,880
declare

491
00:18:58,880 --> 00:19:01,440
how racist it indeed is and connects

492
00:19:01,440 --> 00:19:03,919
that to how it should be more ethical or

493
00:19:03,919 --> 00:19:06,640
objective how privacy should be built in

494
00:19:06,640 --> 00:19:08,720
human rights standards should be built

495
00:19:08,720 --> 00:19:11,600
in in some way though

496
00:19:11,600 --> 00:19:13,919
this work is important but we have to be

497
00:19:13,919 --> 00:19:15,760
wary that there's a danger of

498
00:19:15,760 --> 00:19:18,320
disconnecting the technology

499
00:19:18,320 --> 00:19:21,360
as the politicians do from the power

500
00:19:21,360 --> 00:19:24,000
structures from ideology as if there can

501
00:19:24,000 --> 00:19:27,120
be something as fair tech in an unfair

502
00:19:27,120 --> 00:19:29,919
world as if there can be something like

503
00:19:29,919 --> 00:19:31,120
a fair

504
00:19:31,120 --> 00:19:32,960
uh welfare

505
00:19:32,960 --> 00:19:34,480
fraud

506
00:19:34,480 --> 00:19:37,360
detection algorithm

507
00:19:37,360 --> 00:19:38,960
right

508
00:19:38,960 --> 00:19:39,760
now

509
00:19:39,760 --> 00:19:41,360
because what you get is some kind of

510
00:19:41,360 --> 00:19:44,000
mirror image of tax solutionism as as

511
00:19:44,000 --> 00:19:46,400
this old discourse of tax solutionism

512
00:19:46,400 --> 00:19:48,640
where tech would save us because

513
00:19:48,640 --> 00:19:51,679
you know it solves the flawed human that

514
00:19:51,679 --> 00:19:53,520
is the problem and then the neutral

515
00:19:53,520 --> 00:19:55,440
technology is the solution

516
00:19:55,440 --> 00:19:58,480
now it becomes

517
00:19:58,480 --> 00:20:00,960
the technology the calculating computers

518
00:20:00,960 --> 00:20:03,440
the cold algorithms that we point to as

519
00:20:03,440 --> 00:20:05,280
the cause of evil

520
00:20:05,280 --> 00:20:07,919
as if what and how they calculate is not

521
00:20:07,919 --> 00:20:10,559
completely human designed as if not all

522
00:20:10,559 --> 00:20:12,640
tech is human

523
00:20:12,640 --> 00:20:15,039
and what i hear a lot in the touslav

524
00:20:15,039 --> 00:20:16,960
affair and this is why i kind of

525
00:20:16,960 --> 00:20:19,280
extensively make that point

526
00:20:19,280 --> 00:20:22,240
around me i hear a lot of people say

527
00:20:22,240 --> 00:20:24,880
they're like oh it's such a scandal and

528
00:20:24,880 --> 00:20:26,799
these algorithms the computers in

529
00:20:26,799 --> 00:20:29,679
humanity of those automated systems and

530
00:20:29,679 --> 00:20:32,640
there's this like common consensus that

531
00:20:32,640 --> 00:20:34,880
what is called in dutch the mensa

532
00:20:34,880 --> 00:20:37,039
command was lost which means like the

533
00:20:37,039 --> 00:20:39,520
humor measure or scale the human in the

534
00:20:39,520 --> 00:20:40,400
loop

535
00:20:40,400 --> 00:20:43,200
but it was full of humans in the loop

536
00:20:43,200 --> 00:20:45,360
and it makes me shiver as i think of all

537
00:20:45,360 --> 00:20:47,120
the humans making the policies and the

538
00:20:47,120 --> 00:20:49,039
plans the humans assessing the cases the

539
00:20:49,039 --> 00:20:50,080
humans

540
00:20:50,080 --> 00:20:52,320
um of the control teams

541
00:20:52,320 --> 00:20:55,120
racist slurs their house visits and i

542
00:20:55,120 --> 00:20:56,799
think of the very humans on our

543
00:20:56,799 --> 00:20:59,520
doorsteps in my youth their tone of

544
00:20:59,520 --> 00:21:02,480
voice the contempt the content born from

545
00:21:02,480 --> 00:21:05,679
the way the poor are seen as suspect and

546
00:21:05,679 --> 00:21:07,760
criminal

547
00:21:07,760 --> 00:21:09,520
there's a whole range of papers and

548
00:21:09,520 --> 00:21:11,919
research on the repressive welfare state

549
00:21:11,919 --> 00:21:15,200
for people to dig in

550
00:21:15,200 --> 00:21:18,559
and but i want to con like i also want

551
00:21:18,559 --> 00:21:22,640
us to propose kind of an alternative um

552
00:21:22,640 --> 00:21:23,760
and

553
00:21:23,760 --> 00:21:26,000
um

554
00:21:26,240 --> 00:21:27,520
first of all

555
00:21:27,520 --> 00:21:29,919
this debate from the expert driven

556
00:21:29,919 --> 00:21:32,080
spaces that

557
00:21:32,080 --> 00:21:34,640
tend to come with maybe ex like

558
00:21:34,640 --> 00:21:38,480
including me here right um

559
00:21:38,480 --> 00:21:40,960
like uh

560
00:21:40,960 --> 00:21:43,440
the debate is often happening in based

561
00:21:43,440 --> 00:21:46,320
as this like technology

562
00:21:46,320 --> 00:21:48,159
legal spaces

563
00:21:48,159 --> 00:21:51,200
um and we did with the table of justice

564
00:21:51,200 --> 00:21:53,120
equity and technology we did the session

565
00:21:53,120 --> 00:21:56,320
with the stop lapd spying coalition

566
00:21:56,320 --> 00:22:00,960
and they kind of advise that their work

567
00:22:00,960 --> 00:22:02,880
um

568
00:22:02,880 --> 00:22:04,720
really

569
00:22:04,720 --> 00:22:07,600
looks to kind of go further

570
00:22:07,600 --> 00:22:09,919
um because they feel that these expert

571
00:22:09,919 --> 00:22:12,320
conversations kind of narrowly define

572
00:22:12,320 --> 00:22:14,640
how the harms of these systems are to be

573
00:22:14,640 --> 00:22:17,039
understood it creates an emphasis on

574
00:22:17,039 --> 00:22:19,440
issues of bias and transparency or

575
00:22:19,440 --> 00:22:22,240
debates around constitutional violations

576
00:22:22,240 --> 00:22:24,799
and these kind of priorities and

577
00:22:24,799 --> 00:22:27,280
politics are not necessarily relevant

578
00:22:27,280 --> 00:22:30,400
for people experience to harm most

579
00:22:30,400 --> 00:22:32,320
for them it doesn't really matter

580
00:22:32,320 --> 00:22:34,720
if there's that if they're surveyed

581
00:22:34,720 --> 00:22:36,960
legally or illegally like the

582
00:22:36,960 --> 00:22:39,280
consequence is still there so they

583
00:22:39,280 --> 00:22:41,039
really say we've had enough

584
00:22:41,039 --> 00:22:43,360
um

585
00:22:43,360 --> 00:22:46,159
and i see that i'm a bit over time

586
00:22:46,159 --> 00:22:49,760
so i'm gonna round up here um

587
00:22:49,760 --> 00:22:52,799
and they propose a different model which

588
00:22:52,799 --> 00:22:55,679
is here which is looking at the systems

589
00:22:55,679 --> 00:22:56,640
with

590
00:22:56,640 --> 00:22:58,000
your system

591
00:22:58,000 --> 00:22:59,039
um

592
00:22:59,039 --> 00:23:01,919
because they said you know you need to

593
00:23:01,919 --> 00:23:04,880
look beyond the programs to the broader

594
00:23:04,880 --> 00:23:06,960
inputs and ecosystems surrounding them

595
00:23:06,960 --> 00:23:08,799
if you're fighting against a particular

596
00:23:08,799 --> 00:23:12,320
problem program or system or algorithm

597
00:23:12,320 --> 00:23:14,559
the response will be you're right that's

598
00:23:14,559 --> 00:23:16,799
bad

599
00:23:17,520 --> 00:23:19,679
new thing which is in fact designed to

600
00:23:19,679 --> 00:23:22,240
me to be more durable and can

601
00:23:22,240 --> 00:23:24,799
outmaneuver the criticism that the

602
00:23:24,799 --> 00:23:26,480
community was making

603
00:23:26,480 --> 00:23:29,440
so to really address the root causes the

604
00:23:29,440 --> 00:23:33,120
way poverty is is is seen the kind of

605
00:23:33,120 --> 00:23:36,400
ideology behind this hunt

606
00:23:36,400 --> 00:23:38,799
on these parents and dismantle the

607
00:23:38,799 --> 00:23:40,799
system you cannot just be organizing

608
00:23:40,799 --> 00:23:43,039
against one example you need to target

609
00:23:43,039 --> 00:23:45,840
not only the particular program but the

610
00:23:45,840 --> 00:23:49,279
entire ecosystem that surrounds this

611
00:23:49,279 --> 00:23:50,400
um

612
00:23:50,400 --> 00:23:52,240
and the question of how to do that we

613
00:23:52,240 --> 00:23:55,039
can discuss later

614
00:23:55,039 --> 00:23:56,799
thank you so much sana for this very

615
00:23:56,799 --> 00:23:59,200
powerful um and also inspiring

616
00:23:59,200 --> 00:24:00,799
presentation

617
00:24:00,799 --> 00:24:03,679
and last but certainly not least

618
00:24:03,679 --> 00:24:05,279
onto nadia

619
00:24:05,279 --> 00:24:07,919
we'll take the first steps into drawing

620
00:24:07,919 --> 00:24:10,000
the discussion a bit broader and the

621
00:24:10,000 --> 00:24:12,080
guiding question that we

622
00:24:12,080 --> 00:24:14,960
discussed is how does this relate to

623
00:24:14,960 --> 00:24:17,120
broader issues on the use of technology

624
00:24:17,120 --> 00:24:21,918
in government and its related agencies

625
00:24:22,240 --> 00:24:24,559
yeah thank you thank you mira and summer

626
00:24:24,559 --> 00:24:27,279
for sharing your thoughts i agree with a

627
00:24:27,279 --> 00:24:29,039
lot that has been said so i'm sure there

628
00:24:29,039 --> 00:24:31,600
will be some overlapping

629
00:24:31,600 --> 00:24:33,440
in my thoughts that i'm about to share

630
00:24:33,440 --> 00:24:35,919
but i would still like to stress out two

631
00:24:35,919 --> 00:24:37,039
issues

632
00:24:37,039 --> 00:24:38,559
that the child benefits scandal

633
00:24:38,559 --> 00:24:41,360
illustrates really well which is

634
00:24:41,360 --> 00:24:43,360
discrimination

635
00:24:43,360 --> 00:24:45,200
firstly and the second one is lack of

636
00:24:45,200 --> 00:24:47,440
transparency accountability

637
00:24:47,440 --> 00:24:50,640
and effective redress

638
00:24:50,799 --> 00:24:52,640
so like

639
00:24:52,640 --> 00:24:54,640
middle has mentioned before

640
00:24:54,640 --> 00:24:56,720
while hunting on fought cases in which

641
00:24:56,720 --> 00:24:59,440
parents might have received unlawful

642
00:24:59,440 --> 00:25:01,120
child benefit announced the tax

643
00:25:01,120 --> 00:25:02,720
authority thought that people with a

644
00:25:02,720 --> 00:25:04,720
second nationality meaning ethnic

645
00:25:04,720 --> 00:25:07,279
minorities and people with a lower

646
00:25:07,279 --> 00:25:08,880
income meaning people who need the

647
00:25:08,880 --> 00:25:10,720
benefits most

648
00:25:10,720 --> 00:25:12,080
might have a

649
00:25:12,080 --> 00:25:14,960
higher risk of committing fraud

650
00:25:14,960 --> 00:25:17,919
so many were categorized as fosters and

651
00:25:17,919 --> 00:25:20,559
the allowances in many cases thousands

652
00:25:20,559 --> 00:25:22,559
of euros

653
00:25:22,559 --> 00:25:24,720
were recovered and people lost their

654
00:25:24,720 --> 00:25:26,320
jobs because they couldn't afford

655
00:25:26,320 --> 00:25:29,039
daycare anymore people lost their houses

656
00:25:29,039 --> 00:25:30,080
because

657
00:25:30,080 --> 00:25:32,400
the debts were unpayable

658
00:25:32,400 --> 00:25:34,400
and this has led to broken homes in

659
00:25:34,400 --> 00:25:38,159
which children were grown up in poverty

660
00:25:38,159 --> 00:25:40,840
and some were even removed from their

661
00:25:40,840 --> 00:25:44,159
parents so this was a result of racism

662
00:25:44,159 --> 00:25:46,799
and social economic discrimination some

663
00:25:46,799 --> 00:25:49,679
blame this on a broken system

664
00:25:49,679 --> 00:25:51,120
we disagree

665
00:25:51,120 --> 00:25:53,440
the system isn't broken the system and

666
00:25:53,440 --> 00:25:55,840
its algorithms are doing exactly what

667
00:25:55,840 --> 00:25:58,559
they're told by people

668
00:25:58,559 --> 00:26:01,360
we need to face that we as a people face

669
00:26:01,360 --> 00:26:03,679
serious problems

670
00:26:03,679 --> 00:26:05,440
in our societies that need to be

671
00:26:05,440 --> 00:26:07,919
addressed

672
00:26:08,480 --> 00:26:11,760
we face discrimination we face a lack of

673
00:26:11,760 --> 00:26:13,200
freedom of speech and freedom of

674
00:26:13,200 --> 00:26:15,279
religion violations to the right of

675
00:26:15,279 --> 00:26:18,320
privacy and so on and so forth

676
00:26:18,320 --> 00:26:20,080
so the standards that many countries

677
00:26:20,080 --> 00:26:22,640
have agreed upon

678
00:26:22,640 --> 00:26:24,400
years ago when it comes to fundamental

679
00:26:24,400 --> 00:26:26,799
rights are still not effective in

680
00:26:26,799 --> 00:26:28,159
reality

681
00:26:28,159 --> 00:26:30,159
therefore it isn't strange that the data

682
00:26:30,159 --> 00:26:32,720
that comes out from our society are a

683
00:26:32,720 --> 00:26:34,559
reflection of the problems that we're

684
00:26:34,559 --> 00:26:35,840
facing

685
00:26:35,840 --> 00:26:38,480
this data is in neutral or objective

686
00:26:38,480 --> 00:26:41,200
because society isn't and neither is

687
00:26:41,200 --> 00:26:43,440
government policy

688
00:26:43,440 --> 00:26:46,159
we've already seen too many cases in

689
00:26:46,159 --> 00:26:48,400
which our government has

690
00:26:48,400 --> 00:26:50,000
willingly

691
00:26:50,000 --> 00:26:52,640
profiled an ethnic minorities and people

692
00:26:52,640 --> 00:26:55,760
with a low income not accidentally

693
00:26:55,760 --> 00:26:58,000
not because of a broken system

694
00:26:58,000 --> 00:27:00,080
but willingly because some governments

695
00:27:00,080 --> 00:27:02,320
and policy makers thinks it's i think

696
00:27:02,320 --> 00:27:04,480
it's a good idea to make discriminating

697
00:27:04,480 --> 00:27:06,320
policy

698
00:27:06,320 --> 00:27:08,799
we often speak of algorithm algorithmic

699
00:27:08,799 --> 00:27:11,200
biases and we should but let's also not

700
00:27:11,200 --> 00:27:14,000
forget that these biases are mirroring

701
00:27:14,000 --> 00:27:16,159
our societies

702
00:27:16,159 --> 00:27:17,840
and therefore we need to make sure that

703
00:27:17,840 --> 00:27:19,520
ai legislation

704
00:27:19,520 --> 00:27:21,760
protect fundamental rights which brings

705
00:27:21,760 --> 00:27:23,760
me to my second issue

706
00:27:23,760 --> 00:27:26,480
lack of transparency accountability and

707
00:27:26,480 --> 00:27:28,320
effective redress

708
00:27:28,320 --> 00:27:30,960
so many people who were affected by the

709
00:27:30,960 --> 00:27:33,360
tax authority were people of color and

710
00:27:33,360 --> 00:27:35,440
many had a feeling they were ethnically

711
00:27:35,440 --> 00:27:38,320
profiled but they were unable unable to

712
00:27:38,320 --> 00:27:40,399
prove it and at some point

713
00:27:40,399 --> 00:27:42,320
a group of people use their right of

714
00:27:42,320 --> 00:27:44,240
access to find out what has led the

715
00:27:44,240 --> 00:27:48,080
decision-making in their dolce

716
00:27:48,080 --> 00:27:50,640
in response to that the text authority

717
00:27:50,640 --> 00:27:53,679
sent them large directories

718
00:27:53,679 --> 00:27:56,720
they were almost completely blackened

719
00:27:56,720 --> 00:28:00,240
and therefore unable to read and even

720
00:28:00,240 --> 00:28:02,720
though this part wasn't even about

721
00:28:02,720 --> 00:28:04,320
algorithms

722
00:28:04,320 --> 00:28:06,960
this illustrates perfectly

723
00:28:06,960 --> 00:28:09,279
how important transparency and decision

724
00:28:09,279 --> 00:28:11,440
making really is

725
00:28:11,440 --> 00:28:13,360
so in dutch administrative law

726
00:28:13,360 --> 00:28:14,960
government bodies are obliged to

727
00:28:14,960 --> 00:28:17,279
motivate their decisions in an

728
00:28:17,279 --> 00:28:19,440
understandable way

729
00:28:19,440 --> 00:28:20,799
and decisions should be free from

730
00:28:20,799 --> 00:28:24,240
discrimination and arbitrary enough

731
00:28:24,240 --> 00:28:26,399
and people need to be able to object to

732
00:28:26,399 --> 00:28:27,600
a decision

733
00:28:27,600 --> 00:28:29,919
and have access to a judge to fight a

734
00:28:29,919 --> 00:28:31,840
decision

735
00:28:31,840 --> 00:28:34,720
so these are basic rights

736
00:28:34,720 --> 00:28:36,960
that should be normal in a democratic

737
00:28:36,960 --> 00:28:39,840
society however the current law proposal

738
00:28:39,840 --> 00:28:42,320
the artificial intelligence act

739
00:28:42,320 --> 00:28:45,279
does not provide this level of legal

740
00:28:45,279 --> 00:28:46,799
protection yet

741
00:28:46,799 --> 00:28:49,440
people have hardly any rights to

742
00:28:49,440 --> 00:28:51,679
to enforce transparency much less

743
00:28:51,679 --> 00:28:53,440
accountability

744
00:28:53,440 --> 00:28:56,000
and for some reason we're setting the

745
00:28:56,000 --> 00:28:56,799
bar

746
00:28:56,799 --> 00:28:58,880
really low when it comes to legal

747
00:28:58,880 --> 00:29:01,279
protection against ai

748
00:29:01,279 --> 00:29:03,200
and of course we know that transparency

749
00:29:03,200 --> 00:29:05,039
and accountability are words that are

750
00:29:05,039 --> 00:29:07,840
easily easier said than done

751
00:29:07,840 --> 00:29:11,120
as technology doesn't seem to be ready

752
00:29:11,120 --> 00:29:13,679
to make decisions that are kind of

753
00:29:13,679 --> 00:29:15,840
understandable for humans

754
00:29:15,840 --> 00:29:19,039
however by setting the bar this low

755
00:29:19,039 --> 00:29:20,960
no developer will be challenged to

756
00:29:20,960 --> 00:29:23,279
improve that

757
00:29:23,279 --> 00:29:26,159
so to sum up the eu seems to be in a

758
00:29:26,159 --> 00:29:30,640
race trying to compete with major powers

759
00:29:30,640 --> 00:29:34,080
that are running ahead in ai development

760
00:29:34,080 --> 00:29:36,960
which is really a stupid thing to do

761
00:29:36,960 --> 00:29:40,080
the eu should be setting an example on

762
00:29:40,080 --> 00:29:42,799
how to innovate while protecting human

763
00:29:42,799 --> 00:29:45,440
rights we think that's the only way that

764
00:29:45,440 --> 00:29:48,320
is futureful

765
00:29:52,559 --> 00:29:54,960
thank you so much nadia for also

766
00:29:54,960 --> 00:29:57,440
connecting this discussion to the eu

767
00:29:57,440 --> 00:29:59,120
regulatory debate and what's going on

768
00:29:59,120 --> 00:30:01,600
there i think it's a very productive

769
00:30:01,600 --> 00:30:02,799
perspective

770
00:30:02,799 --> 00:30:04,960
now before we move on to the broader

771
00:30:04,960 --> 00:30:08,159
discussion and uh jill's questions i do

772
00:30:08,159 --> 00:30:09,840
want to give you three the opportunity

773
00:30:09,840 --> 00:30:11,919
to respond to each other if there are

774
00:30:11,919 --> 00:30:14,320
already concrete things you want to say

775
00:30:14,320 --> 00:30:15,520
or ask

776
00:30:15,520 --> 00:30:17,440
and if not we can

777
00:30:17,440 --> 00:30:19,919
move on to the questions and keep that a

778
00:30:19,919 --> 00:30:21,679
different equation for

779
00:30:21,679 --> 00:30:24,240
the q a

780
00:30:24,240 --> 00:30:27,360
but please grab the floor if there are

781
00:30:27,360 --> 00:30:28,559
any

782
00:30:28,559 --> 00:30:31,279
reactions

783
00:30:31,840 --> 00:30:36,000
if not then i am

784
00:30:36,000 --> 00:30:39,600
just to stress that um

785
00:30:39,600 --> 00:30:41,919
the um

786
00:30:41,919 --> 00:30:44,960
the root causes that were um outlined by

787
00:30:44,960 --> 00:30:46,080
sauna

788
00:30:46,080 --> 00:30:47,919
um this is

789
00:30:47,919 --> 00:30:50,640
absolutely something that is seen

790
00:30:50,640 --> 00:30:52,720
by a human rights organization such as

791
00:30:52,720 --> 00:30:54,720
amnesty international

792
00:30:54,720 --> 00:30:57,760
looking at the

793
00:30:57,760 --> 00:31:00,000
the child care benefits scandal on the

794
00:31:00,000 --> 00:31:02,640
tech side

795
00:31:02,960 --> 00:31:06,640
for for us is a way to

796
00:31:06,640 --> 00:31:08,799
have an example and show

797
00:31:08,799 --> 00:31:11,440
how these systemic injustices

798
00:31:11,440 --> 00:31:14,640
translate to um once they are being

799
00:31:14,640 --> 00:31:16,240
automated

800
00:31:16,240 --> 00:31:19,760
um so i i i do feel that maybe we're not

801
00:31:19,760 --> 00:31:22,240
that far apart um

802
00:31:22,240 --> 00:31:24,320
in um

803
00:31:24,320 --> 00:31:26,159
in in seeing the issue

804
00:31:26,159 --> 00:31:29,519
uh absolutely in how we then uh address

805
00:31:29,519 --> 00:31:31,200
it as um

806
00:31:31,200 --> 00:31:32,799
msd words with international human

807
00:31:32,799 --> 00:31:35,519
rights standards is very differently

808
00:31:35,519 --> 00:31:38,240
and i see

809
00:31:38,640 --> 00:31:41,440
amazing opportunities to

810
00:31:41,440 --> 00:31:44,320
to merge these two and

811
00:31:44,320 --> 00:31:46,399
also the work of chat so i have a

812
00:31:46,399 --> 00:31:48,080
question actually can i pose a question

813
00:31:48,080 --> 00:31:50,399
to a different panel member yeah

814
00:31:50,399 --> 00:31:53,200
um because um

815
00:31:53,200 --> 00:31:56,080
i am very um

816
00:31:56,080 --> 00:31:57,360
curious

817
00:31:57,360 --> 00:32:01,120
what type of um solutions or what type

818
00:32:01,120 --> 00:32:03,279
of

819
00:32:04,720 --> 00:32:06,960
processes or

820
00:32:06,960 --> 00:32:08,880
work you are doing

821
00:32:08,880 --> 00:32:10,480
and um

822
00:32:10,480 --> 00:32:11,760
also

823
00:32:11,760 --> 00:32:13,840
you mentioned bias transparency these

824
00:32:13,840 --> 00:32:16,240
are very expert solutions that do not

825
00:32:16,240 --> 00:32:18,799
directly address the situation or the

826
00:32:18,799 --> 00:32:21,200
problems that

827
00:32:21,200 --> 00:32:23,519
affected communities

828
00:32:23,519 --> 00:32:25,200
feel or that they

829
00:32:25,200 --> 00:32:28,000
feel that should be dealt with can you

830
00:32:28,000 --> 00:32:30,080
elaborate on that a bit more because i

831
00:32:30,080 --> 00:32:32,000
think that would be really valuable for

832
00:32:32,000 --> 00:32:35,279
a digital rights um crowd such as we

833
00:32:35,279 --> 00:32:38,799
have here at privacy camp

834
00:32:39,919 --> 00:32:42,399
thank you mel and of course i i

835
00:32:42,399 --> 00:32:43,760
i agree that

836
00:32:43,760 --> 00:32:44,640
we

837
00:32:44,640 --> 00:32:47,279
have similar i think we are allies in

838
00:32:47,279 --> 00:32:48,720
this fight right

839
00:32:48,720 --> 00:32:50,320
um

840
00:32:50,320 --> 00:32:51,519
so

841
00:32:51,519 --> 00:32:53,919
what what do we do to address these

842
00:32:53,919 --> 00:32:55,519
issues i think

843
00:32:55,519 --> 00:32:58,320
our focus as a table is much more to to

844
00:32:58,320 --> 00:33:00,480
look at like how can you strengthen the

845
00:33:00,480 --> 00:33:01,919
communities

846
00:33:01,919 --> 00:33:02,880
um

847
00:33:02,880 --> 00:33:05,279
to um kind of

848
00:33:05,279 --> 00:33:07,440
yeah to content with these systems like

849
00:33:07,440 --> 00:33:10,399
build grassroots power organizing

850
00:33:10,399 --> 00:33:11,360
um

851
00:33:11,360 --> 00:33:13,200
do their own community different

852
00:33:13,200 --> 00:33:15,360
research choose their own priorities on

853
00:33:15,360 --> 00:33:18,080
what they can want to campaign about etc

854
00:33:18,080 --> 00:33:19,919
and also support each other learn from

855
00:33:19,919 --> 00:33:22,320
each other in these fights so that's the

856
00:33:22,320 --> 00:33:24,559
european network is trying to do

857
00:33:24,559 --> 00:33:25,600
um

858
00:33:25,600 --> 00:33:27,360
and we're also trying to find points of

859
00:33:27,360 --> 00:33:29,679
interventions at different levels so

860
00:33:29,679 --> 00:33:32,000
like for example i think this community

861
00:33:32,000 --> 00:33:34,240
grassroots driven world

862
00:33:34,240 --> 00:33:36,399
work is something that a lot of people

863
00:33:36,399 --> 00:33:38,320
saying like that's a good idea we should

864
00:33:38,320 --> 00:33:39,679
do more of that

865
00:33:39,679 --> 00:33:41,840
sadly there's hardly any funding for it

866
00:33:41,840 --> 00:33:45,200
i must say that capacity mostly goes to

867
00:33:45,200 --> 00:33:47,279
these expert places of like legal

868
00:33:47,279 --> 00:33:49,279
contestation or

869
00:33:49,279 --> 00:33:52,080
um

870
00:33:52,080 --> 00:33:54,320
technology design etc

871
00:33:54,320 --> 00:33:58,000
um so there's an issue there um

872
00:33:58,000 --> 00:34:01,919
but um this process organizing i think

873
00:34:01,919 --> 00:34:04,799
how do you do that without i think

874
00:34:04,799 --> 00:34:06,559
kind of

875
00:34:06,559 --> 00:34:09,440
exploiting these communities again you

876
00:34:09,440 --> 00:34:10,800
know or

877
00:34:10,800 --> 00:34:12,159
being

878
00:34:12,159 --> 00:34:14,560
fatigising community activism i think

879
00:34:14,560 --> 00:34:16,719
that's a very interesting question

880
00:34:16,719 --> 00:34:18,960
and there i would like to pose or to

881
00:34:18,960 --> 00:34:20,639
propose that we

882
00:34:20,639 --> 00:34:23,839
as experts in a certain um i don't know

883
00:34:23,839 --> 00:34:25,359
if i want to call myself experts but

884
00:34:25,359 --> 00:34:27,040
here i am right

885
00:34:27,040 --> 00:34:30,320
um that we in our uh all address this

886
00:34:30,320 --> 00:34:33,199
issue also in our environment so for

887
00:34:33,199 --> 00:34:36,480
example um i want to give this example

888
00:34:36,480 --> 00:34:38,960
for this crowd specifically one of our

889
00:34:38,960 --> 00:34:41,760
members no tech for tyrants addresses

890
00:34:41,760 --> 00:34:45,599
the role of academia in designing a kind

891
00:34:45,599 --> 00:34:48,560
of harmful technologies addresses the

892
00:34:48,560 --> 00:34:50,159
funding from

893
00:34:50,159 --> 00:34:52,399
like deloitte from all these like

894
00:34:52,399 --> 00:34:55,520
welfare frauds detection algorithm

895
00:34:55,520 --> 00:34:57,599
algorithm designers

896
00:34:57,599 --> 00:35:00,960
um address kind of these programs that

897
00:35:00,960 --> 00:35:04,640
for example eu funded in the ai arms

898
00:35:04,640 --> 00:35:05,599
race

899
00:35:05,599 --> 00:35:09,839
promise all the solutions of uh of ai to

900
00:35:09,839 --> 00:35:12,240
solve our discrimination to solve our

901
00:35:12,240 --> 00:35:15,119
biases etc but of course don't deliver

902
00:35:15,119 --> 00:35:18,160
that in a very interest system system as

903
00:35:18,160 --> 00:35:21,280
nadia um already spoke out

904
00:35:21,280 --> 00:35:24,000
um and this is

905
00:35:24,000 --> 00:35:25,440
i think what's

906
00:35:25,440 --> 00:35:27,440
uh hearing from people and also from my

907
00:35:27,440 --> 00:35:29,520
experience i think what is super

908
00:35:29,520 --> 00:35:30,400
important

909
00:35:30,400 --> 00:35:32,320
this what is a super important

910
00:35:32,320 --> 00:35:34,000
distinction to make

911
00:35:34,000 --> 00:35:37,359
is this focus on universal human rights

912
00:35:37,359 --> 00:35:39,839
standards versus a community that's

913
00:35:39,839 --> 00:35:42,560
criminalized and for whom this unif

914
00:35:42,560 --> 00:35:44,560
so-called universal human rights

915
00:35:44,560 --> 00:35:47,440
standards often do not apply

916
00:35:47,440 --> 00:35:49,359
by because they're by governments

917
00:35:49,359 --> 00:35:52,160
they're pushed into anti-terrorism

918
00:35:52,160 --> 00:35:56,720
um uh like programs into fraudsters into

919
00:35:56,720 --> 00:35:59,520
etc so of course like

920
00:35:59,520 --> 00:36:02,800
the fight for for strong for for the

921
00:36:02,800 --> 00:36:05,440
actual implementation of human rights i

922
00:36:05,440 --> 00:36:07,680
mean if we could already do that it's

923
00:36:07,680 --> 00:36:10,480
amazing and i support all the court

924
00:36:10,480 --> 00:36:12,560
cases that are being fought there etc

925
00:36:12,560 --> 00:36:13,599
etc

926
00:36:13,599 --> 00:36:15,200
but there's a

927
00:36:15,200 --> 00:36:18,560
big part that even if you say like we're

928
00:36:18,560 --> 00:36:19,680
against

929
00:36:19,680 --> 00:36:23,359
like we want to um make sure that there

930
00:36:23,359 --> 00:36:25,760
is a better legal framework

931
00:36:25,760 --> 00:36:26,480
that

932
00:36:26,480 --> 00:36:29,200
you will see that these communities are

933
00:36:29,200 --> 00:36:31,200
criminalized and therefore the

934
00:36:31,200 --> 00:36:33,680
protection that that offers to many of

935
00:36:33,680 --> 00:36:36,480
us won't offer them protection and this

936
00:36:36,480 --> 00:36:39,839
is also my experience in in my personal

937
00:36:39,839 --> 00:36:42,640
life and this is also why i'm a very

938
00:36:42,640 --> 00:36:47,680
cynical person about like legal kind of

939
00:36:47,920 --> 00:36:51,040
pathways so i apologize for that because

940
00:36:51,040 --> 00:36:51,920
i do

941
00:36:51,920 --> 00:36:54,320
see that it's one of the parts of the

942
00:36:54,320 --> 00:36:56,880
struggle that it's important just as the

943
00:36:56,880 --> 00:37:00,160
grassroots activism is and just as like

944
00:37:00,160 --> 00:37:02,800
let's just be honest the material

945
00:37:02,800 --> 00:37:03,920
supports

946
00:37:03,920 --> 00:37:06,480
like working for that for people you

947
00:37:06,480 --> 00:37:08,400
know supporting them

948
00:37:08,400 --> 00:37:10,000
helping people to

949
00:37:10,000 --> 00:37:12,640
fill out their documents or forms for

950
00:37:12,640 --> 00:37:14,800
example that's also really important

951
00:37:14,800 --> 00:37:16,800
part of the struggle of the grassroots

952
00:37:16,800 --> 00:37:20,800
groups that we're working with

953
00:37:21,040 --> 00:37:24,400
i hope that answers the question

954
00:37:26,000 --> 00:37:27,920
thank you senator perfect um i think

955
00:37:27,920 --> 00:37:29,520
it's such an important point also to

956
00:37:29,520 --> 00:37:31,119
emphasize the

957
00:37:31,119 --> 00:37:32,720
limitations of these human rights

958
00:37:32,720 --> 00:37:34,560
standards in this regard and the

959
00:37:34,560 --> 00:37:36,560
exceptions and the suspension of normal

960
00:37:36,560 --> 00:37:37,680
protection

961
00:37:37,680 --> 00:37:40,560
if you talk about these type of cases

962
00:37:40,560 --> 00:37:43,440
if um there are no other specific

963
00:37:43,440 --> 00:37:45,920
interventions and points more focused on

964
00:37:45,920 --> 00:37:48,240
the child benefit scandal i would like

965
00:37:48,240 --> 00:37:51,599
to give the floor to jill

966
00:37:51,599 --> 00:37:54,880
yeah hi thank you naomi um also for your

967
00:37:54,880 --> 00:37:57,280
wonderfully composed and calm moderating

968
00:37:57,280 --> 00:37:59,359
skills and also for the speakers for

969
00:37:59,359 --> 00:38:01,520
sharing um your perspective i think i

970
00:38:01,520 --> 00:38:03,280
also want to pick up i mean we've

971
00:38:03,280 --> 00:38:04,400
already kind of

972
00:38:04,400 --> 00:38:05,599
gone into

973
00:38:05,599 --> 00:38:07,119
a little bit like reflecting on the

974
00:38:07,119 --> 00:38:10,240
broader kind of uh discussions beyond

975
00:38:10,240 --> 00:38:12,560
the dutch childcare benefit scandal uh

976
00:38:12,560 --> 00:38:14,240
but then i think bringing back to

977
00:38:14,240 --> 00:38:16,839
santa's last point and also nadia's uh

978
00:38:16,839 --> 00:38:20,320
previous uh presentation um in the sense

979
00:38:20,320 --> 00:38:23,680
of nadia how do you kind of see um

980
00:38:23,680 --> 00:38:25,440
or how would you react i think to some

981
00:38:25,440 --> 00:38:27,440
of the issues that that sana has raised

982
00:38:27,440 --> 00:38:30,079
right like this kind of um

983
00:38:30,079 --> 00:38:32,800
not tension but kind of how uh there's

984
00:38:32,800 --> 00:38:36,400
this a lot of energy being put into um

985
00:38:36,400 --> 00:38:39,680
uh reacting or contributing to to some

986
00:38:39,680 --> 00:38:41,520
of the legal frameworks that are ongoing

987
00:38:41,520 --> 00:38:44,240
uh at the eu level and increasingly so

988
00:38:44,240 --> 00:38:46,160
because there are even more proposals

989
00:38:46,160 --> 00:38:48,720
which also requires the effort of civil

990
00:38:48,720 --> 00:38:51,680
society and ngos uh

991
00:38:51,680 --> 00:38:53,920
reacting to those but then also where

992
00:38:53,920 --> 00:38:57,440
does that leave uh the resources uh that

993
00:38:57,440 --> 00:38:59,839
are needed um or organization

994
00:38:59,839 --> 00:39:02,240
organizations and expert expertise that

995
00:39:02,240 --> 00:39:04,160
are needed for these kind of communities

996
00:39:04,160 --> 00:39:06,480
how do you see i would say not

997
00:39:06,480 --> 00:39:09,359
reconciling but how do we move forward

998
00:39:09,359 --> 00:39:12,800
uh in terms of the idea that um

999
00:39:12,800 --> 00:39:14,560
well there are just a lack of resources

1000
00:39:14,560 --> 00:39:17,440
in so many areas um could you respond a

1001
00:39:17,440 --> 00:39:20,800
little bit to do that

1002
00:39:21,760 --> 00:39:24,160
of course and um yeah i can i can

1003
00:39:24,160 --> 00:39:26,560
definitely understand uh where sun is

1004
00:39:26,560 --> 00:39:30,320
coming from uh

1005
00:39:30,320 --> 00:39:34,320
she says a physical view on on law

1006
00:39:34,320 --> 00:39:35,440
um

1007
00:39:35,440 --> 00:39:37,440
i as a lawyer was

1008
00:39:37,440 --> 00:39:40,560
obviously triggered by that as a

1009
00:39:40,560 --> 00:39:43,680
became a lawyer to fight this social

1010
00:39:43,680 --> 00:39:45,040
injustice

1011
00:39:45,040 --> 00:39:48,800
uh i believe view the law as a tool to

1012
00:39:48,800 --> 00:39:52,400
challenge these injustices

1013
00:39:52,400 --> 00:39:54,800
but i definitely agree that

1014
00:39:54,800 --> 00:39:58,960
uh our human rights framework

1015
00:39:58,960 --> 00:40:01,920
is great in theory but

1016
00:40:01,920 --> 00:40:04,319
is not effective yet in

1017
00:40:04,319 --> 00:40:07,759
many countries that have

1018
00:40:08,000 --> 00:40:10,560
agreed upon this framework

1019
00:40:10,560 --> 00:40:13,760
so there is a lot of work that still

1020
00:40:13,760 --> 00:40:16,400
needs to be done um

1021
00:40:16,400 --> 00:40:18,480
in that way and i think

1022
00:40:18,480 --> 00:40:22,319
um digital rights organizations but also

1023
00:40:22,319 --> 00:40:24,720
uh

1024
00:40:25,880 --> 00:40:28,319
anti-racism organizations and other

1025
00:40:28,319 --> 00:40:30,880
organizations that are

1026
00:40:30,880 --> 00:40:32,370
trying to improve

1027
00:40:32,370 --> 00:40:33,520
[Music]

1028
00:40:33,520 --> 00:40:35,520
the protection of human rights can

1029
00:40:35,520 --> 00:40:38,640
contribute to that

1030
00:40:38,640 --> 00:40:42,319
but we're facing a situation in which uh

1031
00:40:42,319 --> 00:40:43,520
governments

1032
00:40:43,520 --> 00:40:45,440
especially in europe

1033
00:40:45,440 --> 00:40:47,760
and u.s are

1034
00:40:47,760 --> 00:40:49,680
are very racist

1035
00:40:49,680 --> 00:40:50,960
and are

1036
00:40:50,960 --> 00:40:53,280
willingly

1037
00:40:53,280 --> 00:40:57,119
making policies that are racist

1038
00:40:57,119 --> 00:40:58,720
so

1039
00:40:58,720 --> 00:41:02,720
there is a huge gap between

1040
00:41:02,720 --> 00:41:06,959
rights of the need to protect

1041
00:41:08,160 --> 00:41:09,599
minorities

1042
00:41:09,599 --> 00:41:11,280
and affected groups

1043
00:41:11,280 --> 00:41:14,160
on one side

1044
00:41:14,400 --> 00:41:16,480
and what these governments are doing on

1045
00:41:16,480 --> 00:41:19,960
the other side

1046
00:41:22,720 --> 00:41:25,359
yeah thank you nadia um sorry that's a

1047
00:41:25,359 --> 00:41:27,920
question from claire that i i think uh i

1048
00:41:27,920 --> 00:41:30,319
will pick up uh before i i have a lot of

1049
00:41:30,319 --> 00:41:31,920
questions actually but i'm gonna

1050
00:41:31,920 --> 00:41:35,440
prioritize that q a okay um so claire is

1051
00:41:35,440 --> 00:41:38,240
asking uh whether anyone from the panel

1052
00:41:38,240 --> 00:41:40,079
can tell us more about the legislative

1053
00:41:40,079 --> 00:41:42,319
developments in the netherlands um as

1054
00:41:42,319 --> 00:41:44,240
the scandal didn't affect the election

1055
00:41:44,240 --> 00:41:45,680
results at all

1056
00:41:45,680 --> 00:41:47,599
and the government is still planning to

1057
00:41:47,599 --> 00:41:50,640
continue with the fraud detection system

1058
00:41:50,640 --> 00:41:51,680
right

1059
00:41:51,680 --> 00:41:52,960
yeah

1060
00:41:52,960 --> 00:41:54,720
um

1061
00:41:54,720 --> 00:41:58,560
yeah so so far what we have seen um and

1062
00:41:58,560 --> 00:42:00,720
what i've heard from the

1063
00:42:00,720 --> 00:42:03,040
secretary of state and the minister that

1064
00:42:03,040 --> 00:42:05,760
more than 320 initiatives have been

1065
00:42:05,760 --> 00:42:07,520
started to

1066
00:42:07,520 --> 00:42:10,160
fix uh the damage from the child care

1067
00:42:10,160 --> 00:42:11,440
benefits care

1068
00:42:11,440 --> 00:42:13,680
scandal but there's no comprehensive

1069
00:42:13,680 --> 00:42:16,960
list on what these initiatives are so um

1070
00:42:16,960 --> 00:42:18,160
or

1071
00:42:18,160 --> 00:42:20,560
specific

1072
00:42:20,560 --> 00:42:22,800
criteria

1073
00:42:22,800 --> 00:42:25,680
so this is very worrisome because the

1074
00:42:25,680 --> 00:42:30,399
same group of government workers that

1075
00:42:30,960 --> 00:42:34,480
continued and built this uh this place

1076
00:42:34,480 --> 00:42:37,599
of inequality in their systems

1077
00:42:37,599 --> 00:42:41,040
now are internally fixing this of course

1078
00:42:41,040 --> 00:42:43,839
with the help from outsiders but if you

1079
00:42:43,839 --> 00:42:46,560
have if you had an anti-discrimination

1080
00:42:46,560 --> 00:42:50,480
course for maybe three afternoons

1081
00:42:50,480 --> 00:42:53,040
that is not enough to fix these

1082
00:42:53,040 --> 00:42:55,440
inequalities like that is absolutely not

1083
00:42:55,440 --> 00:42:57,920
enough so this is highly problematic

1084
00:42:57,920 --> 00:43:00,319
also what we see in the because there's

1085
00:43:00,319 --> 00:43:02,079
now finally a new government the

1086
00:43:02,079 --> 00:43:03,839
government fell over the child care

1087
00:43:03,839 --> 00:43:05,520
benefits scandal in the beginning of

1088
00:43:05,520 --> 00:43:07,200
2021

1089
00:43:07,200 --> 00:43:10,400
now the beginning of or the end of 2022

1090
00:43:10,400 --> 00:43:12,640
there was finally a new government form

1091
00:43:12,640 --> 00:43:15,520
and there's now a government

1092
00:43:15,520 --> 00:43:18,800
how do you say that statement like um

1093
00:43:18,800 --> 00:43:21,440
a plan for the next years

1094
00:43:21,440 --> 00:43:23,040
and the

1095
00:43:23,040 --> 00:43:24,640
uh

1096
00:43:24,640 --> 00:43:27,280
the human rights protection in order to

1097
00:43:27,280 --> 00:43:32,480
uh one address um systemic racism and to

1098
00:43:32,480 --> 00:43:36,160
also address the technology side

1099
00:43:36,160 --> 00:43:38,640
of this

1100
00:43:38,880 --> 00:43:40,960
of the inequality that we saw

1101
00:43:40,960 --> 00:43:43,920
um with the um child care benefit

1102
00:43:43,920 --> 00:43:47,280
scandal it is not enough it looks great

1103
00:43:47,280 --> 00:43:49,119
if you look at the government statement

1104
00:43:49,119 --> 00:43:51,839
that there will be an ai supervisor but

1105
00:43:51,839 --> 00:43:54,319
guess what that comes from eu regulation

1106
00:43:54,319 --> 00:43:57,599
the ai act so it's really like

1107
00:43:57,599 --> 00:44:00,480
presenting it as a step forward whereas

1108
00:44:00,480 --> 00:44:02,560
that is the absolute minimum that you

1109
00:44:02,560 --> 00:44:04,880
could copy and put in your

1110
00:44:04,880 --> 00:44:07,040
your plans just what is happening in the

1111
00:44:07,040 --> 00:44:09,839
eu

