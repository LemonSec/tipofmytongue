1
00:00:08,880 --> 00:00:11,440
hi everyone good afternoon and thank you

2
00:00:11,440 --> 00:00:13,759
for joining us my name is vidushi mata

3
00:00:13,759 --> 00:00:15,920
i'm a senior program officer

4
00:00:15,920 --> 00:00:19,119
at article 19 where i work on artificial

5
00:00:19,119 --> 00:00:21,199
intelligence and machine learning and

6
00:00:21,199 --> 00:00:23,279
its impacts on human rights

7
00:00:23,279 --> 00:00:24,320
um

8
00:00:24,320 --> 00:00:25,920
i'm just going to make sure that we have

9
00:00:25,920 --> 00:00:28,000
all our speakers yes we do

10
00:00:28,000 --> 00:00:30,400
so we're ready to get started

11
00:00:30,400 --> 00:00:32,159
um just to give you kind of a background

12
00:00:32,159 --> 00:00:34,000
on what this this conversation is

13
00:00:34,000 --> 00:00:36,160
supposed to be about um when we talk

14
00:00:36,160 --> 00:00:37,920
about the ai act a lot of the

15
00:00:37,920 --> 00:00:40,239
conversation revolves around you know

16
00:00:40,239 --> 00:00:42,079
what kind of regulation we want what

17
00:00:42,079 --> 00:00:44,320
kind of safeguards we need and in this

18
00:00:44,320 --> 00:00:46,399
session we're hoping to kind of talk

19
00:00:46,399 --> 00:00:48,640
about how do we ensure that some

20
00:00:48,640 --> 00:00:50,160
technologies that are fundamentally

21
00:00:50,160 --> 00:00:51,920
problematic or fundamentally dubious

22
00:00:51,920 --> 00:00:54,320
don't make it to the stage of deployment

23
00:00:54,320 --> 00:00:57,039
at all um as i'm sure many of you know

24
00:00:57,039 --> 00:00:59,840
the eu ai act contemplates currently a

25
00:00:59,840 --> 00:01:01,920
risk-based approach to regulating ai

26
00:01:01,920 --> 00:01:04,479
systems where we have some systems that

27
00:01:04,479 --> 00:01:06,479
cause unacceptable risks that should be

28
00:01:06,479 --> 00:01:08,080
banned and prohibited from being placed

29
00:01:08,080 --> 00:01:09,439
on the market

30
00:01:09,439 --> 00:01:12,000
other ai systems are classified as high

31
00:01:12,000 --> 00:01:13,520
risk that can be placed on the market

32
00:01:13,520 --> 00:01:16,240
subject to mandatory requirements and

33
00:01:16,240 --> 00:01:18,240
then air systems that pose limited risks

34
00:01:18,240 --> 00:01:21,040
that only have to subject themselves to

35
00:01:21,040 --> 00:01:23,600
transparency obligations so while this

36
00:01:23,600 --> 00:01:25,200
classification and this risk-based

37
00:01:25,200 --> 00:01:27,280
approach is a welcome one uh what we're

38
00:01:27,280 --> 00:01:29,119
finding is that there is kind of an

39
00:01:29,119 --> 00:01:32,159
inconsistency in how certain ai systems

40
00:01:32,159 --> 00:01:34,640
are classified under this particular

41
00:01:34,640 --> 00:01:36,720
schema where you know things like

42
00:01:36,720 --> 00:01:38,720
emotional recognition and biometric

43
00:01:38,720 --> 00:01:41,280
categorization systems for instance are

44
00:01:41,280 --> 00:01:42,640
you know as we know fundamentally

45
00:01:42,640 --> 00:01:44,479
problematic inconsistent with

46
00:01:44,479 --> 00:01:46,240
international human rights standards but

47
00:01:46,240 --> 00:01:48,399
are also currently considered limited

48
00:01:48,399 --> 00:01:51,200
risk um so in today's session we kind of

49
00:01:51,200 --> 00:01:52,799
want to explore what are the various

50
00:01:52,799 --> 00:01:55,040
arguments and strategies that we can and

51
00:01:55,040 --> 00:01:56,320
should use

52
00:01:56,320 --> 00:01:57,840
when it comes to

53
00:01:57,840 --> 00:01:59,360
you know kind of engaging on these

54
00:01:59,360 --> 00:02:01,520
issues for civil society what will lead

55
00:02:01,520 --> 00:02:03,360
to a more rigorous consistent and

56
00:02:03,360 --> 00:02:05,840
critical classification of unacceptable

57
00:02:05,840 --> 00:02:08,399
uses under the current proposal

58
00:02:08,399 --> 00:02:10,560
um and so before we get into uh

59
00:02:10,560 --> 00:02:12,239
questions and here for my excellent

60
00:02:12,239 --> 00:02:14,080
panelists i just want to introduce the

61
00:02:14,080 --> 00:02:16,319
panel to you um

62
00:02:16,319 --> 00:02:18,400
so the first speaker that we have is uh

63
00:02:18,400 --> 00:02:20,879
maria maria luisa stassi who is my

64
00:02:20,879 --> 00:02:23,040
colleague at article 19

65
00:02:23,040 --> 00:02:24,640
she works as the head of law and policy

66
00:02:24,640 --> 00:02:27,040
with a focus on digital markets

67
00:02:27,040 --> 00:02:28,720
and her work contributes to the

68
00:02:28,720 --> 00:02:30,959
development of article 19's policies on

69
00:02:30,959 --> 00:02:34,879
biometrics ai infrastructure competition

70
00:02:34,879 --> 00:02:36,640
and the regulatory framework for digital

71
00:02:36,640 --> 00:02:39,360
markets she also provides legal support

72
00:02:39,360 --> 00:02:41,280
to article 19's regional offices on

73
00:02:41,280 --> 00:02:43,599
digital rights and media policy issues

74
00:02:43,599 --> 00:02:45,519
welcome maria

75
00:02:45,519 --> 00:02:48,000
daniel leufer works as europe policy

76
00:02:48,000 --> 00:02:50,959
analyst at accessnow's brussels office

77
00:02:50,959 --> 00:02:53,040
he works on issues around artificial

78
00:02:53,040 --> 00:02:54,879
intelligence and data protection with a

79
00:02:54,879 --> 00:02:56,959
focus on face and

80
00:02:56,959 --> 00:02:59,360
facial recognition and other biometrics

81
00:02:59,360 --> 00:03:01,840
he has worked extensively on the euai

82
00:03:01,840 --> 00:03:03,599
act he studied a wealth of knowledge

83
00:03:03,599 --> 00:03:06,400
with a particular focus on prohibitions

84
00:03:06,400 --> 00:03:07,680
um

85
00:03:07,680 --> 00:03:09,599
surrounding emerging technologies like

86
00:03:09,599 --> 00:03:11,440
emotion recognition and biometric

87
00:03:11,440 --> 00:03:14,159
catheterization uh thanks for joining us

88
00:03:14,159 --> 00:03:16,640
danielle and finally we have professor

89
00:03:16,640 --> 00:03:18,400
lorna mcgregor who's a professor of

90
00:03:18,400 --> 00:03:20,640
international human rights law in the

91
00:03:20,640 --> 00:03:22,800
law school

92
00:03:22,800 --> 00:03:25,120
at the university of essex she's also

93
00:03:25,120 --> 00:03:26,799
the director of the human rights big

94
00:03:26,799 --> 00:03:29,120
data and technology project her current

95
00:03:29,120 --> 00:03:31,760
research focuses on data analytics and

96
00:03:31,760 --> 00:03:33,920
emerging technologies like ai she

97
00:03:33,920 --> 00:03:36,159
currently co-leads a project that i'm

98
00:03:36,159 --> 00:03:38,959
also a part of that uh is is a privilege

99
00:03:38,959 --> 00:03:40,959
to work with lorna on to examine the

100
00:03:40,959 --> 00:03:43,040
case for red lines in technology design

101
00:03:43,040 --> 00:03:45,200
development and deployment and to assess

102
00:03:45,200 --> 00:03:47,120
the most effective strategies for

103
00:03:47,120 --> 00:03:49,040
influencing law and policy at the

104
00:03:49,040 --> 00:03:50,720
international level as well as in

105
00:03:50,720 --> 00:03:53,519
different national and regional contexts

106
00:03:53,519 --> 00:03:56,159
uh so thank you so much for joining us

107
00:03:56,159 --> 00:03:59,360
and uh without any further ado i'm gonna

108
00:03:59,360 --> 00:04:01,680
turn over to isa

109
00:04:01,680 --> 00:04:04,000
by asking you a broad question given

110
00:04:04,000 --> 00:04:06,319
your work on biometrics in the european

111
00:04:06,319 --> 00:04:08,239
context um which

112
00:04:08,239 --> 00:04:09,840
you know really involves a range of

113
00:04:09,840 --> 00:04:11,120
issues whether it's biometric

114
00:04:11,120 --> 00:04:12,959
technologies or infrastructure or

115
00:04:12,959 --> 00:04:14,720
surveillance ads

116
00:04:14,720 --> 00:04:16,000
so when it comes to biometric

117
00:04:16,000 --> 00:04:18,160
technologies in particular what would

118
00:04:18,160 --> 00:04:20,478
you say are the main challenges facing

119
00:04:20,478 --> 00:04:23,520
civil society in securing bans against

120
00:04:23,520 --> 00:04:25,199
these technologies that enable mass

121
00:04:25,199 --> 00:04:26,720
surveillance

122
00:04:26,720 --> 00:04:28,880
and also a violation of human rights for

123
00:04:28,880 --> 00:04:33,199
example um like emotional recognition

124
00:04:33,199 --> 00:04:35,280
thank you thank you vedushi and uh hi

125
00:04:35,280 --> 00:04:37,120
everyone uh it's a pleasure to be here

126
00:04:37,120 --> 00:04:39,919
uh and um i'm so happy that this

127
00:04:39,919 --> 00:04:42,479
conversation is happening

128
00:04:42,479 --> 00:04:43,759
so um

129
00:04:43,759 --> 00:04:45,840
as a way of introduction let me take a

130
00:04:45,840 --> 00:04:47,759
little bit of a step back and look at a

131
00:04:47,759 --> 00:04:50,880
broad picture i do believe that the

132
00:04:50,880 --> 00:04:52,880
the main message to be given here is

133
00:04:52,880 --> 00:04:54,320
that when it comes to

134
00:04:54,320 --> 00:04:56,080
the advocacy in terms of biometric

135
00:04:56,080 --> 00:04:57,280
technologies we're talking about

136
00:04:57,280 --> 00:04:59,840
something that is extremely complex and

137
00:04:59,840 --> 00:05:03,199
uh the complexity is not uh only or

138
00:05:03,199 --> 00:05:05,440
necessarily a traditional policy and

139
00:05:05,440 --> 00:05:06,880
legal complexity but it's also a

140
00:05:06,880 --> 00:05:08,880
technological complexity so from the

141
00:05:08,880 --> 00:05:11,280
very beginning that one of the major

142
00:05:11,280 --> 00:05:13,440
obstacles that our civil society we have

143
00:05:13,440 --> 00:05:15,280
we have to face and we have confronted

144
00:05:15,280 --> 00:05:16,880
and many colleagues have done an

145
00:05:16,880 --> 00:05:19,039
excellent job in that

146
00:05:19,039 --> 00:05:21,520
is uh to sort of uh try to figure out

147
00:05:21,520 --> 00:05:24,479
what exactly are we talking about um so

148
00:05:24,479 --> 00:05:26,320
this this is um

149
00:05:26,320 --> 00:05:28,960
extremely difficult because as we all

150
00:05:28,960 --> 00:05:32,639
know it's a sector that is uh permeated

151
00:05:32,639 --> 00:05:35,120
by an informational symmetry so uh there

152
00:05:35,120 --> 00:05:37,199
is not a lot of transparency it's super

153
00:05:37,199 --> 00:05:38,639
difficult to get access to the

154
00:05:38,639 --> 00:05:40,479
information we need to we need to we

155
00:05:40,479 --> 00:05:42,639
need and we want to have in order to

156
00:05:42,639 --> 00:05:44,800
properly shape our own

157
00:05:44,800 --> 00:05:48,720
ideas and advocacy goals um so um and

158
00:05:48,720 --> 00:05:50,479
this even if we had the second guy

159
00:05:50,479 --> 00:05:52,320
background so

160
00:05:52,320 --> 00:05:54,240
you know you go figure if we don't even

161
00:05:54,240 --> 00:05:56,960
have that technical background to start

162
00:05:56,960 --> 00:05:58,479
with um

163
00:05:58,479 --> 00:06:01,680
it's not a case uh i think that um on a

164
00:06:01,680 --> 00:06:04,400
good number of of litigation about the

165
00:06:04,400 --> 00:06:07,039
users of biometric technologies have

166
00:06:07,039 --> 00:06:08,639
started with access to information

167
00:06:08,639 --> 00:06:10,400
requests

168
00:06:10,400 --> 00:06:13,600
you know a lot about the level of lack

169
00:06:13,600 --> 00:06:15,360
of information

170
00:06:15,360 --> 00:06:17,840
and lack of transparency and i think

171
00:06:17,840 --> 00:06:22,319
this is a big obstacles that we can find

172
00:06:22,319 --> 00:06:24,319
equally in the public sector and the

173
00:06:24,319 --> 00:06:25,840
private sector there is no much

174
00:06:25,840 --> 00:06:28,080
difference unfortunately out there so i

175
00:06:28,080 --> 00:06:30,319
would say the first main obstacle for

176
00:06:30,319 --> 00:06:32,720
civil society is to try to overtake this

177
00:06:32,720 --> 00:06:34,479
information symmetry

178
00:06:34,479 --> 00:06:37,919
that it's um it's absolutely um

179
00:06:37,919 --> 00:06:39,520
undermining every every fighter

180
00:06:39,520 --> 00:06:41,840
possibility uh to properly act the

181
00:06:41,840 --> 00:06:45,360
second one is to try also to sort of

182
00:06:45,360 --> 00:06:48,720
identify the right actors to talk to and

183
00:06:48,720 --> 00:06:50,400
the right language to talk to when it

184
00:06:50,400 --> 00:06:52,080
comes to you know you have mentioned

185
00:06:52,080 --> 00:06:54,560
already emotional recognition and i know

186
00:06:54,560 --> 00:06:56,639
we had the discussion with a few of you

187
00:06:56,639 --> 00:06:58,880
and and many more people

188
00:06:58,880 --> 00:07:01,520
what do we mean about what about

189
00:07:01,520 --> 00:07:02,639
emotional recognition what are the

190
00:07:02,639 --> 00:07:04,319
definitions with the workable

191
00:07:04,319 --> 00:07:06,240
definitions we're all using because we

192
00:07:06,240 --> 00:07:07,440
might end up having the same

193
00:07:07,440 --> 00:07:08,880
conversation again and again and again

194
00:07:08,880 --> 00:07:10,479
but the premises are different because

195
00:07:10,479 --> 00:07:11,360
people

196
00:07:11,360 --> 00:07:13,120
use different concepts

197
00:07:13,120 --> 00:07:16,639
so this is i think a second a major

198
00:07:16,639 --> 00:07:18,479
problem that we have when we call about

199
00:07:18,479 --> 00:07:21,199
bans or red lines because we need to be

200
00:07:21,199 --> 00:07:22,880
convincing and we need to have that

201
00:07:22,880 --> 00:07:24,720
therefore we need to have uh exact

202
00:07:24,720 --> 00:07:27,440
information shape our project our calls

203
00:07:27,440 --> 00:07:30,400
in a very defined and targeted way and

204
00:07:30,400 --> 00:07:32,800
so all those are essential elements that

205
00:07:32,800 --> 00:07:34,560
i i think

206
00:07:34,560 --> 00:07:36,400
we need to work on we've been working on

207
00:07:36,400 --> 00:07:38,400
and some of us have done as i said a

208
00:07:38,400 --> 00:07:41,039
very great job about this um there are

209
00:07:41,039 --> 00:07:43,280
this is the pessimistic notes the

210
00:07:43,280 --> 00:07:45,840
optimistic note uh it's that this has

211
00:07:45,840 --> 00:07:48,080
also provided the opportunity for

212
00:07:48,080 --> 00:07:50,400
a variety of

213
00:07:50,400 --> 00:07:51,840
people from a different background to

214
00:07:51,840 --> 00:07:53,599
come together and try to shape those

215
00:07:53,599 --> 00:07:54,960
goals and especially when it comes to

216
00:07:54,960 --> 00:07:56,400
the bands

217
00:07:56,400 --> 00:07:58,720
so we have uh you know typical advocacy

218
00:07:58,720 --> 00:08:00,160
community and legal community and

219
00:08:00,160 --> 00:08:01,759
technical community coming together and

220
00:08:01,759 --> 00:08:04,479
trying to get to a proper uh

221
00:08:04,479 --> 00:08:06,879
to put together a proper message uh and

222
00:08:06,879 --> 00:08:08,639
a proper assessment of what what what

223
00:08:08,639 --> 00:08:10,800
those uh challenges are and why we need

224
00:08:10,800 --> 00:08:13,840
to go for it um if you think about it

225
00:08:13,840 --> 00:08:15,840
narrowing down to the eu

226
00:08:15,840 --> 00:08:17,440
this conversation started a few years

227
00:08:17,440 --> 00:08:19,199
ago already and at the very beginning

228
00:08:19,199 --> 00:08:21,199
you might all remember that that the

229
00:08:21,199 --> 00:08:25,440
major uh the the buzzword was ethical ai

230
00:08:25,440 --> 00:08:27,919
so in this sense we've gone a long way

231
00:08:27,919 --> 00:08:29,680
through right so

232
00:08:29,680 --> 00:08:32,240
we have we have i i can you know

233
00:08:32,240 --> 00:08:34,399
confidently uh say that we have

234
00:08:34,399 --> 00:08:37,360
abandoned the idea at least you know uh

235
00:08:37,360 --> 00:08:39,039
in certain parts of this conversation

236
00:08:39,039 --> 00:08:41,120
we've abandoned the idea that an ethical

237
00:08:41,120 --> 00:08:43,279
framework could be sufficient to achieve

238
00:08:43,279 --> 00:08:45,680
our goals and we have shifted this this

239
00:08:45,680 --> 00:08:47,360
attention to a human rights framework

240
00:08:47,360 --> 00:08:49,279
and human rights impact assessment and

241
00:08:49,279 --> 00:08:51,120
and so on and so forth so

242
00:08:51,120 --> 00:08:53,760
this as broad as let's say a little bit

243
00:08:53,760 --> 00:08:56,399
closer to the possibility to advocate

244
00:08:56,399 --> 00:08:58,160
for proper bonds

245
00:08:58,160 --> 00:09:01,040
because bans in the ethical framework

246
00:09:01,040 --> 00:09:03,519
were extremely more difficult to

247
00:09:03,519 --> 00:09:05,120
introduce

248
00:09:05,120 --> 00:09:07,839
yet as you rightly spotted with the

249
00:09:07,839 --> 00:09:12,240
there is a still a sort of resistance um

250
00:09:12,240 --> 00:09:14,080
because a ban comes together with the

251
00:09:14,080 --> 00:09:16,560
concept of an acceptable risk and we

252
00:09:16,560 --> 00:09:19,920
still have a long way to go before we

253
00:09:19,920 --> 00:09:22,080
get to a consensus of what unacceptable

254
00:09:22,080 --> 00:09:23,760
risk means

255
00:09:23,760 --> 00:09:25,760
i know other people in this conversation

256
00:09:25,760 --> 00:09:27,680
are going to focus on the provisions of

257
00:09:27,680 --> 00:09:30,080
the ai act when it comes to this these

258
00:09:30,080 --> 00:09:33,040
categories but let me just say

259
00:09:33,040 --> 00:09:34,839
that

260
00:09:34,839 --> 00:09:36,720
the one

261
00:09:36,720 --> 00:09:39,839
one of the elements that i think

262
00:09:39,839 --> 00:09:41,519
should be highlighted and is one of the

263
00:09:41,519 --> 00:09:44,880
dearest to me is that the unacceptable

264
00:09:44,880 --> 00:09:46,240
uh

265
00:09:46,240 --> 00:09:48,080
concept seems to be attributed to

266
00:09:48,080 --> 00:09:49,600
individual harm

267
00:09:49,600 --> 00:09:51,440
while here we're very much talking about

268
00:09:51,440 --> 00:09:53,519
societal harm

269
00:09:53,519 --> 00:09:55,519
or harm to

270
00:09:55,519 --> 00:09:57,839
collectives communities etc and this is

271
00:09:57,839 --> 00:10:00,560
why and i i see this extremely linked to

272
00:10:00,560 --> 00:10:02,399
the idea of having a ban for mass

273
00:10:02,399 --> 00:10:04,959
surveillance uh and not for individuals

274
00:10:04,959 --> 00:10:08,480
with variants uh i think another

275
00:10:08,480 --> 00:10:11,360
let's say major challenge that we are we

276
00:10:11,360 --> 00:10:13,279
are facing a civil society in this in

277
00:10:13,279 --> 00:10:14,880
this environment

278
00:10:14,880 --> 00:10:17,680
um is exactly this idea about resisting

279
00:10:17,680 --> 00:10:21,040
to a narrative that is widespread so the

280
00:10:21,040 --> 00:10:23,600
the mass surveillance in in a vacuum

281
00:10:23,600 --> 00:10:26,800
let's say it looked like a bad concept

282
00:10:26,800 --> 00:10:29,120
and no one wants to have a system where

283
00:10:29,120 --> 00:10:31,440
everyone is surveilled but the narrative

284
00:10:31,440 --> 00:10:33,440
that we've seen and i think the

285
00:10:33,440 --> 00:10:36,880
the cop the government pandemic

286
00:10:36,880 --> 00:10:39,279
has created even more momentum for this

287
00:10:39,279 --> 00:10:40,480
narrative

288
00:10:40,480 --> 00:10:42,000
to be uh

289
00:10:42,000 --> 00:10:44,959
introduced and affirmed uh is

290
00:10:44,959 --> 00:10:47,600
the idea that you know there are uh good

291
00:10:47,600 --> 00:10:50,640
uh good reasons while we should uh give

292
00:10:50,640 --> 00:10:51,680
up

293
00:10:51,680 --> 00:10:54,160
our privacy or give up uh you know our

294
00:10:54,160 --> 00:10:56,800
private sphere uh in order to ensure

295
00:10:56,800 --> 00:10:59,760
security for a society or a societal

296
00:10:59,760 --> 00:11:03,200
level uh or um there's been all this

297
00:11:03,200 --> 00:11:06,160
this idea uh you know this this um their

298
00:11:06,160 --> 00:11:08,480
work streams on on tax solutionism the

299
00:11:08,480 --> 00:11:10,480
idea that so as long as you know you

300
00:11:10,480 --> 00:11:11,839
have you have major problems you can

301
00:11:11,839 --> 00:11:13,760
only fix with a number of different

302
00:11:13,760 --> 00:11:15,760
technological developments

303
00:11:15,760 --> 00:11:16,720
i think

304
00:11:16,720 --> 00:11:18,959
as an activist this has been one of the

305
00:11:18,959 --> 00:11:21,040
major problems to try to keep the

306
00:11:21,040 --> 00:11:23,120
general audience focused on the fact

307
00:11:23,120 --> 00:11:26,839
that uh this extremely invasive

308
00:11:26,839 --> 00:11:29,600
technologies uh and especially um

309
00:11:29,600 --> 00:11:31,680
irrespective of the specific use that is

310
00:11:31,680 --> 00:11:34,480
done um in certain circumstances they

311
00:11:34,480 --> 00:11:35,360
will

312
00:11:35,360 --> 00:11:37,519
undermine our fundamental rights if we

313
00:11:37,519 --> 00:11:40,079
are in the eu framework human rights if

314
00:11:40,079 --> 00:11:42,720
if we are somewhere else uh to an extent

315
00:11:42,720 --> 00:11:45,440
of this unacceptable um so that the

316
00:11:45,440 --> 00:11:48,079
proportionality and the necessity uh the

317
00:11:48,079 --> 00:11:49,920
dive you know the usual test that we

318
00:11:49,920 --> 00:11:51,760
that we apply when when it's about

319
00:11:51,760 --> 00:11:53,760
limiting fundamental rights uh will

320
00:11:53,760 --> 00:11:57,600
never uh be respected in those cases

321
00:11:57,600 --> 00:11:59,200
i am

322
00:11:59,200 --> 00:12:01,519
extremely convinced that we have done so

323
00:12:01,519 --> 00:12:03,920
far a very good job in trying to resist

324
00:12:03,920 --> 00:12:06,800
this narrative yet uh it has been pushed

325
00:12:06,800 --> 00:12:08,399
from different ways and different

326
00:12:08,399 --> 00:12:10,480
stakeholders so it's not necessarily

327
00:12:10,480 --> 00:12:11,440
easy

328
00:12:11,440 --> 00:12:12,320
um

329
00:12:12,320 --> 00:12:14,720
and but i say uh you know those those

330
00:12:14,720 --> 00:12:19,040
are definitely the main challenges for

331
00:12:19,040 --> 00:12:21,040
trying to to

332
00:12:21,040 --> 00:12:25,040
call for such strong uh acts uh

333
00:12:25,040 --> 00:12:27,920
putting red lines on what is uh

334
00:12:27,920 --> 00:12:29,680
something that we don't want to have if

335
00:12:29,680 --> 00:12:33,120
i have uh other 30 seconds i think the

336
00:12:33,120 --> 00:12:34,880
the last point i would like to make is

337
00:12:34,880 --> 00:12:37,440
that you also mentioned this video ready

338
00:12:37,440 --> 00:12:40,160
it's this idea of trying to to figure

339
00:12:40,160 --> 00:12:43,440
out um to expand the ban not only to the

340
00:12:43,440 --> 00:12:45,360
specific specific use of the technology

341
00:12:45,360 --> 00:12:47,440
but to go all the way back and look at

342
00:12:47,440 --> 00:12:48,800
and explore

343
00:12:48,800 --> 00:12:50,959
uh also the the area of designing a

344
00:12:50,959 --> 00:12:52,880
technology not only deploying it into

345
00:12:52,880 --> 00:12:55,760
concrete cases uh i think uh what i've

346
00:12:55,760 --> 00:12:58,560
said uh so far so the the all the points

347
00:12:58,560 --> 00:13:00,160
and especially the lack of transparency

348
00:13:00,160 --> 00:13:02,320
is is

349
00:13:02,320 --> 00:13:03,360
um

350
00:13:03,360 --> 00:13:06,000
it does concern also the design and i

351
00:13:06,000 --> 00:13:09,120
think that there we're even farther

352
00:13:09,120 --> 00:13:10,880
to the object

353
00:13:10,880 --> 00:13:13,519
in the deployment or concrete use

354
00:13:13,519 --> 00:13:15,200
scenario because of the wording of the

355
00:13:15,200 --> 00:13:17,440
ai that as we know focuses on on put

356
00:13:17,440 --> 00:13:19,839
into the market and practical uses but

357
00:13:19,839 --> 00:13:21,920
also because we're seeing in europe at

358
00:13:21,920 --> 00:13:24,079
least in the european context messages

359
00:13:24,079 --> 00:13:27,120
coming from different different places

360
00:13:27,120 --> 00:13:29,040
that go against

361
00:13:29,040 --> 00:13:30,160
this

362
00:13:30,160 --> 00:13:31,839
idea that

363
00:13:31,839 --> 00:13:34,000
research or design in itself should be

364
00:13:34,000 --> 00:13:36,320
or could be prohibited

365
00:13:36,320 --> 00:13:37,839
in certain cases

366
00:13:37,839 --> 00:13:40,720
i just mentioned the eye border control

367
00:13:40,720 --> 00:13:43,040
case that you are familiar with i can i

368
00:13:43,040 --> 00:13:45,680
can discuss you know some details about

369
00:13:45,680 --> 00:13:47,120
that later on if we have time but the

370
00:13:47,120 --> 00:13:49,199
main point is that it seems to me that

371
00:13:49,199 --> 00:13:51,120
the court in luxembourg has

372
00:13:51,120 --> 00:13:52,720
as um

373
00:13:52,720 --> 00:13:55,440
provided a message that is a very

374
00:13:55,440 --> 00:13:57,680
worrying message for us

375
00:13:57,680 --> 00:13:59,199
because it has said that basically the

376
00:13:59,199 --> 00:14:01,360
public interest in having transparency

377
00:14:01,360 --> 00:14:04,399
about the the development of emotional

378
00:14:04,399 --> 00:14:06,639
recognition technologies uh

379
00:14:06,639 --> 00:14:09,519
it comes only when it when is the stage

380
00:14:09,519 --> 00:14:11,920
of deploying them and not at the stage

381
00:14:11,920 --> 00:14:14,160
of designing them and i do believe that

382
00:14:14,160 --> 00:14:16,320
we need to have a public debate starting

383
00:14:16,320 --> 00:14:18,800
way before the deployment and possibly

384
00:14:18,800 --> 00:14:21,040
also considered the idea of having bands

385
00:14:21,040 --> 00:14:24,160
waiting way before the deployment stage

386
00:14:24,160 --> 00:14:26,079
uh so yes it's a worrying message it

387
00:14:26,079 --> 00:14:27,760
comes from you know another institution

388
00:14:27,760 --> 00:14:29,839
so it sort of sends the water where we

389
00:14:29,839 --> 00:14:31,920
are and

390
00:14:31,920 --> 00:14:33,680
it means we still have a lot to do i

391
00:14:33,680 --> 00:14:35,920
guess

392
00:14:37,519 --> 00:14:40,000
thank you so much isa for kicking off

393
00:14:40,000 --> 00:14:42,399
this conversation with so much rich um

394
00:14:42,399 --> 00:14:45,680
so many rich thoughts um

395
00:14:45,680 --> 00:14:48,880
daniel um i'd like to turn to you now uh

396
00:14:48,880 --> 00:14:50,320
especially given you know everything

397
00:14:50,320 --> 00:14:52,399
that essa just said about how

398
00:14:52,399 --> 00:14:54,720
technologies are being classified and

399
00:14:54,720 --> 00:14:56,240
also how they're being conceptualized

400
00:14:56,240 --> 00:14:58,160
not not just by the regulation but also

401
00:14:58,160 --> 00:15:00,480
by the courts um could you talk a little

402
00:15:00,480 --> 00:15:03,040
bit about how you're thinking through

403
00:15:03,040 --> 00:15:05,440
these risk classifications especially

404
00:15:05,440 --> 00:15:07,680
when it comes to technologies like

405
00:15:07,680 --> 00:15:09,760
emotional recognition and what do you

406
00:15:09,760 --> 00:15:11,519
think civil society needs to be paying

407
00:15:11,519 --> 00:15:14,079
attention to at the moment

408
00:15:14,079 --> 00:15:15,920
all right yeah thanks and yeah thanks

409
00:15:15,920 --> 00:15:17,760
again to all the organizers and to

410
00:15:17,760 --> 00:15:20,000
vadushi for organizing this session the

411
00:15:20,000 --> 00:15:22,160
invitation um

412
00:15:22,160 --> 00:15:25,360
yeah i mean an also um really great

413
00:15:25,360 --> 00:15:27,440
introduction there from from issa and i

414
00:15:27,440 --> 00:15:29,199
think i'll definitely come back to some

415
00:15:29,199 --> 00:15:30,959
of the points um

416
00:15:30,959 --> 00:15:32,959
that you raised

417
00:15:32,959 --> 00:15:35,279
the first thing i would say is and again

418
00:15:35,279 --> 00:15:36,800
this is to pick up on one of these

419
00:15:36,800 --> 00:15:39,120
points we should be happy that we're not

420
00:15:39,120 --> 00:15:41,120
still having the ethics conversation i

421
00:15:41,120 --> 00:15:43,120
think that's really a success from civil

422
00:15:43,120 --> 00:15:46,560
society you know since 2018 um there's

423
00:15:46,560 --> 00:15:48,720
been initiatives kind of drawing

424
00:15:48,720 --> 00:15:51,040
critical attention to this ethics

425
00:15:51,040 --> 00:15:52,399
narrative

426
00:15:52,399 --> 00:15:54,000
and i think we were we were successful

427
00:15:54,000 --> 00:15:55,600
in pushing back on that you know we've

428
00:15:55,600 --> 00:15:57,519
had right from then things like the

429
00:15:57,519 --> 00:15:59,360
toronto declaration

430
00:15:59,360 --> 00:16:02,160
pushing for human rights standards um in

431
00:16:02,160 --> 00:16:04,720
machine learning development and it's

432
00:16:04,720 --> 00:16:06,399
really i share that feeling that the

433
00:16:06,399 --> 00:16:07,360
ethics

434
00:16:07,360 --> 00:16:09,120
uh conversation is off the table and

435
00:16:09,120 --> 00:16:11,040
we're really focused on rights and

436
00:16:11,040 --> 00:16:12,800
regulation which is a great place to be

437
00:16:12,800 --> 00:16:14,720
the other thing that i think is a

438
00:16:14,720 --> 00:16:16,800
success although i will qualify it

439
00:16:16,800 --> 00:16:19,199
success is that there are prohibitions

440
00:16:19,199 --> 00:16:21,040
in the current ai act i think that was

441
00:16:21,040 --> 00:16:23,199
not guaranteed from the beginning and

442
00:16:23,199 --> 00:16:26,160
there was a lot of very hard work from

443
00:16:26,160 --> 00:16:28,320
our activists a lot of whom are i see in

444
00:16:28,320 --> 00:16:31,519
the audience here um to get those red

445
00:16:31,519 --> 00:16:35,040
lines um and so yeah we should all be

446
00:16:35,040 --> 00:16:37,600
very happy that um that that's in there

447
00:16:37,600 --> 00:16:38,959
nevertheless

448
00:16:38,959 --> 00:16:40,800
there's a lot of work to be done to

449
00:16:40,800 --> 00:16:42,959
actually ensure that

450
00:16:42,959 --> 00:16:44,639
those prohibitions which are currently

451
00:16:44,639 --> 00:16:46,399
under article five

452
00:16:46,399 --> 00:16:48,639
in the act um are

453
00:16:48,639 --> 00:16:51,120
an effective way to protect people from

454
00:16:51,120 --> 00:16:52,800
from what we're calling unacceptable

455
00:16:52,800 --> 00:16:53,680
risk

456
00:16:53,680 --> 00:16:56,000
um the first issue and like

457
00:16:56,000 --> 00:16:58,399
inconsistency that i would point to

458
00:16:58,399 --> 00:16:59,759
you know there's a

459
00:16:59,759 --> 00:17:01,519
clear conversation that people are

460
00:17:01,519 --> 00:17:03,360
having about the prohibitions that are

461
00:17:03,360 --> 00:17:04,959
in there are not great they're badly

462
00:17:04,959 --> 00:17:06,799
framed on the remote biometric

463
00:17:06,799 --> 00:17:08,160
identification when there's too many

464
00:17:08,160 --> 00:17:10,240
exceptions to the extent that it looks

465
00:17:10,240 --> 00:17:12,480
like almost providing a legal basis for

466
00:17:12,480 --> 00:17:13,520
its use

467
00:17:13,520 --> 00:17:14,480
um

468
00:17:14,480 --> 00:17:15,919
and there are things that should be in

469
00:17:15,919 --> 00:17:17,520
there that aren't in there like a motion

470
00:17:17,520 --> 00:17:20,400
recognition um biometric categorization

471
00:17:20,400 --> 00:17:21,760
and certain uses

472
00:17:21,760 --> 00:17:23,039
um

473
00:17:23,039 --> 00:17:25,039
but kind of beyond the question of

474
00:17:25,039 --> 00:17:26,720
fixing what's in there and adding other

475
00:17:26,720 --> 00:17:29,360
things the fact that that list of

476
00:17:29,360 --> 00:17:31,600
prohibited practices cannot be updated

477
00:17:31,600 --> 00:17:34,080
and does not have any criteria is very

478
00:17:34,080 --> 00:17:37,200
very problematic you know if this

479
00:17:37,200 --> 00:17:40,320
risk-based approach were to be kind of

480
00:17:40,320 --> 00:17:43,039
really consistent and effective and

481
00:17:43,039 --> 00:17:44,480
i will say as well that from the

482
00:17:44,480 --> 00:17:46,799
beginning we were critical of taking a

483
00:17:46,799 --> 00:17:49,200
risk-based approach um and pointed out

484
00:17:49,200 --> 00:17:52,480
some of the inherent problems with that

485
00:17:52,480 --> 00:17:55,280
there should be a clear list of criteria

486
00:17:55,280 --> 00:17:58,720
for each risk level and some form of you

487
00:17:58,720 --> 00:18:01,400
know flagging mechanism to allow a

488
00:18:01,400 --> 00:18:03,679
re-evaluation of how a system has been

489
00:18:03,679 --> 00:18:05,919
classified so that it could for example

490
00:18:05,919 --> 00:18:08,240
be moved up so say for example

491
00:18:08,240 --> 00:18:10,960
you've got some crazy stuff in annex 3

492
00:18:10,960 --> 00:18:13,039
which is the list of high-risk practices

493
00:18:13,039 --> 00:18:15,919
including the use of ai polygraphs in a

494
00:18:15,919 --> 00:18:18,640
migration context which for me is one of

495
00:18:18,640 --> 00:18:20,240
the clearest things that should be

496
00:18:20,240 --> 00:18:22,799
banned um i think ahoy polygraphs across

497
00:18:22,799 --> 00:18:25,360
the board but certainly in the context

498
00:18:25,360 --> 00:18:26,880
of migration

499
00:18:26,880 --> 00:18:28,640
should be prohibited there has to be

500
00:18:28,640 --> 00:18:31,360
some way that a practice like that that

501
00:18:31,360 --> 00:18:33,360
is currently listed as high risk could

502
00:18:33,360 --> 00:18:36,240
be flagged for re-evaluation evaluated

503
00:18:36,240 --> 00:18:38,400
according to clear criteria and moved up

504
00:18:38,400 --> 00:18:39,919
to the

505
00:18:39,919 --> 00:18:41,200
higher risk classification of

506
00:18:41,200 --> 00:18:43,360
unacceptable risks so we're doing some

507
00:18:43,360 --> 00:18:45,280
work around that um

508
00:18:45,280 --> 00:18:46,880
with edri with some other partners

509
00:18:46,880 --> 00:18:49,600
algorithm watch um and then edgy members

510
00:18:49,600 --> 00:18:53,200
um including uh vidushi and isa um so

511
00:18:53,200 --> 00:18:56,320
you know think about how we can open up

512
00:18:56,320 --> 00:18:59,200
article 5 so that it becomes a you know

513
00:18:59,200 --> 00:19:01,679
a useful tool in the future that we can

514
00:19:01,679 --> 00:19:03,200
add new practices i think there's

515
00:19:03,200 --> 00:19:06,480
complete hubris and delusion um in the

516
00:19:06,480 --> 00:19:08,880
idea that that list of four

517
00:19:08,880 --> 00:19:12,080
badly defined practices uh encompasses

518
00:19:12,080 --> 00:19:14,240
every single unacceptable risk that

519
00:19:14,240 --> 00:19:16,320
could be posed by an ai system so that

520
00:19:16,320 --> 00:19:18,160
that needs to be sort of

521
00:19:18,160 --> 00:19:19,840
loosened up so that it can become an

522
00:19:19,840 --> 00:19:21,200
effective

523
00:19:21,200 --> 00:19:23,039
instrument um

524
00:19:23,039 --> 00:19:27,120
then i mean in terms of uh

525
00:19:27,120 --> 00:19:29,280
biometric categorization and emotion

526
00:19:29,280 --> 00:19:30,480
recognition

527
00:19:30,480 --> 00:19:33,039
um i think isa's point about definitions

528
00:19:33,039 --> 00:19:33,840
is

529
00:19:33,840 --> 00:19:35,760
really important uh we've pointed out

530
00:19:35,760 --> 00:19:37,679
problems with the current definitions

531
00:19:37,679 --> 00:19:39,679
that are in there since you know the

532
00:19:39,679 --> 00:19:40,640
initial

533
00:19:40,640 --> 00:19:42,880
launch of the act both the definition of

534
00:19:42,880 --> 00:19:44,640
biometric categorization and the motion

535
00:19:44,640 --> 00:19:46,799
recognition are tied

536
00:19:46,799 --> 00:19:47,840
to

537
00:19:47,840 --> 00:19:50,400
being based on biometric data which is a

538
00:19:50,400 --> 00:19:52,559
problem because biometric data has you

539
00:19:52,559 --> 00:19:55,039
know two aspects to its definition it's

540
00:19:55,039 --> 00:19:56,960
uh data relating to the physical

541
00:19:56,960 --> 00:19:58,240
physiological or behavioral

542
00:19:58,240 --> 00:20:00,559
characteristics of a natural person and

543
00:20:00,559 --> 00:20:03,120
it has to enable unique identification

544
00:20:03,120 --> 00:20:05,360
now you can do biometric categorization

545
00:20:05,360 --> 00:20:08,240
and emotion recognition using physical

546
00:20:08,240 --> 00:20:09,919
physiological or behavioral data that

547
00:20:09,919 --> 00:20:12,799
does not meet the bar for identification

548
00:20:12,799 --> 00:20:14,640
you know you have crazy systems out

549
00:20:14,640 --> 00:20:17,280
there like gender recognition using nose

550
00:20:17,280 --> 00:20:18,559
sweat

551
00:20:18,559 --> 00:20:20,320
and i don't know i don't not sure if

552
00:20:20,320 --> 00:20:23,280
nose swash meets the the bar for unique

553
00:20:23,280 --> 00:20:25,120
identification but also things like

554
00:20:25,120 --> 00:20:27,440
heartbeat galvanic skin response so the

555
00:20:27,440 --> 00:20:30,799
definitions need to be fixed um

556
00:20:30,799 --> 00:20:33,520
for sure but then we need to have a

557
00:20:33,520 --> 00:20:35,520
clear conversation about

558
00:20:35,520 --> 00:20:37,039
how we need to ban them with emotion

559
00:20:37,039 --> 00:20:38,559
recognition i think that's complex

560
00:20:38,559 --> 00:20:40,400
because the way emotion recognition as a

561
00:20:40,400 --> 00:20:42,720
term is used both by industry and civil

562
00:20:42,720 --> 00:20:45,360
society is extremely broad it covers

563
00:20:45,360 --> 00:20:48,080
things like polygraphs you know the

564
00:20:48,080 --> 00:20:49,840
straightforward systems that kind of

565
00:20:49,840 --> 00:20:52,320
categorize facial expressions into

566
00:20:52,320 --> 00:20:54,559
discrete emotional categories

567
00:20:54,559 --> 00:20:56,480
of seven emotions based on like paul

568
00:20:56,480 --> 00:20:58,720
ekman's theory but then when i mentioned

569
00:20:58,720 --> 00:21:00,159
that we're advocating for a ban on

570
00:21:00,159 --> 00:21:01,679
emotion recognition someone brought up

571
00:21:01,679 --> 00:21:04,640
the example of a system that identifies

572
00:21:04,640 --> 00:21:06,720
if callers to emergency services could

573
00:21:06,720 --> 00:21:08,720
be having a heart attack that's not a

574
00:21:08,720 --> 00:21:10,640
motion recognition

575
00:21:10,640 --> 00:21:14,080
you know there's no inference there from

576
00:21:14,080 --> 00:21:16,080
biometric data to some kind of emotional

577
00:21:16,080 --> 00:21:18,799
state that's just detecting uh stressors

578
00:21:18,799 --> 00:21:20,320
so we need to be careful about the

579
00:21:20,320 --> 00:21:23,679
definition uh to be effective in in

580
00:21:23,679 --> 00:21:25,919
really advocating for a bun

581
00:21:25,919 --> 00:21:27,360
um

582
00:21:27,360 --> 00:21:29,600
i think uh just

583
00:21:29,600 --> 00:21:31,600
to finish um i'll see i've got about a

584
00:21:31,600 --> 00:21:33,520
minute left i do want to come to to

585
00:21:33,520 --> 00:21:34,799
sarah's point

586
00:21:34,799 --> 00:21:37,840
um that she raised in the chat about

587
00:21:37,840 --> 00:21:39,679
you know how can we successfully

588
00:21:39,679 --> 00:21:41,600
mobilize against red lines that are

589
00:21:41,600 --> 00:21:43,919
primarily targeted as relatively

590
00:21:43,919 --> 00:21:46,000
underrepresented groups in our community

591
00:21:46,000 --> 00:21:48,080
i think this is a big issue

592
00:21:48,080 --> 00:21:48,960
um

593
00:21:48,960 --> 00:21:50,159
you know

594
00:21:50,159 --> 00:21:52,960
the stage we're at in the negotiations

595
00:21:52,960 --> 00:21:55,440
we sort of depend on

596
00:21:55,440 --> 00:21:58,159
getting popular supports getting buy-in

597
00:21:58,159 --> 00:22:00,080
to the arguments that we're making about

598
00:22:00,080 --> 00:22:02,000
certain red lines and unfortunately that

599
00:22:02,000 --> 00:22:03,840
kind of depends on

600
00:22:03,840 --> 00:22:05,840
you know having some sort of a hook that

601
00:22:05,840 --> 00:22:08,640
gets people interested and

602
00:22:08,640 --> 00:22:11,039
there is as sarah's question points out

603
00:22:11,039 --> 00:22:13,200
an inherent conflict that if you know

604
00:22:13,200 --> 00:22:16,880
the majority of meps are kind of from

605
00:22:16,880 --> 00:22:18,960
middle-class white backgrounds and not

606
00:22:18,960 --> 00:22:20,720
affected by these type of systems maybe

607
00:22:20,720 --> 00:22:22,720
they're not going to get particularly

608
00:22:22,720 --> 00:22:25,440
engaged um by these and the same applies

609
00:22:25,440 --> 00:22:27,679
to activists as well if you know you're

610
00:22:27,679 --> 00:22:29,200
coming from a background like mine you

611
00:22:29,200 --> 00:22:30,480
know then

612
00:22:30,480 --> 00:22:33,200
if only the concerns related to to those

613
00:22:33,200 --> 00:22:35,840
profiles are raised that's an extreme

614
00:22:35,840 --> 00:22:36,640
issue

615
00:22:36,640 --> 00:22:38,960
um how to address that i think

616
00:22:38,960 --> 00:22:41,039
again i would point to i think sarah who

617
00:22:41,039 --> 00:22:42,640
asked the question is doing incredibly

618
00:22:42,640 --> 00:22:45,440
good work on bringing in uh

619
00:22:45,440 --> 00:22:47,280
organizations from outside the typical

620
00:22:47,280 --> 00:22:48,799
digital rights community building

621
00:22:48,799 --> 00:22:51,200
bridges between different civil society

622
00:22:51,200 --> 00:22:54,480
organizations um and really the really

623
00:22:54,480 --> 00:22:56,960
important work of coordination so that

624
00:22:56,960 --> 00:22:59,760
uh what might seem like marginal issues

625
00:22:59,760 --> 00:23:02,640
uh get placed in the center

626
00:23:02,640 --> 00:23:04,080
and i think you know there's really

627
00:23:04,080 --> 00:23:06,320
important work going on there but it's

628
00:23:06,320 --> 00:23:08,640
it's certainly a challenge and yeah i

629
00:23:08,640 --> 00:23:11,600
hope that we can ensure this you know

630
00:23:11,600 --> 00:23:13,919
all of these violations and again it's

631
00:23:13,919 --> 00:23:15,919
it's typically if you look at biometric

632
00:23:15,919 --> 00:23:18,320
categorization as an example

633
00:23:18,320 --> 00:23:20,799
um i am probably not that likely to to

634
00:23:20,799 --> 00:23:22,880
fall victim to the harms of biometric

635
00:23:22,880 --> 00:23:24,880
categorization it's going to be people

636
00:23:24,880 --> 00:23:26,720
who don't fit a gender binary it's going

637
00:23:26,720 --> 00:23:28,880
to be uh people who could be racially

638
00:23:28,880 --> 00:23:31,120
profiled by a system like that so we

639
00:23:31,120 --> 00:23:32,640
really need to

640
00:23:32,640 --> 00:23:35,360
ensure that yeah we do all that we can

641
00:23:35,360 --> 00:23:38,799
to get these systems created

642
00:23:40,080 --> 00:23:42,400
thank you daniel there's so much to to

643
00:23:42,400 --> 00:23:44,480
think about and also so much to you kind

644
00:23:44,480 --> 00:23:47,679
of revisit uh periodically because we

645
00:23:47,679 --> 00:23:49,440
as you said right there's so much that

646
00:23:49,440 --> 00:23:51,919
we need to build bridges um

647
00:23:51,919 --> 00:23:53,679
for and like think think really

648
00:23:53,679 --> 00:23:56,000
critically about you know who do we need

649
00:23:56,000 --> 00:23:57,919
in the room and why and what are the

650
00:23:57,919 --> 00:23:59,679
what are the limitations within our own

651
00:23:59,679 --> 00:24:01,679
communities um

652
00:24:01,679 --> 00:24:03,760
and now i'd like to turn to lana which

653
00:24:03,760 --> 00:24:06,159
actually this is a great segue because

654
00:24:06,159 --> 00:24:07,200
um

655
00:24:07,200 --> 00:24:09,760
lana in our work where we're looking at

656
00:24:09,760 --> 00:24:12,240
you know red lines for biometric systems

657
00:24:12,240 --> 00:24:14,320
um we've been talking to a lot of people

658
00:24:14,320 --> 00:24:15,840
from across the world we're looking at

659
00:24:15,840 --> 00:24:18,559
different jurisdictions all the way from

660
00:24:18,559 --> 00:24:19,360
you know

661
00:24:19,360 --> 00:24:21,600
argentina and brazil to india to of

662
00:24:21,600 --> 00:24:24,640
course the eu and the us and um i guess

663
00:24:24,640 --> 00:24:26,400
at this point it's also helpful to think

664
00:24:26,400 --> 00:24:28,480
about you know what are the challenges

665
00:24:28,480 --> 00:24:30,320
that we've seen across the board when it

666
00:24:30,320 --> 00:24:31,440
comes to

667
00:24:31,440 --> 00:24:33,520
getting these deadlines in place

668
00:24:33,520 --> 00:24:36,559
what can we learn from those experiences

669
00:24:36,559 --> 00:24:38,480
that could be helpful in the context of

670
00:24:38,480 --> 00:24:41,360
the euai act and do you see there being

671
00:24:41,360 --> 00:24:43,520
kind of a push and pull

672
00:24:43,520 --> 00:24:45,360
when it comes to securing red lines in

673
00:24:45,360 --> 00:24:47,679
one place um as that being kind of like

674
00:24:47,679 --> 00:24:49,520
an encouragement to to have it in more

675
00:24:49,520 --> 00:24:51,360
places especially when we think about

676
00:24:51,360 --> 00:24:54,559
the companies that tend to you know um

677
00:24:54,559 --> 00:24:56,000
provide this technology across

678
00:24:56,000 --> 00:24:58,559
jurisdictions

679
00:24:58,559 --> 00:25:02,000
yeah um thanks fideshi um and as you say

680
00:25:02,000 --> 00:25:04,720
this is our joint work so um please jump

681
00:25:04,720 --> 00:25:06,320
in um

682
00:25:06,320 --> 00:25:08,720
in um as i'm talking but

683
00:25:08,720 --> 00:25:10,960
um i think that when we've been speaking

684
00:25:10,960 --> 00:25:13,279
to people um in different jurisdictions

685
00:25:13,279 --> 00:25:15,679
it's been really fascinating and i think

686
00:25:15,679 --> 00:25:18,799
we've learned a lot about the challenges

687
00:25:18,799 --> 00:25:21,600
and the strategies for securing red

688
00:25:21,600 --> 00:25:24,080
lines as well as the obstacles

689
00:25:24,080 --> 00:25:26,240
um i think sort of the first takeaway

690
00:25:26,240 --> 00:25:29,039
that we've had um really

691
00:25:29,039 --> 00:25:31,679
aligns with isil when she was talking

692
00:25:31,679 --> 00:25:33,600
about the information

693
00:25:33,600 --> 00:25:34,880
asymmetry

694
00:25:34,880 --> 00:25:37,440
um and i think that what we find

695
00:25:37,440 --> 00:25:40,720
in so many places is that it's really

696
00:25:40,720 --> 00:25:43,120
difficult to talk about red lines

697
00:25:43,120 --> 00:25:46,080
without knowing if where and how

698
00:25:46,080 --> 00:25:48,000
biometric technologies are actually

699
00:25:48,000 --> 00:25:50,960
being used and that seems to be a major

700
00:25:50,960 --> 00:25:52,880
continuing project

701
00:25:52,880 --> 00:25:55,279
for actors i'm across a whole range of

702
00:25:55,279 --> 00:25:57,919
different jurisdictions

703
00:25:57,919 --> 00:26:00,159
and that is particularly when we're

704
00:26:00,159 --> 00:26:02,640
thinking outside of the law enforcement

705
00:26:02,640 --> 00:26:04,799
and facial recognition technology

706
00:26:04,799 --> 00:26:06,720
context

707
00:26:06,720 --> 00:26:08,000
so i think that that's a really

708
00:26:08,000 --> 00:26:09,440
important take when i think that it's

709
00:26:09,440 --> 00:26:11,279
sort of on dovetails with what daniel

710
00:26:11,279 --> 00:26:13,200
was talking about in terms of what are

711
00:26:13,200 --> 00:26:15,919
we prohibiting um that there can be a

712
00:26:15,919 --> 00:26:18,640
risk that we really focus on what we

713
00:26:18,640 --> 00:26:21,440
know is happening but there's so little

714
00:26:21,440 --> 00:26:23,440
transparency around what's happening

715
00:26:23,440 --> 00:26:25,520
with biometric technologies that we

716
00:26:25,520 --> 00:26:28,400
might just go for red lines that are too

717
00:26:28,400 --> 00:26:30,320
narrow and that may affect us down the

718
00:26:30,320 --> 00:26:32,799
line particularly when we're talking

719
00:26:32,799 --> 00:26:35,039
about prohibitions that are quite um

720
00:26:35,039 --> 00:26:36,799
narrowly set

721
00:26:36,799 --> 00:26:37,760
um

722
00:26:37,760 --> 00:26:39,440
i think that when we think about this

723
00:26:39,440 --> 00:26:42,080
from sort of the eu ai act perspective

724
00:26:42,080 --> 00:26:44,640
what's really important there is also

725
00:26:44,640 --> 00:26:46,720
it's not just the technology and the

726
00:26:46,720 --> 00:26:48,559
sector that it's being used and which

727
00:26:48,559 --> 00:26:50,080
technology

728
00:26:50,080 --> 00:26:52,640
but there can also um

729
00:26:52,640 --> 00:26:55,679
be a risk that we think about um

730
00:26:55,679 --> 00:26:57,840
technologies that are being used in our

731
00:26:57,840 --> 00:27:00,000
own jurisdictions

732
00:27:00,000 --> 00:27:03,120
by global actors and we don't think

733
00:27:03,120 --> 00:27:06,080
about the transparency around how are

734
00:27:06,080 --> 00:27:09,440
those same global actors sharing selling

735
00:27:09,440 --> 00:27:12,000
offering the same technologies

736
00:27:12,000 --> 00:27:14,240
in in other jurisdictions so that we

737
00:27:14,240 --> 00:27:17,039
think about how do we secure red lines

738
00:27:17,039 --> 00:27:20,159
here but not red lines um throughout the

739
00:27:20,159 --> 00:27:22,320
whole global supply chain and the way in

740
00:27:22,320 --> 00:27:24,080
which these actors are maybe using them

741
00:27:24,080 --> 00:27:25,760
in different ways and i think that

742
00:27:25,760 --> 00:27:27,440
that's sort of a theme that comes out

743
00:27:27,440 --> 00:27:30,159
where we see certain actors committing

744
00:27:30,159 --> 00:27:32,880
to red lines in particular jurisdictions

745
00:27:32,880 --> 00:27:35,039
committing to moratoriums committing to

746
00:27:35,039 --> 00:27:36,000
bans

747
00:27:36,000 --> 00:27:37,600
but we don't know if those same

748
00:27:37,600 --> 00:27:40,559
practices are happening um elsewhere

749
00:27:40,559 --> 00:27:42,559
so that transparency piece is so

750
00:27:42,559 --> 00:27:45,360
multi-layered and really um affects how

751
00:27:45,360 --> 00:27:46,840
we think about red

752
00:27:46,840 --> 00:27:49,039
lines um

753
00:27:49,039 --> 00:27:51,360
i think the other thing that we

754
00:27:51,360 --> 00:27:53,840
are really i'm suffering from which has

755
00:27:53,840 --> 00:27:55,760
already come up in the conversation but

756
00:27:55,760 --> 00:27:58,240
many people have have raised it

757
00:27:58,240 --> 00:28:00,480
is that it's not easy to get political

758
00:28:00,480 --> 00:28:02,960
commitment to red lines regulation

759
00:28:02,960 --> 00:28:04,559
particularly when we're talking about a

760
00:28:04,559 --> 00:28:07,520
ban as opposed to a moratorium

761
00:28:07,520 --> 00:28:08,399
um

762
00:28:08,399 --> 00:28:10,320
when you don't have these stories about

763
00:28:10,320 --> 00:28:12,080
how they're being used so you lack these

764
00:28:12,080 --> 00:28:14,960
sort of personal stories um about what's

765
00:28:14,960 --> 00:28:17,600
happening um and then that makes it

766
00:28:17,600 --> 00:28:19,200
really difficult to actually get

767
00:28:19,200 --> 00:28:21,600
momentum for um

768
00:28:21,600 --> 00:28:23,919
for um

769
00:28:23,919 --> 00:28:26,080
achieving red lines regulations so i

770
00:28:26,080 --> 00:28:28,080
think that we find in many contexts that

771
00:28:28,080 --> 00:28:29,520
there's a lot of

772
00:28:29,520 --> 00:28:32,159
public and political and legal obstacles

773
00:28:32,159 --> 00:28:34,159
to securing red lines

774
00:28:34,159 --> 00:28:37,520
but what has come up is that if there

775
00:28:37,520 --> 00:28:38,799
are

776
00:28:38,799 --> 00:28:41,600
if there's momentum for regulation on

777
00:28:41,600 --> 00:28:44,000
red lines elsewhere

778
00:28:44,000 --> 00:28:46,640
and obviously we've seen that in the us

779
00:28:46,640 --> 00:28:48,480
with the movements on moratoriums and

780
00:28:48,480 --> 00:28:50,640
bands on facial recognition technology

781
00:28:50,640 --> 00:28:53,520
as well as now within the euai act

782
00:28:53,520 --> 00:28:55,600
where there is actually space and

783
00:28:55,600 --> 00:28:59,120
opening um for red line regulation

784
00:28:59,120 --> 00:29:01,760
elsewhere that that can provide a lever

785
00:29:01,760 --> 00:29:04,240
or an opening for actors to really

786
00:29:04,240 --> 00:29:06,960
advocate for um the adoption of red

787
00:29:06,960 --> 00:29:08,880
lines so where you may within your

788
00:29:08,880 --> 00:29:10,480
jurisdictions finding it really

789
00:29:10,480 --> 00:29:12,159
difficult to see where is there a space

790
00:29:12,159 --> 00:29:13,200
for this

791
00:29:13,200 --> 00:29:15,840
if you see elsewhere

792
00:29:15,840 --> 00:29:17,919
and you can point to it particularly if

793
00:29:17,919 --> 00:29:20,240
it's regional or international

794
00:29:20,240 --> 00:29:21,840
if you can point to this is what's

795
00:29:21,840 --> 00:29:24,880
happening elsewhere then it may help you

796
00:29:24,880 --> 00:29:26,799
with momentum in your own jurisdiction

797
00:29:26,799 --> 00:29:28,480
particularly as we see

798
00:29:28,480 --> 00:29:30,799
pretty much all jurisdictions are having

799
00:29:30,799 --> 00:29:33,200
some kind of conversation about what ai

800
00:29:33,200 --> 00:29:35,120
regulation looks like

801
00:29:35,120 --> 00:29:37,360
whether it is still just in the ethical

802
00:29:37,360 --> 00:29:39,919
regulation conversation or whether we're

803
00:29:39,919 --> 00:29:41,840
getting more to human rights compliant

804
00:29:41,840 --> 00:29:43,840
conversation about regulation of ai

805
00:29:43,840 --> 00:29:45,919
there's some kind of conversation going

806
00:29:45,919 --> 00:29:48,320
on and if you can point to well this is

807
00:29:48,320 --> 00:29:50,240
what the international best practice is

808
00:29:50,240 --> 00:29:52,640
looking like it may provide some

809
00:29:52,640 --> 00:29:54,799
momentum it may provide some advocacy

810
00:29:54,799 --> 00:29:56,480
tools

811
00:29:56,480 --> 00:29:58,480
whether that's at the political level

812
00:29:58,480 --> 00:30:00,480
parliamentary level or whether it's in

813
00:30:00,480 --> 00:30:02,000
courts there may be some kind of

814
00:30:02,000 --> 00:30:04,960
momentum there

815
00:30:04,960 --> 00:30:07,200
it may also provide momentum for other

816
00:30:07,200 --> 00:30:08,399
actors

817
00:30:08,399 --> 00:30:11,120
for example talking to investors

818
00:30:11,120 --> 00:30:13,440
within major companies to say well look

819
00:30:13,440 --> 00:30:15,279
it's not good enough if you if you're

820
00:30:15,279 --> 00:30:16,960
regulated

821
00:30:16,960 --> 00:30:19,520
in one jurisdiction or you've committed

822
00:30:19,520 --> 00:30:21,279
to red lines in one jurisdiction this

823
00:30:21,279 --> 00:30:22,399
needs to be throughout your

824
00:30:22,399 --> 00:30:24,000
international practice and you can't be

825
00:30:24,000 --> 00:30:26,159
testing technology in one place while

826
00:30:26,159 --> 00:30:27,600
committing to

827
00:30:27,600 --> 00:30:30,720
a banner or moratorium in another place

828
00:30:30,720 --> 00:30:32,480
um but i think it's also important for

829
00:30:32,480 --> 00:30:34,240
us to think about what the extra

830
00:30:34,240 --> 00:30:37,120
territorial effects of the euai act may

831
00:30:37,120 --> 00:30:38,159
have

832
00:30:38,159 --> 00:30:40,320
for actors that are regulated with it

833
00:30:40,320 --> 00:30:42,080
and in the way that we have that under

834
00:30:42,080 --> 00:30:44,720
the gdpr so there's lots of ways in

835
00:30:44,720 --> 00:30:46,320
which we need to think about what are

836
00:30:46,320 --> 00:30:48,240
the effects

837
00:30:48,240 --> 00:30:52,480
of the euai act and red lines outside of

838
00:30:52,480 --> 00:30:53,760
the eu

839
00:30:53,760 --> 00:30:55,520
and i think that that really then comes

840
00:30:55,520 --> 00:30:57,919
um to daniel's last point and maybe just

841
00:30:57,919 --> 00:31:00,720
to close um in case magician you also

842
00:31:00,720 --> 00:31:04,399
want to um to elaborate on points but

843
00:31:04,399 --> 00:31:05,279
um

844
00:31:05,279 --> 00:31:08,399
what is contained within the eu ai act

845
00:31:08,399 --> 00:31:11,279
is sort of one of the first pieces um of

846
00:31:11,279 --> 00:31:12,559
regulation

847
00:31:12,559 --> 00:31:15,840
that will have red lines in it is really

848
00:31:15,840 --> 00:31:19,279
critical for what the rest of redline

849
00:31:19,279 --> 00:31:21,440
and ai regulation looks like i mean

850
00:31:21,440 --> 00:31:23,039
we've already seen this in relation to

851
00:31:23,039 --> 00:31:24,720
things like gdpr

852
00:31:24,720 --> 00:31:26,559
that what is contained here we have to

853
00:31:26,559 --> 00:31:28,880
be careful that the bar is not set so

854
00:31:28,880 --> 00:31:30,960
low that it becomes the model

855
00:31:30,960 --> 00:31:32,480
for other places

856
00:31:32,480 --> 00:31:35,200
and so if we are worried about not only

857
00:31:35,200 --> 00:31:37,760
what is contained within article 5 but

858
00:31:37,760 --> 00:31:38,880
also

859
00:31:38,880 --> 00:31:41,120
the idea that it can't be reevaluated

860
00:31:41,120 --> 00:31:42,399
and grow

861
00:31:42,399 --> 00:31:44,559
but we're also still talking about very

862
00:31:44,559 --> 00:31:46,640
narrow contexts in which biometric

863
00:31:46,640 --> 00:31:48,799
technologies are used we're at a real

864
00:31:48,799 --> 00:31:51,600
risk that we achieve red lines but it

865
00:31:51,600 --> 00:31:53,600
sort of becomes frozen in time and then

866
00:31:53,600 --> 00:31:55,919
as the information asymmetry is

867
00:31:55,919 --> 00:31:57,840
addressed and we start to understand

868
00:31:57,840 --> 00:32:00,640
much more about the impact of these

869
00:32:00,640 --> 00:32:02,799
technologies and as these technologies

870
00:32:02,799 --> 00:32:04,880
evolve that we're kind of stuck with

871
00:32:04,880 --> 00:32:07,840
trying with a model that is is not fit

872
00:32:07,840 --> 00:32:09,760
for purpose so i think that we have to

873
00:32:09,760 --> 00:32:12,240
think in many levels about how we

874
00:32:12,240 --> 00:32:15,120
address red lines within regulation and

875
00:32:15,120 --> 00:32:19,360
how it affects other jurisdictions

876
00:32:20,559 --> 00:32:22,640
yeah thank you thank you so much lana i

877
00:32:22,640 --> 00:32:23,760
think the

878
00:32:23,760 --> 00:32:26,559
the question of how do we future proof

879
00:32:26,559 --> 00:32:29,519
even perfectly you know in 2022

880
00:32:29,519 --> 00:32:33,200
perfectly um stated kind of prohibitions

881
00:32:33,200 --> 00:32:35,039
is a big question that daniel also

882
00:32:35,039 --> 00:32:37,279
brought up um the only thing i'll add is

883
00:32:37,279 --> 00:32:39,519
that um you know in our research on

884
00:32:39,519 --> 00:32:41,440
emotional recognition in china we found

885
00:32:41,440 --> 00:32:44,480
that some of the most horrific uses

886
00:32:44,480 --> 00:32:46,799
of emotional recognition in xinjiang

887
00:32:46,799 --> 00:32:49,840
according to an amnesty international

888
00:32:49,840 --> 00:32:52,240
investigation was actually supplied by

889
00:32:52,240 --> 00:32:54,480
french and dutch companies right

890
00:32:54,480 --> 00:32:57,200
and so we're not we we kind of like miss

891
00:32:57,200 --> 00:32:59,039
the the really important question of

892
00:32:59,039 --> 00:33:01,279
like what is the supply chain of these

893
00:33:01,279 --> 00:33:03,200
technologies and what does you know

894
00:33:03,200 --> 00:33:04,799
import and export of these technologies

895
00:33:04,799 --> 00:33:06,880
mean as opposed to just thinking even

896
00:33:06,880 --> 00:33:09,120
about design development and deployment

897
00:33:09,120 --> 00:33:11,120
right we got to think of it as like a

898
00:33:11,120 --> 00:33:12,880
consumer protection issue as a trade

899
00:33:12,880 --> 00:33:15,120
issue as a as a data protection issue

900
00:33:15,120 --> 00:33:17,200
and of course as a as a human rights

901
00:33:17,200 --> 00:33:18,000
issue

902
00:33:18,000 --> 00:33:19,840
um so we're almost i mean actually we're

903
00:33:19,840 --> 00:33:22,240
exactly at the halfway point um and at

904
00:33:22,240 --> 00:33:24,320
this point i want to go back to our

905
00:33:24,320 --> 00:33:26,559
panelists and kind of you know having

906
00:33:26,559 --> 00:33:28,399
reflected on what other challenges that

907
00:33:28,399 --> 00:33:31,039
we have and why they're so um important

908
00:33:31,039 --> 00:33:34,240
and urgent to kind of address i want to

909
00:33:34,240 --> 00:33:36,000
you know kind of reorient the questions

910
00:33:36,000 --> 00:33:38,640
to what we can do

911
00:33:38,640 --> 00:33:40,159
going forward

912
00:33:40,159 --> 00:33:42,000
and think about what useful and and

913
00:33:42,000 --> 00:33:44,240
constructive civil society work could

914
00:33:44,240 --> 00:33:45,679
look like so

915
00:33:45,679 --> 00:33:48,880
issa if i could turn to you now um

916
00:33:48,880 --> 00:33:51,120
can i ask for you to share

917
00:33:51,120 --> 00:33:53,039
what you think are some of the arguments

918
00:33:53,039 --> 00:33:55,360
that could work or have worked

919
00:33:55,360 --> 00:33:56,960
particularly well in the past when it

920
00:33:56,960 --> 00:33:59,519
comes to securing red lines or actually

921
00:33:59,519 --> 00:34:01,919
just you know even more broadly um

922
00:34:01,919 --> 00:34:04,000
championing human rights um in the

923
00:34:04,000 --> 00:34:07,360
context of eu policymaking

924
00:34:07,360 --> 00:34:09,760
uh yes well uh

925
00:34:09,760 --> 00:34:12,879
the the short answer to that is is still

926
00:34:12,879 --> 00:34:14,719
a work in progress so we need to see

927
00:34:14,719 --> 00:34:17,918
where we end up before you know um

928
00:34:17,918 --> 00:34:20,000
saying this has been effective or this

929
00:34:20,000 --> 00:34:22,560
has not been effective i do believe that

930
00:34:22,560 --> 00:34:24,800
there's been there is an increasing

931
00:34:24,800 --> 00:34:26,719
increasing uh

932
00:34:26,719 --> 00:34:29,440
sensitivity to the fact that

933
00:34:29,440 --> 00:34:30,719
we

934
00:34:30,719 --> 00:34:33,280
we winning the european union as we said

935
00:34:33,280 --> 00:34:35,040
it could be a standard sector

936
00:34:35,040 --> 00:34:37,679
and i think there is also increasing

937
00:34:37,679 --> 00:34:40,000
attention to this fact that we can't use

938
00:34:40,000 --> 00:34:41,599
two different

939
00:34:41,599 --> 00:34:43,839
uh we can't set to different frameworks

940
00:34:43,839 --> 00:34:45,119
when it comes to

941
00:34:45,119 --> 00:34:47,199
european citizens within the eu and

942
00:34:47,199 --> 00:34:49,839
everywhere everywhere else uh so the

943
00:34:49,839 --> 00:34:50,879
number of developments that are

944
00:34:50,879 --> 00:34:52,719
happening at the eu border for example

945
00:34:52,719 --> 00:34:53,918
they're they're getting some some

946
00:34:53,918 --> 00:34:55,119
attention especially from the civil

947
00:34:55,119 --> 00:34:57,280
society angle but also you know they're

948
00:34:57,280 --> 00:34:59,359
getting way more often into the

949
00:34:59,359 --> 00:35:01,599
headlines of newspapers and so on and so

950
00:35:01,599 --> 00:35:03,280
forth so i think there we are sort of

951
00:35:03,280 --> 00:35:05,680
evolving in complexity when it comes to

952
00:35:05,680 --> 00:35:06,480
this

953
00:35:06,480 --> 00:35:08,880
uh this perspective and i and i strongly

954
00:35:08,880 --> 00:35:11,839
believe it's the way to go and we have a

955
00:35:11,839 --> 00:35:13,760
variety of uh

956
00:35:13,760 --> 00:35:16,079
examples of

957
00:35:16,079 --> 00:35:18,160
using different populations in different

958
00:35:18,160 --> 00:35:20,000
areas where for example the data

959
00:35:20,000 --> 00:35:22,960
protection regime is extremely

960
00:35:22,960 --> 00:35:26,800
uh weak or not it is even unexistent

961
00:35:26,800 --> 00:35:28,480
as a training

962
00:35:28,480 --> 00:35:30,800
setting for this the development of

963
00:35:30,800 --> 00:35:32,320
these technologies in a complete

964
00:35:32,320 --> 00:35:34,720
darkness and this has to stop of course

965
00:35:34,720 --> 00:35:37,200
so i think there we can also you know

966
00:35:37,200 --> 00:35:38,320
try to be

967
00:35:38,320 --> 00:35:39,520
i think we can be a little bit more

968
00:35:39,520 --> 00:35:41,280
optimistic and this goes back also to

969
00:35:41,280 --> 00:35:43,440
this idea of trying to combine different

970
00:35:43,440 --> 00:35:44,480
communities

971
00:35:44,480 --> 00:35:46,880
so this is for this exactly for this

972
00:35:46,880 --> 00:35:48,720
reason it's not a struggle of wide

973
00:35:48,720 --> 00:35:51,200
middle class people it is something that

974
00:35:51,200 --> 00:35:54,480
that concerns of everyone and every

975
00:35:54,480 --> 00:35:57,200
every community and in in a certain way

976
00:35:57,200 --> 00:35:58,960
this is something we have already uh

977
00:35:58,960 --> 00:36:01,599
tackled in a number of fights uh

978
00:36:01,599 --> 00:36:03,760
when it comes to ai systems when it

979
00:36:03,760 --> 00:36:06,720
comes to algorithms uh it is true that

980
00:36:06,720 --> 00:36:08,400
certain communities and certain people

981
00:36:08,400 --> 00:36:10,839
they will have us they will suffer a

982
00:36:10,839 --> 00:36:13,839
stronger negative impact

983
00:36:13,839 --> 00:36:14,800
so

984
00:36:14,800 --> 00:36:16,560
but i don't think we can we can

985
00:36:16,560 --> 00:36:18,960
reasonably say that any of us is not

986
00:36:18,960 --> 00:36:20,560
exposed to the risk whatsoever

987
00:36:20,560 --> 00:36:22,320
especially because the majority of times

988
00:36:22,320 --> 00:36:24,000
what we're talking we're asking for a

989
00:36:24,000 --> 00:36:27,200
ban we're asking at least in in the ais

990
00:36:27,200 --> 00:36:29,280
i think uh one of the main main causes

991
00:36:29,280 --> 00:36:31,520
is a band for mass surveillance so there

992
00:36:31,520 --> 00:36:33,920
is very much the collective um

993
00:36:33,920 --> 00:36:36,160
generalized uh

994
00:36:36,160 --> 00:36:39,359
impact that we're looking at not not

995
00:36:39,359 --> 00:36:42,079
specific individual or specific category

996
00:36:42,079 --> 00:36:44,480
i think this idea of having trying to uh

997
00:36:44,480 --> 00:36:46,320
as main point

998
00:36:46,320 --> 00:36:49,280
to try to obtain a future proof

999
00:36:49,280 --> 00:36:51,040
regulatory framework is going to be

1000
00:36:51,040 --> 00:36:53,280
essential and as uh daniel and lorna

1001
00:36:53,280 --> 00:36:56,000
they already uh

1002
00:36:56,000 --> 00:36:56,960
flagged

1003
00:36:56,960 --> 00:36:58,079
this

1004
00:36:58,079 --> 00:36:59,920
implies a number of things and we're

1005
00:36:59,920 --> 00:37:02,160
trying to work on on on a few of those

1006
00:37:02,160 --> 00:37:04,320
so it implies to have proper definitions

1007
00:37:04,320 --> 00:37:06,880
it requires to have criteria for

1008
00:37:06,880 --> 00:37:08,720
identifying what's what's

1009
00:37:08,720 --> 00:37:11,680
accessible and and what it's not um and

1010
00:37:11,680 --> 00:37:14,400
i think there it could be a successful

1011
00:37:14,400 --> 00:37:18,400
strategy if we sort of try to use it in

1012
00:37:18,400 --> 00:37:20,160
in a narrative that focuses on the

1013
00:37:20,160 --> 00:37:22,480
alignment of incentives rather than on

1014
00:37:22,480 --> 00:37:26,560
the conflicts um so what i mean is

1015
00:37:26,560 --> 00:37:29,280
also for developers uh

1016
00:37:29,280 --> 00:37:31,599
it's it's it's going legal legal

1017
00:37:31,599 --> 00:37:33,520
certainty certainly about you know what

1018
00:37:33,520 --> 00:37:35,599
are the criteria and how this is going

1019
00:37:35,599 --> 00:37:37,359
to work in the next years is going to

1020
00:37:37,359 --> 00:37:39,599
create some advantages for their

1021
00:37:39,599 --> 00:37:42,880
business cases um so

1022
00:37:42,880 --> 00:37:46,160
maybe what we could do we could try to

1023
00:37:46,160 --> 00:37:48,400
highlight those those

1024
00:37:48,400 --> 00:37:50,160
align those little areas where we have

1025
00:37:50,160 --> 00:37:52,079
an alignment of incentive is entitled

1026
00:37:52,079 --> 00:37:53,440
you know to be a little bit stronger in

1027
00:37:53,440 --> 00:37:54,560
our calls

1028
00:37:54,560 --> 00:37:56,720
um another

1029
00:37:56,720 --> 00:37:57,680
another

1030
00:37:57,680 --> 00:37:59,760
area where i sort of

1031
00:37:59,760 --> 00:38:00,720
uh

1032
00:38:00,720 --> 00:38:04,480
i see i think i see alignment of of

1033
00:38:04,480 --> 00:38:05,920
approaches is when it comes to

1034
00:38:05,920 --> 00:38:08,079
transparency i think we are already you

1035
00:38:08,079 --> 00:38:10,160
know well advanced on that and

1036
00:38:10,160 --> 00:38:13,200
transparency is called for by a number

1037
00:38:13,200 --> 00:38:14,720
of stakeholders

1038
00:38:14,720 --> 00:38:17,200
now the major struggle there is going to

1039
00:38:17,200 --> 00:38:19,760
be which kind of transparency is

1040
00:38:19,760 --> 00:38:22,000
sufficient and adequate transparency so

1041
00:38:22,000 --> 00:38:25,359
what we have in article 6 16 the ai

1042
00:38:25,359 --> 00:38:28,400
ai act can be strongly improved i think

1043
00:38:28,400 --> 00:38:30,400
and we're doing some work on that as

1044
00:38:30,400 --> 00:38:32,160
well but i also think that if we look at

1045
00:38:32,160 --> 00:38:33,839
a broader picture transparency is not

1046
00:38:33,839 --> 00:38:36,720
something that is needed only in the ai

1047
00:38:36,720 --> 00:38:39,200
act in europe this is something that is

1048
00:38:39,200 --> 00:38:40,800
called for in a number of regulatory

1049
00:38:40,800 --> 00:38:44,000
proposals in the dsa in the dna so

1050
00:38:44,000 --> 00:38:46,400
it's sort of um there there again we can

1051
00:38:46,400 --> 00:38:48,320
try to figure out where synergies are in

1052
00:38:48,320 --> 00:38:49,920
different communities or struggling for

1053
00:38:49,920 --> 00:38:52,240
transparency and different settings and

1054
00:38:52,240 --> 00:38:53,599
have it you know

1055
00:38:53,599 --> 00:38:55,920
try to bring it very much as a sort of a

1056
00:38:55,920 --> 00:38:58,800
a societal struggle way more than a

1057
00:38:58,800 --> 00:39:00,400
specific struggle with a specific

1058
00:39:00,400 --> 00:39:02,240
technology because it goes well beyond

1059
00:39:02,240 --> 00:39:03,040
that

1060
00:39:03,040 --> 00:39:05,200
and i think uh yes

1061
00:39:05,200 --> 00:39:07,839
this this this could be a functional

1062
00:39:07,839 --> 00:39:10,320
uh strategy uh

1063
00:39:10,320 --> 00:39:12,160
for the rest i couldn't agree more with

1064
00:39:12,160 --> 00:39:16,400
what lorna said that a possible good way

1065
00:39:16,400 --> 00:39:19,040
to go would be to bring

1066
00:39:19,040 --> 00:39:20,800
on the surface and on the light and give

1067
00:39:20,800 --> 00:39:22,400
the attend all the new attention to

1068
00:39:22,400 --> 00:39:25,040
specific individual cases so we need to

1069
00:39:25,040 --> 00:39:26,640
be able to have

1070
00:39:26,640 --> 00:39:29,359
narratives and cases that speak for

1071
00:39:29,359 --> 00:39:31,680
themselves and they are sufficient to be

1072
00:39:31,680 --> 00:39:33,920
strong and sufficiently

1073
00:39:33,920 --> 00:39:35,280
um

1074
00:39:35,280 --> 00:39:36,960
the gains officially have sufficient

1075
00:39:36,960 --> 00:39:39,520
attention as to resist to the counter

1076
00:39:39,520 --> 00:39:41,359
narrative that we have

1077
00:39:41,359 --> 00:39:43,040
everywhere

1078
00:39:43,040 --> 00:39:47,040
which which is um you know

1079
00:39:47,040 --> 00:39:50,480
supported especially by the

1080
00:39:50,480 --> 00:39:51,599
companies that are interested in

1081
00:39:51,599 --> 00:39:53,359
developing all these

1082
00:39:53,359 --> 00:39:54,960
technologies

1083
00:39:54,960 --> 00:39:57,920
and i think there uh once again i think

1084
00:39:57,920 --> 00:39:59,680
is a collective uh

1085
00:39:59,680 --> 00:40:02,240
action do we need to do uh we at an

1086
00:40:02,240 --> 00:40:04,560
article we work a lot with journalists

1087
00:40:04,560 --> 00:40:06,800
and media outlets and activists and i

1088
00:40:06,800 --> 00:40:09,680
think uh journalists will be extremely

1089
00:40:09,680 --> 00:40:12,160
uh sensitive to those individual stories

1090
00:40:12,160 --> 00:40:14,079
as well so it could help for example to

1091
00:40:14,079 --> 00:40:16,240
bring those stories in the eyes of

1092
00:40:16,240 --> 00:40:19,599
politicians or or legislators so i think

1093
00:40:19,599 --> 00:40:20,960
there

1094
00:40:20,960 --> 00:40:23,440
we have another possibility to get into

1095
00:40:23,440 --> 00:40:26,400
for our messages together and involve a

1096
00:40:26,400 --> 00:40:29,200
broader audience because that will make

1097
00:40:29,200 --> 00:40:31,839
make a significant difference in the

1098
00:40:31,839 --> 00:40:34,400
strength of our arguments i would say

1099
00:40:34,400 --> 00:40:36,640
yeah

1100
00:40:36,720 --> 00:40:38,400
thank you issa

1101
00:40:38,400 --> 00:40:40,480
um i think you know related to that

1102
00:40:40,480 --> 00:40:42,720
point of like what civil society can do

1103
00:40:42,720 --> 00:40:45,440
and perhaps should do in the future um

1104
00:40:45,440 --> 00:40:47,599
daniel i wanted to turn to you and ask

1105
00:40:47,599 --> 00:40:49,440
especially when it comes to the risk

1106
00:40:49,440 --> 00:40:51,839
classifications um because you've been

1107
00:40:51,839 --> 00:40:53,520
you know so steeped in the work and you

1108
00:40:53,520 --> 00:40:55,520
have a real pulse of how these things

1109
00:40:55,520 --> 00:40:56,800
are moving and what works and what

1110
00:40:56,800 --> 00:41:00,240
doesn't um what kind of strategies or

1111
00:41:00,240 --> 00:41:02,400
arguments are you hoping to see

1112
00:41:02,400 --> 00:41:04,480
more of from civil society as well as i

1113
00:41:04,480 --> 00:41:06,000
think equally importantly what are you

1114
00:41:06,000 --> 00:41:08,800
hoping to see less of in the future

1115
00:41:08,800 --> 00:41:12,319
great job so good very good question um

1116
00:41:12,319 --> 00:41:13,119
i mean

1117
00:41:13,119 --> 00:41:15,280
one thing i would totally

1118
00:41:15,280 --> 00:41:18,319
uh plus one to what was raised by baltis

1119
00:41:18,319 --> 00:41:20,319
and lorna about the importance of

1120
00:41:20,319 --> 00:41:22,240
personal stories i think

1121
00:41:22,240 --> 00:41:23,760
if we look at things like biometric

1122
00:41:23,760 --> 00:41:25,520
categorization you know

1123
00:41:25,520 --> 00:41:27,119
looking at how

1124
00:41:27,119 --> 00:41:28,800
gender recognition is a form of that

1125
00:41:28,800 --> 00:41:30,319
looking at how gender recognition

1126
00:41:30,319 --> 00:41:32,480
affects trans people affects non-binary

1127
00:41:32,480 --> 00:41:34,319
people i'm putting those personal

1128
00:41:34,319 --> 00:41:36,160
stories at the front

1129
00:41:36,160 --> 00:41:37,200
um

1130
00:41:37,200 --> 00:41:38,720
i think it's really key

1131
00:41:38,720 --> 00:41:41,200
and it has to go hand in hand also with

1132
00:41:41,200 --> 00:41:43,839
deflating the kind of marketing hype and

1133
00:41:43,839 --> 00:41:44,800
rubbish

1134
00:41:44,800 --> 00:41:47,440
um because so often these systems are

1135
00:41:47,440 --> 00:41:50,640
put forward as futuristic uh bringing

1136
00:41:50,640 --> 00:41:52,560
all these high-tech benefits and they're

1137
00:41:52,560 --> 00:41:54,160
actually just about cutting costs

1138
00:41:54,160 --> 00:41:56,400
they're like systems of austerity you

1139
00:41:56,400 --> 00:41:57,119
know

1140
00:41:57,119 --> 00:41:59,920
ai hiring systems are used because

1141
00:41:59,920 --> 00:42:01,599
people are cutting costs and don't want

1142
00:42:01,599 --> 00:42:04,079
to have enough stuff to properly

1143
00:42:04,079 --> 00:42:05,839
review people you know and these things

1144
00:42:05,839 --> 00:42:08,000
are always brought in to cut corners

1145
00:42:08,000 --> 00:42:11,119
they're not optimal solutions and if you

1146
00:42:11,119 --> 00:42:13,839
can burst that bubble to kind of take

1147
00:42:13,839 --> 00:42:15,440
the shine off them

1148
00:42:15,440 --> 00:42:17,680
plus bring forward those personal

1149
00:42:17,680 --> 00:42:19,520
stories that show the harm i think

1150
00:42:19,520 --> 00:42:23,040
that's a really really effective um

1151
00:42:23,040 --> 00:42:25,040
tactic and that links into you know what

1152
00:42:25,040 --> 00:42:27,920
sarah said as well asked about how do we

1153
00:42:27,920 --> 00:42:29,440
ensure that the red lines that really

1154
00:42:29,440 --> 00:42:31,760
affect marginalized communities are

1155
00:42:31,760 --> 00:42:33,839
taken seriously and it's nothing putting

1156
00:42:33,839 --> 00:42:36,240
forward these personal stories i think

1157
00:42:36,240 --> 00:42:37,280
is um

1158
00:42:37,280 --> 00:42:38,560
is really key

1159
00:42:38,560 --> 00:42:39,680
um

1160
00:42:39,680 --> 00:42:41,440
another narrative that i think we really

1161
00:42:41,440 --> 00:42:44,480
need to push back on is thus

1162
00:42:44,480 --> 00:42:46,400
you know especially in the parliament

1163
00:42:46,400 --> 00:42:48,240
discussions that there's sort of

1164
00:42:48,240 --> 00:42:51,440
extremes and that we need the moderate

1165
00:42:51,440 --> 00:42:53,920
compromise position between them in the

1166
00:42:53,920 --> 00:42:56,480
case of the red lines that is really

1167
00:42:56,480 --> 00:42:58,960
really not the case you know there's no

1168
00:42:58,960 --> 00:43:01,359
compromise position there's no

1169
00:43:01,359 --> 00:43:03,680
i don't know centrist dad middle of the

1170
00:43:03,680 --> 00:43:07,599
road uh objective position on systems

1171
00:43:07,599 --> 00:43:09,599
that violate the core of fundamental

1172
00:43:09,599 --> 00:43:12,240
rights like automated gender recognition

1173
00:43:12,240 --> 00:43:15,200
can't be fixed it can only be abusive

1174
00:43:15,200 --> 00:43:17,680
uh it totally and irrevocably undermines

1175
00:43:17,680 --> 00:43:21,440
the rights of uh trans non-binary people

1176
00:43:21,440 --> 00:43:22,880
and it needs to be banned and there's no

1177
00:43:22,880 --> 00:43:25,599
compromise on that so we need to kind of

1178
00:43:25,599 --> 00:43:28,160
stick to our guns on on some of these

1179
00:43:28,160 --> 00:43:30,640
positions uh against the idea that

1180
00:43:30,640 --> 00:43:32,079
there's some

1181
00:43:32,079 --> 00:43:33,520
okay

1182
00:43:33,520 --> 00:43:35,599
solution on them they didn't you know

1183
00:43:35,599 --> 00:43:37,359
and that that goes in line with you know

1184
00:43:37,359 --> 00:43:39,280
formulating the criteria about what

1185
00:43:39,280 --> 00:43:41,920
constitutes unacceptable risk um

1186
00:43:41,920 --> 00:43:45,119
so i think that's really key um i just

1187
00:43:45,119 --> 00:43:46,880
also want to respond to one of the

1188
00:43:46,880 --> 00:43:48,800
questions in the

1189
00:43:48,800 --> 00:43:50,880
in the chat because it it relates to the

1190
00:43:50,880 --> 00:43:52,640
question you just asked me for dushi

1191
00:43:52,640 --> 00:43:54,640
someone asked um

1192
00:43:54,640 --> 00:43:56,400
the use of an ai system to infer

1193
00:43:56,400 --> 00:43:58,480
emotions can be useful in medicine and

1194
00:43:58,480 --> 00:44:00,160
should we have an exception for this and

1195
00:44:00,160 --> 00:44:02,800
this also came up in the edps edpb joint

1196
00:44:02,800 --> 00:44:04,560
statement where they called for a ban on

1197
00:44:04,560 --> 00:44:05,920
emotion recognition but they asked for

1198
00:44:05,920 --> 00:44:08,880
an exception for medical use

1199
00:44:08,880 --> 00:44:10,560
i think this really like gets in a

1200
00:44:10,560 --> 00:44:12,400
nutshell the complexity of this question

1201
00:44:12,400 --> 00:44:14,319
of defining emotion recognition all of

1202
00:44:14,319 --> 00:44:16,319
this because

1203
00:44:16,319 --> 00:44:18,079
first things first is that emotion

1204
00:44:18,079 --> 00:44:20,000
recognition systems at the moment do not

1205
00:44:20,000 --> 00:44:21,440
work um

1206
00:44:21,440 --> 00:44:24,240
there again there is a correlation

1207
00:44:24,240 --> 00:44:26,560
between facial configuration and

1208
00:44:26,560 --> 00:44:28,480
emotional state between a smile and

1209
00:44:28,480 --> 00:44:30,960
feeling happy but it's not a straight

1210
00:44:30,960 --> 00:44:32,160
one-on-one

1211
00:44:32,160 --> 00:44:33,920
correlation it's not strong enough to

1212
00:44:33,920 --> 00:44:35,520
make the inference that that person is

1213
00:44:35,520 --> 00:44:38,560
happy um and it's an insufficient basis

1214
00:44:38,560 --> 00:44:40,880
uh for emotion recognition we've even

1215
00:44:40,880 --> 00:44:43,520
had so paul ekman's theory on which most

1216
00:44:43,520 --> 00:44:45,520
of these things are based

1217
00:44:45,520 --> 00:44:48,160
is heavily criticized and for very good

1218
00:44:48,160 --> 00:44:49,680
reason and there's serious doubts about

1219
00:44:49,680 --> 00:44:52,560
it but even paul ekman himself has come

1220
00:44:52,560 --> 00:44:54,160
out in an interview the financial times

1221
00:44:54,160 --> 00:44:56,000
and said emotion recognition

1222
00:44:56,000 --> 00:44:58,960
technologies that use his theory are not

1223
00:44:58,960 --> 00:45:00,880
robust you know he does not see his

1224
00:45:00,880 --> 00:45:03,200
theory as a basis for these

1225
00:45:03,200 --> 00:45:05,599
technological applications so there's no

1226
00:45:05,599 --> 00:45:07,440
basis there

1227
00:45:07,440 --> 00:45:08,480
um

1228
00:45:08,480 --> 00:45:10,160
for these technologies so the idea that

1229
00:45:10,160 --> 00:45:14,240
we should have an exception for critical

1230
00:45:14,240 --> 00:45:17,359
uses like medicine which are you know

1231
00:45:17,359 --> 00:45:18,960
another example that's often brought up

1232
00:45:18,960 --> 00:45:20,800
is like i think microsoft had a thing

1233
00:45:20,800 --> 00:45:22,880
with their hololens augmented reality

1234
00:45:22,880 --> 00:45:26,000
headset to help autistic people identify

1235
00:45:26,000 --> 00:45:28,880
emotions of people but it can't do that

1236
00:45:28,880 --> 00:45:30,480
so it's giving them incorrect

1237
00:45:30,480 --> 00:45:33,440
information what it could do is identify

1238
00:45:33,440 --> 00:45:35,280
facial configurations it could tell you

1239
00:45:35,280 --> 00:45:36,560
the person who's looking at you is

1240
00:45:36,560 --> 00:45:38,640
smiling but they already know that the

1241
00:45:38,640 --> 00:45:40,480
problem is the inference and the

1242
00:45:40,480 --> 00:45:42,400
technology can't help there

1243
00:45:42,400 --> 00:45:43,200
so

1244
00:45:43,200 --> 00:45:44,880
the idea that this should be an

1245
00:45:44,880 --> 00:45:46,880
exception is really problematic but it

1246
00:45:46,880 --> 00:45:48,720
again points to the the problem of

1247
00:45:48,720 --> 00:45:50,000
defining it

1248
00:45:50,000 --> 00:45:50,800
if

1249
00:45:50,800 --> 00:45:52,720
but this person who asked the question

1250
00:45:52,720 --> 00:45:54,800
means or if what the edp scdpb were

1251
00:45:54,800 --> 00:45:56,640
thinking is that example that i

1252
00:45:56,640 --> 00:45:58,319
mentioned earlier of identifying if

1253
00:45:58,319 --> 00:46:00,000
someone calling an emergency line is

1254
00:46:00,000 --> 00:46:01,599
having a heart attack that's not a

1255
00:46:01,599 --> 00:46:03,359
motion recognition we don't need an

1256
00:46:03,359 --> 00:46:06,079
exception if we define it properly so so

1257
00:46:06,079 --> 00:46:09,599
that's really key and i i just think um

1258
00:46:09,599 --> 00:46:12,160
yeah the

1259
00:46:12,160 --> 00:46:14,079
especially with the motion recognition

1260
00:46:14,079 --> 00:46:15,040
it's

1261
00:46:15,040 --> 00:46:17,599
easy to throw the it's pseudoscience it

1262
00:46:17,599 --> 00:46:19,440
doesn't work

1263
00:46:19,440 --> 00:46:21,520
argument out there but we need to be

1264
00:46:21,520 --> 00:46:23,839
careful with that because you know we're

1265
00:46:23,839 --> 00:46:25,839
saying there's evidence that it doesn't

1266
00:46:25,839 --> 00:46:27,760
work that evidence only applies to

1267
00:46:27,760 --> 00:46:29,839
face-based emotion recognition there's

1268
00:46:29,839 --> 00:46:31,839
no analogous study to lisa feldman's

1269
00:46:31,839 --> 00:46:34,560
parish pirates paper for voice based

1270
00:46:34,560 --> 00:46:36,400
emotion recognition someone should do

1271
00:46:36,400 --> 00:46:38,160
that we should build up the evidence but

1272
00:46:38,160 --> 00:46:41,599
we you know we need to use our arguments

1273
00:46:41,599 --> 00:46:43,040
correctly and

1274
00:46:43,040 --> 00:46:44,720
make sure that our definitions are all

1275
00:46:44,720 --> 00:46:48,160
tight so that we don't come out and then

1276
00:46:48,160 --> 00:46:50,800
leave ourselves vulnerable to examples

1277
00:46:50,800 --> 00:46:52,640
like someone having a heart attack while

1278
00:46:52,640 --> 00:46:54,640
calling an emergency service so yeah

1279
00:46:54,640 --> 00:46:56,720
just i think

1280
00:46:56,720 --> 00:46:59,040
tight definitions tight arguments good

1281
00:46:59,040 --> 00:47:01,440
references um is going to get us over

1282
00:47:01,440 --> 00:47:04,880
the line on these buns

1283
00:47:05,920 --> 00:47:08,000
thank you so much daniel i was reminded

1284
00:47:08,000 --> 00:47:08,880
of

1285
00:47:08,880 --> 00:47:10,800
you know a similar situation that we

1286
00:47:10,800 --> 00:47:12,720
found ourselves in with respect to face

1287
00:47:12,720 --> 00:47:14,880
recognition where for three years

1288
00:47:14,880 --> 00:47:16,480
everyone just said it's not accurate

1289
00:47:16,480 --> 00:47:17,839
it's not accurate without saying it's

1290
00:47:17,839 --> 00:47:20,000
fundamentally problematic and then we

1291
00:47:20,000 --> 00:47:21,760
had ibm and microsoft say well we've

1292
00:47:21,760 --> 00:47:23,599
made it accurate for everyone including

1293
00:47:23,599 --> 00:47:25,680
black women and white men and then we

1294
00:47:25,680 --> 00:47:27,200
said no no the problem is actually we

1295
00:47:27,200 --> 00:47:28,960
don't want these technologies at all and

1296
00:47:28,960 --> 00:47:32,240
i think um a similar kind of risk really

1297
00:47:32,240 --> 00:47:34,079
is facing us at the moment when it comes

1298
00:47:34,079 --> 00:47:36,079
to newer technologies right that we

1299
00:47:36,079 --> 00:47:37,599
don't have

1300
00:47:37,599 --> 00:47:40,160
a gender shades um equivalent to our

1301
00:47:40,160 --> 00:47:42,880
lisa film and parrot kind of study um

1302
00:47:42,880 --> 00:47:44,800
equivalent of so thank you so much for

1303
00:47:44,800 --> 00:47:47,440
that um and i think that you know you

1304
00:47:47,440 --> 00:47:49,119
bought up this idea of like the

1305
00:47:49,119 --> 00:47:52,160
marketing hype right like efficiency um

1306
00:47:52,160 --> 00:47:54,960
or as um you know we've also seen like

1307
00:47:54,960 --> 00:47:57,040
national security or public safety and

1308
00:47:57,040 --> 00:47:59,359
things like that um and in that then i

1309
00:47:59,359 --> 00:48:01,680
kind of i wanted to uh turn to you lorna

1310
00:48:01,680 --> 00:48:03,520
did can you talk about you know what are

1311
00:48:03,520 --> 00:48:05,599
some of the things that we've learned

1312
00:48:05,599 --> 00:48:07,599
from jurisdictions around the world you

1313
00:48:07,599 --> 00:48:10,800
know do we get a sense of what kinds of

1314
00:48:10,800 --> 00:48:13,680
arguments work well or what kinds of

1315
00:48:13,680 --> 00:48:15,680
strategies are particularly compelling

1316
00:48:15,680 --> 00:48:18,000
when we're trying to you know secure red

1317
00:48:18,000 --> 00:48:20,000
lines in the face of many powerful

1318
00:48:20,000 --> 00:48:21,680
actors who have no interest in these

1319
00:48:21,680 --> 00:48:23,680
deadlines is there anything in

1320
00:48:23,680 --> 00:48:26,720
particular that has stuck out for you

1321
00:48:26,720 --> 00:48:30,000
um thanks fujishi um so i think that

1322
00:48:30,000 --> 00:48:32,720
from um our research many of the points

1323
00:48:32,720 --> 00:48:34,559
that have already been made we've we've

1324
00:48:34,559 --> 00:48:36,559
seen them come out within the research

1325
00:48:36,559 --> 00:48:40,000
that we've been doing but i think um

1326
00:48:40,000 --> 00:48:41,680
we've also seen that there's lots of

1327
00:48:41,680 --> 00:48:45,520
structural problems that affect how um

1328
00:48:45,520 --> 00:48:48,640
how red lines um strategies are pursued

1329
00:48:48,640 --> 00:48:49,520
so

1330
00:48:49,520 --> 00:48:51,760
um i think the points that have come up

1331
00:48:51,760 --> 00:48:54,880
quite a lot already um in relation to

1332
00:48:54,880 --> 00:48:57,440
coordination who's in the room plurality

1333
00:48:57,440 --> 00:48:59,200
of voices

1334
00:48:59,200 --> 00:49:03,280
that has come up for us repeatedly were

1335
00:49:03,280 --> 00:49:06,160
different people have said you know

1336
00:49:06,160 --> 00:49:09,040
it's really challenging to pursue red

1337
00:49:09,040 --> 00:49:11,760
lines particularly when you don't have

1338
00:49:11,760 --> 00:49:13,760
strong data protection and strong

1339
00:49:13,760 --> 00:49:16,960
safeguards already in place because if

1340
00:49:16,960 --> 00:49:19,599
you try to pursue this there's a risk

1341
00:49:19,599 --> 00:49:21,760
that you may just be completely shut out

1342
00:49:21,760 --> 00:49:24,640
of the conversation um it may just seem

1343
00:49:24,640 --> 00:49:26,079
too radical

1344
00:49:26,079 --> 00:49:28,319
a step too far

1345
00:49:28,319 --> 00:49:31,040
and so there there's a real concern

1346
00:49:31,040 --> 00:49:33,920
around um if you pursue that you don't

1347
00:49:33,920 --> 00:49:36,160
get to be part of of the conversation

1348
00:49:36,160 --> 00:49:37,680
and we already know that there's a real

1349
00:49:37,680 --> 00:49:40,079
democratic deficit in terms of who

1350
00:49:40,079 --> 00:49:42,160
participates in these regulations

1351
00:49:42,160 --> 00:49:45,200
regulation and governance conversations

1352
00:49:45,200 --> 00:49:47,200
so i think that we have to think about

1353
00:49:47,200 --> 00:49:48,319
you know how

1354
00:49:48,319 --> 00:49:50,880
how have we managed to get to a place

1355
00:49:50,880 --> 00:49:51,760
where

1356
00:49:51,760 --> 00:49:53,520
you know six seven years ago it was very

1357
00:49:53,520 --> 00:49:55,599
difficult to make human rights arguments

1358
00:49:55,599 --> 00:49:57,440
in this space but now we're at a point

1359
00:49:57,440 --> 00:49:58,960
where we've really

1360
00:49:58,960 --> 00:50:01,280
had great achievements in centralizing

1361
00:50:01,280 --> 00:50:03,200
human rights even if it's just often

1362
00:50:03,200 --> 00:50:05,200
feeling quite rhetorical

1363
00:50:05,200 --> 00:50:07,280
within these conversations about ai

1364
00:50:07,280 --> 00:50:09,920
regulations so how do we make red lines

1365
00:50:09,920 --> 00:50:13,040
not seen a radical

1366
00:50:13,040 --> 00:50:14,880
but normalize that is something that

1367
00:50:14,880 --> 00:50:17,040
needs to be squarely within regulation

1368
00:50:17,040 --> 00:50:19,200
conversations and critically how do we

1369
00:50:19,200 --> 00:50:20,240
build

1370
00:50:20,240 --> 00:50:21,440
um

1371
00:50:21,440 --> 00:50:23,040
on the call for dealing with his

1372
00:50:23,040 --> 00:50:25,200
democratic deficit to ensure that

1373
00:50:25,200 --> 00:50:27,440
there's a plurality of voices always in

1374
00:50:27,440 --> 00:50:29,440
the room so that there can be these

1375
00:50:29,440 --> 00:50:31,200
conversations really

1376
00:50:31,200 --> 00:50:34,160
um taking place and that people um

1377
00:50:34,160 --> 00:50:36,319
don't need to have to feel concerned

1378
00:50:36,319 --> 00:50:38,800
about pursuing these arguments and then

1379
00:50:38,800 --> 00:50:40,640
being shut out so i think that that's

1380
00:50:40,640 --> 00:50:41,839
one big

1381
00:50:41,839 --> 00:50:44,000
thing that has come out for us um i

1382
00:50:44,000 --> 00:50:46,880
think the other um big thing that has

1383
00:50:46,880 --> 00:50:49,119
come out is as she said fujishi going

1384
00:50:49,119 --> 00:50:51,359
beyond technical errors and talking

1385
00:50:51,359 --> 00:50:54,240
about wider acceptability

1386
00:50:54,240 --> 00:50:57,040
but i think that what we have seen

1387
00:50:57,040 --> 00:50:58,240
is that

1388
00:50:58,240 --> 00:51:00,960
people you know repeatedly raising how

1389
00:51:00,960 --> 00:51:03,520
difficult it is to talk about red lines

1390
00:51:03,520 --> 00:51:06,559
in the face of security

1391
00:51:06,559 --> 00:51:09,520
and dealing with crime and so crime and

1392
00:51:09,520 --> 00:51:11,200
insecurity being

1393
00:51:11,200 --> 00:51:13,920
um very difficult overcoming

1394
00:51:13,920 --> 00:51:16,800
ideas that biometric technologies may be

1395
00:51:16,800 --> 00:51:19,680
a way in which to make us feel safer

1396
00:51:19,680 --> 00:51:20,800
um

1397
00:51:20,800 --> 00:51:22,640
and i think to be able to deal with that

1398
00:51:22,640 --> 00:51:25,520
big obstacle um it's not enough just to

1399
00:51:25,520 --> 00:51:27,680
focus on the technology and the problems

1400
00:51:27,680 --> 00:51:29,359
of the technology but we'll really have

1401
00:51:29,359 --> 00:51:33,040
to locate this within this much bigger

1402
00:51:33,040 --> 00:51:36,400
idea that human rights um

1403
00:51:36,400 --> 00:51:38,559
can be dispensed with when you're trying

1404
00:51:38,559 --> 00:51:42,079
to deal with security and crime issues

1405
00:51:42,079 --> 00:51:44,160
and really get at the heart of the of

1406
00:51:44,160 --> 00:51:46,319
the um

1407
00:51:46,319 --> 00:51:48,079
the way in which law enforcement and

1408
00:51:48,079 --> 00:51:50,240
security can be pursued at the expense

1409
00:51:50,240 --> 00:51:52,000
of human rights and particularly

1410
00:51:52,000 --> 00:51:53,599
structural and institutional

1411
00:51:53,599 --> 00:51:55,680
discrimination that can come out of that

1412
00:51:55,680 --> 00:51:57,760
and to then show how

1413
00:51:57,760 --> 00:52:00,079
biometric technologies in these contexts

1414
00:52:00,079 --> 00:52:02,640
can accentuate and add new dimensions to

1415
00:52:02,640 --> 00:52:04,800
these problems

1416
00:52:04,800 --> 00:52:08,720
so i think um really trying to locate

1417
00:52:08,720 --> 00:52:10,400
technologies within these problems i

1418
00:52:10,400 --> 00:52:11,839
think has come out is something that's

1419
00:52:11,839 --> 00:52:14,960
really important um to address

1420
00:52:14,960 --> 00:52:17,280
um and then just maybe the last point

1421
00:52:17,280 --> 00:52:19,520
has been the importance of dealing with

1422
00:52:19,520 --> 00:52:21,280
multi-layered strategies so we're

1423
00:52:21,280 --> 00:52:23,280
talking a lot of here about formal

1424
00:52:23,280 --> 00:52:24,720
regulation

1425
00:52:24,720 --> 00:52:28,000
in the context of the eui act um but

1426
00:52:28,000 --> 00:52:29,839
what has come out from uh from our

1427
00:52:29,839 --> 00:52:32,880
research is also you know when is it the

1428
00:52:32,880 --> 00:52:36,079
right time to use courts um and you know

1429
00:52:36,079 --> 00:52:37,839
he said talked about

1430
00:52:37,839 --> 00:52:40,000
maybe some of the challenges with that

1431
00:52:40,000 --> 00:52:42,240
um but sometimes litigation will be a

1432
00:52:42,240 --> 00:52:44,480
really important precursor or the really

1433
00:52:44,480 --> 00:52:46,960
important route but also thinking about

1434
00:52:46,960 --> 00:52:49,040
other actors like investors being a

1435
00:52:49,040 --> 00:52:51,359
really important forum for influencing

1436
00:52:51,359 --> 00:52:54,800
um how red lines are pursued so

1437
00:52:54,800 --> 00:52:55,839
um

1438
00:52:55,839 --> 00:52:57,280
really trying to think about the

1439
00:52:57,280 --> 00:52:58,720
different levels

1440
00:52:58,720 --> 00:53:00,400
of regulation and governance and

1441
00:53:00,400 --> 00:53:02,720
different points in which we can resist

1442
00:53:02,720 --> 00:53:05,280
um the use of these technologies

1443
00:53:05,280 --> 00:53:06,960
they'll stop here because i think we're

1444
00:53:06,960 --> 00:53:09,920
we're running short some time

1445
00:53:09,920 --> 00:53:12,079
thank you thank you so much uh no no

1446
00:53:12,079 --> 00:53:13,359
that's a lot of food for that and i

1447
00:53:13,359 --> 00:53:15,119
think what i was struck by when you when

1448
00:53:15,119 --> 00:53:16,720
you were talking about you know how

1449
00:53:16,720 --> 00:53:19,280
insecurity and and um

1450
00:53:19,280 --> 00:53:22,559
kind of safety related uh

1451
00:53:22,559 --> 00:53:23,440
you know

1452
00:53:23,440 --> 00:53:25,440
issues kind of warrant almost an

1453
00:53:25,440 --> 00:53:27,680
exception which is quite ironic because

1454
00:53:27,680 --> 00:53:29,359
the safeguards that we have and the

1455
00:53:29,359 --> 00:53:30,880
standards of proportionality and

1456
00:53:30,880 --> 00:53:33,200
necessity actually exist for that very

1457
00:53:33,200 --> 00:53:35,200
situation but they're abandoned exactly

1458
00:53:35,200 --> 00:53:36,079
then

1459
00:53:36,079 --> 00:53:39,040
um so thank you so much to um to our

1460
00:53:39,040 --> 00:53:41,520
panelists we have 10 minutes we're gonna

1461
00:53:41,520 --> 00:53:43,680
be going five minutes over

1462
00:53:43,680 --> 00:53:46,240
the stipulated time if that's okay

1463
00:53:46,240 --> 00:53:48,079
um i just wanted to make sure that we

1464
00:53:48,079 --> 00:53:50,800
answered all of the questions that we

1465
00:53:50,800 --> 00:53:52,559
have in the chat

1466
00:53:52,559 --> 00:53:53,680
um

1467
00:53:53,680 --> 00:53:56,400
i think daniel you kind of answered

1468
00:53:56,400 --> 00:53:58,559
sarah's questions about um you know

1469
00:53:58,559 --> 00:54:00,480
whether we will be able to successfully

1470
00:54:00,480 --> 00:54:02,640
mobilize as a community against those

1471
00:54:02,640 --> 00:54:04,400
deadlines which primarily are targeted

1472
00:54:04,400 --> 00:54:06,800
against relatively underrepresented uh

1473
00:54:06,800 --> 00:54:09,839
groups um issa and lana did you want to

1474
00:54:09,839 --> 00:54:13,839
jump in on that particular question

1475
00:54:17,280 --> 00:54:19,280
sorry which which kind of question do

1476
00:54:19,280 --> 00:54:21,440
you want to address sorry i just i'm

1477
00:54:21,440 --> 00:54:24,720
opening the chat now seeing it's pretty

1478
00:54:24,720 --> 00:54:26,880
yeah it's it's it's gotten very active

1479
00:54:26,880 --> 00:54:29,680
at the moment which is great um so sarah

1480
00:54:29,680 --> 00:54:31,839
asked i'd be interested to know how far

1481
00:54:31,839 --> 00:54:33,680
the panelists feel we will be able to

1482
00:54:33,680 --> 00:54:36,079
successfully mobilize as a community

1483
00:54:36,079 --> 00:54:38,240
against those red lines which primarily

1484
00:54:38,240 --> 00:54:39,920
are targeted against relatively

1485
00:54:39,920 --> 00:54:41,920
underrepresented groups in digital

1486
00:54:41,920 --> 00:54:44,160
rights communities such as racializing

1487
00:54:44,160 --> 00:54:46,559
people and migrants um you touched upon

1488
00:54:46,559 --> 00:54:48,000
this a little bit in your intervention

1489
00:54:48,000 --> 00:54:50,880
issa but i'd love to hear um

1490
00:54:50,880 --> 00:54:53,280
yes so i i do believe for example that

1491
00:54:53,280 --> 00:54:56,319
it is this this um

1492
00:54:56,319 --> 00:54:58,079
one of the most convincing argument is

1493
00:54:58,079 --> 00:55:00,079
where we look at it at outside the

1494
00:55:00,079 --> 00:55:02,079
european union context which is by

1495
00:55:02,079 --> 00:55:03,839
definition you know

1496
00:55:03,839 --> 00:55:06,640
looking at a diverse uh

1497
00:55:06,640 --> 00:55:08,799
at a more diverse environment

1498
00:55:08,799 --> 00:55:10,720
and um i i

1499
00:55:10,720 --> 00:55:13,200
you know it it's once again it's uh it's

1500
00:55:13,200 --> 00:55:16,480
um uh an element that we have with the

1501
00:55:16,480 --> 00:55:17,920
ai

1502
00:55:17,920 --> 00:55:19,920
systems not only in the within the

1503
00:55:19,920 --> 00:55:22,079
framework of the ai act but also when we

1504
00:55:22,079 --> 00:55:23,839
talk about you know many of us are

1505
00:55:23,839 --> 00:55:26,000
involved in the content moderation uh

1506
00:55:26,000 --> 00:55:28,720
discussions as well and there it's it so

1507
00:55:28,720 --> 00:55:32,559
the way i picture it is uh that um

1508
00:55:32,559 --> 00:55:34,480
the there are certain categories always

1509
00:55:34,480 --> 00:55:36,000
there will be certain categories that

1510
00:55:36,000 --> 00:55:38,559
will suffer more harm than others right

1511
00:55:38,559 --> 00:55:41,119
uh it doesn't really need to refrain us

1512
00:55:41,119 --> 00:55:42,960
from but what we are reasoning about

1513
00:55:42,960 --> 00:55:45,920
here is not uh uh whether you know we

1514
00:55:45,920 --> 00:55:48,640
are we can accept a system

1515
00:55:48,640 --> 00:55:51,119
that creates those trade-offs and harms

1516
00:55:51,119 --> 00:55:52,000
for

1517
00:55:52,000 --> 00:55:54,640
x percentage of society and then the

1518
00:55:54,640 --> 00:55:56,240
another percentage is safe so we're not

1519
00:55:56,240 --> 00:55:58,559
discussing about you know if 50 plus one

1520
00:55:58,559 --> 00:56:00,559
percent of society is safe then we are

1521
00:56:00,559 --> 00:56:03,040
okay with this it's not this this sort

1522
00:56:03,040 --> 00:56:04,720
of proportionality test that we need to

1523
00:56:04,720 --> 00:56:06,480
apply the point is

1524
00:56:06,480 --> 00:56:08,720
uh for each and every individual because

1525
00:56:08,720 --> 00:56:10,400
what we're talking about is human rights

1526
00:56:10,400 --> 00:56:12,559
and and human rights applies to each and

1527
00:56:12,559 --> 00:56:14,480
every individual everywhere we are so to

1528
00:56:14,480 --> 00:56:15,839
each and every individual is

1529
00:56:15,839 --> 00:56:17,359
disproportionate

1530
00:56:17,359 --> 00:56:20,079
and necessary this kind of violation so

1531
00:56:20,079 --> 00:56:22,640
what i would would use in terms of you

1532
00:56:22,640 --> 00:56:25,599
know the the narrative the in order in

1533
00:56:25,599 --> 00:56:28,319
order to to make clear that we're very

1534
00:56:28,319 --> 00:56:30,880
much all in the same boat is this one

1535
00:56:30,880 --> 00:56:33,359
and not to forget that um

1536
00:56:33,359 --> 00:56:34,720
the moment

1537
00:56:34,720 --> 00:56:36,799
the moment as a specific technology is

1538
00:56:36,799 --> 00:56:38,799
designed and deployed

1539
00:56:38,799 --> 00:56:40,640
it's extremely the kind of harm that

1540
00:56:40,640 --> 00:56:42,799
that

1541
00:56:43,520 --> 00:56:45,839
they could create um

1542
00:56:45,839 --> 00:56:48,240
or generate it's extremely context

1543
00:56:48,240 --> 00:56:50,160
dependent can be extremely context

1544
00:56:50,160 --> 00:56:53,839
dependent so this can be linked to the

1545
00:56:53,839 --> 00:56:56,880
uh i don't know the the um

1546
00:56:56,880 --> 00:56:59,200
economic context but especially but also

1547
00:56:59,200 --> 00:57:01,760
the political one so those

1548
00:57:01,760 --> 00:57:05,200
those are variables right and you can

1549
00:57:05,200 --> 00:57:07,920
even within the eu you can have the

1550
00:57:07,920 --> 00:57:09,920
different different approaches and to

1551
00:57:09,920 --> 00:57:11,359
those specific technologies or how

1552
00:57:11,359 --> 00:57:13,520
they're going to be used in practice

1553
00:57:13,520 --> 00:57:14,799
so i don't think

1554
00:57:14,799 --> 00:57:17,040
uh what we need to think of is how to

1555
00:57:17,040 --> 00:57:18,880
predict specific categories we need to

1556
00:57:18,880 --> 00:57:20,880
to sort of figure out a system that has

1557
00:57:20,880 --> 00:57:23,520
sufficient guarantees and when it comes

1558
00:57:23,520 --> 00:57:25,200
to efficient bonds

1559
00:57:25,200 --> 00:57:28,319
to sort of eliminate um critics and

1560
00:57:28,319 --> 00:57:29,280
possible

1561
00:57:29,280 --> 00:57:31,760
any any any risk whatsoever and not to

1562
00:57:31,760 --> 00:57:34,480
be uh linked to you know okay it's okay

1563
00:57:34,480 --> 00:57:35,920
because the majority of people they're

1564
00:57:35,920 --> 00:57:37,599
gonna be better off it's okay because we

1565
00:57:37,599 --> 00:57:39,119
trust the political system in place in

1566
00:57:39,119 --> 00:57:40,720
this specific environment and so on and

1567
00:57:40,720 --> 00:57:42,960
so forth so i would i would go for a

1568
00:57:42,960 --> 00:57:44,720
completely different uh scenario and

1569
00:57:44,720 --> 00:57:46,240
there i think

1570
00:57:46,240 --> 00:57:49,440
we can we can easily create exploit and

1571
00:57:49,440 --> 00:57:51,280
focus on the synergies among the

1572
00:57:51,280 --> 00:57:52,880
different communities rather than the

1573
00:57:52,880 --> 00:57:56,480
differences um yeah and if i if i can

1574
00:57:56,480 --> 00:57:58,720
just um uh very very quickly come back

1575
00:57:58,720 --> 00:58:01,200
to a point it was raised so far uh i do

1576
00:58:01,200 --> 00:58:03,520
agree that a number of these things come

1577
00:58:03,520 --> 00:58:06,000
up uh with litigation and i do also

1578
00:58:06,000 --> 00:58:07,760
agree that the difference in in for

1579
00:58:07,760 --> 00:58:09,440
example the data protection framework do

1580
00:58:09,440 --> 00:58:10,799
you have enough

1581
00:58:10,799 --> 00:58:12,240
is um

1582
00:58:12,240 --> 00:58:15,119
it's gonna it's gonna be a game changer

1583
00:58:15,119 --> 00:58:17,760
but i would warn this is not a data

1584
00:58:17,760 --> 00:58:20,000
protection fight not only a data

1585
00:58:20,000 --> 00:58:22,000
protection fight so when we talk about

1586
00:58:22,000 --> 00:58:23,440
you know risks and where we talk about

1587
00:58:23,440 --> 00:58:25,040
red lines we're talking about human

1588
00:58:25,040 --> 00:58:26,799
rights and there are way more human

1589
00:58:26,799 --> 00:58:28,559
rights to are to be added in the list of

1590
00:58:28,559 --> 00:58:30,799
what we we you know we need to safeguard

1591
00:58:30,799 --> 00:58:33,280
here so the simple fact in brackets

1592
00:58:33,280 --> 00:58:35,119
simple that we have a good data

1593
00:58:35,119 --> 00:58:38,480
protection framework uh it's not gonna

1594
00:58:38,480 --> 00:58:41,760
be make us safe and that's why we as a

1595
00:58:41,760 --> 00:58:43,520
collective and i don't know many of us

1596
00:58:43,520 --> 00:58:45,599
have done that we've always appreciated

1597
00:58:45,599 --> 00:58:46,960
this idea of human rights impact

1598
00:58:46,960 --> 00:58:48,640
assessment not only data protection

1599
00:58:48,640 --> 00:58:50,319
assessment impact assessments because

1600
00:58:50,319 --> 00:58:51,760
this is an account it is not going to

1601
00:58:51,760 --> 00:58:54,240
capture the entire story

1602
00:58:54,240 --> 00:58:56,558
um

1603
00:58:57,040 --> 00:59:01,920
thank you thank you issa um we have two

1604
00:59:01,920 --> 00:59:04,319
remaining questions one i mean each of

1605
00:59:04,319 --> 00:59:06,960
which i'm going to uh pose to lana and

1606
00:59:06,960 --> 00:59:08,799
danielle if i could request you to keep

1607
00:59:08,799 --> 00:59:10,079
your answers to about two and a half

1608
00:59:10,079 --> 00:59:11,760
minutes that would be great

1609
00:59:11,760 --> 00:59:13,200
um

1610
00:59:13,200 --> 00:59:14,480
uh so

1611
00:59:14,480 --> 00:59:17,520
lorna uh we have a question that says i

1612
00:59:17,520 --> 00:59:19,119
am currently looking at transnational

1613
00:59:19,119 --> 00:59:20,640
issues and there is and is there

1614
00:59:20,640 --> 00:59:22,720
sufficient agreement on what human

1615
00:59:22,720 --> 00:59:26,000
rights values we share i have analyzed

1616
00:59:26,000 --> 00:59:27,760
various proposals and they often refer

1617
00:59:27,760 --> 00:59:30,640
to this as being uncontested is this the

1618
00:59:30,640 --> 00:59:32,799
case i realize this is a bit outside the

1619
00:59:32,799 --> 00:59:34,720
wheelhouse of our current conversation

1620
00:59:34,720 --> 00:59:36,799
but i do think it's an important um you

1621
00:59:36,799 --> 00:59:38,880
know kind of foundational question and

1622
00:59:38,880 --> 00:59:40,720
assumption to be uh

1623
00:59:40,720 --> 00:59:44,200
for us to discuss

1624
00:59:47,119 --> 00:59:47,920
sorry

1625
00:59:47,920 --> 00:59:50,319
did you want me to answer that or daniel

1626
00:59:50,319 --> 00:59:52,559
or both lorna it would be great if you

1627
00:59:52,559 --> 00:59:55,280
could answer that yeah yeah sure

1628
00:59:55,280 --> 00:59:56,720
um so

1629
00:59:56,720 --> 00:59:58,839
i'm not very sure which um

1630
00:59:58,839 --> 01:00:01,760
proposals um are being referred to but i

1631
01:00:01,760 --> 01:00:03,760
think that certainly when we've looked

1632
01:00:03,760 --> 01:00:06,240
at different national ai strategies for

1633
01:00:06,240 --> 01:00:07,440
example

1634
01:00:07,440 --> 01:00:10,160
um there are different approaches to how

1635
01:00:10,160 --> 01:00:11,839
human rights are referred to so

1636
01:00:11,839 --> 01:00:14,799
sometimes we find only some individual

1637
01:00:14,799 --> 01:00:16,799
human rights are referred to and

1638
01:00:16,799 --> 01:00:19,200
sometimes human rights are referred to

1639
01:00:19,200 --> 01:00:22,160
generally um but the

1640
01:00:22,160 --> 01:00:24,000
requirements of say accountability and

1641
01:00:24,000 --> 01:00:27,200
remedies and oversight are not there so

1642
01:00:27,200 --> 01:00:28,640
um

1643
01:00:28,640 --> 01:00:30,720
it's certainly true that human rights

1644
01:00:30,720 --> 01:00:34,000
are sort of used in um in different ways

1645
01:00:34,000 --> 01:00:36,400
um in national ai strategies and not

1646
01:00:36,400 --> 01:00:38,480
always comprehensively

1647
01:00:38,480 --> 01:00:41,440
um but i think that for our work where

1648
01:00:41,440 --> 01:00:45,359
we start with um you know art taking um

1649
01:00:45,359 --> 01:00:47,359
human rights in in the rhine just

1650
01:00:47,359 --> 01:00:49,119
derived from

1651
01:00:49,119 --> 01:00:50,960
international and regional human rights

1652
01:00:50,960 --> 01:00:52,880
treaties looking at civil and political

1653
01:00:52,880 --> 01:00:55,680
and economic social and cultural

1654
01:00:55,680 --> 01:00:57,760
human rights as sort of the universal

1655
01:00:57,760 --> 01:01:00,319
understanding of what those rights are

1656
01:01:00,319 --> 01:01:02,480
and also the um

1657
01:01:02,480 --> 01:01:04,079
the obligations and states and

1658
01:01:04,079 --> 01:01:05,839
responsibilities of businesses that

1659
01:01:05,839 --> 01:01:08,640
attach um to them

1660
01:01:08,640 --> 01:01:11,680
so um and i i think that's where

1661
01:01:11,680 --> 01:01:14,079
most people um are starting with when

1662
01:01:14,079 --> 01:01:16,240
they're having this conversation but i

1663
01:01:16,240 --> 01:01:18,880
think there are big risks that um that

1664
01:01:18,880 --> 01:01:19,839
we

1665
01:01:19,839 --> 01:01:22,480
focus only on certain rights and

1666
01:01:22,480 --> 01:01:25,200
therefore miss the full human rights

1667
01:01:25,200 --> 01:01:27,839
impact and the intersecting

1668
01:01:27,839 --> 01:01:29,680
human rights impact

1669
01:01:29,680 --> 01:01:32,319
and intercepting forms of discrimination

1670
01:01:32,319 --> 01:01:35,200
that are often coming up um when we too

1671
01:01:35,200 --> 01:01:37,520
narrowly focus on on one right i think

1672
01:01:37,520 --> 01:01:39,280
sort of the better way to look at it is

1673
01:01:39,280 --> 01:01:41,440
even if there is a particular impact on

1674
01:01:41,440 --> 01:01:43,280
the right like freedom of expression or

1675
01:01:43,280 --> 01:01:46,079
privacy to really think through what are

1676
01:01:46,079 --> 01:01:48,880
the consequences for other human rights

1677
01:01:48,880 --> 01:01:52,160
and if those rights are violated and i

1678
01:01:52,160 --> 01:01:53,680
think when you do that then you will

1679
01:01:53,680 --> 01:01:55,359
often see the right to liberties at

1680
01:01:55,359 --> 01:01:58,240
stake the right to education you see

1681
01:01:58,240 --> 01:02:00,720
um intersecting forms of discrimination

1682
01:02:00,720 --> 01:02:03,280
um as i said

1683
01:02:03,280 --> 01:02:04,960
so i think really thinking about it

1684
01:02:04,960 --> 01:02:07,760
fully in the round of the international

1685
01:02:07,760 --> 01:02:10,079
and regional treaties and critically

1686
01:02:10,079 --> 01:02:11,760
thinking about what the obligations of

1687
01:02:11,760 --> 01:02:14,799
prevention are accountability access to

1688
01:02:14,799 --> 01:02:16,720
justice remedies

1689
01:02:16,720 --> 01:02:19,280
um oversight and as lisa said human

1690
01:02:19,280 --> 01:02:21,359
rights and pet assessments

1691
01:02:21,359 --> 01:02:23,520
and other tools to identify and address

1692
01:02:23,520 --> 01:02:27,359
human rights risks as they arise

1693
01:02:27,440 --> 01:02:30,480
thank you thank you so much lorna um in

1694
01:02:30,480 --> 01:02:32,240
interest of time i'm going to quickly

1695
01:02:32,240 --> 01:02:34,079
turn to you danielle there's a question

1696
01:02:34,079 --> 01:02:35,280
that says

1697
01:02:35,280 --> 01:02:37,520
um wondering what the panelists think of

1698
01:02:37,520 --> 01:02:39,280
the argument that we are holding ai

1699
01:02:39,280 --> 01:02:41,280
systems to a higher degree of accuracy

1700
01:02:41,280 --> 01:02:43,359
than we do not that we do not apply to

1701
01:02:43,359 --> 01:02:45,920
humans for example judges can be biased

1702
01:02:45,920 --> 01:02:48,400
too and are often not transparent in how

1703
01:02:48,400 --> 01:02:49,599
they decide

1704
01:02:49,599 --> 01:02:51,200
um

1705
01:02:51,200 --> 01:02:52,799
which i think is is the question that

1706
01:02:52,799 --> 01:02:54,960
we've we come across quite often in our

1707
01:02:54,960 --> 01:02:56,799
work done and i'd love to to hear you

1708
01:02:56,799 --> 01:02:58,400
weigh in on that one

1709
01:02:58,400 --> 01:03:00,720
sure thanks yeah i mean i whenever

1710
01:03:00,720 --> 01:03:02,960
people bring up judges i i usually

1711
01:03:02,960 --> 01:03:04,319
assume that they're thinking of the

1712
01:03:04,319 --> 01:03:07,200
famous infamous israeli judges study

1713
01:03:07,200 --> 01:03:09,119
where like these judges were shown to

1714
01:03:09,119 --> 01:03:10,799
rule in a much

1715
01:03:10,799 --> 01:03:13,119
uh more negative way when they were

1716
01:03:13,119 --> 01:03:14,559
hungry before lunch or something and

1717
01:03:14,559 --> 01:03:16,559
that that study has been i think pretty

1718
01:03:16,559 --> 01:03:17,920
conclusively

1719
01:03:17,920 --> 01:03:20,960
uh debunked it's not a good study to

1720
01:03:20,960 --> 01:03:21,760
base

1721
01:03:21,760 --> 01:03:23,920
any arguments on and i would also i mean

1722
01:03:23,920 --> 01:03:25,359
the first thing i'd like to do to

1723
01:03:25,359 --> 01:03:27,839
deflate that question a bit is to say

1724
01:03:27,839 --> 01:03:29,680
that there's not a distinction between

1725
01:03:29,680 --> 01:03:32,160
humans and ai systems there's humans

1726
01:03:32,160 --> 01:03:34,480
using ai systems to do things and we're

1727
01:03:34,480 --> 01:03:36,880
holding them to high standards we're not

1728
01:03:36,880 --> 01:03:38,640
holding ai's

1729
01:03:38,640 --> 01:03:40,079
so it's it's again it's a bit that

1730
01:03:40,079 --> 01:03:42,280
problem of kind of personification

1731
01:03:42,280 --> 01:03:44,799
anthropomorphization of of the systems

1732
01:03:44,799 --> 01:03:46,480
that were holding them to standards no

1733
01:03:46,480 --> 01:03:48,720
we're holding the companies developing

1734
01:03:48,720 --> 01:03:50,799
then we're holding the public authority

1735
01:03:50,799 --> 01:03:53,359
using an ai system to determine access

1736
01:03:53,359 --> 01:03:56,400
to social welfare to a high standard um

1737
01:03:56,400 --> 01:03:57,680
and we should hold them all to high

1738
01:03:57,680 --> 01:03:59,760
standards you know we should have

1739
01:03:59,760 --> 01:04:01,760
uh transparency

1740
01:04:01,760 --> 01:04:04,480
yeah i mean it fits in a bit with that

1741
01:04:04,480 --> 01:04:06,720
problematic argument that oh we're we

1742
01:04:06,720 --> 01:04:08,640
have a problem with ai black boxes but

1743
01:04:08,640 --> 01:04:11,200
humans are black boxes too and

1744
01:04:11,200 --> 01:04:14,640
yeah i think all of those arguments fall

1745
01:04:14,640 --> 01:04:17,520
down a bit when you sort of unpack a bit

1746
01:04:17,520 --> 01:04:18,559
um

1747
01:04:18,559 --> 01:04:21,280
who who you're actually talking about

1748
01:04:21,280 --> 01:04:23,599
there um so

1749
01:04:23,599 --> 01:04:27,119
i i think we should you know and again

1750
01:04:27,119 --> 01:04:28,799
just to come back to the original

1751
01:04:28,799 --> 01:04:30,000
question

1752
01:04:30,000 --> 01:04:32,400
these people who are promoting ai

1753
01:04:32,400 --> 01:04:35,520
solutions are touting them as better

1754
01:04:35,520 --> 01:04:36,480
than

1755
01:04:36,480 --> 01:04:38,880
humans and so we should hold them to a

1756
01:04:38,880 --> 01:04:40,880
high standard

1757
01:04:40,880 --> 01:04:42,799
and then i i find that often that the

1758
01:04:42,799 --> 01:04:45,039
conversation goes that they're promoted

1759
01:04:45,039 --> 01:04:47,760
in the marketing material as absolutely

1760
01:04:47,760 --> 01:04:49,839
optimum super futuristic amazing the

1761
01:04:49,839 --> 01:04:51,440
best solution ever and then you

1762
01:04:51,440 --> 01:04:53,280
criticize them and they say oh but

1763
01:04:53,280 --> 01:04:55,119
actually we're just automating an

1764
01:04:55,119 --> 01:04:57,599
existing crappy imperfect human process

1765
01:04:57,599 --> 01:04:58,799
and you shouldn't hold this to a high

1766
01:04:58,799 --> 01:05:02,319
standard so they they tend to flip from

1767
01:05:02,319 --> 01:05:04,240
kind of one one to the other very

1768
01:05:04,240 --> 01:05:06,240
quickly and yeah sorry

1769
01:05:06,240 --> 01:05:09,200
we will hold them to high standards and

1770
01:05:09,200 --> 01:05:11,680
doing that doesn't preclude that we also

1771
01:05:11,680 --> 01:05:13,359
hold existing

1772
01:05:13,359 --> 01:05:15,359
uh fully human based institutions to

1773
01:05:15,359 --> 01:05:17,440
high standards um you know there's a

1774
01:05:17,440 --> 01:05:20,000
full spectrum of civil society human

1775
01:05:20,000 --> 01:05:22,240
rights activist work and it involves

1776
01:05:22,240 --> 01:05:23,680
holding all aspects and all

1777
01:05:23,680 --> 01:05:26,480
consequential decisions and actors to

1778
01:05:26,480 --> 01:05:31,200
the high bar of human rights complaints

1779
01:05:31,760 --> 01:05:34,480
thank you daniel and what a great note

1780
01:05:34,480 --> 01:05:36,960
on which we can end our panel uh thank

1781
01:05:36,960 --> 01:05:39,119
you so much to all the panelists we've

1782
01:05:39,119 --> 01:05:40,960
run out of time and gone a little bit

1783
01:05:40,960 --> 01:05:43,200
over so my apologies to the organizers

1784
01:05:43,200 --> 01:05:45,280
but if anything it's always a good sign

1785
01:05:45,280 --> 01:05:47,680
when uh we have more questions than time

1786
01:05:47,680 --> 01:05:49,599
but um i just wanted to say thank you so

1787
01:05:49,599 --> 01:05:51,200
much to our panelists again this has

1788
01:05:51,200 --> 01:05:52,880
been a fascinating conversation and

1789
01:05:52,880 --> 01:05:55,119
thank you to all our participants and

1790
01:05:55,119 --> 01:05:58,920
adrie for having us

1791
01:06:06,559 --> 01:06:08,640
you

