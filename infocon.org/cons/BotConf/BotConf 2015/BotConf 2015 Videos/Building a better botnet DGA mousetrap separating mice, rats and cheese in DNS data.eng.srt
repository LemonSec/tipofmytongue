1
00:00:00,080 --> 00:00:03,040
okay so josiah is adventurous and

2
00:00:03,040 --> 00:00:07,279
uh well yeah he's using a prezi so

3
00:00:07,279 --> 00:00:09,759
hang on

4
00:00:10,719 --> 00:00:12,880
you have the floor as soon as as the

5
00:00:12,880 --> 00:00:16,480
presentation appears

6
00:00:16,480 --> 00:00:19,920
there we go good morning my name is

7
00:00:19,920 --> 00:00:20,880
josiah hagan

8
00:00:20,880 --> 00:00:24,160
i'm a researcher at tippingpoint mostly

9
00:00:24,160 --> 00:00:25,199
using

10
00:00:25,199 --> 00:00:26,960
math statistical analysis to look at

11
00:00:26,960 --> 00:00:29,039
large volumes of network data

12
00:00:29,039 --> 00:00:30,720
for the past couple of years i've been

13
00:00:30,720 --> 00:00:33,600
looking at dga traffic

14
00:00:33,600 --> 00:00:36,399
thanks to daniel earlier with the dj

15
00:00:36,399 --> 00:00:37,680
archive talk

16
00:00:37,680 --> 00:00:40,399
and the alexanders yesterday with the

17
00:00:40,399 --> 00:00:41,840
other dj talk for giving

18
00:00:41,840 --> 00:00:43,440
an introduction to my talk so i won't

19
00:00:43,440 --> 00:00:45,600
have to go too much in depth on the

20
00:00:45,600 --> 00:00:47,440
history of dga or

21
00:00:47,440 --> 00:00:51,520
what they're used for and in particular

22
00:00:51,520 --> 00:00:54,160
i'm looking at classifying domains one

23
00:00:54,160 --> 00:00:55,600
at a time to determine whether they're

24
00:00:55,600 --> 00:00:58,000
djs or not i've been doing this work for

25
00:00:58,000 --> 00:01:00,000
the last year with colleagues at hp labs

26
00:01:00,000 --> 00:01:02,719
including miranda and prasant

27
00:01:02,719 --> 00:01:04,080
you know we wanted to move from

28
00:01:04,080 --> 00:01:06,479
something that was pretty primitive

29
00:01:06,479 --> 00:01:08,720
in what we had to to collect djs to

30
00:01:08,720 --> 00:01:12,080
something with a little more teeth

31
00:01:12,080 --> 00:01:14,720
so what are we trying to do we want to

32
00:01:14,720 --> 00:01:16,240
classify domains first

33
00:01:16,240 --> 00:01:17,520
as to whether they're malicious or

34
00:01:17,520 --> 00:01:20,000
benign

35
00:01:20,159 --> 00:01:22,640
we don't want to have false positive

36
00:01:22,640 --> 00:01:24,840
classifications of malicious domains

37
00:01:24,840 --> 00:01:27,439
because it's more expensive for us to

38
00:01:27,439 --> 00:01:29,119
say that something is malicious when

39
00:01:29,119 --> 00:01:30,000
it's actually

40
00:01:30,000 --> 00:01:33,680
just cheese and then

41
00:01:33,680 --> 00:01:35,200
we want to take these classifications

42
00:01:35,200 --> 00:01:36,960
and drill down and if we can we want to

43
00:01:36,960 --> 00:01:37,600
say

44
00:01:37,600 --> 00:01:39,040
okay where did this come from what

45
00:01:39,040 --> 00:01:40,880
family of malware or what type of benign

46
00:01:40,880 --> 00:01:43,600
domain is this

47
00:01:43,920 --> 00:01:46,640
for the purposes of of this work what's

48
00:01:46,640 --> 00:01:48,320
what's not in scope is determining

49
00:01:48,320 --> 00:01:50,479
infected hosts we sort of use

50
00:01:50,479 --> 00:01:52,000
this work as an input for other

51
00:01:52,000 --> 00:01:54,240
classifiers we're only going to consider

52
00:01:54,240 --> 00:01:55,680
domains one at a time

53
00:01:55,680 --> 00:01:58,399
and sometimes we may be wrong so when

54
00:01:58,399 --> 00:01:58,960
you get

55
00:01:58,960 --> 00:02:01,119
thousands of requests a day and some of

56
00:02:01,119 --> 00:02:02,320
them are wrong and a lot of them are

57
00:02:02,320 --> 00:02:03,920
right you tend to be able to make a good

58
00:02:03,920 --> 00:02:06,399
prediction

59
00:02:07,119 --> 00:02:10,479
so what did we do well our solution is

60
00:02:10,479 --> 00:02:10,800
like

61
00:02:10,800 --> 00:02:12,640
many machine learning framework

62
00:02:12,640 --> 00:02:14,000
solutions we have a

63
00:02:14,000 --> 00:02:16,400
training phase and an evaluation phase

64
00:02:16,400 --> 00:02:18,400
when we're evaluating domains

65
00:02:18,400 --> 00:02:20,879
uh you know in in the ips world we're

66
00:02:20,879 --> 00:02:21,680
operating

67
00:02:21,680 --> 00:02:23,840
in real time and in line we want to do

68
00:02:23,840 --> 00:02:25,200
things as quickly as possible so

69
00:02:25,200 --> 00:02:28,640
evaluation has to be fast

70
00:02:28,959 --> 00:02:30,640
for training first we have to gather a

71
00:02:30,640 --> 00:02:34,080
bunch of data labeled by family

72
00:02:34,080 --> 00:02:36,319
then we're going to take that data and

73
00:02:36,319 --> 00:02:37,680
look at some of the syntactical

74
00:02:37,680 --> 00:02:39,920
syntactical features that we can

75
00:02:39,920 --> 00:02:42,319
directly group things by so that we'll

76
00:02:42,319 --> 00:02:44,319
we'll create groups according to

77
00:02:44,319 --> 00:02:46,640
some syntactical features which agree

78
00:02:46,640 --> 00:02:49,360
for everything in the group

79
00:02:49,360 --> 00:02:51,680
then we'll build uh classifiers using

80
00:02:51,680 --> 00:02:52,640
machine learning

81
00:02:52,640 --> 00:02:54,560
uh typically linear classifiers like

82
00:02:54,560 --> 00:02:56,400
logistic regression

83
00:02:56,400 --> 00:02:58,000
to determine if things are benign or

84
00:02:58,000 --> 00:02:59,680
malicious within a given

85
00:02:59,680 --> 00:03:02,319
group and then we'll use that to drill

86
00:03:02,319 --> 00:03:03,360
down to give the

87
00:03:03,360 --> 00:03:06,159
the family or origin of the dga or the

88
00:03:06,159 --> 00:03:08,239
domain in question

89
00:03:08,239 --> 00:03:10,000
now when we do this our feature space

90
00:03:10,000 --> 00:03:11,680
may be quite large so

91
00:03:11,680 --> 00:03:13,040
when we build the classifiers we may

92
00:03:13,040 --> 00:03:14,720
want to determine which features

93
00:03:14,720 --> 00:03:17,200
are particularly salient for for this

94
00:03:17,200 --> 00:03:17,760
group

95
00:03:17,760 --> 00:03:20,959
as far as classification goes

96
00:03:20,959 --> 00:03:23,440
so you can imagine when we're evaluating

97
00:03:23,440 --> 00:03:25,040
we're looking at domains one at a time

98
00:03:25,040 --> 00:03:27,120
as people are making requests

99
00:03:27,120 --> 00:03:29,920
and uh again we'll determine the

100
00:03:29,920 --> 00:03:32,159
syntactic features very quickly

101
00:03:32,159 --> 00:03:35,599
and then apply our classifiers

102
00:03:36,640 --> 00:03:39,760
so the first part of that is coming up

103
00:03:39,760 --> 00:03:41,599
with syntactical rules that allow us to

104
00:03:41,599 --> 00:03:43,599
group these domains

105
00:03:43,599 --> 00:03:45,360
and these are we call them the course

106
00:03:45,360 --> 00:03:48,159
and the fines and tactical features

107
00:03:48,159 --> 00:03:49,920
the first of the of the course ones is

108
00:03:49,920 --> 00:03:52,000
the top-level domain malware families

109
00:03:52,000 --> 00:03:55,680
are typically restricted to several tlds

110
00:03:55,680 --> 00:03:58,080
or in the case of conflict or 100 some

111
00:03:58,080 --> 00:03:59,120
odd domains

112
00:03:59,120 --> 00:04:00,720
but they are somewhat rigid for the

113
00:04:00,720 --> 00:04:03,120
malware family

114
00:04:03,120 --> 00:04:05,120
then we look also at the length of the

115
00:04:05,120 --> 00:04:06,480
second level domain that

116
00:04:06,480 --> 00:04:08,560
that's the part that comes just before

117
00:04:08,560 --> 00:04:11,200
the public suffix or top level domain

118
00:04:11,200 --> 00:04:13,120
again this doesn't have much variance

119
00:04:13,120 --> 00:04:15,680
within a given family

120
00:04:15,680 --> 00:04:16,959
we look at the length of the prefix

121
00:04:16,959 --> 00:04:18,959
everything that comes before the second

122
00:04:18,959 --> 00:04:19,680
level domain

123
00:04:19,680 --> 00:04:21,918
in the domain name and then we also look

124
00:04:21,918 --> 00:04:23,600
at the number of levels

125
00:04:23,600 --> 00:04:24,880
total within the domain you could

126
00:04:24,880 --> 00:04:26,960
imagine that extracting this information

127
00:04:26,960 --> 00:04:29,520
from a string is pretty quick

128
00:04:29,520 --> 00:04:31,600
we call these in our classification

129
00:04:31,600 --> 00:04:33,360
scheme these these form the leaves of

130
00:04:33,360 --> 00:04:35,440
our data

131
00:04:35,440 --> 00:04:36,720
when we're looking at the fine

132
00:04:36,720 --> 00:04:39,199
syntactical features we want to consider

133
00:04:39,199 --> 00:04:42,240
everything all of the the families

134
00:04:42,240 --> 00:04:43,520
within a leaf

135
00:04:43,520 --> 00:04:45,120
what are their regular expressions

136
00:04:45,120 --> 00:04:46,800
because we'll take these and then

137
00:04:46,800 --> 00:04:48,320
compute the intersections

138
00:04:48,320 --> 00:04:50,720
of the regular expressions so that that

139
00:04:50,720 --> 00:04:51,680
we can then

140
00:04:51,680 --> 00:04:53,919
have all the possible subsets of of

141
00:04:53,919 --> 00:04:56,160
combined regular expressions

142
00:04:56,160 --> 00:04:58,000
for families within that leaf that gives

143
00:04:58,000 --> 00:05:00,720
us two to the n possible subsets but

144
00:05:00,720 --> 00:05:04,320
in practice many of the malware families

145
00:05:04,320 --> 00:05:06,000
use the same alphabets we end up with

146
00:05:06,000 --> 00:05:07,759
the same regular expressions

147
00:05:07,759 --> 00:05:09,759
and there are only a handful in practice

148
00:05:09,759 --> 00:05:11,520
we call these subparts of our leaves

149
00:05:11,520 --> 00:05:12,240
lobes

150
00:05:12,240 --> 00:05:13,440
i don't know why we called them these

151
00:05:13,440 --> 00:05:16,080
things but it just stuck

152
00:05:16,080 --> 00:05:18,000
so you might you might get a sort of

153
00:05:18,000 --> 00:05:19,440
overall view of this where we have all

154
00:05:19,440 --> 00:05:21,280
the domains in question at the top

155
00:05:21,280 --> 00:05:23,199
and then first we're separating them by

156
00:05:23,199 --> 00:05:24,320
these course features

157
00:05:24,320 --> 00:05:26,400
the top level domain the length of the

158
00:05:26,400 --> 00:05:27,759
second level domain

159
00:05:27,759 --> 00:05:29,360
the length of the prefix and the number

160
00:05:29,360 --> 00:05:31,600
of levels and then from there

161
00:05:31,600 --> 00:05:33,840
we take all of the candidate families

162
00:05:33,840 --> 00:05:34,639
within that

163
00:05:34,639 --> 00:05:36,800
that leap and we group our domains

164
00:05:36,800 --> 00:05:38,400
according to

165
00:05:38,400 --> 00:05:40,800
the intersection of regular expressions

166
00:05:40,800 --> 00:05:42,560
that are matched

167
00:05:42,560 --> 00:05:45,280
for the domain

168
00:05:45,919 --> 00:05:47,919
so after we've done this this

169
00:05:47,919 --> 00:05:49,199
syntactical grouping

170
00:05:49,199 --> 00:05:51,199
then we're ready to create classifiers

171
00:05:51,199 --> 00:05:52,479
using machine learning

172
00:05:52,479 --> 00:05:55,199
and when we're creating classifiers we

173
00:05:55,199 --> 00:05:57,280
need to be able to

174
00:05:57,280 --> 00:06:00,960
to turn our domain names into a point in

175
00:06:00,960 --> 00:06:02,639
feature space so to do this we have to

176
00:06:02,639 --> 00:06:04,639
quantify everything that we can

177
00:06:04,639 --> 00:06:06,479
about a string here we have a string

178
00:06:06,479 --> 00:06:08,479
that's you know maybe a couple dozen

179
00:06:08,479 --> 00:06:10,479
characters long on average

180
00:06:10,479 --> 00:06:12,000
how is it that we turn that into

181
00:06:12,000 --> 00:06:15,520
something as point and n space

182
00:06:15,520 --> 00:06:17,840
now again we already know that the the

183
00:06:17,840 --> 00:06:19,039
course and fine elements

184
00:06:19,039 --> 00:06:20,639
the syntax for these domains are the

185
00:06:20,639 --> 00:06:22,960
same for everything within the lobe

186
00:06:22,960 --> 00:06:25,120
and we're creating our classifiers

187
00:06:25,120 --> 00:06:27,520
within the lobe

188
00:06:27,520 --> 00:06:30,800
so there are four main types of

189
00:06:30,800 --> 00:06:33,120
features that we extract from a given

190
00:06:33,120 --> 00:06:34,960
domain name

191
00:06:34,960 --> 00:06:36,720
the first are aggregates they're counts

192
00:06:36,720 --> 00:06:39,520
of different different character classes

193
00:06:39,520 --> 00:06:41,360
for the second level domain we consider

194
00:06:41,360 --> 00:06:43,360
counts of character classes like are

195
00:06:43,360 --> 00:06:45,440
they hex characters consonants

196
00:06:45,440 --> 00:06:48,319
vowels do they come from another

197
00:06:48,319 --> 00:06:49,840
language

198
00:06:49,840 --> 00:06:51,599
and then we also look at at some

199
00:06:51,599 --> 00:06:53,039
aggregates of bigrams

200
00:06:53,039 --> 00:06:55,599
namely are there repeated characters

201
00:06:55,599 --> 00:06:56,160
like a

202
00:06:56,160 --> 00:06:59,360
a or yy in the domain name and then we

203
00:06:59,360 --> 00:07:00,639
also look for

204
00:07:00,639 --> 00:07:03,039
how many non-linguistic bi-grams how

205
00:07:03,039 --> 00:07:04,240
many how many

206
00:07:04,240 --> 00:07:06,720
character pairs are there in the domain

207
00:07:06,720 --> 00:07:08,000
name that don't occur

208
00:07:08,000 --> 00:07:10,880
in any indo-european or phenoergic

209
00:07:10,880 --> 00:07:13,360
languages

210
00:07:13,360 --> 00:07:15,360
and then we also look for overall counts

211
00:07:15,360 --> 00:07:17,440
all the the valid punctuation

212
00:07:17,440 --> 00:07:19,520
that's in the domain name including the

213
00:07:19,520 --> 00:07:20,960
top level i'm not the top level the

214
00:07:20,960 --> 00:07:21,919
second level domain

215
00:07:21,919 --> 00:07:26,400
and the prefix and we also look for

216
00:07:26,400 --> 00:07:29,120
the count of rfc 1034 violations which

217
00:07:29,120 --> 00:07:30,319
specifies

218
00:07:30,319 --> 00:07:33,360
the proper syntax for a domain name

219
00:07:33,360 --> 00:07:36,639
so these would be some of those rules

220
00:07:36,639 --> 00:07:38,960
there are only 22 of these buckets which

221
00:07:38,960 --> 00:07:40,319
isn't a whole lot and these

222
00:07:40,319 --> 00:07:43,840
aggregates give us a good idea of some

223
00:07:43,840 --> 00:07:44,560
notion of

224
00:07:44,560 --> 00:07:48,720
of the shape of this uh this domain

225
00:07:48,800 --> 00:07:51,680
next we'll look at n grams and grams are

226
00:07:51,680 --> 00:07:52,639
simply

227
00:07:52,639 --> 00:07:55,680
character counts character pairs and

228
00:07:55,680 --> 00:07:57,680
character triples

229
00:07:57,680 --> 00:08:00,479
now we want to reduce the the set that

230
00:08:00,479 --> 00:08:02,160
we're considering somewhat so we

231
00:08:02,160 --> 00:08:03,520
compress our character pairs and

232
00:08:03,520 --> 00:08:05,919
character triples to 40 just by

233
00:08:05,919 --> 00:08:07,919
taking the alphabetic characters the

234
00:08:07,919 --> 00:08:08,960
digits

235
00:08:08,960 --> 00:08:11,199
and dot dash and underscore and then

236
00:08:11,199 --> 00:08:12,879
putting everything else into the 40th

237
00:08:12,879 --> 00:08:15,120
bucket

238
00:08:15,120 --> 00:08:18,000
so this this is good these these

239
00:08:18,000 --> 00:08:19,280
features are good

240
00:08:19,280 --> 00:08:22,479
um because they they do capture um

241
00:08:22,479 --> 00:08:24,400
linguistic versus non-linguistic

242
00:08:24,400 --> 00:08:26,479
elements you'd imagine that a benign

243
00:08:26,479 --> 00:08:27,360
domain

244
00:08:27,360 --> 00:08:29,360
is generally going to contain some

245
00:08:29,360 --> 00:08:31,280
linguistic elements people want

246
00:08:31,280 --> 00:08:33,519
to associate a domain with something

247
00:08:33,519 --> 00:08:34,719
that they can speak

248
00:08:34,719 --> 00:08:38,159
or understand and these features are

249
00:08:38,159 --> 00:08:39,599
useful in separating

250
00:08:39,599 --> 00:08:42,479
those domains from ones which aren't

251
00:08:42,479 --> 00:08:44,480
it's also intended to catch bias

252
00:08:44,480 --> 00:08:46,399
in the dga's pseudorandom number

253
00:08:46,399 --> 00:08:48,080
generator if if they have some

254
00:08:48,080 --> 00:08:48,880
predilection

255
00:08:48,880 --> 00:08:51,120
for for landing on certain characters or

256
00:08:51,120 --> 00:08:53,279
character pairs

257
00:08:53,279 --> 00:08:55,120
this sort of statistical analysis will

258
00:08:55,120 --> 00:08:56,640
catch that it's the same sort of game

259
00:08:56,640 --> 00:08:57,760
that you play

260
00:08:57,760 --> 00:08:59,920
in uh trying to decode something

261
00:08:59,920 --> 00:09:01,279
cryptographically where you're just

262
00:09:01,279 --> 00:09:01,920
looking at

263
00:09:01,920 --> 00:09:04,399
um sort of the expected frequency of

264
00:09:04,399 --> 00:09:06,640
given characters or character pairs

265
00:09:06,640 --> 00:09:08,959
and then seeing uh what it is that you'd

266
00:09:08,959 --> 00:09:10,640
match

267
00:09:10,640 --> 00:09:13,760
so you know there are 256 ascii

268
00:09:13,760 --> 00:09:16,880
plus characters they're uh if we reduce

269
00:09:16,880 --> 00:09:19,200
these to 40 they're 40 squared and 40 to

270
00:09:19,200 --> 00:09:19,680
the third

271
00:09:19,680 --> 00:09:23,600
buckets for these two and three grams

272
00:09:23,600 --> 00:09:27,519
that can use a bit of space and if you

273
00:09:27,519 --> 00:09:29,279
you know just assume memory is cheap and

274
00:09:29,279 --> 00:09:31,120
you don't care about space it still

275
00:09:31,120 --> 00:09:33,680
presents a problem with overfitting in

276
00:09:33,680 --> 00:09:35,360
in machine learning situations when you

277
00:09:35,360 --> 00:09:36,080
have

278
00:09:36,080 --> 00:09:38,720
a large number of features compared to

279
00:09:38,720 --> 00:09:40,720
your number of training examples

280
00:09:40,720 --> 00:09:41,760
and again we're building this

281
00:09:41,760 --> 00:09:43,680
classifiers per lobe so we've already

282
00:09:43,680 --> 00:09:45,600
kind of filtered our data into smaller

283
00:09:45,600 --> 00:09:46,560
buckets

284
00:09:46,560 --> 00:09:48,240
then we can we can get something that

285
00:09:48,240 --> 00:09:50,000
looks very good in training performs

286
00:09:50,000 --> 00:09:51,519
very well in training but

287
00:09:51,519 --> 00:09:53,440
then when it encounters new data it's

288
00:09:53,440 --> 00:09:55,440
it's over fit to its training data and

289
00:09:55,440 --> 00:09:57,839
it doesn't classify very well

290
00:09:57,839 --> 00:10:00,320
so we need to perform feature selection

291
00:10:00,320 --> 00:10:00,880
on these

292
00:10:00,880 --> 00:10:03,040
these sort of large feature sets to

293
00:10:03,040 --> 00:10:06,160
reduce our overfitting

294
00:10:06,720 --> 00:10:08,480
the next set of features that we

295
00:10:08,480 --> 00:10:10,880
consider are the characters by position

296
00:10:10,880 --> 00:10:13,440
and we do this both forward and backward

297
00:10:13,440 --> 00:10:15,040
through the domain name excluding the

298
00:10:15,040 --> 00:10:16,399
top level domain

299
00:10:16,399 --> 00:10:19,360
so you can imagine bullying slots for

300
00:10:19,360 --> 00:10:21,040
whether a character occurs in a given

301
00:10:21,040 --> 00:10:22,480
position in a string and

302
00:10:22,480 --> 00:10:24,640
and maybe limit the the length of the

303
00:10:24,640 --> 00:10:26,800
domain name to 256.

304
00:10:26,800 --> 00:10:30,240
um you know this this allows us

305
00:10:30,240 --> 00:10:32,480
to catch fixed sub strings in the string

306
00:10:32,480 --> 00:10:34,320
so if we had an a at the beginning of

307
00:10:34,320 --> 00:10:36,480
of every domain of a given family then

308
00:10:36,480 --> 00:10:38,079
that bucket's always going to be filled

309
00:10:38,079 --> 00:10:39,600
or likewise for the backward direction

310
00:10:39,600 --> 00:10:41,680
if an a is always the last character

311
00:10:41,680 --> 00:10:44,720
then that bucket will be filled there

312
00:10:44,720 --> 00:10:47,200
are a number of malware families that

313
00:10:47,200 --> 00:10:50,160
do have fixed drinks relatively fixed at

314
00:10:50,160 --> 00:10:50,720
least

315
00:10:50,720 --> 00:10:53,440
you know only several options maybe

316
00:10:53,440 --> 00:10:54,399
several dozen

317
00:10:54,399 --> 00:10:56,560
and this is these features are very good

318
00:10:56,560 --> 00:10:57,600
for catching

319
00:10:57,600 --> 00:11:01,839
um these sorts of things again

320
00:11:01,839 --> 00:11:04,399
there are a lot of buckets to fill so we

321
00:11:04,399 --> 00:11:06,240
end up with a problem of space

322
00:11:06,240 --> 00:11:09,760
and overfitting the last

323
00:11:09,760 --> 00:11:13,040
set of features that we consider when

324
00:11:13,040 --> 00:11:15,360
when sort of turning a domain into some

325
00:11:15,360 --> 00:11:17,040
point in feature space or word based

326
00:11:17,040 --> 00:11:18,000
features

327
00:11:18,000 --> 00:11:19,600
i mentioned before that linguistic

328
00:11:19,600 --> 00:11:21,519
elements are are somewhat useful in

329
00:11:21,519 --> 00:11:22,959
distinguishing benign and malicious

330
00:11:22,959 --> 00:11:24,000
domains

331
00:11:24,000 --> 00:11:26,079
so we extract word-based features from

332
00:11:26,079 --> 00:11:28,000
both the second level domain and

333
00:11:28,000 --> 00:11:30,320
from the the prefix that stuff that

334
00:11:30,320 --> 00:11:32,560
comes before the second level domain

335
00:11:32,560 --> 00:11:34,800
we look at the overall account of words

336
00:11:34,800 --> 00:11:36,320
in the domain

337
00:11:36,320 --> 00:11:37,839
for a given dictionary for a given

338
00:11:37,839 --> 00:11:40,240
minimum length

339
00:11:40,240 --> 00:11:43,519
i chose english and words greater than

340
00:11:43,519 --> 00:11:44,000
three

341
00:11:44,000 --> 00:11:46,640
in general for my testing we look at the

342
00:11:46,640 --> 00:11:49,040
maximum count of non-overlapping words

343
00:11:49,040 --> 00:11:50,000
so

344
00:11:50,000 --> 00:11:52,320
how many words could be strung together

345
00:11:52,320 --> 00:11:56,079
that don't overlap in the domain

346
00:11:56,079 --> 00:11:59,839
the maximum percentage uh of the domain

347
00:11:59,839 --> 00:12:02,000
comprised of words so if we select

348
00:12:02,000 --> 00:12:04,079
greedily to cover the most of the domain

349
00:12:04,079 --> 00:12:05,760
or the second level domain or the prefix

350
00:12:05,760 --> 00:12:07,360
as possible with words

351
00:12:07,360 --> 00:12:10,399
what percentage is covered by words

352
00:12:10,399 --> 00:12:13,360
and then the length of the longest word

353
00:12:13,360 --> 00:12:14,800
and again we do this for

354
00:12:14,800 --> 00:12:16,639
for both the second level domain and the

355
00:12:16,639 --> 00:12:18,959
prefix

356
00:12:18,959 --> 00:12:21,519
this is really good at classifying

357
00:12:21,519 --> 00:12:23,440
benign versus malicious domains most

358
00:12:23,440 --> 00:12:24,800
people who are registering benign

359
00:12:24,800 --> 00:12:25,920
domains

360
00:12:25,920 --> 00:12:27,680
want you to remember it associated with

361
00:12:27,680 --> 00:12:29,200
some word

362
00:12:29,200 --> 00:12:33,680
whereas dgas typically not so much

363
00:12:33,680 --> 00:12:36,000
it does turn out that it's also good for

364
00:12:36,000 --> 00:12:36,720
finding

365
00:12:36,720 --> 00:12:40,000
word-based dgas because they are

366
00:12:40,000 --> 00:12:42,240
relying specifically on concatenating

367
00:12:42,240 --> 00:12:44,320
words for instance they almost always

368
00:12:44,320 --> 00:12:47,519
have a 100 percent

369
00:12:47,519 --> 00:12:51,200
covered by 100 of the characters are

370
00:12:51,200 --> 00:12:52,480
covered by words

371
00:12:52,480 --> 00:12:54,480
and they also tend to have a very

372
00:12:54,480 --> 00:12:55,680
restricted

373
00:12:55,680 --> 00:12:57,519
maximum account of overlapping words and

374
00:12:57,519 --> 00:12:59,120
they're easy to characterize by these

375
00:12:59,120 --> 00:13:01,600
features

376
00:13:01,760 --> 00:13:04,320
disadvantages are this this takes time

377
00:13:04,320 --> 00:13:04,880
in a

378
00:13:04,880 --> 00:13:07,519
it's a real-time setting it may not seem

379
00:13:07,519 --> 00:13:08,959
like a lot of time to

380
00:13:08,959 --> 00:13:11,440
to calculate these on relatively small

381
00:13:11,440 --> 00:13:12,320
strings but

382
00:13:12,320 --> 00:13:14,480
they're you know sort of order n squared

383
00:13:14,480 --> 00:13:15,839
for both the length of the second level

384
00:13:15,839 --> 00:13:17,920
domain and the length of the prefix

385
00:13:17,920 --> 00:13:20,240
and that's more time than than i want to

386
00:13:20,240 --> 00:13:22,160
spend doing something

387
00:13:22,160 --> 00:13:23,360
and then again this can lead to

388
00:13:23,360 --> 00:13:25,200
overfitting where we might

389
00:13:25,200 --> 00:13:28,000
uh prefer to call something a word-based

390
00:13:28,000 --> 00:13:28,880
dga

391
00:13:28,880 --> 00:13:32,399
when when it's not um you know

392
00:13:32,399 --> 00:13:34,320
in the example that we saw in the dj

393
00:13:34,320 --> 00:13:35,440
archive talk

394
00:13:35,440 --> 00:13:38,000
there were a large number of um supple

395
00:13:38,000 --> 00:13:40,000
box domains that were pre-registered

396
00:13:40,000 --> 00:13:42,240
likely valid and here we would just go

397
00:13:42,240 --> 00:13:43,440
ahead and classify them as

398
00:13:43,440 --> 00:13:47,040
supple box dga and

399
00:13:47,040 --> 00:13:50,320
get false positive so that covers

400
00:13:50,320 --> 00:13:53,040
feature space

401
00:13:53,279 --> 00:13:56,079
which leads us to results how well do we

402
00:13:56,079 --> 00:13:56,399
do

403
00:13:56,399 --> 00:13:59,839
in classification so normally in

404
00:13:59,839 --> 00:14:02,079
machine learning you want to measure

405
00:14:02,079 --> 00:14:03,920
your

406
00:14:03,920 --> 00:14:06,800
your performance by doing some

407
00:14:06,800 --> 00:14:08,320
cross-fold validation where you hold

408
00:14:08,320 --> 00:14:10,000
some data elements out

409
00:14:10,000 --> 00:14:11,519
that you don't train with and then you

410
00:14:11,519 --> 00:14:13,839
see how well it performs on those

411
00:14:13,839 --> 00:14:16,079
and we usually grade by precision and

412
00:14:16,079 --> 00:14:17,040
recall and

413
00:14:17,040 --> 00:14:20,000
precision is our you can think of it as

414
00:14:20,000 --> 00:14:21,600
our false positive rate

415
00:14:21,600 --> 00:14:24,480
how how often do we predict something as

416
00:14:24,480 --> 00:14:25,680
a class that it's not

417
00:14:25,680 --> 00:14:27,600
and then recall is our false negative

418
00:14:27,600 --> 00:14:29,760
rate how well do we catch the things

419
00:14:29,760 --> 00:14:31,680
that we're intending to catch

420
00:14:31,680 --> 00:14:35,120
so um you know we do fairly well we're

421
00:14:35,120 --> 00:14:35,839
trying to

422
00:14:35,839 --> 00:14:37,760
in in our sort of overall goals we're

423
00:14:37,760 --> 00:14:39,279
trying to maximize

424
00:14:39,279 --> 00:14:41,839
the precision for benign classifications

425
00:14:41,839 --> 00:14:42,880
we don't want

426
00:14:42,880 --> 00:14:46,399
to say that something is malicious

427
00:14:46,399 --> 00:14:50,079
when it's not now doing

428
00:14:50,079 --> 00:14:51,920
domains one at a time and trying to use

429
00:14:51,920 --> 00:14:54,480
this as an oracle to determine whether

430
00:14:54,480 --> 00:14:57,920
a host is infected or not that number's

431
00:14:57,920 --> 00:15:00,240
still not high enough because

432
00:15:00,240 --> 00:15:02,880
you're going to request many domains per

433
00:15:02,880 --> 00:15:03,440
day

434
00:15:03,440 --> 00:15:06,800
and and even a small false positive rate

435
00:15:06,800 --> 00:15:07,519
is going to

436
00:15:07,519 --> 00:15:10,839
to flag some things as dgas which are

437
00:15:10,839 --> 00:15:12,160
not

438
00:15:12,160 --> 00:15:13,360
well how well does this do at

439
00:15:13,360 --> 00:15:16,079
classifying different malware families

440
00:15:16,079 --> 00:15:18,240
um you can see that for some of these

441
00:15:18,240 --> 00:15:20,000
families it does quite well

442
00:15:20,000 --> 00:15:23,760
um and and those tend to have

443
00:15:23,760 --> 00:15:27,279
a distinctive syntax even

444
00:15:27,279 --> 00:15:29,120
even domains that we didn't have a

445
00:15:29,120 --> 00:15:30,880
number of training examples for

446
00:15:30,880 --> 00:15:34,079
like expero it has a

447
00:15:34,079 --> 00:15:36,320
consonant vowel consonant val dash

448
00:15:36,320 --> 00:15:37,519
consonant vowel

449
00:15:37,519 --> 00:15:39,920
pattern so it's pretty easy to pick up

450
00:15:39,920 --> 00:15:42,160
by classification

451
00:15:42,160 --> 00:15:43,600
there are some things that we didn't do

452
00:15:43,600 --> 00:15:45,680
well with and

453
00:15:45,680 --> 00:15:48,800
it seems that that's largely a case of

454
00:15:48,800 --> 00:15:51,680
of data imbalances where we have much

455
00:15:51,680 --> 00:15:54,639
more data in given lobes for one family

456
00:15:54,639 --> 00:15:56,160
than another and we just end up

457
00:15:56,160 --> 00:15:58,079
preferring that family regardless of how

458
00:15:58,079 --> 00:16:00,719
we weight our class

459
00:16:03,600 --> 00:16:06,800
so this leads us to several conclusions

460
00:16:06,800 --> 00:16:08,320
syntactical rules have helped our

461
00:16:08,320 --> 00:16:10,320
classification immensely we do a lot

462
00:16:10,320 --> 00:16:11,040
better

463
00:16:11,040 --> 00:16:12,720
with the pre-filtering into leaves and

464
00:16:12,720 --> 00:16:16,560
lobes of syntactical rules

465
00:16:16,959 --> 00:16:20,000
having unbalanced data it hurts that

466
00:16:20,000 --> 00:16:21,199
when we don't have enough training

467
00:16:21,199 --> 00:16:22,800
examples for a given family

468
00:16:22,800 --> 00:16:25,279
that we typically won't classify it very

469
00:16:25,279 --> 00:16:27,120
well

470
00:16:27,120 --> 00:16:30,240
and that the results look good on paper

471
00:16:30,240 --> 00:16:32,399
but when we put it into practice and

472
00:16:32,399 --> 00:16:34,240
hp's corporate environment we're looking

473
00:16:34,240 --> 00:16:34,560
at

474
00:16:34,560 --> 00:16:37,279
two billion domains a day or so it

475
00:16:37,279 --> 00:16:38,720
doesn't do as well

476
00:16:38,720 --> 00:16:42,959
in particular it will flag valid domains

477
00:16:42,959 --> 00:16:46,719
as word-based dgas

478
00:16:47,519 --> 00:16:49,199
we have noticed that some features are

479
00:16:49,199 --> 00:16:50,720
particularly good

480
00:16:50,720 --> 00:16:54,480
at classifying dgas

481
00:16:54,480 --> 00:16:55,920
the aggregates that i mentioned in the

482
00:16:55,920 --> 00:16:57,759
beginning are

483
00:16:57,759 --> 00:17:01,680
few and powerful looking at the

484
00:17:01,680 --> 00:17:04,959
most frequent linguistic bi-grams and

485
00:17:04,959 --> 00:17:07,839
and the the opposite those bi-grams and

486
00:17:07,839 --> 00:17:09,439
trigrams which don't appear in any

487
00:17:09,439 --> 00:17:10,959
language we know

488
00:17:10,959 --> 00:17:14,480
is also a very useful set of features

489
00:17:14,480 --> 00:17:16,160
and then finally the word-based features

490
00:17:16,160 --> 00:17:19,839
give us a lot of power

491
00:17:20,000 --> 00:17:20,959
another thing just through

492
00:17:20,959 --> 00:17:22,559
experimentation not really in the in the

493
00:17:22,559 --> 00:17:24,640
scope of this paper or talk but

494
00:17:24,640 --> 00:17:26,799
in order to get past the false positives

495
00:17:26,799 --> 00:17:28,480
for the word based

496
00:17:28,480 --> 00:17:31,280
dga predictions if we if we were to hash

497
00:17:31,280 --> 00:17:32,559
the words rather than

498
00:17:32,559 --> 00:17:35,600
sort of pre-storing a dictionary that

499
00:17:35,600 --> 00:17:37,200
maps the words one-to-one in feature

500
00:17:37,200 --> 00:17:38,559
space but just

501
00:17:38,559 --> 00:17:40,960
compress the words into some other parts

502
00:17:40,960 --> 00:17:42,000
of feature space

503
00:17:42,000 --> 00:17:44,400
we can we can also reduce our false

504
00:17:44,400 --> 00:17:46,960
positives

505
00:17:47,679 --> 00:17:50,320
again this isn't a standalone solution

506
00:17:50,320 --> 00:17:50,799
this is

507
00:17:50,799 --> 00:17:53,840
just answering the problem of how it is

508
00:17:53,840 --> 00:17:55,200
that we could build

509
00:17:55,200 --> 00:17:58,080
as good a domain classifier one domain

510
00:17:58,080 --> 00:17:59,600
at a time as possible

511
00:17:59,600 --> 00:18:01,919
and that we'd want to while we do use

512
00:18:01,919 --> 00:18:03,840
this information as inputs for other

513
00:18:03,840 --> 00:18:06,000
classifiers that then help us identify

514
00:18:06,000 --> 00:18:08,880
infected hosts

515
00:18:10,080 --> 00:18:13,200
that's all i got how about y'all

516
00:18:13,200 --> 00:18:15,840
thank you

517
00:18:19,520 --> 00:18:23,200
okay we have time for one question

518
00:18:23,200 --> 00:18:29,840
or two very quick questions

519
00:18:32,320 --> 00:18:37,360
one two three okay thank you

520
00:18:40,080 --> 00:18:42,480
thanks

